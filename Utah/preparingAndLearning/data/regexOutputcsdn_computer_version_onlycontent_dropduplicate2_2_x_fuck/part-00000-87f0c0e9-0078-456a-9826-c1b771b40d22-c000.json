{"content2":"提到人工智能，大多数人的第一反应就是距离我们太远了。智能机器人、无人驾驶，这些好像都是未来式。我们一直以来都在告诉大家，人工智能其实就在我们身边。\n比如，应用最广的美颜自拍，更准确的说，是人像处理。\n现在的人像软件之所以能帮助人们从繁琐的PS中解放出来，就是因为利用了大量计算机视觉技术，像是人脸定点识别、图像分割、边缘融合等等。在美颜、抠图、结构光等等有趣玩法的背后，其实是算法和大数据的加持。很多人像处理软件，本质上是AI产品。\n今天就以天天P图为例，看看是什么让他们成了一款AI软件。\nAI赋能，让手机如何读懂你的脸\n人像处理软件之所以能成为AI产品，是因为有了大量图片数据，尤其是人脸数据的累积。而通过大量图片数据进行训练，可以给AI累积各种能力。\n以天天P图的自动美颜功能为例，软件之所以能放大眼睛、添加贴图动效，是因为准确的找到了人脸和五官的位置。这里面应用到的技术是优图的人脸配准，通过摄像头采集图像，基于能够在移动端运行的深度神经网络，实时得从图像中提取人脸特征，完成人脸检测，五官定位和实时追踪。\n在每帧图像中准确的找到人脸和五官后，就可以“加特效”了——增加美妆、萌宠贴图，自然美妆。\n除了对人脸的识别和处理，为了给用户提供更多丰富智能的玩法，P图团队还联合优图团队对视频流进行了手部的识别。这里深度学习就能发挥很大作用，优图团队研发了一套实时、精准的手势跟踪和识别技术。这套解决方案可以在移动端实现高效的手势检测、实时的手势跟踪以及超过十种以上手势的精准识别。依靠这些能力集群，P图可以在用户自拍时识别用户的手势，实时触发一些手势动效，控制相机拍照。极大地增强了产品的趣味性。\n背景分割也是另一项基于AI的创造性玩法，通过深度优化加速后的神经网络，使得P图可以在移动端实现对人像的实时分割操作，区分出前背景，基于前背景信息，可以实时将用户背景替换成火星表面，海底世界等丰富的场景，或者把背景虚化，突出人物，增强趣味性。\n打造图像处理云，美颜AI\n能做到的不仅仅是变脸\n美颜AI能做到的不仅仅是变脸。\n在大多数人的印象中，天天P图这类人像处理软件即使有AI技术，基本也是应用于自己的产品之中，缺乏进一步的算法变现能力。但如果如果玩过今年特别火的军装H5，就已经体验过天天P图的云端AI能力。\n细心的人会发现，军装H5并非在终端上进行运算，而是通过H5上传到云端处理。基于云端的人脸识别，五官定位，人脸融合能力，P图在云端完成了海量的活动照片处理，提供了一周内10亿+人次参与的运营活动，还申请了吉尼斯世界纪录。\n除了家喻户晓的军装照，天天P图最近推出的萌偶功能也利用了AI图像处理云。\n通过在云端的神经网络，找到与用户五官相似度最高的卡通素材。建立标准人脸和标准卡通人脸间的映射关系，再把用户的人脸特征放大映射到卡通人脸中，就形成了和用户五官非常相像的卡通形象。萌偶功能的背后，体现了不少技术优势：1.以神经网络的技术将自拍自动转化为卡通形象；2.通过与亚洲标准人脸的对比，强化卡通形象的五官特征；3.通过卡通形象生成带序列帧变化的动图表情包。\n这些能够提供丰富玩法的AI图像处理云，也解决了深度神经网络模型可能过大，无法在终端运行的问题。天天P图技术团队在腾讯云上部署了基于GPU集群，这也是腾讯最早2C提供服务的GPU集群。基于云端的GPU服务器，让云端帮助手机实现原先不能承受的复杂AI深度神经网络计算，为用户提供了基于AI能力的更为趣味性的玩法。\n强大的分布式部署能力降低了客户端的门槛，使得算法可以配适各种环境：手机、电脑、电视、App、H5……有了图像处理云，天天P图的变脸玩法可以不局限于自己的产品上，而是可以方便各项产品打造出病毒式的营销玩法，甚至向照片后期、影视后期等等行业输出自己的神经网络模型和计算能力。\n作为用户可能很难明确感受到图像处理云的存在，但这项能力却为天天P图打开了更多依靠AI创造营收的路径。\n从隐性到显性，人像处理AI\n可能出现在线下场景吗？\n如果说App端功能和云端的智能图像处理是AI的隐性体现，那天天P图推出的智能硬件则是AI的显性体现。\n提到人像处理的智能硬件，可能很多人的第一反应是相机或手机，可这两个领域是出了名的红海。不如换个角度想想，当手机和相机还没那么普及的时候，我们是如何拍照的？\n当然是大头贴机啦！\n来自于台湾、日本的大头贴机曾经也在中国风靡了一阵，只是如今渐渐落寞。其实在日本，大头贴已经形成了一种文化，机器内置的人像处理算法会把人脸处理成巴掌脸大眼睛的夸张模样，很受年轻女性欢迎。\n现在天天P图正在尝试让大陆的大头贴热“回潮”。天天P图大头贴机将AI技术移植到了传统的大头贴机器中，给用户提供了多种有趣的拍照玩法。尤其是依靠强大的人脸识别和手势识别能力，让用户可以通过手势和表情控制拍摄内容。把原来在手机端的趣味体验移植到线下场景中，极大的放大了AI功能的趣味性。\n天天P图在智能硬件方面的尝试正在让人像处理AI更加可知可感，以后要再有厂商推出智能音箱类产品，或许可以换个角度思考，图像处理其实和语音能力一样重要。\n写在最后\n看到这里我们就能发现，光是一款小小的美颜软件中都能找到很多AI相关技术的身影。\n在天天P图这些看似简单应用场景的背后，是对技术和技术背后想象空间的考量。像能对人脸进行精准的识别，就意味着技术可以被应用在安防、身份认证等等多个场景中。学会识别手势也不仅仅为了放上贴纸，而是给了日后用手势进行游戏、甚至把手语翻译成文字的可能。和智能硬件的结合，证明生活中还会有无数线下场景可以应用到人像处理的算法算力。这些隐藏在人像处理背后的场景，像是一座座宝藏。\n拍照或许是件小事，却能从中窥视到关于AI的无尽变量。"}
{"content2":"计算机视觉领域经典论文源码\n在读一些大牛的论文后，总是想找些代码读一读，可是查找代码资源是如此的痛苦，经过一番请教和查找，将比较好的资源贴出来，方便大家使用，希望大家有什么更好的资源也能分享出来，可以贴在留言qu\n2016-CVPR论文代码资源：\nhttps://tensortalk.com/?cat=conference-cvpr-2016\n\n一个GitHub账号，里面有很多计算机视觉领域最新论文的代码实现：\nhttps://github.com/kjw0612/awesome-deep-vision\n\n\nType\nTopic\nName\nReference\nLink\nCode\nStructure from motion\nlibmv\nhttp://code.google.com/p/libmv/\nCode\nDimension Reduction\nLLE\nhttp://www.cs.nyu.edu/~roweis/lle/code.html\nCode\nClustering\nSpectral Clustering - UCSD Project\nhttp://vision.ucsd.edu/~sagarwal/spectral-0.2.tgz\nCode\nClustering\nK-Means 323个Item- Oxford Code\nhttp://www.cs.ucf.edu/~vision/Code/vggkmeans.zip\nCode\nImage Deblurring\nNon-blind deblurring (and blind denoising) with integrated noise estimation\nU. Schmidt, K. Schelten, and S. Roth. Bayesian deblurring with integrated noise estimation, CVPR 2011\nhttp://www.gris.tu-darmstadt.de/research/visinf/software/index.en.htm\nCode\nStructure from motion\nStructure from Motion toolbox for Matlab by Vincent Rabaud\nhttp://code.google.com/p/vincents-structure-from-motion-matlab-toolbox/\nCode\nMultiple View Geometry\nMatlab Functions for Multiple View Geometry\nhttp://www.robots.ox.ac.uk/~vgg/hzbook/code/\nCode\nObject Detection\nMax-Margin Hough Transform\nS. Maji and J. Malik, Object Detection Using a Max-Margin Hough Transform. CVPR 2009\nhttp://www.cs.berkeley.edu/~smaji/projects/max-margin-hough/\nCode\nImage Segmentation\nSLIC Superpixels\nR. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk, SLIC Superpixels, EPFL Technical Report, 2010\nhttp://ivrg.epfl.ch/supplementary_material/RK_SLICSuperpixels/index.html\nCode\nVisual Tracking\nTracking using Pixel-Wise Posteriors\nC. Bibby and I. Reid, Tracking using Pixel-Wise Posteriors, ECCV 2008\nhttp://www.robots.ox.ac.uk/~cbibby/research_pwp.shtml\nCode\nVisual Tracking\nVisual Tracking with Histograms and Articulating Blocks\nS. M. Shshed Nejhum, J. Ho, and M.-H.Yang, Visual Tracking with Histograms and Articulating Blocks, CVPR 2008\nhttp://www.cise.ufl.edu/~smshahed/tracking.htm\nCode\nSparse Representation\nRobust Sparse Coding for Face Recognition\nM. Yang, L. Zhang, J. Yang and D. Zhang, “Robust Sparse Coding for Face Recognition,” CVPR 2011\nhttp://www4.comp.polyu.edu.hk/~cslzhang/code/RSC.zip\nCode\nFeature Detection andFeature Extraction\nGroups of Adjacent Contour Segments\nV. Ferrari, L. Fevrier, F. Jurie, and C. Schmid, Groups of Adjacent Contour Segments for Object Detection, PAMI, 2007\nhttp://www.robots.ox.ac.uk/~vgg/share/ferrari/release-kas-v102.tgz\nCode\nDensity Estimation\nKernel Density Estimation Toolbox\nhttp://www.ics.uci.edu/~ihler/code/kde.html\nCode\nIllumination, Reflectance, and Shadow\nGround shadow detection\nJ.-F. Lalonde, A. A. Efros, S. G. Narasimhan, Detecting Ground Shadowsin Outdoor Consumer Photographs, ECCV 2010\nhttp://www.jflalonde.org/software.html#shadowDetection\nCode\nImage Denoising,Image Super-resolution, andImage Deblurring\nLearning Models of Natural Image Patches\nD. Zoran and Y. Weiss, From Learning Models of Natural Image Patches to Whole Image Restoration, ICCV, 2011\nhttp://www.cs.huji.ac.il/~daniez/\nCode\nIllumination, Reflectance, and Shadow\nEstimating Natural Illumination from a Single Outdoor Image\nJ-F. Lalonde, A. A. Efros, S. G. Narasimhan, Estimating Natural Illumination from a Single Outdoor Image , ICCV 2009\nhttp://www.cs.cmu.edu/~jlalonde/software.html#skyModel\nCode\nVisual Tracking\nLucas-Kanade affine template tracking\nS. Baker and I. Matthews, Lucas-Kanade 20 Years On: A Unifying Framework, IJCV 2002\nhttp://www.mathworks.com/matlabcentral/fileexchange/24677-lucas-kanade-affine-template-tracking\nCode\nSaliency Detection\nSaliency-based video segmentation\nK. Fukuchi, K. Miyazato, A. Kimura, S. Takagi and J. Yamato, Saliency-based video segmentation with graph cuts and sequentially updated priors, ICME 2009\nhttp://www.brl.ntt.co.jp/people/akisato/saliency3.html\nCode\nDimension Reduction\nLaplacian Eigenmaps\nhttp://www.cse.ohio-state.edu/~mbelkin/algorithms/Laplacian.tar\nCode\nIllumination, Reflectance, and Shadow\nWhat Does the Sky Tell Us About the Camera?\nJ-F. Lalonde, S. G. Narasimhan, A. A. Efros, What Does the Sky Tell Us About the Camera?, ECCV 2008\nhttp://www.cs.cmu.edu/~jlalonde/software.html#skyModel\nCode\nImage Filtering\nSVM for Edge-Preserving Filtering\nQ. Yang, S. Wang, and N. Ahuja, SVM for Edge-Preserving Filtering, CVPR 2010\nhttp://vision.ai.uiuc.edu/~qyang6/publications/code/cvpr-10-svmbf/program_video_conferencing.zip\nCode\nImage Segmentation\nRecovering Occlusion Boundaries from a Single Image\nD. Hoiem, A. Stein, A. A. Efros, M. Hebert, Recovering Occlusion Boundaries from a Single Image, ICCV 2007.\nhttp://www.cs.cmu.edu/~dhoiem/software/\nCode\nVisual Tracking\nVisual Tracking Decomposition\nJ Kwon and K. M. Lee, Visual Tracking Decomposition, CVPR 2010\nhttp://cv.snu.ac.kr/research/~vtd/\nCode\nVisual Tracking\nGPU Implementation of Kanade-Lucas-Tomasi Feature Tracker\nS. N Sinha, J.-M. Frahm, M. Pollefeys and Y. Genc, Feature Tracking and Matching in Video Using Programmable Graphics Hardware, MVA, 2007\nhttp://cs.unc.edu/~ssinha/Research/GPU_KLT/\nCode\nObject Detection\nRecognition using regions\nC. Gu, J. J. Lim, P. Arbelaez, and J. Malik, CVPR 2009\nhttp://www.cs.berkeley.edu/~chunhui/publications/cvpr09_v2.zip\nCode\nSaliency Detection\nSaliency Using Natural statistics\nL. Zhang, M. Tong, T. Marks, H. Shan, and G. Cottrell. Sun: A bayesian framework for saliency using natural statistics. Journal of Vision, 2008\nhttp://cseweb.ucsd.edu/~l6zhang/\nCode\nImage Filtering\nLocal Laplacian Filters\nS. Paris, S. Hasinoff, J. Kautz, Local Laplacian Filters: Edge-Aware Image Processing with a Laplacian Pyramid, SIGGRAPH 2011\nhttp://people.csail.mit.edu/sparis/publi/2011/siggraph/matlab_source_code.zip\nCode\nCommon Visual Pattern Discovery\nSketching the Common\nS. Bagon, O. Brostovsky, M. Galun and M. Irani, Detecting and Sketching the Common, CVPR 2010\nhttp://www.wisdom.weizmann.ac.il/~bagon/matlab_code/SketchCommonCVPR10_v1.1.tar.gz\nCode\nImage Denoising\nBLS-GSM\nhttp://decsai.ugr.es/~javier/denoise/\nCode\nCamera Calibration\nEpipolar Geometry Toolbox\nG.L. Mariottini, D. Prattichizzo, EGT: a Toolbox for Multiple View Geometry and Visual Servoing, IEEE Robotics & Automation Magazine, 2005\nhttp://egt.dii.unisi.it/\nCode\nDepth Sensor\nKinect SDK\nhttp://www.microsoft.com/en-us/kinectforwindows/\nhttp://www.microsoft.com/en-us/kinectforwindows/\nCode\nImage Super-resolution\nSelf-Similarities for Single Frame Super-Resolution\nC.-Y. Yang, J.-B. Huang, and M.-H. Yang, Exploiting Self-Similarities for Single Frame Super-Resolution, ACCV 2010\nhttps://eng.ucmerced.edu/people/cyang35/ACCV10.zip\nCode\nImage Denoising\nGaussian Field of Experts\nhttp://www.cs.huji.ac.il/~yweiss/BRFOE.zip\nCode\nObject Detection\nPoselet\nL. Bourdev, J. Malik, Poselets: Body Part Detectors Trained Using 3D Human Pose Annotations, ICCV 2009\nhttp://www.eecs.berkeley.edu/~lbourdev/poselets/\nCode\nKernels and Distances\nEfficient Earth Mover’s Distance with L1 Ground Distance (EMD_L1)\nH. Ling and K. Okada, An Efficient Earth Mover’s Distance Algorithm for Robust Histogram Comparison, PAMI 2007\nhttp://www.dabi.temple.edu/~hbling/code/EmdL1_v3.zip\nCode\nNearest Neighbors Matching\nSpectral Hashing\nY. Weiss, A. Torralba, R. Fergus, Spectral Hashing, NIPS 2008\nhttp://www.cs.huji.ac.il/~yweiss/SpectralHashing/\nCode\nImage Denoising\nField of Experts\nhttp://www.cs.brown.edu/~roth/research/software.html\nCode\nImage Segmentation\nMultiscale Segmentation Tree\nE. Akbas and N. Ahuja, “From ramp discontinuities to segmentation tree,” ACCV 2009 andN. Ahuja, “A Transform for Multiscale Image Segmentation by Integrated Edge and Region Detection,” PAMI 1996\nhttp://vision.ai.uiuc.edu/segmentation\nCode\nMultiple Instance Learning\nMILIS\nZ. Fu, A. Robles-Kelly, and J. Zhou, MILIS: Multiple instance learning with instance selection, PAMI 2010\nCode\nNearest Neighbors Matching\nFLANN: Fast Library for Approximate Nearest Neighbors\nhttp://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN\nCode\nFeature Detection andFeature Extraction\nMaximally stable extremal regions (MSER) - VLFeat\nJ. Matas, O. Chum, M. Urba, and T. Pajdla. Robust wide baseline stereo from maximally stable extremal regions. BMVC, 2002\nhttp://www.vlfeat.org/\nCode\nAlpha Matting\nSpectral Matting\nA. Levin, A. Rav-Acha, D. Lischinski. Spectral Matting. PAMI 2008\nhttp://www.vision.huji.ac.il/SpectralMatting/\nCode\nMulti-View Stereo\nPatch-based Multi-view Stereo Software\nY. Furukawa and J. Ponce, Accurate, Dense, and Robust Multi-View Stereopsis, PAMI 2009\nhttp://grail.cs.washington.edu/software/pmvs/\nCode\nClustering\nSelf-Tuning Spectral Clustering\nhttp://www.vision.caltech.edu/lihi/Demos/SelfTuningClustering.html\nCode\nFeature Extraction andObject Detection\nHistogram of Oriented Graidents - OLT for windows\nN. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005\nhttp://www.computing.edu.au/~12482661/hog.html\nCode\nImage Understanding\nNonparametric Scene Parsing via Label Transfer\nC. Liu, J. Yuen, and Antonio Torralba, Nonparametric Scene Parsing via Label Transfer, PAMI 2011\nhttp://people.csail.mit.edu/celiu/LabelTransfer/index.html\nCode\nMultiple Kernel Learning\nDOGMA\nF. Orabona, L. Jie, and B. Caputo. Online-batch strongly convex multi kernel learning. CVPR, 2010\nhttp://dogma.sourceforge.net/\nCode\nDistance Metric Learning\nMatlab Toolkit for Distance Metric Learning\nhttp://www.cs.cmu.edu/~liuy/distlearn.htm\nCode\nOptical Flow\nBlack and Anandan’s Optical Flow\nhttp://www.cs.brown.edu/~dqsun/code/ba.zip\nCode\nText Recognition\nText recognition in the wild\nK. Wang, B. Babenko, and S. Belongie, End-to-end Scene Text Recognition, ICCV 2011\nhttp://vision.ucsd.edu/~kai/grocr/\nCode\nMRF Optimization\nMRF Minimization Evaluation\nR. Szeliski et al., A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors, PAMI, 2008\nhttp://vision.middlebury.edu/MRF/\nCode\nSaliency Detection\nContext-aware saliency detection\nS. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware saliency detection. In CVPR, 2010.\nhttp://webee.technion.ac.il/labs/cgm/Computer-Graphics-Multimedia/Software/Saliency/Saliency.html\nCode\nSaliency Detection\nLearning to Predict Where Humans Look\nT. Judd and K. Ehinger and F. Durand and A. Torralba, Learning to Predict Where Humans Look, ICCV, 2009\nhttp://people.csail.mit.edu/tjudd/WherePeopleLook/index.html\nCode\nStereo\nStereo Evaluation\nD. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms, IJCV 2001\nhttp://vision.middlebury.edu/stereo/\nCode\nImage Segmentation\nQuick-Shift\nA. Vedaldi and S. Soatto, Quick Shift and Kernel Methodsfor Mode Seeking, ECCV, 2008\nhttp://www.vlfeat.org/overview/quickshift.html\nCode\nSaliency Detection\nGraph-based visual saliency\nJ. Harel, C. Koch, and P. Perona. Graph-based visual saliency. NIPS, 2007\nhttp://www.klab.caltech.edu/~harel/share/gbvs.php\nCode\nClustering\nK-Means - VLFeat\nhttp://www.vlfeat.org/\nCode\nObject Detection\nA simple object detector with boosting\nICCV 2005 short courses on Recognizing and Learning Object Categories\nhttp://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html\nCode\nImage Quality Assessment\nStructural SIMilarity\nhttps://ece.uwaterloo.ca/~z70wang/research/ssim/\nCode\nStructure from motion\nFIT3D\nhttp://www.fit3d.info/\nCode\nImage Denoising\nBM3D\nhttp://www.cs.tut.fi/~foi/GCF-BM3D/\nCode\nSaliency Detection\nDiscriminant Saliency for Visual Recognition from Cluttered Scenes\nD. Gao and N. Vasconcelos, Discriminant Saliency for Visual Recognition from Cluttered Scenes, NIPS, 2004\nhttp://www.svcl.ucsd.edu/projects/saliency/\nCode\nImage Denoising\nNonlocal means with cluster trees\nT. Brox, O. Kleinschmidt, D. Cremers, Efficient nonlocal means for denoising of textural patterns, TIP 2008\nhttp://lmb.informatik.uni-freiburg.de/resources/binaries/nlmeans_brox_tip08Linux64.zip\nCode\nSaliency Detection\nGlobal Contrast based Salient Region Detection\nM.-M. Cheng, G.-X. Zhang, N. J. Mitra, X. Huang, S.-M. Hu. Global Contrast based Salient Region Detection. CVPR, 2011\nhttp://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/\nCode\nVisual Tracking\nMotion Tracking in Image Sequences\nC. Stauffer and W. E. L. Grimson. Learning patterns of activity using real-time tracking, PAMI, 2000\nhttp://www.cs.berkeley.edu/~flw/tracker/\nCode\nSaliency Detection\nItti, Koch, and Niebur’ saliency detection\nL. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, 1998\nhttp://www.saliencytoolbox.net/\nCode\nFeature Detection,Feature Extraction, andAction Recognition\nSpace-Time Interest Points (STIP)\nI. Laptev, On Space-Time Interest Points, IJCV, 2005 and I. Laptev and T. Lindeberg, On Space-Time Interest Points, IJCV 2005\nhttp://www.irisa.fr/vista/Equipe/People/Laptev/download/stip-1.1-winlinux.zipandhttp://www.nada.kth.se/cvap/abstracts/cvap284.html\nCode\nTexture Synthesis\nImage Quilting for Texture Synthesis and Transfer\nA. A. Efros and W. T. Freeman, Image Quilting for Texture Synthesis and Transfer, SIGGRAPH 2001\nhttp://www.cs.cmu.edu/~efros/quilt_research_code.zip\nCode\nImage Denoising\nNon-local Means\nhttp://dmi.uib.es/~abuades/codis/NLmeansfilter.m\nCode\nLow-Rank Modeling\nTILT: Transform Invariant Low-rank Textures\nZ. Zhang, A. Ganesh, X. Liang, and Y. Ma, TILT: Transform Invariant Low-rank Textures, IJCV 2011\nhttp://perception.csl.uiuc.edu/matrix-rank/tilt.html\nCode\nObject Proposal\nObjectness measure\nB. Alexe, T. Deselaers, V. Ferrari, What is an Object?, CVPR 2010\nhttp://www.vision.ee.ethz.ch/~calvin/objectness/objectness-release-v1.01.tar.gz\nCode\nImage Filtering\nReal-time O(1) Bilateral Filtering\nQ. Yang, K.-H. Tan and N. Ahuja, Real-time O(1) Bilateral Filtering, CVPR 2009\nhttp://vision.ai.uiuc.edu/~qyang6/publications/code/qx_constant_time_bilateral_filter_ss.zip\nCode\nImage Quality Assessment\nSPIQA\nhttp://vision.ai.uiuc.edu/~bghanem2/shared_code/SPIQA_code.zip\nCode\nObject Recognition\nBiologically motivated object recognition\nT. Serre, L. Wolf and T. Poggio. Object recognition with features inspired by visual cortex, CVPR 2005\nhttp://cbcl.mit.edu/software-datasets/standardmodel/index.html\nCode\nIllumination, Reflectance, and Shadow\nShadow Detection using Paired Region\nR. Guo, Q. Dai and D. Hoiem, Single-Image Shadow Detection and Removal using Paired Regions, CVPR 2011\nhttp://www.cs.illinois.edu/homes/guo29/projects/shadow.html\nCode\nIllumination, Reflectance, and Shadow\nReal-time Specular Highlight Removal\nQ. Yang, S. Wang and N. Ahuja, Real-time Specular Highlight Removal Using Bilateral Filtering, ECCV 2010\nhttp://www.cs.cityu.edu.hk/~qiyang/publications/code/eccv-10.zip\nCode\nMRF Optimization\nMax-flow/min-cut\nY. Boykov and V. Kolmogorov, An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision, PAMI 2004\nhttp://vision.csd.uwo.ca/code/maxflow-v3.01.zip\nCode\nOptical Flow\nOptical Flow Evaluation\nS. Baker et al. A Database and Evaluation Methodology for Optical Flow, IJCV, 2011\nhttp://vision.middlebury.edu/flow/\nCode\nImage Super-resolution\nMRF for image super-resolution\nW. T Freeman and C. Liu. Markov Random Fields for Super-resolution and Texture Synthesis. In A. Blake, P. Kohli, and C. Rother, eds., Advances in Markov Random Fields for Vision and Image Processing, Chapter 10. MIT Press, 2011\nhttp://people.csail.mit.edu/billf/project%20pages/sresCode/Markov%20Random%20Fields%20for%20Super-Resolution.html\nCode\nMRF Optimization\nPlanar Graph Cut\nF. R. Schmidt, E. Toppe and D. Cremers, Efﬁcient Planar Graph Cuts with Applications in Computer Vision, CVPR 2009\nhttp://vision.csd.uwo.ca/code/PlanarCut-v1.0.zip\nCode\nObject Detection\nFeature Combination\nP. Gehler and S. Nowozin, On Feature Combination for Multiclass Object Detection, ICCV, 2009\nhttp://www.vision.ee.ethz.ch/~pgehler/projects/iccv09/index.html\nCode\nStructure from motion\nVisualSFM : A Visual Structure from Motion System\nhttp://www.cs.washington.edu/homes/ccwu/vsfm/\nCode\nNearest Neighbors Matching\nANN: Approximate Nearest Neighbor Searching\nhttp://www.cs.umd.edu/~mount/ANN/\nCode\nSaliency Detection\nLearning Hierarchical Image Representation with Sparsity, Saliency and Locality\nJ. Yang and M.-H. Yang, Learning Hierarchical Image Representation with Sparsity, Saliency and Locality, BMVC 2011\nCode\nOptical Flow\nOptical Flow by Deqing Sun\nD. Sun, S. Roth, M. J. Black, Secrets of Optical Flow Estimation and Their Principles, CVPR, 2010\nhttp://www.cs.brown.edu/~dqsun/code/flow_code.zip\nCode\nImage Understanding\nDiscriminative Models for Multi-Class Object Layout\nC. Desai, D. Ramanan, C. Fowlkes. “Discriminative Models for Multi-Class Object Layout, IJCV 2011\nhttp://www.ics.uci.edu/~desaic/multiobject_context.zip\nCode\nGraph Matching\nHyper-graph Matching via Reweighted Random Walks\nJ. Lee, M. Cho, K. M. Lee. “Hyper-graph Matching via Reweighted Random Walks”, CVPR 2011\nhttp://cv.snu.ac.kr/research/~RRWHM/\nCode\nObject Detection\nHough Forests for Object Detection\nJ. Gall and V. Lempitsky, Class-Speciﬁc Hough Forests for Object Detection, CVPR, 2009\nhttp://www.vision.ee.ethz.ch/~gallju/projects/houghforest/index.html\nCode\nObject Discovery\nUsing Multiple Segmentations to Discover Objects and their Extent in Image Collections\nB. Russell, A. A. Efros, J. Sivic, W. T. Freeman, A. Zisserman, Using Multiple Segmentations to Discover Objects and their Extent in Image Collections, CVPR 2006\nhttp://people.csail.mit.edu/brussell/research/proj/mult_seg_discovery/index.html\nCode\nDimension Reduction\nDiffusion maps\nhttp://www.stat.cmu.edu/~annlee/software.htm\nCode\nMultiple Kernel Learning\nSHOGUN\nS. Sonnenburg, G. Rätsch, C. Schäfer, B. Schölkopf . Large scale multiple kernel learning. JMLR, 2006\nhttp://www.shogun-toolbox.org/\nCode\nDistance Transformation\nDistance Transforms of Sampled Functions\nhttp://people.cs.uchicago.edu/~pff/dt/\nCode\nImage Filtering\nImage smoothing via L0 Gradient Minimization\nL. Xu, C. Lu, Y. Xu, J. Jia, Image smoothing via L0 Gradient Minimization, SIGGRAPH Asia 2011\nhttp://www.cse.cuhk.edu.hk/~leojia/projects/L0smoothing/L0smoothing.zip\nCode\nFeature Extraction\nPCA-SIFT\nY. Ke and R. Sukthankar, PCA-SIFT: A More Distinctive Representation for Local Image Descriptors,CVPR, 2004\nhttp://www.cs.cmu.edu/~yke/pcasift/\nCode\nVisual Tracking\nParticle Filter Object Tracking\nhttp://blogs.oregonstate.edu/hess/code/particles/\nCode\nFeature Extraction\nsRD-SIFT\nM. Lourenco, J. P. Barreto and A. Malti, Feature Detection and Matching in Images with Radial Distortion, ICRA 2010\nhttp://arthronav.isr.uc.pt/~mlourenco/srdsift/index.html#\nCode\nMultiple Instance Learning\nMILES\nY. Chen, J. Bi and J. Z. Wang, MILES: Multiple-Instance Learning via Embedded Instance Selection. PAMI 2006\nhttp://infolab.stanford.edu/~wangz/project/imsearch/SVM/PAMI06/\nCode\nAction Recognition\nDense Trajectories Video Description\nH. Wang and A. Klaser and C. Schmid and C.- L. Liu, Action Recognition by Dense Trajectories, CVPR, 2011\nhttp://lear.inrialpes.fr/people/wang/dense_trajectories\nCode\nImage Segmentation\nEfficient Graph-based Image Segmentation - C++ code\nP. Felzenszwalb and D. Huttenlocher. Efficient Graph-Based Image Segmentation, IJCV 2004\nhttp://people.cs.uchicago.edu/~pff/segment/\nCode\nObject Proposal\nParametric min-cut\nJ. Carreira and C. Sminchisescu. Constrained Parametric Min-Cuts for Automatic Object Segmentation, CVPR 2010\nhttp://sminchisescu.ins.uni-bonn.de/code/cpmc/\nCode\nCommon Visual Pattern Discovery\nCommon Visual Pattern Discovery via Spatially Coherent Correspondences\nH. Liu, S. Yan, “Common Visual Pattern Discovery via Spatially Coherent Correspondences”, CVPR 2010\nhttps://sites.google.com/site/lhrbss/home/papers/SimplifiedCode.zip?attredirects=0\nCode\nSparse Representation\nSparse coding simulation software\nOlshausen BA, Field DJ, “Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images”, Nature 1996\nhttp://redwood.berkeley.edu/bruno/sparsenet/\nCode\nMRF Optimization\nMax-flow/min-cut for massive grids\nA. Delong and Y. Boykov, A Scalable Graph-Cut Algorithm for N-D Grids, CVPR 2008\nhttp://vision.csd.uwo.ca/code/regionpushrelabel-v1.03.zip\nCode\nOptical Flow\nHorn and Schunck’s Optical Flow\nhttp://www.cs.brown.edu/~dqsun/code/hs.zip\nCode\nSparse Representation\nSparse and Redundant Representations: From Theory to Applications in Signal and Image Processing\nM. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing\nhttp://www.cs.technion.ac.il/~elad/Various/Matlab-Package-Book.rar\nCode\nImage Understanding\nTowards Total Scene Understanding\nL.-J. Li, R. Socher and Li F.-F.. Towards Total Scene Understanding:Classification, Annotation and Segmentation in an Automatic Framework, CVPR 2009\nhttp://vision.stanford.edu/projects/totalscene/index.html\nCode\nCamera Calibration\nCamera Calibration Toolbox for Matlab\nhttp://www.vision.caltech.edu/bouguetj/calib_doc/htmls/ref.html\nhttp://www.vision.caltech.edu/bouguetj/calib_doc/\nCode\nImage Segmentation\nTurbepixels\nA. Levinshtein, A. Stere, K. N. Kutulakos, D. J. Fleet, S. J. Dickinson, and K. Siddiqi, TurboPixels: Fast Superpixels Using Geometric Flows, PAMI 2009\nhttp://www.cs.toronto.edu/~babalex/research.html\nCode\nFeature Detection\nEdge Foci Interest Points\nL. Zitnickand K. Ramnath, Edge Foci Interest Points, ICCV, 2011\nhttp://research.microsoft.com/en-us/um/people/larryz/edgefoci/edge_foci.htm\nCode\nFeature Extraction\nLocal Self-Similarity Descriptor\nE. Shechtman and M. Irani. Matching local self-similarities across images and videos, CVPR, 2007\nhttp://www.robots.ox.ac.uk/~vgg/software/SelfSimilarity/\nCode\nSubspace Learning\nGeneralized Principal Component Analysis\nR. Vidal, Y. Ma and S. Sastry. Generalized Principal Component Analysis (GPCA), CVPR 2003\nhttp://www.vision.jhu.edu/downloads/main.php?dlID=c1\nCode\nCamera Calibration\nEasyCamCalib\nJ. Barreto, J. Roquette, P. Sturm, and F. Fonseca, Automatic camera calibration applied to medical endoscopy, BMVC, 2009\nhttp://arthronav.isr.uc.pt/easycamcalib/\nCode\nImage Segmentation\nSuperpixel by Gerg Mori\nX. Ren and J. Malik. Learning a classification model for segmentation. ICCV, 2003\nhttp://www.cs.sfu.ca/~mori/research/superpixels/\nCode\nImage Understanding\nObject Bank\nLi-Jia Li, Hao Su, Eric P. Xing and Li Fei-Fei. Object Bank: A High-Level Image Representation for Scene Classification and Semantic Feature Sparsification, NIPS 2010\nhttp://vision.stanford.edu/projects/objectbank/index.html\nCode\nSaliency Detection\nSpectrum Scale Space based Visual Saliency\nJ Li, M D. Levine, X An and H. He, Saliency Detection Based on Frequency and Spatial Domain Analyses, BMVC 2011\nhttp://www.cim.mcgill.ca/~lijian/saliency.htm\nCode\nSparse Representation\nFisher Discrimination Dictionary Learning for Sparse Representation\nM. Yang, L. Zhang, X. Feng and D. Zhang, Fisher Discrimination Dictionary Learning for Sparse Representation, ICCV 2011\nhttp://www4.comp.polyu.edu.hk/~cslzhang/code/FDDL.zip\nCode\nObject Detection\nCascade Object Detection with Deformable Part Models\nP. Felzenszwalb, R. Girshick, D. McAllester. Cascade Object Detection with Deformable Part Models. CVPR, 2010\nhttp://people.cs.uchicago.edu/~rbg/star-cascade/\nCode\nObject Segmentation\nSparse to Dense Labeling\nP. Ochs, T. Brox, Object Segmentation in Video: A Hierarchical Variational Approach for Turning Point Trajectories into Dense Regions, ICCV 2011\nhttp://lmb.informatik.uni-freiburg.de/resources/binaries/SparseToDenseLabeling.tar.gz\nCode\nOptical Flow\nDense Point Tracking\nN. Sundaram, T. Brox, K. Keutzer Dense point trajectories by GPU-accelerated large displacement optical flow, ECCV 2010\nhttp://lmb.informatik.uni-freiburg.de/resources/binaries/\nCode\nVisual Tracking\nTracking with Online Multiple Instance Learning\nB. Babenko, M.-H. Yang, S. Belongie, Visual Tracking with Online Multiple Instance Learning, PAMI 2011\nhttp://vision.ucsd.edu/~bbabenko/project_miltrack.shtml\nCode\nGraph Matching\nReweighted Random Walks for Graph Matching\nM. Cho, J. Lee, and K. M. Lee, Reweighted Random Walks for Graph Matching, ECCV 2010\nhttp://cv.snu.ac.kr/research/~RRWM/\nCode\nMachine Learning\nStatistical Pattern Recognition Toolbox\nM.I. Schlesinger, V. Hlavac: Ten lectures on the statistical and structural pattern recognition, Kluwer Academic Publishers, 2002\nhttp://cmp.felk.cvut.cz/cmp/software/stprtool/\nCode\nImage Super-resolution\nSprarse coding super-resolution\nJ. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation, TIP 2010\nhttp://www.ifp.illinois.edu/~jyang29/ScSR.htm\nCode\nObject Detection\nDiscriminatively Trained Deformable Part Models\nP. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan. Object Detection with Discriminatively Trained Part Based Models, PAMI, 2010\nhttp://people.cs.uchicago.edu/~pff/latent/\nCode\nMultiple Instance Learning\nMIForests\nC. Leistner, A. Saffari, and H. Bischof, MIForests: Multiple-Instance Learning with Randomized Trees, ECCV 2010\nhttp://www.ymer.org/amir/software/milforests/\nCode\nOptical Flow\nLarge Displacement Optical Flow\nT. Brox, J. Malik, Large displacement optical flow: descriptor matching in variational motion estimation, PAMI 2011\nhttp://lmb.informatik.uni-freiburg.de/resources/binaries/\nCode\nMultiple View Geometry\nMATLAB and Octave Functions for Computer Vision and Image Processing\nP. D. Kovesi. MATLAB and Octave Functions for Computer Vision and Image Processing, http://www.csse.uwa.edu.au/~pk/research/matlabfns\nhttp://www.csse.uwa.edu.au/~pk/Research/MatlabFns/index.html\nCode\nImage Filtering\nAnisotropic Diffusion\nP. Perona and J. Malik, Scale-space and edge detection using anisotropic diffusion, PAMI 1990\nhttp://www.mathworks.com/matlabcentral/fileexchange/14995-anisotropic-diffusion-perona-malik\nCode\nFeature Detection andFeature Extraction\nGeometric Blur\nA. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition using low distortion correspondences. CVPR, 2005\nhttp://www.robots.ox.ac.uk/~vgg/software/MKL/\nCode\nLow-Rank Modeling\nLow-Rank Matrix Recovery and Completion\nhttp://perception.csl.uiuc.edu/matrix-rank/sample_code.html\nCode\nObject Detection\nA simple parts and structure object detector\nICCV 2005 short courses on Recognizing and Learning Object Categories\nhttp://people.csail.mit.edu/fergus/iccv2005/partsstructure.html\nCode\nKernels and Distances\nDiffusion-based distance\nH. Ling and K. Okada, Diffusion Distance for Histogram Comparison, CVPR 2006\nhttp://www.dabi.temple.edu/~hbling/code/DD_v1.zip\nCode\nImage Denoising\nK-SVD\nhttp://www.cs.technion.ac.il/~ronrubin/Software/ksvdbox13.zip\nCode\nMultiple Kernel Learning\nSimpleMKL\nA. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. Simplemkl. JMRL, 2008\nhttp://asi.insa-rouen.fr/enseignants/~arakotom/code/mklindex.html\nCode\nFeature Extraction\nPyramids of Histograms of Oriented Gradients (PHOG)\nA. Bosch, A. Zisserman, and X. Munoz, Representing shape with a spatial pyramid kernel, CIVR, 2007\nhttp://www.robots.ox.ac.uk/~vgg/research/caltech/phog/phog.zip\nCode\nSparse Representation\nEfficient sparse coding algorithms\nH. Lee, A. Battle, R. Rajat and A. Y. Ng, Efficient sparse coding algorithms, NIPS 2007\nhttp://ai.stanford.edu/~hllee/softwares/nips06-sparsecoding.htm\nCode\nMulti-View Stereo\nClustering Views for Multi-view Stereo\nY. Furukawa, B. Curless, S. M. Seitz, and R. Szeliski, Towards Internet-scale Multi-view Stereo, CVPR 2010\nhttp://grail.cs.washington.edu/software/cmvs/\nCode\nMulti-View Stereo\nMulti-View Stereo Evaluation\nS. Seitz et al. A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms, CVPR 2006\nhttp://vision.middlebury.edu/mview/\nCode\nStructure from motion\nStructure and Motion Toolkit in Matlab\nhttp://cms.brookes.ac.uk/staff/PhilipTorr/Code/code_page_4.htm\nCode\nPose Estimation\nTraining Deformable Models for Localization\nRamanan, D. “Learning to Parse Images of Articulated Bodies.” NIPS 2006\nhttp://www.ics.uci.edu/~dramanan/papers/parse/index.html\nCode\nLow-Rank Modeling\nRASL: Robust Batch Alignment of Images by Sparse and Low-Rank Decomposition\nY. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma, RASL: Robust Batch Alignment of Images by Sparse and Low-Rank Decomposition, CVPR 2010\nhttp://perception.csl.uiuc.edu/matrix-rank/rasl.html\nCode\nDimension Reduction\nISOMAP\nhttp://isomap.stanford.edu/\nCode\nAlpha Matting\nLearning-based Matting\nY. Zheng and C. Kambhamettu, Learning Based Digital Matting, ICCV 2009\nhttp://www.mathworks.com/matlabcentral/fileexchange/31412\nCode\nImage Segmentation\nNormalized Cut\nJ. Shi and J Malik, Normalized Cuts and Image Segmentation, PAMI, 2000\nhttp://www.cis.upenn.edu/~jshi/software/\nCode\nImage Denoising andStereo Matching\nEfficient Belief Propagation for Early Vision\nP. F. Felzenszwalb and D. P. Huttenlocher, Efficient Belief Propagation for Early Vision, IJCV, 2006\nhttp://www.cs.brown.edu/~pff/bp/\nCode\nSparse Representation\nA Linear Subspace Learning Approach via Sparse Coding\nL. Zhang, P. Zhu, Q. Hu and D. Zhang, “A Linear Subspace Learning Approach via Sparse Coding,” ICCV 2011\nhttp://www4.comp.polyu.edu.hk/~cslzhang/code/LSL_SC.zip\nCode\nText Recognition\nNeocognitron for handwritten digit recognition\nK. Fukushima: “Neocognitron for handwritten digit recognition”, Neurocomputing, 2003\nhttp://visiome.neuroinf.jp/modules/xoonips/detail.php?item_id=375\nCode\nImage Classification\nSparse Coding for Image Classification\nJ. Yang, K. Yu, Y. Gong, T. Huang, Linear Spatial Pyramid Matching using Sparse Coding for Image Classification, CVPR, 2009\nhttp://www.ifp.illinois.edu/~jyang29/ScSPM.htm\nCode\nNearest Neighbors Matching\nLDAHash: Binary Descriptors for Matching in Large Image Databases\nC. Strecha, A. M. Bronstein, M. M. Bronstein and P. Fua. LDAHash: Improved matching with smaller descriptors, PAMI, 2011.\nhttp://cvlab.epfl.ch/research/detect/ldahash/index.php\nCode\nObject Segmentation\nClassCut for Unsupervised Class Segmentation\nB. Alexe, T. Deselaers and V. Ferrari, ClassCut for Unsupervised Class Segmentation, ECCV 2010\nhttp://www.vision.ee.ethz.ch/~calvin/classcut/ClassCut-release.zip\nCode\nImage Quality Assessment\nFeature SIMilarity Index\nhttp://www4.comp.polyu.edu.hk/~cslzhang/IQA/FSIM/FSIM.htm\nCode\nSaliency Detection\nAttention via Information Maximization\nN. Bruce and J. Tsotsos. Saliency based on information maximization. In NIPS, 2005\nhttp://www.cse.yorku.ca/~neil/AIM.zip\nCode\nImage Denoising\nWhat makes a good model of natural images ?\nY. Weiss and W. T. Freeman, CVPR 2007\nhttp://www.cs.huji.ac.il/~yweiss/BRFOE.zip\nCode\nImage Segmentation\nMean-Shift Image Segmentation - Matlab Wrapper\nD. Comaniciu, P Meer. Mean Shift: A Robust Approach Toward Feature Space Analysis. PAMI 2002\nhttp://www.wisdom.weizmann.ac.il/~bagon/matlab_code/edison_matlab_interface.tar.gz\nCode\nObject Segmentation\nGeodesic Star Convexity for Interactive Image Segmentation\nV. Gulshan, C. Rother, A. Criminisi, A. Blake and A. Zisserman. Geodesic star convexity for interactive image segmentation\nhttp://www.robots.ox.ac.uk/~vgg/software/iseg/index.shtml\nCode\nFeature Detection andFeature Extraction\nAffine-SIFT\nJ.M. Morel and G.Yu, ASIFT, A new framework for fully affine invariant image comparison. SIAM Journal on Imaging Sciences, 2009\nhttp://www.ipol.im/pub/algo/my_affine_sift/\nCode\nMRF Optimization\nMulti-label optimization\nY. Boykov, O. Verksler, and R. Zabih, Fast Approximate Energy Minimization via Graph Cuts, PAMI 2001\nhttp://vision.csd.uwo.ca/code/gco-v3.0.zip\nCode\nFeature Detection andFeature Extraction\nScale-invariant feature transform (SIFT) - Demo Software\nD. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004.\nhttp://www.cs.ubc.ca/~lowe/keypoints/\nCode\nVisual Tracking\nKLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker\nB. D. Lucas and T. Kanade. An Iterative Image Registration Technique with an Application to Stereo Vision. IJCAI, 1981\nhttp://www.ces.clemson.edu/~stb/klt/\nCode\nFeature Detection andFeature Extraction\nAffine Covariant Features\nT. Tuytelaars and K. Mikolajczyk, Local Invariant Feature Detectors: A Survey, Foundations and Trends in Computer Graphics and Vision, 2008\nhttp://www.robots.ox.ac.uk/~vgg/research/affine/\nCode\nImage Segmentation\nSegmenting Scenes by Matching Image Composites\nB. Russell, A. A. Efros, J. Sivic, W. T. Freeman, A. Zisserman, NIPS 2009\nhttp://www.cs.washington.edu/homes/bcr/projects/SceneComposites/index.html\nCode\nImage Segmentation\nOWT-UCM Hierarchical Segmentation\nP. Arbelaez, M. Maire, C. Fowlkes and J. Malik. Contour Detection and Hierarchical Image Segmentation. PAMI, 2011\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html\nCode\nFeature Matching andImage Classification\nThe Pyramid Match: Efficient Matching for Retrieval and Recognition\nK. Grauman and T. Darrell. The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features, ICCV 2005\nhttp://www.cs.utexas.edu/~grauman/research/projects/pmk/pmk_projectpage.htm\nCode\nAlpha Matting\nBayesian Matting\nY. Y. Chuang, B. Curless, D. H. Salesin, and R. Szeliski, A Bayesian Approach to Digital Matting, CVPR, 2001\nhttp://www1.idc.ac.il/toky/CompPhoto-09/Projects/Stud_projects/Miki/index.html\nCode\nImage Deblurring\nRichardson-Lucy Deblurring for Scenes under Projective Motion Path\nY.-W. Tai, P. Tan, M. S. Brown: Richardson-Lucy Deblurring for Scenes under Projective Motion Path, PAMI 2011\nhttp://yuwing.kaist.ac.kr/projects/projectivedeblur/projectivedeblur_files/ProjectiveDeblur.zip\nCode\nPose Estimation\nArticulated Pose Estimation using Flexible Mixtures of Parts\nY. Yang, D. Ramanan, Articulated Pose Estimation using Flexible Mixtures of Parts, CVPR 2011\nhttp://phoenix.ics.uci.edu/software/pose/\nCode\nFeature Extraction\nBRIEF: Binary Robust Independent Elementary Features\nM. Calonder, V. Lepetit, C. Strecha, P. Fua, BRIEF: Binary Robust Independent Elementary Features, ECCV 2010\nhttp://cvlab.epfl.ch/research/detect/brief/\nCode\nFeature Extraction\nGlobal and Efficient Self-Similarity\nT. Deselaers and V. Ferrari. Global and Efficient Self-Similarity for Object Classification and Detection. CVPR 2010andT. Deselaers, V. Ferrari, Global and Efficient Self-Similarity for Object Classification and Detection, CVPR 2010\nhttp://www.vision.ee.ethz.ch/~calvin/gss/selfsim_release1.0.tgz\nCode\nImage Super-resolution\nMulti-frame image super-resolution\nPickup, L. C. Machine Learning in Multi-frame Image Super-resolution, PhD thesis\nhttp://www.robots.ox.ac.uk/~vgg/software/SR/index.html\nCode\nFeature Detection andFeature Extraction\nScale-invariant feature transform (SIFT) - Library\nD. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004.\nhttp://blogs.oregonstate.edu/hess/code/sift/\nCode\nImage Denoising\nClustering-based Denoising\nP. Chatterjee and P. Milanfar, Clustering-based Denoising with Locally Learned Dictionaries (K-LLD), TIP, 2009\nhttp://users.soe.ucsc.edu/~priyam/K-LLD/\nCode\nObject Recognition\nRecognition by Association via Learning Per-exemplar Distances\nT. Malisiewicz, A. A. Efros, Recognition by Association via Learning Per-exemplar Distances, CVPR 2008\nhttp://www.cs.cmu.edu/~tmalisie/projects/cvpr08/dfuns.tar.gz\nCode\nVisual Tracking\nSuperpixel Tracking\nS. Wang, H. Lu, F. Yang, and M.-H. Yang, Superpixel Tracking, ICCV 2011\nhttp://faculty.ucmerced.edu/mhyang/papers/iccv11a.html\nCode\nSparse Representation\nSPArse Modeling Software\nJ. Mairal, F. Bach, J. Ponce and G. Sapiro. Online Learning for Matrix Factorization and Sparse Coding, JMLR 2010\nhttp://www.di.ens.fr/willow/SPAMS/\nCode\nSaliency Detection\nSaliency detection: A spectral residual approach\nX. Hou and L. Zhang. Saliency detection: A spectral residual approach. CVPR, 2007\nhttp://www.klab.caltech.edu/~xhou/projects/spectralResidual/spectralresidual.html\nCode\nImage Filtering\nGuided Image Filtering\nK. He, J. Sun, X. Tang, Guided Image Filtering, ECCV 2010\nhttp://personal.ie.cuhk.edu.hk/~hkm007/eccv10/guided-filter-code-v1.rar\nCode\nKernels and Distances\nFast Directional Chamfer Matching\nhttp://www.umiacs.umd.edu/~mingyliu/src/fdcm_matlab_wrapper_v0.2.zip\nCode\nVisual Tracking\nL1 Tracking\nX. Mei and H. Ling, Robust Visual Tracking using L1 Minimization, ICCV, 2009\nhttp://www.dabi.temple.edu/~hbling/code_data.htm\nCode\nObject Proposal\nRegion-based Object Proposal\nI. Endres and D. Hoiem. Category Independent Object Proposals, ECCV 2010\nhttp://vision.cs.uiuc.edu/proposals/\nCode\nObject Detection\nEnsemble of Exemplar-SVMs for Object Detection and Beyond\nT. Malisiewicz, A. Gupta, A. A. Efros, Ensemble of Exemplar-SVMs for Object Detection and Beyond , ICCV 2011\nhttp://www.cs.cmu.edu/~tmalisie/projects/iccv11/\nCode\nDimension Reduction\nDimensionality Reduction Toolbox\nhttp://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html\nCode\nObject Detection\nViola-Jones Object Detection\nP. Viola and M. Jones, Rapid Object Detection Using a Boosted Cascade of Simple Features, CVPR, 2001\nhttp://pr.willowgarage.com/wiki/FaceDetection\nCode\nObject Detection\nImplicit Shape Model\nB. Leibe, A. Leonardis, B. Schiele. Robust Object Detection with Interleaved Categorization and Segmentation, IJCV, 2008\nhttp://www.vision.ee.ethz.ch/~bleibe/code/ism.html\nCode\nSaliency Detection\nSaliency detection using maximum symmetric surround\nR. Achanta and S. Susstrunk. Saliency detection using maximum symmetric surround. In ICIP, 2010\nhttp://ivrg.epfl.ch/supplementary_material/RK_ICIP2010/index.html\nCode\nImage Filtering\nFast Bilateral Filter\nS. Paris and F. Durand, A Fast Approximation of the Bilateral Filter using a Signal Processing Approach, ECCV, 2006\nhttp://people.csail.mit.edu/sparis/bf/\nCode\nMachine Learning\nFastICA package for MATLAB\nhttp://research.ics.tkk.fi/ica/book/\nhttp://research.ics.tkk.fi/ica/fastica/\nCode\nFeature Detection andFeature Extraction\nMaximally stable extremal regions (MSER)\nJ. Matas, O. Chum, M. Urba, and T. Pajdla. Robust wide baseline stereo from maximally stable extremal regions. BMVC, 2002\nhttp://www.robots.ox.ac.uk/~vgg/research/affine/\nCode\nStructure from motion\nBundler\nN. Snavely, S M. Seitz, R Szeliski. Photo Tourism: Exploring image collections in 3D. SIGGRAPH 2006\nhttp://phototour.cs.washington.edu/bundler/\nCode\nVisual Tracking\nOnline Discriminative Object Tracking with Local Sparse Representation\nQ. Wang, F. Chen, W. Xu, and M.-H. Yang, Online Discriminative Object Tracking with Local Sparse Representation, WACV 2012\nhttp://faculty.ucmerced.edu/mhyang/code/wacv12a_code.zip\nCode\nAlpha Matting\nClosed Form Matting\nA. Levin D. Lischinski and Y. Weiss. A Closed Form Solution to Natural Image Matting, PAMI 2008.\nhttp://people.csail.mit.edu/alevin/matting.tar.gz\nCode\nImage Filtering\nGradientShop\nP. Bhat, C.L. Zitnick, M. Cohen, B. Curless, and J. Kim, GradientShop: A Gradient-Domain Optimization Framework for Image and Video Filtering, TOG 2010\nhttp://grail.cs.washington.edu/projects/gradientshop/\nCode\nVisual Tracking\nIncremental Learning for Robust Visual Tracking\nD. Ross, J. Lim, R.-S. Lin, M.-H. Yang, Incremental Learning for Robust Visual Tracking, IJCV 2007\nhttp://www.cs.toronto.edu/~dross/ivt/\nCode\nFeature Detection andFeature Extraction\nColor Descriptor\nK. E. A. van de Sande, T. Gevers and Cees G. M. Snoek, Evaluating Color Descriptors for Object and Scene Recognition, PAMI, 2010\nhttp://koen.me/research/colordescriptors/\nCode\nImage Segmentation\nEntropy Rate Superpixel Segmentation\nM.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, Entropy Rate Superpixel Segmentation, CVPR 2011\nhttp://www.umiacs.umd.edu/~mingyliu/src/ers_matlab_wrapper_v0.1.zip\nCode\nImage Filtering\nDomain Transformation\nE. Gastal, M. Oliveira, Domain Transform for Edge-Aware Image and Video Processing, SIGGRAPH 2011\nhttp://inf.ufrgs.br/~eslgastal/DomainTransform/DomainTransformFilters-Source-v1.0.zip\nCode\nMultiple Kernel Learning\nOpenKernel.org\nF. Orabona and L. Jie. Ultra-fast optimization algorithm for sparse multi kernel learning. ICML, 2011\nhttp://www.openkernel.org/\nCode\nImage Segmentation\nEfficient Graph-based Image Segmentation - Matlab Wrapper\nP. Felzenszwalb and D. Huttenlocher. Efficient Graph-Based Image Segmentation, IJCV 2004\nhttp://www.mathworks.com/matlabcentral/fileexchange/25866-efficient-graph-based-image-segmentation\nCode\nImage Segmentation\nBiased Normalized Cut\nS. Maji, N. Vishnoi and J. Malik, Biased Normalized Cut, CVPR 2011\nhttp://www.cs.berkeley.edu/~smaji/projects/biasedNcuts/\nCode\nStereo\nConstant-Space Belief Propagation\nQ. Yang, L. Wang, and N. Ahuja, A Constant-Space Belief Propagation Algorithm for Stereo Matching, CVPR 2010\nhttp://www.cs.cityu.edu.hk/~qiyang/publications/code/cvpr-10-csbp/csbp.htm\nCode\nFeature Detection andFeature Extraction\nSpeeded Up Robust Feature (SURF) - Open SURF\nH. Bay, T. Tuytelaars and L. V. Gool SURF: Speeded Up Robust Features, ECCV, 2006\nhttp://www.chrisevansdev.com/computer-vision-opensurf.html\nCode\nVisual Tracking\nOnline boosting trackers\nH. Grabner, and H. Bischof, On-line Boosting and Vision, CVPR, 2006\nhttp://www.vision.ee.ethz.ch/boostingTrackers/\nCode\nImage Denoising\nSparsity-based Image Denoising\nW. Dong, X. Li, L. Zhang and G. Shi, Sparsity-based Image Denoising vis Dictionary Learning and Structural Clustering, CVPR, 2011\nhttp://www.csee.wvu.edu/~xinl/CSR.html\nCode\nFeature Detection andFeature Extraction\nScale-invariant feature transform (SIFT) - VLFeat\nD. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004.\nhttp://www.vlfeat.org/\nCode\nClustering\nSpectral Clustering - UW Project\nhttp://www.stat.washington.edu/spectral/\nCode\nImage Deblurring\nAnalyzing spatially varying blur\nA. Chakrabarti, T. Zickler, and W. T. Freeman, Analyzing Spatially-varying Blur, CVPR 2010\nhttp://www.eecs.harvard.edu/~ayanc/svblur/\nCode\nMultiple Instance Learning\nDD-SVM\nYixin Chen and James Z. Wang, Image Categorization by Learning and Reasoning with Regions, JMLR 2004\nCode\nFeature Extraction\nGIST Descriptor\nA. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope, IJCV, 2001\nhttp://people.csail.mit.edu/torralba/code/spatialenvelope/\nCode\nImage Classification\nTexture Classification\nM. Varma and A. Zisserman, A statistical approach to texture classification from single images, IJCV2005\nhttp://www.robots.ox.ac.uk/~vgg/research/texclass/index.html\nCode\nStructure from motion\nNonrigid Structure From Motion in Trajectory Space\nhttp://cvlab.lums.edu.pk/nrsfm/index.html\nCode\nAlpha Matting\nShared Matting\nE. S. L. Gastal and M. M. Oliveira, Computer Graphics Forum, 2010\nhttp://www.inf.ufrgs.br/~eslgastal/SharedMatting/\nCode\nAction Recognition\n3D Gradients (HOG3D)\nA. Klaser, M. Marszałek, and C. Schmid, BMVC, 2008.\nhttp://lear.inrialpes.fr/people/klaeser/research_hog3d\nCode\nImage Denoising\nKernel Regressions\nhttp://www.soe.ucsc.edu/~htakeda/MatlabApp/KernelRegressionBasedImageProcessingToolBox_ver1-1beta.zip\nCode\nFeature Detection\nBoundary Preserving Dense Local Regions\nJ. Kim and K. Grauman, Boundary Preserving Dense Local Regions, CVPR 2011\nhttp://vision.cs.utexas.edu/projects/bplr/bplr.html\nCode\nImage Understanding\nSuperParsing\nJ. Tighe and S. Lazebnik, SuperParsing: Scalable Nonparametric Image Parsing with Superpixels, ECCV 2010\nhttp://www.cs.unc.edu/~jtighe/Papers/ECCV10/eccv10-jtighe-code.zip\nCode\nImage Filtering\nWeighted Least Squares Filter\nZ. Farbman, R. Fattal, D. Lischinski, R. Szeliski, Edge-Preserving Decompositions for Multi-Scale Tone and Detail Manipulation, SIGGRAPH 2008\nhttp://www.cs.huji.ac.il/~danix/epd/\nCode\nImage Super-resolution\nSingle-Image Super-Resolution Matlab Package\nR. Zeyde, M. Elad, and M. Protter, On Single Image Scale-Up using Sparse-Representations, LNCS 2010\nhttp://www.cs.technion.ac.il/~elad/Various/Single_Image_SR.zip\nCode\nImage Understanding\nBlocks World Revisited: Image Understanding using Qualitative Geometry and Mechanics\nA. Gupta, A. A. Efros, M. Hebert, Blocks World Revisited: Image Understanding using Qualitative Geometry and Mechanics, ECCV 2010\nhttp://www.cs.cmu.edu/~abhinavg/blocksworld/#downloads\nCode\nFeature Extraction\nShape Context\nS. Belongie, J. Malik and J. Puzicha. Shape matching and object recognition using shape contexts, PAMI, 2002\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sc_digits.html\nCode\nImage Processing andImage Filtering\nPiotr’s Image & Video Matlab Toolbox\nPiotr Dollar, Piotr’s Image & Video Matlab Toolbox, http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html\nhttp://vision.ucsd.edu/~pdollar/toolbox/doc/index.html\nCode\nIllumination, Reflectance, and Shadow\nWebcam Clip Art: Appearance and Illuminant Transfer from Time-lapse Sequences\nJ-F. Lalonde, A. A. Efros, S. G. Narasimhan, Webcam Clip Art: Appearance and Illuminant Transfer from Time-lapse Sequences, SIGGRAPH Asia 2009\nhttp://www.cs.cmu.edu/~jlalonde/software.html#skyModel\nCode\nPose Estimation\nCalvin Upper-Body Detector\nE. Marcin, F. Vittorio, Better Appearance Models for Pictorial Structures, BMVC 2009\nhttp://www.vision.ee.ethz.ch/~calvin/calvin_upperbody_detector/\nCode\nImage Classification\nLocality-constrained Linear Coding\nJ. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained Linear Coding for Image Classification, CVPR, 2010\nhttp://www.ifp.illinois.edu/~jyang29/LLC.htm\nCode\nFeature Detection andFeature Extraction\nSpeeded Up Robust Feature (SURF) - Matlab Wrapper\nH. Bay, T. Tuytelaars and L. V. Gool SURF: Speeded Up Robust Features, ECCV, 2006\nhttp://www.maths.lth.se/matematiklth/personal/petter/surfmex.php\nCode\nPose Estimation\nEstimating Human Pose from Occluded Images\nJ.-B. Huang and M.-H. Yang, Estimating Human Pose from Occluded Images, ACCV 2009\nhttp://faculty.ucmerced.edu/mhyang/code/accv09_pose.zip\nCode\nStructure from motion\nOpenSourcePhotogrammetry\nhttp://opensourcephotogrammetry.blogspot.com/\nCode\nImage Classification\nSpatial Pyramid Matching\nS. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories, CVPR 2006\nhttp://www.cs.unc.edu/~lazebnik/research/SpatialPyramid.zip\nCode\nNearest Neighbors Matching\nCoherency Sensitive Hashing\nS. Korman, S. Avidan, Coherency Sensitive Hashing, ICCV 2011\nhttp://www.eng.tau.ac.il/~simonk/CSH/index.html\nCode\nImage Segmentation\nSegmentation by Minimum Code Length\nA. Y. Yang, J. Wright, S. Shankar Sastry, Y. Ma , Unsupervised Segmentation of Natural Images via Lossy Data Compression, CVIU, 2007\nhttp://perception.csl.uiuc.edu/coding/image_segmentation/\nCode\nSaliency Detection\nFrequency-tuned salient region detection\nR. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In CVPR, 2009\nhttp://ivrgwww.epfl.ch/supplementary_material/RK_CVPR09/index.html\nCode\nMRF Optimization\nMax-flow/min-cut for shape fitting\nV. Lempitsky and Y. Boykov, Global Optimization for Shape Fitting, CVPR 2007\nhttp://www.csd.uwo.ca/faculty/yuri/Implementations/TouchExpand.zip\nCode\nFeature Detection\nCanny Edge Detection\nJ. Canny, A Computational Approach To Edge Detection, PAMI, 1986\nhttp://www.mathworks.com/help/toolbox/images/ref/edge.html\nCode\nObject Detection\nMultiple Kernels\nA. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman, Multiple Kernels for Object Detection. ICCV, 2009\nhttp://www.robots.ox.ac.uk/~vgg/software/MKL/\nCode\nImage Segmentation\nMean-Shift Image Segmentation - EDISON\nD. Comaniciu, P Meer. Mean Shift: A Robust Approach Toward Feature Space Analysis. PAMI 2002\nhttp://coewww.rutgers.edu/riul/research/code/EDISON/index.html\nCode\nImage Quality Assessment\nDegradation Model\nhttp://users.ece.utexas.edu/~bevans/papers/2000/imageQuality/index.html\nCode\nObject Detection\nEnsemble of Exemplar-SVMs\nT. Malisiewicz, A. Gupta, A. Efros. Ensemble of Exemplar-SVMs for Object Detection and Beyond . ICCV, 2011\nhttp://www.cs.cmu.edu/~tmalisie/projects/iccv11/\nCode\nImage Deblurring\nRadon Transform\nT. S. Cho, S. Paris, B. K. P. Horn, W. T. Freeman, Blur kernel estimation using the radon transform, CVPR 2011\nhttp://people.csail.mit.edu/taegsang/Documents/RadonDeblurringCode.zip\nCode\nImage Deblurring\nEficient Marginal Likelihood Optimization in Blind Deconvolution\nA. Levin, Y. Weiss, F. Durand, W. T. Freeman. Efficient Marginal Likelihood Optimization in Blind Deconvolution, CVPR 2011\nhttp://www.wisdom.weizmann.ac.il/~levina/papers/LevinEtalCVPR2011Code.zip\nCode\nFeature Detection\nFAST Corner Detection\nE. Rosten and T. Drummond, Machine learning for high-speed corner detection, ECCV, 2006\nhttp://www.edwardrosten.com/work/fast.html\nCode\nImage Super-resolution\nMDSP Resolution Enhancement Software\nS. Farsiu, D. Robinson, M. Elad, and P. Milanfar, Fast and Robust Multi-frame Super-resolution, TIP 2004\nhttp://users.soe.ucsc.edu/~milanfar/software/superresolution.html\nCode\nFeature Extraction andObject Detection\nHistogram of Oriented Graidents - INRIA Object Localization Toolkit\nN. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005\nhttp://www.navneetdalal.com/software\nCode\nVisual Tracking\nGlobally-Optimal Greedy Algorithms for Tracking a Variable Number of Objects\nH. Pirsiavash, D. Ramanan, C. Fowlkes. “Globally-Optimal Greedy Algorithms for Tracking a Variable Number of Objects, CVPR 2011\nhttp://www.ics.uci.edu/~hpirsiav/papers/tracking_cvpr11_release_v1.0.tar.gz\nCode\nSaliency Detection\nSegmenting salient objects from images and videos\nE. Rahtu, J. Kannala, M. Salo, and J. Heikkila. Segmenting salient objects from images and videos. CVPR, 2010\nhttp://www.cse.oulu.fi/MVG/Downloads/saliency\nCode\nVisual Tracking\nObject Tracking\nA. Yilmaz, O. Javed and M. Shah, Object Tracking: A Survey, ACM Journal of Computing Surveys, Vol. 38, No. 4, 2006\nhttp://plaza.ufl.edu/lvtaoran/object%20tracking.htm\nCode\nMachine Learning\nBoosting Resources by Liangliang Cao\nhttp://www.ifp.illinois.edu/~cao4/reading/boostingbib.htm\nhttp://www.ifp.illinois.edu/~cao4/reading/boostingbib.htm\nCode\nMachine Learning\nNetlab Neural Network Software\nC. M. Bishop, Neural Networks for Pattern RecognitionㄝOxford University Press, 1995\nhttp://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/\nCode\nOptical Flow\nClassical Variational Optical Flow\nT. Brox, A. Bruhn, N. Papenberg, J. Weickert, High accuracy optical flow estimation based on a theory for warping, ECCV 2004\nhttp://lmb.informatik.uni-freiburg.de/resources/binaries/\nCode\nSparse Representation\nCentralized Sparse Representation for Image Restoration\nW. Dong, L. Zhang and G. Shi, “Centralized Sparse Representation for Image Restoration,” ICCV 2011\nhttp://www4.comp.polyu.edu.hk/~cslzhang/code/CSR_IR.zip\nCourse\nComputer Vision\nIntroduction to Computer Vision, Stanford University, Winter 2010-2011\nFei-Fei Li\nhttp://vision.stanford.edu/teaching/cs223b/\nCourse\nComputer Vision\nComputer Vision: From 3D Reconstruction to Visual Recognition, Fall 2012\nSilvio Savarese and Fei-Fei Li\nhttps://www.coursera.org/course/computervision\nCourse\nComputer Vision\nComputer Vision, University of Texas at Austin, Spring 2011\nKristen Grauman\nhttp://www.cs.utexas.edu/~grauman/courses/spring2011/index.html\nCourse\nComputer Vision\nLearning-Based Methods in Vision, CMU, Spring 2012\nAlexei “Alyosha” Efros and Leonid Sigal\nhttps://docs.google.com/document/pub?id=1jGBn7zPDEaU33fJwi3YI_usWS-U6gpSSJotV_2gDrL0\nCourse\nVisual Recognition\nVisual Recognition, University of Texas at Austin, Fall 2011\nKristen Grauman\nhttp://www.cs.utexas.edu/~grauman/courses/fall2011/schedule.html\nCourse\nComputer Vision\nIntroduction to Computer Vision\nJames Hays, Brown University, Fall 2011\nhttp://www.cs.brown.edu/courses/cs143/\nCourse\nComputer Vision\nComputer Vision, University of North Carolina at Chapel Hill, Spring 2010\nSvetlana Lazebnik\nhttp://www.cs.unc.edu/~lazebnik/spring10/\nCourse\nComputer Vision\nComputer Vision: The Fundamentals, University of California at Berkeley, Fall 2012\nJitendra Malik\nhttps://www.coursera.org/course/vision\nCourse\nComputational Photography\nComputational Photography, University of Illinois, Urbana-Champaign, Fall 2011\nDerek Hoiem\nhttp://www.cs.illinois.edu/class/fa11/cs498dh/\nCourse\nGraphical Models\nInference in Graphical Models, Stanford University, Spring 2012\nAndrea Montanari, Stanford University\nhttp://www.stanford.edu/~montanar/TEACHING/Stat375/stat375.html\nCourse\nComputer Vision\nComputer Vision, New York University, Fall 2012\nRob Fergus\nhttp://cs.nyu.edu/~fergus/teaching/vision_2012/index.html\nCourse\nComputer Vision\nAdvances in Computer Vision\nAntonio Torralba, MIT, Spring 2010\nhttp://groups.csail.mit.edu/vision/courses/6.869/\nCourse\nComputer Vision\nComputer Vision, University of Illinois, Urbana-Champaign, Spring 2012\nDerek Hoiem\nhttp://www.cs.illinois.edu/class/sp12/cs543/\nCourse\nComputational Photography\nComputational Photography, CMU, Fall 2011\nAlexei “Alyosha” Efros\nhttp://graphics.cs.cmu.edu/courses/15-463/2011_fall/463.html\nCourse\nComputer Vision\nComputer Vision, University of Washington, Winter 2012\nSteven Seitz\nhttp://www.cs.washington.edu/education/courses/cse455/12wi/\nLink\nSource code\nSource Code Collection for Reproducible Research\ncollected by Xin Li, Lane Dept of CSEE, West Virginia University\nhttp://www.csee.wvu.edu/~xinl/reproducible_research.html\nLink\nComputer Vision\nComputer Image Analysis, Computer Vision Conferences\nUSC\nhttp://iris.usc.edu/information/Iris-Conferences.html\nLink\nComputer Vision\nCV Papers on the web\nCVPapers\nhttp://www.cvpapers.com/index.html\nLink\nComputer Vision\nCVonline\nCVonline: The Evolving, Distributed, Non-Proprietary, On-Line Compendium of Computer Vision\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/\nLink\nDataset\nCompiled list of recognition datasets\ncompiled by Kristen Grauman\nhttp://www.cs.utexas.edu/~grauman/courses/spring2008/datasets.htm\nLink\nComputer Vision\nAnnotated Computer Vision Bibliography\ncompiled by Keith Price\nhttp://iris.usc.edu/Vision-Notes/bibliography/contents.html\nLink\nComputer Vision\nThe Computer Vision homepage\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\nLink\nComputer Vision Industry\nThe Computer Vision Industry\nDavid Lowe\nhttp://www.cs.ubc.ca/~lowe/vision.html\nLink\nSource code\nComputer Vision Algorithm Implementations\nCVPapers\nhttp://www.cvpapers.com/rr.html\nLink\nComputer Vision\nCV Datasets on the web\nCVPapers\nhttp://www.cvpapers.com/datasets.html\nTalk\nVisual Recognition\nUnderstanding Visual Scenes\nAntonio Torralba, MIT\nhttp://videolectures.net/nips09_torralba_uvs/\nTalk\nNeuroscience\nLearning in Hierarchical Architectures: from Neuroscience to Derived Kernels\nTomaso A. Poggio, McGovern Institute for Brain Research, Massachusetts Institute of Technology\nhttp://videolectures.net/mlss09us_poggio_lhandk/\nTalk\nDeep Learning\nA tutorial on Deep Learning\nGeoffrey E. Hinton, Department of Computer Science, University of Toronto\nhttp://videolectures.net/jul09_hinton_deeplearn/\nTalk\nBoosting\nTheory and Applications of Boosting\nRobert Schapire, Department of Computer Science, Princeton University\nhttp://videolectures.net/mlss09us_schapire_tab/\nTalk\nGraphical Models\nGraphical Models and message-passing algorithms\nMartin J. Wainwright, University of California at Berkeley\nhttp://videolectures.net/mlss2011_wainwright_messagepassing/\nTalk\nStatistical Learning Theory\nStatistical Learning Theory\nJohn Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London\nhttp://videolectures.net/mlss04_taylor_slt/\nTalk\nGaussian Process\nGaussian Process Basics\nDavid MacKay, University of Cambridge\nhttp://videolectures.net/gpip06_mackay_gpb/\nTalk\nInformation Theory\nInformation Theory\nDavid MacKay, University of Cambridge\nhttp://videolectures.net/mlss09uk_mackay_it/\nTalk\nOptimization\nOptimization Algorithms in Machine Learning\nStephen J. Wright, Computer Sciences Department, University of Wisconsin - Madison\nhttp://videolectures.net/nips2010_wright_oaml/\nTalk\nBayesian Inference\nIntroduction To Bayesian Inference\nChristopher Bishop, Microsoft Research\nhttp://videolectures.net/mlss09uk_bishop_ibi/\nTalk\nBayesian Nonparametrics\nModern Bayesian Nonparametrics\nPeter Orbanz and Yee Whye Teh\nhttp://www.youtube.com/watch?v=F0_ih7THV94&feature=relmfu\nTalk\nKernels and Distances\nMachine learning and kernel methods for computer vision\nFrancis R. Bach, INRIA\nhttp://videolectures.net/etvc08_bach_mlakm/\nTalk\nOptimization\nConvex Optimization\nLieven Vandenberghe, Electrical Engineering Department, University of California, Los Angeles\nhttp://videolectures.net/mlss2011_vandenberghe_convex/\nTalk\nOptimization\nEnergy Minimization with Label costs and Applications in Multi-Model Fitting\nYuri Boykov, Department of Computer Science, University of Western Ontario\nhttp://videolectures.net/nipsworkshops2010_boykov_eml/\nTalk\nObject Detection\nObject Recognition with Deformable Models\nPedro Felzenszwalb, Brown University\nhttp://www.youtube.com/watch?v=_J_clwqQ4gI\nTalk\nLow-level vision\nLearning and Inference in Low-Level Vision\nYair Weiss, School of Computer Science and Engineering, The Hebrew University of Jerusalem\nhttp://videolectures.net/nips09_weiss_lil/\nTalk\n3D Computer Vision\n3D Computer Vision: Past, Present, and Future\nSteven Seitz, University of Washington, Google Tech Talk, 2011\nhttp://www.youtube.com/watch?v=kyIzMr917Rc\nTalk\nOptimization\nWho is Afraid of Non-Convex Loss Functions?\nYann LeCun, New York University\nhttp://videolectures.net/eml07_lecun_wia/\nTalk\nSparse Representation\nSparse Methods for Machine Learning: Theory and Algorithms\nFrancis R. Bach, INRIA\nhttp://videolectures.net/nips09_bach_smm/\nTalk\nOptimization and Support Vector Machines\nOptimization Algorithms in Support Vector Machines\nStephen J. Wright, Computer Sciences Department, University of Wisconsin - Madison\nhttp://videolectures.net/mlss09us_wright_oasvm/\nTalk\nInformation Theory\nInformation Theory in Learning and Control\nNaftali (Tali) Tishby, The Hebrew University\nhttp://www.youtube.com/watch?v=GKm53xGbAOk&feature=relmfu\nTalk\nRelative Entropy\nRelative Entropy\nSergio Verdu, Princeton University\nhttp://videolectures.net/nips09_verdu_re/\nTutorial\nObject Detection\nGeometry constrained parts based detection\nSimon Lucey, Jason Saragih, ICCV 2011 Tutorial\nhttp://ci2cv.net/tutorials/iccv-2011/\nTutorial\nGraphical Models\nLearning with inference for discrete graphical models\nNikos Komodakis, Pawan Kumar, Nikos Paragios, Ramin Zabih, ICCV 2011 Tutorial\nhttp://www.csd.uoc.gr/~komod/ICCV2011_tutorial/\nTutorial\nVariational Calculus\nVariational methods for computer vision\nDaniel Cremers, Bastian Goldlucke, Thomas Pock, ICCV 2011 Tutorial\nhttp://cvpr.in.tum.de/tutorials/iccv2011\nTutorial\n3D perception\nComputer Vision and 3D Perception for Robotics\nRadu Bogdan Rusu, Gary Bradski, Caroline Pantofaru, Stefan Hinterstoisser, Stefan Holzer, Kurt Konolige and Andrea Vedaldi, ECCV 2010 Tutorial\nhttp://www.willowgarage.com/workshops/2010/eccv\nTutorial\nAction Recognition\nLooking at people: The past, the present and the future\nL. Sigal, T. Moeslund, A. Hilton, V. Kruger, ICCV 2011 Tutorial\nhttp://www.cs.brown.edu/~ls/iccv2011tutorial.html\nTutorial\nNon-linear Least Squares\nComputer vision fundamentals: robust non-linear least-squares and their applications\nPascal Fua, Vincent Lepetit, ICCV 2011 Tutorial\nhttp://cvlab.epfl.ch/~fua/courses/lsq/\nTutorial\nAction Recognition\nFrontiers of Human Activity Analysis\nJ. K. Aggarwal, Michael S. Ryoo, and Kris Kitani, CVPR 2011 Tutorial\nhttp://cvrc.ece.utexas.edu/mryoo/cvpr2011tutorial/\nTutorial\nStructured Prediction\nStructured Prediction and Learning in Computer Vision\nS. Nowozin and C. Lampert, CVPR 2011 Tutorial\nhttp://www.nowozin.net/sebastian/cvpr2011tutorial/\nTutorial\nAction Recognition\nStatistical and Structural Recognition of Human Actions\nIvan Laptev and Greg Mori, ECCV 2010 Tutorial\nhttps://sites.google.com/site/humanactionstutorialeccv10/\nTutorial\nComputational Symmetry\nComputational Symmetry: Past, Current, Future\nYanxi Liu, ECCV 2010 Tutorial\nhttp://vision.cse.psu.edu/research/symmComp/index.shtml\nTutorial\nMatlab\nMatlab Tutorial\nDavid Kriegman and Serge Belongie\nhttp://www.cs.unc.edu/~lazebnik/spring10/matlab.intro.html\nTutorial\nMatlab\nWriting Fast MATLAB Code\nPascal Getreuer, Yale University\nhttp://www.mathworks.com/matlabcentral/fileexchange/5685\nTutorial\nSpectral Clustering\nA Tutorial on Spectral Clustering\nUlrike von Luxburg, Max Planck Institute for Biological Cybernetics\nhttp://web.mit.edu/~wingated/www/introductions/tutorial_on_spectral_clustering.pdf\nTutorial\nFeature Learning, Image Classification\nFeature Learning for Image Classification\nKai Yu and Andrew Ng, ECCV 2010 Tutorial\nhttp://ufldl.stanford.edu/eccv10-tutorial/\nTutorial\nShape Analysis, Diffusion Geometry\nDiffusion Geometry Methods in Shape Analysis\nA. Brontein and M. Bronstein, ECCV 2010 Tutorial\nhttp://tosca.cs.technion.ac.il/book/course_eccv10.html\nTutorial\nGraphical Models\nGraphical Models, Exponential Families, and Variational Inference\nMartin J. Wainwright and Michael I. Jordan, University of California at Berkeley\nhttp://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf\nTutorial\nColor Image Processing\nColor image understanding: from acquisition to high-level image understanding\nTheo Gevers, Keigo Hirakawa, Joost van de Weijer, ICCV 2011 Tutorial\nhttp://www.cat.uab.cat/~joost/tutorial_iccv.html\nTutorial\nStructure from motion\nNonrigid Structure from Motion\nY. Sheikh and Sohaib Khan, ECCV 2010 Tutorial\nhttp://www.cs.cmu.edu/~yaser/ECCV2010Tutorial.html\nTutorial\nExpectation Maximization\nA Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models\nJeff A. Bilmes, University of California at Berkeley\nhttp://crow.ee.washington.edu/people/bulyko/papers/em.pdf\nTutorial\nDecision Forests\nDecision forests for classification, regression, clustering and density estimation\nA. Criminisi, J. Shotton and E. Konukoglu, ICCV 2011 Tutorial\nhttp://research.microsoft.com/en-us/groups/vision/decisionforests.aspx\nTutorial\n3D point cloud processing\n3D point cloud processing: PCL (Point Cloud Library)\nR. Rusu, S. Holzer, M. Dixon, V. Rabaud, ICCV 2011 Tutorial\nhttp://www.pointclouds.org/media/iccv2011.html\nTutorial\nImage Registration\nTools and Methods for Image Registration\nBrown, G. Carneiro, A. A. Farag, E. Hancock, A. A. Goshtasby (Organizer), J. Matas, J.M. Morel, N. S. Netanyahu, F. Sur, and G. Yu, CVPR 2011 Tutorial\nhttp://www.imgfsr.com/CVPR2011/Tutorial6/\nTutorial\nNon-rigid registration\nNon-rigid registration and reconstruction\nAlessio Del Bue, Lourdes Agapito, Adrien Bartoli, ICCV 2011 Tutorial\nhttp://www.isr.ist.utl.pt/~adb/tutorial/\nTutorial\nVariational Calculus\nVariational Methods in Computer Vision\nD. Cremers, B. Goldlücke, T. Pock, ECCV 2010 Tutorial\nhttp://cvpr.cs.tum.edu/tutorials/eccv2010\nTutorial\nDistance Metric Learning\nDistance Functions and Metric Learning\nM. Werman, O. Pele and B. Kulis, ECCV 2010 Tutorial\nhttp://www.cs.huji.ac.il/~ofirpele/DFML_ECCV2010_tutorial/\nTutorial\nFeature Extraction\nImage and Video Description with Local Binary Pattern Variants\nM. Pietikainen and J. Heikkila, CVPR 2011 Tutorial\nhttp://www.ee.oulu.fi/research/imag/mvg/files/pdf/CVPR-tutorial-final.pdf\nTutorial\nGame Theory\nGame Theory in Computer Vision and Pattern Recognition\nMarcello Pelillo and Andrea Torsello, CVPR 2011 Tutorial\nhttp://www.dsi.unive.it/~atorsell/cvpr2011tutorial/\nTutorial\nComputational Imaging\nFcam: an architecture and API for computational cameras\nKari Pulli, Andrew Adams, Timo Ahonen, Marius Tico, ICCV 2011 Tutorial\nhttp://fcam.garage.maemo.org/iccv2011.html\nOther useful links (dataset, lectures, and other softwares)\nConference Information\nComputer Image Analysis, Computer Vision Conferences\n\n另一博客整理的资源：\n一、特征提取Feature Extraction：\nSIFT [1] [Demo program][SIFT Library] [VLFeat]\nPCA-SIFT [2] [Project]\nAffine-SIFT [3] [Project]\nSURF [4] [OpenSURF] [Matlab Wrapper]\nAffine Covariant Features [5] [Oxford project]\nMSER [6] [Oxford project] [VLFeat]\nGeometric Blur [7] [Code]\nLocal Self-Similarity Descriptor [8] [Oxford implementation]\nGlobal and Efficient Self-Similarity [9] [Code]\nHistogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]\nGIST [11] [Project]\nShape Context [12] [Project]\nColor Descriptor [13] [Project]\nPyramids of Histograms of Oriented Gradients [Code]\nSpace-Time Interest Points (STIP) [14][Project] [Code]\nBoundary Preserving Dense Local Regions [15][Project]\nWeighted Histogram[Code]\nHistogram-based Interest Points Detectors[Paper][Code]\nAn OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]\nFast Sparse Representation with Prototypes[Project]\nCorner Detection [Project]\nAGAST Corner Detector: faster than FAST and even FAST-ER[Project]\nReal-time Facial Feature Detection using Conditional Regression Forests[Project]\nGlobal and Efficient Self-Similarity for Object Classification and Detection[code]\nWαSH: Weighted α-Shapes for Local Feature Detection[Project]\nHOG[Project]\nOnline Selection of Discriminative Tracking Features[Project]\n二、图像分割Image Segmentation：\nNormalized Cut [1] [Matlab code]\nGerg Mori’ Superpixel code [2] [Matlab code]\nEfficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]\nMean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]\nOWT-UCM Hierarchical Segmentation [5] [Resources]\nTurbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]\nQuick-Shift [7] [VLFeat]\nSLIC Superpixels [8] [Project]\nSegmentation by Minimum Code Length [9] [Project]\nBiased Normalized Cut [10] [Project]\nSegmentation Tree [11-12] [Project]\nEntropy Rate Superpixel Segmentation [13] [Code]\nFast Approximate Energy Minimization via Graph Cuts[Paper][Code]\nEfﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]\nIsoperimetric Graph Partitioning for Image Segmentation[Paper][Code]\nRandom Walks for Image Segmentation[Paper][Code]\nBlossom V: A new implementation of a minimum cost perfect matching algorithm[Code]\nAn Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]\nGeodesic Star Convexity for Interactive Image Segmentation[Project]\nContour Detection and Image Segmentation Resources[Project][Code]\nBiased Normalized Cuts[Project]\nMax-flow/min-cut[Project]\nChan-Vese Segmentation using Level Set[Project]\nA Toolbox of Level Set Methods[Project]\nRe-initialization Free Level Set Evolution via Reaction Diffusion[Project]\nImproved C-V active contour model[Paper][Code]\nA Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]\nLevel Set Method Research by Chunming Li[Project]\nClassCut for Unsupervised Class Segmentation[code]\nSEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]\n三、目标检测Object Detection：\nA simple object detector with boosting [Project]\nINRIA Object Detection and Localization Toolkit [1] [Project]\nDiscriminatively Trained Deformable Part Models [2] [Project]\nCascade Object Detection with Deformable Part Models [3] [Project]\nPoselet [4] [Project]\nImplicit Shape Model [5] [Project]\nViola and Jones’s Face Detection [6] [Project]\nBayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]\nHand detection using multiple proposals[Project]\nColor Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]\nDiscriminatively trained deformable part models[Project]\nGradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]\nImage Processing On Line[Project]\nRobust Optical Flow Estimation[Project]\nWhere’s Waldo: Matching People in Images of Crowds[Project]\nScalable Multi-class Object Detection[Project]\nClass-Specific Hough Forests for Object Detection[Project]\nDeformed Lattice Detection In Real-World Images[Project]\nDiscriminatively trained deformable part models[Project]\n四、显著性检测Saliency Detection：\nItti, Koch, and Niebur’ saliency detection [1] [Matlab code]\nFrequency-tuned salient region detection [2] [Project]\nSaliency detection using maximum symmetric surround [3] [Project]\nAttention via Information Maximization [4] [Matlab code]\nContext-aware saliency detection [5] [Matlab code]\nGraph-based visual saliency [6] [Matlab code]\nSaliency detection: A spectral residual approach. [7] [Matlab code]\nSegmenting salient objects from images and videos. [8] [Matlab code]\nSaliency Using Natural statistics. [9] [Matlab code]\nDiscriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]\nLearning to Predict Where Humans Look [11] [Project]\nGlobal Contrast based Salient Region Detection [12] [Project]\nBayesian Saliency via Low and Mid Level Cues[Project]\nTop-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]\nSaliency Detection: A Spectral Residual Approach[Code]\n五、图像分类、聚类Image Classification, Clustering\nPyramid Match [1] [Project]\nSpatial Pyramid Matching [2] [Code]\nLocality-constrained Linear Coding [3] [Project] [Matlab code]\nSparse Coding [4] [Project] [Matlab code]\nTexture Classification [5] [Project]\nMultiple Kernels for Image Classification [6] [Project]\nFeature Combination [7] [Project]\nSuperParsing [Code]\nLarge Scale Correlation Clustering Optimization[Matlab code]\nDetecting and Sketching the Common[Project]\nSelf-Tuning Spectral Clustering[Project][Code]\nUser Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]\nFilters for Texture Classification[Project]\nMultiple Kernel Learning for Image Classification[Project]\nSLIC Superpixels[Project]\n六、抠图Image Matting\nA Closed Form Solution to Natural Image Matting [Code]\nSpectral Matting [Project]\nLearning-based Matting [Code]\n七、目标跟踪Object Tracking：\nA Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]\nObject Tracking via Partial Least Squares Analysis[Paper][Code]\nRobust Object Tracking with Online Multiple Instance Learning[Paper][Code]\nOnline Visual Tracking with Histograms and Articulating Blocks[Project]\nIncremental Learning for Robust Visual Tracking[Project]\nReal-time Compressive Tracking[Project]\nRobust Object Tracking via Sparsity-based Collaborative Model[Project]\nVisual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]\nOnline Discriminative Object Tracking with Local Sparse Representation[Paper][Code]\nSuperpixel Tracking[Project]\nLearning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]\nOnline Multiple Support Instance Tracking [Paper][Code]\nVisual Tracking with Online Multiple Instance Learning[Project]\nObject detection and recognition[Project]\nCompressive Sensing Resources[Project]\nRobust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]\nTracking-Learning-Detection[Project][OpenTLD/C++ Code]\nthe HandVu：vision-based hand gesture interface[Project]\nLearning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]\n八、Kinect：\nKinect toolbox[Project]\nOpenNI[Project]\nzouxy09 CSDN Blog[Resource]\nFingerTracker 手指跟踪[code]\n九、3D相关：\n3D Reconstruction of a Moving Object[Paper] [Code]\nShape From Shading Using Linear Approximation[Code]\nCombining Shape from Shading and Stereo Depth Maps[Project][Code]\nShape from Shading: A Survey[Paper][Code]\nA Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]\nMulti-camera Scene Reconstruction via Graph Cuts[Paper][Code]\nA Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]\nReconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]\nMonocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]\nLearning 3-D Scene Structure from a Single Still Image[Project]\n十、机器学习算法：\nMatlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]\nRandom Sampling[code]\nProbabilistic Latent Semantic Analysis (pLSA)[Code]\nFASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]\nFast Intersection / Additive Kernel SVMs[Project]\nSVM[Code]\nEnsemble learning[Project]\nDeep Learning[Net]\nDeep Learning Methods for Vision[Project]\nNeural Network for Recognition of Handwritten Digits[Project]\nTraining a deep autoencoder or a classifier on MNIST digits[Project]\nTHE MNIST DATABASE of handwritten digits[Project]\nErsatz：deep neural networks in the cloud[Project]\nDeep Learning [Project]\nsparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]\nWeka 3: Data Mining Software in Java[Project]\nInvited talk “A Tutorial on Deep Learning” by Dr. Kai Yu (余凯)[Video]\nCNN - Convolutional neural network class[Matlab Tool]\nYann LeCun’s Publications[Wedsite]\nLeNet-5, convolutional neural networks[Project]\nTraining a deep autoencoder or a classifier on MNIST digits[Project]\nDeep Learning 大牛Geoffrey E. Hinton’s HomePage[Website]\nMultiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]\nSparse coding simulation software[Project]\nVisual Recognition and Machine Learning Summer School[Software]\n十一、目标、行为识别Object, Action Recognition：\nAction Recognition by Dense Trajectories[Project][Code]\nAction Recognition Using a Distributed Representation of Pose and Appearance[Project]\nRecognition Using Regions[Paper][Code]\n2D Articulated Human Pose Estimation[Project]\nFast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]\nEstimating Human Pose from Occluded Images[Paper][Code]\nQuasi-dense wide baseline matching[Project]\nChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]\nReal Time Head Pose Estimation with Random Regression Forests[Project]\n2D Action Recognition Serves 3D Human Pose Estimation[Project]\nA Hough Transform-Based Voting Framework for Action Recognition[Project]\nMotion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]\n2D articulated human pose estimation software[Project]\nLearning and detecting shape models [code]\nProgressive Search Space Reduction for Human Pose Estimation[Project]\nLearning Non-Rigid 3D Shape from 2D Motion[Project]\n十二、图像处理：\nDistance Transforms of Sampled Functions[Project]\nThe Computer Vision Homepage[Project]\nEfficient appearance distances between windows[code]\nImage Exploration algorithm[code]\nMotion Magnification 运动放大 [Project]\nBilateral Filtering for Gray and Color Images 双边滤波器 [Project]\nA Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]\n十三、一些实用工具：\nEGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]\na development kit of matlab mex functions for OpenCV library[Project]\nFast Artificial Neural Network Library[Project]\n十四、人手及指尖检测与识别：\nfinger-detection-and-gesture-recognition [Code]\nHand and Finger Detection using JavaCV[Project]\nHand and fingers detection[Code]\n十五、场景解释：\nNonparametric Scene Parsing via Label Transfer [Project]\n十六、光流Optical flow：\nHigh accuracy optical flow using a theory for warping [Project]\nDense Trajectories Video Description [Project]\nSIFT Flow: Dense Correspondence across Scenes and its Applications[Project]\nKLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]\nTracking Cars Using Optical Flow[Project]\nSecrets of optical flow estimation and their principles[Project]\nimplmentation of the Black and Anandan dense optical flow method[Project]\nOptical Flow Computation[Project]\nBeyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]\nA Database and Evaluation Methodology for Optical Flow[Project]\noptical flow relative[Project]\nRobust Optical Flow Estimation [Project]\noptical flow[Project]\n十七、图像检索Image Retrieval：\nSemi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]\n十八、马尔科夫随机场Markov Random Fields：\nMarkov Random Fields for Super-Resolution [Project]\nA Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]\n十九、运动检测Motion detection：\nMoving Object Extraction, Using Models or Analysis of Regions [Project]\nBackground Subtraction: Experiments and Improvements for ViBe [Project]\nA Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]\nchangedetection.net: A new change detection benchmark dataset[Project]\nViBe - a powerful technique for background detection and subtraction in video sequences[Project]\nBackground Subtraction Program[Project]\nMotion Detection Algorithms[Project]\nStuttgart Artificial Background Subtraction Dataset[Project]\nObject Detection, Motion Estimation, and Tracking[Project]\nFeature Detection and Description\nGeneral Libraries:\nVLFeat – Implementation of various feature descriptors (including SIFT, HOG, and LBP) and covariant feature detectors (including DoG, Hessian, Harris Laplace, Hessian Laplace, Multiscale Hessian, Multiscale Harris). Easy-to-use Matlab interface. See Modern features: Software – Slides providing a demonstration of VLFeat and also links to other software. Check also VLFeat hands-on session training\nOpenCV – Various implementations of modern feature detectors and descriptors (SIFT, SURF, FAST, BRIEF, ORB, FREAK, etc.)\nFast Keypoint Detectors for Real-time Applications:\nFAST – High-speed corner detector implementation for a wide variety of platforms\nAGAST – Even faster than the FAST corner detector. A multi-scale version of this method is used for the BRISK descriptor (ECCV 2010).\nBinary Descriptors for Real-Time Applications:\nBRIEF – C++ code for a fast and accurate interest point descriptor (not invariant to rotations and scale) (ECCV 2010)\nORB – OpenCV implementation of the Oriented-Brief (ORB) descriptor (invariant to rotations, but not scale)\nBRISK – Efficient Binary descriptor invariant to rotations and scale. It includes a Matlab mex interface. (ICCV 2011)\nFREAK – Faster than BRISK (invariant to rotations and scale) (CVPR 2012)\nSIFT and SURF Implementations:\nSIFT: VLFeat, OpenCV, Original code by David Lowe, GPU implementation, OpenSIFT\nSURF: Herbert Bay’s code, OpenCV, GPU-SURF\nOther Local Feature Detectors and Descriptors:\nVGG Affine Covariant features – Oxford code for various affine covariant feature detectors and descriptors.\nLIOP descriptor – Source code for the Local Intensity order Pattern (LIOP) descriptor (ICCV 2011).\nLocal Symmetry Features – Source code for matching of local symmetry features under large variations in lighting, age, and rendering style (CVPR 2012).\nGlobal Image Descriptors:\nGIST – Matlab code for the GIST descriptor\nCENTRIST – Global visual descriptor for scene categorization and object detection (PAMI 2011)\nFeature Coding and Pooling\nVGG Feature Encoding Toolkit – Source code for various state-of-the-art feature encoding methods – including Standard hard encoding, Kernel codebook encoding, Locality-constrained linear encoding, and Fisher kernel encoding.\nSpatial Pyramid Matching – Source code for feature pooling based on spatial pyramid matching (widely used for image classification)\nConvolutional Nets and Deep Learning\nEBLearn – C++ Library for Energy-Based Learning. It includes several demos and step-by-step instructions to train classifiers based on convolutional neural networks.\nTorch7 – Provides a matlab-like environment for state-of-the-art machine learning algorithms, including a fast implementation of convolutional neural networks.\nDeep Learning - Various links for deep learning software.\nPart-Based Models\nDeformable Part-based Detector – Library provided by the authors of the original paper (state-of-the-art in PASCAL VOC detection task)\nEfficient Deformable Part-Based Detector – Branch-and-Bound implementation for a deformable part-based detector.\nAccelerated Deformable Part Model – Efficient implementation of a method that achieves the exact same performance of deformable part-based detectors but with significant acceleration (ECCV 2012).\nCoarse-to-Fine Deformable Part Model – Fast approach for deformable object detection (CVPR 2011).\nPoselets – C++ and Matlab versions for object detection based on poselets.\nPart-based Face Detector and Pose Estimation – Implementation of a unified approach for face detection, pose estimation, and landmark localization (CVPR 2012).\nAttributes and Semantic Features\nRelative Attributes – Modified implementation of RankSVM to train Relative Attributes (ICCV 2011).\nObject Bank – Implementation of object bank semantic features (NIPS 2010). See also ActionBank\nClassemes, Picodes, and Meta-class features – Software for extracting high-level image descriptors (ECCV 2010, NIPS 2011, CVPR 2012).\nLarge-Scale Learning\nAdditive Kernels – Source code for fast additive kernel SVM classifiers (PAMI 2013).\nLIBLINEAR – Library for large-scale linear SVM classification.\nVLFeat – Implementation for Pegasos SVM and Homogeneous Kernel map.\nFast Indexing and Image Retrieval\nFLANN – Library for performing fast approximate nearest neighbor.\nKernelized LSH – Source code for Kernelized Locality-Sensitive Hashing (ICCV 2009).\nITQ Binary codes – Code for generation of small binary codes using Iterative Quantization and other baselines such as Locality-Sensitive-Hashing (CVPR 2011).\nINRIA Image Retrieval – Efficient code for state-of-the-art large-scale image retrieval (CVPR 2011).\nObject Detection\nSee Part-based Models and Convolutional Nets above.\nPedestrian Detection at 100fps – Very fast and accurate pedestrian detector (CVPR 2012).\nCaltech Pedestrian Detection Benchmark – Excellent resource for pedestrian detection, with various links for state-of-the-art implementations.\nOpenCV – Enhanced implementation of Viola&Jones real-time object detector, with trained models for face detection.\nEfficient Subwindow Search – Source code for branch-and-bound optimization for efficient object localization (CVPR 2008).\n3D Recognition\nPoint-Cloud Library – Library for 3D image and point cloud processing.\nAction Recognition\nActionBank – Source code for action recognition based on the ActionBank representation (CVPR 2012).\nSTIP Features – software for computing space-time interest point descriptors\nIndependent Subspace Analysis – Look for Stacked ISA for Videos (CVPR 2011)\nVelocity Histories of Tracked Keypoints - C++ code for activity recognition using the velocity histories of tracked keypoints (ICCV 2009)\nDatasets\nAttributes\nAnimals with Attributes – 30,475 images of 50 animals classes with 6 pre-extracted feature representations for each image.\naYahoo and aPascal – Attribute annotations for images collected from Yahoo and Pascal VOC 2008.\nFaceTracer – 15,000 faces annotated with 10 attributes and fiducial points.\nPubFig – 58,797 face images of 200 people with 73 attribute classifier outputs.\nLFW – 13,233 face images of 5,749 people with 73 attribute classifier outputs.\nHuman Attributes – 8,000 people with annotated attributes. Check also this link for another dataset of human attributes.\nSUN Attribute Database – Large-scale scene attribute database with a taxonomy of 102 attributes.\nImageNet Attributes – Variety of attribute labels for the ImageNet dataset.\nRelative attributes – Data for OSR and a subset of PubFig datasets. Check also this link for the WhittleSearch data.\nAttribute Discovery Dataset – Images of shopping categories associated with textual descriptions.\nFine-grained Visual Categorization\nCaltech-UCSD Birds Dataset – Hundreds of bird categories with annotated parts and attributes.\nStanford Dogs Dataset – 20,000 images of 120 breeds of dogs from around the world.\nOxford-IIIT Pet Dataset – 37 category pet dataset with roughly 200 images for each class. Pixel level trimap segmentation is included.\nLeeds Butterfly Dataset – 832 images of 10 species of butterflies.\nOxford Flower Dataset – Hundreds of flower categories.\nFace Detection\nFDDB – UMass face detection dataset and benchmark (5,000+ faces)\nCMU/MIT – Classical face detection dataset.\nFace Recognition\nFace Recognition Homepage – Large collection of face recognition datasets.\nLFW – UMass unconstrained face recognition dataset (13,000+ face images).\nNIST Face Homepage – includes face recognition grand challenge (FRGC), vendor tests (FRVT) and others.\nCMU Multi-PIE – contains more than 750,000 images of 337 people, with 15 different views and 19 lighting conditions.\nFERET – Classical face recognition dataset.\nDeng Cai’s face dataset in Matlab Format – Easy to use if you want play with simple face datasets including Yale, ORL, PIE, and Extended Yale B.\nSCFace – Low-resolution face dataset captured from surveillance cameras.\nHandwritten Digits\nMNIST – large dataset containing a training set of 60,000 examples, and a test set of 10,000 examples.\nPedestrian Detection\nCaltech Pedestrian Detection Benchmark – 10 hours of video taken from a vehicle,350K bounding boxes for about 2.3K unique pedestrians.\nINRIA Person Dataset – Currently one of the most popular pedestrian detection datasets.\nETH Pedestrian Dataset – Urban dataset captured from a stereo rig mounted on a stroller.\nTUD-Brussels Pedestrian Dataset – Dataset with image pairs recorded in an crowded urban setting with an onboard camera.\nPASCAL Human Detection – One of 20 categories in PASCAL VOC detection challenges.\nUSC Pedestrian Dataset – Small dataset captured from surveillance cameras.\nGeneric Object Recognition\nImageNet – Currently the largest visual recognition dataset in terms of number of categories and images.\nTiny Images – 80 million 32x32 low resolution images.\nPascal VOC – One of the most influential visual recognition datasets.\nCaltech 101 / Caltech 256 – Popular image datasets containing 101 and 256 object categories, respectively.\nMIT LabelMe – Online annotation tool for building computer vision databases.\nScene Recognition\nMIT SUN Dataset – MIT scene understanding dataset.\nUIUC Fifteen Scene Categories – Dataset of 15 natural scene categories.\nFeature Detection and Description\nVGG Affine Dataset – Widely used dataset for measuring performance of feature detection and description. CheckVLBenchmarks for an evaluation framework.\nAction Recognition\nBenchmarking Activity Recognition – CVPR 2012 tutorial covering various datasets for action recognition.\nRGBD Recognition\nRGB-D Object Dataset – Dataset containing 300 common household objects\n下面是另一博客整理的资源：\n从cvchina搞到的机器视觉开源处理库汇总，转来了，很给力，还在不断更新。。。\n通用库/General Library\nOpenCV\n无需多言。\nRAVL\nRecognition And Vision Library. 线程安全。强大的IO机制。包含AAM。\nCImg\n很酷的一个图像处理包。整个库只有一个头文件。包含一个基于PDE的光流算法。\n图像，视频IO/Image, Video IO\nFreeImage\nDevIL\nImageMagick\nFFMPEG\nVideoInput\nportVideo\nAR相关/Augmented Reality\nARToolKit\n基于Marker的AR库\nARToolKitPlus\nARToolKit的增强版。实现了更好的姿态估计算法。\nPTAM\n实时的跟踪、SLAM、AR库。无需Marker，模板，内置传感器等。\nBazAR\n基于特征点检测和识别的AR库。\n局部不变特征/Local Invariant Feature\nVLFeat\n目前最好的Sift开源实现。同时包含了KD-tree，KD-Forest，BoW实现。\nFerns\n基于Naive Bayesian Bundle的特征点识别。高速，但占用内存高。\nSIFT By Rob Hess\n基于OpenCV的Sift实现。\n目标检测/Object Detection\nAdaBoost By JianXin.Wu\n又一个AdaBoost实现。训练速度快。\n行人检测 By JianXin.Wu\n基于Centrist和Linear SVM的快速行人检测。\n（近似）最近邻/ANN\nFLANN\n目前最完整的（近似）最近邻开源库。不但实现了一系列查找算法，还包含了一种自动选取最快算法的机制。\nANN\n另外一个近似最近邻库。\nSLAM & SFM\nSceneLib [LGPL]\nmonoSLAM库。由Androw Davison开发。\n图像分割/Segmentation\nSLIC Super Pixel\n使用Simple Linear Iterative Clustering产生指定数目，近似均匀分布的Super Pixel。\n目标跟踪/Tracking\nTLD\n基于Online Random Forest的目标跟踪算法。\nKLT\nKanade-Lucas-Tracker\nOnline boosting trackers\nOnline Boosting Trackers\n直线检测/Line Detection\nDSCC\n基于联通域连接的直线检测算法。\nLSD [GPL]\n基于梯度的，局部直线段检测算子。\n指纹/Finger Print\npHash [GPL]\n基于感知的多媒体文件Hash算法。（提取，对比图像、视频、音频的指纹）\n视觉显著性/Visual Salience\nGlobal Contrast Based Salient Region Detection\nMing-Ming Cheng的视觉显著性算法。\nFFT/DWT\nFFTW [GPL]\n最快，最好的开源FFT。\nFFTReal [WTFPL]\n轻量级的FFT实现。许可证是亮点。\n音频处理/Audio processing\nSTK [Free]\n音频处理，音频合成。\nlibsndfile [LGPL]\n音频文件IO。\nlibsamplerate [GPL ]\n音频重采样。\n小波变换\n快速小波变换（FWT）\nFWT\nBRIEF: Binary Robust Independent Elementary Feature 一个很好的局部特征描述子，里面有FAST corner + BRIEF实现特征点匹配的DEMO：http://cvlab.epfl.ch/software/brief/\nhttp://code.google.com/p/javacv\nJava打包的opencv, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, and ARToolKitPlus库。可以放在Android上用~\nlibHIK,HIK SVM，计算HIK SVM跟Centrist的Lib。http://c2inet.sce.ntu.edu.sg/Jianxin/projects/libHIK/libHIK.htm\n一组视觉显著性检测代码的链接：http://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/\n介绍n款计算机视觉库/人脸识别开源库/软件\n计算机视觉库 OpenCV\nOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业…\n人脸识别 faceservice.cgi\nfaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。\nOpenCV的.NET版 OpenCVDotNet\nOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。\n人脸检测算法 jViolajones\njViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033\nJava视觉处理库 JavaCV\nJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并…\n运动检测程序 QMotion\nQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。\n视频监控系统 OpenVSS\nOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。\n手势识别 hand-gesture-detection\n手势识别，用OpenCV实现\n人脸检测识别 mcvai-tracking\n提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);…\n人脸检测与跟踪库 asmlibrary\nActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。\nLua视觉开发库 libecv\nECV 是 lua 的计算机视觉开发库(目前只提供Linux支持)\nOpenCV的.Net封装 OpenCVSharp\nOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。\n3D视觉库 fvision2010\n基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(“c:/a/im_%03d.jpg”, 0, 20); //ImageSequenceReader* reader = factory.avi(“a.avi”); if (reader == NULL) { …\n基于QT的计算机视觉库 QVision\n基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。\n图像特征提取 cvBlob\ncvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.\n实时图像/视频处理滤波开发包 GShow\nGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。…\n视频捕获 API VideoMan\nVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。\n开放模式识别项目 OpenPR\nPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。\nOpenCV的Python封装 pyopencv\nOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost…\n视觉快速开发平台 qcv\n计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。\n图像捕获 libv4l2cam\n对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出\n计算机视觉算法 OpenVIDIA\nOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;…\n高斯模型点集配准算法 gmmreg\n实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口…\n模式识别和视觉库 RAVL\nRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。\n图像处理和计算机视觉常用算法库 LTI-Lib\nLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具\nOpenCV优化 opencv-dsp-acceleration\n优化了OpenCV库在DSP上的速度。\nC++计算机视觉库 Integrating Vision Toolkit\nIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV\n计算机视觉和机器人技术的工具包 EGT\nThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se…\nOpenCV的扩展库 ImageNets\nImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。\nlibvideogfx\n视频处理、计算机视觉和计算机图形学的快速开发库。\nMatlab计算机视觉包 mVision\nMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。\nScilab的计算机视觉库 SIP\nSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。\nSTAIR Vision Library\nSTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模\n几种图像处理类库的比较\n作者：王先荣\n原文；http://www.cnblogs.com/xrwang/archive/2010/01/26/TheComparisonOfImageProcessingLibraries.html\n前言\n近期需要做一些图像处理方面的学习和研究，首要任务就是选择一套合适的图像处理类库。目前较知名且功能完善的图像处理类库有OpenCv、EmguCv、AForge.NET等等。本文将从许可协议、下载、安装、文档资料、易用性、性能等方面对这些类库进行比较，然后给出选择建议，当然也包括我自己的选择。\n许可协议\n类库\n许可协议\n许可协议网址\n大致介绍\nOpenCv\nBSD\nwww.opensource.org/licenses/bsd-license.html\n在保留原来BSD协议声明的前提下，随便怎么用都行\nEmguCv\nGPL v3\nhttp://www.gnu.org/licenses/gpl-3.0.txt\n你的产品必须也使用GPL协议，开源且免费\n商业授权\nhttp://www.emgu.com/wiki/files/CommercialLicense.txt\n给钱之后可以用于闭源的商业产品\nAForge.net\nLGPL v3\nhttp://www.gnu.org/licenses/lgpl.html\n如果不修改类库源代码，引用该类库的产品可以闭源和（或）收费\n以上三种类库都可以用于开发商业产品，但是EmguCv需要付费；因为我只是用来学习和研究，所以这些许可协议对我无所谓。不过鉴于我们身在中国，如果脸皮厚点，去他丫的许可协议。\n下载\n可以很方便的下载到这些类库，下载地址分别为：\n类库\n下载地址\nOpenCv\nhttp://sourceforge.net/projects/opencvlibrary/files/\nEmguCv\nhttp://www.emgu.com/wiki/index.PHP/Download_And_Installation\nAForge.Net\nhttp://www.aforgenet.com/framework/downloads.html\n安装\n这些类库的安装都比较简单，直接运行安装程序，并点“下一步”即可完成。但是OpenCv在安装完之后还需要一些额外的处理才能在VS2008里面使用，在http://www.opencv.org.cn有一篇名为《VC2008 Express下安装OpenCv 2.0》的文章专门介绍了如何安装OpenCv。\n类库\n安装难易度\n备注\nOpenCv\n比较容易\nVC下使用需要重新编译\nEmguCv\n容易\nAForge.net\n容易\n相信看这篇文章的人都不会被安装困扰。\n文档资料\n类库\n总体评价\n书籍\n网站\n文档\n示例\n社区\n备注\nOpenCv\n中等\n中英文\n中英文\n中英文\n较多\n中文论坛\n有中文资料但不完整\nEmguCv\n少\n无\n英文\n英文\n少\n英文论坛\n论坛人气很差\nAForge.net\n少\n无\n英文\n英文\n少\n英文论坛\n论坛人气很差\nOpenCv有一些中文资料，另外两种的资料全是英文的；不过EmguCv建立在OpenCv的基础上，大部分OpenCv的资料可以用于EmguCv；而AForge.net是原生的.net类库，对GDI+有很多扩展，一些MSDN的资料可以借鉴。如果在查词典的基础上还看不懂英文文档，基本上可以放弃使用这些类库了。\n易用性\n易用性这玩意，主观意志和个人能力对它影响很大，下面是我的看法：\n类库\n易用性\n备注\nOpenCv\n比较差\nOpenCv大多数功能都以C风格函数形式提供，少部分功能以C++类提供。注意：2.0版将更多的功能封装成类了。\nEmguCv\n比较好\n将OpenCv的绝大部分功能都包装成了.net类、结构或者枚举。不过文档不全，还是得对照OpenCv的文档去看才行。\nAForge.net\n好\n纯.net类库，用起来很方便。\n最近几年一直用的是C# ，把C和C++忘记得差不多了，况且本来C/C++我就不太熟，所以对OpenCv的看法恐怕有偏见。\n视觉相关网站\n这段时间因为项目的需要，我一直在折腾计算机视觉，尤其是双目立体视觉，代码、论文、工具箱等……占用了我几乎90%的工作时间，还在一点点地摸索，但进度实在不敢恭维，稍后我会把情况作个总结。\n今天的主要任务就是和大家分享一些鄙人收藏的认为相当研究价值的网页：\nOxford大牛：Andrew Zisserman，http://www.robots.ox.ac.uk/~vgg/hzbook/code/，此人主要研究多幅图像的几何学，该网站提供了部分工具，相当实用，还有例子\n西澳大利亚大学的Peter Kovesi：http://www.csse.uwa.edu.au/~pk/research/matlabfns/，提供了一些基本的matlab工具，主要内容涉及Computer Vision, Image Processing\nCMU：http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html,该网站是我的最爱，尤其后面这个地址http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/v-groups.html，在这里提供了世界各地机构、大学在Computer Vision所涉及各领域的研究情况，包括Image Processing, Machine Vision，我后来也是通过它连接到了很多国外的网站\nCambridge：http://mi.eng.cam.ac.uk/milab.html，这是剑桥大学的机器智能实验室，里面有三个小组，Computer Vision & Robotics, Machine Intelligence, Speech，目前为止，Computer Vision & Robotics的一些研究成果对我日后的帮助可能会比较大，所以在此提及\n大量计算机视觉方面的原版电子书：http://homepages.inf.ed.ac.uk/rbf/CVonline/books.htm，我今天先下了本Zisserman的书，呵呵，国外的原版书，虽然都是比较老的，但是对于基础的理解学习还是很有帮助的，至于目前的研究现状只能通过论文或者一些研究小组的网站\nstanford：http://ai.stanford.edu/~asaxena/reconstruction3d/，这个网站是Andrew N.G老师和一个印度阿三的博士一起维护的，主要对于单张照片的三维重建，尤其他有个网页make3d.stanford.edu可以让你自己上传你的照片，通过网站来重建三维模型，这个网站对于刚开始接触Computer Vision的我来说，如获至宝，但有个致命问题就是make3d已经无法注册，我也多次给Andrew和印度阿三email，至今未回，郁闷，要是有这个网站的帐号，那还是相当爽的，不知道是不是由于他们的邮箱把我的email当成垃圾邮件过滤，哎，但这个stanford网站的贡献主要是代码，有很多computer vision的基础工具，貌似40M左右，全都是基于matlab的\ncaltech：http://www.vision.caltech.edu/bouguetj/calib_doc/，这是我们Computer Vision老师课件上的连接，主要是用于摄像机标定的工具集，当然也有涉及对标定图像三维重建的前期处理过程\nJP Tarel：http://perso.lcpc.fr/tarel.jean-philippe/，这是他的个人主页，也是目前为止我发的email中，唯一一个给我回信的老外，因为我需要重建练习的正是他的图片集，我读过他的论文，但没有涉及代码的内容，再加上又是94年以前的论文，很多相关的引文，我都无法下载，在我的再三追问下，Tarel教授只告诉我，你可以按照我的那篇论文对足球进行重建，可是…你知道吗，你有很多图像处理的引文都下不了了，我只知道你通过那篇文章做了图像的预处理，根本不知道具体过程，当然我有幸找到过一篇90左右的论文，讲的是region-based segmentation，可是这文章里所有引文又是找不到的….悲剧的人生\n开源软件网站：www.sourceforge.net\n最后就是我们工大的Computer Vision大牛：sychen.com，我们Computer Vision课的老师，谦虚、低调，很有学者风范\n总结：目前为止，我的个人感觉就是国外学者的论文包括刊登的资料大部分都是对原理进行的说明，并不是很在意具体的代码实现的讲解，而我却过分的关注于代码的实现，忽视Computer Vision的原理，国外学者对与自己相关领域的研究现状了解相当充分，对自己的工作进度更新也很勤快，很多好的网站我并没有完全列出来，在这里只是提了主要的几个，在这方面，我们国内的研究氛围有所不及，当然我选择的一些网站可能更多的是个人小组的研究介绍，不像一些专门从事领域研究的机构，会有那么多的权威资料，国外的网站有个很好的地方，就是有很多的免费资源，免费的matlab或者openCV工具集，免费的论文下载，课件下载等等，在这方面国内对于研究资源的共享，做得又有所差距，同样，国外的研究工具很多样，主要是matlab，一些发布的demo都使用C++写的，不过今天看到一个西班牙的研究机构（university of las palmas）用了个XMW的软件平台来实现图片的三维重建，data用的是人脸，而且国外的很多源代码基本上是在linux平台下完成的，对于我来说又是不方便，哎，可能要考虑装VM Ware了，不然双系统太累…..\n目前，Computer Vision是全世界范围内自动化、计算机、数学领域的研究热点，综合性高，应用于医疗、军事、民用等等领域，其中有突出成绩的还是一下几所学校（个人见解）：Cambridge(UK), Oxford(UK), CMU(US),Stanford(US),MIT(US),U.C.Berkeley(US)，而UK的两所老牌高校，他们的实际应用领域丝毫不逊于stanford和CMU….\n世界就是这样，当你不断的接触，不断的扩展你所能够及的边际就会发现自己越来越无知，还有很多很多不知道，发现还有很多自己都想不到但却已经实现的东西…..\n革命远未成功，同志仍须努力，在CV的道路上前进…….\n三篇整理了很多特征、机器学习算法、计算机视觉和模式识别领域经典论文等源码资源的\n博客：\n1.http://www.cnblogs.com/ajian005/archive/2012/11/04/2841171.html\n2.http://blog.csdn.net/yf0811240333/article/details/42076677\n3.http://www.cnblogs.com/einyboy/p/3594432.html\n如果大家有什么好的资源，希望大家能在留言上共享一下，方便大家学习～～"}
{"content2":"经典论文\n计算机视觉论文\nImageNet分类\n物体检测\n物体跟踪\n低级视觉\n边缘检测\n语义分割\n视觉注意力和显著性\n物体识别\n人体姿态估计\nCNN原理和性质（Understanding CNN）\n图像和语言\n图像解说\n视频解说\n图像生成\n微软ResNet\n论文：用于图像识别的深度残差网络\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1512.03385v1.pdf\n微软PRelu（随机纠正线性单元/权重初始化）\n论文：深入学习整流器：在ImageNet分类上超越人类水平\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1502.01852.pdf\n谷歌Batch Normalization\n论文：批量归一化：通过减少内部协变量来加速深度网络训练\n作者：Sergey Ioffe, Christian Szegedy\n链接：http://arxiv.org/pdf/1502.03167.pdf\n谷歌GoogLeNet\n论文：更深的卷积，CVPR 2015\n作者：Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich\n链接：http://arxiv.org/pdf/1409.4842.pdf\n牛津VGG-Net\n论文：大规模视觉识别中的极深卷积网络，ICLR 2015\n作者：Karen Simonyan & Andrew Zisserman\n链接：http://arxiv.org/pdf/1409.1556.pdf\nAlexNet\n论文：使用深度卷积神经网络进行ImageNet分类\n作者：Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n链接：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n物体检测\nPVANET\n论文：用于实时物体检测的深度轻量神经网络（PVANET：Deep but Lightweight Neural Networks for Real-time Object Detection）\n作者：Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park\n链接：http://arxiv.org/pdf/1608.08021\n纽约大学OverFeat\n论文：使用卷积网络进行识别、定位和检测（OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks），ICLR 2014\n作者：Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun\n链接：http://arxiv.org/pdf/1312.6229.pdf\n伯克利R-CNN\n论文：精确物体检测和语义分割的丰富特征层次结构（Rich feature hierarchies for accurate object detection and semantic segmentation），CVPR 2014\n作者：Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik\n链接：http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\n微软SPP\n论文：视觉识别深度卷积网络中的空间金字塔池化（Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition），ECCV 2014\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1406.4729.pdf\n微软Fast R-CNN\n论文：Fast R-CNN\n作者：Ross Girshick\n链接：http://arxiv.org/pdf/1504.08083.pdf\n微软Faster R-CNN\n论文：使用RPN走向实时物体检测（Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks）\n作者：任少卿、何恺明、Ross Girshick、孙剑\n链接：http://arxiv.org/pdf/1506.01497.pdf\n牛津大学R-CNN minus R\n论文：R-CNN minus R\n作者：Karel Lenc, Andrea Vedaldi\n链接：http://arxiv.org/pdf/1506.06981.pdf\n端到端行人检测\n论文：密集场景中端到端的行人检测（End-to-end People Detection in Crowded Scenes）\n作者：Russell Stewart, Mykhaylo Andriluka\n链接：http://arxiv.org/pdf/1506.04878.pdf\n实时物体检测\n论文：你只看一次：统一实时物体检测（You Only Look Once: Unified, Real-Time Object Detection）\n作者：Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\n链接：http://arxiv.org/pdf/1506.02640.pdf\nInside-Outside Net\n论文：使用跳跃池化和RNN在场景中检测物体（Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks）\n作者：Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick\n链接：http://arxiv.org/abs/1512.04143.pdf\n微软ResNet\n论文：用于图像识别的深度残差网络\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1512.03385v1.pdf\nR-FCN\n论文：通过区域全卷积网络进行物体识别（R-FCN: Object Detection via Region-based Fully Convolutional Networks）\n作者：代季峰，李益，何恺明，孙剑\n链接：http://arxiv.org/abs/1605.06409\nSSD\n论文：单次多框检测器（SSD: Single Shot MultiBox Detector）\n作者：Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg\n链接：http://arxiv.org/pdf/1512.02325v2.pdf\n速度/精度权衡\n论文：现代卷积物体检测器的速度/精度权衡（Speed/accuracy trade-offs for modern convolutional object detectors）\n作者：Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy\n链接：http://arxiv.org/pdf/1611.10012v1.pdf\n物体跟踪\n论文：用卷积神经网络通过学习可区分的显著性地图实现在线跟踪（Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network）\n作者：Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han\n地址：arXiv:1502.06796.\n论文：DeepTrack：通过视觉跟踪的卷积神经网络学习辨别特征表征（DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking）\n作者：Hanxi Li, Yi Li and Fatih Porikli\n发表： BMVC, 2014.\n论文：视觉跟踪中，学习深度紧凑图像表示（Learning a Deep Compact Image Representation for Visual Tracking）\n作者：N Wang, DY Yeung\n发表：NIPS, 2013.\n论文：视觉跟踪的分层卷积特征（Hierarchical Convolutional Features for Visual Tracking）\n作者：Chao Ma, Jia-Bin Huang, Xiaokang Yang and Ming-Hsuan Yang\n发表： ICCV 2015\n论文：完全卷积网络的视觉跟踪（Visual Tracking with fully Convolutional Networks）\n作者：Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu,\n发表：ICCV 2015\n论文：学习多域卷积神经网络进行视觉跟踪（Learning Multi-Domain Convolutional Neural Networks for Visual Tracking）\n作者：Hyeonseob Namand Bohyung Han\n对象识别（Object Recognition）\n论文：卷积神经网络弱监督学习（Weakly-supervised learning with convolutional neural networks）\n作者：Maxime Oquab，Leon Bottou，Ivan Laptev，Josef Sivic，CVPR，2015\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf\nFV-CNN\n论文：深度滤波器组用于纹理识别和分割（Deep Filter Banks for Texture Recognition and Segmentation）\n作者：Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Cimpoi_Deep_Filter_Banks_2015_CVPR_paper.pdf\n人体姿态估计（Human Pose Estimation）\n论文：使用 Part Affinity Field的实时多人2D姿态估计（Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields）\n作者：Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh, CVPR, 2017.\n论文：Deepcut：多人姿态估计的联合子集分割和标签（Deepcut: Joint subset partition and labeling for multi person pose estimation）\n作者：Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele, CVPR, 2016.\n论文：Convolutional pose machines\n作者：Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh, CVPR, 2016.\n论文：人体姿态估计的 Stacked hourglass networks（Stacked hourglass networks for human pose estimation）\n作者：Alejandro Newell, Kaiyu Yang, and Jia Deng, ECCV, 2016.\n论文：用于视频中人体姿态估计的Flowing convnets（Flowing convnets for human pose estimation in videos）\n作者：Tomas Pfister, James Charles, and Andrew Zisserman, ICCV, 2015.\n论文：卷积网络和人类姿态估计图模型的联合训练（Joint training of a convolutional network and a graphical model for human pose estimation）\n作者：Jonathan J. Tompson, Arjun Jain, Yann LeCun, Christoph Bregler, NIPS, 2014.\n理解CNN\n论文：通过测量同变性和等价性来理解图像表示(Understanding image representations by measuring their equivariance and equivalence)\n作者：Karel Lenc, Andrea Vedaldi, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf\n论文：深度神经网络容易被愚弄：无法识别的图像的高置信度预测（Deep Neural Networks are Easily Fooled:High Confidence Predictions for Unrecognizable Images）\n作者：Anh Nguyen, Jason Yosinski, Jeff Clune, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf\n论文：通过反演理解深度图像表示（Understanding Deep Image Representations by Inverting Them）\n作者：Aravindh Mahendran, Andrea Vedaldi, CVPR, 2015\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf\n论文：深度场景CNN中的对象检测器（Object Detectors Emerge in Deep Scene CNNs）\n作者：Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, ICLR, 2015.\n链接：http://arxiv.org/abs/1412.6856\n论文：用卷积网络反演视觉表示（Inverting Visual Representations with Convolutional Networks）\n作者：Alexey Dosovitskiy, Thomas Brox, arXiv, 2015.\n链接：http://arxiv.org/abs/1506.02753\n论文：可视化和理解卷积网络（Visualizing and Understanding Convolutional Networks）\n作者：Matthrew Zeiler, Rob Fergus, ECCV, 2014.\n链接：http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\n图像与语言\n图像说明（Image Captioning）\nUCLA / Baidu\n用多模型循环神经网络解释图像（Explain Images with Multimodal Recurrent Neural Networks）\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille, arXiv:1410.1090\nhttp://arxiv.org/pdf/1410.1090\nToronto\n使用多模型神经语言模型统一视觉语义嵌入（Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models）\nRyan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, arXiv:1411.2539.\nhttp://arxiv.org/pdf/1411.2539\nBerkeley\n用于视觉识别和描述的长期循环卷积网络（Long-term Recurrent Convolutional Networks for Visual Recognition and Description）\nJeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, arXiv:1411.4389.\nhttp://arxiv.org/pdf/1411.4389\nGoogle\n看图写字：神经图像说明生成器（Show and Tell: A Neural Image Caption Generator）\nOriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, arXiv:1411.4555.\nhttp://arxiv.org/pdf/1411.4555\nStanford\n用于生成图像描述的深度视觉语义对齐（Deep Visual-Semantic Alignments for Generating Image Description）\nAndrej Karpathy, Li Fei-Fei, CVPR, 2015.\nWeb：http://cs.stanford.edu/people/karpathy/deepimagesent/\nPaper：http://cs.stanford.edu/people/karpathy/cvpr2015.pdf\nUML / UT\n使用深度循环神经网络将视频转换为自然语言（Translating Videos to Natural Language Using Deep Recurrent Neural Networks）\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, NAACL-HLT, 2015.\nhttp://arxiv.org/pdf/1412.4729\nCMU / Microsoft\n学习图像说明生成的循环视觉表示（Learning a Recurrent Visual Representation for Image Caption Generation）\nXinlei Chen, C. Lawrence Zitnick, arXiv:1411.5654.\nXinlei Chen, C. Lawrence Zitnick, Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation, CVPR 2015\nhttp://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf\nMicrosoft\n从图像说明到视觉概念（From Captions to Visual Concepts and Back）\nHao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig, CVPR, 2015.\nhttp://arxiv.org/pdf/1411.4952\nUniv. Montreal / Univ. Toronto\nShow, Attend, and Tell：视觉注意力与神经图像标题生成（Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention）\nKelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, arXiv:1502.03044 / ICML 2015\nhttp://www.cs.toronto.edu/~zemel/documents/captionAttn.pdf\nIdiap / EPFL / Facebook\n基于短语的图像说明（Phrase-based Image Captioning）\nRemi Lebret, Pedro O. Pinheiro, Ronan Collobert, arXiv:1502.03671 / ICML 2015\nhttp://arxiv.org/pdf/1502.03671\nUCLA / Baidu\n像孩子一样学习：从图像句子描述快速学习视觉的新概念（Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images）\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L. Yuille, arXiv:1504.06692\nhttp://arxiv.org/pdf/1504.06692\nMS + Berkeley\n探索图像说明的最近邻方法（ Exploring Nearest Neighbor Approaches for Image Captioning）\nJacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, arXiv:1505.04467\nhttp://arxiv.org/pdf/1505.04467.pdf\n图像说明的语言模型（Language Models for Image Captioning: The Quirks and What Works）\nJacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, arXiv:1505.01809\nhttp://arxiv.org/pdf/1505.01809.pdf\n阿德莱德\n具有中间属性层的图像说明（ Image Captioning with an Intermediate Attributes Layer）\nQi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, arXiv:1506.01144\n蒂尔堡\n通过图片学习语言(Learning language through pictures)\nGrzegorz Chrupala, Akos Kadar, Afra Alishahi, arXiv:1506.03694\n蒙特利尔大学\n使用基于注意力的编码器-解码器网络描述多媒体内容（Describing Multimedia Content using Attention-based Encoder-Decoder Networks）\nKyunghyun Cho, Aaron Courville, Yoshua Bengio, arXiv:1507.01053\n康奈尔\n图像表示和神经图像说明的新领域（Image Representations and New Domains in Neural Image Captioning）\nJack Hessel, Nicolas Savva, Michael J. Wilber, arXiv:1508.02091\nMS + City Univ. of HongKong\nLearning Query and Image Similarities with Ranking Canonical Correlation Analysis\nTing Yao, Tao Mei, and Chong-Wah Ngo, ICCV, 2015\n视频字幕（Video Captioning）\n伯克利\nJeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR, 2015.\n犹他州/ UML / 伯克利\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729.\n微软\nYingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui, Joint Modeling Embedding and Translation to Bridge Video and Language, arXiv:1505.01861.\n犹他州/ UML / 伯克利\nSubhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence–Video to Text, arXiv:1505.00487.\n蒙特利尔大学/ 舍布鲁克\nLi Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029\nMPI / 伯克利\nAnna Rohrbach, Marcus Rohrbach, Bernt Schiele, The Long-Short Story of Movie Description, arXiv:1506.01698\n多伦多大学 / MIT\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books, arXiv:1506.06724\n蒙特利尔大学\nKyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053\nTAU / 美国南加州大学\nDotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf, Temporal Tessellation for Video Annotation and Summarization, arXiv:1612.06950.\n图像生成\n卷积/循环网络\n论文：Conditional Image Generation with PixelCNN Decoders”\n作者：Aäron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu\n论文：Learning to Generate Chairs with Convolutional Neural Networks\n作者：Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox\n发表：CVPR, 2015.\n论文：DRAW: A Recurrent Neural Network For Image Generation\n作者：Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra\n发表：ICML, 2015.\n对抗网络\n论文：生成对抗网络（Generative Adversarial Networks）\n作者：Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n发表：NIPS, 2014.\n论文：使用对抗网络Laplacian Pyramid 的深度生成图像模型（Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks）\n作者：Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus\n发表：NIPS, 2015.\n论文：生成模型演讲概述 （A note on the evaluation of generative models）\n作者：Lucas Theis, Aäron van den Oord, Matthias Bethge\n发表：ICLR 2016.\n论文：变分自动编码深度高斯过程（Variationally Auto-Encoded Deep Gaussian Processes）\n作者：Zhenwen Dai, Andreas Damianou, Javier Gonzalez, Neil Lawrence\n发表：ICLR 2016.\n论文：用注意力机制从字幕生成图像 （Generating Images from Captions with Attention）\n作者：Elman Mansimov, Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov\n发表： ICLR 2016\n论文：分类生成对抗网络的无监督和半监督学习（Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks）\n作者：Jost Tobias Springenberg\n发表：ICLR 2016\n论文：用一个对抗检测表征（Censoring Representations with an Adversary）\n作者：Harrison Edwards, Amos Storkey\n发表：ICLR 2016\n论文：虚拟对抗训练实现分布式顺滑 （Distributional Smoothing with Virtual Adversarial Training）\n作者：Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii\n发表：ICLR 2016\n论文：自然图像流形上的生成视觉操作（Generative Visual Manipulation on the Natural Image Manifold）\n作者：朱俊彦, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros\n发表： ECCV 2016.\n论文：深度卷积生成对抗网络的无监督表示学习（Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks）\n作者：Alec Radford, Luke Metz, Soumith Chintala\n发表： ICLR 2016\n问题回答\n弗吉尼亚大学 / 微软研究院\n论文：VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.\n作者：Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh\nMPI / 伯克利\n论文：Ask Your Neurons: A Neural-based Approach to Answering Questions about Images\n作者：Mateusz Malinowski, Marcus Rohrbach, Mario Fritz,\n发布 ： arXiv:1505.01121.\n多伦多\n论文： Image Question Answering: A Visual Semantic Embedding Model and a New Dataset\n作者：Mengye Ren, Ryan Kiros, Richard Zemel\n发表： arXiv:1505.02074 / ICML 2015 deep learning workshop.\n百度/ 加州大学洛杉矶分校\n作者：Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, 徐伟\n论文：Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering\n发表： arXiv:1505.05612.\nPOSTECH（韩国）\n论文：Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction\n作者：Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han\n发表： arXiv:1511.05765\nCMU / 微软研究院\n论文：Stacked Attention Networks for Image Question Answering\n作者：Yang, Z., He, X., Gao, J., Deng, L., & Smola, A. (2015)\n发表： arXiv:1511.02274.\nMetaMind\n论文：Dynamic Memory Networks for Visual and Textual Question Answering\n作者：Xiong, Caiming, Stephen Merity, and Richard Socher\n发表： arXiv:1603.01417 (2016).\n首尔国立大学 + NAVER\n论文：Multimodal Residual Learning for Visual QA\n作者：Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang\n发表：arXiv:1606:01455\nUC Berkeley + 索尼\n论文：Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\n作者：Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach\n发表：arXiv:1606.01847\nPostech\n论文：Training Recurrent Answering Units with Joint Loss Minimization for VQA\n作者：Hyeonwoo Noh and Bohyung Han\n发表： arXiv:1606.03647\n首尔国立大学 + NAVER\n论文： Hadamard Product for Low-rank Bilinear Pooling\n作者：Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhan\n发表：arXiv:1610.04325.\n视觉注意力和显著性\n论文：Predicting Eye Fixations using Convolutional Neural Networks\n作者：Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu\n发表：CVPR, 2015.\n学习地标的连续搜索\n作者：Learning a Sequential Search for Landmarks\n论文：Saurabh Singh, Derek Hoiem, David Forsyth\n发表：CVPR, 2015.\n视觉注意力机制实现多物体识别\n论文：Multiple Object Recognition with Visual Attention\n作者：Jimmy Lei Ba, Volodymyr Mnih, Koray Kavukcuoglu,\n发表：ICLR, 2015.\n视觉注意力机制的循环模型\n作者：Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu\n论文：Recurrent Models of Visual Attention\n发表：NIPS, 2014.\n低级视觉\n超分辨率\nIterative Image Reconstruction\nSven Behnke: Learning Iterative Image Reconstruction. IJCAI, 2001.\nSven Behnke: Learning Iterative Image Reconstruction in the Neural Abstraction Pyramid. International Journal of Computational Intelligence and Applications, vol. 1, no. 4, pp. 427-438, 2001.\nSuper-Resolution (SRCNN)\nChao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, ECCV, 2014.\nChao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Image Super-Resolution Using Deep Convolutional Networks, arXiv:1501.00092.\nVery Deep Super-Resolution\nJiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Accurate Image Super-Resolution Using Very Deep Convolutional Networks, arXiv:1511.04587, 2015.\nDeeply-Recursive Convolutional Network\nJiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Deeply-Recursive Convolutional Network for Image Super-Resolution, arXiv:1511.04491, 2015.\nCasade-Sparse-Coding-Network\nZhaowen Wang, Ding Liu, Wei Han, Jianchao Yang and Thomas S. Huang, Deep Networks for Image Super-Resolution with Sparse Prior. ICCV, 2015.\nPerceptual Losses for Super-Resolution\nJustin Johnson, Alexandre Alahi, Li Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, arXiv:1603.08155, 2016.\nSRGAN\nChristian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, arXiv:1609.04802v3, 2016.\n其他应用\nOptical Flow (FlowNet)\nPhilipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox, FlowNet: Learning Optical Flow with Convolutional Networks, arXiv:1504.06852.\nCompression Artifacts Reduction\nChao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, Compression Artifacts Reduction by a Deep Convolutional Network, arXiv:1504.06993.\nBlur Removal\nChristian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf, Learning to Deblur, arXiv:1406.7444\nJian Sun, Wenfei Cao, Zongben Xu, Jean Ponce, Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal, CVPR, 2015\nImage Deconvolution\nLi Xu, Jimmy SJ. Ren, Ce Liu, Jiaya Jia, Deep Convolutional Neural Network for Image Deconvolution, NIPS, 2014.\nDeep Edge-Aware Filter\nLi Xu, Jimmy SJ. Ren, Qiong Yan, Renjie Liao, Jiaya Jia, Deep Edge-Aware Filters, ICML, 2015.\nComputing the Stereo Matching Cost with a Convolutional Neural Network\nJure Žbontar, Yann LeCun, Computing the Stereo Matching Cost with a Convolutional Neural Network, CVPR, 2015.\nColorful Image Colorization Richard Zhang, Phillip Isola, Alexei A. Efros, ECCV, 2016\nFeature Learning by Inpainting\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros, Context Encoders: Feature Learning by Inpainting, CVPR, 2016\n边缘检测\nSaining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.\nDeepEdge\nGedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.\nDeepContour\nWei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.\n语义分割\nSEC: Seed, Expand and Constrain\nAlexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016.\nAdelaide\nGuosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. (1st ranked in VOC2012)\nGuosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. (4th ranked in VOC2012)\nDeep Parsing Network (DPN)\nZiwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 / ICCV 2015 (2nd ranked in VOC 2012)\nCentraleSuperBoundaries, INRIA\nIasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)\nBoxSup\nJifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)\nPOSTECH\nHyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. (7th ranked in VOC2012)\nSeunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924.\nSeunghoon Hong,Junhyuk Oh,Bohyung Han, andHonglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928\nConditional Random Fields as Recurrent Neural Networks\nShuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)\nDeepLab\nLiang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. (9th ranked in VOC2012)\nZoom-out\nMohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015\nJoint Calibration\nHolger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.\nFully Convolutional Networks for Semantic Segmentation\nJonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.\nHypercolumn\nBharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.\nDeep Hierarchical Parsing\nAbhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015.\nLearning Hierarchical Features for Scene Labeling\nClement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.\nClement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.\nUniversity of Cambridge\nVijay Badrinarayanan, Alex Kendall and Roberto Cipolla “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.” arXiv preprint arXiv:1511.00561, 2015.\nAlex Kendall, Vijay Badrinarayanan and Roberto Cipolla “Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.” arXiv preprint arXiv:1511.02680, 2015.\nPrinceton\nFisher Yu, Vladlen Koltun, “Multi-Scale Context Aggregation by Dilated Convolutions”, ICLR 2016\nUniv. of Washington, Allen AI\nHamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, “Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing”, ICCV, 2015\nINRIA\nIasonas Kokkinos, “Pusing the Boundaries of Boundary Detection Using deep Learning”, ICLR 2016\nUCSB\nNiloufar Pourian, S. Karthikeyan, and B.S. Manjunath, “Weakly supervised graph based semantic segmentation by learning communities of image-parts”, ICCV, 2015\n其他资源\n课程\n深度视觉\n[斯坦福] CS231n: Convolutional Neural Networks for Visual Recognition\n[香港中文大学] ELEG 5040: Advanced Topics in Signal Processing(Introduction to Deep Learning)\n· 更多深度课程推荐\n[斯坦福] CS224d: Deep Learning for Natural Language Processing\n[牛津 Deep Learning by Prof. Nando de Freitas\n[纽约大学] Deep Learning by Prof. Yann LeCun\n图书\n免费在线图书\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\nNeural Networks and Deep Learning by Michael Nielsen\nDeep Learning Tutorial by LISA lab, University of Montreal\n视频\n演讲\nDeep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng\nRecent Developments in Deep Learning By Geoff Hinton\nThe Unreasonable Effectiveness of Deep Learning by Yann LeCun\nDeep Learning of Representations by Yoshua bengio\n软件\n框架\nTensorflow: An open source software library for numerical computation using data flow graph by Google [Web]\nTorch7: Deep learning library in Lua, used by Facebook and Google Deepmind [Web]\nTorch-based deep learning libraries: [torchnet],\nCaffe: Deep learning framework by the BVLC [Web]\nTheano: Mathematical library in Python, maintained by LISA lab [Web]\nTheano-based deep learning libraries: [Pylearn2], [Blocks], [Keras], [Lasagne]\nMatConvNet: CNNs for MATLAB [Web]\nMXNet: A flexible and efficient deep learning library for heterogeneous distributed systems with multi-language support [Web]\nDeepgaze: A computer vision library for human-computer interaction based on CNNs [Web]\n应用\n对抗训练 Code and hyperparameters for the paper “Generative Adversarial Networks” [Web]\n理解与可视化 Source code for “Understanding Deep Image Representations by Inverting Them,” CVPR, 2015. [Web]\n词义分割 Source code for the paper “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR, 2014. [Web] ； Source code for the paper “Fully Convolutional Networks for Semantic Segmentation,” CVPR, 2015. [Web]\n超分辨率 Image Super-Resolution for Anime-Style-Art [Web]\n边缘检测 Source code for the paper “DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection,” CVPR, 2015. [Web]\nSource code for the paper “Holistically-Nested Edge Detection”, ICCV 2015. [Web]\n讲座\n[CVPR 2014] Tutorial on Deep Learning in Computer Vision\n[CVPR 2015] Applied Deep Learning for Computer Vision with Torch\n博客\nDeep down the rabbit hole: CVPR 2015 and beyond@Tombone’s Computer Vision Blog\nCVPR recap and where we’re going@Zoya Bylinskii (MIT PhD Student)’s Blog\nFacebook’s AI Painting@Wired\nInceptionism: Going Deeper into Neural Networks@Google Research\nImplementing Neural networks"}
{"content2":"计算机视觉的发展历史：\n动物进化出眼睛；生物视觉-》机器视觉-》照相机；\n生物学家开始研究视觉的机理，Hubel & Wiesel，1959，他们的问题是：哺乳动物的视觉处理机制是怎样的呢？他们将电极插进主要控制猫视觉的后脑上的初级视觉皮层（V1），然后观察，何种刺激会引起视觉皮层神经元的反应。他们发现猫的大脑的初级视觉皮层有各种各样的细胞，其中最重要的细胞是当它们朝着某个特定的方向运动的时候，对面向边缘产生反应的细胞。当然还有更加复杂的细胞，但是总的来说，它们发现视觉初级是始于视觉世界的简单结构，面向边缘，沿着视觉处理途径的移动，信息也在变化，大脑建立了复杂的视觉信息，直到它可以识别更为复杂的视觉世界。\n计算机视觉的历史是从60年代初开始的，Block World 是由Larry Roberts出版的一部作品，被广泛地称为计算机视觉的第一篇博士论文，其中视觉世界被简化为简单的几何形状，目的是能够识别它们，重建这些形状是什么。1966年MIT的暑期视觉项目，目的是为了构建视觉系统的重要组成部分。David Marr，一个MIT 视觉科学家提出了使得计算机识别视觉世界的算法，他指出，为了获取视觉世界完整的3D图像，需要经历几个阶段：第一个阶段是原始草图，大部分边缘、端点和虚拟线条，这是受到了神经科学家的启发，Hubel &Wiesel 告诉我们视觉处理的早期阶段有很多关于像边缘的简单结构；第二阶段是David Marr 所说的“2.5维草图”我们开始将表面、深度信息、不同的层次以及视觉场景的不连续性拼凑在一起的；最后一个阶段是将所有的内容放在一起，组成一个3D模型。这是一个非常理想化的思想过程，这种思维方式实际上已经在计算机视觉领域影响了几十年。这也是一个非常直观的方式，并考虑如何解构视觉信息。\n七十年代另外一个非常重要的工作（Brooks&Binford,1979 Fischler & Elschlager 1973），这个时候他们提出了一个问题，我们如何越过简单的块状世界，开始识别和表示现实世界的对象。70年代是一个没有数据可用的时代，计算机的速度很慢，计算机科学家开始思考如何识别和表示对象，在斯坦福大学的帕洛阿尔托以及斯里兰卡提出了类似的想法，一个被称为广义圆柱体，一个被称为圆形结构，他们的基本思想是每个对象都是由简单的几何图单位组成，任何一种表示的方法就是讲物体的复杂结构，简约成一个集合体，有更简单的形状和几何结构，这些研究已经影响了很长很长的一段时间。\n80年代，David Lowe思考如何重建或者识别由简单的物体结构组成的视觉空间，他尝试识别剃须刀，通过先和边缘进行构建，其中大部分都是直线以及直线之间的组合。那个时候由于样本小，物体识别是很难的。\n如果物体识别太难了，那么我们首先要做的是目标分割，这个任务就是把一张图片中的像素点归类到有意义的区域，我们可能不知道这些像素点组合到一起是一个人型，但是我们可以把这些属于这人的像素点从背景中抠出来，这个过程就叫做图像分割，这项工作是由Berkeley的 Jitendra Malik和他的学生Jianbo Shi 所完成的。他们用一个图论算法对图像进行分割，还有另外一个问题，先于其他计算机视觉问题有进展，也就是面部检测，脸部是人类最重要的部位之一。\n1999-2000年机器学习技术，特别是统计机器学习方法，开始加速发展，出现了很多方法：支持向量机模型，boosting方法，图模型。有一种工作做出了很多贡献，技术使用AdaBoost 算法进行实时面部检测，由Paul Viola和Michal Jones 完成。在他们发表论文后的第五年，也就是2006年，富士康推出了第一个具有实时面部识别的照相机。这是从基础科学研究到实际应用的一个快速转化，关于如何才能能够做到更好的目标识别，这是一个我们可以继续研究的领域。从90年代末到2000年的前十年有一个非常有影响力的思想方法是基于特征的目标识别，这里有一个影响深远的工作，由 David Lowe完成，叫做SIFT特征，思路就是去匹配整个目标。例如这里有一个stop标识去匹配另外一个stop标识是非常困难的，因为有很多变化的因素，比如相机的角度、遮挡、视角、光线以及目标自身的内在变化，但是可以得到一些启发，通过观察目标的某些部分，某些特征是能够在变化中保持不变性，所以目标识别的首要任务是在目标上确认这些关键的特征，然后把这些特征与相似的特征进行匹配，它比匹配整个目标要容易得多。我们这个领域另外一些进展是识别整幅图的场景，有一个算法叫空间金字塔匹配，背后的思想是图片里面有各种特征，这些特征可以告诉我们这是哪种场景，到底是风景还是厨房，或者是高速公路等等 。这个算法从图片的各部分，各个像素抽取特征，并把他们放在一起，作为一个特征描述符，然后在特征描述符上做一个支持向量机。有个在人类认知方面很类似的工作正处于风头浪尖。有些工作是把这些特征放在一起之后，研究如果在实际图片中合理地设计人体姿态和辨认人体姿态，这方面一个工作被称为方向梯度直方图；另外一个被称为可变形部件模型。\n可以看到我们从60年代、70年代、80年代一步步走到20世纪，有一件事情一直在变化，就是图片的质量，随着互联网的发展，随着数码相机的发展，计算机视觉的研究也能拥有更好的数据了，计算机视觉在21世纪早期提出了一个非常重要的基本问题，我们一直在目标识别，但是直到21世纪的早期，我们才开始真正拥有标注的数据集，能供我们衡量在弥补识别方面取得的成果，其中最具有影响力的标记数据集之一叫PASCAL Visual Object Challenge 这个数据集由20个类别的图片，数据集中的每个种类都有成千上万张图片，\n现场不同的团队开发算法来和数据测试集做对抗训练，来看有没有优化，这里有一张图表列举了从2007年到2012年在基准数据集上检测图像中的20中目标的检测效果，可以看到在稳步提升。在差不多时候，普林斯顿和斯坦福中的一批人开始，向我们或者说我们这个领域提出了一个更加困难的问题，我们是否具备了识别真实世界中的每一个物体的能力，或者说大部分物体。这个问题是由机器学习中的一个现象驱动的，就是大部分的机器学习算法，无论是图模型，还是支持向量机或者是AdaBoost都可能会在训练过程中过拟合，部分原因是可视化的数据非常复杂，我们的模型的维数就很高，参数量就很大，输入是高维的模型，则还有一堆参数需要调优，当我们的训练数据量不够时，很快就产生了过拟合的现象，这样我们就无法很好地泛化，因此即使有两方面的动力，一是我们单纯地想识别自然世界中的万物，二是要回归机器学习克服机器学习中的瓶颈问题，过拟合问题。 LIfeifei开展了一个叫ImageNet的项目，汇集所有能够找到的图片，包含世界万物，组建一个尽可能大的数据集，用一个称为WorldNet的字典来排序，这个字典里有上万个物体类别，用亚马逊土耳其机器人平台进行排序清洗数据，给每张图片打上标签，最终的结果是一个ImageNet,最后由将近500万甚至4000万多的图片，分成22000类的舞台或者场景，这是一个巨大的，很有可能是由当时AI领域最大的数据集，它将目标检测算法的发展推到了一个新的高度。从2009年开始，ImageNet团队组织了一场国际比赛，叫做ImageNet大规模视觉识别竞赛，这是一个筛选更严格的测试集，总共140万的目标图像有1000种目标类别，分别识别来测试计算机视觉算法。2012年卷积神经网络算法击败了所有其他的算法。CNN模型展现了强大的能量。"}
{"content2":"新智元编译\n来源：aiindex.org\n编译：编辑部\n【新智元导读】“AI Index”（AI指数）近日重磅发布，这是斯坦福大学AI百年研究（AI 100）的一个项目，旨在追踪人工智能的活动和进展。该报告列出了2017年人工智能在计算机视觉、自然语言理解等方向上的最新进展，分学术、产业多个角度盘点人工智能进度。报告还综合学术论文数量、招生数量和VC投资数量，得出AI发展活力指数，数据显示，最新一波AI浪潮在2015年活力最高，自那以后其实活力开始有小幅减弱。\n报告全文：https://aiindex.org/2017-report.pdf\n如果缺乏AI技术的相关数据，我们在有关AI的讨论和决策中，基本上是“盲目的”。\n在与人工智能相关的讨论和决策中，我们本质上是“盲目的”。\n“AI Index”（AI指数）是斯坦福大学AI百年研究的一个项目，它是一个开放的非营利性项目，旨在追踪人工智能的活动和进展。它的目的是促进以数据为基础的对AI的了解。本报告是AI Index的第一份年度报告，在这份报告中，我们通过多个视角来观察AI的活动和进展。我们汇总了网络上的数据，也贡献了原始数据，并从数据序列的组合中提取新的度量标准。\n本报告的数据都将在AI Index网站（aiindex.org）上公开。但是，提供数据只是一个开始。为了真正实现作用，AI指数需要来自更大的社区的支持。最后，这份报告呼吁更多人的参与。你有能力提供数据、分析收集的数据，并列出你希望跟踪的数据。无论你是否有答案或问题，我们都希望这份报告能让你了解AI指数，并成为有关AI的话题的一部分。\n报告总览\n报告的前半部分展示了AI Index团队收集的数据。后半部分，我们讨论了报告中没有提到的一些关键领域、专家对报告中显示的趋势的评论，最后呼吁采取行动支持我们的数据收集工作，并加入关于AI技术的度量和交流进展的讨论。\n数据部分\n本报告中的数据包括4个主要部分：\n活动量\n技术表现\n衍生测量\n人类水平表现?\n活动量（Volume of Activity）部分有关这个领域的“多少”（how much）的方面，例如参加AI会议的人数、VC对开发AI系统的初创公司的投资等。技术表现的部分有关“how good”，例如计算机在理解图像和证明数学定理方面已经做到什么程度。在报告附录中详细描述了每个数据集的收集方法。\n这两组数据证实了实际上是公认的一个事实，即：所有的图表都是“向上和向右的”，反映了AI的活动是不断增加，AI技术是不断进步的趋势。在衍生测量（Derivative Measures）部分，我们调查了趋势之间的关系。我们还引入了一个探索性的测量方法——AI活力指数（AI Vibrancy Index），结合了学术界和工业界的趋势，量化了AI作为一个领域的活力。\n在衡量AI系统的表现时，很自然地会将其与人类的表现进行比较。在“人类水平表现”面这一节中，我们列出了一些值得注意的领域，其中AI系统在达到甚至超越人类水平方面取得了重大进展。我们还讨论了进行这种比较时存在的困难，并提出了适当的警告。\n讨论部分\n在报告了团队收集的数据之后，我们将对报告中所强调的趋势进行一些讨论，并对该报告的重要领域进行全面的讨论。\n部分讨论集中在报告的局限性上。这份报告的数据源倾向于以美国为中心，并且可能只通过跟踪了定义良好的基准，因此可能高估了技术领域的进展。它还缺乏数据的人口统计数据，也不包含政府和企业对AI研发投资的信息。这些领域是非常重要的，我们打算在未来的报告中解决这些问题。\n我们将进一步讨论这些局限，以及其他一些在报告中缺失的部分。正如该报告的局限性所显示的， AI Index 只是描绘了局部图景。出于这个原因，这份报告也加入了各个领域的AI专家的主观评论。专家评论部分补充了对数据背后的故事的生动解释。\n最后，我们将需要更多来自社区的反馈和参与来解决报告中显示的局限，揭示我们遗漏的问题，并建立一个追踪AI活动和进展的有效程序。\n人工智能和机器学习全景式概览：学术、产业、人才流动、开源生态，各方各面活动量大增\n这份报告做了大量调查和统计，从学术（论文发表、会议参加、学生课程选修）、产业（创业、投资）、人才（招聘、职位空缺）、开源生态（Github AI和ML软件包）、媒体报道等方面，比较全面地展现了AI和ML的图景。\n1、学术\n首先，论文发表数量激增：自从1996年以来，每年发表的AI论文数量增加了9倍以上。\n再看不同类别的学术论文的年度发表率与1996年的发表率相比较。下图显示了所有领域的论文、计算机科学领域的论文和计算机科学领域的AI论文的增长。数据表明，人工智能发表论文数量增多，不仅受计算机科学领域升温所致。具体而言，自1996年以来，计算机科学一般领域的论文数量增长了6倍，同期，每年的人工智能论文数量增长了9倍以上。\n斯坦福大学入学选修人工智能和机器学习入门课程的学生人数，自从1996年以来增长了11倍以上。报告指出，由于其他大学的数据掌握有限，因此突出了斯坦福的数据。但是，有理由认为，其他大学的情况应该类似。同时，报告表示这只代表了高等教育图景的一个具体细节，不一定代表更广的趋势。\n\n会议出席情况。业内人士都知道，在计算机科学领域，各种学术会议十分重要。这些出席人数表明，研究重点已经从符号推理转向了机器学习和深度学习。\n\n再来看小一些的会议的情况。尽管研究重点有所转换，但是在小一些的研究社区，仍然在符号推理方面稳步进展。\n2、产业\n现在将目光转向产业界。下图展示了在美国，有资本支持的AI创业公司数量，从2000年以来增加了14倍：\n在美国投资AI创业的基金数量也在增长，从2000年以来，每年投入AI创业的资本额增加了6倍：\n根据两个在线求职平台Indeed和Monster的数据，人工智能相关岗位需求也在增长。下图展示了Indeed.com平台上，从2013年1月份起，对AI技术相关工作岗位的份额的增长。\n\n而在美国，需要AI技术的工作岗位，在职业市场所占份额，从2013年到现在，有了4.5倍的增长。\n按国家看，加拿大和英国的AI人才招聘市场规模也增长迅速。不过，Indeed.com报告指出，两者的绝对值仍然是美国AI招聘市场的5％和27％。\nMonster平台上，按具体要求的技能细分，给定年份人工智能职位空缺的总数量：\n\n再来看自动化应用的情况，下图展示了北美和全球工业机器人的购买以及购买增幅。工业机器人的使用正在增加。\n3、开源生态\n最后看开源软件使用和生态。下图展示了TensorFlow和Scikit-Learn软件包在GitHub上加星标的次数。\n\n这张图展示了Github上其他AI和ML软件包的星标情况。\n\n4、公众认知 / 媒体报道\n包含术语“人工智能”的主流媒体文章占所有报道的比例，按照正面情绪（蓝线）、负面情绪（紫线）分类：\n\n技术表现\n\n1. 视觉\n物体识别\n大规模视觉识别挑战赛（LSVRC）比赛中AI系统对物体检测任务的性能\n\n图像标签的错误率从2010年的28.5%下降到了2.5%。\n视觉问答\n人工智能系统在完成回答有关图像的开放式问题任务上的表现。截止2017年8月，最好的AI系统准确率还不到70%，而人类水平在85%左右。\n2. 自然语言理解\n词语解析\n人工智能系统在确定句子句法结构上的表现。\n\n人工智能系统在翻译英文和德文的任务上的表现。\n人工智能系统在从文档中找到既定问题答案任务的表现，已经越来越接近人类。\n语音识别\n人工智能系统识别语音录音的表现，2016已经达到人类水平。\n\n定理证明\n自动定理证明指的是一大组定理证明问题的平均易处理性。 “可追踪性”用来测量可以解决问题中最先进的自动定理证明器的一部分。\nSAT Solving\n具有竞争力的SAT解决者在行业应用问题上的平均表现。\n\n另一种衡量方法：AI活力指数\n通过检查各种趋势之间的关系，我们可以从前面部分衡量的标准中获得额外的洞见。下面这一部分的内容展示了AI指数所搜集到的数据如何被用于进一步分析和推动对AI发展和整个原始标准的再定义。\n正如一个案例研究所展示的那样，我们通过研究学术和产业界的趋势，来探索他们的动能。进一步地，我们将这些标准综合起来，形成一个AI 活力指数。\nAcademia-Industry Dynamics\n为了探索学术和产业界AI相关活动的关系，我们首先从前面部分的内容中选择了一些有代表性的衡量指标。值得一提的是，我们调查了AI论文的发表情况，结合斯坦福大学入门级 AI 和ML课程的报名情况、VC对AI相关初创企业的投资。这些衡量标准数据是不能直接被拿来比较的：论文发表情况、学生报名情况、投资数额。为了分析这些趋势之间的关系，我们将历史追溯到2000年，这能让我们衡量标准是如何随着时间发生变化的。\n数据显示，最初，学术活动（论文发表和招生）驱动稳步前进。 2010年前后，投资者开始注意到这一趋势，这成为2013年投资者总体活动急剧增加的驱动因素。再后来，学术界逐渐赶上了工业的繁荣。\nAI活力指数\nAI活力指数（AI Vibrancy Index）汇集了对学术和产业的衡量标准（研究成果的发表、招生和VC投资）以对AI领域进行量化。为了计算AI活力指数，我们不断地对研究成果发表数量、招生、投资的标准取平均数。\n\n达到人类水平表现的AI，里程碑\n\n很自然地，我们会在同一个任务上将AI系统和人类的表现进行比较。显然，在某些任务中，计算机比人类要优秀得多，例如，1970年代的小计算器就可以比人类更好地完成算术运算。但是，AI系统在处理诸如回答问题、玩游戏和进行医学诊断等更通用的任务时更加困难。\nAI系统的任务往往是在非常窄的背景下进行的，这样能在特定的问题或应用上取得进展。 虽然机器在特定的任务上可能表现出卓越的性能，但是如果任务稍微有所改动，系统性能可能会大大降低。 例如，一个能读懂汉字的人能够理解中国人的言论，了解中国文化，或者在中国餐馆无障碍点餐。相比之下，这些任务中的每一项都需要不同的AI系统来完成。\n尽管将人类和AI系统进行比较不是件容易的事情，但列举那些声称计算机已达到或超过人类表现的那些成就很有意思。不过，需要说明的是，这些成就没有说明这些系统具有推广能力。我们还注意到下面的列表包含许多游戏上的成就。游戏是一个相对简单，可控的实验环境，因此经常用于AI研究。\n里程碑\n黑白棋\n在20世纪80年代，李开复和Sanjoy Mahajan开发了一个人工智能系统BILL，这是一个玩“黑白棋”（Othello）游戏的贝叶斯学习系统。1989年这个系统拿了全美冠军，并以56-8击败了排名最高的美国玩家Brian Rose。在1997年，一个名为Logistello的黑白棋程序以6-0占战胜当时的冠军棋手。\n跳棋\n1952年，Arthur Samuel 设计了一系列玩西洋跳棋的程序，并通过自我对弈进行改进。但是，直到1995年，才出现一个击败人类世界冠军的跳棋程序Chinook。\n国际象棋\n上世纪50年代的一些计算机科学家预测，到1967年，计算机将击败人类象棋冠军。但直到1997年，IBM的“深蓝”系统才击败当时的国际象棋冠军Gary Kasparov。如今，在智能手机上运行的国际象棋程序可以表现出大师级的水平。\nJeopardy!\n2011年，IBM的Watson计算机系统在流行电视节目“Jeopardy!”参与挑战，赢了前冠军Brad Rutter和Ken Jennings。\n雅达利游戏\n2015年，谷歌DeepMind的一个团队使用强化学习系统来学习如何玩49个Atari游戏。该系统在大多数游戏中都能达到人类水平的表现（例如Breakout打砖块游戏，虽然也有些仍然无法达到人类水平（例如，蒙特祖玛的复仇）。\nImageNet对象检测\n2016年，ImageNet自动标注任务的错误率从2010年的28%下降到低于3%。人类的表现大约是5%的错误率。\n围棋\n2016年3月，谷歌DeepMind团队开发的AlphaGo系统击败了围棋冠军李世乭。DeepMind后来发布了AlphaGo Master，在2017年3月击败了排名第一的柯洁。2017年10月，DeepMind发表在Nature的论文详细介绍了AlphaGo的另一个新版本——AlphaGo Zero，它以100-0击败了最初的AlphaGo系统。\n皮肤癌分类\n在2017年的一篇Nature论文文章中，Esteva等人描述了一个AI系统，该系统在包含2032种不同疾病的129450张临床图像组成的数据集上训练，研究者将AI系统的诊断结果与21位皮肤科医生的结果进行比较，他们发现AI系统在分类皮肤癌任务上达到与人类皮肤科医生相当的水平。\nSwitchboard 语音识别\n在2017年，微软和IBM都在Switchboard语音识别基准测试中实现了“人类同等水平”的语音识别词错率。\n扑克\n2017年1月，来自CMU的一个名为Libratus的AI系统在一场包含12万局游戏的双人无限注德州扑克比赛中击败了四名顶尖的人类选手。 2017年2月，来自阿尔伯塔大学的一个名为DeepStack的系统与11名专业玩家分别比赛超过3000局，胜率10/11。\n吃豆人\nMaluuba是微软收购的一个深度学习团队，他们创建了一个AI系统，该系统学会了在Atari 2600上玩吃豆人游戏打出999900的最高分。\n报告原文：https://aiindex.org/2017-report.pdf\n人工智能赛博物理操作系统\nAI-CPS OS\n“人工智能赛博物理操作系统”（新一代技术+商业操作系统“AI-CPS OS”：云计算+大数据+物联网+区块链+人工智能）分支用来的今天，企业领导者必须了解如何将“技术”全面渗入整个公司、产品等“商业”场景中，利用AI-CPS OS形成数字化+智能化力量，实现行业的重新布局、企业的重新构建和自我的焕然新生。\nAI-CPS OS的真正价值并不来自构成技术或功能，而是要以一种传递独特竞争优势的方式将自动化+信息化、智造+产品+服务和数据+分析一体化，这种整合方式能够释放新的业务和运营模式。如果不能实现跨功能的更大规模融合，没有颠覆现状的意愿，这些将不可能实现。\n领导者无法依靠某种单一战略方法来应对多维度的数字化变革。面对新一代技术+商业操作系统AI-CPS OS颠覆性的数字化+智能化力量，领导者必须在行业、企业与个人这三个层面都保持领先地位：\n重新行业布局：你的世界观要怎样改变才算足够？你必须对行业典范进行怎样的反思？\n重新构建企业：你的企业需要做出什么样的变化？你准备如何重新定义你的公司？\n重新打造自己：你需要成为怎样的人？要重塑自己并在数字化+智能化时代保有领先地位，你必须如何去做？\nAI-CPS OS是数字化智能化创新平台，设计思路是将大数据、物联网、区块链和人工智能等无缝整合在云端，可以帮助企业将创新成果融入自身业务体系，实现各个前沿技术在云端的优势协同。AI-CPS OS形成的数字化+智能化力量与行业、企业及个人三个层面的交叉，形成了领导力模式，使数字化融入到领导者所在企业与领导方式的核心位置：\n精细：这种力量能够使人在更加真实、细致的层面观察与感知现实世界和数字化世界正在发生的一切，进而理解和更加精细地进行产品个性化控制、微观业务场景事件和结果控制。\n智能：模型随着时间（数据）的变化而变化，整个系统就具备了智能（自学习）的能力。\n高效：企业需要建立实时或者准实时的数据采集传输、模型预测和响应决策能力，这样智能就从批量性、阶段性的行为变成一个可以实时触达的行为。\n不确定性：数字化变更颠覆和改变了领导者曾经仰仗的思维方式、结构和实践经验，其结果就是形成了复合不确定性这种颠覆性力量。主要的不确定性蕴含于三个领域：技术、文化、制度。\n边界模糊：数字世界与现实世界的不断融合成CPS不仅让人们所知行业的核心产品、经济学定理和可能性都产生了变化，还模糊了不同行业间的界限。这种效应正在向生态系统、企业、客户、产品快速蔓延。\nAI-CPS OS形成的数字化+智能化力量通过三个方式激发经济增长：\n创造虚拟劳动力，承担需要适应性和敏捷性的复杂任务，即“智能自动化”，以区别于传统的自动化解决方案；\n对现有劳动力和实物资产进行有利的补充和提升，提高资本效率；\n人工智能的普及，将推动多行业的相关创新，开辟崭新的经济增长空间。\n给决策制定者和商业领袖的建议：\n超越自动化，开启新创新模式：利用具有自主学习和自我控制能力的动态机器智能，为企业创造新商机；\n迎接新一代信息技术，迎接人工智能：无缝整合人类智慧与机器智能，重新\n评估未来的知识和技能类型；\n制定道德规范：切实为人工智能生态系统制定道德准则，并在智能机器的开\n发过程中确定更加明晰的标准和最佳实践；\n重视再分配效应：对人工智能可能带来的冲击做好准备，制定战略帮助面临\n较高失业风险的人群；\n开发数字化+智能化企业所需新能力：员工团队需要积极掌握判断、沟通及想象力和创造力等人类所特有的重要能力。对于中国企业来说，创造兼具包容性和多样性的文化也非常重要。\n子曰：“君子和而不同，小人同而不和。”  《论语·子路》云计算、大数据、物联网、区块链和 人工智能，像君子一般融合，一起体现科技就是生产力。\n如果说上一次哥伦布地理大发现，拓展的是人类的物理空间。那么这一次地理大发现，拓展的就是人们的数字空间。在数学空间，建立新的商业文明，从而发现新的创富模式，为人类社会带来新的财富空间。云计算，大数据、物联网和区块链，是进入这个数字空间的船，而人工智能就是那船上的帆，哥伦布之帆！\n新一代技术+商业的人工智能赛博物理操作系统AI-CPS OS作为新一轮产业变革的核心驱动力，将进一步释放历次科技革命和产业变革积蓄的巨大能量，并创造新的强大引擎。重构生产、分配、交换、消费等经济活动各环节，形成从宏观到微观各领域的智能化新需求，催生新技术、新产品、新产业、新业态、新模式。引发经济结构重大变革，深刻改变人类生产生活方式和思维模式，实现社会生产力的整体跃升。\n\n产业智能官  AI-CPS\n用“人工智能赛博物理操作系统”（新一代技术+商业操作系统“AI-CPS OS”：云计算+大数据+物联网+区块链+人工智能），在场景中构建状态感知-实时分析-自主决策-精准执行-学习提升的认知计算和机器智能；实现产业转型升级、DT驱动业务、价值创新创造的产业互联生态链。\n\n\n长按上方二维码关注微信公众号： AI-CPS，更多信息回复：\n新技术：“云计算”、“大数据”、“物联网”、“区块链”、“人工智能”；新产业：“智能制造”、“智能农业”、“智能金融”、“智能零售”、“智能城市”、“智能驾驶”；新模式：“财富空间”、“特色小镇”、“赛博物理”、“供应链金融”。\n点击“阅读原文”，访问AI-CPS OS官网\n\n本文系“产业智能官”（公众号ID：AI-CPS）收集整理，转载请注明出处！\n版权声明：由产业智能官（公众号ID：AI-CPS）推荐的文章，除非确实无法确认，我们都会注明作者和来源。部分文章推送时未能与原作者取得联系。若涉及版权问题，烦请原作者联系我们，与您共同协商解决。联系、投稿邮箱：erp_vip@hotmail.com"}
{"content2":"从0开始学习“OPENCV”第一天-概述\n在学习任何一门新的语言或者框架时都应该了解这个行业的背景知识，正所谓工欲善其事，必先利其器！\n一、Opencv概述\n1.      什么是计算机视觉？计算机视觉有多难、\n1.1     什么是计算机视觉？\n1.2\n在说Opencv之前要说一下什么是计算机视觉，计算机视觉是在图像基础上发展起来的一门新兴学科，计算机视觉是研究让机器如何看世界，认识这个五彩缤纷的世界，就是让摄像头代替人眼来对目标进行识别，跟踪和测量，并进一步对捕获的图像数据(视频数据)转换成一种新的表达方式或者一个新的决策的过程！在转换过程中进行的转换都是为了达到某一目标。\n举个列子：通过输入设备(摄像头、扫描仪)将前方1米处发现的物体输入到电脑中，并对这些数据进行处理，然后与数据库里的模型比对，那么最后得到的决策可能是前方有一辆汽车或者站着一个人，处理的过程可能是把彩色图像转换成单通道的灰色图(灰色图要比彩色图容易处理后面会说为什么)，对图像降噪声，或者通过图像序列分析去除摄像机晃动的影响，这些转换过程/处理过程最终将会转换成一种新的决策，表达方式！\n这里稍微补充一下什么是图像序列分析，这里说的图像序列分析和图像序列不同！\n图像序列是就是一组图像(或者拍摄时的图像)的先后顺序！\n图像序列分析利用计算机视觉技术从一组图像序列中检测运动及运动物体并对其进行运动分析、跟踪或识别。图像序列分析在国民经济和军事领域的许多方面有着广泛的应用。\n随着计算机视觉的诞生，人工智能技术也随着和诞生，其中人工智能技术中生物识别技术能从计算机处理的图像数据(多维数据)中获取信息，并对这个信息进行识别，并做相应的处理，人工智能领域下有很多技术比如最著名的机器学习等等这里就不做太多的详细介绍，后面学到机器学习时会和大家详细介绍人工智能技术下各个领域作用！\n因为计算机视觉是计算机学科所以在、工程、信号处理、物理学、应用数学和统计学、神经生理学和认知科学等都有研究方面，在制造业、检验、文档分析、医疗诊断、和军事等领域等各种智能/自主应用方面，都有非常广阔的前景发展！\n1.2计算机视觉实现起来难吗？\n人类本身是视觉动物，所以觉得人类觉得可以很容易实现计算机视觉，假如说让你从一个场景中找到一辆汽车，显然很容易，因为汽车本身较大，容易被眼睛所捕获，但是其中在捕获的过程中有着很复杂的过程：\n人脑将视觉信号划分入很多个通道，将各种不同的信息输入你的大脑。你的大脑有一个关注系统，会根据任务识别出场景的重要部分，并做重点分析，而其他部分则分析的较少。在人类视觉流中存在着大量的反馈，但是目前人类对之了解甚少，肌肉控制的传感器以及其他所有传感器的输入信息之间存在着广泛的关联，这使得大脑可以依赖从出生以来所学到的信息，大脑中的反馈在信息处理的各个阶段都存在，在眼睛(传感器)中也存在。在眼睛中反馈来调节通过瞳孔的进光量，以及调节视网膜表面上的接受单元！\n所以我们要想真正的实现一个人工智能产品的话就要把人类自己本身的所有信息模拟到计算机上，比如大脑=CPU，眼睛=摄像头，感官=传感器，并且要让之间协调工作，相对来说是非常复杂的！\n其次计算机接受到的数据主要来源于摄像头，磁盘文件中的数值矩阵！\n.1（取之“学习Opencv“）中的汽车有一个反光镜但是计算机只看到一组数值矩阵：\n由于该图是单通道(黑白图)所以一个矩阵数值就可以表示一个像素点，如果是多通道的RGB颜色就需要三个数值表示，比如194 210 201表示一个像素点，而单通道194就可以表示像素点！\n其中非常令人头疼的问题就是图像噪声\n左：正常图片右：带图像噪声的图片\n如果一张图里每个像素点上都掺杂着图像噪声的话会降低图像识别的准确率\n图像噪声产生的问题主要来自输入设备（摄像机），造成摄像机产生图像噪声的几种原因如下：\n1.     外部噪声：\n即指系统外部干扰以电磁波或经电源串进系统内部而引起的噪声。如电气设备，天体放电现象等引起的噪声。\n2.     内部噪声(分为四种)：\n（1）由光和电的基本性质所引起的噪声。如电流的产生是由电子或空穴粒子的集合，定向运动所形成。因这些粒子运动的随机性而形成的散粒噪声；导体中自由电子的无规则热运动所形成的热噪声；根据光的粒子性，图像是由光量子所传输，而光量子密度随时间和空间变化所形成的光量子噪声等。\n（2）电器的机械运动产生的噪声。如各种接头因抖动引起电流变化所产生的噪声；磁头、磁带等抖动或一起的抖动等。\n（3）器材材料本身引起的噪声。如正片和负片的表面颗粒性和磁带磁盘表面缺陷所产生的噪声。随着材料科学的发展，这些噪声有望不断减少，但在目前来讲，还是不可避免的。\n（4）系统内部设备电路所引起的噪声。如电源引入的交流噪声；偏转系统和箝位电路所引起的噪声等。\n3.     网络噪声\n这个只是简单提一下一般的单机视觉开发一般用不到：网络噪声就是在通过UDP传输图像数据时因为网络不稳定造成传输时出现丢包的现象，导致传输过去的矩阵数值与原数值不一样，导致每个像素点上的值出现损坏的情况，每个像素点上就出现很多白色小斑点的图状物就叫图像噪声！\nTCP不会出现图像噪声的问题，因为TCP为了确保数据的准确性，有重发机制，这里不做详细介绍，想详细了解可以在我的分栏里“网络层原理”这一栏中找到关于对TCP详细介绍的文章！\n如果一个视觉系统里没有模式识别系统，自动控制的对焦和光圈，没有多年来的经验累计的视觉系统通常属于很低级的视觉系统！\n4.     根据特征切割场景\n除了噪声以外还有许多其他阻挡计算机视觉处理的难题，列如场景物体的干扰，在三维场景中重建二维图\n场景物体的干扰：\n假如我们要做一个能够自动把房间里掉地上的书捡起来放到书架上，那么我们需要从这个房间场景中找出我们所需要的目标物品：书。\n假如说这个人的房间非常大或者在客厅，那么时首先如果从右到左或者从左到右采用地毯式的搜索的话会需要进行大量的分析算法同时因为CPU/ALT运算单元会进行过多的算法运算一直处于高电平状态。\n会加快消耗机器人的电能，在这样的情况下我们可以告诉机器人书一般会在某个地方出现：书柜、桌子、床上，沙发的周边地区。\n然后将这三个模型导入到捡书机器人的比对数据库里，首先一点是在拍摄这些配对模型时，要将物品放到最能表现其特征的地方：“正中心位置”。\n为了让捡书机器人在比对模型时准确率更高可以为其比对模型添加一些隐含的变量：大小，重力方向以及其他变量，然后在比对时将捕获的床或者沙发进行分析推断出物品体积并通过机器学习技术不停的根据上下文解释信息进行建模训练，校正变量，让其准确率更高！\n(这里说一下重力方向：给予重力方向的优点是可以通过目标物体的重力方向推断出该物体会在那个位置出现，这样在一个非常大的宫殿里寻找一张床，有了这张床的重力方向，可以以自身为中心并根据床的重力方向推断出大概会在那个方位！)\n有了这些信息之后那么机器人可以很快的过滤掉场景中书籍不可能会掉落的地方，那么机器人可以很快的找出书籍并放到书架上！当然你也可以给机器人安装激光扫描仪使其捕获的物品体积使其在机器训练时用捕获的数据与模型数据进行校正时更加准确！\n并且捡书机器人上的摄像机并非属于固定摄像机，固定摄像机对场景约束较多，但是可以通过这些约束简化问题，但是移动摄像机需要不停的变更场景，所以移动摄像机的场景约束较少，需要做更多的简化工作！\n重建二维图\n就像上面说的，要从一个房间里找到书可以根据特征来寻找加快寻找时间，那么在找到一个目标时首先要将这个目标转换成二维图，也就是说三维图是立体的，存在前后之分，而二维图不存在前后之分，只有宽高，为什么要转换成二维图？可以想一下是三维图图二是二维图（图像来源：http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/features2d/feature_homography/feature_homography.html）这一部分不必管是如何实现的！\n二维图的方法就是从一个三维图(立体)中根据二维特征(平面)将二维数据提取出来并映射到另外一个图像数据上！\n可以可看到二维图可以更好的方便识别所需表面特征！\n1.     opencv还可以很好的修复图像中的畸变\n下图列子展示了图像畸变和畸变后校正的图像(转自：http://www.cnblogs.com/Lemon-Li/p/3283059.html)\n图一畸变图像\n图像空间畸变图：\n图二opencv畸变校正后的图像\n现在先不管是如何利用opencv修复的，到后面的文章会慢慢和大家讲解！\n图像畸变会给人一种凹凸的感觉，所以在视觉上看起来并不是特别美观！\n二.  Opencv发展历程\n1.\n早期在做图像处理时所需要的算法运算量是非常大的，所以那个时候在对图像做基础处理都要耗费很长的时间，正因如此1996年时lntel发布奔腾处理器时同时发布MMX指令集“看过我那篇“深度理解指令集”的朋友应该都对这个指令集有所了解“，MMX（后来的SSE）这种单指令多数据的多媒体指令集在运算时运算速度要比平常的图像算法快上几倍甚至几十倍，这才把图像处理从慢车道推向了快车道！\n如果想深度了解MMX指令集的发展史可以去看我那篇“深度理解指令集”的最后一段！\n可是如果想要使用MMX(SSE)指令集的话必须会汇编语言才行，所以基于汇编的算法开发和优化需要耗费时间比较长。\n所以后来Intel基于MMX（SSE）指令集推出了IPL库，IPL是基于MMX指令集，后来因为MMX指令集的缺陷推出SSE指令集同时推出封装SSE指令集的IPP库，换句话说IPP库就是基于IPL库的！\nMMX(SSE)指令集里包含的大多都是对图像处理的基础函数，在对图像进行复杂处理时短时间里比较难以实现，而且MMX(SSE)指令集是非开源的，在那个年代追求效率的企业都希望既能开发出性能优越的视觉系统，提高开发效率，降低开发成本，所以后来1999年Intenl启动CLV项目主要目标是人机界面，能被UI调用的实时计算机视觉库，为Intel处理器做了特定优化。\n后来2000年6月正式发布的第一个在Windows平台下第一个Opencv开源版本“OpenCV alpha 3”同年12月发布在Linux平台下“OpenCV beta 1”开源版本。\nOpencv不仅开源免费，内部对图像处理的函数非常丰富，内部函数的实现一般都使用IPP库做优化，同上其实Opencv并不是完全开源，因为IPP库是非开源的，所以内部使用IPP做优化的函数属于非开源没有使用IPP做优化的属于开源，可以说Opencv属于半开源的项目！\n2.     可移植性\nOpencv采用C/C++编写在不同的系统环境上只要稍微修改一下代码就可以编译通过，可以在Mac/Linux/Windows系统上运行，并且为python，Ruby，MATLAB等编程语言提供接口！\n3.     运行效率\nOpencv在设计时的目标就是执行速度尽量快所以内部函数都是标C函数来编写的，如果想要起到硬件加速（内部函数用IPP优化）需要购买IPP库，购买IPP库后Opencv在运行时会自动调用IPP库做优化！\n4.     应用领域\n目前Opencv应用领域非常广泛，在医疗设备、工厂检验、立体视觉、机器学习、人脸别识别、图像拼接、生物医学分析、无人机、等人工智能领域有广泛应用！\n甚至计算机视觉可以用在声谱图上，对声音和音乐进行分析！\n并且计算机视觉被广泛应用于工厂检验，大规模的产品制造在流水线上的某一环节都使用计算机视觉做检测！\n5.     Opencv目标\nOpencv的目标是为解决计算机视觉提供基本工具，当然在有些情况下，Opencv还提供了许多高层函数用于解决复杂式图像处理，当然如果没有这些高层函数也完全可以基于Opencv提供的基础函数上建立一个完整的解决方案，在用Opencv建立一个解决方案时，尽管这个解决方案不是特别完美，但是有了第一个解决方案之后，便会从这个解决方案中找到许多不足的地方，但是可以基于这个解决方案之上来不停的对其优化整改，到一套完整的解决方案体系，虽然说很难达到十全十美但是达到十全九美就可以了，当然解决方案的不足也可以通过系统所使用的环境来解决，比如要识别出场景中这个人的身高，可以为计算机安装激光红外扫描仪来精准的捕获目标物体的身高并输入到计算机里更加方便的处理数据！\n6.     Opencv库组成体系(取自：学习Opencv-5)\n这些体系可能与你当前使用的Opencv版本不同\n图中没用包含CvAux模块，因为该模块中一般包含一些即将淘汰的算法和函数(比如基于嵌入式隐马尔可夫模型的人脸识别等等)，所以如果突然有一天你发现你要使用的基于某个算法写出来的函数不见了，可以到这个模块里或许能找到！\n1.     版权\nOpencv开源协议允许你使用Opencv库的全部代码，生成商业产品，并且不需要公开源代码，或对Opencv库中的算法改善后的算法！\n2.     预备\n在学习Opencv之前要懂得C/C++编程，和一些数学基础！\n3.     总结\n1.Opencv第一个windows版本是2000年6月推出的，“OpenCV alpha 3”同年12月发布在Linux平台下！\n2.Opencv第一个开源版本是OpenCV beta 1\n3.Opencv是属于Intel公司的一个开源项目（IPP不开源），\n4.Opencv目前可以运用在制造业、机器学习、生物识别、检验、文档分析、医疗诊断、和军事等领域等各种智能/自主应用方面，应用范围非常广泛！\n5.Opencv源代码是C/C++编写的，如果想要调用IPP库加速内部函数代码需要购买！\n6.Opencv库可以在Windows、Linux、Mac平台下运行，并为python，Ruby，MATLAB等流行编程语言提供接口\n7.opencv是由cv(图像处理和视觉算法)，mll(统计分类器)，highgui(GUI/图像和视频输入/输出)，cxcore(基本结构和算法,xml支持,绘图函数),五大模块组成！\n8.图像识别令人最头疼的地方是图像噪声,场景重塑\n9.图像噪声产生原因由：外部噪声，内部噪声，还有网络噪声。\n10.图像序列是就是一组图像(或者拍摄时的图像)的先后顺序！\n11.图像序列分析是对一组已经排序好的图像进行运动分析！\n12.图像分析分为两种：实时分析，非实时分析\n13.实时分析就是对输入设备里的数据进行实施动作分析，而非实时分析就是对一组有序存储于本地存储器上的图片进行动作分析！\n14.想要真正实现一个完全人工智能视觉产品是很复杂的，其中要考虑到很多复杂因素！\n练习：安装Opencv练习\n因为博主目前是在Windows领域下用Opencv库编程开发，所以单独写了一篇博客“VisualStudio2013配置安装Opencv2.4.9“\n有需要的可以去看下我的这篇安装教程写的非常详细-点击打开链接"}
{"content2":"作者：元峰\n链接：https://zhuanlan.zhihu.com/p/24699780\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n引言\n在今年的神经网络顶级会议NIPS2016上，深度学习三大牛之一的Yann Lecun教授给出了一个关于机器学习中的有监督学习、无监督学习和增强学习的一个有趣的比喻，他说：如果把智能（Intelligence）比作一个蛋糕，那么无监督学习就是蛋糕本体，增强学习是蛋糕上的樱桃，那么监督学习，仅仅能算作蛋糕上的糖霜（）。\n. Yann LeCun 对监督学习，增强学习和无监督学习的价值的形象比喻\n本文结合本人最近看的一些论文和文章，给出一个深度学习在计算机视觉领域的前沿进展的一个简介，如有错误或瑕疵，请留言告知我，不胜感激！\n1. 深度有监督学习在计算机视觉领域的进展\n1.1 图像分类（Image Classification）\n自从Alex和他的导师Hinton（深度学习鼻祖）在2012年的ImageNet大规模图像识别竞赛（ILSVRC2012）中以超过第二名10个百分点的成绩(83.6%的Top5精度)碾压第二名（74.2%，使用传统的计算机视觉方法）后，深度学习真正开始火热，卷积神经网络（CNN）开始成为家喻户晓的名字，从12年的AlexNet（83.6%），到2013年ImageNet 大规模图像识别竞赛冠军的88.8%，再到2014年VGG的92.7%和同年的GoogLeNet的93.3%，终于，到了2015年，在1000类的图像识别中，微软提出的残差网（ResNet）以96.43%的Top5正确率，达到了超过人类的水平（人类的正确率也只有94.9%）.\nTop5精度是指在给出一张图片，模型给出5个最有可能的标签，只要在预测的5个结果中包含正确标签，即为正确\n图２. 2010-2015年ILSVRC竞赛图像识别错误率演进趋势\n1.2 图像检测（Image Dection）\n伴随着图像分类任务，还有另外一个更加有挑战的任务–图像检测，图像检测是指在分类图像的同时把物体用矩形框给圈起来。从14年到16年，先后涌现出R-CNN,Fast R-CNN, Faster R-CNN, YOLO, SSD等知名框架，其检测平均精度（mAP），在计算机视觉一个知名数据集上PASCAL VOC上的检测平均精度（mAP），也从R-CNN的53.3%，到Fast RCNN的68.4%，再到Faster R-CNN的75.9%，最新实验显示，Faster RCNN结合残差网（Resnet-101），其检测精度可以达到83.8%。深度学习检测速度也越来越快，从最初的RCNN模型，处理一张图片要用2秒多，到Faster RCNN的198毫秒/张，再到YOLO的155帧/秒（其缺陷是精度较低，只有52.7%），最后出来了精度和速度都较高的SSD，精度75.1%，速度23帧/秒。\n1.3 图像分割（Semantic Segmentation）\n图像分割也是一项有意思的研究领域，它的目的是把图像中各种不同物体给用不同颜色分割出来，如下图所示，其平均精度（mIoU，即预测区域和实际区域交集除以预测区域和实际区域的并集），也从最开始的FCN模型（图像语义分割全连接网络，该论文获得计算机视觉顶会CVPR2015的最佳论文的）的62.2%，到DeepLab框架的72.7%，再到牛津大学的CRF as RNN的74.7%。该领域是一个仍在进展的领域，仍旧有很大的进步空间。\n1.4 图像标注–看图说话（Image Captioning）\n图像标注是一项引人注目的研究领域，它的研究目的是给出一张图片，你给我用一段文字描述它，如图中所示，图片中第一个图，程序自动给出的描述是“一个人在尘土飞扬的土路上骑摩托车”，第二个图片是“两只狗在草地上玩耍”。由于该研究巨大的商业价值（例如图片搜索），近几年，工业界的百度，谷歌和微软 以及学术界的加大伯克利，深度学习研究重地多伦多大学都在做相应的研究。\n.图像标注，根据图片生成描述文字\n1.5 图像生成–文字转图像（Image Generator）\n图片标注任务本来是一个半圆，既然我们可以从图片产生描述文字，那么我们也能从文字来生成图片。如所示，第一列“一架大客机在蓝天飞翔”，模型自动根据文字生成了16张图片，第三列比较有意思，“一群大象在干燥草地行走”（这个有点违背常识，因为大象一般在雨林，不会在干燥草地上行走），模型也相应的生成了对应图片，虽然生成的质量还不算太好，但也已经中规中矩。\n.根据文字生成图片\n2.强化学习（Reinforcement Learning）\n在监督学习任务中，我们都是给定样本一个固定标签，然后去训练模型，可是，在真实环境中，我们很难给出所有样本的标签，这时候，强化学习就派上了用场。简单来说，我们给定一些奖励或惩罚，强化学习就是让模型自己去试错，模型自己去优化怎么才能得到更多的分数。2016年大火的AlphaGo就是利用了强化学习去训练，它在不断的自我试错和博弈中掌握了最优的策略。利用强化学习去玩flyppy bird，已经能够玩到几万分了。\n图７. 强化学习玩flappy bird\n谷歌DeepMind发表的使用增强学习来玩Atari游戏，其中一个经典的游戏是打砖块（breakout），DeepMind提出的模型仅仅使用像素作为输入，没有任何其他先验知识，换句话说，模型并不认识球是什么，它玩的是什么，令人惊讶的是，在经过240分钟的训练后，它不光学会了正确的接球，击打砖块，它甚至学会了持续击打同一个位置，游戏就胜利的越快（它的奖励也越高）。视频链接:Youtbe(需翻墙),优酷\n.使用深度增强学习来玩Atari Breakout\n强化学习在机器人领域和自动驾驶领域有极大的应用价值，当前arxiv上基本上每隔几天就会有相应的论文出现。机器人去学习试错来学习最优的表现，这或许是人工智能进化的最优途径，估计也是通向强人工智能的必经之路。\n3深度无监督学习（Deep Unsupervised Learning）–预测学习\n相比有限的监督学习数据，自然界有无穷无尽的未标注数据。试想，如果人工智能可以从庞大的自然界自动去学习，那岂不是开启了一个新纪元？当前，最有前景的研究领域或许应属无监督学习，这也正是Yann Lecun教授把无监督学习比喻成人工智能大蛋糕的原因吧。\n深度学习牛人Ian Goodfellow在2014年提出生成对抗网络后，该领域越来越火，成为16年研究最火热的一个领域之一。大牛Yann LeCun曾说：“对抗网络是切片面包发明以来最令人激动的事情。”大牛这句话足以说明生成对抗网络有多重要。\n生成对抗网络的一个简单解释如下：假设有两个模型，一个是生成模型（Generative Model，下文简写为G），一个是判别模型（Discriminative Model，下文简写为D），判别模型(D)的任务就是判断一个实例是真实的还是由模型生成的，生成模型(G)的任务是生成一个实例来骗过判别模型（D），两个模型互相对抗，发展下去就会达到一个平衡，生成模型生成的实例与真实的没有区别，判别模型无法区分自然的还是模型生成的。以赝品商人为例，赝品商人（生成模型）制作出假的毕加索画作来欺骗行家（判别模型D），赝品商人一直提升他的高仿水平来区分行家，行家也一直学习真的假的毕加索画作来提升自己的辨识能力，两个人一直博弈，最后赝品商人高仿的毕加索画作达到了以假乱真的水平，行家最后也很难区分正品和赝品了。下图是Goodfellow在发表生成对抗网络论文中的一些生成图片，可以看出，模型生成的模型与真实的还是有大差别，但这是14年的论文了，16年这个领域进展非常快，相继出现了条件生成对抗网络（Conditional Generative Adversarial Nets）和信息生成对抗网络（InfoGAN），深度卷积生成对抗网络（Deep Convolutional Generative Adversarial Network, DCGAN），更重要的是，当前生成对抗网络把触角伸到了视频预测领域，众所周知，人类主要是靠视频序列来理解自然界的，图片只占非常小的一部分，当人工智能学会理解视频后，它也真正开始显现出威力了。\n这里推荐一篇2017年初Ian GoodFellow结合他在NIPS2016的演讲写出的综述性论文NIPS 2016 Tutorial: Generative Adversarial Networks\n\n3.1条件生成对抗网络（Conditional Generative Adversarial Nets，CGAN）\n生成对抗网络一般是根据随机噪声来生成特定类型的图像等实例，条件生成对抗网络则是根据一定的输入来限定输出，例如根据几个描述名词来生成特定的实例，这有点类似1.5节介绍的由文字生成图像，下图是Conditioanal Generative Adversarial Nets论文中的一张图片，根据特定的名词描述来生成图片。（注意：左边的一列图片的描述文字是训练集中不存在的，也就是说是模型根据没有见过的描述来生成的图片，右边的一列图片的描述是训练集中存在的）\n. 根据文字来生成图片\n条件生成对抗网络的另一篇有意思的论文是图像到图像的翻译，该论文提出的模型能够根据一张输入图片，然后给出模型生成的图片，下图是论文中的一张图，其中左上角第一对非常有意思，模型输入图像分割的结果，给出了生成的真实场景的结果，这类似于图像分割的反向工程。\n. 根据特定输入来生成一些有意思的输出图片\n生成对抗网络也用在了图像超分辨率上，2016年有人提出SRGAN模型，它把原高清图下采样后，试图用生成对抗网络模型来还原图片来生成更为自然的，更逼近原图像的图像。下图中最右边是原图，把他降采样后采用三次差值（Bicubic Interpolation）得到的图像比较模糊，采用残差网络的版本（SRResNet）已经干净了很多，我们可以看到SRGAN生成的图片更为真实一些。\n.生成对抗网络做超分辨率的例子，最右边是原始图像\n生成对抗网络的另一篇有影响力的论文是深度卷积生成对抗网络DCGAN,作者把卷积神经网络和生成对抗网络结合起来，作者指出该框架可以很好的学习事物的特征，论文在图像生成和图像操作上给出了很有意思的结果，例如，带眼睛的男人-不戴眼镜的男人+不带眼睛的女人=带眼睛的女人,该模型给出了图片的类似向量化操作。\n. DCGAN论文中的例图\n生成对抗网络的发展是在是太火爆，一篇文章难以罗列完全，对此感兴趣的朋友们可以自己在网络搜素相关论文来研究\nopenAI的一篇描述生成对抗网络的博客非常棒，因为Ian Goodfellow就在OpenAI工作，所以这篇博客的质量还是相当有保障的。链接为：Open AI 生成对抗网络博客\n3.2 视频预测\n该方向是笔者自己最感兴趣的方向，Yann LeCun也提出，“用预测学习来替代无监督学习”,预测学习通过观察和理解这个世界是如何运作的，然后对世界的变化做出预测，机器学会了感知世界的变化，然后对世界的状态进行了推断。\n今年的NIPS上，MIT的学者Vondrick等人发表了一篇名为Generating Videos with Scene Dynamics的论文,该论文提出了基于一幅静态的图片，模型自动推测接下来的场景，例如给出一张人站在沙滩的图片，模型自动给出一段接下来的海浪涌动的小视频。该模型是以无监督的方式，在大量的视频上训练而来的。该模型表明它可以自动学习到视频中有用的特征。下图是作者的官方主页上给出的图，是动态图，如果无法正常查看，请转入官方网站\n视频生成例子，下图的视频是模型自动生成的，我们可以看到图片不太完美，但已经能相当好的表示一个场景了。\n. 随机生成的视频，沙滩上波涛涌动，火车奔驰的场景\n条件视频生成，下图是输入一张静态图，模型自动推演出一段小视频。\n.根据一张草地静态图，模型自动推测人的移动场景,该图为动图，如果无法查看，请访问\n.给出一张铁道图，模型自动推测火车跑过的样子,该图为动图，如果无法查看，请访问\nMIT的CSAIL实验室也放出了一篇博客，题目是《教会机器去预测未来》,该模型在youtube视频和电视剧上（例如The Office和《绝望主妇》）训练，训练好以后，如果你给该模型一个亲吻之前的图片，该模型能自动推测出加下来拥抱亲吻的动作，具体的例子见下图。\n. 给出一张静态图，模型自动推测接下来的动作\n哈佛大学的Lotter等人提出了PredNet，该模型也是在KITTI数据集上训练,然后该模型就可以根据前面的视频，预测行车记录仪接下来几帧的图像，模型是用长短期记忆神经网络（LSTM）训练得到的。具体例子见下图,给出行车记录仪前几张的图片，自动预测接下来的五帧场景，模型输入几帧图像后，预测接下来的5帧，由图可知，越往后，模型预测的越是模糊,但模型已经可以给出有参加价值的预测结果了。图片是动图，如果无法正常查看，请访问论文作者的博客\n. 给出行车记录仪前几张的图片，自动预测接下来的五帧场景,该图为动图，如果无法查看，请访问\n4 总结\n生成对抗网络，无监督学习视频预测的论文实在是太多，本人精力实在有限，对此感兴趣的读者可以每天刷一下arxiv的计算机视觉版块的计算机视觉和模型识别，神经网络和进化计算和人工智能等相应版块，基本上每天都有这方面新论文出现。图像检测和分割，增强学习，生成对抗网络，预测学习都是人工智能发展火热的方向，希望对深度学习感兴趣的我们在这方面能做出来点成果。谢谢朋友们的阅读，对深度无监督学习感兴趣的朋友，欢迎一起学习交流，请私信我。\n5 参考文献\n在写本文的过程中，我尽量把论文网址以链接的形式附着在正文中.本文参考的大部分博客和论文整理如下，方便大家和自己以后研究查看。\n参考博客\n【NIPS 主旨演讲】Yann LeCun：用预测学习替代无监督学习\n计算机视觉和 CNN 发展十一座里程碑\nGenerative Models\nGenerating Videos with Scene Dynamics\nTeaching machines to predict the future\n参考论文\nResnet模型，图像分类，超过人类的计算机识别水平。Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n图像检测 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n图像分割Conditional Random Fields as Recurrent Neural Networks\n图像标注，看图说话 Show and Tell: A Neural Image Caption Generator\n文字生成图像Generative Adversarial Text to Image Synthesis\n强化学习玩flyppy bird Using Deep Q-Network to Learn How To Play Flappy Bird\n强化学习玩Atari游戏 Playing Atari with Deep Reinforcement Learning\n生成对抗网络 Generative Adversarial Networks\n条件生成对抗网络Conditional Generative Adversarial Nets\n生成对抗网络做图像超分辨率Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n深度卷积生成对抗网络Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n由图片推演视频Generating Videos with Scene Dynamics\n视频预测和无监督学习Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning"}
{"content2":"1 直方图均衡化\n图像灰度的分布特性：如果大部分像素集中于低灰度区域，直方图呈暗特性；如果大部分像素值集中于亮灰度区域直方图呈亮特性。\n直方图均衡化就是将随机分布的灰度直方图修改成为均匀分布的灰度直方图，基本思想是对原始像素做某种映射变换，使变换后的图像的灰度概率密度成均匀分布。\n2"}
{"content2":"对计算机视觉的浅显认识\n什么是计算机视觉？\n计算机视觉任务方向\n分类 定位 识别\n目标检测\n目标跟踪\n图像处理\n分割\n为什么要学习计算机视觉？\n什么是计算机视觉？\n计算机视觉是一门研究如何使机器“看”的科学，更进一步的说，就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。作为一个科学学科，计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。这里所 指的信息指Shannon定义的，可以用来帮助做一个“决定”的信息。因为感知可以看作是从感官信号中提 取信息，所以计算机视觉也可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学。\n计算机视觉任务方向\n分类 定位 识别\n图像分类任务通常是指为整张图像分配特定的标签，如下左图整张图像的标签为 CAT。而定位是指找到识别目标在图像中出现的位置，通常这种位置信息将由对象周围的一些边界框表示出来。识别技术根据从图象抽取的统计特性或结构信息，把图像分成予定的类别。\n图 1：计算机视觉任务，来源 cs231n 课程资料。\n目标检测\n目标检测（Object Detection）即如字面所说的检测图像中包含的物体或目标。最受大众关注且目前应用较为广泛的应属人脸识别。\n目标跟踪\n目标跟踪，是指在特定场景跟踪某一个或多个特定感兴趣对象的过程。传统的应用就是视频和真实世界的交互，在检测到初始对象之后进行观察。现在，目标跟踪在无人驾驶领域也很重要，例如 Uber 和特斯拉等公司的无人驾驶。\n图像处理\n图像处理技术把输入图像转换成具有所希望特性的另一幅图像。例如，可通过处理使输出图象有较高的信-噪比，或通过增强处理突出图象的细节，以便于操作员的检验。在计算机视觉研究中经常利用图象处理技术进行预处理和特征抽取。\n分割\n计算机视觉的核心是分割，它将整个图像分成一个个像素组，然后对其进行标记和分类。特别地，语义分割试图在语义上理解图像中每个像素的角色（比如，识别它是汽车、摩托车还是其他的类别）。如上图所示，除了识别人、道路、汽车、树木等之外，我们还必须确定每个物体的边界。因此，与分类不同，我们需要用模型对密集的像素进行预测。\n为什么要学习计算机视觉？\n人脸识别： Snapchat 和 Facebook 使用人脸检测算法来识别人脸。\n图像检索：Google Images 使用基于内容的查询来搜索相关图片，算法分析查询图像中的内容并根据最佳匹配内容返回结果。\n游戏和控制：使用立体视觉较为成功的游戏应用产品是：微软 Kinect。\n监测：用于监测可疑行为的监视摄像头遍布于各大公共场所中。\n生物识别技术：指纹、虹膜和人脸匹配仍然是生物识别领域的一些常用方法。\n智能汽车：计算机视觉仍然是检测交通标志、灯光和其他视觉特征的主要信息来源。"}
{"content2":"装载自：https://blog.csdn.net/gdengden/article/details/80369458#commentBox\n目录\n简介\n方向\n热点\n简介\n计算机视觉（Computer Vision）又称为机器视觉（Machine Vision），顾名思义是一门“教”会计算机如何去“看”世界的学科。在机器学习大热的前景之下，计算机视觉与自然语言处理（Natural Language Process， NLP）及语音识别（Speech Recognition）并列为机器学习方向的三大热点方向。而计算机视觉也由诸如梯度方向直方图（Histogram of Gradient， HOG）以及尺度不变特征变换（Scale-Invariant Feature Transform， SIFT）等传统的手办特征（Hand-Crafted Feature）与浅层模型的组合逐渐转向了以卷积神经网络（Convolutional Neural Network， CNN）为代表的深度学习模型。\n方式\n特征提取\n决策模型\n传统方式\nSIFT，HOG， Raw Pixel …\nSVM， Random Forest， Linear Regression …\n深度学习\nCNN …\nCNN …\nsvm（Support Vector Machine） ： 支持向量机\nRandom Forest ： 随机森林\nLinear Regression ： 线性回归\nRaw Pixel ： 原始像素\n传统的计算机视觉对待问题的解决方案基本上都是遵循： 图像预处理 → 提取特征 → 建立模型（分类器/回归器） → 输出 的流程。 而在深度学习中，大多问题都会采用端到端（End to End）的解决思路，即从输入到输出一气呵成。本次计算机视觉的入门系列，将会从浅层学习入手，由浅入深过渡到深度学习方面。\n方向\n计算机视觉本身又包括了诸多不同的研究方向，比较基础和热门的几个方向主要包括了：物体识别和检测（Object Detection），语义分割（Semantic Segmentation），运动和跟踪（Motion & Tracking），三维重建（3D Reconstruction），视觉问答（Visual Question & Answering），动作识别（Action Recognition）等。\n物体识别和检测\n物体检测一直是计算机视觉中非常基础且重要的一个研究方向，大多数新的算法或深度学习网络结构都首先在物体检测中得以应用如VGG-net， GoogLeNet， ResNet等等，每年在imagenet数据集上面都不断有新的算法涌现，一次次突破历史，创下新的记录，而这些新的算法或网络结构很快就会成为这一年的热点，并被改进应用到计算机视觉中的其它应用中去，可以说很多灌水的文章也应运而生。\n物体识别和检测，顾名思义，即给定一张输入图片，算法能够自动找出图片中的常见物体，并将其所属类别及位置输出出来。当然也就衍生出了诸如人脸检测（Face Detection），车辆检测（Viechle Detection）等细分类的检测算法。\n近年代表论文\nHe, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nLiu, Wei, et al. “SSD: Single shot multibox detector.” European Conference on Computer Vision. Springer International Publishing, 2016.\nSzegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\nRen, Shaoqing, et al. “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems. 2015.\nSimonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.\n数据集\nIMAGENET\nPASCAL VOC\nMS COCO\nCaltech\n语义分割\n语义分割是近年来非常热门的方向，简单来说，它其实可以看做一种特殊的分类——将输入图像的每一个像素点进行归类，用一张图就可以很清晰地描述出来。\n很清楚地就可以看出，物体检测和识别通常是将物体在原图像上框出，可以说是“宏观”上的物体，而语义分割是从每一个像素上进行分类，图像中的每一个像素都有属于自己的类别。\n近年代表论文\nLong, Jonathan, Evan Shelhamer, and Trevor Darrell. “Fully convolutional networks for semantic segmentation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\nChen, Liang-Chieh, et al. “Semantic image segmentation with deep convolutional nets and fully connected crfs.” arXiv preprint arXiv:1412.7062 (2014).\nNoh, Hyeonwoo, Seunghoon Hong, and Bohyung Han. “Learning deconvolution network for semantic segmentation.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\nZheng, Shuai, et al. “Conditional random fields as recurrent neural networks.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\n数据集\nPASCAL VOC\nMS COCO\n运动和跟踪\n跟踪也属于计算机视觉领域内的基础问题之一，在近年来也得到了非常充足的发展，方法也由过去的非深度算法跨越向了深度学习算法，精度也越来越高，不过实时的深度学习跟踪算法精度一直难以提升，而精度非常高的跟踪算法的速度又十分之慢，因此在实际应用中也很难派上用场。\n那么什么是跟踪呢？就目前而言，学术界对待跟踪的评判标准主要是在一段给定的视频中，在第一帧给出被跟踪物体的位置及尺度大小，在后续的视频当中，跟踪算法需要从视频中去寻找到被跟踪物体的位置，并适应各类光照变换，运动模糊以及表观的变化等。但实际上跟踪是一个不适定问题（ill posed problem），比如跟踪一辆车，如果从车的尾部开始跟踪，若是车辆在行进过程中表观发生了非常大的变化，如旋转了180度变成了侧面，那么现有的跟踪算法很大的可能性是跟踪不到的，因为它们的模型大多基于第一帧的学习，虽然在随后的跟踪过程中也会更新，但受限于训练样本过少，所以难以得到一个良好的跟踪模型，在被跟踪物体的表观发生巨大变化时，就难以适应了。所以，就目前而言，跟踪算不上是计算机视觉内特别热门的一个研究方向，很多算法都改进自检测或识别算法。\n近年代表论文\nNam, Hyeonseob, and Bohyung Han. “Learning multi-domain convolutional neural networks for visual tracking.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nHeld, David, Sebastian Thrun, and Silvio Savarese. “Learning to track at 100 fps with deep regression networks.” European Conference on Computer Vision. Springer International Publishing, 2016.\nHenriques, João F., et al. “High-speed tracking with kernelized correlation filters.” IEEE Transactions on Pattern Analysis and Machine Intelligence 37.3 (2015): 583-596.\nMa, Chao, et al. “Hierarchical convolutional features for visual tracking.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\nBertinetto, Luca, et al. “Fully-convolutional siamese networks for object tracking.” European Conference on Computer Vision. Springer International Publishing, 2016.\nDanelljan, Martin, et al. “Beyond correlation filters: Learning continuous convolution operators for visual tracking.” European Conference on Computer Vision. Springer International Publishing, 2016.\nLi, Hanxi, Yi Li, and Fatih Porikli. “Deeptrack: Learning discriminative feature representations online for robust visual tracking.” IEEE Transactions on Image Processing 25.4 (2016): 1834-1848.\n数据集\nOTB(Object Tracking Benchmark)\nVOT(Visual Object Tracking)\n视觉问答\n视觉问答也简称VQA（Visual Question Answering），是近年来非常热门的一个方向，其研究目的旨在根据输入图像，由用户进行提问，而算法自动根据提问内容进行回答。除了问答以外，还有一种算法被称为标题生成算法（Caption Generation），即计算机根据图像自动生成一段描述该图像的文本，而不进行问答。对于这类跨越两种数据形态（如文本和图像）的算法，有时候也可以称之为多模态，或跨模态问题。\n近年代表论文\nXiong, Caiming, Stephen Merity, and Richard Socher. “Dynamic memory networks for visual and textual question answering.” arXiv 1603 (2016).\nWu, Qi, et al. “Ask me anything: Free-form visual question answering based on knowledge from external sources.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nZhu, Yuke, et al. “Visual7w: Grounded question answering in images.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n数据集\nVQA\n热点\n随着深度学习的大举侵入，现在几乎所有人工智能方向的研究论文几乎都被深度学习占领了，传统方法已经很难见到了。有时候在深度网络上改进一个非常小的地方，就可以发一篇还不错的论文。并且，随着深度学习的发展，很多领域的现有数据集内的记录都在不断刷新，已经向人类记录步步紧逼，有的方面甚至已经超越了人类的识别能力。那么，下一步的研究热点到底会在什么方向呢？就我个人的一些观点如下：\n多模态研究： 目前的许多领域还是仅仅停留在单一的模态上，如单一分物体检测，物体识别等，而众所周知的是现实世界就是有多模态数据构成的，语音，图像，文字等等。 VQA 在近年来兴起的趋势可见，未来几年内，多模态的研究方向还是比较有前景的，如语音和图像结合，图像和文字结合，文字和语音结合等等。\n数据生成： 现在机器学习领域的许多数据还是由现实世界拍摄的视频及图片经过人工标注后用作于训练或测试数据的，标注人员的职业素养和经验，以及多人标注下的规则统一难度在一定程度上也直接影响了模型的最终结果。而利用深度模型自动生成数据已经成为了一个新的研究热点方向，如何使用算法来自动生成数据相信在未来一段时间内都是不错的研究热点。\n无监督学习：人脑的在学习过程中有许多时间都是无监督（Un-supervised Learning）的，而现有的算法无论是检测也好识别也好，在训练上都是依赖于人工标注的有监督（Supervised Learning）。如何将机器学习从有监督学习转变向无监督学习，应该是一个比较有挑战性的研究方向，当然这里的无监督学习当然不是指简单的如聚类算法（Clustering）这样的无监督算法。而LeCun也曾说： 如果将人工智能比喻作一块蛋糕的话，有监督学习只能算是蛋糕上的糖霜，而增强学习（Reinforce Learning）则是蛋糕上的樱桃，无监督学习才是真正蛋糕的本体。\n最后，想要把握领域内最新的研究成果和动态，还需要多看论文，多写代码。\n计算机视觉领域内的三大顶级会议有：\nConference on Computer Vision and Pattern Recognition （CVPR）\nInternational Conference on Computer Vision （ICCV）\nEuropean Conference on Computer Vision （ECCV）\n较好的会议有以下几个：\nThe British Machine Vision Conference （BMVC）\nInternational Conference on Image Processing （ICIP）\nWinter Conference on Applications of Computer Vision （WACV）\nAsian Conference on Computer Vision (ACCV)\n当然，毕竟文章的发表需要历经审稿和出版的阶段，因此当会议论文集出版的时候很可能已经过了小半年了，如果想要了解最新的研究，建议每天都上ArXiv的cv板块看看，ArXiv上都是预出版的文章，并不一定最终会被各类会议和期刊接收，所以质量也就良莠不齐，对于没有分辨能力的入门新手而言，还是建议从顶会和顶级期刊上的经典论文入手。\n这是一篇对计算机视觉目前研究领域的几个热门方向的一个非常非常简单的介绍，希望能对想要入坑计算机视觉方向的同学有一定的帮助。由于个人水平十分有限，错误在所难免，欢迎大家对文中的错误进行批评和指正。\n小白入门计算机视觉：这是最全的一份CV技术学习之路\n2017年12月02日 00:00:00\n阅读数：4377\nAI\n菌\n最近AI菌决定把自己的机器学习之路向计算机视觉方面发展。所以今天就来给大家分享一下AI菌收集到的资料以及心得\nThe M Tank 编辑了一份报告《A Year in Computer Vision》，记录了 2016 至 2017 年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。该材料共包括四大部分\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率、风格迁移、着色\n动作识别\n第三部分\n3D 目标\n人体姿势估计\n3D 重建\n其他未分类 3D\n总结\n第四部分\n卷积架构\n数据集\n不可分类的其他材料与有趣趋势\n结论\n有兴趣的同学可以读一读，完整 PDF 地址：http://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf\n下面是收集到的学习资料与心得的汇总：\n（文中没有发的资源将在之后陆续放出）\n01 掌握好相应的基础能力\n计算机视觉的理念其实与很多概念有部分重叠，包括：人工智能、数字图像处理、机器学习、深度学习、模式识别、概率图模型、科学计算以及一系列的数学计算等。\n所以在入门CV之前，同学们最好对基础的学术课程都有对应的了解，比如数学方面的微积分，概率学，统计学，线性代数这几门基础课程。\n在编程语言方面，Matlab，Python，C++，最好熟悉其中2种，因为计算机视觉离开计算机编程是完全行不通的\n\n02 需要的专业工具\n工欲善其事，必先利其器。对于想要学好计算机视觉的同学来说，一个专业的工具，绝对是助攻的不二神器。\nOpenCV（开源计算机视觉库）是一个非常强大的学习资料库，包括了计算机视觉，模式识别，图像处理等许多基本算法。\n它免费提供给学术和商业用途，有C++，C，Python和java接口，支持Windows、Linux、Mac OS、iOS和Android。\n而关于OpenCV的学习，AI菌推荐（其中第三本目前无中文版）：\n学习OpenCV(Learning.OpenCV)\n链接：\nhttps://pan.baidu.com/s/1c2GrPEK 密码：7012\n毛星云老师编著的OpenCV3编程入门\n链接：\nhttps://pan.baidu.com/s/1c2xuVFq 密码：2s4a\n学习OpenCV3（\nLearning OpenCV 3\n）\n链接：\nhttps://pan.baidu.com/s/1geQeT0J 密码：cuco\n而深度学习方面，有TensorFlow，PyTorch，Caffe等深度学习框架，它们也内置了OpenCV的API接口。而哪种框架好，就要看你自己的需要了\n推荐资料：\n莫凡教程系列之PyTorch :https://morvanzhou.github.io/tutorials/machine-learning/torch/\nTensorFlow中文社区：\nhttp://www.tensorfly.cn/\n深度学习 21天实战Caffe\n\n03 绕不开的数字图像处理与模式识别\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。\n入门的同学推荐\n冈萨雷斯的《数字图像处理》《数字图像处理(第3版)(英文版)》和对应的Matlab版本\n一本讲基础的理论，一本讲怎么用Matlab实现。\n除此之外同学们还可以去YouTube上找到相关的课程信息，相信大家会有所收获的。\n模式识别（Pattern Recognition），就是通过计算机用数学技术方法来研究模式的自动处理和判读。我们把环境与客体统称为“模式”。\n计算机视觉很多东西都是基于图像识别的，图像识别就是模式识别的一种。\n模式识别通常是训练一个模型来拟合当前的数据，当我们拿到一堆数据或图片，需要从当中找到它们的关系，最便捷的便是用模式识别算法来训练一个模型。\nAI菌推荐一本模式识别入门级的教材《模式分类》，相对于《模式识别》这本书来说可能比较难，但书中介绍了很多模式识别经典的分类器，还是很值得一读。\n其中的一些思想在神经网络中也可以应用的\n\n04 系统的学习下计算机视觉课程\n对于CV新手来说，想要从小白到大神，最快的方法就是先系统的学习一下计算机视觉的课程，全面了解一下计算机视觉这个领域的背景及其发展、这个领域有哪些基本的问题、哪些问题的研究已经比较成熟了，哪些问题的研究还处于基础阶段。\n在这里AI菌推荐3本经典教材：\n《计算机视觉：一种现代方法》（Computer Vision: A Modern Approach）\n《计算机视觉_算法与应用》\n（Computer Vision: Algorithms and Applications）\n《计算机视觉：模型 学习和推理》\n（Computer Vision: Models, Learning, and Inference）\n这三本教材AI菌认为是计算机视觉最好的入门教材了，内容丰富，难度适中，其中第二本书涉及大量的文献，很适合对计算机视觉没什么概念的同学。\n虽然其中的一些方法在现在看来已经过时了，但还是值得一读\n05 深度学习与CNN\n关于深度学习这几年讲的已经太多了，资料也非常多，AI菌在这里就不在赘述啦\n计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。\n什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。\n同学们可以试着学习下CNN在计算机视觉当中的应用\n推荐的资料：\n斯坦福CS231n—深度学习与计算机视觉网易云课堂课程：http://study.163.com/course/introduction.htm?courseId=1003223001\n斯坦福CS231n—深度学习与计算机视觉官方课程：http://cs231n.stanford.edu/\nCS231n官方笔记授权翻译总集篇：https://www.52ml.net/17723.html\n吴恩达 deeplearning.ai与网易云课堂的微专业深度学习工程师卷积神经网络\nhttp://mooc.study.163.com/course/2001281004?tid=2001392030#/info\n神经网络方面的经典教材\n《深度学习》\n（Deep Learning）\n《神经⽹络与深度学习》\n（Neural Networks and Deep Learning(Nielsen,2017)）\n\n06 了解最新领域动态\n很多同学做研究的时候，容易陷入自我封闭的“怪圈”，过于执着于埋头学习相关知识，有时候会忘记及时了解相关领域的最新动态，这是非常不科学的。\n同学们在学习计算机视觉相关知识的时候，可以通过最新的paper来了解这个领域最新提出的一些概念以及发展的情况。\n计算机视觉的期刊有两个PAMI（模式分析与机器智能汇刊）和IJCV（计算机视觉国际期刊）\n顶级的学术会议有 CVPR、ICCV、 ECCV、 BMVC这四个，同学们可以跟着浏览这些期刊论文以及会议文章，相信一定可以学到不少有用的知识。\n\nAI\n菌\n听做视觉的师兄师姐硕：做好计算机视觉研究并不是一件容易的事情，在大多数情况下它甚至是一件很枯燥的事情。\n研究成果毫无进展，研究方向不在明朗等等，这一切都会给你前所未有的压力\n所以希望同学们在决定入这一行的时候，是出于自己的热爱，而不是出于当前的趋势。\n因为热爱不会变，但趋势每一年都在变。\n计算机视觉是人工智能技术的一个重要领域，打个比方（不一定恰当），我认为计算机视觉是人工智能时代的眼睛，可见其重要程度。计算机视觉其实是一个很宏大的概念，下图是有人总结的计算机视觉所需要的技能树。\n如果你是一个对计算机视觉一无所知的小白，千万不要被这棵技能树吓到。没有哪个人能够同时掌握以上所有的技能，这棵树只是让你对计算机视觉有个粗浅的认识。\n以下是我站在一个小白的视角给出一个入门计算机视觉的相对轻松的姿势。\n一、宏观认识\n小白通常看到这么多的细分方向大脑一片茫然，到底是学习人脸识别、物体跟踪，又或者是计算摄影，三维重建呢？不知道该怎么下手。其实这些细分方向有很多共通的知识，我的建议是心急吃不了热豆腐，只有对计算机视觉这个领域有了一个初步的全面了解，你才能够结合实际问题找到自己感兴趣的研究方向，而兴趣能够支持一个自学的小白克服困难持续走下去。\n1、入门书籍\n既然说是入门，这里就不推荐类似《 Multiple View Geometry in Computer Vision》这种虽然经典但是小白看了容易放弃的书了。\n像素级的图像处理知识是计算机视觉的底层基础知识。不管你以后从事计算机视觉的哪个细分领域，这些基础知识都是必须要了解的。即使一个急切入门的小白，这一关也必须走的踏实。看到网上有人说直接从某个项目开始，边做边学，这样学的快。对此我表示部分赞成，原因是他忽略了基础知识的重要性，脑子里没有基本的术语概念知识打底，很多问题他根本不知道如何恰当的表达，遇到问题也没有思路，不知道如何搜索，这会严重拖慢进度，也无法做较深入的研究，欲速则不达。\n入门图像处理的基础知识也不是直接去啃死书，否则几个公式和术语可能就会把小白打翻在地。这里推荐两条途径，都是从实践出发并与理论结合：一个是OpenCV，一个是MATLAB。\nOpenCV以C++为基础，需要具备一定的编程基础，可移植性强，运行速度比较快，比较适合实际的工程项目，在公司里用的较多；MATLAB只需要非常简单的编程基础就可以很快上手，实现方便，代码比较简洁，可参考的资料非常丰富，方便快速尝试某个算法效果，适合做学术研究。当然两者搭配起来用更好啦。下面分别介绍一下。\n用MATLAB学习图像处理\n推荐使用冈萨雷斯的《数字图像处理（MATLAB版）》（英文原版2001年出版，中译版2005年）。不需要一上来就全部过一遍，只需要结合MATLAB学习一下基本原理、图像变换、形态学处理、图像分割，以上章节强烈建议按照书上手动敲一遍代码（和看一遍的效果完全不同），其他章节可快速扫描一遍即可。但这本书比较注重实践，对理论的解释不多，理论部分不明白的可以在配套的冈萨雷斯的《数字图像处理（第二版）》这本书里查找，这本书主要是作为工具书使用，以后遇到相关术语知道去哪里查就好。\n用OpenCV学习图像处理\nOpenCV（Open Source Computer Vision Library）是一个开源跨平台计算机视觉程序库，主要有C++预研编写，包含了500多个用于图像/视频处理和计算机视觉的通用算法。\n学习OpenCV参考《学习OpenCV》或者《OpenCV 2 计算机视觉编程手册》都可以。这两本都是偏实践的书，理论知识较少，按照书上的步骤敲代码，可以快速了解到OpenCV的强大，想要实现某个功能，只要学会查函数（在https://www.docs.opencv.org/查询对应版本），调函数就可以轻松搞定。由于每个例子都有非常直观的可视化图像输出，所以学起来比较轻松有趣。\n2、进阶书籍\n经过前面对图像处理的基本学习，小白已经了解了图像处理的基础知识，并且会使用OpenCV或MATLAB来实现某个简单的功能。但是这些知识太单薄了，并且比较陈旧，计算机视觉领域还有大量的新知识在等你。\n同样给你两种选择，当然两个都选更佳。一本书是2010年出版的美国华盛顿大学Richard Szeliski写的《Computer Vision: Algorithms and Application》；一本是2012年出版的，加拿大多伦多大学Simon J.D. Prince写的《Computer Vision: Models, Learning, and Inference》。两本书侧重点不同，前者侧重视觉和几何知识，后者侧重机器学习模型。当然两本书也有互相交叉的部分。虽然都有中文版，但是如果有一定的英语阅读基础，推荐看英文原版（见文末获取方式）。老外写的书，图和示例还是挺丰富的，比较利于 理解。\n《Computer Vision: Algorithms and Application》\n这本书图文并茂地介绍了计算机视觉这门学科的诸多大方向，有了前面《数字图像处理》的基础，这本书里有些内容你已经熟悉了，没有那么强的畏惧感。相对前面的图像处理基础本书增加了许多新的内容，比如特征检测匹配、运动恢复结构、稠密运动估计、图像拼接、计算摄影、立体匹配、三维重建等，这些都是目前比较火非常实用的方向。如果有时间可以全书浏览，如果时间不够，你可以根据兴趣，选择性的看一些感兴趣的方向。这本书的中文版翻译的不太好，可以结合英文原版看。\n《Computer Vision: Models, Learning, and Inference》\n该书从基础的概率模型讲起，涵盖了计算机视觉领域常用的概率模型、回归分类模型、图模型、优化方法等，以及偏底层的图像处理、多视角几何知识，图文并茂，并辅以非常多的例子和应用，非常适合入门。在其主页：\nhttp://www.computervisionmodels.com/\n上可以免费下载电子书。此外还有非常丰富的学习资源，包括给教师用的PPT、每章节对应的开源项目、代码、数据集链接等，非常有用。\n二、深入实践\n当你对计算机视觉领域有了比较宏观的了解，下一步就是选一个感兴趣的具体的领域去深耕。这个时期就是具体编程实践环节啦，实践过程中有疑问，根据相关术语去书里查找，结合Google，基本能够解决你大部分问题。\n那么具体选择什么方向呢？\n如果你实验室或者公司有实际的项目，最好选择当前项目方向深耕下去。如果没有具体方向，那么继续往下看。\n我个人认为计算机视觉可以分为两大方向：基于学习的方法和基于几何的方法。其中基于学习的方法最火的就是深度学习，而基于几何方法最火的就是视觉SLAM。下面就这两个方向给出一个相对轻松的入门姿势。\n1、深度学习\n深度学习（Deep Learning）的概念是Hinton等人于2006年提出的，最早最成功的应用领域就是计算机视觉，经典的卷积神经网络就是为专门处理图片数据而生。目前深度学习已经广泛应用在计算机视觉、语音识别、自然语言处理、智能推荐等领域。\n学习深度学习需要一定的数学基础，包括微积分、线性代数，很多小白一听到这些课程就想起了大学时的噩梦，其实只用了非常基础的概念，完全不用担心。不过如果一上来就啃书本，可能会有强烈的畏难情绪，很容易早早的放弃。\nAndrew Ng (吴恩达)的深度学习视频课程我觉得是一个非常好的入门资料。首先他本人就是斯坦福大学的教授，所以很了解学生，可以很清晰形象、深入浅出的从最基本的导数开始讲起，真的非常难得。\n该课程可以在网易云课程上免费观看，有中文字幕，但没有配套习题。也可以在吴恩达自己创办的在线教育平台Coursera上学习，有配套习题，限时免费，结业通过后有相应证书。\n该课程非常火爆，不用担心听不懂，网上有数不清的学习笔记可以参考。简直小白入门必备佳肴。\n2、视觉SLAM\nSLAM（Simultaneous Localization and Mapping）（详见《SLAM初识》），中文译作同时定位与地图创建。视觉SLAM就是用摄像头作为主传感器，用拍摄的视频流作为输入来实现SLAM。视觉SLAM广泛应用于VR/AR、自动驾驶、智能机器人、无人机等前沿领域。\n视觉SLAM最好的入门资料是高翔（清华博士，慕尼黑理工博后）的《视觉SLAM十四讲-从理论到实践》。该书每章节都涵盖了基础理论和代码示例，深入浅出，非常注重理论与实践结合，大大降低了小白的学习门槛。\n好了，入门介绍到此为止，你可以开始你的计算机视觉学习之旅了！\n温馨提示：本文提到的部分书籍资料，公众号：“计算机视觉life” 已经为你准备好了，公众号下方回复“入门”即可获取。\n以下内容整理自 2017 年 6 月 29 日由“趣直播–知识直播平台”邀请的嘉宾实录。\n分享嘉宾: 罗韵\n目前，人工智能，机器学习，深度学习，计算机视觉等已经成为新时代的风向标。这篇文章主要介绍了下面几点：\n第一点，如果说你要入门计算机视觉，需要了解哪一些基础知识？\n第二点，既然你要往这方面学习，你要了解的参考书籍，可以学习的一些公开课有哪些？\n第三点，可能是大家都比较感兴趣的，就是计算机视觉作为人工智能的一个分支，它不可避免的要跟深度学习做结合，而深度学习也可以说是融合到了计算机视觉、图像处理，包括我们说的自然语言处理，所以本文也会简单介绍一下计算机视觉与深度学习的结合。\n第四点，身处计算机领域，我们不可避免的会去做开源的工作，所以本文会给大家介绍一些开源的软件。\n第五点，要学习或者研究计算机视觉，肯定是需要去阅读一些文献的，那么我们如何开始阅读文献，以及慢慢的找到自己在这个领域的方向，这些都会在本文理进行简单的介绍。\n1.基础知识\n接下来要介绍的，第一点是计算机视觉是什么意思，其次是图像、视频的一些基础知识。包括摄像机的硬件，以及 CPU 和 GPU 的运算。\n在计算机视觉里面，我们也不可避免的会涉及到考虑去使用 CPU 还是使用 GPU 去做运算。然后就是它跟其他学科的交叉，因为计算机视觉可以和很多的学科做交叉，而且在做学科交叉的时候，能够发挥的意义和使用价值也会更大。另外，对于以前并不是做人工智能的朋友，可能是做软件开发的，想去转型做计算机视觉，该如何转型？需要学习哪些编程语言以及数学基础？这些都会在第一小节给大家介绍。\n1.0 什么是计算机视觉\n计算机视觉是一门研究如何使机器“看”的科学。\n更进一步的说，就是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给一起检测的图像\n作为一个科学学科，计算机视觉研究相关的理论和技术，视图建立能够从图像或者多维数据中获取“信息”的人工智能系统。\n目前，非常火的VR、AR，3D处理等方向，都是计算机视觉的一部分。\n计算机视觉的应用\n无人驾驶\n无人安防\n人脸识别\n车辆车牌识别\n以图搜图\nVR/AR\n3D重构\n医学图像分析\n无人机\n其他\n了解了计算机视觉是什么之后，给大家列了一下当前计算机视觉领域的一些应用，几乎可以说是无处不在，而且当前最火的所有创业的方向都涵盖在里面了。其中包括我们经常提到的无人驾驶、无人安防、人脸识别。人脸识别相对来说已经是一个最成熟的应用领域了，然后还有文字识别、车辆车牌识别，还有以图搜图、 VR/AR，还包括 3D 重构，以及当下很有前景的领域–医学图像分析。\n医学图像分析他在很早就被提出来了，已经研究了很久，但是现在得到了一个重新的发展，更多的研究人员包括无论是做图像的研究人员，还是本身就在医疗领域的研究人员，都越来越关注计算机视觉、人工智能跟医学图像的分析。而且在当下，医学图像分析也孕育了不少的创业公司，这个方向的未来前景还是很值得期待的。然后除此之外还包括无人机，无人驾驶等，都应用到了计算机视觉的技术。\n1.1图像和视频，你要知道的概念\n图像\n一张图片包含了：维数、高度、宽度、深度、通道数、颜色格式、数据首地址、结束地址、数据量等等。\n图像深度：存储每个像素所用的位数（bits）\n当一个像素占用的位数越多时，它所能表现的颜色就更多，更丰富。\n举例：一张400*400的8位图，这张图的原始数据量是多少？像素值如果是整型的话，取值范围是多少？\n1，原始数据量计算：400 * 400 * ( 8/8 )=160,000Bytes\n(约为160K)\n2，取值范围：2的8次方，0~255\n图片格式与压缩：常见的图片格式JPEG，PNG，BMP等本质上都是图片的一种压缩编码方式\n举例：JPEG压缩\n1，将原始图像分为8*8的小块，每个block里有64pixels。\n2，将图像中每个8*8的block进行DCT变换（越是复杂的图像，越不容易被压缩）\n3，不同的图像被分割后，每个小块的复杂度不一样，所以最终的压缩结果也不一样\n视频\n原始视频=图片序列。\n视频中的每张有序图片称为“帧（frame）”。压缩后的视频，会采取各种算法减少数据的容量，其中IPB就是最常见的。\nI帧：表示关键帧，可以理解为这一幅画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）\nP帧：表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧画面差别的数据）\nB帧表示双向差别帧，记录的本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，要通过前后画面与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码比较麻烦。\n码率：码率越大，体积越大；码率越小，体积越小。\n码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。也就是取样率（并不等同于采样率，采样率用的单位是Hz，表示每秒采样的次数），单位时间内取样率越大，精度就越高，处理出来的文件就越接近原始文件，但是文件体积与取样率是成正比的，所以几乎所有的编码格式重视的都是如何用最低的码率达到最少的失真，围绕这个核心衍生出来cbr（固定码率）与vbr（可变码率），码率越高越清晰，反之则画面粗糙而且多马赛克。\n帧率\n影响画面流畅度，与画面流畅度成正比：帧率越大，画面越流畅；帧率越小，画面越有跳动感。如果码率为变量，则帧率也会影响体积，帧率越高，每秒钟经过的画面就越多，需要的码率也越高，体积也越大。\n帧率就是在一秒钟时间里传输的图片的帧数，也可以理解为图形处理器每秒钟刷新的次数。\n分辨率\n影响图像大小，与图像大小成正比；分辨率越高，图像越大；分辨率越低，图像越小。\n清晰度\n在码率一定的情况下，分辨率与清晰度成反比关系：分辨率越高，图像越不清晰，分辨率越低，图像越清晰\n在分辨率一定的情况下，码率与清晰度成正比关系：码率越高，图像越清晰；码率越低，图像越不清晰\n带宽、帧率\n例如在ADSL线路上传输图像，上行带宽只有512Kbps，但要传输4路CIF分辨率的图像。按照常规，CIF分辨率建议码率是512Kbps，那么照此计算就只能传一路，降低码率势必会影响图像质量。那么为了确保图像质量，就必须降低帧率，这样一来，即便降低码率也不会影响图像质量，但在图像的连贯性上会有影响。\n1.2摄像机\n摄像机的分类：\n监控摄像机（网络摄像机和摸你摄像机）\n不同行业需求的摄像机（超宽动态摄像机、红外摄像机、热成像摄像机等）\n智能摄像机\n工业摄像机\n当前的摄像机硬件我们可以分为监控摄像机、专业行业应用的摄像机、智能摄像机和工业摄像机。而在监控摄像机里面，当前用的比较多的两个类型一个叫做网络摄像机，一个叫做模拟摄相机，他们主要是成像的原理不太一样。\n网络摄像机一般比传统模拟摄相机的清晰度要高一些，模拟摄像机当前应该说是慢慢处于一个淘汰的状态，它可以理解为是上一代的监控摄像机，而网络摄像机是当前的一个主流的摄相机，大概在 13 年的时候，可能市场上 70% 到 80% 多都是模拟摄像机，而现在可能 60% 到 70% 都是的网络摄像机。\n除此之外，不同的行业其时会有特定的相机，想超宽动态摄像机以及红外摄像机、热成像摄像机，都是在专用的特定的领域里面可能用到的，而且他获得的画面跟图像是完全不一样的。如果我们要做图像处理跟计算机视觉分析，什么样的相机对你更有利，我们要学会利用硬件的优势。\n如果是做研究的话一般是可以控制我们用什么样的摄相机，但如果是在实际的应用场景，这个把控的可能性会稍微小一点，但是在这里你要知道，有些问题可能你换一种硬件，它就能够很好的被解决，这是一个思路。\n还有些问题你可能用算法弄了很久也没能解决，甚至是你的效率非常差，成本非常高，但是稍稍换一换硬件，你会发现原来的问题都不存在了，都被很好的解决了，这个就是硬件对你的一个新的处境了。\n包括现在还有智能摄像机、工业摄像机，工业摄像机一般的价格也会比较贵，因为他专用于各种工业领域，或者是做一些精密仪器，高精度高清晰度要求的摄像机。\n1.3 CPU和GPU\n接下来给大家讲一下 CPU 跟 GPU，如果说你要做计算机视觉跟图像处理，那么肯定跳不过 GPU 运算，GPU 运算这一块可能也是接下来需要学习或者自学的一个知识点。\n因为可以看到，当前大部分关于计算机视觉的论文，很多实现起来都是用 GPU 去实现的，但是在应用领域，因为 GPU 的价格比较昂贵，所以 CPU 的应用场景相对来说还是占大部分。\n而 CPU 跟 GPU 的差别主要在哪里呢？ 它们的差别主要可以在两个方面去对比，第一个叫性能，第二个叫做吞吐量。\n性能，换言之，性能会换成另外一个单词叫做 Latency（低延时性）。低延时性就是当你的性能越好，你处理分析的效率越高，相当于你的延时性就越低，这个是性能。另外一个叫做吞吐量，吞吐量的意思就是你同时能够处理的数据量。\n而 CPU 跟 GPU 的差别在哪里呢？主要就在于这两个地方，CPU 它是一个高性能，就是超低延时性的，他能够快速的去做复杂运算，并且能达到一个很好的性能要求。而 GPU是以一个叫做运算单元为格式的，所以他的优点不在于低延时性，因为他确实不善于做复杂运算，他每一个处理器都非常的小，相对来说会很弱，但是它可以让它所有的弱处理器，同时去做处理，那相当于他就能够同时处理大量的数据，那这个就意味着它的吞吐量非常大，所以 CPU重视的是性能，GPU重视的是吞吐量。\n所以大部分时候，GPU 他会跟另外一个词语联系在一起，叫做并行计算，意思就是它可以同时做大量的线程运算，为什么图像会特别适合用 GPU 运算呢？这是因为 GPU 它最开始的设计就是叫做图形处理单元，它的意思就是我可以把每一个像素，分割为一个线程去运算，每一个像素只做一些简单的运算，这个就是最开始图形处理器出现的原理。\n它要做图形渲染的时候，要计算的是每一个像素的变换。所以每一个像素变换的计算量是很小很小的，可能就是一个公式的计算，计算量很少，它可以放在一个简单的计算单元里面去做计算，那这个就是 CPU 跟 GPU 的差别。\n基于这样的差别，我们才会去设计什么时候用 CPU，什么时候用 GPU。如果你当前设计的算法，它的并行能力不是很强，从头到尾从上到下都是一个复杂的计算，没有太多可并性的地方，那么即使你用了 GPU，也不能帮助你很好提升计算性能。\n所以，不要说别人都在用 GPU 那你就用 GPU，我们要了解的是为什么要用 GPU ，以及什么样的情况下用 GPU，它效果能够发挥出来最好。\n1.4计算机视觉与其他学科的关系\n计算机视觉目前跟其他学科的关系非常的多，包括机器人，以及刚才提到的医疗、物理、图像、卫星图片的处理，这些都会经常使用到计算机视觉，那这里呢，最常问到的问题无非就是有三个概念，一个叫做计算机视觉，一个叫做机器视觉，一个叫做图像处理，那这三个东西有什么区别呢？\n这三个东西的区别还是挺因人而异的，每一个研究人员对它的理解都不一样。\n首先，Image Processing更多的是图形图像的一些处理，图像像素级别的一些处理，包括 3D 的处理，更多的会理解为是一个图像的处理；而机器视觉呢，更多的是它还结合到了硬件层面的处理，就是软硬件结合的图形计算的能力，跟图形智能化的能力，我们一般会理解为他就是所谓的机器视觉。\n而我们今天所说的计算机视觉，更多的是偏向于软件层面的计算机处理，而且不是说做图像的识别这么简单，更多的还包括了对图像的理解，甚至是对图像的一些变换处理，当前我们涉及到的一些图像的生成，也是可以归类到这个计算机视觉领域里面的。\n所以说计算机视觉它本身的也是一个很基础的学科，可以跟各个学科做交叉，同时，它自己内部也会分的比较细，包括机器视觉、图像处理。\n1.5 编程语言AND数学基础\n这一部分的内容可以参见《非计算机专业，如何学习计算机视觉》\n2.参考书籍和公开课\n参考书\n第一本叫《Computer Vision：Models, Learning and Inference》written by Simon J.D. prince，这个主要讲的更适合入门级别的，因为这本书里面配套了非常多的代码，Matlab 代码，C 的代码都有，配套了非常多的学习代码，以及参考资料、文献，都配得非常详细，所以它很适合入门级别的同学去看。\n第二本《Computer Vision：Algorithms and Applications》written by Richard Szeliski，这是一本非常经典，非常权威的参考资料，这本书不是用来看的，是用来查的，类似于一本工具书，它是涵盖面最广的一本参考书籍，所以一般会可以当成工具书去看，去查阅。\n第三本《OpenCV3编程入门》作者：毛星云，冷雪飞 ，如果想快速的上手去实现一些项目，可以看看这本书，它可以教你动手实现一些例子，并且学习到 OpenCV 最经典、最广泛的计算机视觉开源库。\n公开课：\nStanford CS223B\n比较适合基础，适合刚刚入门的同学，跟深度学习的结合相对来说会少一点，不会整门课讲深度学习，而是主要讲计算机视觉，方方面面都会讲到。\nStanford CS231N\n这个应该不用介绍了，一般很多人都知道，这个是计算机视觉和深度学习结合的一门课，我们上 YouTube 就能够看到，这门课的授课老师就是李飞飞老师，如果说不知道的话可以查一下，做计算机视觉的话，此人算是业界和学术界的“执牛耳”了。\n3.需要了解的深度学习知识\n深度学习没有太多的要讲的，不是说内容不多，是非常多，这里只推荐一本书给大家，这本书是去年年底才出的，是最新的一本深度学习的书，它讲得非常全面，从基础的数学，到刚才说的概率学、统计学、机器学习以及微积分、线性几何的知识点，非常的全面。\n4.需要了解和学习的开源软件\nOpenCV\n它是一个很经典的计算机视觉库，实现了很多计算机视觉的常用算法。可以帮助大家快速上手。\nCaffe\n如果是做计算机视觉的话，比较建议 Caffe。Caffe 更擅长做的是卷积神经网络，卷积神经网络在计算机视觉里面用的是最多的。\n所以无论你后面学什么样其它的开源软件， Caffe 是必不可免的，因为学完 Caffe 之后你会发现，如果你理解了 Caffe，会用 Caffe，甚至是有能力去改它的源代码，你就会发现你对深度学习有了一个质的飞跃的理解。\nTensorFlow\nTensorFlow 最近很火，但是它的入门门槛不低，你要学会使用它需要的时间远比其他所有的软件都要多，其次就是它当前还不是特别的成熟稳定，所以版本之间的更新迭代非常的多，兼容性并不好，运行效率还有非常大的提升空间。\n5.如何阅读相关的文献\n先熟悉所在方向的发展历程，然后精读历程中的里程碑式的文献。\n例如：深度学习做目标检测，RCNN，Fast RCNN，Faster RCNN，SPPNET，SSD和YOLO这些模型肯定是要知道的。又例如，深度学习做目标跟踪，DLT，SO-DLT等。\n计算机视觉的顶会：\nICCV：International Conference on Computer Vision，国际计算机视觉大会\nCVPR：International Conference on Computer Vision and Pattern Recognition，国际计算机视觉与模式识别大会\nECCV：European Conference on Computer Vision，欧洲计算机视觉大会\n除了顶会之外呢，还有顶刊。像 PAMI、IJCV，这些都是顶刊，它代表着这个领域里面最尖端最前沿以及当下的研究方向。 目录\n简介\n方向\n热点\n简介\n计算机视觉（Computer Vision）又称为机器视觉（Machine Vision），顾名思义是一门“教”会计算机如何去“看”世界的学科。在机器学习大热的前景之下，计算机视觉与自然语言处理（Natural Language Process， NLP）及语音识别（Speech Recognition）并列为机器学习方向的三大热点方向。而计算机视觉也由诸如梯度方向直方图（Histogram of Gradient， HOG）以及尺度不变特征变换（Scale-Invariant Feature Transform， SIFT）等传统的手办特征（Hand-Crafted Feature）与浅层模型的组合逐渐转向了以卷积神经网络（Convolutional Neural Network， CNN）为代表的深度学习模型。\n方式\n特征提取\n决策模型\n传统方式\nSIFT，HOG， Raw Pixel …\nSVM， Random Forest， Linear Regression …\n深度学习\nCNN …\nCNN …\nsvm（Support Vector Machine） ： 支持向量机\nRandom Forest ： 随机森林\nLinear Regression ： 线性回归\nRaw Pixel ： 原始像素\n传统的计算机视觉对待问题的解决方案基本上都是遵循： 图像预处理 → 提取特征 → 建立模型（分类器/回归器） → 输出 的流程。 而在深度学习中，大多问题都会采用端到端（End to End）的解决思路，即从输入到输出一气呵成。本次计算机视觉的入门系列，将会从浅层学习入手，由浅入深过渡到深度学习方面。\n方向\n计算机视觉本身又包括了诸多不同的研究方向，比较基础和热门的几个方向主要包括了：物体识别和检测（Object Detection），语义分割（Semantic Segmentation），运动和跟踪（Motion & Tracking），三维重建（3D Reconstruction），视觉问答（Visual Question & Answering），动作识别（Action Recognition）等。\n物体识别和检测\n物体检测一直是计算机视觉中非常基础且重要的一个研究方向，大多数新的算法或深度学习网络结构都首先在物体检测中得以应用如VGG-net， GoogLeNet， ResNet等等，每年在imagenet数据集上面都不断有新的算法涌现，一次次突破历史，创下新的记录，而这些新的算法或网络结构很快就会成为这一年的热点，并被改进应用到计算机视觉中的其它应用中去，可以说很多灌水的文章也应运而生。\n物体识别和检测，顾名思义，即给定一张输入图片，算法能够自动找出图片中的常见物体，并将其所属类别及位置输出出来。当然也就衍生出了诸如人脸检测（Face Detection），车辆检测（Viechle Detection）等细分类的检测算法。\n近年代表论文\nHe, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nLiu, Wei, et al. “SSD: Single shot multibox detector.” European Conference on Computer Vision. Springer International Publishing, 2016.\nSzegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\nRen, Shaoqing, et al. “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems. 2015.\nSimonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.\n数据集\nIMAGENET\nPASCAL VOC\nMS COCO\nCaltech\n语义分割\n语义分割是近年来非常热门的方向，简单来说，它其实可以看做一种特殊的分类——将输入图像的每一个像素点进行归类，用一张图就可以很清晰地描述出来。\n很清楚地就可以看出，物体检测和识别通常是将物体在原图像上框出，可以说是“宏观”上的物体，而语义分割是从每一个像素上进行分类，图像中的每一个像素都有属于自己的类别。\n近年代表论文\nLong, Jonathan, Evan Shelhamer, and Trevor Darrell. “Fully convolutional networks for semantic segmentation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\nChen, Liang-Chieh, et al. “Semantic image segmentation with deep convolutional nets and fully connected crfs.” arXiv preprint arXiv:1412.7062 (2014).\nNoh, Hyeonwoo, Seunghoon Hong, and Bohyung Han. “Learning deconvolution network for semantic segmentation.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\nZheng, Shuai, et al. “Conditional random fields as recurrent neural networks.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\n数据集\nPASCAL VOC\nMS COCO\n运动和跟踪\n跟踪也属于计算机视觉领域内的基础问题之一，在近年来也得到了非常充足的发展，方法也由过去的非深度算法跨越向了深度学习算法，精度也越来越高，不过实时的深度学习跟踪算法精度一直难以提升，而精度非常高的跟踪算法的速度又十分之慢，因此在实际应用中也很难派上用场。\n那么什么是跟踪呢？就目前而言，学术界对待跟踪的评判标准主要是在一段给定的视频中，在第一帧给出被跟踪物体的位置及尺度大小，在后续的视频当中，跟踪算法需要从视频中去寻找到被跟踪物体的位置，并适应各类光照变换，运动模糊以及表观的变化等。但实际上跟踪是一个不适定问题（ill posed problem），比如跟踪一辆车，如果从车的尾部开始跟踪，若是车辆在行进过程中表观发生了非常大的变化，如旋转了180度变成了侧面，那么现有的跟踪算法很大的可能性是跟踪不到的，因为它们的模型大多基于第一帧的学习，虽然在随后的跟踪过程中也会更新，但受限于训练样本过少，所以难以得到一个良好的跟踪模型，在被跟踪物体的表观发生巨大变化时，就难以适应了。所以，就目前而言，跟踪算不上是计算机视觉内特别热门的一个研究方向，很多算法都改进自检测或识别算法。\n近年代表论文\nNam, Hyeonseob, and Bohyung Han. “Learning multi-domain convolutional neural networks for visual tracking.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nHeld, David, Sebastian Thrun, and Silvio Savarese. “Learning to track at 100 fps with deep regression networks.” European Conference on Computer Vision. Springer International Publishing, 2016.\nHenriques, João F., et al. “High-speed tracking with kernelized correlation filters.” IEEE Transactions on Pattern Analysis and Machine Intelligence 37.3 (2015): 583-596.\nMa, Chao, et al. “Hierarchical convolutional features for visual tracking.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\nBertinetto, Luca, et al. “Fully-convolutional siamese networks for object tracking.” European Conference on Computer Vision. Springer International Publishing, 2016.\nDanelljan, Martin, et al. “Beyond correlation filters: Learning continuous convolution operators for visual tracking.” European Conference on Computer Vision. Springer International Publishing, 2016.\nLi, Hanxi, Yi Li, and Fatih Porikli. “Deeptrack: Learning discriminative feature representations online for robust visual tracking.” IEEE Transactions on Image Processing 25.4 (2016): 1834-1848.\n数据集\nOTB(Object Tracking Benchmark)\nVOT(Visual Object Tracking)\n视觉问答\n视觉问答也简称VQA（Visual Question Answering），是近年来非常热门的一个方向，其研究目的旨在根据输入图像，由用户进行提问，而算法自动根据提问内容进行回答。除了问答以外，还有一种算法被称为标题生成算法（Caption Generation），即计算机根据图像自动生成一段描述该图像的文本，而不进行问答。对于这类跨越两种数据形态（如文本和图像）的算法，有时候也可以称之为多模态，或跨模态问题。\n近年代表论文\nXiong, Caiming, Stephen Merity, and Richard Socher. “Dynamic memory networks for visual and textual question answering.” arXiv 1603 (2016).\nWu, Qi, et al. “Ask me anything: Free-form visual question answering based on knowledge from external sources.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nZhu, Yuke, et al. “Visual7w: Grounded question answering in images.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n数据集\nVQA\n热点\n随着深度学习的大举侵入，现在几乎所有人工智能方向的研究论文几乎都被深度学习占领了，传统方法已经很难见到了。有时候在深度网络上改进一个非常小的地方，就可以发一篇还不错的论文。并且，随着深度学习的发展，很多领域的现有数据集内的记录都在不断刷新，已经向人类记录步步紧逼，有的方面甚至已经超越了人类的识别能力。那么，下一步的研究热点到底会在什么方向呢？就我个人的一些观点如下：\n多模态研究： 目前的许多领域还是仅仅停留在单一的模态上，如单一分物体检测，物体识别等，而众所周知的是现实世界就是有多模态数据构成的，语音，图像，文字等等。 VQA 在近年来兴起的趋势可见，未来几年内，多模态的研究方向还是比较有前景的，如语音和图像结合，图像和文字结合，文字和语音结合等等。\n数据生成： 现在机器学习领域的许多数据还是由现实世界拍摄的视频及图片经过人工标注后用作于训练或测试数据的，标注人员的职业素养和经验，以及多人标注下的规则统一难度在一定程度上也直接影响了模型的最终结果。而利用深度模型自动生成数据已经成为了一个新的研究热点方向，如何使用算法来自动生成数据相信在未来一段时间内都是不错的研究热点。\n无监督学习：人脑的在学习过程中有许多时间都是无监督（Un-supervised Learning）的，而现有的算法无论是检测也好识别也好，在训练上都是依赖于人工标注的有监督（Supervised Learning）。如何将机器学习从有监督学习转变向无监督学习，应该是一个比较有挑战性的研究方向，当然这里的无监督学习当然不是指简单的如聚类算法（Clustering）这样的无监督算法。而LeCun也曾说： 如果将人工智能比喻作一块蛋糕的话，有监督学习只能算是蛋糕上的糖霜，而增强学习（Reinforce Learning）则是蛋糕上的樱桃，无监督学习才是真正蛋糕的本体。\n最后，想要把握领域内最新的研究成果和动态，还需要多看论文，多写代码。\n计算机视觉领域内的三大顶级会议有：\nConference on Computer Vision and Pattern Recognition （CVPR）\nInternational Conference on Computer Vision （ICCV）\nEuropean Conference on Computer Vision （ECCV）\n较好的会议有以下几个：\nThe British Machine Vision Conference （BMVC）\nInternational Conference on Image Processing （ICIP）\nWinter Conference on Applications of Computer Vision （WACV）\nAsian Conference on Computer Vision (ACCV)\n当然，毕竟文章的发表需要历经审稿和出版的阶段，因此当会议论文集出版的时候很可能已经过了小半年了，如果想要了解最新的研究，建议每天都上ArXiv的cv板块看看，ArXiv上都是预出版的文章，并不一定最终会被各类会议和期刊接收，所以质量也就良莠不齐，对于没有分辨能力的入门新手而言，还是建议从顶会和顶级期刊上的经典论文入手。\n这是一篇对计算机视觉目前研究领域的几个热门方向的一个非常非常简单的介绍，希望能对想要入坑计算机视觉方向的同学有一定的帮助。由于个人水平十分有限，错误在所难免，欢迎大家对文中的错误进行批评和指正。\n小白入门计算机视觉：这是最全的一份CV技术学习之路\n2017年12月02日 00:00:00\n阅读数：4377\nAI\n菌\n最近AI菌决定把自己的机器学习之路向计算机视觉方面发展。所以今天就来给大家分享一下AI菌收集到的资料以及心得\nThe M Tank 编辑了一份报告《A Year in Computer Vision》，记录了 2016 至 2017 年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。该材料共包括四大部分\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率、风格迁移、着色\n动作识别\n第三部分\n3D 目标\n人体姿势估计\n3D 重建\n其他未分类 3D\n总结\n第四部分\n卷积架构\n数据集\n不可分类的其他材料与有趣趋势\n结论\n有兴趣的同学可以读一读，完整 PDF 地址：http://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf\n下面是收集到的学习资料与心得的汇总：\n（文中没有发的资源将在之后陆续放出）\n01 掌握好相应的基础能力\n计算机视觉的理念其实与很多概念有部分重叠，包括：人工智能、数字图像处理、机器学习、深度学习、模式识别、概率图模型、科学计算以及一系列的数学计算等。\n所以在入门CV之前，同学们最好对基础的学术课程都有对应的了解，比如数学方面的微积分，概率学，统计学，线性代数这几门基础课程。\n在编程语言方面，Matlab，Python，C++，最好熟悉其中2种，因为计算机视觉离开计算机编程是完全行不通的\n\n02 需要的专业工具\n工欲善其事，必先利其器。对于想要学好计算机视觉的同学来说，一个专业的工具，绝对是助攻的不二神器。\nOpenCV（开源计算机视觉库）是一个非常强大的学习资料库，包括了计算机视觉，模式识别，图像处理等许多基本算法。\n它免费提供给学术和商业用途，有C++，C，Python和java接口，支持Windows、Linux、Mac OS、iOS和Android。\n而关于OpenCV的学习，AI菌推荐（其中第三本目前无中文版）：\n学习OpenCV(Learning.OpenCV)\n链接：\nhttps://pan.baidu.com/s/1c2GrPEK 密码：7012\n毛星云老师编著的OpenCV3编程入门\n链接：\nhttps://pan.baidu.com/s/1c2xuVFq 密码：2s4a\n学习OpenCV3（\nLearning OpenCV 3\n）\n链接：\nhttps://pan.baidu.com/s/1geQeT0J 密码：cuco\n而深度学习方面，有TensorFlow，PyTorch，Caffe等深度学习框架，它们也内置了OpenCV的API接口。而哪种框架好，就要看你自己的需要了\n推荐资料：\n莫凡教程系列之PyTorch :https://morvanzhou.github.io/tutorials/machine-learning/torch/\nTensorFlow中文社区：\nhttp://www.tensorfly.cn/\n深度学习 21天实战Caffe\n\n03 绕不开的数字图像处理与模式识别\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。\n入门的同学推荐\n冈萨雷斯的《数字图像处理》《数字图像处理(第3版)(英文版)》和对应的Matlab版本\n一本讲基础的理论，一本讲怎么用Matlab实现。\n除此之外同学们还可以去YouTube上找到相关的课程信息，相信大家会有所收获的。\n模式识别（Pattern Recognition），就是通过计算机用数学技术方法来研究模式的自动处理和判读。我们把环境与客体统称为“模式”。\n计算机视觉很多东西都是基于图像识别的，图像识别就是模式识别的一种。\n模式识别通常是训练一个模型来拟合当前的数据，当我们拿到一堆数据或图片，需要从当中找到它们的关系，最便捷的便是用模式识别算法来训练一个模型。\nAI菌推荐一本模式识别入门级的教材《模式分类》，相对于《模式识别》这本书来说可能比较难，但书中介绍了很多模式识别经典的分类器，还是很值得一读。\n其中的一些思想在神经网络中也可以应用的\n\n04 系统的学习下计算机视觉课程\n对于CV新手来说，想要从小白到大神，最快的方法就是先系统的学习一下计算机视觉的课程，全面了解一下计算机视觉这个领域的背景及其发展、这个领域有哪些基本的问题、哪些问题的研究已经比较成熟了，哪些问题的研究还处于基础阶段。\n在这里AI菌推荐3本经典教材：\n《计算机视觉：一种现代方法》（Computer Vision: A Modern Approach）\n《计算机视觉_算法与应用》\n（Computer Vision: Algorithms and Applications）\n《计算机视觉：模型 学习和推理》\n（Computer Vision: Models, Learning, and Inference）\n这三本教材AI菌认为是计算机视觉最好的入门教材了，内容丰富，难度适中，其中第二本书涉及大量的文献，很适合对计算机视觉没什么概念的同学。\n虽然其中的一些方法在现在看来已经过时了，但还是值得一读\n05 深度学习与CNN\n关于深度学习这几年讲的已经太多了，资料也非常多，AI菌在这里就不在赘述啦\n计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。\n什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。\n同学们可以试着学习下CNN在计算机视觉当中的应用\n推荐的资料：\n斯坦福CS231n—深度学习与计算机视觉网易云课堂课程：http://study.163.com/course/introduction.htm?courseId=1003223001\n斯坦福CS231n—深度学习与计算机视觉官方课程：http://cs231n.stanford.edu/\nCS231n官方笔记授权翻译总集篇：https://www.52ml.net/17723.html\n吴恩达 deeplearning.ai与网易云课堂的微专业深度学习工程师卷积神经网络\nhttp://mooc.study.163.com/course/2001281004?tid=2001392030#/info\n神经网络方面的经典教材\n《深度学习》\n（Deep Learning）\n《神经⽹络与深度学习》\n（Neural Networks and Deep Learning(Nielsen,2017)）\n\n06 了解最新领域动态\n很多同学做研究的时候，容易陷入自我封闭的“怪圈”，过于执着于埋头学习相关知识，有时候会忘记及时了解相关领域的最新动态，这是非常不科学的。\n同学们在学习计算机视觉相关知识的时候，可以通过最新的paper来了解这个领域最新提出的一些概念以及发展的情况。\n计算机视觉的期刊有两个PAMI（模式分析与机器智能汇刊）和IJCV（计算机视觉国际期刊）\n顶级的学术会议有 CVPR、ICCV、 ECCV、 BMVC这四个，同学们可以跟着浏览这些期刊论文以及会议文章，相信一定可以学到不少有用的知识。\n\nAI\n菌\n听做视觉的师兄师姐硕：做好计算机视觉研究并不是一件容易的事情，在大多数情况下它甚至是一件很枯燥的事情。\n研究成果毫无进展，研究方向不在明朗等等，这一切都会给你前所未有的压力\n所以希望同学们在决定入这一行的时候，是出于自己的热爱，而不是出于当前的趋势。\n因为热爱不会变，但趋势每一年都在变。\n计算机视觉是人工智能技术的一个重要领域，打个比方（不一定恰当），我认为计算机视觉是人工智能时代的眼睛，可见其重要程度。计算机视觉其实是一个很宏大的概念，下图是有人总结的计算机视觉所需要的技能树。\n如果你是一个对计算机视觉一无所知的小白，千万不要被这棵技能树吓到。没有哪个人能够同时掌握以上所有的技能，这棵树只是让你对计算机视觉有个粗浅的认识。\n以下是我站在一个小白的视角给出一个入门计算机视觉的相对轻松的姿势。\n一、宏观认识\n小白通常看到这么多的细分方向大脑一片茫然，到底是学习人脸识别、物体跟踪，又或者是计算摄影，三维重建呢？不知道该怎么下手。其实这些细分方向有很多共通的知识，我的建议是心急吃不了热豆腐，只有对计算机视觉这个领域有了一个初步的全面了解，你才能够结合实际问题找到自己感兴趣的研究方向，而兴趣能够支持一个自学的小白克服困难持续走下去。\n1、入门书籍\n既然说是入门，这里就不推荐类似《 Multiple View Geometry in Computer Vision》这种虽然经典但是小白看了容易放弃的书了。\n像素级的图像处理知识是计算机视觉的底层基础知识。不管你以后从事计算机视觉的哪个细分领域，这些基础知识都是必须要了解的。即使一个急切入门的小白，这一关也必须走的踏实。看到网上有人说直接从某个项目开始，边做边学，这样学的快。对此我表示部分赞成，原因是他忽略了基础知识的重要性，脑子里没有基本的术语概念知识打底，很多问题他根本不知道如何恰当的表达，遇到问题也没有思路，不知道如何搜索，这会严重拖慢进度，也无法做较深入的研究，欲速则不达。\n入门图像处理的基础知识也不是直接去啃死书，否则几个公式和术语可能就会把小白打翻在地。这里推荐两条途径，都是从实践出发并与理论结合：一个是OpenCV，一个是MATLAB。\nOpenCV以C++为基础，需要具备一定的编程基础，可移植性强，运行速度比较快，比较适合实际的工程项目，在公司里用的较多；MATLAB只需要非常简单的编程基础就可以很快上手，实现方便，代码比较简洁，可参考的资料非常丰富，方便快速尝试某个算法效果，适合做学术研究。当然两者搭配起来用更好啦。下面分别介绍一下。\n用MATLAB学习图像处理\n推荐使用冈萨雷斯的《数字图像处理（MATLAB版）》（英文原版2001年出版，中译版2005年）。不需要一上来就全部过一遍，只需要结合MATLAB学习一下基本原理、图像变换、形态学处理、图像分割，以上章节强烈建议按照书上手动敲一遍代码（和看一遍的效果完全不同），其他章节可快速扫描一遍即可。但这本书比较注重实践，对理论的解释不多，理论部分不明白的可以在配套的冈萨雷斯的《数字图像处理（第二版）》这本书里查找，这本书主要是作为工具书使用，以后遇到相关术语知道去哪里查就好。\n用OpenCV学习图像处理\nOpenCV（Open Source Computer Vision Library）是一个开源跨平台计算机视觉程序库，主要有C++预研编写，包含了500多个用于图像/视频处理和计算机视觉的通用算法。\n学习OpenCV参考《学习OpenCV》或者《OpenCV 2 计算机视觉编程手册》都可以。这两本都是偏实践的书，理论知识较少，按照书上的步骤敲代码，可以快速了解到OpenCV的强大，想要实现某个功能，只要学会查函数（在https://www.docs.opencv.org/查询对应版本），调函数就可以轻松搞定。由于每个例子都有非常直观的可视化图像输出，所以学起来比较轻松有趣。\n2、进阶书籍\n经过前面对图像处理的基本学习，小白已经了解了图像处理的基础知识，并且会使用OpenCV或MATLAB来实现某个简单的功能。但是这些知识太单薄了，并且比较陈旧，计算机视觉领域还有大量的新知识在等你。\n同样给你两种选择，当然两个都选更佳。一本书是2010年出版的美国华盛顿大学Richard Szeliski写的《Computer Vision: Algorithms and Application》；一本是2012年出版的，加拿大多伦多大学Simon J.D. Prince写的《Computer Vision: Models, Learning, and Inference》。两本书侧重点不同，前者侧重视觉和几何知识，后者侧重机器学习模型。当然两本书也有互相交叉的部分。虽然都有中文版，但是如果有一定的英语阅读基础，推荐看英文原版（见文末获取方式）。老外写的书，图和示例还是挺丰富的，比较利于 理解。\n《Computer Vision: Algorithms and Application》\n这本书图文并茂地介绍了计算机视觉这门学科的诸多大方向，有了前面《数字图像处理》的基础，这本书里有些内容你已经熟悉了，没有那么强的畏惧感。相对前面的图像处理基础本书增加了许多新的内容，比如特征检测匹配、运动恢复结构、稠密运动估计、图像拼接、计算摄影、立体匹配、三维重建等，这些都是目前比较火非常实用的方向。如果有时间可以全书浏览，如果时间不够，你可以根据兴趣，选择性的看一些感兴趣的方向。这本书的中文版翻译的不太好，可以结合英文原版看。\n《Computer Vision: Models, Learning, and Inference》\n该书从基础的概率模型讲起，涵盖了计算机视觉领域常用的概率模型、回归分类模型、图模型、优化方法等，以及偏底层的图像处理、多视角几何知识，图文并茂，并辅以非常多的例子和应用，非常适合入门。在其主页：\nhttp://www.computervisionmodels.com/\n上可以免费下载电子书。此外还有非常丰富的学习资源，包括给教师用的PPT、每章节对应的开源项目、代码、数据集链接等，非常有用。\n二、深入实践\n当你对计算机视觉领域有了比较宏观的了解，下一步就是选一个感兴趣的具体的领域去深耕。这个时期就是具体编程实践环节啦，实践过程中有疑问，根据相关术语去书里查找，结合Google，基本能够解决你大部分问题。\n那么具体选择什么方向呢？\n如果你实验室或者公司有实际的项目，最好选择当前项目方向深耕下去。如果没有具体方向，那么继续往下看。\n我个人认为计算机视觉可以分为两大方向：基于学习的方法和基于几何的方法。其中基于学习的方法最火的就是深度学习，而基于几何方法最火的就是视觉SLAM。下面就这两个方向给出一个相对轻松的入门姿势。\n1、深度学习\n深度学习（Deep Learning）的概念是Hinton等人于2006年提出的，最早最成功的应用领域就是计算机视觉，经典的卷积神经网络就是为专门处理图片数据而生。目前深度学习已经广泛应用在计算机视觉、语音识别、自然语言处理、智能推荐等领域。\n学习深度学习需要一定的数学基础，包括微积分、线性代数，很多小白一听到这些课程就想起了大学时的噩梦，其实只用了非常基础的概念，完全不用担心。不过如果一上来就啃书本，可能会有强烈的畏难情绪，很容易早早的放弃。\nAndrew Ng (吴恩达)的深度学习视频课程我觉得是一个非常好的入门资料。首先他本人就是斯坦福大学的教授，所以很了解学生，可以很清晰形象、深入浅出的从最基本的导数开始讲起，真的非常难得。\n该课程可以在网易云课程上免费观看，有中文字幕，但没有配套习题。也可以在吴恩达自己创办的在线教育平台Coursera上学习，有配套习题，限时免费，结业通过后有相应证书。\n该课程非常火爆，不用担心听不懂，网上有数不清的学习笔记可以参考。简直小白入门必备佳肴。\n2、视觉SLAM\nSLAM（Simultaneous Localization and Mapping）（详见《SLAM初识》），中文译作同时定位与地图创建。视觉SLAM就是用摄像头作为主传感器，用拍摄的视频流作为输入来实现SLAM。视觉SLAM广泛应用于VR/AR、自动驾驶、智能机器人、无人机等前沿领域。\n视觉SLAM最好的入门资料是高翔（清华博士，慕尼黑理工博后）的《视觉SLAM十四讲-从理论到实践》。该书每章节都涵盖了基础理论和代码示例，深入浅出，非常注重理论与实践结合，大大降低了小白的学习门槛。\n好了，入门介绍到此为止，你可以开始你的计算机视觉学习之旅了！\n温馨提示：本文提到的部分书籍资料，公众号：“计算机视觉life” 已经为你准备好了，公众号下方回复“入门”即可获取。\n以下内容整理自 2017 年 6 月 29 日由“趣直播–知识直播平台”邀请的嘉宾实录。\n分享嘉宾: 罗韵\n目前，人工智能，机器学习，深度学习，计算机视觉等已经成为新时代的风向标。这篇文章主要介绍了下面几点：\n第一点，如果说你要入门计算机视觉，需要了解哪一些基础知识？\n第二点，既然你要往这方面学习，你要了解的参考书籍，可以学习的一些公开课有哪些？\n第三点，可能是大家都比较感兴趣的，就是计算机视觉作为人工智能的一个分支，它不可避免的要跟深度学习做结合，而深度学习也可以说是融合到了计算机视觉、图像处理，包括我们说的自然语言处理，所以本文也会简单介绍一下计算机视觉与深度学习的结合。\n第四点，身处计算机领域，我们不可避免的会去做开源的工作，所以本文会给大家介绍一些开源的软件。\n第五点，要学习或者研究计算机视觉，肯定是需要去阅读一些文献的，那么我们如何开始阅读文献，以及慢慢的找到自己在这个领域的方向，这些都会在本文理进行简单的介绍。\n1.基础知识\n接下来要介绍的，第一点是计算机视觉是什么意思，其次是图像、视频的一些基础知识。包括摄像机的硬件，以及 CPU 和 GPU 的运算。\n在计算机视觉里面，我们也不可避免的会涉及到考虑去使用 CPU 还是使用 GPU 去做运算。然后就是它跟其他学科的交叉，因为计算机视觉可以和很多的学科做交叉，而且在做学科交叉的时候，能够发挥的意义和使用价值也会更大。另外，对于以前并不是做人工智能的朋友，可能是做软件开发的，想去转型做计算机视觉，该如何转型？需要学习哪些编程语言以及数学基础？这些都会在第一小节给大家介绍。\n1.0 什么是计算机视觉\n计算机视觉是一门研究如何使机器“看”的科学。\n更进一步的说，就是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给一起检测的图像\n作为一个科学学科，计算机视觉研究相关的理论和技术，视图建立能够从图像或者多维数据中获取“信息”的人工智能系统。\n目前，非常火的VR、AR，3D处理等方向，都是计算机视觉的一部分。\n计算机视觉的应用\n无人驾驶\n无人安防\n人脸识别\n车辆车牌识别\n以图搜图\nVR/AR\n3D重构\n医学图像分析\n无人机\n其他\n了解了计算机视觉是什么之后，给大家列了一下当前计算机视觉领域的一些应用，几乎可以说是无处不在，而且当前最火的所有创业的方向都涵盖在里面了。其中包括我们经常提到的无人驾驶、无人安防、人脸识别。人脸识别相对来说已经是一个最成熟的应用领域了，然后还有文字识别、车辆车牌识别，还有以图搜图、 VR/AR，还包括 3D 重构，以及当下很有前景的领域–医学图像分析。\n医学图像分析他在很早就被提出来了，已经研究了很久，但是现在得到了一个重新的发展，更多的研究人员包括无论是做图像的研究人员，还是本身就在医疗领域的研究人员，都越来越关注计算机视觉、人工智能跟医学图像的分析。而且在当下，医学图像分析也孕育了不少的创业公司，这个方向的未来前景还是很值得期待的。然后除此之外还包括无人机，无人驾驶等，都应用到了计算机视觉的技术。\n1.1图像和视频，你要知道的概念\n图像\n一张图片包含了：维数、高度、宽度、深度、通道数、颜色格式、数据首地址、结束地址、数据量等等。\n图像深度：存储每个像素所用的位数（bits）\n当一个像素占用的位数越多时，它所能表现的颜色就更多，更丰富。\n举例：一张400*400的8位图，这张图的原始数据量是多少？像素值如果是整型的话，取值范围是多少？\n1，原始数据量计算：400 * 400 * ( 8/8 )=160,000Bytes\n(约为160K)\n2，取值范围：2的8次方，0~255\n图片格式与压缩：常见的图片格式JPEG，PNG，BMP等本质上都是图片的一种压缩编码方式\n举例：JPEG压缩\n1，将原始图像分为8*8的小块，每个block里有64pixels。\n2，将图像中每个8*8的block进行DCT变换（越是复杂的图像，越不容易被压缩）\n3，不同的图像被分割后，每个小块的复杂度不一样，所以最终的压缩结果也不一样\n视频\n原始视频=图片序列。\n视频中的每张有序图片称为“帧（frame）”。压缩后的视频，会采取各种算法减少数据的容量，其中IPB就是最常见的。\nI帧：表示关键帧，可以理解为这一幅画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）\nP帧：表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧画面差别的数据）\nB帧表示双向差别帧，记录的本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，要通过前后画面与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码比较麻烦。\n码率：码率越大，体积越大；码率越小，体积越小。\n码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。也就是取样率（并不等同于采样率，采样率用的单位是Hz，表示每秒采样的次数），单位时间内取样率越大，精度就越高，处理出来的文件就越接近原始文件，但是文件体积与取样率是成正比的，所以几乎所有的编码格式重视的都是如何用最低的码率达到最少的失真，围绕这个核心衍生出来cbr（固定码率）与vbr（可变码率），码率越高越清晰，反之则画面粗糙而且多马赛克。\n帧率\n影响画面流畅度，与画面流畅度成正比：帧率越大，画面越流畅；帧率越小，画面越有跳动感。如果码率为变量，则帧率也会影响体积，帧率越高，每秒钟经过的画面就越多，需要的码率也越高，体积也越大。\n帧率就是在一秒钟时间里传输的图片的帧数，也可以理解为图形处理器每秒钟刷新的次数。\n分辨率\n影响图像大小，与图像大小成正比；分辨率越高，图像越大；分辨率越低，图像越小。\n清晰度\n在码率一定的情况下，分辨率与清晰度成反比关系：分辨率越高，图像越不清晰，分辨率越低，图像越清晰\n在分辨率一定的情况下，码率与清晰度成正比关系：码率越高，图像越清晰；码率越低，图像越不清晰\n带宽、帧率\n例如在ADSL线路上传输图像，上行带宽只有512Kbps，但要传输4路CIF分辨率的图像。按照常规，CIF分辨率建议码率是512Kbps，那么照此计算就只能传一路，降低码率势必会影响图像质量。那么为了确保图像质量，就必须降低帧率，这样一来，即便降低码率也不会影响图像质量，但在图像的连贯性上会有影响。\n1.2摄像机\n摄像机的分类：\n监控摄像机（网络摄像机和摸你摄像机）\n不同行业需求的摄像机（超宽动态摄像机、红外摄像机、热成像摄像机等）\n智能摄像机\n工业摄像机\n当前的摄像机硬件我们可以分为监控摄像机、专业行业应用的摄像机、智能摄像机和工业摄像机。而在监控摄像机里面，当前用的比较多的两个类型一个叫做网络摄像机，一个叫做模拟摄相机，他们主要是成像的原理不太一样。\n网络摄像机一般比传统模拟摄相机的清晰度要高一些，模拟摄像机当前应该说是慢慢处于一个淘汰的状态，它可以理解为是上一代的监控摄像机，而网络摄像机是当前的一个主流的摄相机，大概在 13 年的时候，可能市场上 70% 到 80% 多都是模拟摄像机，而现在可能 60% 到 70% 都是的网络摄像机。\n除此之外，不同的行业其时会有特定的相机，想超宽动态摄像机以及红外摄像机、热成像摄像机，都是在专用的特定的领域里面可能用到的，而且他获得的画面跟图像是完全不一样的。如果我们要做图像处理跟计算机视觉分析，什么样的相机对你更有利，我们要学会利用硬件的优势。\n如果是做研究的话一般是可以控制我们用什么样的摄相机，但如果是在实际的应用场景，这个把控的可能性会稍微小一点，但是在这里你要知道，有些问题可能你换一种硬件，它就能够很好的被解决，这是一个思路。\n还有些问题你可能用算法弄了很久也没能解决，甚至是你的效率非常差，成本非常高，但是稍稍换一换硬件，你会发现原来的问题都不存在了，都被很好的解决了，这个就是硬件对你的一个新的处境了。\n包括现在还有智能摄像机、工业摄像机，工业摄像机一般的价格也会比较贵，因为他专用于各种工业领域，或者是做一些精密仪器，高精度高清晰度要求的摄像机。\n1.3 CPU和GPU\n接下来给大家讲一下 CPU 跟 GPU，如果说你要做计算机视觉跟图像处理，那么肯定跳不过 GPU 运算，GPU 运算这一块可能也是接下来需要学习或者自学的一个知识点。\n因为可以看到，当前大部分关于计算机视觉的论文，很多实现起来都是用 GPU 去实现的，但是在应用领域，因为 GPU 的价格比较昂贵，所以 CPU 的应用场景相对来说还是占大部分。\n而 CPU 跟 GPU 的差别主要在哪里呢？ 它们的差别主要可以在两个方面去对比，第一个叫性能，第二个叫做吞吐量。\n性能，换言之，性能会换成另外一个单词叫做 Latency（低延时性）。低延时性就是当你的性能越好，你处理分析的效率越高，相当于你的延时性就越低，这个是性能。另外一个叫做吞吐量，吞吐量的意思就是你同时能够处理的数据量。\n而 CPU 跟 GPU 的差别在哪里呢？主要就在于这两个地方，CPU 它是一个高性能，就是超低延时性的，他能够快速的去做复杂运算，并且能达到一个很好的性能要求。而 GPU是以一个叫做运算单元为格式的，所以他的优点不在于低延时性，因为他确实不善于做复杂运算，他每一个处理器都非常的小，相对来说会很弱，但是它可以让它所有的弱处理器，同时去做处理，那相当于他就能够同时处理大量的数据，那这个就意味着它的吞吐量非常大，所以 CPU重视的是性能，GPU重视的是吞吐量。\n所以大部分时候，GPU 他会跟另外一个词语联系在一起，叫做并行计算，意思就是它可以同时做大量的线程运算，为什么图像会特别适合用 GPU 运算呢？这是因为 GPU 它最开始的设计就是叫做图形处理单元，它的意思就是我可以把每一个像素，分割为一个线程去运算，每一个像素只做一些简单的运算，这个就是最开始图形处理器出现的原理。\n它要做图形渲染的时候，要计算的是每一个像素的变换。所以每一个像素变换的计算量是很小很小的，可能就是一个公式的计算，计算量很少，它可以放在一个简单的计算单元里面去做计算，那这个就是 CPU 跟 GPU 的差别。\n基于这样的差别，我们才会去设计什么时候用 CPU，什么时候用 GPU。如果你当前设计的算法，它的并行能力不是很强，从头到尾从上到下都是一个复杂的计算，没有太多可并性的地方，那么即使你用了 GPU，也不能帮助你很好提升计算性能。\n所以，不要说别人都在用 GPU 那你就用 GPU，我们要了解的是为什么要用 GPU ，以及什么样的情况下用 GPU，它效果能够发挥出来最好。\n1.4计算机视觉与其他学科的关系\n计算机视觉目前跟其他学科的关系非常的多，包括机器人，以及刚才提到的医疗、物理、图像、卫星图片的处理，这些都会经常使用到计算机视觉，那这里呢，最常问到的问题无非就是有三个概念，一个叫做计算机视觉，一个叫做机器视觉，一个叫做图像处理，那这三个东西有什么区别呢？\n这三个东西的区别还是挺因人而异的，每一个研究人员对它的理解都不一样。\n首先，Image Processing更多的是图形图像的一些处理，图像像素级别的一些处理，包括 3D 的处理，更多的会理解为是一个图像的处理；而机器视觉呢，更多的是它还结合到了硬件层面的处理，就是软硬件结合的图形计算的能力，跟图形智能化的能力，我们一般会理解为他就是所谓的机器视觉。\n而我们今天所说的计算机视觉，更多的是偏向于软件层面的计算机处理，而且不是说做图像的识别这么简单，更多的还包括了对图像的理解，甚至是对图像的一些变换处理，当前我们涉及到的一些图像的生成，也是可以归类到这个计算机视觉领域里面的。\n所以说计算机视觉它本身的也是一个很基础的学科，可以跟各个学科做交叉，同时，它自己内部也会分的比较细，包括机器视觉、图像处理。\n1.5 编程语言AND数学基础\n这一部分的内容可以参见《非计算机专业，如何学习计算机视觉》\n2.参考书籍和公开课\n参考书\n第一本叫《Computer Vision：Models, Learning and Inference》written by Simon J.D. prince，这个主要讲的更适合入门级别的，因为这本书里面配套了非常多的代码，Matlab 代码，C 的代码都有，配套了非常多的学习代码，以及参考资料、文献，都配得非常详细，所以它很适合入门级别的同学去看。\n第二本《Computer Vision：Algorithms and Applications》written by Richard Szeliski，这是一本非常经典，非常权威的参考资料，这本书不是用来看的，是用来查的，类似于一本工具书，它是涵盖面最广的一本参考书籍，所以一般会可以当成工具书去看，去查阅。\n第三本《OpenCV3编程入门》作者：毛星云，冷雪飞 ，如果想快速的上手去实现一些项目，可以看看这本书，它可以教你动手实现一些例子，并且学习到 OpenCV 最经典、最广泛的计算机视觉开源库。\n公开课：\nStanford CS223B\n比较适合基础，适合刚刚入门的同学，跟深度学习的结合相对来说会少一点，不会整门课讲深度学习，而是主要讲计算机视觉，方方面面都会讲到。\nStanford CS231N\n这个应该不用介绍了，一般很多人都知道，这个是计算机视觉和深度学习结合的一门课，我们上 YouTube 就能够看到，这门课的授课老师就是李飞飞老师，如果说不知道的话可以查一下，做计算机视觉的话，此人算是业界和学术界的“执牛耳”了。\n3.需要了解的深度学习知识\n深度学习没有太多的要讲的，不是说内容不多，是非常多，这里只推荐一本书给大家，这本书是去年年底才出的，是最新的一本深度学习的书，它讲得非常全面，从基础的数学，到刚才说的概率学、统计学、机器学习以及微积分、线性几何的知识点，非常的全面。\n4.需要了解和学习的开源软件\nOpenCV\n它是一个很经典的计算机视觉库，实现了很多计算机视觉的常用算法。可以帮助大家快速上手。\nCaffe\n如果是做计算机视觉的话，比较建议 Caffe。Caffe 更擅长做的是卷积神经网络，卷积神经网络在计算机视觉里面用的是最多的。\n所以无论你后面学什么样其它的开源软件， Caffe 是必不可免的，因为学完 Caffe 之后你会发现，如果你理解了 Caffe，会用 Caffe，甚至是有能力去改它的源代码，你就会发现你对深度学习有了一个质的飞跃的理解。\nTensorFlow\nTensorFlow 最近很火，但是它的入门门槛不低，你要学会使用它需要的时间远比其他所有的软件都要多，其次就是它当前还不是特别的成熟稳定，所以版本之间的更新迭代非常的多，兼容性并不好，运行效率还有非常大的提升空间。\n5.如何阅读相关的文献\n先熟悉所在方向的发展历程，然后精读历程中的里程碑式的文献。\n例如：深度学习做目标检测，RCNN，Fast RCNN，Faster RCNN，SPPNET，SSD和YOLO这些模型肯定是要知道的。又例如，深度学习做目标跟踪，DLT，SO-DLT等。\n计算机视觉的顶会：\nICCV：International Conference on Computer Vision，国际计算机视觉大会\nCVPR：International Conference on Computer Vision and Pattern Recognition，国际计算机视觉与模式识别大会\nECCV：European Conference on Computer Vision，欧洲计算机视觉大会\n除了顶会之外呢，还有顶刊。像 PAMI、IJCV，这些都是顶刊，它代表着这个领域里面最尖端最前沿以及当下的研究方向。"}
{"content2":"随着计算机视觉算法和现代人工智能（AI）技术应用的发展，编写可视化测试已经成为现实。由于在测试中加入AI，自动测试也变为可能。我们可以将无聊、重复的工作交给AI，这样测试人员就可以用更多时间做其他测试方面的思考。\n\\\\\n布道师、Applitools的高级架构师Gil Tayar在Craft Conference 2018上和我们介绍了如何将AI技术应用到自动化测试的内容。InfoQ以采访的形式报道了这次大会。\n\\\\\nInfoQ采访了Tayar，咨询了他有关现如今测试面临的主要挑战，自动化测试分为哪六个层次，软件产业在这六个层次中取得了什么样的进展，如何实现自动可视化测试，我们在测试中如何应用机器学习技术，以及AI给测试人员的工作会带来什么影响。\n\\\\\nInfoQ：你认为现如今测试面临的主要挑战是什么？\n\\\\\n\\\nGil Tayar：很遗憾的是，测试还没有成为主流。我还记得在80年代、90年代的时候，整个行业都没有把QA这个职业和手动测试软件当一回事。不只是说没有自动化测试，而是根本没有测试！好在现在时代已经不一样了，在交付产品之前测试人员都会定期测试软件，而且很多人开始使用自动化测试。\n\\\\\n但在如今的“互联网时代”，这还不够。由于使用了敏捷方法，我们需要部署得越来越快，这其实是一件好事。因为越来越多开发人员会自己测试自己的软件，而不是慢悠悠地等待专门的测试人员来测试他们的软件。\n\\\\\n但是，开发人员测试自己的软件还没有成为主流。大多数开发人员不会自己写测试来检查代码，他们选择手动测试，或者等待测试人员进行测试，保证他们的软件正常运作。\n\\\\\n这正是测试面临的主要挑战，怎么让开发人员编写自己的测试。而不幸的是，AI在这方面起不到什么作用，AI只是测试时使用的工具，但如果你自己不测试，AI就没用了。事实就是这样，AI从现在到未来都只能作为测试的辅助，如果你自己不想测试，AI不可能自动帮你完成测试。\n\\\\\n开发人员必须要有测试的想法，这就是我们在测试方面面临的主要挑战。\n\\\n\\\\\nInfoQ：你在谈话中介绍了自动化测试的六个层次。它们分别是什么？\n\\\\\n\\\nTayar：这个概念来源于自动驾驶的五个层次。它们描述了AI如何帮助我们进行测试。\n\\\\\n完全没有自动，你需要自己写测试！\\\\t\n驾驶辅助：AI可以查看到页面，帮助你写出断言。你还是要自己写“驱动”应用程序的代码，但是AI可以检查页面，并确保页面中的期望值是正确的。\\\\t\n部分自动化：虽然能分辨实际页面和期望值的区别这一点已经很好了，但是第二层次的AI需要有更深层的理解。比如说，如果所有页面都有相同的变更，AI需要认识到这是相同的页面，并向我们展示出这些变更。进一步来说，AI需要查看页面的布局和内容，将每个变更分类为内容变更或是布局变更。如果我们要测试响应式web网站，这会非常有帮助，即使布局有细微变更，内容也应该是相同的。这是Applitools Eyes这样的工具所处的层次。\\\\t\n条件自动化：在第二层，软件中检测的问题和变更仍然需要人来审查。第二层的AI可以帮助我们分析变更，但不能仅仅通过查看页面判断页面是否正确，需要和期望值进行对比才能判断。但是第三层的AI可以做到这一方面，甚至更多其他方面，因为它会使用到机器学习的技术。比如说，第三层的AI可以从可视化角度查看页面，根据标准设计规则，例如对齐、空格、颜色和字体使用以及布局规则，判断设计是否过关。AI也能查看页面的内容，基于相同页面之前的视图，在没有人工干预的情况下，判断内容是否合理。我们还不能达到这一层次，但我们正在向这个方面努力。\\\\t\n高度自动化：直到现在，所有AI都只是在自动化地进行检查。人类尽管使用自动化软件，还是需要手动启动测试，需要点击链接，而第四层的AI可以自动启动测试本身。AI将通过观察启动应用程序的真实用户的行为，理解如何自己启动测试。这层的AI可以编写测试，可以通过检查点来测试页面。但这不是终点，它还需观察人的行为，偶尔需要听从测试人员的指令。\\\\t\n完全自动化：我必须承认，这个层次有点恐怖。这个层次的AI可以和产品经理“交流”，理解产品的标准，自己写测试，不需要人的帮助。\\\n\\\\\nInfoQ：软件产业在这六个层次中取得了什么样的进展？\n\\\\\n\\\nTayar：大多数公司肯定在第二层，目标向第三层努力。我相信要达到第四层需要一段时间，但我们终有一天是可以达到的。至于是否能达到第五层，我非常怀疑，但未来不可估量！（原文中引用了古犹太人毁坏第二圣殿的典故，比喻未来不可估量。）\n\\\n\\\\\nInfoQ：如何实现自动可视化测试？\n\\\\\n\\\nTayar：其实在几年前，即使是开发人员都不能自动进行可视化测试，更不要说使用AI了！由于基于像素的算法误报率过高，截图应用程序，根据预期进行检查是不可能的。\n\\\\\n但是随着计算机视觉算法和现代人工智能（AI）技术的应用的发展，编写可视化测试已经成为现实。人们发现测试中缺少可视化测试。由于可视化测试的存在，现在我们可以测试到软件的方方面面。\n\\\\\n那我们如何使用AI技术避免误报，AI是否能比单纯比较像素的算法做得更好？答案不仅仅牵涉到一个技术，需要将不同算法结合在一起，一个算法解决一个准确度的问题，通过决策树结合不同算法的结果（有时候将一个算法的结果应用到另一个算法中）来确定最后的结果。但要使用什么算法呢？分割算法就是一个例子，这个算法试图确定图片的文字和图像部分。这不是一个简单的问题，因为我们需要弄清，表情符号其实是文字，而图像中的文本是图像的一部分，并不是新的文字。这就是深度学习的魅力所在，由于我们代码中使用了深度学习技术，我们的准确率从88%提高到了96%。\n\\\\\n所以要做到自动视觉化测试，你需要使用现代的数据化测试工具，比如Applitools Eyes。这些工具有一些简单的高级AI技术，因此你不需要理解并实现。看看这个例子：\n\\\\\n\\\\\n在这个例子中，我们验证了一个博客应用程序。在常规的、非视觉化、非AI的测试里面，我们需要验证主页和特定的博文，对于页面中的每个元素进行几十次检查（即使这样我们还是会遗漏一些）。但是在这个例子里面，我们只是“checkWindow”，截图上传到服务器，服务器使用AI技术检验截图。\n\\\\\n我一直觉得，测试是为开发过程服务的，它避免了在代码中添加新功能的烦恼，更重要的是，避免了代码重构的烦恼。但为什么这只能运用在业务逻辑上？为什么我们的可视化代码（在CSS、HTML和JS文件中）不能测试，我们还要再为它们担忧？AI可以帮助我们实现这一目标。\n\\\n\\\\\nAppraise是另外一款可以用于视觉化测试的工具。早先，InfoQ采访了Gojko Adzic有关使用Appraise进行自动视觉化测试的内容：\n\\\\\n\\\nAdzic：Appraise采用实例化需求的方式，但将其运用于视觉化内容。我们采用具体的例子，让其从自动化层面创建可执行的实例化需求，用headless Chrome截屏，与预期的结果进行对比。和普通的实例化需求工具（期望结果和实际结果不同就认为是错误的）不一样，Appraise采用确证测试方法。它将所有的不同展现给人来评估，让人决定差别是否正确，如果是的话就进行确认。\n\\\n\\\\\nInfoQ：我们在测试中如何运用机器学习？\n\\\\\n\\\nTayar：我最近看到了一个讲座，讲的是如何使用机器学习测试Candy Crush游戏。由于Candy Crush是高随机的游戏，任何情况都可能发生，标准的测试技术很难实现。King的目标是想要了解是否这局游戏可以成功完成，要完成有多复杂。他们一开始使用蛮力法尝试所有路径，计算其中有多少可以成功。这个方法确实有用，但需要花好几个小时，因为组合可能太多了（使用去除无效路径组合的算法）。要解决这个问题，他们使用了Genetic Algorithms，使用一些最基本的算法（如只使用粉色糖果，一直消除障碍等），在一局游戏中进行试验。之后他们使用最佳算法（基于如何胜利完成一局游戏的算法），再结合两者，“创建”一个新的算法。不断重复，最后得到一个最佳算法。\n\\\\\n再举一个例子，卷积神经网络帮助我们分析图像，并从中提取语义信息。在生成图像的应用程序中是非常实用的。可视化测试工具使用它来理解页面的不同部分，以及相互之间的联系，正如我上文所述一样提取不同的文字和图像块。\n\\\n\\\\\nInfoQ：AI会对测试人员的工作产生什么影响？\n\\\\\n\\\nTayar：请问拖拉机会给农民的工作带来什么影响？当然是巨大的影响！农民不再需要做枯燥、乏味的体力劳动工作，只要交给机器来做就可以了。我相信AI也会给测试人员带来相同的影响。乏味、重复的工作交给AI，测试人员就可以用更多时间来思考。\n\\\\\n大多数测试人员已经习惯思考测试相关的方方面面：思考产品，思考怎么测试，思考边界情况等等，他们也需要工具来完成无聊、重复的工作。一些测试人员需要学会怎么思考更高层次的问题，确实会有一部分测试人员无法适应，但我相信他们只是少数。\n\\\n\\\\\n查看英文原文：Testing Software with Artificial Intelligence"}
{"content2":"人工智能的时代，深度学习这个热点是每个程序员必须了解的内容。近年来深度学习研究得到了充分的发展，但系统的课程少之又少，能够理论联系实际，适合初学程序员学习的课程更是凤毛麟角。\n叶梓老师，拥有多年的企业实践经验，结合实践在小象学院平台直播\n《计算机视觉的深度学习实践》14堂课细说深度学习之计算机视觉\n第一讲 课程概述\n第二讲 图像预处理\n第三讲 图像特征提取\n第四讲 未有深度学习之前\n第五讲 神经网络与误差反向传播算法\n第六讲 深度学习基础\n第七讲 图像分类\n第八讲 图像检索\n第九讲 目标检测（上）\n第十讲 目标检测（下）\n第十一讲 通用场景下的图像分割\n第十二讲 医疗影像分割\n第十三讲 图像描述（图说）\n第十四讲 图像生成"}
{"content2":"你可能已经听说过深度学习并认为它是骇人的数据科学里的一个领域。怎么可能让机器像人类一样学习呢?再者，对于某些人而言，更为骇人的是，我们为什么要让机器展现出类人的行为?这里，请看深度学习在实际应用中的十大案例，以便将其潜能视觉化。\n深度学习是什么?\n机器学习和深度学习都是人工智能的分支，但深度学习是机器学习的进一步深化。在机器学习中，由人类程序员设计的算法负责分析、研究数据，然后根据数据分析和研究作出决策。深度学习通过一个人造的神经网络来学习，这一人造神经网络运转起来与人类大脑非常相似，它可以让机器在一个框架内像人一样进行分析数据。深度学习的机器不需要人类程序员告诉他们要用数据做什么，这得赖于我们收集并消耗了大量的数据——数据是深入学习模型的燃料。\n在这里相信有许多想要学习大数据的同学，大家可以+下大数据学习裙：957205962，即可免费领取套系统的大数据学习教程\n深度学习的十大应用案例\n1. Customer experience 用户体验\n机器学习已经被很多企业用来改善用户体验。部分案例诸如在线自助服务方案、定制靠谱的工作流程，部分聊天机器人等都已运用到深度学习模型。随着深度学习发展日趋成熟，我们可以预期，未来这一领域将被更多企业用来改善用户体验。\n2、 Translations 翻译\n尽管自动机器翻译并不新鲜，但深度学习正着力于使用神经网络的堆叠网络和图像翻译来增强文本的自动翻译。\n3、 Adding color to black-and-white images and videos 为黑白图像、视频着色\n过去，人们手动为黑白图像及视频着色的过程往往旷日持久，如今，这一工作可以完全由深度学习模型自动完成。\n4、 Language recognition 语言识别\n目前，深度学习机器开始致力于辨别不同的方言。机器确定某人说的是英语，然后利用AI学习辨别方言之间的差异。一旦确定是某种方言，另一个AI会继续专研这种方言，而这所有的过程均不需要人类参与。\n5、 Autonomous vehicles 自动驾驶汽车\n自动驾驶汽车在街上行驶时，并不只有一个AI模型在起作用。一些深度学习模型专门研究街道标识，而另一些则训练识别行人。当一辆自动驾驶的汽车在公路上行驶时，它将接收到成千上万条人工智能模型的信息来辅助其行驶。\n6、 Computer vision 计算机视觉\n在图片分类、目标检测、图片复原和分割方面，深度学习已经展现出超越人类的精确性——他们甚至能识别手写的数字。深度学习借助庞大的神经网络，利用机器自动化人类视觉系统所执行的任务。\n7、 Text generation 创作文本\n机器可以学习一段文本的标点、语法和风格，然后利用这个模式自动创作一篇全新的文章，这篇文章的拼写和语法都是正确的且风格与样本文章一致。从莎士比亚到维基百科，所有的文章都能由此创作。\n8、 Image caption generation 生成图片标题\n深度学习另一个能力也着实备受瞩目——识别图像，并创建一个符合语句结构的连贯标题，宛如人写的一样。\n9、News aggregator based on sentiment 基于情感的新闻聚合器\n如果你想要过滤掉消极新闻，不让它们进入你的世界，先进的自然语言处理程序和深度学习可以帮助你。使用这种新技术的新闻聚合器能够基于用户情感过滤新闻，因此你可以创建只报道正面消息的新闻流。\n10、 Deep-learning robots 深度学习机器人\n机器人的深度学习应用程序丰富而强大，它来自一个令人印象深刻的深度学习系统。通过观察人类完成任务的行为机器人就能学会家务，并通过几个其他人工智能的输入来进行操作。就像人类大脑如何处理来自过去的经验、当前的感官以及任何附加数据信息一样，深度学习模型将帮助机器人执行基于多个不同人工智能意见输入的任务。"}
{"content2":"一、光和电磁波谱\n二、彩色模型\n1.RGB彩色模型\n2.HSV彩色模型\n3.从RGB到HSV的彩色转换\n4.从HSV到RGB的彩色转换\n5.RGB和HSV的互换代码实现（Python+OpenCV）\n三、灰度图像\n1.灰度级与灰度图像\n2.RGB转灰度图像\n3. RGB转灰度图像代码实现（Python+OpenCV）\n一、光和电磁波谱\n1666年，艾萨克牛顿发现，当一束太阳光通过一个玻璃棱镜后，显示的光束不再是白光，而是由一端为紫色而另一段为红色的连续色谱组成。如所示，我们感受到的可见光的彩色范围只占电磁波的一小部分。在波谱的一端是无线电波，其波长是可见波长的几十亿倍。波谱的另一端是伽马射线，其波长比可见光小几百万倍。电磁波谱可用波长、频率或能量来描述。波长（\nλ\nλ\nλ）和频率（\nv\nv\nv）的关系可见下式描述：\nλ\n=\nc\n/\nv\nλ=c/v\nλ=c/v式中\nc\nc\nc是光速（\n2.998\n×\n1\n0\n8\nm\n/\ns\n2.998×10^8m/s\n2.998×108m/s）。电磁波谱的各个分量的能量由下式给出：\nE\n=\nh\nv\nE=hv\nE=hv式中\nh\nh\nh是普朗克常数。\n    电磁波谱    为便于解释，可见光谱已被放大，但请注意，可见光谱是电磁波谱中相当窄的一部分\n电磁波可视为以波长\nλ\nλ\nλ传播的正弦波，从上面的式子可以看出，能量与频率成正比，因此更高频率（更短波长）的电磁现象的每个光子携带更多的能量。\n二、彩色模型\n1.RGB彩色模型\n在RGB模型中，每种颜色出现在红(Red)、绿(Green)、蓝(Blue)的原色光谱成分中。该模型基于笛卡儿坐标系。在RGB彩色模型中表示的图像由3个分量图像组成，每种原色一幅分量图像。当送入RGB监视器时，这3幅图像在屏幕上混合生成一幅合成的彩色图像。\n见，为方便起见，假定所有颜色值均已归一化，即R、G和B的所有值都在区间[0,1]中。0表示不发光，1表示完全发光。以(1,0,0)为例，R为1即完全发光，G与B为0即不发光，最终颜色为红色；以(1,1,0)为例，R与G为1即完全发光，B为0即不发光，即红色跟绿色完全发光，蓝色不发光，最终颜色为黄色。\n    RGB彩色立方体示意图    沿主对角线的点有从原点的黑点至点(1,1,1)的白色的灰度值\n2.HSV彩色模型\n通常用以区别不同颜色特性的是明度、色调和饱和度。明度具体表达了无色的强度概念；色调是光波混合中与主波长有关的属性，表示观察者感知的主要颜色，当我们说一个物体是红色、橙色时，指的是其色调；饱和度指的是相对纯净度，或一种颜色混合白光的数量，所加白光越少饱和度越高。色调与饱和度一起称为色度，因此，颜色可用其明度和色度来表征。\nHSV（色调、饱和度、明度）彩色模型，正是由这三个分量组成的图像。\n3.从RGB到HSV的彩色转换\n下式中RGB的值需归一化，即所有值在区间[0,1]中。\n先定义：\nm\na\nx\n=\nm\na\nx\n(\nR\n,\nG\n,\nB\n)\nm\ni\nn\n=\nm\ni\nn\n(\nR\n,\nG\n,\nB\n)\nmax = max(R,G,B) \\ \\ \\ \\ \\ min = min(R,G,B)\nmax=max(R,G,B)     min=min(R,G,B)\n① 色调H，范围 [0,360]\n② 饱和度S，范围 [0,1]\na)\nm\na\nx\n=\n0\nmax = 0\nmax=0\nS\n=\n0\nS = 0\nS=0b)\nm\na\nx\n≠\n0\nmax \\neq 0\nmax̸ =0\nS\n=\n(\nm\na\nx\n−\nm\ni\nn\n)\n/\nm\na\nx\nS=(max-min)/max\nS=(max−min)/max\n③ 亮度V，范围 [0,1]\nV\n=\nm\na\nx\n(\nR\n,\nG\n,\nB\n)\nV=max(R,G,B)\nV=max(R,G,B)\n4.从HSV到RGB的彩色转换\n其中\nH\nH\nH的取值范围为[0,360]，\nS\n、\nV\n、\nR\n、\nG\n、\nB\nS、V、R、G、B\nS、V、R、G、B的取值范围为[0,1]\nh\ni\n≡\n⌊\nh\n60\n⌋\n(\nm\no\nd\n6\n)\nh_{i}\\equiv \\left\\lfloor {\\frac {h}{60}}\\right\\rfloor {\\pmod {6}}\nhi ≡⌊60h ⌋(mod6)\nf\n=\nh\n60\n−\nh\ni\nf={\\frac {h}{60}}-h_{i}\nf=60h −hi\np\n=\nv\n×\n(\n1\n−\ns\n)\n&ThinSpace;\np=v\\times (1-s)\\,\np=v×(1−s)\nq\n=\nv\n×\n(\n1\n−\nf\n×\ns\n)\n&ThinSpace;\nq=v\\times (1-f\\times s)\\,\nq=v×(1−f×s)\nt\n=\nv\n×\n(\n1\n−\n(\n1\n−\nf\n)\n×\ns\n)\n&ThinSpace;\nt=v\\times (1-(1-f)\\times s)\\,\nt=v×(1−(1−f)×s)\n对于每个颜色向量\n(\nr\n,\ng\n,\nb\n)\n(r, g, b)\n(r,g,b)\n5.RGB和HSV的互换代码实现（Python+OpenCV）\nOpenCV的cvtColor()函数能支持各种彩色模型的转换，其中包括RGB和HSI的转换，部分用法如下：\ncv2.cvtColor(src, code) src为要转换的图片，code为转换的格式，code参数： 1、BGR <=> RGB : COLOR_BGR2RGB、COLOR_RGB2BGR 2、BGRA <=> RGBA :COLOR_BGRA2RGBA、 COLOR_RGBA2BGRA 3、BGR 和 RGB 添加 alpha 通道 : COLOR_BGR2BGRA、COLOR_RGB2RGBA 4、BGRA 和 RGBA 删除 alpha 通道 :COLOR_BGRA2BGR、 COLOR_RGBA2RGB 5、BGR 和 RGB <=> HSV : COLOR_BGR2HSV、COLOR_RGB2HSV、COLOR_HSV2BGR、COLOR_HSV2RGB（转换后H的取值范围为[0,180]，S和V的取值范围为[0,255]）\n现用一张8位的图片01.jpg演示代码：\nimport cv2 img = cv2.imread('01.jpg') # 获取字节顺序为BGR的图片 img1 = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # 添加 alpha 通道 img2 = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # BGR转HSV img3 = cv2.cvtColor(img2, cv2.COLOR_HSV2RGB) # HSV 转 RGB img[0][0] img1[0][0] img2[0][0] img3[0][0] # 显示结果如下： array([119, 203, 238], dtype=uint8) array([119, 203, 238, 255], dtype=uint8) # 增加了第4个分量即 alpha 通道 ''' 根据上文算法将 BGR 转换成 HSV H = (G-B) / (max-min) * 60 = (203-119) / (238-119) * 60 = 42.35，H的取值范围在[0,360]，需转换到[0,180]，得42.35/2 = 21 S = (max-min) / max = (238-119) / 238 = 0.5，0.5 * 255 = 128 V = max(R,G,B) = 238 ''' array([ 21, 128, 238], dtype=uint8) ''' 根据上文算法将 HSV 转换成 RGB 将hsv分别转换到范围[0,180]、[0,1]、[0,1] 得 [21*2, 128/255, 238/255] = [42, 0.502, 0.933] hi = h/60 mod 6 = 42 / 60 mod 6 = 0 f = h/60 - hi = 42 / 60 - 0 = 0.7 p = v*(1-s) = 0.933*(1-0.502) = 0.4646 q = v*(1-f*s) = 0.933*(1-0.7*0.502) = 0.6051 t = v*(1-(1-f)*s) = 0.933*(1-(1-0.7)*0.502) = 0.7925 因为hi = 0，RGB = [v, t, p] = [0.933, 0.7925, 0.4646]，转换到范围[0,255] 得 [238, 202, 118] ''' array([238, 202, 118], dtype=uint8)\n三、灰度图像\n1.灰度级与灰度图像\n没有颜色的光称为单色光或无色光。单色光的唯一属性是其强度或大小。因为感知单色光的强度从黑色到灰色变化，最后到白色，灰度级一次通常用来表示单色光的强度。\n从黑到白的单色光的度量值范围通常称为灰度级，而单色图像通常称为灰度图像。\n2.RGB转灰度图像\n使用不同的算法，会得到不同的灰度图像，例如若要查看红色在图像的分布情况，可使用：\nG\nr\na\ny\n=\n1\n×\nR\n+\n0\n×\nG\n+\n0\n×\nB\nGray = 1×R + 0×G + 0×B\nGray=1×R+0×G+0×B    从上式可知，选择不同的系数，可得到不同的灰度图像，目前常使用下面的一个心理学公式：\nG\nr\na\ny\n=\n0.299\n×\nR\n+\n0.587\n×\nG\n+\n0.114\n×\nB\n(\n1\n)\nGray = 0.299×R + 0.587×G + 0.114×B\\ \\ \\ \\ \\ \\ \\ \\ (1)\nGray=0.299×R+0.587×G+0.114×B        (1)\n3. RGB转灰度图像代码实现（Python+OpenCV）\n① cv2.imread()实现RGB转灰度\nOpenCV的imread()函数能支持各种静态图像文件格式，其中包括RGB转灰度，部分用法如下：\ncv2.imread(filename[, flags]) 该函数返回的图像格式为BGR，与RGB表示的色彩空间相同，但是字节顺序相反。 filename为图片路径，flags表示用何种方式读取图片，默认为IMREAD_COLOR ，flags参数： 1、IMREAD_COLOR = 1 : 加载彩色图像，最多8位 2、IMREAD_GRAYSCALE = 0 : 以灰度模式加载图像，算法为式(1)，最多8位 3、IMREAD_UNCHANGED = -1: 包含alpha通道，可加载8位和16位图像\n现用两张8位的图片01.jpg和02.png演示代码：\nimport cv2 img1 = cv2.imread('01.jpg', cv2.IMREAD_COLOR) img2 = cv2.imread('01.jpg', cv2.IMREAD_GRAYSCALE) # 转换成灰度图像 img3 = cv2.imread('01.jpg', cv2.IMREAD_UNCHANGED) img4 = cv2.imread('02.png', cv2.IMREAD_UNCHANGED) img1[0][0] img2[0][0:1] img3[0][0] img4[0][0] # 显示结果如下： array([119, 203, 238], dtype=uint8) # 8位彩色图片，若图片为16位也会转换成8位 array([204], dtype=uint8) # 计算式(1)得 0.299*238+0.587*203+0.114* 119 = 204 array([119, 203, 238], dtype=uint8) # 无alpha通道 array([119, 203, 238, 255], dtype=uint8) # 第4个分量为alpha通道，若图片为16位则可保留16位\n② cv2.cvtColor()实现RGB转灰度\nOpenCV的cvtColor()函数能支持各种彩色模型的转换，其中包括RGB和灰度的转换，部分用法如下：\ncv2.cvtColor(src, code) src为要转换的图片，code为转换的格式，code参数： 1、BGR => 灰度 : COLOR_BGR2GRAY\n现用一张8位的图片01.jpg演示代码：\nimport cv2 img = cv2.imread('01.jpg') # 获取字节顺序为BGR的图片 img1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # BGR转灰度 img[0][0] img1[0][0:1] # 显示结果如下： array([119, 203, 238], dtype=uint8) array([204], dtype=uint8) # 计算式(1)得 0.299*238+0.587*203+0.114* 119 = 204\n\n以上全部内容参考书籍如下：\n冈萨雷斯《数字图像处理（第三版）》\nHSL和HSV色彩空间"}
{"content2":"《计算机视觉特征提取与图像处理（第三版）》第一章（1）\nCpt1 绪论（人类视觉系统/计算机视觉系统/数学系统/参考文献）\n1.2 人类视觉和计算机视觉\na. 图像处理领域常用图片/视频：openCV自带的图片库/视频库，例如Lena；\nb. 广义图像采集：摄像机图像/MRI（软组织图像/computerized tomography计算断层摄影术/infrared sensor或synthetic-aperture radar遥感图像/sonar array等的空间信息，即计算机视觉系统对广义的图像进行处理；\nc. 使用合成图像评价不同方法各自性能的局限；\n纹理库：Brodatz album of texture，Brodatz纹理相簿；\nhttp://multibandtexture.recherche.usherbrooke.ca/original_brodatz.html\n1.3 人类视觉系统\na. 人类视觉善于区分相对距离，而不善于估计绝对距离，计算机视觉正好相反，善于估测绝对差异，但对相对差异估测的效果较差；\nb. 人类视觉系统可以用三个部分来建模：\n眼睛，物理模型，很多功能可以用病理学来确定；\n处理系统，实验模型，不能精确断定；\n脑分析，心理模型，通过实验或推理来判断它的活动；\n1.3.1 眼睛物理模型\na. 虹膜iris/瞳孔pupil；角膜cornea或巩膜sclera；脉络膜choroid；视网膜retina；中央凹forvea；盲点blind spot；\nb. 感光细胞分为：\nrod柱细胞（暗视觉，scotopic vision，1千万左右）\ncone锥细胞（明视觉，photopic vision，1亿左右），分为S短波长（蓝光，数量少）、M中波长（绿光）、L长波长（红光）；\nc. 视敏度，通常表现为空间分辨率（清晰度）和亮度/色彩分辨率，；\nd. 视觉响应成对数特性，依赖于从暗处到亮处的亮度适应；\ne. 马赫带效果（Mach band）：不同亮度区域的交界处在人眼中存在过冲响应以便于区分视界内的不同目标；人类视觉实际上可以辨别5bit的灰度等级；\n1.3.2 神经系统\n眼睛产生的神经信号实际上是视锥细胞和视杆细胞两种细胞所传送的响应，通过加法模型和对数函数模型映射眼睛产生的已知响应；然后与权重因子相乘调节特定单元权重；\n1.3.3 处理系统\na. 神经信号被传送到大脑的两个区域进行进一步处理，分别为联络皮层和枕叶皮层；\nb. 参考书：The Joy of Vision，Schwarz；\nwww.yorku.ca./eye/thejoy.htm\n1.4 计算机视觉系统\n1.4.1 摄像机\na. 成像原理：空穴/电子对的数量同入射光量成正比，亮度增强则电流增大，电压增大；\nb. 摄像机主要分为：摄像管（早期模拟技术，需要扫描过程，性能较差，存在部件磨损；）、CCD（Charge Coupled Devices）相机和CMOS(Complementary Metal Oxide Silicon)相机；\n半导体像素传感器可以分为passive sensor无源传感器和active sensor有源传感器；\n1.4.2 图像处理\na. C、C++、Java是目前为止最受欢迎的视觉系统实现语言，在集成高低和低级功能方面力量强大且编译能力较强；\n基于Java的演示系统：http://www.southampton.ac.uk/~msn/book/new_demo/\n软件包：OpenCV，“通过创建一个免费和开放的基础平台把视觉领域的成果进行整合和性能优化，帮助计算机视觉在人机交互、机器人、监测、生物测量和安全方面实现商业应用”；\nVXL库（vision-something-libraries），为计算机视觉研究和实现而设计的而设计的C++库集https://vxl.github.io/；\nAdobe通用图像库（GIL）,旨在减少图像相关代码的编写难度http://stlab.adobe.com/group__asl__gil.html；\nCImg Library，图像处理算法通用平台系统，http://www.cimg.eu/；\nVLFeat，http://www.vlfeat.org/；\n此外还包括部分有开源软件的竞赛网站，如ACM Multimedia；\n1.5 数学系统\n1.5.1 数学工具\na. 目前最受欢迎的数学系统有Mathematica、Maple、MATLAB、Mathcad等；\n1.5.2 Hello MATLAB，hello images\na. Matlab提供强有力的矩阵计算来开发和测试复杂的实现；\n本书成像模型采用正投影，，世界坐标直接映射到图像；通过Matlab的应用代码转换比其他系统容易，能够将代码直接编译；\n1.5.3 Hello, Mathcad!\na. Mathcad是一个所见即所得系统，而不是基于屏幕的（把Mathcad看作Word，则Matlab看作Latex）；Mathcad使用Mapple数学库来扩展其功能；\n因为必须包含低级函数，Mathcad代码的应用比Matlab复杂；\n可以从参考网站下载worksheet；\n1.6 相关资料\n1.6.1 期刊杂志和学术会议\na. Vision System Design和Advanced Imaging\nb. IEEE Transactions on: Pattern Analysis and Machine Intelligence\nImage Processing；\nSystem, Man and Cybernetics；\nMedical Imaging；\nPattern Recognition；\nPattern Recognition Letters；\nInternational Journal of Computer Vision；\nImage and Vision Computing；\n1.6.2学术会议：\nIEEE；\nCVPR；\nICCV；\nSpringer，Lecture Notes in Computer Science\n1.6.3 书籍推荐\na. World of Mathematics；\nb. Numerical Recipes信号处理相关；\nc. Digital Signal Processing；\nd. Joy of Visual Perception；\ne. CVOnline，\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/（强烈推荐！！！！）\nf. Vision\nFundamentals of Computer Vision\nRobot Vision\nImage Processing, Analysis and Computer Vision(Sonka et al., 2007)\nMachine Vision(Jain et al., 1995)\nDigital Image Processing(Gonzalez and Woods, 2008)，冈萨雷斯的著名教程\nDigital Picture Processing(Rosenfeld and Kak, 1982)\nDigital Image Processing(Pratt, 2001)\nActive Contour(Blake and Isard, 1998)\nImage Processing The Fundamentals(Petrou and Petrou, 2010)\nComputer Vision(Shapiro and Stockman, 2001)，图像数据库和虚拟及增强现实\nComputer Imaging: Digital Image Analysis and Processing(Umbaugh, 2005)\nComputer Vision: A Modern Approach(Forsyth and Ponce, 2002)\nImage Processing and Computer Vision(Parker, 2010)\nPractical Algorithms for Image Analysis(O’ Gorman et al., 2008 )\nComputer Vision and Image Processing(Umbaugh, 2005) ，侧重工程性\nThe Art of Image Processing with Java(Hunt, 2011)，Java教程\nThe Image Processing Handbooks(Russ, 2006)\nMachine Vision: Theory, Algorithms and Practicalities(Davies, 2005)，侧重工业视觉系统\nHandbook of Pattern Recognition and Computer Vision(Cheng and Wang, 2009);"}
{"content2":"视觉跟踪综述\n目标跟踪是绝大多数视觉系统中不可或缺的环节。在二维视频跟踪算法中，基于目标颜色信息或基于目标运动信息等方法是常用的跟踪方法。从以往的研究中我们发现，大多数普通摄像头（彩色摄像头）下非基于背景建模的跟踪算法都极易受光照条件的影响。这是因为颜色变化在某种程度上是光学的色彩变化造成的。如基于体素和图像像素守恒假设的光流算法它也是假设一个物体的颜色在前后两帧没有巨大而明显的变化。\n但在特定的场景应用中（如视频监控等领域），不失有一些经典的跟踪算法可以实现较好的跟踪效果。以下主要介绍三种经典的跟踪算法：CamShift算法、光流跟踪以及粒子滤波算法。最后将给出一个各种跟踪方法间的比较。\n1、  CamShift（Continuously Adaptive Mean Shift）跟踪算法\nCamShift算法是一种基于均值漂移的算法。均值移动的理论基础是概率密度估计。均值移动的过程实际上就是在概率密度空间中寻找局部极大点。从其全称可知CamShift的算法基础实际上是MeanShift算法，均值移动的操作过程可用如下几步来表示：\n(a)    计算以初始点x­0为中心的某一核窗所对应的均值移动向量mG(x0)；\n(b)    根据mG(x0)来移动核窗的中心位置，也即把mG(x0)中的加权平均值部分赋予x0，把x0作为新的初始点，并转回步骤(a)；\n(c)    重复(a)、(b)过程，直到满足某一预定的条件。\n因此，均值移动过程就是寻找数据分布最密处的过程。\n均值移动的实现过程可图示为：\n(1) 计算目标区域的均值、移动目标区域\n(2) 重新计算目标区域均值，还存在移动向量，继续移动目标区域\n(3) 移动向量越来越小\n(4)  找到局部极大点，停止移动\n以上过程只是一次MeanShift算法过程，在连续帧上使用MeanShift算法就是CamShift跟踪算法。CamShift同经典的均值移动跟踪算法的基本思想是相同的，所不同的它是建立在颜色概率分布图和矩的基础之上。CamShift对室内环境下的目标跟踪具有较高的鲁棒性。\n1、  光流跟踪算法\n将三维空间中的目标和场景对应于二维图像平面运动时，他们在二维图像平面的投影就形成了运动，这种运动以图像平面亮度模式表现出来的流动就称为光流。光流法是对运动序列图像进行分析的一个重要方法，光流不仅包含图像中目标的运动信息，而且包含了三维物理结构的丰富信息，因此可用来确定目标的运动情况以及反映图像其它等信息。\n光流是空间运动物体在观测成像面上的像素运动的瞬时速度。光流的研究是利用图像序列中的像素强度数据的时域变化和相关性来确定各自像素位置的“运动”，即研究图像灰度在时间上的变化与景象中物体结构及其运动的关系。一般情况下，光流由相机运动、场景中目标运动或两者的共同运动产生。光流计算方法大致可分为三类：基于匹配的、频域的和梯度的方法。\n(1) 基于匹配的光流计算方法包括基于特征和基于区域两种。基于特征的方法不断地对目标主要特征进行定位和跟踪，对大目标的运动和亮度变化具有鲁棒性。存在的问题是光流通常很稀疏，而且特征提取和精确匹配也十分困难。基于区域的方法先对类似的区域进行定位，然后通过相似区域的位移计算光流。这种方法在视频编码中得到了广泛的应用。然而，它计算的光流仍不稠密。\n(2) 基于频域的方法利用速度可调的滤波组输出频率或相位信息。虽然能获得高精度的初始光流估计，但往往涉及复杂的计算。另外，进行可靠性评价也十分困难。\n(3) 基于梯度的方法利用图像序列的时空微分计算2D速度场（光流）。由于计算简单和较好的效果，基于梯度的方法得到了广泛的研究。虽然很多基于梯度的光流估计方法取得了较好的光流估计，但由于在计算光流时涉及到可调参数的人工选取、可靠性评价因子的选择困难，以及预处理对光流计算结果的影响，在应用光流对目标进行实时监测与自动跟踪时仍存在很多问题。\n光流法检测运动物体的基本原理是：给图像中的每一个像素点赋予一个速度矢量，这就形成了一个图像运动场，在运动的一个特定时刻，图像上的点与三维物体上的点一一对应，这种对应关系可由投影关系得到，根据各个像素点的速度矢量特征，可以对图像进行动态分析。如果图像中没有运动物体，则光流矢量在整个图像区域是连续变化的。当图像中有运动物体时，目标和图像背景存在相对运动，运动物体所形成的速度矢量必然和邻域背景速度矢量不同，从而检测出运动物体及位置。采用光流法进行运动物体检测的问题主要在于大多数光流法计算耗时，实时性和实用性都较差。但是光流法的优点在于光流不仅携带了运动物体的运动信息，而且还携带了有关景物三维结构的丰富信息，它能够在不知道场景的任何信息的情况下，检测出运动对象。\n对于视频监控系统来说，所用的图像基本都是摄像机静止状态下摄取得，所以对有实时性和准确性要求的系统来说，纯粹使用光流法来检测目标不太实际。更多的是利用光流计算方法与其它方法相结合来实现对目标检测和运动估计。\n然而，在实际应用中，由于遮挡性、多光源、透明性和噪声等原因，使得光流场基本方程的灰度守恒假设条件不能满足，不能求解出正确的光流场，同时大多数的光流计算方法相当复杂，计算量巨大，不能满足实时的要求，因此，一般不被对精度和实时性要求比较高的监控系统所采用。\n3、  粒子滤波跟踪算法\n粒子滤波算法有很多变种，以Rob Hess实现的这种最基本的粒子滤波算法为例。它的核心思想是随机采样和重要性重采样。在不知道目标在哪里的情况下，随机向场景中分散粒子，撒完粒子后，根据特征相似度计算每个粒子的重要性，然后在重要的地方多撒粒子，不重要的地方少撒粒子。所以说粒子滤波较之蒙特卡洛滤波计算量较小。这种思想虽然简单，但效果往往很好。\n粒子滤波实现对目标的跟踪通常分以下四个步骤：\n(1)    初始化阶段-提取跟踪目标特征\n该阶段要人工指定跟踪目标，程序计算跟踪目标的特征，比如可以采用目标的颜色特征。这点和CamShift算法类似，不能实现自动初始化。但我们可以在初始时给定一个颜色样本，实现程序的半自动初始化。然后计算该区域色调(Hue)空间的直方图，即为目标的特征。直方图可以用一个向量来表示，所以目标特征就是一个N*1的向量V。\n(2)    搜索阶段—分撒搜索粒子\n获取目标特征后，在场景中分撒许多搜索粒子去搜索目标对象。粒子分撒有许多种方式。比如，a) 均匀分撒。即在整个图像平面均匀的撒粒子(uniform distribution)；b)在上一帧得到的目标附近按照高斯分布来放，可以理解成，靠近目标的地方多放，远离目标的地方少放。Rob Hess的代码用的是后一种方法。粒子放出去后按照初始化阶段得到的目标特征(色调直方图，向量V)计算它所处的位置处图像的颜色特征，得到一个色调直方图，向量Vi，计算该直方图与目标直方图的相似性（直方图匹配）。相似性有多种度量，最简单的一种是计算sum(abs(Vi-V))。每个粒子算出相似度后再做一次归一化，使得所有的粒子得到的相似度加起来等于1。\n(3)      决策阶段\n分撒出去的每个粒子将返回其所处位置的图像信息。比如，“一号粒子处图像与目标的相似度是0.3”,“二号粒子处图像与目标的相似度是0.02”,“三号粒子处图像与目标的相似度是0.0003”,“N号粒子处图像与目标的相似度是0.013”然后做加权平均。设N号粒子的图像像素坐标是(Xn,Yn),它报告的相似度是Wn,于是目标最可能的像素坐标X = sum(Xn*Wn),Y = sum(Yn*Wn)。\n(4)      重采样阶段Resampling\n在新的一帧图像里，为了搜索到目标的新位置，需要再分撒粒子进行搜索。但现在应该怎样分撒呢？这要根据上一帧各个粒子返回的相似度报告。比如，“一号粒子处图像与目标的相似度是0.3”,“二号粒子处图像与目标的相似度是0.02”,“三号粒子处图像与目标的相似度是0.0003”,“N号粒子处图像与目标的相似度是0.013”。综合所有粒子的报告，一号粒子处的相似度最高，三号粒子处的相似度最低，于是要重新分撒粒子，在相似度最高的粒子那里放更多条粒子，在相似度最低的粒子那里少放粒子，甚至把原来那条粒子也撤回来。这就是Sampling Importance Resampling，根据重要性重采样(更具重要性重新放粒子)。\n(2)->(3)->(4)->(2)如是反复循环，即完成了目标的动态跟踪。\n粒子滤波跟踪算法可用于视频监控领域，可以跟踪速度较快的跟踪目标。\n4、  其他跟踪算法及优缺点\n将其他一些常用的跟踪算法及优缺点形成了一个表，其原理不做赘述，可参阅相关文献。\nlocal orientation correlation (LOC) , flocks of features tracking (FF) , optical flow tracking using templates on a regular grid (OF) and local feature tracking, KLT-tracker(KLT) , and boosted detection (BD).\n参考\n[1] 《基于均值移动的人脸跟踪简介》 未公开\n[2] http://kb.cnblogs.com/a/1742263/\n[3] AIDIA – Adaptive Interface for Display Interaction\n[4] http://baike.baidu.com/view/2810997.htm\n分类: 目标跟踪\nposted @ 2012-04-18 16:05 Hanson-jun 阅读(68) 评论(0) 编辑\n机器视觉开源处理库汇总\n从cvchina搞到的机器视觉开源处理库汇总，转来了，很给力，还在不断更新。。。\n通用库/General Library\nOpenCV\n无需多言。\nRAVL\nRecognition And Vision Library. 线程安全。强大的IO机制。包含AAM。\nCImg\n很酷的一个图像处理包。整个库只有一个头文件。包含一个基于PDE的光流算法。\n图像，视频IO/Image, Video IO\nFreeImage\nDevIL\nImageMagick\nFFMPEG\nVideoInput\nportVideo\nAR相关/Augmented Reality\nARToolKit\n基于Marker的AR库\nARToolKitPlus\nARToolKit的增强版。实现了更好的姿态估计算法。\nPTAM\n实时的跟踪、SLAM、AR库。无需Marker，模板，内置传感器等。\nBazAR\n基于特征点检测和识别的AR库。\n局部不变特征/Local Invariant Feature\nVLFeat\n目前最好的Sift开源实现。同时包含了KD-tree，KD-Forest，BoW实现。\nFerns\n基于Naive Bayesian Bundle的特征点识别。高速，但占用内存高。\nSIFT By Rob Hess\n基于OpenCV的Sift实现。\n目标检测/Object Detection\nAdaBoost By JianXin.Wu\n又一个AdaBoost实现。训练速度快。\n行人检测 By JianXin.Wu\n基于Centrist和Linear SVM的快速行人检测。\n（近似）最近邻/ANN\nFLANN\n目前最完整的（近似）最近邻开源库。不但实现了一系列查找算法，还包含了一种自动选取最快算法的机制。\nANN\n另外一个近似最近邻库。\nSLAM & SFM\nSceneLib [LGPL]\nmonoSLAM库。由Androw Davison开发。\n图像分割/Segmentation\nSLIC Super Pixel\n使用Simple Linear Iterative Clustering产生指定数目，近似均匀分布的Super Pixel。\n目标跟踪/Tracking\nTLD\n基于Online Random Forest的目标跟踪算法。\nKLT\nKanade-Lucas-Tracker\nOnline boosting trackers\nOnline Boosting Trackers\n直线检测/Line Detection\nDSCC\n基于联通域连接的直线检测算法。\nLSD [GPL]\n基于梯度的，局部直线段检测算子。\n指纹/Finger Print\npHash [GPL]\n基于感知的多媒体文件Hash算法。（提取，对比图像、视频、音频的指纹）\n视觉显著性/Visual Salience\nGlobal Contrast Based Salient Region Detection\nMing-Ming Cheng的视觉显著性算法。\nFFT/DWT\nFFTW [GPL]\n最快，最好的开源FFT。\nFFTReal [WTFPL]\n轻量级的FFT实现。许可证是亮点。\n音频处理/Audio processing\nSTK [Free]\n音频处理，音频合成。\nlibsndfile [LGPL]\n音频文件IO。\nlibsamplerate [GPL ]\n音频重采样。\n小波变换\n快速小波变换（FWT）\nFWT\nBRIEF: Binary Robust Independent Elementary Feature 一个很好的局部特征描述子，里面有FAST corner + BRIEF实现特征点匹配的DEMO：http://cvlab.epfl.ch/software/brief/\nhttp://code.google.com/p/javacv\nJava打包的OpenCV, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, and ARToolKitPlus库。可以放在Android上用~\nlibHIK,HIK SVM，计算HIK SVM跟Centrist的Lib。http://c2inet.sce.ntu.edu.sg/Jianxin/projects/libHIK/libHIK.htm\n一组视觉显著性检测代码的链接：http://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/\n分类: 编码杂记\nposted @ 2012-04-18 16:05 Hanson-jun 阅读(175) 评论(0) 编辑\n不规则物体形状匹配综述\n不规则物体形状匹配综述\n物体识别是计算机视觉应用的一项基本任务。识别通常基于目标物体的灰度信息、颜色信息或形状信息。物体识别的目的就是要找到一个包含可以区分不同目标物体的有效信息的描述。由于要识别的物体是事先知道的，所以目标物体的几何特征可以被直接应用到识别任务中。\n不规则物体的形状匹配是一种有效的利用物体几何特征进行识别的方法。根据匹配对象的不同，可以将不规则物体的形状匹配分为基于区域的匹配方法和基于轮廓的匹配方法。\n一、基于轮廓特征的形状匹配\n基于轮廓特征的形状匹配在实际中更为常用，这主要有两方面的原因：一是基于轮廓特征的匹配计算量小，可以较好的满足实时性要求；二是要识别的目标物通常是预先知道的，那么它的几何信息完全可以被用于识别过程中。为了识别不规则物体，主要任务就是设计一种基于（少量的）目标物几何约束先验知识的有效匹配方法。\n1.   链码直方图(chain code histogram)\n链码直方图将人眼看上去相似的物体归为一类。因此利用它不能进行精确的识别和分类。\n方向链码（Freeman链码）是用来表示物体轮廓的典型链码表示法。一条离散曲线可以定义为Z2域内一组数量有限的8联通点。因此，一条数字化二值曲线可以用方向链码表示方向链码是相邻两像素连线的8种可能的方向值。一条曲线被网格离散化后形成n个链码方向，最终此曲线链码可表示为{ai}n，每条链指向8个方向重的一个方向，ai={0,1,2,3,4,5,6,7}，i为像素的索引值，ai是由像素(i)指向像素(i+1)的方向链码。\n链码直方图的计算简单而且快速。计算公式如下：\n其中，nk是一个链码中链码值k的数目，n是一个链码中的节点数。\n(a)编码的方向示意，(b)简单物体形状，(c)形状的链码表示，(d)链码直方图\n链码法的特点：\n(1) 计算量小，可满足实时性要求；\n(2) 具有平移、尺度不变性；\n(3) 具有90度旋转不变性；\n(4) 规格化链码直方图可以达到更好的旋转不变性。\n2.   成对几何直方图(Pairwise Geometric Histogram)\n成对几何直方图通过相对角和相对位置特征来描述目标轮廓，并采用关系直方图统计这对几何特征来进行形状索引。采用这种编码方式需要具备一定的前提，即对于一个不规则物体我们可以将其近似为一个几何多边形。这种编码方法可以很好地描述一个多边形物体。\n将不规则形状近似为多边形，并将其定义为边缘点的集合。这样它所包含的轮廓边缘（线段）就可以由连续的边缘点来表示。接下来我们计算多边形的PGH：将每一个轮廓边缘视为其方向上的基准线，那么它与其他轮廓边缘间的相对角 以及最大最小垂直距离(dmin和dmax)可以被计算出来。边缘之间的角度定义了直方图的行，然后在其中增加对应的计算出来的最大和最小距离的所有直方块，就得到了当前多边形的PGH。\n它具有如下特点：\n(1) 计算简单，可满足实时性要求；\n(2) 具有平移、尺度不变性；\n(3) 具有360度旋转不变性。\n3.   简单形状描述符的结合(Combination of Simple Shape Descriptors)\n如图，它们是几种简单的形状描述符，分别代表了凹凸性、主轴、致密性、差异性和椭圆差异性。\n凹凸性：轮廓凸包周长与原轮廓周长的比率。（所有凸起的覆盖轮廓称为凸包）\n主轴：过物体质心的正交轴，主轴之间的比例可以由物体轮廓的协方差矩阵计算出来\n致密性：物体区域面积与等面积的正方形周长的比例，也可以是圆。\n差异性：表现为与模板比较的比例均方误差。\n单独用这几种简单描述符的任何一种来表示形状进行匹配，都不能达到较好的匹配结果。但是如果我们将这五种简单形状描述符进行结合，同时用它们来描述一个形状，那么这个形状的描述信息就非常丰富了，匹配的结果也会很好。\n这种形状描述方式具有如下的特点：\n(1) 单独的任何一种简单描述符都不能用于精确识别物体，但是多种简单描述符的结合可以达到很高的识别效率；\n(2) 计算简单，可以达到实时性；\n(3) 具有平移、尺度不变性；\n(4) 理论上360度旋转不变性。\n4.   基于hausdorff距离的形状匹配\nHausdorff距离用来计算两个点集之间的匹配程度。给定两个有限集A={a1,a2,…,ap}和B={b1,b2,…,bq},A，B之间的Hausdorff距离定义如下：\n其中：\nHausdorff距离H(A,B)取h(A,B)和h(B,A)的最大值，这样通过计算h(A,B)和h(B,A)就可以获得两个点集A，B之间的匹配程度。\n为了减少计算量，可以取角点进行匹配。但这样匹配率将降低。基于hausdorff距离的形状匹配的特点;\n(1) 对每个边缘点进行hausdorff距离计算，计算量稍大，但对不是过于复杂的轮廓（如小尺寸轮廓），可以满足实时性；\n(2) 具有平移、尺度不变性；\n(3) 具有旋转不变性；\n二、基于区域特征的形状匹配\n基于不变矩的形状匹配是典型的基于区域的匹配方法。其中，基于Hu不变矩的形状匹配应用最为广泛。\n图像的矩函数在模式识别、目标分类中得到了广泛的应用。在1961年首先基于代数不变量引入矩不变量。通过对几何矩的非线性组合，导出了一组对于图像平移、尺度、旋转变化不变的矩，这种矩就成为Hu矩。\n一幅大小为M×N的二维图像其中(p+q)阶矩：\n对于二值图像，其零阶矩就是该形状区域的面积。因此，将面积归一化，每一个图像矩除以零阶矩得到的商具有形状的尺度变化无关性。\n求图像的p+q阶中心矩，面积归一化，使得具有平移、尺度不变性。\n(1)\n(2)\n计算图像的7个面积归一化的中心矩，{m11,m02,m20,m21,m12,m03,m30}Hu不变矩是关于这7个矩的函数。具有平移、旋转和尺度不变性。\n彩色图像Hu不变矩的计算流程如下：\n基于Hu矩的形状匹配所具有的特点：\n(1) Hu不变矩只能用于对区域的检测，不能用于边界的检测，但由于计算简单，计算量不大，可以满足实时性；\n(2) 具有平移、尺度不变性；\n(3) 具有旋转不变性。\n三、匹配方法间的比较\nCCH（链码直方图）：是一种基于轮廓匹配方法。具有较强的平移不变性，尺度不变性一般，具有90度的旋转不变性。由于编码简单，执行速度快。计算量和所需内存都较小，适合差别明显的物体，对平滑和非平滑物体的识别并不明显；\nPGH（成对几何直方图）：是一种基于轮廓匹配方法。具有较强的平移不变性和尺度不变性，具有360度的旋转不变性。执行速度快，可以较好地识别多边形物体和部分自封闭的物体，由于它的计算过程，对非多边形物体的识别可能会浪费计算量；\nCFSS（五种简单形状描述符结合）：是一种基于轮廓匹配方法。具有较强的平移不变性和尺度不变性，具有360度的旋转不变性。执行速度处于CCH方法和PGH方法之间。识别率与PGH相当，但是比它需要更少的计算时间和内存。\nHAUSDORFF距离：是一种基于轮廓匹配方法。具有较强的平移不变性，但是尺度不变性和旋转不变性都较差。由于处理的数据维数较多，执行效率是这五种方法中最慢的一个。可用于匹配部分重和形状物体。\nHu不变矩：是一种基于区域的形状匹配方法。具有较强的平移、尺度和旋转不变性，其中旋转不变性为360度。但由于匹配的数据量大，执行速度较慢。适合于进行一些更精确的匹配。\n分类: 目标识别\nposted @ 2012-04-18 16:04 Hanson-jun 阅读(42) 评论(0) 编辑\nHough变换原理\n一、简单介绍\nHough变换是图像处理中从图像中识别几何形状的基本方法之一。Hough变换的基本原理在于利用点与线的对偶性，将原始图像空间的给定的曲线通过曲线表达形式变为参数空间的一个点。这样就把原始图像中给定曲线的检测问题转化为寻找参数空间中的峰值问题。也即把检测整体特性转化为检测局部特性。比如直线、椭圆、圆、弧线等。\n二、Hough变换的基本思想\n设已知一黑白图像上画了一条直线，要求出这条直线所在的位置。我们知道，直线的方程可以用y=k*x+b 来表示，其中k和b是参数，分别是斜率和截距。过某一点(x0,y0)的所有直线的参数都会满足方程y0=kx0+b。即点(x0,y0)确定了一族直线。方程y0=kx0+b在参数k--b平面上是一条直线，(你也可以是方程b=-x0*k+y0对应的直线)。这样，图像x--y平面上的一个前景像素点就对应到参数平面上的一条直线。我们举个例子说明解决前面那个问题的原理。设图像上的直线是y=x, 我们先取上面的三个点：A(0,0), B(1,1), C(22)。可以求出，过A点的直线的参数要满足方程b=0, 过B点的直线的参数要满足方程1=k+b, 过C点的直线的参数要满足方程2=2k+b, 这三个方程就对应着参数平面上的三条直线，而这三条直线会相交于一点(k=1,b=0)。　同理，原图像上直线y=x上的其它点(如(3,3),(4,4)等)　对应参数平面上的直线也会通过点(k=1,b=0)。这个性质就为我们解决问题提供了方法，就是把图像平面上的点对应到参数平面上的线，最后通过统计特性来解决问题。假如图像平面上有两条直线，那么最终在参数平面上就会看到两个峰值点，依此类推。\n简而言之，Hough变换思想为：在原始图像坐标系下的一个点对应了参数坐标系中的一条直线，同样参数坐标系的一条直线对应了原始坐标系下的一个点，然后，原始坐标系下呈现直线的所有点，它们的斜率和截距是相同的，所以它们在参数坐标系下对应于同一个点。这样在将原始坐标系下的各个点投影到参数坐标系下之后，看参数坐标系下有没有聚集点，这样的聚集点就对应了原始坐标系下的直线。\n在实际应用中，y=k*x+b形式的直线方程没有办法表示x=c形式的直线(这时候，直线的斜率为无穷大)。所以实际应用中，是采用参数方程p=x*cos(theta)+y*sin(theta)。这样，图像平面上的一个点就对应到参数p---theta平面上的一条曲线上，其它的还是一样。\n三、Hough变换推广\n1、已知半径的圆\n其实Hough变换可以检测任意的已知表达形式的曲线，关键是看其参数空间的选择，参数空间的选择可以根据它的表达形式而定。比如圆的表达形式为 ，所以当检测某一半径的圆的时候，可以选择与原图像空间同样的空间作为参数空间。那么圆图像空间中的一个圆对应了参数空间中的一个点，参数空间中的一个点对应了图像空间中的一个圆，圆图像空间中在同一个圆上的点，它们的参数相同即a，b相同，那么它们在参数空间中的对应的圆就会过同一个点（a，b），所以，将原图像空间中的所有点变换到参数空间后，根据参数空间中点的聚集程度就可以判断出图像空间中有没有近似于圆的图形。如果有的话，这个参数就是圆的参数。\n2、未知半径的圆\n对于圆的半径未知的情况下，可以看作是有三个参数的圆的检测，中心和半径。这个时候原理仍然相同，只是参数空间的维数升高，计算量增大。图像空间中的任意一个点都对应了参数空间中的一簇圆曲线。 ，其实是一个圆锥型。参数空间中的任意一个点对应了图像空间中的一个圆。\n3、椭圆\n椭圆有5个自由参数，所以它的参数空间是5维的，因此他的计算量非常大，所以提出了许多的改进算法。\n四、总结\n图像空间中的在同一个圆，直线，椭圆上的点，每一个点都对应了参数空间中的一个图形，在图像空间中这些点都满足它们的方程这一个条件，所以这些点，每个投影后得到的图像都会经过这个参数空间中的点。也就是在参数空间中它们会相交于一点。所以，当参数空间中的这个相交点的越大的话，那么说明元图像空间中满足这个参数的图形越饱满。越象我们要检测的东西。\nHough变换能够查找任意的曲线，只要你给定它的方程。Hough变换在检验已知形状的目标方面具有受曲线间断影响小和不受图形旋转的影响的优点，即使目标有稍许缺损或污染也能被正确识别。\n转自：http://blog.csdn.net/icerain_3321/article/details/1665280\nposted @ 2012-04-18 16:02 Hanson-jun 阅读(24) 评论(0) 编辑\n介绍n款计算机视觉库/人脸识别开源库/软件\n计算机视觉库 OpenCV\nOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...\n人脸识别 faceservice.cgi\nfaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。\nOpenCV的.NET版 OpenCVDotNet\nOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。\n人脸检测算法 jViolajones\njViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033\nJava视觉处理库 JavaCV\nJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...\n运动检测程序 QMotion\nQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。\n视频监控系统 OpenVSS\nOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。\n手势识别 hand-gesture-detection\n手势识别，用OpenCV实现\n人脸检测识别 mcvai-tracking\n提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...\n人脸检测与跟踪库 asmlibrary\nActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。\nLua视觉开发库 libecv\nECV 是 lua 的计算机视觉开发库(目前只提供linux支持)\nOpenCV的.Net封装 OpenCVSharp\nOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。\n3D视觉库 fvision2010\n基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...\n基于QT的计算机视觉库 QVision\n基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。\n图像特征提取 cvBlob\ncvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.\n实时图像/视频处理滤波开发包 GShow\nGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...\n视频捕获 API VideoMan\nVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。\n开放模式识别项目 OpenPR\nPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。\nOpenCV的Python封装 pyopencv\nOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...\n视觉快速开发平台 qcv\n计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。\n图像捕获 libv4l2cam\n对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出\n计算机视觉算法 OpenVIDIA\nOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;...\n高斯模型点集配准算法 gmmreg\n实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...\n模式识别和视觉库 RAVL\nRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。\n图像处理和计算机视觉常用算法库 LTI-Lib\nLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具\nOpenCV优化 opencv-dsp-acceleration\n优化了OpenCV库在DSP上的速度。\nC++计算机视觉库 Integrating Vision Toolkit\nIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV\n计算机视觉和机器人技术的工具包 EGT\nThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...\nOpenCV的扩展库 ImageNets\nImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。\nlibvideogfx\n视频处理、计算机视觉和计算机图形学的快速开发库。\nMatlab计算机视觉包 mVision\nMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。\nScilab的计算机视觉库 SIP\nSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。\nSTAIR Vision Library\nSTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。\nposted @ 2012-04-18 15:57 Hanson-jun 阅读(111) 评论(0) 编辑\nUIUC某童鞋收集的代码合集\nJia-Bin Huang童鞋收集，此童鞋毕业于国立交通大学，之前拍过很多CVPR举办地科罗拉多州的照片，这里大多为matlab code,\nlink: https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n包括：\nFeature Extraction：\nSIFT [1] [Demo program][SIFT Library] [VLFeat]\nPCA-SIFT [2] [Project]\nAffine-SIFT [3] [Project]\nSURF [4] [OpenSURF] [Matlab Wrapper]\nAffine Covariant Features [5] [Oxford project]\nMSER [6] [Oxford project] [VLFeat]\nGeometric Blur [7] [Code]\nLocal Self-Similarity Descriptor [8] [Oxford implementation]\nGlobal and Efficient Self-Similarity [9] [Code]\nHistogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]\nGIST [11] [Project]\nShape Context [12] [Project]\nColor Descriptor [13] [Project]\nPyramids of Histograms of Oriented Gradients [Code]\nSpace-Time Interest Points (STIP) [14] [Code]\nBoundary Preserving Dense Local Regions [15][Project]\nImage Segmentation：\nNormalized Cut [1] [Matlab code]\nGerg Mori’ Superpixel code [2] [Matlab code]\nEfficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]\nMean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]\nOWT-UCM Hierarchical Segmentation [5] [Resources]\nTurbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]\nQuick-Shift [7] [VLFeat]\nSLIC Superpixels [8] [Project]\nSegmentation by Minimum Code Length [9] [Project]\nBiased Normalized Cut [10] [Project]\nSegmentation Tree [11-12] [Project]\nEntropy Rate Superpixel Segmentation [13] [Code]\nObject Detection：\nA simple object detector with boosting [Project]\nINRIA Object Detection and Localization Toolkit [1] [Project]\nDiscriminatively Trained Deformable Part Models [2] [Project]\nCascade Object Detection with Deformable Part Models [3] [Project]\nPoselet [4] [Project]\nImplicit Shape Model [5] [Project]\nViola and Jones’s Face Detection [6] [Project]\nSaliency Detection\nItti, Koch, and Niebur’ saliency detection [1] [Matlab code]\nFrequency-tuned salient region detection [2] [Project]\nSaliency detection using maximum symmetric surround [3] [Project]\nAttention via Information Maximization [4] [Matlab code]\nContext-aware saliency detection [5] [Matlab code]\nGraph-based visual saliency [6] [Matlab code]\nSaliency detection: A spectral residual approach. [7] [Matlab code]\nSegmenting salient objects from images and videos. [8] [Matlab code]\nSaliency Using Natural statistics. [9] [Matlab code]\nDiscriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]\nLearning to Predict Where Humans Look [11] [Project]\nGlobal Contrast based Salient Region Detection [12] [Project]\nImage Classification\nPyramid Match [1] [Project]\nSpatial Pyramid Matching [2] [Code]\nLocality-constrained Linear Coding [3] [Project] [Matlab code]\nSparse Coding [4] [Project] [Matlab code]\nTexture Classification [5] [Project]\nMultiple Kernels for Image Classification [6] [Project]\nFeature Combination [7] [Project]\nSuperParsing [Code]\nImage Matting\nClosed Form Matting [Code]\nSpectral Matting [Project]\nLearning-based Matting [Code]\n等等等等。。。。\n大家可以去那个网址自己看。。。。\nposted @ 2012-04-18 15:53 Hanson-jun 阅读(49) 评论(0) 编辑\n图像处理方面的网站\nhttp://blog.damiles.com\nwww.bernardotti.it\nhttp://www.ohloh.net/tags/recognition\nhttp://www.diphernet.com/\nhttp://www.mat.ucsb.edu/projects/tater/\nhttp://enblend.sourceforge.net/\nhttp://www.infra.kth.se/courses/1N1652/\nhttp://www.csie.ntu.edu.tw/~b93082/VFX/hw2/vfx02.htm#t3\nhttp://graphics.cs.msu.ru/en/research/calibration/\nhttp://www.vlfeat.org/~vedaldi/\nhttp://svn.openframeworks.cc/browser/listing.php?repname=addons&path=%2FofxOpenCv%2Ftrunk%2FofxOpenCv%2F&rev=29&sc=1\nhttp://cvlab.epfl.ch/software/ferns/index.php\nhttp://staff.science.uva.nl/~rvalenti/index.php?content=projects\nhttp://mpac.ee.ntu.edu.tw/~ck/project_panorama/#Downloads\nhttp://www.sharewareconnection.com/titles/cross-stitch.htm\nhttp://mpac.ee.ntu.edu.tw/~sutony/vfx_stitching/pano.htm\nhttp://mpac.ee.ntu.edu.tw/people.php\nhttp://mpac.ee.ntu.edu.tw/index.php\nhttp://www.cse.cuhk.edu.hk/~csc5280/project3/RoyChan/index.htm\nhttp://personal.ie.cuhk.edu.hk/~gbq008/csc_project_3.htm\nhttp://graphics.cs.cmu.edu/courses/15-463/2008_fall/463.html\nhttp://www-2.cs.cmu.edu/%7ecdtwigg/\nhttp://cs-people.bu.edu/edwardaa/cs580/p1/p1.html#goals\nhttp://www.cs.toronto.edu/~smalik/2530/mosaic/results.html\nhttp://www.cs.princeton.edu/gfx/\nhttp://idea.hosting.lv/a/gfx/\nhttp://www.cs.toronto.edu/~esteger/mosaic/index.html\nhttp://home.so-net.net.tw/lioucy\nhttp://web.ics.purdue.edu/~kim497/\nCUDA:\nhttp://gforge.man.poznan.pl/gf/project/cudaopencv/scmsvn/\nhttp://wiki.livedoor.jp/mikk_ni3_92/d/CUDA::2%C3%CD%B2%BD::%CA%A3%BF%F4%CB%E7\nhttp://cudasample.net/\n一、研究群体\nhttp://www-2.cs.cmu.edu/~cil/vision.html\n这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。\nhttp://www.cmis.csiro.au/IAP/zimage.htm\n这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。\nhttp://www.via.cornell.edu/\n康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。\nhttp://www2.parc.com/istl/groups/did/didoverview.shtml\n有一个很有意思的项目：DID(文档图像解码)。\nhttp://www-cs-students.stanford.edu/\n斯坦福大学计算机系主页，自己找吧:(\nhttp://www.fmrib.ox.ac.uk/analysis/\n主要研究：Brain Extraction Tool,Nonlinear noise reduction,Linear Image Registration,\nAutomated Segmentation,Structural brain change analysis,motion correction,etc.\nhttp://www.cse.msu.edu/prip/\n这是密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。\nhttp://pandora.inf.uni-jena.de/p/e/index.html\n德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。\nhttp://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.html\nCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.\nhttp://cfia.gmu.edu/\nThe mission of the Center for Image Analysis is to foster multi-disciplinary research in image, multimedia and related technologies by establishing links\nbetween academic institutes, industry and government agencies, and to transfer key technologies to\nhelp industry build next\ngeneration commercial and military imaging and multimedia systems.\nhttp://peipa.essex.ac.uk/info/groups.html\n可以通过它来搜索全世界各地的知名的计算机视觉研究组(CV Groups)，极力推荐。\n二、图像处理GPL库\nhttp://www.ph.tn.tudelft.nl/~klamer/cppima.html\nCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。\nhttp://iraf.noao.edu/\nWelcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility, a general purpose software\nsystem for the reduction and analysis of astronomical data.\nhttp://entropy.brni-jhu.org/tnimage.html\n一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。\nhttp://sourceforge.net/projects/\n这是GPL软件集散地，到这里找你想要得到的IP库吧。\n三、搜索资源\n当然这里基本的搜索引擎还是必须要依靠的，比如Google等，可以到我常用的链接看看。下面的链接可能会节省你一些时间：\nhttp://sal.kachinatech.com/\nhttp://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml\n四、大拿网页\nhttp://www.ai.mit.edu/people/wtf/\n这位可是MIT人工智能实验室的BILL FREEMAN。大名鼎鼎！专长是：理解--贝叶斯模型。\nhttp://www.merl.com/people/brand/\nMERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”高手。\nhttp://research.microsoft.com/~ablake/\nCV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。\nhttp://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html\n这位牛人好像正在学习汉语，并且搜集了诸如“两只老虎(Two Tigers)”的歌曲，嘿嘿:)\n他的主页上面还有几个牛：Shumeet Baluja, Takeo Kanade。他们的Face Detection作的绝对是世界一流。他毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。\nhttp://www.ifp.uiuc.edu/yrui_ifp_home/html/huang_frame.html\n这位老牛在1963年就获得了MIT的博士学位！他领导的Image Lab比较出名的是指纹识别。\n--------------------------------------------------------------------------------\n下面这些是我搜集的牛群(大部分是如日中天的Ph.D们)，可以学习的是他们的Study Ways!\nFinn Lindgren(Sweden):Statistical image analysis http://www.maths.lth.se/matstat/staff/finn/\nPavel Paclik(Prague):statistical pattern recognition http://www.ph.tn.tudelft.nl/~pavel/\nDr. Mark Burge:machine learning and graph theory http://cs.armstrong.edu/burge/\nyalin Wang:Document Image Analysis http://students.washington.edu/~ylwang/\nGeir Storvik: Image analysis http://www.math.uio.no/~geirs/\nHeidorn http://alexia.lis.uiuc.edu/~heidorn/\nJoakim Lindblad:Digital Image Cytometry http://www.cb.uu.se/~joakim/index_eng.html\nS.Lavirotte: http://www-sop.inria.fr/cafe/Stephane.Lavirotte/\nSporring:scale-space techniques http://www.lab3d.odont.ku.dk/~sporring/\nMark Jenkinson:Reduction of MR Artefacts http://www.fmrib.ox.ac.uk/~mark/\nJustin K. Romberg:digital signal processing http://www-dsp.rice.edu/~jrom/\nFauqueur:Image retrieval by regions of interest http://www-rocq.inria.fr/~fauqueur/\nJames J. Nolan:Computer Vision http://cs.gmu.edu/~jnolan/\nDaniel X. Pape:Information http://www.bucho.org/~dpape/\nDrew Pilant:remote sensing technology http://www.geo.mtu.edu/~anpilant/index.html\n五、前沿期刊(TOP10)\n这里的期刊大部分都可以通过上面的大拿们的主页间接找到，在这列出主要是为了节省直接想找期刊投稿的兄弟的时间:)\nIEEE Trans. On PAMI http://www.computer.org/tpami/index.htm\nIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htm\nPattern Recognition http://www.elsevier.com/locate/issn/00313203\nPattern Recognition Letters http://www.elsevier.com/locate/issn/01678655\n神经网络\nNeural Networks Tutorial Review\nhttp://hem.hj.se/~de96klda/NeuralNetworks.htm\nftp://ftp.sas.com/pub/neural/FAQ.html\nImage Compression with Neural Networks\nhttp://www.comp.glam.ac.uk/digimaging/neural.htm\nBackpropagator's Review\nhttp://www.dontveter.com/bpr/bpr.html\nBibliographies on Neural Networks\nhttp://liinwww.ira.uka.de/bibliography/Neural/\nIntelligent Motion Control with an Artificial Cerebellum\nhttp://www.q12.org/phd.html\nKernel Machines\nhttp://www.kernel-machines.org/\nSome Neural Networks Research Organizations\nhttp://www.ieee.org/nnc/\nhttp://www.inns.org/\nNeural Network Modeling in Vision Research\nhttp://www.rybak-et-al.net/nisms.html\nNeural Networks and Machine Learning\nhttp://learning.cs.toronto.edu/\nNeural Application Software\nhttp://attrasoft.com\nNeural Network Toolbox for MATLAB\nhttp://www.mathworks.com/products/neuralnet/\nNetlab Software\nhttp://www.ncrg.aston.ac.uk/netlab/\nKunama Systems Limited\nhttp://www.kunama.co.uk/\nComputer Vision\nComputer Vision Homepage, Carnegie Mellon University\nwww.cs.cmu.edu/~cil/vision.html\nAnnotated Computer Vision Bibliography\nhttp://iris.usc.edu/Vision-Notes/bibliography/contents.html\nhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.html\nLawrence Berkeley National Lab Computer Vision and Robotics Applications\nhttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.html\nCVonline by University of Edinburgh\nThe Evolving, Distributed, Non-Proprietary, On-Line Compendium of Computer Vision, www.dai.ed.ac.uk/CVonline\nComputer Vision Handbook, www.cs.hmc.edu/~fleck/computer-vision-handbook\nVision Systems Courseware\nwww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.html\nResearch Activities in Computer Vision\nhttp://www-syntim.inria.fr/syntim/analyse/index-eng.html\nVision Systems Acronyms\nwww.vision-systems-design.com/vsd/archive/acronyms.html\nDictionary of Terms in Human and Animal Vision\nhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.html\nMetrology based on Computer Vision\nwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html\nDigital Photography\nDigital Photography, Scanning, and Image Processing\nwww.dbusch.com/scanners/scanners.html\nEducational Resources, Universities\nCenter for Image Processing in Education\nwww.cipe.com\nLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technology\nhttp://wally2.rit.edu/pubs/guides/imagingcall.html\nMathematical Experiences through Image Processing, University of Washington\nwww.cs.washington.edu/research/metip/metip.html\nVismod Tech Reports and Publications, MIT\nhttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemaker\nVision Lab PhD dissertation list, University of Antwerp\nhttp://wcc.ruca.ua.ac.be/~visielab/theses.html\nINRIA (France) Research Projects: Human-Computer Interaction, Image Processing, Data Management, Knowledge Systems\nwww.inria.fr/Themes/Theme3-eng.html\nImage Processing Resources\nhttp://eleceng.ukc.ac.uk/~rls3/Contents.htm\nPublications of Carsten Steger\nhttp://www9.informatik.tu-muenchen.de/people/steger/publications.html\nFAQs\ncomp.dsp FAQ\nwww.bdti.com/faq/dsp_faq.htm\nRobotics FAQ\nwww.frc.ri.cmu.edu/robotics-faq\nWhere's the sci.image.processing FAQ?\nwww.cc.iastate.edu/olc_answers/packages/graphics/sci.image.processing.faq.html\ncomp.graphics.algorithms FAQ, Section 3, 2D Image/Pixel Computations\nwww.exaflop.org/docs/cgafaq\nAstronomical Image Processing System FAQ\nwww.cv.nrao.edu/aips/aips_faq.html\n来自: http://hi.baidu.com/jiamn/blog/item/aaa063f9ae34141d6c22ebce.html\nposted @ 2012-04-18 15:48 Hanson-jun 阅读(45) 评论(0) 编辑\n计算机视觉文献与代码资源及资料\n下面是前端时间搜集整理的一些和计算机视觉、模式识别的资源，拿出来与大家分享下。以后，我将把图像处理真正的作为我的兴趣来玩玩了，也许不把研究作为谋生的手段，会更好些。\n标题\n作者\n主题\n关键字\n类别\n来源\n备注\nnipsfast.ppt\nNando de Freitas\nN-Body problems in learning\nFast N-Body Learning\nPpt\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nnipsfgtf.ppt\nRamani Duraiswami\nFast Multipole Methods Fast Gaussian Transform\nFM and FGT\nppt\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nGray.pdf/ppt\nAlex Gray\nStatistical N-Body/Proximity Data Structures\nN-Body and Data Structures\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\ndt-nips04.pdf/ppt\nDan Huttenlocher\nFast Distance Transforms\nFDT\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nHigh.pdf/ppt\nAlexander Gray\nFast high-dimensional function integration\nFast integration\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nFast04.pdf/ppt\nDavid Lowe\nFast high-dimensional feature indexing for object recognition\nFeature indexing\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nihler-fast.pdf/ppt\nAlexander lhler\nFast methods and non-parametric BP\nNon-parametric BP\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nfastview.pdf\nDustin Lang\nComparing fast methods\nOverview fast methods\npdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nnbody_methods.tar.gz\ncode\nhttp://www.cs.ubc.ca/~awll/nbody_methods.html\ndemo_rbpf_gauss.tar\nRao Blackwellised particle filtering for conditionally Gaussian Models\nparticle filtering for conditionally\ncode\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\ndemorbpfdbn.tar.gz\nRao Blackwellised Particle Filtering\ncode\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nhttp://www.cs.ubc.ca/~nando/software.html\nupf_demos.tar.gz\nUnscented Particle Filter\nParticle Filter\ncode\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nBPF_1_3.zip\nBoosted Particle Filter\nTracking\ncode\nhttp://www.cs.ubc.ca/~okumak/research.html\n1\nflyer_14_800.mpg\nSource image\nDatabase\nImage\nhttp://www.cs.ubc.ca/~okumak/research.html\n1\ntrans_flyer_14_800.mpg\nimage transformed\nDatabase\nImage\nhttp://www.cs.ubc.ca/~okumak/research.html\n1\nLBP.c/h\nTopi Mäenpää\nLBP operator\nTexture\ncode\nhttp://www.ee.oulu.fi/~topiolli/cpplibs/files/\ncalibr_v30.zip\nCamera Calibration\nComputer vision\ncode\nhttp://www.ee.oulu.fi/mvg/page/camera_calibration\n_toolbox_for_matlab\n2\nLEAR(Learning and Recognition in Vision\nCommon dataset\nHuman/car horse soccer human actions\ndataset\nhttp://lear.inrialpes.fr/data\n3\nLic.zip/highlight.zip\nRobby T. Tan\nColor Constancy Through Inverse Intensity Chromaticity Space\nHighlight Removal from single image\ncode\nhttp://www.commsp.ee.ic.ac.uk/~rtan/\n2008_oxford_fog.pdf\nRobby T. Tan\nDefog\nDefog from single\npdf\nhttp://www.commsp.ee.ic.ac.uk/~rtan/\n08_cvpr.pdf\nRobby T. Tan\nDefog\nDefog from single\npdf\nhttp://www.commsp.ee.ic.ac.uk/~rtan/\nRetinex_frankle_mccann\nRetinex\nCode\nhttp://www.cs.sfu.ca/~colour/publications/IST-2000/\nSome\nRetinex_maccann99\nRetinex\ncode\nhttp://www.cs.sfu.ca/~colour/publications/IST-2000/\npictures\nGamut.tar.bz2\nRetinex\ncode\nhttp://kobus.ca/research/programs/colour_constancy/index.html\nVideo.avi/dehaze.m\ndehazing\nRaanan Fattal\ncode\nhttp://www.cs.huji.ac.il/~raananf/projects/defog/index.html\nMPTK-Windows-bin-0-5-6-beta.zip\nMatching pursuit(MP)\nAlogrithm\nCNRS\nCode\nhttp://mptk.irisa.fr/downloads\ngenerateDictionaries.txt\nGenerateGabor\nAlogrithm\ncode\nhttp://www.scholarpedia.org/article/Matching_pursuit\nNotes:\n1.      视频和源码都是对应的文章的：\nKenji Okuma, Ali Taleghani, Nando De Freitas, Jim Little, David G. Lowe. Boosted Particle Filter: Multitarget Detection and Tracking. the European Conference on Computer Vision(ECCV), May 2004.\n2.      该网站下面还有其他一些资源可以下载：\nhttp://www.ee.oulu.fi/mvg/page/downloads\n是个研究组织：http://lear.inrialpes.fr/ ， 除此之外，还有一些源码。\n计算机视觉文献与代码资源\nCVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htm\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm\n李子青的大作：\nMarkov Random Field Modeling in Computer Vision\nhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.html\nHandbook of Face Recognition (PDF)\nhttp://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf\n张正友的有关参数鲁棒估计著作：\nParameter Estimation Techniques:A Tutorial with Application to Conic Fitting\nhttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.html\nAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Vision\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007\n有关马尔可夫蒙特卡罗方法的资料：\nAn introduction to Markov chain Monte Carlo\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.html\nMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05\nhttp://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm\n有关独立成分分析（Independent Component Analysis , ICA）的资料：\nAn ICA-Page\nhttp://www.cnl.salk.edu/~tony/ica.html\nFast ICA\nhttp://www.cis.hut.fi/projects/ica/fastica/\nThe Kalman Filter (介绍卡尔曼滤波器的终极网页)\nhttp://www.cs.unc.edu/~welch/kalman/index.html\nCached k-d tree search for ICP algorithms\nhttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html\n几个计算机视觉研究工具\nMachine Vision Toolbox for Matlab\nhttp://www.petercorke.com/Machine%20Vision%20Toolbox.html\nMatlab and Octave Function for Computer Vision and Image Processing\nhttp://www.csse.uwa.edu.au/~pk/research/matlabfns/\nBayes Net Toolbox for Matlab\nhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.html\nOpenCV (Chinese)\nhttp://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\nGandalf (A Computer Vision and Numerical Algorithm Labrary)\nhttp://gandalf-library.sourceforge.net/\nCMU Computer Vision Home Page\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\nMachine Learning Resource Links\nhttp://www.cse.ust.hk/~ivor/resource.htm\nThe Bayesian Filtering Library\nhttp://www.orocos.org/bfl\nOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)\nhttp://of-eval.sourceforge.net/\nMATLAB code for ICP algorithm\nhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html\n牛人主页：\n朱松纯（Song-Chun Zhu）\nhttp://www.stat.ucla.edu/~sczhu/\nDavid Lowe (SIFT) (很帅的一个老头哦 ^ ^)\nhttp://www.cs.ubc.ca/~lowe/\nAndrea Vedaldi (SIFT)\nhttp://vision.ucla.edu/~vedaldi/index.html\nPedro F. Felzenszwalb\nhttp://people.cs.uchicago.edu/~pff/\nDougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)\nhttp://mesh.brown.edu/dlanman/courses.html\nJianbo Shi (Ncuts 的始作俑者)\nhttp://www.cis.upenn.edu/~jshi/\nActive Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)\nhttp://www.robots.ox.ac.uk/ActiveVision/index.html\nJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）\nhttp://www.cse.msu.edu/~weng/\n测试图片或视频：\nMiddlebury College‘s Stereo Vision Data Set\nhttp://cat.middlebury.edu/stereo/data.html\nIntelligent Vehicle:\nIVSource\nwww.ivsoruce.net\nRobot Car\nhttp://www.plyojump.com/robot_cars.html\nHow to Build a Robot: The Computer Vision Part\nhttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml\n计算机视觉应关注的资源\n来自美国帝腾大学的链接。\nCamera Calibration Links to toolboxes (mostly MATLAB) for camera calibration.\nPaul Debevec. Modeling and Rendering Architecture from Photographs.\nMarc Pollefeys, Tutorial on 3D Modeling from Images,, ECCV 2000,\nAvailable here: notes (12.1MB pdf)\nRichard Szeliski NIPS 2004 Tutorial on Acquiring Detailed 3D Models From Images and Video,\nAvailable here: slides (37.6 MB, ppt)\nPeter Corke did his thesis work on visual servoing for robot applications and has authored a robotics toolkit and vision toolkit for MATLAB.\nlocal copy of thesis: Corke thesis (4.36 MB, pdf)\nrobot toolkit: robot.zip (568 KB, zip)\nvision toolkit: mv.zip (1.08 MB, zip)\nP. D. Kovesi., MATLAB Functions for Computer Vision and Image Analysis.\nSchool of Computer Science & Software Engineering, The University of Western Australia.\nAvailable locally as a zip archive MatlabFns.zip (4.8 MB, updated 21 May 2005)\nPhilip Torr, among many other contributions, submitted a Structure and motion toolkit in Matlab to the MathSoft File Exhange.\nLocal copy here: torrsam.zip (2.4 MB, zip).\n本文引用地址：http://blog.sciencenet.cn/home.php?mod=space&uid=454498&do=blog&id=456240\nposted @ 2012-04-18 15:45 Hanson-jun 阅读(54) 评论(0) 编辑\n描述子距离种类\n1.hausdorff距离\n微分动力系统原理 这本书里有介绍\nHausdorff距离是描述两组点集之间相似程度的一种量度，它是两个点集之间距离的一种定义形式：假设有两组集合A={a1,…,ap},B={b1,…,bq},则这两个点集合之间的Hausdorff距离定义为H(A,B)=max(h(A,B),h(B,A)) (1)\n其中,\nh(A,B)=max（a∈A）min（b∈B）‖a-b‖ (2)\nh(B,A)=max（b∈B）min（a∈A）‖b-a‖ (3)\n‖·‖是点集A和B点集间的距离范式(如:L2或Euclidean距离).\n这里,式(1)称为双向Hausdorff距离,是Hausdorff距离的最基本形式;式(2)中的h(A,B)和h(B,A)分别称为从A集合到B集合和从B集合到A集合的单向Hausdorff距离.即h(A,B)实际上首先对点集A中的每个点ai到距离此点ai最近的B集合中点bj之间的距离‖ai-bj‖进行排序,然后取该距离中的最大值作为h(A,B)的值.h(B,A)同理可得.\n由式(1)知,双向Hausdorff距离H(A,B)是单向距离h(A,B)和h(B,A)两者中的较大者,它度量了两个点集间的最大不匹配程度\n2.欧式距离\n欧几里得距离定义： 欧几里得距离（ Euclidean distance）也称欧式距离，它是一个通常采用的距离定义，它是在m维空间中两个点之间的真实距离。\n在二维和三维空间中的欧式距离的就是两点之间的距离，二维的公式是\nd = sqrt((x1-x2)^+(y1-y2)^)\n三维的公式是\nd=sqrt((x1-x2)^+(y1-y2)^+(z1-z2)^)\n推广到n维空间，欧式距离的公式是\nd=sqrt( ∑(xi1-xi2)^ ) 这里i=1,2..n\nxi1表示第一个点的第i维坐标,xi2表示第二个点的第i维坐标\nn维欧氏空间是一个点集,它的每个点可以表示为(x(1),x(2),...x(n)),其中x(i)(i=1,2...n)是实数,称为x的第i个坐标,两个点x和y=(y(1),y(2)...y(n))之间的距离d(x,y)定义为上面的公式.\n欧氏距离看作信号的相似程度。 距离越近就越相似，就越容易相互干扰，误码率就越高。\n所谓欧氏距离变换，是指对于一张二值图像（再次我们假定白色为前景色，黑色为背景色），将前景中的像素的值转化为该点到达最近的背景点的距离。\n欧氏距离变换在数字图像处理中的应用范围很广泛，尤其对于图像的骨架提取，是一个很好的参照。\n所谓欧氏距离变换，是指对于一张二值图像（再次我们假定白色为前景色，黑色为背景色），将前景中的像素的值转化为该点到达最近的背景点的距离。\n欧氏距离变换在数字图像处理中的应用范围很广泛，尤其对于图像的骨架提取，是一个很好的参照。\n========\n欧氏距离：（∑（Xi-Yi）2）1/2，即两项间的差是每个变量值差的平方和再平方根，目的是计算其间的整体距离即不相似性。\n我们熟悉的 欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中， 经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，有时需要采用不同的距离函数。\n3.马氏距离：\n马氏距离是由印度统计学家马哈拉诺比斯(P. C. Mahalanobis)提出的，表示数据的协方差距离。它是一种有效的计算两个未知样本集的相似度的方法。与欧式距离不同的是它考虑到各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的）并且是尺度无关的(scale-invariant)，即独立于测量尺度。对于一个均值μ，为协方差矩阵为Σ的多变量向量,其马氏距离为((x-μ)'Σ^(-1)(x-μ))^(1/2)。\n马氏距离也可以定义为两个服从同一分布并且其协方差矩阵为Σ的随机变量与的差异程度:\n如果协方差矩阵为单位矩阵,那么马氏距离就简化为欧式距离,如果协方差矩阵为对角阵,则其也可称为正规化的欧氏距离'.\n其中σi 是 xi 的标准差.\n马氏优缺点：\n1）马氏距离的计算是建立在总体样本的基础上的，这一点可以从上述协方差矩阵的解释中可以得出，也就是说，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；\n2）在计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离计算即可。\n3）还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如三个样本点（3，4），（5，6）和（7，8），这种情况是因为这三个样本在其所处的二维空间平面内共线。这种情况下，也采用欧式距离计算。\n4）在实际应用中“总体样本数大于样本的维数”这个条件是很容易满足的，而所有样本点出现3）中所描述的情况是很少出现的，所以在绝大多数情况下，马氏距离是可以顺利计算的，但是马氏距离的计算是不稳定的，不稳定的来源是协方差矩阵，这也是马氏距离与欧式距离的最大差异之处。\n优点：它不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。缺点：它的缺点是夸大了变化微小的变量的作用。\n如果用dij表示第i个样品和第j个样品之间的距离，那么对一切i，j和k，dij应该满足如下四个条件：\n①当且仅当i=j时，dij=0\n②dij>0\n③dij=dji（对称性）\n④dij≤dik+dkj（三角不等式）\n显然，欧氏距离满足以上四个条件。满足以上条件的函数有多种，本节将要用到的马氏距离也是其中的一种。\n第i个样品与第j个样品的马氏距离dij用下式计算：\ndij =((x i 一x j)TS-1(x i一xj) )1/2(T、-1、1/2都是上标)\n其中，T表示转置，x i 和x j分别为第i个和第j个样品的m个指标所组成的向量，S为样本协方差矩阵。\n\n本文引用地址：http://blog.sciencenet.cn/home.php?mod=space&uid=261330&do=blog&id=526762\nposted @ 2012-04-18 15:44 Hanson-jun 阅读(40) 评论(0) 编辑\n涉足计算机视觉领域要知道的\n做机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法就是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个实际应用的项目，边做边写文章。 做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。\n依照下面目录整理：\n[1]研究群体(国际国内)[2]专家主页[3]前沿国际国内期刊与会议[4]搜索资源[5]GPL软件资源\n一、研究群体\n用来搜索国际知名计算机视觉研究组(CV Groups)：\n国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html\n美国计算机视觉研究组清单 http://peipa.essex.ac.uk/info/groups.html#USA\nhttp://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html\n这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等。\n卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/\n卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.html\n还有几个实验室：\nCalibrated Imaging Laboratory 图像\nDigital Mapping Laboratory 映射\nInteractive Systems Laboratory 互动\nVision and Autonomous Systems Center视觉自适应\nhttp://www.via.cornell.edu/\n康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。\nCornell University——Robotics and Vision group\nhttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页\n1. http://white.stanford.edu/\n2. http://vision.stanford.edu/\n3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室\nThe Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...\nVision and Imaging Science and Technology\nhttp://www.fmrib.ox.ac.uk/analysis/\n主要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.\nhttp://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。\n美国密歇根州大学认知模型和图像处理实验室\nThe Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/\nhttp://pandora.inf.uni-jena.de/p/e/index.html\n德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。\n柏林大学 http://www.cv.tu-berlin.de/\n德国波恩大学视觉和认识模型小组\nComputer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/\nhttp://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.html\nCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.\nhttp://cfia.gmu.edu/\nThe mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.\n英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境 (照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk/Research/Digitalmedia/\n而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了\nhttp://www.cmis.csiro.au/IAP/zimage.htm\n这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。\n麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/\nAI Laboratory Computer Vision group\nCenter for Biological and Computational Learning\nMedia Laboratory， Vision and Modeling Group\nPerceptual Science group\nUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.html\nhttp://www.cs.berkeley.edu.ilste ... n/vision_group.html\n加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/\nUCLA(加州大学洛杉矶分校) http://vision.ucla.edu/视觉实验室\n英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室\n美国南加州大学智能机器人和智能系统研究所University of Southern California， Los Angeles\nIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with... http://iris.usc.edu/ Computer Vision 实验室\n美国南加州大学计算机视觉实验室介绍：\nComputer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html\n英国约克大学高级计算机结构神经网络小组\nThe Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/\n瑞士戴尔莫尔感知人工智能研究所\nIDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/\n英国萨里大学视觉，语言和信号处理中心\nThe Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/\n美国阿默斯特马萨诸塞州立大学计算机视觉实验室\nThe Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.edu\nUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics\n美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室\nIncludes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/\nComputer Vision and Robotics Laboratory\nVision Interfaces and Systems Laboratory (VISLab)\n英国伯明翰大学计算机科学学校视觉研究小组\nThe vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/\n微软研究院机器学习与理解研究小组 / 计算机视觉小组\nThe research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/\nhttp://research.microsoft.com/en-us/groups/vision/\n微软公司的文献：http://research.microsoft.com/research/pubs\n微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.\n瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/\n感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。\n澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/\n美国北卡大学：http://www.cs.unc.edu/~marc/\n法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。\n比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/\n据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.\n美国明德http://vision.middlebury.edu/stereo/\n以下含有非顶尖美国学校研究组，没有链接(个别的上面已经提到)，供参考。\nAmerinex Applied Imaging， Inc.\nBoston University\nImage and Video Computing Research group\nUniversity of California at Santa Barbara加州大学芭芭拉分校\nVision Research Lab\nUniversity of California at San Diego加州大学圣迭戈分校\nComputer Vision & Robotics Research Laboratory\nVisual Computing laboratory\nUniversity of California at Irvine加州大学欧文分校，加州南部一城，在圣安娜东南，\nComputer Vision laboratory\nUniversity of California， Riverside加州大学河滨分校\nVisualization and Intelligent Systems Laboratory (VISLab)\nUniversity of California at Santa Cruz\nPerceptual Science Laboratory\nCaltech (加州理工)\nVision group\nUniversity of Central Florida\nComputer Vision laboratory\nUniversity of Florida\nCenter for Computer Vision and Visualization\nColorado State University\nComputer Vision group\nColumbia University\nAutomated Vision Environment (CAVE)\nRobotics group\nUniversity of Georgia， Athens\nVisual and Parallel Computing Laboratory\nHarvard University（哈佛）\nRobotics Laboratory\nUniversity of Illinois at Urbana-Champaign\nRobotics and Computer Vision\nUniversity of Iowa\nDivision of Physiologic Imaging\nJet Propulsion Laboratory\nMachine Vision and Tracking Sensors group\nKhoral Research， Inc\nLawrence Berkeley Laboratories\nImaging and Collaborative Computing Group\nImaging and Distributed Computing\nLehigh University\nImage Processing and Pattern Analysis Lab\nVision And Software Technology Laboratory\nUniversity of Louisville\nComputer Vision and Image Processing Lab\nUniversity of Maryland\nComputer Vision Laboratory\nUniversity of Miami\nUnderwater Vision and Imaging Laboratory\nUniversity of Michigan密歇根\nAI Laboratory\nMichigan State University 密歇根州立\nPattern Recognition and Image Processing laboratory\nEnvironmental Research Institute of Michigan (ERIM) 密歇根大学有汽车车身检测研究\nUniversity of Missouri-Columbia\nComputational Intelligence Research Laboratory\nNEC\nComputer Vision and Image Processing\nUniversity of Nevada\nComputer Vision Laboratory\nNotre-Dame University\nVision-Based Robotics using Estimation\nOhio State University\nSignal Analysis and Machine Perception Laboratory\nUniversity of Pennsylvania\nGRASP laboratory\nMedical Image Processing group\nVision Analysis and Simulation Technologies (VAST) Laboratory\nPenn State University 宾夕法尼亚大学\nComputer Vision\nPrecision Digital Images\nPurdue University普渡大学\nRobot Vision laboratory\nVideo and Image Processing Laboratory (VIPER)\nRensselaer Polytechnic Institute (RPI)\nComputer Science Vision\nUniversity of Rochester\nCenter for Electronic Imaging Systems\nVision and Robotics laboratory\nRutgers University (The State University of New Jersey)\nImage Understanding Lab\nUniversity of Southern California\nComputer Vision\nUniversity of South Florida\nImage Analysis Research group\nStanford Research Institute International (SRI)\nRADIUS -- Research and Development for Image Understanding Systems\nThe Perception program at SRI's AI Center\nSUNY at Stony Brook\nComputer Vision Lab\nUniversity of Tennessee\nImaging， Robotics and Intelligent Systems laboratory\nUniversity of Texas， Austin\nLaboratory for Vision Systems\nUniversity of Utah\nCenter for Scientific Computing and Imaging\nRobotics and Computer Vision\nUniversity of Virginia\nComputer Vision Research (CS)\nUniversity of Washington\nImage Computing Systems Laboratory\nInformation Processing Laboratory\nCVIA Laboratory\nUniversity of West Florida\nImage Analysis/Robotics Research Laboratory\nUniversity of Wisconsin\nComputer Vision group\nVanderbilt University\nCenter for Intelligent Systems\nWashington State University\nImaging Research laboratory\nWright-Patterson\nModel-Based Vision laboratory\nWright State University\nIntelligent Systems Laboratory\nUniversity of Wyoming\nWyoming Image and Signal Processing Research (WISPR)\nYale University\nComputational Vision Group http://www.cs.yale.edu/\nSchool of Medicine， Image Processing and Analysis group\n国内：\n中科院模式识别国家重点实验室 http://www.nlpr.ia.ac.cn/English/rv/mainpage.html\n虹膜识别、掌纹识别、人脸识别、\n莲花山http://www.stat.ucla.edu/~sczhu/Lotus/\n天津大学精密测试技术及仪器国家重点实验室\n研究方向包括：激光及光电测试技术、传感及测量信息技术、微纳测试与制造技术、制造质量控制技术。该实验室是国内精密测试领域惟一的国家重点实验室。\n“智能微系统及其集成应用技术”、“微结构光学测试技术”、“油气储运安全检测技术”、“先进制造中的视觉测量及其关键技术”、“正交偏振激光器原理、特性及其在精密计量中的应用研究”等5项代表性成果（07.3）。\n中科院长春光机所 http://www.ciomp.ac.cn/ny/keyan.asp\n中科院沈阳自动化所http://www.sia.ac.cn/index.php\n中科院西安光机所http://www.opt.ac.cn/yanjiushi/gpcxjs1.htm\n北京大学智能科学系http://www.cis.pku.edu.cn/vision/vision.htm\n三维视觉计算与机器人，生物特征识别与图像识别\n二、专家网页\nhttp://www.ai.mit.edu/people/wtf/\n这位可是MIT人工智能实验室的BILL FREEMAN。专长是：理解--贝叶斯模型。\nhttp://www.merl.com/people/brand/\nMERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”。\nhttp://research.microsoft.com/~ablake/\nCV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。\nhttp://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html\n这位专家好像正在学习汉语，主页并且搜集了诸如“两只老虎(Two Tigers)”的歌曲。\n他的主页上面还有几个专家：Shumeet Baluja， Takeo Kanade。他们的Face Detection作的绝对是世界一流。毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。\n三、前沿国际国内期刊与会议\n这里的期刊大部分都可以通过上面的专家们的主页间接找到\n1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题\n1. 国际会议\n现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。\nICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。\nCVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。\nICIP—\nBMVC—\nMVA—\n国际模式识别会议(ICPR )：\n亚洲计算机视觉会议(ACCV)：\n2.国际期刊\n以计算机视觉为主要内容之一的国际刊物也有很多，如:\nInternational Journal of Computer Vision\nIEEE Trans. On PAMI http://www.computer.org/tpami/index.htm\nIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htm\nPattern Recognition http://www.elsevier.com/locate/issn/00313203\nPattern Recognition Letters http://www.elsevier.com/locate/issn/01678655\nIEEE Trans. on Robotics and Automation，\nIEEE TPAMI\nIEEE TIP\nCVGIP Computer Vision. Graphics and Image Processing，\nVisual Image Computing，\nIJPRAI(Internatiorial Journat of Pattern Recognition and Artificial Intelligence)\n众所周知， computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议，它们档次差不多，都应该在一流会议行列， 没有必要给个高下。有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR，某些英国的人甚至认为BMVC好于CVPR。简言之， 三个会议差不多， 各有侧重和偏好。\n笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话， 想知道某个领域在做些什么，找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次，各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。\n就录取率而言， 三会都有波动。 如ICCV2001录取率>30%，且出现两个人(华人)各有三篇第一作者的paper的情况， 这在顶级牛会是不常见的 (灌水嫌疑)。 但是， ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高，从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高， 反之偏低，近几年三大会议的投稿数量全部超过1000， 相对2000年前， 三会录取率均大幅度降低，最大幅度50%->20%。 对录取率走势感兴趣的朋友， 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http://www.adaptivebox.net/research/bookmark/CICON_stat.html.\n显然， 投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic，而cvpr会收少量的pattern recognition paper， 如finger print等，但是不收和image/video完全不占边的pr paper，如speech recognition等。我一个朋友曾经review过一篇投往CVPR的speech的paper， 三个reviewer一致拒绝，其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。 就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。\n以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。 避免做无用功，选择切合的topic，改善presentation， 注意格式 (遵守规定的模板)， 我想这是很多新手需要注意的问题。如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视， 这是相当不值得的。\n3.国内期刊\n自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。\n4.神经网络\n神经网络-Neural Networks Tutorial Review\nhttp://hem.hj.se/~de96klda/NeuralNetworks.htm\nftp://ftp.sas.com/pub/neural/FAQ.html\nImage Compression with Neural Networks\nhttp://www.comp.glam.ac.uk/digimaging/neural.htm\nBackpropagator's Review\nhttp://www.dontveter.com/bpr/bpr.html\nBibliographies on Neural Networks\nhttp://liinwww.ira.uka.de/bibliography/Neural/\nIntelligent Motion Control with an Artificial Cerebellum\nhttp://www.q12.org/phd.html\nKernel Machines\nhttp://www.kernel-machines.org/\nSome Neural Networks Research Organizations\nhttp://www.ieee.org/nnc/\nhttp://www.inns.org/\nNeural Network Modeling in Vision Research\nhttp://www.rybak-et-al.net/nisms.html\nNeural Networks and Machine Learning\nhttp://learning.cs.toronto.edu/\nNeural Application Software\nhttp://attrasoft.com\nNeural Network Toolbox for MATLAB\nhttp://www.mathworks.com/products/neuralnet/\nNetlab Software\nhttp://www.ncrg.aston.ac.uk/netlab/\nKunama Systems Limited http://www.kunama.co.uk/\n5.Computer Vision(计算机视觉)\nAnnotated Computer Vision Bibliography\nhttp://iris.usc.edu/Vision-Notes/bibliography/contents.html\nhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.html\nLawrence Berkeley National Lab Computer Vision and Robotics Applications\nhttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.html\nCVonline by University of Edinburgh\nThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision， www.dai.ed.ac.uk/CVonline\nComputer Vision Handbook，\nwww.cs.hmc.edu/~fleck/computer-vision-handbook\nVision Systems Courseware\nwww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.html\nResearch Activities in Computer Vision\nhttp://www-syntim.inria.fr/syntim/analyse/index-eng.html\nVision Systems Acronyms\nwww.vision-systems-design.com/vsd/archive/acronyms.html\nDictionary of Terms in Human and Animal Vision\nhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.html\nMetrology based on Computer Vision\nwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html\n6.Digital Photography 数字图像\nDigital Photography， Scanning， and Image Processing\nwww.dbusch.com/scanners/scanners.htm l\n7.Educational Resources， Universities 教育资源，大学\nCenter for Image Processing in Education\nwww.cipe.com\nLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technology\nhttp://wally2.rit.edu/pubs/guides/imagingcall.html\nMathematical Experiences through Image Processing， University of Washington\nwww.cs.washington.edu/research/metip/metip.html\nVismod Tech Reports and Publications， MIT\nhttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemaker\nVision Lab PhD dissertation list， University of Antwerp\nhttp://wcc.ruca.ua.ac.be/~visielab/theses.html\nINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systems\nwww.inria.fr/Themes/Theme3-eng.html\nImage Processing Resources\nhttp://eleceng.ukc.ac.uk/~rls3/Contents.htm\nPublications of Carsten Steger\nhttp://www9.informatik.tu-muench ... r/publications.html\n8.FAQs（常见问题）\ncomp.dsp FAQ\nwww.bdti.com/faq/dsp_faq.htm\nRobotics FAQ\nwww.frc.ri.cmu.edu/robotics-faq\nWhere's the sci.image.processing FAQ?\nwww.cc.iastate.edu/olc_answers/p ... processing.faq.html\ncomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computations\nwww.exaflop.org/docs/cgafaq\nAstronomical Image Processing System FAQ\nwww.cv.nrao.edu/aips/aips_faq.html\n四、搜索资源\nhttp://sal.kachinatech.com/\nhttp://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学\nGoogle输入：computer vision 或computer vision groups可以获得很多结果\n网络资源：\nCVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表\nComputer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库\n视觉论文搜索：Paper search\nhttp://www.researchindex.com\n五、图像处理GPL库（代码库图像库等）\nhttp://www.ph.tn.tudelft.nl/~klamer/cppima.html\nCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。\nhttp://iraf.noao.edu/\nWelcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical data\nhttp://entropy.brni-jhu.org/tnimage.html\n一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。\nhttp://sourceforge.net/projects/\n这是GPL软件集散地，可以搜索IP库。\n国内的CSDN http://www.csdn.net/\n转载：http://blog.sciencenet.cn/home.php?mod=space&uid=509980&do=blog&id=436782\n原文：http://blog.csdn.net/shaoshuaiche/article/details/16850167#t9"}
{"content2":"色情内容在中国一直处于严格的监管，即使这样，互联网上还是很容易就能访问到色情内容。还记得曾经的“绿坝-花季护航”软件么？由于其识别效果差、软件不稳定，最后不了了之，浪费了大量的人力和金钱。\n随着计算机视觉和深度学习的发展，算法已经成熟，利用人工智能，我们能够更加精确的识别色情内容。现在有很多云服务商提供鉴黄服务，通过集成鉴黄API到产品中，就可以给产品增加色情过滤功能。这种模式对于大多数互联网产品非常有效，可以极大的降低运营风险（对于博客、图床、直播等服务提供商而言，过滤色情、暴力等内容是重中之重）。但对于一部分产品而言，这种模式存在一些不足：\n鉴别图片、视频内容等必须通过网络服务进行，响应速度难以保证;\n通常鉴黄服务按次或者按照流量计费，对于个人开发者而言，有成本负担;\nopen_nsfw\n现在有个好消息，雅虎开源了其深度学习色情图片检测模型open_nsfw，项目地址: http://github.com/yahoo/open_nsfw，这里的NSFW代表Not Suitable for Work。项目提供了基于caffe的深度神经网络模型和一个python脚本，可以供测试：\npython ./classify_nsfw.py \\ --model_def nsfw_model/deploy.prototxt \\ --pretrained_model nsfw_model/resnet_50_1by2_nsfw.caffemodel \\ test_image.jpg\n定义NSFW内容是非常主观的，在某个国家或地区会引起反感的内容可能在另一个地方完全没问题。所以，open_nsfw模型只关注一种类型的NSFW内容：色情图片。但需要注意的是，该模型不能解决草图、漫画、文本等内容的识别。\n色情图片的判别也是非常主观的，所以该模型并不会直接给出某个图片是否色情的结果，而是给出一个概率（0-1之间的分数）。一般而言，得分小于0.2表示图像很可能是安全的，评分大于0.8则基本可判定图片属于色情图片。如果得分介于这两个值之间，则需要程序员根据需求来设定一个阀值。如果需要比较严格的过滤，设一个比较低的阀值，反之设一个比较高的阀值。\n集成open_nsfw到C++程序\nopen_nsfw提供了一个python脚本，google了一圈，也没有找到有人将open_nsfw集成到C++代码中。好在classify_nsfw.py这个脚本比较简单，而caffe提供了C++的例子cpp_classification，结合这两部分的代码，我编写了一个C++的鉴黄程序，源码参考：https://gitee.com/mogoweb/dpexamples。\n当然看似简单的代码翻译工作，遇到的坑也不少，下面就总结一下C++代码中需要注意的地方。\n图片预处理\n在classify_nsfw.py中，编写了一个resize_image函数来处理图片缩放，没有采用caffe内置的图片缩放程序。代码注释中解释是因为训练时使用了这个缩放算法，所以为了达到最好的效果，测试中也需要采用该缩放算法。随后又对图片进行了一次裁剪，因为调用resize_image缩放为256x256大小，而模型接受的size为224x224。\nimg_data_rs = resize_image(pimg, sz=(256, 256)) image = caffe.io.load_image(StringIO(img_data_rs)) H, W, _ = image.shape _, _, h, w = caffe_net.blobs['data'].data.shape h_off = max((H - h) / 2, 0) w_off = max((W - w) / 2, 0) crop = image[h_off:h_off + h, w_off:w_off + w, :]\n在C++代码中，我使用了caffe中提供的opencv方法处理这个步骤：\ncv::Mat img = ReadImageToCVMat(file, 256, 256); ... // crop image cv::Size size = sample.size(); int H = size.height; int W = size.width; int h = input_geometry_.height; int w = input_geometry_.width; int h_off = std::max((H - h) / 2, 0); int w_off = std::max((W - w) / 2, 0); sample(cv::Rect(w_off, h_off, w, h)).copyTo(sample_resized);\n数据预处理\n在classify_nsfw.py中，我们可以看到这样一段代码：\ncaffe_transformer = caffe.io.Transformer({'data': nsfw_net.blobs['data'].data.shape}) # move image channels to outermost caffe_transformer.set_transpose('data', (2, 0, 1)) # subtract the dataset-mean value in each channel caffe_transformer.set_mean('data', np.array([104, 117, 123])) # rescale from [0, 1] to [0, 255] caffe_transformer.set_raw_scale('data', 255) # swap channels from RGB to BGR caffe_transformer.set_channel_swap('data', (2, 1, 0))\n这段代码其实是对数据做某种变换，将读入的数据转换为caffe模型能够接受的格式。\ncaffe_transformer.set_transpose('data', (2, 0, 1))\n该语句困扰了我好长时间，不知道在C++程序中该如何处理。后查网络资料，才了解到因为caffe.io读取的图像为HWC（H：高，W：宽，C：颜色）矩阵，而caffe模型需要(CHW)格式，所以需要对矩阵做一个变换，将颜色值维度提前到最前面。(2, 0, 1)的含义就是将原来数据的第2, 0, 1列按照新的顺序重新排列。\ncaffe_transformer.set_raw_scale('data', 255)\n这个处理是因为使用caffe.io读取的颜色值归一化为0～1之间的数，而caffe模型接受的是一个字节的整数，范围0~255，所以需要进行一个放大。\n在C++代码中，由于采用了opencv进行图像处理，不需要上面两步的变换处理。\ncaffe_transformer.set_mean('data', np.array([104, 117, 123]))\n在很多示例中，均值通常从均值文件中加载，这里直接给了一个固定值。所谓均值，可以理解为数据的算术平均值。通常输入数据减去均值，是为了减少奇异值（异常的值，比平均值大很多或小很多的值）的影响。这里是一个三元组，分别代表RGB通道上的均值。\n对应的C++代码如下：\ncv::Mat sample_normalized; cv::subtract(sample_float, mean_, sample_normalized);\n接下来的代码是RGB转BGR，这个比较容易理解。\ncaffe_transformer.set_channel_swap('data', (2, 1, 0))\n查看了一些caffe的C++例子，均没有这个步骤，可能cv::Mat中已经能够正确判断出RGB和BGR，所以代码中没有增加这一步骤。\n对比测试\n这个程序是否能够如愿工作呢？让我们找一些图片测试一下。考虑到内容审查，这里进行测试的图片均不是严格意义上的色情图片，只是裸露程度不同。下面使用C++程序和open_nsfw python脚本测试的结果进行对比。\nC++ : 0.6122\npython: 0.875480949879\nC++ : 0.2536\npython: 0.0773676931858\nC++ : 0.6319\npython: 0.782215833664\nC++ : 0.0914\npython: 0.0774072110653\nC++ : 0.0073\npython: 3.04092318402e-05\n从结果可以看出，使用C++程序进行测试，结果基本符合预期，但是和python版本还是有一些差距，猜测问题可能在于对图片进行缩放采用的算法不同，如果要获得好的结果，训练和测试阶段对数据的预处理需要一致。另外一个可能是没有RGB到BGR的转换，这个还需要再验证。\n如果你有兴趣，可以找一些尺度更大的图片测试，看看是不是能够正确的识别出来。"}
{"content2":"来源：专知\n概要：正像其它学科一样，一个大量人员研究了多年的学科，却很难给出一个严格的定义，模式识别如此，目前火热的人工智能如此，计算机视觉亦如此。\n【导读】本文由中国科学院自动化研究所模式识别国家重点实验室胡占义研究员撰写，对计算机视觉40多年的发展历程进行了简要总结，包括：马尔计算视觉理论，主动视觉与目的视觉，多视几何与摄像机自标定，以及基于学习的视觉。在此基础上，对计算机视觉的未来发展趋势给出了一些展望。\n1.1 什么是计算机视觉\n正像其它学科一样，一个大量人员研究了多年的学科，却很难给出一个严格的定义，模式识别如此，目前火热的人工智能如此，计算机视觉亦如此。与计算机视觉密切相关的概念有视觉感知（visual perception）,视觉认知(visual cognition),图像和视频理解( image and video understanding). 这些概念有一些共性之处，也有本质不同。从广义上说，计算机视觉就是“赋予机器自然视觉能力”的学科。自然视觉能力，就是指生物视觉系统体现的视觉能力。一则生物自然视觉无法严格定义，在加上这种广义视觉定义又“包罗万象”，同时也不太符合40多年来计算机视觉的研究状况，所以这种“广义计算机视觉定义”，虽无可挑剔，但也缺乏实质性内容，不过是一种“循环式游戏定义”而已。实际上，计算机视觉本质上就是研究视觉感知问题。视觉感知，根据维科百基（Wikipedia）的定义, 是指对“环境表达和理解中，对视觉信息的组织、识别和解释的过程”。根据这种定义，计算机视觉的目标是对环境的表达和理解，核心问题是研究如何对输入的图像信息进行组织，对物体和场景进行识别，进而对图像内容给予解释。\n计算机视觉与人工智能有密切联系，但也有本质的不同。人工智能更强调推理和决策，但至少计算机视觉目前还主要停留在图像信息表达和物体识别阶段。“物体识别和场景理解”也涉及从图像特征的推理与决策，但与人工智能的推理和决策有本质区别。应该没有一个严肃的计算机视觉研究人员会认为AlphaGo, AlphaZero 是计算机视觉，但都会认为它们是典型的人工智能内容。\n简言之，计算机视觉是以图像（视频）为输入，以对环境的表达（representation）和理解为目标，研究图像信息组织、物体和场景识别、进而对事件给予解释的学科。从目前的研究现状看，目前还主要聚焦在图像信息的组织和识别阶段，对事件解释还鲜有涉及，至少还处于非常初级的阶段。\n这里需要强调的是，每个人由于背景不同，偏好不同，知识面不同，对同一问题的观点亦会不同，甚至出现大相径庭的局面。上面为笔者对计算机视觉的理解，也许是片面或错误的。如不少人认为“纹理分析”是计算机视觉的一个重要研究方向，笔者不敢苟同。另外，很多场合，人们把“图像处理”也认为是“计算机视觉”，这也是不恰当的。图像处理是一门独立的学科，图像处理研究图像去噪、图像增强等内容，输入为图像，输出也是图像。计算机视觉利用图像处理技术进行图像预处理，但图像处理本身构不成计算机视觉的核心内容。\n这里顺便说一下，目前很多人对“感知”和“认知”不加区分，给读者带来不必要的困惑和误解。在不少场合下，经常会见到有些“视觉专家”把“认知”和“推理与决策”（reasoning and decision）作为平行概念使用，这事实上是不太严谨的。根据“维基百科”，“认知”是指通过感觉（senses）、经历 (experience)和思考(thoughts)来获取知识(knowledge)和进行理解(understanding)的思维过程（mental process）。认知包括：知识形成（knowledge），注视（attention），记忆（memory），推理（reasoning），问题求解（problem solving）、决策（ decision making）以及语言生成（language production）等。所以，“感知”与“认知”有区别，推理和决策是典型的认知过程，是认知的重要组成部分，它们之间是包含关系，不是平行关系。\n1.2  计算机视觉发展的四个主要阶段\n尽管人们对计算机视觉这门学科的起始时间和发展历史有不同的看法，但应该说， 1982年马尔( David Marr )《视觉》（Marr, 1982）一书的问世，标志着计算机视觉成为了一门独立学科。计算机视觉的研究内容，大体可以分为物体视觉（object vision）和空间视觉（spatial vision）二大部分. 物体视觉在于对物体进行精细分类和鉴别，而空间视觉在于确定物体的位置和形状，为“动作（action）” 服务。正像著名的认知心理学家J.J. Gibson 所言，视觉的主要功能在于“适应外界环境，控制自身运动”。 适应外界环境和控制自身运动，是生物生存的需求，这些功能的实现需要靠物体视觉和空间视觉协调完成。\n计算机视觉40多年的发展中，尽管人们提出了大量的理论和方法，但总体上说，计算机视觉经历了4个主要历程。即： 马尔计算视觉、主动和目的视觉、多视几何与分层三维重建和基于学习的视觉。下面将对这4项主要内容进行简要介绍。\n1.2.1 马尔计算视觉（Computational Vision）\n现在很多计算机视觉的研究人员，恐怕对“马尔计算视觉”根本不了解，这不能不说是一件非常遗憾的事。目前，在计算机上调“深度网络”来提高物体识别的精度似乎就等于从事“视觉研究”。事实上，马尔的计算视觉的提出，不论在理论上还是研究视觉的方法论上，均具有划时代的意义。\n马尔的计算视觉分为三个层次： 计算理论、表达和算法以及算法实现。由于马尔认为算法实现并不影响算法的功能和效果，所以，马尔计算视觉理论主要讨论“计算理论”和“表达与算法”二部分内容。马尔认为，大脑的神经计算和计算机的数值计算没有本质区别，所以马尔没有对“算法实现”进行任何探讨。从现在神经科学的进展看，“神经计算”与数值计算在有些情况下会产生本质区别，如目前兴起的神经形态计算（ Neuromorphological computing），但总体上说，“数值计算”可以“模拟神经计算”。至少从现在看，“算法的不同实现途径”，并不影响马尔计算视觉理论的本质属性。\n1）计算理论(Computational Theory)\n计算理论需要明确视觉目的， 或视觉的主要功能是什么。上世纪70年代，人们对大脑的认识还非常粗浅，目前普遍使用的非创伤型成像手段，如功能核磁共振（FMRI）等，还没有普及。所以，人们主要靠病理学和心理学结果来推断生理功能。即使目前，人们对“视觉的主要功能”到底是什么，也仍然没有定论。如最近几年，MIT的 DiCarlo等人提出了所谓的“目标驱动的感知信息建模”方法（Yamins &DiCarlo et al. 2016a）。他们猜测，猴子IT区（IT: interiortemporal cortex, 物体识别区）的神经元对物体的响应（neuronal responses）“可以通过层次化的卷积神经网络”（HCNN: Hierarchical Convolutional Neural Networks ）来建模。他们认为，只要对HCNN在图像物体分类任务下进行训练，则训练好的HCNN 可以很好定量预测IT 区神经元的响应（Yamins et al. 2014, 2016b）。由于仅仅“控制图像分类性能”对IT神经元响应（群体神经元对某一输入图像物体的响应，就是神经元对该物体的表达或编码）进行定量预测，所以他们将这种框架称之为“目标驱动的框架”。目标驱动的框架提供了一种新的比较通用的建模群体神经元编码的途径，但也存在很大的不足。能否真正像作者所言的那样，仅仅靠“训练图像分类的HCNN”就可以定量预测神经元对图像物体的响应，仍是一个有待进一步深入研究的课题。\n马尔认为视觉不管有多少功能，主要功能在于“从视网膜成像的二维图像来恢复空间物体的可见三维表面形状”，称之为“三维重建”（3D reconstruction）。而且，马尔认为，这种重建过程不是天生就有的，而是可以通过计算完成的。J.J. Gibson 等心理学家，包括格式塔心里学学派( Gestalt psychology)，认为视觉的很多功能是天生就有的。可以想想，如果一种视觉功能与生具有，不可建模，就谈不上计算，也许就不存在今天的“计算机视觉”这门学科了。\n那么，马尔的计算理论是什么呢？这一方面，马尔在其书中似乎并不是介绍得特别具体。他举了一个购买商品的例子，说明计算理论的重要性。如商店结账要用加法而不是乘法。试想如果用乘法结账，每个商品1元钱，则不管你购买多少件商品，你仅仅需要付一元钱。\n马尔的计算理论认为，图像是物理空间在视网膜上的投影，所以图像信息蕴含了物理空间的内在信息，因此，任何计算视觉计算理论和方法都应该从图像出发，充分挖掘图像所蕴含的对应物理空间的内在属性。也就是说，马尔的视觉计算理论就是要“挖掘关于成像物理场景的内在属性来完成相应的视觉问题计算”。因为从数学的观点看，仅仅从图像出发，很多视觉问题具有“歧义性”，如典型的左右眼图像之间的对应问题。如果没有任何先验知识，图像点对应关系不能唯一确定。不管任何动物或人，生活的环境都不是随机的，不管有意识或无意识，时时刻刻都在利用这些先验知识，来解释看到的场景和指导日常的行为和行动。如桌子上放一个水杯的场景，人们会正确地解释为桌子上放了一个水杯，而不把他们看作一个新物体。当然，人类也会经常出错，如大量错觉现象。从这个意义上来说，让计算机来模仿人类视觉是否一定是一条好的途径也是一个未知的命题。飞机的飞行需要借助空气动力学知识，而不是机械地模仿鸟如何飞。\n2）表达和算法（Representationand Algorithm）\n识别物体之前，不管是计算机还是人，大脑（或计算机内存）中事先要有对该物体的存储形式，称之为物体表达（object representation）. 马尔视觉计算理论认为，物体的表达形式为该物体的三维几何形状。马尔当时猜测，由于人在识别物体时与观察物体的视角无关，而不同视角下同一物体在视网膜上的成像又不同，所以物体在大脑中的表达不可能是二维的，可能是三维形状，因为三维形状不依赖于观察视角。另外，当时病理学研究发现，有些病人无法辨认“茶杯”，但可以毫无困难地画出茶杯的形状，因此马尔觉得，这些病人也佐证了他的猜测。从目前对大脑的研究看，大脑的功能是分区的。物体的“几何形状”和“语义”储存在不同的脑区。另外，物体识别也不是绝对地与视角无关，仅仅在一个比较小的变化范围内与视角无关。所以，从当前的研究看，马尔的物体的“三维表达”猜测基本上是不正确的，至少是不完全正确的，但马尔的计算理论仍具有重要的理论意义和应用价值。\n简言之，马尔视觉计算理论的“物体表达”，是指“物体坐标系下的三维形状表达”。注意，从数学上来说，一个三维几何形状，选取的坐标系不同，表达函数亦不同。如一个球体，如果以球心为坐标原点，则球面可以简单表达为：x^2+y^2+z^2=1。 但如果观测者在x轴上2倍半径处观测，则可见球面部分在观测者坐标系下的方程为：x=2-sqrt(1-y^2-z^2)。由此可见，同一物体，选用的坐标系不同，表达方式亦不同。马尔将“观测者坐标系下的三维几何形状表达”称之为“2.5维表达”，物体坐标系下的表达为“三维表达”。所以，在后续的算法部分，马尔重点研究了如何从图像先计算“2.5维表达”，然后转化为“三维表达”的计算方法和过程。\n算法部分是马尔计算视觉的主体内容。马尔认为，从图像到三维表达，要经过三个计算层次：首先从图像得到一些基元（primal sketch）, 然后通过立体视觉（stereopsis）等模块将基元提升到2.5维表达，最后提升到三维表达。\n下图总结给出了马尔视觉计算理论的算法流程：\n马尔计算理论中算法的三个计算层次\n由上图所示，首先从图像提取边缘信息（二阶导数的过零点），然后提取点状基元（blob, 线状基元（edge）和杆状基元 (bar), 进而对这些初级基元（raw primal sketch）组合形成完整基元（full primal sketch），上述过程为视觉计算理论的特征提取阶段。在此基础上，通过立体视觉和运动视觉等模块，将基元提升到2.5维表达。最后，将2.5维表达提升到三维表达。在马尔的《视觉》一书中，重点介绍了特征提取和2.5维表达对应的计算方法。在2.5维表达部分，也仅仅重点介绍了立体视觉和运动视觉部分。由于当双眼（左右相机）的相互位置已知时（计算机视觉中称之为相机外参数），立体视觉就转化为“左右图像点的对应问题”（image point correspondence）, 所以，马尔在立体视觉部分重点介绍了图像点之间的匹配问题，即如何剔除误匹配，并给出了对应算法。\n立体视觉等计算得到的三维空间点仅仅是在“观测者坐标系下的坐标”，是物体的2.5维表示。如何进一步提升到物体坐标系下的三维表示，马尔给出了一些思路，但这方面都很粗泛。如确定物体的旋转主轴等等，这部分内容，类似于后来人们提出的“骨架模型”（skeleton model）构造.\n需要指出的是，马尔的视觉计算理论是一种理论体系。在此体系下，可以进一步丰富具体的计算模块，构建“通用性视觉系统”（general vision system）。只可惜马尔（Jan.15,1945 ~ Nov.17,1980 ）1980年底就因白血病去世，包括他的《视觉》一书，也是他去世后出版的。马尔的英年早逝，不能说不是计算机视觉界的一大损失。由于马尔的贡献，所以二年一度的国际计算机视觉大会（ICCV: International Conference on Computer Vision）设有马尔奖（MarrPrize），作为会议的最佳论文奖。另外，在认知科学领域，也设有马尔奖，因为马尔对认知科学也有巨大的贡献。以同一人名在不同领域设立奖项，实属罕见，可见马尔对计算机视觉的影响有多深远。正如S. Edelman 和 L. M. Vaina 在《 International Encyclopedia of the Social & Behavioral Sciences 》中对马尔的评价那样，“马尔前期给出的集成数学和神经生物学对大脑理解的三项工作，已足以使他在任何情况下在英国经验主义二个半世纪的科学殿堂中占有重要的一席，…, 然而，他进一步提出了更加有影响的计算视觉理论”。所以，从事计算机视觉研究的人员对马尔计算视觉不了解，实在是一件比较遗憾的事。\n1.2.2 昙花一现的主动和目的视觉\n很多人介绍计算机视觉时，将这部分内容不作为一个单独部分加以介绍，主要是因为“主动视觉和目的视觉”并没有对计算机视觉后续研究形成持续影响。但作为计算机视觉发展的一个重要阶段，这里还是有必要予以介绍一下。\n上世纪80年代初马尔视觉计算理论提出后，学术界兴起了“计算机视觉”的热潮。人们想到的这种理论的一种直接应用就是给工业机器人赋予视觉能力，典型的系统就是所谓的“基于部件的系统”（parts-based system）。然而，10多年的研究，使人们认识到，尽管马尔计算视觉理论非常优美，但“鲁棒性”（Robustness）不够，很难想人们预想的那样在工业界得到广泛应用。这样，人们开始质疑这种理论的合理性，甚至提出了尖锐的批评。\n对马尔计算视觉理论提出批评最多的有二点：一是认为这种三维重建过程是“纯粹自底向上的过程”（pure bottom-up process），缺乏高层反馈（top-down feedback）；二是“重建”缺乏“目的性和主动性”。由于不同的用途，要求重建的精度不同，而不考虑具体任务，仅仅“盲目地重建一个适合任何任务的三维模型”似乎不合理。\n对马尔视觉计算理论提出批评的代表性人物有：马里兰大学的 J. Y. Aloimonos;宾夕法尼亚大学的R. Bajcsy和密西根州立大学的A. K. Jaini。 Bajcsy 认为，视觉过程必然存在人与环境的交互，提出了主动视觉的概念（active vision）. Aloimonos认为视觉要有目的性，且在很多应用，不需要严格三维重建，提出了“目的和定性视觉”（purpose and qualitative vision） 的概念。 Jain 认为应该重点强调应用，提出了“应用视觉”（ practicing vision）的概念。上世纪80年代末到90年代初，可以说是计算机视觉领域的“彷徨”阶段。真有点“批评之声不绝，视觉之路茫茫”之势。\n针对这种情况，当时视觉领域的一个著名刊物（CVGIP: Image Understanding）于1994年组织了一期专刊对计算视觉理论进行了辩论。首先由耶鲁大学的M. J. Tarr和布朗大学的M. J.Black写了一篇非常有争议性的观点文章（Tarr & Black, 1994），认为马尔的计算视觉并不排斥主动性，但把马尔的“通用视觉理论”（general vision）过分地强调“应用视觉”是“短见”（myopic）之举。通用视觉尽管无法给出严格定义，但“人类视觉”是最好的样板。这篇观点文章发表后，国际上20多位著名的视觉专家也发表了他们的观点和评论。大家普遍的观点是，“主动性”“目的性”是合理的，但问题是如何给出新的理论和方法。而当时提出的一些主动视觉方法，一则仅仅是算法层次上的改进，缺乏理论框架上的创新，另外，这些内容也完全可以纳入到马尔计算视觉框架下。所以，从1994年这场视觉大辩论后，主动视觉在计算机视觉界基本没有太多实质性进展。这段“彷徨阶段”持续不长，对后续计算机视觉的发展产生的影响不大，犹如“昙花一现”之状。\n值得指出的是，“主动视觉”应该是一个非常好的概念，但困难在于“如何计算”。 主动视觉往往需要“视觉注视”（visual attention），需要研究脑皮层（cerebral cortex）高层区域到低层区域的反馈机制，这些问题，即使脑科学和神经科学已经较20年前取得了巨大进展的今天，仍缺乏“计算层次上的进展”可为计算机视觉研究人员提供实质性的参考和借鉴。近年来，各种脑成像手段的发展，特别是 “连接组学”（Connectomics）的进展，可望为计算机视觉人员研究大脑反馈机制提供“反馈途径和连接强度”提供一些借鉴。\n1.2.3 多视几何和分层三维重建（Multiple View Geometry and Stratified 3D Reconstruction）\n上世纪90年代初计算机视觉从“萧条”走向进一步“繁荣”，主要得益于以下二方面的因素：首先，瞄准的应用领域从精度和鲁棒性要求太高的“工业应用”转到要求不太高，特别是仅仅需要“视觉效果”的应用领域，如远程视频会议（teleconference），考古，虚拟现实，视频监控等。另一方面，人们发现，多视几何理论下的分层三维重建能有效提高三维重建的鲁棒性和精度。\n多视几何的代表性人物首数法国INRIA的O. Faugeras ( Faugeras O, 1993), 美国GE 研究院的R.Hartely （现已回到了澳大利亚国立大学）和英国牛津大学的 A. Zisserman。应该说，多视几何的理论于2000年已基本完善。 2000 年Hartley 和Zisserman 合著的书 (Hartley & Zisserman 2000) 对这方面的内容给出了比较系统的总结，而后这方面的工作主要集中在如何提高“大数据下鲁棒性重建的计算效率”。大数据需要全自动重建，而全自动重建需要反复优化，而反复优化需要花费大量计算资源。所以，如何在保证鲁棒性的前提下快速进行大场景的三维重建是后期研究的重点。举一个简单例子，假如要三维重建北京中关村地区，为了保证重建的完整性，需要获取大量的地面和无人机图像。假如获取了1万幅地面高分辨率图像（4000×3000），5 千幅高分辨率无人机图像（8000×7000）（这样的图像规模是当前的典型规模），三维重建要匹配这些图像，从中选取合适的图像集，然后对相机位置信息进行标定并重建出场景的三维结构，如此大的数据量，人工干预是不可能的，所以整个三维重建流程必须全自动进行。这样需要重建算法和系统具有非常高的鲁棒性，否则根本无法全自动三维重建。在鲁棒性保证的情况下，三维重建效率也是一个巨大的挑战。所以，目前在这方面的研究重点是如何快速、鲁棒地重建大场景。\n1）多视几何（ Multiple View Geometry）\n由于图像的成像过程是一个中心投影过程（perspective projection），所以“多视几何”本质上就是研究射影变换下图像对应点之间以及空间点与其投影的图像点之间的约束理论和计算方法的学科（注意：针孔成像模型（The pinhole camera model）是一种中心投影， 当相机有畸变时，需要将畸变后的图像点先校正到无畸变后才可以使用多视几何理论）。计算机视觉领域，多视几何主要研究二幅图像对应点之间的对极几何约束（epipolar geometry）, 三幅图像对应点之间的三焦张量约束（tri-focal tensor），空间平面点到图像点，或空间点为平面点投影的多幅图像点之间的单应约束（homography）等。在多视几何中，射影变换下的不变量，如绝对二次曲线的像（The image of the absolute conic）,绝对二次曲面的像（Theimage of the absolute quadric）, 无穷远平面的单应矩阵（infinite homography），是非常重要的概念，是摄像机能够自标定的“参照物”。由于这些量是无穷远处“参照物”在图像上的投影，所以这些量与相机的位置和运动无关（原则上任何有限的运动不会影响无限远处的物体的性质），所以可以用这些“射影不变量”来自标定摄像机。关于多视几何和摄像机自标定的详细内容，可参阅Hartley 和Zisserman 合著的书（Hartley & Zisserman,2000）.\n总体上说，多视几何就其理论而言，在射影几何中不能算新内容。Hartley, Faugeras,  Zissermann等将多视几何理论引入到计算机视觉中，提出了分层三维重建理论和摄像机自标定理论，丰富了马尔三维重建理论，提高了三维重建的鲁棒性和对大数据的适应性，有力推动了三维重建的应用范围。所以，计算机视觉中的多视几何研究，是计算机视觉发展历程中的一个重要阶段和事件。\n多视几何需要射影几何（projectivegeometry）的数学基础。射影几何是非欧几何，涉及平行直线相交，平行平面相交等抽象概念，表达和计算要在“齐次坐标”（homogeneous coordinates）下进行，这给“工科学生”带来不小的困难。所以，大家要从事这方面的研究，一定要先打好基础，至少要具备必要的射影几何知识。否则，做这方面的工作，无异于浪费时间。\n2）分层三维重建（ Stratified 3D Reconstruction）\n所谓的分层三维重建，如下图所示，就是指从多幅二维图像恢复欧几里德空间的三维结构时，不是从图像一步到欧几里德空间下的三维结构，而是分步分层地进行。即先从多幅图像的对应点重建射影空间下的对应空间点(即射影重建：projective reconstruction)，然后把射影空间下重建的点提升到仿射空间下(即仿射重建：affine reconstruction)，最后把仿射空间下重建的点再提升到欧几里德空间（或度量空间: metric reconstruction）（注：度量空间与欧几里德空间差一个常数因子。由于分层三维重建仅仅靠图像进行空间点重建，没有已知的“绝对尺度”，如“窗户的长为1米”等，所以从图像仅仅能够把空间点恢复到度量空间）。\n这里有几个概念需要解释一下。以空间三维点的三维重建为例，所谓的“射影重建”，是指重建的点的坐标与该点在欧几里德空间下的坐标差一个“射影变换”。所谓的“仿射重建”，是指重建的点的坐标与该点在欧几里德空间下的坐标差一个“仿射变换”。所谓的“度量重建”，是指重建的点的坐标与该点在欧几里德空间下的坐标差一个“相似变换”。\n由于任何一个视觉问题最终都可以转化为一个多参数下的非线性优化问题，而非线性优化的困难在于找到一个合理的初值。由于待优化的参数越多，一般来说解空间越复杂，寻找合适的初值越困难，所以，如果一个优化问题如能将参数分组分步优化，则一般可以大大简化优化问题的难度。分层三维重建计算上的合理性正是利用了这种“分组分步”的优化策略。以三幅图像为例，直接从图像对应点重建度量空间的三维点需要非线性优化16个参数（假定相机内参数不变，5个相机内参数，第二幅和第三幅图像相对于第一幅图像的相机的旋转和平移参数，去掉一个常数因子，所以5+2×(3+3)-1=16），　这是一个非常困难的优化问题。但从图像对应点到射影重建需要“线性”估计22个参数，由于是线性优化，所以优化问题并不困难。从射影重建提升到仿射重建需要“非线性”优化三个参数（无穷远平面的3个平面参数），而从仿射重建提升到度量重建需要“非线性”优化5个参数（摄像机的5个内参数）。因此，分层三维重建仅仅需要分步优化3个和5个参数的非线性优化问题，从而大大减小了三维重建的计算复杂度。\n分层三维重建的另一个特点是其理论的优美性。射影重建下，空间直线的投影仍为直线，二条相交直线其投影直线仍相交，但空间直线之间的平行性和垂直性不再保持。仿射重建下可以保持直线的平行性，但不能保持直线的垂直性。度量重建既可以保持直线之间的平行线，也可以保持垂直性。在具体应用中，可以利用这些性质逐级提升重建结果。\n分层三维重建理论可以说是计算机视觉界继马尔计算视觉理论提出后又一个最重要和最具有影响力的理论。目前很多大公司的三维视觉应用，如苹果公司的三维地图，百度公司的三维地图，诺基亚的Streetview, 微软的虚拟地球，其后台核心支撑技术的一项重要技术就是分层三维重建技术。\n3）摄像机自标定（Cameraself-calibration）\n所谓摄像机标定，狭义上讲，就是确定摄像机内部机械和光电参数的过程，如焦距，光轴与像平面的交点等。尽管相机出厂时都标有一些标准参数，但这些参数一般不够精确，很难直接在三维重建和视觉测量中应用。所以，为了提高三维重建的精度，需要对这些相机内参数（intrinsic parameters）进行估计。估计相机的内参数的过程，称为相机标定。在文献中，有时把估计相机在给定物体坐标系下的坐标，或相机之间相互之间的位置关系，称为相机外参数（extrinsic parameters）标定。但一般无明确指定时，相机标定就是指对相机内参数的标定。\n相机标定包含二方面的内容：“成像模型选择”和“模型参数估计”。相机标定时首先需要确定“合理的相机成像模型”，如是不是针孔模型，有没有畸变等。目前关于相机模型选择方面，没有太好的指导理论，只能根据具体相机和具体应用确定。随着相机加工工艺的提高，一般来说，普通相机（非鱼眼或大广角镜头等特殊相机）一般使用针孔成像模型（加一阶或二阶径向畸变）就足以了。其它畸变很小，可以不加考虑。当相机成像模型确定后，进一步需要估计对应的模型参数。文献中人们往往将成像模型参数估计简单地认为就是相机标定，是不全面的。事实上，相机模型选择是相机标定最关键的步骤。一种相机如果无畸变而在标定时考虑了畸变，或有畸变而未加考虑，都会产生大的误差。视觉应用人员应该特别关注“相机模型选择”问题。\n相机参数估计原则上均需要一个“已知三维结构”的“标定参考物”，如平面棋盘格，立体块等。所谓相机标定，就是利用已知标定参考物和其投影图像，在已知成像模型下建立模型参数的约束方程，进而估计模型参数的过程。所谓“自标定”，就是指“仅仅利用图像特征点之间的对应关系，不需要借助具体物理标定参考物，进行模型参数估计的过程”。“传统标定”需要使用加工尺寸已知的标定参考物，自标定不需要这类物理标定物，正像前面多视几何部分所言，使用的是抽象的无穷远平面上的“绝对二次曲线”和“绝对二次曲面”。从这个意义上来说，自标定也需要参考物，仅仅是“虚拟的无穷远处的参考物”而已。\n摄像机自标定需要用到两幅图像之间的约束，如基础矩阵（fundamental matrix）, 本质矩阵（essential matrix）, 以及三幅图像之间的三焦张量约束等。另外，Kruppa 方程也是一个重要的概念。这些内容是多视几何的重要内容，后续章节将进行详细介绍。\n1.2.4 基于学习的视觉（Learning based vision）\n基于学习的视觉，是指以机器学习为主要技术手段的计算机视觉研究。基于学习的视觉研究，文献中大体上分为二个阶段：本世纪初的以流形学习( manifold Learning)为代表的子空间法( subspace method)和目前以深度神经网络和深度学习（deep neural networks and deep learning）为代表的视觉方法。\n1）流形学习（Manifold Learning）\n正像前面所指出的，物体表达是物体识别的核心问题。给定图像物体，如人脸图像，不同的表达，物体的分类和识别率不同。另外，直接将图像像素作为表达是一种“过表达”，也不是一种好的表达。流形学习理论认为，一种图像物体存在其“内在流形”（intrinsic manifold）, 这种内在流形是该物体的一种优质表达。所以，流形学习就是从图像表达学习其内在流形表达的过程，这种内在流形的学习过程一般是一种非线性优化过程。\n流形学习始于2000年在Science 上发表的二篇文章（ Tenenbaum et al., 2000） (Roweis & Lawrence 2000)。流形学习一个困难的问题是没有严格的理论来确定内在流形的维度。人们发现，很多情况下流形学习的结果还不如传统的PCA （Principal Component Analysis），LDA（ linear DiscriminantAnalysis ）， MDS（ Multidimensional Scaling）等. 流形学习的代表方法有：LLE（Locally Linear Embedding ）(Roweis & Lawrence 2000)，Isomap （ Tenenbaum et al., 2000）， Laplacian Eigenmaps (Belkin & Niyogi, 2001)等。\n2）深度学习（Deep Learning）\n深度学习( LeCunet al. 2015) 的成功，主要得益于数据积累和计算能力的提高。深度网络的概念上世纪80年代就已提出来了，只是因为当时发现“深度网络”性能还不如“浅层网络”，所以没有得到大的发展。目前似乎有点计算机视觉就是深度学习的应用之势，这可以从计算机视觉的三大国际会议：国际计算机视觉会议（ICCV），欧洲计算机视觉会议（ECCV）和计算机视觉和模式识别会议（CVPR），上近年来发表的论文可见一般。目前的基本状况是，人们都在利用深度学习来“取代”计算机视觉中的传统方法。“研究人员”成了“调程序的机器”，这实在是一种不正常的“群众式运动”。牛顿的万有引力定律，麦克斯韦的电磁方程，爱因斯坦的质能方程，量子力学中的薛定谔方程，似乎还是人们应该追求的目标。\n关于深度网络和深度学习，详细内容可参阅相关文献，这里仅仅强调以下几点：\n（1）深度学习在物体视觉方面较传统方法体现了巨大优势，但在空间视觉，如三维重建，物体定位方面，仍无法与基于几何的方法相媲美。这主要是因为深度学习很难处理图像特征之间的误匹配现象。在基于几何的三维重建中，RANSAC （Random Sample Consensus）等鲁棒外点（误匹配点）剔除模块可以反复调用，而在深度学习中，目前还很难集成诸如RANSAC等外点剔除机制。笔者认为，如果深度网络不能很好地集成外点剔除模块，深度学习在三维重建中将很难与基于几何的方法相媲美，甚至很难在空间视觉中得到有效应用；\n（2） 深度学习在静态图像物体识别方面已经成熟，这也是为什么在ImageNet上的物体分类竞赛已不再举行的缘故；\n（3） 目前的深度网络，基本上是前馈网络（feedforwardNetworks）.不同网络主要体现在使用的代价函数不同。下一步预计要探索具有“反馈机制”的层次化网络。反馈机制，需要借鉴脑神经网络机制，特别是连接组学的成果。\n（4） 目前对视频的处理，人们提出了RCNN (Recurrent Neural Networks). 循环( recurrent) 是一种有效的同层作用机制，但不能代替反馈。大脑皮层远距离的反馈（将在生物视觉简介一章介绍）可能是形成大脑皮层不同区域具有不同特定功能的神经基础。所以，研究反馈机制，特别具有“长距离反馈”（跨多层之间）的深度网络, 将是今后研究图像理解的一个重要方向；\n（5）尽管深度学习和深度网络在图像物体识别方面取得了“变革性”成果，但为什么“深度学习”会取得如此好的结果目前仍然缺乏坚实的理论基础。目前已有一些这方面的研究，但仍缺乏系统性的理论。事实上，“层次化”是本质，不仅深度网络，其它层次化模型，如Hmax 模型（Riesenhuber & Poggio,1999） HTM （Hierarchical Temporal memory）模型（George & Hawkins, 2009）存在同样的理论困惑。为什么“层次化结构”（ hierarchical structure ）具有优势仍是一个巨大的迷。\n1.3 计算机视觉的若干发展趋势\n信息科学发展之迅速，对未来10年的发展趋势进行预测，有点“算命”的感觉。 对计算机视觉而言，笔者有以下几点对未来发展的展望：\n（1） 基于学习的物体视觉和基于几何的空间视觉继续“相互独立”进行。深度学习在短时期内很难代替几何视觉。在深度网络中如何引入“鲁棒外点剔除模块”将是一个探索方向，但短时间内估计很难有实质性进展；\n（2） 基于视觉的定位将更加趋向“应用性研究”，特别是多传感器融合的视觉定位技术。\n（3） 三维点云重建技术已经比较成熟，如何从“点云”到“语义”是未来研究重点。“语义重建”将点云重建、物体分割和物体识别同时进行，是三维重建走向实用的前提。\n（4）对室外场景的三维重建，如何重建符合“城市管理规范”的模型是一个有待解决的问题。室内场景重建估计最大的潜在应用是“家庭服务机器人”。　鉴于室内重建的应用还缺乏非常具体的应用需求和驱动，在加上室内环境的复杂性，估计在３－５年内很难有突破性进展。\n（5）对物体识别而言，基于深度学习的物体识别估计将从“通用识别”向“特定领域物体的识别”发展。“特定领域”可以提供更加明确和具体的先验信息，可以有效提高识别的精度和效率，更加具有实用性；\n（６）目前基于RCNN 对视频理解的趋势将会持续；\n（7） 解析深度网络机理的工作具有重大的理论意义和挑战性，鉴于深度网络的复杂性，估计近期很难取得突破性进展；\n（8）具有“反馈机制”的深度网络结构（architecture）研究必将是下一个研究热点。\n1.4 几种典型的物体表达理论（Object representation theories）\n正像前面所述，物体表达是计算机视觉的一个核心科学问题。这里，“物体表达理论”与“物体表达模型”需要加以区别。“表达理论”是指文献中大家比较认可的方法。“表达模型”容易误解为“数学上对物体的某种描述”。计算机视觉领域，比较著名的物体表达理论有以下三种：\n1）马尔的三维物体表达\n前面已经介绍过，马尔视觉计算理论认为物体的表达是物体坐标系下的三维表达\n2）基于二维图像的物体表达（View-basedobject representation）\n尽管理论上一个三维物体可以成像为无限多不同的二维图像，但人的视觉系统仅仅可以识别“有限个图像”。鉴于神经科学对于猴子腹部通道（ventral pathway）(注：腹部通道认为是物体识别通道)的研究进展，T. Poggio 等提出了基于图像的物体表达（Poggio & Bizzi, 2004），即对一个三维物体的表达是该物体的一组典型的二维图像（view）。目前，也有人认为 Poggio等的”view”不能狭义地理解为二维图像，也包含以观测者为坐标系下的三维表示，即马尔的2.5维表示（Anzai & DeAngelis，2010）。\n3）逆生成模型表达（Inversegenerative model representation ）\n长期以来，人们认为物体识别模型为“鉴别模型”（ discriminative model），而不是“生成模型”（ generative model ）。近期对猴子腹部通道的物体识别研究表明，猴子大脑皮层的IT 区（ Inferior Temporal: 物体表达区域）可能在于编码物体及其成像参数（如光照和姿态，几何形状，纹理等）（Yildirim et al. 2015）（Yamins &DiCarlo，2016b.）。由于已知这些参数就可以生成对应图像，所以对这些参数的编码可以认为是逆生成模型表达。逆生成模型表达可以解释为什么深度学习中的Encoder-decoder 网络结( Badrinarayanan et al. 2015) 可以取得比较好的效果，因为Encoder本质上就是图像的逆生成模型。另外，深度学习中提出的“逆图形学”概念（ Inverse Graphic）( Kulkarniet al. 2015),从原理上也是一种逆生成模型。逆图形学是指先从图像学习到图像生成参数，然后把同一物体在不同参数下的图像归类为同一物体，通过这种“等变物体识别”(Equivariant recognition) 来达到最终的“不变物体识别”（invariantrecognition）。\n总之，本文对计算机视觉的理论、现状和未来发展趋势进行了一些总结和展望，希望能给读者了解该领域提供一些帮助。特别需要指出的是，这里很多内容也仅仅是笔者的一些“个人观点”和“个人偏好”下总结的一些内容，以期对读者有所帮助但不引起误导。\n---------------------\n作者：AI先锋\n来源：CSDN\n原文：https://blog.csdn.net/JtNbCOC8N2I9/article/details/78610301\n版权声明：本文为博主原创文章，转载请附上博文链接！"}
{"content2":"本文章有转载自其它博文，也有自己发现的新库添加进来的，如果发现有新的库，可以推荐我加进来\n转自：http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html\nhttp://www.cnblogs.com/mothe123/p/4267248.html\nDeep Learning（深度学习）：\nufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：一\nufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：二\nBengio团队的deep learning教程，用的theano库，主要是rbm系列，搞python的可以参考，很不错。\ndeeplearning.net主页，里面包含的信息量非常多，有software, reading list, research lab, dataset, demo等，强烈推荐，自己去发现好资料。\nDeep learning的toolbox，matlab实现的，对应源码来学习一些常见的DL模型很有帮助，这个库我主要是用来学习算法实现过程的。\n2013年龙星计划深度学习教程，邓力大牛主讲，虽然老师准备得不充分，不过还是很有收获的。\nHinton大牛在coursera上开的神经网络课程，DL部分有不少，非常赞，没有废话，课件每句话都包含了很多信息，有一定DL基础后去听收获更大。\nLarochelle关于DL的课件，逻辑清晰，覆盖面广,包含了rbm系列，autoencoder系列，sparse coding系列，还有crf，cnn，rnn等。虽然网页是法文，但是课件是英文。\nCMU大学2013年的deep learning课程，有不少reading paper可以参考。\n达慕思大学Lorenzo Torresani的2013Deep learning课程reading list.\nDeep Learning Methods for Vision(余凯等在cvpr2012上组织一个workshop，关于DL在视觉上的应用)。\n斯坦福Ng团队成员链接主页，可以进入团队成员的主页，比较熟悉的有Richard Socher, Honglak Lee, Quoc Le等。\n多伦多ML团队成员链接主页，可以进入团队成员主页，包括DL鼻祖hinton，还有Ruslan Salakhutdinov , Alex Krizhevsky等。\n蒙特利尔大学机器学习团队成员链接主页，包括大牛Bengio，还有Ian Goodfellow 等。\n纽约大学的机器学习团队成员链接主页，包括大牛Lecun，还有Rob Fergus等。\nCharlie Tang个人主页，结合DL+SVM.\n豆瓣上的脑与deep learning读书会，有讲义和部分视频，主要介绍了一些于deep learning相关的生物神经网络。\nLarge Scale ML的课程，由Lecun和Langford讲的，能不推荐么。\nYann Lecun的2014年Deep Learning课程主页。视频链接。\n吴立德老师《深度学习课程》\n一些常见的DL code列表，csdn博主zouxy09的博文，Deep Learning源代码收集-持续更新…\nDeep Learning for NLP (without Magic)，由DL界5大高手之一的Richard Socher小组搞的，他主要是NLP的。\n2012 Graduate Summer School: Deep Learning, Feature Learning，高手云集，深度学习盛宴，几乎所有的DL大牛都有参加。\nmatlab下的maxPooling速度优化，调用C++实现的。\n2014年ACL机器学习领域主席Kevin Duh的深度学习入门讲座视频。\nR-CNN code: Regions with Convolutional Neural Network Features.\nMachine Learning（机器学习）：\n介绍图模型的一个ppt，非常的赞，ppt作者总结得很给力，里面还包括了HMM，MEM, CRF等其它图模型。反正看完挺有收获的。\n机器学习一个视频教程，youtube上的，翻吧，内容很全面，偏概率统计模型，每一小集只有几分钟。\n龙星计划2012机器学习，由余凯和张潼主讲。\ndemonstrate 的 blog :关于PGM(概率图模型)系列，主要按照Daphne Koller的经典PGM教程介绍的，大家依次google之。\nFreeMind的博客，主要关于机器学习的。\nTom Mitchell大牛的机器学习课程，他的machine learning教科书非常出名。\nCS109,Data Science,用python介绍机器学习算法的课程。\nCCF主办的一些视频讲座。\n国外技术团队博客：\nNetflix技术博客,很多干货。\nComputer Vision（计算机视觉）：\nMIT2013年秋季课程：Advances in Computer Vision，有练习题，有些有code.\nIPAM一个计算机视觉的短期课程，有不少牛人参加。\nOpenCV相关：\nhttp://opencv.org/\n2012年7月4日随着opencv2.4.2版本的发布，opencv更改了其最新的官方网站地址。\nhttp://www.opencvchina.com/\n好像12年才有这个论坛的，比较新。里面有针对《learning opencv》这本书的视频讲解，不过视频教学还没出完，正在更新中。对刚入门学习opencv的人来说很不错。\nhttp://www.opencv.org.cn/forum/\nopencv中文论坛，对于初次接触opencv的学者来说比较不错，入门资料多，opencv的各种英文文档也翻译成中文了。不足是感觉这个论坛上发帖提问很少人回答，也就是说讨论不够激烈。\nhttp://opencv.jp/\nopencv的日文网站，里面有不少例子代码，看不懂日文可以用网站自带的翻译，能看个大概。\nhttp://code.opencv.org/projects/opencv\nopencv版本bug修补，版本更新，以及各种相关大型活动安排,还包含了opencv最近几个月内的活动路线，即未来将增加的功能等，可以掌握各种关于opencv进展情况的最新进展。\nhttp://tech.groups.yahoo.com/group/OpenCV/\nopencv雅虎邮件列表，据说是最好的opencv论坛，信息更新最新的地方。不过个人认为要查找相关主题的内容，在邮件列表中非常不方便。\nhttp://www.cmlab.csie.ntu.edu.tw/~jsyeh/wiki/doku.php\n台湾大学暑假集训网站，内有链接到与opencv集训相关的网页。感觉这种教育形式还蛮不错的。\nhttp://sourceforge.net/projects/opencvlibrary/\nopencv版本发布地方。\nhttp://code.opencv.org/projects/opencv/wiki/ChangeLog#241http://opencv.willowgarage.com/wiki/OpenCV%20Change%20Logs\nopencv版本内容更改日志网页,前面那个网页更新最快。\nhttp://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/tutorials.html\nopencv中文教程网页，分几个模块讲解，有代码有过程。内容是网友翻译opencv自带的doc文件里的。\nhttps://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n网友总结的常用带有cvpr领域常见算法code链接的网址，感觉非常的不错。\nhttp://fossies.org/dox/OpenCV-2.4.2/\n该网站可以查看opencv中一些函数的变量接口，还会列出函数之间的结构图。\nhttp://opencv.itseez.com/\nopencv的函数、类等查找网页，有导航，查起来感觉不错。\n优化：\nsubmodual优化网页。\nGeoff Gordon的优化课程，youtube上有对应视频。\n数学：\nhttp://www.youku.com/playlist_show/id_19465801.html\n《计算机中的数学》系列视频，8位老师10讲内容，生动介绍微积分和线性代数基本概念在计算机学科中的各种有趣应用！\nLinux学习资料：\nhttp://itercast.com/library/1\nlinux入门的基础视频教程，对于新手可选择看第一部分，视频来源于LinuxCast.net网站，还不错。\nOpenNI+Kinect相关：\nhttp://1.yuhuazou.sinaapp.com/\n网友晨宇思远的博客，主攻cvpr，ai等。\nhttp://blog.csdn.net/chenli2010/article/details/6887646\nkinect和openni学习资料汇总。\nhttp://blog.csdn.net/moc062066/article/category/871261\nOpenCV 计算机视觉 kinect的博客:\nhttp://kheresy.wordpress.com/index_of_openni_and_kinect/comment-page-5/\n网友Heresy的博客，里面有不少kinect的文章，写的比较详细。\nhttp://www.cnkinect.com/\n体感游戏中文网，有不少新的kinect资讯。\nhttp://www.kinectutorial.com/\nKinect体感开发网。\nhttp://code.google.com/p/openni-hand-tracker\nopenni_hand_tracking google code项目。\nhttp://blog.candescent.ch/\n网友的kinect博客，里面有很多手势识别方面的文章介绍，还有源码，不过貌似是基于c#的。\nhttps://sites.google.com/site/colordepthfusion/\n一些关于深度信息和颜色信息融合（fusion）的文章。\nhttp://projects.ict.usc.edu/mxr/faast/\nkinect新的库，可以结合OpenNI使用。\nhttps://sites.google.com/a/chalearn.org/gesturechallenge/\nkinect手势识别网站。\nhttp://www.ros.org/wiki/mit-ros-pkg\nmit的kinect项目，有code。主要是与手势识别相关。\nhttp://www.thoughtden.co.uk/blog/2012/08/kinecting-people-our-top-6-kinect-projects/\nkinect 2012年度最具创新的6个项目，有视频，确实够创新的！\nhttp://www.cnblogs.com/yangyangcv/archive/2011/01/07/1930349.html\nkinect多点触控的一篇博文。\nhttp://sourceforge.net/projects/kinect-mex/\nhttp://www.mathworks.com/matlabcentral/fileexchange/30242-kinect-matlab\n有关matlab for kinect的一些接口。\nhttp://news.9ria.com/2012/1212/25609.html\nAIR和Kinect的结合，有一些手指跟踪的code。\nhttp://eeeweba.ntu.edu.sg/computervision/people/home/renzhou/index.htm\n研究kinect手势识别的，任洲。刚毕业不久。\n其他网友cvpr领域的链接总结：\nhttp://www.cnblogs.com/kshenf/\n网友整理常用牛人链接总结，非常多。不过个人没有没有每个网站都去试过。所以本文也是我自己总结自己曾经用过的或体会过的。\nOpenGL有关:\nhttp://nehe.gamedev.net/\nNeHe的OpenGL教程英文版。\nhttp://www.owlei.com/DancingWind/\nNeHe的OpenGL教程对应的中文版，由网友周玮翻译的。\nhttp://www.qiliang.net/old/nehe_qt/\nNeHe的OpengGL对应的Qt版中文教程。\nhttp://blog.csdn.net/qp120291570\n网友\"左脑设计，右脑编程\"的Qt_OpenGL博客,写得还不错。\nhttp://guiliblearning.blogspot.com/\n这个博客对opengl的机制有所剖析，貌似要FQ才能进去。\ncvpr综合网站论坛博客等：\nhttp://www.cvchina.net/\n中国计算机视觉论坛\nhttp://www.cvchina.info/\n这个博客很不错，每次看完都能让人兴奋，因为有很多关于cv领域的科技新闻，还时不时有视频显示。另外这个博客里面的资源也整理得相当不错。中文的。\nhttp://www.bfcat.com/\n一位网友的个人计算机视觉博客，有很多关于计算机视觉前沿的东西介绍，与上面的博客一样，看了也能让人兴奋。\nhttp://blog.csdn.net/v_JULY_v/\n牛人博客，主攻数据结构，机器学习数据挖掘算法等。\nhttp://blog.youtueye.com/\n该网友上面有一些计算机视觉方向的博客,博客中附有一些实验的测试代码.\nhttp://blog.sciencenet.cn/u/jingyanwang\n多看pami才扯谈的博客，其中有不少pami文章的中文介绍。\nhttp://chentingpc.me/\n做网络和自然语言处理的，有不少机器学习方面的介绍。\nML常用博客资料等：\nhttp://freemind.pluskid.org/\n由 pluskid 所维护的 blog，主要记录一些机器学习、程序设计以及各种技术和非技术的相关内容，写得很不错。\nhttp://datasciencemasters.org/\n里面包含学ML/DM所需要的一些知识链接，且有些给出了视频教程，网页资料，电子书，开源code等，推荐！\nhttp://cs.nju.edu.cn/zhouzh/index.htm\n周志华主页，不用介绍了，机器学习大牛，更可贵的是他的很多文章都有源码公布。\nhttp://www.eecs.berkeley.edu/~jpaisley/Papers.htm\nJohn Paisley的个人主页，主要研究机器学习领域，有些文章有代码提供。\nhttp://foreveralbum.yo2.cn/\n里面有一些常见机器学习算法的详细推导过程。\nhttp://blog.csdn.net/abcjennifer\n浙江大学CS硕士在读，关注计算机视觉，机器学习，算法研究，博弈， 人工智能， 移动互联网等学科和产业。该博客中有很多机器学习算法方面的介绍。\nhttp://www.wytk2008.net/\n无垠天空的机器学习博客。\nhttp://www.chalearn.org/index.html\n机器学习挑战赛。\nhttp://licstar.net/\nlicstar的技术博客，偏自然语言处理方向。\n国内科研团队和牛人网页：\nhttp://vision.ia.ac.cn/zh/index_cn.html\n中科院自动化所机器视觉课题小组，有相关数据库、论文、课件等下载。\nhttp://www.cbsr.ia.ac.cn/users/szli/\n李子青教授个人主页，中科院自动化所cvpr领域牛叉人！\nhttp://www4.comp.polyu.edu.hk/~cslzhang/\n香港理工大学教授lei zhang个人主页，也是cvpr领域一大牛人啊，cvpr，iccv各种发表。更重要的是他所以牛叉论文的code全部公开，非常难得！\nhttp://liama.ia.ac.cn/wiki/start\n中法信息、自动化与应用联合实验室，里面很多内容不仅限而cvpr，还有ai领域一些其他的研究。\nhttp://www.cogsci.xmu.edu.cn/cvl/english/\n厦门大学特聘教授，cv领域一位牛人。研究方向主要为目标检测，目标跟踪，运动估计，三维重建，鲁棒统计学，光流计算等。\nhttp://idm.pku.edu.cn/index.aspx\n北京大学数字视频编码技术国家实验室。\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvm/\nlibsvm项目网址，台湾大学的，很火！\nhttp://www.jdl.ac.cn/user/sgshan/index.htm\n山世光，人脸识别研究比较牛。在中国科学院智能信息处理重点实验室\n国外科研团队和牛人网页：\nhttps://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n常见计算机视觉资源整理索引，国外学者整理，全是出名的算法，并且带有代码的，这个非常有帮助，其链接都是相关领域很火的代码。\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/txtv-groups.html\n国外学者整理的各高校研究所团队网站\nhttp://research.microsoft.com/en-us/groups/vision/\n微软视觉研究小组，不解释，大家懂的，牛！\nhttp://lear.inrialpes.fr/index.php\n法国国家信息与自动化研究所，有对应牛人的链接，论文项目网页链接，且一些code对应链接等。\nhttp://www.cs.ubc.ca/~pcarbo/objrecls/\nLearning to recognize objects with little supervision该篇论文的项目网页，有对应的code下载，另附有详细说明。\nhttp://www.eecs.berkeley.edu/~lbourdev/poselets/\nposelets相关研究界面，关于poselets的第一手资料。\nhttp://www.cse.oulu.fi/CMV/Research\n芬兰奥卢大学计算机科学与工程学院网页，里面有很多cv领域相关的研究，比如说人脸，脸部表情，人体行为识别，跟踪，人机交互等cv基本都涉及有。\nhttp://www.cs.cmu.edu/~cil/vision.html\n卡耐基梅隆大学计算机视觉主页，内容非常多。可惜的是该网站内容只更新到了2004年。\nhttp://vision.stanford.edu/index.html\n斯坦福大学计算机视觉主页，里面有非常非常多的牛人，比如说大家熟悉的lifeifei.\nhttp://www.wavelet.org/index.php\n关于wavelet研究的网页。\nhttp://civs.ucla.edu/\n加州大学洛杉矶分校统计学院，关于统计学习方面各种资料，且有相应的网上公开课。\nhttp://www.cs.cmu.edu/~efros/\n卡耐基梅隆大学Alexei(Alyosha)Efros教授个人网站，计算机图形学高手。\nhttp://web.mit.edu/torralba/www//\nmit牛人Associate教授个人网址，主要研究计算机视觉人体视觉感知，目标识别和场景理解等。\nhttp://people.csail.mit.edu/billf/\nmit牛人William T. Freeman教授，主要研究计算机视觉和图像学\nhttp://www.research.ibm.com/peoplevision/\nIBM人体视觉研究中心，里面除了有其研究小组的最新成果外，还有很多测试数据(特别是视频）供下载。\nhttp://www.vlfeat.org/\nvlfeat主页，vlfeat也是一个开源组织，主要定位在一些最流行的视觉算法开源上，Ｃ编写，其很多算法效果比opencv要好,不过数量不全，但是非常有用。\nhttp://www.robots.ox.ac.uk/~az/\nAndrew Zisserman的个人主页，这人大家应该熟悉，《计算机视觉中的多视几何》这本神书的作者之一。\nhttp://www.cs.utexas.edu/~grauman/\nKristenGrauman教授的个人主页，是个大美女，且是2011年“马尔奖”获得者，”马尔奖“大家都懂的，计算机视觉领域的最高奖项，目前无一个国内学者获得过。她的主要研究方法是视觉识别。\nhttp://groups.csail.mit.edu/vision/welcome/\nmit视觉实验室主页。\nhttp://code.google.com/p/sixthsense/\n曾经在网络上非常出名一个视频，一个作者研究的第六感装置，现在这个就是其开源的主页。\nhttp://vision.ucsd.edu/~pdollar/research.html#BehaviorRecognitionAnimalBehavior\nPiotr Dollar的个人主要，主要研究方向是人体行为识别。\nhttp://www.mmp.rwth-aachen.de/\n移动多媒体处理，将移动设备，计算机图像学，视觉，图像处理等结合的领域。\nhttp://www.di.ens.fr/~laptev/index.html\nIvan Laptev牛人主页，主要研究人体行为识别。有很多数据库可以下载。\nhttp://blogs.oregonstate.edu/hess/\nRob Hess的个人主要，里面有源码下载，比如说粒子滤波，他写的粒子滤波在网上很火。\nhttp://morethantechnical.googlecode.com/svn/trunk/\ncvpr领域一些小型的开源代码。\nhttp://iica.de/pd/index.py\n做行人检测的一个团队，内部有一些行人检测的代码下载。\nhttp://www.cs.utexas.edu/~grauman/research/pubs.html\nUT-Austin计算机视觉小组，包含的视觉研究方向比较广，且有的文章有源码，你只需要填一个邮箱地址，系统会自动发跟源码相关的信息过来。\nhttp://www.robots.ox.ac.uk/~vgg/index.html\nvisual geometry group\n图像:\nhttp://blog.sina.com.cn/s/blog_4cccd8d301012pw5.html\n交互式图像分割代码。\nhttp://vision.csd.uwo.ca/code/\ngraphcut优化代码。\n语音：\nhttp://danielpovey.com/kaldi-lectures.html\n语音处理中的kaldi学习。\n算法分析与设计（计算机领域的基础算法）：\nhttp://www.51nod.com/focus.html\n该网站主要是讨论一些算法题。里面的李陶冶是个大牛，回答了很多算法题。\n一些综合topic列表：\nhttp://www.cs.cornell.edu/courses/CS7670/2011fa/\n计算机视觉中的些topic（Special Topics in Computer Vision），截止到2011年为止，其引用的文章都是非常顶级的topic。\n书籍相关网页：\nhttp://www.imageprocessingplace.com/index.htm\n冈萨雷斯的《数字图像处理》一书网站，包含课程材料，matlab图像处理工具包，课件ppt等相关素材。\nConsumer Depth Cameras for Computer Vision\n很优秀的一本书，不过很贵，买不起啊！做深度信息的使用这本书还不错，google图中可以预览一部分。\nMaking.Things.See\n针对Kinect写的，主要关注深度信息，较为基础。书籍中有不少例子，貌似是java写的。\n国内一些AI相关的研讨会：\nhttp://www.iipl.fudan.edu.cn/MLA13/index.htm\n中国机器学习及应用研讨会(这个是2013年的)\n期刊会议论文下载：\nhttp://cvpapers.com/\n几个顶级会议论文公开下载界面，比如说ICCV,CVPR,ECCV,ACCV,ICPR,SIGGRAPH等。\nhttp://www.cvpr2012.org/\ncvpr2012的官方地址，里面有各种资料和信息，其他年份的地址类似推理更改即可。\nhttp://www.sciencedirect.com/science/journal/02628856\nICV期刊下载\nhttp://www.computer.org/portal/web/tpami\nTPAMI期刊，AI领域中可以算得上是最顶级的期刊了，里面有不少cvpr方面的内容。\nhttp://www.springerlink.com/content/100272/\nIJCV的网址。\nhttp://books.nips.cc/\nNIPS官网，有论文下载列表。\nhttp://graphlab.org/lsrs2013/program/\nLSRS （会议）地址，大规模推荐系统，其它年份依次类推。\n会议期刊相关信息：\nhttp://conferences.visionbib.com/Iris-Conferences.html\n该网页列出了图像处理，计算机视觉领域相关几乎所有比较出名的会议时间表。\nhttp://conferences.visionbib.com/Browse-conf.php\n上面网页的一个子网页，列出了最近的CV领域提交paper的deadline。\ncvpr相关数据库下载：\nhttp://research.microsoft.com/en-us/um/people/jckrumm/WallFlower/TestImages.htm\n微软研究院牛人Wallflower Paper的论文中用到的目标检测等测试图片\nhttp://archive.ics.uci.edu/ml/\nUCI数据库列表下载，最常用的机器学习数据库列表。\nhttp://www.cs.rochester.edu/~rmessing/uradl/\n人体行为识别通过关键点的跟踪视频数据库，Rochester university的\nhttp://www.research.ibm.com/peoplevision/performanceevaluation.html\nIBM人体视觉研究中心，有视频监控等非常多的测试视频。\nhttp://www.cvpapers.com/datasets.html\n该网站上列出了常见的cvpr研究的数据库。\nhttp://www.cs.washington.edu/rgbd-dataset/index.html\nRGB-D Object Dataset.做目标识别的。\nAI相关娱乐网页：\nhttp://en.akinator.com/\n该网站很好玩，可以测试你心里想出的一个人名(当然前提是这个人必须有一定的知名度)，然后该网站会提出一系列的问题，你可以选择yes or no,or I don’t know等等，最后系统会显示你心中所想的那个人。\nhttp://www.doggelganger.co.nz/\n人与狗的匹配游戏，摄像头采集人脸，呵呵…\nAndroid相关：\nhttps://code.google.com/p/android-ui-utils/\n该网站上有一些android图标,菜单等跟界面有关的设计工具,可以用来做一些简单的UI设计.\n工具和code下载：\nhttp://lear.inrialpes.fr/people/dorko/downloads.html\n6种常见的图像特征点检测子，linux下环境运行。不过只提供了二进制文件，不提供源码。\nhttp://www.cs.ubc.ca/~pcarbo/objrecls/index.html#code\nssmcmc的matlab代码,是Learning to recognize objects with little supervision这一系列文章用的源码，属于目标识别方面的研究。\nhttp://www.robots.ox.ac.uk/~timork/\n仿射无关尺度特征点检测算子源码，还有些其它算子的源码或二进制文件。\nhttp://www.vision.ee.ethz.ch/~bleibe/code/ism.html\n隐式形状模型(ISM)项目主页，作者Bastian Leibe提供了linux下运行的二进制文件。\nhttp://www.di.ens.fr/~laptev/download.html#stip\nIvan Laptev牛人主页中的STIP特征点检测code，但是也只是有二进制文件，无源码。该特征点在行为识别中该特征点非常有名。\nhttp://ai.stanford.edu/~quocle/\n斯坦福大学Quoc V.Le主页，上有它2011年行为识别文章的代码。\n开源软件：\nhttp://mloss.org/software/\n一些ML开源软件在这里基本都可以搜到，有上百个。\nhttps://github.com/myui/hivemall\nScalable machine learning library for Hive/Hadoop.\nhttp://scikit-learn.org/stable/\n基于python的机器学习开源软件，文档写得不错。\n挑战赛：\nhttp://www.chioka.in/kaggle-competition-solutions/\nkaggle一些挑战赛的code.\n公开课：\n网易公开课，国内做得很不错的公开课，翻译了一些国外出名的公开课教程，与国外公开课平台coursera有合作。\ncoursera在线教育网上公开课，很新，有个邮箱注册即可学习，有不少课程，且有对应的练习，特别是编程练习，超赞。\n斯坦福网上公开课链接，有统计学习，凸优化等课程。\nudacity公开课程下载链接，其实速度还可以。里面有不少好教程。\n机器学习公开课的连接，有不少课。\n转自：http://blog.csdn.net/tainyiliusha/article/details/10077081\n开源生物特征识别库 OpenBR\nOpenBR 是一个用来从照片中识别人脸的工具。还支持推算性别与年龄。 使用方法：$ br -algorithm FaceRecognition -compare me.jpg you.jpg更多OpenBR信息\n最近更新： OpenBR —— 开源的生物识别工具 发布于 13天前\n计算机视觉库 OpenCV\nOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...更多OpenCV信息\n最近更新： OpenCV 2.4.5 发布，开源计算机视觉库 发布于 2个月前\n人脸识别 faceservice.cgi\nfaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。更多faceservice.cgi信息\nJava视觉处理库 JavaCV\nJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...更多JavaCV信息\n视频监控系统 OpenVSS\nOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。更多OpenVSS信息\nOpenCV的.NET版 OpenCVDotNet\nOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。更多OpenCVDotNet信息\n人脸检测算法 jViolajones\njViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033更多jViolajones信息\n手势识别 hand-gesture-detection\n手势识别，用OpenCV实现更多hand-gesture-detection信息\n人脸检测与跟踪库 asmlibrary\nActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。更多asmlibrary信息\n开放模式识别项目 OpenPR\nPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。更多OpenPR信息\n运动检测程序 QMotion\nQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。更多QMotion信息\n图像特征提取 cvBlob\ncvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.更多cvBlob信息\nOpenCV的.Net封装 OpenCVSharp\nOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。更多OpenCVSharp信息\n人脸检测识别 mcvai-tracking\n提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...更多mcvai-tracking信息\n视频捕获 API VideoMan\nVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。更多VideoMan信息\n基于QT的计算机视觉库 QVision\n基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。更多QVision信息\n开源视线跟踪软件 ITU Gaze Tracker\n哥本哈根大学开源视线跟踪软件 The ITU Gaze Tracker is an open-source eye tracker that aims to provide a low-cost alternative to commercial gaze tracking systems and to make this technology more accessible. It is developed by the Gaze Grou...更多ITU Gaze Tracker信息\n图像处理和计算机视觉常用算法库 LTI-Lib\nLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具更多LTI-Lib信息\n实时图像/视频处理滤波开发包 GShow\nGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...更多GShow信息\nC++计算机视觉库 Integrating Vision Toolkit\nIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV\nOpenCV的Python封装 pyopencv\nOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...更多pyopencv信息\n模式识别和视觉库 RAVL\nRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。更多RAVL信息\nOpenSURF\n利用OpenCV和C++编写的SURF算法，作者Christopher Evans是首个利用OpenCV和C++结合的方法实现SURF算法。更多OpenSURF信息\n人脸识别库 rpflex\nrpflex 是一个 Flex 开发的库，用来识别照片中的人脸、眼镜和脖子。更多rpflex信息\nOpenCV优化 opencv-dsp-acceleration\n优化了OpenCV库在DSP上的速度。更多opencv-dsp-acceleration信息\nJava 计算机视觉库 BoofCV\nBoofCV 是一个 Java 的全新实时的计算机视觉库，BoofCV 易于使用而且具有非常高的性能。它提供了一系列从低层次的图像处理、小波去噪功能以及更高层次的三维几何视野。使用 BSD 许可证可在商业应用中使用。 这里有篇英文文章用来介绍 BoofCV 的使用。...更多BoofCV信息\n计算机视觉库 SimpleCV\nSimpleCV 将很多强大的开源计算机视觉库包含在一个便捷的Python包中。使用SimpleCV，你可以在统一的框架下使用高级算法，例如特征检测、滤波和模式识别。使用者不用清楚一些细节，比如图像比特深度、文件格式、颜色空间、缓冲区管理、特征值还有矩阵和图像...更多SimpleCV信息\n3D视觉库 fvision2010\n基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...更多fvision2010信息\n视觉快速开发平台 qcv\n计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。更多qcv信息\n计算机视觉算法 OpenVIDIA\nOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API'...更多OpenVIDIA信息\nC++计算机视觉库 ICL\nICL (Image Component Library) 是一种新型的C + +计算机视觉库，由比勒费尔德大学神经信息学组和CITEC开发。它兼顾了性能和用户友好性。 ICL提供了一个易于使用的类和函数的集合，可以开发复杂的计算机视觉应用。 在不到15行的C + +代码（见例子）可以写成...更多ICL信息\nMatlab计算机视觉包 mVision\nMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。更多mVision信息\nLua视觉开发库 libecv\nECV 是 lua 的计算机视觉开发库(目前只提供linux支持)更多libecv信息\nOpenCV的扩展库 ImageNets\nImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。更多ImageNets信息\n图像捕获 libv4l2cam\n对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出更多libv4l2cam信息\n高斯模型点集配准算法 gmmreg\n实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...更多gmmreg信息\nScilab的计算机视觉库 SIP\nSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。更多SIP信息\n计算机视觉和机器人技术的工具包 EGT\nThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...更多EGT信息\n计算机视觉库 BazAR\nBazAR 是基于特征点检测和匹配的计算机视觉库。 它能够快速检测和匹配图像中的已知物体，并且能够用于增强现实，它是计算机视觉研究的先进成果。更多BazAR信息\n计算机视觉库 VLFeat\n一个开源的计算机视觉库，实现了 SIFT,MSER, k-means, hierarchical k-means, agglomerative information bottleneck, quick shift等算法。由C语言编写,提供MATLAB接口，文档详细。支持跨平台。...更多VLFeat信息\nSTAIR Vision Library\nSTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。更多STAIR Vision Library信息\nScilab Image Processing Toolbox\nSIP 提供了图像处理、模式识别以及计算机视觉处理。 SIP is able to read/write images in almost 90 major formats, including JPEG, PNG, BMP, GIF, FITS, and TIFF. It includes routines for filtering, segmentation, edge detection, morphology, cu...更多Scilab Image Processing Toolbox信息\n3D计算机视觉库 openvis3d\n这个项目的目的是提供一个高效的3D计算机视觉库，用于图像和视频处理。它包括深度立体匹配、光流（运动）估计、遮挡检测和运动平台估计更多openvis3d信息\nlibvideogfx\n视频处理、计算机视觉和计算机图形学的快速开发库。更多libvideogfx信息\ngo-opencv\nGo-OpenCV 是 Go 语言版的 OpenCV 封装。更多go-opencv信息\nJavaScript图形绘制库 Toxiclibs.js\nToxiclibs.js 是一个开源的计算机图形设计库，无需外部依赖，使用 <canvas> 元素进行图形绘制。更多Toxiclibs.js信息\nOpenCL 封装库 CLOGS\nCLOGS 是 OpenCL C++ API 的高级封装库，其设计目的是集成其他 OpenCL 代码，包括同步 OpenCL 事件，当前支持两个操作：基数排序和独立扫描。更多CLOGS信息\n最近更新： CLOGS 1.2.0 发布，OpenCL 的封装库 发布于 2个月前\nopenvgr\nOpenVGR 包含以下几个实时处理模块 (基于 OpenRTM-1.0): 立体相机采集 (对于 IEEE 1394b 相机), 立体图像浏览器, 3-D 点云重建 (使用 OpenCV),  基于边缘的 3-D 物体检测 包含以下几个命令行工具: 模型建立, 多相机标定....更多openvgr信息\nsparse-stereo-vision\n使用 OpenCV 函数, 这个项目能从成对的立体图像中重建场景。更多sparse-stereo-vision信息\nPIV图形软件包 Fluere\nFluere是粒子图像测速（PIV）的图形软件包。 Fluere是高度优化的并行处理，并在多个平台上运行。该项目的目标是提供高质量的测速软件，采用PIV技术处理的最新进展的研究人员和教育工作者，而所使用的算法的完整的知识。更多Fluere信息\nstereoview\nstereoview 是一个立体可视化和标定工具更多stereoview信息\nResources in Visual Tracking  转自 ：http://blog.csdn.net/minstyrain/article/details/38640541\n这个应该是目前最全的Tracking相关的文章了,转载请注明出处。\n一、Surveyand benchmark：\n1.      PAMI2014：VisualTracking_ An Experimental Survey，代码：http://alov300pp.joomlafree.it/trackers-resource.html\n2.      CVPR2013:Online Object Tracking: A Benchmark(需FQ)\n3.      SignalProcessing  2011：Video Tracking Theory andPractice\n4.      ACCV2006：Tutorials-Advances in VisualTracking：中文：视觉跟踪的进展\n5.      Evaluationof an online learning approach for robust object tracking\n二、研究团体：\n1.      Universityof California at Merced：Ming-HsuanYang视觉跟踪当之无愧第一人，后面的人基本上都和气其有合作关系，他引近9000\nPublicationsPAMI：6，CVPR：26，ECCV：17，BMCV：6，NIPS：6，IJCV：3，ACCV：3\n代表作：RobustVisual Tracking via Consistent Low-Rank Sparse Learning\nFCT,IJCV2014:FastCompressive Tracking\nRST,PAMI2014:RobustSuperpixel Tracking; SPT,ICCV2011, Superpixeltracking\nSVD,TIP2014:LearningStructured Visual Dictionary for Object Tracking\nECCV2014: SpatiotemporalBackground Subtraction Using Minimum Spanning Tree and Optical Flow\nPAMI2011:RobustObject Tracking with Online Multiple Instance Learning\nMIT,CVPR2009: Visualtracking with online multiple instance learning\nIJCV2008: IncrementalLearning for Robust Visual Tracking\n2.      SeoulNational University Professor：KyoungMuLee2013年在PAMI上发表5篇，至今无人能及\n文献列表PAMI:13,CVPR:30,ECCV:12,ICCV:8,PR:4\nPAMI2014：A GeometricParticle Filter for Template-Based Visual Tracking\nECCV2014: Robust Visual Tracking with Double Bounding Box Model\nPAMI2013:HighlyNonrigid Object Tracking via Patch-based Dynamic Appearance Modeling\nCVPR2014: Interval Tracker: Tracking by Interval Analysis\nCVPR2013: MinimumUncertainty Gap for Robust Visual Tracking\nCVPR2012:RobustVisual Tracking using Autoregressive Hidden Markov Model\nVTS,ICCV2011:Tracking by Sampling Trackers.\nVTD,CVPR2010: VisualTracking Decomposition\nTST,ICCV2011:Tracking by sampling trackers\n3.      TempleUniversity，凌海滨\nPublication List PMAI:4,CVPR:19,ICCV:17,ECCV:5,TIP:9\nCVPR2014:Multi-targetTracking with Motion Context in Tenor Power Iteration\nECCV2014:TransferLearning Based Visual Tracking with Gaussian Process Regression\nICCV2013:Findingthe Best from the Second Bests - Inhibiting Subjective Bias in Evaluation ofVisual Tracking Algorithms\nCVPR2013: Multi-targetTracking by Rank-1 Tensor Approximation\nCVPR2012:RealTime Robust L1 Tracker Using Accelerated Proximal Gradient Approach\nTIP2012: Real-timeProbabilistic Covariance Tracking with Efficient Model Update\nICCV2011: BlurredTarget Tracking by Blur-driven Tracker\nPAMI2011ICCV2009: RobustVisual Tracking and Vehicle Classification via Sparse Representation\nICCV2011:RobustVisual Tracking using L1 Minimization\nL1O,CVPR2011: Minimumerror bounded efficient l1 tracker with occlusion detection\nL1T, ICCV2009:Robustvisual tracking using l1 minimization\n4.      HongKong Polytechnic University AssociateProfessor: Lei Zhang\nPapersPAMI:2,CVPR:18,ICCV:14,ECCV:12,ICPR:6,PR:28,TIP:4\nSTC,ECCV2014: FastTracking via Dense Spatio-Temporal Context Learning\nFCT,PAMI2014,ECCV2012:Fast CompressiveTracking, Minghsuan Yang\nIETComputer Vision2012:Scale and Orientation Adaptive Mean Shift Tracking\nIJPRAI2009:RobustObject Tracking using Joint Color-Texture Histogram\n5.      大连理工大学教授 卢湖川国内追踪领域第一人\nCVPR2014:VisualTracking via Probability Continuous Outlier Model\nTIP2014:VisualTracking via Discriminative Sparse Similarity Map\nTIP2014: RobustSuperpixel Tracking\nTIP2014: RobustObject Tracking via Sparse Collaborative Appearance Model\nCVPR2013: LeastSoft-threshold Squares Tracking, MinghsuanYang\nTIP2013:Online Object Trackingwith Sparse Prototypes, Minghsuan Yang\nSignalProcessing Letters2013: Graph-RegularizedSaliency Detection With Convex-Hull-Based Center Prior\nSignalProcessing2013: On-line LearningParts-based Representation via Incremental Orthogonal Projective Non-negativeMatrix Factorization\nCVPR2012:RobustObject Tracking viaSparsity-based Collaborative Model, MinghsuanYang\nCVPR2012:VisualTracking via Adaptive Structural Local Sparse Appearance Model, MinghsuanYang\nSignalProcessing Letters 2012:Object tracking via 2DPCA and L1-regularization\nIETImage Processing 2012:Visual Tracking via Bag of Features\nICPR2012:Superpixel Level Object Recognition Under Local Learning Framework\nICPR2012: Fragment-BasedTracking Using Online Multiple Kernel Learning\nICPR2012: ObjectTracking Based On Local Learning\nICPR2012: ObjectTracking with L2_RLS\nICPR2011:ComplementaryVisual Tracking\nFG2011:OnlineMultiple Support Instance Tracking\nSignalProcessing2010: A novel methodfor gaze tracking by local pattern model and support vector regressor\nACCV2010: OnFeature Combination and Multiple Kernel Learning for Object Tracking\nACCV: RobustTracking Based on Pixel-wise Spatial Pyramid and Biased Fusion\nACCV2010: HumanTracking by Multiple Kernel Boosting with Locality Affinity Constraints\nICCV2011:SuperpixelTracking, Minghsuan Yang\nICPR2010: RobustTracking Based on Boosted Color Soft Segmentation and ICA-R\nICPR2010: IncrementalMPCA for Color Object Tracking\nICPR2010: Bagof Features Tracking\nICPR2008: GazeTracking By Binocular Vision and LBP Features\n6.      南京信息工程大学教授,KaiHua Zhang\n7.      OregonstateProfessor,Sinisa Todorovic由视频分割转向Tracking\nCSL,CVPR2014: Multi-ObjectTracking via Constrained Sequential Labeling\nCVPR2011:MultiobjectTracking as Maximum Weight Independent Set\n8.      GrazUniversity of Technology, Austria，Horst Possegger博士\nCVPR2014:OcclusionGeodesics for Online Multi-Object Tracking\nCVPR2013: RobustReal-Time Tracking of Multiple Objects by Volumetric Mass Densities\n9.      马里兰大学Zdenek Kalal博士\nTLD,PAMI2011: Tracking-Learning-Detection\nTIP2010: Face-TLD:Tracking-Learning-Detection Applied to Faces\nICPR2010:Forward-BackwardError: Automatic Detection of Tracking Failures\nCVPR2010: P-N Learning:Bootstrapping Binary Classifiers by Structural Constraints\nBMVC2008: Weighted Sampling forLarge-Scale Boosting\n中文讲解：\nTLD视觉跟踪算法\nTLD源码深度分析\n庖丁解牛TLD\nTLD（Tracking-Learning-Detection）学习与源码理解\n三、其他早期工作：\nTracking of a Non-Rigid ObjectviaPatch-based Dynamic Appearance Modeling and Adaptive Basin Hopping Monte CarloSampling\ntracking-by-detection\n粒子滤波演示与opencv代码\nopencv学习笔记-入门（6）-camshift\nCamshift算法原理及其Opencv实现\nCamshift算法\nCamShift算法，OpenCV实现1--Back Projection\n目标跟踪学习笔记_2(particle filter初探1)\n目标跟踪学习笔记_3(particle filter初探2)\n目标跟踪学习笔记_4(particle filter初探3)\n目标跟踪学习系列一:on-line boosting and vision 阅读\n最近由于转行做医学图像处理，因此加入几个医学方面的库以及最近在微博疯传的几个库：\nCCV库\nccv是一个基于C语言的、带缓存的现代计算机视觉库。\n链接：https://github.com/liuliu/ccv，http://libccv.org/\nvisionworkbench库\nvisionworkbench是NASA开发的通用图像处理、计算机视觉算法工具库。代码充分利用C++模板和泛型编程等特性，结构清晰，使用方便。工具库功能也很多，包括了常用数学计算、文件IO、相机模型、几何计算、特征点、Bundle Adjustment。对机器人视觉、三维视觉方面的研发想必很有帮助。\n链接：https://github.com/nasa/visionworkbench/\nITK库\nITK(Insight Segmentation and Registration Toolkit) 是一个开源，跨平台的图像分析框架，里面有大量的前沿算法，广泛用于图像配准和分割。ITK使用C++开发，可由CMake生成不同环境下的可编译工程，并且ITK有对Tcl, Python和Java的封装层，使得开发者可以使用不同的语言进行开发。\n链接：http://www.itk.org/\nVTK库\nVtk，（visualization toolkit）是一个开放资源的免费软件系统，主要用于三维计算机图形学、图像处理和可视化。Vtk是在面向对象原理的基础上设计和实现的，它的内核是用C++构建的，包含有大约250,000行代码，2000多个类，还包含有几个转换界面，因此也可以自由的通过Java，Tcl/Tk和Python各种语言使用vtk。\nhttp://www.vtk.org/\nIGSTK\nThe Image-Guided Surgery Toolkit is a high-level, component-based framework which provides a common functionality for image-guided surgery applications. The framework is a set of high-level components integrated with low-level open source software libraries and application programming interfaces (API) from hardware vendors.\n链接：http://www.igstk.org/\nDCMTK\nDCMTK是由德国offis公司提供的开源项目，并拥有相应的版权。这个开发包经过10多年的开发和维护，已经基本实现了DICOM协议的所有内容。该开发包提供所有的源代码、支持库和帮助文档。DCMTK提供了在各种操作系统下使用的可能版本，如LINUX、SUN、MACOS、WINDOWS等，用户可根据自己的开发平台进行编译。\n链接：http://www.dcmtk.org/\n陆续更新中。"}
{"content2":"计算机视觉、计算机图形学、图像处理的区别\n计算机视觉的历史\n计算机视觉的概念\n计算机视觉的任务\n- 计算机视觉的目标\n计算机视觉、计算机图形学、图像处理的区别\n计算机视觉（Computer Vision）=图像处理+机器学习，计算机视觉是是给定图象，从图象提取信息，包括景象的三维结构，运动检测，识别物体等。\n计算机图形学（Computer Graphics）讲的是图形，也就是图形的构造方式，是一种从无到有的概念，从数据得到图像。是给定关于景象结构、表面反射特性、光源配置及相机模型的信息，生成图像。\n数字图像处理（Digital Image Processing）是对已有的图像进行变换、分析、重构，得到的仍是图像。\n区别：\nComputer Vision，简称 CV。输入的是图像或图像序列，通常来自相机或usb摄像头。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。\nComputer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb颜色等。输出的是图像，即二维像素数组。\nDigital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。\nCreated with Raphaël 2.1.0\n图像\n图像\n结构+纹理+光照+语义\n结构+纹理+光照+语义\n数字图像处理\n计算机视觉\n计算机图形\n计算机视觉历史\nJames Jerome Gibson理论（整体感知，不可计算）：\n-适应外界环境\n-控制自身的运动\nD Marr（可计算）：\n-重构可见表面的几何形\n##计算机视觉的概念\n-计算机视觉是研究用计算机来模拟人和生物视觉系统功能的技术学科。\n-计算机视觉的研究目标是使计算机具有通过一副或多幅图像认知周围环境信息的能力\n计算机视觉的任务\n中心任务：\n- 对图像进行理解\n这里的理解包括形状、位置、运动\n具体任务：\n- 特征提取、特征描述、特征匹配等\n- 图像/视频分割\n- 目标跟踪\n- 目标（物体）识别\n- 目标三维重建\n- 等等\n计算机视觉的目标\n主要目标：\n- 物体识别\n计算机视觉的4个重要阶段\n马尔计算机视觉建立->主动视觉大辩论->分层三维重建理论->基于学习的视觉\n马尔计算机视觉建立:1981-\n主动视觉大辩论:1988-1994-\n分层三维重建理论: 1992-\n马尔计算机基于学习的视觉:2001-"}
{"content2":"本文转载于http://www.yuanyong.org/cv/cv-code-three.html\n按类别分类\n特征提取\nSURF特征: http://www.vision.ee.ethz.ch/software/index.de.html(当然这只是其中之一)\nLBP特征(一种纹理特征)：http://www.comp.hkbu.edu.hk/~icpr06/tutorials/Pietikainen.html\nFast Corner Detection（OpenCV中的Fast算法）:FAST Corner Detection -- Edward Rosten\n机器视觉\nA simple object detector with boosting(Awarded the Best Short Course Prize at ICCV 2005，So了解adaboost的推荐之作)：http://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html\nBoosting(该网页上有相当全的Boosting的文章和几个Boosting代码，本人推荐)：http://cbio.mskcc.org/~aarvey/boosting_papers.html\nAdaboost Matlab 工具：http://graphics.cs.msu.ru/en/science/research/machinelearning/adaboosttoolbox\nMultiBoost(不说啥了，多类Adaboost算法的程序)：http://sourceforge.net/projects/multiboost/\nTextonBoost(我们教研室王冠夫师兄的毕设): Jamie Shotton - Code\nLibSvm的老爹（推荐）: http://www.csie.ntu.edu.tw/~cjlin/\nConditional Random Fields（CRF论文+Code列表，推荐）\nCRF++: Yet Another CRF toolkit\nConditional Random Field (CRF) Toolbox for Matlab\nTree CRFs\nLingPipe: Installation\nHidden Markov Models（推荐）\n隐马尔科夫模型(Hidden Markov Models)系列之一 - eaglex的专栏 - 博客频道 - CSDN.NET（推荐）\n综合代码\nCvPapers(好吧，牛吧网站，里面有ICCV，CVPR，ECCV，SIGGRAPH的论文收录，然后还有一些论文的代码搜集，要求加精！)：http://www.cvpapers.com/\nComputer Vision Software(里面代码很多，并详细的给出了分类)：http://peipa.essex.ac.uk/info/software.html\n某人的Windows Live（我看里面东东不少就收藏了）：https://skydrive.live.com/?cid=3b6244088fd5a769#cid=3B6244088FD5A769&id=3B6244088FD5A769!523\nMATLAB and Octave Functions for Computer Vision and Image Processing（这个里面的东西也很全，只是都是用Matlab和Octave开发的）：http://www.csse.uwa.edu.au/~pk/research/matlabfns/\nComputer Vision Resources（里面的视觉算法很多，给出了相应的论文和Code，挺好的）：https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\nMATLAB Functions for Multiple View Geometry（关于物体多视角计算的库）：http://www.robots.ox.ac.uk/~vgg/hzbook/code/\nEvolutive Algorithm based on Naïve Bayes models Estimation（单独列了一个算法的Code）：http://www.cvc.uab.cat/~xbaro/eanbe/#_Software\n主页代码\nPablo Negri's Home Page\nJianxin Wu's homepage\nPeter Carbonetto\nMarkov Random Fields for Super-Resolution\nDetecting and Sketching the Common\nPedro Felzenszwalb\nHae JONG, SEO\nCAP 5416 - Computer Vision\nParallel Tracking and Mapping for Small AR Workspaces (PTAM)\nDeva Ramanan - UC Irvine - Computer Vision\nRaghuraman Gopalan\nHui Kong\nJamie Shotton - Post-Doctoral Researcher in Computer Vision\nJean-Yves AUDIBERT\nOlga Veksler\nStephen Gould\nPublications (Last Update: 09/30/10)\nKarim Ali - FlowBoost\nA simple parts and structure object detector\nCode - Oxford Brookes Vision Group\nTaku Kudo\n按专题分类\n行人检测\nHistogram of Oriented Gradient (Windows)\nINRIA Pedestrian detector\nPoselets\nWilliam Robson Schwartz - Softwares\ncalvin upper-body detector v1.02\nRPT@CVG\nMain Page\nSource Code\nDr. Luciano Spinello\nPedestrian Detection\nClass-Specific Hough Forests for Object Detection\nJianxin Wu's homepage（就是上面的）\nBerkeley大学做的Pedestrian Detector，使用交叉核的支持向量机，特征使用HOG金字塔，提供Matlab和C++混编的代码：http://www.cs.berkeley.edu/~smaji/projects/ped-detector/\n视觉壁障\nHigh Speed Obstacle Avoidance using Monocular Vision and Reinforcement Learning\nTLD(2010年很火的tracking算法)\nonline boosting trackers\nBoris Babenko\nOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递 归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/\n物体检测算法\nObject Detection\nSoftware for object detection\n人脸检测\nSource Code\n10个人脸检测项目\nJianxin Wu's homepage（又是这货）\nICA独立成分分析\nAn ICA page-papers,code,demo,links (Tony Bell)\nFastICA\nCached k-d tree search for ICP algorithms\n滤波算法\n卡尔曼滤波：The Kalman Filter(终极网页)\nBayesian Filtering Library: The Bayesian Filtering Library\n路面识别\nSource Code\nVanishing point detection for general road detection\n分割算法\nMATLAB Normalized Cuts Segmentation Code：software\n超像素分割：SLIC Superpixels\nReference:\n[1]. http://blog.sina.com.cn/s/blog_5086c3e20101kdy5.html"}
{"content2":"Summary：国内提供计算机视觉(CV)算法岗位的公司名单\nAuthor：Amusi\nDate：2019-01-15\n微信公众号：CVer\ngithub：https://github.com/amusi/CV-Jobs\n国内提供计算机视觉(CV)算法岗位的公司名单(含外企和国内公司)，欢迎大家到CV-Jobs中提交issues进行补充\n文章目录\n北京\n上海\nTODO\n北京\n上海\nTODO\n深圳\n杭州\n南京\n广州"}
{"content2":"搜集如下博客的资源：以供自己学习是使用，感谢被转载的博客，以后抽时间再汇总综合一下，去除重复的。\n计算机视觉CV领域大牛及研究组主页链接: http://blog.csdn.net/libing403/article/details/70473859?locationNum=4&fps=1\n【机器视觉】计算机视觉牛人博客和代码汇总（全）\n: http://blog.csdn.net/Taily_Duan/article/details/53171065?locationNum=8&fps=1\n计算机视觉Computer Vision领域博客资源: http://blog.csdn.net/garfielder007/article/details/50866107\n计算机视觉整理库：http://blog.csdn.net/c2a2o2/article/details/77920507?locationNum=2&fps=1\n计算机视觉领域经典论文源码大全\n： http://blog.csdn.net/ddreaming/article/details/52416643\n计算机视觉牛人博客和代码汇总（全）\n每个做过或者正在做研究工作的人都会关注一些自己认为有价值的、活跃的研究组和个人的主页，关注他们的主页有时候比盲目的去搜索一些论文有用多了，大牛的或者活跃的研究者主页往往提供了他们的最新研究线索，顺便还可八一下各位大牛的经历，对于我这样的小菜鸟来说最最实惠的是有时可以找到源码，很多时候光看论文是理不清思路的。\n1 牛人Homepages（随意排序，不分先后）：\n1.USC Computer Vision Group：南加大，多目标跟踪/检测等；\n2.ETHZ Computer Vision Laboratory：苏黎世联邦理工学院，欧洲最好的几个CV/ML研究机构；\n3.Helmut Grabner：Online Boosting and Vision的作者，tracking by online feature selection的早期经典，貌似现在不是很活跃了，跑去创业了；\n4.Robert T. Collins：PSU，也是跟踪界的大牛；\n5.Ying Wu：美国西北大学，华人学者中的翘楚；\n6.Junsong Yuan：NTU，上面Wu老师的学生；\n7.James W. Davis：俄亥俄州立，视频监控；\n8. The Australian Centre for Visual Technologies：阿德莱德大学的CV组，最近也是exceedingly active & fruitful；\n9.Chunhua Shen：属上面的ACVT组，最近非常活跃；\n10.Xi Li：同属ACVT，之前是中科院的PHD，跟踪方面的论文很多，有理论深度；\n11.Haibin Ling：天普大学，L1-Tracker及后续扩展，源码分享；\n12.Learning, Recognition, and Surveillance：奥地利 TU Graz，在线学习，跟踪/检测等，active！源码分享；\n13.Statistical Visual Computing Laboratory：UCSD，光听名字就很学术吧，Saliency研究很有名；\n14.David Ross：多伦多大学，IVT的作者，跟踪中Generative表观的经典中的经典，提供源码，IVT的代码结构被后来很多人引用，值得一读；\n15.EPFL, Computer Vision Laboratory：洛桑理工的学院，和上面的的ETHZ CV lab同样是欧洲最好的CV研究大组；\n16.Jamie Shotton：属微软剑桥研究中心，Decision/Regression Forests；\n17.Sinisa Todorovic：俄勒冈州立，行为分析等；\n18.Shi Jianbo：大名鼎鼎的Good Feature to Track作者，目前方向行为分析和多目标跟踪等；\n19.Shai Avidan：特拉维夫大学，大牛级，可算是Tracking-by-detection的开创者，Ensemble Tracking, SVM Tracking；\n20.Visual Information Processing and Learning：中科院计算所，山世光老师的研究组，不需介绍了吧；\n21.Shaogang Gong：Queen Mary University of London，各种PAMI，IJCV；\n22.Yang Jian：南京理工大学，2DPCA，人脸识别；\n23.CALVIN：weakly supervised learning，objectness；\n24.Learning & Vision Group：NUS，稀疏表示；\n26.Xiaogang Wang：CUHK，active & fruitful，行人检测，群体行为分析；\n27.Zhou, Bolei：上面Wang老师硕士研究生，群体行为，看看人家的Publications已经轻松甩国内博士好几条街；\n28.Computational Vision Group：Leader--Deva Ramanan；\n29.Zhang Lei：香港理工，稀疏表示，人脸识别，可以算大中华区比较活跃的研究组了，几乎每篇论文都有对应源码；\n30.Zhang Kaihua：上面Zhang老师学生，Compressive Tracking；\n31.Pramod Sharma：离线训练检测器的在线自适应，貌似是个不错的topic；\n32.Loris Bazzani：person re-id，他的SDALF(code)描述子经常被用来做为比较对象，说明还是有参考价值的；\n33.Pedro Felzenszwalb：布朗大学，目标检测，新新N人一枚；\n34.Vijayakumar Bhagavatula：IEEE Fellow， correlation filters；\n35.Laurens van der Maaten：MLer.\n牛人主页（主页有很多论文代码）\nSerge Belongie at UC San Diego\nAntonio Torralba at MIT\nAlexei Ffros at CMU\nCe Liu at Microsoft Research New England\nVittorio Ferrari at Univ.of Edinburgh\nKristen Grauman at UT Austin\nDevi Parikh at  TTI-Chicago (Marr Prize at ICCV2011)\nJohn Wright at Columbia Univ.\nPiotr Dollar at CalTech\nBoris Babenko at UC San Diego\nDavid Ross at Google/Youtube\nDavid Donoho at Stanford Univ.\n大神们：\nWilliam T. Freeman at MIT\nRoberto Cipolla at Cambridge\nDavid Lowe at Univ. of British Columbia\nMubarak Shah at Univ. of Central Florida\nYi Ma at MSRA\nTinne Tuytelaars at K.U. Leuven\nTrevor Darrell at U.C. Berkeley\nMichael J. Black at Brown Univ.\n重要研究组：\nComputer Vision Group at UC Berkeley\nRobotics Research Group at Univ. of Oxford\nLEAR at INRIA\nComputer Vision Lab at Stanford\nComputer Vision Lab at EPFL\nComputer Vision Lab at ETH Zurich\nComputer Vision Lab at Seoul National Univ.\nComputer Vision Lab at UC San Diego\nComputer Vision Lab at UC Santa Cruz\nComputer Vision Lab at Univ. of Southern California\nComputer Vision Lab at Univ. of Central Florida\nComputer Vision Lab at Columbia Univ.\nUCLA Vision Lab\nMotion and Shape Computing Group at George Mason Univ.\nRobust Image Understanding Lab at Rutgers Univ.\nIntelligent Vision Systems Group at Univ. of Bonn\nInstitute for Computer Graphics and Vision at Graz Univ. of Tech.\nComputer Vision Lab. at Vienna Univ. of Tech.\nComputational Image Analysis and Radiology at Medical Univ. of Vienna\nPersonal Robotics Lab at CMU\nVisual Perception Lab at Purdue Univ.\n潜力牛人：\nJuergen Gall at ETH Zurich\nMatt Flagg at Georgia Tech.\nMathieu Salzmann at TTI-Chicago\nGerg Shakhnarovich at TTI-Chicago\nTaeg Sang Cho at MIT\nJianchao Yang at UIUC\nStefan Roth at TU Darmstadt\nPeter Kontschieder at Graz Univ. of Tech.\nDominik Alexander Klein at Univ. of Bonn\nYinan Yu at CASIA (PASCAL VOC 2010 Detection Challenge Winner)\nZdenek Kalal at FPFL\nJulien Pilet at FPFL\nKenji Okuma\n2 个人、研究机构链接\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华；http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站；http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山；http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）清华大学程明明博士：http://mmcheng.net/ 图像分割、检索\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/\n（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；\n（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、\n（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库\n（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理\n（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、\n（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n3 代码汇总\n一、特征提取Feature Extraction：\nSIFT [1] [Demo program][SIFT Library] [VLFeat]\nPCA-SIFT [2] [Project]\nAffine-SIFT [3] [Project]\nSURF [4] [OpenSURF] [Matlab Wrapper]\nAffine Covariant Features [5] [Oxford project]\nMSER [6] [Oxford project] [VLFeat]\nGeometric Blur [7] [Code]\nLocal Self-Similarity Descriptor [8] [Oxford implementation]\nGlobal and Efficient Self-Similarity [9] [Code]\nHistogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]\nGIST [11] [Project]\nShape Context [12] [Project]\nColor Descriptor [13] [Project]\nPyramids of Histograms of Oriented Gradients [Code]\nSpace-Time Interest Points (STIP) [14][Project] [Code]\nBoundary Preserving Dense Local Regions [15][Project]\nWeighted Histogram[Code]\nHistogram-based Interest Points Detectors[Paper][Code]\nAn OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]\nFast Sparse Representation with Prototypes[Project]\nCorner Detection [Project]\nAGAST Corner Detector: faster than FAST and even FAST-ER[Project]\nReal-time Facial Feature Detection using Conditional Regression Forests[Project]\nGlobal and Efficient Self-Similarity for Object Classification and Detection[code]\nWαSH: Weighted α-Shapes for Local Feature Detection[Project]\nHOG[Project]\nOnline Selection of Discriminative Tracking Features[Project]\n二、图像分割Image Segmentation：\nNormalized Cut [1] [Matlab code]\nGerg Mori’ Superpixel code [2] [Matlab code]\nEfficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]\nMean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]\nOWT-UCM Hierarchical Segmentation [5] [Resources]\nTurbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]\nQuick-Shift [7] [VLFeat]\nSLIC Superpixels [8] [Project]\nSegmentation by Minimum Code Length [9] [Project]\nBiased Normalized Cut [10] [Project]\nSegmentation Tree [11-12] [Project]\nEntropy Rate Superpixel Segmentation [13] [Code]\nFast Approximate Energy Minimization via Graph Cuts[Paper][Code]\nEfﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]\nIsoperimetric Graph Partitioning for Image Segmentation[Paper][Code]\nRandom Walks for Image Segmentation[Paper][Code]\nBlossom V: A new implementation of a minimum cost perfect matching algorithm[Code]\nAn Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]\nGeodesic Star Convexity for Interactive Image Segmentation[Project]\nContour Detection and Image Segmentation Resources[Project][Code]\nBiased Normalized Cuts[Project]\nMax-flow/min-cut[Project]\nChan-Vese Segmentation using Level Set[Project]\nA Toolbox of Level Set Methods[Project]\nRe-initialization Free Level Set Evolution via Reaction Diffusion[Project]\nImproved C-V active contour model[Paper][Code]\nA Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]\nLevel Set Method Research by Chunming Li[Project]\nClassCut for Unsupervised Class Segmentation[code]\nSEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]\n三、目标检测Object Detection：\nA simple object detector with boosting [Project]\nINRIA Object Detection and Localization Toolkit [1] [Project]\nDiscriminatively Trained Deformable Part Models [2] [Project]\nCascade Object Detection with Deformable Part Models [3] [Project]\nPoselet [4] [Project]\nImplicit Shape Model [5] [Project]\nViola and Jones’s Face Detection [6] [Project]\nBayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]\nHand detection using multiple proposals[Project]\nColor Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]\nDiscriminatively trained deformable part models[Project]\nGradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]\nImage Processing On Line[Project]\nRobust Optical Flow Estimation[Project]\nWhere's Waldo: Matching People in Images of Crowds[Project]\nScalable Multi-class Object Detection[Project]\nClass-Specific Hough Forests for Object Detection[Project]\nDeformed Lattice Detection In Real-World Images[Project]\nDiscriminatively trained deformable part models[Project]\n四、显著性检测Saliency Detection：\nItti, Koch, and Niebur’ saliency detection [1] [Matlab code]\nFrequency-tuned salient region detection [2] [Project]\nSaliency detection using maximum symmetric surround [3] [Project]\nAttention via Information Maximization [4] [Matlab code]\nContext-aware saliency detection [5] [Matlab code]\nGraph-based visual saliency [6] [Matlab code]\nSaliency detection: A spectral residual approach. [7] [Matlab code]\nSegmenting salient objects from images and videos. [8] [Matlab code]\nSaliency Using Natural statistics. [9] [Matlab code]\nDiscriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]\nLearning to Predict Where Humans Look [11] [Project]\nGlobal Contrast based Salient Region Detection [12] [Project]\nBayesian Saliency via Low and Mid Level Cues[Project]\nTop-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]\nSaliency Detection: A Spectral Residual Approach[Code]\n五、图像分类、聚类Image Classification, Clustering\nPyramid Match [1] [Project]\nSpatial Pyramid Matching [2] [Code]\nLocality-constrained Linear Coding [3] [Project] [Matlab code]\nSparse Coding [4] [Project] [Matlab code]\nTexture Classification [5] [Project]\nMultiple Kernels for Image Classification [6] [Project]\nFeature Combination [7] [Project]\nSuperParsing [Code]\nLarge Scale Correlation Clustering Optimization[Matlab code]\nDetecting and Sketching the Common[Project]\nSelf-Tuning Spectral Clustering[Project][Code]\nUser Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]\nFilters for Texture Classification[Project]\nMultiple Kernel Learning for Image Classification[Project]\nSLIC Superpixels[Project]\n六、抠图Image Matting\nA Closed Form Solution to Natural Image Matting [Code]\nSpectral Matting [Project]\nLearning-based Matting [Code]\n七、目标跟踪Object Tracking：\nA Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]\nObject Tracking via Partial Least Squares Analysis[Paper][Code]\nRobust Object Tracking with Online Multiple Instance Learning[Paper][Code]\nOnline Visual Tracking with Histograms and Articulating Blocks[Project]\nIncremental Learning for Robust Visual Tracking[Project]\nReal-time Compressive Tracking[Project]\nRobust Object Tracking via Sparsity-based Collaborative Model[Project]\nVisual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]\nOnline Discriminative Object Tracking with Local Sparse Representation[Paper][Code]\nSuperpixel Tracking[Project]\nLearning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]\nOnline Multiple Support Instance Tracking [Paper][Code]\nVisual Tracking with Online Multiple Instance Learning[Project]\nObject detection and recognition[Project]\nCompressive Sensing Resources[Project]\nRobust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]\nTracking-Learning-Detection[Project][OpenTLD/C++ Code]\nthe HandVu：vision-based hand gesture interface[Project]\nLearning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]\n八、Kinect：\nKinect toolbox[Project]\nOpenNI[Project]\nzouxy09 CSDN Blog[Resource]\nFingerTracker 手指跟踪[code]\n九、3D相关：\n3D Reconstruction of a Moving Object[Paper] [Code]\nShape From Shading Using Linear Approximation[Code]\nCombining Shape from Shading and Stereo Depth Maps[Project][Code]\nShape from Shading: A Survey[Paper][Code]\nA Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]\nMulti-camera Scene Reconstruction via Graph Cuts[Paper][Code]\nA Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]\nReconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]\nMonocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]\nLearning 3-D Scene Structure from a Single Still Image[Project]\n十、机器学习算法：\nMatlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]\nRandom Sampling[code]\nProbabilistic Latent Semantic Analysis (pLSA)[Code]\nFASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]\nFast Intersection / Additive Kernel SVMs[Project]\nSVM[Code]\nEnsemble learning[Project]\nDeep Learning[Net]\nDeep Learning Methods for Vision[Project]\nNeural Network for Recognition of Handwritten Digits[Project]\nTraining a deep autoencoder or a classifier on MNIST digits[Project]\nTHE MNIST DATABASE of handwritten digits[Project]\nErsatz：deep neural networks in the cloud[Project]\nDeep Learning [Project]\nsparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]\nWeka 3: Data Mining Software in Java[Project]\nInvited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]\nCNN - Convolutional neural network class[Matlab Tool]\nYann LeCun's Publications[Wedsite]\nLeNet-5, convolutional neural networks[Project]\nTraining a deep autoencoder or a classifier on MNIST digits[Project]\nDeep Learning 大牛Geoffrey E. Hinton's HomePage[Website]\nMultiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]\nSparse coding simulation software[Project]\nVisual Recognition and Machine Learning Summer School[Software]\n十一、目标、行为识别Object, Action Recognition：\nAction Recognition by Dense Trajectories[Project][Code]\nAction Recognition Using a Distributed Representation of Pose and Appearance[Project]\nRecognition Using Regions[Paper][Code]\n2D Articulated Human Pose Estimation[Project]\nFast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]\nEstimating Human Pose from Occluded Images[Paper][Code]\nQuasi-dense wide baseline matching[Project]\nChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]\nReal Time Head Pose Estimation with Random Regression Forests[Project]\n2D Action Recognition Serves 3D Human Pose Estimation[\nA Hough Transform-Based Voting Framework for Action Recognition[\nMotion Interchange Patterns for Action Recognition in Unconstrained Videos[\n2D articulated human pose estimation software[Project]\nLearning and detecting shape models [code]\nProgressive Search Space Reduction for Human Pose Estimation[Project]\nLearning Non-Rigid 3D Shape from 2D Motion[Project]\n十二、图像处理：\nDistance Transforms of Sampled Functions[Project]\nThe Computer Vision Homepage[Project]\nEfficient appearance distances between windows[code]\nImage Exploration algorithm[code]\nMotion Magnification 运动放大 [Project]\nBilateral Filtering for Gray and Color Images 双边滤波器 [Project]\nA Fast Approximation of the Bilateral Filter using a Signal Processing Approach [\n十三、一些实用工具：\nEGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]\na development kit of matlab mex functions for OpenCV library[Project]\nFast Artificial Neural Network Library[Project]\n十四、人手及指尖检测与识别：\nfinger-detection-and-gesture-recognition [Code]\nHand and Finger Detection using JavaCV[Project]\nHand and fingers detection[Code]\n十五、场景解释：\nNonparametric Scene Parsing via Label Transfer [Project]\n十六、光流Optical flow：\nHigh accuracy optical flow using a theory for warping [Project]\nDense Trajectories Video Description [Project]\nSIFT Flow: Dense Correspondence across Scenes and its Applications[Project]\nKLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]\nTracking Cars Using Optical Flow[Project]\nSecrets of optical flow estimation and their principles[Project]\nimplmentation of the Black and Anandan dense optical flow method[Project]\nOptical Flow Computation[Project]\nBeyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]\nA Database and Evaluation Methodology for Optical Flow[Project]\noptical flow relative[Project]\nRobust Optical Flow Estimation [Project]\noptical flow[Project]\n十七、图像检索Image Retrieval：\nSemi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]\n十八、马尔科夫随机场Markov Random Fields：\nMarkov Random Fields for Super-Resolution [Project]\nA Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]\n十九、运动检测Motion detection：\nMoving Object Extraction, Using Models or Analysis of Regions [Project]\nBackground Subtraction: Experiments and Improvements for ViBe [Project]\nA Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]\nchangedetection.net: A new change detection benchmark dataset[Project]\nViBe - a powerful technique for background detection and subtraction in video sequences[Project]\nBackground Subtraction Program[Project]\nMotion Detection Algorithms[Project]\nStuttgart Artificial Background Subtraction Dataset[Project]\nObject Detection, Motion Estimation, and Tracking[Project]\nFeature Detection and Description\nGeneral Libraries:\nVLFeat – Implementation of various feature descriptors (including SIFT, HOG, and LBP) and covariant feature detectors (including DoG, Hessian, Harris Laplace, Hessian Laplace, Multiscale Hessian, Multiscale Harris). Easy-to-use Matlab interface. See Modern features: Software – Slides providing a demonstration of VLFeat and also links to other software. Check also VLFeat hands-on session training\nOpenCV – Various implementations of modern feature detectors and descriptors (SIFT, SURF, FAST, BRIEF, ORB, FREAK, etc.)\nFast Keypoint Detectors for Real-time Applications:\nFAST – High-speed corner detector implementation for a wide variety of platforms\nAGAST – Even faster than the FAST corner detector. A multi-scale version of this method is used for the BRISK descriptor (ECCV 2010).\nBinary Descriptors for Real-Time Applications:\nBRIEF – C++ code for a fast and accurate interest point descriptor (not invariant to rotations and scale) (ECCV 2010)\nORB – OpenCV implementation of the Oriented-Brief (ORB) descriptor (invariant to rotations, but not scale)\nBRISK – Efficient Binary descriptor invariant to rotations and scale. It includes a Matlab mex interface. (ICCV 2011)\nFREAK – Faster than BRISK (invariant to rotations and scale) (CVPR 2012)\nSIFT and SURF Implementations:\nSIFT: VLFeat, OpenCV, Original code by David Lowe, GPU implementation, OpenSIFT\nSURF: Herbert Bay’s code, OpenCV, GPU-SURF\nOther Local Feature Detectors and Descriptors:\nVGG Affine Covariant features – Oxford code for various affine covariant feature detectors and descriptors.\nLIOP descriptor – Source code for the Local Intensity order Pattern (LIOP) descriptor (ICCV 2011).\nLocal Symmetry Features – Source code for matching of local symmetry features under large variations in lighting, age, and rendering style (CVPR 2012).\nGlobal Image Descriptors:\nGIST – Matlab code for the GIST descriptor\nCENTRIST – Global visual descriptor for scene categorization and object detection (PAMI 2011)\nFeature Coding and Pooling\nVGG Feature Encoding Toolkit – Source code for various state-of-the-art feature encoding methods – including Standard hard encoding, Kernel codebook encoding, Locality-constrained linear encoding, and Fisher kernel encoding.\nSpatial Pyramid Matching – Source code for feature pooling based on spatial pyramid matching (widely used for image classification)\nConvolutional Nets and Deep Learning\nEBLearn – C++ Library for Energy-Based Learning. It includes several demos and step-by-step instructions to train classifiers based on convolutional neural networks.\nTorch7 – Provides a matlab-like environment for state-of-the-art machine learning algorithms, including a fast implementation of convolutional neural networks.\nDeep Learning - Various links for deep learning software.\nPart-Based Models\nDeformable Part-based Detector – Library provided by the authors of the original paper (state-of-the-art in PASCAL VOC detection task)\nEfficient Deformable Part-Based Detector – Branch-and-Bound implementation for a deformable part-based detector.\nAccelerated Deformable Part Model – Efficient implementation of a method that achieves the exact same performance of deformable part-based detectors but with significant acceleration (ECCV 2012).\nCoarse-to-Fine Deformable Part Model – Fast approach for deformable object detection (CVPR 2011).\nPoselets – C++ and Matlab versions for object detection based on poselets.\nPart-based Face Detector and Pose Estimation – Implementation of a unified approach for face detection, pose estimation, and landmark localization (CVPR 2012).\nAttributes and Semantic Features\nRelative Attributes – Modified implementation of RankSVM to train Relative Attributes (ICCV 2011).\nObject Bank – Implementation of object bank semantic features (NIPS 2010). See also ActionBank\nClassemes, Picodes, and Meta-class features – Software for extracting high-level image descriptors (ECCV 2010, NIPS 2011, CVPR 2012).\nLarge-Scale Learning\nAdditive Kernels – Source code for fast additive kernel SVM classifiers (PAMI 2013).\nLIBLINEAR – Library for large-scale linear SVM classification.\nVLFeat – Implementation for Pegasos SVM and Homogeneous Kernel map.\nFast Indexing and Image Retrieval\nFLANN – Library for performing fast approximate nearest neighbor.\nKernelized LSH – Source code for Kernelized Locality-Sensitive Hashing (ICCV 2009).\nITQ Binary codes – Code for generation of small binary codes using Iterative Quantization and other baselines such as Locality-Sensitive-Hashing (CVPR 2011).\nINRIA Image Retrieval – Efficient code for state-of-the-art large-scale image retrieval (CVPR 2011).\nObject Detection\nSee Part-based Models and Convolutional Nets above.\nPedestrian Detection at 100fps – Very fast and accurate pedestrian detector (CVPR 2012).\nCaltech Pedestrian Detection Benchmark – Excellent resource for pedestrian detection, with various links for state-of-the-art implementations.\nOpenCV – Enhanced implementation of Viola&Jones real-time object detector, with trained models for face detection.\nEfficient Subwindow Search – Source code for branch-and-bound optimization for efficient object localization (CVPR 2008).\n3D Recognition\nPoint-Cloud Library – Library for 3D image and point cloud processing.\nAction Recognition\nActionBank – Source code for action recognition based on the ActionBank representation (CVPR 2012).\nSTIP Features – software for computing space-time interest point descriptors\nIndependent Subspace Analysis – Look for Stacked ISA for Videos (CVPR 2011)\nVelocity Histories of Tracked Keypoints - C++ code for activity recognition using the velocity histories of tracked keypoints (ICCV 2009)\nDatasets\nAttributes\nAnimals with Attributes – 30,475 images of 50 animals classes with 6 pre-extracted feature representations for each image.\naYahoo and aPascal – Attribute annotations for images collected from Yahoo and Pascal VOC 2008.\nFaceTracer – 15,000 faces annotated with 10 attributes and fiducial points.\nPubFig – 58,797 face images of 200 people with 73 attribute classifier outputs.\n[url=http://vis-www.cs.umass.edu/lfw/]LFW[/url] – 13,233 face images of 5,749 people with 73 attribute classifier outputs.\nHuman Attributes – 8,000 people with annotated attributes. Check also this link for another dataset of human attributes.\nSUN Attribute Database – Large-scale scene attribute database with a taxonomy of 102 attributes.\nImageNet Attributes – Variety of attribute labels for the ImageNet dataset.\nRelative attributes – Data for OSR and a subset of PubFig datasets. Check also this link for the WhittleSearch data.\nAttribute Discovery Dataset – Images of shopping categories associated with textual descriptions.\nFine-grained Visual Categorization\nCaltech-UCSD Birds Dataset – Hundreds of bird categories with annotated parts and attributes.\nStanford Dogs Dataset – 20,000 images of 120 breeds of dogs from around the world.\nOxford-IIIT Pet Dataset – 37 category pet dataset with roughly 200 images for each class. Pixel level trimap segmentation is included.\nLeeds Butterfly Dataset – 832 images of 10 species of butterflies.\nOxford Flower Dataset – Hundreds of flower categories.\nFace Detection\n[url=http://vis-www.cs.umass.edu/fddb/]FDDB[/url] – UMass face detection dataset and benchmark (5,000+ faces)\nCMU/MIT – Classical face detection dataset.\nFace Recognition\nFace Recognition Homepage – Large collection of face recognition datasets.\n[url=http://vis-www.cs.umass.edu/lfw/]LFW[/url] – UMass unconstrained face recognition dataset (13,000+ face images).\nNIST Face Homepage – includes face recognition grand challenge (FRGC), vendor tests (FRVT) and others.\nCMU Multi-PIE – contains more than 750,000 images of 337 people, with 15 different views and 19 lighting conditions.\nFERET – Classical face recognition dataset.\nDeng Cai’s face dataset in Matlab Format – Easy to use if you want play with simple face datasets including Yale, ORL, PIE, and Extended Yale B.\nSCFace – Low-resolution face dataset captured from surveillance cameras.\nHandwritten Digits\nMNIST – large dataset containing a training set of 60,000 examples, and a test set of 10,000 examples.\nPedestrian Detection\nCaltech Pedestrian Detection Benchmark – 10 hours of video taken from a vehicle,350K bounding boxes for about 2.3K unique pedestrians.\nINRIA Person Dataset – Currently one of the most popular pedestrian detection datasets.\nETH Pedestrian Dataset – Urban dataset captured from a stereo rig mounted on a stroller.\nTUD-Brussels Pedestrian Dataset – Dataset with image pairs recorded in an crowded urban setting with an onboard camera.\nPASCAL Human Detection – One of 20 categories in PASCAL VOC detection challenges.\nUSC Pedestrian Dataset – Small dataset captured from surveillance cameras.\nGeneric Object Recognition\nImageNet – Currently the largest visual recognition dataset in terms of number of categories and images.\nTiny Images – 80 million 32x32 low resolution images.\nPascal VOC – One of the most influential visual recognition datasets.\nCaltech 101 / Caltech 256 – Popular image datasets containing 101 and 256 object categories, respectively.\nMIT LabelMe – Online annotation tool for building computer vision databases.\nScene Recognition\nMIT SUN Dataset – MIT scene understanding dataset.\nUIUC Fifteen Scene Categories – Dataset of 15 natural scene categories.\nFeature Detection and Description\nVGG Affine Dataset – Widely used dataset for measuring performance of feature detection and description. CheckVLBenchmarksfor an evaluation framework.\nAction Recognition\nBenchmarking Activity Recognition – CVPR 2012 tutorial covering various datasets for action recognition.\nRGBD Recognition\nRGB-D Object Dataset – Dataset containing 300 common household objects\nReference:\n[1]: http://rogerioferis.com/VisualRecognitionAndSearch/Resources.html\n特征提取\nSURF特征: http://www.vision.ee.ethz.ch/software/index.de.html(当然这只是其中之一)\nLBP特征(一种纹理特征)：http://www.comp.hkbu.edu.hk/~icpr06/tutorials/Pietikainen.html\nFast Corner Detection（OpenCV中的Fast算法）:FAST Corner Detection -- Edward Rosten\n机器视觉\nA simple object detector with boosting(Awarded the Best Short Course Prize at ICCV 2005，So了解adaboost的推荐之作)：http://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html\nBoosting(该网页上有相当全的Boosting的文章和几个Boosting代码，本人推荐)：http://cbio.mskcc.org/~aarvey/boosting_papers.html\nAdaboost Matlab 工具：http://graphics.cs.msu.ru/en/science/research/machinelearning/adaboosttoolbox\nMultiBoost(不说啥了，多类Adaboost算法的程序)：http://sourceforge.net/projects/multiboost/\nTextonBoost(我们教研室王冠夫师兄的毕设): Jamie Shotton - Code\nLibSvm的老爹（推荐）: http://www.csie.ntu.edu.tw/~cjlin/\nConditional Random Fields（CRF论文+Code列表，推荐）\nCRF++: Yet Another CRF toolkit\nConditional Random Field (CRF) Toolbox for Matlab\nTree CRFs\nLingPipe: Installation\nHidden Markov Models（推荐）\n隐马尔科夫模型(Hidden Markov Models)系列之一 - eaglex的专栏 - 博客频道 - CSDN.NET（推荐）\n综合代码\nCvPapers(好吧，牛吧网站，里面有ICCV，CVPR，ECCV，SIGGRAPH的论文收录，然后还有一些论文的代码搜集，要求加精！)：http://www.cvpapers.com/\nComputer Vision Software(里面代码很多，并详细的给出了分类)：http://peipa.essex.ac.uk/info/software.html\n某人的Windows Live（我看里面东东不少就收藏了）：https://skydrive.live.com/?cid=3b6244088fd5a769#cid=3B6244088FD5A769&id=3B6244088FD5A769!523\nMATLAB and Octave Functions for Computer Vision and Image Processing（这个里面的东西也很全，只是都是用Matlab和Octave开发的）：http://www.csse.uwa.edu.au/~pk/research/matlabfns/\nComputer Vision Resources（里面的视觉算法很多，给出了相应的论文和Code，挺好的）：https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\nMATLAB Functions for Multiple View Geometry（关于物体多视角计算的库）：http://www.robots.ox.ac.uk/~vgg/hzbook/code/\nEvolutive Algorithm based on Naïve Bayes models Estimation（单独列了一个算法的Code）：http://www.cvc.uab.cat/~xbaro/eanbe/#_Software\n主页代码\nPablo Negri's Home Page\nJianxin Wu's homepage\nPeter Carbonetto\nMarkov Random Fields for Super-Resolution\nDetecting and Sketching the Common\nPedro Felzenszwalb\nHae JONG, SEO\nCAP 5416 - Computer Vision\nParallel Tracking and Mapping for Small AR Workspaces (PTAM)\nDeva Ramanan - UC Irvine - Computer Vision\nRaghuraman Gopalan\nHui Kong\nJamie Shotton - Post-Doctoral Researcher in Computer Vision\nJean-Yves AUDIBERT\nOlga Veksler\nStephen Gould\nPublications (Last Update: 09/30/10)\nKarim Ali - FlowBoost\nA simple parts and structure object detector\nCode - Oxford Brookes Vision Group\nTaku Kudo\n行人检测\nHistogram of Oriented Gradient (Windows)\nINRIA Pedestrian detector\nPoselets\nWilliam Robson Schwartz - Softwares\ncalvin upper-body detector v1.02\nRPT@CVG\nMain Page\nSource Code\nDr. Luciano Spinello\nPedestrian Detection\nClass-Specific Hough Forests for Object Detection\nJianxin Wu's homepage（就是上面的）\nBerkeley大学做的Pedestrian Detector，使用交叉核的支持向量机，特征使用HOG金字塔，提供Matlab和C++混编的代码：http://www.cs.berkeley.edu/~smaji/projects/ped-detector/\n视觉壁障\nHigh Speed Obstacle Avoidance using Monocular Vision and Reinforcement Learning\nTLD(2010年很火的tracking算法)\nonline boosting trackers\nBoris Babenko\nOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递 归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/\n物体检测算法\nObject Detection\nSoftware for object detection\n人脸检测\nSource Code\n10个人脸检测项目\nJianxin Wu's homepage（又是这货）\nICA独立成分分析\nAn ICA page-papers,code,demo,links (Tony Bell)\nFastICA\nCached k-d tree search for ICP algorithms\n滤波算法\n卡尔曼滤波：The Kalman Filter(终极网页)\nBayesian Filtering Library: The Bayesian Filtering Library\n路面识别\nSource Code\nVanishing point detection for general road detection\n分割算法\nMATLAB Normalized Cuts Segmentation Code：software\n超像素分割：SLIC Superpixels\n附： http://blog.sina.com.cn/s/blog_5086c3e20101kdy5.html，http://www.yuanyong.org/cv/cv-code-three.html\n参考：\nhttp://blog.csdn.net/carson2005/article/details/6601109\nhttp://blog.csdn.net/chlele0105/article/details/16880049\nhttp://blog.csdn.net/yihaizhiyan/article/details/6583727\nhttp://www.sigvc.org/bbs/forum.phpmod=viewthread&tid=3126&highlight=%BC%C6%CB%E3%BB%FA%CA%D3%BE%F5%B4%FA%C2%EB\n//=========================================================================================================================//\n1  中国内地\n1.1    程明明\n清华大学程明明博士，南开大学媒体计算实验室\nhttp://mmcheng.net/\n方向：图像分割、检索，显著性计算，目标检测。\n资源：Paper、Code、data。\n更新：2015\n1.2    冯建江\n清华大学助理教授\nhttp://ivg.au.tsinghua.edu.cn/~jfeng/\n方向：图像处理、模式识别、生物认证\n资源：Paper、Matlab code, C++ library, or executable file。\n更新：2015\n1.3    樊彬\n中科院助理教授樊彬\nhttp://www.sigvc.org/bfan/\n方向：计算机视觉，局部不变特征。\n资源：Paper、Code。\n更新：2015\n1.4    吴建鑫\n南京大学教授，LAMDA group\nhttp://cs.nju.edu.cn/wujx/\n方向：大规模数据的机器学习算法、物体的实时检测与识别、人的行为识别、场景分类和理解、计算机视觉和机器学习在其他领域的应用\n资源：Paper、Code、\n更新：2015\n1.5    Junjie Yan\nhttp://www.cbsr.ia.ac.cn/users/jjyan/main.htm\n方向：Object Detection, Face Detection, Pedestrian Detection\n资源：Paper\n更新：2015\n1.6    余轶南\n百度深度学习研究中心博士后(PASCAL VOC 2010 Detection Challenge Winner)：\nhttp://www.cbsr.ia.ac.cn/users/ynyu/index.htm\n方向：Multiple Transform Estimation、Object Detection、Image Matching、Interest point detection、Image Classification and Object Recognition\n资源：Paper\n更新：2015\n1.7    周志华\nhttp://cs.nju.edu.cn/zhouzh/index.htm\n南京大学教授\n书籍：Ensemble Methods: Foundations and Algorithms。Ensemble Methods领域首屈一指，主要兴趣是机器学习，应用方面关注图像检索、人脸识别、数据挖掘等。负责LAMDA（Learning And Mining from DatA）\n资源：Paper和课程，LAMDA有部分代码和数据\n更新：2015\n2   中国香港\n2.1    贾佳亚\n香港中文大学教授贾佳亚\nhttp://www.cse.cuhk.edu.hk/~leojia/index.html\n方向： computer vision, computational photography, and machine learning.\n资源：Paper、项目\n更新：2015\n2.2    汤晓鸥\n香港中文大学多媒体实验室（汤晓鸥）\nhttp://mmlab.ie.cuhk.edu.hk/\n方向：深度学习、人脸分析、视频监控、图像视觉搜索、图像视觉编辑、3D画直线\n资源：Paper、dataset、项目\n更新：2015\n2.3    王晓刚\n香港中文大学助理教授王晓刚\nhttp://www.ee.cuhk.edu.hk/~xgwang/\n方向：computer vision, medical imaging, machine learning, and applications to visual surveillance, face recognition, image and video searching, and diffusion weighted imaging\n资源：Paper、dataset\n更新：2014\n2.4    张磊\n香港理工大学教授张磊\nhttp://www4.comp.polyu.edu.hk/~cslzhang/\n方向：图像质量、Metric Learning 等\n资源：Paper、code\n更新：2015\n3   中国台湾\n3.1    Chih-Chung Chang and Chih-Jen Lin\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvm/\n方向：著名的svm库，OpenCV采用的svm算法\n资源：Paper、code\n更新：2014\n1        美国\n1.1     MIT\n1.1.1   Antonio Torralba\nMIT助理教授Antonio Torralba\nhttp://web.mit.edu/torralba/www/\n方向：计算机视觉，机器学习和人类视觉感知。场景和物体识别。比较好的交互可视化web工具：Interactive visualization of deep networks\n资源：Paper，project，Databases，code，teaching，demo。\n更新：2015\n1.1.2   Bolei Zhou\nhttp://people.csail.mit.edu/bzhou/\n方向：计算机视觉和机器学习，专为数据驱动的视觉和场景的理解.\n资源：Paper、project、demo\n更新：2015\n1.1.3   Ce Liu\nMIT前微软研究员现谷歌Ce Liu:\nhttp://people.csail.mit.edu/celiu/\n方向：CNN、Face hallucination、去噪、超分辨率、去模糊、分割\n资源：Paper、software、project、Code\n更新：2014\n1.1.4   Douglas Lanman\nResearch Scientist at Oculus Research （虚拟现实一家公司）\nhttp://alumni.media.mit.edu/~dlanman/index.html\n方向：跨越计算机图形学，视觉，光学，信号处理与应用程序的计算显示和成像系统，特别是头戴式显示器和虚拟/增强现实.\n资源：Paper、Code、Video\n更新：2014\n1.1.5   Michael (Miki) Rubinstein\nResearch Scientist, Google\nhttp://people.csail.mit.edu/mrub/\n方向：计算机视觉和图形的交叉点的图像和视频的分析的各个领域的工作。是低层次的图像/视频处理和计算摄影和视频.\n资源：Paper、Code、Video、software、data\n更新：2015\n1.1.6   William Freeman\nMIT教授William Freeman：\nhttp://billf.mit.edu/\n方向：图像纹理合成，运动重新渲染，计算摄影，和视觉学习。\n经典：图像融合算法Phase-based Video Motion Processing。\nAlex Efros和Freeman在2001年SIGGRAPH上发表了”Image quilting for texture synthesis and transfer”，其思想是从已知图像中获得小块，然后将这些小块拼接mosaic一起，形成新的图像。该算法是图像纹理合成中经典中的经典\n资源：Paper，project。\n更新：2015\n1.1.7   林达华\nMIT博士，汤晓欧学生林达华个人博客\nhttp://dahua.me/\n方向：机器学习，数据挖掘，计算机视觉和自然语言理解\n资源：Paper\n更新：2014\n1.1.8   周博磊\nMIT周博磊博士\nhttp://people.csail.mit.edu/bzhou/\n方向：计算机视觉和机器学习，专为数据驱动的视觉和场景的理解.\n资源：Paper，project，demo。\n更新：2015\n1.2     斯坦福\n1.2.1   Andrew Ng\n斯坦福大学Andrew Ng教授\nhttp://cs.stanford.edu/people/ang/\n方向：深度神经网络，深度学习，very large neural networks to learn from labeled and unlabeled data\n资源：Paper，project，courses。\n更新：2013\n1.2.2   Quoc V.Le\n斯坦福大学，现谷歌研究科学家\nhttp://ai.stanford.edu/~quocle/\n方向：机器学习、RNN、ICA、行为识别\n资源：Paper、Code、2011年行为识别文章的代码？\n更新：2015\n1.2.3   Sebastian Thrun\n斯坦福大学Sebastian Thrun教授：\nhttp://robots.stanford.edu/index.html\n方向：机器人，自动驾驶汽车，家庭自动化，医疗保健，无人机和其他一些应用程序的工作。目前集中在两个方面：早期癌症检测和跟踪，以及智能家园.\n资源：Paper，project，courses。\n更新：2007\n1.2.4   李菲菲\nStanford大学vision实验室；李菲菲所在实验室\nhttp://vision.stanford.edu/index.html\n方向：计算机视觉和人的视觉。物体识别，场景分类，综合场景理解，人体运动识别，材料识别，神经机制等。\n经典：建立了图像识别领域的标准测试库Caltech101/256、ImageNet？\n资源：Paper，Tedtalk， teaching。\n更新：2015\n1.3     CMU\n1.3.1   Daniel Huber\nCMU研究员Daniel Huber\nhttp://www.ri.cmu.edu/person.html?person_id=123\n方向：三维（3D）计算机视觉，特别是使用高精确度范围的传感器，例如激光扫描仪在建模，识别和可视化的领域中的问题。提取三维模型的高层次语义，如建筑物的模型.\n资源：Project、paper\n更新：2015\n1.3.2   Martial Hebert\nCMU机器人学院教授Martial Hebert:\nhttp://www.cs.cmu.edu/~hebert/\n方向：目标/类别识别；利用上下文信息，从图像的特定的3-D几何进行场景分析；场景解释和重建；在视频剪辑的特征提取和事件检测运动分析；动态三维点云的分析（“3-D信号处理”），高效的工具；感知的自治系统；在动态环境探测，跟踪和预测\n资源：Project、paper、software、data\n更新：2014\n1.3.3   Yang Wang\nCMU大学研究员Yang Wang\nhttp://www.cs.cmu.edu/~wangy/home.html\n方向：计算机视觉，图形学，医学图像分析，生物识别，机器学习，计算机动画，以及增强现实.\n资源：Project、demo、paper\n更新：2010\n//=========================================================================================================================//\n转自：http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html\nDeep Learning（深度学习）：\nufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：一\nufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：二\nBengio团队的deep learning教程，用的theano库，主要是rbm系列，搞python的可以参考，很不错。\ndeeplearning.net主页，里面包含的信息量非常多，有software, reading list, research lab, dataset, demo等，强烈推荐，自己去发现好资料。\nDeep learning的toolbox，matlab实现的，对应源码来学习一些常见的DL模型很有帮助，这个库我主要是用来学习算法实现过程的。\n2013年龙星计划深度学习教程，邓力大牛主讲，虽然老师准备得不充分，不过还是很有收获的。\nHinton大牛在coursera上开的神经网络课程，DL部分有不少，非常赞，没有废话，课件每句话都包含了很多信息，有一定DL基础后去听收获更大。\nLarochelle关于DL的课件，逻辑清晰，覆盖面广,包含了rbm系列，autoencoder系列，sparse coding系列，还有crf，cnn，rnn等。虽然网页是法文，但是课件是英文。\nCMU大学2013年的deep learning课程，有不少reading paper可以参考。\n达慕思大学Lorenzo Torresani的2013Deep learning课程reading list.\nDeep Learning Methods for Vision(余凯等在cvpr2012上组织一个workshop，关于DL在视觉上的应用)。\n斯坦福Ng团队成员链接主页，可以进入团队成员的主页，比较熟悉的有Richard Socher, Honglak Lee, Quoc Le等。\n多伦多ML团队成员链接主页，可以进入团队成员主页，包括DL鼻祖hinton，还有Ruslan Salakhutdinov , Alex Krizhevsky等。\n蒙特利尔大学机器学习团队成员链接主页，包括大牛Bengio，还有Ian Goodfellow 等。\n纽约大学的机器学习团队成员链接主页，包括大牛Lecun，还有Rob Fergus等。\nCharlie Tang个人主页，结合DL+SVM.\n豆瓣上的脑与deep learning读书会，有讲义和部分视频，主要介绍了一些于deep learning相关的生物神经网络。\nLarge Scale ML的课程，由Lecun和Langford讲的，能不推荐么。\nYann Lecun的2014年Deep Learning课程主页。视频链接。\n吴立德老师《深度学习课程》\n一些常见的DL code列表，csdn博主zouxy09的博文，Deep Learning源代码收集-持续更新…\nDeep Learning for NLP (without Magic)，由DL界5大高手之一的Richard Socher小组搞的，他主要是NLP的。\n2012 Graduate Summer School: Deep Learning, Feature Learning，高手云集，深度学习盛宴，几乎所有的DL大牛都有参加。\nmatlab下的maxPooling速度优化，调用C++实现的。\n2014年ACL机器学习领域主席Kevin Duh的深度学习入门讲座视频。\nR-CNN code: Regions with Convolutional Neural Network Features.\nMachine Learning（机器学习）：\n介绍图模型的一个ppt，非常的赞，ppt作者总结得很给力，里面还包括了HMM，MEM, CRF等其它图模型。反正看完挺有收获的。\n机器学习一个视频教程，youtube上的，翻吧，内容很全面，偏概率统计模型，每一小集只有几分钟。\n龙星计划2012机器学习，由余凯和张潼主讲。\ndemonstrate 的 blog :关于PGM(概率图模型)系列，主要按照Daphne Koller的经典PGM教程介绍的，大家依次google之。\nFreeMind的博客，主要关于机器学习的。\nTom Mitchell大牛的机器学习课程，他的machine learning教科书非常出名。\nCS109,Data Science,用python介绍机器学习算法的课程。\nCCF主办的一些视频讲座。\n国外技术团队博客：\nNetflix技术博客,很多干货。\nComputer Vision（计算机视觉）：\nMIT2013年秋季课程：Advances in Computer Vision，有练习题，有些有code.\nIPAM一个计算机视觉的短期课程，有不少牛人参加。\nOpenCV相关：\nhttp://opencv.org/\n2012年7月4日随着opencv2.4.2版本的发布，opencv更改了其最新的官方网站地址。\nhttp://www.opencvchina.com/\n好像12年才有这个论坛的，比较新。里面有针对《learning opencv》这本书的视频讲解，不过视频教学还没出完，正在更新中。对刚入门学习opencv的人来说很不错。\nhttp://www.opencv.org.cn/forum/\nopencv中文论坛，对于初次接触opencv的学者来说比较不错，入门资料多，opencv的各种英文文档也翻译成中文了。不足是感觉这个论坛上发帖提问很少人回答，也就是说讨论不够激烈。\nhttp://opencv.jp/\nopencv的日文网站，里面有不少例子代码，看不懂日文可以用网站自带的翻译，能看个大概。\nhttp://code.opencv.org/projects/opencv\nopencv版本bug修补，版本更新，以及各种相关大型活动安排,还包含了opencv最近几个月内的活动路线，即未来将增加的功能等，可以掌握各种关于opencv进展情况的最新进展。\nhttp://tech.groups.yahoo.com/group/OpenCV/\nopencv雅虎邮件列表，据说是最好的opencv论坛，信息更新最新的地方。不过个人认为要查找相关主题的内容，在邮件列表中非常不方便。\nhttp://www.cmlab.csie.ntu.edu.tw/~jsyeh/wiki/doku.php\n台湾大学暑假集训网站，内有链接到与opencv集训相关的网页。感觉这种教育形式还蛮不错的。\nhttp://sourceforge.net/projects/opencvlibrary/\nopencv版本发布地方。\nhttp://code.opencv.org/projects/opencv/wiki/ChangeLog#241http://opencv.willowgarage.com/wiki/OpenCV%20Change%20Logs\nopencv版本内容更改日志网页,前面那个网页更新最快。\nhttp://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/tutorials.html\nopencv中文教程网页，分几个模块讲解，有代码有过程。内容是网友翻译opencv自带的doc文件里的。\nhttps://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n网友总结的常用带有cvpr领域常见算法code链接的网址，感觉非常的不错。\nhttp://fossies.org/dox/OpenCV-2.4.2/\n该网站可以查看opencv中一些函数的变量接口，还会列出函数之间的结构图。\nhttp://opencv.itseez.com/\nopencv的函数、类等查找网页，有导航，查起来感觉不错。\n优化：\nsubmodual优化网页。\nGeoff Gordon的优化课程，youtube上有对应视频。\n数学：\nhttp://www.youku.com/playlist_show/id_19465801.html\n《计算机中的数学》系列视频，8位老师10讲内容，生动介绍微积分和线性代数基本概念在计算机学科中的各种有趣应用！\nLinux学习资料：\nhttp://itercast.com/library/1\nlinux入门的基础视频教程，对于新手可选择看第一部分，视频来源于LinuxCast.NET网站，还不错。\nOpenNI+Kinect相关：\nhttp://1.yuhuazou.sinaapp.com/\n网友晨宇思远的博客，主攻cvpr，ai等。\nhttp://blog.csdn.net/chenli2010/article/details/6887646\nkinect和openni学习资料汇总。\nhttp://blog.csdn.net/moc062066/article/category/871261\nOpenCV 计算机视觉 kinect的博客:\nhttp://kheresy.wordpress.com/index_of_openni_and_kinect/comment-page-5/\n网友Heresy的博客，里面有不少kinect的文章，写的比较详细。\nhttp://www.cnkinect.com/\n体感游戏中文网，有不少新的kinect资讯。\nhttp://www.kinectutorial.com/\nKinect体感开发网。\nhttp://code.google.com/p/openni-hand-tracker\nopenni_hand_tracking google code项目。\nhttp://blog.candescent.ch/\n网友的kinect博客，里面有很多手势识别方面的文章介绍，还有源码，不过貌似是基于c#的。\nhttps://sites.google.com/site/colordepthfusion/\n一些关于深度信息和颜色信息融合（fusion）的文章。\nhttp://projects.ict.usc.edu/mxr/faast/\nkinect新的库，可以结合OpenNI使用。\nhttps://sites.google.com/a/chalearn.org/gesturechallenge/\nkinect手势识别网站。\nhttp://www.ros.org/wiki/mit-ros-pkg\nmit的kinect项目，有code。主要是与手势识别相关。\nhttp://www.thoughtden.co.uk/blog/2012/08/kinecting-people-our-top-6-kinect-projects/\nkinect 2012年度最具创新的6个项目，有视频，确实够创新的！\nhttp://www.cnblogs.com/yangyangcv/archive/2011/01/07/1930349.html\nkinect多点触控的一篇博文。\nhttp://sourceforge.net/projects/kinect-mex/\nhttp://www.mathworks.com/matlabcentral/fileexchange/30242-kinect-matlab\n有关matlab for kinect的一些接口。\nhttp://news.9ria.com/2012/1212/25609.html\nAIR和Kinect的结合，有一些手指跟踪的code。\nhttp://eeeweba.ntu.edu.sg/computervision/people/home/renzhou/index.htm\n研究kinect手势识别的，任洲。刚毕业不久。\n其他网友cvpr领域的链接总结：\nhttp://www.cnblogs.com/kshenf/\n网友整理常用牛人链接总结，非常多。不过个人没有没有每个网站都去试过。所以本文也是我自己总结自己曾经用过的或体会过的。\nOpenGL有关:\nhttp://nehe.gamedev.net/\nNeHe的OpenGL教程英文版。\nhttp://www.owlei.com/DancingWind/\nNeHe的OpenGL教程对应的中文版，由网友周玮翻译的。\nhttp://www.qiliang.net/old/nehe_qt/\nNeHe的OpengGL对应的Qt版中文教程。\nhttp://blog.csdn.net/qp120291570\n网友\"左脑设计，右脑编程\"的Qt_OpenGL博客,写得还不错。\nhttp://guiliblearning.blogspot.com/\n这个博客对opengl的机制有所剖析，貌似要FQ才能进去。\ncvpr综合网站论坛博客等：\nhttp://www.cvchina.net/\n中国计算机视觉论坛\nhttp://www.cvchina.info/\n这个博客很不错，每次看完都能让人兴奋，因为有很多关于cv领域的科技新闻，还时不时有视频显示。另外这个博客里面的资源也整理得相当不错。中文的。\nhttp://www.bfcat.com/\n一位网友的个人计算机视觉博客，有很多关于计算机视觉前沿的东西介绍，与上面的博客一样，看了也能让人兴奋。\nhttp://blog.csdn.net/v_JULY_v/\n牛人博客，主攻数据结构，机器学习数据挖掘算法等。\nhttp://blog.youtueye.com/\n该网友上面有一些计算机视觉方向的博客,博客中附有一些实验的测试代码.\nhttp://blog.sciencenet.cn/u/jingyanwang\n多看pami才扯谈的博客，其中有不少pami文章的中文介绍。\nhttp://chentingpc.me/\n做网络和自然语言处理的，有不少机器学习方面的介绍。\nML常用博客资料等：\nhttp://freemind.pluskid.org/\n由 pluskid 所维护的 blog，主要记录一些机器学习、程序设计以及各种技术和非技术的相关内容，写得很不错。\nhttp://datasciencemasters.org/\n里面包含学ML/DM所需要的一些知识链接，且有些给出了视频教程，网页资料，电子书，开源code等，推荐！\nhttp://cs.nju.edu.cn/zhouzh/index.htm\n周志华主页，不用介绍了，机器学习大牛，更可贵的是他的很多文章都有源码公布。\nhttp://www.eecs.berkeley.edu/~jpaisley/Papers.htm\nJohn Paisley的个人主页，主要研究机器学习领域，有些文章有代码提供。\nhttp://foreveralbum.yo2.cn/\n里面有一些常见机器学习算法的详细推导过程。\nhttp://blog.csdn.net/abcjennifer\n浙江大学CS硕士在读，关注计算机视觉，机器学习，算法研究，博弈， 人工智能， 移动互联网等学科和产业。该博客中有很多机器学习算法方面的介绍。\nhttp://www.wytk2008.net/\n无垠天空的机器学习博客。\nhttp://www.chalearn.org/index.html\n机器学习挑战赛。\nhttp://licstar.net/\nlicstar的技术博客，偏自然语言处理方向。\n国内科研团队和牛人网页：\nhttp://vision.ia.ac.cn/zh/index_cn.html\n中科院自动化所机器视觉课题小组，有相关数据库、论文、课件等下载。\nhttp://www.cbsr.ia.ac.cn/users/szli/\n李子青教授个人主页，中科院自动化所cvpr领域牛叉人！\nhttp://www4.comp.polyu.edu.hk/~cslzhang/\n香港理工大学教授lei zhang个人主页，也是cvpr领域一大牛人啊，cvpr，iccv各种发表。更重要的是他所以牛叉论文的code全部公开，非常难得！\nhttp://liama.ia.ac.cn/wiki/start\n中法信息、自动化与应用联合实验室，里面很多内容不仅限而cvpr，还有ai领域一些其他的研究。\nhttp://www.cogsci.xmu.edu.cn/cvl/english/\n厦门大学特聘教授，cv领域一位牛人。研究方向主要为目标检测，目标跟踪，运动估计，三维重建，鲁棒统计学，光流计算等。\nhttp://idm.pku.edu.cn/index.aspx\n北京大学数字视频编码技术国家实验室。\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvm/\nlibsvm项目网址，台湾大学的，很火！\nhttp://www.jdl.ac.cn/user/sgshan/index.htm\n山世光，人脸识别研究比较牛。在中国科学院智能信息处理重点实验室\n国外科研团队和牛人网页：\nhttps://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n常见计算机视觉资源整理索引，国外学者整理，全是出名的算法，并且带有代码的，这个非常有帮助，其链接都是相关领域很火的代码。\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/txtv-groups.html\n国外学者整理的各高校研究所团队网站\nhttp://research.microsoft.com/en-us/groups/vision/\n微软视觉研究小组，不解释，大家懂的，牛！\nhttp://lear.inrialpes.fr/index.php\n法国国家信息与自动化研究所，有对应牛人的链接，论文项目网页链接，且一些code对应链接等。\nhttp://www.cs.ubc.ca/~pcarbo/objrecls/\nLearning to recognize objects with little supervision该篇论文的项目网页，有对应的code下载，另附有详细说明。\nhttp://www.eecs.berkeley.edu/~lbourdev/poselets/\nposelets相关研究界面，关于poselets的第一手资料。\nhttp://www.cse.oulu.fi/CMV/Research\n芬兰奥卢大学计算机科学与工程学院网页，里面有很多cv领域相关的研究，比如说人脸，脸部表情，人体行为识别，跟踪，人机交互等cv基本都涉及有。\nhttp://www.cs.cmu.edu/~cil/vision.html\n卡耐基梅隆大学计算机视觉主页，内容非常多。可惜的是该网站内容只更新到了2004年。\nhttp://vision.stanford.edu/index.html\n斯坦福大学计算机视觉主页，里面有非常非常多的牛人，比如说大家熟悉的lifeifei.\nhttp://www.wavelet.org/index.php\n关于wavelet研究的网页。\nhttp://civs.ucla.edu/\n加州大学洛杉矶分校统计学院，关于统计学习方面各种资料，且有相应的网上公开课。\nhttp://www.cs.cmu.edu/~efros/\n卡耐基梅隆大学Alexei(Alyosha)Efros教授个人网站，计算机图形学高手。\nhttp://web.mit.edu/torralba/www//\nmit牛人Associate教授个人网址，主要研究计算机视觉人体视觉感知，目标识别和场景理解等。\nhttp://people.csail.mit.edu/billf/\nmit牛人William T. Freeman教授，主要研究计算机视觉和图像学\nhttp://www.research.ibm.com/peoplevision/\nIBM人体视觉研究中心，里面除了有其研究小组的最新成果外，还有很多测试数据(特别是视频）供下载。\nhttp://www.vlfeat.org/\nvlfeat主页，vlfeat也是一个开源组织，主要定位在一些最流行的视觉算法开源上，Ｃ编写，其很多算法效果比opencv要好,不过数量不全，但是非常有用。\nhttp://www.robots.ox.ac.uk/~az/\nAndrew Zisserman的个人主页，这人大家应该熟悉，《计算机视觉中的多视几何》这本神书的作者之一。\nhttp://www.cs.utexas.edu/~grauman/\nKristenGrauman教授的个人主页，是个大美女，且是2011年“马尔奖”获得者，”马尔奖“大家都懂的，计算机视觉领域的最高奖项，目前无一个国内学者获得过。她的主要研究方法是视觉识别。\nhttp://groups.csail.mit.edu/vision/welcome/\nmit视觉实验室主页。\nhttp://code.google.com/p/sixthsense/\n曾经在网络上非常出名一个视频，一个作者研究的第六感装置，现在这个就是其开源的主页。\nhttp://vision.ucsd.edu/~pdollar/research.html#BehaviorRecognitionAnimalBehavior\nPiotr Dollar的个人主要，主要研究方向是人体行为识别。\nhttp://www.mmp.rwth-aachen.de/\n移动多媒体处理，将移动设备，计算机图像学，视觉，图像处理等结合的领域。\nhttp://www.di.ens.fr/~laptev/index.html\nIvan Laptev牛人主页，主要研究人体行为识别。有很多数据库可以下载。\nhttp://blogs.oregonstate.edu/hess/\nRob Hess的个人主要，里面有源码下载，比如说粒子滤波，他写的粒子滤波在网上很火。\nhttp://morethantechnical.googlecode.com/svn/trunk/\ncvpr领域一些小型的开源代码。\nhttp://iica.de/pd/index.py\n做行人检测的一个团队，内部有一些行人检测的代码下载。\nhttp://www.cs.utexas.edu/~grauman/research/pubs.html\nUT-Austin计算机视觉小组，包含的视觉研究方向比较广，且有的文章有源码，你只需要填一个邮箱地址，系统会自动发跟源码相关的信息过来。\nhttp://www.robots.ox.ac.uk/~vgg/index.html\nvisual geometry group\n图像:\nhttp://blog.sina.com.cn/s/blog_4cccd8d301012pw5.html\n交互式图像分割代码。\nhttp://vision.csd.uwo.ca/code/\ngraphcut优化代码。\n语音：\nhttp://danielpovey.com/kaldi-lectures.html\n语音处理中的kaldi学习。\n算法分析与设计（计算机领域的基础算法）：\nhttp://www.51nod.com/focus.html\n该网站主要是讨论一些算法题。里面的李陶冶是个大牛，回答了很多算法题。\n一些综合topic列表：\nhttp://www.cs.cornell.edu/courses/CS7670/2011fa/\n计算机视觉中的些topic（Special Topics in Computer Vision），截止到2011年为止，其引用的文章都是非常顶级的topic。\n书籍相关网页：\nhttp://www.imageprocessingplace.com/index.htm\n冈萨雷斯的《数字图像处理》一书网站，包含课程材料，matlab图像处理工具包，课件ppt等相关素材。\nConsumer Depth Cameras for Computer Vision\n很优秀的一本书，不过很贵，买不起啊！做深度信息的使用这本书还不错，google图中可以预览一部分。\nMaking.Things.See\n针对Kinect写的，主要关注深度信息，较为基础。书籍中有不少例子，貌似是Java写的。\n国内一些AI相关的研讨会：\nhttp://www.iipl.fudan.edu.cn/MLA13/index.htm\n中国机器学习及应用研讨会(这个是2013年的)\n期刊会议论文下载：\nhttp://cvpapers.com/\n几个顶级会议论文公开下载界面，比如说ICCV,CVPR,ECCV,ACCV,ICPR,SIGGRAPH等。\nhttp://www.cvpr2012.org/\ncvpr2012的官方地址，里面有各种资料和信息，其他年份的地址类似推理更改即可。\nhttp://www.sciencedirect.com/science/journal/02628856\nICV期刊下载\nhttp://www.computer.org/portal/web/tpami\nTPAMI期刊，AI领域中可以算得上是最顶级的期刊了，里面有不少cvpr方面的内容。\nhttp://www.springerlink.com/content/100272/\nIJCV的网址。\nhttp://books.nips.cc/\nNIPS官网，有论文下载列表。\nhttp://graphlab.org/lsrs2013/program/\nLSRS （会议）地址，大规模推荐系统，其它年份依次类推。\n会议期刊相关信息：\nhttp://conferences.visionbib.com/Iris-Conferences.html\n该网页列出了图像处理，计算机视觉领域相关几乎所有比较出名的会议时间表。\nhttp://conferences.visionbib.com/Browse-conf.php\n上面网页的一个子网页，列出了最近的CV领域提交paper的deadline。\ncvpr相关数据库下载：\nhttp://research.microsoft.com/en-us/um/people/jckrumm/WallFlower/TestImages.htm\n微软研究院牛人Wallflower Paper的论文中用到的目标检测等测试图片\nhttp://archive.ics.uci.edu/ml/\nUCI数据库列表下载，最常用的机器学习数据库列表。\nhttp://www.cs.rochester.edu/~rmessing/uradl/\n人体行为识别通过关键点的跟踪视频数据库，Rochester university的\nhttp://www.research.ibm.com/peoplevision/performanceevaluation.html\nIBM人体视觉研究中心，有视频监控等非常多的测试视频。\nhttp://www.cvpapers.com/datasets.html\n该网站上列出了常见的cvpr研究的数据库。\nhttp://www.cs.washington.edu/rgbd-dataset/index.html\nRGB-D Object Dataset.做目标识别的。\nAI相关娱乐网页：\nhttp://en.akinator.com/\n该网站很好玩，可以测试你心里想出的一个人名(当然前提是这个人必须有一定的知名度)，然后该网站会提出一系列的问题，你可以选择yes or no,or I don’t know等等，最后系统会显示你心中所想的那个人。\nhttp://www.doggelganger.co.nz/\n人与狗的匹配游戏，摄像头采集人脸，呵呵…\nAndroid相关：\nhttps://code.google.com/p/android-ui-utils/\n该网站上有一些android图标,菜单等跟界面有关的设计工具,可以用来做一些简单的UI设计.\n工具和code下载：\nhttp://lear.inrialpes.fr/people/dorko/downloads.html\n6种常见的图像特征点检测子，linux下环境运行。不过只提供了二进制文件，不提供源码。\nhttp://www.cs.ubc.ca/~pcarbo/objrecls/index.html#code\nssmcmc的matlab代码,是Learning to recognize objects with little supervision这一系列文章用的源码，属于目标识别方面的研究。\nhttp://www.robots.ox.ac.uk/~timork/\n仿射无关尺度特征点检测算子源码，还有些其它算子的源码或二进制文件。\nhttp://www.vision.ee.ethz.ch/~bleibe/code/ism.html\n隐式形状模型(ISM)项目主页，作者Bastian Leibe提供了linux下运行的二进制文件。\nhttp://www.di.ens.fr/~laptev/download.html#stip\nIvan Laptev牛人主页中的STIP特征点检测code，但是也只是有二进制文件，无源码。该特征点在行为识别中该特征点非常有名。\nhttp://ai.stanford.edu/~quocle/\n斯坦福大学Quoc V.Le主页，上有它2011年行为识别文章的代码。\n开源软件：\nhttp://mloss.org/software/\n一些ML开源软件在这里基本都可以搜到，有上百个。\nhttps://github.com/myui/hivemall\nScalable machine learning library for Hive/Hadoop.\nhttp://scikit-learn.org/stable/\n基于Python的机器学习开源软件，文档写得不错。\n挑战赛：\nhttp://www.chioka.in/kaggle-competition-solutions/\nkaggle一些挑战赛的code.\n公开课：\n网易公开课，国内做得很不错的公开课，翻译了一些国外出名的公开课教程，与国外公开课平台coursera有合作。\ncoursera在线教育网上公开课，很新，有个邮箱注册即可学习，有不少课程，且有对应的练习，特别是编程练习，超赞。\n斯坦福网上公开课链接，有统计学习，凸优化等课程。\nudacity公开课程下载链接，其实速度还可以。里面有不少好教程。\n机器学习公开课的连接，有不少课。\n转自：http://blog.csdn.Net/tainyiliusha/article/details/10077081\n//============================================================================================================================//\n牛人主页（主页有很多论文代码） Serge Belongie atUC San Diego AntonioTorralba at MIT Alexei Ffros atCMU Ce Liu atMicrosoft Research New England VittorioFerrari at Univ.of Edinburgh KristenGrauman at UT Austin Devi Parikh at TTI-Chicago (MarrPrize at ICCV2011) John Wright atColumbia Univ. Piotr Dollar atCalTech Boris Babenko atUC San Diego David Ross atGoogle/Youtube\n相关领域： Terence Tao at UCLA David Donoho atStanford Univ.\n大神们： William T.Freeman at MIT RobertoCipolla at Cambridge David Lowe atUniv. of British Columbia Mubarak Shah at Univ. of Central Florida Yi Ma atMSRA TinneTuytelaars at K.U. Leuven Trevor Darrell atU.C. Berkeley Michael J.Black at Brown Univ.\n重要研究组： Computer Vision Group at UC Berkeley RoboticsResearch Group at Univ. of Oxford LEAR atINRIA ComputerVision Lab at Stanford ComputerVision Lab at EPFL ComputerVision Lab at ETH Zurich Computer Vision Lab atSeoul National Univ. Computer Vision Lab atUC San Diego ComputerVision Lab at UC Santa Cruz ComputerVision Lab at Univ. of Southern California ComputerVision Lab at Univ. of Central Florida ComputerVision Lab at Columbia Univ. UCLA VisionLab Motion andShape Computing Group at George Mason Univ. Robust ImageUnderstanding Lab at Rutgers Univ. IntelligentVision Systems Group at Univ. of Bonn Institute forComputer Graphics and Vision at Graz Univ. of Tech. ComputerVision Lab. at Vienna Univ. of Tech. ComputationalImage Analysis and Radiology at Medical Univ. of Vienna Personal Robotics Lab atCMU VisualPerception Lab at Purdue Univ.\n潜力牛人： Juergen Gall at ETHZurich Matt Flagg atGeorgia Tech. MathieuSalzmann at TTI-Chicago GergShakhnarovich at TTI-Chicago Taeg Sang Cho atMIT Jianchao Yang atUIUC Stefan Roth at TU Darmstadt Peter Kontschieder at Graz Univ. of Tech. DominikAlexander Klein at Univ. of Bonn Yinan Yu atCASIA (PASCAL VOC 2010 Detection Challenge Winner) Zdenek Kalal atFPFL Julien Pilet atFPFL Kenji Okuma\n（1）googleResearch； http://research.google.com/index.html （2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html （3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/ （4）opencv中文网站； http://www.opencv.org.cn/index.php/首页 （5）Stanford大学vision实验室； http://vision.stanford.edu/research.html （6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/ （7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/ （8）中国人工智能网； http://www.chinaai.org/ （9）中国视觉网； http://www.china-vision.net/ （10）中科院自动化所； http://www.ia.cas.cn/ （11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/ （12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/ （13）人脸识别主页； http://www.face-rec.org/ （14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/ （15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html （16）卡内基梅隆大学CV主页； http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html （17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/ （18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/ （19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx （20）研学论坛； http://bbs.matwav.com/ （21）美国Rutgers大学助理教授刘青山；http://www.research.rutgers.edu/~qsliu/ （22）计算机视觉最新资讯网； http://www.cvchina.info/ （23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287 （24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/ (25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/ (26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home (27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/ (28)computer vision software; http://peipa.essex.ac.uk/info/software.html (29)Computer Vision Resource; http://www.cvpapers.com/ (30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html (31)computer vision center; http://computervisioncentral.com/cvcnews (32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/ (33)自动识别网：http://www.autoid-china.com.cn/ (34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html (35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/ (36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/ (37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/ (38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ (39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp (40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/ (41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz (42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/indexCH.asp (43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html (44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/ (45)深圳大学于仕祺副教授：http://yushiqi.cn/ (46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/ (47)卡内基梅隆大学研究员Robert T.Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background (48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php (49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/ (50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1 (51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp (52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/ (53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html (54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/ (55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/ (56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html (57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml (58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/ (59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php (60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html (61)Universityof Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html (62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi (63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html (64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm (65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/ (66)中科院自动化所医学影像研究室：http://www.3dmed.net/ (67)中科院田捷研究员：http://www.3dmed.net/tian/ (68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/ (69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/ (70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/ (71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/ (72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/ (73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/ (74)微软剑桥研究院研究员John Winn: http://johnwinn.org/ (75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html (76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/ (77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/ (78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/ (79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/ (80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/ (81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/ (82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/ (83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/ (84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123 (85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/ (86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/ (87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/ (88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/ (89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/ (90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/ (91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/ (92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/ (93)CMU博士田渊栋:http://www.cs.cmu.edu/~yuandong/ (94)CMU副教授Srinivasa Narasimhan:http://www.cs.cmu.edu/~srinivas/ (95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/ (96)哥伦比亚大学教授Sheer K.Nayar:http://www.cs.columbia.edu/~nayar/ (97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/ (98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/ (99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm (100)芝加哥丰田技术研究所助理教授Devi Parikh:http://ttic.uchicago.edu/~dparikh/index.html (101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV (102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html (103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html (104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/ (105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/ (106)图片检索国际会议VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/ (107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/ (108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\naboutmulti-camera: http://server.cs.ucf.edu/~vision/projects.html\nabout 3D VoxelColoring Rob Hess: http://blogs.oregonstate.edu/hess/code/voxels/\nAbout theparticle filters–condensation filter:http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/ISARD1/condensation.html\nMachine LearningOpen Source Software： http://jmlr.csail.mit.edu/mloss/ 1、动作识别数据库：Recognition ofhuman actions：http://www.nada.kth.se/cvap/actions/ 2、Datasets for Computer Vision Research：http://www-cvr.ai.uiuc.edu/ponce_grp/data/ 3、Computer VisionDatasets:http://clickdamage.com/sourcecode/cv_datasets.php 4、里面有好多基本算法 matlab： http://www.mathworks.cn/index.html\n· Matlab Codefor Graph EmbeddingDiscriminant Analysis on Grassmannian Manifolds for Improved Image Set Matching (CVPR),2011. · Matlab Codefor Optimal Local Basis: AReinforcement Learning Approach for Face Recognition(IJCV), vol. 81,no. 2, pp. 191-204, 2009.\n牛人bolg：\n1、Hong KongPolytechnic University ：http://www4.comp.polyu.edu.hk/~cslzhang/\n2、Computer VisionResources：资源非常丰富，包含有基本算法。https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n3、源代码非常丰富~~ http://homepage.tudelft.nl/19j49/Publications.html\nCVonline http://homepages.inf.ed.ac.uk/rbf/CVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htm http://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm\n李子青的大作： Markov Random Field Modeling in Computer Vision http://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.html Handbook of Face Recognition (PDF) http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf\n张正友的有关参数鲁棒估计著作：\nParameter Estimation Techniques:A Tutorial with Application to Conic Fitting\nhttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.html Andrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Vision http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007 有关马尔可夫蒙特卡罗方法的资料： An introduction to Markov chain Monte Carlo http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.html Markov Chain Monte Carlo for Computer Vision— A tutorial at ICCV05 http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm\n有关独立成分分析（Independent Component Analysis , ICA）的资料： An ICA-Page http://www.cnl.salk.edu/~tony/ica.html Fast ICA http://www.cis.hut.fi/projects/ica/fastica/\nThe Kalman Filter (介绍卡尔曼滤波器的终极网页) http://www.cs.unc.edu/~welch/kalman/index.html\n1\n2\n3\nCached k-d tree search for ICP algorithms\nhttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html\n几个计算机视觉研究工具\nMachine Vision Toolbox for Matlab\nhttp://www.petercorke.com/MachineVision Toolbox.html\nMatlab and Octave Function for Computer Vision and Image Processing\nhttp://www.csse.uwa.edu.au/~pk/research/matlabfns/\nBayes Net Toolbox for Matlab\nhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.html\nOpenCV (Chinese)\nhttp://www.opencv.org.cn/index.php/首页\nGandalf (A Computer Vision and Numerical Algorithm Labrary)\nhttp://gandalf-library.sourceforge.net/\nCMU Computer Vision Home Page\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\nMachine Learning Resource Links\nhttp://www.cse.ust.hk/~ivor/resource.htm\nThe Bayesian Filtering Library\nhttp://www.orocos.org/bfl\nOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)\nhttp://of-eval.sourceforge.net/\nMATLAB code for ICP algorithm\nhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html\n牛人主页：\n朱松纯 （Song-Chun Zhu）\nhttp://www.stat.ucla.edu/~sczhu/\nDavid Lowe (SIFT) (很帅的一个老头哦 ^ ^)\nhttp://www.cs.ubc.ca/~lowe/\nAndrea Vedaldi (SIFT)\nhttp://vision.ucla.edu/~vedaldi/index.html\nPedro F. Felzenszwalb\nhttp://people.cs.uchicago.edu/~pff/\nDougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)\nhttp://mesh.brown.edu/dlanman/courses.html\nJianbo Shi (Ncuts 的始作俑者)\nhttp://www.cis.upenn.edu/~jshi/\nActive Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)\nhttp://www.robots.ox.ac.uk/ActiveVision/index.html\nJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）\nhttp://www.cse.msu.edu/~weng/\n测试图片或视频：\nMiddlebury College‘s Stereo Vision Data Set\nhttp://cat.middlebury.edu/stereo/data.html\nIntelligent Vehicle:\nIVSource\nwww.ivsoruce.net\nRobot Car\nhttp://www.plyojump.com/robot_cars.html\nHow to Build a Robot: The Computer Vision Part\nhttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml\n收集的一般牛人主页（带代码）:\nXiaofei He(machine learning code)\nhttp://people.cs.uchicago.edu/~xiaofei/\nYingNian Wu(active base model code)\nhttp://www.stat.ucla.edu/~ywu/research.html\n布朗大学计算机主页（可找到该校CS牛人博客）\nhttp://www.cs.brown.edu/research/areas.html\nNavneet Dalal(Histograms of Oriented Gradients for Human Detection )\nhttp://www.navneetdalal.com/software\nPaul Viola(Robust Real-timeObject Detection)\nhttp://research.microsoft.com/en-us/um/people/viola/\n人工智能与模式识别国际顶级期刊会议目录（包含该领域最权威的期刊和会议）"}
{"content2":"来源：中国科学院自动化所机器视觉课题组\n本文由中国科学院自动化研究所模式识别国家重点实验室胡占义研究员撰写，对计算机视觉40多年的发展历程进行了简要总结，包括：马尔计算视觉理论，主动视觉与目的视觉，多视几何与摄像机自标定，以及基于学习的视觉。在此基础上，对计算机视觉的未来发展趋势给出了一些展望。\n全文约1.53万字，建议阅读PDF版\n下载方式\n关注公众号，后台回复关键词\n20180921\n1.1 什么是计算机视觉\n正像其它学科一样，一个大量人员研究了多年的学科，却很难给出一个严格的定义，模式识别如此，目前火热的人工智能如此，计算机视觉亦如此。与计算机视觉密切相关的概念有视觉感知（visual perception）,视觉认知(visual cognition),图像和视频理解( image and video understanding). 这些概念有一些共性之处，也有本质不同。从广义上说，计算机视觉就是“赋予机器自然视觉能力”的学科。自然视觉能力，就是指生物视觉系统体现的视觉能力。一则生物自然视觉无法严格定义，在加上这种广义视觉定义又“包罗万象”，同时也不太符合40多年来计算机视觉的研究状况，所以这种“广义计算机视觉定义”，虽无可挑剔，但也缺乏实质性内容，不过是一种“循环式游戏定义”而已。实际上，计算机视觉本质上就是研究视觉感知问题。视觉感知，根据维科百基（Wikipedia）的定义, 是指对“环境表达和理解中，对视觉信息的组织、识别和解释的过程”。根据这种定义，计算机视觉的目标是对环境的表达和理解，核心问题是研究如何对输入的图像信息进行组织，对物体和场景进行识别，进而对图像内容给予解释。\n计算机视觉与人工智能有密切联系，但也有本质的不同。人工智能更强调推理和决策，但至少计算机视觉目前还主要停留在图像信息表达和物体识别阶段。“物体识别和场景理解”也涉及从图像特征的推理与决策，但与人工智能的推理和决策有本质区别。应该没有一个严肃的计算机视觉研究人员会认为AlphaGo, AlphaZero 是计算机视觉，但都会认为它们是典型的人工智能内容。\n简言之，计算机视觉是以图像（视频）为输入，以对环境的表达（representation）和理解为目标，研究图像信息组织、物体和场景识别、进而对事件给予解释的学科。从目前的研究现状看，目前还主要聚焦在图像信息的组织和识别阶段，对事件解释还鲜有涉及，至少还处于非常初级的阶段。\n这里需要强调的是，每个人由于背景不同，偏好不同，知识面不同，对同一问题的观点亦会不同，甚至出现大相径庭的局面。上面为笔者对计算机视觉的理解，也许是片面或错误的。如不少人认为“纹理分析”是计算机视觉的一个重要研究方向，笔者不敢苟同。另外，很多场合，人们把“图像处理”也认为是“计算机视觉”，这也是不恰当的。图像处理是一门独立的学科，图像处理研究图像去噪、图像增强等内容，输入为图像，输出也是图像。计算机视觉利用图像处理技术进行图像预处理，但图像处理本身构不成计算机视觉的核心内容。\n这里顺便说一下，目前很多人对“感知”和“认知”不加区分，给读者带来不必要的困惑和误解。在不少场合下，经常会见到有些“视觉专家”把“认知”和“推理与决策”（reasoning and decision）作为平行概念使用，这事实上是不太严谨的。根据“维基百科”，“认知”是指通过感觉（senses）、经历 (experience)和思考(thoughts)来获取知识(knowledge)和进行理解(understanding)的思维过程（mental process）。认知包括：知识形成（knowledge），注视（attention），记忆（memory），推理（reasoning），问题求解（problem solving）、决策（ decision making）以及语言生成（language production）等。所以，“感知”与“认知”有区别，推理和决策是典型的认知过程，是认知的重要组成部分，它们之间是包含关系，不是平行关系。\n1.2  计算机视觉发展的四个主要阶段\n尽管人们对计算机视觉这门学科的起始时间和发展历史有不同的看法，但应该说， 1982年马尔( David Marr )《视觉》（Marr, 1982）一书的问世，标志着计算机视觉成为了一门独立学科。计算机视觉的研究内容，大体可以分为物体视觉（object vision）和空间视觉（spatial vision）二大部分. 物体视觉在于对物体进行精细分类和鉴别，而空间视觉在于确定物体的位置和形状，为“动作（action）” 服务。正像著名的认知心理学家J.J. Gibson 所言，视觉的主要功能在于“适应外界环境，控制自身运动”。 适应外界环境和控制自身运动，是生物生存的需求，这些功能的实现需要靠物体视觉和空间视觉协调完成。\n计算机视觉40多年的发展中，尽管人们提出了大量的理论和方法，但总体上说，计算机视觉经历了4个主要历程。即： 马尔计算视觉、主动和目的视觉、多视几何与分层三维重建和基于学习的视觉。下面将对这4项主要内容进行简要介绍。\n1.2.1 马尔计算视觉（Computational Vision）\n现在很多计算机视觉的研究人员，恐怕对“马尔计算视觉”根本不了解，这不能不说是一件非常遗憾的事。目前，在计算机上调“深度网络”来提高物体识别的精度似乎就等于从事“视觉研究”。事实上，马尔的计算视觉的提出，不论在理论上还是研究视觉的方法论上，均具有划时代的意义。\n马尔的计算视觉分为三个层次： 计算理论、表达和算法以及算法实现。由于马尔认为算法实现并不影响算法的功能和效果，所以，马尔计算视觉理论主要讨论“计算理论”和“表达与算法”二部分内容。马尔认为，大脑的神经计算和计算机的数值计算没有本质区别，所以马尔没有对“算法实现”进行任何探讨。从现在神经科学的进展看，“神经计算”与数值计算在有些情况下会产生本质区别，如目前兴起的神经形态计算（ Neuromorphological computing），但总体上说，“数值计算”可以“模拟神经计算”。至少从现在看，“算法的不同实现途径”，并不影响马尔计算视觉理论的本质属性。\n1）计算理论(Computational Theory)\n计算理论需要明确视觉目的， 或视觉的主要功能是什么。上世纪70年代，人们对大脑的认识还非常粗浅，目前普遍使用的非创伤型成像手段，如功能核磁共振（FMRI）等，还没有普及。所以，人们主要靠病理学和心理学结果来推断生理功能。即使目前，人们对“视觉的主要功能”到底是什么，也仍然没有定论。如最近几年，MIT的 DiCarlo等人提出了所谓的“目标驱动的感知信息建模”方法（Yamins &DiCarlo et al. 2016a）。他们猜测，猴子IT区（IT: interiortemporal cortex, 物体识别区）的神经元对物体的响应（neuronal responses）“可以通过层次化的卷积神经网络”（HCNN: Hierarchical Convolutional Neural Networks ）来建模。他们认为，只要对HCNN在图像物体分类任务下进行训练，则训练好的HCNN 可以很好定量预测IT 区神经元的响应（Yamins et al. 2014, 2016b）。由于仅仅“控制图像分类性能”对IT神经元响应（群体神经元对某一输入图像物体的响应，就是神经元对该物体的表达或编码）进行定量预测，所以他们将这种框架称之为“目标驱动的框架”。目标驱动的框架提供了一种新的比较通用的建模群体神经元编码的途径，但也存在很大的不足。能否真正像作者所言的那样，仅仅靠“训练图像分类的HCNN”就可以定量预测神经元对图像物体的响应，仍是一个有待进一步深入研究的课题。\n马尔认为视觉不管有多少功能，主要功能在于“从视网膜成像的二维图像来恢复空间物体的可见三维表面形状”，称之为“三维重建”（3D reconstruction）。而且，马尔认为，这种重建过程不是天生就有的，而是可以通过计算完成的。J.J. Gibson 等心理学家，包括格式塔心里学学派( Gestalt psychology)，认为视觉的很多功能是天生就有的。可以想想，如果一种视觉功能与生具有，不可建模，就谈不上计算，也许就不存在今天的“计算机视觉”这门学科了。\n那么，马尔的计算理论是什么呢？这一方面，马尔在其书中似乎并不是介绍得特别具体。他举了一个购买商品的例子，说明计算理论的重要性。如商店结账要用加法而不是乘法。试想如果用乘法结账，每个商品1元钱，则不管你购买多少件商品，你仅仅需要付一元钱。\n马尔的计算理论认为，图像是物理空间在视网膜上的投影，所以图像信息蕴含了物理空间的内在信息，因此，任何计算视觉计算理论和方法都应该从图像出发，充分挖掘图像所蕴含的对应物理空间的内在属性。也就是说，马尔的视觉计算理论就是要“挖掘关于成像物理场景的内在属性来完成相应的视觉问题计算”。因为从数学的观点看，仅仅从图像出发，很多视觉问题具有“歧义性”，如典型的左右眼图像之间的对应问题。如果没有任何先验知识，图像点对应关系不能唯一确定。不管任何动物或人，生活的环境都不是随机的，不管有意识或无意识，时时刻刻都在利用这些先验知识，来解释看到的场景和指导日常的行为和行动。如桌子上放一个水杯的场景，人们会正确地解释为桌子上放了一个水杯，而不把他们看作一个新物体。当然，人类也会经常出错，如大量错觉现象。从这个意义上来说，让计算机来模仿人类视觉是否一定是一条好的途径也是一个未知的命题。飞机的飞行需要借助空气动力学知识，而不是机械地模仿鸟如何飞。\n2）表达和算法（Representationand Algorithm）\n识别物体之前，不管是计算机还是人，大脑（或计算机内存）中事先要有对该物体的存储形式，称之为物体表达（object representation）. 马尔视觉计算理论认为，物体的表达形式为该物体的三维几何形状。马尔当时猜测，由于人在识别物体时与观察物体的视角无关，而不同视角下同一物体在视网膜上的成像又不同，所以物体在大脑中的表达不可能是二维的，可能是三维形状，因为三维形状不依赖于观察视角。另外，当时病理学研究发现，有些病人无法辨认“茶杯”，但可以毫无困难地画出茶杯的形状，因此马尔觉得，这些病人也佐证了他的猜测。从目前对大脑的研究看，大脑的功能是分区的。物体的“几何形状”和“语义”储存在不同的脑区。另外，物体识别也不是绝对地与视角无关，仅仅在一个比较小的变化范围内与视角无关。所以，从当前的研究看，马尔的物体的“三维表达”猜测基本上是不正确的，至少是不完全正确的，但马尔的计算理论仍具有重要的理论意义和应用价值。\n简言之，马尔视觉计算理论的“物体表达”，是指“物体坐标系下的三维形状表达”。注意，从数学上来说，一个三维几何形状，选取的坐标系不同，表达函数亦不同。如一个球体，如果以球心为坐标原点，则球面可以简单表达为：x^2+y^2+z^2=1。 但如果观测者在x轴上2倍半径处观测，则可见球面部分在观测者坐标系下的方程为：x=2-sqrt(1-y^2-z^2)。由此可见，同一物体，选用的坐标系不同，表达方式亦不同。马尔将“观测者坐标系下的三维几何形状表达”称之为“2.5维表达”，物体坐标系下的表达为“三维表达”。所以，在后续的算法部分，马尔重点研究了如何从图像先计算“2.5维表达”，然后转化为“三维表达”的计算方法和过程。\n算法部分是马尔计算视觉的主体内容。马尔认为，从图像到三维表达，要经过三个计算层次：首先从图像得到一些基元（primal sketch）, 然后通过立体视觉（stereopsis）等模块将基元提升到2.5维表达，最后提升到三维表达。\n下图总结给出了马尔视觉计算理论的算法流程：\n马尔计算理论中算法的三个计算层次\n由上图所示，首先从图像提取边缘信息（二阶导数的过零点），然后提取点状基元（blob, 线状基元（edge）和杆状基元 (bar), 进而对这些初级基元（raw primal sketch）组合形成完整基元（full primal sketch），上述过程为视觉计算理论的特征提取阶段。在此基础上，通过立体视觉和运动视觉等模块，将基元提升到2.5维表达。最后，将2.5维表达提升到三维表达。在马尔的《视觉》一书中，重点介绍了特征提取和2.5维表达对应的计算方法。在2.5维表达部分，也仅仅重点介绍了立体视觉和运动视觉部分。由于当双眼（左右相机）的相互位置已知时（计算机视觉中称之为相机外参数），立体视觉就转化为“左右图像点的对应问题”（image point correspondence）, 所以，马尔在立体视觉部分重点介绍了图像点之间的匹配问题，即如何剔除误匹配，并给出了对应算法。\n立体视觉等计算得到的三维空间点仅仅是在“观测者坐标系下的坐标”，是物体的2.5维表示。如何进一步提升到物体坐标系下的三维表示，马尔给出了一些思路，但这方面都很粗泛。如确定物体的旋转主轴等等，这部分内容，类似于后来人们提出的“骨架模型”（skeleton model）构造.\n需要指出的是，马尔的视觉计算理论是一种理论体系。在此体系下，可以进一步丰富具体的计算模块，构建“通用性视觉系统”（general vision system）。只可惜马尔（Jan.15,1945 ~ Nov.17,1980 ）1980年底就因白血病去世，包括他的《视觉》一书，也是他去世后出版的。马尔的英年早逝，不能说不是计算机视觉界的一大损失。由于马尔的贡献，所以二年一度的国际计算机视觉大会（ICCV: International Conference on Computer Vision）设有马尔奖（MarrPrize），作为会议的最佳论文奖。另外，在认知科学领域，也设有马尔奖，因为马尔对认知科学也有巨大的贡献。以同一人名在不同领域设立奖项，实属罕见，可见马尔对计算机视觉的影响有多深远。正如S. Edelman 和 L. M. Vaina 在《 International Encyclopedia of the Social & Behavioral Sciences 》中对马尔的评价那样，“马尔前期给出的集成数学和神经生物学对大脑理解的三项工作，已足以使他在任何情况下在英国经验主义二个半世纪的科学殿堂中占有重要的一席，…, 然而，他进一步提出了更加有影响的计算视觉理论”。所以，从事计算机视觉研究的人员对马尔计算视觉不了解，实在是一件比较遗憾的事。\n1.2.2 昙花一现的主动和目的视觉\n很多人介绍计算机视觉时，将这部分内容不作为一个单独部分加以介绍，主要是因为“主动视觉和目的视觉”并没有对计算机视觉后续研究形成持续影响。但作为计算机视觉发展的一个重要阶段，这里还是有必要予以介绍一下。\n上世纪80年代初马尔视觉计算理论提出后，学术界兴起了“计算机视觉”的热潮。人们想到的这种理论的一种直接应用就是给工业机器人赋予视觉能力，典型的系统就是所谓的“基于部件的系统”（parts-based system）。然而，10多年的研究，使人们认识到，尽管马尔计算视觉理论非常优美，但“鲁棒性”（Robustness）不够，很难想人们预想的那样在工业界得到广泛应用。这样，人们开始质疑这种理论的合理性，甚至提出了尖锐的批评。\n对马尔计算视觉理论提出批评最多的有二点：一是认为这种三维重建过程是“纯粹自底向上的过程”（pure bottom-up process），缺乏高层反馈（top-down feedback）；二是“重建”缺乏“目的性和主动性”。由于不同的用途，要求重建的精度不同，而不考虑具体任务，仅仅“盲目地重建一个适合任何任务的三维模型”似乎不合理。\n对马尔视觉计算理论提出批评的代表性人物有：马里兰大学的 J. Y. Aloimonos;宾夕法尼亚大学的R. Bajcsy和密西根州立大学的A. K. Jaini。 Bajcsy 认为，视觉过程必然存在人与环境的交互，提出了主动视觉的概念（active vision）. Aloimonos认为视觉要有目的性，且在很多应用，不需要严格三维重建，提出了“目的和定性视觉”（purpose and qualitative vision） 的概念。 Jain 认为应该重点强调应用，提出了“应用视觉”（ practicing vision）的概念。上世纪80年代末到90年代初，可以说是计算机视觉领域的“彷徨”阶段。真有点“批评之声不绝，视觉之路茫茫”之势。\n针对这种情况，当时视觉领域的一个著名刊物（CVGIP: Image Understanding）于1994年组织了一期专刊对计算视觉理论进行了辩论。首先由耶鲁大学的M. J. Tarr和布朗大学的M. J.Black写了一篇非常有争议性的观点文章（Tarr & Black, 1994），认为马尔的计算视觉并不排斥主动性，但把马尔的“通用视觉理论”（general vision）过分地强调“应用视觉”是“短见”（myopic）之举。通用视觉尽管无法给出严格定义，但“人类视觉”是最好的样板。这篇观点文章发表后，国际上20多位著名的视觉专家也发表了他们的观点和评论。大家普遍的观点是，“主动性”“目的性”是合理的，但问题是如何给出新的理论和方法。而当时提出的一些主动视觉方法，一则仅仅是算法层次上的改进，缺乏理论框架上的创新，另外，这些内容也完全可以纳入到马尔计算视觉框架下。所以，从1994年这场视觉大辩论后，主动视觉在计算机视觉界基本没有太多实质性进展。这段“彷徨阶段”持续不长，对后续计算机视觉的发展产生的影响不大，犹如“昙花一现”之状。\n值得指出的是，“主动视觉”应该是一个非常好的概念，但困难在于“如何计算”。 主动视觉往往需要“视觉注视”（visual attention），需要研究脑皮层（cerebral cortex）高层区域到低层区域的反馈机制，这些问题，即使脑科学和神经科学已经较20年前取得了巨大进展的今天，仍缺乏“计算层次上的进展”可为计算机视觉研究人员提供实质性的参考和借鉴。近年来，各种脑成像手段的发展，特别是 “连接组学”（Connectomics）的进展，可望为计算机视觉人员研究大脑反馈机制提供“反馈途径和连接强度”提供一些借鉴。\n1.2.3 多视几何和分层三维重建（Multiple View Geometry and Stratified 3D Reconstruction）\n上世纪90年代初计算机视觉从“萧条”走向进一步“繁荣”，主要得益于以下二方面的因素：首先，瞄准的应用领域从精度和鲁棒性要求太高的“工业应用”转到要求不太高，特别是仅仅需要“视觉效果”的应用领域，如远程视频会议（teleconference），考古，虚拟现实，视频监控等。另一方面，人们发现，多视几何理论下的分层三维重建能有效提高三维重建的鲁棒性和精度。\n多视几何的代表性人物首数法国INRIA的O. Faugeras ( Faugeras O, 1993), 美国GE 研究院的R.Hartely （现已回到了澳大利亚国立大学）和英国牛津大学的 A. Zisserman。应该说，多视几何的理论于2000年已基本完善。 2000 年Hartley 和Zisserman 合著的书 (Hartley & Zisserman 2000) 对这方面的内容给出了比较系统的总结，而后这方面的工作主要集中在如何提高“大数据下鲁棒性重建的计算效率”。大数据需要全自动重建，而全自动重建需要反复优化，而反复优化需要花费大量计算资源。所以，如何在保证鲁棒性的前提下快速进行大场景的三维重建是后期研究的重点。举一个简单例子，假如要三维重建北京中关村地区，为了保证重建的完整性，需要获取大量的地面和无人机图像。假如获取了1万幅地面高分辨率图像（4000×3000），5 千幅高分辨率无人机图像（8000×7000）（这样的图像规模是当前的典型规模），三维重建要匹配这些图像，从中选取合适的图像集，然后对相机位置信息进行标定并重建出场景的三维结构，如此大的数据量，人工干预是不可能的，所以整个三维重建流程必须全自动进行。这样需要重建算法和系统具有非常高的鲁棒性，否则根本无法全自动三维重建。在鲁棒性保证的情况下，三维重建效率也是一个巨大的挑战。所以，目前在这方面的研究重点是如何快速、鲁棒地重建大场景。\n1）多视几何（ Multiple View Geometry）\n由于图像的成像过程是一个中心投影过程（perspective projection），所以“多视几何”本质上就是研究射影变换下图像对应点之间以及空间点与其投影的图像点之间的约束理论和计算方法的学科（注意：针孔成像模型（The pinhole camera model）是一种中心投影， 当相机有畸变时，需要将畸变后的图像点先校正到无畸变后才可以使用多视几何理论）。计算机视觉领域，多视几何主要研究二幅图像对应点之间的对极几何约束（epipolar geometry）, 三幅图像对应点之间的三焦张量约束（tri-focal tensor），空间平面点到图像点，或空间点为平面点投影的多幅图像点之间的单应约束（homography）等。在多视几何中，射影变换下的不变量，如绝对二次曲线的像（The image of the absolute conic）,绝对二次曲面的像（Theimage of the absolute quadric）, 无穷远平面的单应矩阵（infinite homography），是非常重要的概念，是摄像机能够自标定的“参照物”。由于这些量是无穷远处“参照物”在图像上的投影，所以这些量与相机的位置和运动无关（原则上任何有限的运动不会影响无限远处的物体的性质），所以可以用这些“射影不变量”来自标定摄像机。关于多视几何和摄像机自标定的详细内容，可参阅Hartley 和Zisserman 合著的书（Hartley & Zisserman,2000）.\n总体上说，多视几何就其理论而言，在射影几何中不能算新内容。Hartley, Faugeras,  Zissermann等将多视几何理论引入到计算机视觉中，提出了分层三维重建理论和摄像机自标定理论，丰富了马尔三维重建理论，提高了三维重建的鲁棒性和对大数据的适应性，有力推动了三维重建的应用范围。所以，计算机视觉中的多视几何研究，是计算机视觉发展历程中的一个重要阶段和事件。\n多视几何需要射影几何（projectivegeometry）的数学基础。射影几何是非欧几何，涉及平行直线相交，平行平面相交等抽象概念，表达和计算要在“齐次坐标”（homogeneous coordinates）下进行，这给“工科学生”带来不小的困难。所以，大家要从事这方面的研究，一定要先打好基础，至少要具备必要的射影几何知识。否则，做这方面的工作，无异于浪费时间。\n2）分层三维重建（ Stratified 3D Reconstruction）\n所谓的分层三维重建，如下图所示，就是指从多幅二维图像恢复欧几里德空间的三维结构时，不是从图像一步到欧几里德空间下的三维结构，而是分步分层地进行。即先从多幅图像的对应点重建射影空间下的对应空间点(即射影重建：projective reconstruction)，然后把射影空间下重建的点提升到仿射空间下(即仿射重建：affine reconstruction)，最后把仿射空间下重建的点再提升到欧几里德空间（或度量空间: metric reconstruction）（注：度量空间与欧几里德空间差一个常数因子。由于分层三维重建仅仅靠图像进行空间点重建，没有已知的“绝对尺度”，如“窗户的长为1米”等，所以从图像仅仅能够把空间点恢复到度量空间）。\n这里有几个概念需要解释一下。以空间三维点的三维重建为例，所谓的“射影重建”，是指重建的点的坐标与该点在欧几里德空间下的坐标差一个“射影变换”。所谓的“仿射重建”，是指重建的点的坐标与该点在欧几里德空间下的坐标差一个“仿射变换”。所谓的“度量重建”，是指重建的点的坐标与该点在欧几里德空间下的坐标差一个“相似变换”。\n由于任何一个视觉问题最终都可以转化为一个多参数下的非线性优化问题，而非线性优化的困难在于找到一个合理的初值。由于待优化的参数越多，一般来说解空间越复杂，寻找合适的初值越困难，所以，如果一个优化问题如能将参数分组分步优化，则一般可以大大简化优化问题的难度。分层三维重建计算上的合理性正是利用了这种“分组分步”的优化策略。以三幅图像为例，直接从图像对应点重建度量空间的三维点需要非线性优化16个参数（假定相机内参数不变，5个相机内参数，第二幅和第三幅图像相对于第一幅图像的相机的旋转和平移参数，去掉一个常数因子，所以5+2×(3+3)-1=16），　这是一个非常困难的优化问题。但从图像对应点到射影重建需要“线性”估计22个参数，由于是线性优化，所以优化问题并不困难。从射影重建提升到仿射重建需要“非线性”优化三个参数（无穷远平面的3个平面参数），而从仿射重建提升到度量重建需要“非线性”优化5个参数（摄像机的5个内参数）。因此，分层三维重建仅仅需要分步优化3个和5个参数的非线性优化问题，从而大大减小了三维重建的计算复杂度。\n分层三维重建的另一个特点是其理论的优美性。射影重建下，空间直线的投影仍为直线，二条相交直线其投影直线仍相交，但空间直线之间的平行性和垂直性不再保持。仿射重建下可以保持直线的平行性，但不能保持直线的垂直性。度量重建既可以保持直线之间的平行线，也可以保持垂直性。在具体应用中，可以利用这些性质逐级提升重建结果。\n分层三维重建理论可以说是计算机视觉界继马尔计算视觉理论提出后又一个最重要和最具有影响力的理论。目前很多大公司的三维视觉应用，如苹果公司的三维地图，百度公司的三维地图，诺基亚的Streetview, 微软的虚拟地球，其后台核心支撑技术的一项重要技术就是分层三维重建技术。\n3）摄像机自标定（Cameraself-calibration）\n所谓摄像机标定，狭义上讲，就是确定摄像机内部机械和光电参数的过程，如焦距，光轴与像平面的交点等。尽管相机出厂时都标有一些标准参数，但这些参数一般不够精确，很难直接在三维重建和视觉测量中应用。所以，为了提高三维重建的精度，需要对这些相机内参数（intrinsic parameters）进行估计。估计相机的内参数的过程，称为相机标定。在文献中，有时把估计相机在给定物体坐标系下的坐标，或相机之间相互之间的位置关系，称为相机外参数（extrinsic parameters）标定。但一般无明确指定时，相机标定就是指对相机内参数的标定。\n相机标定包含二方面的内容：“成像模型选择”和“模型参数估计”。相机标定时首先需要确定“合理的相机成像模型”，如是不是针孔模型，有没有畸变等。目前关于相机模型选择方面，没有太好的指导理论，只能根据具体相机和具体应用确定。随着相机加工工艺的提高，一般来说，普通相机（非鱼眼或大广角镜头等特殊相机）一般使用针孔成像模型（加一阶或二阶径向畸变）就足以了。其它畸变很小，可以不加考虑。当相机成像模型确定后，进一步需要估计对应的模型参数。文献中人们往往将成像模型参数估计简单地认为就是相机标定，是不全面的。事实上，相机模型选择是相机标定最关键的步骤。一种相机如果无畸变而在标定时考虑了畸变，或有畸变而未加考虑，都会产生大的误差。视觉应用人员应该特别关注“相机模型选择”问题。\n相机参数估计原则上均需要一个“已知三维结构”的“标定参考物”，如平面棋盘格，立体块等。所谓相机标定，就是利用已知标定参考物和其投影图像，在已知成像模型下建立模型参数的约束方程，进而估计模型参数的过程。所谓“自标定”，就是指“仅仅利用图像特征点之间的对应关系，不需要借助具体物理标定参考物，进行模型参数估计的过程”。“传统标定”需要使用加工尺寸已知的标定参考物，自标定不需要这类物理标定物，正像前面多视几何部分所言，使用的是抽象的无穷远平面上的“绝对二次曲线”和“绝对二次曲面”。从这个意义上来说，自标定也需要参考物，仅仅是“虚拟的无穷远处的参考物”而已。\n摄像机自标定需要用到两幅图像之间的约束，如基础矩阵（fundamental matrix）, 本质矩阵（essential matrix）, 以及三幅图像之间的三焦张量约束等。另外，Kruppa 方程也是一个重要的概念。这些内容是多视几何的重要内容，后续章节将进行详细介绍。\n1.2.4 基于学习的视觉（Learning based vision）\n基于学习的视觉，是指以机器学习为主要技术手段的计算机视觉研究。基于学习的视觉研究，文献中大体上分为二个阶段：本世纪初的以流形学习( manifold Learning)为代表的子空间法( subspace method)和目前以深度神经网络和深度学习（deep neural networks and deep learning）为代表的视觉方法。\n1）流形学习（Manifold Learning）\n正像前面所指出的，物体表达是物体识别的核心问题。给定图像物体，如人脸图像，不同的表达，物体的分类和识别率不同。另外，直接将图像像素作为表达是一种“过表达”，也不是一种好的表达。流形学习理论认为，一种图像物体存在其“内在流形”（intrinsic manifold）, 这种内在流形是该物体的一种优质表达。所以，流形学习就是从图像表达学习其内在流形表达的过程，这种内在流形的学习过程一般是一种非线性优化过程。\n流形学习始于2000年在Science 上发表的二篇文章（ Tenenbaum et al., 2000） (Roweis & Lawrence 2000)。流形学习一个困难的问题是没有严格的理论来确定内在流形的维度。人们发现，很多情况下流形学习的结果还不如传统的PCA （Principal Component Analysis），LDA（ linear DiscriminantAnalysis ）， MDS（ Multidimensional Scaling）等. 流形学习的代表方法有：LLE（Locally Linear Embedding ）(Roweis & Lawrence 2000)，Isomap （ Tenenbaum et al., 2000）， Laplacian Eigenmaps (Belkin & Niyogi, 2001)等。\n2）深度学习（Deep Learning）\n深度学习( LeCunet al. 2015) 的成功，主要得益于数据积累和计算能力的提高。深度网络的概念上世纪80年代就已提出来了，只是因为当时发现“深度网络”性能还不如“浅层网络”，所以没有得到大的发展。目前似乎有点计算机视觉就是深度学习的应用之势，这可以从计算机视觉的三大国际会议：国际计算机视觉会议（ICCV），欧洲计算机视觉会议（ECCV）和计算机视觉和模式识别会议（CVPR），上近年来发表的论文可见一般。目前的基本状况是，人们都在利用深度学习来“取代”计算机视觉中的传统方法。“研究人员”成了“调程序的机器”，这实在是一种不正常的“群众式运动”。牛顿的万有引力定律，麦克斯韦的电磁方程，爱因斯坦的质能方程，量子力学中的薛定谔方程，似乎还是人们应该追求的目标。\n关于深度网络和深度学习，详细内容可参阅相关文献，这里仅仅强调以下几点：\n（1）深度学习在物体视觉方面较传统方法体现了巨大优势，但在空间视觉，如三维重建，物体定位方面，仍无法与基于几何的方法相媲美。这主要是因为深度学习很难处理图像特征之间的误匹配现象。在基于几何的三维重建中，RANSAC （Random Sample Consensus）等鲁棒外点（误匹配点）剔除模块可以反复调用，而在深度学习中，目前还很难集成诸如RANSAC等外点剔除机制。笔者认为，如果深度网络不能很好地集成外点剔除模块，深度学习在三维重建中将很难与基于几何的方法相媲美，甚至很难在空间视觉中得到有效应用；\n（2） 深度学习在静态图像物体识别方面已经成熟，这也是为什么在ImageNet上的物体分类竞赛已不再举行的缘故；\n（3） 目前的深度网络，基本上是前馈网络（feedforwardNetworks）.不同网络主要体现在使用的代价函数不同。下一步预计要探索具有“反馈机制”的层次化网络。反馈机制，需要借鉴脑神经网络机制，特别是连接组学的成果。\n（4） 目前对视频的处理，人们提出了RCNN (Recurrent Neural Networks). 循环( recurrent) 是一种有效的同层作用机制，但不能代替反馈。大脑皮层远距离的反馈（将在生物视觉简介一章介绍）可能是形成大脑皮层不同区域具有不同特定功能的神经基础。所以，研究反馈机制，特别具有“长距离反馈”（跨多层之间）的深度网络, 将是今后研究图像理解的一个重要方向；\n（5）尽管深度学习和深度网络在图像物体识别方面取得了“变革性”成果，但为什么“深度学习”会取得如此好的结果目前仍然缺乏坚实的理论基础。目前已有一些这方面的研究，但仍缺乏系统性的理论。事实上，“层次化”是本质，不仅深度网络，其它层次化模型，如Hmax 模型（Riesenhuber & Poggio,1999） HTM （Hierarchical Temporal memory）模型（George & Hawkins, 2009）存在同样的理论困惑。为什么“层次化结构”（ hierarchical structure ）具有优势仍是一个巨大的迷。\n1.3 计算机视觉的若干发展趋势\n信息科学发展之迅速，对未来10年的发展趋势进行预测，有点“算命”的感觉。 对计算机视觉而言，笔者有以下几点对未来发展的展望：\n（1） 基于学习的物体视觉和基于几何的空间视觉继续“相互独立”进行。深度学习在短时期内很难代替几何视觉。在深度网络中如何引入“鲁棒外点剔除模块”将是一个探索方向，但短时间内估计很难有实质性进展；\n（2） 基于视觉的定位将更加趋向“应用性研究”，特别是多传感器融合的视觉定位技术。\n（3） 三维点云重建技术已经比较成熟，如何从“点云”到“语义”是未来研究重点。“语义重建”将点云重建、物体分割和物体识别同时进行，是三维重建走向实用的前提。\n（4）对室外场景的三维重建，如何重建符合“城市管理规范”的模型是一个有待解决的问题。室内场景重建估计最大的潜在应用是“家庭服务机器人”。　鉴于室内重建的应用还缺乏非常具体的应用需求和驱动，在加上室内环境的复杂性，估计在３－５年内很难有突破性进展。\n（5）对物体识别而言，基于深度学习的物体识别估计将从“通用识别”向“特定领域物体的识别”发展。“特定领域”可以提供更加明确和具体的先验信息，可以有效提高识别的精度和效率，更加具有实用性；\n（６）目前基于RCNN 对视频理解的趋势将会持续；\n（7） 解析深度网络机理的工作具有重大的理论意义和挑战性，鉴于深度网络的复杂性，估计近期很难取得突破性进展；\n（8）具有“反馈机制”的深度网络结构（architecture）研究必将是下一个研究热点。\n1.4 几种典型的物体表达理论（Object representation theories）\n正像前面所述，物体表达是计算机视觉的一个核心科学问题。这里，“物体表达理论”与“物体表达模型”需要加以区别。“表达理论”是指文献中大家比较认可的方法。“表达模型”容易误解为“数学上对物体的某种描述”。计算机视觉领域，比较著名的物体表达理论有以下三种：\n1）马尔的三维物体表达\n前面已经介绍过，马尔视觉计算理论认为物体的表达是物体坐标系下的三维表达\n2）基于二维图像的物体表达（View-basedobject representation）\n尽管理论上一个三维物体可以成像为无限多不同的二维图像，但人的视觉系统仅仅可以识别“有限个图像”。鉴于神经科学对于猴子腹部通道（ventral pathway）(注：腹部通道认为是物体识别通道)的研究进展，T. Poggio 等提出了基于图像的物体表达（Poggio & Bizzi, 2004），即对一个三维物体的表达是该物体的一组典型的二维图像（view）。目前，也有人认为 Poggio等的”view”不能狭义地理解为二维图像，也包含以观测者为坐标系下的三维表示，即马尔的2.5维表示（Anzai & DeAngelis，2010）。\n3）逆生成模型表达（Inversegenerative model representation ）\n长期以来，人们认为物体识别模型为“鉴别模型”（ discriminative model），而不是“生成模型”（ generative model ）。近期对猴子腹部通道的物体识别研究表明，猴子大脑皮层的IT 区（ Inferior Temporal: 物体表达区域）可能在于编码物体及其成像参数（如光照和姿态，几何形状，纹理等）（Yildirim et al. 2015）（Yamins &DiCarlo，2016b.）。由于已知这些参数就可以生成对应图像，所以对这些参数的编码可以认为是逆生成模型表达。逆生成模型表达可以解释为什么深度学习中的Encoder-decoder 网络结( Badrinarayanan et al. 2015) 可以取得比较好的效果，因为Encoder本质上就是图像的逆生成模型。另外，深度学习中提出的“逆图形学”概念（ Inverse Graphic）( Kulkarniet al. 2015),从原理上也是一种逆生成模型。逆图形学是指先从图像学习到图像生成参数，然后把同一物体在不同参数下的图像归类为同一物体，通过这种“等变物体识别”(Equivariant recognition) 来达到最终的“不变物体识别”（invariantrecognition）。\n总之，本文对计算机视觉的理论、现状和未来发展趋势进行了一些总结和展望，希望能给读者了解该领域提供一些帮助。特别需要指出的是，这里很多内容也仅仅是笔者的一些“个人观点”和“个人偏好”下总结的一些内容，以期对读者有所帮助但不引起误导。"}
{"content2":"关于计算机视觉的介绍性文章，包括计算机视觉的定义，和人类视觉的区别以及涉及到的学科等等。\n1. 什么是计算机视觉\n计算机视觉既是工程领域，也是科学领域中的一个富有挑战性重要研究领域。计算机视觉是一门综合性的学科，它已经吸引了来自各个学科的研究者参加到对它的研究之中。其中包括计算机科学和工程、信号处理、物理学、应用数学和统计学，神经生理学和认知科学等。\n视觉是各个应用领域，如制造业、检验、文档分析、医疗诊断，和军事等领域中各种智能／自主系统中不可分割的一部分。由于它的重要性，一些先进国家，例如美国把对计算机视觉的研究列为对经济和科学有广泛影响的科学和工程中的重大基本问题，即所谓的重大挑战（grandchallenge）。\"计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。\"[Neg91]作为一门学科，计算机视觉开始于60年代初，但在计算机视觉的基本研究中的许多重要进展是在80年代取得的。现在计算机视觉已成为一门不同于人工智能、图象处理、模式识别等相关领域的成熟学科。计算机视觉与人类视觉密切相关，对人类视觉有一个正确的认识将对计算机视觉的研究非常有益。为此我们将先介绍人类视觉。\n1.1 人类视觉\n感觉是人的大脑与周围世界联系的窗口，它的任务是识别周围的物体，并告诉这些物体之间的关系。我们的思维活动是以我们对客观世界与环境的认识为基础的，而感觉则是外界是客观世界与我们对环境的认识之间的桥梁，使我们的思维与周围世界建立某种对应关系。视觉则是人最重要的感觉，它是人的主要感觉来源，人类认识外界信息的80%来自视觉。人有多种感觉，但对人的智力产生影响的主要是视觉和听觉。味觉和嗅觉是丰富多样的，但很少有人去思考它们。在视觉和听觉中形状、色彩、运动、声音等就很容易被结合成各种明确和高度复杂、多样的空间和时间的组织结构。所以这两种感觉就成了理智活动得以行使和发挥作用的非常合适的媒介和环境。但人听到的声音要想具有意义还需要联系其它的感性材料。而视觉则不同，它是一种高度清晰的媒介，它提供关于外界世界中各种物体和事件的丰富信息。因此它是思维的一种最基本的工具。\n视觉对正常人来说是生而有之，毫不费力的能力。但实际上视觉系统所完成的功能却十分复杂的。有人认为视觉本身就包含了思维的一切基本因素。设想你要在一个会场中寻找一位朋友，呈现在你眼前的是由参加会议的人、桌、椅、主席台等组成的复杂景物。眼睛得到这些信息以后先要对景物的各部分进行分类，然后从中选出与朋友的外表有关的特征作出判断，那么在人的眼睛视网膜上映照的景物成象是否就能直接提供判断时所需要的有关特征呢？不是的，这里需要大脑的思考。例如，虽然人在不同距离处观察同一物体时在眼睛中成象的大小是不同的。但人们在观察某人以便估计他的身高时却不会因为他在近处而感到他高些，也不会因他在远处而感到他矮些。这是由于大脑根据被观察物体的距离和与周围物体的比较，并依靠有关的知识对输入的图象信息进行处理，解释的结果。如果你是在一个灯光暗淡的剧院中寻找朋友，这个问题就变得更为困难。\n你刚走进剧院时开始会感到一片漆黑看不清东西，过了几分钟你的眼睛变得习惯于在黑暗中观察。事实上你的视觉系统在此期间中对微光变得更敏感了。但这时许多本来可用的信息丧失了，物体可能难以与背景相区分，许多细节难以分辨。即使这样人也总能认出朋友。总之，视觉是一个复杂的感知和思维的过程，视觉器官-眼睛接受外界的刺激信息，而大脑对这些信息通过复杂的机理进行处理和解释，使这些刺激具有明确的物理意义。\n从以上分析我们还可以看到敏感（Sansation）、感觉（Perception）、认知（Cognition）这三个概念之间的联系和差别。敏感是把外界的各种刺激转换成人体神经系统能够接受的生物电信号。它所完成的是信号的转换，并不涉及对信号的理解。例如，人眼是视觉的敏感器官，它使光信号通过视网膜转换电信号。与摄象机的光电传感器相似，视网膜的感光细胞对光信号在平面上进行采样，产生点阵形式的电信号，所不同的仅是摄象机的空间采样是均匀的，而视网膜的采样是不均匀的，在中央凹附近采样分辨率高，而在周围的分辨率低。而感觉的任务是把敏感器官的各种输入转换和处理成为对外部世界的理解。例如，对视觉来说就是能说出周围世界中有什么东西和这些东西之间的空间关系。这些都是关于周围世界的概念。从输入的点阵形式的信号到形式对客观世界的各种概念其中要经过复杂的信息处理和推理。而认知是以人们对周围客观世界的概念为基础的。如果没有感觉这个人与外部世界的桥梁或窗口，人的思维活动就换去基本的依据。\n1.2 计算机视觉\n人类正在进入信息时代，计算机将越来越广泛地进入几乎所有领域。一方面是更多未经计算机专业训练的人也需要应用计算机，而另一方面是计算机的功能越来越强，使用方法越来越复杂。这就使人在进行交谈和通讯时的灵活性与目前在使用计算机时所要求的严格和死板之间产生了尖锐的矛盾。人可通过视觉和听觉，语言与外界交换信息，并且可用不同的方式表示相同的含义，而目前的计算机却要求严格按照各种程序语言来编写程序，只有这样计算机才能运行。为使更多的人能使用复杂的计算机，必须改变过去的那种让人来适应计算机，来死记硬背计算机的使用规则的情况。而是反过来让计算机来适应人的习惯和要求，以人所习惯的方式与人进行信息交换，也就是让计算机具有视觉、听觉和说话等能力。这时计算机必须具有逻辑推理和决策的能力。具有上述能力的计算机就是智能计算机。\n智能计算机不但使计算机更便于为人们所使用，同时如果用这样的计算机来控制各种自动化装置特别是智能机器人，就可以使这些自动化系统和智能机器人具有适应环境，和自主作出决策的能力。这就可以在各种场合取代人的繁重工作，或代替人到各种危险和恶劣环境中完成任务。\n计算机视觉就是用各种成象系统代替视觉器官作为输入敏感手段，由计算机来代替大脑完成处理和解释。计算机视觉的最终研究目标就是使计算机能象人那样通过视觉观察和理解世界，具有自主适应环境的能力。要经过长期的努力才能达到的目标。因此，在实现最终目标以前，人们努力的中期目标是建立一种视觉系统，这个系统能依据视觉敏感和反馈的某种程度的智能完成一定的任务。例如，计算机视觉的一个重要应用领域就是自主车辆的视觉导航，目前还没有条件实现象人那样能识别和理解任何环境，完成自主导航的系统。因此，目前人们努力的研究目标是实现在高速公路上具有道路跟踪能力，可避免与前方车辆碰撞的视觉辅助驾驶系统。这里要指出的一点是在计算机视觉系统中计算机起代替人脑的作用，但并不意味着计算机必须按人类视觉的方法完成视觉信息的处理。计算机视觉可以而且应该根据计算机系统的特点来进行视觉信息的处理。但是，人类视系统是迄今为止，人们所知道的功能最强大和完善的视觉系统。如在以下的章节中会看到的那样，对人类视觉处理机制的研究将给计算机视觉的研究提供启发和指导。因此，用计算机信息处理的方法研究人类视觉的机理，建立人类视觉的计算理论，也是一个非常重要和信人感兴趣的研究领域。这方面的研究被称为计算视觉（ComputationalVision）。计算视觉可被认为是计算机视觉中的一个研究领域。\n有不少学科的研究目标与计算机视觉相近或与此有关。这些学科中包括图象处理、模式识别或图象识别、景物分析、图象理解等。由于历史发展或领域本身的特点这些学科互有差别，但又有某种程度的相互重迭。为了清晰起见，我们把这些与计算机视觉有关的学科研究目标和方法的角度加以归纳。\n图象处理\n图象处理技术把输入图象转换成具有所希望特性的另一幅图象。例如，可通过处理使输出图象有较高的信-噪比，或通过增强处理突出图象的细节，以便于操作员的检验。在计算机视觉研究中经常利用图象处理技术进行预处理和特征抽取。\n模式识别（图象识别）\n模式识别技术根据从图象抽取的统计特性或结构信息，把图象分成予定的类别。例如，文字识别或指纹识别。在计算机视觉中模式识别技术经常用于对图象中的某些部分，例如分割区域的识别和分类。\n图象理解（景物分析）\n给定一幅图象，图象理解程序不仅描述图象本身，而且描述和解释图象所代表的景物，以便对图象代表的内容作出决定。在人工智能视觉研究的初期经常使用景物分析这个术语，以强调二维图象与三维景物之间的区别。图象理解除了需要复杂的图象处理以外还需要具有关于景物成象的物理规律的知识以及与景物内容有关的知识。\n在建立计算机视觉系统时需要用到上述学科中的有关技术，但计算机视觉研究的内容要比这些学科更为广泛。计算机视觉的研究与人类视觉的研究密切相关（见1.3.5中的论述）。为实现建立与人的视觉系统相类似的通用计算机视觉系统的目标需要建立人类视觉的计算机理论。\n本文转自：http://www.chinaai.org/ip/image-processing/computer-vision.html"}
{"content2":"提示：本文为笔者原创，转载请注明出处：\nhttps://blog.csdn.net/carson2005/article/details/6601109\n关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）南开大学程明明助教：http://mmcheng.net/ 图像分割、检索, bing特征快速目标（行人）检测；\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://www.robots.ox.ac.uk/~phst/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/\n（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；\n（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、\n（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库\n（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理\n（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、\n（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n（213）北京航空航天大学周付根教授：http://www.sa.buaa.edu.cn/html/yhxy/szll/jsfc/txcl/1521.html  医学图像处理\n（214）北京航空航天大学姜志国教授：http://www.sa.buaa.edu.cn/html/yhxy/xygk/zyld/1595.html 医学图像处理，遥感图像分割；\n（215）德国萨尔布吕肯大学博士后R. Benenson： http://rodrigob.github.io/# 行人检测，无人驾驶汽车\n（216）西南财经大学段江教授：http://it.swufe.edu.cn/2011-09/25/201109251002096701.html 高动态范围图像处理\n（217）中科院沈阳自动化所华春生研究员：http://people.ucas.ac.cn/~huacs 行人检测、目标跟踪、聚类分析\n（218）华中科技大学自动化学院张天序教授：http://auto.hust.edu.cn/viewnews-1978 红外图像处理，医学图像处理，武器装备图像处理\n（219）普林斯顿大学Jianxiong Xiao助理教授：http://vision.princeton.edu/people/xj/ 3D重建、3D识别、深度学习"}
{"content2":"计算机视觉五大技术\n文章来源：企鹅号 - AI火箭营\n计算机视觉五大技术 - 云+社区 - 腾讯云\n当前计算机视觉是深度学习领域最热门的研究领域之一。\n计算机视觉实际上是一个跨领域的交叉学科，包括计算机科学（图形、算法、理论、系统、体系结构），数学（信息检索、机器学习），工程学（机器人、语音、自然语言处理、图像处理），物理学（光学 ），生物学（神经科学）和心理学（认知科学）等等。许多科学家认为，计算机视觉为人工智能的发展开拓了道路。\n那么什么是计算机视觉呢？ 这里给出了几个比较严谨的定义：\n“对图像中的客观对象构建明确而有意义的描述”（Ballard＆Brown，1982）\n“从一个或多个数字图像中计算三维世界的特性”（Trucco＆Verri，1998）\n“基于感知图像做出对客观对象和场景有用的决策”（Sockman＆Shapiro，2001）\n为什么要学习计算机视觉？\n一个显而易见的答案就是，这个研究领域已经衍生出了一大批快速成长的、有实际作用的应用，例如：\n视觉识别是计算机视觉的关键组成部分，如图像分类、定位和检测。神经网络和深度学习的最新进展极大地推动了这些最先进的视觉识别系统的发展。在本文中，我将分享 5 种主要的计算机视觉技术，并介绍几种基于计算机视觉技术的深度学习模型与应用。\n1、图像分类\n给定一组各自被标记为单一类别的图像，我们对一组新的测试图像的类别进行预测，并测量预测的准确性结果，这就是图像分类问题。图像分类问题需要面临以下几个挑战：\n视点变化，尺度变化，类内变化，图像变形，图像遮挡，照明条件和背景杂斑\n我们怎样来编写一个图像分类算法呢？\n计算机视觉研究人员提出了一种基于数据驱动的方法。\n该算法并不是直接在代码中指定每个感兴趣的图像类别，而是为计算机每个图像类别都提供许多示例，然后设计一个学习算法，查看这些示例并学习每个类别的视觉外观。也就是说，首先积累一个带有标记图像的训练集，然后将其输入到计算机中，由计算机来处理这些数据。\n因此，可以按照下面的步骤来分解：\n目前较为流行的图像分类架构是卷积神经网络（CNN）——将图像送入网络，然后网络对图像数据进行分类。卷积神经网络从输入“扫描仪”开始，该输入“扫描仪”也不会一次性解析所有的训练数据。比如输入一个大小为 100*100 的图像，你也不需要一个有 10,000 个节点的网络层。相反，你只需要创建一个大小为 10 *10 的扫描输入层，扫描图像的前 10*10 个像素。然后，扫描仪向右移动一个像素，再扫描下一个 10 *10 的像素，这就是滑动窗口。\n输入数据被送入卷积层，而不是普通层。每个节点只需要处理离自己最近的邻近节点，卷积层也随着扫描的深入而趋于收缩。除了卷积层之外，通常还会有池化层。池化是过滤细节的一种方法，常见的池化技术是最大池化，它用大小为 2*2 的矩阵传递拥有最多特定属性的像素。\n现在，大部分图像分类技术都是在 ImageNet 数据集上训练的， ImageNet 数据集中包含了约 120 万张高分辨率训练图像。测试图像没有初始注释（即没有分割或标签），并且算法必须产生标签来指定图像中存在哪些对象。\n现存的很多计算机视觉算法，都是被来自牛津、 INRIA 和 XRCE 等顶级的计算机视觉团队在 ImageNet 数据集上实现的。通常来说，计算机视觉系统使用复杂的多级管道，并且，早期阶段的算法都是通过优化几个参数来手动微调的。\n第一届 ImageNet 竞赛的获奖者是 Alex Krizhevsky（NIPS 2012） ，他在 Yann LeCun 开创的神经网络类型基础上，设计了一个深度卷积神经网络。该网络架构除了一些最大池化层外，还包含 7 个隐藏层，前几层是卷积层，最后两层是全连接层。在每个隐藏层内，激活函数为线性的，要比逻辑单元的训练速度更快、性能更好。除此之外，当附近的单元有更强的活动时，它还使用竞争性标准化来压制隐藏活动，这有助于强度的变化。\n就硬件要求而言， Alex 在 2 个 Nvidia GTX 580 GPU （速度超过 1000 个快速的小内核）上实现了非常高效的卷积网络。 GPU 非常适合矩阵间的乘法且有非常高的内存带宽。这使他能在一周内完成训练，并在测试时快速的从 10 个块中组合出结果。如果我们能够以足够快的速度传输状态，就可以将网络分布在多个内核上。\n随着内核越来越便宜，数据集越来越大，大型神经网络的速度要比老式计算机视觉系统更快。在这之后，已经有很多种使用卷积神经网络作为核心，并取得优秀成果的模型，如 ZFNet（2013），GoogLeNet（2014）， VGGNet（2014）， RESNET（2015），DenseNet（2016）等。\n2、对象检测\n识别图像中的对象这一任务，通常会涉及到为各个对象输出边界框和标签。这不同于分类/定位任务——对很多对象进行分类和定位，而不仅仅是对个主体对象进行分类和定位。在对象检测中，你只有 2 个对象分类类别，即对象边界框和非对象边界框。例如，在汽车检测中，你必须使用边界框检测所给定图像中的所有汽车。\n如果使用图像分类和定位图像这样的滑动窗口技术，我们则需要将卷积神经网络应用于图像上的很多不同物体上。由于卷积神经网络会将图像中的每个物体识别为对象或背景，因此我们需要在大量的位置和规模上使用卷积神经网络，但是这需要很大的计算量！\n为了解决这一问题，神经网络研究人员建议使用区域（region）这一概念，这样我们就会找到可能包含对象的“斑点”图像区域，这样运行速度就会大大提高。第一种模型是基于区域的卷积神经网络（ R-CNN ），其算法原理如下：\n实质上，我们将对象检测转换为一个图像分类问题。但是也存在这些问题：训练速度慢，需要大量的磁盘空间，推理速度也很慢。\nR-CNN 的第一个升级版本是 Fast R-CNN，通过使用了 2 次增强，大大提了检测速度：\nFast R-CNN 的运行速度要比 R-CNN 快的多，因为在一幅图像上它只能训练一个 CNN 。 但是，择性搜索算法生成区域提议仍然要花费大量时间。\nFaster R-CNN 是基于深度学习对象检测的一个典型案例。\n该算法用一个快速神经网络代替了运算速度很慢的选择性搜索算法：通过插入区域提议网络（ RPN ），来预测来自特征的建议。 RPN 决定查看“哪里”，这样可以减少整个推理过程的计算量。\nRPN 快速且高效地扫描每一个位置，来评估在给定的区域内是否需要作进一步处理，其实现方式如下：通过输出 k 个边界框建议，每个边界框建议都有 2 个值——代表每个位置包含目标对象和不包含目标对象的概率。\n一旦我们有了区域建议，就直接将它们送入 Fast R-CNN 。 并且，我们还添加了一个池化层、一些全连接层、一个 softmax 分类层以及一个边界框回归器。\n总之，Faster R-CNN 的速度和准确度更高。值得注意的是，虽然以后的模型在提高检测速度方面做了很多工作，但很少有模型能够大幅度的超越 Faster R-CNN 。换句话说， Faster R-CNN 可能不是最简单或最快速的目标检测方法，但仍然是性能最好的方法之一。\n近年来，主要的目标检测算法已经转向更快、更高效的检测系统。这种趋势在 You Only Look Once（YOLO），Single Shot MultiBox Detector（SSD）和基于区域的全卷积网络（ R-FCN ）算法中尤为明显，这三种算法转向在整个图像上共享计算。因此，这三种算法和上述的3种造价较高的R-CNN 技术有所不同。\n3、目标跟踪\n目标跟踪，是指在特定场景跟踪某一个或多个特定感兴趣对象的过程。传统的应用就是视频和真实世界的交互，在检测到初始对象之后进行观察。现在，目标跟踪在无人驾驶领域也很重要，例如 Uber 和特斯拉等公司的无人驾驶。\n根据观察模型，目标跟踪算法可分成 2 类：生成算法和判别算法。\n为了通过检测实现跟踪，我们检测所有帧的候选对象，并使用深度学习从候选对象中识别想要的对象。有两种可以使用的基本网络模型：堆叠自动编码器（ SAE ）和卷积神经网络（ CNN ）。\n目前，最流行的使用 SAE 进行目标跟踪的网络是 Deep Learning Tracker（DLT），它使用了离线预训练和在线微调。其过程如下：\n鉴于 CNN 在图像分类和目标检测方面的优势，它已成为计算机视觉和视觉跟踪的主流深度模型。 一般来说，大规模的卷积神经网络既可以作为分类器和跟踪器来训练。具有代表性的基于卷积神经网络的跟踪算法有全卷积网络跟踪器（ FCNT ）和多域卷积神经网络（ MD Net ）。\nFCNT 充分分析并利用了 VGG 模型中的特征映射，这是一种预先训练好的 ImageNet 数据集，并有如下效果：\n因此， FCNT 设计了特征选择网络，在 VGG 网络的卷积 4-3 和卷积 5-3 层上选择最相关的特征映射。 然后为避免噪音的过拟合， FCNT 还为这两个层的选择特征映射单独设计了两个额外的通道（即 SNet 和 GNet ）： GNet 捕获对象的类别信息； SNet 将该对象从具有相似外观的背景中区分出来。\n这两个网络的运作流程如下：都使用第一帧中给定的边界框进行初始化，以获取对象的映射。而对于新的帧，对其进行剪切并传输最后一帧中的感兴趣区域，该感兴趣区域是以目标对象为中心。最后，通过 SNet 和 GNet ，分类器得到两个预测热映射，而跟踪器根据是否存在干扰信息，来决定使用哪张热映射生成的跟踪结果。 FCNT 的图如下所示。\n与 FCNT 的思路不同， MD Net 使用视频的所有序列来跟踪对象的移动。上述网络使用不相关的图像数据来减少跟踪数据的训练需求，并且这种想法与跟踪有一些偏差。该视频中的一个类的对象可以是另一个视频中的背景，因此， MD Net 提出了“多域”这一概念，它能够在每个域中独立的区分对象和背景，而一个域表示一组包含相同类型对象的视频。\n如下图所示， MD Net 可分为两个部分，即 K 个特定目标分支层和共享层：每个分支包含一个具有 softmax 损失的二进制分类层，用于区分每个域中的对象和背景；共享层与所有域共享，以保证通用表示。\n近年来，深度学习研究人员尝试使用了不同的方法来适应视觉跟踪任务的特征，并且已经探索了很多方法：\n4、语义分割\n计算机视觉的核心是分割，它将整个图像分成一个个像素组，然后对其进行标记和分类。特别地，语义分割试图在语义上理解图像中每个像素的角色（比如，识别它是汽车、摩托车还是其他的类别）。如上图所示，除了识别人、道路、汽车、树木等之外，我们还必须确定每个物体的边界。因此，与分类不同，我们需要用模型对密集的像素进行预测。\n与其他计算机视觉任务一样，卷积神经网络在分割任务上取得了巨大成功。最流行的原始方法之一是通过滑动窗口进行块分类，利用每个像素周围的图像块，对每个像素分别进行分类。但是其计算效率非常低，因为我们不能在重叠块之间重用共享特征。\n解决方案就是加州大学伯克利分校提出的全卷积网络（ FCN ），它提出了端到端的卷积神经网络体系结构，在没有任何全连接层的情况下进行密集预测。\n这种方法允许针对任何尺寸的图像生成分割映射，并且比块分类算法快得多，几乎后续所有的语义分割算法都采用了这种范式。\n但是，这也仍然存在一个问题：在原始图像分辨率上进行卷积运算非常昂贵。为了解决这个问题， FCN 在网络内部使用了下采样和上采样：下采样层被称为条纹卷积（ striped convolution ）；而上采样层被称为反卷积（ transposed convolution ）。\n尽管采用了上采样和下采样层，但由于池化期间的信息丢失， FCN 会生成比较粗糙的分割映射。SegNet 是一种比 FCN （使用最大池化和编码解码框架）更高效的内存架构。在 SegNet 解码技术中，从更高分辨率的特征映射中引入了 shortcut/skip connections ，以改善上采样和下采样后的粗糙分割映射。\n目前的语义分割研究都依赖于完全卷积网络，如空洞卷积 ( Dilated Convolutions ），DeepLab 和 RefineNet 。\n5、实例分割\n除了语义分割之外，实例分割将不同类型的实例进行分类，比如用 5 种不同颜色来标记 5 辆汽车。分类任务通常来说就是识别出包含单个对象的图像是什么，但在分割实例时，我们需要执行更复杂的任务。我们会看到多个重叠物体和不同背景的复杂景象，我们不仅需要将这些不同的对象进行分类，而且还要确定对象的边界、差异和彼此之间的关系！\n到目前为止，我们已经看到了如何以多种有趣的方式使用卷积神经网络的特征，通过边界框有效定位图像中的不同对象。我们可以将这种技术进行扩展吗？也就是说，对每个对象的精确像素进行定位，而不仅仅是用边界框进行定位？ Facebook AI 则使用了 Mask R-CNN 架构对实例分割问题进行了探索。\n就像 Fast R-CNN 和 Faster R-CNN 一样， Mask R-CNN 的底层是鉴于 Faster R-CNN 在物体检测方面效果很好，我们是否可以将其扩展到像素级分割？\nMask R-CNN 通过向 Faster R-CNN 添加一个分支来进行像素级分割，该分支输出一个二进制掩码，该掩码表示给定像素是否为目标对象的一部分：该分支是基于卷积神经网络特征映射的全卷积网络。将给定的卷积神经网络特征映射作为输入，输出为一个矩阵，其中像素属于该对象的所有位置用 1 表示，其他位置则用 0 表示，这就是二进制掩码。\n另外，当在原始 Faster R-CNN 架构上运行且没有做任何修改时，感兴趣池化区域（ RoIPool ） 选择的特征映射区域或原始图像的区域稍微错开。由于图像分割具有像素级特性，这与边界框不同，自然会导致结果不准确。 Mas R-CNN 通过调整 RoIPool 来解决这个问题，使用感兴趣区域对齐（ Roialign ）方法使其变的更精确。本质上， RoIlign 使用双线性插值来避免舍入误差，这会导致检测和分割不准确。\n一旦生成这些掩码， Mask R-CNN 将 RoIAlign 与来自 Faster R-CNN 的分类和边界框相结合，以便进行精确的分割：\n结语\n上述这 5 种主要的计算机视觉技术可以协助计算机从单个或一系列图像中提取、分析和理解有用的信息。通过 GitHub 存储库（https://github.com/khanhnamle1994/computer-vision）获取所有的演讲幻灯片以及指南。\n原文链接 https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b\n关键词：计算机视觉、图像分类、对象检测、目标跟踪、语义分割、实例分割\n发表于: 2018-10-242018-10-24 13:42:12\n原文链接：https://kuaibao.qq.com/s/20181024A0WV8C00?refer=cp_1026"}
{"content2":"人工智能需要设计的学科：\n1.语言学 （语音识别）\n2.脑学（神经系统学）神经网络模型建模\n3.数学，包含 统计学（随机过程），线性数学（线性理论），离散数学\n4.心理学，人机交互，人的交互心理，情感心理\n5.哲学，可控理论，有机可造性，自我无限优化、扩张、理论\n人工智能涉及到的技术：\n语音识别\n图像识别\n模式（手势）识别，计算机视觉技术\n机器学习（数学模型，算法和优化参数）\n深度学习"}
{"content2":"第二届人工智能竞赛——题目八、语音识别器\n文章目录\n第二届人工智能竞赛——题目八、语音识别器\n一、简介\n二、题目要求\n三、参考资料\n一、简介\n语音识别技术主要包括特征提取技术、模式匹配准则及模型训练技术三个方面。与机器进行语音交流，让机器明白你说什么，这是人们长期以来梦寐以求的事情。语音识别技术就是让机器通过识别和理解过程把语音信号转变为相应的文本或命令的高技术\n二、题目要求\n用TensorFlow制作一个语音识别器。\n利用 QT 设计 UI 界面，界面要求:友好、功能完善、简介。\n三、参考资料\nhttps://github.com/llSourcell/tensorflow_speech_recognition_demo\nhttp://weibo.com/p/2304444948c16e85c39a07240b193cd5509574\nUI界面可用QT编写"}
{"content2":"来源：http://www.leiphone.com/news/201605/zZqsZiVpcBBPqcGG.html#rd\n人工智能是人类一个非常美好的梦想，跟星际漫游和长生不老一样。我们想制造出一种机器，使得它跟人一样具有一定的对外界事物感知能力，比如看见世界。\n在上世纪50年代，数学家图灵提出判断机器是否具有人工智能的标准：图灵测试。即把机器放在一个房间，人类测试员在另一个房间，人跟机器聊天，测试员事先不知道另一房间里是人还是机器 。经过聊天，如果测试员不能确定跟他聊天的是人还是机器的话，那么图灵测试就通过了，也就是说这个机器具有与人一样的感知能力。\n但是从图灵测试提出来开始到本世纪初，50多年时间有无数科学家提出很多机器学习的算法，试图让计算机具有与人一样的智力水平，但直到2006年深度学习算法的成功，才带来了一丝解决的希望。\n众星捧月的深度学习\n深度学习在很多学术领域，比非深度学习算法往往有20-30%成绩的提高。很多大公司也逐渐开始出手投资这种算法，并成立自己的深度学习团队，其中投入最大的就是谷歌，2008年6月披露了谷歌脑项目。2014年1月谷歌收购DeepMind，然后2016年3月其开发的Alphago算法在围棋挑战赛中，战胜了韩国九段棋手李世石，证明深度学习设计出的算法可以战胜这个世界上最强的选手。\n在硬件方面，Nvidia最开始做显示芯片，但从2006及2007年开始主推用GPU芯片进行通用计算，它特别适合深度学习中大量简单重复的计算量。目前很多人选择Nvidia的CUDA工具包进行深度学习软件的开发。\n微软从2012年开始，利用深度学习进行机器翻译和中文语音合成工作，其人工智能小娜背后就是一套自然语言处理和语音识别的数据算法。\n百度在2013年宣布成立百度研究院，其中最重要的就是百度深度学习研究所，当时招募了著名科学家余凯博士。不过后来余凯离开百度，创立了另一家从事深度学习算法开发的公司地平线。\nFacebook和Twitter也都各自进行了深度学习研究，其中前者携手纽约大学教授Yann Lecun，建立了自己的深度学习算法实验室；2015年10月，Facebook宣布开源其深度学习算法框架，即Torch框架。Twitter在2014年7月收购了Madbits，为用户提供高精度的图像检索服务。\n前深度学习时代的计算机视觉\n互联网巨头看重深度学习当然不是为了学术，主要是它能带来巨大的市场。那为什么在深度学习出来之前，传统算法为什么没有达到深度学习的精度？\n在深度学习算法出来之前，对于视觉算法来说，大致可以分为以下5个步骤：特征感知，图像预处理，特征提取，特征筛选，推理预测与识别。早期的机器学习中，占优势的统计机器学习群体中，对特征是不大关心的。\n我认为，计算机视觉可以说是机器学习在视觉领域的应用，所以计算机视觉在采用这些机器学习方法的时候，不得不自己设计前面4个部分。\n但对任何人来说这都是一个比较难的任务。传统的计算机识别方法把特征提取和分类器设计分开来做，然后在应用时再合在一起，比如如果输入是一个摩托车图像的话，首先要有一个特征表达或者特征提取的过程，然后把表达出来的特征放到学习算法中进行分类的学习。\n过去20年中出现了不少优秀的特征算子，比如最著名的SIFT算子，即所谓的对尺度旋转保持不变的算子。它被广泛地应用在图像比对，特别是所谓的structure from motion这些应用中，有一些成功的应用例子。另一个是HoG算子，它可以提取物体，比较鲁棒的物体边缘，在物体检测中扮演着重要的角色。\n这些算子还包括Textons，Spin image，RIFT和GLOH，都是在深度学习诞生之前或者深度学习真正的流行起来之前，占领视觉算法的主流。\n几个（半）成功例子\n这些特征和一些特定的分类器组合取得了一些成功或半成功的例子，基本达到了商业化的要求但还没有完全商业化。\n一是八九十年代的指纹识别算法，它已经非常成熟，一般是在指纹的图案上面去寻找一些关键点，寻找具有特殊几何特征的点，然后把两个指纹的关键点进行比对，判断是否匹配。\n然后是2001年基于Haar的人脸检测算法，在当时的硬件条件下已经能够达到实时人脸检测，我们现在所有手机相机里的人脸检测，都是基于它或者它的变种。\n第三个是基于HoG特征的物体检测，它和所对应的SVM分类器组合起来的就是著名的DPM算法。DPM算法在物体检测上超过了所有的算法，取得了比较不错的成绩。\n但这种成功例子太少了，因为手工设计特征需要大量的经验，需要你对这个领域和数据特别了解，然后设计出来特征还需要大量的调试工作。说白了就是需要一点运气。\n另一个难点在于，你不只需要手工设计特征，还要在此基础上有一个比较合适的分类器算法。同时设计特征然后选择一个分类器，这两者合并达到最优的效果，几乎是不可能完成的任务。\n仿生学角度看深度学习\n如果不手动设计特征，不挑选分类器，有没有别的方案呢？能不能同时学习特征和分类器？即输入某一个模型的时候，输入只是图片，输出就是它自己的标签。比如输入一个明星的头像，出来的标签就是一个50维的向量（如果要在50个人里识别的话），其中对应明星的向量是1，其他的位置是0。\n这种设定符合人类脑科学的研究成果。\n1981年诺贝尔医学生理学奖颁发给了David Hubel，一位神经生物学家。他的主要研究成果是发现了视觉系统信息处理机制，证明大脑的可视皮层是分级的。他的贡献主要有两个，一是他认为人的视觉功能一个是抽象，一个是迭代。抽象就是把非常具体的形象的元素，即原始的光线像素等信息，抽象出来形成有意义的概念。这些有意义的概念又会往上迭代，变成更加抽象，人可以感知到的抽象概念。\n像素是没有抽象意义的，但人脑可以把这些像素连接成边缘，边缘相对像素来说就变成了比较抽象的概念；边缘进而形成球形，球形然后到气球，又是一个抽象的过程，大脑最终就知道看到的是一个气球。\n模拟人脑识别人脸，也是抽象迭代的过程，从最开始的像素到第二层的边缘，再到人脸的部分，然后到整张人脸，是一个抽象迭代的过程。\n再比如看到图片中的摩托车，我们可能在脑子里就几微秒的时间，但是经过了大量的神经元抽象迭代。对计算机来说最开始看到的根本也不是摩托车，而是RGB图像三个通道上不同的数字。\n所谓的特征或者视觉特征，就是把这些数值给综合起来用统计或非统计的形式，把摩托车的部件或者整辆摩托车表现出来。深度学习的流行之前，大部分的设计图像特征就是基于此，即把一个区域内的像素级别的信息综合表现出来，利于后面的分类学习。\n如果要完全模拟人脑，我们也要模拟抽象和递归迭代的过程，把信息从最细琐的像素级别，抽象到“种类”的概念，让人能够接受。\n卷积的概念\n计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。\n什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。实际上在计算机视觉里面，可以把卷积当做一个抽象的过程，就是把小区域内的信息统计抽象出来。\n比如，对于一张爱因斯坦的照片，我可以学习n个不同的卷积和函数，然后对这个区域进行统计。可以用不同的方法统计，比如着重统计中央，也可以着重统计周围，这就导致统计的和函数的种类多种多样，为了达到可以同时学习多个统计的累积和。\n上图中是，如何从输入图像怎么到最后的卷积，生成的响应map。首先用学习好的卷积和对图像进行扫描，然后每一个卷积和会生成一个扫描的响应图，我们叫response map，或者叫feature map。如果有多个卷积和，就有多个feature map。也就说从一个最开始的输入图像（RGB三个通道）可以得到256个通道的feature map，因为有256个卷积和，每个卷积和代表一种统计抽象的方式。\n在卷积神经网络中，除了卷积层，还有一种叫池化的操作。池化操作在统计上的概念更明确，就是一个对一个小区域内求平均值或者求最大值的统计操作。\n带来的结果是，如果之前我输入有两个通道的，或者256通道的卷积的响应feature map，每一个feature map都经过一个求最大的一个池化层，会得到一个比原来feature map更小的256的feature map。\n在上面这个例子里，池化层对每一个2X2的区域求最大值，然后把最大值赋给生成的feature map的对应位置。如果输入图像是100×100的话，那输出图像就会变成50×50，feature map变成了一半。同时保留的信息是原来2X2区域里面最大的信息。\n操作的实例：LeNet网络\nLe顾名思义就是指人工智能领域的大牛Lecun。这个网络是深度学习网络的最初原型，因为之前的网络都比较浅，它较深的。LeNet在98年就发明出来了，当时Lecun在AT&T的实验室，他用这一网络进行字母识别，达到了非常好的效果。\n怎么构成呢？输入图像是32×32的灰度图，第一层经过了一组卷积和，生成了6个28X28的feature map，然后经过一个池化层，得到得到6个14X14的feature map，然后再经过一个卷积层，生成了16个10X10的卷积层，再经过池化层生成16个5×5的feature map。\n从最后16个5X5的feature map开始，经过了3个全连接层，达到最后的输出，输出就是标签空间的输出。由于设计的是只要对0到9进行识别，所以输出空间是10，如果要对10个数字再加上26个大小字母进行识别的话，输出空间就是62。62维向量里，如果某一个维度上的值最大，它对应的那个字母和数字就是就是预测结果。\n压在骆驼身上的最后一根稻草\n从98年到本世纪初，深度学习兴盛起来用了15年，但当时成果泛善可陈，一度被边缘化。到2012年，深度学习算法在部分领域取得不错的成绩，而压在骆驼身上最后一根稻草就是AlexNet。\nAlexNet由多伦多大学几个科学家开发，在ImageNet比赛上做到了非常好的效果。当时AlexNet识别效果超过了所有浅层的方法。此后，大家认识到深度学习的时代终于来了，并有人用它做其它的应用，同时也有些人开始开发新的网络结构。\n其实AlexNet的结构也很简单，只是LeNet的放大版。输入是一个224X224的图片，是经过了若干个卷积层，若干个池化层，最后连接了两个全连接层，达到了最后的标签空间。\n去年，有些人研究出来怎么样可视化深度学习出来的特征。那么，AlexNet学习出的特征是什么样子？在第一层，都是一些填充的块状物和边界等特征；中间的层开始学习一些纹理特征；更高接近分类器的层级，则可以明显看到的物体形状的特征。\n最后的一层，即分类层，完全是物体的不同的姿态，根据不同的物体展现出不同姿态的特征了。\n可以说，不论是对人脸，车辆，大象或椅子进行识别，最开始学到的东西都是边缘，继而就是物体的部分，然后在更高层层级才能抽象到物体的整体。整个卷积神经网络在模拟人的抽象和迭代的过程。\n为什么时隔20年卷土重来？\n我们不禁要问：似乎卷积神经网络设计也不是很复杂，98年就已经有一个比较像样的雏形了。自由换算法和理论证明也没有太多进展。那为什么时隔20年，卷积神经网络才能卷土重来，占领主流？\n这一问题与卷积神经网络本身的技术关系不太大，我个人认为与其他一些客观因素有关。\n首先，卷积神经网络的深度太浅的话，识别能力往往不如一般的浅层模型，比如SVM或者boosting。但如果做得很深，就需要大量数据进行训练，否则机器学习中的过拟合将不可避免。而2006及2007年开始，正好是互联网开始大量产生各种各样的图片数据的时候。\n另外一个条件是运算能力。卷积神经网络对计算机的运算要求比较高，需要大量重复可并行化的计算，在当时CPU只有单核且运算能力比较低的情况下，不可能进行个很深的卷积神经网络的训练。随着GPU计算能力的增长，卷积神经网络结合大数据的训练才成为可能。\n最后一点就是人和。卷积神经网络有一批一直在坚持的科学家（如Lecun）才没有被沉默，才没有被海量的浅层方法淹没。然后最后终于看到卷积神经网络占领主流的曙光。\n深度学习在视觉上的应用\n计算机视觉中比较成功的深度学习的应用，包括人脸识别，图像问答，物体检测，物体跟踪。\n人脸识别\n这里说人脸识别中的人脸比对，即得到一张人脸，与数据库里的人脸进行比对；或同时给两张人脸，判断是不是同一个人。\n这方面比较超前的是汤晓鸥教授，他们提出的DeepID算法在LWF上做得比较好。他们也是用卷积神经网络，但在做比对时，两张人脸分别提取了不同位置特征，然后再进行互相比对，得到最后的比对结果。最新的DeepID-3算法，在LWF达到了99.53%准确度，与肉眼识别结果相差无几。\n图片问答问题\n这是2014年左右兴起的课题，即给张图片同时问个问题，然后让计算机回答。比如有一个办公室靠海的图片，然后问“桌子后面有什么”，神经网络输出应该是“椅子和窗户”。\n这一应用引入了LSTM网络，这是一个专门设计出来具有一定记忆能力的神经单元。特点是，会把某一个时刻的输出当作下一个时刻的输入。可以认为它比较适合语言等，有时间序列关系的场景。因为我们在读一篇文章和句子的时候，对句子后面的理解是基于前面对词语的记忆。\n图像问答问题是基于卷积神经网络和LSTM单元的结合，来实现图像问答。LSTM输出就应该是想要的答案，而输入的就是上一个时刻的输入，以及图像的特征，及问句的每个词语。\n物体检测问题\nRegion CNN\n深度学习在物体检测方面也取得了非常好的成果。2014年的Region CNN算法，基本思想是首先用一个非深度的方法，在图像中提取可能是物体的图形块，然后深度学习算法根据这些图像块，判断属性和一个具体物体的位置。\n为什么要用非深度的方法先提取可能的图像块？因为在做物体检测的时候，如果你用扫描窗的方法进行物体监测，要考虑到扫描窗大小的不一样，长宽比和位置不一样，如果每一个图像块都要过一遍深度网络的话，这种时间是你无法接受的。\n所以用了一个折中的方法，叫Selective Search。先把完全不可能是物体的图像块去除，只剩2000左右的图像块放到深度网络里面判断。那么取得的成绩是AP是58.5，比以往几乎翻了一倍。有一点不尽如人意的是，region CNN的速度非常慢，需要10到45秒处理一张图片。\nFaster R-CNN方法\n而且我在去年NIPS上，我们看到的有Faster R-CNN方法，一个超级加速版R-CNN方法。它的速度达到了每秒七帧，即一秒钟可以处理七张图片。技巧在于，不是用图像块来判断是物体还是背景，而把整张图像一起扔进深度网络里，让深度网络自行判断哪里有物体，物体的方块在哪里，种类是什么？\n经过深度网络运算的次数从原来的2000次降到一次，速度大大提高了。\nFaster R-CNN提出了让深度学习自己生成可能的物体块，再用同样深度网络来判断物体块是否是背景？同时进行分类，还要把边界和给估计出来。\nFaster R-CNN可以做到又快又好，在VOC2007上检测AP达到73.2，速度也提高了两三百倍。\nYOLO\n去年FACEBOOK提出来的YOLO网络，也是进行物体检测，最快达到每秒钟155帧，达到了完全实时。它让一整张图像进入到神经网络，让神经网络自己判断这物体可能在哪里，可能是什么。但它缩减了可能图像块的个数，从原来Faster R-CNN的2000多个缩减缩减到了98个。\n同时取消了Faster R-CNN里面的RPN结构，代替Selective Search结构。YOLO里面没有RPN这一步，而是直接预测物体的种类和位置。\nYOLO的代价就是精度下降，在155帧的速度下精度只有52.7，45帧每秒时的精度是63.4。\nSSD\n在arXiv上出现的最新算法叫Single Shot MultiBox Detector，即SSD。\n它是YOLO的超级改进版，吸取了YOLO的精度下降的教训，同时保留速度快的特点。它能达到58帧每秒，精度有72.1。速度超过Faster R-CNN 有8倍，但达到类似的精度。\n物体跟踪\n所谓跟踪，就是在视频里面第一帧时锁定感兴趣的物体，让计算机跟着走，不管怎么旋转晃动，甚至躲在树丛后面也要跟踪。\n深度学习对跟踪问题有很显著的效果。DeepTrack算法是我在澳大利亚信息科技研究院时和同事提出的，是第一在线用深度学习进行跟踪的文章，当时超过了其它所有的浅层算法。\n今年有越来越多深度学习跟踪算法提出。去年十二月ICCV 2015上面，马超提出的Hierarchical Convolutional Feature算法，在数据上达到最新的记录。它不是在线更新一个深度学习网络，而是用一个大网络进行预训练，然后让大网络知道什么是物体什么不是物体。\n将大网络放在跟踪视频上面，然后再分析网络在视频上产生的不同特征，用比较成熟的浅层跟踪算法来进行跟踪，这样利用了深度学习特征学习比较好的好处，同时又利用了浅层方法速度较快的优点。效果是每秒钟10帧，同时精度破了记录。\n最新的跟踪成果是基于Hierarchical Convolutional Feature，由一个韩国的科研组提出的MDnet。它集合了前面两种深度算法的集大成，首先离线的时候有学习，学习的不是一般的物体检测，也不是ImageNet，学习的是跟踪视频，然后在学习视频结束后，在真正在使用网络的时候更新网络的一部分。这样既在离线的时候得到了大量的训练，在线的时候又能够很灵活改变自己的网络。\n基于嵌入式系统的深度学习\n回到ADAS问题（慧眼科技的主业），它完全可以用深度学习算法，但对硬件平台有比较高的要求。在汽车上不太可能把一台电脑放上去，因为功率是个问题，很难被市场所接受。\n现在的深度学习计算主要是在云端进行，前端拍摄照片，传给后端的云平台处理。但对于ADAS而言，无法接受长时间的数据传输的，或许发生事故后，云端的数据还没传回来。\n那是否可以考虑NVIDIA推出的嵌入式平台？NVIDIA推出的嵌入式平台，其运算能力远远强过了所有主流的嵌入式平台，运算能力接近主流的顶级CPU，如台式机的i7。那么慧眼科技在做工作就是要使得深度学习算法，在嵌入式平台有限的资源情况下能够达到实时效果，而且精度几乎没有减少。\n具体做法是，首先对网络进行缩减，可能是对网络的结构缩减，由于识别场景不同，也要进行相应的功能性缩减；另外要用最快的深度检测算法，结合最快的深度跟踪算法，同时自己研发出一些场景分析算法。三者结合在一起，目的是减少运算量，减少检测空间的大小。在这种情况下，在有限资源上实现了使用深度学习算法，但精度减少的非常少。"}
{"content2":"计算机视觉是人工智能（AI）中的热门研究课题，它已经存在多年。然而，计算机视觉仍然是人工智能面临的最大挑战之一。在本文中，我们将探讨使用深度神经网络来解决计算机视觉的一些基本挑战。特别是，我们将研究神经网络压缩，细粒度图像分类，纹理合成，图像搜索和对象跟踪等应用。\n1、神经网络压缩\n尽管深度神经网络具有令人难以置信的性能，但它们对计算能力和存储的需求对其在实际应用中的部署提出了重大挑战。研究表明，神经网络中使用的参数可能非常多余。因此，在提高精度的同时还需要投入大量的工作来降低了网络的复杂性。\n低秩近似用于接近原始权重矩阵。例如，SVD可用于获得矩阵的最佳低秩近似，或者Toeplitz矩阵可与Krylov分析结合使用以近似的原始矩阵。\n1.1：修剪\n一旦训练完成，一些不相关的神经元连接（可以在损失算法中加权值平衡和稀疏约束）或者将所有这些连接过滤掉，然后执行几轮微调。在实际应用中，修剪神经元连接的级别将使结果稀疏，难以缓存，并且难以从存储器访问。有时，我们需要特别设计一个合作运营数据库。\n相比之下，过滤级修剪可以直接在已经存在的操作数据库上运行，过滤级修剪的关键是确定如何平衡过滤器的重要性。例如，我们可以使用卷积结果的稀疏性、滤波器对损失算法的影响或者卷积对下一层结果的影响进行平衡。\n1.2：量化\n我们可以将权重值分成组，然后使用组中的中值来替换原始权重，并通过霍夫曼编码运行它。但是，如果我们只考虑权重本身，则可以减少量化过程的误差偏差。随后，分类操作的误差偏差将显着增加。因此，量化CNN的优化目标是重构以最小化误差偏差。此外，我们可以使用哈希编码并投影相同的哈希权重（hashbucket weights）来共享相同的值。\n1.3：减少数据值的范围\n在默认情况下，数据由单精度浮点组成，占32位。研究人员发现，使用半精度浮点（16位）对性能的影响几乎为零。谷歌的TPU使用8位整数来表示数据，这种情况是值的范围是两个或三个值（0/1或-1/0/1）。仅使用位进行操作可以使我们快速完成各种计算，但是训练两个或三个价值网络是一个至关重要的问题。\n传统方法是使用两个或三个值作为前馈过程并在更新过程中传递实数。此外，研究人员认为两个值的表达能力是有限的，因此可以使用额外的浮点缩放二进制卷积结果来改善网络表示。\n1.4：简化的结构设计\n研究人员一直致力于创建简化的网络结构，例如：\n1. 1x1卷积：这种设计理念已经在Inception和ResNet系列网络设计中得到了广泛应用；\n2. 分组卷积；\n3. 扩展卷积：只要值不变，使用扩展卷积就可以扩展感知域。\n知识蒸馏（Knowledge distillation）训练小网络接近广泛的网络。但是，目前还不清楚如何正确地接近庞大的网络。\n1.5：硬件-软件协议设计\n常用硬件：\n1. 常见硬件，如CPU（低延迟，复杂操作）和GPU（高吞吐量，适合并发，简单过程）；\n2. 专用硬件，包括ASIC（专用集成电路，例如Google的TPU）和FPGA（现场可编程门阵列，灵活但效率较低）。\n2、细粒度图像分类\n与（普通）图像分类相比，细粒度图像分类在确定图像类别时需要更高的精度。例如，我们可能需要确定目标鸟的确切种类、汽车的品牌和型号、飞机的型号。通常，这些类之间的差异很小。例如，波音737-300和波音737-400之间唯一明显不同的区别就是窗户的数量。因此，细粒度图像分类比标准图像分类更具挑战性。\n细粒度图像分类的经典方法是首先在图像上定义不同的位置，例如，鸟的头部、脚部或翅膀。然后我们必须从这些位置提取特征，最后，组合这些特征并使用它们来完成分类。这种方法具有非常高的准确性，但它需要大量的数据集和手动标记位置信息。细粒度分类的一个主要趋势是没有额外监督信息的训练，而不是仅使用图像笔记，该方法由双线性CNN方法表示。\n2.1：双线性（Bilinear）CNN\n首先计算卷积描述符的外积，以找出不同维度之间的相互关系。因为不同描述符的维度对应于卷积特征的不同通道，并且不同的通道提取不同的语义特征，所以使用双线性操作允许我们捕获输入图像上的不同语义元素之间的关系。\n2.2:流线型双线性汇合（Streamlined Bilinear Confluence）\n双线性汇合的结果是非常高维的，这需要大量的计算和存储资源，也明显增加了下一个完全连接层上的参数数量。后续研究旨在制定简化双线性汇合的战略，其结果包括以下内容：\n1. PCA维数减少：在双线性汇合出现之前，我们会在深度描述符上使用PCA投影维数减少，但这会影响影响性能的每个维度。一个折中的方案是仅将PCA降维应用于一条线。\n2. 近似核估计：证明在双线性收敛之后使用线性SVM分类与在描述符上使用多项式核一样有价值。因为两个描述符的向外投影等于两个独立描述符的卷积投影，所以一些研究集中于使用随机矩阵来近似描述符投影。此外，通过近似核估计，我们可以捕获超过二阶信息（见下图）。\n3. 低秩近似：使用来自全连接层的参数矩阵进行低秩近似使得不必明确地计算双线性汇合的结果。\n3、图像描述\n图像描述是生成图像的一个或两个句子描述的过程。这是一项涉及计算机视觉和自然语言处理的跨学科任务。\n3.1：编码器-解码器网络\n设计图像字幕网络背后的基本思想基于自然语言处理领域中机器翻译的概念。在具有图像CNN编码网络的机器翻译器中替换源语言编码网络并提取图像的特征之后，我们可以使用解码器网络作为目标语言来创建文本描述。\n3.2:ShowAttend and Tell\n注意力机制是机器翻译器用来捕获远程依赖关系的标准技术，也可以用于图像字幕。在解码器网络中，除了预测下一个单词之外，在每个时刻，我们还需要输出二维注意力图像并将其用于深度卷积特征的加权收敛。使用注意力机制的另一个好处是网络可以被可视化，这样我们就可以轻松地看到网络在生成每个单词时所看到的图像部分。\n3.3:AdaptiveAttention（自适应注意力机制）\n先前的注意力机制将为每个预测的单词产生二维注意图像（图像（a））。但是，对于一些停止词，我们不需要使用图像中的线索。相反，某些单词可以根据上下文生成，完全独立于图像本身。这项工作在LSTM上进行了扩展，并产生了“视觉哨兵”机制，该机制确定是否应根据上下文或图像信息（图像（b））预测当前单词。\n此外，与先前在根据隐藏层的状态计算注意图像的先前方法不同，该方法根据隐藏层的当前状态执行计算。\n4、视觉问答\n给定图像和与该图像相关的问题，视觉问答旨在从选择的候选答案中回答该问题。从本质上讲，这是一个分类任务，有时它使用递归神经网络解码来产生文本答案。视觉问答也是一项涉及视觉和自然语言处理的跨学科任务。\n4.1:基本思想过程\n问题概念是使用CNN从图像中提取特征，RNN从文本问题中提取文本特征，然后组合视觉和文本特征，最后使用完全连接后进行分类。这项任务的关键是弄清楚如何连接这两种类型的功能。直接组合这些特征的方法将它们转换为矢量，或者通过添加或乘以元素来添加或生成视觉和文本矢量。\n注意使用注意力机制的图像字幕系统可提高视觉问答的性能。注意力机制包括视觉注意（“我在哪里看”）和文本注意力（“我在看哪个词？”）HieCoAtten可以同时或依次创建视觉和文本注意力。DAN在同一空间内投射视觉和文本注意力的结果; 然后它同时产生视觉和文本注意力的下一步。\n4.2:双线性整合\n该方法使用视觉特征向量和文本特征向量的外积来捕获每个维度上这些状态的特征之间的关系。为了避免明确地计算双线性汇合的高维度结果，我们可以将在细粒度识别中发现的流线型双线性汇合背后的思想应用于视觉问题回答。例如，MFB使用低速率近似背后的概念以及视觉和文本注意机制。\n5、神经网络可视化与神经网络理解\n下文提供了许多可视化方法，以帮助理解卷积和神经网络。\n5.1:直接可视化第一个过滤器\n由于第一个卷积层上的滤镜在输入图像上滑动，我们可以直接在第一层上显示滤镜。我们可以看到第一层权重集中在特定方向的边缘和指定的颜色组合，这类似于视觉生物学机制。但是，由于高级过滤器不直接用于输入图像，因此直接可视化只能应用于第一层上的过滤器。\n5.2:T-SNE\n此方法在图像的fc7和pool5特征上使用低维嵌入。例如，将它们缩小到2维并允许它们在2维平面上绘制，具有类似语义信息的图像应该产生与t-SNE类似的结果。该方法与PCA之间的区别在于t-SNE是一种非线性减少方法，可以保留地点之间的距离。我们得到以下图像，该图像是将t-SNE应用于原始MNIST图像的结果。我们可以看到MNIST是一个相对简单的数据集，其中不同分类的图像之间的差异是显而易见的。\n5.3:可视化中间层激活值\n我们可以看到，即使ImageNet没有人脸类别，网络仍将学会区分这种语义信息并捕获未来的分类。\n5.4:最大化响应图像区域\n为了阻止这种情况，在中间层选择一个指定的神经元，然后将多个不同的图像输入网络，以找到导致神经元最大响应的图像区域。这允许我们观察神经元对应的语义特征，我们使用“图像区域”而不是“完整图像”的原因是中间层神经元的感受野受限并且不能覆盖整个图像。\n5.5:梯度上升优化\n该方法选择特定的神经元，然后计算该神经元对输入图像的反应产生的偏导数，然后使用梯度上升优化图像直到收敛。此外，我们需要一些标准化的项目来使生成的图像更接近自然模型。除了优化输入图像，我们还可以优化fc6功能并创建所需的图像。\n6、对抗性的例子\n选择图像和不正确的分类。然后，系统计算该分类对图像的偏导数，然后对图像应用梯度上升优化。实验表明，在使用小的，几乎察觉不到的变化之后，我们可以使网络以高可信度对模型实现不正确的类。\n在实际应用中，对抗性示例在金融和安全领域非常有用。研究人员发现，这是因为图像空间的维度非常高。即使有大量的训练数据，我们也只能覆盖该空间的一小部分。如果输入图像从该不同空间稍微变化，那么网络将难以做出合理的决定。\n7、纹理生成（Texture Synthesis）与风格迁移（Style Transform）\n纹理生成用于生成包含相同纹理的较大图像。给定正常图像和包含特定风格的图像，然后通过风格迁移不仅保留图像的原始内容，而且将该图像转换为指定的风格。\n7.1:特征反演（Feature Inversion）\n特征反演是纹理生成和风格迁移背后的核心概念。给定一个中间层特征，我们希望迭代来创建与给定特征类似图像。特征反演还可以告诉我们在中间层特征中包含多少图像信息。\n给定DxHxW的深度卷积特征，我们将它们转换为Dx（HW）矩阵X，因此我们可以将对应Gram矩阵定义为:G = XX ^ T\n通过外积，Gram矩阵捕获不同特征之间的关系。\n7.2:纹理生成的概念\n它对给定纹理图案的Gram矩阵进行特征逆向工程。使生成图像的各层特征的Gram矩阵接近给定纹理图像的各层Gram。低层特征倾向于捕获细节信息，而高层特征可以捕获更大面积的特征。\n7.3:风格迁移的概念\n此优化有两个主要目标：第一个是使生成的图像的内容更接近原始图像的内容，而第二个是使生成的图像的风格与指定的风格匹配。风格由Gram矩阵体现，而内容直接由神经元的激活值体现。\n7.4:直接生成风格迁移的图像\n上面直接生成风格迁移的图像的方法的缺点是需要多次迭代才能收敛。解决该问题的方案是训练一个神经网络来直接生成风格迁移的图像。一旦训练结束，进行风格迁移只需前馈网络一次，十分高效。在训练时，将生成图像、原始图像、风格图像三者前馈一固定网络以提取不同层特征用于计算损失函数。\n实验证明，通过使用实例归一化，风格变换网络可以移除与图像相关的比较信息以简化生成过程。\n7.5:条件示例规范化\n上述方法的一个问题是我们必须为每种不同的风格训练一个单独的模型。由于不同的风格有时包含相似性，因此可以通过在不同风格的风格变换网络之间共享参数来完成这项工作。具体来说，它更改了风格转换网络的示例规范化，使其具有N组缩放和平移参数，每个组对应于特定风格。这样我们就可以从单个前馈过程中获得N个风格的变换图像。\n8、面部验证/识别\n人脸验证/识别可以认为是一种更加精细的细粒度图像识别任务。人脸验证是给定两张图像、判断其是否属于同一个人，而人脸识别是回答图像中的人是谁。一个人脸验证/识别系统通常包括三大步：检测图像中的人脸，特征点定位、及对人脸进行验证/识别。人脸验证/识别的难题在于需要进行小样本学习。通常情况下，数据集中每人只有对应的一张图像，这称为一次性学习(one-shot  learning)。\n8.1:面部识别系统背后的概念\n作为分类问题（非常多的类别数），或作为度量学习的问题。如果两个图像属于同一个人，那么我们希望它们的深层特征非常相似。否则，它们的特征应该不同。之后，根据深度特征之间的距离进行验证或识别（k最近邻居分类）。\n8.2:DeepFace\n第一个成功将深度神经网络应用于面部验证/识别模型的系统。DeepFace使用非共享参数局部性连接。这是因为人脸的不同部分具有不同的特征（例如眼睛和嘴唇具有不同的特征），因此传统卷积层的经典“共享参数”不适用于面部验证。因此，面部识别网络使用非共享参数局部性连接。它使用的孪生（Siamese  network）网络用于面部验证。当两个图像的深度特征小于给定阈值时，它们被认为是同一个人。\n8.3:FaceNet\nFaceNet通过三因子输入，希望负样本之间的距离大于正样本之间的距离给定量。此外，三个输入因子并不是随机的，否则，因为负样本的差异样本太大，网络将无法学习。选择最具挑战性的三个元素组（例如最远的正样本和最接近的负样本）会使该网络陷入局部最优。FaceNet使用半困难策略，选择比正样本更远的负样本。\n8.4:大区间交叉熵损失\n近年来，这一直是一个热门的研究课题。由于类内波动大而类间相似度高，有研究工作旨在提升经典的交叉熵损失对深度特征的判断能力。例如，L-Softmax加强优化目标，使对应类别的参数向量和深度特征夹角增大。\nA-Softmax进一步约束L-Softmax的参数向量长度为1，使训练更集中到优化深度特征和夹角上。实际中，L-Softmax和A-Softmax都很难收敛，训练时采用了退火方法，从标准softmax逐渐退火至L-Softmax或A-Softmax。\n8.5:实时检测\n该系统确定面部图像是来自真人还是来自照片，这是面部验证/识别任务的关键障碍。目前在业界流行的一些方法是读取人的面部表情，纹理信息，眨眼或要求用户完成一系列动作的变化。\n9、图像搜索和检索\n给定一个包含特定实例（例如特定目标，场景或建筑物）的图像，图像搜索用于在数据库中查找包含与给定实例类似的元素的图像。然而，由于两个图像中的角度，光照和障碍物通常不相同，因此创建能够处理图像类别中的这些差异的搜索算法的问题对研究人员构成了重大挑战。\n9.1:经典图像搜索的过程\n首先，我们必须从图像中提取适当的代表性矢量。其次，将欧氏距离或余弦距离应用于这些矢量以执行最近邻居搜索并找到最相似的图像。最后，我们使用特定的处理技术对搜索结果进行小幅调整。我们可以看到图像搜索引擎性能的限制因素是图像的表示：\n9.2:无监督的图像搜索\n无监督图像搜索使用预先训练的ImageNet模型，没有外部信息作为特征提取引擎来提取图像的表示。\n直观的思路：因为深度全连接特征提供了图像的高级描述，并且是一个“自然”矢量，直观的思维过程是直接提取深度全连接特征作为图像的代表矢量。但是，由于图像分类中使用完全连接的特征缺乏对图像的详细描述，因此该思维过程仅产生平均准确度。\n使用深度卷积特征：因为深度卷积具有更好的详细信息，并且可以用于处理任何大小的图像，目前流行的方法是提取深度卷积特征，然后使用加权全局搜索和求和池来获得图像的代表向量。权重表示不同位置的特征的必要性，并且可以采用空间矢量权重或信道矢量权重的形式。\nCroW：深度卷积特征是一种分布式表示。尽管来自神经元的响应值在确定区域是否有目标方面不是非常有用，但如果多个神经元同时具有大量反应，那么该区域很可能包含目标。因此，CroW沿着通道添加了特征图以获得二维合成图，对其进行标准化，并根据数量标准化的结果将其用作空间权重。CroW的通道权重由特征图的稀疏性决定，类似于TF-IDF中的IDF特征，自然语言处理中的特征可用于提升不常见但具有高度确定性的特征。\n类加权特征：该方法尝试利用图像集成网络的类别预测信息使空间权重更具确定性。具体地，它使用CAM来获得预训练网络中每个类别的最具代表性的区域的语义信息;然后它使用标准化的CAM结果作为空间权重。\nPWA：PWA发现，深度卷积特征的不同通道对应于目标上不同区域的响应。因此，PWA可以选择一系列确定性特征映射，并将其标准化结果用作收敛的空间权重。然后系统级联结果以形成最终图像的表示。\n9.3:有监督图像搜索\n有监督图像搜索首先采用预先训练的ImageNet模型并将其调整到另一个训练数据集上。然后，它从这个调整的模型中提取图像表示。为了获得更好的结果，用于优化模型的训练数据集通常类似于搜索数据集。此外，我们可以使用候选区域网络从可能包含目标的图像中提取前景区域。\n孪生网络：类似于人脸识别的思想，该系统使用两个元素或三个元素输入（++ -）来训练模型，以最小化两个样本之间的距离，并最大化两个不同样本之间的距离。\n9.4:对象跟踪\n对象跟踪的目标是跟踪视频中目标的移动。通常，目标位于视频的第一帧中并由框标记。我们需要预测框在下一帧中的位置。对象跟踪与目标测试类似。然而，对象跟踪的难点在于我们不知道我们跟踪哪个目标。因此，我们无法在任务之前收集足够的训练数据并训练专门的测试。\n9.5:孪生网络\n类似于面部验证的概念，利用孪生网络可以在一条线上的目标框内输入图像，并且在另一条线上输入候选图像区域，然后输出两个图像之间的相似度。我们不需要遍历不同帧中的所有其他候选区域;相反，我们可以使用卷积网络，只需要将每个图像前馈一次，通过卷积，我们可以获得二维的响应图，其中最重要的响应位置确定了框的位置。基于孪生网络的方法非常快并且能够处理任何大小的图像。\n9.6:CFNet\n相关滤波器训练线性模板以区分图像区域和它们周围的区域，然后使用傅立叶变换。CFNet与离线训练的孪生网络和相关的在线滤波模板相结合，能够提高加权网络的跟踪性能。\n10、生成式模型（generative models）\n这种类型的模型用于学习数据（图像）的分布或从其分布中采样新图像。生成模型可用于超分辨率重建、图像着色、图像转换、从文本生成图像、学习隐藏的图像表示、半监督学习等。此外，生成式模型可以与强化学习相结合，用于模拟和逆强化学习。\n10.1:显式建模\n使用条件概率的公式来对图像的分布进行最大似然估计并从中学习。该方法的缺点在于，由于每个图像中的像素取决于先前的像素，因此必须在一个角开始并以有序的方式进行，所以生成图像的过程将稍微缓慢。例如，WaveNet可以产生类似于人类创建的语音，但由于它不能同时产生，一秒钟的语音需要2分钟来计算，并且实时生成是不可能的。\n10.2:变分自编码器\n为了避免显式建模的缺陷，变分自编码器对数据分布进行了隐式建模。它认为生成图像受隐藏变量控制的影响，并假设隐藏变量受到对角高斯分布的影响。\n变分自编码器使用解码网络根据隐藏变量生成图像。由于我们无法直接应用最大似然估计，因此在训练时，类似于EM算法，变分自编码器构造似然函数的下界函数，然后使用该下界函数进行优化。变分自编码器的好处是因为每个维度的独立性;我们可以通过控制隐藏变量来控制影响输出图像变化的因素。\n10.3:生成对抗式网络（GAN）\n由于学习数据分布极其困难，生成对抗式网络完全避免了这一步骤并立即生成图像。生成对抗式网络使用生成网络G从随机噪声创建图像，并使用判别网络D来确定输入图像是真实的还是伪造的。\n在训练期间，判别网络D的目标是确定图像是真实的还是伪造的，并且生成式网络G的目的是使判别网络D倾向于确定其输出图像是真实的。在实践中，训练生成式对抗网络会带来模型崩溃的问题，其中生成对抗式网络无法学习完整的数据分布。这在LS-GAN和W-GAN中产生了改进，与变分自编码器一样，生成对抗式网络提供更好的详细信息。\n11、视频分类\n上述大多数任务都可以用于视频分类，这里我们将以视频分类为例来说明处理视频数据的一些基本方法。\n11.1:多帧图像特征汇合\n这类方法将视频视为一系列帧图像，网络接收属于视频的一组多帧图像（例如15帧），然后从这些图像中提取深度特征，并最终集成这些图像特征以获得视频的该部分的特征以对其进行分类。实验表明，使用“慢速融合（slow  fusion）”效果最佳。此外，独立组织单个帧也可以得到非常有竞争力的结果，这意味着来自单个帧的图像包含大量相关信息。\n11.2:三维卷积\n将标准的二维卷积扩展为三维卷积，以在时间维度上连接局部。例如，系统可以采用VGG 3x3卷积并将其扩展为3x3x3卷积或2x2收敛扩展为2x2x2收敛。\n11.3:图像+序列两个分支结构\n这种类型的方法使用两个独立的网络来区分从视频捕获的图像信息和时间信息。图像信息可以从单帧中的静止图像获得，并且是图像分类的经典问题。然后通过光流获得运动信息，跟踪目标在相邻帧上的运动。\n11.4:CNN + RNN捕获远程依赖关系\n先前的方法仅能够捕获几帧图像之间的依赖关系。此方法使用CNN从单个帧中提取图像特征，然后使用RNN捕获帧之间的依赖关系。\n此外，研究人员已尝试将CNN和RNN结合起来，以便每个卷积层能够捕获远距离依赖性。\n以上为译文。\n本文由阿里云云栖社区组织翻译。\n文章原标题《deep-dive-into-computer-vision-with-neural-network-2》，\n作者： Leona Zhang译者：虎说八道，审校：。\n\nend\n关于TensorFlow你需要了解的9件事\n阿里云Redis多线程性能提升思路解析\n程序员精选：TensorFlow和ML前5名的课程\n阿里云总监课第二期：如何打造智能语音交互爆款产品？\n更多精彩"}
{"content2":"原文链接：https://zhuanlan.zhihu.com/p/31727402\n引言\n深度学习目前已成为发展最快、最令人兴奋的机器学习领域之一，许多卓有建树的论文已经发表，而且已有很多高质量的开源深度学习框架可供使用。然而，论文通常非常简明扼要并假设读者已对深度学习有相当的理解，这使得初学者经常卡在一些概念的理解上，读论文似懂非懂，十分吃力。另一方面，即使有了简单易用的深度学习框架，如果对深度学习常见概念和基本思路不了解，面对现实任务时不知道如何设计、诊断、及调试网络，最终仍会束手无策。\n本系列文章旨在直观系统地梳理深度学习各领域常见概念与基本思想，使读者对深度学习的重要概念与思想有一直观理解，做到“知其然，又知其所以然”，从而降低后续理解论文及实际应用的难度。本系列文章力图用简练的语言加以描述，避免数学公式和繁杂细节。本文是该系列文章中的第二篇，旨在介绍深度学习在计算机视觉领域四大基本任务中的应用，包括分类(图a)、定位、检测(图b)、语义分割(图c)、和实例分割(图d)。后续文章将关注深度学习在计算机视觉领域的其他任务的应用，以及自然语言处理和语音识别。\n(本文作者为我本人，部分内容首发于新智元)\n计算机视觉(computer vision)简介\n计算机视觉旨在识别和理解图像/视频中的内容。其诞生于1966年MIT AI Group的\"the summer vision project\"。当时，人工智能其他分支的研究已经有一些初步成果。由于人类可以很轻易地进行视觉认知，MIT的教授们希望通过一个暑期项目解决计算机视觉问题。当然，计算机视觉没有被一个暑期内解决，但计算机视觉经过50余年发展已成为一个十分活跃的研究领域。如今，互联网上超过70%的数据是图像/视频，全世界的监控摄像头数目已超过人口数，每天有超过八亿小时的监控视频数据生成。如此大的数据量亟待自动化的视觉理解与分析技术。\n计算机视觉的难点在于语义鸿沟。这个现象不仅出现在计算机视觉领域，Moravec悖论发现，高级的推理只需要非常少的计算资源，而低级的对外界的感知却需要极大的计算资源。要让计算机如成人般地下棋是相对容易的，但是要让电脑有如一岁小孩般的感知和行动能力却是相当困难甚至是不可能的。\n语义鸿沟(semantic gap) 人类可以轻松地从图像中识别出目标，而计算机看到的图像只是一组0到255之间的整数。\n计算机视觉任务的其他困难 拍摄视角变化、目标占据图像的比例变化、光照变化、背景融合、目标形变、遮挡等。\n计算机视觉的顶级会议和期刊 顶级会议有CVPR、ICCV、和ECCV，此外ICLR也有不少计算机视觉论文。顶级期刊有IJCV和TPAMI。由于计算机视觉领域发展十分迅速，不论身处学术界或产业界，通过阅读顶级会议和期刊论文了解计算机视觉的最近研究成果都十分必要。\n卷积神经网络(convolutional neural networks, CNN)\n经典的多层感知机由一系列全连接层组成，卷积神经网络中除全连接层外，还有卷积层和汇合(pooling)层。\n(1) 卷积层\n为什么要用卷积层 输入图像通常很维数很高，例如，1,000×1,000大小的彩色图像对应于三百万维特征。因此，继续沿用多层感知机中的全连接层会导致庞大的参数量。大参数量需要繁重的计算，而更重要的是，大参数量会有更高的过拟合风险。卷积是局部连接、共享参数版的全连接层。这两个特性使参数量大大降低。卷积层中的权值通常被成为滤波器(filter)或卷积核(convolution kernel)。\n局部连接 在全连接层中，每个输出通过权值(weight)和所有输入相连。而在视觉识别中，关键性的图像特征、边缘、角点等只占据了整张图像的一小部分，图像中相距很远的两个像素之间有相互影响的可能性很小。因此，在卷积层中，每个输出神经元在通道方向保持全连接，而在空间方向上只和一小部分输入神经元相连。\n共享参数 如果一组权值可以在图像中某个区域提取出有效的表示，那么它们也能在图像的另外区域中提取出有效的表示。也就是说，如果一个模式(pattern)出现在图像中的某个区域，那么它们也可以出现在图像中的其他任何区域。因此，卷积层不同空间位置的神经元共享权值，用于发现图像中不同空间位置的模式。共享参数是深度学习一个重要的思想，其在减少网络参数的同时仍然能保持很高的网络容量(capacity)。卷积层在空间方向共享参数，而循环神经网络(recurrent neural networks)在时间方向共享参数。\n卷积层的作用 通过卷积，我们可以捕获图像的局部信息。通过多层卷积层堆叠，各层提取到特征逐渐由边缘、纹理、方向等低层级特征过度到文字、车轮、人脸等高层级特征。\n卷积层中的卷积和数学教材中的卷积是什么关系 基本没有关系。卷积层中的卷积实质是输入和权值的互相关(cross-correlation)函数，而不是数学教材中的卷积。\n描述卷积的四个量 一个卷积层的配置由如下四个量确定。1. 滤波器个数。使用一个滤波器对输入进行卷积会得到一个二维的特征图(feature map)。我们可以用时使用多个滤波器对输入进行卷积，以得到多个特征图。2. 感受野(receptive field) F，即滤波器空间局部连接大小。3. 零填补(zero-padding) P。随着卷积的进行，图像大小将缩小，图像边缘的信息将逐渐丢失。因此，在卷积前，我们在图像上下左右填补一些0，使得我们可以控制输出特征图的大小。4. 步长(stride)S。滤波器在输入每移动S个位置计算一个输出神经元。\n卷积输入输出的大小关系 假设输入高和宽为H和W，输出高和宽为H'和W', 则H'=(H-F+2P)/S+1, W'=(W-F+2P)/S+1. 当S=1时，通过设定P=(F-1)/2, 可以保证输入输出空间大小相同。例如，3*3的卷积需要填补一个像素使得输入输出空间大小不变。\n应该使用多大的滤波器 尽量使用小的滤波器，如3×3卷积。通过堆叠多层3×3卷积，可以取得与大滤波器相同的感受野，例如三层3×3卷积等效于一层7×7卷积的感受野。但使用小滤波器有以下两点好处。1. 更少的参数量。假设通道数为D，三层3×3卷积的参数量为3×(D×D×3×3)=27D^2, 而一层7×7卷积的参数量为D×D×7×7=49D^2。2. 更多非线性。由于每层卷积层后都有非线性激活函数，三层3×3卷积一共经过三次非线性激活函数，而一层7×7卷积只经过一次。\n1×1卷积 旨在对每个空间位置的D维向量做一个相同的线性变换。通常用于增加非线性，或降维，这相当于在通道数方向上进行了压缩。1×1卷积是减少网络计算量和参数的重要方式。\n全连接层的卷积层等效 由于全连接层和卷积层都是做点乘，这两种操作可以相互等效。全连接层的卷积层等效只需要设定好卷积层的四个量：滤波器个数等于原全连接层输出神经元个数、感受野等于输入的空间大小、没有零填补、步长为1。\n为什么要将全连接层等效为卷积层 全连接层只能处理固定大小的输入，而卷积层可以处理任意大小输入。假设训练图像大小是224×224，而当测试图像大小是256×256。如果不进行全连接层的卷积层等效，我们需要从测试图像中裁剪出多个224×224区域分别前馈网络。而进行卷积层等效后，我们只需要将256×256输入前馈网络一次，即可达到多次前馈224×224区域的效果。\n卷积结果的两种视角 卷积结果是一个D×H×W的三维张量。其可以被认为是有D个通道，每个通道是一个二维的特征图，从输入中捕获了某种特定的特征。也可以被认为是有H×W个空间位置，每个空间位置是一个D维的描述向量，描述了对应感受野的图像局部区域的语义特征。\n卷积结果的分布式表示 卷积结果的各通道之间不是独立的。卷积结果的各通道的神经元和语义概念之间是一个“多对多”的映射。即，每个语义概念由多个通道神经元一起表示，而每个神经元又同时参与到多个语义概念中去。并且，神经元响应是稀疏的，即大部分的神经元输出为0。\n卷积操作的实现 有如下几种基本思路。1. 快速傅里叶变换(FFT)。通过变换到频域，卷积运算将变为普通矩阵乘法。实际中，当滤波器尺寸大时效果好，而对于通常使用的1×1和3×3卷积，加速不明显。2. im2col (image to column)。im2col将与每个输出神经元相连的局部输入区域展成一个列向量，并将所有得到的向量拼接成一个矩阵。这样卷积运算可以用矩阵乘法实现。im2col的优点是可以利用矩阵乘法的高效实现，而弊端是会占用很大存储，因为输入元素会在生成的矩阵中多次出现。此外，Strassen矩阵乘法和Winograd也常被使用。现有的计算库如MKL和cuDNN，会根据滤波器大小选择合适的算法。\n(2) 汇合层\n汇合层 根据特征图上的局部统计信息进行下采样，在保留有用信息的同时减少特征图的大小。和卷积层不同的是，汇合层不包含需要学习的参数。最大汇合(max-pooling)在一个局部区域选最大值作为输出，而平均汇合(average pooling)计算一个局部区域的均值作为输出。局部区域汇合中最大汇合使用更多，而全局平均汇合(global average pooling)是更常用的全局汇合方法。\n汇合层的作用 汇合层主要有以下三点作用。1. 增加特征平移不变性。汇合可以提高网络对微小位移的容忍能力。2. 减小特征图大小。汇合层对空间局部区域进行下采样，使下一层需要的参数量和计算量减少，并降低过拟合风险。3. 最大汇合可以带来非线性。这是目前最大汇合更常用的原因之一。近年来，有人使用步长为2的卷积层代替汇合层。而在生成式模型中，有研究发现，不使用汇合层会使网络更容易训练。\n图像分类(image classification)\n给定一张输入图像，图像分类任务旨在判断该图像所属类别。\n(1) 图像分类常用数据集\n以下是几种常用分类数据集，难度依次递增。http://rodrigob.github.io/are_we_there_yet/build/列举了各算法在各数据集上的性能排名。\nMNIST 60k训练图像、10k测试图像、10个类别、图像大小1×28×28、内容是0-9手写数字。\nCIFAR-10 50k训练图像、10k测试图像、10个类别、图像大小3×32×32。\nCIFAR-100 50k训练图像、10k测试图像、100个类别、图像大小3×32×32。\nImageNet 1.2M训练图像、50k验证图像、1k个类别。2017年及之前，每年会举行基于ImageNet数据集的ILSVRC竞赛，这相当于计算机视觉界奥林匹克。\n(2) 图像分类经典网络结构\n基本架构 我们用conv代表卷积层、bn代表批量归一层、pool代表汇合层。最常见的网络结构顺序是conv -> bn -> relu -> pool，其中卷积层用于提取特征、汇合层用于减少空间大小。随着网络深度的进行，图像的空间大小将越来越小，而通道数会越来越大。\n针对你的任务，如何设计网络？ 当面对你的实际任务时，如果你的目标是解决该任务而不是发明新算法，那么不要试图自己设计全新的网络结构，也不要试图从零复现现有的网络结构。找已经公开的实现和预训练模型进行微调。去掉最后一个全连接层和对应softmax，加上对应你任务的全连接层和softmax，再固定住前面的层，只训练你加的部分。如果你的训练数据比较多，那么可以多微调几层，甚至微调所有层。\nLeNet-5 60k参数。网络基本架构为：conv1 (6) -> pool1 -> conv2 (16) -> pool2 -> fc3 (120) -> fc4 (84) -> fc5 (10) -> softmax。括号中的数字代表通道数，网络名称中有5表示它有5层conv/fc层。当时，LeNet-5被成功用于ATM以对支票中的手写数字进行识别。LeNet取名源自其作者姓LeCun。\nAlexNet 60M参数，ILSVRC 2012的冠军网络。网络基本架构为：conv1 (96) -> pool1 -> conv2 (256) -> pool2 -> conv3 (384) -> conv4 (384) -> conv5 (256) -> pool5 -> fc6 (4096) -> fc7 (4096) -> fc8 (1000) -> softmax。AlexNet有着和LeNet-5相似网络结构，但更深、有更多参数。conv1使用11×11的滤波器、步长为4使空间大小迅速减小(227×227 -> 55×55)。AlexNet的关键点是：(1). 使用了ReLU激活函数，使之有更好的梯度特性、训练更快。(2). 使用了随机失活(dropout)。(3). 大量使用数据扩充技术。AlexNet的意义在于它以高出第二名10%的性能取得了当年ILSVRC竞赛的冠军，这使人们意识到卷机神经网络的优势。此外，AlexNet也使人们意识到可以利用GPU加速卷积神经网络训练。AlexNet取名源自其作者名Alex。\nVGG-16/VGG-19 138M参数，ILSVRC 2014的亚军网络。VGG-16的基本架构为：conv1^2 (64) -> pool1 -> conv2^2 (128) -> pool2 -> conv3^3 (256) -> pool3 -> conv4^3 (512) -> pool4 -> conv5^3 (512) -> pool5 -> fc6 (4096) -> fc7 (4096) -> fc8 (1000) -> softmax。 ^3代表重复3次。VGG网络的关键点是：(1). 结构简单，只有3×3卷积和2×2汇合两种配置，并且重复堆叠相同的模块组合。卷积层不改变空间大小，每经过一次汇合层，空间大小减半。(2). 参数量大，而且大部分的参数集中在全连接层中。网络名称中有16表示它有16层conv/fc层。(3). 合适的网络初始化和使用批量归一(batch normalization)层对训练深层网络很重要。VGG-19结构类似于VGG-16，有略好于VGG-16的性能，但VGG-19需要消耗更大的资源，因此实际中VGG-16使用得更多。由于VGG-16网络结构十分简单，并且很适合迁移学习，因此至今VGG-16仍在广泛使用。VGG-16和VGG-19取名源自作者所处研究组名(Visual Geometry Group)。\nGoogLeNet 5M参数，ILSVRC 2014的冠军网络。GoogLeNet试图回答在设计网络时究竟应该选多大尺寸的卷积、或者应该选汇合层。其提出了Inception模块，同时用1×1、3×3、5×5卷积和3×3汇合，并保留所有结果。网络基本架构为：conv1 (64) -> pool1 -> conv2^2 (64, 192) -> pool2 -> inc3 (256, 480) -> pool3 -> inc4^5 (512, 512, 512, 528, 832) -> pool4 -> inc5^2 (832, 1024) -> pool5 -> fc (1000)。GoogLeNet的关键点是：(1). 多分支分别处理，并级联结果。(2). 为了降低计算量，用了1×1卷积降维。GoogLeNet使用了全局平均汇合替代全连接层，使网络参数大幅减少。GoogLeNet取名源自作者所处单位(Google)，其中L大写是为了向LeNet致敬，而Inception的名字来源于盗梦空间中的\"we need to go deeper\"梗。\nInception v3/v4 在GoogLeNet的基础上进一步降低参数。其和GoogLeNet有相似的Inception模块，但将7×7和5×5卷积分解成若干等效3×3卷积，并在网络中后部分把3×3卷积分解为1×3和3×1卷积。这使得在相似的网络参数下网络可以部署到42层。此外，Inception v3使用了批量归一层。Inception v3是GoogLeNet计算量的2.5倍，而错误率较后者下降了3%。Inception v4在Inception模块基础上结合了residual模块(见下文)，进一步降低了0.4%的错误率。\nResNet ILSVRC 2015的冠军网络。ResNet旨在解决网络加深后训练难度增大的现象。其提出了residual模块，包含两个3×3卷积和一个短路连接(左图)。短路连接可以有效缓解反向传播时由于深度过深导致的梯度消失现象，这使得网络加深之后性能不会变差。短路连接是深度学习又一重要思想，除计算机视觉外，短路连接也被用到了机器翻译、语音识别/合成领域。此外，具有短路连接的ResNet可以看作是许多不同深度而共享参数的网络的集成，网络数目随层数指数增加。ResNet的关键点是：(1). 使用短路连接，使训练深层网络更容易，并且重复堆叠相同的模块组合。(2). ResNet大量使用了批量归一层。(3). 对于很深的网络(超过50层)，ResNet使用了更高效的瓶颈(bottleneck)结构(右图)。ResNet在ImageNet上取得了超过人的准确率。\n下图对比了上述几种网络结构。\npreResNet ResNet的改进。preResNet整了residual模块中各层的顺序。相比经典residual模块(a)，(b)将BN共享会更加影响信息的短路传播，使网络更难训练、性能也更差；(c)直接将ReLU移到BN后会使该分支的输出始终非负，使网络表示能力下降；(d)将ReLU提前解决了(e)的非负问题，但ReLU无法享受BN的效果；(e)将ReLU和BN都提前解决了(d)的问题。preResNet的短路连接(e)能更加直接的传递信息，进而取得了比ResNet更好的性能。\nResNeXt ResNet的另一改进。传统的方法通常是靠加深或加宽网络来提升性能，但计算开销也会随之增加。ResNeXt旨在不改变模型复杂度的情况下提升性能。受精简而高效的Inception模块启发，ResNeXt将ResNet中非短路那一分支变为多个分支。和Inception不同的是，每个分支的结构都相同。ResNeXt的关键点是：(1). 沿用ResNet的短路连接，并且重复堆叠相同的模块组合。(2). 多分支分别处理。(3). 使用1×1卷积降低计算量。其综合了ResNet和Inception的优点。此外，ResNeXt巧妙地利用分组卷积进行实现。ResNeXt发现，增加分支数是比加深或加宽更有效地提升网络性能的方式。ResNeXt的命名旨在说明这是下一代(next)的ResNet。\n随机深度 ResNet的改进。旨在缓解梯度消失和加速训练。类似于随机失活(dropout)，其以一定概率随机将residual模块失活。失活的模块直接由短路分支输出，而不经过有参数的分支。在测试时，前馈经过全部模块。随机深度说明residual模块是有信息冗余的。\nDenseNet 其目的也是避免梯度消失。和residual模块不同，dense模块中任意两层之间均有短路连接。也就是说，每一层的输入通过级联(concatenation)包含了之前所有层的结果，即包含由低到高所有层次的特征。和之前方法不同的是，DenseNet中卷积层的滤波器数很少。DenseNet只用ResNet一半的参数即可达到ResNet的性能。实现方面，作者在大会报告指出，直接将输出级联会占用很大GPU存储。后来，通过共享存储，可以在相同的GPU存储资源下训练更深的DenseNet。但由于有些中间结果需要重复计算，该实现会增加训练时间。\nSENet ILSVRC 2017的冠军网络。SENet通过额外的分支(gap-fc-fc-sigm)来得到每个通道的[0, 1]权重，自适应地校正原各通道激活值响应。以提升有用的通道响应并抑制对当前任务用处不大的通道响应。\n目标定位(object localization)\n在图像分类的基础上，我们还想知道图像中的目标具体在图像的什么位置，通常是以包围盒的(bounding box)形式。\n基本思路 多任务学习，网络带有两个输出分支。一个分支用于做图像分类，即全连接+softmax判断目标类别，和单纯图像分类区别在于这里还另外需要一个“背景”类。另一个分支用于判断目标位置，即完成回归任务输出四个数字标记包围盒位置(例如中心点横纵坐标和包围盒长宽)，该分支输出结果只有在分类分支判断不为“背景”时才使用。\n人体位姿定位/人脸定位 目标定位的思路也可以用于人体位姿定位或人脸定位。这两者都需要我们对一系列的人体关节或人脸关键点进行回归。\n弱监督定位 由于目标定位是相对比较简单的任务，近期的研究热点是在只有标记信息的条件下进行目标定位。其基本思路是从卷积结果中找到一些较高响应的显著性区域，认为这个区域对应图像中的目标。\n目标检测(object detection)\n在目标定位中，通常只有一个或固定数目的目标，而目标检测更一般化，其图像中出现的目标种类和数目都不定。因此，目标检测是比目标定位更具挑战性的任务。\n(1) 目标检测常用数据集\nPASCAL VOC 包含20个类别。通常是用VOC07和VOC12的trainval并集作为训练，用VOC07的测试集作为测试。\nMS COCO COCO比VOC更困难。COCO包含80k训练图像、40k验证图像、和20k没有公开标记的测试图像(test-dev)，80个类别，平均每张.2个目标。通常是用80k训练和35k验证图像的并集作为训练，其余5k图像作为验证，20k测试图像用于线上测试。\nmAP (mean average precision) 目标检测中的常用评价指标，计算方法如下。当预测的包围盒和真实包围盒的交并比大于某一阈值(通常为0.5)，则认为该预测正确。对每个类别，我们画出它的查准率-查全率(precision-recall)曲线，平均准确率是曲线下的面积。之后再对所有类别的平均准确率求平均，即可得到mAP，其取值为[0, 100%]。\n交并比(intersection over union, IoU) 算法预测的包围盒和真实包围盒交集的面积除以这两个包围盒并集的面积，取值为[0, 1]。交并比度量了算法预测的包围盒和真实包围盒的接近程度，交并比越大，两个包围盒的重叠程度越高。\n(2) 基于候选区域的目标检测算法\n基本思路 使用不同大小的窗口在图像上滑动，在每个区域，对窗口内的区域进行目标定位。即，将每个窗口内的区域前馈网络，其分类分支用于判断该区域的类别，回归分支用于输出包围盒。基于滑动窗的目标检测动机是，尽管原图中可能包含多个目标，但滑动窗对应的图像局部区域内通常只会有一个目标(或没有)。因此，我们可以沿用目标定位的思路对窗口内区域逐个进行处理。但是，由于该方法要把图像所有区域都滑动一遍，而且滑动窗大小不一，这会带来很大的计算开销。\nR-CNN 先利用一些非深度学习的类别无关的无监督方法，在图像中找到一些可能包含目标的候选区域。之后，对每个候选区域前馈网络，进行目标定位，即两分支(分类+回归)输出。其中，我们仍然需要回归分支的原因是，候选区域只是对包含目标区域的一个粗略的估计，我们需要有监督地利用回归分支得到更精确的包围盒预测结果。R-CNN的重要性在于当时目标检测已接近瓶颈期，而R-CNN利于在ImageNet预训练模型微调的方法一举将VOC上mAP由35.1%提升至53.7%，确定了深度学习下目标检测的基本思路。一个有趣之处是R-CNN论文开篇第一句只有两个词\"Features matter.\" 这点明了深度学习方法的核心。\n候选区域(region proposal) 候选区域生成算法通常基于图像的颜色、纹理、面积、位置等合并相似的像素，最终可以得到一系列的候选矩阵区域。这些算法，如selective search或EdgeBoxes，通常只需要几秒的CPU时间，而且，一个典型的候选区域数目是2k，相比于用滑动窗把图像所有区域都滑动一遍，基于候选区域的方法十分高效。另一方面，这些候选区域生成算法的查准率(precision)一般，但查全率(recall)通常比较高，这使得我们不容易遗漏图像中的目标。\nFast R-CNN R-CNN的弊端是需要多次前馈网络，这使得R-CNN的运行效率不高，预测一张图像需要47秒。Fast R-CNN同样基于候选区域进行目标检测，但受SPPNet启发，在Fast R-CNN中，不同候选区域的卷积特征提取部分是共享的。也就是说，我们先将整副图像前馈网络，并提取conv5卷积特征。之后，基于候选区域生成算法的结果在卷积特征上进行采样，这一步称为兴趣区域汇合。最后，对每个候选区域，进行目标定位，即两分支(分类+回归)输出。\n兴趣区域汇合(region of interest pooling, RoI pooling) 兴趣区域汇合旨在由任意大小的候选区域对应的局部卷积特征提取得到固定大小的特征，这是因为下一步的两分支网络由于有全连接层，需要其输入大小固定。其做法是，先将候选区域投影到卷积特征上，再把对应的卷积特征区域空间上划分成固定数目的网格(数目根据下一步网络希望的输入大小确定，例如VGGNet需要7×7的网格)，最后在每个小的网格区域内进行最大汇合，以得到固定大小的汇合结果。和经典最大汇合一致，每个通道的兴趣区域汇合是独立的。\nFaster R-CNN Fast R-CNN测试时每张图像前馈网络只需0.2秒，但瓶颈在于提取候选区域需要2秒。Faster R-CNN不再使用现有的无监督候选区域生成算法，而利用候选区域网络从conv5特征中产生候选区域，并且将候选区域网络集成到整个网络中端到端训练。Faster R-CNN的测试时间是0.2秒，接近实时。后来有研究发现，通过使用更少的候选区域，可以在性能损失不大的条件下进一步提速。\n候选区域网络(region proposal networks, RPN) 在卷积特征上的通过两层卷积(3×3和1×1卷积)，输出两个分支。其中，一个分支用于判断每个锚盒是否包含了目标，另一个分支对每个锚盒输出候选区域的4个坐标。候选区域网络实际上延续了基于滑动窗进行目标定位的思路，不同之处在于候选区域网络在卷积特征而不是在原图上进行滑动。由于卷积特征的空间大小很小而感受野很大，即使使用3×3的滑动窗，也能对应于很大的原图区域。Faster R-CNN实际使用了3组大小(128×128、256×256、512×512)、3组长宽比(1:1、1:2、2:1)，共计9个锚盒，这里锚盒的大小已经超过conv5特征感受野的大小。对一张1000×600的图像，可以得到20k个锚盒。\n为什么要使用锚盒(anchor box) 锚盒是预先定义形状和大小的包围盒。使用锚盒的原因包括：(1). 图像中的候选区域大小和长宽比不同，直接回归比对锚盒坐标修正训练起来更困难。(2). conv5特征感受野很大，很可能该感受野内包含了不止一个目标，使用多个锚盒可以同时对感受野内出现的多个目标进行预测。(3). 使用锚盒也可以认为这是向神经网络引入先验知识的一种方式。我们可以根据数据中包围盒通常出现的形状和大小设定一组锚盒。锚盒之间是独立的，不同的锚盒对应不同的目标，比如高瘦的锚盒对应于人，而矮胖的锚盒对应于车辆。\nR-FCN Faster R-CNN在RoI pooling之后，需要对每个候选区域单独进行两分支预测。R-FCN旨在使几乎所有的计算共享，以进一步加快速度。由于图像分类任务不关心目标具体在图像的位置，网络具有平移不变性。但目标检测中由于要回归出目标的位置，所以网络输出应当受目标平移的影响。为了缓和这两者的矛盾，R-FCN显式地给予深度卷积特征各通道以位置关系。在RoI汇合时，先将候选区域划分成3×3的网格，之后将不同网格对应于候选卷积特征的不同通道，最后每个网格分别进行平均汇合。R-FCN同样采用了两分支(分类+回归)输出。\n小结 基于候选区域的目标检测算法通常需要两步：第一步是从图像中提取深度特征，第二步是对每个候选区域进行定位(包括分类和回归)。其中，第一步是图像级别计算，一张图像只需要前馈该部分网络一次，而第二步是区域级别计算，每个候选区域都分别需要前馈该部分网络一次。因此，第二步占用了整体主要的计算开销。R-CNN, Fast R-CNN, Faster R-CNN, R-FCN这些算法的演进思路是逐渐提高网络中图像级别计算的比例，同时降低区域级别计算的比例。R-CNN中几乎所有的计算都是区域级别计算，而R-FCN中几乎所有的计算都是图像级别计算。\n(3) 基于直接回归的目标检测算法\n基本思路 基于候选区域的方法由于有两步操作，虽然检测性能比较好，但速度上离实时仍有一些差距。基于直接回归的方法不需要候选区域，直接输出分类/回归结果。这类方法由于图像只需前馈网络一次，速度通常更快，可以达到实时。\nYOLO 将图像划分成7×7的网格，其中图像中的真实目标被其划分到目标中心所在的网格及其最接近的锚盒。对每个网格区域，网络需要预测：每个锚盒包含目标的概率(不包含目标时应为0，否则为锚盒和真实包围盒的IoU)、每个锚盒的4个坐标、该网格的类别概率分布。每个锚盒的类别概率分布等于每个锚盒包含目标的概率乘以该网格的类别概率分布。相比基于候选区域的方法，YOLO需要预测包含目标的概率的原因是，图像中大部分的区域不包含目标，而训练时只有目标存在时才对坐标和类别概率分布进行更新。YOLO的优点在于：(1). 基于候选区域的方法的感受野是图像中的局部区域，而YOLO可以利用整张图像的信息。(2). 有更好的泛化能力。YOLO的局限在于：(1). 不能很好处理网格中目标数超过预设固定值，或网格中有多个目标同时属于一个锚盒的情况。(2). 对小目标的检测能力不够好。(3). 对不常见长宽比的包围盒的检测能力不强。(4). 计算损失时没有考虑包围盒大小。大的包围盒中的小偏移和小的包围盒中的小偏移应有不同的影响。\nSSD 相比YOLO，SSD在卷积特征后加了若干卷积层以减小特征空间大小，并通过综合多层卷积层的检测结果以检测不同大小的目标。此外，类似于Faster R-CNN的RPN，SSD使用3×3卷积取代了YOLO中的全连接层，以对不同大小和长宽比的锚盒来进行分类/回归。SSD取得了比YOLO更快，接近Faster R-CNN的检测性能。后来有研究发现，相比其他方法，SSD受基础模型性能的影响相对较小。\nFPN 之前的方法都是取高层卷积特征。但由于高层特征会损失一些细节信息，FPN融合多层特征，以综合高层、低分辨率、强语义信息和低层、高分辨率、弱语义信息来增强网络对小目标的处理能力。此外，和通常用多层融合的结果做预测的方法不同，FPN在不同层独立进行预测。FPN既可以与基于候选区域的方法结合，也可以与基于直接回归的方法结合。FPN在和Faster R-CNN结合后，在基本不增加原有模型计算量的情况下，大幅提高对小目标的检测性能。\nRetinaNet RetinaNet认为，基于直接回归的方法性能通常不如基于候选区域方法的原因是，前者会面临极端的类别不平衡现象。基于候选区域的方法可以通过候选区域过滤掉大部分的背景区域，但基于直接回归的方法需要直接面对类别不平衡。因此，RetinaNet通过改进经典的交叉熵损失以降低对已经分的很好的样例的损失值，提出了焦点(focal)损失函数，以使模型训练时更加关注到困难的样例上。RetinaNet取得了接近基于直接回归方法的速度，和超过基于候选区域的方法的性能。\n(4) 目标检测常用技巧\n非最大抑制(non-max suppression, NMS) 目标检测可能会出现的一个问题是，模型会对同一目标做出多次预测，得到多个包围盒。NMS旨在保留最接近真实包围盒的那一个预测结果，而抑制其他的预测结果。NMS的做法是，首先，对每个类别，NMS先统计每个预测结果输出的属于该类别概率，并将预测结果按该概率由高至低排序。其次，NMS认为对应概率很小的预测结果并没有找到目标，所以将其抑制。然后，NMS在剩余的预测结果中，找到对应概率最大的预测结果，将其输出，并抑制和该包围盒有很大重叠(如IoU大于0.3)的其他包围盒。重复上一步，直到所有的预测结果均被处理。\n在线困难样例挖掘(online hard example mining, OHEM) 目标检测的另一个问题是类别不平衡，图像中大部分的区域是不包含目标的，而只有小部分区域包含目标。此外，不同目标的检测难度也有很大差异，绝大部分的目标很容易被检测到，而有一小部分目标却十分困难。OHEM和Boosting的思路类似，其根据损失值将所有候选区域进行排序，并选择损失值最高的一部分候选区域进行优化，使网络更关注于图像中更困难的目标。此外，为了避免选到相互重叠很大的候选区域，OHEM对候选区域根据损失值进行NMS。\n在对数空间回归 回归相比分类优化难度大了很多。 损失对异常值比较敏感，由于有平方，异常值会有大的损失值，同时会有很大的梯度，使训练时很容易发生梯度爆炸。而  损失的梯度不连续。在对数空间中，由于数值的动态范围小了很多，回归训练起来也会容易很多。此外，也有人用平滑的  损失进行优化。预先将回归目标规范化也会有助于训练。\n语义分割(semantic segmentation)\n语义分割是目标检测更进阶的任务，目标检测只需要框出每个目标的包围盒，语义分割需要进一步判断图像中哪些像素属于哪个目标。\n(1) 语义分割常用数据集\nPASCAL VOC 2012 1.5k训练图像，1.5k验证图像，20个类别(包含背景)。\nMS COCO COCO比VOC更困难。有83k训练图像，41k验证图像，80k测试图像，80个类别。\n(2) 语义分割基本思路\n基本思路 逐像素进行图像分类。我们将整张图像输入网络，使输出的空间大小和输入一致，通道数等于类别数，分别代表了各空间位置属于各类别的概率，即可以逐像素地进行分类。\n全卷积网络+反卷积网络 为使得输出具有三维结构，全卷积网络中没有全连接层，只有卷积层和汇合层。但是随着卷积和汇合的进行，图像通道数越来越大，而空间大小越来越小。要想使输出和输入有相同的空间大小，全卷积网络需要使用反卷积和反汇合来增大空间大小。\n反卷积(deconvolution)/转置卷积(transpose convolution) 标准卷积的滤波器在输入图像中进行滑动，每次和输入图像局部区域点乘得到一个输出，而反卷积的滤波器在输出图像中进行滑动，每个由一个输入神经元乘以滤波器得到一个输出局部区域。反卷积的前向过程和卷积的反向过程完成的是相同的数学运算。和标准卷积的滤波器一样，反卷积的滤波器也是从数据中学到的。\n反最大汇合(max-unpooling) 通常全卷积网络是对称的结构，在最大汇合时需要记下最大值所处局部区域位置，在对应反最大汇合时将对应位置输出置为输入，其余位置补零。反最大汇合可以弥补最大汇合时丢失的空间信息。反最大汇合的前向过程和最大汇合的反向过程完成的是相同的数学运算。\n(3) 语义分割常用技巧\n扩张卷积(dilated convolution) 经常用于分割任务以增大有效感受野的一个技巧。标准卷积操作中每个输出神经元对应的输入局部区域是连续的，而扩张卷积对应的输入局部区域在空间位置上不连续。扩张卷积向标准卷积运算中引入了一个新的超参数扩张量(dilation)，用于描述输入局部区域在空间位置上的间距。当扩张量为1时，扩张卷积退化为标准卷积。扩张卷积可以在参数量不变的情况下有效提高感受野。例如，当有多层3×3标准卷积堆叠时，第l 层卷积(l 从1开始)的输出神经元的感受野为2l +1。与之相比，当有多层3×3扩张卷积堆叠，其中第l 层卷积的扩张量为2^{l-1}时，第l 层卷积的输出神经元的感受野为2^{l +1}-1。感受野越大，神经元能利用的相关信息越多。和经典计算机视觉手工特征相比，大的感受野是深度学习方法能取得优异性能的重要原因之一。\n条件随机场(conditional random field, CRF) 条件随机场是一种概率图模型，常被用于微修全卷积网络的输出结果，使细节信息更好。其动机是距离相近的像素、或像素值相近的像素更可能属于相同的类别。此外，有研究工作用循环神经网络(recurrent neural networks)近似条件随机场。条件随机场的另一弊端是会考虑两两像素之间的关系，这使其运行效率不高。\n利用低层信息 综合利用低层结果可以弥补随着网络加深丢失的细节和边缘信息，利用方式可以是加和(如FCN)或沿通道方向拼接(如U-net)，后者效果通常会更好一些。\n实例分割(instance segmentation)\n语义分割不区分属于相同类别的不同实例。例如，当图像中有多只猫时，语义分割会将两只猫整体的所有像素预测为“猫”这个类别。与此不同的是，实例分割需要区分出哪些像素属于第一只猫、哪些像素属于第二只猫。\n基本思路 目标检测+语义分割。先用目标检测方法将图像中的不同实例框出，再用语义分割方法在不同包围盒内进行逐像素标记。\nMask R-CNN 用FPN进行目标检测，并通过添加额外分支进行语义分割(额外分割分支和原检测分支不共享参数)，即Mask R-CNN有三个输出分支(分类、坐标回归、和分割)。此外，Mask R-CNN的其他改进有：(1). 改进了RoI汇合，通过双线性差值使候选区域和卷积特征的对齐不因量化而损失信息。(2). 在分割时，Mask R-CNN将判断类别和输出模板(mask)这两个任务解耦合，用sigmoid配合对率(logistic)损失函数对每个类别的模板单独处理，取得了比经典分割方法用softmax让所有类别一起竞争更好的效果。\n致谢\n感谢\n@UnFound\n指出的一处笔误。\n参考文献\nV. Badrinarayanan, et al. SegNet: A deep convolutional encoder-decoder architecture for image segmentation. TPAMI, 2017.\nY. Bengio, et al. Representation learning: A review and new perspectives. TPAMI, 2013.\nL.-C. Chen, et al. SegNet: A deep convolutional encoder-decoder architecture for image segmentation. PAMI, 2017.\nS. Chetlur, et al. cuDNN: Efficient primitives for deep learning. arXiv: 1410.0759, 2014.\nJ. Cong, and B. Xiao. Minimizing computation in convolutional neural networks. ICANN, 2014.\nJ. Dai, et al. R-FCN: Object detection via region-based fully convolutional networks. NIPS, 2016.\nA. Garcia-Garcia, et al. A review on deep learning techniques applied to semantic segmentation. arXiv: 1704.06857, 2017.\nR. Girshick, et al. Rich feature hierarchies for accurate object detection and semantic segmentation. CVPR, 2014.\nR. Girshick. Fast R-CNN. ICCV, 2015.\nK. He, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition. ECCV, 2014.\nK. He, et al. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. ICCV, 2015.\nK. He, et al. Deep residual learning for image recognition. CVPR, 2016.\nK. He, et al. Identity mappings in deep residual networks. ECCV, 2016.\nK. He, et al. Mask R-CNN. ICCV, 2017.\nJ. Hu, et al. Squeeze-and-excitation networks. CVPR, 2018.\nG. Huang, et al. Deep networks with stochastic depth. ECCV, 2016.\nG. Huang, et al. Densely connected convolutional networks. CVPR, 2017.\nJ. Huang, et al. Speed/Accuracy trade-offs for modern convolutional object detectors. CVPR, 2017.\nA. Krizhevsky, and G. Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009.\nA. Krizhevsky, et al. ImageNet classification with deep convolutional neural networks. NIPS, 2012.\nA. Lavin, and S. Gray. Fast algorithms for convolutional neural networks. CVPR, 2016.\nY. LeCun, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.\nM. Lin, et al. Network in network. ICLR, 2014.\nT.-Y. Lin, et al. Microsoft COCO: Common objects in context. ECCV, 2014.\nT.-Y. Lin, et al. Feature pyramid networks for object detection. CVPR, 2017.\nT.-Y. Lin, et al. Focal loss for dense object detection. ICCV, 2017.\nW. Liu, et al. SSD: Single shot multibox detector. ECCV, 2016.\nJ. Long, et al. Fully convolutional networks for semantic segmentation. CVPR, 2015.\nH. Noh, et al. Learning deconvolution network for semantic segmentation. ICCV, 2015.\nG. Pleiss, et al. Memory-efficient implementation of DenseNets. arXiv: 1707.06990, 2017.\nJ. Redmon, et al. You only look once: Unified, real-time object detection. CVPR, 2016.\nS. Ren, et al. Faster R-CNN: Towards real-time object detection with region proposal networks. NIPS, 2015.\nS. Ren, et al. Object detection networks on convolutional feature maps. TPAMI, 2017.\nO. Ronneberger, et al. U-net: Convolutional networks for biomedical image segmentation. MICCAI, 2015.\nO. Russakovsky, et al. ImageNet large scale visual recognition challenge. IJCV, 2015.\nP. Sermanet, et al. OverFeat: Integrated recognition, localization, and detection using convolutional networks. ICLR, 2014.\nA. Shrivastava, et al. Training region-based object detectors with online hard example mining. CVPR, 2016.\nK. Simonyan, and A. Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015.\nJ. T. Springenberg, et al. Striving for simplicity: The all convolutional net. ICLR Workshop, 2015.\nV. Sze, et al. Efficient processing of deep neural networks: A tutorial and survey. Proceedings of IEEE, 2017.\nC. Szegedy, et al. Going deep with convolutions. CVPR, 2015.\nC. Szegedy, et al. Rethinking the Inception architecture for computer vision. CVPR, 2016.\nC. Szegedy, et al. Inception v4, Inception-ResNet and the impact of residual connections on learning. AAAI, 2017.\nA. Toshev, and C. Szegedy. DeepPose: Human pose estimation via deep neural networks. CVPR, 2014.\nA. Veit, et al. Residual networks behave like ensembles of relatively shallow networks. NIPS, 2016.\nS. Xie, et al. Aggregated residual transformations for deep neural networks. CVPR, 2017.\nF. Yu, and V. Koltun. Multi-scale context aggregation by dilated convolutions. ICLR, 2016.\nM. D. Zeiler, and R. Fergus. Visualizing and understanding convolutional networks. ECCV, 2014.\nS. Zheng, et al. Conditional random fields as recurrent neural networks. ICCV, 2015."}
{"content2":"今天，听了邱老前辈的课程，下来后，对人与计算机反应时间的差别这一问题，莫名的感兴趣，之前就有听说过什么计算机视觉，道听途说也罢，略知一二也罢，下来查查资料，这才恍然大悟~\n转自：http://blog.csdn.net/zouxy09/article/details/38639349\n之前看了这么一本说自然图像统计学的书，本来是想着要好好看，然后每天翻译几页的。但实习的时候太忙了，没有什么时间，所以只把目录给翻译了，哈哈。这本书叫：Natural Image Statistics: A Probabilistic Approach to Early Computational Vision大家可以瞧瞧。不过，看到里面视觉概述的时候，自己也想扯扯点东西，按捺不住，就吐了不少文字。如果有什么不对的地方，也还希望大家不吝指正，谢谢。\n一、什么叫视觉Vision？\n什么叫视觉？可能这个问题的第一反应就是：不就是看东西么。好吧，那么高大上的问题就被这么轻描淡写了。看东西不是目的，目的是看到后知道是个什么东西，然后基于这种物体与事件的联系服务于高层意识理解。哈哈，瞬间档次又拉上去了。个人浅见，目前大体视觉可以分为生物视觉和机器视觉。生物视觉就是有眼睛的动物能什么看东西的。例如喵星人、汪星人还有地球人啥的。每种动物的视觉感官不同，能感受的光波长也不同。我们如果可以知道它们是怎么工作了，就可以借鉴着做出具有类似强大功能的设备了。这其中涉及到的东西有很多，它们的视觉流是怎样的？包含怎么接受光信号的输入，到如何逐级的提取信息，再到高层的推理，还有高层到底层的反馈等等。有的已经被解密了，有的还犹抱琵琶半遮面，也的神秘依旧。遗憾的是，后者的比重更大。\n目前来说，大家最感兴趣的还是人的视觉系统。它实在是太强大了，但却又太神秘，使得众多探秘者倾注心血也只能窥知一二。不过也还是有令人欣慰的消息可以传出，然后对某些成果，也有种能用之一二，受用一生的感觉。哈哈。\n人脑中大约70%的信息是来自于眼睛。大概20-30%的皮层区域是拿来做视觉处理的。生物视觉的最终目标还是想把人的视觉系统彻底告破于天下。嗯，理想是应该丰满点。那机器视觉又是啥呢？就是机器看东西嘛。因为内在的平台还是计算机，所以叫计算机视觉。Computer Vision！它属于人工智能里面很核心的一部分。机器要像人一样，会看，会听，会说，会学习，会思考等等。其中，像人一样，视觉的信息的比重依旧是最大的。所以计算机视觉的重要性毋庸置疑。计算机视觉研究的终极目标也是如何让机器具有人一样的视觉！这听起来就挺难的是吗？嗯，之前人工智能的发展实在太缓慢了，近几年Deep Learning的出现、大数据和大机器的支撑才给人工智能的发展稍微推进了一段距离，但还有很远的路要走。很远。\n好了，视觉是啥？视觉就是看东西，呃，不是很专业。视觉是为了获取关于环境中物体和事件的信息，从物体发射或者反射出来的光中提取信息的过程。所以我们第一件需要考虑的事情是，这些信息在什么形式的时候才是有效的？\n物体发出或者反射的光会被收集和度量，当然了，这里没有特定任务的信息提取的处理。生物视觉系统和人工视觉系统都通过同一种方式来完成这第一个步骤，也就是将这些光投影到一个二维的图像中。对于人眼和摄像头，虽然存在不少差别，但图像信息的处理基本是相同的。通过非常多的感光的细胞或者感光的原件接受光，然后将这些光的强度变成一个二维图像。然后图像的每个点的光强度就保存了某种信息。\n一般来说，投影图像还具有时间和彩色的维度。但我们大部分关注静态和灰度的图像。这个图像可以表示为二维的标量函数I(x,y)，也就是给定每个位置(x,y)，会得到一个对应的灰度值I(x,y)。尽管位置和灰度值都应该是连续的，但典型的情况是离散采样。也就是说x和y是整数，而灰度值在每个点采样。在数字系统中，采样一般也是矩形。但实际上，生物系统中的空间采集采样并不是矩形的，甚至是无规律的。\n视觉就是从这种图像数据中提取信息的。物理环境的信息包含在这个图像中的，但很遗憾，是隐含着的。视觉系统必须将这种隐含的信息变换成明确的形式，例如识别环境中的物体。但这不是一件容易的事情。\n二、人类视觉系统的魔法\n视觉是个非常棘手的任务。尽管这对搞视觉的人来说没什么可以惊讶的，但对其他人来说，他们可能会觉得非常惊讶，他似乎没有意识到自己与生俱来的双眼是如此的强大和不可复制。因为他几乎毫不费力的无时无刻的既有效又快速执行这个任务。但实际上在你看我的文字的时候，你大脑的整个计算过程是非常复杂的，但可惜呀，我们一般只在乎这个结果，大脑直接给我们它的计算结果，而没有告诉我们它完成这个是多么的辛苦。默默无闻，以使得被忽略和遗忘。\n为了说明视觉的难度，我们看下图。其实一个度量后的图像就是这样子的，每个空间位置有个光的强度值。那请你告诉我，下面这幅图像是个啥？你可以破解吗？\nOK，我知道这个对大家很难，那我们看看他究竟是啥，我们把每个方块的数字换成同等幅度的灰度。瞬间清晰了，有木有，这是一张男人的脸！虽然眼睛接收到的就是上面的那些类似数字之类的东西，但我们的视觉系统把这些数字都变得有意义了，然后发现了真实世界的物体。\n到目前为止，我们做了大量惨绝人寰的简化。实际上人的视网膜的光接收器是可以接收不同波长的光的，同时我们一般是双眼看世界，而不是只用一个。还有一个最大的差别是，我们是看动态的图像，而不是静态的。相机是一次成像，而人眼则是眼睛和大脑的组合，眼睛持续不断把图像信息传递给大脑，并且眼睛在不挺的转动，让高分辨率的感应区域扫过对象的各个细节。大脑把所获得的信息进行动态累加，就得到了我们所感觉到的图像了。你惊呼，差点被忽悠了。但这些差别并没有改变一个事实，就是光信息的确是通过上图那么的数据展示的。所以视觉系统的使命就是读懂这些数据。\n大部分凡人都会同意说，这些任务刚看起来会很难，但经过一定的思考后，就会觉得应该没那么难吧？图像灰度的边缘不是可以检测出来吗？只要找到小数字和大数字相邻的地方就是边缘了呀。嗯，是的，边缘特征检测的方法目前已经可以公式计算和实现了。那边缘得到了，将这些边缘片段连接到一起，那物体的轮廓或者说某种直观的物体表达形式是不是就出来了呢？嗯，你是聪明的，这种直观的想法造就了目前很多计算机视觉的算法。很遗憾的是，这些算法在一些合成图像或者一些在高度限制环境中的图像才有好的表现，但对于无约束的自然图像来说，他们的表现就很一般。经过多年的研究，有一个结论：real-world images is extremely difficult！就算是找到物体轮廓这样一个任务也变得非常困难，因为一般物体的轮廓在物理世界中是不够清晰的，如下图：\n这不就是一个杯子吗？你看，你看，你就只在乎结果！！！我们慢慢分析下嘛。图中放大了两个地方，一个是本来杯子有边缘的地方，在图像中却没有了边缘，因为背景和杯子很像，边缘不清晰了。另一个是本来杯子没有边缘的地方，在图像中因为光照的关系，却变得有边缘了。这无中生有，有中化无，大自然的招数，怎么破！\n也许这还不能说服你。我们看看下图。我们教小孩子识图的时候，是不会拿个这么变态的图来吓他的。但不拿这个给你看，是说服不了你的。说服什么？就一点：视觉有多难，你的视觉系统有多牛掰！\n不要惊讶。实际上一个小孩在六岁之前，就已经认识一万到三万类的物体了。而这个物理世界，大约也就是三万种物体。你说，目前的算法如何才能够强大到囊括这三万种物体（包括能识别它们在所有环境下的所有形态）。那你会想，那人为什么可以呢？你终于问出这个问题了，累觉不爱。因为人有种能从少量样本学习的能力！一类物体，变化万千。但人只要认识其中一两个，剩下的就不攻自破了。融会贯通，举一反三，乃人类与生俱来的才能！说这个好像又没什么实际意义，是吧。要怎么利用这个来指导目前的视觉才是王道。但鄙人才疏学浅，往下就讲不了了。这也是生物视觉和机器视觉研究者穷尽一生想要共同探索的神秘世界。希望有生之年，能瞥见这美丽的星空吧。\n我有时候会想。人从呱呱坠地开始，花了六年的时间，来建立自己强大的大脑，也就是硬件设施。在这里，我还是要对人脑的硬件设备惊叹一番。据了解（这三个字就表示了不知道对不对，呵呵），人脑的CPU主频是320GHZ，目前电脑因为半导体工艺的限制，能做到3GHZ左右每个CPU核。所以咱们搞个300个CPU就可以了么？NO，NO。要运算，每个CPU之间还得通信，访问内存数据等，这些通信带宽的限制才是这种大集群的最大瓶颈。但人脑呢？它的存储体您得按斤算，哈哈。据猜测（哈哈，这算严谨么），人脑存储大概10^15GB，这还不是牛的，最牛的来了，这些空间可以作为内存和缓存使用！不要问我这代表什么了！怀疑怀疑为什么自己老忘记前天老师上课讲的东西，我昨天还记得的啊，怎么就忘了呢，这海马体废了么！哈哈，可能是忘了保存到硬盘了！然后大脑一死机，就没有了！所以记得每天要记得好好吃饭，补充能量。再来膜拜下人眼。人眼是很不可思议的精密系统，它能自动对焦，曝光自动补偿，自动运动模糊处理，目前所有让凡人穷三代的单反都望尘莫及的！还包括免费赠送的夜视功能……最神奇的是，实现这些功能，不需要升级windows和安装任何驱动，不需要耗电！每个人出生的时候就被装配了所有这些功能。所以在这里也呼吁大家好好珍爱如此高档的设备吧，追剧不要太晚，打游戏不要太晚，上班看电脑多用眼药水，哈哈。\n扯远了。回来说说大脑的系统和软件吧。我们除了睡觉的时间，每时每刻都在接收输入，然后通过自己强大的硬件设施来处理输入。非常关键的一点是，大脑里面会慢慢的建立历史学习到的东西的知识关联库。随着知识的扩充与完善，大脑的网络越来越复杂。神经元个数，神经元连接的个数，千丝万缕却又理不清。但对于计算机呢？我们的硬件设施呢？最牛掰的就是超算了。我们也建立了一个很大的网络，与人的大脑相当的网络，然后期待着几天、几个月的时间就可以让我们的计算机能达到人脑的水平，这会不会太苛刻了呢？当然了，这其中有几点不公平对比的差别。一是大脑的高层结构和运作机制实际上是不清晰的，所以目前计算机所做的高层模拟是否与大脑高层类似，这个也没有答案。二是到底是计算机快还是人脑快？做同样的事情，超算运作需要消耗一个城市一个月的电量，但人脑只需要你吃个鸡腿补充下就好了。OK，what is your point?! I am Sorry!\n生物上的启发对计算机视觉的研究者是非常重要的。很多视觉的处理过程和算法就是模拟生物视觉系统来进行设计的，而且也显示出了有效性。但到目前为止，人类对自身大脑所知还是九牛一毛，更谈不上对视觉更有力的指导了。但实际上，视觉计算理论和生物界实验没有什么谁靠谁，谁欠谁的说法。这里面官方点算就是相辅相成：视觉计算理论可以指导生物实验性研究，实验性研究结果又可以指导计算理论的研究。"}
{"content2":"一、背景\n近期一直再补图像处理方面的基础，偶然看到了布朗大学的计算机视觉课程，所以就计划把这些课程的核心内容简单记录下来，也顺便为后边学习的人提供一个参考，下面直接开始。\n二、课程介绍\n这门课程是布朗大学2010年的春季课程，布朗大学毕竟也是藤校之一，而且这门课程的内容还挺丰富的，所以非常值得一看。这么课程的参考书所用的是《Computer Vision: Algorithms and Applications》，作者：Richard Szeliski，课程内容主要以这部分书为主。课程的主要链接为：http://cs.brown.edu/courses/csci1950-g/。\n另外，由于课程资源下载特别慢，所以我把这本参考书还有课程所有ppt都打包在了一起，上传到了CSDN上，如果需要的话可以自己下载，下载地址：https://download.csdn.net/download/z704630835/11135666。因为这部分工作自己也付出了一些时间来整理，所以就收点积分吧。\n下面开始介绍主要课程内容。个别课程没有ppt，所以目前没有讲解。\n三、课程内容\n基础部分\n1. Introduction to computational photography（计算图形学介绍，对应参考书第一章）\n第一节主要讲了一点点计算机视觉的历史和这门课老师的介绍，老师是James Hays，他是在MIT做的博后，主要研究图形学和视觉，一个是IM2GPS（类似于给你一张图片，你找出这张图片是在哪拍的），一个是目标检测。\n2. Cameras and optics（相机和光学，对应参考书第二章）\n第二节主要讲了小孔成像，还有相机的一些工作原理，聚焦和失焦的现象。还有镜头的一些相关知识，比如光圈的大小，瞬时视场角FOV，扭曲等现象。\n3. Capturing light, man vs machine（人和机器对光线的捕捉，对应参考书第二章）\n第三节主要讲了传感器形成图像的方式（扫描），采样量化，人眼结构，电磁光谱，颜色空间（RGB和HSV），图像下采样，图像高斯滤波，图像金字塔。\n同时这一章节布置了project #1：如何比较RGB通道？\n4. Point Processing and DCT/JPEG（点处理及图像格式，对应参考书第三章第一节）\n点处理包括两类，一类是滤波filter，一类是扭曲war。点处理包括图像拉伸，反转，直方图均衡化。\n另外还DCT（离散余弦变换）是用于JPEG图像的压缩方法。这节大概就讲了这些。\n5. Sampling and reconstruction（采样和重建，对应参考书第三章第二节）\n采样过程是从连续值到离散值的过程，不仅可以对图像进行采样，也可以对音频进行采样。\n重建就是将离散值模拟为连续值得过程，这里还介绍了频谱混叠（以前学信号处理的时候还遇到过），交叉相关滤波，高斯滤波，均值滤波，卷积操作，线性滤波，模糊，锐化，边缘检测，梯度，中值滤波等（这节的所有概念都是从信号的角度来描述的）。\n6. Frequency Domain（频域，对应参考书第三章第三节）\n提到频域就不得不说傅里叶变换了（数字信号处理里面的核心内容），另外还涉及到卷积，低通，高通，高斯金字塔，拉普拉斯金字塔，图像梯度（这里还蛮重要的，今后用的也挺多），噪声的处理方法，形态学处理及其预备知识（膨胀腐蚀，开运算闭运算），形态学算法提取边界信息。\n7. Blending and compositing（图像融合，对应参考书第九章第三节）\n最近我自己也在做这方面的工作。image blending一般指的是图像融合，作者给了一个示意图：\n而image compositing指的是图像合成，这两者稍微有一点点区别，下面是图像合成的示意图：\n先来看看融合，融合需要一个trimap，也就是三分图，因为融合是一个病态过程，必须对该过程进行约束才能求解。作者介绍了融合的流程，还有一些融合的方法：金字塔融合，双波段融合，图割法，梯度融合， 无缝融合。\n8. Morphology（形态学，对应参考书第三章第二节）\n其实第6节提到了，这节课又过了一遍。\n9. Image Warping（图像校正，对应参考书第三章第五节）\n这节主要针对六种图像扭曲进行了数学校正的公式推导，先来看看六种扭曲：\n这六种扭曲实在不知道该怎么用专业术语来翻译（以前遥感里学过由于飞行器姿态问题产生的扭曲包括，俯仰，翻滚，航偏，和这里的几乎不对应），之后是对这六种变形的进一步数学推导。\n图像校正的方式有两种，一种是前向校正（Forward warping），一种是逆向校正（Inverse warping），计算待校正图像和原图像之间的目标关系时，可以使用warping triangles方法。\n10. Image morphing（图像渐变）\n先来理解下什么是图像渐变，假如现在给定一张人脸和一张豹子脸：\n那么渐变的效果则为：\n渐变实际上是目标均值(object averaging)，但这不是两张目标图像的平均，而是两个目标平均的图像。当然这个功能的实现，其实只用找到两个目标对应的关键点，然后计算关键点对应的平均位置，两张目标图像分别向这个平均位置做一个warping就可以了。\n11. Matting and Transparancy（抠图和透明度，对应参考书第十章第四节）\n我自己最近做了很多image matting方面的工作，先来看下理想的matting效果：\n任何一张图都可以表示为： I = F * α + B * （1 - α） ，其中I表示一张图片，F表示图像的前景，可以理解为我们想要扣出来的目标区域，B表示背景，可以理解为我们不需要的区域，α表示透明度。image matting其实要解决的就是求解F, B, α这三个参数，然而只有一个方程的约束，那么我们求得的解必然是病态的，因此实际做的过程中还需要trimap进行进一步约束。当然这也是后话了，关于如何制作trimap，一直是问题的关键，好在现在有一些不错的方法可以来制作。深度学习在这方面也有一定的成果了，最新的可以参考2017年提出的deep image mattiing。\n关于透明度，其实是image matting的进一步加难。因为有的时候需要考虑到透明物体发生光线折射的问题，比如：\n此时，上面的方程变化为了：\n相当于增加了一个光线参数φ，所以难度自然更大一些，目前的解决办法一般是高频光流法（High Frequency Illumination）\n12. Modeling light and lightfields（模型光线，对应参考书第十三章第三节）\n这一章没有过多要介绍的。\n13. Homographies and mosaics（图像拼接和镶嵌，对应参考书第九章第一节）\n这一章节对图像镶嵌和拼接做了介绍，要做图像的镶嵌和拼接，其实关键还是在于特征点的匹配。比如多张图片的拼接：\n虽然多张图片能够拼接，但是拼接好的图片变形非常严重，这里涉及到了球面变换：\n14. Automatic image correspondence（自动图像响应，对应参考书第九章第一节）\n15. RANSAC and mosaic wrapup（随机样本匹配和马赛克，对应参考书第九章第一节）\n这节很多内容和第13节的内容一样，不过这节更偏向于RANSAC算法。放一张照片吧：\n16. Capturing and compressing high dynamic range（捕捉和高动态范围压缩，对应参考书第十章第二节）\n我们一般看到的现实世界的灰度值范围远远大于[0, 255]，而我们的图像一般用[0, 255]范围表示，这时候就要用的HDR。\n17. Local tone mapping and bilateral filters（局部映射和双边滤波）\n18. Image-based lighting\n这一章主要讲的还是环境光线的问题。\n处理好了光线场，就可以做出很多不可思议的图：\n19. Photo quality assessment\n这节比较轻松，主要讲如何通过摄影获得高质量的图像，核心就是构图：\n数据驱动方法Data-driven methods部分\n20. video and texture\n这节涉及到的概念包括：markov链，纹理。内容比较少。\n21. texture synthesis and filling\n这部分内容我感觉就是poisson image editing（泊松融合）：\n22. Scene Completion\n23. leveraging the Internet\n这一部分主要涉及到的概念包括：自动上色（Automatic Colorization Result），自动转向（Automatic Orientation），场景描述子（Gist Scene Descriptor），场景匹配（Scene matching ），SIFT（Scale Invariant Feature Transform），方向估计（Orientation assignment）\n24. features and image comparisons\n这一节内容是im2gps的基础内容，首先非常直白的提出，如何比较两张图片是否相似或者一致，前面提到了SIFT可以用于特征点的检测，这一节又提出了用直方图的思想来比较两张图，比如像这样：\n既然能够用直方图来比较两幅图是否类似或一致，那这个思路就应该可以用于im2gps中，我们以Flickr（一个摄影论坛，不过需要翻墙才能登陆）上的照片为例，Flickr上全球的照片分布为：\n在im2gps具体做的过程中，我们还需要补充颜色信息和纹理信息，比如下面这张图片的识别结果：\n再比如下面这张建筑：\n对此，有人提出疑问，im2gps只能识别典型地标吗？它只能进行场景分类吗？它的性能是否依赖于测试集？\n经过进一步的测试得到结论，im2gps识别正确的结果中，有58%是典型地标建筑：\n当然，单纯用im2gps的精度很低，添加一些image features之后的精度会大大提高：\n最后的效果会稍微好点：\n对比仅仅使用im2gps，精度能有很大程度的提升：\n25. more features with im2gps\n这个还挺有意思的，根据图片来判断是在哪里拍摄的，最近这几年好像也有了这方面的研究，不过研究还是挺少的，下面给张有意思的图：\n啊"}
{"content2":"一、转自： http://blog.csdn.net/dcraw/article/details/7617891\n二、转自：http://blog.csdn.net/qq_26499769/article/details/78989088\n遇到了大神一样的博客存在~总结的非常全面到位！\n一、由于新浪爱问关闭了，把文章都放在了百度云盘里\nhttp://pan.baidu.com/s/1hqf4SkO\nhttp://pan.baidu.com/s/1hq253z2\n历时一个多月，终于用业余时间把这些资料整理出来了。以后可能会有些小修小补，但不会有太大的变化了。万里长征走完了第一步，剩下的就是理解和消化了。借新浪ishare共享出来，希望能够对你的科研也有一定的帮助。现在已经把所有的文章打包，分成了16个子文件，欢迎整体下载。\n图像处理与计算机视觉：基础，经典以及最近发展（1）序\n图像处理与计算机视觉：基础，经典以及最近发展（2）图像处理与计算机视觉相关的书籍\n图像处理与计算机视觉：基础，经典以及最近发展（3）计算机视觉中的信号处理与模式识别\n图像处理与计算机视觉：基础，经典以及最近发展（4）图像处理与分析\n图像处理与计算机视觉：基础，经典以及最近发展（5）计算机视觉\n下面这个是以前整理的一个版本，按年份归类的，不全\n图像处理和计算机视觉中的经典论文（部分）\nUIUC的Jia-Bin Huang同学整理很多计算机视觉的资源，主要是代码，很全。\n同样是UIUC（现在在IBM）的Cao liangliang同学也整理了一些资料，很不错。主要包括\nBoosting (updated 08/2008)\nSalient patches (updated 08/2008)  实际上就是特征提取，检测和匹配\nMean Shift (updated 2008)\nAction recognition (updated 2009)\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n二、图像处理与计算机视觉 基础、经典以及最近发展\n图像处理与计算机视觉基础，经典以及最近发展\nBy xdyang（杨晓冬xdyang.ustc@gmail.com）\n一、 绪论\n1. 为什么要写这篇文章\n从2002年到现在，接触图像快十年了。虽然没有做出什么很出色的工作，不过在这个领域摸爬滚打了十年之后，发现自己对图像处理和计算机视觉的感情越来越深厚。下班之后看看相关的书籍和文献是一件很惬意的事情。平常的一大业余爱好就是收集一些相关的文章，尤其是经典的文章，到现在我的电脑里面已经有了几十G的文章。写这个文档的想法源于我前一段时间整理文献时的一个突发奇想，既然有这个多文献，何不整理出其中的经典，抓住重点来阅读，同时也可以共享给大家。于是当时即兴写了一个《图像处理与计算机视觉中的经典论文》。现在来看，那个文档写得很一般，所共享的论文也非常之有限。就算如此，还是得到了一些网友的夸奖，心里感激不尽。因此，一直想下定决心把这个工作给完善，力求做到尽量全面。\n本文是对现有的图像处理和计算机视觉的经典书籍（后面会有推荐）的一个补充。一般的图像处理书籍都是介绍性的介绍某个方法，在每个领域内都会引用几十上百篇参考文献。有时候想深入研究这个领域的时候却发现文献太多，不知如何选择。但实际上在每个领域都有那么三五篇抑或更多是非读不可的经典文献。这些文献除了提出了很经典的算法，同时他们的Introduction和Related work也是对所在的领域很好的总结。读通了这几篇文献也就等于深入了解了这个领域，比单纯的看书收获要多很多。写本文的目的就是想把自己所了解到的各个领域的经典文章整理出来,不用迷失在参考文献的汪洋大海里。\n2. 图像处理和计算机视觉的分类\n按照当前流行的分类方法，可以分为以下三部分：\nA.图像处理：对输入的图像做某种变换，输出仍然是图像，基本不涉及或者很少涉及图像内容的分析。比较典型的有图像变换，图像增强，图像去噪，图像压      缩，图像恢复，二值图像处理等等。基于阈值的图像分割也属于图像处理的范畴。一般处理的是单幅图像。\nB.图像分析：对图像的内容进行分析，提取有意义的特征，以便于后续的处理。处理的仍然是单幅图像。\nC.计算机视觉：对图像分析得到的特征进行分析，提取场景的语义表示，让计算机具有人眼和人脑的能力。这时处理的是多幅图像或者序列图像，当然也包括部分单幅图像。\n关于图像处理，图像分析和计算机视觉的划分并没有一个很统一的标准。一般的来说，图像处理的书籍总会或多或少的介绍一些图像分析和计算机视觉的知识，比如冈萨雷斯的数字图像处理。而计算机视觉的书籍基本上都会包括图像处理和图像分析，只是不会介绍的太详细。其实图像处理，图像分析和计算机视觉都可以纳入到计算机视觉的范畴：图像处理->低层视觉（low level vision），图像分析->中间层视觉（middle level vision），计算机视觉->高层视觉（high level vision）。这是一般的计算机视觉或者机器视觉的划分方法。在本文中，仍然按照传统的方法把这个领域划分为图像处理，图像分析和计算机视觉。\n3. 图像处理和计算机视觉开源库以及编程语言选择\n目前在图像处理中有两种最重要的语言：c/c++和matlab。它们各有优点：c/c++比较适合大型的工程，效率较高，而且容易转成硬件语言，是工业界的默认语言之一。而matlab实现起来比较方便，适用于算法的快速验证，而且matlab有成熟的工具箱可以使用，比如图像处理工具箱，信号处理工具箱。它们有一个共同的特点：开源的资源非常多。在学术界matlab使用的非常多，很多作者给出的源代码都是matlab版本。最近由于OpenCV的兴起和不断完善，c/c++在图像处理中的作用越来越大。总的来说，c/c++和matlab都必须掌握，最好是精通，当然侧重在c/c++上对找工作会有很大帮助。\n至于开源库，个人非常推荐OpenCV，主要有以下原因：\n（1）简单易入手。OpenCV进入OpenCV2.x的时代后，使用起来越来越简单,接口越来越傻瓜化，越来越matlab化。只要会imread,imwrite,imshow和了解Mat的基本操作就可以开 始入手了。\n（2）OpenCV有一堆图像处理和计算机视觉的大牛在维护，bug在逐步减少，每个新的版本都会带来不同的惊喜。而且它已经或者逐步在移植到不懂的平台,并提供了对Python的很好的支持。\n（3）在opencv上可以尝试各种最新以及成熟的技术，而不需要自己从头去写，比如人脸检测（Harr，LBP），DPM（Latent SVM），高斯背景模型，特征检测，聚类，hough变换等等 。而且它还支持各种机器学习方法（SVM，NN，KNN，决策树，Boosting等），使用起来很简单。\n（4）文档内容丰富，并且给出了很多示例程序。当然也有一些地方文档描述不清楚，不过看看代码就很清楚了。\n（5）完全开源。可以从中间提取出任何需要的算法。\n（6）从学校出来后，除极少数会继续在学术圈里，大部分还是要进入工业界。现在在工 业界，c/c++仍是主流，很多公司都会优先考虑熟悉或者精通OpenCV的。事实上，在学术界，现在OpenCV也大有取代matlab之势。以前的demo或者source code，很多作者都愿意给出matlab版本的，然后别人再呼哧呼哧改成c版本的。现在作者干脆给出c/c++版本，或者自己集成到OpenCV中去，这样能快速提升自己的影响力。\n如果想在图像处理和计算机视觉界有比较深入的研究，并且以后打算进入这个领域工作的话，建议把OpenCV作为自己的主攻方向。如果找工作的时候敢号称自己精通OpenCV的话，肯定可以找到一份满意的工作。\n4. 本文的特点和结构，以及适合的对象\n在本文面向的对象是即将进入或者刚刚进入图像处理和计算机视觉领域的童鞋，可以在阅读书籍的同时参阅这些文献，能对书中提到的算法有比较深刻的理解。由于本文涉及到的范围比较广，如果能对计算机视觉的资深从业者也有一定的帮助，我将倍感欣慰。为了不至太误人子弟，每一篇文章都或多或少的看了一下，最不济也看了摘要(这句话实在整理之前写的，实际上由于精力有限，好多文献都只是大概扫了一眼，然后看了看google的引用数，一般在1000以上就放上来了，把这些文章细细品味一遍也是我近一两年之内的目标)。在成文的过程中，我本人也受益匪浅，希望能对大家也有所帮助。\n由于个人精力和视野的关系，有一些我未涉足过的领域不敢斗胆推荐，只是列出了一些引用率比较高的文章，比如摄像机标定和立体视觉。不过将来，由于工作或者其他原因，这些领域也会接触到，我会逐步增减这些领域的文章。尽管如此，仍然会有疏漏，忘见谅。同时文章的挑选也夹带了一些个人的喜好，比如我个人比较喜欢low level方向的，尤其是IJCV和PAMI上面的文章，因此这方面也稍微多点，希望不要引起您的反感。如果有什么意见或者建议，欢迎mail我。文章和资源我都会在我的csdn blog和sina ishare同步更新。此申明：这些论文的版权归作者及其出版商所有，请勿用于商业目的。\n个人blog：       http://blog.csdn.NET/dcraw\n新浪iask地址：http://iask.sina.com.cn/u/2252291285/ish?folderid=868438\n本文的安排如下。第一部分是绪论。第二部分是图像处理中所需要用到的理论基础，主要是这个领域所涉及到的一些比较好的参考书籍。第三部分是计算机视觉中所涉及到的信号处理和模式识别文章。由于图像处理与图像分析太难区分了，第四部分集中讨论了它们。第五部分是计算机视觉部分。最后是小结。\n二、 图像处理与计算机视觉相关的书籍\n1. 数学\n我们所说的图像处理实际上就是数字图像处理，是把真实世界中的连续三维随机信号投影到传感器的二维平面上，采样并量化后得到二维矩阵。数字图像处理就是二维矩阵的处理，而从二维图像中恢复出三维场景就是计算机视觉的主要任务之一。这里面就涉及到了图像处理所涉及到的三个重要属性：连续性，二维矩阵，随机性。所对应的数学知识是高等数学（微积分），线性代数（矩阵论），概率论和随机过程。这三门课也是考研数学的三个组成部分，构成了图像处理和计算机视觉最基础的数学基础。如果想要更进一步，就要到网上搜搜林达华推荐的数学书目了。\n2. 信号处理\n图像处理其实就是二维和三维信号处理，而处理的信号又有一定的随机性，因此经典信号处理和随机信号处理都是图像处理和计算机视觉中必备的理论基础。\n2.1经典信号处理\n信号与系统(第2版) Alan V.Oppenheim等著 刘树棠译\n离散时间信号处理(第2版) A.V.奥本海姆等著 刘树棠译\n数字信号处理:理论算法与实现 胡广书 (编者)\n2.2随机信号处理\n现代信号处理 张贤达著\n统计信号处理基础:估计与检测理论 Steven M.Kay等著 罗鹏飞等译\n自适应滤波器原理(第4版) Simon Haykin著 郑宝玉等译\n2.3 小波变换\n信号处理的小波导引:稀疏方法(原书第3版) tephane Malla著, 戴道清等译\n2.4 信息论\n信息论基础(原书第2版) Thomas M.Cover等著 阮吉寿等译\n3. 模式识别\nPattern Recognition and Machine Learning Bishop, Christopher M. Springer\n模式识别(英文版)(第4版) 西奥多里德斯著\nPattern Classification (2nd Edition) Richard O. Duda等著\nStatistical Pattern Recognition, 3rd Edition Andrew R. Webb等著\n模式识别(第3版) 张学工著\n4. 图像处理与计算机视觉的书籍推荐\n图像处理，分析与机器视觉 第三版 Sonka等著 艾海舟等译\nImage Processing, Analysis and Machine Vision\n( 附：这本书是图像处理与计算机视觉里面比较全的一本书了，几乎涵盖了图像视觉领域的各个方面。中文版的个人感觉也还可以，值得一看。)\n数字图像处理 第三版 冈萨雷斯等著\nDigital Image Processing\n(附：数字图像处理永远的经典，现在已经出到了第三版，相当给力。我的导师曾经说过，这本书写的很优美，对写英文论文也很有帮助，建议购买英文版的。)\n计算机视觉：理论与算法 Richard Szeliski著\nComputer Vision: Theory and Algorithm\n(附：微软的Szeliski写的一本最新的计算机视觉著作。内容非常丰富，尤其包括了作者的研究兴趣，比如一般的书里面都没有的Image Stitching和                       Image Matting等。这也从另一个侧面说明这本书的通用性不如Sonka的那本。不过作者开放了这本书的电子版，可以有选择性的阅读。\nhttp://szeliski.org/Book/\nMultiple View Geometry in Computer Vision 第二版Harley等著\n引用达一万多次的经典书籍了。第二版到处都有电子版的。第一版曾出过中文版的，后来绝版了。网上也可以找到中英文版的电子版。)\n计算机视觉：一种现代方法 DA Forsyth等著\nComputer Vision: A Modern Approach\nMIT的经典教材。虽然已经过去十年了，还是值得一读。期待第二版\nMachine vision: theory, algorithms, practicalities 第三版 Davies著\n(附：为数不多的英国人写的书，偏向于工业应用。)\n数字图像处理 第四版 Pratt著\nDigital Image Processing\n(附：写作风格独树一帜，也是图像处理领域很不错的一本书。网上也可以找到非常清晰的电子版。)\n5. 小结\n罗嗦了这么多，实际上就是几个建议：\n（1）基础书千万不可以扔，也不能低价处理给同学或者师弟师妹。不然到时候还得一本本从书店再买回来的。钱是一方面的问题，对着全新的书看完全没有看自己当年上过的课本有感觉。\n（2）遇到有相关的课，果断选修或者蹭之，比如随机过程，小波分析，模式识别，机器学习，数据挖掘，现代信号处理甚至泛函。多一些理论积累对将来科研和工作都有好处。\n（3）资金允许的话可以多囤一些经典的书，有的时候从牙缝里面省一点都可以买一本好书。不过千万不要像我一样只囤不看。\n三、 计算机视觉中的信号处理与模式识别\n从本章开始，进入本文的核心章节。一共分三章，分别讲述信号处理与模式识别，图像处理与分析以及计算机视觉。与其说是讲述，不如说是一些经典文章的罗列以及自己的简单点评。与前一个版本不同的是，这次把所有的文章按类别归了类，并且增加了很多文献。分类的时候并没有按照传统的分类方法，而是划分成了一个个小的门类，比如SIFT，Harris都作为了单独的一类，虽然它们都可以划分到特征提取里面去。这样做的目的是希望能突出这些比较实用且比较流行的方法。为了以后维护的方便，按照字母顺序排的序。\n1. Boosting\nBoosting是最近十来年来最成功的一种模式识别方法之一，个人认为可以和SVM并称为模式识别双子星。它真正实现了“三个臭皮匠，赛过诸葛亮”。只要保证每个基本分类器的正确率超过50%，就可以实现组合成任意精度的分类器。这样就可以使用最简单的线性分类器。Boosting在计算机视觉中的最成功的应用无疑就是Viola-Jones提出的基于Haar特征的人脸检测方案。听起来似乎不可思议，但Haar+Adaboost确实在人脸检测上取得了巨大的成功，已经成了工业界的事实标准，并且逐步推广到其他物体的检测。\nRainer Lienhart在2002 ICIP发表的这篇文章是Haar+Adaboost的最好的扩展，他把原始的两个方向的Haar特征扩展到了四个方向，他本人是OpenCV积极的参与者。现在OpenCV的库里面实现的Cascade Classification就包含了他的方法。这也说明了盛会（如ICIP，ICPR，ICASSP）也有好文章啊，只要用心去发掘。\n[1997] A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\n[1998] Boosting the margin A new explanation for the effectiveness of voting methods\n[2002 ICIP TR] Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection\n[2003] The Boosting Approach to Machine Learning An Overview\n[2004 IJCV] Robust Real-time Face Detection\n2. Clustering\n聚类主要有K均值聚类，谱聚类和模糊聚类。在聚类的时候如果自动确定聚类中心的数目是一个一直没有解决的问题。不过这也很正常，评价标准不同，得到的聚类中心数目也不一样。不过这方面还是有一些可以参考的文献，在使用的时候可以基于这些方法设计自己的准则。关于聚类，一般的模式识别书籍都介绍的比较详细，不过关于cluster validity讲的比较少，可以参考下面的文章看看。\n[1989 PAMI] Unsupervised Optimal Fuzzy Clustering\n[1991 PAMI] A validity measure for fuzzy clustering\n[1995 PAMI] On cluster validity for the fuzzy c-means model\n[1998] Some New Indexes of Cluster Validity\n[1999 ACM] Data Clustering A Review\n[1999 JIIS] On Clustering Validation Techniques\n[2001] Estimating the number of clusters in a dataset via the Gap statistic\n[2001 NIPS] On Spectral Clustering\n[2002] A stability based method for discovering structure in clustered data\n[2007] A tutorial on spectral clustering\n3. Compressive Sensing\n最近大红大紫的压缩感知理论。\n[2006 TIT] Compressed Sensing\n[2008 SPM] An Introduction to Compressive Sampling\n[2011 TSP] Structured Compressed Sensing From Theory to Applications\n4. Decision Trees\n对决策树感兴趣的同学这篇文章是非看不可的了。\n[1986] Introduction to Decision Trees\n5. Dynamical Programming\n动态规划也是一个比较使用的方法，这里挑选了一篇PAMI的文章以及一篇Book Chapter\n[1990 PAMI] using dynamic programming for solving variational problems in vision\n[Book Chapter] Dynamic Programming\n6. Expectation Maximization\nEM是计算机视觉中非常常见的一种方法，尤其是对参数的估计和拟合，比如高斯混合模型。EM和GMM在Bishop的PRML里单独的作为一章，讲的很不错。关于EM的tutorial，网上也可以搜到很多。\n[1977] Maximum likelihood from incomplete data via the EM algorithm\n[1996 SPM] The Expectation-Maximzation Algorithm\n7. Graphical Models\n伯克利的乔丹大师的Graphical Model，可以配合这Bishop的PRML一起看。\n[1999 ML] An Introduction to Variational Methods for Graphical Models\n8. Hidden Markov Model\nHMM在语音识别中发挥着巨大的作用。在信号处理和图像处理中也有一定的应用。最早接触它是跟小波和检索相关的，用HMM来描述小波系数之间的相互关系，并用来做检索。这里提供一篇1989年的经典综述，几篇HMM在小波，分割，检索和纹理上的应用以及一本比较早的中文电子书，现在也不知道作者是谁，在这里对作者表示感谢。\n[1989 ] A tutorial on hidden markov models and selected applications in speech recognition\n[1998 TSP] Wavelet-based statistical signal processing using hidden Markov models\n[2001 TIP] Multiscale image segmentation using wavelet-domain hidden Markov models\n[2002 TMM] Rotation invariant texture characterization and retrieval using steerable wavelet-domain hidden Markov models\n[2003 TIP] Wavelet-based texture analysis and synthesis using hidden Markov models\nHmm Chinese book.pdf\n9. Independent Component Analysis\n同PCA一样，独立成分分析在计算机视觉中也发挥着重要的作用。这里介绍两篇综述性的文章，最后一篇是第二篇的TR版本，内容差不多，但比较清楚一些。\n[1999] Independent Component Analysis A Tutorial\n[2000 NN] Independent component analysis algorithms and applications\n[2000] Independent Component Analysis Algorithms and Applications\n10. Information Theory\n计算机视觉中的信息论。这方面有一本很不错的书Information Theory in Computer Vision and Pattern Recognition。这本书有电子版，如果需要用到的话，也可以参考这本书。\n[1995 NC] An Information-Maximization Approach to Blind Separation and Blind Deconvolution\n[2010] An information theory perspective on computational vision\n11. Kalman Filter\n这个话题在张贤达老师的现代信号处理里面讲的比较深入，还给出了一个有趣的例子。这里列出了Kalman的最早的论文以及几篇综述，还有Unscented Kalman Filter。同时也有一篇Kalman Filter在跟踪中的应用以及两本电子书。\n[1960 Kalman] A New Approach to Linear Filtering and Prediction Problems Kalman\n[1970] Least-squares estimation_from Gauss to Kalman\n[1997 SPIE] A New Extension of the Kalman Filter to Nonlinear System\n[2000] The Unscented Kalman Filter for Nonlinear Estimation\n[2001 Siggraph] An Introduction to the Kalman Filter_full\n[2003] A Study of the Kalman Filter applied to Visual Tracking\n12. Pattern Recognition and Machine Learning\n模式识别名气比较大的几篇综述\n[2000 PAMI] Statistical pattern recognition a review\n[2004 CSVT] An Introduction to Biometric Recognition\n[2010 SPM] Machine Learning in Medical Imaging\n13. Principal Component Analysis\n著名的PCA，在特征的表示和特征降维上非常有用。\n[2001 PAMI] PCA versus LDA\n[2001] Nonlinear component analysis as a kernel eigenvalue problem\n[2002] A Tutorial on Principal Component Analysis\n[2009] A Tutorial on Principal Component Analysis\n[2011] Robust Principal Component Analysis\n[Book Chapter] Singular Value Decomposition and Principal Component Analysis\n14. Random Forest\n随机森林\n[2001 ML] Random Forests\n15. RANSAC\n随机抽样一致性方法，与传统的最小均方误差等完全是两个路子。在Sonka的书里面也有提到。\n[2009 BMVC] Performance Evaluation of RANSAC Family\n16. Singular Value Decomposition\n对于非方阵来说，就是SVD发挥作用的时刻了。一般的模式识别书都会介绍到SVD。这里列出了K-SVD以及一篇Book Chapter\n[2006 TSP] K-SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation\n[Book Chapter] Singular Value Decomposition and Principal Component Analysis\n17. Sparse Representation\n这里主要是Proceeding of IEEE上的几篇文章\n[2009 PAMI] Robust Face Recognition via Sparse Representation\n[2009 PIEEE] Image Decomposition and Separation Using Sparse Representations An Overview\n[2010 PIEEE] Dictionaries for Sparse Representation Modeling\n[2010 PIEEE] It's All About the Data\n[2010 PIEEE] Matrix Completion With Noise\n[2010 PIEEE] On the Role of Sparse and Redundant Representations in Image Processing\n[2010 PIEEE] Sparse Representation for Computer Vision and Pattern Recognition\n[2011 SPM] Directionary Learning\n18. Support Vector Machines\n[1998] A Tutorial on Support Vector Machines for Pattern Recognition\n[2004] LIBSVM A Library for Support Vector Machines\n19. Wavelet\n在小波变换之前，时频分析的工具只有傅立叶变换。众所周知，傅立叶变换在时域没有分辨率，不能捕捉局部频域信息。虽然短时傅立叶变换克服了这个缺点，但只能刻画恒定窗口的频率特性，并且不能很好的扩展到二维。小波变换的出现很好的解决了时频分析的问题，作为一种多分辨率分析工具，在图像处理中得到了极大的发展和应用。在小波变换的发展过程中，有几个人是不得不提的，Mallat， Daubechies，Vetteri， M.N.Do， Swelden，Donoho。Mallat和Daubechies奠定了第一代小波的框架，他们的著作更是小波变换的必读之作，相对来说，小波十讲太偏数学了，比较难懂。而Mallat的信号处理的小波导引更偏应用一点。Swelden提出了第二代小波，使小波变换能够快速方便的实现，他的功劳有点类似于FFT。而Donoho，Vetteri，Mallat及其学生们提出了Ridgelet, Curvelet, Bandelet,Contourlet等几何小波变换，让小波变换有了方向性，更便于压缩，去噪等任务。尤其要提的是M.N.Do，他是一个越南人，得过IMO的银牌，在这个领域著作颇丰。我们国家每年都有5个左右的IMO金牌，希望也有一两个进入这个领域，能够也让我等也敬仰一下。而不是一股脑的都进入金融，管理这种跟数学没有多大关系的行业，呵呵。很希望能看到中国的陶哲轩，中国的M.N.Do。\n说到小波，就不得不提JPEG2000。在JPEG2000中使用了Swelden和Daubechies提出的用提升算法实现的9/7小波和5/3小波。如果对比JPEG和JPEG2000，就会发现JPEG2000比JPEG在性能方面有太多的提升。本来我以为JPEG2000的普及只是时间的问题。但现在看来，这个想法太Naive了。现在已经过去十几年了，JPEG2000依然没有任何出头的迹象。不得不说，工业界的惯性力量太强大了。如果以前的东西没有什么硬伤的话，想改变太难了。不巧的是，JPEG2000的种种优点在最近的硬件上已经有了很大的提升。压缩率？现在动辄1T，2T的硬盘，没人太在意压缩率。渐进传输？现在的网速包括无线传输的速度已经相当快了，渐进传输也不是什么优势。感觉现在做图像压缩越来越没有前途了，从最近的会议和期刊文档也可以看出这个趋势。不管怎么说，JPEG2000的Overview还是可以看看的。\n[1989 PAMI] A theory for multiresolution signal decomposition__the wavelet representation\n[1996 PAMI] Image Representation using 2D Gabor Wavelet\n[1998 ] FACTORING WAVELET TRANSFORMS INTO LIFTING STEPS\n[1998] The Lifting Scheme_ A Construction Of Second Generation Wavelets\n[2000 TCE] The JPEG2000 still image coding system_ an overview\n[2002 TIP] The curvelet transform for image denoising\n[2003 TIP] Gray and color image contrast enhancement by the curvelet transform\n[2003 TIP] Mathematical Properties of the jpeg2000 wavelet filters\n[2003 TIP] The finite ridgelet transform for image representation\n[2005 TIP] Sparse Geometric Image Representations With Bandelets\n[2005 TIP] The Contourlet Transform_ An Efficient Directional Multiresolution Image Representation\n[2010 SPM] The Curvelet Transform\n四、 图像处理与分析\n本章主要讨论图像处理与分析。虽然后面计算机视觉部分的有些内容比如特征提取等也可以归结到图像分析中来，但鉴于它们与计算机视觉的紧密联系，以及它们的出处，没有把它们纳入到图像处理与分析中来。同样，这里面也有一些也可以划归到计算机视觉中去。这都不重要，只要知道有这么个方法，能为自己所用，或者从中得到灵感，这就够了。\n1. Bilateral Filter\nBilateral Filter俗称双边滤波器是一种简单实用的具有保持边缘作用的平缓滤波器，由Tomasi等在1998年提出。它现在已经发挥着重大作用，尤其是在HDR领域。\n[1998 ICCV] Bilateral Filtering for Gray and Color Images\n[2008 TIP] Adaptive Bilateral Filter for Sharpness Enhancement and Noise Removal\n2. Color\n如果对颜色的形成有一定的了解，能比较深刻的理解一些算法。这方面推荐冈萨雷斯的数字图像处理中的相关章节以及Sharma在Digital Color Imaging Handbook中的第一章“Color fundamentals for digital imaging”。跟颜色相关的知识包括Gamma，颜色空间转换，颜色索引以及肤色模型等，这其中也包括著名的EMD。\n[1991 IJCV] Color Indexing\n[2000 IJCV] The Earth Mover's Distance as a Metric for Image Retrieval\n[2001 PAMI] Color invariance\n[2002 IJCV] Statistical Color Models with Application to Skin Detection\n[2003] A review of RGB color spaces\n[2007 PR]A survey of skin-color modeling and detection methods\nGamma.pdf\nGammaFAQ.pdf\n3. Compression and Encoding\n个人以为图像压缩编码并不是当前很热的一个话题，原因前面已经提到过。这里可以看看一篇对编码方面的展望文章\n[2005 IEEE] Trends and perspectives in image and video coding\n4. Contrast Enhancement\n对比度增强一直是图像处理中的一个恒久话题，一般来说都是基于直方图的，比如直方图均衡化。冈萨雷斯的书里面对这个话题讲的比较透彻。这里推荐几篇个人认为不错的文章。\n[2002 IJCV] Vision and the Atmosphere\n[2003 TIP] Gray and color image contrast enhancement by the curvelet transform\n[2006 TIP] Gray-level grouping (GLG) an automatic method for optimized image contrast enhancement-part II\n[2006 TIP] Gray-level grouping (GLG) an automatic method for optimized image contrast Enhancement-part I\n[2007 TIP] Transform Coefficient Histogram-Based Image Enhancement Algorithms Using Contrast Entropy\n[2009 TIP] A Histogram Modification Framework and Its Application for Image Contrast Enhancement\n5. Deblur (Restoration)\n图像恢复或者图像去模糊一直是一个非常难的问题，尤其是盲图像恢复。港中文的jiaya jia老师在这方面做的不错，他在主页也给出了可执行文件。这方面的内容也建议看冈萨雷斯的书。这里列出了几篇口碑比较好的文献，包括古老的Richardson-Lucy方法，几篇盲图像恢复的综述以及最近的几篇文章，尤以Fergus和Jiaya Jia的为经典。\n[1972] Bayesian-Based Iterative Method of Image Restoration\n[1974] an iterative technique for the rectification of observed distributions\n[1990 IEEE] Iterative methods for image deblurring\n[1996 SPM] Blind Image Deconvolution\n[1997 SPM] Digital image restoration\n[2005] Digital Image Reconstruction - Deblurring and Denoising\n[2006 Siggraph] Removing Camera Shake from a Single Photograph\n[2008 Siggraph] High-quality Motion Deblurring from a Single Image\n[2011 PAMI] Richardson-Lucy Deblurring for Scenes under a Projective Motion Path\n6. Dehazing and Defog\n严格来说去雾化也算是图像对比度增强的一种。这方面最近比较好的工作就是He kaiming等提出的Dark Channel方法。这篇论文也获得了2009的CVPR 最佳论文奖。2这位003年的广东高考状元已经于2011年从港中文博士毕业加入MSRA（估计当时也就二十五六岁吧），相当了不起。\n[2008 Siggraph] Single Image Dehazing\n[2009 CVPR] Single Image Haze Removal Using Dark Channel Prior\n[2011 PAMI] Single Image Haze Removal Using Dark Channel Prior\n7. Denoising\n图像去噪也是图像处理中的一个经典问题，在数码摄影中尤其重要。主要的方法有基于小波的方法和基于偏微分方程的方法。\n[1992 SIAM] Image selective smoothing and edge detection by nonlinear diffusion. II\n[1992 SIAM] Image selective smoothing and edge detection by nonlinear diffusion\n[1992] Nonlinear total variation based noise removal algorithms\n[1994 SIAM] Signal and image restoration using shock filters and anisotropic diffusion\n[1995 TIT] De-noising by soft-thresholding\n[1998 TIP] Orientation diffusions\n[2000 TIP] Adaptive wavelet thresholding for image denoising and compression\n[2000 TIP] Fourth-order partial differential equations for noise removal\n[2001] Denoising through wavelet shrinkage\n[2002 TIP] The Curvelet Transform for Image Denoising\n[2003 TIP] Noise removal using fourth-order partial differential equation with applications to medical magnetic resonance images in space and time\n[2008 PAMI] Automatic Estimation and Removal of Noise from a Single Image\n[2009 TIP] Is Denoising Dead\n8. Edge Detection\n边缘检测也是图像处理中的一个基本任务。传统的边缘检测方法有基于梯度算子，尤其是Sobel算子，以及经典的Canny边缘检测。到现在，Canny边缘检测及其思想仍在广泛使用。关于Canny算法的具体细节可以在Sonka的书以及canny自己的论文中找到，网上也可以搜到。最快最直接的方法就是看OpenCV的源代码，非常好懂。在边缘检测方面，Berkeley的大牛J Malik和他的学生在2004年的PAMI提出的方法效果非常好，当然也比较复杂。在复杂度要求不高的情况下，还是值得一试的。MIT的Bill Freeman早期的代表作Steerable Filter在边缘检测方面效果也非常好，并且便于实现。这里给出了几篇比较好的文献，包括一篇最新的综述。边缘检测是图像处理和计算机视觉中任何方向都无法逃避的一个问题，这方面研究多深都不为过。\n[1980] theory of edge detection\n[1983 Canny Thesis] find edge\n[1986 PAMI] A Computational Approach to Edge Detection\n[1990 PAMI] Scale-space and edge detection using anisotropic diffusion\n[1991 PAMI] The design and use of steerable filters\n[1995 PR] Multiresolution edge detection techniques\n[1996 TIP] Optimal edge detection in two-dimensional images\n[1998 PAMI] Local Scale Control for Edge Detection and Blur Estimation\n[2003 PAMI] Statistical edge detection_ learning and evaluating edge cues\n[2004 IEEE] Edge Detection Revisited\n[2004 PAMI] Design of steerable filters for feature detection using canny-like criteria\n[2004 PAMI] Learning to Detect Natural Image Boundaries Using Local Brightness, Color, and Texture Cues\n[2011 IVC] Edge and line oriented contour detection State of the art\n9. Graph Cut\n基于图割的图像分割算法。在这方面没有研究，仅仅列出几篇引用比较高的文献。这里又见J Malik，当然还有华人杰出学者Jianbo Shi，他的主页非常搞笑，在醒目的位置标注Do not fly China Eastern Airlines ... 看来是被坑过，而且坑的比较厉害。这个领域，俄罗斯人比较厉害。\n[2000 PAMI] Normalized cuts and image segmentation\n[2001 PAMI] Fast approximate energy minimization via graph cuts\n[2004 PAMI] What energy functions can be minimized via graph cuts\n10. Hough Transform\n虽然霍夫变换可以扩展到广义霍夫变换，但最常用的还是检测圆和直线。这方面同样推荐看OpenCV的源代码，一目了然。Matas在2000年提出的PPHT已经集成到OpenCV中去了。\n[1986 CVGIU] A Survey of the Hough Transform\n[1989] A Comparative study of Hough transform methods for circle finding\n[1992 PAMI] Shapes recognition using the straight line Hough transform_ theory and generalization\n[1997 PR] Extraction of line features in a noisy image\n[2000 CVIU] Robust Detection of Lines Using the Progressive Probabilistic Hough Transform\n11. Image Interpolation\n图像插值，偶尔也用得上。一般来说，双三次也就够了\n[2000 TMI] Interpolation revisited\n12. Image Matting\n也就是最近，我才知道这个词翻译成中文是抠图，比较难听，不知道是谁开始这么翻译的。没有研究，请看文章以及Richard Szeliski的相关章节。以色列美女Levin在这方面有两篇PAMI。\n[2008 Fnd] Image and Video Matting A Survey\n[2008 PAMI] A Closed-Form Solution to Natural Image Matting\n[2008 PAMI] Spectral Matting\n13. Image Modeling\n图像的统计模型。这方面有一本专门的著作Natural Image Statistics\n[1994] The statistics of natural images\n[2003 JMIV] On Advances in Statistical Modeling of Natural Images\n[2009 IJCV] Fields of Experts\n[2009 PAMI] Modeling multiscale subbands of photographic images with fields of Gaussian scale mixtures\n14. Image Quality Assessment\n在图像质量评价方面，Bovik是首屈一指的。这位老师也很有意思，作为编辑出版了很多书。他也是IEEE的Fellow\n[2004 TIP] Image quality assessment from error visibility to structural similarity\n[2011 TIP] blind image quality assessment From Natural Scene Statistics to Perceptual Quality\n15. Image Registration\n图像配准最早的应用在医学图像上，在图像融合之前需要对图像进行配准。在现在的计算机视觉中，配准也是一个需要理解的概念，比如跟踪，拼接等。在KLT中，也会涉及到配准。这里主要是综述文献。\n[1992 MIA] Image matching as a diffusion process\n[1992 PAMI] A Method for Registration of 3-D shapes\n[1992] a survey of image registration techniques\n[1998 MIA] A survey of medical image registration\n[2003 IVC] Image registration methods a survey\n[2003 TMI] Mutual-Information-Based Registration of Medical Survey\n[2011 TIP] Hairis registration\n16. Image Retrieval\n图像检索曾经很热，在2000年之后似乎消停了一段时间。最近各种图像的不变性特征提出来之后，再加上互联网搜索的商业需求，这个方向似乎又要火起来了，尤其是在商业界，比如淘淘搜。这仍然是一个非常值得关注的方面。而且图像检索与目标识别具有相通之处，比如特征提取和特征降维。这方面的文章值得一读。在最后给出了两篇Book chapter，其中一篇还是中文的。\n[2000 PAMI] Content-based image retrieval at the end of the early years\n[2000 TIP] PicToSeek Combining Color and Shape Invariant Features for Image Retrieval\n[2002] Content-Based Image Retrieval Systems A Survey\n[2008] Content-Based Image Retrieval-Literature Survey\n[2010] Plant Image Retrieval Using Color,Shape and Texture Features\n[2012 PAMI] A Multimedia Retrieval Framework Based on Semi-Supervised Ranking and Relevance Feedback\nCBIR Chinese\nfundament of cbir\n17. Image Segmentation\n图像分割，非常基本但又非常难的一个问题。建议看Sonka和冈萨雷斯的书。这里给出几篇比较好的文章，再次看到了J Malik。他们给出了源代码和测试集，有兴趣的话可以试试。\n[2004 IJCV] Efficient Graph-Based Image Segmentation\n[2008 CVIU] Image segmentation evaluation A survey of unsupervised methods\n[2011 PAMI] Contour Detection and Hierarchical Image Segmentation\n18. Level Set\n大名鼎鼎的水平集，解决了Snake固有的缺点。Level set的两位提出者Sethian和Osher最后反目，实在让人遗憾。个人以为，这种方法除了迭代比较费时，在真实场景中的表现让人生疑。不过，2008年ECCV上的PWP方法在结果上很吸引人。在重初始化方面，Chunming Li给出了比较好的解决方案\n[1995 PAMI] Shape modeling with front propagation_ a level set approach\n[2001 JCP] Level Set Methods_ An Overview and Some Recent Results\n[2005 CVIU] Geodesic active regions and level set methods for motion estimation and tracking\n[2007 IJCV] A Review of Statistical Approaches to Level Set Segmentation\n[2008 ECCV] Robust Real-Time Visual Tracking using Pixel-Wise Posteriors\n[2010 TIP] Distance Regularized Level Set Evolution and its Application to Image Segmentation\n19. Pyramid\n其实小波变换就是一种金字塔分解算法，而且具有无失真重构和非冗余的优点。Adelson在1983年提出的Pyramid优点是比较简单，实现起来比较方便。\n[1983] The Laplacian Pyramid as a Compact Image Code\n20. Radon Transform\nRadon变换也是一种很重要的变换，它构成了图像重建的基础。关于图像重建和radon变换，可以参考章毓晋老师的书，讲的比较清楚。\n[1993 PAMI] Image representation via a finite Radon transform\n[1993 TIP] The fast discrete radon transform I theory\n[2007 IVC] Generalised finite radon transform for N×N images\n21. Scale Space\n尺度空间滤波在现代不变特征中是一个非常重要的概念，有人说SIFT的提出者Lowe是不变特征之父，而Linderburg是不变特征之母。虽然尺度空间滤波是Witkin最早提出的，但其理论体系的完善和应用还是Linderburg的功劳。其在1998年IJCV上的两篇文章值得一读，不管是特征提取方面还是边缘检测方面。\n[1987] Scale-space filtering\n[1990 PAMI] Scale-Space for Discrete Signals\n[1994] Scale-space theory A basic tool for analysing structures at different scales\n[1998 IJCV] Edge Detection and Ridge Detection with Automatic Scale Selection\n[1998 IJCV] Feature Detection with Automatic Scale Selection\n22. Snake\n活动轮廓模型，改变了传统的图像分割的方法，用能量收缩的方法得到一个统计意义上的能量最小（最大）的边缘。\n[1987 IJCV] Snakes Active Contour Models\n[1996 ] deformable model in medical image A Survey\n[1997 IJCV] geodesic active contour\n[1998 TIP] Snakes, shapes, and gradient vector flow\n[2000 PAMI] Geodesic active contours and level sets for the detection and tracking of moving objects\n[2001 TIP] Active contours without edges\n23. Super Resolution\n超分辨率分析。对这个方向没有研究，简单列几篇文章。其中Yang Jianchao的那篇在IEEE上的下载率一直居高不下。\n[2002] Example-Based Super-Resolution\n[2009 ICCV] Super-Resolution from a Single Image\n[2010 TIP] Image Super-Resolution Via Sparse Representation\n24. Thresholding\n阈值分割是一种简单有效的图像分割算法。这个topic在冈萨雷斯的书里面讲的比较多。这里列出OTSU的原始文章以及一篇不错的综述。\n[1979 IEEE] OTSU A threshold selection method from gray-level histograms\n[2001 JISE] A Fast Algorithm for Multilevel Thresholding\n[2004 JEI] Survey over image thresholding techniques and quantitative performance evaluation\n25. Watershed\n分水岭算法是一种非常有效的图像分割算法，它克服了传统的阈值分割方法的缺点，尤其是Marker-Controlled Watershed，值得关注。Watershed在冈萨雷斯的书里面讲的比较详细。\n[1991 PAMI] Watersheds in digital spaces an efficient algorithm based on immersion simulations\n[2001]The Watershed Transform Definitions, Algorithms and Parallelizat on Strategies\n五、 计算机视觉\n这一章是计算机视觉部分，主要侧重在底层特征提取，视频分析，跟踪，目标检测和识别方面等方面。对于自己不太熟悉的领域比如摄像机标定和立体视觉，仅仅列出上google上引用次数比较多的文献。有一些刚刚出版的文章，个人非常喜欢，也列出来了。\n1. Active Appearance Models\n活动表观模型和活动轮廓模型基本思想来源Snake，现在在人脸三维建模方面得到了很成功的应用，这里列出了三篇最早最经典的文章。对这个领域有兴趣的可以从这三篇文章开始入手。\n[1998 ECCV] Active Appearance Models\n[2001 PAMI] Active Appearance Models\n2. Active Shape Models\n[1995 CVIU]Active Shape Models-Their Training and Application\n3. Background modeling and subtraction\n背景建模一直是视频分析尤其是目标检测中的一项关键技术。虽然最近一直有一些新技术的产生，demo效果也很好，比如基于dynamical texture的方法。但最经典的还是Stauffer等在1999年和2000年提出的GMM方法，他们最大的贡献在于不用EM去做高斯拟合，而是采用了一种迭代的算法，这样就不需要保存很多帧的数据，节省了buffer。Zivkovic在2004年的ICPR和PAMI上提出了动态确定高斯数目的方法，把混合高斯模型做到了极致。这种方法效果也很好，而且易于实现。在OpenCV中有现成的函数可以调用。在背景建模大家族里，无参数方法（2000 ECCV）和Vibe方法也值得关注。\n[1997 PAMI] Pfinder Real-Time Tracking of the Human Body\n[1999 CVPR] Adaptive background mixture models for real-time tracking\n[1999 ICCV] Wallflower Principles and Practice of Background Maintenance\n[2000 ECCV] Non-parametric Model for Background Subtraction\n[2000 PAMI] Learning Patterns of Activity Using Real-Time Tracking\n[2002 PIEEE] Background and foreground modeling using nonparametric\nkernel density estimation for visual surveillance\n[2004 ICPR] Improved adaptive Gaussian mixture model for background subtraction\n[2004 PAMI] Recursive unsupervised learning of finite mixture models\n[2006 PRL] Efficient adaptive density estimation per image pixel for the task of background subtraction\n[2011 TIP] ViBe A Universal Background Subtraction Algorithm for Video Sequences\n4. Bag of Words\n词袋，在这方面暂时没有什么研究。列出三篇引用率很高的文章，以后逐步解剖之。\n[2003 ICCV] Video Google A Text Retrieval Approach to Object Matching in Videos\n[2004 ECCV] Visual Categorization with Bags of Keypoints\n[2006 CVPR] Beyond bags of features Spatial pyramid matching for recognizing natural scene categories\n5. BRIEF\nBRIEF是Binary Robust Independent Elementary Features的简称，是近年来比较受关注的特征描述的方法。ORB也是基于BRIEF的。\n[2010 ECCV] BRIEF Binary Robust Independent Elementary Features\n[2011 ICCV] ORB an efficient alternative to SIFT or SURF\n[2012 PAMI] BRIEF Computing a Local Binary Descriptor Very Fast\n6. Camera Calibration and Stereo Vision\n非常不熟悉的领域。仅仅列出了十来篇重要的文献，供以后学习。\n[1979 Marr] A Computational Theory of Human Stereo Vision\n[1985] Computational vision and regularization theory\n[1987 IEEE] A versatile camera calibration technique for\nhigh-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses\n[1987] Probabilistic Solution of Ill-Posed Problems in Computational Vision\n[1988 PIEEE] Ill-Posed Problems in Early Vision\n[1989 IJCV] Kalman Filter-based Algorithms for Estimating Depth from Image Sequences\n[1990 IJCV] Relative Orientation\n[1990 IJCV] Using vanishing points for camera calibration\n[1992 ECCV] Camera self-calibration Theory and experiments\n[1992 IJCV] A theory of self-calibration of a moving camera\n[1992 PAMI] Camera calibration with distortion models and accuracy evaluation\n[1994 IJCV] The Fundamental Matrix Theory, Algorithms, and Stability Analysis\n[1994 PAMI] a stereo matching algorithm with an adaptive window theory and experiment\n[1999 ICCV] Flexible camera calibration by viewing a plane from unknown orientations\n[1999 IWAR] Marker tracking and hmd calibration for a video-based augmented reality conferencing system\n[2000 PAMI] A flexible new technique for camera calibration\n7. Color and Histogram Feature\n这里面主要来源于图像检索，早期的图像检测基本基于全局的特征，其中最显著的就是颜色特征。这一部分可以和前面的Color知识放在一起的。\n[1995 SPIE] Similarity of color images\n[1996 PR] IMAGE RETRIEVAL USING COLOR AND SHAPE\n[1996] comparing images using color coherence vectors\n[1997 ] Image Indexing Using Color Correlograms\n[2001 TIP] An Efficient Color Representation for Image Retrieval\n[2009 CVIU] Performance evaluation of local colour invariants\n8. Deformable Part Model\n大红大热的DPM，在OpenCV中有一个专门的topic讲DPM和latent svm\n[2008 CVPR] A Discriminatively Trained, Multiscale, Deformable Part Model\n[2010 CVPR] Cascade Object Detection with Deformable Part Models\n[2010 PAMI] Object Detection with Discriminatively Trained Part-Based Models\n9. Distance Transformations\n距离变换，在OpenCV中也有实现。用来在二值图像中寻找种子点非常方便。\n[1986 CVGIP] Distance Transformations in Digital Images\n[2008 ACM] 2D Euclidean Distance Transform Algorithms A Comparative Survey\n10. Face Detection\n最成熟最有名的当属Haar+Adaboost\n[1998 PAMI] Neural Network-Based Face Detection\n[2002 PAMI] Detecting faces in images a survey\n[2002 PAMI] Face Detection in Color Images\n[2004 IJCV] Robust Real-Time Face Detection\n11. Face Recognition\n不熟悉，简单罗列之。\n[1991] Face Recognition Using Eigenfaces\n[2000 PAMI] Automatic Analysis of Facial Expressions The State of the Art\n[2000] Face Recognition A Literature Survey\n[2006 PR] Face recognition from a single image per person A survey\n[2009 PAMI] Robust Face Recognition via Sparse Representation\n12. FAST\n用机器学习的方法来提取角点，号称很快很好。\n[2006 ECCV] Machine learning for high-speed corner detection\n[2010 PAMI] Faster and Better A Machine Learning Approach to Corner Detection\n13. Feature Extraction\n这里的特征主要都是各种不变性特征，SIFT，Harris，MSER等也属于这一类。把它们单独列出来是因为这些方法更流行一点。关于不变性特征，王永明与王贵锦合著的《图像局部不变性特征与描述》写的还不错。Mikolajczyk在2005年的PAMI上的文章以及2007年的综述是不错的学习材料。\n[1989 PAMI] On the detection of dominant points on digital curves\n[1997 IJCV] SUSAN—A New Approach to Low Level Image Processing\n[2004 IJCV] Matching Widely Separated Views Based on Affine Invariant Regions\n[2004 IJCV] Scale & Affine Invariant Interest Point Detectors\n[2005 PAMI] A performance evaluation of local descriptors\n[2006 IJCV] A Comparison of Affine Region Detectors\n[2007 FAT] Local Invariant Feature Detectors - A Survey\n[2011 IJCV] Evaluation of Interest Point Detectors and Feature Descriptors\n14. Feature Matching\nFua课题组在今年PAMI上的一篇文章，感觉还不错\n[2012 PAMI] LDAHash Improved Matching with Smaller Descriptors\n15. Harris\n虽然过去了很多年，Harris角点检测仍然广泛使用，而且基于它有很多变形。如果仔细看了这种方法，从直观也可以感觉到这是一种很稳健的方法。\n[1988 Harris] A combined corner and edge detector\n16. Histograms of Oriented Gradients\nHoG方法也在OpenCV中实现了：HOGDescriptor。\n[2005 CVPR] Histograms of Oriented Gradients for Human Detection\nNavneetDalalThesis.pdf\n17. Image Distance\n[1993 PAMI] Comparing Images Using the Hausdorff Distance\n18. Image Stitching\n图像拼接，另一个相关的词是Panoramic。在Computer Vision: Algorithms and Applications一书中，有专门一章是讨论这个问题。这里的两面文章一篇是综述，一篇是这方面很经典的文章。\n[2006 Fnd] Image Alignment and Stitching A Tutorial\n[2007 IJCV] Automatic Panoramic Image Stitching using Invariant Features\n19. KLT\nKLT跟踪算法，基于Lucas-Kanade提出的配准算法。除了三篇很经典的文章，最后一篇给出了OpenCV实现KLT的细节。\n[1981] An Iterative Image Registration Technique with an Application to Stereo Vision full version\n[1994 CVPR] Good Features to Track\n[2004 IJCV] Lucas-Kanade 20 Years On A Unifying Framework\nPyramidal Implementation of the Lucas Kanade Feature Tracker OpenCV\n20. Local Binary Pattern\nLBP。OpenCV的Cascade分类器也支持LBP，用来取代Haar特征。\n[2002 PAMI] Multiresolution gray-scale and rotation Invariant Texture Classification with Local Binary Patterns\n[2004 ECCV] Face Recognition with Local Binary Patterns\n[2006 PAMI] Face Description with Local Binary Patterns\n[2011 TIP] Rotation-Invariant Image and Video Description With Local Binary Pattern Features\n21. Low-Level Vision\n关于Low level vision的两篇很不错的文章\n[1998 TIP] A general framework for low level vision\n[2000 IJCV] Learning Low-Level Vision\n22. Mean Shift\n均值漂移算法，在跟踪中非常流行的方法。Comaniciu在这个方面做出了重要的贡献。最后三篇，一篇是CVIU上的top download文章，一篇是最新的PAMI上关于Mean Shift的文章，一篇是OpenCV实现的文章。\n[1995 PAMI] Mean shift, mode seeking, and clustering\n[2002 PAMI] Mean shift a robust approach toward feature space analysis\n[2003 CVPR] Mean-shift blob tracking through scale space\n[2009 CVIU] Object tracking using SIFT features and mean shift\n[2012 PAMI] Mean Shift Trackers with Cross-Bin Metrics\nOpenCV Computer Vision Face Tracking For Use in a Perceptual User Interface\n23. MSER\n这篇文章发表在2002年的BMVC上，后来直接录用到2004年的IVC上，内容差不多。MSER在Sonka的书里面也有提到。\n[2002 BMVC] Robust Wide Baseline Stereo from Maximally Stable Extremal Regions\n[2003] MSER Author Presentation\n[2004 IVC] Robust wide-baseline stereo from maximally stable extremal regions\n[2011 PAMI] Are MSER Features Really Interesting\n24. Object Detection\n首先要说的是第一篇文章的作者，Kah-Kay Sung。他是MIT的博士，后来到新加坡国立任教，极具潜力的一个老师。不幸的是，他和他的妻子都在2000年的新加坡空难中遇难，让人唏嘘不已。\nhttp://en.wikipedia.org/wiki/Singapore_Airlines_Flight_006\n最后一篇文章也是Fua课题组的，作者给出的demo效果相当好。\n[1998 PAMI] Example-based learning for view-based human face detection\n[2003 IJCV] Learning the Statistics of People in Images and Video\n[2011 PAMI] Learning to Detect a Salient Object\n[2012 PAMI] A Real-Time Deformable Detector\n25. Object Tracking\n跟踪也是计算机视觉中的经典问题。粒子滤波，卡尔曼滤波，KLT，mean shift，光流都跟它有关系。这里列出的是传统意义上的跟踪，尤其值得一看的是2008的Survey和2003年的Kernel based tracking。\n[2003 PAMI] Kernel-based object tracking\n[2007 PAMI] Tracking People by Learning Their Appearance\n[2008 ACM] Object Tracking A Survey\n[2008 PAMI] Segmentation and Tracking of Multiple Humans in Crowded Environments\n[2011 PAMI] Hough Forests for Object Detection, Tracking, and Action Recognition\n[2011 PAMI] Robust Object Tracking with Online Multiple Instance Learning\n[2012 IJCV] PWP3D Real-Time Segmentation and Tracking of 3D Objects\n26. OCR\n一个非常成熟的领域，已经很好的商业化了。\n[1992 IEEE] Historical review of OCR research and development\nVideo OCR A Survey and Practitioner's Guide\n27. Optical Flow\n光流法，视频分析所必需掌握的一种算法。\n[1981 AI] Determine Optical Flow\n[1994 IJCV] Performance of optical flow techniques\n[1995 ACM] The Computation of Optical Flow\n[2004 TR] Tutorial Computing 2D and 3D Optical Flow\n[2005 BOOK] Optical Flow Estimation\n[2008 ECCV] Learning Optical Flow\n[2011 IJCV] A Database and Evaluation Methodology for Optical Flow\n28. Particle Filter\n粒子滤波，主要给出的是综述以及1998 IJCV上的关于粒子滤波发展早期的经典文章。\n[1998 IJCV] CONDENSATION—Conditional Density Propagation for Visual Tracking\n[2002 TSP] A tutorial on particle filters for online nonlinear non-Gaussian Bayesian tracking\n[2002 TSP] Particle filters for positioning, navigation, and tracking\n[2003 SPM] particle filter\n29. Pedestrian and Human detection\n仍然是综述类，关于行人和人体的运动检测和动作识别。\n[1999 CVIU] Visual analysis of human movement_ A survey\n[2001 CVIU] A Survey of Computer Vision-Based Human Motion Capture\n[2005 TIP] Image change detection algorithms a systematic survey\n[2006 CVIU] a survey of avdances in vision based human motion capture\n[2007 CVIU] Vision-based human motion analysis An overview\n[2007 IJCV] Pedestrian Detection via Periodic Motion Analysis\n[2007 PR] A survey of skin-color modeling and detection methods\n[2010 IVC] A survey on vision-based human action recognition\n[2012 PAMI] Pedestrian Detection An Evaluation of the State of the Art\n30. Scene Classification\n当相机越来越傻瓜化的时候，自动场景识别就非常重要。这是比拼谁家的Auto功能做的比较好的时候了。\n[2001 IJCV] Modeling the Shape of the Scene A Holistic Representation of the Spatial Envelope\n[2001 PAMI] Visual Word Ambiguity\n[2007 PAMI] A Thousand Words in a Scene\n[2010 PAMI] Evaluating Color Descriptors for Object and Scene Recognition\n[2011 PAMI] CENTRIST A Visual Descriptor for Scene Categorization\n31. Shadow Detection\n[2003 PAMI] Detecting moving shadows-- algorithms and evaluation\n32. Shape\n关于形状，主要是两个方面：形状的表示和形状的识别。形状的表示主要是从边缘或者区域当中提取不变性特征，用来做检索或者识别。这方面Sonka的书讲的比较系统。2008年的那篇综述在这方面也讲的不错。至于形状识别，最牛的当属J Malik等提出的Shape Context。\n[1993 PR] IMPROVED MOMENT INVARIANTS FOR SHAPE DISCRIMINATION\n[1993 PR] Pattern Recognition by Affine Moment Invariants\n[1996 PR] IMAGE RETRIEVAL USING COLOR AND SHAPE\n[2001 SMI] Shape matching similarity measures and algorithms\n[2002 PAMI] Shape matching and object recognition using shape contexts\n[2004 PR] Review of shape representation and description techniques\n[2006 PAMI] Integral Invariants for Shape Matching\n[2008] A Survey of Shape Feature Extraction Techniques\n33. SIFT\n关于SIFT，实在不需要介绍太多，一万多次的引用已经说明问题了。SURF和PCA-SIFT也是属于这个系列。后面列出了几篇跟SIFT有关的问题。\n[1999 ICCV] Object recognition from local scale-invariant features\n[2000 IJCV] Evaluation of Interest Point Detectors\n[2003 CVIU] Speeded-Up Robust Features (SURF)\n[2004 CVPR] PCA-SIFT A More Distinctive Representation for Local Image Descriptors\n[2004 IJCV] Distinctive Image Features from Scale-Invariant Keypoints\n[2010 IJCV] Improving Bag-of-Features for Large Scale Image Search\n[2011 PAMI] SIFTflow Dense Correspondence across Scenes and its Applications\n34. SLAM\nSimultaneous Localization and Mapping, 同步定位与建图。\nSLAM问题可以描述为: 机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置估计和地图进行自身定位,同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。\n[2002 PAMI] Simultaneous Localization and Map-Building Using Active Vision\n[2007 PAMI] MonoSLAM Real-Time Single Camera SLAM\n35. Texture Feature\n纹理特征也是物体识别和检索的一个重要特征集。\n[1973] Textural features for image classification\n[1979 ] Statistical and structural approaches to texture\n[1996 PAMI] Texture features for browsing and retrieval of image data\n[2002 PR] Brief review of invariant texture analysis methods\n[2012 TIP] Color Local Texture Features for Color Face Recognition\n36. TLD\nKadal创立了TLD，跟踪学习检测同步进行，达到稳健跟踪的目的。他的两个导师也是大名鼎鼎，一个是发明MSER的Matas，一个是Mikolajczyk。他还创立了一个公司TLD Vision s.r.o. 这里给出了他的系列文章，最后一篇是刚出来的PAMI。\n[2009] Online learning of robust object detectors during unstable tracking\n[2010 CVPR] P-N Learning Bootstrapping Binary Classifiers by Structural Constraints\n[2010 ICIP] FACE-TLD TRACKING-LEARNING-DETECTION APPLIED TO FACES\n[2012 PAMI] Tracking-Learning-Detection\n37. Video Surveillance\n前两篇是两个很有名的视频监控系统，里面包含了很丰富的信息量，比如CMU的那个系统里面的背景建模算法也是相当简单有效的。最后一篇是比较近的综述。\n[2000 CMU TR] A System for Video Surveillance and Monitoring\n[2000 PAMI] W4-- real-time surveillance of people and their activities\n[2008 MVA] The evolution of video surveillance an overview\n38. Viola-Jones\nHaar+Adaboost的弱弱联手，组成了最强大的利器。在OpenCV里面有它的实现，也可以选择用LBP来代替Haar特征。\n[2001 CVPR] Rapid object detection using a boosted cascade of simple features\n[2004 IJCV] Robust Real-time Face Detection\n六、 结束语\n历时一个多月，终于用业余时间把这些资料整理出来了，总算了却了一块心病，也不至于再看着一堆资料发愁了。以后可能会有些小修小补，但不会有太大的变化了。万里长征走完了第一步，剩下的就是理解和消化了。借新浪ishare共享出来，希望能够对你的科研也有一定的帮助。最后简单统计一下各个年份出现的频率。\n文章总数：372\n2012年： 10\n2011年： 20\n2010年： 20\n2009年： 14\n2008年： 18\n2007年： 13\n2006年： 14\n2005年： 9\n2004年： 24\n2003年： 22\n2002年： 21\n2001年： 21\n2000年： 23\n1999年： 10\n1998年： 22\n1997年： 8\n1996年： 9\n1995年： 9\n1994年： 7\n1993年： 5\n1992年： 11\n1991年： 5\n1990年： 6\n1980-1989： 22\n1960-1979： 9"}
{"content2":"计算机视觉领域经典论文整理\n计算机视觉论文整理git项目\n计算机视觉的研究方向：\nImageNet分类\n物体检测\n物体跟踪\n低级视觉\n边缘检测\n语义分割\n视觉注意力和显著性\n物体识别\n人体姿态估计\nCNN原理和性质（Understanding CNN）\n图像和语言\n图像解说\n视频解说\n图像生成\n微软ResNet\n论文：用于图像识别的深度残差网络\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1512.03385v1.pdf\n微软PRelu（随机纠正线性单元/权重初始化）\n论文：深入学习整流器：在ImageNet分类上超越人类水平\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1502.01852.pdf\n谷歌Batch Normalization\n论文：批量归一化：通过减少内部协变量来加速深度网络训练\n作者：Sergey Ioffe, Christian Szegedy\n链接：http://arxiv.org/pdf/1502.03167.pdf\n谷歌GoogLeNet\n论文：更深的卷积，CVPR 2015\n作者：Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich\n链接：http://arxiv.org/pdf/1409.4842.pdf\n牛津VGG-Net\n论文：大规模视觉识别中的极深卷积网络，ICLR 2015\n作者：Karen Simonyan & Andrew Zisserman\n链接：http://arxiv.org/pdf/1409.1556.pdf\nAlexNet\n论文：使用深度卷积神经网络进行ImageNet分类\n作者：Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n链接：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n物体检测\nPVANET\n论文：用于实时物体检测的深度轻量神经网络（PVANET：Deep but Lightweight Neural Networks for Real-time Object Detection）\n作者：Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park\n链接：http://arxiv.org/pdf/1608.08021\n纽约大学OverFeat\n论文：使用卷积网络进行识别、定位和检测（OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks），ICLR 2014\n作者：Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun\n链接：http://arxiv.org/pdf/1312.6229.pdf\n伯克利R-CNN\n论文：精确物体检测和语义分割的丰富特征层次结构（Rich feature hierarchies for accurate object detection and semantic segmentation），CVPR 2014\n作者：Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik\n链接：http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\n微软SPP\n论文：视觉识别深度卷积网络中的空间金字塔池化（Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition），ECCV 2014\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1406.4729.pdf\n微软Fast R-CNN\n论文：Fast R-CNN\n作者：Ross Girshick\n链接：http://arxiv.org/pdf/1504.08083.pdf\n微软Faster R-CNN\n论文：使用RPN走向实时物体检测（Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks）\n作者：任少卿、何恺明、Ross Girshick、孙剑\n链接：http://arxiv.org/pdf/1506.01497.pdf\n牛津大学R-CNN minus R\n论文：R-CNN minus R\n作者：Karel Lenc, Andrea Vedaldi\n链接：http://arxiv.org/pdf/1506.06981.pdf\n端到端行人检测\n论文：密集场景中端到端的行人检测（End-to-end People Detection in Crowded Scenes）\n作者：Russell Stewart, Mykhaylo Andriluka\n链接：http://arxiv.org/pdf/1506.04878.pdf\n实时物体检测\n论文：你只看一次：统一实时物体检测（You Only Look Once: Unified, Real-Time Object Detection）\n作者：Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\n链接：http://arxiv.org/pdf/1506.02640.pdf\nInside-Outside Net\n论文：使用跳跃池化和RNN在场景中检测物体（Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks）\n作者：Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick\n链接：http://arxiv.org/abs/1512.04143.pdf\n微软ResNet\n论文：用于图像识别的深度残差网络\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1512.03385v1.pdf\nR-FCN\n论文：通过区域全卷积网络进行物体识别（R-FCN: Object Detection via Region-based Fully Convolutional Networks）\n作者：代季峰，李益，何恺明，孙剑\n链接：http://arxiv.org/abs/1605.06409\nSSD\n论文：单次多框检测器（SSD: Single Shot MultiBox Detector）\n作者：Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg\n链接：http://arxiv.org/pdf/1512.02325v2.pdf\n速度/精度权衡\n论文：现代卷积物体检测器的速度/精度权衡（Speed/accuracy trade-offs for modern convolutional object detectors）\n作者：Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy\n链接：http://arxiv.org/pdf/1611.10012v1.pdf\n物体跟踪\n论文：用卷积神经网络通过学习可区分的显著性地图实现在线跟踪（Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network）\n作者：Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han\n地址：arXiv:1502.06796.\n论文：DeepTrack：通过视觉跟踪的卷积神经网络学习辨别特征表征（DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking）\n作者：Hanxi Li, Yi Li and Fatih Porikli\n发表： BMVC, 2014.\n论文：视觉跟踪中，学习深度紧凑图像表示（Learning a Deep Compact Image Representation for Visual Tracking）\n作者：N Wang, DY Yeung\n发表：NIPS, 2013.\n论文：视觉跟踪的分层卷积特征（Hierarchical Convolutional Features for Visual Tracking）\n作者：Chao Ma, Jia-Bin Huang, Xiaokang Yang and Ming-Hsuan Yang\n发表： ICCV 2015\n论文：完全卷积网络的视觉跟踪（Visual Tracking with fully Convolutional Networks）\n作者：Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu,\n发表：ICCV 2015\n论文：学习多域卷积神经网络进行视觉跟踪（Learning Multi-Domain Convolutional Neural Networks for Visual Tracking）\n作者：Hyeonseob Namand Bohyung Han\n对象识别（Object Recognition）\n论文：卷积神经网络弱监督学习（Weakly-supervised learning with convolutional neural networks）\n作者：Maxime Oquab，Leon Bottou，Ivan Laptev，Josef Sivic，CVPR，2015\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf\nFV-CNN\n论文：深度滤波器组用于纹理识别和分割（Deep Filter Banks for Texture Recognition and Segmentation）\n作者：Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Cimpoi_Deep_Filter_Banks_2015_CVPR_paper.pdf\n人体姿态估计（Human Pose Estimation）\n论文：使用 Part Affinity Field的实时多人2D姿态估计（Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields）\n作者：Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh, CVPR, 2017.\n论文：Deepcut：多人姿态估计的联合子集分割和标签（Deepcut: Joint subset partition and labeling for multi person pose estimation）\n作者：Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele, CVPR, 2016.\n论文：Convolutional pose machines\n作者：Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh, CVPR, 2016.\n论文：人体姿态估计的 Stacked hourglass networks（Stacked hourglass networks for human pose estimation）\n作者：Alejandro Newell, Kaiyu Yang, and Jia Deng, ECCV, 2016.\n论文：用于视频中人体姿态估计的Flowing convnets（Flowing convnets for human pose estimation in videos）\n作者：Tomas Pfister, James Charles, and Andrew Zisserman, ICCV, 2015.\n论文：卷积网络和人类姿态估计图模型的联合训练（Joint training of a convolutional network and a graphical model for human pose estimation）\n作者：Jonathan J. Tompson, Arjun Jain, Yann LeCun, Christoph Bregler, NIPS, 2014.\n理解CNN\n论文：通过测量同变性和等价性来理解图像表示(Understanding image representations by measuring their equivariance and equivalence)\n作者：Karel Lenc, Andrea Vedaldi, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf\n论文：深度神经网络容易被愚弄：无法识别的图像的高置信度预测（Deep Neural Networks are Easily Fooled:High Confidence Predictions for Unrecognizable Images）\n作者：Anh Nguyen, Jason Yosinski, Jeff Clune, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf\n论文：通过反演理解深度图像表示（Understanding Deep Image Representations by Inverting Them）\n作者：Aravindh Mahendran, Andrea Vedaldi, CVPR, 2015\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf\n论文：深度场景CNN中的对象检测器（Object Detectors Emerge in Deep Scene CNNs）\n作者：Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, ICLR, 2015.\n链接：http://arxiv.org/abs/1412.6856\n论文：用卷积网络反演视觉表示（Inverting Visual Representations with Convolutional Networks）\n作者：Alexey Dosovitskiy, Thomas Brox, arXiv, 2015.\n链接：http://arxiv.org/abs/1506.02753\n论文：可视化和理解卷积网络（Visualizing and Understanding Convolutional Networks）\n作者：Matthrew Zeiler, Rob Fergus, ECCV, 2014.\n链接：http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\n图像与语言\n图像说明（Image Captioning）\nUCLA / Baidu\n用多模型循环神经网络解释图像（Explain Images with Multimodal Recurrent Neural Networks）\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille, arXiv:1410.1090\nhttp://arxiv.org/pdf/1410.1090\nToronto\n使用多模型神经语言模型统一视觉语义嵌入（Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models）\nRyan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, arXiv:1411.2539.\nhttp://arxiv.org/pdf/1411.2539\nBerkeley\n用于视觉识别和描述的长期循环卷积网络（Long-term Recurrent Convolutional Networks for Visual Recognition and Description）\nJeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, arXiv:1411.4389.\nhttp://arxiv.org/pdf/1411.4389\nGoogle\n看图写字：神经图像说明生成器（Show and Tell: A Neural Image Caption Generator）\nOriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, arXiv:1411.4555.\nhttp://arxiv.org/pdf/1411.4555\nStanford\n用于生成图像描述的深度视觉语义对齐（Deep Visual-Semantic Alignments for Generating Image Description）\nAndrej Karpathy, Li Fei-Fei, CVPR, 2015.\nWeb：http://cs.stanford.edu/people/karpathy/deepimagesent/\nPaper：http://cs.stanford.edu/people/karpathy/cvpr2015.pdf\nUML / UT\n使用深度循环神经网络将视频转换为自然语言（Translating Videos to Natural Language Using Deep Recurrent Neural Networks）\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, NAACL-HLT, 2015.\nhttp://arxiv.org/pdf/1412.4729\nCMU / Microsoft\n学习图像说明生成的循环视觉表示（Learning a Recurrent Visual Representation for Image Caption Generation）\nXinlei Chen, C. Lawrence Zitnick, arXiv:1411.5654.\nXinlei Chen, C. Lawrence Zitnick, Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation, CVPR 2015\nhttp://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf\nMicrosoft\n从图像说明到视觉概念（From Captions to Visual Concepts and Back）\nHao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig, CVPR, 2015.\nhttp://arxiv.org/pdf/1411.4952\nUniv. Montreal / Univ. Toronto\nShow, Attend, and Tell：视觉注意力与神经图像标题生成（Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention）\nKelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, arXiv:1502.03044 / ICML 2015\nhttp://www.cs.toronto.edu/~zemel/documents/captionAttn.pdf\nIdiap / EPFL / Facebook\n基于短语的图像说明（Phrase-based Image Captioning）\nRemi Lebret, Pedro O. Pinheiro, Ronan Collobert, arXiv:1502.03671 / ICML 2015\nhttp://arxiv.org/pdf/1502.03671\nUCLA / Baidu\n像孩子一样学习：从图像句子描述快速学习视觉的新概念（Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images）\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L. Yuille, arXiv:1504.06692\nhttp://arxiv.org/pdf/1504.06692\nMS + Berkeley\n探索图像说明的最近邻方法（ Exploring Nearest Neighbor Approaches for Image Captioning）\nJacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, arXiv:1505.04467\nhttp://arxiv.org/pdf/1505.04467.pdf\n图像说明的语言模型（Language Models for Image Captioning: The Quirks and What Works）\nJacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, arXiv:1505.01809\nhttp://arxiv.org/pdf/1505.01809.pdf\n阿德莱德\n具有中间属性层的图像说明（ Image Captioning with an Intermediate Attributes Layer）\nQi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, arXiv:1506.01144\n蒂尔堡\n通过图片学习语言(Learning language through pictures)\nGrzegorz Chrupala, Akos Kadar, Afra Alishahi, arXiv:1506.03694\n蒙特利尔大学\n使用基于注意力的编码器-解码器网络描述多媒体内容（Describing Multimedia Content using Attention-based Encoder-Decoder Networks）\nKyunghyun Cho, Aaron Courville, Yoshua Bengio, arXiv:1507.01053\n康奈尔\n图像表示和神经图像说明的新领域（Image Representations and New Domains in Neural Image Captioning）\nJack Hessel, Nicolas Savva, Michael J. Wilber, arXiv:1508.02091\nMS + City Univ. of HongKong\nLearning Query and Image Similarities with Ranking Canonical Correlation Analysis\nTing Yao, Tao Mei, and Chong-Wah Ngo, ICCV, 2015\n视频字幕（Video Captioning）\n伯克利\nJeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR, 2015.\n犹他州/ UML / 伯克利\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729.\n微软\nYingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui, Joint Modeling Embedding and Translation to Bridge Video and Language, arXiv:1505.01861.\n犹他州/ UML / 伯克利\nSubhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence–Video to Text, arXiv:1505.00487.\n蒙特利尔大学/ 舍布鲁克\nLi Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029\nMPI / 伯克利\nAnna Rohrbach, Marcus Rohrbach, Bernt Schiele, The Long-Short Story of Movie Description, arXiv:1506.01698\n多伦多大学 / MIT\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books, arXiv:1506.06724\n蒙特利尔大学\nKyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053\nTAU / 美国南加州大学\nDotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf, Temporal Tessellation for Video Annotation and Summarization, arXiv:1612.06950.\n图像生成\n卷积/循环网络\n论文：Conditional Image Generation with PixelCNN Decoders”\n作者：Aäron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu\n论文：Learning to Generate Chairs with Convolutional Neural Networks\n作者：Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox\n发表：CVPR, 2015.\n论文：DRAW: A Recurrent Neural Network For Image Generation\n作者：Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra\n发表：ICML, 2015.\n对抗网络\n论文：生成对抗网络（Generative Adversarial Networks）\n作者：Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n发表：NIPS, 2014.\n论文：使用对抗网络Laplacian Pyramid 的深度生成图像模型（Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks）\n作者：Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus\n发表：NIPS, 2015.\n论文：生成模型演讲概述 （A note on the evaluation of generative models）\n作者：Lucas Theis, Aäron van den Oord, Matthias Bethge\n发表：ICLR 2016.\n论文：变分自动编码深度高斯过程（Variationally Auto-Encoded Deep Gaussian Processes）\n作者：Zhenwen Dai, Andreas Damianou, Javier Gonzalez, Neil Lawrence\n发表：ICLR 2016.\n论文：用注意力机制从字幕生成图像 （Generating Images from Captions with Attention）\n作者：Elman Mansimov, Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov\n发表： ICLR 2016\n论文：分类生成对抗网络的无监督和半监督学习（Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks）\n作者：Jost Tobias Springenberg\n发表：ICLR 2016\n论文：用一个对抗检测表征（Censoring Representations with an Adversary）\n作者：Harrison Edwards, Amos Storkey\n发表：ICLR 2016\n论文：虚拟对抗训练实现分布式顺滑 （Distributional Smoothing with Virtual Adversarial Training）\n作者：Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii\n发表：ICLR 2016\n论文：自然图像流形上的生成视觉操作（Generative Visual Manipulation on the Natural Image Manifold）\n作者：朱俊彦, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros\n发表： ECCV 2016.\n论文：深度卷积生成对抗网络的无监督表示学习（Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks）\n作者：Alec Radford, Luke Metz, Soumith Chintala\n发表： ICLR 2016\n问题回答\n弗吉尼亚大学 / 微软研究院\n论文：VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.\n作者：Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh\nMPI / 伯克利\n论文：Ask Your Neurons: A Neural-based Approach to Answering Questions about Images\n作者：Mateusz Malinowski, Marcus Rohrbach, Mario Fritz,\n发布 ： arXiv:1505.01121.\n多伦多\n论文： Image Question Answering: A Visual Semantic Embedding Model and a New Dataset\n作者：Mengye Ren, Ryan Kiros, Richard Zemel\n发表： arXiv:1505.02074 / ICML 2015 deep learning workshop.\n百度/ 加州大学洛杉矶分校\n作者：Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, 徐伟\n论文：Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering\n发表： arXiv:1505.05612.\nPOSTECH（韩国）\n论文：Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction\n作者：Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han\n发表： arXiv:1511.05765\nCMU / 微软研究院\n论文：Stacked Attention Networks for Image Question Answering\n作者：Yang, Z., He, X., Gao, J., Deng, L., & Smola, A. (2015)\n发表： arXiv:1511.02274.\nMetaMind\n论文：Dynamic Memory Networks for Visual and Textual Question Answering\n作者：Xiong, Caiming, Stephen Merity, and Richard Socher\n发表： arXiv:1603.01417 (2016).\n首尔国立大学 + NAVER\n论文：Multimodal Residual Learning for Visual QA\n作者：Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang\n发表：arXiv:1606:01455\nUC Berkeley + 索尼\n论文：Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\n作者：Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach\n发表：arXiv:1606.01847\nPostech\n论文：Training Recurrent Answering Units with Joint Loss Minimization for VQA\n作者：Hyeonwoo Noh and Bohyung Han\n发表： arXiv:1606.03647\n首尔国立大学 + NAVER\n论文： Hadamard Product for Low-rank Bilinear Pooling\n作者：Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhan\n发表：arXiv:1610.04325.\n视觉注意力和显著性\n论文：Predicting Eye Fixations using Convolutional Neural Networks\n作者：Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu\n发表：CVPR, 2015.\n学习地标的连续搜索\n作者：Learning a Sequential Search for Landmarks\n论文：Saurabh Singh, Derek Hoiem, David Forsyth\n发表：CVPR, 2015.\n视觉注意力机制实现多物体识别\n论文：Multiple Object Recognition with Visual Attention\n作者：Jimmy Lei Ba, Volodymyr Mnih, Koray Kavukcuoglu,\n发表：ICLR, 2015.\n视觉注意力机制的循环模型\n作者：Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu\n论文：Recurrent Models of Visual Attention\n发表：NIPS, 2014.\n低级视觉\n超分辨率\nIterative Image Reconstruction\nSven Behnke: Learning Iterative Image Reconstruction. IJCAI, 2001.\nSven Behnke: Learning Iterative Image Reconstruction in the Neural Abstraction Pyramid. International Journal of Computational Intelligence and Applications, vol. 1, no. 4, pp. 427-438, 2001.\nSuper-Resolution (SRCNN)\nChao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, ECCV, 2014.\nChao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Image Super-Resolution Using Deep Convolutional Networks, arXiv:1501.00092.\nVery Deep Super-Resolution\nJiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Accurate Image Super-Resolution Using Very Deep Convolutional Networks, arXiv:1511.04587, 2015.\nDeeply-Recursive Convolutional Network\nJiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Deeply-Recursive Convolutional Network for Image Super-Resolution, arXiv:1511.04491, 2015.\nCasade-Sparse-Coding-Network\nZhaowen Wang, Ding Liu, Wei Han, Jianchao Yang and Thomas S. Huang, Deep Networks for Image Super-Resolution with Sparse Prior. ICCV, 2015.\nPerceptual Losses for Super-Resolution\nJustin Johnson, Alexandre Alahi, Li Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, arXiv:1603.08155, 2016.\nSRGAN\nChristian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, arXiv:1609.04802v3, 2016.\n其他应用\nOptical Flow (FlowNet)\nPhilipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox, FlowNet: Learning Optical Flow with Convolutional Networks, arXiv:1504.06852.\nCompression Artifacts Reduction\nChao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, Compression Artifacts Reduction by a Deep Convolutional Network, arXiv:1504.06993.\nBlur Removal\nChristian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf, Learning to Deblur, arXiv:1406.7444\nJian Sun, Wenfei Cao, Zongben Xu, Jean Ponce, Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal, CVPR, 2015\nImage Deconvolution\nLi Xu, Jimmy SJ. Ren, Ce Liu, Jiaya Jia, Deep Convolutional Neural Network for Image Deconvolution, NIPS, 2014.\nDeep Edge-Aware Filter\nLi Xu, Jimmy SJ. Ren, Qiong Yan, Renjie Liao, Jiaya Jia, Deep Edge-Aware Filters, ICML, 2015.\nComputing the Stereo Matching Cost with a Convolutional Neural Network\nJure Žbontar, Yann LeCun, Computing the Stereo Matching Cost with a Convolutional Neural Network, CVPR, 2015.\nColorful Image Colorization Richard Zhang, Phillip Isola, Alexei A. Efros, ECCV, 2016\nFeature Learning by Inpainting\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros, Context Encoders: Feature Learning by Inpainting, CVPR, 2016\n边缘检测\nSaining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.\nDeepEdge\nGedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.\nDeepContour\nWei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.\n语义分割\nSEC: Seed, Expand and Constrain\nAlexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016.\nAdelaide\nGuosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. (1st ranked in VOC2012)\nGuosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. (4th ranked in VOC2012)\nDeep Parsing Network (DPN)\nZiwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 / ICCV 2015 (2nd ranked in VOC 2012)\nCentraleSuperBoundaries, INRIA\nIasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)\nBoxSup\nJifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)\nPOSTECH\nHyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. (7th ranked in VOC2012)\nSeunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924.\nSeunghoon Hong,Junhyuk Oh,Bohyung Han, andHonglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928\nConditional Random Fields as Recurrent Neural Networks\nShuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)\nDeepLab\nLiang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. (9th ranked in VOC2012)\nZoom-out\nMohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015\nJoint Calibration\nHolger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.\nFully Convolutional Networks for Semantic Segmentation\nJonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.\nHypercolumn\nBharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.\nDeep Hierarchical Parsing\nAbhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015.\nLearning Hierarchical Features for Scene Labeling\nClement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.\nClement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.\nUniversity of Cambridge\nVijay Badrinarayanan, Alex Kendall and Roberto Cipolla “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.” arXiv preprint arXiv:1511.00561, 2015.\nAlex Kendall, Vijay Badrinarayanan and Roberto Cipolla “Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.” arXiv preprint arXiv:1511.02680, 2015.\nPrinceton\nFisher Yu, Vladlen Koltun, “Multi-Scale Context Aggregation by Dilated Convolutions”, ICLR 2016\nUniv. of Washington, Allen AI\nHamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, “Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing”, ICCV, 2015\nINRIA\nIasonas Kokkinos, “Pusing the Boundaries of Boundary Detection Using deep Learning”, ICLR 2016\nUCSB\nNiloufar Pourian, S. Karthikeyan, and B.S. Manjunath, “Weakly supervised graph based semantic segmentation by learning communities of image-parts”, ICCV, 2015\n其他资源\n课程\n深度视觉\n[斯坦福] CS231n: Convolutional Neural Networks for Visual Recognition\n[香港中文大学] ELEG 5040: Advanced Topics in Signal Processing(Introduction to Deep Learning)\n· 更多深度课程推荐\n[斯坦福] CS224d: Deep Learning for Natural Language Processing\n[牛津 Deep Learning by Prof. Nando de Freitas\n[纽约大学] Deep Learning by Prof. Yann LeCun\n图书\n免费在线图书\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\nNeural Networks and Deep Learning by Michael Nielsen\nDeep Learning Tutorial by LISA lab, University of Montreal\n视频\n演讲\nDeep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng\nRecent Developments in Deep Learning By Geoff Hinton\nThe Unreasonable Effectiveness of Deep Learning by Yann LeCun\nDeep Learning of Representations by Yoshua bengio\n软件\n框架\nTensorflow: An open source software library for numerical computation using data flow graph by Google [Web]\nTorch7: Deep learning library in Lua, used by Facebook and Google Deepmind [Web]\nTorch-based deep learning libraries: [torchnet],\nCaffe: Deep learning framework by the BVLC [Web]\nTheano: Mathematical library in Python, maintained by LISA lab [Web]\nTheano-based deep learning libraries: [Pylearn2], [Blocks], [Keras], [Lasagne]\nMatConvNet: CNNs for MATLAB [Web]\nMXNet: A flexible and efficient deep learning library for heterogeneous distributed systems with multi-language support [Web]\nDeepgaze: A computer vision library for human-computer interaction based on CNNs [Web]\n应用\n对抗训练 Code and hyperparameters for the paper “Generative Adversarial Networks” [Web]\n理解与可视化 Source code for “Understanding Deep Image Representations by Inverting Them,” CVPR, 2015. [Web]\n词义分割 Source code for the paper “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR, 2014. [Web] ； Source code for the paper “Fully Convolutional Networks for Semantic Segmentation,” CVPR, 2015. [Web]\n超分辨率 Image Super-Resolution for Anime-Style-Art [Web]\n边缘检测 Source code for the paper “DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection,” CVPR, 2015. [Web]\nSource code for the paper “Holistically-Nested Edge Detection”, ICCV 2015. [Web]\n讲座\n[CVPR 2014] Tutorial on Deep Learning in Computer Vision\n[CVPR 2015] Applied Deep Learning for Computer Vision with Torch\n博客\nDeep down the rabbit hole: CVPR 2015 and beyond@Tombone’s Computer Vision Blog\nCVPR recap and where we’re going@Zoya Bylinskii (MIT PhD Student)’s Blog\nFacebook’s AI Painting@Wired\nInceptionism: Going Deeper into Neural Networks@Google Research\nImplementing Neural networks"}
{"content2":"UIUC的Jia-Bin Huang同学收集了很多计算机视觉方面的代码，链接如下：\nhttps://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n这些代码很实用，可以让我们站在巨人的肩膀上~~\nTopic\nResources\nReferences\nFeature Extraction\nSIFT [1] [Demo program][SIFT Library] [VLFeat]\nPCA-SIFT [2] [Project]\nAffine-SIFT [3] [Project]\nSURF [4] [OpenSURF] [Matlab Wrapper]\nAffine Covariant Features [5] [Oxford project]\nMSER [6] [Oxford project] [VLFeat]\nGeometric Blur [7] [Code]\nLocal Self-Similarity Descriptor [8] [Oxford implementation]\nGlobal and Efficient Self-Similarity [9] [Code]\nHistogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]\nGIST [11] [Project]\nShape Context [12] [Project]\nColor Descriptor [13] [Project]\nPyramids of Histograms of Oriented Gradients [Code]\nSpace-Time Interest Points (STIP) [14] [Code]\nBoundary Preserving Dense Local Regions [15][Project]\nD. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004. [PDF]\nY. Ke and R. Sukthankar, PCA-SIFT: A More Distinctive Representation for Local Image Descriptors,CVPR, 2004. [PDF]\nJ.M. Morel and G.Yu, ASIFT, A new framework for fully affine invariant image comparison. SIAM Journal on Imaging Sciences, 2009. [PDF]\nH. Bay, T. Tuytelaars and L. V. Gool SURF: Speeded Up Robust Features,ECCV, 2006. [PDF]\nK. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir and L. Van Gool, A comparison of affine region detectors. IJCV, 2005. [PDF]\nJ. Matas, O. Chum, M. Urba, and T. Pajdla. Robust wide baseline stereo from maximally stable extremal regions. BMVC, 2002. [PDF]\nA. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition using low distortion correspondences. CVPR, 2005. [PDF]\nE. Shechtman and M. Irani. Matching local self-similarities across images and videos, CVPR, 2007. [PDF]\nT. Deselaers and V. Ferrari. Global and Efficient Self-Similarity for Object Classification and Detection. CVPR 2010. [PDF]\nN. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005. [PDF]\nA. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope, IJCV, 2001. [PDF]\nS. Belongie, J. Malik and J. Puzicha. Shape matching and object recognition using shape contexts, PAMI, 2002. [PDF]\nK. E. A. van de Sande, T. Gevers and Cees G. M. Snoek, Evaluating Color Descriptors for Object and Scene Recognition, PAMI, 2010.\nI. Laptev, On Space-Time Interest Points, IJCV, 2005. [PDF]\nJ. Kim and K. Grauman, Boundary Preserving Dense Local Regions, CVPR 2011. [PDF]\nImage Segmentation\nNormalized Cut [1] [Matlab code]\nGerg Mori' Superpixel code [2] [Matlab code]\nEfficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]\nMean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]\nOWT-UCM Hierarchical Segmentation [5] [Resources]\nTurbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]\nQuick-Shift [7] [VLFeat]\nSLIC Superpixels [8] [Project]\nSegmentation by Minimum Code Length [9] [Project]\nBiased Normalized Cut [10] [Project]\nSegmentation Tree [11-12] [Project]\nEntropy Rate Superpixel Segmentation [13] [Code]\nJ. Shi and J Malik, Normalized Cuts and Image Segmentation, PAMI, 2000 [PDF]\nX. Ren and J. Malik. Learning a classification model for segmentation.ICCV, 2003. [PDF]\nP. Felzenszwalb and D. Huttenlocher. Efficient Graph-Based Image Segmentation, IJCV 2004. [PDF]\nD. Comaniciu, P Meer. Mean Shift: A Robust Approach Toward Feature Space Analysis. PAMI 2002. [PDF]\nP. Arbelaez, M. Maire, C. Fowlkes and J. Malik. Contour Detection and Hierarchical Image Segmentation. PAMI, 2011. [PDF]\nA. Levinshtein, A. Stere, K. N. Kutulakos, D. J. Fleet, S. J. Dickinson, and K. Siddiqi, TurboPixels: Fast Superpixels Using Geometric Flows, PAMI 2009. [PDF]\nA. Vedaldi and S. Soatto, Quick Shift and Kernel Methodsfor Mode Seeking,ECCV, 2008. [PDF]\nR. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk, SLIC Superpixels, EPFL Technical Report, 2010. [PDF]\nA. Y. Yang, J. Wright, S. Shankar Sastry, Y. Ma , Unsupervised Segmentation of Natural Images via Lossy Data Compression, CVIU, 2007. [PDF]\nS. Maji, N. Vishnoi and J. Malik, Biased Normalized Cut, CVPR 2011\nE. Akbas and N. Ahuja, “From ramp discontinuities to segmentation tree,”  ACCV 2009. [PDF]\nN. Ahuja, “A Transform for Multiscale Image Segmentation by Integrated Edge and Region Detection,” PAMI 1996 [PDF]\nM.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, Entropy Rate Superpixel Segmentation, CVPR 2011 [PDF]\nObject Detection\nA simple object detector with boosting [Project]\nINRIA Object Detection and Localization Toolkit [1] [Project]\nDiscriminatively Trained Deformable Part Models [2] [Project]\nCascade Object Detection with Deformable Part Models [3] [Project]\nPoselet [4] [Project]\nImplicit Shape Model [5] [Project]\nViola and Jones's Face Detection [6] [Project]\nN. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005. [PDF]\nP. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan.\nObject Detection with Discriminatively Trained Part Based Models, PAMI, 2010 [PDF]\nP. Felzenszwalb, R. Girshick, D. McAllester. Cascade Object Detection with Deformable Part Models. CVPR 2010 [PDF]\nL. Bourdev, J. Malik, Poselets: Body Part Detectors Trained Using 3D Human Pose Annotations, ICCV 2009 [PDF]\nB. Leibe, A. Leonardis, B. Schiele. Robust Object Detection with Interleaved Categorization and Segmentation, IJCV, 2008. [PDF]\nP. Viola and M. Jones, Rapid Object Detection Using a Boosted Cascade of Simple Features, CVPR 2001. [PDF]\nSaliency Detection\nItti, Koch, and Niebur' saliency detection [1] [Matlab code]\nFrequency-tuned salient region detection [2] [Project]\nSaliency detection using maximum symmetric surround [3] [Project]\nAttention via Information Maximization [4] [Matlab code]\nContext-aware saliency detection [5] [Matlab code]\nGraph-based visual saliency [6] [Matlab code]\nSaliency detection: A spectral residual approach. [7] [Matlab code]\nSegmenting salient objects from images and videos. [8] [Matlab code]\nSaliency Using Natural statistics. [9] [Matlab code]\nDiscriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]\nLearning to Predict Where Humans Look [11] [Project]\nGlobal Contrast based Salient Region Detection [12] [Project]\nL. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, 1998. [PDF]\nR. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In CVPR, 2009. [PDF]\nR. Achanta and S. Susstrunk. Saliency detection using maximum symmetric surround. In ICIP, 2010. [PDF]\nN. Bruce and J. Tsotsos. Saliency based on information maximization. InNIPS, 2005. [PDF]\nS. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware saliency detection. In CVPR, 2010. [PDF]\nJ. Harel, C. Koch, and P. Perona. Graph-based visual saliency. NIPS, 2007. [PDF]\nX. Hou and L. Zhang. Saliency detection: A spectral residual approach.CVPR, 2007. [PDF]\nE. Rahtu, J. Kannala, M. Salo, and J. Heikkila. Segmenting salient objects from images and videos. CVPR, 2010. [PDF]\nL. Zhang, M. Tong, T. Marks, H. Shan, and G. Cottrell. Sun: A bayesian framework for saliency using natural statistics. Journal of Vision, 2008. [PDF]\nD. Gao and N. Vasconcelos, Discriminant Saliency for Visual Recognition from Cluttered Scenes, NIPS, 2004. [PDF]\nT. Judd and K. Ehinger and F. Durand and A. Torralba, Learning to Predict Where Humans Look, ICCV, 2009. [PDF]\nM.-M. Cheng, G.-X. Zhang, N. J. Mitra, X. Huang, S.-M. Hu. Global Contrast based Salient Region Detection. CVPR 2011.\nImage Classification\nPyramid Match [1] [Project]\nSpatial Pyramid Matching [2] [Code]\nLocality-constrained Linear Coding [3] [Project] [Matlab code]\nSparse Coding [4] [Project] [Matlab code]\nTexture Classification [5] [Project]\nMultiple Kernels for Image Classification [6] [Project]\nFeature Combination [7] [Project]\nSuperParsing [Code]\nK. Grauman and T. Darrell, The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features, ICCV 2005. [PDF]\nS. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories, CVPR 2006[PDF]\nJ. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained Linear Coding for Image Classification, CVPR, 2010 [PDF]\nJ. Yang, K. Yu, Y. Gong, T. Huang, Linear Spatial Pyramid Matching using Sparse Coding for Image Classification, CVPR, 2009 [PDF]\nM. Varma and A. Zisserman, A statistical approach to texture classification from single images, IJCV2005. [PDF]\nA. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman, Multiple Kernels for Object Detection. ICCV, 2009. [PDF]\nP. Gehler and S. Nowozin, On Feature Combination for Multiclass Object Detection, ICCV, 2009. [PDF]\nJ. Tighe and S. Lazebnik, SuperParsing: Scalable Nonparametric Image\nParsing with Superpixels, ECCV 2010. [PDF]\nCategory-Independent Object Proposal\nObjectness measure [1] [Code]\nParametric min-cut [2] [Project]\nObject proposal [3] [Project]\nB. Alexe, T. Deselaers, V. Ferrari, What is an Object?, CVPR 2010 [PDF]\nJ. Carreira and C. Sminchisescu. Constrained Parametric Min-Cuts for Automatic Object Segmentation, CVPR 2010. [PDF]\nI. Endres and D. Hoiem. Category Independent Object Proposals, ECCV 2010. [PDF]\nMRF\nGraph Cut [Project] [C++/Matlab Wrapper Code]\nY. Boykov, O. Veksler and R. Zabih, Fast Approximate Energy Minimization via Graph Cuts, PAMI 2001 [PDF]\nShadow Detection\nShadow Detection using Paired Region [Project]\nGround shadow detection [Project]\nR. Guo, Q. Dai and D. Hoiem, Single-Image Shadow Detection and Removal using Paired Regions, CVPR 2011 [PDF]\nJ.-F. Lalonde, A. A. Efros, S. G. Narasimhan, Detecting Ground Shadowsin Outdoor Consumer Photographs, ECCV 2010 [PDF]\nOptical Flow\nKanade-Lucas-Tomasi Feature Tracker [C Code]\nOptical Flow Matlab/C++ code by Ce Liu [Project]\nHorn and Schunck's method by Deqing Sun [Code]\nBlack and Anandan's method by Deqing Sun [Code]\nOptical flow code by Deqing Sun [Matlab Code] [Project]\nLarge Displacement Optical Flow by Thomas Brox [Executable for 64-bit Linux] [ Matlab Mex-functions for 64-bit Linux and 32-bit Windows] [Project]\nVariational Optical Flow by Thomas Brox [Executable for 64-bit Linux] [ Executable for 32-bit Windows ] [ Matlab Mex-functions for 64-bit Linux and 32-bit Windows ] [Project]\nB.D. Lucas and T. Kanade, An Iterative Image Registration Technique with an Application to Stereo Vision, IJCAI 1981. [PDF]\nJ. Shi, C. Tomasi, Good Feature to Track, CVPR 1994. [PDF]\nC. Liu. Beyond Pixels: Exploring New Representations and Applications for Motion Analysis. Doctoral Thesis. MIT 2009. [PDF]\nB.K.P. Horn and B.G. Schunck, Determining Optical Flow, Artificial Intelligence 1981. [PDF]\nM. J. Black and P. Anandan, A framework for the robust estimation of optical flow, ICCV 93. [PDF]\nD. Sun, S. Roth, and M. J. Black, Secrets of optical flow estimation and their principles, CVPR 2010. [PDF]\nT. Brox, J. Malik, Large displacement optical flow: descriptor matching in variational motion estimation, PAMI, 2010 [PDF]\nT. Brox, A. Bruhn, N. Papenberg, J. Weickert, High accuracy optical flow estimation based on a theory for warping, ECCV 2004 [PDF]\nObject Tracking\nParticle filter object tracking [1] [Project]\nKLT Tracker [2-3] [Project]\nMILTrack [4] [Code]\nIncremental Learning for Robust Visual Tracking [5] [Project]\nOnline Boosting Trackers [6-7] [Project]\nL1 Tracking [8] [Matlab code]\nP. Perez, C. Hue, J. Vermaak, and M. Gangnet. Color-Based Probabilistic Tracking ECCV, 2002. [PDF]\nB.D. Lucas and T. Kanade, An Iterative Image Registration Technique with an Application to Stereo Vision, IJCAI 1981. [PDF]\nJ. Shi, C. Tomasi, Good Feature to Track, CVPR 1994. [PDF]\nB. Babenko, M. H. Yang, S. Belongie, Robust Object Tracking with Online Multiple Instance Learning, PAMI 2011 [PDF]\nD. Ross, J. Lim, R.-S. Lin, M.-H. Yang, Incremental Learning for Robust Visual Tracking, IJCV 2007 [PDF]\nH. Grabner, and H. Bischof, On-line Boosting and Vision, CVPR 2006 [PDF]\nH. Grabner, C. Leistner, and H. Bischof, Semi-supervised On-line Boosting for Robust Tracking, ECCV 2008 [PDF]\nX. Mei and H. Ling, Robust Visual Tracking using L1 Minimization, ICCV, 2009. [PDF]\nImage Matting\nClosed Form Matting [Code]\nSpectral Matting [Project]\nLearning-based Matting [Code]\nA. Levin D. Lischinski and Y. Weiss. A Closed Form Solution to Natural Image Matting, PAMI 2008 [PDF]\nA. Levin, A. Rav-Acha, D. Lischinski. Spectral Matting. PAMI 2008. [PDF]\nY. Zheng and C. Kambhamettu, Learning Based Digital Matting, ICCV 2009 [PDF]\nBilateral Filtering\nFast Bilateral Filter [Project]\nReal-time O(1) Bilateral Filtering [Code]\nSVM for Edge-Preserving Filtering [Code]\nQ. Yang, K.-H. Tan and N. Ahuja,  Real-time O(1) Bilateral Filtering,\nCVPR 2009. [PDF]\nQ. Yang, S. Wang, and N. Ahuja, SVM for Edge-Preserving Filtering,\nCVPR 2010. [PDF]\nImage Denoising\nK-SVD [Matlab code]\nBLS-GSM [Project]\nBM3D [Project]\nFoE [Code]\nGFoE [Code]\nNon-local means [Code]\nKernel regression [Code]\nImage Super-Resolution\nMRF for image super-resolution [Project]\nMulti-frame image super-resolution [Project]\nUCSC Super-resolution [Project]\nSprarse coding super-resolution [Code]\nImage Deblurring\nEficient Marginal Likelihood Optimization in Blind Deconvolution [Code]\nAnalyzing spatially varying blur [Project]\nRadon Transform [Code]\nImage Quality Assessment\nFSIM [1] [Project]\nDegradation Model [2] [Project]\nSSIM [3] [Project]\nSPIQA [Code]\nL. Zhang, L. Zhang, X. Mou and D. Zhang, FSIM: A Feature Similarity Index for Image Quality Assessment, TIP 2011. [PDF]\nN. Damera-Venkata, and T. D. Kite, W. S. Geisler, B. L. Evans, and A. C. Bovik,Image Quality Assessment Based on a Degradation Model, TIP 2000. [PDF]\nZ. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, TIP 2004. [PDF]\nB. Ghanem, E. Resendiz, and N. Ahuja, Segmentation-Based Perceptual Image Quality Assessment (SPIQA), ICIP 2008. [PDF]\nDensity Estimation\nKernel Density Estimation Toolbox [Project]\nDimension Reduction\nDimensionality Reduction Toolbox [Project]\nISOMAP [Code]\nLLE [Project]\nLaplacian Eigenmaps [Code]\nDiffusion maps [Code]\nSparse Coding\nLow-Rank Matrix Completion\nNearest Neighbors matching\nANN: Approximate Nearest Neighbor Searching [Project] [Matlab wrapper]\nFLANN: Fast Library for Approximate Nearest Neighbors [Project]\nSteoreo\nStereoMatcher [Project]\nD. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms, IJCV 2002 [PDF]\nStructure from motion\nBoundler [1] [Project]\nN. Snavely, S. M. Seitz, R. Szeliski. Photo Tourism: Exploring image collections in 3D. SIGGRAPH, 2006. [PDF]\nDistance Transformation\nDistance Transforms of Sampled Functions [1] [Project]\nP. F. Felzenszwalb and D. P. Huttenlocher. Distance transforms of sampled functions. Technical report, Cornell University, 2004. [PDF]\nChamfer Matching\nFast Directional Chamfer Matching [Code]\nM.-Y. Liu, O. Tuzel, A. Veeraraghavan, and R. Chellappa, Fast Directional Chamfer Matching, CVPR 2010 [PDF]\nClustering\nK-Means [VLFeat] [Oxford code]\nSpectral Clustering [UW Project][Code] [Self-Tuning code]\nAffinity Propagation [Project]\nClassification\nSVM [Libsvm] [SVM-Light] [SVM-Struct]\nBoosting\nNaive Bayes\nRegression\nSVM\nRVM\nGPR\nMultiple Kernel Learning (MKL)\nSHOGUN [Project]\nOpenKernel.org [Project]\nDOGMA (online algorithms) [Project]\nSimpleMKL [Project]\nS. Sonnenburg, G. Rätsch, C. Schäfer, B. Schölkopf . Large scale multiple kernel learning. JMLR, 2006. [PDF]\nF. Orabona and L. Jie. Ultra-fast optimization algorithm for sparse multi kernel learning. ICML, 2011. [PDF]\nF. Orabona, L. Jie, and B. Caputo. Online-batch strongly convex multi kernel learning. CVPR, 2010. [PDF]\nA. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. Simplemkl. JMRL, 2008. [PDF]\nMultiple Instance Learning (MIL)\nMIForests [1] [Project]\nMILIS [2]\nMILES [3] [Project] [Code]\nDD-SVM [4] [Project]\nC. Leistner, A. Saffari, and H. Bischof, MIForests: Multiple-Instance Learning with Randomized Trees, ECCV 2010. [PDF]\nZ. Fu, A. Robles-Kelly, and J. Zhou, MILIS: Multiple instance learning with instance selection, PAMI 2010. [PDF]\nY. Chen, J. Bi and J. Z. Wang, MILES: Multiple-Instance Learning via Embedded Instance Selection. PAMI 2006 [PDF]\nYixin Chen and James Z. Wang, Image Categorization by Learning and Reasoning with Regions, JMLR 2004. [PDF]\nOther Utilities\nCode for downloading Flickr images, by James Hays [Code]\nThe Lightspeed Matlab Toolbox by Tom Minka [Code]\nMATLAB Functions for Multiple View Geometry [Code]\nPeter's Functions for Computer Vision [Code]\nStatistical Pattern Recognition Toolbox [Code]\nUseful Links (dataset, lectures, and other softwares)\nConference Information\nComputer Image Analysis, Computer Vision Conferences\nPapers\nComputer vision paper on the web\nNIPS Proceedings\nDatasets\nCompiled list of recognition datasets\nComputer vision dataset from CMU\nLectures\nVideolectures\nSource Codes\nComputer Vision Algorithm Implementations\nOpenCV\nSource Code Collection for Reproducible Research"}
{"content2":"【新智元导读】The M Tank发布了一份对计算机视觉领域最近一年进展的报告《A Year in Computer Vision》，详述了四大部分的内容，包括：分类/定位，目标检测，目标追踪；分割，超分辨率，自动上色，风格迁移，动作识别；3D世界理解；卷积网络架构，数据集，新兴应用等。不管对于初学者还是紧追前沿的研究者，这些都是不可多得的有用资料。\n报告下载地址：http://www.themtank.org/a-year-in-computer-vision\n本报告包括以下内容：\n第一部分：分类/定位，目标检测，目标追踪\n第二部分：分割，超分辨率，自动上色，风格迁移，动作识别\n第三部分：3D世界理解\n第四部分：卷积网络架构，数据集，新兴应用\n综述：计算机视觉最重要的进展\n计算机视觉通常是指赋予机器视觉的能力，或赋予机器能够直观地分析它们的环境和内在的刺激。这个过程通常包括对一个图像、很多图像或视频的评估。英国机器视觉协会（BMVA）将计算机视觉定义为“自动提取、分析和理解来自单个图像或一系列图像的有用信息的过程”。\n这个定义中的“理解”这个词说明了计算机视觉的重要性和复杂性。对我们的环境的真正理解不是仅仅通过视觉表现来实现的。相反，视觉信号通过视觉神经传递给主视觉皮层，并由大脑来解释。从这些感官信息中得出的解释包含了我们的自然编程和主观体验的总体，即进化是如何让我们生存下来，以及我们在生活中对世界的理解。\n从这个角度看，视觉仅仅与图像的传输有关；虽然计算机认为图像与思想或认知更相似，涉及多个大脑区域的协作。因此，许多人认为由于计算机视觉的跨领域性质，对视觉环境及其背景的真正理解能为未来的强人工智能的迭代开拓道路。\n然而，我们仍然处于这个迷人的领域的萌芽阶段。这份报告的目的是为了让我们对近年计算机视觉领域一些最重要的进展。尽管我们尽可能写得简明，但由于领域的特殊性，可能有些部分读起来比较晦涩。我们为每个主题提供了基本的定义，但这些定义通常只是对关键概念的基本解释。为了将关注的重点放在2016年的新工作，限于篇幅，这份报告会遗漏一些内容。\n其中明显省略的一个内容是卷积神经网络（以下简称CNN或ConvNet）的功能，因为它在计算机视觉领域无处不在。2012年出现的 AlexNet（一个在ImageNet竞赛获得冠军的CNN架构）的成功带来了计算机视觉研究的转折点，许多研究人员开始采用基于神经网络的方法，开启了计算机视觉的新时代。\n4年过去了，CNN的各种变体仍然是视觉任务中新的神经网络架构的主要部分，研究人员像搭乐高积木一样创造它们，这是对开源信息和深度学习能力的有力证明。不过，解释CNN的事情最好留给在这方面有更深入的专业知识的人。\n对于那些希望在继续进行之前快速了解基础知识的读者，我们推荐下面的参考资料的前两个。对于那些希望进一步了解的人，以下的资料都值得一看：\n深度神经网络如何看待你的自拍？by Andrej Karpathy 这篇文章能很好地帮助你了解产品和应用背后的CNN技术。\nQuora：什么是卷积神经网络。这个quora问题下的回答有很多很好的参考链接和解释，适合初学者。\nCS231n：视觉识别的卷积神经网络。这是斯坦福大学的一门深度的课程。\n《深度学习》（Goodfellow, Bengio & Courville, 2016）第九章对CNN特征和功能提供了详细的解释。\n对于那些希望更多地了解关于神经网络和深度学习的读者，我们推荐:\n神经网络和深度学习(Nielsen，2017)，这是一本免费的电子版教科书，它为读者提供了对于神经网络和深度学习的复杂性的非常直观的理解。\n我们希望读者能从这份报告的信息汇总中获益，无论以往的经验如何，都可以进一步增加知识。\n本报告包括以下部分（限于篇幅，文章省略了参考文献标识，请至原文查看）：\n第一部分：分类/定位，目标检测，目标追踪\n第二部分：分割，超分辨率，自动上色，风格迁移，动作识别\n第三部分：3D世界理解\n第四部分：卷积网络架构，数据集，新兴应用\n第一部分：分类/定位，目标检测，目标追踪\n分类/定位\n涉及到图像时，“分类”任务通常是指给一个图像分配一个标签，例如“猫”。这种情况下，“定位”（locolisation）指的是找到某个对象（object）在图像中的位置，通常输出为对象周围的某种形式的边界框。当前在ImageNet竞赛的图像分类/定位技术准确性超过一个经训练的人类。\n图：计算机视觉任务\nSource: Fei-Fei Li, Andrej Karpathy & Justin Johnson (2016) cs231n, Lecture 8 - Slide 8, Spatial Localization and Detection (01/02/2016). Available: http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf\n然而，由于更大的数据集（增加了11个类别）的引入，这很可能为近期的进展提供新的度量标准。在这一点上，Keras的作者François Chollet已经在有超过3.5亿的多标签图像，包含17000个类的谷歌内部数据集应用了新的技术，包括流行的Xception架构。\n图：ILSVRC（2010-2016）图像分类/定位结果\nSource: Jia Deng (2016). ILSVRC2016 object localisation: introduction, results. Slide 2. Available: http://image-net.org/challenges/talks/2016/ILSVRC2016_10_09_clsloc.pdf\n2016年在ImageNet LSVRC 的一些主要进步：\n场景分类（Scene Classification）是指用“温室”、“体育馆”、“大教堂”等特定场景来给图像贴上标签的任务。去年，ImageNet 进行了一个场景分类竞赛，使用Places2数据集的一个子集：包含800万张图片，用365类场景训练。Hikvision 以 9% top-5 error赢了比赛，利用一个深 Inception-style 网络，以及一个不特别深的残差网络。\nTrimps-Soushen 以 2.99% 的top-5分类错误和7.71％的定位错误赢得了ImageNet分类任务。\nFacebook的ResNeXt通过使用扩展原始ResNet架构的新架构，以3.03％在top-5 分类错误中排名第二。\n对象检测（Object Dection）\n对象检测的过程即检测图像中的某个对象。ILSVRC 2016 对对象检测的定义包括为单个对象输出边界框和标签。这不同于分类/定位任务，分类和定位的应用是多个对象，而不是一个对象。\n图：对象检测（人脸是该情况需要检测的唯一一个类别）\nSource: Hu and Ramanan (2016, p. 1）\n2016年对象检测的主要趋势是转向更快、更高效的检测系统。这在YOLO、SSD和R-FCN等方法中表现出来，目的是为了在整个图像上共享计算。因此，这些与计算昂贵的Fast R-CNN和Faster R-CNN相区别。这通常被称为“端到端训练/学习”。\n其基本原理是避免将单独的算法集中在各自的子问题上，因为这通常会增加训练时间，并降低网络的准确性。也就是说，这种网络的端到端适应通常是在初始的子网络解决方案之后进行的，因此，是一种回顾性优化（ retrospective optimisation）。当然，Fast R-CNN和Faster R-CNN仍然是非常有效的，并且被广泛应用于物体检测。\nSSD：Single Shot MultiBox Detector 这篇论文利用单个神经网络来封装所有必要的计算，它实现了“75.1％的mAP，超越了更先进的R-CNN模型”（Liu et al., 2016）。我们在2016年看到的最令人印象深刻的系统之一是“YOLO9000：Better, Faster, Stronger”，其中介绍了YOLOv2和YOLO9000检测系统。YOLOv2大大改善了初始的YOLO模型，并且能够以非常高的FPS获得更好的结果。除了完成速度之外，系统在特定对象检测数据集上的性能优于使用ResNet和SSD的Faster-RCNN。\nFAIR的Feature Pyramid Networks for Object Detection\nR-FCN: Object Detection via Region-based Fully Convolutional Networks\n图：不同架构在对象检测任务的准确率\nSource: Huang et al. (2016, p. 9)\nILSVRC 和 COCO Challenge的结果\nCOCO（Common Objects in Context）是另一个流行的图像数据集。不过，它比ImageNet小，也更具有策略性，在更广泛的场景理解的背景下着重于对象识别。组织者每年都要针对对象检测，分割和关键点组织竞赛。 ILSVRC 和COCO 对象检测挑战的检测是：\nImageNet LSVRC Object Detection from Images (DET):CUImage 66% meanAP. Won 109 out of 200 object categories.\nImageNet LSVRC Object Detection from video (VID): NUIST 80.8% mean AP\nImageNet LSVRC Object Detection from video with tracking: CUvideo 55.8% mean AP\nCOCO 2016 Detection Challenge (bounding boxes): G-RMI (Google) 41.5% AP (4.2% absolute percentage increase from 2015 winner MSRAVC)\n图：ILSVRC 对象检测结果（2013-2016）\nSource: ImageNet. 2016. [Online] Workshop Presentation, Slide 2. Available: http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf\n对象跟踪\n对象跟踪（Object Tracking）是指在给定场景中跟踪特定对象或多个对象的过程。传统上，它在视频和现实世界的交互中都有应用，例如，对象跟踪对自动驾驶系统至关重要。\n用于对象跟踪的全卷积的Siamese网络（Fully-Convolutional Siamese Networks for Object Tracking）结合了一个基本的跟踪算法和一个Siamese网络，经过端到端的训练，它实现了SOTA，并且可以在帧速率超过实时的情况下进行操作。\n利用深度回归网络学习以100 FPS跟踪（Learning to Track at 100 FPS with Deep Regression Networks）是另一篇试图通过在线训练方法改善现有问题的论文。作者提出了一种利用前馈网络的跟踪器来学习对象运动、外观和定位的一般关系，从而有效地跟踪没有在线训练的新对象。它提供了SOTA标准跟踪基准，同时实现了“以100 fps跟踪通用对象”(Held et al., 2016)。\nDeep Motion Features for Visual Tracking 综合了人工特征，deep RGB/外观特征(来自CNN)，以及深度运动特性(在光流图像上训练)来实现SOTA。虽然Deep Motion Feature在动作识别和视频分类中很常见，但作者称这是第一次使用视觉追踪技术。这篇论文获得了2016年ICPR的最佳论文，用于“计算机视觉和机器人视觉”跟踪。\nVirtual Worlds as Proxy for Multi-Object Tracking Analysis，这篇文章在现有的视频跟踪基准和数据集中，提出了一种新的现实世界克隆方法，该方法可以从零开始生成丰富的、虚拟的、合成的、逼真的环境，并使用全标签来克服现有数据集的不足。这些生成的图像被自动地标记为准确的ground truth，允许包括对象检测/跟踪等一系列应用。\n全卷积网络的全局最优对象跟踪（Globally Optimal Object Tracking with Fully Convolutional Networks），这篇文章解决了对象的变化和遮挡问题，并将它们作为对象跟踪中的两个根限制。作者称，“我们提出的方法利用一个全卷积的网络解决了对象的外形变化问题，并处理了动态规划的遮挡问题”(Lee et al., 2016)。\n第二部分：分割、 超分辨率/色彩化/风格迁移、 行为识别\n计算机视觉的中心就是分割的过程，它将整个图像分成像素组，然后可以对这些组进行标记和分类。此外，语义分割通过试图在语义上理解图像中每个像素的角色是猫，汽车还是其他类型的，又在这一方向上前进了一步。实例分割通过分割不同类的实例来进一步实现这一点，比如，用三种不同颜色标记三只不同的狗。这是目前在自动驾驶技术套件中使用的计算机视觉应用的一大集中点。\n也许今年分割领域的一些最好的提升来自FAIR，他们从2015年开始继续深入研究DeepMask。DeepMask生成粗糙的“mask”作为分割的初始形式。 2016年，Fair推出了SharpMask ，它改进了DeepMask提供的“mask”，纠正了细节的缺失，改善了语义分割。除此之外，MultiPathNet 标识了每个mask描绘的对象。\n“为了捕捉一般的物体形状，你必须对你正在看的东西有一个高水平的理解（DeepMask），但是要准确地描述边界，你需要再回过去看低水平的特征，一直到像素（SharpMask）。“ - Piotr Dollar，2016\n图：Demonstration of FAIR techniques in action\n视频传播网络（Vedio Propagation Network）试图创建一个简单的模型来传播准确的对象mask，在第一帧分配整个视频序列以及一些附加信息。\n2016年，研究人员开始寻找替代网络配置来解决上述的规模和本地化问题。 DeepLab 就是这样一个例子，它为语义图像分割任务取得了令人激动的结果。 Khoreva等人（2016）基于Deeplab早期的工作（大约在2015年），提出了一种弱监督训练方法，可以获得与完全监督网络相当的结果。\n计算机视觉通过使用端到端网络进一步完善了有用信息网络的共享方式，减少了分类中，多个全向子任务的计算需求。两个关键的论文使用这种方法是：\n100 Layers Tiramisu是一个完全卷积的DenseNet，它以前馈的方式将每一层连接到每一层。它还通过较少的参数和训练/处理在多个基准数据集上实现SOTA。\nFully Convolutional Instance-aware Semantic Segmentation共同执行实例掩码预测和分类（两个子任务）。COCO分割挑战冠军MSRA。 37.3％AP。比起2015 COCO挑战赛中的MSRAVC，绝对跃升了9.1％。\n虽然ENet是一种用于实时语义分割的DNN体系结构，但它并不属于这一类别，它证明了降低计算成本和提供更多移动设备访问的商业价值。\n我们的工作希望将尽可能多的这些进步回溯到有形的公开应用。考虑到这一点，以下内容包含2016年一些最有意义的医疗保健应用细分市场：\nA Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images\n3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study\nSemi-supervised Learning using Denoising Autoencoders for Brain Lesion Detection and Segmentation\n3D Ultrasound image segmentation: A Survey\nA Fully Convolutional Neural Network based Structured Prediction Approach Towards the Retinal Vessel Segmentation\n3-D Convolutional Neural Networks for Glioblastoma Segmentation\n我们最喜欢的准医学分割应用之一是FusionNet——一个深度全卷积神经网络，用于连接组学的图像分割，基于SOTA电子显微镜（EM）分割方法。\n超分辨率、风格迁移和着色\n并非计算机视觉领域的所有研究都是为了扩展机器的伪认知能力，而且神经网络的神话般的可塑性以及其他ML技术常常适用于各种其他新颖的应用，这些应用可以渗透到公共空间中。超分辨率方案，风格转移和着色去年的进步占据了整个领域。\n超分辨率指的是从低分辨率对应物估计高分辨率图像的过程，以及不同放大倍数下图像特征的预测，这是人脑几乎毫不费力地完成的。最初的超分辨率是通过简单的技术，如bicubic-interpolation和最近邻。在商业应用方面，克服低分辨率限制和实现“CSI Miami”风格图像增强的愿望推动了该领域的研究。以下是今年的一些进展及其潜在的影响：\nNeural Enhance 是Alex J. Champandard的创意，结合四篇不同研究论文的方法来实现超分辨率方法。\n实时视频超分辨率解决方案也在2016年进行了两次著名的尝试。\nRAISR：来自Google的快速而准确的图像超分辨率方法。通过使用低分辨率和高分辨率图像对训练滤波器，避免了神经网络方法的昂贵内存和速度要求。作为基于学习的框架，RAISR比同类算法快两个数量级，并且与基于神经网络的方法相比，具有最小的存储器需求。因此超分辨率可以扩展到个人设备。\n生成对抗网络（GAN）的使用代表了当前用于超分辨率的SOTA：\nSRGAN 通过训练区分超分辨率和原始照片真实图像的辨别器网络，在公共基准测试中提供多采样图像的逼真纹理。\n尽管SRResNet在峰值信噪比（PSNR）方面的表现最佳，但SRGAN获得更精细的纹理细节并达到最佳的平均评分（MOS），SRGAN表现最佳。\n“据我们所知，这是第一个能够推出4倍放大因子的照片般真实的自然图像的框架。”以前所有的方法都无法在较大的放大因子下恢复更精细的纹理细节。\nAmortised MAP Inference for Image Super-resolution 提出了一种使用卷积神经网络计算最大后验（MAP）推断的方法。但是，他们的研究提出了三种优化方法，GAN在其中实时图像数据上表现明显更好。\n毫无疑问，Style Transfer集中体现了神经网络在公共领域的新用途，特别是去年的Facebook集成以及像Prisma 和Artomatix 这样的公司。风格转换是一种较旧的技术，但在2015年出版了一个神经算法的艺术风格转换为神经网络。从那时起，风格转移的概念被Nikulin和Novak扩展，并且也被用于视频，就像计算机视觉中其他的共同进步一样。\n图：风格迁移的例子\n风格转换作为一个主题，一旦可视化是相当直观的，比如，拍摄一幅图像，并用不同的图像的风格特征呈现。例如，以着名的绘画或艺术家的风格。今年Facebook发布了Caffe2Go，将其深度学习系统整合到移动设备中。谷歌也发布了一些有趣的作品，试图融合多种风格，生成完全独特的图像风格。\n除了移动端集成之外，风格转换还可以用于创建游戏资产。我们团队的成员最近看到了Artomatix的创始人兼首席技术官Eric Risser的演讲，他讨论了该技术在游戏内容生成方面的新颖应用（纹理突变等），因此大大减少了传统纹理艺术家的工作。\n着色\n着色是将单色图像更改为新的全色版本的过程。最初，这是由那些精心挑选的颜色由负责每个图像中的特定像素的人手动完成的。2016年，这一过程自动化成为可能，同时保持了以人类为中心的色彩过程的现实主义的外观。虽然人类可能无法准确地表现给定场景的真实色彩，但是他们的真实世界知识允许以与图像一致的方式和观看所述图像的另一个人一致的方式应用颜色。\n着色的过程是有趣的，因为网络基于对物体位置，纹理和环境的理解（例如，图像）为图像分配最可能的着色。它知道皮肤是粉红色，天空是蓝色的。\n“而且，我们的架构可以处理任何分辨率的图像，而不像现在大多数基于CNN的方法。”\n在一个测试中，他们的色彩是多么的自然，用户从他们的模型中得到一个随机的图像，并被问到，“这个图像看起来是自然的吗？\n他们的方法达到了92.6％，基线达到了大约70％，而实际情况（实际彩色照片）被认为是自然的97.7％。\n行为识别\n行为识别的任务是指在给定的视频帧内动作的分类，以及最近才出现的，用算法预测在动作发生之前几帧的可能的相互作用的结果。在这方面，我们看到最近的研究尝试将上下文语境嵌入到算法决策中，类似于计算机视觉的其他领域。这个领域的一些关键论文是：\nLong-term Temporal Convolutions for Action Recognition利用人类行为的时空结构，即特定的移动和持续时间，以使用CNN变体正确识别动作。为了克服CNN在长期行为的次优建模，作者提出了一种具有长时间卷积（LTC-CNN）的神经网络来提高动作识别的准确性。简而言之，LTC可以查看视频的较大部分来识别操作。他们的方法使用和扩展了3D CNN，以便在更充分的时间尺度上进行行动表示。\n“我们报告了人类行为识别UCF101（92.7％）和HMDB51（67.2％）两个具有挑战性的基准的最新成果。\n用于视频动作识别的时空残差网络将两个流CNN的变体应用于动作识别的任务，该任务结合了来自传统CNN方法和最近普及的残留网络（ResNet）的技术。这两种方法从视觉皮层功能的神经科学假设中获得灵感，即分开的路径识别物体的形状/颜色和运动。作者通过注入两个CNN流之间的剩余连接来结合ResNets的分类优势。\nAnticipating Visual Representations from Unlabeled Video[89]是一个有趣的论文，尽管不是严格的行为分类。该程序预测了在一个动作之前一个视频帧序列可能发生的动作。该方法使用视觉表示而不是逐像素分类，这意味着程序可以在没有标记数据的情况下运行，利用深度神经网络的特征学习特性。\nThumos Action Recognition Challenge 的组织者发表了一篇论文，描述了最近几年来Action Action Recognition的一般方法。本文还提供了2013-2015年挑战的概要，以及如何通过行动识别让计算机更全面地了解视频的挑战和想法的未来方向。\n第三部分 走向理解3D世界\n在计算机视觉中，正如我们所看到的，场景，对象和活动的分类以及边界框和图像分割的输出是许多新研究的重点。实质上，这些方法应用计算来获得图像的二维空间的“理解”。然而，批评者指出，3D理解对于解释系统成功和现实世界导航是必不可少的。\n例如，一个网络可能会在图像中找到一只猫，为它的所有像素着色，并将其归类为一只猫。但是，在猫所处的环境中，网络是否完全理解图像中猫的位置？\n有人认为，从上述任务中，计算机对于3D世界的了解很少。与此相反，即使在看2D图片（即，透视图，遮挡，深度，场景中的对象如何相关）等情况下，人们也能够以3D来理解世界。将这些3D表示及其相关知识传递给人造系统代表了下一个伟大计算机视觉的前沿。一般认为这样做的一个主要原因是：\n“场景的2D投影是构成场景的相机，灯光和物体的属性和位置的复杂功能的组合。如果赋予3D理解，智能体可以从这种复杂性中抽象出来，形成稳定的，不受限制的表示，例如，认识到在不同的光照条件下，或者在部分遮挡下，是从上面或从侧面看的椅子。“\n但是，3D理解传统上面临着几个障碍。首先关注“自我和正常遮挡”问题以及适合给定2D表示的众多3D形状。由于无法将相同结构的不同图像映射到相同的3D空间以及处理这些表示的多模态，所以理解问题变得更加复杂。最后，实况3D数据集传统上相当昂贵且难以获得，当与表示3D结构的不同方法结合时，可能导致训练限制。\n我们认为，在这个领域进行的工作很重要，需要注意。从早期的AGI系统和机器人技术的早期理论应用，到在不久的将来会影响我们社会，尽管还在萌芽期，由于利润丰厚的商业应用，我们谨慎地预测这一计算机视觉领域的指数级增长，这意味着计算机很快就可以开始推理世界，而不仅仅是像素。\nOctNet: Learning Deep 3D Representations at High Resolutions\nObjectNet3D: A Large Scale Database for 3D Object Recognition\n3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction\n3D Shape Induction from 2D Views of Multiple Objects\nUnsupervised Learning of 3D Structure from Images\n人类姿势预估和关键点监测\n人体姿势估计试图找出人体部位的方向和构型。 2D人体姿势估计或关键点检测一般是指定人体的身体部位，例如寻找膝盖，眼睛，脚等的二维位置。\n然而，三维姿态估计通过在三维空间中找到身体部位的方向来进一步进行，然后可以执行形状估计/建模的可选步骤。这些分支已经有了很大的改进。\n在过去的几年中，在竞争性评估方面，“COCO2016挑战包括同时检测人和本地化关键点”。 ECCV 供了有关这些主题的更多的文献，但是我们想强调以下几篇论文：\nRealtime Multi-Person 2D Pose Estimation using Part Affinity Fields\nKeep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image\n重构\n如前所述，前面的部分介绍了重构的一些例子，但总的来说重点是物体，特别是它们的形状和姿态。虽然其中一些在技术上是重构的，但是该领域本身包括许多不同类型的重构，例如，场景重构，多视点和单视点重建，运动结构（SfM），SLAM等。此外，一些重构方法利用附加（和多个）传感器和设备，例如事件或RGB-D摄像机，多种技术来推动进步。\n结果？整个场景可以非刚性地重建并且在时空上改变，例如，对你自己的高保真重构，以及你的动作进行实时更新。\n如前所述，围绕2D图像映射到3D空间的问题持续存在。以下文章介绍了大量创建高保真实时重建的方法：\nFusion4D: Real-time Performance Capture of Challenging Scenes\nReal-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera\nUnsupervised CNN for Single View Depth Estimation: Geometry to the Rescue\n其他未分类3D\nIM2CA\nLearning Motion Patterns in Videos\nDeep Image Homography Estimation\ngvnn: Neural Network Library for Geometric Computer Vision\n3D summation and SLAM\n在整个这一节中，我们在3D理解领域进行了一个横切面似的介绍，主要侧重于姿态估计，重构，深度估计和同形目录。但是，还有更多的精彩的工作被我们忽略了，我们在数量上受到限制。所以，我们希望给读者提供一个宝贵的出发点。\n大部分突出显示的作品可能被归类于几何视觉，它通常涉及从图像直接测量真实世界的数量，如距离，形状，面积和体积。我们的启发是基于识别的任务比通常涉及几何视觉中的应用程序更关注更高级别的语义信息。但是，我们经常发现，这些3D理解的不同领域大部分是密不可分的。\n最大的几何问题之一是SLAM，研究人员正在考虑SLAM是否会成为深度学习所面临的下一个问题。所谓“深度学习的普遍性”的怀疑论者，其中有很多都指出了SLAM作为算法的重要性和功能性：\n“视觉SLAM算法能够同时建立世界三维地图，同时跟踪摄像机的位置和方向。” SLAM方法的几何估计部分目前不适合深度学习方法，所以端到端学习不太可能。 SLAM代表了机器人中最重要的算法之一，并且是从计算机视觉领域的大量输入设计的。该技术已经在Google Maps，自动驾驶汽车，Google Tango 等AR设备，甚至Mars Luver等应用。\n第四部分：卷积架构、数据集、新兴应用\nConvNet架构最近在计算机视觉之外发现了许多新颖的应用程序，其中一些应用程序将在我们即将发布的论文中出现。然而，他们继续在计算机视觉领域占有突出的地位，架构上的进步为本文提到的许多应用和任务提供了速度，准确性和训练方面的改进。\n图：DenseNet架构\n基于这个原因，ConvNet体系结构对整个计算机视觉至关重要。以下是2016年以来一些值得关注的ConvNet架构，其中许多从ResNets最近的成功中获得灵感。\nInception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\nDensely Connected Convolutional Networks\nFractalNet Ultra-Deep Neural Networks without Residuals\nLets keep it simple: using simple architectures to outperform deeper architectures\nSwapout: Learning an ensemble of deep architectures\nSqueezeNet\nConcatenated Rectified Linear Units (CRelu)\nExponential Linear Units (ELUs)\nParametric Exponential Linear Unit (PELU)\nHarmonic CNNs\nExploiting Cyclic Symmetry in Convolutional Neural Networks\nSteerable CNNs\n残差网络（Residual Networks）\n图：Test-Error Rates on CIFAR Datasets\n随着微软ResNet的成功，Residual Networks及其变体在2016年变得非常受欢迎，现在提供了许多开源版本和预训练模型。在2015年，ResNet在ImageNet的检测，本地化和分类任务以及COCO的检测和分段挑战中获得了第一名。虽然深度问题仍然存在，但ResNet处理梯度消失的问题为“深度增加产生超级抽象”提供了更多的动力，这是目前深度学习的基础。\nResNet通常被概念化为一个较浅的网络集合，它通过运行平行于其卷积层的快捷连接来抵消深度神经网络（DNN）的层次性。这些快捷方式或跳过连接可减轻与DNN相关的消失/爆炸梯度问题，从而允许在网络层中更容易地反向传播梯度。\n残差学习、理论与进展\nWide Residual Networks\nDeep Networks with Stochastic Depth\nLearning Identity Mappings with Residual Gates\nResidual Networks Behave Like Ensembles of Relatively Shallow Networks\nIdentity Mappings in Deep Residual Networks\nMulti-Residual Networks: Improving the Speed and Accuracy of Residual Networks\nHighway and Residual Networks learn Unrolled Iterative Estimation\nResidual Networks of Residual Networks: Multilevel Residual Networks\nResnet in Resnet: Generalizing Residual Architectures\nWider or Deeper: Revisiting the ResNet Model for Visual Recognition\nBridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex\nConvolutional Residual Memory Networks\nIdentity Matters in Deep Learning\nDeep Residual Networks with Exponential Linear Unit\nWeighted Residuals for Very Deep Networks\n数据集\nPlaces2\nSceneNet RGB-D\nCMPlaces\nMS-Celeb-1M\nOpen Images\nYouTube-8M\n一些用例和趋势\n来自Facebook的盲人应用程序和百度的硬件\n情感检测结合了面部检测和语义分析，并且正在迅速增长。目前有20多个API可用。\n从航空影像中提取道路，从航空地图和人口密度地图中分类土地。\n尽管目前还存在一些功能性问题，但Amazon Go进一步提高了计算机视觉的形象，证明了无排队的购物体验。\n对于我们基本上没有提到无人驾驶，我们做了大量的工作。然而，对于那些希望深入研究一般市场趋势的人来说，莫里茨·穆勒 - 弗雷塔格（Moritz Mueller-Freitag）就德国汽车工业和自动驾驶汽车的影响作了精彩的介绍。\n其他有趣的领域：图像检索/搜索，手势识别，修复和面部重建。\n数字成像与医学通讯（DICOM）和其他医学应用（特别是与成像相关的）。例如，有许多Kaggle检测竞赛（肺癌，宫颈癌），其中一些有较大的金钱诱因，其中的算法试图在分类/检测任务中胜过专家。\n硬件和市场\n机器人视觉/机器视觉（独立领域）和物联网的潜在目标市场不断壮大。我们个人最喜欢的是一个日本的农民的孩子使用深度学习，树莓派和TensorFlow对黄瓜形状，大小和颜色进行分类。这使他的母亲分拣黄瓜所花的人力时间大大减少。\n计算需求的缩减和移动到移动的趋势是显而易见的，但是它也是通过硬件加速来实现的。很快我们会看到口袋大小的CNN和视觉处理单元（VPUs）到处都是。例如，Movidius Myriad2被谷歌的Project Tango和无人机所使用。\nMovidius Fathom 也使用了Myriad2的技术，允许用户将SOTA计算机视觉性能添加到消费类设备中。具有USB棒的物理特性的Fathom棒将神经网络的能力带到几乎任何设备：一根棒上的大脑。\n传感器和系统使用可见光以外的东西。例子包括雷达，热像仪，高光谱成像，声纳，磁共振成像等。\nLIDAR的成本降低，它使用光线和雷达来测量距离，与普通的RGB相机相比具有许多优点。目前有不少于500美元的LIDAR设备。\nHololens和近乎无数的其他增强现实头盔进入市场。\nGoogle的Project Tango 代表了SLAM的下一个大型商业化领域。 Tango是一个增强现实计算平台，包含新颖的软件和硬件。 Tango允许在不使用GPS或其他外部信息的情况下检测移动设备相对于世界的位置，同时以3D形式绘制设备周围的区域。\nGoogle合作伙伴联想于2016年推出了价格适中的Tango手机，允许数百名开发人员开始为该平台创建应用程序。 Tango采用以下软件技术：运动跟踪，区域学习和深度感知。\n与其他领域结合的前沿研究：\n唇语\n生成模型\n结论\n总之，我们想突出一些在我们的研究回顾过程中反复出现的趋势和反复出现的主题。首先，我们希望引起人们对机器学习研究社区极度追求优化的关注。这是最值得注意的，体现在这一年里精确率的不断提升。\n错误率不是唯一的狂热优化参数，研究人员致力于提高速度、效率，甚至算法能够以全新的方式推广到其他任务和问题。我们意识到这是研究的前沿，包括one-shot learning、生成模型、迁移学习，以及最近的evolutionary learning，我们认为这些研究原则正逐渐产生更大的影响。\n虽然这最后一点毫无疑问是值得称赞的，而不是对这一趋势的贬低，但人们还是禁不住要把他们的注意力放在（非常）的通用人工智能。我们只是希望向专家和非专业人士强调，这一担忧源自于此，来自计算机视觉和其他人工智能领域的惊人进展。通过对这些进步及其总体影响的教育，可以减少公众不必要的担忧。这可能会反过来冷却媒体的情绪和减少有关AI的错误信息。\n出于两个原因，我们选择专注于一年的时间里的进展。第一个原因与这一领域的新工作数量之大有关。即使对那些密切关注这一领域的人来说，随着出版物数量呈指数级的增长，跟上研究的步伐也变得越来越困难。第二个原因，让我们回头看看这一年内的变化。\n在了解这一年的进展的同时，读者可以了解目前的研究进展。在这么短的时间跨度里，我们看到了这么多的进步，这是如何得到的？研究人员形成了以以前的方法（架构、元架构、技术、想法、技巧、结果等）和基础设施（Keras、TensorFlow、PyTorch、TPU等）的全球社区，这不禁值得鼓励，也值得庆祝。很少有开源社区像这样不断吸引新的研究人员，并将它的技术应用于经济学、物理学和其他无数领域。\n对于那些尚未注意到的人来说，理解这一点非常重要，即在许多不同声音中，宣称对这种技术的本质有理解，至少有共识，认同这项技术将以新的令人兴奋的方式改变世界。然而，在这些改变实现之前，仍存在许多分歧。\n我们将继续尽最大的努力提供信息。有了这样的资源，我们希望满足那些希望跟踪计算机视觉和人工智能的进展的人的需求，我们的项目希望为开源革命增添一些价值，而这个革命正在技术领域悄然发生。\n报告地址：http://www.themtank.org/a-year-in-computer-vision"}
{"content2":"计算机视觉顾名思义，就是让计算机具备像人眼一样观察和识别的能力，更进一步的说，就是指用摄像机和电脑代替人眼对目标进行识别、跟踪和测量，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。\n那么计算机视觉和人工智能是什么联系呢？\n作为一个科学学科，计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取“信息”的人工智能系统。计算机视觉目前还主要停留在图像信息表达和物体识别阶段，人工智能更强调推理和决策。\n目前计算机视觉主要应用在安防摄像头、交通摄像头、无人驾驶、无人机、金融、医疗等方面。国内代表性公司有海康威视、大华股份等传统大公司，还有商汤科技、云从科技、依图科技以及旷视科技等独角兽企业，还有思岚科技、速感科技、阅面科技、云天励飞、Yi+、图漾信息、码隆科技、格灵深瞳、Insta360等创业企业。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n人工智能时代，AI人才都有哪些特征？\nhttp://www.duozhishidai.com/article-1792-1.html\n深度学习与计算机视觉的具体介绍\nhttp://www.duozhishidai.com/article-15924-1.html\n计算机视觉如何入门\nhttp://www.duozhishidai.com/article-8235-1.html\n计算机视觉影响人工智能的发展方式，主要有哪五种？\nhttp://www.duozhishidai.com/article-2903-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"转载自：\nhttps://mp.weixin.qq.com/s?__biz=MzI4MDMwMDM3NA==&mid=2247485542&idx=2&sn=6c6e04179aad2e37926ca94a3a58dba4&chksm=ebbbd65fdccc5f499f5fc938472017601e1afcde9b5404b3e94b0fa9a8847d3c00a3dc3b5390&mpshare=1&scene=1&srcid=0316tdVEMPsKKb0oTxuSc5yG#rd\n当你辗转于各种论坛时，相信会经常看到这样的问题：深度学习是否会取代传统的计算机视觉？或者说，当深度学习看起来如此有效时，是否还有必要研究传统的计算机视觉技术？\n这是一个非常好的问题。\n深度学习已经彻底改变了计算机视觉和人工智能这一领域，许多曾经看起来不可能解决的问题，深度学习都能够解决——尤其是在图像识别和分类问题上，机器已经超越人类（短链：http://t.cn/Rnzv2JX）。事实上，深度学习也强化了计算机视觉在行业中的重要地位。\n但是，深度学习对计算机视觉来说仅仅是一种工具，它不可能成为解决所有问题的万能药。所以，在这篇文章中，我想阐述一下为什么传统计算机视觉技术仍然很重要，并且值得我们去深入学习和研究。\n本文将分为以下三个部分：\n深度学习需要大数据\n深度学习有时过于深度（杀鸡焉用牛刀）\n传统的计算机视觉有助于更好的使用深度学习\n首先我需要解释下什么是传统的计算机视觉技术，什么是深度学习，以及深度学习为什么如此具有革命性。\n▌背景知识\n在深度学习出现之前，如果你想对图像进行分类，首先需要执行一个特征提取（wiki：http://t.cn/RnzvIJ8）的步骤。特征是图像中比较小的“有趣的”、具有描述性的或包含信息的块。在这篇文章中你将会了解到传统的计算机视觉技术，包括边缘检测（wiki：http://t.cn/RnzvqtE），角点检测（wiki：http://t.cn/RnzvSVQ），对象检测（wiki：http://t.cn/RnzvpqH）等等。\n就特征提取和图像分类而言，使用这些技术的思路是：从统一类别对象的图像中（椅子、马等）提取尽可能多的特征，并将这些特征视为对象的一个“定义”（众所周知的词袋模型），然后在其他图像中搜索这些“定义”，如果词袋模型中有相当一部分的特征都可以在这幅图像中找到，那么这幅图像被分类为包含该特定对象的类别（椅子，马等）。\n这种特征提取方法的难点在于，在给定图像中，必须选择需要查找哪些特征。当图像中类别过多时（如10或20个类别），就会变得复杂而难以分类。角点？边缘？还是纹理特征？只有使用不同的特征才可以更好地描述不同类别的对象。如果你在分类时使用很多特征，就必须对大量的参数进行微调。\n深度学习为我们展示了端到端学习（ end-to-endlearning）这一概念，简而言之，针对每个特定类别的对象，机器会自动学习需要查找什么特征。它为每个对象提供了最具描述性和显著性的特征。换句话说，神经网络可以探索图像类别中的底层模式。\n因此，通过端到端的学习，你不再需要自己动手来决定使用哪种传统计算机视觉技术来描述这些特征，机器将会替你做这些工作。《连线》杂志这样描述：\n“举个例子来说，如果你想训练一个[深度]神经网络来识别一只猫，你不需要告诉它要寻找图像上的胡须、耳朵、毛发和眼睛。你需要做的就是向它展示成千上万张猫的照片，这就能解决问题。如果它将狐狸误认为猫，你也不需要重写代码，只需要继续训练即可。”\n下图展示了特征提取（使用传统的计算机视觉技术）和端到端学习二者之间的差异：\n下面我们将继续讨论，传统的计算机视觉为什么仍然有必要且值得我们去学习。\n▌深度学习需要大数据\n首先，深度学习需要数据，并且是大量的数据！上面提到的那些经典的图像分类模型都是在大型数据集上进行训练的。常用于训练的三种数据集分别是：\nImageNet数据集——包含150万张图像，有1000个类别。\nMicrosoftCommon Objects in Context（COCO）数据集——包含250万张图像，有91个类别。\nPASCAL VOC数据集——包含500万张图像，有20个类别。\n比图像分类简单的任务或许并不需要如此多数据，但也少不到哪里去。你必须在你所拥有的数据上进行训练（有些技巧能够增强训练数据，但也都是人为处理的方法）。\n在训练数据范围之外的数据上，已训练模型的表现就会很差，这是因为机器并没有理解这个问题，所以不能在没有训练过的数据上进行泛化。\n我们很难看到训练过的模型的内在机制，手动调参也相当困难，因为深度学习模型里面有数百万个参数——每个参数在训练过程中都需要调整。从某种意义上来说，深度学习模型就是一个黑匣子。\n传统的计算机视觉具有充分的透明度，这能够使你对解决方案能否在训练环境之外运行做一个更好的评估和判断。你可以更容易地了解算法中存在的问题，弄清楚什么地方需要调整。\n▌杀鸡焉用牛刀\n这也许是我支持继续研究传统计算机视觉技术的最佳理由。\n训练一个深度神经网络需要很长时间。如果使用专用硬件（例如高性能GPU）训练最先进的图像分类模型，也得需要将近一天的时间；如果使用标准笔记本电脑进行训练，甚至需要一周或者更长的时间。\n此外，如果你的训练模型表现不佳应该怎么办？你必须重来一遍，使用不同的训练参数重新进行训练，而且这个过程有时候还得重复数百次。\n并不是所有问题需要使用深度学习。在某些问题上，传统的计算机视觉技术的表现比深度学习更好，而且需要的代码更少。\n例如，我曾经参与过一个项目——检测每个通过传送带的锡罐中是否有红色的勺子。你可以训练一个深度神经网络来检测勺子并完成上述过程，但这比较耗费时间；或者你也可以编写一个简单的关于红色的颜色阈值算法（在红色范围内的任何像素都标记成白色，其他像素则都是黑色），然后计算有多少白色像素，这样就可以快速检测勺子。第二个方法很简单，并且能在一个小时以内完成！\n了解传统的计算机视觉技术会为你节省大量时间以及减少不必要的麻烦。\n▌理解传统的计算机视觉方法可以提升你的深度学习技巧\n理解传统的计算机视觉实际上真的有助于你更好的使用深度学习。例如，计算机视觉中最常见的神经网络是卷积神经网络。但是什么是卷积？它实际上是一种广泛使用的图像处理技术（例如Sobel边缘检测）。了解卷积有助于了解神经网络的内在机制，在解决问题时，它可以帮助你设计和调整模型。\n其次是预处理，这通常是针对训练数据而言。预处理这一步骤用到的主要是传统的计算机视觉技术。例如，如果你没有足够多的训练数据，则可以采用数据增强的方法来处理：通过对原来的图像进行随机旋转、移位、剪切的方式来创建“新”的图像。这些操作可以大大增加训练数据的数量。\n▌结论\n在这篇文章中，我解释了为什么深度学习仍然没有取代传统的计算机视觉技术，以及传统的计算机视觉技术为何值得我们去学习和研究。\n首先，深度学习通常需要大量的数据才能达到较好的性能，但是有时候这是不可能实现的。在这些情况下，传统的计算机视觉技术就可以成为替代方案。\n其次，对于某些特定的任务来说，有时候深度学习过于深度。在这种情况下，标准的计算机视觉技术可以更有效地解决问题，并且使用较少的代码。\n第三，了解传统的计算机视觉技术实际上可以让你更好地使用深度学习。这是因为通过传统的计算机视觉，你可以更好地了解深度学习的内在机制，并且可以执行某些预处理步骤来提升深度学习的性能。\n简而言之，深度学习只是计算机视觉的一种工具，并不是万能药。不要因为深度学习现在较为流行就只使用深度学习。传统的计算机视觉技术仍然非常重要，它可以为你节省很多时间，并减少许多不必要的麻烦。\n对于这个观点，你怎么看呢？\n原文地址：\nhttp://zbigatron.com/has-deep-learning-superseded-traditional-computer-vision-techniques/"}
{"content2":"关注网易智能，聚焦AI大事件，读懂下一个大时代！\n选自 | AMiner\n整理 | 小羿 杜瑶 姚怿立\n人工智能发展迅速，尤其是近10年，人工智能领域涵盖了包括自然语言处理、计算机视觉、Web与知识工程、机器人、图形学、可视化、虚拟现实、多媒体、人机交互、语音识别、物联网、计算经济学、计算理论、信息系统、计算机安全、信息检索、数据库、机器学习、数据挖掘等诸多研究方向。\nAMiner节选了和人工智能相关的21个子领域，每个子领域选择1-2个顶级会议或者期刊，根据这些会议和期刊上近10年发表论文的引用情况（根据Goolge Scholar）生成了高引学者列表，仅供参考。\n01\n21个子领域有哪些？\n这21个子领域具体包含了下图中的20个领域及经典人工智能（AAAI、IJCAI）：\n02\n哪些顶级会议和期刊？\nAMiner在每个子领域中选取了1到2个顶级期刊和会议，选取2个的较多，从这些期刊和会议中抽取了近10年发表的论文，并统计每篇论文的引用次数，最后生成了高引学者。\n例如针对“经典人工智能(Artificial Intelligence)”领域，选取了“AAAI Conference on Artificial Intelligence (AAAI)”和“International Joint Conference on Artificial Intelligence (IJCAI)”两个顶级期刊和会议。\n03\n各领域引用量TOP3学者大放送\n高引学者由算法自动统计学者论文引用次数计算得出。其中：\n1.在经典人工智能领域，高引学者排名前三的学者依次是：中国人工智能学会副理事长杨强、以色列理工学院以色列理工学院计算机科学系教授Shaul Markovitch 、艾伦人工智能研究所CEO Oren Etzioni\n2. 在数据挖掘领域，高引学者排名前三的学者依次是：斯坦福大学的帅哥副教授Jure Leskovec、清华大学数据科学研究院院长俞士纶和卡内基·梅隆大学Christos Faloutsos教授。\n3.在计算理论领域，高引学者排名前三的学者依次是：麻省理工学院人工智能实验室教授 Vinod Vaikuntanathan、密歇根大学计算机科学与工程系副教授Chris Peikert、德克萨斯大学奥斯汀分校计算机科学系副教授Brent Waters。\n4. 在计算经济学领域，高引学者排名前三的学者依次是：麻省理工学院经济系教授Daron Acemoglu、特拉维夫大学计算机科学学院教授Yishay Mansour、哈佛大学工程与应用科学学院教授David C. Parkes。\n5.在计算机安全与隐患领域，高引学者排名前三的学者依次是：康奈尔科技学院教授Ari Juels、加州大学圣地亚哥分校计算机科学与工程系副教授Hovav Shacham、加州大学圣地亚哥分校计算机科学与工程系教授Stefan Savage。\n6.在人机交互领域，高引学者排名前三的学者依次是：华盛顿大学计算机科学与工程系副教授Jacob O. Wobbrock 、 斯坦福大学计算机科学系教授James A. Landay、perceptiveIO联合创始人兼首席技术官Shahram Izadi 。\n7.在可视化领域，高引学者排名前三的学者依次是：华盛顿大学计算机科学与工程系副教授Jeffrey Heer、佐治亚理工学院计算机学院和GVU中心交互计算学院教授John T. Stasko、巴黎南部大学高级研究员Jean-Daniel Fekete。\n8.在信息检索领域，高引学者排名前三的学者依次是：马萨诸塞大学信息与计算机科学学院教授W. Bruce Croft 、微软亚洲研究院副院长刘铁岩、字节跳动AI实验室首席科学家李航。\n9.在机器学习领域，高引学者排名前三的学者依次是：多伦多大学教授Geoffrey Hinton、蒙特利尔大学教授Yoshua Bengio、Facebook人工智能研究总监Yann LeCun。\n10.在知识工程领域，高引学者排名前三的学者依次是：乔治亚理工学院计算机学院交互计算学院教授Thad Starner、德国不来梅大学教授Carsten Lutz、利物浦大学计算机科学系教授Frank Wolter。\n11.在计算机视觉领域，高音学者排名前三的学者依次是：旷视科技首席科学家孙剑、法国国家信息与自动化研究所INRIA研究总监Cordelia Schmid、牛津大学教授Andrew Zisserman。\n12.在计算机图形学领域，高引学者排名前三的学者依次是：南加州大学助理教授Paul E. Debevec、布朗大学计算机科学教授Andries Van Dam、佐治亚理工学院教授James D. Foley。\n13.在自然语言处理领域，高引学者排名前三的学者依次是：卡内基梅隆大学机器学系助理教授Chris Dyer、宾夕法尼亚大学计算机与信息科学系副教授Chris Callison-Burch、爱丁堡大学信息学院认知与计算研究所（ILCC）高级研究员Alexandra Birch。\n14.在语音识别领域，高引学者排名前三的学者依次是：多伦多大学教授Geoffrey Hinton、亚马逊Alexa 首席研究员Abdel-Rahman Mohamed、IBM研究院首席研究员Brian Kingsbury。\n15.在机器人学领域，高引学者排名前三的学者依次是：弗赖堡大学教授Wolfram Burgard、不来梅大学教授Michael Beetz、日本先进工业科学技术研究所研究员Fumio Kanehiro。\n16.在数据库领域，高引学者排名前三的学者依次是：麻省理工的教授Samuel Madden、耶鲁大学的助理教授Daniel J. Abadi 、伊利诺伊大学厄巴纳 - 香槟分校的教授Jiawei Han  (韩家炜)。\n17.在多媒体技术领域，高引学者排名前三的学者依次是：加州大学伯克利分校EECS教授Trevor Darrell、Facebook研究科学家贾扬清、Facebook人工智能研究科学家Ross B. Girshick。\n18.在信息系统领域，高引学者排名前三的学者依次是：克利夫兰州立大学电气与计算机工程系教授Dan Simon、卡内基梅隆大学机器人研究所教授Katia Sycara、滑铁卢大学系统设计工程系教授Keith W. Hipel。\n19.在推荐系统领域（Recommender System），高引学者排名前三的学者依次是：纽约大学教授Alexander Tuzhilin、明尼苏达大学卡尔森管理学院信息与决策科学系教授Gediminas Adomavicius、谷歌研究科学家Yehuda Koren。\n20.在物联网领域，高引学者排名前三的学者依次是：苏黎世联邦理工教授Friedemann Mattern、卡尔斯鲁厄大学教授Hannes Hartenstein、苏黎世系统集团计算机科学系教授 Gustavo Alonso。\n21.在虚拟现实领域，高引学者排名前三的学者依次是：伦敦大学学院计算机科学教授Mel Slater、南澳大利亚大学移情计算实验室教授Mark Billinghurst、米兰圣心天主教大学教授Giuseppe Riva。\n04\n跨越多个子领域的高引学者\n报告显示，跨越4个领域出现的高引学者有五名，他们分别是：香港科技大学教授杨强、香港中文大学计算机科学与工程学系教授金国庆、香港中文大学计算机科学与工程学系教授吕荣聪、上海交通大学计算机系教授俞勇和360人工智能研究院院长颜水成。\n值得一提的是，这5名高引学者在研究领域上有大幅度交叉，其中5人入围经典人工智能和数据挖掘领域高引学者，4人入围信息检索领域高引学者，2人入围信息系统领域高引学者，2人入围机器学习领域高引学者。\n跨越3个领域出现的高引学者有18名。其中，有4名华人学者和2名国内学者入围，分别是亚利桑那州立大学刘欢、伊利诺伊大学香槟分校韩家炜和翟成祥、微软研究院Hao Ma、字节跳动AI实验室李航、地平线机器人创始人余凯。\n这18人中，7人入围机器学习领域高引学者，6人入围信息检索领域高引学者，5人入围数据挖掘领域高引学者。\n05\n高引学者的机构分布\n高引学者一共21个领域，AMiner从每个领域中选取这十年论文引用量的TOP100学者，共计2100人。\n总体来看，在所有领域的学者占有量，排名前十的研究机构依次是：谷歌（80），微软（78），卡内基·梅隆大学（68），华盛顿大学（45），斯坦福大学（39），加州大学伯克利分校（36），加利福尼亚大学（34），Facebook（30），伊利诺伊大学（25），佐治亚理工学院（23）。\n06\n性别比例\n21个子领域的高引学者性别比例中，男性占据了很大比例。人机交互领域的女性学者比例最高，为25%。语音识别、机器人和物联网领域的女性比例最低，均为5%。\n点击【阅读原文】获得21个领域完整榜单。\n- 加入社群吧 -\n网易智能AI社群（AI专家群、AI黑板报）火热招募中，对AI感兴趣的小伙伴，添加智能菌微信 kaiwu_club，说明身份即可加入。\n本报告来自学术头条（公众号 SciTouTiao），版权归属学术头条，转载需联系源出处，特此说明。"}
{"content2":"图像分类\n图像分类：在已有固定分类标签的集合下，对输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像的过程。综上，其主要任务是对于一个给定的图像，预测它属于哪个分类标签或者给出属于一系列不同标签的可能性。\n计算机视觉算法在图像识别方面的一些难点：\n1）视角变化：同一物体，摄像头可以从多个角度来展现；\n2）大小变化：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是由变化的）；\n3）形变：很多东西的形状并非一成不变，会有很大变化；\n4）遮挡：目标物体可能被遮挡。有时候只有物体的一部分（可以小到几个像素）是可见的；\n5）光照条件：在像素层面上，光照的影响非常大；\n6）背景干扰：物体可能混入背景之中，使之难以被辨认；\n7）类内差异：一类物体的个体之间的外形差异很大，如椅子。这一类物体有许多不同的对象，每个都有自己的外形\n面对以上难题，好的图像分类此模型是能够在维持分类结论稳定的同时，保持对类间差异足够敏感。\n识别图像分类的方法：\n数据驱动方法：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。此方法第一步为收集已经做好分类标注的图片来作为训练集。\n图像分类就是输入一个元素为像素值的数组，然后给它分配一个分类标签。其流程如下：\n1）输入：输入是包含N个图像的集合，每个图像的标签是K中分类标签中的一种。这个集合称为训练集；\n2）学习：使用训练集来学习每个类到底长什么样，即训练分类器或学习一个模型；\n3）评价：利用分类来预测其未曾见过的图像分类标签，并以此来评价分类器的质量。然后把分类器预测的标签和图像真正的分类标签对比。分类器预测的分类标签与图像真正的分类标签一致的数目越多，则分类效果越好。\nK-NN分类器需要设定k值，那么选择哪个k值最合适？此外，可以选择不同的距离函数，如L1范数和L2范数等，那么选择哪个好？所有这些选择，被称为超参数（hyperparameter）。在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般来说，这些超参数具体怎么设置或取值并不是显而易见的。\n可以尝试使用不同的值，看哪个值最好，就选用哪个。但是要特别注意：绝不能使用训练集来进行调优。在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，决不使用它。如果使用测试集来调优，而且算法看起来效果不错，则真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集过拟合。从另一个角度说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实过于乐观了，实际部署起来效果就会差很多。所以最终测试的时候再使用测试，可以很好地近似度量你设计的分类器的泛化性能。\n测试数据集只使用一次，即训练完成后评价最终的模型时使用。\n不使用测试集调优的方法：从训练集中取出一部分数据用来调优，这部分数据称为验证集。\n把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。\n交叉验证：有时候，训练集数据量小（因此验证集的数量更小），故会使用交叉验证的方法进行调参。交叉验证集，就是对所有的训练样本，平均分成k分，用k-1份训练，1份用来验证。然后循环去k-1份来训练，其中1份来验证，最后将k此实验结果的平均值作为算法验证结果。\n实际应用中，人们不是很喜欢交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%~90%的比例分成训练集和验证集。不过还要看具体情况来，如果超参数数量多，可能就想用更大的验证集，而验证集的数量不够，此时最好使用交叉验证。\nNN（最近邻）分类器的优缺点：\n1）NN分类器易于理解，实现简单；\n2）算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来；\n3）测试则需要花费大量时间计算，因为每个测试集都要与所有的训练集进行比较，而实际中更关注测试效率，其对比就是卷积神经网络，其训练花费很多时间，但一旦训练完成，对新的测试数据进行分类非常快。\nANN（近似最近邻）算法可以提升NN分类器在数据上的计算速度。不过这些算法需要在准确率和时空复杂度之间进行权衡，并通常依赖一个预处理/索引过程，这个过程中一般包含kd树的创建和k-means算法的运用。\nNN分类器在某些特定情况（如数据维度较低）下，可能是不错的选择。但是在高维数据上，很少使用。\n总结：\n1）图像分类问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价；\n2）最近邻分类器（Nearest Neighbor classifier）：分类器中存在不同的超参数（比如k值或距离类型的选取），要想选取好的超参数不是一件轻而易举的事；\n3）选取超参数的正确方法是：将原始训练集分为训练集和验证集，在验证集上尝试不同的超参数，最后保留表现最好那个；\n4）一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法；\n5）最近邻分类器简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力；\nK-NN使用需按照以下流程：\n1）预处理数据：对数据中的特征进行归一化，让其具有零平均值和单位方差；\n2）如果数据是高位数据，考虑使用降维方法，如PCA或随机投影；\n3）将数据随机分入训练集和验证集。按照一般规律，70%~90%数据作为训练集。这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。如果需要预测的超参数很多，则就应该使用更大的验证集来有效地估计它们。如果担心验证集数量不够，则可以尝试使用交叉验证方法。如果计算资源足够，使用交叉验证总是更加安全（分数越多，效果越好，同时耗费计算资源越多）；\n4）在验证集上调优，尝试足够多的k值，尝试L1和L2两种范数计算方式；\n5）如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor来加速这个过程，其代价是降低一些准确率；\n6）对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上进行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。实践中，不要这样做。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。直接使用测试集来测试用最优参数设置好的最优模型，得到测试集数据的分类器准确率，并依次作为KNN分类器在该数据上的性能的表现。\nKNN分类器不足：\n1）分类器必须记住所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计算；\n2）对一个测试图像进行分类器需要和所有训练图像作为比较，算法的资源耗费高。\n评分函数：它是原始图像数据到类别分支的映射；\n损失函数：它是用来量化预测分类标签的得分与真实标签之间的一致性。\n最优化过程：通过更新评分函数的参数来最小化损失函数值。\n从图像到标签分值的参数化映射\n该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。\n图像中的零均值中心化很重要。\n直观地：当评分函数输出结果与真实结果之间的差异越大，损色函数输出越大，反之越小。\nSVM的损失函数想要正确分类类别的分数比不正确类别的分数高，而且至少要高一定值。\nmax(0,-) 函数被称为折叶损失（hinge loss）；而当max(0,-)^2函数为平方折叶损失SVM（L2-SVM）它更强烈地惩罚过边界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失工作得更好。可以通过交叉验证来决定到底使用哪个。\n正则化：正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数，它由两部分组成：数据损失（data loss），即所有样例的平均损失Li，以及正则化损失（regularization loss）。\n引入正则化惩罚能够带来很多的良好性质。如引入L2惩罚后，SVM就有了最大边界。其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这意味着没有哪个维度能够独自于整体分值有过大的影响。\n对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。\nSoftmax分类器\nSVM是最常用的两个分类器之一，另一个就是Softmax分类器，它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。Softmax分类器中使用交叉熵损失函数。Softmax函数：其输入值是一个向量，向量中元素为任意实数的评分值（z中的），函数对其进行压缩，输出一个向量，其中每个元素值都在0到1之间，且所有元素之和为1。\nSoftmax分类器所做的就是最小化在估计分类概率和真实分布之间的交叉熵。交叉熵损失函数“想要”预测分布的所有概率密度都在正确分类上。\n精确地说，SVM分类器使用的是折叶损失（hinge loss），有时候又被称为最大边界损失（max-margin loss）。Softmax分类器使用的是交叉熵损失（cross-entropy loss）。Softmax分类器函数是从softmax函数得来，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意：从技术上说，softmax损失没有意义，因为softmax只是一个压缩数值的函数。\nSVM分类器将计算的结果看成是分类评分，它的损失函数鼓励正确的分类的分值比其它分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的对数概率，鼓励正确的归一化的对数概率变高，其余的变低。\n相对于Softmax分类器，SVM更加局部目标化，这可以看做是一个特性，也可以看做是一个优势。\n总结：\n1）定义了从图像像素映射到不同类别的分类评分函数。评分函数是一个基于权重W和偏差b的线性函数；\n2）与KNN分类器不同，参数方法的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃。同时该方法对于新的测试数据的预测非常快，因为只需要与权重w进行一个矩阵乘法运算。\n3）介绍偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵；\n4）定义了损失函数（SVM和Softmax线性分类器）。损失函数能够衡量给出的参数集和训练数据集真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练数据做出良好预测与得到一个足够低的损失函数是等价的。\n图像分类任务中的两个关键部分：\n1）基于参数的评分函数。该函数将原始图像像素映射为分类评分值（如一个线性函数）；\n2）损失函数。该函数能够根据分配评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式。（如Softmax和SVM）\n最优化：寻找能够使得损失函数值最小化的参数过程。\n实现最优化策略：\n1）随机搜索，其核心思想是迭代优化。核心思路是：找到最优化的权重W非常困难，甚至是不可能的（尤其是W中存的是整个神经网络的权重的时候），但如果问题转化为对一个权重矩阵W取优，使其损失值稍微减少。那么问题的难度就大大降低了。即从一个随机的W开始，然后对其迭代取优，每次让它的损失值变得更小一点。整体策略：从随机权重开始，然后迭代取优，从而获得更低的损失值。\n2）随机本地搜索：此策略为从一个随机W开始，然后生成一个随机的扰动，直到当权重与扰动的和使得损失值减少，才进行更新。\n3）跟随梯度：前两个策略中，是尝试在权重空间找到一个方向，沿着该方向能降低损失函数的损失值。其实，不需要随机寻找方向，因为可以直接计算出最好的方向，在数学上计算出最陡峭的方面，此方向即为损失函数的梯度。此方向就像是感受一下脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数）。当函数有多个参数的时候，称导数为偏导数。而梯度就是在各个维度上偏导数所形成的向量。\n梯度计算\n计算梯度有两种方法：一个是缓慢的近似方法（数值梯度法），但实现相对简单。另一个方法（分析梯度法）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。\n在梯度负方向上更新：当向着梯度df的负方向去更新时，损失函数值是降低而不是升高。\n步长影响：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。选择步长（即学习率）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。就好比蒙眼下上，可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多少步长？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在某些点如果步长过大，反而可能越过最低点导致更高的损失值。\n小步长下降稳定但进度慢，大步长进展快但是分险更大。采取大步长可能导致错过最优点，然损失值上升。步长（学习率）将会是调参中最重要的超参数之一。\n效率问题：计算数值梯度的复杂度和参数的量线性相关。此策略不适合大规模数据。\n微分分析计算梯度：使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为对于h值只是选取了一个很小的数值，但真正的梯度定义中h趋向0的基线），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，一次来检查其实现的正确性。此步骤为梯度检查。\n梯度下降\n计算损失函数的梯度，即程序重复的计算梯度然后对参数进行更新，此过程为梯度下降。梯度下降是对神经网络的损失函数最优化中最常用的方法。\n小批量数据梯度下降（Mini-batch gradient descent）：在大规模的应用中，训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的小批量数据。实际情况中，数据集不会包含重复，则小批量数据的梯度就是对整个数据集梯度的一个近似。在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。小批量数据策略中有个极端情况，就是每个批量中只有1个数据样本，这种策略称为随机梯度下降（SGD）。向量化操作的代码一次计算100个数据比100此计算1个数据要高效得多。即使SGD在技术上是指每次使用1个数据来计算梯度，但可能有人会使用SGD来指代小批量数据梯度下降（或用MGD指代小批量数据梯度下降，而BGD指代较少）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定，或者干脆设置为同样大小，如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。\n总结：\n信息流的总结如上图，数据集中的 (x, y) 是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量f中。损失函数包含两个部分：数据损失和正则化损失。其中数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，计算权重的梯度，然后使用它们来实现参数的更新。\n总结：\n1）将损失函数比作了一个高纬度的最优化地形，并尝试到达它的最底部。最优化的工作工程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。\n2）提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。\n3）函数的梯度给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是h，用来计算数值梯度）。\n4）参数更新需要有技巧地设置步长，也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。\n5）讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用梯度检查来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。\n6）介绍了梯度下降方法，它在循环中迭代地计算梯度并更新参数。\n反向传播\n反向传播是利用链式法则递归计算表达式的梯度的方法。\n导数意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。\n函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。\n使用链式方法计算复合表达式\n链式法则指出将这些梯度表达式链起来的正确方式是相乘。在实际操作中，知识简单地将两个梯度数值相乘。\n前向传播：从输入计算到输出，方向传播：从尾部开始，根据链式法则递归地向前计算梯度，一直到网络的输入端。可以认为，梯度是从计算链路中回流。\n反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：1）这个门的输出值；2）其输出值关于输入值的局部梯度。前向传播完毕，在反向传播过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其他的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络。\n对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上他们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算他们。\n在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。此为遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候就应该进行累加。\n可以使用向量化操作计算梯度。\n总结：\n1）对梯度的含义有了直观理解，直到了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的；\n2）讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其链起来。重要的是，不需要把这些表达式写在之上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。\n一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器\n常用的激活函数：Sigmoid非线性函数，tanh函数等。\nSigmod函数非常常用，其原因是它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和的激活。但现在Sigmod函数已经不太受欢迎，原因是它存在以下两点缺点：\n1）Sigmoid函数饱和使梯度消失；2）Sigmod函数的输出不是零中心的。\nTanh函数与Sigmoid神经元一样，也存在饱和问题，但是它的输出是零中心的，故tanh非线性函数比sigmoid非线性函数更受欢迎。\nReLUctant。它的函数公式是f(x)=max(0,x)。即这个激活函数就是一个关于0阈值的。它存在以下优缺点：\n1）优点：相较于sigmoid和tanh函数，ReLUctant对于随机梯度下降的收敛有巨大的加速作用。这是由它的线性，非饱和的公式导致的。\n2）优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLUctant可以简单地通过一个矩阵进行阈值计算得到。\n3）缺点：在训练的时候，ReLU单元比较脆弱并且可能死掉。\nLeaky ReLU：是为了解决“ReLUctant死亡”问题。其在x小于0时，给出一个很小的负数梯度值，如0.01。\nMaxout：它是对ReLU和Leaky ReLU的一般化归纳，它拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然后和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。\n总结：使用ReLU非线性函数时，注意设置好学习率，或许可以监控网络中死亡的神经元占的比例。如果单元死亡问题出现，可以尝试Leaky ReLU或者Maxout，不建议使用sigmoid，可以尝试tanh，其效果可能不如ReLU或者Maxout。\n全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。\n神经网络在实践中非常好用，是因为他们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法（梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。\n在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因此（比如数十（以10为量级）个可学习的层）。对于该现象的一种解释观点是：因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据有直观意义。\n设置层的数量和尺寸\n首先，要知道当增加层的数量和尺寸时，网络的容量上升了。即神经元可以合作表达许多复杂函数，所以表达函数的空间增加。更多神经元的神经网络可以表达更复杂的函数。然而这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。过拟合是网络对数据中的噪声有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。防止神经网络过拟合的方法有很多中（如L2正则化，dropout和输出噪音等）。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。\n不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。在实际中，将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果训练一个大的网络，将会发现许多不同的解决方法，但是最终损失值的差异将会小很多。即所有解决办法差不多，而且对于随机初始化参数好坏的依赖也会小很多。\n注意：正则化强度是控制神经网络过拟合的好方法。不同正则化强度的效果：每个神经网络都有20个隐层单元，但是随着正则化强度增加，它的决策边界变得更加平滑。故不应该因害怕出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。\n总结：\n1）不同类型的激活函数中，ReLU是最佳推荐；\n2）神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接；\n3）分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算；\n4）神经网络是一个通用函数近似器，但是该性质与其广泛使用无太大关系。之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够在某种程度上做出“正确”假设。\n5）更大的网络一般会更好一些。更大容量的模型一定要和更强的正则化（更高的权重衰减）配合，否则它们就会过拟合。\n神经网络就是进行一系列的线性映射与非线性激活函数交织的运算。\n数据预处理\n1）均值减去法：是预处理最常用的形式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。\n2）归一化：是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化处理，然后每个维度都除以其标准差。第二种方法是对每个维度都做归一化，使其每个维度的最大和最小值是1和-1。\n注意：预处理操作只有在确性不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。\n3）PCA和白化是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性质。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储空间。\n白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。此变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。\n注意：该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性（方差小）而大多是噪声的维度。实际操作中，此问题可以用更强的平滑来解决。\n实践操作：实际在卷积神经网络中并不会采用PCA和白化。然后对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。\n常见错误：进行预处理很重要的一点是任何预处理策略（如数据均值）都只能在训练集数据上进行，算法训练完毕后再应用到验证集或者测试集。具体做法应如下：应该先分成训练、验证、测试集，只是从训练集中求图片平均值，然后各个集（训练、验证、测试集）中的图像再减去这个平均值。\n权重初始化\n1）错误：全零初始化。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，可以假设所有权重数值中大约一半为正数，一半为负数。但不要全零进行初始化，因为如果权重被初始化为同样的值，神经元之间就失去了不对称的源头。\n2）小随机数初始化：权重初始值非常接近0又不能等于0。解决方法是将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。\n警告：并不是小数值一定会得到好的结果。一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。\n3）使用1/sqrt(n)校准方差。随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。可以处以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1。建议是用1/sqrt(n)来初始化神经元的权重向量。其中n是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。这样做可以提高收敛的速度。\n4）系数初始化：另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。\n5）偏置的初始化：通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。\n6）批量归一化：该方法减轻了如何合理初始化神经网络这个棘手问题，其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。在神经网络中使用批量归一化变得非常常见。在实践中，使用批量归一化的网络对于不好的初始值有更强的鲁棒性。总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在一起。\n正则化\n有不少方法是通过控制神经网络的容量来防止其过拟合的：\nL2正则化可能是最常用的正则化方法。可以通过惩罚目标函数中所有参数的平法将其实现。L2正则化可以直观理解为对它大于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。\nL1正则化：它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。即使用L1正则化的神经元最后使用的是它们最重要的输入数据的系数子集，同时对于噪音输入则几乎是不变的。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。\n最大范式约束：是指给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。\n随机失活：是一个简单又极其有效的正则化方法。它与L1正则化，L2正则化，最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数p的概率被激活或者被设置为0。在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数。（数量巨大的子网络们并不是相互独立，因为他们都是共享参数）。测试过程中不使用随机失活可以理解为是对数量巨大的子网络们做了模型集成，以此来计算出一个平均的预测。\n前向传播中的噪声。测试时，通过分析法或数值法（通过抽象出很多的子网络随机选择不同子网络进行前向传播，最后对它们取平均）将噪声边缘化。\n偏置正则化：不建议使用。\n每层正则化。很少见，不建议使用。\n实践中，通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。\n损失函数：正则化损失 + 数据损失。\n数据损失是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有的样本的数据损失求平均。\n实际中，需要解决以下几类问题：\n1）分类问题。注意，当类别数目巨大时，可以考虑使用分层Softmax损失函数。分层Softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分支之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。\n2）属性分类：方法有两种，其一，为每个属性创建一个独立的二分类的分类器；其二，对每种属性训练一个独立的逻辑回归分类器。\n3）回归问题：它是预测实数的值的问题，如预测房价，预测图片中某个东西的长度等。对这种问题，通常计算预测值和真实值之间的损失。然后用L2平方范式或L1范式度量差异。\n当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。\n4）结构化预测：结构化损失是指标签可以是任意的结构，如图表、树或者其它复杂物体的情况。此情况下，还会假设结构空间非常巨大，不容易进行遍历。结构化SVM背后的基本思想是在正确的结构yi和得分最高的非正确结构之间画出一个边界。解决此类方法，需要特殊的解决方案，这样可以有效利用对于结构空间的特殊简化假设。\n总结：\n1）推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。\n2）使用标准差为根号2/n的高斯分布来初始化权重，其中n是输入的神经元数。\n3）使用L2正则化和随机失活的倒置版本。\n3）使用批量归一化。\n4）在实践中，可能要面对不同的任务，以及每个任务对应的常用损失函数问题。"}
{"content2":"转自 http://blog.csdn.net/qq_14845119/article/details/51913171\nImageNet\nImageNet是一个计算机视觉系统识别项目，是目前世界上图像识别最大的数据库。是美国斯坦福的计算机科学家李飞飞模拟人类的识别系统建立的。能够从图片识别物体。目前已经包含14197122张图像，是已知的最大的图像数据库。每年的ImageNet大赛更是魂萦梦牵着国内外各个名校和大型IT公司以及网络巨头的心。图像如下图所示，需要注册ImageNet帐号才可以下载，下载链接为http://www.image-net.org/\nPASCAL VOC\nPASCALVOC 数据集是视觉对象的分类识别和检测的一个基准测试，提供了检测算法和学习性能的标准图像注释数据集和标准的评估系统。图像如下图所示，包含VOC2007（430M），VOC2012（1.9G）两个下载版本。下载链接为http://pjreddie.com/projects/pascal-voc-dataset-mirror/\nLabelme\nLabelme是斯坦福一个学生的母亲利用休息时间帮儿子做的标注，后来便发展为一个数据集。该数据集的主要特点包括\n（1）专门为物体分类识别设计，而非仅仅是实例识别\n（2）专门为学习嵌入在一个场景中的对象而设计\n（3）高质量的像素级别标注，包括多边形框（polygons）和背景标注（segmentation masks）\n（4）物体类别多样性大，每种物体的差异性，多样性也大。\n（5）所有图像都是自己通过相机拍摄，而非copy\n（6）公开的，免费的\n图像如下图所示，需要通过matlab来下载，一种奇特的下载方式，下载链接为http://labelme2.csail.mit.edu/Release3.0/index.php\nCOCO\nCOCO是一种新的图像识别，分割和加字幕标注的数据集。主要由Tsung-Yi Lin（Cornell Tech），Genevieve Patterson （Brown），MatteoRuggero Ronchi （Caltech），Yin Cui （Cornell Tech），Michael Maire （TTI Chicago），Serge Belongie （Cornell Tech），Lubomir Bourdev （UC Berkeley），Ross Girshick （Facebook AI), James Hays (Georgia Tech),PietroPerona (Caltech)，Deva Ramanan (CMU），Larry Zitnick （Facebook AI）， Piotr Dollár （Facebook AI）等人收集而成。其主要特征如下\n（1）目标分割\n（2）通过上下文进行识别\n（3）每个图像包含多个目标对象\n（4）超过300000个图像\n（5）超过2000000个实例\n（6）80种对象\n（7）每个图像包含5个字幕\n（8）包含100000个人的关键点\n图像如下图所示，支持Matlab和Python两种下载方式，下载链接为http://mscoco.org/\nSUN\nSUN数据集包含131067个图像，由908个场景类别和4479个物体类别组成，其中背景标注的物体有313884个。图像如下图所示，下载链接为http://groups.csail.mit.edu/vision/SUN/\nCaltech\nCaltech是加州理工学院的图像数据库，包含Caltech101和Caltech256两个数据集。该数据集是由Fei-FeiLi, Marco Andreetto, Marc 'Aurelio Ranzato在2003年9月收集而成的。Caltech101包含101种类别的物体，每种类别大约40到800个图像，大部分的类别有大约50个图像。Caltech256包含256种类别的物体，大约30607张图像。图像如下图所示，下载链接为http://www.vision.caltech.edu/Image_Datasets/Caltech101/\nCorel5k\n这是Corel5K图像集，共包含科雷尔（Corel）公司收集整理的5000幅图片，故名：Corel5K，可以用于科学图像实验：分类、检索等。Corel5k数据集是图像实验的事实标准数据集。请勿用于商业用途。私底下学习交流使用。Corel图像库涵盖多个主题，由若干个CD组成，每个CD包含100张大小相等的图像，可以转换成多种格式。每张CD代表一个语义主题，例如有公共汽车、恐龙、海滩等。Corel5k自从被提出用于图像标注实验后，已经成为图像实验的标准数据集，被广泛应用于标注算法性能的比较。Corel5k由50张CD组成，包含50个语义主题。\nCorel5k图像库通常被分成三个部分：4000张图像作为训练集，500张图像作为验证集用来估计模型参数，其余500张作为测试集评价算法性能。使用验证集寻找到最优模型参数后4000张训练集和500张验证集混合起来组成新的训练集。\n该图像库中的每张图片被标注1~5个标注词，训练集中总共有374个标注词，在测试集中总共使用了263个标注词。图像如下图所示，很遗憾本人也未找到官方下载路径，于是github上传了一份，下载链接为https://github.com/watersink/Corel5K\nCIFAR（Canada Institude For Advanced Research）\nCIFAR是由加拿大先进技术研究院的AlexKrizhevsky, Vinod Nair和Geoffrey Hinton收集而成的80百万小图片数据集。包含CIFAR-10和CIFAR-100两个数据集。 Cifar-10由60000张32*32的RGB彩色图片构成，共10个分类。50000张训练，10000张测试（交叉验证）。这个数据集最大的特点在于将识别迁移到了普适物体，而且应用于多分类。CIFAR-100由60000张图像构成，包含100个类别，每个类别600张图像，其中500张用于训练，100张用于测试。其中这100个类别又组成了20个大的类别，每个图像包含小类别和大类别两个标签。官网提供了Matlab,C，python三个版本的数据格式。图像如下图所示，下载链接为http://www.cs.toronto.edu/~kriz/cifar.html\n人脸数据库:\nAFLW（Annotated Facial Landmarks in the Wild）\nAFLW人脸数据库是一个包括多姿态、多视角的大规模人脸数据库，而且每个人脸都被标注了21个特征点。此数据库信息量非常大，包括了各种姿态、表情、光照、种族等因素影响的图片。AFLW人脸数据库大约包括25000万已手工标注的人脸图片，其中59%为女性，41%为男性，大部分的图片都是彩色，只有少部分是灰色图片。该数据库非常适合用于人脸识别、人脸检测、人脸对齐等方面的研究，具有很高的研究价值。图像如下图所示，需要申请帐号才可以下载，下载链接为http://lrs.icg.tugraz.at/research/aflw/\nLFW（Labeled Faces in the Wild）\nLFW是一个用于研究无约束的人脸识别的数据库。该数据集包含了从网络收集的13000张人脸图像，每张图像都以被拍摄的人名命名。其中，有1680个人有两个或两个以上不同的照片。这些数据集唯一的限制就是它们可以被经典的Viola-Jones检测器检测到（a hummor）。图像如下图所示，下载链接为http://vis-www.cs.umass.edu/lfw/index.html#download\nAFW（Annotated Faces in the Wild）\nAFW数据集是使用Flickr（雅虎旗下图片分享网站）图像建立的人脸图像库，包含205个图像，其中有473个标记的人脸。对于每一个人脸都包含一个长方形边界框，6个地标和相关的姿势角度。数据库虽然不大，额外的好处是作者给出了其2012 CVPR的论文和程序以及训练好的模型。图像如下图所示，下载链接为http://www.ics.uci.edu/~xzhu/face/\nFDDB（Face Detection Data Set and Benchmark）\nFDDB数据集主要用于约束人脸检测研究，该数据集选取野外环境中拍摄的2845个图像，从中选择5171个人脸图像。是一个被广泛使用的权威的人脸检测平台。图像如下图所示，下载链接为http://vis-www.cs.umass.edu/fddb/\nWIDER FACE\nWIDER FACE是香港中文大学的一个提供更广泛人脸数据的人脸检测基准数据集，由YangShuo， Luo Ping ，Loy ，Chen Change ，Tang Xiaoou收集。它包含32203个图像和393703个人脸图像，在尺度，姿势，闭塞，表达，装扮，关照等方面表现出了大的变化。WIDER FACE是基于61个事件类别组织的，对于每一个事件类别，选取其中的40%作为训练集，10%用于交叉验证（cross validation），50%作为测试集。和PASCAL VOC数据集一样，该数据集也采用相同的指标。和MALF和Caltech数据集一样，对于测试图像并没有提供相应的背景边界框。图像如下图所示，下载链接为http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/\nCMU-MIT\nCMU-MIT是由卡内基梅隆大学和麻省理工学院一起收集的数据集，所有图片都是黑白的gif格式。里面包含511个闭合的人脸图像，其中130个是正面的人脸图像。图像如下图所示，没有找到官方链接，Github下载链接为https://github.com/watersink/CMU-MIT\nGENKI\nGENKI数据集是由加利福尼亚大学的机器概念实验室收集。该数据集包含GENKI-R2009a,GENKI-4K,GENKI-SZSL三个部分。GENKI-R2009a包含11159个图像，GENKI-4K包含4000个图像，分为“笑”和“不笑”两种，每个图片的人脸的尺度大小，姿势，光照变化，头的转动等都不一样，专门用于做笑脸识别。GENKI-SZSL包含3500个图像，这些图像包括广泛的背景，光照条件，地理位置，个人身份和种族等。图像如下图所示，下载链接为http://mplab.ucsd.edu，如果进不去可以，同样可以去下面的github下载，链接https://github.com/watersink/GENKI\nIJB-A (IARPA JanusBenchmark A)\nIJB-A是一个用于人脸检测和识别的数据库，包含24327个图像和49759个人脸。图像如下图所示，需要邮箱申请相应帐号才可以下载，下载链接为http://www.nist.gov/itl/iad/ig/ijba_request.cfm\nMALF (Multi-Attribute Labelled Faces)\nMALF是为了细粒度的评估野外环境中人脸检测模型而设计的数据库。数据主要来源于Internet，包含5250个图像，11931个人脸。每一幅图像包含正方形边界框，俯仰、蜷缩等姿势等。该数据集忽略了小于20*20的人脸，大约838个人脸，占该数据集的7%。同时，该数据集还提供了性别，是否带眼镜，是否遮挡，是否是夸张的表情等信息。图像如下图所示，需要申请才可以得到官方的下载链接，链接为http://www.cbsr.ia.ac.cn/faceevaluation/\nMegaFace\nMegaFace资料集包含一百万张图片，代表690000个独特的人。所有数据都是华盛顿大学从Flickr（雅虎旗下图片分享网站）组织收集的。这是第一个在一百万规模级别的面部识别算法测试基准。 现有脸部识别系统仍难以准确识别超过百万的数据量。为了比较现有公开脸部识别算法的准确度，华盛顿大学在去年年底开展了一个名为“MegaFace Challenge”的公开竞赛。这个项目旨在研究当数据库规模提升数个量级时，现有的脸部识别系统能否维持可靠的准确率。图像如下图所示，需要邮箱申请才可以下载，下载链接为http://megaface.cs.washington.edu/dataset/download.html\n300W\n300W数据集是由AFLW，AFW，Helen，IBUG，LFPW，LFW等数据集组成的数据库。图像如下图所示，需要邮箱申请才可以下载，下载链接为http://ibug.doc.ic.ac.uk/resources/300-W/\nIMM Data Sets\nIMM人脸数据库包括了240张人脸图片和240个asf格式文件（可以用UltraEdit打开，记录了58个点的地标），共40个人（7女33男），每人6张人脸图片，每张人脸图片被标记了58个特征点。所有人都未戴眼镜,图像如下图所示，下载链接为http://www2.imm.dtu.dk/~aam/datasets/datasets.html\nMUCT Data Sets\nMUCT人脸数据库由3755个人脸图像组成，每个人脸图像有76个点的地标（landmark），图片为jpg格式，地标文件包含csv,rda,shape三种格式。该图像库在种族、关照、年龄等方面表现出更大的多样性。具体图像如下图所示，下载链接为http://www.milbo.org/muct/\nORL  (AT&T Dataset)\nORL数据集是剑桥大学AT&T实验室收集的一个人脸数据集。包含了从1992.4到1994.4该实验室的成员。该数据集中图像分为40个不同的主题，每个主题包含10幅图像。对于其中的某些主题，图像是在不同的时间拍摄的。在关照，面部表情（张开眼睛，闭合眼睛，笑，非笑），面部细节（眼镜）等方面都变现出了差异性。所有图像都是以黑色均匀背景，并且从正面向上方向拍摄。\n其中图片都是PGM格式，图像大小为92*102，包含256个灰色通道。具体图像如下图所示，下载链接为http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\nVGG Face dataset\n该数据集包含了2622个不同的人，每个人包含1000张图片，是一个训练人脸识别的大的数据集，官网提供了每个图片的URL，需要自己解析下载，当然有些链接是需要翻墙的，要不可能下载不全哦。\n下载链接：http://www.robots.ox.ac.uk/~vgg/data/vgg_face/\nCASIA WebFace Database\n该数据集为中科院自动化所，李子青老师组开源的数据集，包含了10575类人，一共494414张图片，其中有3类人和lfw中的一样。该数据集主要用于人脸识别。图像都是著名电影中crop而出的，每个图片的大小都是250*250，每个类下面都有3张以上的图片，非常适合做人脸识别的训练。现在发paper比较一致的做法都是在该数据集上训练下，再在lfw数据集做个测试。需要邮箱申请，下载链接：http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html\nCelebA（Large-scale CelebFaces Attributes dataset）\n该数据集为香港中文大学汤晓鸥老师组开源的数据集，主要包含了5个关键点，40个属性值等，包含了202599张图片，图片都是高清的名人图片，可以用于人脸检测，5点训练，人脸头部姿势的训练等。下载链接：http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\nYouTuBe Faces DB\n该数据集主要用于非约束条件下的视频中人脸识别，姿势判定等。该数据集包含1595个不同人的3425个视频，平均每个人的类别包含了2.15个视频，每个类别最少包含48帧，最多包含6070帧，平均包含181.3帧。下载链接：http://www.cslab.openu.ac.il/agas/，或者，http://www.cslab.openu.ac.il/download/，如果没有效果，可以尝试filezilla下载，\nserver:agas.openu.ac.il\nPath: /v/data9/cslab/wolftau/\nfilezilla模式设置为\"Transfer mode\"\n行人检测数据库\nINRIA Person Dataset\nInria数据集是最常使用的行人检测数据集。其中正样本（行人）为png格式，负样本为jpg格式。里面的图片分为只有车，只有人，有车有人，无车无人四个类别。图片像素为70*134，96*160，64*128等。具体图像如下图所示，下载链接为http://pascal.inrialpes.fr/data/human/\nCaltechPedestrian Detection Benchmark\n加州理工学院的步行数据集包含大约包含10个小时640x480 30Hz的视频。其主要是在一个在行驶在乡村街道的小车上拍摄。视频大约250000帧（在137个约分钟的长段），共有350000个边界框和2300个独特的行人进行了注释。注释包括包围盒和详细的闭塞标签之间的时间对应关系。更多信息可在其PAMI 2012 CVPR 2009标杆的论文获得。具体图像如下图所示，下载链接为http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\nMIT cbcl (center for biological and computational learning)Pedestrian Data\n该数据集主要包含2个部分，一部分为128*64的包含924个图片的ppm格式的图片，另一部分为从打图中分别切割而出的小图，主要包含胳膊，脑袋，脚，腿，头肩，身体等。具体图像如下图所示，下载链接为http://cbcl.mit.edu/software-datasets/PedestrianData.html，需要翻墙才可以。\n年龄，性别数据库\nAdience\n该数据集来源为Flickr相册，由用户使用iPhone5或者其它智能手机设备拍摄，同时具有相应的公众许可。该数据集主要用于进行年龄和性别的未经过滤的面孔估计。同时，里面还进行了相应的landmark的标注。是做性别年龄估计和人脸对齐的一个数据集。图片包含2284个类别和26580张图片。具体图像如下图所示，下载链接为http://www.openu.ac.il/home/hassner/Adience/data.html#agegender\n车辆数据库\nKITTI（Karlsruhe Institute ofTechnology and Toyota Technological Institute）\nKITTI包含7481个训练图片和7518个测试图片。所有图片都是真彩色png格式。该数据集中标注了车辆的类型，是否截断，遮挡情况，角度值，2维和3维box框，位置，旋转角度，分数等重要的信息，绝对是做车载导航的不可多得的数据集。具体图像如下图所示，下载链接为http://www.cvlibs.net/datasets/kitti/\n字符数据库\nMNIST（Mixed National Instituteof Standards and Technology）\nMNIST是一个大型的手写数字数据库，广泛用于机器学习领域的训练和测试，由纽约大学的Yann LeCun整理。MNIST包含60000个训练集，10000个测试集，每张图都进行了尺度归一化和数字居中处理，固定尺寸大小为28*28。具体图像如下图所示，下载链接为http://yann.lecun.com/exdb/mnist/\n人群密度估计数据库\nUCSD\n该数据集分为，UCSD Pedestrain ,people annotation，people counting三个部分，下载链接为：http://visal.cs.cityu.edu.hk/downloads/\nPETS\n该数据集包含S0，S1，S2，S3四个子集，S0为训练数据，S1为行人计数和密度估计，S2为行人跟踪，S3为流分析和事件识别，下载链接为：http://www.cvg.reading.ac.uk/PETS2009/a.html\n\nMall dataset\n下载链接为：http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html\nShanghaiTech_Crowd_Counting_Dataset:\n该数据集为上海科技大学研究生张营营，在其2016cvpr中所使用的数据集，数据集分为A,B两部分，每一部分都分好了train和test，下载链接为:https://pan.baidu.com/s/1gfyNBTh\nUCF_CC_50：\n官方的我也没找到，自己传一个自己的，下载链接为:http://download.csdn.net/detail/qq_14845119/9800218\n人头检测数据库\nHollywoodHeads dataset\n该数据集为从视频中截取的图片，包含224740张jpeg格式图片，还有xml格式的标注，和VOC的标注方式一样。下载链接为:http://www.di.ens.fr/willow/research/headdetection/release/HollywoodHeads.zip\n车型识别数据库\nCompCars\n该数据集包含208826个车辆图片工1716种最新款的车辆型号，是由实际场景和网上图片组成的数据集。包含了车辆的，\ncar hierarchy（car make ,car model,year of manufacture），\ncar attribute（maximum speed, displacement, num of doors, num of seats, type of car），\nviewpoints（front(F), rear(R), side(S), front-side(FS), rear-side(RS)），\ncar parts（headlight ,taillight, fog light, air intake, console, steering wheel, dashboard, gear lever ）\n等属性。下载链接为，http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html"}
{"content2":"1. 图像分割\n从图像中将某个特定区域与其他部分进行分离并提取出来的处理就是图像分割。因为图像分割处理实际上就是区分图像中的“前景目标”和“背景”，所以通常又称之为图像的二值化处理。图像分割在图像分析、图像识别、图像检测等方面占有非常重要的低位。\n在计算机视觉领域，图像分割（Segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）（也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。图像分割通常用于定位图像中的物体和边界（线，曲线等）\n更精确的，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。图像分割的结果是图像上子区域的集合（这些子区域的全体覆盖了整个图像），或是从图像中提取的轮廓线的集合（例如边缘检测）。一个子区域中的每个像素在某种特性的度量下或是由计算得出的特性都是相似的，例如颜色、亮度、纹理。邻接区域在某种特性的度量下有很大的不同。\n在图像分割的处理中，其实可以将图像视作是由像素组成的有序集合，而图像分割就是将此集合按照某种规则划分出若干子集的过程。\n图像分割的方法依照分割时所依据的图像特征不同，大致可以分为三大类：\n1）阈值方法：这种方法是根据图像的灰度值分布特性来确定某个阈值来进行图像分割；\n2）边界分割法：这种方法是通过检测出封闭某个区域的边界来进行图像分割的。通俗的讲，这类方法实际上就是沿着闭合的边缘线将其包围的区域剪切出来；\n3）区域提取方法：这类方法的特点是根据特定区域与其他背景区域特性上的不同来进行图像分割。\n2. 欧氏距离( Euclidean Distance)\n通常我们总是习惯在相应的起点和终点之间用直线段相连, 并求取相应的直线距离, 即欧氏距离。\n但是, 这种方法并非对所有的情况都有效, 当两点间的直线段有一部分不落在所考虑的区域之内时(如小船在湖泊中航行的例子), 欧氏距离对所讨论的问题实际上是没有意义的, 这就是欧距离在 空间分析过程中的局限性。其原因在于定义区域中两点间的距离时, 没有考虑到区域的连通性, 只考虑了起点和终点间的抽象距离。\n3. 测地距离(Geodesic Distance)\n测地距离是数学形态学中的一个重要概念，主要用于流域分割（流域又称集水区域，是指流经其中的水流和其它物质从一个公共的出水口排出从而形 成一个集中的排水区域）。\n如下图一连通图形所示，A、B是其中两点，按通常欧式距离（ Euclidean distance）也称欧几里得距离，它是一个通常采用的距离定义，它是在m维空间中两个点之间的真实距离）的定义，A、B间的距离应为直线段AB的长度，但是有时线段AB的一部分可能会不包括在连通图形X内，如在下图中线段AB就有一段没有包含在连通的图形中，因此这种距离有其不合理的一面。现用如下方法重新定义A、B之间的距离：由于下图是连通的，故在所给图一的连通图形中至少有一条线路可以连接A、B两点，如下图一所示，所有这些线中最短的一条称为A、B间的测地弧。测地弧的长度称为A、B间的测地距离，记为D(A-B)。\n4. SIFT特征\nSIFT（Scale-invariant feature transform）：尺度不变特征变换，是用于图像处理领域的一种描述。这种描述具有尺度不变性，可在图像中检测出关键点，是一种局部特征描述子。\nSIFT特征是基于物体上的一些局部外观的兴趣点而与影像的大小和旋转无关。对于光线、噪声、微视角改变的容忍度也相当高。基于这些特性，它们是高度显著而且相对容易撷取，在母数庞大的特征数据库中，很容易辨识物体而且鲜有误认。使用SIFT特征描述对于部分物体遮蔽的侦测率也相当高，甚至只需要3个以上的SIFT物体特征就足以计算出位置与方位。在现今的电脑硬件速度下和小型的特征数据库条件下，辨识速度可接近即时运算。SIFT特征的信息量大，适合在海量数据库中快速准确匹配。\n4.1 SIFT算法的特点\nSIFT算法具有如下一些特点：\n1）SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性；\n2）独特性（Distinctiveness）好：信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配；\n3）多量性：即使少数的几个物体也可以产生大量的SIFT特征向量；\n4）高速性：经优化的SIFT匹配算法甚至可以达到实时的要求；\n5）可扩展性：可以很方便的与其他形式的特征向量进行联合。\n4.2 SIFT特征检测编辑\nSIFT特征检测主要包括以下4个基本步骤：\n1）尺度空间极值检测\n搜索所有尺度上的图像位置。通过高斯微分函数来识别潜在的对于尺度和旋转不变的兴趣点。\n2）关键点定位\n在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度。\n3）方向确定\n基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而提供对于这些变换的不变性。\n4）关键点描述\n在每个关键点周围的邻域内，在选定的尺度上测量图像局部的梯度。这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变化。\n4.3 SIFT特征匹配\nSIFT特征匹配主要包括2个阶段：\n第一阶段：SIFT特征的生成，即从多幅图像中提取对尺度缩放、旋转、亮度变化无关的特征向量。\n第二阶段：SIFT特征向量的匹配。\n5. BOW (bag of words) 词袋模型\nSIFT特征虽然也能描述一幅图像，但是每个SIFT矢量都是128维的，而且一幅图像通常都包含成百上千个SIFT矢量，在进行相似度计算时，这个计算量是非常大的，通行的做法是用聚类算法(如K-means)对这些矢量数据进行聚类，然后用聚类中的一个簇代表BOW中的一个视觉词，将同一幅图像的SIFT矢量映射到视觉词序列生成码本，这样每一幅图像只用一个码本矢量来描述，这样计算相似度时效率就大大提高了。\n6. Haar-like特征\nHaar-like特征：Haar特征值反映了图像的灰度变化情况。最早是由Papageorgiou等应用于人脸表示。\nHaar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。\n例如：脸部的一些特征能由矩形特征简单的描述，如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。但矩形特征只对一些简单的图形结构，如边缘、线段较敏感，所以只能描述特定走向（水平、垂直、对角）的结构。\n7. DPM特征（可变部件模型）\nDPM(Deformable Part Model)：可变部件模型\nDPM是一个非常成功的目标检测算法，连续获得VOC（Visual Object Class）07,08,09年的检测冠军。目前已成为众多分类器、分割、人体姿态和行为分类的重要部分。2010年Pedro Felzenszwalb被VOC授予\"终身成就奖\"。DPM可以看做是HOG（Histogrrams of Oriented Gradients）的扩展，大体思路与HOG一致。先计算梯度方向直方图，然后用SVM（Surpport Vector Machine ）训练得到物体的梯度模型（Model）。有了这样的模板就可以直接用来分类了，简单理解就是模型和目标匹配。DPM只是在模型上做了很多改进工作。\n8. 计算机视觉基本任务\n计算机视觉的三个基本任务：\n1）对象检测(object detection)\n2）对象跟踪(object tracking)\n3）对象分割(object segmentation)"}
{"content2":"计算机视觉方向网站\n1 Visual 3D Modeling\n2 University of California 的计算机视觉课程\n3 多视几何教程的代码\n4 VGG\n计算机视觉方向博客\n人工智能方向博客\n尹相志Allans blog\n人工智能方向网站\n1 AI Weekly\n2 CSDN-机器学习-知识库\n3 CUM计算机学院\n实用网站\nQuora\n其他\n1 在线玩转深度学习\n2 一个数学问题讲解网站\n计算机视觉书籍\n1.计算机视觉方向网站\n1.1 Visual 3D Modeling\n美国北卡罗来纳大学教堂山分校（UNC）的Visual 3D Modeling from Images的课程notes，详细内容见该课程目录页，有一些关于摄影几何基本知识、摄像机模型、多视几何等的基本概念的介绍，浅显易懂，特别是配了很多特别棒的图，易于理解！\n1.2 University of California 的计算机视觉课程\nCSE 252B: Computer Vision II\n1.3 多视几何教程的代码\nRichard Hartley和Andrew Zisserman的多视几何的MATLAB代码，只有书中部分内容对应的代码\n1.4 VGG\n超级权威的一个网站，里面有大量计算机视觉相关算法、文章和软件\nhttp://www.robots.ox.ac.uk/~vgg/\n2.计算机视觉方向博客\nDamier Teney的个人网站，里面有许多关于该作者自己的各种关于computer vision的m文件：http://damienteney.info/dml.htm\n3. 人工智能方向博客\n尹相志Allan’s blog\n尹相志Allan’s blog主要介绍人工智能方向的内容，内容为繁体字书写，应该是香港人吧\n4.人工智能方向网站\n4.1 AI Weekly\nAI Weekly是一个人工资能方向的日报，可以输入邮箱订阅，每天更新人工智能领域的最新进展、想课程等，但为英文版本\n4.2 CSDN-机器学习-知识库\nCSDN-机器学习-知识库\n4.3 CUM计算机学院\nCUM机器学习团队-joe\n5.实用网站\nQuora\n美国的一个问答网站，类似于国内的知乎，比较活跃，但需要使用英文进行问答\n６.其他\n6.1 在线玩转深度学习\nGoogle Tensorflow给出了一个非常直观的playground，任何人都可以通过该网站玩转深度学习，非常酷炫\n6.2 一个数学问题讲解网站\ninteractive mathematics\n该网站以浅显易懂的语言讲解了许多基本的数学知识，比如，什么是向量、圆、….等\n7.计算机视觉书籍\n《Computer Vision for Visual Effects》\n现在的很多大片例如阿凡达等，里面的特效制作都使用了大量的计算机视觉技术。这本书就是用来介绍如何使用现在最先进的计算机视觉技术来制作电影电视特效。"}
{"content2":"以下链接是转载自blog.csdn.NET/carson2005的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）OpenCV中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）南开大学程明明助教：http://mmcheng.net/ 图像分割、检索, bing特征快速目标（行人）检测；\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://www.robots.ox.ac.uk/~phst/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/\n（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；\n（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.Net/index.html 跟踪、行人检测、\n（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库\n（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理\n（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、\n（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n（213）北京航空航天大学周付根教授：http://www.sa.buaa.edu.cn/html/yhxy/szll/jsfc/txcl/1521.html  医学图像处理\n（214）北京航空航天大学姜志国教授：http://www.sa.buaa.edu.cn/html/yhxy/xygk/zyld/1595.html 医学图像处理，遥感图像分割；\n（215）德国萨尔布吕肯大学博士后R. Benenson： http://rodrigob.github.io/# 行人检测，无人驾驶汽车\n（216）西南财经大学段江教授：http://it.swufe.edu.cn/2011-09/25/201109251002096701.html 高动态范围图像处理\n（217）中科院沈阳自动化所华春生研究员：http://people.ucas.ac.cn/~huacs 行人检测、目标跟踪、聚类分析\n（218）华中科技大学自动化学院张天序教授：http://auto.hust.edu.cn/viewnews-1978 红外图像处理，医学图像处理，武器装备图像处理\n（219）普林斯顿大学Jianxiong Xiao助理教授：http://vision.princeton.edu/people/xj/ 3D重建、3D识别、深度学习"}
{"content2":"计算机视觉三大顶级会议\n1.ICCV\nICCV 的全称是 IEEE International Conference on Computer Vision，即国际计算机视觉大会，由IEEE主办，与计算机视觉模式识别会议（CVPR）和欧洲计算机视觉会议（ECCV）并称计算机视觉方向的三大顶级会议，被澳大利亚ICT学术会议排名和中国计算机学会等机构评为最高级别学术会议，在业内具有极高的评价。不同于在美国每年召开一次的CVPR和只在欧洲召开的ECCV，ICCV在世界范围内每两年召开一次。ICCV论文录用率非常低，是三大会议中公认级别最高的。ICCV会议时间通常在四到五天，相关领域的专家将会展示最新的研究成果。该会议由美国电气和电子工程师学会（IEEE，Institute of Electrical & Electronic Engineers）主办，主要在欧洲、亚洲、美洲的一些科研实力较强的国家举行。作为世界顶级的学术会议，首届国际计算机视觉大会于1987年在伦敦揭幕，其后两年举办一届。\n2.ECCV\nECCV的全称是European Conference on Computer Vision(欧洲计算机视觉国际会议) ，两年一次，是计算机视觉三大会议（另外两个是ICCV和CVPR）之一。每次会议在全球范围录用论文300篇左右，主要的录用论文都来自美国、欧洲等顶尖实验室及研究所，中国大陆的论文数量一般在10-20篇之间。ECCV2010的论文录取率为27%。\nECCV是一个欧洲会议，欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n3.CVPR\nCVPR是IEEE Conference on Computer Vision and Pattern Recognition的缩写，即IEEE国际计算机视觉与模式识别会议。该会议是由IEEE举办的计算机视觉和模式识别领域的顶级会议。\nCVPR是世界顶级的计算机视觉会议，近年来每年有约1500名参加者，收录的论文数量一般300篇左右。本会议每年都会有固定的研讨主题，而每一年都会有公司赞助该会议并获得在会场展示的机会。\nCVPR有着较为严苛的录用标准，会议整体的录取率通常不超过30%，而口头报告的论文比例更是不高于5%。而会议的组织方是一个循环的志愿群体，通常在某次会议召开的三年之前通过遴选产生。CVPR的审稿一般是双盲的，也就是说会议的审稿与投稿方均不知道对方的信息。通常某一篇论文需要由三位审稿者进行审读。最后再由会议的领域主席(area chair)决定论文是否可被接收。\n计算机视觉国际期刊\n1.IJCV\nInternational Journal of Computer Vision(计算机视觉国际期刊)\n国际期刊计算机视觉，详细描绘了信息科学与工程这一领域的快速发展。一般性发表的文章提出广泛普遍关心的重大技术进步。短文章提供了一个新的研究成果快速发布通道。综述性文章给与了重要的评论，以及当今发展现状的概括。\n主要内容包括：\n数学，物理，计算机视觉计算方面：图像的形成，处理，分析和解释;机器学习方法，统计方法，传感器。\n应用：基于图像的绘制，计算机图形学，机器人，照片判读，图像检索，视频分析和注释，多媒体等。\n人类认知相关联：人类视觉方面的计算与架构。\n该杂志还设有书评，建议书，社论，领先的科学数据，以及在线的材料，如静止图像，视频，数据集和软件。"}
{"content2":"作者 | YaqiLYU\n来源 | YaqiLYU的知乎问答\n相信很多来这里的人和我第一次到这里一样，都是想找一种比较好的目标跟踪算法，或者想对目标跟踪这个领域有比较深入的了解，虽然这个问题是经典目标跟踪算法，但事实上，可能我们并不需要那些曾经辉煌但已被拍在沙滩上的tracker(目标跟踪算法)，而是那些即将成为经典的，或者就目前来说最好用、速度和性能都看的过去tracker。我比较关注目标跟踪中的相关滤波方向，接下来我帮您介绍下我所认识的目标跟踪，尤其是相关滤波类方法，分享一些我认为比较好的算法，顺便谈谈我的看法。\n1.图片来自某些slides和paper，如有侵权请提醒删除。\n2.以下内容主要是论文的简单总结，代码分析和个人看法，不涉及任何公司内部资料。\n3.转载请注明出处，谢谢。\n4.如有错误欢迎指出，非常感谢。有问题可以私信我，也可以在评论区提出，时间有限但也都会尽量回复，同时感谢各位道友帮忙解答。\n▌第一部分：目标跟踪速览\n先跟几个SOTA的tracker混个脸熟，大概了解一下目标跟踪这个方向都有些什么。一切要从2013年的那个数据库说起。。如果你问别人近几年有什么比较niubility的跟踪算法，大部分人都会扔给你吴毅老师的论文，OTB50和OTB100(OTB50这里指OTB-2013，OTB100这里指OTB-2015，50和100分别代表视频数量，方便记忆)：\nWu Y, Lim J, Yang M H. Online object tracking: A benchmark [C]// CVPR, 2013.\nWu Y, Lim J, Yang M H. Object tracking benchmark [J]. TPAMI, 2015.\n顶会转顶刊的顶级待遇，在加上引用量1480+320多，影响力不言而喻，已经是做tracking必须跑的数据库了，测试代码和序列都可以下载： Visual Tracker Benchmark（http://cvlab.hanyang.ac.kr/tracker_benchmark/），OTB50包括50个序列，都经过人工标注：\n两篇论文在数据库上对比了包括2012年及之前的29个顶尖的tracker，有大家比较熟悉的OAB, IVT, MIL, CT, TLD, Struck等，大都是顶会转顶刊的神作，由于之前没有比较公认的数据库，论文都是自卖自夸，大家也不知道到底哪个好用，所以这个database的意义非常重大，直接促进了跟踪算法的发展，后来又扩展为OTB100发到TPAMI，有100个序列，难度更大更加权威，我们这里参考OTB100的结果，首先是29个tracker的速度和发表时间(标出了一些性能速度都比较好的算法)：\n接下来再看结果(更加详细的情况建议您去看论文比较清晰)：\n\n直接上结论：平均来看Struck, SCM, ASLA的性能比较高，排在前三不多提，着重强调CSK，第一次向世人展示了相关滤波的潜力，排第四还362FPS简直逆天了。速度排第二的是经典算法CT(64fps)(与SCM, ASLA等都是那个年代最热的稀疏表示)。如果对更早期的算法感兴趣，推荐另一篇经典的survey(反正我是没兴趣也没看过):\nYilmaz A, Javed O, Shah M. Object tracking: A survey [J]. CSUR, 2006.\n2012年以前的算法基本就是这样，自从2012年AlexNet问世以后，CV各个领域都有了巨大变化，所以我猜你肯定还想知道2013到2017年发生了什么，抱歉我也不知道(容我卖个关子)，不过我们可以肯定的是，2013年以后的论文一定都会引用OTB50这篇论文，借助谷歌学术中的被引用次数功能，得到如下结果：\n这里仅列举几个引用量靠前的，依次是Struck转TPAMI, 三大相关滤波方法KCF, CN, DSST, 和VOT竞赛，这里仅作示范，有兴趣可以亲自去试试。(这么做的理论依据是：一篇论文，在它之前的工作可以看它的引用文献，之后的工作可以看谁引用了它；虽然引用量并不能说明什么，但好的方法大家基本都会引用的(表示尊重和认可)；之后还可以通过限定时间来查看某段时间的相关论文，如2016-2017就能找到最新的论文了，至于论文质量需要仔细甄别；其他方向的重要论文也可以这么用，顺藤摸瓜，然后你就知道大牛是哪几位，接着关注跟踪一下他们的工作 ) 这样我们就大致知道目标跟踪领域的最新进展应该就是相关滤波无疑了，再往后还能看到相关滤波类算法有SAMF, LCT, HCF, SRDCF等等。\n当然，引用量也与时间有关，建议分每年来看。此外，最新版本OPENCV3.2除了TLD，也包括了几个很新的跟踪算法 OpenCV: Tracking API（https://www.docs.opencv.org/3.2.0/d9/df8/group__tracking.html）  ：\nTrackerKCF接口实现了KCF和CN，影响力可见一斑，还有个GOTURN是基于深度学习的方法，速度虽快但精度略差，值得去看看。tracking方向的最新论文，可以跟进三大会议(CVPR/ICCV/ECCV) 和arXiv。\n▌第二部分：背景介绍\n接下来总体介绍下目标跟踪。这里说的目标跟踪，是通用单目标跟踪，第一帧给个矩形框，这个框在数据库里面是人工标注的，在实际情况下大多是检测算法的结果，然后需要跟踪算法在后续帧紧跟住这个框，以下是VOT对跟踪算法的要求：\n通常目标跟踪面临几大难点(吴毅在VALSE的slides)：外观变形，光照变化，快速运动和运动模糊，背景相似干扰：\n平面外旋转，平面内旋转，尺度变化，遮挡和出视野等情况：\n正因为这些情况才让tracking变得很难，目前比较常用的数据库除了OTB，还有前面找到的VOT竞赛数据库(类比ImageNet)，已经举办了四年，VOT2015和VOT2016都包括60个序列，所有序列也是免费下载 VOT Challenge | Challenges：\nVOT Challenge | Challenges：\nhttp://votchallenge.net/challenges.html\nKristan M, Pflugfelder R, Leonardis A, et al. The visual object tracking vot2013 challenge results [C]// ICCV, 2013.\nKristan M, Pflugfelder R, Leonardis A, et al. The Visual Object Tracking VOT2014 Challenge Results [C]// ECCV, 2014.\nKristan M, Matas J, Leonardis A, et al. The visual object tracking vot2015 challenge results [C]// ICCV, 2015.\nKristan M, Ales L, Jiri M, et al. The Visual Object Tracking VOT2016 Challenge Results [C]// ECCV, 2016.\nOTB和VOT区别：OTB包括25%的灰度序列，但VOT都是彩色序列，这也是造成很多颜色特征算法性能差异的原因；两个库的评价指标不一样，具体请参考论文；VOT库的序列分辨率普遍较高，这一点后面分析会提到。对于一个tracker，如果论文在两个库(最好是OTB100和VOT2016)上都结果上佳，那肯定是非常优秀的(两个库调参你能调好，我服，认了~~)，如果只跑了一个，个人更偏向于VOT2016，因为序列都是精细标注，且评价指标更好(人家毕竟是竞赛，评价指标发过TPAMI的)，差别最大的地方，OTB有随机帧开始，或矩形框加随机干扰初始化去跑，作者说这样更加符合检测算法给的框框；而VOT是第一帧初始化去跑，每次跟踪失败(预测框和标注框不重叠)时，5帧之后重新初始化，VOT以short-term为主，且认为跟踪检测应该在一起不分离，detecter会多次初始化tracker。\n补充：OTB在2013年公开了，对于2013以后的算法是透明的，论文都会去调参，尤其是那些只跑OTB的论文，如果关键参数直接给出还精确到小数点后两位，建议您先实测(人心不古啊~被坑的多了)。VOT竞赛的数据库是每年更新，还动不动就重新标注，动不动就改变评价指标，对当年算法是难度比较大，所以结果相对更可靠。（相信很多人和我一样，看每篇论文都会觉得这个工作太好太重要了，如果没有这篇论文，必定地球爆炸，宇宙重启~~所以就像大家都通过历年ILSVRC竞赛结果为主线了解深度学习的发展一样，第三方的结果更具说服力，所以我也以竞赛排名+是否公开源码+实测性能为标准，优选几个算法分析）\n目标视觉跟踪(Visual Object Tracking)，大家比较公认分为两大类：生成(generative)模型方法和判别(discriminative)模型方法，目前比较流行的是判别类方法，也叫检测跟踪tracking-by-detection，为保持回答的完整性，以下简单介绍。\n生成类方法，在当前帧对目标区域建模，下一帧寻找与模型最相似的区域就是预测位置，比较著名的有卡尔曼滤波，粒子滤波，mean-shift等。举个例子，从当前帧知道了目标区域80%是红色，20%是绿色，然后在下一帧，搜索算法就像无头苍蝇，到处去找最符合这个颜色比例的区域，推荐算法ASMS vojirt/asms（https://github.com/vojirt/asms）：\nVojir T, Noskova J, Matas J. Robust scale-adaptive mean-shift for tracking [J]. Pattern Recognition Letters, 2014.\nASMS与DAT并称“颜色双雄”(版权所有翻版必究)，都是仅颜色特征的算法而且速度很快，依次是VOT2015的第20名和14名，在VOT2016分别是32名和31名(中等水平)。ASMS是VOT2015官方推荐的实时算法，平均帧率125FPS，在经典mean-shift框架下加入了尺度估计，经典颜色直方图特征，加入了两个先验(尺度不剧变+可能偏最大)作为正则项，和反向尺度一致性检查。作者给了C++代码，在相关滤波和深度学习盛行的年代，还能看到mean-shift打榜还有如此高的性价比实在不容易(已泪目~~)，实测性能还不错，如果您对生成类方法情有独钟，这个非常推荐您去试试。(某些算法，如果连这个你都比不过。。天台在24楼，不谢)\n判别类方法，OTB50里面的大部分方法都是这一类，CV中的经典套路图像特征+机器学习， 当前帧以目标区域为正样本，背景区域为负样本，机器学习方法训练分类器，下一帧用训练好的分类器找最优区域：\n与生成类方法最大的区别是，分类器采用机器学习，训练中用到了背景信息，这样分类器就能专注区分前景和背景，所以判别类方法普遍都比生成类好。举个例子，在训练时告诉tracker目标80%是红色，20%是绿色，还告诉它背景中有橘红色，要格外注意别搞错了，这样的分类器知道更多信息，效果也相对更好。tracking-by-detection和检测算法非常相似，如经典行人检测用HOG+SVM，Struck用到了haar+structured output SVM，跟踪中为了尺度自适应也需要多尺度遍历搜索，区别仅在于跟踪算法对特征和在线机器学习的速度要求更高，检测范围和尺度更小而已。\n这点其实并不意外，大多数情况检测识别算法复杂度比较高不可能每帧都做，这时候用复杂度更低的跟踪算法就很合适了，只需要在跟踪失败(drift)或一定间隔以后再次检测去初始化tracker就可以了。其实我就想说，FPS才TMD是最重要的指标，慢的要死的算法可以去死了(同学别这么偏激，速度是可以优化的)。经典判别类方法推荐Struck和TLD，都能实时性能还行，Struck是2012年之前最好的方法，TLD是经典long-term的代表，思想非常值得借鉴：\nHare S, Golodetz S, Saffari A, et al. Struck: Structured output tracking with kernels [J]. IEEE TPAMI, 2016.\nKalal Z, Mikolajczyk K, Matas J. Tracking-learning-detection [J]. IEEE TPAMI, 2012.\n长江后浪推前浪，前面的已被排在沙滩上，这个后浪就是相关滤波和深度学习。相关滤波类方法correlation filter简称CF，也叫做discriminative correlation filter简称DCF，注意和后面的DCF算法区别，包括前面提到的那几个，也是后面要着重介绍的。\n深度学习（Deep ConvNet based）类方法，因为深度学习类目前不适合落地就不瞎推荐了，可以参考Winsty的几篇 Naiyan Wang - Home（链接1），还有VOT2015的冠军MDNet Learning Multi-Domain Convolutional Neural Networks for Visual Tracking（链接2），以及以及VOT2016的冠军TCNNhttp://wwwvotchallenge.net/vot201（链接3）（），速度方面比较突出的如80FPS的SiamFC SiameseFC tracker（链接4）和100FPS的GOTURN davheld/GOTURN（链接5），注意都是在GPU上。基于ResNet的SiamFC-R(ResNet)在VOT2016表现不错，很看好后续发展，有兴趣也可以去VALSE听作者自己讲解 VALSE-20160930-LucaBertinetto-Oxford-JackValmadre-Oxford-pu（链接6），至于GOTURN，效果比较差，但优势是跑的很快100FPS，如果以后效果也能上来就好了。做科研的同学深度学习类是关键，能兼顾速度就更好了。\n链接1.\nhttp://www.winsty.net/\n链接2.\nhttp://cvlab.postech.ac.kr/research/mdnet/\n链接3.\nhttp://www.votchallenge.net/vot2016/download/44_TCNN.zip\n链接4.\nhttp://www.robots.ox.ac.uk/~luca/siamese-fc.html\n链接5.\nhttps://github.com/davheld/GOTURN\n链接6.\nhttp://www.iqiyi.com/w_19ruirwrel.html#vfrm=8-8-0-1\nNam H, Han B. Learning multi-domain convolutional neural networks for visual tracking [C]// CVPR, 2016.\nNam H, Baek M, Han B. Modeling and propagating cnns in a tree structure for visual tracking. arXiv preprint arXiv:1608.07242, 2016.\nBertinetto L, Valmadre J, Henriques J F, et al. Fully-convolutional siamese networks for object tracking [C]// ECCV, 2016.\nHeld D, Thrun S, Savarese S. Learning to track at 100 fps with deep regression networks [C]// ECCV, 2016.\n最后，深度学习END2END的强大威力在目标跟踪方向还远没有发挥出来，还没有和相关滤波类方法拉开多大差距(速度慢是天生的我不怪你，但效果总该很好吧，不然你存在的意义是什么呢。。革命尚未成功，同志仍须努力)。另一个需要注意的问题是目标跟踪的数据库都没有严格的训练集和测试集，需要离线训练的深度学习方法就要非常注意它的训练集有没有相似序列，而且一直到VOT2017官方才指明要限制训练集，不能用相似序列训练模型。\n最后强力推荐两个资源。王强@Qiang Wang维护的benchmark_resultsfoolwood/benchmark_results（https://github.com/foolwood/benchmark_results）：大量顶级方法在OTB库上的性能对比，各种论文代码应有尽有，大神自己C++实现并开源的CSK, KCF和DAT，还有他自己的DCFNet论文加源码，找不着路的同学请跟紧。\n@H Hakase维护的相关滤波类资源\nHakaseH/CF_benchmark_results （https://github.com/HakaseH/TBCF），详细分类和论文代码资源，走过路过别错过，相关滤波类算法非常全面，非常之用心！\n(以上两位，看到了请来我处交一下广告费，9折优惠~~)\n▌第三部分：相关滤波\n介绍最经典的高速相关滤波类跟踪算法CSK, KCF/DCF, CN。很多人最早了解CF，应该和我一样，都是被下面这张图吸引了：\n这是KCF/DCF算法在OTB50上(2014年4月就挂arVix了, 那时候OTB100还没有发表)的实验结果，Precision和FPS碾压了OTB50上最好的Struck，看惯了勉强实时的Struck和TLD，飙到高速的KCF/DCF突然有点让人不敢相信，其实KCF/DCF就是在OTB上大放异彩的CSK的多通道特征改进版本。注意到那个超高速615FPS的MOSSE(严重超速这是您的罚单)，这是目标跟踪领域的第一篇相关滤波类方法，这其实是真正第一次显示了相关滤波的潜力。和KCF同一时期的还有个CN，在2014'CVPR上引起剧烈反响的颜色特征方法，其实也是CSK的多通道颜色特征改进算法。从MOSSE(615)到 CSK(362) 再到 KCF(172FPS), DCF(292FPS), CN(152FPS), CN2(202FPS)，速度虽然是越来越慢，但效果越来越好，而且始终保持在高速水平：\nBolme D S, Beveridge J R, Draper B A, et al. Visual object tracking using adaptive correlation filters [C]// CVPR, 2010.\nHenriques J F, Caseiro R, Martins P, et al. Exploiting the circulant structure of tracking-by- detection with kernels [C]// ECCV, 2012.\nHenriques J F, Rui C, Martins P, et al. High-Speed Tracking with Kernelized Correlation Filters [J]. IEEE TPAMI, 2015.\nDanelljan M, Shahbaz Khan F, Felsberg M, et al. Adaptive color attributes for real-time visual tracking [C]// CVPR, 2014.\nCSK和KCF都是Henriques J F(牛津大学)João F. Henriques大神先后两篇论文，影响后来很多工作，核心部分的岭回归，循环移位的近似密集采样，还给出了整个相关滤波算法的详细推导。还有岭回归加kernel-trick的封闭解，多通道HOG特征。\nMartin Danelljan大牛(林雪平大学)用多通道颜色特征Color Names(CN)去扩展CSK得到了不错的效果，算法也简称CN Coloring Visual Tracking 。\nCN Coloring Visual Tracking\nhttp://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html\nMOSSE是单通道灰度特征的相关滤波，CSK在MOSSE的基础上扩展了密集采样(加padding)和kernel-trick，KCF在CSK的基础上扩展了多通道梯度的HOG特征，CN在CSK的基础上扩展了多通道颜色的Color Names。HOG是梯度特征，而CN是颜色特征，两者可以互补，所以HOG+CN在近两年的跟踪算法中成为了hand-craft特征标配。最后，根据KCF/DCF的实验结果，讨论两个问题：\n1. 为什么只用单通道灰度特征的KCF和用了多通道HOG特征的KCF速度差异很小？\n第一，作者用了HOG的快速算法fHOG，来自Piotr's Computer Vision Matlab Toolbox，C代码而且做了SSE优化。如对fHOG有疑问，请参考论文Object Detection with Discriminatively Trained Part Based Models第12页。\n第二，HOG特征常用cell size是4，这就意味着，100*100的图像，HOG特征图的维度只有25*25，而Raw pixels是灰度图归一化，维度依然是100*100，我们简单算一下：27通道HOG特征的复杂度是27*625*log(625)=47180，单通道灰度特征的复杂度是10000*log(10000)=40000，理论上也差不多，符合表格。\n看代码会发现，作者在扩展后目标区域面积较大时，会先对提取到的图像块做因子2的下采样到50*50，这样复杂度就变成了2500*log(2500)=8495，下降了非常多。那你可能会想，如果下采样再多一点，复杂度就更低了，但这是以牺牲跟踪精度为代价的，再举个例子，如果图像块面积为200*200，先下采样到100*100，再提取HOG特征，分辨率降到了25*25，这就意味着响应图的分辨率也是25*25，也就是说，响应图每位移1个像素，原始图像中跟踪框要移动8个像素，这样就降低了跟踪精度。在精度要求不高时，完全可以稍微牺牲下精度提高帧率(但看起来真的不能再下采样了)。\n2. HOG特征的KCF和DCF哪个更好？\n大部分人都会认为KCF效果超过DCF，而且各属性的准确度都在DCF之上，然而，如果换个角度来看，以DCF为基准，再来看加了kernel-trick的KCF，mean precision仅提高了0.4%，而FPS下降了41%，这么看是不是挺惊讶的呢？除了图像块像素总数，KCF的复杂度还主要和kernel-trick相关。所以，下文中的CF方法如果没有kernel-trick，就简称基于DCF，如果加了kernel-trick，就简称基于KCF(剧透基本各占一半)。当然这里的CN也有kernel-trick，但请注意，这是Martin Danelljan大神第一次使用kernel-trick，也是最后一次。。。\n这就会引发一个疑问，kernel-trick这么强大的东西，怎么才提高这么点？这里就不得不提到Winsty的另一篇大作：\nWang N, Shi J, Yeung D Y, et al. Understanding and diagnosing visual tracking systems[C]// ICCV, 2015.\n一句话总结，别看那些五花八门的机器学习方法，那都是虚的，目标跟踪算法中特征才是最重要的（就是因为这篇文章我粉了WIN叔哈哈），以上就是最经典的三个高速算法，CSK, KCF/DCF和CN，推荐。\n▌第四部分：14年的尺度自适应\nVOT与OTB一样最早都是2013年出现的，但VOT2013序列太少，第一名的PLT代码也找不到，没有参考价值就直接跳过了。直接到了VOT2014竞赛 VOT2014 Benchmark（http://votchallenge.net/vot2014/index.html）。这一年有25个精挑细选的序列，38个算法，那时候深度学习的战火还没有烧到tracking，所以主角也只能是刚刚展露头角就独霸一方的CF，下面是前几名的详细情况：\n前三名都是相关滤波CF类方法，第三名的KCF已经很熟悉了，这里稍微有点区别就是加了多尺度检测和子像素峰值估计，再加上VOT序列的分辨率比较高(检测更新图像块的分辨率比较高)，导致竞赛中的KCF的速度只有24.23(EFO换算66.6FPS)。这里speed是EFO(Equivalent Filter Operations)，在VOT2015和VOT2016里面也用这个参数衡量算法速度，这里一次性列出来供参考(MATLAB实现的tracker实际速度要更高一些)：\n其实前三名除了特征略有差异，核心都是KCF为基础扩展了多尺度检测，概要如下：\n尺度变化是跟踪中比较基础和常见的问题，前面介绍的KCF/DCF和CN都没有尺度更新，如果目标缩小，滤波器就会学习到大量背景信息，如果目标扩大，滤波器就跟着目标局部纹理走了，这两种情况都很可能出现非预期的结果，导致漂移和失败。\nSAMF ihpdep/samf（https://github.com/ihpdep/samf），浙大Yang Li的工作，基于KCF，特征是HOG+CN，多尺度方法是平移滤波器在多尺度缩放的图像块上进行目标检测，取响应最大的那个平移位置及所在尺度：\nLi Y, Zhu J. A scale adaptive kernel correlation filter tracker with feature integration [C]// ECCV, 2014.\nMartin Danelljan的DSST Accurate scale estimation for visual tracking（http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html），只用了HOG特征，DCF用于平移位置检测，又专门训练类似MOSSE的相关滤波器检测尺度变化，开创了平移滤波+尺度滤波，之后转TPAMI做了一系列加速的版本fDSST，非常+非常+非常推荐：\nDanelljan M, Häger G, Khan F, et al. Accurate scale estimation for robust visual tracking [C]// BMVC, 2014.\nDanelljan M, Hager G, Khan F S, et al. Discriminative Scale Space Tracking [J]. IEEE TPAMI, 2017.\n简单对比下这两种尺度自适应的方法：\nDSST和SAMF所采用的尺度检测方法哪个更好？\n首先给大家讲个笑话：Martin Danelljan大神提出DSST之后，他的后续论文就再没有用过(直到最新CVPR的ECO-HC中为了加速用了fDSST)。\n虽然SAMF和DSST都可以跟上普通的目标尺度变化，但SAMF只有7个尺度比较粗，而DSST有33个尺度比较精细准确；\nDSST先检测最佳平移再检测最佳尺度，是分步最优，而SAMF是平移尺度一起检测，是平移和尺度同时最优，而往往局部最优和全局最优是不一样的；\nDSST将跟踪划分为平移跟踪和尺度跟踪两个问题，可以采用不同的方法和特征，更加灵活，但需要额外训练一个滤波器，每帧尺度检测需要采样33个图像块，之后分别计算特征、加窗、FFT等，尺度滤波器比平移滤波器慢很多；SAMF只需要一个滤波器，不需要额外训练和存储，每个尺度检测就一次提特征和FFT，但在图像块较大时计算量比DSST高。\n所以尺度检测DSST并不总是比SAMF好，其实在VOT2015和VOT2016上SAMF都是超过DSST的，当然这主要是因为特征更好，但至少说明尺度方法不差。总的来说，DSST做法非常新颖，速度更快，SAMF同样优秀也更加准确。\nDSST一定要33个尺度吗？\nDSST标配33个尺度非常非常敏感，轻易降低尺度数量，即使你增加相应步长，尺度滤波器也会完全跟不上尺度变化。关于这一点可能解释是，训练尺度滤波器用的是一维样本，而且没有循环移位，这就意味着一次训练更新只有33个样本，如果降低样本数量，会造成训练不足，分类器判别力严重下降，不像平移滤波器有非常多的移位样本(个人看法欢迎交流)。总之，请不要轻易尝试大幅降低尺度数量，如果非要用尺度滤波器33和1.02就很好。\n以上就是两种推荐的尺度检测方法，以后简称为类似DSST的多尺度和类似SAMF的多尺度。如果更看重速度，加速版的fDSST，和仅3个尺度的SAMF(如VOT2014中的KCF)就是比较好的选择；如果更看重精确，33个尺度的DSST，及7个尺度的SAMF就比较合适。\n▌第五部分：边界效应\n接下来到了VOT2015竞赛 VOT2015 Challenge | Home，这一年有60个精挑细选的序列，62个tracker，最大看点是深度学习开始进击tracking领域，MDNet直接拿下当年的冠军，而结合深度特征的相关滤波方法DeepSRDCF是第二名，主要解决边界效应的SRDCF仅HOG特征排在第四：\n随着VOT竞赛的影响力扩大，举办方也是用心良苦，经典的和顶尖的齐聚一堂，百家争鸣，多达62个tracker皇城PK，华山论剑。除了前面介绍的深度学习和相关滤波，还有结合object proposals(类物体区域检测)的EBT(EBT：Proposal与Tracking不得不说的秘密 - 知乎专栏，https://zhuanlan.zhihu.com/p/26654891)排第三，Mean-Shift类颜色算法ASMS是推荐实时算法，还有前面提到的另一个颜色算法DAT，而在第9的那个Struck已经不是原来的Struck了。除此之外，还能看到经典方法如OAB, STC, CMT, CT, NCC等都排在倒数位置， 经典方法已经被远远甩在后面。\n在介绍SRDCF之前，先来分析下相关滤波有什么缺点。总体来说，相关滤波类方法对快速变形和快速运动情况的跟踪效果不好。\n快速变形主要因为CF是模板类方法。容易跟丢这个比较好理解，前面分析了相关滤波是模板类方法，如果目标快速变形，那基于HOG的梯度模板肯定就跟不上了，如果快速变色，那基于CN的颜色模板肯定也就跟不上了。这个还和模型更新策略与更新速度有关，固定学习率的线性加权更新，如果学习率太大，部分或短暂遮挡和任何检测不准确，模型就会学习到背景信息，积累到一定程度模型跟着背景私奔了，一去不复返。如果学习率太小，目标已经变形了而模板还是那个模板，就会变得不认识目标。\n快速运动主要是边界效应(Boundary Effets)，而且边界效应产生的错误样本会造成分类器判别力不够强，下面分训练阶段和检测阶段分别讨论。\n训练阶段，合成样本降低了判别能力。如果不加余弦窗，那么移位样本是长这样的：\n除了那个最原始样本，其他样本都是“合成”的，100*100的图像块，只有1/10000的样本是真实的，这样的样本集根本不能拿来训练。如果加了余弦窗，由于图像边缘像素值都是0，循环移位过程中只要目标保持完整，就认为这个样本是合理的，只有当目标中心接近边缘时，目标跨越了边界的那些样本是错误的，这样虽不真实但合理的样本数量增加到了大约2/3(一维情况padding= 1)。但我们不能忘了即使这样仍然有1/3(3000/10000，http://tel:3000/10000)的样本是不合理的，这些样本会降低分类器的判别能力。再者，加余弦窗也不是“免费的”，余弦窗将图像块的边缘区域像素全部变成0，大量过滤掉了分类器本来非常需要学习的背景信息，原本训练时判别器能看到的背景信息就非常有限，我们还加了个余弦窗挡住了背景，这样进一步降低了分类器的判别力(是不是上帝在我前遮住了帘。。不是上帝，是余弦窗)。\n检测阶段，相关滤波对快速运动的目标检测比较乏力。相关滤波训练的图像块和检测的图像块大小必须是一样的，这就是说你训练了一个100*100的滤波器，那你也只能检测100*100的区域，如果打算通过加更大的padding来扩展检测区域，那样除了扩展了复杂度，并不会有什么好处。目标运动可能是目标自身移动，或摄像机移动，按照目标在检测区域的位置分四种情况来看：\n如果目标在中心附近，检测准确且成功。\n如果目标移动到了边界附近但还没有出边界，加了余弦窗以后，部分目标像素会被过滤掉，这时候就没法保证这里的响应是全局最大的，而且，这时候的检测样本和训练过程中的那些不合理样本很像，所以很可能会失败。\n如果目标的一部分已经移出了这个区域，而我们还要加余弦窗，很可能就过滤掉了仅存的目标像素，检测失败。\n如果整个目标已经位移出了这个区域，那肯定就检测失败了。\n以上就是边界效应(Boundary Effets)，推荐两个主流的解决边界效应的方法，其中SRDCF速度比较慢，并不适合实时场合。\nMartin Danelljan的SRDCF Learning Spatially Regularized Correlation Filters for Visual Tracking，主要思路：既然边界效应发生在边界附近，那就忽略所有移位样本的边界部分像素，或者说限制让边界附近滤波器系数接近0：\nLearning Spatially Regularized Correlation Filters for Visual Tracking\nhttp://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html\nDanelljan M, Hager G, Shahbaz Khan F, et al. Learning spatially regularized correlation filters for visual tracking [C]// ICCV. 2015.\nSRDCF基于DCF，类SAMF多尺度，采用更大的检测区域(padding = 4)，同时加入空域正则化，惩罚边界区域的滤波器系数，由于没有闭合解，采用高斯-塞德尔方法迭代优化。检测区域扩大(1.5->4)，迭代优化(破坏了闭合解)导致SRDCF只有5FP，但效果非常好是2015年的baseline。\n另一种方法是Hamed Kiani提出的MOSSE改进算法，基于灰度特征的CFLM Correlation Filters with Limited Boundaries 和基于HOG特征的BACF Learning Background-Aware Correlation Filters for Visual Tracking，主要思路是采用较大尺寸检测图像块和较小尺寸滤波器来提高真实样本的比例，或者说滤波器填充0以保持和检测图像一样大，同样没有闭合解，采用ADMM迭代优化：\nKiani Galoogahi H, Sim T, Lucey S. Correlation filters with limited boundaries [C]// CVPR, 2015.\nKiani Galoogahi H, Fagg A, Lucey S. Learning Background-Aware Correlation Filters for Visual Tracking [C]// ICCV, 2017.\nCFLB仅单通道灰度特征，虽然速度比较快167FPS，但性能远不如KCF，不推荐；最新BACF将特征扩展为多通道HOG特征，性能超过了SRDCF，而且速度比较快35FPS，非常推荐。\n其实这两个解决方案挺像的，都是用更大的检测及更新图像块，训练作用域比较小的相关滤波器，不同点是SRDCF的滤波器系数从中心到边缘平滑过渡到0，而CFLM直接用0填充滤波器边缘。\nVOT2015相关滤波方面还有排在第二名，结合深度特征的DeepSRDCF，因为深度特征都非常慢，在CPU上别说高速，实时都到不了，虽然性能非常高，但这里就不推荐，先跳过。\n▌第六部分：颜色直方图与相关滤波\nVOT2016竞赛 VOT2016 Challenge | Home，依然是VOT2015那60个序列，不过这次做了重新标注更加公平合理，今年有70位参赛选手，意料之中深度学习已经雄霸天下了，8个纯CNN方法和6个结合深度特征的CF方法大都名列前茅，还有一片的CF方法，最最最重要的是，良心举办方竟然公开了他们能拿到的38个tracker，部分tracker代码和主页，下载地址：VOT2016 Challenge | Trackers (以后妈妈再也不用担心我找不到源码了~)，注意部分是下载链接，部分是源码压缩包，部分源码是二进制文件，好不好用一试便知，方便对比和研究，需要的赶快去试试。马上来看竞赛结果(这里仅列举前60个)：\n高亮标出来了前面介绍过的或比较重要的方法，结合多层深度特征的相关滤波C-COT排第一名，而CNN方法TCNN是VOT2016的冠军，作者也是VOT2015冠军MDNet，纯颜色方法DAT和ASMS都在中等水平(其实两种方法实测表现非常接近)，其他tracker的情况请参考论文。再来看速度，SMACF没有公开代码，ASMS依然那么快，排在前10的方法中也有两个速度比较快，分别是排第5的Staple，和其改进算法排第9的STAPLE+，而且STAPLE+是今年的推荐实时算法。首先恭喜Luca Bertinetto的SiamFC和Staple都表现非常不错，然后再为大牛默哀三分钟(VOT2016的paper原文)：\nThis was particularly obvious in case of SiamFC trackers, which runs orders higher than realtime (albeit on GPU), and Staple, which is realtime, but are incorrectly among the non-realtime trackers.\nVOT2016竟然发生了乌龙事件，Staple在论文中CPU上是80FPS，怎么EFO在这里只有11？幸好公开代码有Staple和STAPLE+，实测下来，虽然我电脑不如Luca Bertinetto大牛但Staple我也能跑76FPS，而更可笑的是，STAPLE+比Staple慢了大约7-8倍，竟然EFO高出4倍，到底怎么回事呢？\n首先看Staple的代码，如果您直接下载Staple并设置params.visualization = 1，Staple默认调用Computer Vision System Toolbox来显示序列图像，而恰好如果您没有这个工具箱，默认每帧都会用imshow(im)来显示图像，所以非常非常慢，而设置params.visualization = 0就跑的飞快(作者你是孙猴子派来的逗逼吗)，建议您将显示图像部分代码替换成DSST中对应部分代码就可以正常速度运行和显示了。\n再来看STAPLE+的代码，对Staple的改进包括额外从颜色概率图中提取HOG特征，特征增加到56通道(Staple是28通道)，平移检测额外加入了大位移光流运动估计的响应，所以才会这么慢，而且肯定要慢很多。\n所以很大可能是VOT举办方把Staple和STAPLE+的EFO弄反了，VOT2016的实时推荐算法应该是排第5的Staple，相关滤波结合颜色方法，没有深度特征更没有CNN，跑80FPS还能排在第五，这就是接下来主要介绍的，2016年最NIUBILITY的目标跟踪算法之一Staple (直接让排在后面的一众深度学习算法怀疑人生)。\n颜色特征，在目标跟踪中颜色是个非常重要的特征，不管多少个人在一起，只要目标穿不用颜色的一幅就非常明显。前面介绍过2014年CVPR的CN是相关滤波框架下的模板颜色方法，这里隆重介绍统计颜色特征方法DAT Learning, Recognition, and Surveillance @ ICG ，帧率15FPS推荐：\nPossegger H, Mauthner T, Bischof H. In defense of color-based model-free tracking [C]// CVPR, 2015.\nDAT统计前景目标和背景区域的颜色直方图并归一化，这就是前景和背景的颜色概率模型，检测阶段，贝叶斯方法判别每个像素属于前景的概率，得到像素级颜色概率图，再加上边缘相似颜色物体抑制就能得到目标的区域了。\n如果要用一句话介绍Luca Bertinetto(牛津大学)的Staple Staple tracker，那就是把模板特征方法DSST(基于DCF)和统计特征方法DAT结合：\nBertinetto L, Valmadre J, Golodetz S, et al. Staple: Complementary Learners for Real-Time Tracking [C]// CVPR, 2016.\n前面分析了相关滤波模板类特征(HOG)对快速变形和快速运动效果不好，但对运动模糊光照变化等情况比较好；而颜色统计特征(颜色直方图)对变形不敏感，而且不属于相关滤波框架没有边界效应，快速运动当然也是没问题的，但对光照变化和背景相似颜色不好。综上，这两类方法可以互补，也就是说DSST和DAT可以互补结合：\n两个框架的算法高效无缝结合，25FPS的DSST和15FPS的DAT，而结合后速度竟然达到了80FPS。DSST框架把跟踪划分为两个问题，即平移检测和尺度检测，DAT就加在平移检测部分，相关滤波有一个响应图，像素级前景概率也有一个响应图，两个响应图线性加权得到最终响应图，其他部分与DSST类似，平移滤波器、尺度滤波器和颜色概率模型都以固定学习率线性加权更新。\n另一种相关滤波结合颜色概率的方法是17CVPR的CSR-DCF，提出了空域可靠性和通道可靠性，没有深度特征性能直逼C-COT，速度可观13FPS：\nLukežič A, Vojíř T, Čehovin L, et al. Discriminative Correlation Filter with Channel and Spatial Reliability [C]// CVPR, 2017.\nCSR-DCF中的空域可靠性得到的二值掩膜就类似于CFLM中的掩膜矩阵P，在这里自适应选择更容易跟踪的目标区域且减小边界效应；以往多通道特征都是直接求和，而CSR-DCF中通道采用加权求和，而通道可靠性就是那个自适应加权系数。采用ADMM迭代优化，可以看出CSR-DCF是DAT和CFLB的结合算法。\nVOT2015相关滤波还有排第一名的C-COT(别问我第一名为什么不是冠军，我也不知道)，和DeepSRDCF一样先跳过。\n▌第七部分：long-term和跟踪置信度\n以前提到的很多CF算法，也包括VOT竞赛，都是针对short-term的跟踪问题，即短期(shor-term)跟踪，我们只关注短期内(如100~500帧)跟踪是否准确。但在实际应用场合，我们希望正确跟踪时间长一点，如几分钟或十几分钟，这就是长期(long-term)跟踪问题。\nLong-term就是希望tracker能长期正确跟踪，我们分析了前面介绍的方法不适合这种应用场合，必须是short-term tracker + detecter配合才能实现正确的长期跟踪。\n用一句话介绍Long-term，就是给普通tracker配一个detecter，在发现跟踪出错的时候调用自带detecter重新检测并矫正tracker。\n介绍CF方向一篇比较有代表性的long-term方法，Chao Ma的LCT chaoma99/lct-tracker：\nMa C, Yang X, Zhang C, et al. Long-term correlation tracking[C]// CVPR, 2015.\nLCT在DSST一个平移相关滤波Rc和一个尺度相关滤波的基础上，又加入第三个负责检测目标置信度的相关滤波Rt，检测模块Online Detector是TLD中所用的随机蔟分类器(random fern)，在代码中改为SVM。第三个置信度滤波类似MOSSE不加padding，而且特征也不加cosine窗，放在平移检测之后。\n如果最大响应小于第一个阈值(叫运动阈值)，说明平移检测不可靠，调用检测模块重新检测。注意，重新检测的结果并不是都采纳的，只有第二次检测的最大响应值比第一次检测大1.5倍时才接纳，否则，依然采用平移检测的结果。\n如果最大响应大于第二个阈值(叫外观阈值)，说明平移检测足够可信，这时候才以固定学习率在线更新第三个相关滤波器和随机蔟分类器。注意，前两个相关滤波的更新与DSST一样，固定学习率在线每帧更新。\nLCT加入检测机制，对遮挡和出视野等情况理论上较好，速度27fps，实验只跑了OTB-2013，跟踪精度非常高，根据其他论文LCT在OTB-2015和 VOT上效果略差一点可能是两个核心阈值没有自适应， 关于long-term，TLD和LCT都可以参考 。\n接下来介绍跟踪置信度。 跟踪算法需要能反映每一次跟踪结果的可靠程度，这一点非常重要，不然就可能造成跟丢了还不知道的情况。生成类(generative)方法有相似性度量函数，判别类(discriminative)方法有机器学习方法的分类概率。有两种指标可以反映相关滤波类方法的跟踪置信度：前面见过的最大响应值，和没见过的响应模式，或者综合反映这两点的指标。\nLMCF(MM Wang的目标跟踪专栏：目标跟踪算法 - 知乎专栏 )提出了多峰检测和高置信度更新：\nWang M, Liu Y, Huang Z. Large Margin Object Tracking with Circulant Feature Maps [C]// CVPR, 2017.\n高置信度更新，只有在跟踪置信度比较高的时候才更新跟踪模型，避免目标模型被污染，同时提升速度。  第一个置信度指标是最大响应分数Fmax，就是最大响应值(Staple和LCT中都有提到)。 第二个置信度指标是平均峰值相关能量(average peak-to correlation energy, APCE)，反应响应图的波动程度和检测目标的置信水平，这个(可能)是目前最好的指标，推荐：\n\n跟踪置信度指标还有，MOSSE中的峰值旁瓣比(Peak to Sidelobe Ratio, PSR)， 由相关滤波峰值，与11*11峰值窗口以外旁瓣的均值与标准差计算得到，推荐：\n还有CSR-DCF的空域可靠性，也用了两个类似指标反映通道可靠性， 第一个指标也是每个通道的最大响应峰值，就是Fmax，第二个指标是响应图中第二和第一主模式之间的比率，反映每个通道响应中主模式的表现力，但需要先做极大值检测：\n▌第八部分：卷积特征\n最后这部分是Martin Danelljan的专场，主要介绍他的一些列工作，尤其是结合深度特征的相关滤波方法，代码都在他主页Visual Tracking，就不一一贴出了。\nDanelljan M, Shahbaz Khan F, Felsberg M, et al. Adaptive color attributes for real-time visual tracking [C]// CVPR, 2014.\n在CN中提出了非常重要的多通道颜色特征Color Names，用于CSK框架取得非常好得效果，还提出了加速算法CN2，通过类PCA的自适应降维方法，对特征通道数量降维(10 -> 2)，平滑项增加跨越不同特征子空间时的代价，也就是PCA中的协方差矩阵线性更新防止降维矩阵变化太大。\nDanelljan M, Hager G, Khan F S, et al. Discriminative Scale Space Tracking [J]. IEEE TPAMI, 2017.\nDSST是VOT2014的第一名，开创了平移滤波+尺度滤波的方式。在fDSST中对DSST进行加速，PCA方法将平移滤波HOG特征的通道降维(31 -> 18)，QR方法将尺度滤波器~1000*17的特征降维到17*17，最后用三角插值(频域插值)将尺度数量从17插值到33以获得更精确的尺度定位。\nSRDCF是VOT2015的第四名，为了减轻边界效应扩大检测区域，优化目标增加了空间约束项，用高斯-塞德尔方法迭代优化，并用牛顿法迭代优化平移检测的子网格精确目标定位。\nDanelljan M, Hager G, Shahbaz Khan F, et al. Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking [C]// CVPR, 2016.\nSRDCFdecon在SRDCF的基础上，改进了样本和学习率问题。以前的相关滤波都是固定学习率线性加权更新模型，虽然这样比较简单不用保存以前样本，但在定位不准确、遮挡、背景扰动等情况会污染模型导致漂移。SRDCFdecon选择保存以往样本(图像块包括正，负样本)，在优化目标函数中添加样本权重参数和正则项，采用交替凸搜索，首先固定样本权重，高斯-塞德尔方法迭代优化模型参数，然后固定模型参数，凸二次规划方法优化样本权重。\nDanelljan M, Hager G, Shahbaz Khan F, et al. Convolutional features for correlation filter based visual tracking [C]// ICCVW, 2015.\nDeepSRDCF是VOT2015的第二名，将SRDCF中的HOG特征替换为CNN中单层卷积层的深度特征(也就是卷积网络的激活值)，效果有了极大提升。这里用imagenet-vgg-2048 network，VGG网络的迁移能力比较强，而且MatConvNet就是VGG组的，MATLAB调用非常方便。论文还测试了不同卷积层在目标跟踪任务中的表现：\n第1层表现最好，第2和第5次之。由于卷积层数越高语义信息越多，但纹理细节越少，从1到4层越来越差的原因之一就是特征图的分辨率越来越低，但第5层反而很高，是因为包括完整的语义信息，判别力比较强(本来就是用来做识别的)。\n注意区分这里的深度特征和基于深度学习的方法，深度特征来自ImageNet上预训练的图像分类网络，没有fine-turn这一过程，不存在过拟合的问题。而基于深度学习的方法大多需要在跟踪序列上end-to-end训练或fine-turn，如果样本数量和多样性有限就很可能过拟合。\nMa C, Huang J B, Yang X, et al. Hierarchical convolutional features for visual tracking [C]// ICCV, 2015.\n值得一提的还有Chao Ma的HCF，结合多层卷积特征提升效果，用了VGG19的Conv5-4, Conv4-4和Conv3-4的激活值作为特征，所有特征都缩放到图像块分辨率，虽然按照论文应该是由粗到细确定目标，但代码中比较直接，三种卷积层的响应以固定权值1, 0.5, 0.02线性加权作为最终响应。虽然用了多层卷积特征，但没有关注边界效应而且线性加权的方式过于简单，HCF在VOT2016仅排在28名（单层卷积深度特征的DeepSRDCF是第13名）。\nDanelljan M, Robinson A, Khan F S, et al. Beyond correlation filters: Learning continuous convolution operators for visual tracking [C]// ECCV, 2016.\nC-COT是VOT2016的第一名，综合了SRDCF的空域正则化和SRDCFdecon的自适应样本权重，还将DeepSRDCF的单层卷积的深度特征扩展为多成卷积的深度特征（VGG第1和5层），为了应对不同卷积层分辨率不同的问题，提出了连续空间域插值转换操作，在训练之前通过频域隐式插值将特征图插值到连续空域，方便集成多分辨率特征图，并且保持定位的高精度。目标函数通过共轭梯度下降方法迭代优化，比高斯-塞德尔方法要快，自适应样本权值直接采用先验权值，没有交替凸优化过程，检测中用牛顿法迭代优化目标位置。\n注意以上SRDCF, SRDCFdecon，DeepSRDCF，C-COT都无法实时，这一系列工作虽然效果越来越好，但也越来越复杂，在相关滤波越来越慢失去速度优势的时候，Martin Danelljan在2017CVPR的ECO来了一脚急刹车，大神来告诉我们什么叫又好又快，不忘初心：\nDanelljan M, Bhat G, Khan F S, et al. ECO: Efficient Convolution Operators for Tracking [C]// CVPR, 2017.\nECO是C-COT的加速版，从模型大小、样本集大小和更新策略三个方便加速，速度比C-COT提升了20倍，加量还减价，EAO提升了13.3%，最最最厉害的是， hand-crafted features的ECO-HC有60FPS。。吹完了，来看看具体做法。\n第一减少模型参数，定义了factorized convolution operator(分解卷积操作)，效果类似PCA，用PCA初始化，然后仅在第一帧优化这个降维矩阵，以后帧都直接用，简单来说就是有监督降维，深度特征时模型参数减少了80%。\n第二减少样本数量， compact generative model(紧凑的样本集生成模型)，采用Gaussian Mixture Model (GMM)合并相似样本，建立更具代表性和多样性的样本集，需要保存和优化的样本集数量降到C-COT的1/8。\n第三改变更新策略，sparser updating scheme(稀疏更新策略)，每隔5帧做一次优化更新模型参数，不但提高了算法速度，而且提高了对突变，遮挡等情况的稳定性。但样本集是每帧都更新的，稀疏更新并不会错过间隔期的样本变化信息。\nECO的成功当然还有很多细节，而且有些我也看的不是很懂，总之很厉害就是了。。ECO实验跑了四个库(VOT2016, UAV123, OTB-2015, and TempleColor)都是第一，而且没有过拟合的问题，仅性能来说ECO是目前最好的相关滤波算法，也有可能是最好的目标跟踪算法。hand-crafted features版本的ECO-HC，降维部分原来HOG+CN的42维特征降到13维，其他部分类似，实验结果ECO-HC超过了大部分深度学习方法，而且论文给出速度是CPU上60FPS。\n最后是来自Luca Bertinetto的CFNet End-to-end representation learning for Correlation Filter based tracking，除了上面介绍的相关滤波结合深度特征，相关滤波也可以end-to-end方式在CNN中训练了：\nValmadre J, Bertinetto L, Henriques J F, et al. End-to-end representation learning for Correlation Filter based tracking [C]// CVPR, 2017.\n在SiamFC的基础上，将相关滤波也作为CNN中的一层，最重要的是cf层的前向传播和反向传播公式推导，两层卷积层的CFNet在GPU上是75FPS，综合表现并没有很多惊艳，可能是难以处理CF层的边界效应吧，持观望态度。\n▌第九部分：2017年CVPR和ICCV结果\n下面是CVPR 2017的目标跟踪算法结果：可能MD大神想说，一个能打的都没有！\n仿照上面的表格，整理了ICCV 2017的相关论文结果对比ECO：哎，还是一个能打的都没有！\n▌第十部分：大牛推荐\n凑个数，目前相关滤波方向贡献最多的是以下两个组(有创新有代码)：\n牛津大学：Joao F. Henriques和Luca Bertinetto，代表：CSK, KCF/DCF, Staple, CFNet (其他SiamFC, Learnet).\n林雪平大学：Martin Danelljan，代表：CN, DSST, SRDCF, DeepSRDCF, SRDCFdecon, C-COT, ECO.\n国内也有很多高校的优秀工作就不一一列举了。\n小编注：欢迎关注作者的知乎专栏：目标跟踪和深度学习\n原贴地址：\nhttps://www.zhihu.com/question/26493945/answer/156025576\n扫描二维码，关注「人工智能头条」\n回复“技术路线图”获取 AI 技术人才成长路线图"}
{"content2":"ICML\nInternational Conference on Machine Learning的缩写，即国际机器学习大会。ICML如今已发展为由国际机器学习学会（IMLS）主办的年度机器学习国际顶级会议。\nCVPR\nIEEE Conference on Computer Vision and Pattern Recognition的缩写，即IEEE国际计算机视觉与模式识别会议。该会议是由IEEE举办的计算机视觉和模式识别领域的顶级会议。\nNIPS（NeurIPS）\n全称神经信息处理系统大会(Conference and Workshop on Neural Information Processing Systems)，是一个关于机器学习和计算神经科学的国际会议。该会议固定在每年的12月举行,由NIPS基金会主办。NIPS是机器学习领域的顶级会议 [1] 。在中国计算机学会的国际学术会议排名中，NIPS为人工智能领域的A类会议 [2] 。\nICPR\nInternational Conference on Pattern Recognition）是模式识别国际重要会议"}
{"content2":"传统的职业人格测试，需要人工填写人格测试题，通过回答问题的结果判断一个人的人格特征，或者通过测评人员多次查看视频，主观判断一个人物的人格。视频识别技术日益成熟，能够智能识别人脸的特征值变化。如果能把视频识别和人物人格特征分析相结合，那么人物人格特征分析可以不用人工参与，由计算机即可完成人物人格特征的分析，极大的提高了职 业人格测试的便利性。为克服上述缺点，本发明的目的在于提供一种视频图像识别人格特征的方法，通过解码分析视频图像来识别人脸特征，进而获得相关的职业人格测试结果。\n为了达到以上目的，采用的技术方案是：根据视频图像识别人格特征的方法，根据视频中区域的变化量来获得人物的细微表情变化和肢体动作变化，包括以下步骤：\n基于人工智能技术改进的技术方案为：根据视频图像识别人格特征的方法，针对多个参考测定人进行人格评估测定得到多项人格特征因素评分值；采集参考测定人的人眼闭合频率、人脸移动轨迹、皱眉频率、手部动作轨迹，将上述数据导入人格预警模型内分析人物人格，得出人物人格特征报告，其中包括习惯性眨眼、人物是否有摇头的习惯、习惯性皱眉、录制视频时习惯性手部动作，并通过上述内容综合判断职业人员的人格情况。通过视频识别技术和人物人格特征分析技术相结合，通过采集人眼闭合度识别、眉心周色素量识别、人脸中心位置识别、手部中心位置识别、人物动作的变化量来智能识别人物人格特征。\n1)职业人员先进行视频沟通，结束后将该视频进行解码，将视频的内容解析成多张连续的图片；\n2)针对解析后的每张图片中的人眼闭合度、人脸中心位置、眉心周色素量和手部中心位置四个区域进行静态分析；\n3)将每张静态分析后的图片数据依次输入视频动态分析模块中，对每张图片的四个区域进行依次比对，进而获得四个区域的变化量，获得人眼闭合频率、人脸移动轨迹、皱眉频率、手部动作轨迹；\n4)根据以上数据导入人格预警模型内分析人物人格，得出人物人格特征报告，其中包括习惯性眨眼、人物是否有摇头的习惯、习惯性皱眉、录制视频时习惯性手部动作，并通过上述内容综合判断职业人员的人格情况。\n通过视频识别技术和人物人格特征分析技术相结合，通过采集人眼闭合度识别、眉心周色素量识别、人脸中心位置识别、手部中心位置识别、人物动作的变化量来智能识别人物人格特征。\n所述人格预警模型的建立包括：选择多个参考测定人员进行人格评估测定得到作为参考测定人的人格特征因素真实基准评分的多项人格特征因素评分值；采集多个所述参考测定人员正常视频沟通的视频片段，对所述视频片段进行预处理并提取四个区域特征，并统计其特征值；将每一个参考测定人的多项人格特征因素评分值以及视频片段输入人格预警模型内，并在人格预警模型形成不同人格特征的基础数据。\n参考测定人员进行人格评估测定具体是指进行大五人格测试、明尼苏达多重人格测试、卡特尔16人格测试中的一种。这样提高预警模型的基础数据的标准性，使其能够更好的预测人格特征。\n人眼闭合频率的计算如下：闭合频率＝C/T，C为总闭合次数，T 为总时间；其中要求视频帧率不小于1000ms/T×R，其中T为人眨眼的时间，R 为采样真实系数。\n皱眉频率＝C/T(次/分钟)，C为总皱眉次数，T为总时间。\n人脸移动轨迹长度＝从0到t总时间长度内，相邻两帧的移动长度之和，x取值范围从0到t，n的取值范围从0到t；x为时间坐标，n为纵向坐标。\n手部动作轨迹长度＝从0到t总时间长度内，相邻两帧的移动长度之和，x取值范围从0到t，n的取值范围从0到t；x为时间坐标，n为纵向坐标。\n一、建立人格预警模型。\n1.1)选择多个参考测定人员进行人格评估测定得到作为参考测定人的人格特征因素真实基准评分的多项人格特征因素评分值。本实施例中，针对选择的多个参考测定人进行人格评估测定具体是指进行大五人格测试(Big Five)，得出各个参考测定人神经质性(Neuroticism)、外向性(Extroversion)、开放性 (Openness)、随和性(Agreeableness)、尽责性(Conscientiousness)五项人格特征因素评分值。此外，针对多个参考测定人进行人格评估还可以采用明尼苏达多重人格测试或者卡特尔16人格测试等，其结果同样也可以得到多项人格特征因素评分值，人格特征因素评分值的项数会由于具体的人格评估测定方法不同而有所不同。\n1.2)采集多个所述参考测定人员正常视频沟通的视频片段，对所述视频片段进行预处理并提取人眼闭合度、人脸中心位置、眉心周色素量和手部中心位置四个区域特征，并统计其特征值。采集的人脸部位如所示，包括眉心区域1、人眼区域2、人鼻区域3。本实施例中，共选择个参考测定人，预先对每个参考测定人进行人格确认，预先将其归纳在相应的五大人格类型中。每个参考测定人进行一段20分钟的视频对话，提取每段视频片段中相关人员的细微表情变化和肢体动作变化，并将其输入人格预警模型内形成不同人格特征的基础数据。\n如所示的人眼闭合度变化曲线，主要包括时间坐标轴(T)、纵向坐标轴(X)、人眼闭合度曲线。该图的数据来源于视频的智能识别，识别的结果中包括人员闭合度大小，随之时间推移，模拟成二维坐标图。由该坐标图，可以计算出人眼闭合频率，达到预测人物人格特征的目的。人眼闭合频率的计算如下：\n人眨眼的平均时间为100ms，为了保证动态曲线更接近真实，所以要求视频帧率不小于1000ms/T×R，其中T为人眨眼的时间，R为采样真实系数。闭合频率＝C/T(次/分钟)，C为总闭合次数，T为总时间。\n如所示的眉心周色素变化曲线，主要包括时间坐标轴(T)、纵向坐标轴(X)、眉心周色素变化曲线。该图的数据来源于视频的智能识别，识别的结果中包括人物眉心像素大小，随之时间推移，模拟成二维坐标图。由该坐标图，可以计算出人物皱眉频率，达到预测人物人格特征的目的。皱眉频率的计算如下：\n皱眉频率＝C/T(次/分钟)，C为总皱眉次数，T为总时间。\n如所示的人脸中心位置变化曲线，主要包括时间坐标轴(y)、横向坐标轴(x)，纵向坐标轴(T)、人脸中心位置变化曲线。该图的数据来源于视频的智能识别，识别的结果中包括人脸中心位置，随之时间推移，模拟成三维坐标图。由该坐标图，可以计算出人物头部移动轨迹，达到预测人物人格特征的目的。人物头部移动轨迹的计算如下：\n人物头部移动轨迹长度＝从0到t总时间长度内，相邻两帧的移动长度之和， x取值范围从0到t，n的取值范围从0到t。\n如所示的手部中心位置变化曲线，主要包括时间坐标轴(y)、横向坐标轴(x)，纵向坐标轴(T)、手部中心位置变化曲线。该图的数据来源于视频的智能识别，识别的结果中包括手部中心位置，随之时间推移，模拟成三维坐标图。由该坐标图，可以计算出手部中心位置移动轨迹，达到预测人物人格特征的目的。手部中心位置移动轨迹的计算如下：\n手部中心位置移动轨迹长度＝从0到t总时间长度内，相邻两帧的移动长度之和，x取值范围从0到t，n的取值范围从0到t。\n为了根据视频图像识别出人格特征，需要经过如下步骤：\n1)先进行视频沟通，结束后将该视频进行解码，将视频的内容解析成多张连续的图片；\n2)针对解析后的每张图片中的人眼闭合度、人脸中心位置、眉心周色素量和手部中心位置四个区域进行静态分析；\n3)将每张静态分析后的图片数据依次输入视频动态分析模块中，对每张图片的四个区域进行依次比对，进而获得四个区域的变化量，获得人眼闭合频率、人脸移动轨迹、皱眉频率、手部动作轨迹；\n4)根据以上数据导入人格预警模型内分析人物人格，得出人物人格特征报告，其中包括习惯性眨眼、人物是否有摇头的习惯、习惯性皱眉、录制视频时习惯性手部动作，并通过上述内容综合判断职业人员的人格情况。\n本发明通过视频识别技术和人物人格特征分析技术相结合，通过采集人眼闭合度识别、眉心周色素量识别、人脸中心位置识别、手部中心位置识别、人物动作的变化量来智能识别人物人格特征。"}
{"content2":"微软Custom Vision提供了成熟开源的计算机视觉开发框架，你只需要上传十张训练图片，即可一键训练图像分类模型（比如识别不同的水果、花卉、地标、人脸）。不需要具备任何深度学习算法知识，小学生都能快速上手。Custom Vision提供了API接口，你还可以将模型部署在网站、手机移动端、微信小程序中。\n作者：张子豪（同济大学微软学生俱乐部）\n微信公众号：人工智能小技巧\n本文配套B站视频：用微软Custom Version识别水果—不用写代码，三分钟做一个人工智能小应用\n发布于2018-11-8\n文章目录\n美剧《硅谷》中的热狗识别app\n微软开源机器视觉开发平台Custom Vision\n第一步：新建模型项目\n第二步：上传训练图片并打标签\n第三步：训练模型\n点击右上角绿色的\"Train\"按钮\n模型评估参数\n第四步：测试模型\n第五步：扩展开发—使用API编写Python脚本程序\n微软开源人工智能工具和深度学习框架\n同济大学微软学生俱乐部\n参考文献与扩展阅读\n美剧《硅谷》中的热狗识别app\n在美剧《硅谷》中，程序员Jian-Yang开发了一款识别图片中物体是不是热狗的app，虽然听名字就知道，功能十分鸡肋弱智，但由于搭上了人工智能和虚拟现实的快车，这个app迅速获得了硅谷风投公司的青睐并大捞一笔。这部剧深刻讽刺了人工智能浪潮下的经济泡沫以及硅谷投资人的盲目冲动。难怪十九大报告中提出要将”人工智能与实体经济深度融合“。\n其实，你也可以用不到三分钟时间轻松开发一款类似的应用，也许下一个硅谷弄潮儿就是你！\n微软开源机器视觉开发平台Custom Vision\n微软Custom Vision提供了成熟开源的计算机视觉开发框架，你只需要上传十张训练图片，即可一键生成图像分类app。你不需要具备任何深度学习、图像处理的算法知识，小学生都能快速上手。Custom Vision提供了API接口，你还可以将模型部署在网站、手机移动端、微信小程序中。\n本文配套B站视频：用微软Custom Version识别水果—不用写代码，三分钟做一个人工智能小应用\n关注微信公众号 人工智能小技巧 回复 苹果 即可看到这个视频。\n第一步：新建模型项目\nCustom Vision官网\n点击进入后免费注册微软账号，即可新建模型项目。\n第二步：上传训练图片并打标签\n注意事项：\n1、不能只上传一个标签的图片，否则模型无法通过交叉验证的方式对照学习。也就是说，不能只上传苹果的图片，而是至少上传苹果和香蕉两种水果的图片并分别打标签。\n2、上传的训练图片要包含对象整体，而非局部。\n3、上传不同背景、角度、大小的照片。\n4、每个标签上传十几张图片就够了。\n第三步：训练模型\n点击右上角绿色的\"Train\"按钮\n等待几秒钟之后，模型就训练完成了。\n模型评估参数\n窗口中显示的Precision和Recall是用于评价我们训练的分类模型分类效果的两个参数。\nPrecision：被预测为苹果的结果中有多少真实就是苹果。\nRecall：真实为苹果的样本中有多少被预测正确了。\n简单来说，Precision就是宁可放过不可杀错，Recall就是宁可杀错不可放过。\n在机器学习领域，通常使用F-measure参数将这两个参数综合起来。\nPrecision和Recall随着分类阈值的变化而此消彼长（举例说明），使用Precision-recall曲线，来显示出分类器在Precision与Recall之间的权衡。\n打个比方，如果有个人号称是地震预测的专家，如果他每天都说第二天不会发生大地震，那么他有相当大的概率能够预测成功，也就是说Precision很大。但我们不能说这就是一个好的模型，因为当地震真来临的时候他能够预测成功的概率是0，也就是Recall很低。综合起来的F-measure也很低，所以这是一个失败的分类器。\n再打个比方，医疗诊断用的试剂有假阳性和假阴性，假阳性指的是一个正常人被测出有癌症，假阴性指的是一个癌症病人测出来没有癌症，这和Precision和Recall的道理也是一样的。\n第四步：测试模型\n点击右上角的Quick Test，进入测试界面，既可以上传图片文件，也可以上传图片的URL地址，模型就能正确识别出图片属于哪一类标签。\n第五步：扩展开发—使用API编写Python脚本程序\nCustom Vision提供了API接口，你可以将训练模型部署在网站、手机移动端、微信小程序中，从而开发自己的用户界面，并大批量识别图片。\n在Performance栏中，选择Prediction URL，打开API界面。\n利用Python的requests库，构造post请求，Python脚本代码如下：\n将body栏里的Url链接更换成你要识别的图片链接。\n# 同济大学张子豪于2018年11月2日编写 # 微信公众号：人工智能小技巧 import requests def getHTMLText(url): try: headers = \\ { \"Prediction-Key\": \"7e1469b8051b45329814ef7f2275a3ff\", \"Content-Type\": \"application/json\", } body = \\ { \"Url\": \"https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=684871772,3282663100&fm=27&gp=0.jpg\", } # Set Prediction-Key Header to : 7e1469b8051b45329814ef7f2275a3ff # Set Content-Type Header to : application/json # Set Body to : {\"Url\": \"https://example.com/image.png\"} r = requests.post(url,timeout=30,headers=headers,json=body) r.raise_for_status() #如果状态不是200，引发HTTPError异常 r.encoding = r.apparent_encoding return r.text except: return '产生异常' if __name__ == \"__main__\": url = \"https://southcentralus.api.cognitive.microsoft.com/customvision/v2.0/Prediction/7c9b4755-271f-4d32-82c5-3c93ba34df8b/url?iterationId=c4927c90-b28a-4c22-9957-3e0b4f2ce99e\" print(getHTMLText(url))\n例如，用下列测试图片做测试，运行Python脚本，结果如下：是香蕉的概率为100%。通过这个程序，你可以将这个图像分类模型部署在自己的云服务器上，搭建自己的网站、手机APP、微信小程序，向用户提供图像分类服务。\n微软开源人工智能工具和深度学习框架\n微软开源人工智能工具和深度学习框架介绍\n关注微信公众号 人工智能小技巧 回复 微软 即可看到这篇文章。\n本文介绍了微软在人工智能领域的领先成果、产品线，开源人工智能框架和工具。读者可以运用这些工具快速开发机器视觉、语音处理、视频检索等丰富的人工智能应用。\n同济大学微软学生俱乐部\n参考文献与扩展阅读\nCustom Vision\n【YOLO学习】召回率（Recall），精确率（Precision），平均正确率（Average_precision(AP) ），交除并（Intersection-over-Union（IoU））\n用Microsoft Custom Vision技术识别点东西吧\n学堂在线慕课：微软人工智能-深度学习框架和工具\n用Microsoft Custom Vision技术识别点东西吧\n科普文：大白话讲解卷积神经网络工作原理\nB站视频：不用写代码，三分钟做一个人工智能小应用\n视频：三分钟走进卷积神经网络\n视频：大白话讲解卷积神经网络工作原理\n微软亚洲研究院\n微软亚洲研究院20年20人\n作者介绍：\n张子豪，同济大学在读研究生。微信公众号人工智能小技巧运营者。致力于用人类能听懂的语言向大众科普人工智能前沿科技。目前正在制作《说人话的人工智能视频教程》、《零基础入门树莓派趣味编程》等视频教程。西南地区人工智能爱好者高校联盟联合创始人，重庆大学人工智能协会联合创始人。充满好奇的终身学习者、崇尚自由的开源社区贡献者、乐于向零基础分享经验的引路人、口才还不错的程序员。\n说人话的零基础深度学习、数据科学视频教程、树莓派趣味开发视频教程等你来看！\n微信公众号：人工智能小技巧\nGithub代码仓库:TommyZihao\n个人主页：www.python666.org\n同济大学开源软件协会\n同济大学微软学生俱乐部\n西南人工智能爱好者联盟\n重庆大学人工智能协会"}
{"content2":"今日CS.CV计算机视觉论文速览\nFri, 22 Feb 2019\nTotally 20 papers\nDaily Computer Vision Papers\n[1] **Title: Deep CNN-based Speech Balloon Detection and Segmentation for Comic Books\nAuthors:David Dubray, Jochen Laubrock\n[2] Title: Boundary-weighted Domain Adaptive Neural Network for Prostate MR Image Segmentation\nAuthors:Qikui Zhu, Bo Du, Pingkun Yan\n[3] *Title: Cross-Sensor Periocular Biometrics: A Comparative Benchmark including Smartphone Authentication\nAuthors:Fernando Alonso-Fernandez, Kiran B. Raja, R. Raghavendra, Cristoph Busch, Josef Bigun, Ruben Vera-Rodriguez, Julian Fierrez\n[4] **Title: GSLAM: A General SLAM Framework and Benchmark\nAuthors:Yong Zhao, Shibiao Xu, Shuhui Bu, Hongkai Jiang, Pengcheng Han\n[5] *Title: A Parallel Optical Image Security System with Cascaded Phase-only Masks\nAuthors:Shuming Jiao, Yang Gao, Ting Lei, Zhenwei Xie, Xiaocong Yuan\n[6] **Title: A Joint Deep Learning Approach for Automated Liver and Tumor Segmentation\nAuthors:Nadja Gruber, Stephan Antholzer, Werner Jaschke, Christian Kremser, Markus Haltmeier\n[7] Title: Deep Discriminative Representation Learning with Attention Map for Scene Classification\nAuthors:Jun Li, Daoyu Lin, Yang Wang, Guangluan Xu, Chibiao Ding\n[8] Title: ComplexFace: a Multi-Representation Approach for Image Classification with Small Dataset\nAuthors:Guiying Zhang, Yuxin Cui, Yong Zhao, Jianjun Hu\n[9] **Title: Long-Bone Fracture Detection using Artificial Neural Networks based on Contour Features of X-ray Images\nAuthors:Alice Yi Yang, Ling Cheng\n[10] **Title: Towards Real-time Eyeblink Detection in The Wild:Dataset,Theory and Practices\nAuthors:Guilei Hu, Yang Xiao, Zhiguo Cao, Lubin Meng, Zhiwen Fang, Joey Tianyi Zhou\n[11] *Title: Evaluation of Algorithms for Multi-Modality Whole Heart Segmentation: An Open-Access Grand Challenge\nAuthors:Xiahai Zhuang, Lei Li, Christian Payer, Darko Stern, Martin Urschler, Mattias P. Heinrich, Julien Oster, Chunliang Wang, Orjan Smedby, Cheng Bian, Xin Yang, Pheng-Ann Heng, Aliasghar Mortazi, Ulas Bagci, Guanyu Yang, Chenchen Sun, Gaetan Galisot, Jean-Yves Ramel, Thierry Brouard, Qianqian Tong, Weixin Si, Xiangyun Liao, Guodong Zeng, Zenglin Shi, Guoyan Zheng, Chengjia Wang, Tom MacGillivray, David Newby, Kawal Rhode, Sebastien Ourselin, Raad Mohiaddin, Jennifer Keegan, David Firmin, Guang Yang\n[12] Title: Atrial Scar Quantification via Multi-scale CNN in the Graph-cuts Framework\nAuthors:Lei Li, Fuping Wu, Guang Yang, Lingchao Xu, Tom Wong, Raad Mohiaddin, David Firmin, Jennifer Keegan, Xiahai Zhuang\n[13] Title: Improvement Multi-Stage Model for Human Pose Estimation\nAuthors:Zhihui Su, Ming Ye, Guohui Zhang, Lei Dai, Jianda Sheng\n[14] Title: Class-independent sequential full image segmentation, using a convolutional net that finds a segment within an attention region, given a pointer pixel within this segment\nAuthors:Sagi Eppel\n[15] Title: Perceptual Quality-preserving Black-Box Attack against Deep Learning Image Classifiers\nAuthors:Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva\n[16] *Title: Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy\nAuthors:Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D. Hager, Austin Reiter, Russell H. Taylor, Mathias Unberath\n[17] Title: Adversarial Augmentation for Enhancing Classification of Mammography Images\nAuthors:Lukas Jendele, Ondrej Skopek, Anton S. Becker, Ender Konukoglu\n[18] Title: Domain Partitioning Network\nAuthors:Botos Csaba, Adnane Boukhayma, Viveka Kulharia, András Horváth, Philip H. S. Torr\n[19] *Title: Cloud-Based Autonomous Indoor Navigation: A Case Study\nAuthors:Uthman Baroudi, M. Alharbi, K. Alhouty, H. Baafeef, K. Alofi\n[20] Title: Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering\nAuthors:Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, Devi Parikh\nPapers from arxiv.org\n更多精彩请移步主页\nInteresting:\n📚\npic from pixels.com"}
{"content2":"首先来一个跟踪算法的大杂烩：\nVOT2016 Trackers repository\n以下是转载内容\n----------------------------------------------------\n作者：YaqiLYU\n链接：https://www.zhihu.com/question/26493945/answer/156025576\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n相信很多来这里的人和我第一次到这里一样，都是想找一种比较好的目标跟踪算法，或者想对目标跟踪这个领域有比较深入的了解，虽然这个问题是经典目标跟踪算法，但事实上，可能我们并不需要那些曾经辉煌但已被拍在沙滩上的tracker(目标跟踪算法)，而是那些即将成为经典的，或者就目前来说最好用、速度和性能都看的过去tracker。我比较关注目标跟踪中的相关滤波方向，接下来我帮您介绍下我所认识的目标跟踪，尤其是相关滤波类方法，分享一些我认为比较好的算法，顺便谈谈我的看法。\n1.图片来自某些slides和paper，如有侵权请提醒删除。\n2.以下内容主要是论文的简单总结，代码分析和个人看法，不涉及任何公司内部资料。\n3.转载请注明出处，谢谢。\n4.如有错误欢迎指出，非常感谢。有问题可以私信我，也可以在评论区提出，时间有限但也都会尽量回复，同时感谢各位道友帮忙解答。\n第一部分：目标跟踪速览\n先跟几个SOTA的tracker混个脸熟，大概了解一下目标跟踪这个方向都有些什么。一切要从2013年的那个数据库说起。。如果你问别人近几年有什么比较niubility的跟踪算法，大部分人都会扔给你吴毅老师的论文，OTB50和OTB100(OTB50这里指OTB-2013，OTB100这里指OTB-2015，50和100分别代表视频数量，方便记忆)：\nWu Y, Lim J, Yang M H. Online object tracking: A benchmark [C]// CVPR, 2013.\nWu Y, Lim J, Yang M H. Object tracking benchmark [J]. TPAMI, 2015.\n顶会转顶刊的顶级待遇，在加上引用量1480+320多，影响力不言而喻，已经是做tracking必须跑的数据库了，测试代码和序列都可以下载： Visual Tracker Benchmark，OTB50包括50个序列，都经过人工标注：\n两篇论文在数据库上对比了包括2012年及之前的29个顶尖的tracker，有大家比较熟悉的OAB, IVT, MIL, CT, TLD, Struck等，大都是顶会转顶刊的神作，由于之前没有比较公认的数据库，论文都是自卖自夸，大家也不知道到底哪个好用，所以这个database的意义非常重大，直接促进了跟踪算法的发展，后来又扩展为OTB100发到TPAMI，有100个序列，难度更大更加权威，我们这里参考OTB100的结果，首先是29个tracker的速度和发表时间(标出了一些性能速度都比较好的算法)：\n接下来再看结果(更加详细的情况建议您去看论文比较清晰)：\n直接上结论：平均来看Struck, SCM, ASLA的性能比较高，排在前三不多提，着重强调CSK，第一次向世人展示了相关滤波的潜力，排第四还362FPS简直逆天了。速度排第二的是经典算法CT(64fps)(与SCM, ASLA等都是那个年代最热的稀疏表示)。如果对更早期的算法感兴趣，推荐另一篇经典的survey(反正我是没兴趣也没看过):\nYilmaz A, Javed O, Shah M. Object tracking: A survey [J]. CSUR, 2006.\n2012年以前的算法基本就是这样，自从2012年AlexNet问世以后，CV各个领域都有了巨大变化，所以我猜你肯定还想知道2013到2017年发生了什么，抱歉我也不知道(容我卖个关子)，不过我们可以肯定的是，2013年以后的论文一定都会引用OTB50这篇论文，借助谷歌学术中的被引用次数功能，得到如下结果：\n这里仅列举几个引用量靠前的，依次是Struck转TPAMI, 三大相关滤波方法KCF, CN, DSST, 和VOT竞赛，这里仅作示范，有兴趣可以亲自去试试。(这么做的理论依据是：一篇论文，在它之前的工作可以看它的引用文献，之后的工作可以看谁引用了它；虽然引用量并不能说明什么，但好的方法大家基本都会引用的(表示尊重和认可)；之后还可以通过限定时间来查看某段时间的相关论文，如2016-2017就能找到最新的论文了，至于论文质量需要仔细甄别；其他方向的重要论文也可以这么用，顺藤摸瓜，然后你就知道大牛是哪几位，接着关注跟踪一下他们的工作 ) 这样我们就大致知道目标跟踪领域的最新进展应该就是相关滤波无疑了，再往后还能看到相关滤波类算法有SAMF, LCT, HCF, SRDCF等等。当然，引用量也与时间有关，建议分每年来看。此外，最新版本OPENCV3.2除了TLD，也包括了几个很新的跟踪算法 OpenCV: Tracking API：\nTrackerKCF接口实现了KCF和CN，影响力可见一斑，还有个GOTURN是基于深度学习的方法，速度虽快但精度略差，值得去看看。tracking方向的最新论文，可以跟进三大会议(CVPR/ICCV/ECCV) 和arXiv。\n第二部分：背景介绍\n接下来总体介绍下目标跟踪。这里说的目标跟踪，是通用单目标跟踪，第一帧给个矩形框，这个框在数据库里面是人工标注的，在实际情况下大多是检测算法的结果，然后需要跟踪算法在后续帧紧跟住这个框，以下是VOT对跟踪算法的要求：\n通常目标跟踪面临几大难点(吴毅在VALSE的slides)：外观变形，光照变化，快速运动和运动模糊，背景相似干扰：\n平面外旋转，平面内旋转，尺度变化，遮挡和出视野等情况：\n正因为这些情况才让tracking变得很难，目前比较常用的数据库除了OTB，还有前面找到的VOT竞赛数据库(类比ImageNet)，已经举办了四年，VOT2015和VOT2016都包括60个序列，所有序列也是免费下载 VOT Challenge | Challenges：\nKristan M, Pflugfelder R, Leonardis A, et al. The visual object tracking vot2013 challenge results [C]// ICCV, 2013.\nKristan M, Pflugfelder R, Leonardis A, et al. The Visual Object Tracking VOT2014 Challenge Results [C]// ECCV, 2014.\nKristan M, Matas J, Leonardis A, et al. The visual object tracking vot2015 challenge results [C]// ICCV, 2015.\nKristan M, Ales L, Jiri M, et al. The Visual Object Tracking VOT2016 Challenge Results [C]// ECCV, 2016.\nOTB和VOT区别：OTB包括25%的灰度序列，但VOT都是彩色序列，这也是造成很多颜色特征算法性能差异的原因；两个库的评价指标不一样，具体请参考论文；VOT库的序列分辨率普遍较高，这一点后面分析会提到。对于一个tracker，如果论文在两个库(最好是OTB100和VOT2016)上都结果上佳，那肯定是非常优秀的(两个库调参你能调好，我服，认了~~)，如果只跑了一个，个人更偏向于VOT2016，因为序列都是精细标注，且评价指标更好(人家毕竟是竞赛，评价指标发过TPAMI的)，差别最大的地方，OTB有随机帧开始，或矩形框加随机干扰初始化去跑，作者说这样更加符合检测算法给的框框；而VOT是第一帧初始化去跑，每次跟踪失败(预测框和标注框不重叠)时，5帧之后重新初始化，VOT以short-term为主，且认为跟踪检测应该在一起不分离，detecter会多次初始化tracker。\n补充：OTB在2013年公开了，对于2013以后的算法是透明的，论文都会去调参，尤其是那些只跑OTB的论文，如果关键参数直接给出还精确到小数点后两位，建议您先实测(人心不古啊~被坑的多了)。VOT竞赛的数据库是每年更新，还动不动就重新标注，动不动就改变评价指标，对当年算法是难度比较大，所以结果相对更可靠。（相信很多人和我一样，看每篇论文都会觉得这个工作太好太重要了，如果没有这篇论文，必定地球爆炸，宇宙重启~~所以就像大家都通过历年ILSVRC竞赛结果为主线了解深度学习的发展一样，第三方的结果更具说服力，所以我也以竞赛排名+是否公开源码+实测性能为标准，优选几个算法分析）\n目标视觉跟踪(Visual Object Tracking)，大家比较公认分为两大类：生成(generative)模型方法和判别(discriminative)模型方法，目前比较流行的是判别类方法，也叫检测跟踪tracking-by-detection，为保持回答的完整性，以下简单介绍。\n生成类方法，在当前帧对目标区域建模，下一帧寻找与模型最相似的区域就是预测位置，比较著名的有卡尔曼滤波，粒子滤波，mean-shift等。举个例子，从当前帧知道了目标区域80%是红色，20%是绿色，然后在下一帧，搜索算法就像无头苍蝇，到处去找最符合这个颜色比例的区域，推荐算法ASMS vojirt/asms：\nVojir T, Noskova J, Matas J. Robust scale-adaptive mean-shift for tracking [J]. Pattern Recognition Letters, 2014.\nASMS与DAT并称“颜色双雄”(版权所有翻版必究)，都是仅颜色特征的算法而且速度很快，依次是VOT2015的第20名和14名，在VOT2016分别是32名和31名(中等水平)。ASMS是VOT2015官方推荐的实时算法，平均帧率125FPS，在经典mean-shift框架下加入了尺度估计，经典颜色直方图特征，加入了两个先验(尺度不剧变+可能偏最大)作为正则项，和反向尺度一致性检查。作者给了C++代码，在相关滤波和深度学习盛行的年代，还能看到mean-shift打榜还有如此高的性价比实在不容易(已泪目~~)，实测性能还不错，如果您对生成类方法情有独钟，这个非常推荐您去试试。(某些算法，如果连这个你都比不过。。天台在24楼，不谢)\n判别类方法，OTB50里面的大部分方法都是这一类，CV中的经典套路图像特征+机器学习， 当前帧以目标区域为正样本，背景区域为负样本，机器学习方法训练分类器，下一帧用训练好的分类器找最优区域：\n与生成类方法最大的区别是，分类器采用机器学习，训练中用到了背景信息，这样分类器就能专注区分前景和背景，所以判别类方法普遍都比生成类好。举个例子，在训练时告诉tracker目标80%是红色，20%是绿色，还告诉它背景中有橘红色，要格外注意别搞错了，这样的分类器知道更多信息，效果也相对更好。tracking-by-detection和检测算法非常相似，如经典行人检测用HOG+SVM，Struck用到了haar+structured output SVM，跟踪中为了尺度自适应也需要多尺度遍历搜索，区别仅在于跟踪算法对特征和在线机器学习的速度要求更高，检测范围和尺度更小而已。这点其实并不意外，大多数情况检测识别算法复杂度比较高不可能每帧都做，这时候用复杂度更低的跟踪算法就很合适了，只需要在跟踪失败(drift)或一定间隔以后再次检测去初始化tracker就可以了。其实我就想说，FPS才TMD是最重要的指标，慢的要死的算法可以去死了(同学别这么偏激，速度是可以优化的)。经典判别类方法推荐Struck和TLD，都能实时性能还行，Struck是2012年之前最好的方法，TLD是经典long-term的代表，思想非常值得借鉴：\nHare S, Golodetz S, Saffari A, et al. Struck: Structured output tracking with kernels [J]. IEEE TPAMI, 2016.\nKalal Z, Mikolajczyk K, Matas J. Tracking-learning-detection [J]. IEEE TPAMI, 2012.\n长江后浪推前浪，前面的已被排在沙滩上，这个后浪就是相关滤波和深度学习。相关滤波类方法correlation filter简称CF，也叫做discriminative correlation filter简称DCF，注意和后面的DCF算法区别，包括前面提到的那几个，也是后面要着重介绍的。深度学习（Deep ConvNet based）类方法，因为深度学习类目前不适合落地就不瞎推荐了，可以参考Winsty的几篇 Naiyan Wang - Home，还有VOT2015的冠军MDNet Learning Multi-Domain Convolutional Neural Networks for Visual Tracking，以及VOT2016的冠军TCNN http://www.votchallenge.net/vot2016/download/44_TCNN.zip，速度方面比较突出的如80FPS的SiamFC SiameseFC tracker和100FPS的GOTURN davheld/GOTURN，注意都是在GPU上。基于ResNet的SiamFC-R(ResNet)在VOT2016表现不错，很看好后续发展，有兴趣也可以去VALSE听作者自己讲解 VALSE-20160930-LucaBertinetto-Oxford-JackValmadre-Oxford-pu，至于GOTURN，效果比较差，但优势是跑的很快100FPS，如果以后效果也能上来就好了。做科研的同学深度学习类是关键，能兼顾速度就更好了。\nNam H, Han B. Learning multi-domain convolutional neural networks for visual tracking [C]// CVPR, 2016.\nNam H, Baek M, Han B. Modeling and propagating cnns in a tree structure for visual tracking. arXiv preprint arXiv:1608.07242, 2016.\nBertinetto L, Valmadre J, Henriques J F, et al. Fully-convolutional siamese networks for object tracking [C]// ECCV, 2016.\nHeld D, Thrun S, Savarese S. Learning to track at 100 fps with deep regression networks [C]// ECCV, 2016.\n最后，深度学习END2END的强大威力在目标跟踪方向还远没有发挥出来，还没有和相关滤波类方法拉开多大差距(速度慢是天生的我不怪你，但效果总该很好吧，不然你存在的意义是什么呢。。革命尚未成功，同志仍须努力)。另一个需要注意的问题是目标跟踪的数据库都没有严格的训练集和测试集，需要离线训练的深度学习方法就要非常注意它的训练集有没有相似序列，而且一直到VOT2017官方才指明要限制训练集，不能用相似序列训练模型。\n最后强力推荐两个资源。王强\n@Qiang Wang\n维护的benchmark_results foolwood/benchmark_results：大量顶级方法在OTB库上的性能对比，各种论文代码应有尽有，大神自己C++实现并开源的CSK, KCF和DAT，还有他自己的DCFNet论文加源码，找不着路的同学请跟紧。\n@H Hakase\n维护的相关滤波类资源 HakaseH/CF_benchmark_results ，详细分类和论文代码资源，走过路过别错过，相关滤波类算法非常全面，非常之用心！\n(以上两位，看到了请来我处交一下广告费，9折优惠~~)\n第三部分：相关滤波\n介绍最经典的高速相关滤波类跟踪算法CSK, KCF/DCF, CN。很多人最早了解CF，应该和我一样，都是被下面这张图吸引了：\n这是KCF/DCF算法在OTB50上(2014年4月就挂arVix了, 那时候OTB100还没有发表)的实验结果，Precision和FPS碾压了OTB50上最好的Struck，看惯了勉强实时的Struck和TLD，飙到高速的KCF/DCF突然有点让人不敢相信，其实KCF/DCF就是在OTB上大放异彩的CSK的多通道特征改进版本。注意到那个超高速615FPS的MOSSE(严重超速这是您的罚单)，这是目标跟踪领域的第一篇相关滤波类方法，这其实是真正第一次显示了相关滤波的潜力。和KCF同一时期的还有个CN，在2014'CVPR上引起剧烈反响的颜色特征方法，其实也是CSK的多通道颜色特征改进算法。从MOSSE(615)到 CSK(362) 再到 KCF(172FPS), DCF(292FPS), CN(152FPS), CN2(202FPS)，速度虽然是越来越慢，但效果越来越好，而且始终保持在高速水平：\nBolme D S, Beveridge J R, Draper B A, et al. Visual object tracking using adaptive correlation filters [C]// CVPR, 2010.\nHenriques J F, Caseiro R, Martins P, et al. Exploiting the circulant structure of tracking-by- detection with kernels [C]// ECCV, 2012.\nHenriques J F, Rui C, Martins P, et al. High-Speed Tracking with Kernelized Correlation Filters [J]. IEEE TPAMI, 2015.\nDanelljan M, Shahbaz Khan F, Felsberg M, et al. Adaptive color attributes for real-time visual tracking [C]// CVPR, 2014.\nCSK和KCF都是Henriques J F(牛津大学)João F. Henriques 大神先后两篇论文，影响后来很多工作，核心部分的岭回归，循环移位的近似密集采样，还给出了整个相关滤波算法的详细推导。还有岭回归加kernel-trick的封闭解，多通道HOG特征。\nMartin Danelljan大牛(林雪平大学)用多通道颜色特征Color Names(CN)去扩展CSK得到了不错的效果，算法也简称CN Coloring Visual Tracking 。\nMOSSE是单通道灰度特征的相关滤波，CSK在MOSSE的基础上扩展了密集采样(加padding)和kernel-trick，KCF在CSK的基础上扩展了多通道梯度的HOG特征，CN在CSK的基础上扩展了多通道颜色的Color Names。HOG是梯度特征，而CN是颜色特征，两者可以互补，所以HOG+CN在近两年的跟踪算法中成为了hand-craft特征标配。最后，根据KCF/DCF的实验结果，讨论两个问题：\n1. 为什么只用单通道灰度特征的KCF和用了多通道HOG特征的KCF速度差异很小？\n第一，作者用了HOG的快速算法fHOG，来自Piotr's Computer Vision Matlab Toolbox，C代码而且做了SSE优化。如对fHOG有疑问，请参考论文Object Detection with Discriminatively Trained Part Based Models第12页。\n第二，HOG特征常用cell size是4，这就意味着，100*100的图像，HOG特征图的维度只有25*25，而Raw pixels是灰度图归一化，维度依然是100*100，我们简单算一下：27通道HOG特征的复杂度是27*625*log(625)=47180，单通道灰度特征的复杂度是10000*log(10000)=40000，理论上也差不多，符合表格。\n看代码会发现，作者在扩展后目标区域面积较大时，会先对提取到的图像块做因子2的下采样到50*50，这样复杂度就变成了2500*log(2500)=8495，下降了非常多。那你可能会想，如果下采样再多一点，复杂度就更低了，但这是以牺牲跟踪精度为代价的，再举个例子，如果图像块面积为200*200，先下采样到100*100，再提取HOG特征，分辨率降到了25*25，这就意味着响应图的分辨率也是25*25，也就是说，响应图每位移1个像素，原始图像中跟踪框要移动8个像素，这样就降低了跟踪精度。在精度要求不高时，完全可以稍微牺牲下精度提高帧率(但看起来真的不能再下采样了)。\n2. HOG特征的KCF和DCF哪个更好？\n大部分人都会认为KCF效果超过DCF，而且各属性的准确度都在DCF之上，然而，如果换个角度来看，以DCF为基准，再来看加了kernel-trick的KCF，mean precision仅提高了0.4%，而FPS下降了41%，这么看是不是挺惊讶的呢？除了图像块像素总数，KCF的复杂度还主要和kernel-trick相关。所以，下文中的CF方法如果没有kernel-trick，就简称基于DCF，如果加了kernel-trick，就简称基于KCF(剧透基本各占一半)。当然这里的CN也有kernel-trick，但请注意，这是Martin Danelljan大神第一次使用kernel-trick，也是最后一次。。。\n这就会引发一个疑问，kernel-trick这么强大的东西，怎么才提高这么点？这里就不得不提到Winsty的另一篇大作：\nWang N, Shi J, Yeung D Y, et al. Understanding and diagnosing visual tracking systems[C]// ICCV, 2015.\n一句话总结，别看那些五花八门的机器学习方法，那都是虚的，目标跟踪算法中特征才是最重要的（就是因为这篇文章我粉了WIN叔哈哈），以上就是最经典的三个高速算法，CSK, KCF/DCF和CN，推荐。\n第四部分：14年的尺度自适应\nVOT与OTB一样最早都是2013年出现的，但VOT2013序列太少，第一名的PLT代码也找不到，没有参考价值就直接跳过了。直接到了VOT2014竞赛 VOT2014 Benchmark 。这一年有25个精挑细选的序列，38个算法，那时候深度学习的战火还没有烧到tracking，所以主角也只能是刚刚展露头角就独霸一方的CF，下面是前几名的详细情况：\n前三名都是相关滤波CF类方法，第三名的KCF已经很熟悉了，这里稍微有点区别就是加了多尺度检测和子像素峰值估计，再加上VOT序列的分辨率比较高(检测更新图像块的分辨率比较高)，导致竞赛中的KCF的速度只有24.23(EFO换算66.6FPS)。这里speed是EFO(Equivalent Filter Operations)，在VOT2015和VOT2016里面也用这个参数衡量算法速度，这里一次性列出来供参考(MATLAB实现的tracker实际速度要更高一些)：\n其实前三名除了特征略有差异，核心都是KCF为基础扩展了多尺度检测，概要如下：\n尺度变化是跟踪中比较基础和常见的问题，前面介绍的KCF/DCF和CN都没有尺度更新，如果目标缩小，滤波器就会学习到大量背景信息，如果目标扩大，滤波器就跟着目标局部纹理走了，这两种情况都很可能出现非预期的结果，导致漂移和失败。\nSAMF ihpdep/samf，浙大Yang Li的工作，基于KCF，特征是HOG+CN，多尺度方法是平移滤波器在多尺度缩放的图像块上进行目标检测，取响应最大的那个平移位置及所在尺度：\nLi Y, Zhu J. A scale adaptive kernel correlation filter tracker with feature integration [C]// ECCV, 2014.\nMartin Danelljan的DSST Accurate scale estimation for visual tracking，只用了HOG特征，DCF用于平移位置检测，又专门训练类似MOSSE的相关滤波器检测尺度变化，开创了平移滤波+尺度滤波，之后转TPAMI做了一系列加速的版本fDSST，非常+非常+非常推荐：\nDanelljan M, Häger G, Khan F, et al. Accurate scale estimation for robust visual tracking [C]// BMVC, 2014.\nDanelljan M, Hager G, Khan F S, et al. Discriminative Scale Space Tracking [J]. IEEE TPAMI, 2017.\n简单对比下这两种尺度自适应的方法：\nDSST和SAMF所采用的尺度检测方法哪个更好？\n首先给大家讲个笑话：Martin Danelljan大神提出DSST之后，他的后续论文就再没有用过(直到最新CVPR的ECO-HC中为了加速用了fDSST)。\n虽然SAMF和DSST都可以跟上普通的目标尺度变化，但SAMF只有7个尺度比较粗，而DSST有33个尺度比较精细准确；\nDSST先检测最佳平移再检测最佳尺度，是分步最优，而SAMF是平移尺度一起检测，是平移和尺度同时最优，而往往局部最优和全局最优是不一样的；\nDSST将跟踪划分为平移跟踪和尺度跟踪两个问题，可以采用不同的方法和特征，更加灵活，但需要额外训练一个滤波器，每帧尺度检测需要采样33个图像块，之后分别计算特征、加窗、FFT等，尺度滤波器比平移滤波器慢很多；SAMF只需要一个滤波器，不需要额外训练和存储，每个尺度检测就一次提特征和FFT，但在图像块较大时计算量比DSST高。\n所以尺度检测DSST并不总是比SAMF好，其实在VOT2015和VOT2016上SAMF都是超过DSST的，当然这主要是因为特征更好，但至少说明尺度方法不差。总的来说，DSST做法非常新颖，速度更快，SAMF同样优秀也更加准确。\nDSST一定要33个尺度吗？\nDSST标配33个尺度非常非常敏感，轻易降低尺度数量，即使你增加相应步长，尺度滤波器也会完全跟不上尺度变化。关于这一点可能解释是，训练尺度滤波器用的是一维样本，而且没有循环移位，这就意味着一次训练更新只有33个样本，如果降低样本数量，会造成训练不足，分类器判别力严重下降，不像平移滤波器有非常多的移位样本(个人看法欢迎交流)。总之，请不要轻易尝试大幅降低尺度数量，如果非要用尺度滤波器33和1.02就很好。\n以上就是两种推荐的尺度检测方法，以后简称为类似DSST的多尺度和类似SAMF的多尺度。如果更看重速度，加速版的fDSST，和仅3个尺度的SAMF(如VOT2014中的KCF)就是比较好的选择；如果更看重精确，33个尺度的DSST，及7个尺度的SAMF就比较合适。\n第五部分：边界效应\n接下来到了VOT2015竞赛 VOT2015 Challenge | Home ，这一年有60个精挑细选的序列，62个tracker，最大看点是深度学习开始进击tracking领域，MDNet直接拿下当年的冠军，而结合深度特征的相关滤波方法DeepSRDCF是第二名，主要解决边界效应的SRDCF仅HOG特征排在第四：\n随着VOT竞赛的影响力扩大，举办方也是用心良苦，经典的和顶尖的齐聚一堂，百家争鸣，多达62个tracker皇城PK，华山论剑。除了前面介绍的深度学习和相关滤波，还有结合object proposals(类物体区域检测)的EBT(EBT：Proposal与Tracking不得不说的秘密 - 知乎专栏)排第三，Mean-Shift类颜色算法ASMS是推荐实时算法，还有前面提到的另一个颜色算法DAT，而在第9的那个Struck已经不是原来的Struck了。除此之外，还能看到经典方法如OAB, STC, CMT, CT, NCC等都排在倒数位置， 经典方法已经被远远甩在后面。\n在介绍SRDCF之前，先来分析下相关滤波有什么缺点。总体来说，相关滤波类方法对快速变形和快速运动情况的跟踪效果不好。\n快速变形主要因为CF是模板类方法。容易跟丢这个比较好理解，前面分析了相关滤波是模板类方法，如果目标快速变形，那基于HOG的梯度模板肯定就跟不上了，如果快速变色，那基于CN的颜色模板肯定也就跟不上了。这个还和模型更新策略与更新速度有关，固定学习率的线性加权更新，如果学习率太大，部分或短暂遮挡和任何检测不准确，模型就会学习到背景信息，积累到一定程度模型跟着背景私奔了，一去不复返。如果学习率太小，目标已经变形了而模板还是那个模板，就会变得不认识目标。\n快速运动主要是边界效应(Boundary Effets)，而且边界效应产生的错误样本会造成分类器判别力不够强，下面分训练阶段和检测阶段分别讨论。\n训练阶段，合成样本降低了判别能力。如果不加余弦窗，那么移位样本是长这样的：\n除了那个最原始样本，其他样本都是“合成”的，100*100的图像块，只有1/10000的样本是真实的，这样的样本集根本不能拿来训练。如果加了余弦窗，由于图像边缘像素值都是0，循环移位过程中只要目标保持完整，就认为这个样本是合理的，只有当目标中心接近边缘时，目标跨越了边界的那些样本是错误的，这样虽不真实但合理的样本数量增加到了大约2/3(一维情况padding= 1)。但我们不能忘了即使这样仍然有1/3(3000/10000)的样本是不合理的，这些样本会降低分类器的判别能力。再者，加余弦窗也不是“免费的”，余弦窗将图像块的边缘区域像素全部变成0，大量过滤掉了分类器本来非常需要学习的背景信息，原本训练时判别器能看到的背景信息就非常有限，我们还加了个余弦窗挡住了背景，这样进一步降低了分类器的判别力(是不是上帝在我前遮住了帘。。不是上帝，是余弦窗)。\n检测阶段，相关滤波对快速运动的目标检测比较乏力。相关滤波训练的图像块和检测的图像块大小必须是一样的，这就是说你训练了一个100*100的滤波器，那你也只能检测100*100的区域，如果打算通过加更大的padding来扩展检测区域，那样除了扩展了复杂度，并不会有什么好处。目标运动可能是目标自身移动，或摄像机移动，按照目标在检测区域的位置分四种情况来看：\n如果目标在中心附近，检测准确且成功。\n如果目标移动到了边界附近但还没有出边界，加了余弦窗以后，部分目标像素会被过滤掉，这时候就没法保证这里的响应是全局最大的，而且，这时候的检测样本和训练过程中的那些不合理样本很像，所以很可能会失败。\n如果目标的一部分已经移出了这个区域，而我们还要加余弦窗，很可能就过滤掉了仅存的目标像素，检测失败。\n如果整个目标已经位移出了这个区域，那肯定就检测失败了。\n以上就是边界效应(Boundary Effets)，推荐两个主流的解决边界效应的方法，其中SRDCF速度比较慢，并不适合实时场合。\nMartin Danelljan的SRDCF Learning Spatially Regularized Correlation Filters for Visual Tracking，主要思路：既然边界效应发生在边界附近，那就忽略所有移位样本的边界部分像素，或者说限制让边界附近滤波器系数接近0：\nDanelljan M, Hager G, Shahbaz Khan F, et al. Learning spatially regularized correlation filters for visual tracking [C]// ICCV. 2015.\nSRDCF基于DCF，类SAMF多尺度，采用更大的检测区域(padding = 4)，同时加入空域正则化，惩罚边界区域的滤波器系数，由于没有闭合解，采用高斯-塞德尔方法迭代优化。检测区域扩大(1.5->4)，迭代优化(破坏了闭合解)导致SRDCF只有5FP，但效果非常好是2015年的baseline。\n另一种方法是Hamed Kiani提出的MOSSE改进算法，基于灰度特征的CFLM Correlation Filters with Limited Boundaries 和基于HOG特征的BACF Learning Background-Aware Correlation Filters for Visual Tracking，主要思路是采用较大尺寸检测图像块和较小尺寸滤波器来提高真实样本的比例，或者说滤波器填充0以保持和检测图像一样大，同样没有闭合解，采用ADMM迭代优化：\nKiani Galoogahi H, Sim T, Lucey S. Correlation filters with limited boundaries [C]// CVPR, 2015.\nKiani Galoogahi H, Fagg A, Lucey S. Learning Background-Aware Correlation Filters for Visual Tracking [C]// ICCV, 2017.\nCFLB仅单通道灰度特征，虽然速度比较快167FPS，但性能远不如KCF，不推荐；最新BACF将特征扩展为多通道HOG特征，性能超过了SRDCF，而且速度比较快35FPS，非常推荐。\n其实这两个解决方案挺像的，都是用更大的检测及更新图像块，训练作用域比较小的相关滤波器，不同点是SRDCF的滤波器系数从中心到边缘平滑过渡到0，而CFLM直接用0填充滤波器边缘。\nVOT2015相关滤波方面还有排在第二名，结合深度特征的DeepSRDCF，因为深度特征都非常慢，在CPU上别说高速，实时都到不了，虽然性能非常高，但这里就不推荐，先跳过。\n第六部分：颜色直方图与相关滤波\nVOT2016竞赛 VOT2016 Challenge | Home，依然是VOT2015那60个序列，不过这次做了重新标注更加公平合理，今年有70位参赛选手，意料之中深度学习已经雄霸天下了，8个纯CNN方法和6个结合深度特征的CF方法大都名列前茅，还有一片的CF方法，最最最重要的是，良心举办方竟然公开了他们能拿到的38个tracker，部分tracker代码和主页，下载地址：VOT2016 Challenge | Trackers (以后妈妈再也不用担心我找不到源码了~)，注意部分是下载链接，部分是源码压缩包，部分源码是二进制文件，好不好用一试便知，方便对比和研究，需要的赶快去试试。马上来看竞赛结果(这里仅列举前60个)：\n高亮标出来了前面介绍过的或比较重要的方法，结合多层深度特征的相关滤波C-COT排第一名，而CNN方法TCNN是VOT2016的冠军，作者也是VOT2015冠军MDNet，纯颜色方法DAT和ASMS都在中等水平(其实两种方法实测表现非常接近)，其他tracker的情况请参考论文。再来看速度，SMACF没有公开代码，ASMS依然那么快，排在前10的方法中也有两个速度比较快，分别是排第5的Staple，和其改进算法排第9的STAPLE+，而且STAPLE+是今年的推荐实时算法。首先恭喜Luca Bertinetto的SiamFC和Staple都表现非常不错，然后再为大牛默哀三分钟(VOT2016的paper原文)：\nThis was particularly obvious in case of SiamFC trackers, which runs orders higher than realtime (albeit on GPU), and Staple, which is realtime, but are incorrectly among the non-realtime trackers.\nVOT2016竟然发生了乌龙事件，Staple在论文中CPU上是80FPS，怎么EFO在这里只有11？幸好公开代码有Staple和STAPLE+，实测下来，虽然我电脑不如Luca Bertinetto大牛但Staple我也能跑76FPS，而更可笑的是，STAPLE+比Staple慢了大约7-8倍，竟然EFO高出4倍，到底怎么回事呢？\n首先看Staple的代码，如果您直接下载Staple并设置params.visualization = 1，Staple默认调用Computer Vision System Toolbox来显示序列图像，而恰好如果您没有这个工具箱，默认每帧都会用imshow(im)来显示图像，所以非常非常慢，而设置params.visualization = 0就跑的飞快(作者你是孙猴子派来的逗逼吗)，建议您将显示图像部分代码替换成DSST中对应部分代码就可以正常速度运行和显示了。\n再来看STAPLE+的代码，对Staple的改进包括额外从颜色概率图中提取HOG特征，特征增加到56通道(Staple是28通道)，平移检测额外加入了大位移光流运动估计的响应，所以才会这么慢，而且肯定要慢很多。\n所以很大可能是VOT举办方把Staple和STAPLE+的EFO弄反了，VOT2016的实时推荐算法应该是排第5的Staple，相关滤波结合颜色方法，没有深度特征更没有CNN，跑80FPS还能排在第五，这就是接下来主要介绍的，2016年最NIUBILITY的目标跟踪算法之一Staple (直接让排在后面的一众深度学习算法怀疑人生)。\n颜色特征，在目标跟踪中颜色是个非常重要的特征，不管多少个人在一起，只要目标穿不用颜色的一幅就非常明显。前面介绍过2014年CVPR的CN是相关滤波框架下的模板颜色方法，这里隆重介绍统计颜色特征方法DAT Learning, Recognition, and Surveillance @ ICG ，帧率15FPS推荐：\nPossegger H, Mauthner T, Bischof H. In defense of color-based model-free tracking [C]// CVPR, 2015.\nDAT统计前景目标和背景区域的颜色直方图并归一化，这就是前景和背景的颜色概率模型，检测阶段，贝叶斯方法判别每个像素属于前景的概率，得到像素级颜色概率图，再加上边缘相似颜色物体抑制就能得到目标的区域了。\n如果要用一句话介绍Luca Bertinetto(牛津大学)的Staple Staple tracker，那就是把模板特征方法DSST(基于DCF)和统计特征方法DAT结合：\nBertinetto L, Valmadre J, Golodetz S, et al. Staple: Complementary Learners for Real-Time Tracking [C]// CVPR, 2016.\n前面分析了相关滤波模板类特征(HOG)对快速变形和快速运动效果不好，但对运动模糊光照变化等情况比较好；而颜色统计特征(颜色直方图)对变形不敏感，而且不属于相关滤波框架没有边界效应，快速运动当然也是没问题的，但对光照变化和背景相似颜色不好。综上，这两类方法可以互补，也就是说DSST和DAT可以互补结合：\n两个框架的算法高效无缝结合，25FPS的DSST和15FPS的DAT，而结合后速度竟然达到了80FPS。DSST框架把跟踪划分为两个问题，即平移检测和尺度检测，DAT就加在平移检测部分，相关滤波有一个响应图，像素级前景概率也有一个响应图，两个响应图线性加权得到最终响应图，其他部分与DSST类似，平移滤波器、尺度滤波器和颜色概率模型都以固定学习率线性加权更新。\n另一种相关滤波结合颜色概率的方法是17CVPR的CSR-DCF，提出了空域可靠性和通道可靠性，没有深度特征性能直逼C-COT，速度可观13FPS：\nLukežič A, Vojíř T, Čehovin L, et al. Discriminative Correlation Filter with Channel and Spatial Reliability [C]// CVPR, 2017.\nCSR-DCF中的空域可靠性得到的二值掩膜就类似于CFLM中的掩膜矩阵P，在这里自适应选择更容易跟踪的目标区域且减小边界效应；以往多通道特征都是直接求和，而CSR-DCF中通道采用加权求和，而通道可靠性就是那个自适应加权系数。采用ADMM迭代优化，可以看出CSR-DCF是DAT和CFLB的结合算法。\nVOT2015相关滤波还有排第一名的C-COT(别问我第一名为什么不是冠军，我也不知道)，和DeepSRDCF一样先跳过。\n第七部分：long-term和跟踪置信度\n以前提到的很多CF算法，也包括VOT竞赛，都是针对short-term的跟踪问题，即短期(shor-term)跟踪，我们只关注短期内(如100~500帧)跟踪是否准确。但在实际应用场合，我们希望正确跟踪时间长一点，如几分钟或十几分钟，这就是长期(long-term)跟踪问题。\nLong-term就是希望tracker能长期正确跟踪，我们分析了前面介绍的方法不适合这种应用场合，必须是short-term tracker + detecter配合才能实现正确的长期跟踪。\n用一句话介绍Long-term，就是给普通tracker配一个detecter，在发现跟踪出错的时候调用自带detecter重新检测并矫正tracker。\n介绍CF方向一篇比较有代表性的long-term方法，Chao Ma的LCT chaoma99/lct-tracker：\nMa C, Yang X, Zhang C, et al. Long-term correlation tracking[C]// CVPR, 2015.\nLCT在DSST一个平移相关滤波Rc和一个尺度相关滤波的基础上，又加入第三个负责检测目标置信度的相关滤波Rt，检测模块Online Detector是TLD中所用的随机蔟分类器(random fern)，在代码中改为SVM。第三个置信度滤波类似MOSSE不加padding，而且特征也不加cosine窗，放在平移检测之后。\n如果最大响应小于第一个阈值(叫运动阈值)，说明平移检测不可靠，调用检测模块重新检测。注意，重新检测的结果并不是都采纳的，只有第二次检测的最大响应值比第一次检测大1.5倍时才接纳，否则，依然采用平移检测的结果。\n如果最大响应大于第二个阈值(叫外观阈值)，说明平移检测足够可信，这时候才以固定学习率在线更新第三个相关滤波器和随机蔟分类器。注意，前两个相关滤波的更新与DSST一样，固定学习率在线每帧更新。\nLCT加入检测机制，对遮挡和出视野等情况理论上较好，速度27fps，实验只跑了OTB-2013，跟踪精度非常高，根据其他论文LCT在OTB-2015和 VOT上效果略差一点可能是两个核心阈值没有自适应， 关于long-term，TLD和LCT都可以参考 。\n接下来介绍跟踪置信度。 跟踪算法需要能反映每一次跟踪结果的可靠程度，这一点非常重要，不然就可能造成跟丢了还不知道的情况。生成类(generative)方法有相似性度量函数，判别类(discriminative)方法有机器学习方法的分类概率。有两种指标可以反映相关滤波类方法的跟踪置信度：前面见过的最大响应值，和没见过的响应模式，或者综合反映这两点的指标。\nLMCF(MM Wang的目标跟踪专栏：目标跟踪算法 - 知乎专栏 )提出了多峰检测和高置信度更新：\nWang M, Liu Y, Huang Z. Large Margin Object Tracking with Circulant Feature Maps [C]// CVPR, 2017.\n高置信度更新，只有在跟踪置信度比较高的时候才更新跟踪模型，避免目标模型被污染，同时提升速度。 第一个置信度指标是最大响应分数Fmax，就是最大响应值(Staple和LCT中都有提到)。 第二个置信度指标是平均峰值相关能量(average peak-to correlation energy, APCE)，反应响应图的波动程度和检测目标的置信水平，这个(可能)是目前最好的指标，推荐：\n跟踪置信度指标还有，MOSSE中的峰值旁瓣比(Peak to Sidelobe Ratio, PSR)， 由相关滤波峰值，与11*11峰值窗口以外旁瓣的均值与标准差计算得到，推荐：\n还有CSR-DCF的空域可靠性，也用了两个类似指标反映通道可靠性， 第一个指标也是每个通道的最大响应峰值，就是Fmax，第二个指标是响应图中第二和第一主模式之间的比率，反映每个通道响应中主模式的表现力，但需要先做极大值检测：\n第八部分：卷积特征\n最后这部分是Martin Danelljan的专场，主要介绍他的一些列工作，尤其是结合深度特征的相关滤波方法，代码都在他主页Visual Tracking，就不一一贴出了。\nDanelljan M, Shahbaz Khan F, Felsberg M, et al. Adaptive color attributes for real-time visual tracking [C]// CVPR, 2014.\n在CN中提出了非常重要的多通道颜色特征Color Names，用于CSK框架取得非常好得效果，还提出了加速算法CN2，通过类PCA的自适应降维方法，对特征通道数量降维(10 -> 2)，平滑项增加跨越不同特征子空间时的代价，也就是PCA中的协方差矩阵线性更新防止降维矩阵变化太大。\nDanelljan M, Hager G, Khan F S, et al. Discriminative Scale Space Tracking [J]. IEEE TPAMI, 2017.\nDSST是VOT2014的第一名，开创了平移滤波+尺度滤波的方式。在fDSST中对DSST进行加速，PCA方法将平移滤波HOG特征的通道降维(31 -> 18)，QR方法将尺度滤波器~1000*17的特征降维到17*17，最后用三角插值(频域插值)将尺度数量从17插值到33以获得更精确的尺度定位。\nSRDCF是VOT2015的第四名，为了减轻边界效应扩大检测区域，优化目标增加了空间约束项，用高斯-塞德尔方法迭代优化，并用牛顿法迭代优化平移检测的子网格精确目标定位。\nDanelljan M, Hager G, Shahbaz Khan F, et al. Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking [C]// CVPR, 2016.\nSRDCFdecon在SRDCF的基础上，改进了样本和学习率问题。以前的相关滤波都是固定学习率线性加权更新模型，虽然这样比较简单不用保存以前样本，但在定位不准确、遮挡、背景扰动等情况会污染模型导致漂移。SRDCFdecon选择保存以往样本(图像块包括正，负样本)，在优化目标函数中添加样本权重参数和正则项，采用交替凸搜索，首先固定样本权重，高斯-塞德尔方法迭代优化模型参数，然后固定模型参数，凸二次规划方法优化样本权重。\nDanelljan M, Hager G, Shahbaz Khan F, et al. Convolutional features for correlation filter based visual tracking [C]// ICCVW, 2015.\nDeepSRDCF是VOT2015的第二名，将SRDCF中的HOG特征替换为CNN中单层卷积层的深度特征(也就是卷积网络的激活值)，效果有了极大提升。这里用imagenet-vgg-2048 network，VGG网络的迁移能力比较强，而且MatConvNet就是VGG组的，MATLAB调用非常方便。论文还测试了不同卷积层在目标跟踪任务中的表现：\n第1层表现最好，第2和第5次之。由于卷积层数越高语义信息越多，但纹理细节越少，从1到4层越来越差的原因之一就是特征图的分辨率越来越低，但第5层反而很高，是因为包括完整的语义信息，判别力比较强(本来就是用来做识别的)。\n注意区分这里的深度特征和基于深度学习的方法，深度特征来自ImageNet上预训练的图像分类网络，没有fine-turn这一过程，不存在过拟合的问题。而基于深度学习的方法大多需要在跟踪序列上end-to-end训练或fine-turn，如果样本数量和多样性有限就很可能过拟合。\nMa C, Huang J B, Yang X, et al. Hierarchical convolutional features for visual tracking [C]// ICCV, 2015.\n值得一提的还有Chao Ma的HCF，结合多层卷积特征提升效果，用了VGG19的Conv5-4, Conv4-4和Conv3-4的激活值作为特征，所有特征都缩放到图像块分辨率，虽然按照论文应该是由粗到细确定目标，但代码中比较直接，三种卷积层的响应以固定权值1, 0.5, 0.02线性加权作为最终响应。虽然用了多层卷积特征，但没有关注边界效应而且线性加权的方式过于简单，HCF在VOT2016仅排在28名（单层卷积深度特征的DeepSRDCF是第13名）。\nDanelljan M, Robinson A, Khan F S, et al. Beyond correlation filters: Learning continuous convolution operators for visual tracking [C]// ECCV, 2016.\nC-COT是VOT2016的第一名，综合了SRDCF的空域正则化和SRDCFdecon的自适应样本权重，还将DeepSRDCF的单层卷积的深度特征扩展为多成卷积的深度特征（VGG第1和5层），为了应对不同卷积层分辨率不同的问题，提出了连续空间域插值转换操作，在训练之前通过频域隐式插值将特征图插值到连续空域，方便集成多分辨率特征图，并且保持定位的高精度。目标函数通过共轭梯度下降方法迭代优化，比高斯-塞德尔方法要快，自适应样本权值直接采用先验权值，没有交替凸优化过程，检测中用牛顿法迭代优化目标位置。\n注意以上SRDCF, SRDCFdecon，DeepSRDCF，C-COT都无法实时，这一系列工作虽然效果越来越好，但也越来越复杂，在相关滤波越来越慢失去速度优势的时候，Martin Danelljan在2017CVPR的ECO来了一脚急刹车，大神来告诉我们什么叫又好又快，不忘初心：\nDanelljan M, Bhat G, Khan F S, et al. ECO: Efficient Convolution Operators for Tracking [C]// CVPR, 2017.\nECO是C-COT的加速版，从模型大小、样本集大小和更新策略三个方便加速，速度比C-COT提升了20倍，加量还减价，EAO提升了13.3%，最最最厉害的是， hand-crafted features的ECO-HC有60FPS。。吹完了，来看看具体做法。\n第一减少模型参数，定义了factorized convolution operator(分解卷积操作)，效果类似PCA，用PCA初始化，然后仅在第一帧优化这个降维矩阵，以后帧都直接用，简单来说就是有监督降维，深度特征时模型参数减少了80%。\n第二减少样本数量， compact generative model(紧凑的样本集生成模型)，采用Gaussian Mixture Model (GMM)合并相似样本，建立更具代表性和多样性的样本集，需要保存和优化的样本集数量降到C-COT的1/8。\n第三改变更新策略，sparser updating scheme(稀疏更新策略)，每隔5帧做一次优化更新模型参数，不但提高了算法速度，而且提高了对突变，遮挡等情况的稳定性。但样本集是每帧都更新的，稀疏更新并不会错过间隔期的样本变化信息。\nECO的成功当然还有很多细节，而且有些我也看的不是很懂，总之很厉害就是了。。ECO实验跑了四个库(VOT2016, UAV123, OTB-2015, and TempleColor)都是第一，而且没有过拟合的问题，仅性能来说ECO是目前最好的相关滤波算法，也有可能是最好的目标跟踪算法。hand-crafted features版本的ECO-HC，降维部分原来HOG+CN的42维特征降到13维，其他部分类似，实验结果ECO-HC超过了大部分深度学习方法，而且论文给出速度是CPU上60FPS。\n最后是来自Luca Bertinetto的CFNet End-to-end representation learning for Correlation Filter based tracking，除了上面介绍的相关滤波结合深度特征，相关滤波也可以end-to-end方式在CNN中训练了：\nValmadre J, Bertinetto L, Henriques J F, et al. End-to-end representation learning for Correlation Filter based tracking [C]// CVPR, 2017.\n在SiamFC的基础上，将相关滤波也作为CNN中的一层，最重要的是cf层的前向传播和反向传播公式推导，两层卷积层的CFNet在GPU上是75FPS，综合表现并没有很多惊艳，可能是难以处理CF层的边界效应吧，持观望态度。\n第九部分：2017年CVPR和ICCV结果\n下面是CVPR 2017的目标跟踪算法结果：可能MD大神想说，一个能打的都没有！\n仿照上面的表格，整理了ICCV 2017的相关论文结果对比ECO：哎，还是一个能打的都没有！\n第十部分：大牛推荐\n凑个数，目前相关滤波方向贡献最多的是以下两个组(有创新有代码)：\n牛津大学：Joao F. Henriques和Luca Bertinetto，代表：CSK, KCF/DCF, Staple, CFNet (其他SiamFC, Learnet).\n林雪平大学：Martin Danelljan，代表：CN, DSST, SRDCF, DeepSRDCF, SRDCFdecon, C-COT, ECO.\n国内也有很多高校的优秀工作就不一一列举了。\n___________________________________________________________________________\n我的目标跟踪专栏：目标跟踪之相关滤波CF，长期更新，专注跟踪，但也会介绍计算机视觉和深度学习的各方面内容，拓展思路。"}
{"content2":"王家林人工智能AI 从零起步(无需数学和Python基础)编码实现AI框架之第八节课：AI的上帝视角Why、How、What，及Perceptron彻底解密及计算机视觉的觉醒 老师微信13928463918\n王家林老师的总结：\n为什么分层导致了人工智能前所未有的突破？做了以前人类从没想过计算机能做的事情，是因为：\n分层之后，第一层进行最原始的处理，第二层可以在第一层的处理结果上进行抽象，第三层可以在第二层的处理结果上进一步抽象，第四层可以在第三层的处理结果上进一步抽象，整个文类社会文明的精髓就在于一层又一层的抽象，当你抽象程度越高的时候，你就越能做出具有洞察力的决策和获得智慧！整个人类社会的文明史，所有国家的发展过程，都是在一步又一步的抽象过程，类似于金字塔的结构的过程。而我们一层又一层的神经网络，可以把它向上旋转90度，其实是一层又一层的抽象，这个过程是所有伟大的数学家，所有伟大的哲学家、文学家，及政治领域取得伟大成就的人，他们共同采取的路径！所以，神经网络真正代表的意义是人类在现实经验不断进行抽象产生洞察力和智慧的过程，这是分层的神经网络有力量的根源！这也是人类自古以来的有力量的根源！所以分层的深度神经网络系统完整的实现了人类获得所有的洞察力的模型的过程！不要仅仅看见第一层，第二层，第三层，这个没有意义，第三层可以在第二层的基础上抽象，第四层可以在第三层的基础上抽象....这是神经网络真正的强大之处，这也是在视觉识别系统，语音识别系统，自然语言处理等诸多方面产生超越人类预期的根源！因为这反映了整个人类的文明史到底怎么运行的，怎么进化的！\n家林老师关于Bias的总结\nBias是前面的因素和权重相乘累加是否能达到某个值，就像决定能不能看电影，Bias就是一个门槛，就像达不到哈佛大学的分数，就不能录取。Bias是决定是否看电影的一个标准，满足了什么条件，对条件进行了衡量之后，达到我们的预期才决定是否看电影，达到这个预期就看电影，达不到预期，就不看电影。Bias是偏好，是Preference。Bias不是随便能调的，涉及一个标准和门槛，这不是加0.1和减0.1的问题。Bias是神经网络系统判断图片是否是一只猫，还是一只狗，是一辆汽车还是一辆飞机，最后综合得到的标准，这不是乱调就能调出来的。\n作业：自己设想一下计算机图像识别的实现思路。\n参考网络资料，接下来我们对计算机进行图像识别，实际编码实现识别图像（手写数字识别），将会使用Python以及Python的许多模块，例如numpy、PIL等。\n#从PIL库中导入Image from PIL import Image #导入numpy import numpy as np #从文件中载入图像 i = Image.open('images/dot.png') #将图像转换成矩阵形式 iar = np.asarray(i) ''' Alright, so an image array comes out as a 3-dimensional array. This means an array within an array an array, or list. So the entire list is the whole image. The 2nd dimension in, corresponds to the row, and the 3rd dimension in corresponds to each pixil in that row.. or u can think of it like a column. within the pixil arrays, you have your typical Red Green Blue, or RGB, and then you are also supplied with an \"alpha\"... and this is for each pixil. So that's just a quick example. In the next example, we'll be breaking things down further, and beginning to build some recognition. ''' print(iar)\n图像“dot.png”基本如下：\n运行结果如下：\n[[[ 0 0 0 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]] [[255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255] [255 255 255 255]]]\n这是一个由8个矩阵块组成的集合。每个矩阵块表示一个“水平行的像素”，如果仔细观察，你会发现图像有8个像素行，因此图像数组有8个矩阵块。进一步观察，图像的第一个像素行是由一个黑色像素后面跟着7个白色像素组成的。现在如果我们回到刚才生成的图像矩阵中，你会注意到像素行的第一个像素[0(R) 0(G) 0(B) 255(A)]为黑色（black），其余的[255(R) 255(G) 255(B) 255(A)]为白色（white）。\n注意：A（α值）表示图像的透明度。α值越低，透明度越高，相反α值越高，透明度越低。\n因此，这里共有8行类似这样的结果，剩下的都是白色。\n当计算机尝试识别数字时，会碰到一个问题，有些数字可能会看上去模糊、褪色或变形，就像验证码中或带有彩色零的图像中显示的那样。机器要处理数百万种颜色，增加了计算的复杂度。并不是所有的图像都有机器容易识别的白色或深色的背景。对于这个问题我们通过设定阈值来解决。我们分析图像的平均颜色，将它作为阈值。高于阈值则认为更接近白色，低于阈值认为接近黑色。我们用阈值函数来解决这个问题。使用阈值函数，我们将把任何图像变成黑白两色，这种方法使得计算机不必困惑于数百万种颜色（256*256*256*256），简化了图像识别的工作量。我们提出了将超过阈值的值转化为白色（很明显，一个图像的色彩越多，越接近与白色），同时低于阈值的值转化为黑色。因此现在最有挑战的部分是阈值的确定，该函数基本上是改变数组的值，使每个像素转换为白色或黑色。\n''' Thresholding! ''' from PIL import Image import numpy as np #import matplotlib import matplotlib.pyplot as plt import time from functools import reduce def threshold(imageArray): balanceAr = [] newAr = imageArray for eachRow in imageArray: for eachPix in eachRow: #print eachPix avgNum = reduce(lambda x, y: x + y, eachPix[:3]) / len(eachPix[:3]) balanceAr.append(avgNum) #time.sleep(3) balance = reduce(lambda x, y: x + y, balanceAr) / len(balanceAr) #print balance for eachRow in newAr: for eachPix in eachRow: if reduce(lambda x, y: x + y, eachPix[:3]) / len(eachPix[:3]) > balance: eachPix[0] = 255 eachPix[1] = 255 eachPix[2] = 255 eachPix[3] = 255 else: eachPix[0] = 0 eachPix[1] = 0 eachPix[2] = 0 eachPix[3] = 255 return newAr i = Image.open('images/numbers/0.1.png') iar = np.array(i) i2 = Image.open('images/numbers/y0.4.png') iar2 = np.array(i2) i3 = Image.open('images/numbers/y0.5.png') iar3 = np.array(i3) i4 = Image.open('images/sentdex.png') iar4 = np.array(i4) fig = plt.figure() ax1 = plt.subplot2grid((8,6),(0,0), rowspan=4, colspan=3) ax2 = plt.subplot2grid((8,6),(4,0), rowspan=4, colspan=3) ax3 = plt.subplot2grid((8,6),(0,3), rowspan=4, colspan=3) ax4 = plt.subplot2grid((8,6),(4,3), rowspan=4, colspan=3) ax1.imshow(iar) ax2.imshow(iar2) ax3.imshow(iar3) ax4.imshow(iar4) plt.show() iar = threshold(iar) iar2 = threshold(iar2) iar3 = threshold(iar3) iar4 = threshold(iar4) fig = plt.figure() ax1 = plt.subplot2grid((8,6),(0,0), rowspan=4, colspan=3) ax2 = plt.subplot2grid((8,6),(4,0), rowspan=4, colspan=3) ax3 = plt.subplot2grid((8,6),(0,3), rowspan=4, colspan=3) ax4 = plt.subplot2grid((8,6),(4,3), rowspan=4, colspan=3) ax1.imshow(iar) ax2.imshow(iar2) ax3.imshow(iar3) ax4.imshow(iar4) #threshold(iar4) plt.show()\n运行结果如下：\n注意到所有的图像都被转化成黑白图像。阈值设定是图像识别的首要步骤之一。\n在阀值函数里，我们通过运行二重循环来读取每个像素行并将该像素行中所有的RGB值取平均值，然后将其添加到balanceAr数组中。这一循环会涉及所有的像素行，计算每一行的像素平均值，然后将其添加到balanceAr数组中。接下来我们将对balanceAr数组中所有的值取平均，并将该平均值存入balance变量中。\n我们再回到每个像素行，取平均值并将均值与阈值相比较。如果该均值大于阈值，我们将像素转换为白色，反之转换为黑色。\n现在我们已经解决了阈值的问题，并且将所有的图像（彩色或黑白）统一转换成了黑白图像。接下来是比较有趣的部分。我们首先要训练数据，然后教会它如何识别和预测图像。\n我们将通过存储本地数据（一些图像）来训练和测试它们，以帮助我们识别图像。\n现在， 我的文件夹里已经有了一些图像（数字）文件。我将要做的是用我的numpy函数将这些图像都转化成数组，然后存入数据库中。这样可以节约时间，不必每次处理图像时都要先将其转化为数组。\n\nfrom PIL import Image import numpy as np import matplotlib.pyplot as plt import time from collections import Counter from matplotlib import style style.use(\"ggplot\") def createExamples(): numberArrayExamples = open('numArEx.txt','a') numbersWeHave = range(1,10) for eachNum in numbersWeHave: for furtherNum in numbersWeHave: imgFilePath = 'images/numbers/'+str(eachNum)+'.'+str(furtherNum)+'.png' ei = Image.open(imgFilePath) eiar = np.array(ei) eiarl = str(eiar.tolist()) lineToWrite = str(eachNum)+'::'+eiarl+'\\n' numberArrayExamples.write(lineToWrite) def threshold(imageArray): balanceAr = [] newAr = imageArray for eachPart in imageArray: for theParts in eachPart: avgNum = reduce(lambda x, y: x + y, theParts[:3]) / len(theParts[:3]) balanceAr.append(avgNum) balance = reduce(lambda x, y: x + y, balanceAr) / len(balanceAr) for eachRow in newAr: for eachPix in eachRow: if reduce(lambda x, y: x + y, eachPix[:3]) / len(eachPix[:3]) > balance: eachPix[0] = 255 eachPix[1] = 255 eachPix[2] = 255 eachPix[3] = 255 else: eachPix[0] = 0 eachPix[1] = 0 eachPix[2] = 0 eachPix[3] = 255 return newAr def whatNumIsThis(filePath): matchedAr = [] loadExamps = open('numArEx.txt','r').read() loadExamps = loadExamps.split('\\n') i = Image.open(filePath) iar = np.array(i) iarl = iar.tolist() inQuestion = str(iarl) for eachExample in loadExamps: try: splitEx = eachExample.split('::') currentNum = splitEx[0] currentAr = splitEx[1] eachPixEx = currentAr.split('],') eachPixInQ = inQuestion.split('],') x = 0 while x < len(eachPixEx): if eachPixEx[x] == eachPixInQ[x]: matchedAr.append(int(currentNum)) x+=1 except Exception as e: print(str(e)) x = Counter(matchedAr) print(x) graphX = [] graphY = [] ylimi = 0 for eachThing in x: print(str(eachThing) + \" \" +str(x[eachThing])) graphX.append(eachThing) graphY.append(x[eachThing]) ylimi = x[eachThing] fig = plt.figure() ax1 = plt.subplot2grid((4,4),(0,0), rowspan=1, colspan=4) ax2 = plt.subplot2grid((4,4),(1,0), rowspan=3,colspan=4) ax1.imshow(iar) ax2.bar(graphX,graphY,align='center') plt.ylim(400) xloc = plt.MaxNLocator(12) ax2.xaxis.set_major_locator(xloc) plt.show() whatNumIsThis('images/test.png') whatNumIsThis('images/numbers/9.5.png')\n在代码中加入了createExamples()函数 。文本文件中基本包含了所有不同的数字以及它们的数组形式。现在我们只需要一个步骤：对比。当我们拿到一个数字并需要识别它时，我们只需尝试将该数字的图像数组和数据库中已有的数组进行对比。如果对比结果是一致的，则输出这个数字。 用一个函数用来对比指定图像和数据库中的图像数组。whatNumIsThis(filePath)这函数，参数为文件路径（filePath）。这个函数的基本功能是根据文件路径提取图像，将其转为数组，并跟数据库中的数组进行比对。分割函数可以分割所有像素数组。计数器可以记录每一次成功的对比。\n运行结果:\nlist index out of range Counter({2: 2766, 3: 2334, 6: 2244, 9: 2196, 5: 2190, 7: 2184, 8: 2166, 1: 1950, 4: 1926}) 1 1950 2 2766 3 2334 4 1926 5 2190 6 2244 7 2184 8 2166 9 2196\n像素数组最多次对比成功的数是2（2766次），并输入图像中的数也是2。因此，我们成功地识别图像。\n再测试一个的示例：\nlist index out of range Counter({9: 3060, 3: 2598, 8: 2514, 4: 2454, 6: 2388, 5: 2358, 7: 2340, 2: 2298, 1: 1878}) 1 1878 2 2298 3 2598 4 2454 5 2358 6 2388 7 2340 8 2514 9 3060\n像素数组最多次对比成功的数是9（3060次），并输入图像中的数也是9。因此，我们成功地识别图像。\n接下来，我们使用SciKitLearn提供的数据集，用Support Vector machine来进行分类。 SVM是监督学习分类器，它的工作原理是在我们给出一些标签数据之后，才开始对数据分类。\n#导入matplotlib、sklearn import matplotlib.pyplot as plt from sklearn import datasets from sklearn import svm #skLearn带有数据集，导入这些数据集 digits =datasets.load_digits() #使用SVM分类器 clf =svm.SVC(gamma =0.001,C=100) # SVM是我们要用的分类器，digits.data是图像数组的特征集，digits.target是实际的数字 #digits.target是我们已经赋值给digits.data的标签 #取图像数组的特征集和标签的最后40个数据赋值给x，y X,y =digits.data[:-40],digits.target[:-40] #给数据和相应的图像数组数据贴标签 clf.fit(X,y) # 训练数据集并拟合，输出结果并画图，预测第42个数。 print (\" Prediction: \" , clf.predict(digits.data[-42].reshape(1,-1))) plt.imshow(digits.images[-42],cmap =plt.cm.gray_r,interpolation ='nearest') plt.show() #colormaps #gray：0-255 级灰度，0：黑色，1：白色，黑底白字； #gray_r：翻转 gray 的显示，如果 gray 将图像显示为黑底白字，gray_r 会将其显示为白底黑字；\n运行结果如下：\nSVM进行预测并正确标记，得到了数字6，也就是第42个数的值！\n输入的第42个数的图片如下：\n\n3980元团购原价19800元的AI课程，团购请加王家林老师微信13928463918\n基于王家林老师独创的人工智能“项目情景投射”学习法，任何IT人员皆可在无需数学和Python语言的基础上的情况下3个月左右的时间成为AI技术实战高手:\n1，五节课（分别在4月9-13号早上YY视频直播）教你从零起步(无需Python和数学基础)开发出自己的AI深度学习框架，五节课的学习可能胜过你五年的自我摸索；\n2，30个真实商业案例代码中习得AI(从零起步到AI实战专家之路)：10大机器学习案例、13大深度学习案例、7大增强学习案例(本文档中有案例的详细介绍和案例实现代码截图)；\n3，100天的涅槃蜕变，平均每天学习1个小时(周一到周六早上6:00-7:00YY频道68917580视频直播)，周末复习每周的6个小时的直播课程(报名学员均可获得所有的直播视频、全部的商业案例完整代码、最具阅读价值的AI资料等)。\n五节课从零起步(无需数学和Python基础)编码实现AI人工智能框架电子书V1\nhttps://download.csdn.net/download/duan_zhihua/10380279\n30个真实商业案例代码中成为AI实战专家(10大机器学习案例、13大深度学习案例、7大增强学习案例)课程大纲\nhttps://download.csdn.net/download/duan_zhihua/10366665\n在人工智能上花一年时间，这足以让人相信上帝的存在。\n——艾伦·佩利"}
{"content2":"编者按 ： 罗布·史密斯（RobSmith）是 Pecabu 首席执行官\n编者按 ： 罗布·史密斯（RobSmith）是 Pecabu 首席执行官。\n人工智能（AI）近来成为媒体报道的热点话题。它拥有像“大数据”、“云”等毫无意义的流行词语的地位，也只是时间问题。通常情况下，我是人工智能的坚定支持者。在人工智能领域，任何一点对我们这个常常被忽视的行业的关注都是受欢迎的一件事。但在这个领域，误传的事情看起来要多过确凿事实。\n公众似乎都将人工智能看作是科技行业神秘的“紫色独角兽”，用难以捉摸、神秘、强大、危险这样的词汇来形容它。尽管科技界在这个问题上存在大量争论，但我至少可以告诉你有关人工智能的一些误区。\n首先，人工智能没什么让人感到害怕的。它不是像 SkyNet 一样的科学，也不是像 HAL 一样的邪恶红色灯泡。人工智能不过是一个计算机项目，只是智能化水平非常高，其所能完成的任务甚至到了通常需要人类分析的地步。也就是说，它并不是机械化、无处不在的战争机器。\n其次，人工智能并不是活着的东西。虽然人工智能可以执行本由人类来完成的任务，但它们并不像我们一样，是“活着”的东西。除了我们给人工智能编入的程序，或是它们从环境中发现的东西以外，人工智能没有任何真正的创造力、情感或渴望。不同于科幻作品中描述的情形，人工智能并没有交配、繁殖或组建小家庭的渴望。\n第三，人工智能一般没有什么野心。在非常有限的情况下，人工智能可以像人类一样自主思考和处理任务，这的确是事实，但它的一般用途及存在的原因最终都由我们来定义。同任何程序或技术一样，我们可以定义人工智能在人类社会中的作用。大家放心吧，人工智能无意奴役人类，也无意作为我们的君主统治人类世界。\n第四，人工智能并不是单一的实体。计算机程序，甚至是人工智能程序，它们作为“行家里手”的作用远远超过其作为“多面手”的作用。若想在我们有生之年实现人工智能的梦想，一个可能性更大的方案是，通过一个子程序网络来处理视觉（计算机视觉）、语言（NLP）、适应性（机器学习）、运动（机器人技术）等等。人工智能不是“他”或“她”，甚至不是“它”，而更像是“他们或它们”。\n最后，同所有计算机程序一样，人工智能最终是由人类来控制的。当然，人类可以将人工智能设计成具有恶意企图的工具，像核技术或生物计划一样具有攻击性，但这并不是这门科学的错误，而是我们人类自身的问题。\n虽然伊隆·马斯克（ElonMusk）是我心目中的英雄，从许多方面讲都是一位天才，但他最近有关人工智能的 一席话 却并不聪明。马斯克称，人工智能比核武器还危险，我们可能会招来人工智能“魔鬼”（这是他的原话，可不是我说的）。对此，我唯一的解释是，他应该是在看《终结者》的时候睡着了。\n与此同时，IBM、苹果和谷歌等科技巨头都在开发下一代人工智能应用，用小小的人工智能专用代码来代替人类在大量无聊、危险且耗时任务中的作用。这些都是非常专业的、几乎像隧道视觉一样的程序，只会改善我们的社会，而不是灌输恐惧心理。\n依我看，我们当前距离马斯克所说的人工智能“奇点”还得有数十年的时间，即便到那时，人工智能的最终特性也取决于我们的意图。如果说历史能告诉我们什么的话，那就是一旦人工智能时代真的到来，我们不应该恐惧人工智能本身，而应该害怕操控人工智能的人。"}
{"content2":"要谈mAP必须先说一下，精准率(Precision， P值)和召回率(Recall，R值).P值和R值最初是信息检索领域的评价指标.\n准确率Accuracy\n这个概念是指分类的准确率，也就是分类准确的样本与样本总数之比.\n举个例子： 假设一个样本集中有100个样本，其中99个负样本，1个正样本.\n碰巧某个分类器的原理就是所有样本均判为负，那个这个分类器的accuracy就是99%.这比很多算法厉害的分类器都厉害.但是仔细想想，如果总样本中只有1个负样本但是有99个正样本，那个这个分类器，那个这个分类器的正确率将是1%.\n由此可以发现，accuracy的值与样本集中正负样本的比例关系很大.通常情况下，我们希望的算法是在不同场景下表现都比较好的算法，也就是泛化能力强的算法，显然accuracy并不能作为算法泛化能力的衡量.\nP值和R值和F值\nPrecision中文可以翻译为“查准率”（有时候也翻译为“准确率”），也就是返回的结果中希望得到的结果所占的比例.\nRecall中文可以翻译为“查全率”或者通常说的“召回率”，也就是返回的结果希望得到的结果占总样本中所有希望的得到的结果的比例.\n这两个值都是标量，需要进行计算.数学通常希望把一个概念数字化，于是想想一种场景:比如一个搜索引擎中共有n条数据，关于“图像语义分割”的数据有m条，经过搜索，返回了k条.这k条中有一部分是和“图像语义分割”相关的，也有一部分不是.于是就有了以下几个量:\n很容易可以看出，查准率P值和查全率(召回率)R值的值应该等于：\n简单理解一下，其实P值就是找的准，R值就是找的全.理想情况下如果以R值为横坐标，P值就一直为1.这样两者的乘积就是1.(其实这种图就是P-R图，可能有的人不太理解里边的原理，下边会说).但是实际中\n往往P值和R值是矛盾的.比如，把所有结果都返回，则R值为100%，但是此时P值就很小；如果只取一个结果且是期望的，那么P值为100%，但是此时显然没有选出全部的（R值很小）.\n通常情况下，对于一个模型的P值和R值需要综合考虑（为什么？既然提出来了两个，肯定都要考虑一下了，就像山就在那里）.这个时候就引出了 F-measure ，也就是F值.\n关于F值这里就不多说了，其中参数beta是根据不同的场景进行调节的.更常用的是F1度量:\nmAP\n本文是要讲mAP的，终于前面铺垫了这么多，可以开始讲mAP了.但是，为了讲述mAP，这里还需要先说一下如何画P-R图.\nP-R图：制定一个类别，根据学习器下每个样本对这个类的预测结果进行排序，排在前面的是学习器认为“最可能”的正例样本，最后的是“最不可能的”.按照此顺序逐个把样本作为正例进行预测（1个正例，2个正例，…），则每次可以计算出当前的R值和P值，然后以R值为横轴、P值为纵轴记性作图.就得到了P-R图.\n应该比较容易想到B系统比C系统在这个场景下表现好(自己想想为什么).\n为了能够量化分类器的性能比较，通常采用Average Precison来作为度量标准， 连续图中AP应该是P对R在[0,1]上的积分，实际上P-R图中的值是离散值，化为sum即可.\n而mAP是对所有类别的AP值的平均，也就是所有的AP值求和除以类数.顺便说一句，average是也就是对recall取平均的，mean是对类取平均.\n现在的图像分类论文基本都是用mAP作为标准，PASCALVOC的多个屏幕项目也都是使用mAP作为标准的.\n图片来自西瓜书"}
{"content2":"Jia-Bin Huang同学收集了很多计算机视觉方面的代码，链接如下：\nhttps://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n这些代码很实用，可以让我们站在巨人的肩膀上~~\nTopic\nResources\nReferences\nFeature Extraction\nSIFT [1] [Demo program][SIFT Library] [VLFeat]\nPCA-SIFT [2] [Project]\nAffine-SIFT [3] [Project]\nSURF [4] [OpenSURF] [Matlab Wrapper]\nAffine Covariant Features [5] [Oxford project]\nMSER [6] [Oxford project] [VLFeat]\nGeometric Blur [7] [Code]\nLocal Self-Similarity Descriptor [8] [Oxford implementation]\nGlobal and Efficient Self-Similarity [9] [Code]\nHistogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]\nGIST [11] [Project]\nShape Context [12] [Project]\nColor Descriptor [13] [Project]\nPyramids of Histograms of Oriented Gradients [Code]\nSpace-Time Interest Points (STIP) [14] [Code]\nBoundary Preserving Dense Local Regions [15][Project]\n1.    D. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004. [PDF]\n2.    Y. Ke and R. Sukthankar, PCA-SIFT: A More Distinctive Representation for Local Image Descriptors,CVPR, 2004. [PDF]\n3.    J.M. Morel and G.Yu, ASIFT, A new framework for fully affine invariant image comparison. SIAM Journal on Imaging Sciences, 2009. [PDF]\n4.    H. Bay, T. Tuytelaars and L. V. Gool SURF: Speeded Up Robust Features,ECCV, 2006. [PDF]\n5.    K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir and L. Van Gool, A comparison of affine region detectors. IJCV, 2005. [PDF]\n6.    J. Matas, O. Chum, M. Urba, and T. Pajdla. Robust wide baseline stereo from maximally stable extremal regions. BMVC, 2002. [PDF]\n7.    A. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition using low distortion correspondences. CVPR, 2005. [PDF]\n8.    E. Shechtman and M. Irani. Matching local self-similarities across images and videos, CVPR, 2007. [PDF]\n9.    T. Deselaers and V. Ferrari. Global and Efficient Self-Similarity for Object Classification and Detection. CVPR 2010. [PDF]\n10.  N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005. [PDF]\n11.  A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope, IJCV, 2001. [PDF]\n12.  S. Belongie, J. Malik and J. Puzicha. Shape matching and object recognition using shape contexts, PAMI, 2002. [PDF]\n13.  K. E. A. van de Sande, T. Gevers and Cees G. M. Snoek, Evaluating Color Descriptors for Object and Scene Recognition, PAMI, 2010.\n14.  I. Laptev, On Space-Time Interest Points, IJCV, 2005. [PDF]\n15.  J. Kim and K. Grauman, Boundary Preserving Dense Local Regions, CVPR 2011. [PDF]\nImage Segmentation\n·         Normalized Cut [1] [Matlab code]\n·         Gerg Mori' Superpixel code [2] [Matlab code]\n·         Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]\n·         Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]\n·         OWT-UCM Hierarchical Segmentation [5] [Resources]\n·         Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]\n·         Quick-Shift [7] [VLFeat]\n·         SLIC Superpixels [8] [Project]\n·         Segmentation by Minimum Code Length [9] [Project]\n·         Biased Normalized Cut [10] [Project]\n·         Segmentation Tree [11-12] [Project]\n·         Entropy Rate Superpixel Segmentation [13] [Code]\n1.    J. Shi and J Malik, Normalized Cuts and Image Segmentation, PAMI, 2000 [PDF]\n2.    X. Ren and J. Malik. Learning a classification model for segmentation.ICCV, 2003. [PDF]\n3.    P. Felzenszwalb and D. Huttenlocher. Efficient Graph-Based Image Segmentation, IJCV 2004. [PDF]\n4.    D. Comaniciu, P Meer. Mean Shift: A Robust Approach Toward Feature Space Analysis. PAMI 2002. [PDF]\n5.    P. Arbelaez, M. Maire, C. Fowlkes and J. Malik. Contour Detection and Hierarchical Image Segmentation. PAMI, 2011. [PDF]\n6.    A. Levinshtein, A. Stere, K. N. Kutulakos, D. J. Fleet, S. J. Dickinson, and K. Siddiqi, TurboPixels: Fast Superpixels Using Geometric Flows, PAMI 2009. [PDF]\n7.    A. Vedaldi and S. Soatto, Quick Shift and Kernel Methodsfor Mode Seeking,ECCV, 2008. [PDF]\n8.    R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk, SLIC Superpixels, EPFL Technical Report, 2010. [PDF]\n9.    A. Y. Yang, J. Wright, S. Shankar Sastry, Y. Ma , Unsupervised Segmentation of Natural Images via Lossy Data Compression, CVIU, 2007. [PDF]\n10.  S. Maji, N. Vishnoi and J. Malik, Biased Normalized Cut, CVPR 2011\n11.  E. Akbas and N. Ahuja, “From ramp discontinuities to segmentation tree,”  ACCV 2009. [PDF]\n12.  N. Ahuja, “A Transform for Multiscale Image Segmentation by Integrated Edge and Region Detection,” PAMI 1996 [PDF]\n13.  M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, Entropy Rate Superpixel Segmentation, CVPR 2011 [PDF]\nObject Detection\n·         A simple object detector with boosting [Project]\n·         INRIA Object Detection and Localization Toolkit [1] [Project]\n·         Discriminatively Trained Deformable Part Models [2] [Project]\n·         Cascade Object Detection with Deformable Part Models [3] [Project]\n·         Poselet [4] [Project]\n·         Implicit Shape Model [5] [Project]\n·         Viola and Jones's Face Detection [6] [Project]\n1.    N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005. [PDF]\n2.    P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan.\nObject Detection with Discriminatively Trained Part Based Models, PAMI, 2010 [PDF]\n3.    P. Felzenszwalb, R. Girshick, D. McAllester. Cascade Object Detection with Deformable Part Models. CVPR 2010 [PDF]\n4.    L. Bourdev, J. Malik, Poselets: Body Part Detectors Trained Using 3D Human Pose Annotations, ICCV 2009 [PDF]\n5.    B. Leibe, A. Leonardis, B. Schiele. Robust Object Detection with Interleaved Categorization and Segmentation, IJCV, 2008. [PDF]\n6.    P. Viola and M. Jones, Rapid Object Detection Using a Boosted Cascade of Simple Features, CVPR 2001. [PDF]\nSaliency Detection\n·         Itti, Koch, and Niebur' saliency detection [1] [Matlab code]\n·         Frequency-tuned salient region detection [2] [Project]\n·         Saliency detection using maximum symmetric surround [3] [Project]\n·         Attention via Information Maximization [4] [Matlab code]\n·         Context-aware saliency detection [5] [Matlab code]\n·         Graph-based visual saliency [6] [Matlab code]\n·         Saliency detection: A spectral residual approach. [7] [Matlab code]\n·         Segmenting salient objects from images and videos. [8] [Matlab code]\n·         Saliency Using Natural statistics. [9] [Matlab code]\n·         Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]\n·         Learning to Predict Where Humans Look [11] [Project]\n·         Global Contrast based Salient Region Detection [12] [Project]\n1.    L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, 1998. [PDF]\n2.    R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In CVPR, 2009. [PDF]\n3.    R. Achanta and S. Susstrunk. Saliency detection using maximum symmetric surround. In ICIP, 2010. [PDF]\n4.    N. Bruce and J. Tsotsos. Saliency based on information maximization. InNIPS, 2005. [PDF]\n5.    S. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware saliency detection. In CVPR, 2010. [PDF]\n6.    J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. NIPS, 2007. [PDF]\n7.    X. Hou and L. Zhang. Saliency detection: A spectral residual approach.CVPR, 2007. [PDF]\n8.    E. Rahtu, J. Kannala, M. Salo, and J. Heikkila. Segmenting salient objects from images and videos. CVPR, 2010. [PDF]\n9.    L. Zhang, M. Tong, T. Marks, H. Shan, and G. Cottrell. Sun: A bayesian framework for saliency using natural statistics. Journal of Vision, 2008. [PDF]\n10.  D. Gao and N. Vasconcelos, Discriminant Saliency for Visual Recognition from Cluttered Scenes, NIPS, 2004. [PDF]\n11.  T. Judd and K. Ehinger and F. Durand and A. Torralba, Learning to Predict Where Humans Look, ICCV, 2009. [PDF]\n12.  M.-M. Cheng, G.-X. Zhang, N. J. Mitra, X. Huang, S.-M. Hu. Global Contrast based Salient Region Detection. CVPR 2011.\nImage Classification\n·         Pyramid Match [1] [Project]\n·         Spatial Pyramid Matching [2] [Code]\n·         Locality-constrained Linear Coding [3] [Project] [Matlab code]\n·         Sparse Coding [4] [Project] [Matlab code]\n·         Texture Classification [5] [Project]\n·         Multiple Kernels for Image Classification [6] [Project]\n·         Feature Combination [7] [Project]\n·         SuperParsing [Code]\n1.    K. Grauman and T. Darrell, The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features, ICCV 2005. [PDF]\n2.    S. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories, CVPR 2006[PDF]\n3.    J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained Linear Coding for Image Classification, CVPR, 2010 [PDF]\n4.    J. Yang, K. Yu, Y. Gong, T. Huang, Linear Spatial Pyramid Matching using Sparse Coding for Image Classification, CVPR, 2009 [PDF]\n5.    M. Varma and A. Zisserman, A statistical approach to texture classification from single images, IJCV2005. [PDF]\n6.    A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman, Multiple Kernels for Object Detection. ICCV, 2009. [PDF]\n7.    P. Gehler and S. Nowozin, On Feature Combination for Multiclass Object Detection, ICCV, 2009. [PDF]\n8.    J. Tighe and S. Lazebnik, SuperParsing: Scalable Nonparametric Image\nParsing with Superpixels, ECCV 2010. [PDF]\nCategory-Independent Object Proposal\n·         Objectness measure [1] [Code]\n·         Parametric min-cut [2] [Project]\n·         Object proposal [3] [Project]\n1.    B. Alexe, T. Deselaers, V. Ferrari, What is an Object?, CVPR 2010 [PDF]\n2.    J. Carreira and C. Sminchisescu. Constrained Parametric Min-Cuts for Automatic Object Segmentation, CVPR 2010. [PDF]\n3.    I. Endres and D. Hoiem. Category Independent Object Proposals, ECCV 2010. [PDF]\nMRF\n·         Graph Cut [Project] [C++/Matlab Wrapper Code]\n1.    Y. Boykov, O. Veksler and R. Zabih, Fast Approximate Energy Minimization via Graph Cuts, PAMI 2001 [PDF]\nShadow Detection\n·         Shadow Detection using Paired Region [Project]\n·         Ground shadow detection [Project]\n1.    R. Guo, Q. Dai and D. Hoiem, Single-Image Shadow Detection and Removal using Paired Regions, CVPR 2011 [PDF]\n2.    J.-F. Lalonde, A. A. Efros, S. G. Narasimhan, Detecting Ground Shadowsin Outdoor Consumer Photographs, ECCV 2010 [PDF]\nOptical Flow\n·         Kanade-Lucas-Tomasi Feature Tracker [C Code]\n·         Optical Flow Matlab/C++ code by Ce Liu [Project]\n·         Horn and Schunck's method by Deqing Sun [Code]\n·         Black and Anandan's method by Deqing Sun [Code]\n·         Optical flow code by Deqing Sun [Matlab Code] [Project]\n·         Large Displacement Optical Flow by Thomas Brox [Executable for 64-bit Linux] [ Matlab Mex-functions for 64-bit Linux and 32-bit Windows] [Project]\n·         Variational Optical Flow by Thomas Brox [Executable for 64-bit Linux] [ Executable for 32-bit Windows ] [ Matlab Mex-functions for 64-bit Linux and 32-bit Windows ] [Project]\n1.    B.D. Lucas and T. Kanade, An Iterative Image Registration Technique with an Application to Stereo Vision, IJCAI 1981. [PDF]\n2.    J. Shi, C. Tomasi, Good Feature to Track, CVPR 1994. [PDF]\n3.    C. Liu. Beyond Pixels: Exploring New Representations and Applications for Motion Analysis. Doctoral Thesis. MIT 2009. [PDF]\n4.    B.K.P. Horn and B.G. Schunck, Determining Optical Flow, Artificial Intelligence 1981. [PDF]\n5.    M. J. Black and P. Anandan, A framework for the robust estimation of optical flow, ICCV 93. [PDF]\n6.    D. Sun, S. Roth, and M. J. Black, Secrets of optical flow estimation and their principles, CVPR 2010. [PDF]\n7.    T. Brox, J. Malik, Large displacement optical flow: descriptor matching in variational motion estimation, PAMI, 2010 [PDF]\n8.    T. Brox, A. Bruhn, N. Papenberg, J. Weickert, High accuracy optical flow estimation based on a theory for warping, ECCV 2004 [PDF]\nObject Tracking\n·         Particle filter object tracking [1] [Project]\n·         KLT Tracker [2-3] [Project]\n·         MILTrack [4] [Code]\n·         Incremental Learning for Robust Visual Tracking [5] [Project]\n·         Online Boosting Trackers [6-7] [Project]\n·         L1 Tracking [8] [Matlab code]\n1.    P. Perez, C. Hue, J. Vermaak, and M. Gangnet. Color-Based Probabilistic Tracking ECCV, 2002. [PDF]\n2.    B.D. Lucas and T. Kanade, An Iterative Image Registration Technique with an Application to Stereo Vision, IJCAI 1981. [PDF]\n3.    J. Shi, C. Tomasi, Good Feature to Track, CVPR 1994. [PDF]\n4.    B. Babenko, M. H. Yang, S. Belongie, Robust Object Tracking with Online Multiple Instance Learning, PAMI 2011 [PDF]\n5.    D. Ross, J. Lim, R.-S. Lin, M.-H. Yang, Incremental Learning for Robust Visual Tracking, IJCV 2007 [PDF]\n6.    H. Grabner, and H. Bischof, On-line Boosting and Vision, CVPR 2006 [PDF]\n7.    H. Grabner, C. Leistner, and H. Bischof, Semi-supervised On-line Boosting for Robust Tracking, ECCV 2008 [PDF]\n8.    X. Mei and H. Ling, Robust Visual Tracking using L1 Minimization, ICCV, 2009. [PDF]\nImage Matting\n·         Closed Form Matting [Code]\n·         Spectral Matting [Project]\n·         Learning-based Matting [Code]\n1.    A. Levin D. Lischinski and Y. Weiss. A Closed Form Solution to Natural Image Matting, PAMI 2008 [PDF]\n2.    A. Levin, A. Rav-Acha, D. Lischinski. Spectral Matting. PAMI 2008. [PDF]\n3.    Y. Zheng and C. Kambhamettu, Learning Based Digital Matting, ICCV 2009 [PDF]\nBilateral Filtering\n·         Fast Bilateral Filter [Project]\n·         Real-time O(1) Bilateral Filtering [Code]\n·         SVM for Edge-Preserving Filtering [Code]\n1.    Q. Yang, K.-H. Tan and N. Ahuja,  Real-time O(1) Bilateral Filtering,\nCVPR 2009. [PDF]\n2.    Q. Yang, S. Wang, and N. Ahuja, SVM for Edge-Preserving Filtering,\nCVPR 2010. [PDF]\nImage Denoising\n·         K-SVD [Matlab code]\n·         BLS-GSM [Project]\n·         BM3D [Project]\n·         FoE [Code]\n·         GFoE [Code]\n·         Non-local means [Code]\n·         Kernel regression [Code]\nImage Super-Resolution\n·         MRF for image super-resolution [Project]\n·         Multi-frame image super-resolution [Project]\n·         UCSC Super-resolution [Project]\n·         Sprarse coding super-resolution [Code]\nImage Deblurring\n·         Eficient Marginal Likelihood Optimization in Blind Deconvolution [Code]\n·         Analyzing spatially varying blur [Project]\n·         Radon Transform [Code]\nImage Quality Assessment\n·         FSIM [1] [Project]\n·         Degradation Model [2] [Project]\n·         SSIM [3] [Project]\n·         SPIQA [Code]\n1.    L. Zhang, L. Zhang, X. Mou and D. Zhang, FSIM: A Feature Similarity Index for Image Quality Assessment, TIP 2011. [PDF]\n2.    N. Damera-Venkata, and T. D. Kite, W. S. Geisler, B. L. Evans, and A. C. Bovik,Image Quality Assessment Based on a Degradation Model, TIP 2000. [PDF]\n3.    Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, TIP 2004. [PDF]\n4.    B. Ghanem, E. Resendiz, and N. Ahuja, Segmentation-Based Perceptual Image Quality Assessment (SPIQA), ICIP 2008. [PDF]\nDensity Estimation\n·         Kernel Density Estimation Toolbox [Project]\nDimension Reduction\n·         Dimensionality Reduction Toolbox [Project]\n·         ISOMAP [Code]\n·         LLE [Project]\n·         LaplacianEigenmaps [Code]\n·         Diffusion maps [Code]\nSparse Coding\nLow-Rank Matrix Completion\nNearest Neighbors matching\n·         ANN: Approximate Nearest Neighbor Searching [Project] [Matlab wrapper]\n·         FLANN: Fast Library for Approximate Nearest Neighbors [Project]\nSteoreo\n·         StereoMatcher [Project]\n1.    D. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms, IJCV 2002 [PDF]\nStructure from motion\n·         Boundler [1] [Project]\n1.    N. Snavely, S. M. Seitz, R. Szeliski. Photo Tourism: Exploring image collections in 3D. SIGGRAPH, 2006. [PDF]\nDistance Transformation\n·         Distance Transforms of Sampled Functions [1] [Project]\n1.    P. F. Felzenszwalb and D. P. Huttenlocher. Distance transforms of sampled functions. Technical report, Cornell University, 2004. [PDF]\nChamfer Matching\n·         Fast Directional Chamfer Matching [Code]\n1.    M.-Y. Liu, O. Tuzel, A. Veeraraghavan, and R. Chellappa, Fast Directional Chamfer Matching, CVPR 2010 [PDF]\nClustering\n·         K-Means [VLFeat] [Oxford code]\n·         Spectral Clustering [UW Project][Code] [Self-Tuning code]\n·         Affinity Propagation [Project]\nClassification\n·         SVM [Libsvm] [SVM-Light] [SVM-Struct]\n·         Boosting\n·         Naive Bayes\nRegression\n·         SVM\n·         RVM\n·         GPR\nMultiple Kernel Learning (MKL)\n·         SHOGUN [Project]\n·         OpenKernel.org [Project]\n·         DOGMA (online algorithms) [Project]\n·         SimpleMKL [Project]\n1.    S. Sonnenburg, G. Rätsch, C. Schäfer, B. Schölkopf . Large scale multiple kernel learning. JMLR, 2006. [PDF]\n2.    F. Orabona and L. Jie. Ultra-fast optimization algorithm for sparse multi kernel learning. ICML, 2011. [PDF]\n3.    F. Orabona, L. Jie, and B. Caputo. Online-batch strongly convex multi kernel learning. CVPR, 2010. [PDF]\n4.    A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. Simplemkl. JMRL, 2008. [PDF]\nMultiple Instance Learning (MIL)\n·         MIForests [1] [Project]\n·         MILIS [2]\n·         MILES [3] [Project] [Code]\n·         DD-SVM [4] [Project]\n1.    C. Leistner, A. Saffari, and H. Bischof, MIForests: Multiple-Instance Learning with Randomized Trees, ECCV 2010. [PDF]\n2.    Z. Fu, A. Robles-Kelly, and J. Zhou, MILIS: Multiple instance learning with instance selection, PAMI 2010. [PDF]\n3.    Y. Chen, J. Bi and J. Z. Wang, MILES: Multiple-Instance Learning via Embedded Instance Selection. PAMI 2006 [PDF]\n4.    Yixin Chen and James Z. Wang, Image Categorization by Learning and Reasoning with Regions, JMLR 2004. [PDF]\nOther Utilities\n·         Code for downloading Flickr images, by James Hays [Code]\n·         The LightspeedMatlab Toolbox by Tom Minka [Code]\n·         MATLAB Functions for Multiple View Geometry [Code]\n·         Peter's Functions for Computer Vision [Code]\n·         Statistical Pattern Recognition Toolbox [Code]\nUseful Links(dataset, lectures, and other softwares)\nConference Information\n·         Computer Image Analysis, Computer Vision Conferences\nPapers\n·         Computer vision paper on the web\n·         NIPS Proceedings\nDatasets\n·         Compiled list of recognition datasets\n·         Computer vision dataset from CMU\nLectures\n·         Videolectures\nSource Codes\n·         Computer Vision Algorithm Implementations\n·         OpenCV\n·         Source Code Collection for Reproducible Research\n图像处理：\n全局特征\n局部特征\n图像质量评价\n显著性检测\n图像滤波\nIP: Image Process\nGlobal Feature\nLocal Feature\nImage Quality Analysis\nSalience Detection\nImage Filtering\nYear\nTopic\nMethod\nReference (Formal)\n2009\nGlobal Feature\nPHOG: Pyramids of Histograms of Oriented Gradients\nA. Bosch, A. Zisserman, and X. Munoz, Representing shape with a spatial pyramid kernel, CIVR, 2007\n2009\nGlobal Feature\nGist\nA. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope, IJCV, 2001\n2009\nLocal Feature\nSIFT: Scale Invariant Feature Transform\nD. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004.\n2010\nLocal Feature\nAffine-SIFT: Affine-Scale Invariant Feature Transform\nJ.M. Morel and G.Yu, ASIFT, A new framework for fully affine invariant image comparison. SIAM Journal on Imaging Sciences, 2009\n2011\nLocal Feature\nLBP: Local Binary Pattern\nM. Pietikainen and J. Heikkila, CVPR 2011 Tutorial\n2012\nLocal Feature\nPCA-SIFT: Principal Component Analysis - Scale Invariant Feature Transform\nY. Ke and R. Sukthankar, PCA-SIFT: A More Distinctive Representation for Local Image Descriptors,CVPR, 2004\n2012\nLocal Feature\nSC: Shape Context\nS. Belongie, J. Malik and J. Puzicha. Shape matching and object recognition using shape contexts, PAMI, 2002\n2012\nImage Quality Analysis\nSSim: Structure Similarity\nImage quality assessment: from error visibility to structural similarity [J]. IEEE Trans. Image Process, 2004, 13(4): 600–612.\n2012\nImage Quality Analysis\nIW-SSim: Information Content Weighted Structure Similarity\nZ. Wang and Q. Li, \"Information content weighting for perceptual image quality assessment,\" IEEE Transactions on Image Processing, vol. 20, no 5, pp. 1185-1198, May 2011.\n2012\nImage Quality Analysis\nMS-SSim: Multi-scale Structure Similarity\nWang Z, Simoncelli E P, Bovik A C. Multi-scale  structural similarity for image quality assessment [J].  Proc. IEEE Asilomar Conf. Signals, Syst.Comput., 2003:. 1398–1402.\n2012\nImage Quality Analysis\nMSE：Mean Square Error\nWang Z,  Bovik A C, Sheikh H R, et al. Image quality assessment: from error visibility to structural similarity [J]. IEEE Trans. Image Process, 2004, 13(4): 600–612.\n2012\nImage Quality Analysis\nVSNR: Visual Signal-to-Noise Ratio\nChandler D M, Hemami S S. VSNR: a Wavelet based visual signal-to-noise ratio for  natural images [J]. IEEE Trans. Image Process, 2007,16(9): 2284–2298.\n2012\nImage Quality Analysis\n3-SSIM: 3 -Chanle Structure Similarity\nLi C and  Bovik A C. Three-component weighted structural similarity index[C]\\\\ Proceedings of the International Society for Optical Engineering, 2009.\n2012\nImage Resizing\nContext-Aware:Context\nShai Avidan, Ariel Shamir. Seam carving for content-aware image resizing. ACM SIGGRAPH '07. 26(3). 2007\n2012\nSalience Detection\nItti Model\nItti, L. A model of saliency-based visual attention for rapid scene analysis . Pattern Analysis and Machine Intelligence, IEEE Transactions on. 20(11): 1254 - 1259. 1998.\n2012\nSalience Detection\nMSSS: Saliency Detection using Maximum Symmetric\nAchanta, R.; Süsstrunk, S. Saliency detection using maximum symmetric surround. Image Processing (ICIP), 2010 17th IEEE International Conference on. 2653 - 2656, 2010.\n2012\nSalience Detection\nAIM: Attention based on Information Maximization\nBruce, N.D.B., Tsotsos, J.K., Saliency Based on Information Maximization. Advances in Neural Information Processing Systems, 18, pp. 155-162, June 2006. Selected for oral presentation\n2012\nSalience Detection\nSF: Saliency Filters: Contrast Based Filtering for Salient Region Detection\nPerazzi, F. Krahenbuhl, P. ; Pritch, Y. ; Hornung, A. Saliency Filters: Contrast Based Filtering for Salient Region Detection. Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. 733 - 740. 2012\n2012\nSalience Detection\nSR: Sspectral Residual\nXiaodi Hou; Liqing Zhang. Saliency Detection: A Spectral Residual Approach. Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on. 1-8. 2007.\n2012\nSalience Detection\nHC: Histogram-based Contrast,  RC: Region-based Contrast\nM.-M. Cheng, G.-X. Zhang, N. J. Mitra, X. Huang, S.-M. Hu. Global Contrast based Salient Region Detection. CVPR, 2011\n2012\nSalience Detection\nCRF: Conditional Random Field\nTie Liu; Jian Sun; Nan-Ning Zheng; Xiaoou Tang; Heung-Yeung Shum. Learning to Detect A Salient Object. Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on. 1-8. 2007.\n2012\nSalience Detection\nIG: Interest Gaussian\nR. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In CVPR, 2009\n2012\nSalience Detection\nContext-Aware:Context\nS. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware saliency detection. In CVPR, 2010.\n2012\nSalience Detection\nSalient region detection and segmentation.\nR. Achanta, F. Estrada, P. Wils, and S. S¨usstrunk. Salient region detection and segmentation. In ICVS, pages 66–75. Springer, 2008. 410, 412, 414\n2012\nSalience Detection\nGBVS：Graph-Based Visual Saliency\nJ. Harel, C. Koch, and P. Perona. Graph-based visua saliency. In NIPS, pages 545–552, 2006. 410, 412, 414\n2012\nSalience Detection\nSUN：Saliency Using Natural statistics\nA Bayesian Framework for Saliency Using Natural Statistics\n2012\nSalience Detection\nFuzzy Growing\nY.-F. Ma and H.-J. Zhang, “Contrast-based image attention analysis by using fuzzy growing,” ACM International Conference on Multimedia, pp. 374–381, November 2003.\n2012\nSalience Detection\nDSD：Discriminant Saliency Detector\nAchanta, R. Discriminant Saliency for Visual Recognition from Cluttered Scenes[C]/Proc. Of IEEE  Conference Publications. On Hong Kong IEEE press. 2010,Pages: 2653 - 2656\n2012\nSalience Detection\nHS：Human Saliency\nJudd, T. Ehinger, K. Learning to Predict Where Humans Look[C]/Proc. Of IEEE  Conference Publications. On Kyoto ,Pages:2106 - 2113\n2009\nImage Filtering\nBF: Bilateral Filtering\nS. Paris and F. Durand, A Fast Approximation of the Bilateral Filter using a Signal Processing Approach, ECCV, 2006\n2012\nImage Filtering\nBF: Bilateral Filtering\nQ. Yang, K.-H. Tan and N. Ahuja,  Real-time O(1) Bilateral Filtering, CVPR 2009\n2012\nImage Filtering\nBF: Bilateral Filtering\nQ. Yang, S. Wang and N. Ahuja , Real-time Specular Highlight Removal Using Bilateral Filtering, ECCV 2010\n2009\nImage Filtering\nBF: Bilateral Filtering\nS. Paris and F. Durand, A Fast Approximation of the Bilateral Filter using a Signal Processing Approach, ECCV, 2006\n\n\n机器学习：\n判决模型\n生成模型\n图模型\n聚类\n流形\n核方法\n距离函数\n迁移学习\n集成学习\nML: Machine Learning\nDiscriminative Model\nGenerated Model\nGraph Model\nClustering\nManifold\nKernel\nDistance\nTransfer Learning\nEnsemble Learning\n2008\nDiscriminative Model\nSVM: Support Vector Machines\nC.-W. Hsu, C.-J. Lin. A simple decomposition method for support vector machines , Machine Learning 46(2002), 291-314\n2010\nDiscriminative Model\nLDA: Linear Discriminant Analysis\nC. Strecha, A. M. Bronstein, M. M. Bronstein and P. Fua. LDAHash: Improved matching with smaller descriptors, PAMI, 2011.\n2012\nDiscriminative Model\nNetlab: Networks Laboratory\nC. M. Bishop, Neural Networks for Pattern RecognitionㄝOxfordUniversity Press, 1995\n2009\nGenerated Model\nPLSA: Probabilistic Latent Semantic Analysis\nFei-Fei, L. and Perona, P., \"A Bayesian Heirarcical Model for Learning Natural Scene Categories\", Proc. CVPR, 2005.\n2010\nGenerated Model\nLDA: Latent Dirichlet Allocation\nTracking E. B. Graphical Models for Visual Object Recognition and Sudderth Doctoral Thesis, Massachusetts Institute of Technology, May 2006.\n2010\nGenerated Model\nHDP: Hierarchical Dirichlet Processes\nTargets E. Fox, E. Sudderth, and A. Willsky. Hierarchical DirichletProcesses for Tracking Maneuvering International Conference on Information Fusion, July 2007.\n2010\nGenerated Model\nTDP: Transformed Dirichlet Processes\nProcesses E. Sudderth, A. Torralba, W. Freeman, and A. Willsky. Describing Visual Scenes using Transformed Dirichlet. Neural Information Processing Systems, Dec. 2005.\n2009\nGraph Model\nCRF: Conditional Random Field, MRF: Markov Random Field\nS. V. N. Vishwanathan. Nicol N. Schraudolph. Mark W. Schmidt. Kevin P. Murphy. Accelerated training of conditional random fields with stochastic gradient methods. Proceeding ICML '06 Proceedings of the 23rd international conference on Machine learning. Pages 969 - 976. 2006.\n2009\nGraph Model\nICM: Iterated Conditional Modes\nS Li. Markov Random Field Modeling in Computer Vision Springer-Verlag, 1995\n2010\nClustering\nAP: Affinity Propagation (k-centers; k-means; klogk; mdgEM: Mixture Directional Gaussian - Exception Maximum; migEM: Mixture Isotropic Gaussian - Exception Maximum;Clusteing with Quantized/ Quantized Extension)\nClustering by Passing Messages Between Data Points. Brendan J. Frey and Delbert Dueck, Science 315, 972–976, February 2007.\n2010\nManifold\nPCA: Principal Component Analysis, LE: Laplacian Eigenmap, LLE: Local Linear Embedding, HLLE: Hessian Local Linear Embedding, Isomap: Isometric Feature Mapping\nL.J.P. van der Maaten, E.O. Postma, and H.J. van den Herik.Dimensionality Reduction: A Comparative Review. Tilburg UniversityTechnical Report, TiCC-TR 2009-005, 2009.\n2012\nKernel\nSKMsmo: Support Kernel Machine - Sequential Minimal Optimization\nBach, F.R. Lanckriet, G.R.G., Jordan , M.I. Fast Kernel Learning using Sequential Minimal Optimization . Technical Report CSD-04-1307, Division of Computer Science, University of California , Berkeley . 2004\n2012\nKernel\nSimpleMKL: Simple Multi-Kernel Learning\nA. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. Simplemkl. JMRL, 2008\n2012\nDistance\nEMD: Earth Mover's Distance\nH. Ling and K. Okada, An Efficient Earth Mover's Distance Algorithm for Robust Histogram Comparison, PAMI 2007\n2012\nDistance\nPwmetric: Pair-Wise Metric\nModeling and Estimating Persistent Motion with Geometric Flows. DahuaLin, Eric Grimson, and John Fisher. 23rd IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.\n2009\nEnsemble Learning\nBoosting\nA. Vezhnevets, O. Barinova . Avoiding Boosting Overfitting by Removing 'Confusing Samples. ECML 2007, Oral.\n2009\nEnsemble Learning\nBoosting\nTheoretical and Empirical Analysis of Diversity in Non-Stationary Learning, R. Stapenhurst and G. Brown, 2011 IEEE Symposium on Computational Intelligence in Dynamic and Uncertain Environments. 2011.\n2009\nEnsemble Learning\nAlignment\nZ. H. Zhou, W. Tang. Clusterer Ensemble [J]. Knowledge-Based Systems, 2006, 19(1): 77-83\n2012\nTransfer Learning\nCCTL: Cross Category Transfer Learning\nGuo-Jun Qi, Charu Aggarwal, Yong Rui, Qi Tian, Shiyu Chang and Thomas Huang. Towards Cross-Category Knowledge Propagation for Learning Visual Concepts, in Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2011), Colorado Springs, Colorado, June 21-23, 2011.\n2012\nTransfer Learning\nMSTR: Multi-Source Transfer Learning\nPing Luo, Fuzhen Zhuang, Hui Xiong, Yuhong Xiong, Qing He. Transfer learning from multiple source domains via consensus regularization. Proceeding CIKM '08 Proceedings of the 17th ACM conference on Information and knowledge management. Pages 103-112. 2008.\n计算机视觉：\n图像超分辨率重建\n图像配准\n图像分割\n图像抠图\n图像修补\n图像分类\n图像检索\n图像理解\n光流\n目标跟踪\n图像深度估计\n语义分析\n数据集\nCV: Computer Vision\nImage Super-Resolution\nImage Registration\nImage Segmentation\nImage Matting\nImage Inpainting\nImage Classification\nImage Retrieval\nImage Understanding\nOptical Flow\nObject Tracking\nImage Depth\nSemantic Analysis\nData Set\n2012\nImage Super-Resolution\nSuper-resolution as Sparse Representation\nJianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image Super-resolution as Sparse Representation of Raw Image Patches. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008.\n2009\nImage Registration\nBase on SIFT(Scale Invariant Feature Transform)\nD. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004.\n2011\nImage Segmentation\nSP: Super Pixcels\nX. Ren and J. Malik. Learning a classification model for segmentation. ICCV, 2003\n2012\nImage Segmentation\nGC: Graph Cut (Max Flow/ Min Cut)\nL. Gorelick, A. Delong, O. Veksler, Y. Boykov, Recursive MDL via Graph Cuts: Application to Segmentation, International Conference on Computer Vision. 2011,\n2012\nImage Segmentation\nNcut: Normal Cut\nJ. Shi and J Malik, Normalized Cuts and Image Segmentation, PAMI, 2000\n2012\nImage Matting\nClosed-Form Solution\nAnatLevin,DaniLischinski,andYairWeiss.A Closed-Form Solution to Natural Imae Matting,2006\n2012\nImage Matting\nSpectralMatting\nAnatLevin,AlexRav-Acha,andDaniLischinski. Spectral Matting,2008\n2012\nImage Matting\nKnockOut\nA. Berman, A. Dadourian, and P. Vlahos. Method for removing from an image the background surrounding a selected object,2000\n2012\nImage Matting\nBayesianMatting\nYung-Yu Chuang,Brian Curless1David H. Salesin1, Richard Szeliski.A Bayesian Approach to Digital Matting,2000\n2012\nImage Matting\nLearning Based Matting\nYuanjieZheng,ChandraKambhamettu.Learning Based Digital Matting,2009\n2012\nImageInpainting\nCriminisi Inpainting\nAntonio Criminisi, Patrick Perez, and KentaroToyama.Object Removal by Exemplar-Based Inpainting,2003\n2012\nimage Classification\nSC: Sparse Coding\nSparse Coding for Image Classification\n2010\nimage Classification\nICA : Independent Component Analysis\nHyvärinen A. Testing the ICA mixing matrix based on inter-subject or inter-session consistency.NeuroImage.\n2010\nimage Classification\nFastICA: Fast Independent Component Analysis\nA. Hyvärinen, J. Karhunen, E. Oja . Independent Component Analysis. Wiley-Interscience. 2001\n2010\nImage Classification\nSPM: Spatial Pyramid Matching, BoF: Bag of Feature (BoW: Bag of Word)\nS. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, New York , June 2006, vol. II, pp. 2169-2178.\n2011\nImage Classification\nLLC: Locality-constrained Linear Coding\nJianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Linear spatial pyramid matching using sparse coding for image classification. CVPR'09.\n2011\nImage Classification\nEMK: Efficient Match Kernels\nLiefeng Bo, Cristian Sminchisescu Efficient Match Kernels between Sets of Features for Visual Recognition, Advances in Neural Information Processing Systems (NIPS), December, 2009.\n2008\nImage Retrieval\nThe Pyramid Match: Efficient Matching for Retrieval and Recognition\nK. Grauman and T. Darrell.  The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features, ICCV 2005\n2012\nImage Understanding\nTSU: Towards Total Scene Understanding\nLi-Jia Li, Richard Socher and Li Fei-Fei. Towards Total Scene Understanding:Classification, Annotation and Segmentation in an Automatic Framework. Computer Vision and Pattern Recognition (CVPR) 2009.\n2012\nImage Understanding\nObject Context\nYong Jae Lee and Kristen Grauman. Object-Graphs for Context-Aware Category Discovery. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), San Francisco , CA , June 2010.\n2012\nOptical Flow\nBlack and Anandan's Optical Flow\nBlack, M.J. Anandan, P. A framework for the robust estimation of optical flow. Computer Vision, 1993. Proceedings. Fourth International Conference on. 1993.\n2012\nObject Tracking\nPF: Particle Filter (LASSO: Least Absolute Shrinkage and Selection Operator)\nX. Mei and H. Ling. Robust Visual Tracking and Vehicle Classification via Sparse Representation. IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 33(11):2259--2272, 2011.\n2012\nObject Tracking\nIncremental Learning\nD. Ross, J. Lim, R.-S. Lin, M.-H. Yang, Incremental Learning for Robust Visual Tracking, IJCV 2007\n2012\nObject Tracking\nOn-Line Boosting\nTracking the Invisible: Learning Where the Object Might be H. Grabner, J. Matas, L. Van Gool, and P.Cattin In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010\n2012\nObject Tracking\nMotion Tracking\nC. Stauffer and W. E. L. Grimson. Learning patterns of activity using real-time tracking, PAMI, 2000\n2012\nObject Tracking\nKanade-Lucas-Tomasi Feature Tracker\nB. D. Lucas and T. Kanade. An Iterative Image Registration Technique with an Application to Stereo Vision. IJCAI, 1981\n2012\nObject Tracking\nTracking Decomposition\nJ Kwon and K. M. Lee, Visual Tracking Decomposition, CVPR 2010\n2012\nObject Tracking\nAdaptive Structural Local Sparse Appearance Model\nXu Jia, Huchuan Lu, Minghsuan Yang, Visual Tracking via Adaptive Structural Local Sparse Appearance Model, International Conference on Computer Vision and Pattern Recognition,2012,.\n2012\nObject Tracking\nSparsity-based Collaborative Model\nWei Zhong, Huchuan Lu, Minghsuan Yang, Robust Object Tracking via Sparsity-based Collaborative Model, International Conference on Computer Vision and Pattern Recognition,2012.\n2012\nImage Depth\nDC: Dark Channel\nKaiming He, Jian Sun, and Xiaoou Tang, Single Image Haze Removal using Dark Channel Prior, by  in TPAMI 2011.\n2010\nSemantic Analysis\nWordnet\nWordNet 3.0 Reference Manual\n2008\nData Set\nCaltech 256: Caltech-256 benchmarks\nCitation: caltech-256 object Gategory dataset[c].Greg Griffin，Alex Holub,California Institute of Technology on 2007\n2008\nData Set\nVOCdevkit: PASCAL VOC Development Kits (PASCAL: Pattern Analysis, Statistical Modelling and Computational Learning)\nCitation: The PASCAL Visual Object Classes Challenge 2011 (VOC2011) Development Kit.Mark EveringhamJohn WinnMark Everingham John Winn\n2009\nData Set\nLabelMe\nCitation: Modeling the shape of the scene: a holistic representation of the spatial envelope. A. Oliva, A.Torralba. International Journal of Computer Vision, Vol. 42(3): 145-175, 2001.\n2009\nData Set\nEight outdoor scene categories\nAude Oliva, Antonio Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope. International Journal of Computer Vision, Vol. 42(3): 145-175, 2001.\n2009\nData Set\nFifteen Scene Categories\nSvetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2006.\n2009\nData Set\nSUN Database: Scene UNderstanding Database.\nJ. Xiao, J. Hays, K. Ehinger, A. Oliva, and A.Torralba. SUN Database: Large scale Scene Recognition from Abbey to Zoo. IEEE Conference on Computer Vision and Pattern Recognition. CVPR. 2010.\n2012\nData Set\nSegBanch: The Berkely Segmentation Dataset and Benchmark\nVOI\n2012\nData Set\nSaliency Benchmark\nR. Subramanian, H. Katti, N. Sebe1, M. Kankanhalli, T-S. Chua, An Eye Fixation Database for Saliency Detection in Images,  European Conference on Computer Vision (ECCV 2010), Heraklion, Greece, September 2010\n2012\nData Set\nSegBanch: The Berkely Segmentation Dataset and Benchmark\nX. Ren, C. Fowlkes, J. Malik. \"Figure/Ground Assignment in Natural Images\", ECCV, Graz , Austria, (May 2006).\n2012\nData Set\nFlikcer\nCitation: Flickr shapetiles : Location data created fromWOEid geotagged Flickr photos\n2012\nData Set\nYL face: Yale Face Database\nCitation: From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting andPose[J].Georghiades, A.S. and Belhumeur .IEEE Trans. Pattern Anal. Mach. Intelligence on 2001.pages:643-660\n2012\nData Set\nSaliency Benchmark\nR. Subramanian, H. Katti, N. Sebe1, M. Kankanhalli, T-S. Chua, An Eye Fixation Database for Saliency Detection in Images,  European Conference on Computer Vision (ECCV 2010), Heraklion, Greece, September 2010\n2012\nData Set\nImageCLEF Plant (CLEF: key/ french)\nGoëau, Hervé; Bonnet, Pierre; Joly, Alexis; Boujemaa,Nozha; Barthelemy, Daniel; Molino, Jean-François;Birnbaum, Philippe; Mouysset, Elise; Picard, Marie. The CLEF 2011 plant image classification task. CLEF 2011 working notes, Amsterdam , The Netherlands, 2011.\n2012\nData Set\nImageCLEFphoto (CLEF: key/ french)\nCitation: Diversity in Photo Retrieval: Overview of theImageCLEFPhoto Task 2009. Monica Lestari Paramita, Mark Sanderson,Lecture Notes in Computer Science, 2010, Volume 6242/2010, 45-59,\nECCV 2012 papers on the web 已经发布了。今天浏览了一下文章列表，找出了自己感兴趣的一些论文。那个列表目前还没有公布论文的下载链接。先把列表记下来，慢慢整理链接把。\n显著性相关\nDepth Matters: Influence of Depth Cues on Visual Saliency\nLang Congyan (Beijijng Jiaotong University), Tam Nguyen (NUS - Singapore),harish Katti (National University of Singapore), Karthik Yadati (National University of Singapore), Shuicheng Yan, Mohan Kankanhalli (National University of Singapore)\nQuaternion-based Spectral Saliency Detection for Human Eye Fixation Point Prediction   [bibtex] [code #1 - saliency - will be updated soon(ish)] [code #2 - Matlab AUC measure implementation]\nBoris Schauerte (Karlsruhe Inst. Tech.), Rainer Stiefelhagen (KarlsruheInst. of Technology)\nGeodesic Saliency Using Background Priors\nYichen Wei (Microsoft Research), Fang Wen, Wangjiang Zhu (Tsinghua University), Jian Sun (Microsoft Research Asia)\nSaliency Modeling from Image Histograms\nShijian Lu (I2R - A*STAR), Joo-Hwee Lim (Institute for Infocomm Research)\nSalient Object Detection: A Benchmark\nAli Borji (University of Southern Califor), Dicky Sihite (University of Southern California), Laurent Itti (University of Southern California)\n跟踪和光流\nOnline Learned Discriminative Part-Based Appearance Models forMulti-Human Tracking\nBo Yang (USC), Ram Nevatia\nReal-Time Camera Tracking: When is High Frame-Rate Best?\nAnkur Handa (Imperial College London), Richard Newcombe (Imperial CollegeLondon), Adrien Angeli, Andrew Davison (Imperial College London)\nOnline Learning of Linear Predictors for Real-Time Tracking\nStefan Holzer (Technische Universität München), Marc Pollefeys,Slobodan Ilic (TUM), David Joseph Tan (Technische Universität München), Nassir Navab (Technische Universität München)\nTracking Using Motion Patterns for Very Crowded Scenes\nXuemei Zhao (Univ. of Southern California), Dian Gong (Univ. of Southern California), Gerard Medioni (University of Southern California)\nDivergence-free motion estimation\nDominque BÃˆrÃˆziat (UPMC), Isabelle Herlin (INRIA), Nicolas Mercier (INRIA),Sergiy Zhuk (CWI)\nCoherent Filtering: Detecting Coherent Motions from Clutters\nBolei Zhou (The Chinese University of HK), Xiaogang Wang (The ChineseUniversity of HK), Xiaoou Tang\nStatistical Inference of Motion in the Invisible\nHaroon Idrees (UCF), Imran Saleemi (UCF), Mubarak Shah (UCF)\nGroup Tracking: Exploring Mutual Relations for Multiple Object Tracking\nGenquan Duan (Tsinghua University), Song Cao (Tsinghua University),Haizhou Ai (Tsinghua University), Shihong Lao (Omron Company)\nStixels motion estimation without optical flow computation\nBertan GÂ¸nyel (KU Leuven), Rodrigo Benenson (KU Leuven), Radu Timofte (KULeuven), Luc Van Gool (KU Leuven)\nSimultaneous Compaction and Factorization of Sparse Image Motion Matrices\nSusanna Ricco (Duke University), Carlo Tomasi\nEfficient Nonlocal Regularization for Optical Flow\nPhilipp KrähenbÂ¸hl (Stanford University), Vladlen Koltun (Stanford University)\nScale Invariant Optical Flow\nLi Xu (CUHK), Zhenlong Dai (CUHK), jiaya Jia (CUHK)\nA Naturalistic Open Source Movie for Optical Flow Evaluation\nDaniel Butler (University of Washington), Jonas Wulff (Max Planck Institute for Intelligent Systems), Garrett Stanley (Department of Biomedical Engineering - Georgia Institute of Technology), Michael Black (Max Planck Institute for Intelligent Systems)\nDynamic Context for Tracking Behind Occlusions\nFei Xiong (Northeastern University), Octavia Camps (Northeastern University), Mario Sznaier (Northeastern University)\n运动和视频分割\nVideo Matting Using Multi-Frame Nonlocal Matting Laplacain\nInchang Choi (KAIST), Yu-Wing Tai (KAIST), Minhaeng Lee (KAIST)\nSemi-Nonnegative Matrix Factorization for Motion Segmentation with Missing Data\nQuanyi Mo (Colorado State University), Bruce Draper (Colorado StateUniversity)\nMulti-Scale Clustering of Frame-to-Frame Correspondences for Motion Segmentation\nRalf Dragon (Leibniz Universit\\auml),t Hannover, Bodo Rosenhahn, Joern Ostermann\nLearning to segment a video to clips based on scene and camera motion\nAdarsh Kowdle (Cornell University), Tsuhan Chen (Cornell University)\nEfficient Articulated Trajectory Reconstruction using Dynamic Programming and Filters\nJack Valmadre (CSIRO), Yingying Zhu (Csiro), Sridha Sridharan (Queensland University of Technology), Simon Lucey (CSIRO)\nBackground Inpainting for Videos with Dynamic Objects and a Free-moving Camera\nMiguel Granados (MPI Informatik), Kwang In Kim (MPI for Informatics), James Tompkin (UCL), Jan Kautz (UCL), Christian Theobalt (MPI Informatik)\nActive Frame Selection for Label Propagation in Videos\nSudheendra Vijayanarasimhan, Kristen Grauman\nStreaming Hierarchical Video Segmentation\nChenliang Xu (SUNY at Buffalo), Caiming Xiong (SUNY at Buffalo), Jason Corso (SUNY at Buffalo)\n行为识别\nModeling Complex Temporal Composition of Actionlets for ActivityPrediction\nKang Li, Jie Hu (State University of New York (SUNY) at Buffalo), YunFu (SUNY at Buffalo)\nCombining Per-Frame and Per-Track Cues for Multi-Person ActionRecognition\nSameh Khamis (University of Maryland), Vlad Morariu (University of Maryland), Larry Davis (University of Maryland)\nScript Data for Attribute-based Recognition of Composite Activities\nMarcus Rohrbach (MPI Informatics), Michaela Regneri (Saarland University), Mykhaylo Andriluka (MPI Informatik), Sikandar Amin (Max-Planck - TU Munich),Manfred Pinkal, Bernt Schiele\nA Unified Framework for Multi-Target Tracking and Collective ActivityRecognition\nWongun Choi (The University of Michigan), Silvio Savarese (The University of Michigan - Ann Arbor)\nActivity Forecasting\nKris Kitani (Carnegie Mellon University), James Bagnell, Martial Hebert\nPropagative Hough Voting for Human Activity Recognition\nGang YU (NTU), Junsong Yuan (NTU), Zicheng Liu (MSR)\nHuman Actions as Stochastic Kronecker Graphs\nSinisa Todorovic (Oregon State University)\nTrajectory-Based Modeling of Human Actions with Motion Reference Points\nYu-Gang Jiang (Fudan University), Qi Dai (Fudan University), XiangyangXue (Fudan University), Wei Liu (Columbia University), Chong-Wah Ngo (CityUniversity of Hong Kong)\nTeam Activity Recognition in Sports\nCem Direkoglu (Dublin City University), Noel O’Connor (Dublin City University)\nReal–Time Human Pose Tracking using Range Cameras\nVarun Ganapathi (Google), Christian Plagemann (Google Research), DaphneKoller (Stanford University), Sebastian Thrun (Google)\n目标检测与分割\nObject Co-detection\nYinzge Bao (U of Michigan at Ann Arbor), Yu Xiang (University of Michigan), Silvio Savarese (The University of Michigan - Ann Arbor)\nHausdorff Distance Constraint for Multi-Surface Segmentation\nFrank Schmidt (ESIEE), Yuri Boykov (University of Western Ontario)\nBackground Subtraction using Group Sparsity and Low Rank constraint\nXinyi Cui (Rutgers University), Junzhou Huang, shaoting Zhang (Rutgers University), Dimitris Metaxas (Rutgers University)\nShape Sharing for Object Segmentation\nJaechul Kim (University of Texas at Austin), Kristen Grauman\nOn Learning Higher-Order Consistency Potentials for Multi-class Pixel Labeling\nKyoungup Park (ANU), Stephen Gould (ANU)\nObject detection using strongly-supervised deformable part models\nHossein Azizpour (KTH), Ivan Laptev\nHough Regions for Joining Instance Localization and Segmentation\nHayko Riemenschneider (Graz University of Technology), Sabine Sternig (Graz University of Technology), Michael Donoser (Graz University ofTechnology), Peter Roth (Graz University of Technology)\nLatent Hough Transform for Object Detection\nNima Razavi (ETH Zurich), Juergen Gall (ETH Zurich), Pushmeet Kohli, LucVan Gool\nAnnotation Propagation in Large Image Databases via Dense Image Correspondence\nMichael Rubinstein (MIT), Ce Liu (Microsoft Research New England), WilliamFreeman (Massachusetts Institute of Technology)\nFast Tiered Labeling with Topological Priors\nYing Zheng (Duke University - Computer Science), Steve Gu (DukeUniversity - Computer Scie), Carlo Tomasi\nMulti-Component Models for Object Detection\nChunhui Gu (UC Berkeley), Pablo Arbelaez (UC Berkeley), Yuanqing Lin (NECLaboratories Amertica), Kai Yu (NEC Laboratories Amertica), Jitendra Malik (UCBerkeley)\nJoint Classification-Regression Forests for Spatially Structured Multi-Object Segmentation\nBen Glocker (Microsoft Research Cambridge), Olivier Pauly (TechnischeUniversitaet Muenchen), Ender Konukoglu (Microsoft Research Cambridge), AntonioCriminisi (Microsoft Research Cambridge)\nUsing linking features in learning non-parametric part models\nLeonid Karlinsky (Weizmann Institute of Science), Shimon Ullman (WeizmannInstitute of Science)\nConnecting Missing Links: Object Discovery from Sparse Observations\nHongwen Kang (Carnegie Mellon University), Martial Hebert, Takeo Kanade\nBeyond the line of sight: labeling the underlying surfaces\nRuiqi Guo (UIUC), Derek Hoiem (University of Illinois)\n立体视觉与重建\nOptimal Templates for Non-Rigid Surface Reconstruction\nMarkus Moll (K.U.Leuven), Luc Van Gool\nScale Robust Multi View Stereo\nChristian Bailer, Manuel Finckh (Tuebingen University), Hendrik Lensch (Tuebingen University)\nMultiple View Object Cosegmentation using Appearance and Stereo Cues\nAdarsh Kowdle (Cornell University), Sudipta Sinha, Rick Szeliski\nDetection of Independently Moving Objects in Non-planar Scenes via Multi-Frame Monocular Epipolar Constraint\nVladimir Reilly (University of Central Florida), Soumyabrata Dey (University of Central Florida), Mubarak Shah (UCF)\n3D Reconstruction of Dynamic Scenes with Multiple Handheld Cameras\nHanqing Jiang (Zhejiang University), Haomin Liu (Zhejiang University), PingTan (National University of Singapore), Guofeng Zhang (Zhejiang University),Hujun Bao (Zhejiang University)\n其他\nAuto-grouped Sparse Representation for Visual Analysis\nJiashi Feng (NUS), Xiaotong Yuan, zilei Wang, Huan Xu, Shuicheng Yan\nUndoing the Damage of Dataset Bias\nAditya Khosla (MIT), Tinghui Zhou (CMU), Tomasz Malisiewicz (MIT), Alyosha Efros (CMU), Antonio Torralba (MIT)\nUnsupervised Discovery of Mid-Level Discriminative Patches\nSaurabh Singh (Carnegie Mellon University), Abhinav Gupta, Alyosha Efros (CMU)\nA new biologically inspired color- and shape-based image descriptor\nJun Zhang (Brown University), Youssef Barhomi (Brown University), ThomasSerre (Brown University)\nContinuous Regression for Non-Rigid Image Alignment\nEnrique Sanchez Lozano (Gradiant), Fernando De la Torre (Carnegie Mellon University), Daniel Gonzalez Jimenez (Gradiant)\nCoregistration: Simultaneous Alignment and Modeling of Articulated 3D Shape\nDavid Hirshberg (MPI for Intelligent Systems), Matthew Loper (MPI for Intelligent Systems), Eric Rachlin (MPI for Intelligent Systems), MichaelBlack (Max Planck Institute for Intelligent Systems)\nDiscovering Latent Domains for Multisource Domain Adaptation\nJudy Hoffman (UC Berkeley), Kate Saenko (UC Berkeley - Harvard - ICSI),Brian Kulis (Ohio State), Trevor Darrell (UC Berkeley - ICSI)"}
{"content2":"计算机视觉相当于是人工智能的大门，如果这个门不打开，就没有办法真的研究真实世界的人工智能。因为视觉信息与听觉触觉相比要重要得多，人的大脑皮层70%的活动都在处理视觉信息，如果没有视觉信息的话，整个人工智能只是一个空架子，只能做符号推理（下棋，定理证明等）。\n计算机视觉相当于一个研究领域，有很多问题要研究。而机器学习更像是一个方法和工具。\n本身应当叫统计学习，方法都是从概率领域拿来的，但是机器学习领军人物把统计和物理的数理模型，改名叫做机器，比如某某模型（model）叫某某机（machine）；把一些层次模型（hierarchical model）说成网（net），搞出了许多机和网，就成了新的领域。\n机器：统计模型。 学习：用数据来拟合模型。是计算机的人将统计的理论与方法应用到视觉、语音等领域，形成了机器学习这个模型。\n机器学习与计算机视觉大概有60~70%的是重合的。\n人工智能是终极目标，让机器像人那样的思考、处理事情。\n解决问题的过程：抽象成问题（表达），寻找算法，实现。视觉是受任务驱动的，而任务是时刻在改变之中，我怎样通过这千千万万的任务，而不是简单一个分类，来驱动我的计算的过程，来找到我的需求，来支持我目前的任务，这是一个巨大的研究的方向 。\n当你要去识别、分析一个模式，比如一个动物、人脸、 一个事件， 你首先要建立一个数理模型，这个模型通过数据来拟合，也就是当前的机器学习。那么，判断这个模型好坏，或者模型是否充分的一个依据是什么？产生式建模的方法就是对这个模型随机抽样，也就是合成（synthesis）。"}
{"content2":"在计算机视觉领域，每年都会有很多顶级会议召开，如比较著名的CVPR，ICCV等，在会议上会有CV各个领域的新思想、新方法被提出来，推动着这个领域的发展，以下为2019年各个会议的时间地点，还有会议相关链接。\n1. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019)\nLocation ：Long Beach Convention & Entertainment Center, Los Angeles CA, United States\nDate：Jun 15 - Jun 21, 2019\nPaper Submission Deadline：Nov 16, 2018 (92)\nhttp://cvpr2019.thecvf.com\n２. IEEE International Conference on Computer Vision (ICCV 2019)\nLocation ：Seoul, South Korea\nDate: Oct 27 - Nov 3, 2019\nPaper Submission Deadline: Mar, 2019\nhttp://iccv2019.thecvf.com\n3. International Conference on Machine Learning（ICML 2019）\nLocation：Long Beach Convention Center, Long Beach, United States\nDate：Jun 10 - Jun 15, 2019\nPaper Submission Deadline: TBD\nhttps://icml.cc/Conferences/2019\n4.Association for the Advancement of Artificial Intelligence（AAAI 2019）\nLocation：Hilton Hawaiian Village, Waikiki Beach, Honolulu, Hawaii, United States\nDate：Jan 27 - Feb 1, 2019\nPaper Submission Deadline:Sep 5, 2018 (20)\nhttp://www.aaai.org/aaai19\n5.International Joint Conference on Artificial Intelligence（IJCAI 2019）\nLocation：Macao, China\nDate：Aug 10 - Aug 16, 2019\nPaper Submission Deadline：Feb 25, 2019\nhttp://www.ijcai19.org\n6. Computer Graphics and Interactive Techniques（SIGGRAPH 2019）\nLocation：Los Angeles, California, United States\nDate：Jul 29 – Aug 1, 2019\nPaper Submission Deadline：TBD\nhttp://s2019.siggraph.org\n7. ACM SIGKDD International Conference on Knowledge discovery and data mining（SIGKDD 2019）\nLocation：Anchorage, Alaska, United States\nDate：Aug 3 - Aug 7, 2019\nPaper Submission Deadline：TBD\nhttp://www.kdd.org/kdd2019\n8. Annual Meeting of the Association for Computational Linguistics（ACL 2019）\nLocation：Florence, Italy\nDate：Jul 28 - Aug 2, 2019\nPaper Submission Deadline：TBD\nhttp://acl2019.org\n9. International Conference on Learning Representations（ICLR 2019)\nLocation：New Orleans, United States\nDate：May 6 - May 9. 2019\nPaper Submission Deadline：Sep 27, 2018 (42)\nhttp://www.iclr.cc\n转载自https://blog.csdn.net/hitzijiyingcai/article/details/81709755"}
{"content2":"随着计算机视觉的发展，摄影测量和计算机视觉逐渐交叉融合，两个不同学科之间的差异逐渐变小。但是在此处，博主仍旧想记录一下两个学科在几何关系描述上的一些本质差异。\n一、坐标的几何表示\n首先，对于两个领域来说，最为明显的一个差异就是对坐标的表示方法。\n在摄影测量中，通常直接直接使用二维坐标或者三维坐标对坐标进行描述。而在计算机视觉中，通常情况下使用齐次坐标对坐标进行表述。\n原则上说，两种表述方法在本质上并没有什么区别。只不过在齐次坐标下能够将一系列变化（旋转、平移、仿射等等）统一成矩阵相乘的形式，处理起来更加方便。但是，一般来说，直接使用坐标进行描述，理解起来更加直观。\n下边给出一个示例，假设对二维点P 进行旋转R 和平移t 。\n对于摄影测量来说，此处可以简单的把公式写成：\nP′=(λRP+t)\nP\n′\n=\n(\nλ\nR\nP\n+\nt\n)\nP' = (\\lambda RP + t)\n而对于计算机视觉来说，上述公式可以更简单的写成：\n二、欧拉旋角和扭转系数与四元数\n第二个较为明显的差异是两者对旋转矩阵的理解方式。在摄影测量中，通常是按照一定顺序和方向在坐标轴的方向上进行旋转，进而得到旋转矩阵。但是在计算机视觉中，最为普遍常用的方法是扭转系数和四元数。\n不过需要注意的是，无论是使用那种表达方式，得到的旋转矩阵都应该是一样的。\n摄影测量的欧拉旋角\n首先介绍一下摄影测量中的欧拉旋角，常见的旋转体系有opk和pok。\n值得注意的是，在摄影测量中通常是通过旋转坐标轴来定义旋转角的，而在公式表达时是则是在旋转坐标的角度上来写的\n。\nopk是国际上较为通用的一种旋转体系，其中旋转顺序是x–>y–>z，旋转方向均以逆时针旋转为正。而pok是国内较为通用的一套旋转体系，其中旋转顺序是y–>x–>z，旋转方向除y轴外都是逆时针旋转为正。\n此处我们用opk写出一个示例，假设要从坐标系A旋转到坐标系B，则旋转矩阵可以写成：\nPA=RxRyRzPB\nP\nA\n=\nR\nx\nR\ny\nR\nz\nP\nB\n{P_A} = {R_x}{R_y}{R_z}{P_B}\n其中，\nPA\nP\nA\n{P_A}、\nPB\nP\nB\n{P_B} 分别是两个坐标系下的坐标，描述的是如何按照opk的顺序从A旋转到B。\n我们可以把上式展开，如下图所示的公式：\n此处，我们也给出如何从旋转矩阵恢复opk的公式，如下：\n计算机视觉中的旋角系统\n相比而言，计算机视觉中的旋角系统定义比较复杂，比较难理解。扭转系数可以理解成绕某一个三维向量旋转一个角度，来描述三维旋转。四元数是在扭转系数的基础上推导出的一种旋转表达方式。\n由于这两种旋转表达方式并不需要指定旋转轴等等，我们直接给出其和旋转矩阵之间的关系式：\n当然，如果想从旋转矩阵恢复四元数，上述公式已经可以完成。但是如果想从旋转矩阵恢复扭转系数，则还需要用到四元数和扭转系数之间的关系式：\n最后关于两种旋转表达的优缺点，我就不再累赘，可以参考huang9012的博客。但是此处我只说欧拉公式中两个我经常遇到的缺点：\n1.opk的旋转矩阵比较难以求导，求导公式较为复杂；\n2.opk的旋转矩阵在矩阵连乘时很容易失去正定性，计算误差导致的。\n三、共线方程与小孔成像\n最后说两个领域中，成像模型的一些差异问题。首先给出两个图，分别表示的是摄影测量和计算机视觉中关于成像模型的表述方式。\n摄影测量\n计算机视觉\n仔细观察两幅图，我们可以发现两个明显的区别：\n在摄影测量中成像平面在Z轴的负半轴，而在摄影测量中成像平面则在Z轴的正半轴。造成这个区别的原因是，摄影测量通常是航空拍摄，相机从天空俯视地面，即成像物体在相机下方；而摄影测量则主要是近景拍摄，成像物体在相机前方。因而根据人们常规对下方和前方的认识，导致了Z轴的朝向不同。\n在摄影测量中，Y轴朝向图片的上方；而在计算机视觉中，Y轴朝向图片的下方。因为摄影测量和计算机视觉都是使用右手坐标系，由于Z轴的改变，Y轴自然也就不一样。\n四、空三结果的互相转换\n由于现在摄影测量和计算机视觉的交叉发展，如何进行将两种空三结果进行相互转换就非常重要。其实也非常简单，推导如下（为推导方便，此处统一不适用齐次坐标系）：\n记某一照片在摄影测量下的成像关系如下：\n⎡⎣⎢xy−f⎤⎦⎥=λRp(⎡⎣⎢XYZ⎤⎦⎥−⎡⎣⎢XsYsZs⎤⎦⎥)\n[\nx\ny\n−\nf\n]\n=\nλ\nR\np\n(\n[\nX\nY\nZ\n]\n−\n[\nX\ns\nY\ns\nZ\ns\n]\n)\n\\begin{bmatrix} x\\\\ y\\\\ -f\\\\ \\end{bmatrix} = \\lambda R_p (\\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ \\end{bmatrix} - \\begin{bmatrix} X_s\\\\ Y_s\\\\ Z_s\\\\ \\end{bmatrix} )\n由于摄影测量和计算机视觉的成像中心在同一个坐标，仅仅Y轴和Z轴方向相反，则其表达式如下：\n⎡⎣⎢x−yf⎤⎦⎥=λRc(⎡⎣⎢XYZ⎤⎦⎥−⎡⎣⎢XsYsZs⎤⎦⎥)\n[\nx\n−\ny\nf\n]\n=\nλ\nR\nc\n(\n[\nX\nY\nZ\n]\n−\n[\nX\ns\nY\ns\nZ\ns\n]\n)\n\\begin{bmatrix} x\\\\ -y\\\\ f\\\\ \\end{bmatrix} = \\lambda R_c (\\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ \\end{bmatrix} - \\begin{bmatrix} X_s\\\\ Y_s\\\\ Z_s\\\\ \\end{bmatrix} )\n等同于\n⎡⎣⎢1000−1000−1⎤⎦⎥⎡⎣⎢x−yf⎤⎦⎥=λRc(⎡⎣⎢XYZ⎤⎦⎥−⎡⎣⎢XsYsZs⎤⎦⎥)\n[\n1\n0\n0\n0\n−\n1\n0\n0\n0\n−\n1\n]\n[\nx\n−\ny\nf\n]\n=\nλ\nR\nc\n(\n[\nX\nY\nZ\n]\n−\n[\nX\ns\nY\ns\nZ\ns\n]\n)\n\\begin{bmatrix} &1 &0 &0 \\\\ &0 &-1 &0 \\\\ &0 &0 &-1 \\\\ \\end{bmatrix} \\begin{bmatrix} x\\\\ -y\\\\ f\\\\ \\end{bmatrix} = \\lambda R_c (\\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ \\end{bmatrix} - \\begin{bmatrix} X_s\\\\ Y_s\\\\ Z_s\\\\ \\end{bmatrix} )\n观察上式就可以发现，摄影测量和计算机视觉仅仅在旋转上有表达区别，在平移量上时相同的，因此只需要对旋转进行转换即可：\nRp=⎡⎣⎢1000−1000−1⎤⎦⎥Rc\nR\np\n=\n[\n1\n0\n0\n0\n−\n1\n0\n0\n0\n−\n1\n]\nR\nc\nR_p = \\begin{bmatrix} &1 &0 &0 \\\\ &0 &-1 &0 \\\\ &0 &0 &-1 \\\\ \\end{bmatrix} R_c"}
{"content2":"***************************************************************************************************************\n在这里，我特别声明：本文章的源作者是   杨晓冬  （个人邮箱：xdyang.ustc@gmail.com）。原文的链接是\nhttp://www.iask.sina.com.cn/u/2252291285/ish。版权归 杨晓冬 朋友所有。\n我非常感谢原作者辛勤地编写本文章，并愿意共享出来。我也希望转载本文的各位朋友，要注明原作者和出处，以尊重原作者！\n***************************************************************************************************************\n图像处理与计算机视觉基础，经典以及最近发展\n一、 绪论\n1. 为什么要写这篇文章\n从2002年到现在，接触图像快十年了。虽然没有做出什么很出色的工作，不过在这个领域摸爬滚打了十年之后，发现自己对图像处理和计算机视觉的感情越来 越深厚。下班之后看看相关的书籍和文献是一件很惬意的事情。平常的一大业余爱好就是收集一些相关的文章，尤其是经典的文章，到现在我的电脑里面已经有了几 十G的文章。写这个文档的想法源于我前一段时间整理文献时的一个突发奇想，既然有这个多文献，何不整理出其中的经典，抓住重点来阅读，同时也可以共享给大 家。于是当时即兴写了一个《图像处理与计算机视觉中的经典论文》。现在来看，那个文档写得很一般，所共享的论文也非常之有限。就算如此，还是得到了一些网 友的夸奖，心里感激不尽。因此，一直想下定决心把这个工作给完善，力求做到尽量全面。\n本文是对现有的图像处理和计算机视觉的经典书籍（后面会有推荐）的一个补充。一般的图像处理书籍都是介绍性的介绍某个方法，在每个领域内都会引用几十上 百篇参考文献。有时候想深入研究这个领域的时候却发现文献太多，不知如何选择。但实际上在每个领域都有那么三五篇抑或更多是非读不可的经典文献。这些文献 除了提出了很经典的算法，同时他们的Introduction和Related work也是对所在的领域很好的总结。读通了这几篇文献也就等于深入了解了这个领域，比单纯的看书收获要多很多。写本文的目的就是想把自己所了解到的各个 领域的经典文章整理出来,不用迷失在参考文献的汪洋大海里。\n2. 图像处理和计算机视觉的分类\n按照当前流行的分类方法，可以分为以下三部分：\nA.图像处理：对输入的图像做某种变换，输出仍然是图像，基本不涉及或者很少涉及图像内容的分析。比较典型的有图像变换，图像增强，图像去噪，图像压      缩，图像恢复，二值图像处理等等。基于阈值的图像分割也属于图像处理的范畴。一般处理的是单幅图像。\nB.图像分析：对图像的内容进行分析，提取有意义的特征，以便于后续的处理。处理的仍然是单幅图像。\nC.计算机视觉：对图像分析得到的特征进行分析，提取场景的语义表示，让计算机具有人眼和人脑的能力。这时处理的是多幅图像或者序列图像，当然也包括部分单幅图像。\n关于图像处理，图像分析和计算机视觉的划分并没有一个很统一的标准。一般的来说，图像处理的书籍总会或多或少的介绍一些图像分析和计算机视觉的知识，比如 冈萨雷斯的数字图像处理。而计算机视觉的书籍基本上都会包括图像处理和图像分析，只是不会介绍的太详细。其实图像处理，图像分析和计算机视觉都可以纳入到 计算机视觉的范畴：图像处理->低层视觉（low level vision），图像分析->中间层视觉（middle level vision），计算机视觉->高层视觉（high level vision）。这是一般的计算机视觉或者机器视觉的划分方法。在本文中，仍然按照传统的方法把这个领域划分为图像处理，图像分析和计算机视觉。\n3. 图像处理和计算机视觉开源库以及编程语言选择\n目前在图像处理中有两种最重要的语言：c/c++和matlab。它们各有优点：c/c++比较适合大型的工程，效率较高，而且容易转成硬件语言，是工 业界的默认语言之一。而matlab实现起来比较方便，适用于算法的快速验证，而且matlab有成熟的工具箱可以使用，比如图像处理工具箱，信号处理工 具箱。它们有一个共同的特点：开源的资源非常多。在学术界matlab使用的非常多，很多作者给出的源代码都是matlab版本。最近由于OpenCV的 兴起和不断完善，c/c++在图像处理中的作用越来越大。总的来说，c/c++和matlab都必须掌握，最好是精通，当然侧重在c/c++上对找工作会 有很大帮助。\n至于开源库，个人非常推荐OpenCV，主要有以下原因：\n（1）简单易入手。OpenCV进入OpenCV2.x的时代后，使用起来越来越简单,接口越来越傻瓜化，越来越matlab化。只要会imread,imwrite,imshow和了解Mat的基本操作就可以开 始入手了。\n（2）OpenCV有一堆图像处理和计算机视觉的大牛在维护，bug在逐步减少，每个新的版本都会带来不同的惊喜。而且它已经或者逐步在移植到不懂的平台,并提供了对Python的很好的支持。\n（3）在OpenCV上可以尝试各种最新以及成熟的技术，而不需要自己从头去写，比如人脸检测（Harr，LBP），DPM（Latent SVM），高斯背景模型，特征检测，聚类，hough变换等等 。而且它还支持各种机器学习方法（SVM，NN，KNN，决策树，Boosting等），使用起来很简单。\n（4）文档内容丰富，并且给出了很多示例程序。当然也有一些地方文档描述不清楚，不过看看代码就很清楚了。\n（5）完全开源。可以从中间提取出任何需要的算法。\n（6）从学校出来后，除极少数会继续在学术圈里，大部分还是要进入工业界。现在在工 业界，c/c++仍是主流，很多公司都会优先考虑熟悉或者精通OpenCV的。事实上，在学术界，现在OpenCV也大有取代matlab之势。以前的 demo或者source code，很多作者都愿意给出matlab版本的，然后别人再呼哧呼哧改成c版本的。现在作者干脆给出c/c++版本，或者自己集成到OpenCV中去， 这样能快速提升自己的影响力。\n如果想在图像处理和计算机视觉界有比较深入的研究，并且以后打算进入这个领域工作的话，建议把OpenCV作为自己的主攻方向。如果找工作的时候敢号称自己精通OpenCV的话，肯定可以找到一份满意的工作。\n4. 本文的特点和结构，以及适合的对象\n本文面向的对象是即将进入或者刚刚进入图像处理和计算机视觉领域的童鞋，可以在阅读书籍的同时参阅这些文献，能对书中提到的算法有比较深刻的理解。由于本文涉及 到的范围比较广，如果能对计算机视觉的资深从业者也有一定的帮助，我将倍感欣慰。为了不至太误人子弟，每一篇文章都或多或少的看了一下，最不济也看了摘要 (这句话实在整理之前写的，实际上由于精力有限，好多文献都只是大概扫了一眼，然后看了看google的引用数，一般在1000以上就放上来了，把这些文 章细细品味一遍也是我近一两年之内的目标)。在成文的过程中，我本人也受益匪浅，希望能对大家也有所帮助。\n由于个人精力和视野的关系，有一些我未涉足过的领域不敢斗胆推荐，只是列出了一些引用 率比较高的文章，比如摄像机标定和立体视觉。不过将来，由于工作或者其他原因，这些领域也会接触到，我会逐步增减这些领域的文章。尽管如此，仍然会有疏 漏，忘见谅。同时文章的挑选也夹带了一些个人的喜好，比如我个人比较喜欢low level方向的，尤其是IJCV和PAMI上面的文章，因此这方面也稍微多点，希望不要引起您的反感。如果有什么意见或者建议，欢迎mail我。文章和 资源我都会在我的csdn blog和sina ishare同步更新。\n此申明：这些论文的版权归作者及其出版商所有，请勿用于商业目的。\n个人blog：       http://blog.csdn.net/dcraw\n新浪iask地址：http://iask.sina.com.cn/u/2252291285/ish?folderid=868438\n本文的安排如下。第一部分是绪论。第二部分是图像处理中所需要用到的理论基础，主要是这个领域所涉及到的一些比较好的参考书籍。第三部分是计算机视觉中所涉 及到的信号处理和模式识别文章。由于图像处理与图像分析太难区分了，第四部分集中讨论了它们。第五部分是计算机视觉部分。最后是小结。\n二、 图像处理与计算机视觉相关的书籍\n1. 数学\n我们所说的图像处理实际上就是数字图像处理，是把真实世界中的连续三维随机信号投影到传感器的二维平面上，采样并量化后得到二维矩阵。数字图像处理就是二维 矩阵的处理，而从二维图像中恢复出三维场景就是计算机视觉的主要任务之一。这里面就涉及到了图像处理所涉及到的三个重要属性：连续性，二维矩阵，随机性。 所对应的数学知识是高等数学（微积分），线性代数（矩阵论），概率论和随机过程。这三门课也是考研数学的三个组成部分，构成了图像处理和计算机视觉最基础 的数学基础。如果想要更进一步，就要到网上搜搜林达华推荐的数学书目了。\n2. 信号处理\n图像处理其实就是二维和三维信号处理，而处理的信号又有一定的随机性，因此经典信号处理和随机信号处理都是图像处理和计算机视觉中必备的理论基础。\n2.1经典信号处理\n信号与系统(第2版) Alan V.Oppenheim等著 刘树棠译\n离散时间信号处理(第2版) A.V.奥本海姆等著 刘树棠译\n数字信号处理:理论算法与实现 胡广书 (编者)\n2.2随机信号处理\n现代信号处理 张贤达著\n统计信号处理基础:估计与检测理论 Steven M.Kay等著 罗鹏飞等译\n自适应滤波器原理(第4版) Simon Haykin著 郑宝玉等译\n2.3 小波变换\n信号处理的小波导引:稀疏方法(原书第3版) tephane Malla著, 戴道清等译\n2.4 信息论\n信息论基础(原书第2版) Thomas M.Cover等著 阮吉寿等译\n3. 模式识别\nPattern Recognition and Machine Learning Bishop, Christopher M. Springer\n模式识别(英文版)(第4版) 西奥多里德斯著\nPattern Classification (2nd Edition) Richard O. Duda等著\nStatistical Pattern Recognition, 3rd Edition Andrew R. Webb等著\n模式识别(第3版) 张学工著\n4. 图像处理与计算机视觉的书籍推荐\n图像处理，分析与机器视觉 第三版 Sonka等著 艾海舟等译\nImage Processing, Analysis and Machine Vision\n( 附：这本书是图像处理与计算机视觉里面比较全的一本书了，几乎涵盖了图像视觉领域的各个方面。中文版的个人感觉也还可以，值得一看。)\n数字图像处理 第三版 冈萨雷斯等著\nDigital Image Processing\n(附：数字图像处理永远的经典，现在已经出到了第三版，相当给力。我的导师曾经说过，这本书写的很优美，对写英文论文也很有帮助，建议购买英文版的。)\n计算机视觉：理论与算法 Richard Szeliski著\nComputer Vision: Theory and Algorithm\n(附：微软的Szeliski写的一本最新的计算机视觉著作。内容非常丰富，尤其包括了作者的研究兴趣，比如一般的书里面都没有的Image Stitching和                       Image Matting等。这也从另一个侧面说明这本书的通用性不如Sonka的那本。不过作者开放了这本书的电子版，可以有选择性的阅读。\nhttp://szeliski.org/Book/\nMultiple View Geometry in Computer Vision 第二版Harley等著\n引用达一万多次的经典书籍了。第二版到处都有电子版的。第一版曾出过中文版的，后来绝版了。网上也可以找到中英文版的电子版。)\n计算机视觉：一种现代方法 DA Forsyth等著\nComputer Vision: A Modern Approach\nMIT的经典教材。虽然已经过去十年了，还是值得一读。期待第二版\nMachine vision: theory, algorithms, practicalities 第三版 Davies著\n(附：为数不多的英国人写的书，偏向于工业应用。)\n数字图像处理 第四版 Pratt著\nDigital Image Processing\n(附：写作风格独树一帜，也是图像处理领域很不错的一本书。网上也可以找到非常清晰的电子版。)\n5. 小结\n罗嗦了这么多，实际上就是几个建议：\n（1）基础书千万不可以扔，也不能低价处理给同学或者师弟师妹。不然到时候还得一本本从书店再买回来的。钱是一方面的问题，对着全新的书看完全没有看自己当年上过的课本有感觉。\n（2）遇到有相关的课，果断选修或者蹭之，比如随机过程，小波分析，模式识别，机器学习，数据挖掘，现代信号处理甚至泛函。多一些理论积累对将来科研和工作都有好处。\n（3）资金允许的话可以多囤一些经典的书，有的时候从牙缝里面省一点都可以买一本好书。不过千万不要像我一样只囤不看。\n三、 计算机视觉中的信号处理与模式识别\n从本章开始，进入本文的核心章节。一共分三章，分别讲述信号处理与模式识别，图像处理与分析以及计算机视觉。与其说是讲述，不如说是一些经典文章的罗列以及 自己的简单点评。与前一个版本不同的是，这次把所有的文章按类别归了类，并且增加了很多文献。分类的时候并没有按照传统的分类方法，而是划分成了一个个小 的门类，比如SIFT，Harris都作为了单独的一类，虽然它们都可以划分到特征提取里面去。这样做的目的是希望能突出这些比较实用且比较流行的方法。 为了以后维护的方便，按照字母顺序排的序。\n1. Boosting\nBoosting是最近十来年来最成功的一种模式识别方法之一，个人认为可以和SVM并称为模式识别双子星。它真正实现了“三个臭皮匠，赛过诸葛亮”。只要保证每个基本分 类器的正确率超过50%，就可以实现组合成任意精度的分类器。这样就可以使用最简单的线性分类器。Boosting在计算机视觉中的最成功的应用无疑就是 Viola-Jones提出的基于Haar特征的人脸检测方案。听起来似乎不可思议，但Haar+Adaboost确实在人脸检测上取得了巨大的成功，已 经成了工业界的事实标准，并且逐步推广到其他物体的检测。\nRainer Lienhart在2002 ICIP发表的这篇文章是Haar+Adaboost的最好的扩展，他把原始的两个方向的Haar特征扩展到了四个方向，他本人是OpenCV积极的参与 者。现在OpenCV的库里面实现的Cascade Classification就包含了他的方法。这也说明了盛会（如ICIP，ICPR，ICASSP）也有好文章啊，只要用心去发掘。\n[1997] A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\n[1998] Boosting the margin A new explanation for the effectiveness of voting methods\n[2002 ICIP TR] Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection\n[2003] The Boosting Approach to Machine Learning An Overview\n[2004 IJCV] Robust Real-time Face Detection\n[1989 PAMI] Unsupervised Optimal Fuzzy Clustering\n[1991 PAMI] A validity measure for fuzzy clustering\n[1995 PAMI] On cluster validity for the fuzzy c-means model\n[1998] Some New Indexes of Cluster Validity\n[1999 ACM] Data Clustering A Review\n[1999 JIIS] On Clustering Validation Techniques\n[2001] Estimating the number of clusters in a dataset via the Gap statistic\n[2001 NIPS] On Spectral Clustering\n[2002] A stability based method for discovering structure in clustered data\n[2007] A tutorial on spectral clustering\n[2006 TIT] Compressed Sensing\n[2008 SPM] An Introduction to Compressive Sampling\n[2011 TSP] Structured Compressed Sensing From Theory to Applications\n4. Decision Trees\n对决策树感兴趣的同学这篇文章是非看不可的了。\n[1986] Introduction to Decision Trees\n5. Dynamical Programming\n动态规划也是一个比较使用的方法，这里挑选了一篇PAMI的文章以及一篇Book Chapter\n[1990 PAMI] using dynamic programming for solving variational problems in vision\n[Book Chapter] Dynamic Programming\n6. Expectation Maximization\nEM是计算机视觉中非常常见的一种方法，尤其是对参数的估计和拟合，比如高斯混合模型。EM和GMM在Bishop的PRML里单独的作为一章，讲的很不错。关于EM的tutorial，网上也可以搜到很多。\n[1977] Maximum likelihood from incomplete data via the EM algorithm\n[1996 SPM] The Expectation-Maximzation Algorithm\n7. Graphical Models\n伯克利的乔丹大师的Graphical Model，可以配合这Bishop的PRML一起看。\n[1999 ML] An Introduction to Variational Methods for Graphical Models\n8. Hidden Markov Model\nHMM在语音识别中发挥着巨大的作用。在信号处理和图像处理中也有一定的应用。最早接触它是跟小波和检索相关的，用HMM来描述小波系数之间的相互关系，并用来做检索。这里提供一篇1989年的经典综述，几篇HMM在小波，分割，检索和纹理上的应用以及一本比较早的中文电子书，现在也不知道作者是谁，在这里对作者表示感谢。\n[1989 ] A tutorial on hidden markov models and selected applications in speech recognition\n[1998 TSP] Wavelet-based statistical signal processing using hidden Markov models\n[2001 TIP] Multiscale image segmentation using wavelet-domain hidden Markov models\n[2002 TMM] Rotation invariant texture characterization and retrieval using steerable wavelet-domain hidden Markov models\n[2003 TIP] Wavelet-based texture analysis and synthesis using hidden Markov models\nHmm Chinese book.pdf\n9. Independent Component Analysis\n同PCA一样，独立成分分析在计算机视觉中也发挥着重要的作用。这里介绍两篇综述性的文章，最后一篇是第二篇的TR版本，内容差不多，但比较清楚一些。\n[1999] Independent Component Analysis A Tutorial\n[2000 NN] Independent component analysis algorithms and applications\n[2000] Independent Component Analysis Algorithms and Applications\n10. Information Theory\n计算机视觉中的信息论。这方面有一本很不错的书Information Theory in Computer Vision and Pattern Recognition。这本书有电子版，如果需要用到的话，也可以参考这本书。\n[1995 NC] An Information-Maximization Approach to Blind Separation and Blind Deconvolution\n[2010] An information theory perspective on computational vision\n11. Kalman Filter\n这个话题在张贤达老师的现代信号处理里面讲的比较深入，还给出了一个有趣的例子。这里列出了Kalman的最早的论文以及几篇综述，还有Unscented Kalman Filter。同时也有一篇Kalman Filter在跟踪中的应用以及两本电子书。\n[1960 Kalman] A New Approach to Linear Filtering and Prediction Problems Kalman\n[1970] Least-squares estimation_from Gauss to Kalman\n[1997 SPIE] A New Extension of the Kalman Filter to Nonlinear System\n[2000] The Unscented Kalman Filter for Nonlinear Estimation\n[2001 Siggraph] An Introduction to the Kalman Filter_full\n[2003] A Study of the Kalman Filter applied to Visual Tracking\n12. Pattern Recognition and Machine Learning\n模式识别名气比较大的几篇综述\n[2000 PAMI] Statistical pattern recognition a review\n[2004 CSVT] An Introduction to Biometric Recognition\n[2010 SPM] Machine Learning in Medical Imaging\n13. Principal Component Analysis\n著名的PCA，在特征的表示和特征降维上非常有用。\n[2001 PAMI] PCA versus LDA\n[2001] Nonlinear component analysis as a kernel eigenvalue problem\n[2002] A Tutorial on Principal Component Analysis\n[2009] A Tutorial on Principal Component Analysis\n[2011] Robust Principal Component Analysis\n[Book Chapter] Singular Value Decomposition and Principal Component Analysis\n14. Random Forest\n随机森林\n[2001 ML] Random Forests\n[2009 BMVC] Performance Evaluation of RANSAC Family\n16. Singular Value Decomposition\n对于非方阵来说，就是SVD发挥作用的时刻了。一般的模式识别书都会介绍到SVD。这里列出了K-SVD以及一篇Book Chapter\n[2006 TSP] K-SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation\n[Book Chapter] Singular Value Decomposition and Principal Component Analysis\n17. Sparse Representation\n这里主要是Proceeding of IEEE上的几篇文章\n[2009 PAMI] Robust Face Recognition via Sparse Representation\n[2009 PIEEE] Image Decomposition and Separation Using Sparse Representations An Overview\n[2010 PIEEE] Dictionaries for Sparse Representation Modeling\n[2010 PIEEE] It's All About the Data\n[2010 PIEEE] Matrix Completion With Noise\n[2010 PIEEE] On the Role of Sparse and Redundant Representations in Image Processing\n[2010 PIEEE] Sparse Representation for Computer Vision and Pattern Recognition\n[2011 SPM] Directionary Learning\n18. Support Vector Machines\n[1998] A Tutorial on Support Vector Machines for Pattern Recognition\n[2004] LIBSVM A Library for Support Vector Machines\n19. Wavelet\n在小波变换之前，时频分析的工具只有傅立叶变换。众所周知，傅立叶变换在时域没有分辨率，不能捕捉局部频域信息。虽然短时傅立叶变换克服了这个缺点，但只能刻画恒定窗口的频率特性，并且不能很好的扩展到二维。小波变换的出现很好的解决了时频分析的问题，作为一种多分辨率分析工具，在图像处理中得到了极大的发展和应用。在小波变换的发展过程中，有几个人是不得不提的，Mallat， Daubechies，Vetteri， M.N.Do， Swelden，Donoho。Mallat和Daubechies奠定了第一代小波的框架，他们的著作更是小波变换的必读之作，相对来说，小波十讲太偏数学了，比较难懂。而Mallat的信号处理的小波导引更偏应用一点。Swelden提出了第二代小波，使小波变换能够快速方便的实现，他的功劳有点类似于FFT。而Donoho，Vetteri，Mallat及其学生们提出了Ridgelet, Curvelet, Bandelet,Contourlet等几何小波变换，让小波变换有了方向性，更便于压缩，去噪等任务。尤其要提的是M.N.Do，他是一个越南人，得过IMO的银牌，在这个领域著作颇丰。我们国家每年都有5个左右的IMO金牌，希望也有一两个进入这个领域，能够也让我等也敬仰一下。而不是一股脑的都进入金融，管理这种跟数学没有多大关系的行业，呵呵。很希望能看到中国的陶哲轩，中国的M.N.Do。\n说到小波，就不得不提JPEG2000。在JPEG2000中使用了Swelden和Daubechies提出的用提升算法实现的9/7小波和5/3小波。如果对比JPEG和JPEG2000，就会发现JPEG2000比JPEG在性能方面有太多的提升。本来我以为JPEG2000的普及只是时间的问题。但现在看来，这个想法太Naive了。现在已经过去十几年了，JPEG2000依然没有任何出头的迹象。不得不说，工业界的惯性力量太强大了。如果以前的东西没有什么硬伤的话，想改变太难了。不巧的是，JPEG2000的种种优点在最近的硬件上已经有了很大的提升。压缩率？现在动辄1T，2T的硬盘，没人太在意压缩率。渐进传输？现在的网速包括无线传输的速度已经相当快了，渐进传输也不是什么优势。感觉现在做图像压缩越来越没有前途了，从最近的会议和期刊文档也可以看出这个趋势。不管怎么说，JPEG2000的Overview还是可以看看的。\n[1989 PAMI] A theory for multiresolution signal decomposition__the wavelet representation\n[1996 PAMI] Image Representation using 2D Gabor Wavelet\n[1998 ] FACTORING WAVELET TRANSFORMS INTO LIFTING STEPS\n[1998] The Lifting Scheme_ A Construction Of Second Generation Wavelets\n[2000 TCE] The JPEG2000 still image coding system_ an overview\n[2002 TIP] The curvelet transform for image denoising\n[2003 TIP] Gray and color image contrast enhancement by the curvelet transform\n[2003 TIP] Mathematical Properties of the jpeg2000 wavelet filters\n[2003 TIP] The finite ridgelet transform for image representation\n[2005 TIP] Sparse Geometric Image Representations With Bandelets\n[2005 TIP] The Contourlet Transform_ An Efficient Directional Multiresolution Image Representation\n[2010 SPM] The Curvelet Transform\n四、 图像处理与分析\n本章主要讨论图像处理与分析。虽然后面计算机视觉部分的有些内容比如特征提取等也可以归结到图像分析中来，但鉴于它们与计算机视觉的紧密联系，以及它们的出处，没有把它们纳入到图像处理与分析中来。同样，这里面也有一些也可以划归到计算机视觉中去。这都不重要，只要知道有这么个方法，能为自己所用，或者从中得到灵感，这就够了。\n1. Bilateral Filter\nBilateral Filter俗称双边滤波器是一种简单实用的具有保持边缘作用的平缓滤波器，由Tomasi等在1998年提出。它现在已经发挥着重大作用，尤其是在HDR领域。\n[1998 ICCV] Bilateral Filtering for Gray and Color Images\n[2008 TIP] Adaptive Bilateral Filter for Sharpness Enhancement and Noise Removal\n2. Color\n如果对颜色的形成有一定的了解，能比较深刻的理解一些算法。这方面推荐冈萨雷斯的数字图像处理中的相关章节以及Sharma在Digital Color Imaging Handbook中的第一章“Color fundamentals for digital imaging”。跟颜色相关的知识包括Gamma，颜色空间转换，颜色索引以及肤色模型等，这其中也包括著名的EMD。\n[1991 IJCV] Color Indexing\n[2000 IJCV] The Earth Mover's Distance as a Metric for Image Retrieval\n[2001 PAMI] Color invariance\n[2002 IJCV] Statistical Color Models with Application to Skin Detection\n[2003] A review of RGB color spaces\n[2007 PR]A survey of skin-color modeling and detection methods\nGamma.pdf\nGammaFAQ.pdf\n3. Compression and Encoding\n个人以为图像压缩编码并不是当前很热的一个话题，原因前面已经提到过。这里可以看看一篇对编码方面的展望文章\n[2005 IEEE] Trends and perspectives in image and video coding\n4. Contrast Enhancement\n对比度增强一直是图像处理中的一个恒久话题，一般来说都是基于直方图的，比如直方图均衡化。冈萨雷斯的书里面对这个话题讲的比较透彻。这里推荐几篇个人认为不错的文章。\n[2002 IJCV] Vision and the Atmosphere\n[2003 TIP] Gray and color image contrast enhancement by the curvelet transform\n[2006 TIP] Gray-level grouping (GLG) an automatic method for optimized image contrast enhancement-part II\n[2006 TIP] Gray-level grouping (GLG) an automatic method for optimized image contrast Enhancement-part I\n[2007 TIP] Transform Coefficient Histogram-Based Image Enhancement Algorithms Using Contrast Entropy\n[2009 TIP] A Histogram Modification Framework and Its Application for Image Contrast Enhancement\n5. Deblur (Restoration)\n图像恢复或者图像去模糊一直是一个非常难的问题，尤其是盲图像恢复。港中文的jiaya jia老师在这方面做的不错，他在主页也给出了可执行文件。这方面的内容也建议看冈萨雷斯的书。这里列出了几篇口碑比较好的文献，包括古老的Richardson-Lucy方法，几篇盲图像恢复的综述以及最近的几篇文章，尤以Fergus和Jiaya Jia的为经典。\n[1972] Bayesian-Based Iterative Method of Image Restoration\n[1974] an iterative technique for the rectification of observed distributions\n[1990 IEEE] Iterative methods for image deblurring\n[1996 SPM] Blind Image Deconvolution\n[1997 SPM] Digital image restoration\n[2005] Digital Image Reconstruction - Deblurring and Denoising\n[2006 Siggraph] Removing Camera Shake from a Single Photograph\n[2008 Siggraph] High-quality Motion Deblurring from a Single Image\n[2011 PAMI] Richardson-Lucy Deblurring for Scenes under a Projective Motion Path\n6. Dehazing and Defog\n严格来说去雾化也算是图像对比度增强的一种。这方面最近比较好的工作就是He kaiming等提出的Dark Channel方法。这篇论文也获得了2009的CVPR 最佳论文奖。2这位003年的广东高考状元已经于2011年从港中文博士毕业加入MSRA（估计当时也就二十五六岁吧），相当了不起。\n[2008 Siggraph] Single Image Dehazing\n[2009 CVPR] Single Image Haze Removal Using Dark Channel Prior\n[2011 PAMI] Single Image Haze Removal Using Dark Channel Prior\n7. Denoising\n图像去噪也是图像处理中的一个经典问题，在数码摄影中尤其重要。主要的方法有基于小波的方法和基于偏微分方程的方法。\n[1992 SIAM] Image selective smoothing and edge detection by nonlinear diffusion. II\n[1992 SIAM] Image selective smoothing and edge detection by nonlinear diffusion\n[1992] Nonlinear total variation based noise removal algorithms\n[1994 SIAM] Signal and image restoration using shock filters and anisotropic diffusion\n[1995 TIT] De-noising by soft-thresholding\n[1998 TIP] Orientation diffusions\n[2000 TIP] Adaptive wavelet thresholding for image denoising and compression\n[2000 TIP] Fourth-order partial differential equations for noise removal\n[2001] Denoising through wavelet shrinkage\n[2002 TIP] The Curvelet Transform for Image Denoising\n[2003 TIP] Noise removal using fourth-order partial differential equation with applications to medical magnetic resonance images in space and time\n[2008 PAMI] Automatic Estimation and Removal of Noise from a Single Image\n[2009 TIP] Is Denoising Dead\n8. Edge Detection\n边缘检测也是图像处理中的一个基本任务。传统的边缘检测方法有基于梯度算子，尤其是Sobel算子，以及经典的Canny边缘检测。到现在，Canny边缘检测及其思想仍在广泛使用。关于Canny算法的具体细节可以在Sonka的书以及canny自己的论文中找到，网上也可以搜到。最快最直接的方法就是看OpenCV的源代码，非常好懂。在边缘检测方面，Berkeley的大牛J Malik和他的学生在2004年的PAMI提出的方法效果非常好，当然也比较复杂。在复杂度要求不高的情况下，还是值得一试的。MIT的Bill Freeman早期的代表作Steerable Filter在边缘检测方面效果也非常好，并且便于实现。这里给出了几篇比较好的文献，包括一篇最新的综述。边缘检测是图像处理和计算机视觉中任何方向都无法逃避的一个问题，这方面研究多深都不为过。\n[1980] theory of edge detection\n[1983 Canny Thesis] find edge\n[1986 PAMI] A Computational Approach to Edge Detection\n[1990 PAMI] Scale-space and edge detection using anisotropic diffusion\n[1991 PAMI] The design and use of steerable filters\n[1995 PR] Multiresolution edge detection techniques\n[1996 TIP] Optimal edge detection in two-dimensional images\n[1998 PAMI] Local Scale Control for Edge Detection and Blur Estimation\n[2003 PAMI] Statistical edge detection_ learning and evaluating edge cues\n[2004 IEEE] Edge Detection Revisited\n[2004 PAMI] Design of steerable filters for feature detection using canny-like criteria\n[2004 PAMI] Learning to Detect Natural Image Boundaries Using Local Brightness, Color, and Texture Cues\n[2011 IVC] Edge and line oriented contour detection State of the art\n9. Graph Cut\n基于图割的图像分割算法。在这方面没有研究，仅仅列出几篇引用比较高的文献。这里又见J Malik，当然还有华人杰出学者Jianbo Shi，他的主页非常搞笑，在醒目的位置标注Do not fly China Eastern Airlines ... 看来是被坑过，而且坑的比较厉害。这个领域，俄罗斯人比较厉害。\n[2000 PAMI] Normalized cuts and image segmentation\n[2001 PAMI] Fast approximate energy minimization via graph cuts\n[2004 PAMI] What energy functions can be minimized via graph cuts\n10. Hough Transform\n虽然霍夫变换可以扩展到广义霍夫变换，但最常用的还是检测圆和直线。这方面同样推荐看OpenCV的源代码，一目了然。Matas在2000年提出的PPHT已经集成到OpenCV中去了。\n[1986 CVGIU] A Survey of the Hough Transform\n[1989] A Comparative study of Hough transform methods for circle finding\n[1992 PAMI] Shapes recognition using the straight line Hough transform_ theory and generalization\n[1997 PR] Extraction of line features in a noisy image\n[2000 CVIU] Robust Detection of Lines Using the Progressive Probabilistic Hough Transform\n11. Image Interpolation\n图像插值，偶尔也用得上。一般来说，双三次也就够了\n[2000 TMI] Interpolation revisited\n[2008 Fnd] Image and Video Matting A Survey\n[2008 PAMI] A Closed-Form Solution to Natural Image Matting\n[2008 PAMI] Spectral Matting\n13. Image Modeling\n图像的统计模型。这方面有一本专门的著作Natural Image Statistics\n[1994] The statistics of natural images\n[2003 JMIV] On Advances in Statistical Modeling of Natural Images\n[2009 IJCV] Fields of Experts\n[2009 PAMI] Modeling multiscale subbands of photographic images with fields of Gaussian scale mixtures\n14. Image Quality Assessment\n在图像质量评价方面，Bovik是首屈一指的。这位老师也很有意思，作为编辑出版了很多书。他也是IEEE的Fellow\n[2004 TIP] Image quality assessment from error visibility to structural similarity\n[2011 TIP] blind image quality assessment From Natural Scene Statistics to Perceptual Quality\n15. Image Registration\n图像配准最早的应用在医学图像上，在图像融合之前需要对图像进行配准。在现在的计算机视觉中，配准也是一个需要理解的概念，比如跟踪，拼接等。在KLT中，也会涉及到配准。这里主要是综述文献。\n[1992 MIA] Image matching as a diffusion process\n[1992 PAMI] A Method for Registration of 3-D shapes\n[1992] a survey of image registration techniques\n[1998 MIA] A survey of medical image registration\n[2003 IVC] Image registration methods a survey\n[2003 TMI] Mutual-Information-Based Registration of Medical Survey\n[2011 TIP] Hairis registration\n16. Image Retrieval\n图像检索曾经很热，在2000年之后似乎消停了一段时间。最近各种图像的不变性特征提出来之后，再加上互联网搜索的商业需求，这个方向似乎又要火起来了，尤其是在商业界，比如淘淘搜。这仍然是一个非常值得关注的方面。而且图像检索与目标识别具有相通之处，比如特征提取和特征降维。这方面的文章值得一读。在最后给出了两篇Book chapter，其中一篇还是中文的。\n[2000 PAMI] Content-based image retrieval at the end of the early years\n[2000 TIP] PicToSeek Combining Color and Shape Invariant Features for Image Retrieval\n[2002] Content-Based Image Retrieval Systems A Survey\n[2008] Content-Based Image Retrieval-Literature Survey\n[2010] Plant Image Retrieval Using Color,Shape and Texture Features\n[2012 PAMI] A Multimedia Retrieval Framework Based on Semi-Supervised Ranking and Relevance Feedback\nCBIR Chinese\nfundament of cbir\n17. Image Segmentation\n图像分割，非常基本但又非常难的一个问题。建议看Sonka和冈萨雷斯的书。这里给出几篇比较好的文章，再次看到了J Malik。他们给出了源代码和测试集，有兴趣的话可以试试。\n[2004 IJCV] Efficient Graph-Based Image Segmentation\n[2008 CVIU] Image segmentation evaluation A survey of unsupervised methods\n[2011 PAMI] Contour Detection and Hierarchical Image Segmentation\n18. Level Set\n大名鼎鼎的水平集，解决了Snake固有的缺点。Level set的两位提出者Sethian和Osher最后反目，实在让人遗憾。个人以为，这种方法除了迭代比较费时，在真实场景中的表现让人生疑。不过，2008年ECCV上的PWP方法在结果上很吸引人。在重初始化方面，Chunming Li给出了比较好的解决方案\n[1995 PAMI] Shape modeling with front propagation_ a level set approach\n[2001 JCP] Level Set Methods_ An Overview and Some Recent Results\n[2005 CVIU] Geodesic active regions and level set methods for motion estimation and tracking\n[2007 IJCV] A Review of Statistical Approaches to Level Set Segmentation\n[2008 ECCV] Robust Real-Time Visual Tracking using Pixel-Wise Posteriors\n[2010 TIP] Distance Regularized Level Set Evolution and its Application to Image Segmentation\n19. Pyramid\n其实小波变换就是一种金字塔分解算法，而且具有无失真重构和非冗余的优点。Adelson在1983年提出的Pyramid优点是比较简单，实现起来比较方便。\n[1983] The Laplacian Pyramid as a Compact Image Code\n[1993 PAMI] Image representation via a finite Radon transform\n[1993 TIP] The fast discrete radon transform I theory\n[2007 IVC] Generalised finite radon transform for N×N images\n21. Scale Space\n尺度空间滤波在现代不变特征中是一个非常重要的概念，有人说SIFT的提出者Lowe是不变特征之父，而Linderburg是不变特征之母。虽然尺度空间滤波是Witkin最早提出的，但其理论体系的完善和应用还是Linderburg的功劳。其在1998年IJCV上的两篇文章值得一读，不管是特征提取方面还是边缘检测方面。\n[1987] Scale-space filtering\n[1990 PAMI] Scale-Space for Discrete Signals\n[1994] Scale-space theory A basic tool for analysing structures at different scales\n[1998 IJCV] Edge Detection and Ridge Detection with Automatic Scale Selection\n[1998 IJCV] Feature Detection with Automatic Scale Selection\n22. Snake\n活动轮廓模型，改变了传统的图像分割的方法，用能量收缩的方法得到一个统计意义上的能量最小（最大）的边缘。\n[1987 IJCV] Snakes Active Contour Models\n[1996 ] deformable model in medical image A Survey\n[1997 IJCV] geodesic active contour\n[1998 TIP] Snakes, shapes, and gradient vector flow\n[2000 PAMI] Geodesic active contours and level sets for the detection and tracking of moving objects\n[2001 TIP] Active contours without edges\n23. Super Resolution\n超分辨率分析。对这个方向没有研究，简单列几篇文章。其中Yang Jianchao的那篇在IEEE上的下载率一直居高不下。\n[2002] Example-Based Super-Resolution\n[2009 ICCV] Super-Resolution from a Single Image\n[2010 TIP] Image Super-Resolution Via Sparse Representation\n24. Thresholding\n阈值分割是一种简单有效的图像分割算法。这个topic在冈萨雷斯的书里面讲的比较多。这里列出OTSU的原始文章以及一篇不错的综述。\n[1979 IEEE] OTSU A threshold selection method from gray-level histograms\n[2001 JISE] A Fast Algorithm for Multilevel Thresholding\n[2004 JEI] Survey over image thresholding techniques and quantitative performance evaluation\n25. Watershed\n分水岭算法是一种非常有效的图像分割算法，它克服了传统的阈值分割方法的缺点，尤其是Marker-Controlled Watershed，值得关注。Watershed在冈萨雷斯的书里面讲的比较详细。\n[1991 PAMI] Watersheds in digital spaces an efficient algorithm based on immersion simulations\n[2001]The Watershed Transform Definitions, Algorithms and Parallelizat on Strategies\n五、 计算机视觉\n这一章是计算机视觉部分，主要侧重在底层特征提取，视频分析，跟踪，目标检测和识别方面等方面。对于自己不太熟悉的领域比如摄像机标定和立体视觉，仅仅列出上google上引用次数比较多的文献。有一些刚刚出版的文章，个人非常喜欢，也列出来了。\n1. Active Appearance Models\n活动表观模型和活动轮廓模型基本思想来源Snake，现在在人脸三维建模方面得到了很成功的应用，这里列出了三篇最早最经典的文章。对这个领域有兴趣的可以从这三篇文章开始入手。\n[1998 ECCV] Active Appearance Models\n[2001 PAMI] Active Appearance Models\n2. Active Shape Models\n[1995 CVIU]Active Shape Models-Their Training and Application\n3. Background modeling and subtraction\n背景建模一直是视频分析尤其是目标检测中的一项关键技术。虽然最近一直有一些新技术的产生，demo效果也很好，比如基于dynamical texture的方法。但最经典的还是Stauffer等在1999年和2000年提出的GMM方法，他们最大的贡献在于不用EM去做高斯拟合，而是采用了一种迭代的算法，这样就不需要保存很多帧的数据，节省了buffer。Zivkovic在2004年的ICPR和PAMI上提出了动态确定高斯数目的方法，把混合高斯模型做到了极致。这种方法效果也很好，而且易于实现。在OpenCV中有现成的函数可以调用。在背景建模大家族里，无参数方法（2000 ECCV）和Vibe方法也值得关注。\n[1997 PAMI] Pfinder Real-Time Tracking of the Human Body\n[1999 CVPR] Adaptive background mixture models for real-time tracking\n[1999 ICCV] Wallflower Principles and Practice of Background Maintenance\n[2000 ECCV] Non-parametric Model for Background Subtraction\n[2000 PAMI] Learning Patterns of Activity Using Real-Time Tracking\n[2002 PIEEE] Background and foreground modeling using nonparametric\nkernel density estimation for visual surveillance\n[2004 ICPR] Improved adaptive Gaussian mixture model for background subtraction\n[2004 PAMI] Recursive unsupervised learning of finite mixture models\n[2006 PRL] Efficient adaptive density estimation per image pixel for the task of background subtraction\n[2011 TIP] ViBe A Universal Background Subtraction Algorithm for Video Sequences\n4. Bag of Words\n词袋，在这方面暂时没有什么研究。列出三篇引用率很高的文章，以后逐步解剖之。\n[2003 ICCV] Video Google A Text Retrieval Approach to Object Matching in Videos\n[2004 ECCV] Visual Categorization with Bags of Keypoints\n[2006 CVPR] Beyond bags of features Spatial pyramid matching for recognizing natural scene categories\n5. BRIEF\nBRIEF是Binary Robust Independent Elementary Features的简称，是近年来比较受关注的特征描述的方法。ORB也是基于BRIEF的。\n[2010 ECCV] BRIEF Binary Robust Independent Elementary Features\n[2011 ICCV] ORB an efficient alternative to SIFT or SURF\n[2012 PAMI] BRIEF Computing a Local Binary Descriptor Very Fast\n6. Camera Calibration and Stereo Vision\n非常不熟悉的领域。仅仅列出了十来篇重要的文献，供以后学习。\n[1979 Marr] A Computational Theory of Human Stereo Vision\n[1985] Computational vision and regularization theory\n[1987 IEEE] A versatile camera calibration technique for\nhigh-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses\n[1987] Probabilistic Solution of Ill-Posed Problems in Computational Vision\n[1988 PIEEE] Ill-Posed Problems in Early Vision\n[1989 IJCV] Kalman Filter-based Algorithms for Estimating Depth from Image Sequences\n[1990 IJCV] Relative Orientation\n[1990 IJCV] Using vanishing points for camera calibration\n[1992 ECCV] Camera self-calibration Theory and experiments\n[1992 IJCV] A theory of self-calibration of a moving camera\n[1992 PAMI] Camera calibration with distortion models and accuracy evaluation\n[1994 IJCV] The Fundamental Matrix Theory, Algorithms, and Stability Analysis\n[1994 PAMI] a stereo matching algorithm with an adaptive window theory and experiment\n[1999 ICCV] Flexible camera calibration by viewing a plane from unknown orientations\n[1999 IWAR] Marker tracking and hmd calibration for a video-based augmented reality conferencing system\n[2000 PAMI] A flexible new technique for camera calibration\n7. Color and Histogram Feature\n这里面主要来源于图像检索，早期的图像检测基本基于全局的特征，其中最显著的就是颜色特征。这一部分可以和前面的Color知识放在一起的。\n[1995 SPIE] Similarity of color images\n[1996 PR] IMAGE RETRIEVAL USING COLOR AND SHAPE\n[1996] comparing images using color coherence vectors\n[1997 ] Image Indexing Using Color Correlograms\n[2001 TIP] An Efficient Color Representation for Image Retrieval\n[2009 CVIU] Performance evaluation of local colour invariants\n8. Deformable Part Model\n大红大热的DPM，在OpenCV中有一个专门的topic讲DPM和latent svm\n[2008 CVPR] A Discriminatively Trained, Multiscale, Deformable Part Model\n[2010 CVPR] Cascade Object Detection with Deformable Part Models\n[2010 PAMI] Object Detection with Discriminatively Trained Part-Based Models\n9. Distance Transformations\n距离变换，在OpenCV中也有实现。用来在二值图像中寻找种子点非常方便。\n[1986 CVGIP] Distance Transformations in Digital Images\n[2008 ACM] 2D Euclidean Distance Transform Algorithms A Comparative Survey\n10. Face Detection\n最成熟最有名的当属Haar+Adaboost\n[1998 PAMI] Neural Network-Based Face Detection\n[2002 PAMI] Detecting faces in images a survey\n[2002 PAMI] Face Detection in Color Images\n[2004 IJCV] Robust Real-Time Face Detection\n11. Face Recognition\n不熟悉，简单罗列之。\n[1991] Face Recognition Using Eigenfaces\n[2000 PAMI] Automatic Analysis of Facial Expressions The State of the Art\n[2000] Face Recognition A Literature Survey\n[2006 PR] Face recognition from a single image per person A survey\n[2009 PAMI] Robust Face Recognition via Sparse Representation\n12. FAST\n用机器学习的方法来提取角点，号称很快很好。\n[2006 ECCV] Machine learning for high-speed corner detection\n[2010 PAMI] Faster and Better A Machine Learning Approach to Corner Detection\n13. Feature Extraction\n这里的特征主要都是各种不变性特征，SIFT，Harris，MSER等也属于这一类。把它们单独列出来是因为这些方法更流行一点。关于不变性特征，王永明与王贵锦合著的《图像局部不变性特征与描述》写的还不错。Mikolajczyk在2005年的PAMI上的文章以及2007年的综述是不错的学习材料。\n[1989 PAMI] On the detection of dominant points on digital curves\n[1997 IJCV] SUSAN—A New Approach to Low Level Image Processing\n[2004 IJCV] Matching Widely Separated Views Based on Affine Invariant Regions\n[2004 IJCV] Scale & Affine Invariant Interest Point Detectors\n[2005 PAMI] A performance evaluation of local descriptors\n[2006 IJCV] A Comparison of Affine Region Detectors\n[2007 FAT] Local Invariant Feature Detectors - A Survey\n[2011 IJCV] Evaluation of Interest Point Detectors and Feature Descriptors\n14. Feature Matching\nFua课题组在今年PAMI上的一篇文章，感觉还不错\n[2012 PAMI] LDAHash Improved Matching with Smaller Descriptors\n15. Harris\n虽然过去了很多年，Harris角点检测仍然广泛使用，而且基于它有很多变形。如果仔细看了这种方法，从直观也可以感觉到这是一种很稳健的方法。\n[1988 Harris] A combined corner and edge detector\n16. Histograms of Oriented Gradients\nHoG方法也在OpenCV中实现了：HOGDescriptor。\n[2005 CVPR] Histograms of Oriented Gradients for Human Detection\nNavneetDalalThesis.pdf\n17. Image Distance\n[1993 PAMI] Comparing Images Using the Hausdorff Distance\n18. Image Stitching\n图像拼接，另一个相关的词是Panoramic。在Computer Vision: Algorithms and Applications一书中，有专门一章是讨论这个问题。这里的两面文章一篇是综述，一篇是这方面很经典的文章。\n[2006 Fnd] Image Alignment and Stitching A Tutorial\n[2007 IJCV] Automatic Panoramic Image Stitching using Invariant Features\n19. KLT\nKLT跟踪算法，基于Lucas-Kanade提出的配准算法。除了三篇很经典的文章，最后一篇给出了OpenCV实现KLT的细节。\n[1981] An Iterative Image Registration Technique with an Application to Stereo Vision full version\n[1994 CVPR] Good Features to Track\n[2004 IJCV] Lucas-Kanade 20 Years On A Unifying Framework\nPyramidal Implementation of the Lucas Kanade Feature Tracker OpenCV\n20. Local Binary Pattern\nLBP。OpenCV的Cascade分类器也支持LBP，用来取代Haar特征。\n[2002 PAMI] Multiresolution gray-scale and rotation Invariant Texture Classification with Local Binary Patterns\n[2004 ECCV] Face Recognition with Local Binary Patterns\n[2006 PAMI] Face Description with Local Binary Patterns\n[2011 TIP] Rotation-Invariant Image and Video Description With Local Binary Pattern Features\n21. Low-Level Vision\n关于Low level vision的两篇很不错的文章\n[1998 TIP] A general framework for low level vision\n[2000 IJCV] Learning Low-Level Vision\n22. Mean Shift\n均值漂移算法，在跟踪中非常流行的方法。Comaniciu在这个方面做出了重要的贡献。最后三篇，一篇是CVIU上的top download文章，一篇是最新的PAMI上关于Mean Shift的文章，一篇是OpenCV实现的文章。\n[1995 PAMI] Mean shift, mode seeking, and clustering\n[2002 PAMI] Mean shift a robust approach toward feature space analysis\n[2003 CVPR] Mean-shift blob tracking through scale space\n[2009 CVIU] Object tracking using SIFT features and mean shift\n[2012 PAMI] Mean Shift Trackers with Cross-Bin Metrics\nOpenCV Computer Vision Face Tracking For Use in a Perceptual User Interface\n23. MSER\n这篇文章发表在2002年的BMVC上，后来直接录用到2004年的IVC上，内容差不多。MSER在Sonka的书里面也有提到。\n[2002 BMVC] Robust Wide Baseline Stereo from Maximally Stable Extremal Regions\n[2003] MSER Author Presentation\n[2004 IVC] Robust wide-baseline stereo from maximally stable extremal regions\n[2011 PAMI] Are MSER Features Really Interesting\n24. Object Detection\n首先要说的是第一篇文章的作者，Kah-Kay Sung。他是MIT的博士，后来到新加坡国立任教，极具潜力的一个老师。不幸的是，他和他的妻子都在2000年的新加坡空难中遇难，让人唏嘘不已。\nhttp://en.wikipedia.org/wiki/Singapore_Airlines_Flight_006\n最后一篇文章也是Fua课题组的，作者给出的demo效果相当好。\n[1998 PAMI] Example-based learning for view-based human face detection\n[2003 IJCV] Learning the Statistics of People in Images and Video\n[2011 PAMI] Learning to Detect a Salient Object\n[2012 PAMI] A Real-Time Deformable Detector\n25. Object Tracking\n跟踪也是计算机视觉中的经典问题。粒子滤波，卡尔曼滤波，KLT，mean shift，光流都跟它有关系。这里列出的是传统意义上的跟踪，尤其值得一看的是2008的Survey和2003年的Kernel based tracking。\n[2003 PAMI] Kernel-based object tracking\n[2007 PAMI] Tracking People by Learning Their Appearance\n[2008 ACM] Object Tracking A Survey\n[2008 PAMI] Segmentation and Tracking of Multiple Humans in Crowded Environments\n[2011 PAMI] Hough Forests for Object Detection, Tracking, and Action Recognition\n[2011 PAMI] Robust Object Tracking with Online Multiple Instance Learning\n[2012 IJCV] PWP3D Real-Time Segmentation and Tracking of 3D Objects\n26. OCR\n一个非常成熟的领域，已经很好的商业化了。\n[1992 IEEE] Historical review of OCR research and development\nVideo OCR A Survey and Practitioner's Guide\n27. Optical Flow\n光流法，视频分析所必需掌握的一种算法。\n[1981 AI] Determine Optical Flow\n[1994 IJCV] Performance of optical flow techniques\n[1995 ACM] The Computation of Optical Flow\n[2004 TR] Tutorial Computing 2D and 3D Optical Flow\n[2005 BOOK] Optical Flow Estimation\n[2008 ECCV] Learning Optical Flow\n[2011 IJCV] A Database and Evaluation Methodology for Optical Flow\n28. Particle Filter\n粒子滤波，主要给出的是综述以及1998 IJCV上的关于粒子滤波发展早期的经典文章。\n[1998 IJCV] CONDENSATION—Conditional Density Propagation for Visual Tracking\n[2002 TSP] A tutorial on particle filters for online nonlinear non-Gaussian Bayesian tracking\n[2002 TSP] Particle filters for positioning, navigation, and tracking\n[2003 SPM] particle filter\n29. Pedestrian and Human detection\n仍然是综述类，关于行人和人体的运动检测和动作识别。\n[1999 CVIU] Visual analysis of human movement_ A survey\n[2001 CVIU] A Survey of Computer Vision-Based Human Motion Capture\n[2005 TIP] Image change detection algorithms a systematic survey\n[2006 CVIU] a survey of avdances in vision based human motion capture\n[2007 CVIU] Vision-based human motion analysis An overview\n[2007 IJCV] Pedestrian Detection via Periodic Motion Analysis\n[2007 PR] A survey of skin-color modeling and detection methods\n[2010 IVC] A survey on vision-based human action recognition\n[2012 PAMI] Pedestrian Detection An Evaluation of the State of the Art\n30. Scene Classification\n当相机越来越傻瓜化的时候，自动场景识别就非常重要。这是比拼谁家的Auto功能做的比较好的时候了。\n[2001 IJCV] Modeling the Shape of the Scene A Holistic Representation of the Spatial Envelope\n[2001 PAMI] Visual Word Ambiguity\n[2007 PAMI] A Thousand Words in a Scene\n[2010 PAMI] Evaluating Color Descriptors for Object and Scene Recognition\n[2011 PAMI] CENTRIST A Visual Descriptor for Scene Categorization\n31. Shadow Detection\n[2003 PAMI] Detecting moving shadows-- algorithms and evaluation\n32. Shape\n关于形状，主要是两个方面：形状的表示和形状的识别。形状的表示主要是从边缘或者区域当中提取不变性特征，用来做检索或者识别。这方面Sonka的书讲的比较系统。2008年的那篇综述在这方面也讲的不错。至于形状识别，最牛的当属J Malik等提出的Shape Context。\n[1993 PR] IMPROVED MOMENT INVARIANTS FOR SHAPE DISCRIMINATION\n[1993 PR] Pattern Recognition by Affine Moment Invariants\n[1996 PR] IMAGE RETRIEVAL USING COLOR AND SHAPE\n[2001 SMI] Shape matching similarity measures and algorithms\n[2002 PAMI] Shape matching and object recognition using shape contexts\n[2004 PR] Review of shape representation and description techniques\n[2006 PAMI] Integral Invariants for Shape Matching\n[2008] A Survey of Shape Feature Extraction Techniques\n33. SIFT\n关于SIFT，实在不需要介绍太多，一万多次的引用已经说明问题了。SURF和PCA-SIFT也是属于这个系列。后面列出了几篇跟SIFT有关的问题。\n[1999 ICCV] Object recognition from local scale-invariant features\n[2000 IJCV] Evaluation of Interest Point Detectors\n[2003 CVIU] Speeded-Up Robust Features (SURF)\n[2004 CVPR] PCA-SIFT A More Distinctive Representation for Local Image Descriptors\n[2004 IJCV] Distinctive Image Features from Scale-Invariant Keypoints\n[2010 IJCV] Improving Bag-of-Features for Large Scale Image Search\n[2011 PAMI] SIFTflow Dense Correspondence across Scenes and its Applications\n34. SLAM\nSimultaneous Localization and Mapping, 同步定位与建图。\nSLAM问题可以描述为: 机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置估计和地图进行自身定位,同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。\n[2002 PAMI] Simultaneous Localization and Map-Building Using Active Vision\n[2007 PAMI] MonoSLAM Real-Time Single Camera SLAM\n35. Texture Feature\n纹理特征也是物体识别和检索的一个重要特征集。\n[1973] Textural features for image classification\n[1979 ] Statistical and structural approaches to texture\n[1996 PAMI] Texture features for browsing and retrieval of image data\n[2002 PR] Brief review of invariant texture analysis methods\n[2012 TIP] Color Local Texture Features for Color Face Recognition\n36. TLD\nKadal创立了TLD，跟踪学习检测同步进行，达到稳健跟踪的目的。他的两个导师也是大名鼎鼎，一个是发明MSER的Matas，一个是Mikolajczyk。他还创立了一个公司TLD Vision s.r.o. 这里给出了他的系列文章，最后一篇是刚出来的PAMI。\n[2009] Online learning of robust object detectors during unstable tracking\n[2010 CVPR] P-N Learning Bootstrapping Binary Classifiers by Structural Constraints\n[2010 ICIP] FACE-TLD TRACKING-LEARNING-DETECTION APPLIED TO FACES\n[2012 PAMI] Tracking-Learning-Detection\n37. Video Surveillance\n前两篇是两个很有名的视频监控系统，里面包含了很丰富的信息量，比如CMU的那个系统里面的背景建模算法也是相当简单有效的。最后一篇是比较近的综述。\n[2000 CMU TR] A System for Video Surveillance and Monitoring\n[2000 PAMI] W4-- real-time surveillance of people and their activities\n[2008 MVA] The evolution of video surveillance an overview\n38. Viola-Jones\nHaar+Adaboost的弱弱联手，组成了最强大的利器。在OpenCV里面有它的实现，也可以选择用LBP来代替Haar特征。\n[2001 CVPR] Rapid object detection using a boosted cascade of simple features\n[2004 IJCV] Robust Real-time Face Detection\n六、 结束语\n历时一个多月，终于用业余时间把这些资料整理出来了，总算了却了一块心病，也不至于再看着一堆资料发愁了。以后可能会有些小修小补，但不会有太大的变化了。万里长征走完了第一步，剩下的就是理解和消化了。借新浪ishare共享出来，希望能够对你的科研也有一定的帮助。最后简单统计一下各个年份出现的频率。\n文章总数：372\n2012年： 10\n2011年： 20\n2010年： 20\n2009年： 14\n2008年： 18\n2007年： 13\n2006年： 14\n2005年： 9\n2004年： 24\n2003年： 22\n2002年： 21\n2001年： 21\n2000年： 23\n1999年： 10\n1998年： 22\n1997年： 8\n1996年： 9\n1995年： 9\n1994年： 7\n1993年： 5\n1992年： 11\n1991年： 5\n1990年： 6\n1980-1989： 22\n1960-1979： 9"}
{"content2":"最近公众号看到讲人工智能、计算机视觉、机器学习三者关系的，摘录一段朱松纯（加州大学洛杉矶分校UCLA统计学和计算机科学教授）原话。\n关于人工智能和计算机视觉：\n人工智能是在60年代中后期起步的。一直到80年代，翻开它的教科书，就是一些启发式搜索，研究最多的是下棋， 从国际象棋一直到最近的围棋，都是比较抽象的表达。棋盘的位置是有限的、下棋的动作也是有限的， 没有感知和动作执行的不确定性。 所有的问题都变成一个图搜索的问题，教科书上甚至出现了一个通用图搜索算法号称可以解决任何人工智能问题。当时视觉问题还没引起大家重视。我这里有一份1966 年7月 的 MIT AI 实验室的第100号报告（备忘录memo 100），很短，题目叫做“The Summer Vision Project”。这个备忘录的基本意思就是暑假的时候找几个学生构造一个视觉系统。他们当时可能就觉得这个问题基本上是不需要做什么研究的。所以你就一个暑假，几个人一起写个程序，就把它干掉算了。现在说起来，当然是个笑话\n到80年代，人工智能， 连带机器人研究就跌入了低谷， 所谓的冬天。那个时候，很多实验室都改名字了， 因为拿不到经费了。 客观来说，80年代， 一个微型计算机的它的内存只有640K字节，还不到一兆（1MB一百万字节），我们现在一张图像，随便就是几个兆的大小，它根本无法读入一张图像，还谈什么理解呢？等到我做博士论文的时候（1992-1996），我导师把当时哈佛机器人实验室最好的SUN工作站给我用，也就是32兆字节。我们实验室花了25万美元构建了一个图像采集系统，因为当时没有数字照相机。可以这么说，一直到90年代中期的时候，我们基本上不具备研究视觉这个问题的硬件条件和数据基础。只能用一些特征点的对应关系做射影几何，用一些线条做形状分析。因为图像做不了，所以80年代计算机视觉的研究，很大部分是做几何。\n关于机器学习和计算机视觉：\n计算机视觉是一个domain， 它有很多问题要研究， 就像物理学。 而机器学习基本是一个方法和工具，就像数学和统计学。 这个名词的兴起应该还是最近的事情， 在我看来，是来自于两股人马。 80年代人工智能走入低谷后，迎来了人工神经网络的一个高潮， 所谓的从符号主义到连接主义的过渡。在中国80年代与气功、人体科学一起走红，但这基本是昙花一现。到了90年代初， 退潮之后，就开始搞 NIPS这个会议， 引入统计的方法来做。还有一股就是做模式识别的一些工程人员EECS 背景的。 按道理来说， 这个领域应该叫做 统计学习 （Statistical Learning），因为它的方法都是由概率统计领域拿来的。这些人中的领军人物很有商业头脑， 把统计和物理的数理模型， 改名叫做机器， 比如模型（model）就叫机（machine），把一些层次模型（hierarchical model）说成是“网”（net）。这样，搞出了几个“机”和“网”之后， 这个领域就有了地盘。另一方面，我的那些做统计的同事们也都老实、图个清静，不与他们去争论， 也大多无力去争。当然，统计学领域也有不少人参与了机器学习的浪潮。简单说，机器学习中的 “机器”就是统计模型，“学习”就是用数据来拟合模型。 是由做计算机的人抢占了统计人的理论和方法，然后，应用到视觉、语音语言等 domains。 我在计算机和统计两个系当教授， 看得一清二楚。 这个问题我以后可以专门讨论。\n这个机器学习的群体在2000年之后，加上大量数据的到来，很快就成长了， 商业上取得很大的成功。机器学习和计算机视觉大概有百分之六七十是重合的。顺便说一句，2019年我们两个领域会在一起在洛杉矶开CVPR 和 ICML年会， 我是CVPR19的大会主席。因为学习搞来搞去，最丰富的数据是在视觉（图像和视频）。现在这次机器学习的一些大的动作和工程上的推广工作，还是从计算机视觉这边开始的。\n感觉是把整个发展的来龙去脉讲得非常清楚了。机器学习是方法，更具体地说是基于统计的方法；计算机视觉是具体的一个领域，而这个领域会用到机器学习的方法来解决问题；人工智能是一个更大的范畴。按朱教授的比喻，机器学习是数学，计算机视觉是物理学，而人工智能则是整个自然科学的范畴。"}
{"content2":"计算机视觉相关面经总结\n1. 梯度下降：为什么多元函数在负梯度方向下降最快？\n答： linkhttps://blog.csdn.net/llwleon/article/details/79237053\n2. 神经网络中正确使用dropout\n答： linkhttps://blog.csdn.net/VioletHan7/article/details/81012993\n3. 表达式为max(x,y)的激活函数，反向传播时，x、y上的梯度如何计算？\n答：较大的输入的梯度为1，较小输入的梯度为0；即较小的输入对输出没有影响；另一个值较大，它通过最大值运算门输出，所以最后只会得到较大输入值的梯度。这也是最大值门是梯度路由的原因。\n前向传播时，最大值往前传播；反向传播时，会把梯度分配给输入值最大的线路，这就是一个梯度路由。\n4. 目标检测中常接触的概念：混合高斯模型、光流法、卡尔曼滤波等？\n答： 混合高斯模型：http://www.cnblogs.com/mindpuzzle/archive/2013/04/24/3036447.html\n光流法：https://blog.csdn.net/carson2005/article/details/7581642\n卡尔曼滤波：https://blog.csdn.net/carson2005/article/details/7367135\n5. 每个项目中的衡量指标是什么，公开数据集是什么，你实现的效果如何？\n答：1）显著性检测的衡量指标：（详细解释） 2）显著性检测公开数据集： 3）自己项目实现相关指标：\n6. 介绍resnet和GoogLeNet中的inception module的结构？（CNN结构的了解程度）\n答： linkhttps://zhuanlan.zhihu.com/p/33020995\n7. 逻辑回归实现多分类？\n答：两种方式：1）多个二分类的LR分类器；2）不在输出0/1两种类别，用softmax来实现多类别。\n8. CNN中反向传播过程实现？\n答： linkhttps://blog.csdn.net/login_sonata/article/details/77488383\n9. Tensorflow中卷积操作是怎样实现的？（感觉这种问题是逃不掉的，可惜没搞太懂）\n答： linkhttps://blog.csdn.net/qq_23225317/article/details/79678285\n10. LR与SVM的异同\n答： linkhttps://www.cnblogs.com/zhizhan/p/5038747.html\n11. 如何处理样本不均衡问题？\n答：https://blog.csdn.net/heyongluoyao8/article/details/49408131\n12. 介绍SVM，为什么要求对偶呢？\n答: linkhttp://guoze.me/2014/11/26/svm-knowledge/\n感觉转换成对偶问题，有一个重要的点就是，原问题求解是基于样本空间，也就是说和样本的数量是密切相关的，但是对偶问题的解是和样本的特征相关的。即从样本空间转换到了特征空间？\n13. 过拟合欠拟合及其背后的本质，偏差、方差角度如何理解？\n答：主要从训练集和测试集上来理解，比较好理解。偏差体现的是训练集上对于数据的集合程度，而偏差是测试集上模型的波动情况（或者说是，当数据集有波动了，模型的拟合情况如何，也就是泛化性能）\n14. VGG16、ResNet、GoogleNet的区别？\n答：整理资料中有，从网络规模，层间关系，应用提升来讲。（说出每一个网络最优秀的创新点）\n15. RF、GBDT区别--并从方差、偏差理论上解释bagging和boosting的区别。\n答：串行和并行本质上就和方差、偏差相关。bagging属于并行的集成算法，每个基学习器的训练集都是随机取样（有放回）得到的，因此具有一定的独立性，所以说在训练中，多个基学习器是独立进行的，所以他们的方差就比较小，这是这种结构自身就带有的，所以说，bagging主要的是要降低偏差，实现比较好的拟合效果，所以说一般RF的每个树的深度都会比较深（15层左右）；而对于boosting是串行的集成方式，并且当前的基学习器是基于上一个学习器学习误差进行训练，主要目的是降低误差（也就是对错分样本增加权重、或者是对上次的误差进行确定梯度进行训练新模型），本质上就是一次一次降低误差（偏差）的过程，使得模型在训练集上拟合的非常好，所以说偏差也就自动的降低了，而boosting最需要做的是降低方差，所以说GBDT中的基学习器树比较浅，层数一般5-10层就有不错的效果，基学习器层数少，模型就简单，一般越简单的模型方差越低，就越不容易过拟合。（奥克姆剃刀原理）"}
{"content2":"[1] 《基于均值移动的人脸跟踪简介》 未公开\n[2] http://kb.cnblogs.com/a/1742263/\n[3] AIDIA – Adaptive Interface for Display Interaction\n[4] http://baike.baidu.com/view/2810997.htm\n分类: 目标跟踪\nposted @ 2012-04-18 16:05 Hanson-jun 阅读(68) 评论(0) 编辑\n机器视觉开源处理库汇总\n从cvchina搞到的很给力，还在不断更新。。。\n通用库/General Library\nOpenCV\n无需多言。\nRAVL\nRecognition And Vision Library. 线程安全。强大的IO机制。包含AAM。\nCImg\n很酷的一个图像处理包。整个库只有一个头文件。包含一个基于PDE的光流算法。\n图像，视频IO/Image, Video IO\nFreeImage\nDevIL\nImageMagick\nFFMPEG\nVideoInput\nportVideo\nAR相关/Augmented Reality\nARToolKit\n基于Marker的AR库\nARToolKitPlus\nARToolKit的增强版。实现了更好的姿态估计算法。\nPTAM\n实时的跟踪、SLAM、AR库。无需Marker，模板，内置传感器等。\nBazAR\n基于特征点检测和识别的AR库。\n局部不变特征/Local Invariant Feature\nVLFeat\n目前最好的Sift开源实现。同时包含了KD-tree，KD-Forest，BoW实现。\nFerns\n基于Naive Bayesian Bundle的特征点识别。高速，但占用内存高。\nSIFT By Rob Hess\n基于OpenCV的Sift实现。\n目标检测/Object Detection\nAdaBoost By JianXin.Wu\n又一个AdaBoost实现。训练速度快。\n行人检测 By JianXin.Wu\n基于Centrist和Linear SVM的快速行人检测。\n（近似）最近邻/ANN\nFLANN\n目前最完整的（近似）最近邻开源库。不但实现了一系列查找算法，还包含了一种自动选取最快算法的机制。\nANN\n另外一个近似最近邻库。\nSLAM & SFM\nSceneLib [LGPL]\nmonoSLAM库。由Androw Davison开发。\n图像分割/Segmentation\nSLIC Super Pixel\n使用Simple Linear Iterative Clustering产生指定数目，近似均匀分布的Super Pixel。\n目标跟踪/Tracking\nTLD\n基于Online Random Forest的目标跟踪算法。\nKLT\nKanade-Lucas-Tracker\nOnline boosting trackers\nOnline Boosting Trackers\n直线检测/Line Detection\nDSCC\n基于联通域连接的直线检测算法。\nLSD [GPL]\n基于梯度的，局部直线段检测算子。\n指纹/Finger Print\npHash [GPL]\n基于感知的多媒体文件Hash算法。（提取，对比图像、视频、音频的指纹）\n视觉显著性/Visual Salience\nGlobal Contrast Based Salient Region Detection\nMing-Ming Cheng的视觉显著性算法。\nFFT/DWT\nFFTW [GPL]\n最快，最好的开源FFT。\nFFTReal [WTFPL]\n轻量级的FFT实现。许可证是亮点。\n音频处理/Audio processing\nSTK [Free]\n音频处理，音频合成。\nlibsndfile [LGPL]\n音频文件IO。\nlibsamplerate [GPL ]\n音频重采样。\n小波变换\n快速小波变换（FWT）\nFWT\nBRIEF: Binary Robust Independent Elementary Feature 一个很好的局部特征描述子，里面有FAST corner + BRIEF实现特征点匹配的DEMO：http://cvlab.epfl.ch/software/brief/\nhttp://code.google.com/p/javacv\nJava打包的OpenCV, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, and ARToolKitPlus库。可以放在Android上用~\nlibHIK,HIK SVM，计算HIK SVM跟Centrist的Lib。http://c2inet.sce.ntu.edu.sg/Jianxin/projects/libHIK/libHIK.htm\n一组视觉显著性检测代码的链接：http://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/\n介绍n款计算机视觉库/人脸识别开源库/软件\n计算机视觉库 OpenCV\nOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...\n人脸识别 faceservice.cgi\nfaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。\nOpenCV的.NET版 OpenCVDotNet\nOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。\n人脸检测算法 jViolajones\njViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033\nJava视觉处理库 JavaCV\nJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...\n运动检测程序 QMotion\nQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。\n视频监控系统 OpenVSS\nOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。\n手势识别 hand-gesture-detection\n手势识别，用OpenCV实现\n人脸检测识别 mcvai-tracking\n提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...\n人脸检测与跟踪库 asmlibrary\nActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。\nLua视觉开发库 libecv\nECV 是 lua 的计算机视觉开发库(目前只提供linux支持)\nOpenCV的.Net封装 OpenCVSharp\nOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。\n3D视觉库 fvision2010\n基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...\n基于QT的计算机视觉库 QVision\n基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。\n图像特征提取 cvBlob\ncvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.\n实时图像/视频处理滤波开发包 GShow\nGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...\n视频捕获 API VideoMan\nVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。\n开放模式识别项目 OpenPR\nPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。\nOpenCV的Python封装 pyopencv\nOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...\n视觉快速开发平台 qcv\n计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。\n图像捕获 libv4l2cam\n对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出\n计算机视觉算法 OpenVIDIA\nOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;...\n高斯模型点集配准算法 gmmreg\n实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...\n模式识别和视觉库 RAVL\nRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。\n图像处理和计算机视觉常用算法库 LTI-Lib\nLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具\nOpenCV优化 opencv-dsp-acceleration\n优化了OpenCV库在DSP上的速度。\nC++计算机视觉库 Integrating Vision Toolkit\nIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV\n计算机视觉和机器人技术的工具包 EGT\nThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...\nOpenCV的扩展库 ImageNets\nImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。\nlibvideogfx\n视频处理、计算机视觉和计算机图形学的快速开发库。\nMatlab计算机视觉包 mVision\nMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。\nScilab的计算机视觉库 SIP\nSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。\nSTAIR Vision Library\nSTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。\nUIUC某童鞋收集的代码合集\nJia-Bin Huang童鞋收集，此童鞋毕业于国立交通大学，之前拍过很多CVPR举办地科罗拉多州的照片，这里大多为matlab code,\nlink: https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n包括：\nFeature Extraction：\nSIFT [1] [Demo program][SIFT Library] [VLFeat]\nPCA-SIFT [2] [Project]\nAffine-SIFT [3] [Project]\nSURF [4] [OpenSURF] [Matlab Wrapper]\nAffine Covariant Features [5] [Oxford project]\nMSER [6] [Oxford project] [VLFeat]\nGeometric Blur [7] [Code]\nLocal Self-Similarity Descriptor [8] [Oxford implementation]\nGlobal and Efficient Self-Similarity [9] [Code]\nHistogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]\nGIST [11] [Project]\nShape Context [12] [Project]\nColor Descriptor [13] [Project]\nPyramids of Histograms of Oriented Gradients [Code]\nSpace-Time Interest Points (STIP) [14] [Code]\nBoundary Preserving Dense Local Regions [15][Project]\nImage Segmentation：\nNormalized Cut [1] [Matlab code]\nGerg Mori’ Superpixel code [2] [Matlab code]\nEfficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]\nMean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]\nOWT-UCM Hierarchical Segmentation [5] [Resources]\nTurbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]\nQuick-Shift [7] [VLFeat]\nSLIC Superpixels [8] [Project]\nSegmentation by Minimum Code Length [9] [Project]\nBiased Normalized Cut [10] [Project]\nSegmentation Tree [11-12] [Project]\nEntropy Rate Superpixel Segmentation [13] [Code]\nObject Detection：\nA simple object detector with boosting [Project]\nINRIA Object Detection and Localization Toolkit [1] [Project]\nDiscriminatively Trained Deformable Part Models [2] [Project]\nCascade Object Detection with Deformable Part Models [3] [Project]\nPoselet [4] [Project]\nImplicit Shape Model [5] [Project]\nViola and Jones’s Face Detection [6] [Project]\nSaliency Detection\nItti, Koch, and Niebur’ saliency detection [1] [Matlab code]\nFrequency-tuned salient region detection [2] [Project]\nSaliency detection using maximum symmetric surround [3] [Project]\nAttention via Information Maximization [4] [Matlab code]\nContext-aware saliency detection [5] [Matlab code]\nGraph-based visual saliency [6] [Matlab code]\nSaliency detection: A spectral residual approach. [7] [Matlab code]\nSegmenting salient objects from images and videos. [8] [Matlab code]\nSaliency Using Natural statistics. [9] [Matlab code]\nDiscriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]\nLearning to Predict Where Humans Look [11] [Project]\nGlobal Contrast based Salient Region Detection [12] [Project]\nImage Classification\nPyramid Match [1] [Project]\nSpatial Pyramid Matching [2] [Code]\nLocality-constrained Linear Coding [3] [Project] [Matlab code]\nSparse Coding [4] [Project] [Matlab code]\nTexture Classification [5] [Project]\nMultiple Kernels for Image Classification [6] [Project]\nFeature Combination [7] [Project]\nSuperParsing [Code]\nImage Matting\nClosed Form Matting [Code]\nSpectral Matting [Project]\nLearning-based Matting [Code]\n等等等等。。。。\n大家可以去那个网址自己看。。。。\nposted @ 2012-04-18 15:53 Hanson-jun 阅读(49) 评论(0) 编辑\n图像处理方面的网站\nhttp://blog.damiles.com\nwww.bernardotti.it\nhttp://www.ohloh.net/tags/recognition\nhttp://www.diphernet.com/\nhttp://www.mat.ucsb.edu/projects/tater/\nhttp://enblend.sourceforge.net/\nhttp://www.infra.kth.se/courses/1N1652/\nhttp://www.csie.ntu.edu.tw/~b93082/VFX/hw2/vfx02.htm#t3\nhttp://graphics.cs.msu.ru/en/research/calibration/\nhttp://www.vlfeat.org/~vedaldi/\nhttp://svn.openframeworks.cc/browser/listing.php?repname=addons&path=%2FofxOpenCv%2Ftrunk%2FofxOpenCv%2F&rev=29&sc=1\nhttp://cvlab.epfl.ch/software/ferns/index.php\nhttp://staff.science.uva.nl/~rvalenti/index.php?content=projects\nhttp://mpac.ee.ntu.edu.tw/~ck/project_panorama/#Downloads\nhttp://www.sharewareconnection.com/titles/cross-stitch.htm\nhttp://mpac.ee.ntu.edu.tw/~sutony/vfx_stitching/pano.htm\nhttp://mpac.ee.ntu.edu.tw/people.php\nhttp://mpac.ee.ntu.edu.tw/index.php\nhttp://www.cse.cuhk.edu.hk/~csc5280/project3/RoyChan/index.htm\nhttp://personal.ie.cuhk.edu.hk/~gbq008/csc_project_3.htm\nhttp://graphics.cs.cmu.edu/courses/15-463/2008_fall/463.html\nhttp://www-2.cs.cmu.edu/%7ecdtwigg/\nhttp://cs-people.bu.edu/edwardaa/cs580/p1/p1.html#goals\nhttp://www.cs.toronto.edu/~smalik/2530/mosaic/results.html\nhttp://www.cs.princeton.edu/gfx/\nhttp://idea.hosting.lv/a/gfx/\nhttp://www.cs.toronto.edu/~esteger/mosaic/index.html\nhttp://home.so-net.net.tw/lioucy\nhttp://web.ics.purdue.edu/~kim497/\nCUDA:\nhttp://gforge.man.poznan.pl/gf/project/cudaopencv/scmsvn/\nhttp://wiki.livedoor.jp/mikk_ni3_92/d/CUDA::2%C3%CD%B2%BD::%CA%A3%BF%F4%CB%E7\nhttp://cudasample.net/\n一、研究群体\nhttp://www-2.cs.cmu.edu/~cil/vision.html\n这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。\nhttp://www.cmis.csiro.au/IAP/zimage.htm\n这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。\nhttp://www.via.cornell.edu/\n康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。\nhttp://www2.parc.com/istl/groups/did/didoverview.shtml\n有一个很有意思的项目：DID(文档图像解码)。\nhttp://www-cs-students.stanford.edu/\n斯坦福大学计算机系主页，自己找吧:(\nhttp://www.fmrib.ox.ac.uk/analysis/\n主要研究：Brain Extraction Tool,Nonlinear noise reduction,Linear Image Registration,\nAutomated Segmentation,Structural brain change analysis,motion correction,etc.\nhttp://www.cse.msu.edu/prip/\n这是密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。\nhttp://pandora.inf.uni-jena.de/p/e/index.html\n德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。\nhttp://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.html\nCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.\nhttp://cfia.gmu.edu/\nThe mission of the Center for Image Analysis is to foster multi-disciplinary research in image, multimedia and related technologies by establishing links\nbetween academic institutes, industry and government agencies, and to transfer key technologies to\nhelp industry build next\ngeneration commercial and military imaging and multimedia systems.\nhttp://peipa.essex.ac.uk/info/groups.html\n可以通过它来搜索全世界各地的知名的计算机视觉研究组(CV Groups)，极力推荐。\n二、图像处理GPL库\nhttp://www.ph.tn.tudelft.nl/~klamer/cppima.html\nCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。\nhttp://iraf.noao.edu/\nWelcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility, a general purpose software\nsystem for the reduction and analysis of astronomical data.\nhttp://entropy.brni-jhu.org/tnimage.html\n一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。\nhttp://sourceforge.net/projects/\n这是GPL软件集散地，到这里找你想要得到的IP库吧。\n三、搜索资源\n当然这里基本的搜索引擎还是必须要依靠的，比如Google等，可以到我常用的链接看看。下面的链接可能会节省你一些时间：\nhttp://sal.kachinatech.com/\nhttp://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml\n四、大拿网页\nhttp://www.ai.mit.edu/people/wtf/\n这位可是MIT人工智能实验室的BILL FREEMAN。大名鼎鼎！专长是：理解--贝叶斯模型。\nhttp://www.merl.com/people/brand/\nMERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”高手。\nhttp://research.microsoft.com/~ablake/\nCV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。\nhttp://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html\n这位牛人好像正在学习汉语，并且搜集了诸如“两只老虎(Two Tigers)”的歌曲，嘿嘿:)\n他的主页上面还有几个牛：Shumeet Baluja, Takeo Kanade。他们的Face Detection作的绝对是世界一流。他毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。\nhttp://www.ifp.uiuc.edu/yrui_ifp_home/html/huang_frame.html\n这位老牛在1963年就获得了MIT的博士学位！他领导的Image Lab比较出名的是指纹识别。\n--------------------------------------------------------------------------------\n下面这些是我搜集的牛群(大部分是如日中天的Ph.D们)，可以学习的是他们的Study Ways!\nFinn Lindgren(Sweden):Statistical image analysis http://www.maths.lth.se/matstat/staff/finn/\nPavel Paclik(Prague):statistical pattern recognition http://www.ph.tn.tudelft.nl/~pavel/\nDr. Mark Burge:machine learning and graph theory http://cs.armstrong.edu/burge/\nyalin Wang:Document Image Analysis http://students.washington.edu/~ylwang/\nGeir Storvik: Image analysis http://www.math.uio.no/~geirs/\nHeidorn http://alexia.lis.uiuc.edu/~heidorn/\nJoakim Lindblad:Digital Image Cytometry http://www.cb.uu.se/~joakim/index_eng.html\nS.Lavirotte: http://www-sop.inria.fr/cafe/Stephane.Lavirotte/\nSporring:scale-space techniques http://www.lab3d.odont.ku.dk/~sporring/\nMark Jenkinson:Reduction of MR Artefacts http://www.fmrib.ox.ac.uk/~mark/\nJustin K. Romberg:digital signal processing http://www-dsp.rice.edu/~jrom/\nFauqueur:Image retrieval by regions of interest http://www-rocq.inria.fr/~fauqueur/\nJames J. Nolan:Computer Vision http://cs.gmu.edu/~jnolan/\nDaniel X. Pape:Information http://www.bucho.org/~dpape/\nDrew Pilant:remote sensing technology http://www.geo.mtu.edu/~anpilant/index.html\n五、前沿期刊(TOP10)\n这里的期刊大部分都可以通过上面的大拿们的主页间接找到，在这列出主要是为了节省直接想找期刊投稿的兄弟的时间:)\nIEEE Trans. On PAMI http://www.computer.org/tpami/index.htm\nIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htm\nPattern Recognition http://www.elsevier.com/locate/issn/00313203\nPattern Recognition Letters http://www.elsevier.com/locate/issn/01678655\n神经网络\nNeural Networks Tutorial Review\nhttp://hem.hj.se/~de96klda/NeuralNetworks.htm\nftp://ftp.sas.com/pub/neural/FAQ.html\nImage Compression with Neural Networks\nhttp://www.comp.glam.ac.uk/digimaging/neural.htm\nBackpropagator's Review\nhttp://www.dontveter.com/bpr/bpr.html\nBibliographies on Neural Networks\nhttp://liinwww.ira.uka.de/bibliography/Neural/\nIntelligent Motion Control with an Artificial Cerebellum\nhttp://www.q12.org/phd.html\nKernel Machines\nhttp://www.kernel-machines.org/\nSome Neural Networks Research Organizations\nhttp://www.ieee.org/nnc/\nhttp://www.inns.org/\nNeural Network Modeling in Vision Research\nhttp://www.rybak-et-al.net/nisms.html\nNeural Networks and Machine Learning\nhttp://learning.cs.toronto.edu/\nNeural Application Software\nhttp://attrasoft.com\nNeural Network Toolbox for MATLAB\nhttp://www.mathworks.com/products/neuralnet/\nNetlab Software\nhttp://www.ncrg.aston.ac.uk/netlab/\nKunama Systems Limited\nhttp://www.kunama.co.uk/\nComputer Vision\nComputer Vision Homepage, Carnegie Mellon University\nwww.cs.cmu.edu/~cil/vision.html\nAnnotated Computer Vision Bibliography\nhttp://iris.usc.edu/Vision-Notes/bibliography/contents.html\nhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.html\nLawrence Berkeley National Lab Computer Vision and Robotics Applications\nhttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.html\nCVonline by University of Edinburgh\nThe Evolving, Distributed, Non-Proprietary, On-Line Compendium of Computer Vision, www.dai.ed.ac.uk/CVonline\nComputer Vision Handbook, www.cs.hmc.edu/~fleck/computer-vision-handbook\nVision Systems Courseware\nwww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.html\nResearch Activities in Computer Vision\nhttp://www-syntim.inria.fr/syntim/analyse/index-eng.html\nVision Systems Acronyms\nwww.vision-systems-design.com/vsd/archive/acronyms.html\nDictionary of Terms in Human and Animal Vision\nhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.html\nMetrology based on Computer Vision\nwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html\nDigital Photography\nDigital Photography, Scanning, and Image Processing\nwww.dbusch.com/scanners/scanners.html\nEducational Resources, Universities\nCenter for Image Processing in Education\nwww.cipe.com\nLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technology\nhttp://wally2.rit.edu/pubs/guides/imagingcall.html\nMathematical Experiences through Image Processing, University of Washington\nwww.cs.washington.edu/research/metip/metip.html\nVismod Tech Reports and Publications, MIT\nhttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemaker\nVision Lab PhD dissertation list, University of Antwerp\nhttp://wcc.ruca.ua.ac.be/~visielab/theses.html\nINRIA (France) Research Projects: Human-Computer Interaction, Image Processing, Data Management, Knowledge Systems\nwww.inria.fr/Themes/Theme3-eng.html\nImage Processing Resources\nhttp://eleceng.ukc.ac.uk/~rls3/Contents.htm\nPublications of Carsten Steger\nhttp://www9.informatik.tu-muenchen.de/people/steger/publications.html\nFAQs\ncomp.dsp FAQ\nwww.bdti.com/faq/dsp_faq.htm\nRobotics FAQ\nwww.frc.ri.cmu.edu/robotics-faq\nWhere's the sci.image.processing FAQ?\nwww.cc.iastate.edu/olc_answers/packages/graphics/sci.image.processing.faq.html\ncomp.graphics.algorithms FAQ, Section 3, 2D Image/Pixel Computations\nwww.exaflop.org/docs/cgafaq\nAstronomical Image Processing System FAQ\nwww.cv.nrao.edu/aips/aips_faq.html\n来自: http://hi.baidu.com/jiamn/blog/item/aaa063f9ae34141d6c22ebce.html\nposted @ 2012-04-18 15:48 Hanson-jun 阅读(45) 评论(0) 编辑\n计算机视觉文献与代码资源及资料\n下面是前端时间搜集整理的一些和计算机视觉、模式识别的资源，拿出来与大家分享下。以后，我将把图像处理真正的作为我的兴趣来玩玩了，也许不把研究作为谋生的手段，会更好些。\n标题\n作者\n主题\n关键字\n类别\n来源\n备注\nnipsfast.ppt\nNando de Freitas\nN-Body problems in learning\nFast N-Body Learning\nPpt\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nnipsfgtf.ppt\nRamani Duraiswami\nFast Multipole Methods Fast Gaussian Transform\nFM and FGT\nppt\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nGray.pdf/ppt\nAlex Gray\nStatistical N-Body/Proximity Data Structures\nN-Body and Data Structures\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\ndt-nips04.pdf/ppt\nDan Huttenlocher\nFast Distance Transforms\nFDT\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nHigh.pdf/ppt\nAlexander Gray\nFast high-dimensional function integration\nFast integration\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nFast04.pdf/ppt\nDavid Lowe\nFast high-dimensional feature indexing for object recognition\nFeature indexing\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nihler-fast.pdf/ppt\nAlexander lhler\nFast methods and non-parametric BP\nNon-parametric BP\nPpt/pdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nfastview.pdf\nDustin Lang\nComparing fast methods\nOverview fast methods\npdf\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nnbody_methods.tar.gz\ncode\nhttp://www.cs.ubc.ca/~awll/nbody_methods.html\ndemo_rbpf_gauss.tar\nRao Blackwellised particle filtering for conditionally Gaussian Models\nparticle filtering for conditionally\ncode\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\ndemorbpfdbn.tar.gz\nRao Blackwellised Particle Filtering\ncode\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nhttp://www.cs.ubc.ca/~nando/software.html\nupf_demos.tar.gz\nUnscented Particle Filter\nParticle Filter\ncode\nhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.html\nBPF_1_3.zip\nBoosted Particle Filter\nTracking\ncode\nhttp://www.cs.ubc.ca/~okumak/research.html\n1\nflyer_14_800.mpg\nSource image\nDatabase\nImage\nhttp://www.cs.ubc.ca/~okumak/research.html\n1\ntrans_flyer_14_800.mpg\nimage transformed\nDatabase\nImage\nhttp://www.cs.ubc.ca/~okumak/research.html\n1\nLBP.c/h\nTopi Mäenpää\nLBP operator\nTexture\ncode\nhttp://www.ee.oulu.fi/~topiolli/cpplibs/files/\ncalibr_v30.zip\nCamera Calibration\nComputer vision\ncode\nhttp://www.ee.oulu.fi/mvg/page/camera_calibration\n_toolbox_for_matlab\n2\nLEAR(Learning and Recognition in Vision\nCommon dataset\nHuman/car horse soccer human actions\ndataset\nhttp://lear.inrialpes.fr/data\n3\nLic.zip/highlight.zip\nRobby T. Tan\nColor Constancy Through Inverse Intensity Chromaticity Space\nHighlight Removal from single image\ncode\nhttp://www.commsp.ee.ic.ac.uk/~rtan/\n2008_oxford_fog.pdf\nRobby T. Tan\nDefog\nDefog from single\npdf\nhttp://www.commsp.ee.ic.ac.uk/~rtan/\n08_cvpr.pdf\nRobby T. Tan\nDefog\nDefog from single\npdf\nhttp://www.commsp.ee.ic.ac.uk/~rtan/\nRetinex_frankle_mccann\nRetinex\nCode\nhttp://www.cs.sfu.ca/~colour/publications/IST-2000/\nSome\nRetinex_maccann99\nRetinex\ncode\nhttp://www.cs.sfu.ca/~colour/publications/IST-2000/\npictures\nGamut.tar.bz2\nRetinex\ncode\nhttp://kobus.ca/research/programs/colour_constancy/index.html\nVideo.avi/dehaze.m\ndehazing\nRaanan Fattal\ncode\nhttp://www.cs.huji.ac.il/~raananf/projects/defog/index.html\nMPTK-Windows-bin-0-5-6-beta.zip\nMatching pursuit(MP)\nAlogrithm\nCNRS\nCode\nhttp://mptk.irisa.fr/downloads\ngenerateDictionaries.txt\nGenerateGabor\nAlogrithm\ncode\nhttp://www.scholarpedia.org/article/Matching_pursuit\nNotes:\n1.      视频和源码都是对应的文章的：\nKenji Okuma, Ali Taleghani, Nando De Freitas, Jim Little, David G. Lowe. Boosted Particle Filter: Multitarget Detection and Tracking. the European Conference on Computer Vision(ECCV), May 2004.\n2.      该网站下面还有其他一些资源可以下载：\nhttp://www.ee.oulu.fi/mvg/page/downloads\n是个研究组织：http://lear.inrialpes.fr/ ， 除此之外，还有一些源码。\n计算机视觉文献与代码资源\nCVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htm\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm\n李子青的大作：\nMarkov Random Field Modeling in Computer Vision\nhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.html\nHandbook of Face Recognition (PDF)\nhttp://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf\n张正友的有关参数鲁棒估计著作：\nParameter Estimation Techniques:A Tutorial with Application to Conic Fitting\nhttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.html\nAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Vision\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007\n有关马尔可夫蒙特卡罗方法的资料：\nAn introduction to Markov chain Monte Carlo\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.html\nMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05\nhttp://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm\n有关独立成分分析（Independent Component Analysis , ICA）的资料：\nAn ICA-Page\nhttp://www.cnl.salk.edu/~tony/ica.html\nFast ICA\nhttp://www.cis.hut.fi/projects/ica/fastica/\nThe Kalman Filter (介绍卡尔曼滤波器的终极网页)\nhttp://www.cs.unc.edu/~welch/kalman/index.html\nCached k-d tree search for ICP algorithms\nhttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html\n几个计算机视觉研究工具\nMachine Vision Toolbox for Matlab\nhttp://www.petercorke.com/Machine%20Vision%20Toolbox.html\nMatlab and Octave Function for Computer Vision and Image Processing\nhttp://www.csse.uwa.edu.au/~pk/research/matlabfns/\nBayes Net Toolbox for Matlab\nhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.html\nOpenCV (Chinese)\nhttp://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\nGandalf (A Computer Vision and Numerical Algorithm Labrary)\nhttp://gandalf-library.sourceforge.net/\nCMU Computer Vision Home Page\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\nMachine Learning Resource Links\nhttp://www.cse.ust.hk/~ivor/resource.htm\nThe Bayesian Filtering Library\nhttp://www.orocos.org/bfl\nOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)\nhttp://of-eval.sourceforge.net/\nMATLAB code for ICP algorithm\nhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html\n牛人主页：\n朱松纯（Song-Chun Zhu）\nhttp://www.stat.ucla.edu/~sczhu/\nDavid Lowe (SIFT) (很帅的一个老头哦 ^ ^)\nhttp://www.cs.ubc.ca/~lowe/\nAndrea Vedaldi (SIFT)\nhttp://vision.ucla.edu/~vedaldi/index.html\nPedro F. Felzenszwalb\nhttp://people.cs.uchicago.edu/~pff/\nDougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)\nhttp://mesh.brown.edu/dlanman/courses.html\nJianbo Shi (Ncuts 的始作俑者)\nhttp://www.cis.upenn.edu/~jshi/\nActive Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)\nhttp://www.robots.ox.ac.uk/ActiveVision/index.html\nJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）\nhttp://www.cse.msu.edu/~weng/\n测试图片或视频：\nMiddlebury College‘s Stereo Vision Data Set\nhttp://cat.middlebury.edu/stereo/data.html\nIntelligent Vehicle:\nIVSource\nwww.ivsoruce.net\nRobot Car\nhttp://www.plyojump.com/robot_cars.html\nHow to Build a Robot: The Computer Vision Part\nhttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml\n计算机视觉应关注的资源\n来自美国帝腾大学的链接。\nCamera Calibration Links to toolboxes (mostly MATLAB) for camera calibration.\nPaul Debevec. Modeling and Rendering Architecture from Photographs.\nMarc Pollefeys, Tutorial on 3D Modeling from Images,, ECCV 2000,\nAvailable here: notes (12.1MB pdf)\nRichard Szeliski NIPS 2004 Tutorial on Acquiring Detailed 3D Models From Images and Video,\nAvailable here: slides (37.6 MB, ppt)\nPeter Corke did his thesis work on visual servoing for robot applications and has authored a robotics toolkit and vision toolkit for MATLAB.\nlocal copy of thesis: Corke thesis (4.36 MB, pdf)\nrobot toolkit: robot.zip (568 KB, zip)\nvision toolkit: mv.zip (1.08 MB, zip)\nP. D. Kovesi., MATLAB Functions for Computer Vision and Image Analysis.\nSchool of Computer Science & Software Engineering, The University of Western Australia.\nAvailable locally as a zip archive MatlabFns.zip (4.8 MB, updated 21 May 2005)\nPhilip Torr, among many other contributions, submitted a Structure and motion toolkit in Matlab to the MathSoft File Exhange.\nLocal copy here: torrsam.zip (2.4 MB, zip).\n涉足计算机视觉领域要知道的\n做机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法就是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个实际应用的项目，边做边写文章。 做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。\n依照下面目录整理：\n[1]研究群体(国际国内)[2]专家主页[3]前沿国际国内期刊与会议[4]搜索资源[5]GPL软件资源\n一、研究群体\n用来搜索国际知名计算机视觉研究组(CV Groups)：\n国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html\n美国计算机视觉研究组清单 http://peipa.essex.ac.uk/info/groups.html#USA\nhttp://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html\n这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等。\n卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/\n卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.html\n还有几个实验室：\nCalibrated Imaging Laboratory 图像\nDigital Mapping Laboratory 映射\nInteractive Systems Laboratory 互动\nVision and Autonomous Systems Center视觉自适应\nhttp://www.via.cornell.edu/\n康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。\nCornell University——Robotics and Vision group\nhttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页\n1. http://white.stanford.edu/\n2. http://vision.stanford.edu/\n3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室\nThe Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...\nVision and Imaging Science and Technology\nhttp://www.fmrib.ox.ac.uk/analysis/\n主要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.\nhttp://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。\n美国密歇根州大学认知模型和图像处理实验室\nThe Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/\nhttp://pandora.inf.uni-jena.de/p/e/index.html\n德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。\n柏林大学 http://www.cv.tu-berlin.de/\n德国波恩大学视觉和认识模型小组\nComputer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/\nhttp://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.html\nCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.\nhttp://cfia.gmu.edu/\nThe mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.\n英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境 (照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk/Research/Digitalmedia/\n而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了\nhttp://www.cmis.csiro.au/IAP/zimage.htm\n这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。\n麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/\nAI Laboratory Computer Vision group\nCenter for Biological and Computational Learning\nMedia Laboratory， Vision and Modeling Group\nPerceptual Science group\nUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.html\nhttp://www.cs.berkeley.edu.ilste ... n/vision_group.html\n加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/\nUCLA(加州大学洛杉矶分校) http://vision.ucla.edu/视觉实验室\n英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室\n美国南加州大学智能机器人和智能系统研究所University of Southern California， Los Angeles\nIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with... http://iris.usc.edu/ Computer Vision 实验室\n美国南加州大学计算机视觉实验室介绍：\nComputer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html\n英国约克大学高级计算机结构神经网络小组\nThe Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/\n瑞士戴尔莫尔感知人工智能研究所\nIDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/\n英国萨里大学视觉，语言和信号处理中心\nThe Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/\n美国阿默斯特马萨诸塞州立大学计算机视觉实验室\nThe Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.edu\nUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics\n美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室\nIncludes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/\nComputer Vision and Robotics Laboratory\nVision Interfaces and Systems Laboratory (VISLab)\n英国伯明翰大学计算机科学学校视觉研究小组\nThe vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/\n微软研究院机器学习与理解研究小组 / 计算机视觉小组\nThe research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/\nhttp://research.microsoft.com/en-us/groups/vision/\n微软公司的文献：http://research.microsoft.com/research/pubs\n微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.\n瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/\n感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。\n澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/\n美国北卡大学：http://www.cs.unc.edu/~marc/\n法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。\n比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/\n据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.\n美国明德http://vision.middlebury.edu/stereo/\n以下含有非顶尖美国学校研究组，没有链接(个别的上面已经提到)，供参考。\nAmerinex Applied Imaging， Inc.\nBoston University\nImage and Video Computing Research group\nUniversity of California at Santa Barbara加州大学芭芭拉分校\nVision Research Lab\nUniversity of California at San Diego加州大学圣迭戈分校\nComputer Vision & Robotics Research Laboratory\nVisual Computing laboratory\nUniversity of California at Irvine加州大学欧文分校，加州南部一城，在圣安娜东南，\nComputer Vision laboratory\nUniversity of California， Riverside加州大学河滨分校\nVisualization and Intelligent Systems Laboratory (VISLab)\nUniversity of California at Santa Cruz\nPerceptual Science Laboratory\nCaltech (加州理工)\nVision group\nUniversity of Central Florida\nComputer Vision laboratory\nUniversity of Florida\nCenter for Computer Vision and Visualization\nColorado State University\nComputer Vision group\nColumbia University\nAutomated Vision Environment (CAVE)\nRobotics group\nUniversity of Georgia， Athens\nVisual and Parallel Computing Laboratory\nHarvard University（哈佛）\nRobotics Laboratory\nUniversity of Illinois at Urbana-Champaign\nRobotics and Computer Vision\nUniversity of Iowa\nDivision of Physiologic Imaging\nJet Propulsion Laboratory\nMachine Vision and Tracking Sensors group\nKhoral Research， Inc\nLawrence Berkeley Laboratories\nImaging and Collaborative Computing Group\nImaging and Distributed Computing\nLehigh University\nImage Processing and Pattern Analysis Lab\nVision And Software Technology Laboratory\nUniversity of Louisville\nComputer Vision and Image Processing Lab\nUniversity of Maryland\nComputer Vision Laboratory\nUniversity of Miami\nUnderwater Vision and Imaging Laboratory\nUniversity of Michigan密歇根\nAI Laboratory\nMichigan State University 密歇根州立\nPattern Recognition and Image Processing laboratory\nEnvironmental Research Institute of Michigan (ERIM) 密歇根大学有汽车车身检测研究\nUniversity of Missouri-Columbia\nComputational Intelligence Research Laboratory\nNEC\nComputer Vision and Image Processing\nUniversity of Nevada\nComputer Vision Laboratory\nNotre-Dame University\nVision-Based Robotics using Estimation\nOhio State University\nSignal Analysis and Machine Perception Laboratory\nUniversity of Pennsylvania\nGRASP laboratory\nMedical Image Processing group\nVision Analysis and Simulation Technologies (VAST) Laboratory\nPenn State University 宾夕法尼亚大学\nComputer Vision\nPrecision Digital Images\nPurdue University普渡大学\nRobot Vision laboratory\nVideo and Image Processing Laboratory (VIPER)\nRensselaer Polytechnic Institute (RPI)\nComputer Science Vision\nUniversity of Rochester\nCenter for Electronic Imaging Systems\nVision and Robotics laboratory\nRutgers University (The State University of New Jersey)\nImage Understanding Lab\nUniversity of Southern California\nComputer Vision\nUniversity of South Florida\nImage Analysis Research group\nStanford Research Institute International (SRI)\nRADIUS -- Research and Development for Image Understanding Systems\nThe Perception program at SRI's AI Center\nSUNY at Stony Brook\nComputer Vision Lab\nUniversity of Tennessee\nImaging， Robotics and Intelligent Systems laboratory\nUniversity of Texas， Austin\nLaboratory for Vision Systems\nUniversity of Utah\nCenter for Scientific Computing and Imaging\nRobotics and Computer Vision\nUniversity of Virginia\nComputer Vision Research (CS)\nUniversity of Washington\nImage Computing Systems Laboratory\nInformation Processing Laboratory\nCVIA Laboratory\nUniversity of West Florida\nImage Analysis/Robotics Research Laboratory\nUniversity of Wisconsin\nComputer Vision group\nVanderbilt University\nCenter for Intelligent Systems\nWashington State University\nImaging Research laboratory\nWright-Patterson\nModel-Based Vision laboratory\nWright State University\nIntelligent Systems Laboratory\nUniversity of Wyoming\nWyoming Image and Signal Processing Research (WISPR)\nYale University\nComputational Vision Group http://www.cs.yale.edu/\nSchool of Medicine， Image Processing and Analysis group\n国内：\n中科院模式识别国家重点实验室 http://www.nlpr.ia.ac.cn/English/rv/mainpage.html\n虹膜识别、掌纹识别、人脸识别、\n莲花山http://www.stat.ucla.edu/~sczhu/Lotus/\n天津大学精密测试技术及仪器国家重点实验室\n研究方向包括：激光及光电测试技术、传感及测量信息技术、微纳测试与制造技术、制造质量控制技术。该实验室是国内精密测试领域惟一的国家重点实验室。\n“智能微系统及其集成应用技术”、“微结构光学测试技术”、“油气储运安全检测技术”、“先进制造中的视觉测量及其关键技术”、“正交偏振激光器原理、特性及其在精密计量中的应用研究”等5项代表性成果（07.3）。\n中科院长春光机所 http://www.ciomp.ac.cn/ny/keyan.asp\n中科院沈阳自动化所http://www.sia.ac.cn/index.php\n中科院西安光机所http://www.opt.ac.cn/yanjiushi/gpcxjs1.htm\n北京大学智能科学系http://www.cis.pku.edu.cn/vision/vision.htm\n三维视觉计算与机器人，生物特征识别与图像识别\n二、专家网页\nhttp://www.ai.mit.edu/people/wtf/\n这位可是MIT人工智能实验室的BILL FREEMAN。专长是：理解--贝叶斯模型。\nhttp://www.merl.com/people/brand/\nMERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”。\nhttp://research.microsoft.com/~ablake/\nCV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。\nhttp://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html\n这位专家好像正在学习汉语，主页并且搜集了诸如“两只老虎(Two Tigers)”的歌曲。\n他的主页上面还有几个专家：Shumeet Baluja， Takeo Kanade。他们的Face Detection作的绝对是世界一流。毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。\n三、前沿国际国内期刊与会议\n这里的期刊大部分都可以通过上面的专家们的主页间接找到\n1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题\n1. 国际会议\n现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。\nICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。\nCVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。\nICIP—\nBMVC—\nMVA—\n国际模式识别会议(ICPR )：\n亚洲计算机视觉会议(ACCV)：\n2.国际期刊\n以计算机视觉为主要内容之一的国际刊物也有很多，如:\nInternational Journal of Computer Vision\nIEEE Trans. On PAMI http://www.computer.org/tpami/index.htm\nIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htm\nPattern Recognition http://www.elsevier.com/locate/issn/00313203\nPattern Recognition Letters http://www.elsevier.com/locate/issn/01678655\nIEEE Trans. on Robotics and Automation，\nIEEE TPAMI\nIEEE TIP\nCVGIP Computer Vision. Graphics and Image Processing，\nVisual Image Computing，\nIJPRAI(Internatiorial Journat of Pattern Recognition and Artificial Intelligence)\n众所周知， computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议，它们档次差不多，都应该在一流会议行列， 没有必要给个高下。有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR，某些英国的人甚至认为BMVC好于CVPR。简言之， 三个会议差不多， 各有侧重和偏好。\n笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话， 想知道某个领域在做些什么，找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次，各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。\n就录取率而言， 三会都有波动。 如ICCV2001录取率>30%，且出现两个人(华人)各有三篇第一作者的paper的情况， 这在顶级牛会是不常见的 (灌水嫌疑)。 但是， ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高，从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高， 反之偏低，近几年三大会议的投稿数量全部超过1000， 相对2000年前， 三会录取率均大幅度降低，最大幅度50%->20%。 对录取率走势感兴趣的朋友， 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http://www.adaptivebox.net/research/bookmark/CICON_stat.html.\n显然， 投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic，而cvpr会收少量的pattern recognition paper， 如finger print等，但是不收和image/video完全不占边的pr paper，如speech recognition等。我一个朋友曾经review过一篇投往CVPR的speech的paper， 三个reviewer一致拒绝，其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。 就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。\n以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。 避免做无用功，选择切合的topic，改善presentation， 注意格式 (遵守规定的模板)， 我想这是很多新手需要注意的问题。如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视， 这是相当不值得的。\n3.国内期刊\n自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。\n4.神经网络\n神经网络-Neural Networks Tutorial Review\nhttp://hem.hj.se/~de96klda/NeuralNetworks.htm\nftp://ftp.sas.com/pub/neural/FAQ.html\nImage Compression with Neural Networks\nhttp://www.comp.glam.ac.uk/digimaging/neural.htm\nBackpropagator's Review\nhttp://www.dontveter.com/bpr/bpr.html\nBibliographies on Neural Networks\nhttp://liinwww.ira.uka.de/bibliography/Neural/\nIntelligent Motion Control with an Artificial Cerebellum\nhttp://www.q12.org/phd.html\nKernel Machines\nhttp://www.kernel-machines.org/\nSome Neural Networks Research Organizations\nhttp://www.ieee.org/nnc/\nhttp://www.inns.org/\nNeural Network Modeling in Vision Research\nhttp://www.rybak-et-al.net/nisms.html\nNeural Networks and Machine Learning\nhttp://learning.cs.toronto.edu/\nNeural Application Software\nhttp://attrasoft.com\nNeural Network Toolbox for MATLAB\nhttp://www.mathworks.com/products/neuralnet/\nNetlab Software\nhttp://www.ncrg.aston.ac.uk/netlab/\nKunama Systems Limited http://www.kunama.co.uk/\n5.Computer Vision(计算机视觉)\nAnnotated Computer Vision Bibliography\nhttp://iris.usc.edu/Vision-Notes/bibliography/contents.html\nhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.html\nLawrence Berkeley National Lab Computer Vision and Robotics Applications\nhttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.html\nCVonline by University of Edinburgh\nThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision， www.dai.ed.ac.uk/CVonline\nComputer Vision Handbook，\nwww.cs.hmc.edu/~fleck/computer-vision-handbook\nVision Systems Courseware\nwww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.html\nResearch Activities in Computer Vision\nhttp://www-syntim.inria.fr/syntim/analyse/index-eng.html\nVision Systems Acronyms\nwww.vision-systems-design.com/vsd/archive/acronyms.html\nDictionary of Terms in Human and Animal Vision\nhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.html\nMetrology based on Computer Vision\nwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html\n6.Digital Photography 数字图像\nDigital Photography， Scanning， and Image Processing\nwww.dbusch.com/scanners/scanners.htm l\n7.Educational Resources， Universities 教育资源，大学\nCenter for Image Processing in Education\nwww.cipe.com\nLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technology\nhttp://wally2.rit.edu/pubs/guides/imagingcall.html\nMathematical Experiences through Image Processing， University of Washington\nwww.cs.washington.edu/research/metip/metip.html\nVismod Tech Reports and Publications， MIT\nhttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemaker\nVision Lab PhD dissertation list， University of Antwerp\nhttp://wcc.ruca.ua.ac.be/~visielab/theses.html\nINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systems\nwww.inria.fr/Themes/Theme3-eng.html\nImage Processing Resources\nhttp://eleceng.ukc.ac.uk/~rls3/Contents.htm\nPublications of Carsten Steger\nhttp://www9.informatik.tu-muench ... r/publications.html\n8.FAQs（常见问题）\ncomp.dsp FAQ\nwww.bdti.com/faq/dsp_faq.htm\nRobotics FAQ\nwww.frc.ri.cmu.edu/robotics-faq\nWhere's the sci.image.processing FAQ?\nwww.cc.iastate.edu/olc_answers/p ... processing.faq.html\ncomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computations\nwww.exaflop.org/docs/cgafaq\nAstronomical Image Processing System FAQ\nwww.cv.nrao.edu/aips/aips_faq.html\n四、搜索资源\nhttp://sal.kachinatech.com/\nhttp://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学\nGoogle输入：computer vision 或computer vision groups可以获得很多结果\n网络资源：\nCVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表\nComputer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库\n视觉论文搜索：Paper search\nhttp://www.researchindex.com\n五、图像处理GPL库（代码库图像库等）\nhttp://www.ph.tn.tudelft.nl/~klamer/cppima.html\nCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。\nhttp://iraf.noao.edu/\nWelcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical data\nhttp://entropy.brni-jhu.org/tnimage.html\n一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。\nhttp://sourceforge.net/projects/\n这是GPL软件集散地，可以搜索IP库。\n国内的CSDN http://www.csdn.net/\n转载：http://blog.sciencenet.cn/home.php?mod=space&uid=509980&do=blog&id=436782"}
{"content2":"当入手一台新的电脑，所有软件都得重新安装一遍，而每个技术人员的习惯不一。就比如我，一名计算机视觉工程师，需要安装什么软件呢？\n一、技术需要\n1.VS2013\n2.OpenCV源码\n3.git\n4.cmake\n5.sourcetree\n6.有道笔记\n7.python2.7\n8.notepad ++\n9.UltraISO\n10.Ubuntu 16.04 iso\n11.kdevelop\n12.robware studio\n13.gitkraken\n14.sublime\n二、办公需要\n1.百度网盘\n2.WINRAR压缩软件\n3.DISM++\n4.markdown\n5.teamviewer\n6.QDir\n7.搜狗浏览器\n8.Everything\n9.FastStone Capture\n10.iSpring Free Cam 8\n11.云竹协作\n12.gaaiho\n13.金山词霸\n14.CAJViewer\n7/17/2018 3:44:16 PM\nAuthor：Z"}
{"content2":"原作者博客主页：http://blog.csdn.net/dcraw\n图像处理与计算机视觉基础，经典以及最近发展\nBy xdyang（杨晓冬xdyang.ustc@gmail.com）\n一、 绪论\n1. 为什么要写这篇文章\n从2002年到现在，接触图像快十年了。虽然没有做出什么很出色的工作，不过在这个领域摸爬滚打了十年之后，发现自己对图像处理和计算机视觉的感情越来越深厚。下班之后看看相关的书籍和文献是一件很惬意的事情。平常的一大业余爱好就是收集一些相关的文章，尤其是经典的文章，到现在我的电脑里面已经有了几十G的文章。写这个文档的想法源于我前一段时间整理文献时的一个突发奇想，既然有这个多文献，何不整理出其中的经典，抓住重点来阅读，同时也可以共享给大家。于是当时即兴写了一个《图像处理与计算机视觉中的经典论文》。现在来看，那个文档写得很一般，所共享的论文也非常之有限。就算如此，还是得到了一些网友的夸奖，心里感激不尽。因此，一直想下定决心把这个工作给完善，力求做到尽量全面。\n本文是对现有的图像处理和计算机视觉的经典书籍（后面会有推荐）的一个补充。一般的图像处理书籍都是介绍性的介绍某个方法，在每个领域内都会引用几十上百篇参考文献。有时候想深入研究这个领域的时候却发现文献太多，不知如何选择。但实际上在每个领域都有那么三五篇抑或更多是非读不可的经典文献。这些文献除了提出了很经典的算法，同时他们的Introduction和Related work也是对所在的领域很好的总结。读通了这几篇文献也就等于深入了解了这个领域，比单纯的看书收获要多很多。写本文的目的就是想把自己所了解到的各个领域的经典文章整理出来,不用迷失在参考文献的汪洋大海里。\n2. 图像处理和计算机视觉的分类\n按照当前流行的分类方法，可以分为以下三部分：\nA.图像处理：对输入的图像做某种变换，输出仍然是图像，基本不涉及或者很少涉及图像内容的分析。比较典型的有图像变换，图像增强，图像去噪，图像压      缩，图像恢复，二值图像处理等等。基于阈值的图像分割也属于图像处理的范畴。一般处理的是单幅图像。\nB.图像分析：对图像的内容进行分析，提取有意义的特征，以便于后续的处理。处理的仍然是单幅图像。\nC.计算机视觉：对图像分析得到的特征进行分析，提取场景的语义表示，让计算机具有人眼和人脑的能力。这时处理的是多幅图像或者序列图像，当然也包括部分单幅图像。\n关于图像处理，图像分析和计算机视觉的划分并没有一个很统一的标准。一般的来说，图像处理的书籍总会或多或少的介绍一些图像分析和计算机视觉的知识，比如冈萨雷斯的数字图像处理。而计算机视觉的书籍基本上都会包括图像处理和图像分析，只是不会介绍的太详细。其实图像处理，图像分析和计算机视觉都可以纳入到计算机视觉的范畴：图像处理->低层视觉（low level vision），图像分析->中间层视觉（middle level vision），计算机视觉->高层视觉（high level vision）。这是一般的计算机视觉或者机器视觉的划分方法。在本文中，仍然按照传统的方法把这个领域划分为图像处理，图像分析和计算机视觉。\n3. 图像处理和计算机视觉开源库以及编程语言选择\n目前在图像处理中有两种最重要的语言：c/c++和matlab。它们各有优点：c/c++比较适合大型的工程，效率较高，而且容易转成硬件语言，是工业界的默认语言之一。而matlab实现起来比较方便，适用于算法的快速验证，而且matlab有成熟的工具箱可以使用，比如图像处理工具箱，信号处理工具箱。它们有一个共同的特点：开源的资源非常多。在学术界matlab使用的非常多，很多作者给出的源代码都是matlab版本。最近由于OpenCV的兴起和不断完善，c/c++在图像处理中的作用越来越大。总的来说，c/c++和matlab都必须掌握，最好是精通，当然侧重在c/c++上对找工作会有很大帮助。\n至于开源库，个人非常推荐OpenCV，主要有以下原因：\n（1）简单易入手。OpenCV进入OpenCV2.x的时代后，使用起来越来越简单,接口越来越傻瓜化，越来越matlab化。只要会imread,imwrite,imshow和了解Mat的基本操作就可以开 始入手了。\n（2）OpenCV有一堆图像处理和计算机视觉的大牛在维护，bug在逐步减少，每个新的版本都会带来不同的惊喜。而且它已经或者逐步在移植到不懂的平台,并提供了对Python的很好的支持。\n（3）在opencv上可以尝试各种最新以及成熟的技术，而不需要自己从头去写，比如人脸检测（Harr，LBP），DPM（Latent SVM），高斯背景模型，特征检测，聚类，hough变换等等 。而且它还支持各种机器学习方法（SVM，NN，KNN，决策树，Boosting等），使用起来很简单。\n（4）文档内容丰富，并且给出了很多示例程序。当然也有一些地方文档描述不清楚，不过看看代码就很清楚了。\n（5）完全开源。可以从中间提取出任何需要的算法。\n（6）从学校出来后，除极少数会继续在学术圈里，大部分还是要进入工业界。现在在工 业界，c/c++仍是主流，很多公司都会优先考虑熟悉或者精通OpenCV的。事实上，在学术界，现在OpenCV也大有取代matlab之势。以前的demo或者source code，很多作者都愿意给出matlab版本的，然后别人再呼哧呼哧改成c版本的。现在作者干脆给出c/c++版本，或者自己集成到OpenCV中去，这样能快速提升自己的影响力。\n如果想在图像处理和计算机视觉界有比较深入的研究，并且以后打算进入这个领域工作的话，建议把OpenCV作为自己的主攻方向。如果找工作的时候敢号称自己精通OpenCV的话，肯定可以找到一份满意的工作。\n4. 本文的特点和结构，以及适合的对象\n在本文面向的对象是即将进入或者刚刚进入图像处理和计算机视觉领域的童鞋，可以在阅读书籍的同时参阅这些文献，能对书中提到的算法有比较深刻的理解。由于本文涉及到的范围比较广，如果能对计算机视觉的资深从业者也有一定的帮助，我将倍感欣慰。为了不至太误人子弟，每一篇文章都或多或少的看了一下，最不济也看了摘要(这句话实在整理之前写的，实际上由于精力有限，好多文献都只是大概扫了一眼，然后看了看google的引用数，一般在1000以上就放上来了，把这些文章细细品味一遍也是我近一两年之内的目标)。在成文的过程中，我本人也受益匪浅，希望能对大家也有所帮助。\n由于个人精力和视野的关系，有一些我未涉足过的领域不敢斗胆推荐，只是列出了一些引用率比较高的文章，比如摄像机标定和立体视觉。不过将来，由于工作或者其他原因，这些领域也会接触到，我会逐步增减这些领域的文章。尽管如此，仍然会有疏漏，忘见谅。同时文章的挑选也夹带了一些个人的喜好，比如我个人比较喜欢low level方向的，尤其是IJCV和PAMI上面的文章，因此这方面也稍微多点，希望不要引起您的反感。如果有什么意见或者建议，欢迎mail我。文章和资源我都会在我的csdn blog和sina ishare同步更新。此申明：这些论文的版权归作者及其出版商所有，请勿用于商业目的。\n个人blog：       http://blog.csdn.NET/dcraw\n新浪iask地址：http://iask.sina.com.cn/u/2252291285/ish?folderid=868438\n本文的安排如下。第一部分是绪论。第二部分是图像处理中所需要用到的理论基础，主要是这个领域所涉及到的一些比较好的参考书籍。第三部分是计算机视觉中所涉及到的信号处理和模式识别文章。由于图像处理与图像分析太难区分了，第四部分集中讨论了它们。第五部分是计算机视觉部分。最后是小结。\n二、 图像处理与计算机视觉相关的书籍\n1. 数学\n我们所说的图像处理实际上就是数字图像处理，是把真实世界中的连续三维随机信号投影到传感器的二维平面上，采样并量化后得到二维矩阵。数字图像处理就是二维矩阵的处理，而从二维图像中恢复出三维场景就是计算机视觉的主要任务之一。这里面就涉及到了图像处理所涉及到的三个重要属性：连续性，二维矩阵，随机性。所对应的数学知识是高等数学（微积分），线性代数（矩阵论），概率论和随机过程。这三门课也是考研数学的三个组成部分，构成了图像处理和计算机视觉最基础的数学基础。如果想要更进一步，就要到网上搜搜林达华推荐的数学书目了。\n2. 信号处理\n图像处理其实就是二维和三维信号处理，而处理的信号又有一定的随机性，因此经典信号处理和随机信号处理都是图像处理和计算机视觉中必备的理论基础。\n2.1经典信号处理\n信号与系统(第2版) Alan V.Oppenheim等著 刘树棠译\n离散时间信号处理(第2版) A.V.奥本海姆等著 刘树棠译\n数字信号处理:理论算法与实现 胡广书 (编者)\n2.2随机信号处理\n现代信号处理 张贤达著\n统计信号处理基础:估计与检测理论 Steven M.Kay等著 罗鹏飞等译\n自适应滤波器原理(第4版) Simon Haykin著 郑宝玉等译\n2.3 小波变换\n信号处理的小波导引:稀疏方法(原书第3版) tephane Malla著, 戴道清等译\n2.4 信息论\n信息论基础(原书第2版) Thomas M.Cover等著 阮吉寿等译\n3. 模式识别\nPattern Recognition and Machine Learning Bishop, Christopher M. Springer\n模式识别(英文版)(第4版) 西奥多里德斯著\nPattern Classification (2nd Edition) Richard O. Duda等著\nStatistical Pattern Recognition, 3rd Edition Andrew R. Webb等著\n模式识别(第3版) 张学工著\n4. 图像处理与计算机视觉的书籍推荐\n图像处理，分析与机器视觉 第三版 Sonka等著 艾海舟等译\nImage Processing, Analysis and Machine Vision\n( 附：这本书是图像处理与计算机视觉里面比较全的一本书了，几乎涵盖了图像视觉领域的各个方面。中文版的个人感觉也还可以，值得一看。)\n数字图像处理 第三版 冈萨雷斯等著\nDigital Image Processing\n(附：数字图像处理永远的经典，现在已经出到了第三版，相当给力。我的导师曾经说过，这本书写的很优美，对写英文论文也很有帮助，建议购买英文版的。)\n计算机视觉：理论与算法 Richard Szeliski著\nComputer Vision: Theory and Algorithm\n(附：微软的Szeliski写的一本最新的计算机视觉著作。内容非常丰富，尤其包括了作者的研究兴趣，比如一般的书里面都没有的Image Stitching和                       Image Matting等。这也从另一个侧面说明这本书的通用性不如Sonka的那本。不过作者开放了这本书的电子版，可以有选择性的阅读。\nhttp://szeliski.org/Book/\nMultiple View Geometry in Computer Vision 第二版Harley等著\n引用达一万多次的经典书籍了。第二版到处都有电子版的。第一版曾出过中文版的，后来绝版了。网上也可以找到中英文版的电子版。)\n计算机视觉：一种现代方法 DA Forsyth等著\nComputer Vision: A Modern Approach\nMIT的经典教材。虽然已经过去十年了，还是值得一读。期待第二版\nMachine vision: theory, algorithms, practicalities 第三版 Davies著\n(附：为数不多的英国人写的书，偏向于工业应用。)\n数字图像处理 第四版 Pratt著\nDigital Image Processing\n(附：写作风格独树一帜，也是图像处理领域很不错的一本书。网上也可以找到非常清晰的电子版。)\n5. 小结\n罗嗦了这么多，实际上就是几个建议：\n（1）基础书千万不可以扔，也不能低价处理给同学或者师弟师妹。不然到时候还得一本本从书店再买回来的。钱是一方面的问题，对着全新的书看完全没有看自己当年上过的课本有感觉。\n（2）遇到有相关的课，果断选修或者蹭之，比如随机过程，小波分析，模式识别，机器学习，数据挖掘，现代信号处理甚至泛函。多一些理论积累对将来科研和工作都有好处。\n（3）资金允许的话可以多囤一些经典的书，有的时候从牙缝里面省一点都可以买一本好书。不过千万不要像我一样只囤不看。\n三、 计算机视觉中的信号处理与模式识别\n从本章开始，进入本文的核心章节。一共分三章，分别讲述信号处理与模式识别，图像处理与分析以及计算机视觉。与其说是讲述，不如说是一些经典文章的罗列以及自己的简单点评。与前一个版本不同的是，这次把所有的文章按类别归了类，并且增加了很多文献。分类的时候并没有按照传统的分类方法，而是划分成了一个个小的门类，比如SIFT，Harris都作为了单独的一类，虽然它们都可以划分到特征提取里面去。这样做的目的是希望能突出这些比较实用且比较流行的方法。为了以后维护的方便，按照字母顺序排的序。\n1. Boosting\nBoosting是最近十来年来最成功的一种模式识别方法之一，个人认为可以和SVM并称为模式识别双子星。它真正实现了“三个臭皮匠，赛过诸葛亮”。只要保证每个基本分类器的正确率超过50%，就可以实现组合成任意精度的分类器。这样就可以使用最简单的线性分类器。Boosting在计算机视觉中的最成功的应用无疑就是Viola-Jones提出的基于Haar特征的人脸检测方案。听起来似乎不可思议，但Haar+Adaboost确实在人脸检测上取得了巨大的成功，已经成了工业界的事实标准，并且逐步推广到其他物体的检测。\nRainer Lienhart在2002 ICIP发表的这篇文章是Haar+Adaboost的最好的扩展，他把原始的两个方向的Haar特征扩展到了四个方向，他本人是OpenCV积极的参与者。现在OpenCV的库里面实现的Cascade Classification就包含了他的方法。这也说明了盛会（如ICIP，ICPR，ICASSP）也有好文章啊，只要用心去发掘。\n[1997] A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\n[1998] Boosting the margin A new explanation for the effectiveness of voting methods\n[2002 ICIP TR] Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection\n[2003] The Boosting Approach to Machine Learning An Overview\n[2004 IJCV] Robust Real-time Face Detection\n2. Clustering\n聚类主要有K均值聚类，谱聚类和模糊聚类。在聚类的时候如果自动确定聚类中心的数目是一个一直没有解决的问题。不过这也很正常，评价标准不同，得到的聚类中心数目也不一样。不过这方面还是有一些可以参考的文献，在使用的时候可以基于这些方法设计自己的准则。关于聚类，一般的模式识别书籍都介绍的比较详细，不过关于cluster validity讲的比较少，可以参考下面的文章看看。\n[1989 PAMI] Unsupervised Optimal Fuzzy Clustering\n[1991 PAMI] A validity measure for fuzzy clustering\n[1995 PAMI] On cluster validity for the fuzzy c-means model\n[1998] Some New Indexes of Cluster Validity\n[1999 ACM] Data Clustering A Review\n[1999 JIIS] On Clustering Validation Techniques\n[2001] Estimating the number of clusters in a dataset via the Gap statistic\n[2001 NIPS] On Spectral Clustering\n[2002] A stability based method for discovering structure in clustered data\n[2007] A tutorial on spectral clustering\n3. Compressive Sensing\n最近大红大紫的压缩感知理论。\n[2006 TIT] Compressed Sensing\n[2008 SPM] An Introduction to Compressive Sampling\n[2011 TSP] Structured Compressed Sensing From Theory to Applications\n4. Decision Trees\n对决策树感兴趣的同学这篇文章是非看不可的了。\n[1986] Introduction to Decision Trees\n5. Dynamical Programming\n动态规划也是一个比较使用的方法，这里挑选了一篇PAMI的文章以及一篇Book Chapter\n[1990 PAMI] using dynamic programming for solving variational problems in vision\n[Book Chapter] Dynamic Programming\n6. Expectation Maximization\nEM是计算机视觉中非常常见的一种方法，尤其是对参数的估计和拟合，比如高斯混合模型。EM和GMM在Bishop的PRML里单独的作为一章，讲的很不错。关于EM的tutorial，网上也可以搜到很多。\n[1977] Maximum likelihood from incomplete data via the EM algorithm\n[1996 SPM] The Expectation-Maximzation Algorithm\n7. Graphical Models\n伯克利的乔丹大师的Graphical Model，可以配合这Bishop的PRML一起看。\n[1999 ML] An Introduction to Variational Methods for Graphical Models\n8. Hidden Markov Model\nHMM在语音识别中发挥着巨大的作用。在信号处理和图像处理中也有一定的应用。最早接触它是跟小波和检索相关的，用HMM来描述小波系数之间的相互关系，并用来做检索。这里提供一篇1989年的经典综述，几篇HMM在小波，分割，检索和纹理上的应用以及一本比较早的中文电子书，现在也不知道作者是谁，在这里对作者表示感谢。\n[1989 ] A tutorial on hidden markov models and selected applications in speech recognition\n[1998 TSP] Wavelet-based statistical signal processing using hidden Markov models\n[2001 TIP] Multiscale image segmentation using wavelet-domain hidden Markov models\n[2002 TMM] Rotation invariant texture characterization and retrieval using steerable wavelet-domain hidden Markov models\n[2003 TIP] Wavelet-based texture analysis and synthesis using hidden Markov models\nHmm Chinese book.pdf\n9. Independent Component Analysis\n同PCA一样，独立成分分析在计算机视觉中也发挥着重要的作用。这里介绍两篇综述性的文章，最后一篇是第二篇的TR版本，内容差不多，但比较清楚一些。\n[1999] Independent Component Analysis A Tutorial\n[2000 NN] Independent component analysis algorithms and applications\n[2000] Independent Component Analysis Algorithms and Applications\n10. Information Theory\n计算机视觉中的信息论。这方面有一本很不错的书Information Theory in Computer Vision and Pattern Recognition。这本书有电子版，如果需要用到的话，也可以参考这本书。\n[1995 NC] An Information-Maximization Approach to Blind Separation and Blind Deconvolution\n[2010] An information theory perspective on computational vision\n11. Kalman Filter\n这个话题在张贤达老师的现代信号处理里面讲的比较深入，还给出了一个有趣的例子。这里列出了Kalman的最早的论文以及几篇综述，还有Unscented Kalman Filter。同时也有一篇Kalman Filter在跟踪中的应用以及两本电子书。\n[1960 Kalman] A New Approach to Linear Filtering and Prediction Problems Kalman\n[1970] Least-squares estimation_from Gauss to Kalman\n[1997 SPIE] A New Extension of the Kalman Filter to Nonlinear System\n[2000] The Unscented Kalman Filter for Nonlinear Estimation\n[2001 Siggraph] An Introduction to the Kalman Filter_full\n[2003] A Study of the Kalman Filter applied to Visual Tracking\n12. Pattern Recognition and Machine Learning\n模式识别名气比较大的几篇综述\n[2000 PAMI] Statistical pattern recognition a review\n[2004 CSVT] An Introduction to Biometric Recognition\n[2010 SPM] Machine Learning in Medical Imaging\n13. Principal Component Analysis\n著名的PCA，在特征的表示和特征降维上非常有用。\n[2001 PAMI] PCA versus LDA\n[2001] Nonlinear component analysis as a kernel eigenvalue problem\n[2002] A Tutorial on Principal Component Analysis\n[2009] A Tutorial on Principal Component Analysis\n[2011] Robust Principal Component Analysis\n[Book Chapter] Singular Value Decomposition and Principal Component Analysis\n14. Random Forest\n随机森林\n[2001 ML] Random Forests\n15. RANSAC\n随机抽样一致性方法，与传统的最小均方误差等完全是两个路子。在Sonka的书里面也有提到。\n[2009 BMVC] Performance Evaluation of RANSAC Family\n16. Singular Value Decomposition\n对于非方阵来说，就是SVD发挥作用的时刻了。一般的模式识别书都会介绍到SVD。这里列出了K-SVD以及一篇Book Chapter\n[2006 TSP] K-SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation\n[Book Chapter] Singular Value Decomposition and Principal Component Analysis\n17. Sparse Representation\n这里主要是Proceeding of IEEE上的几篇文章\n[2009 PAMI] Robust Face Recognition via Sparse Representation\n[2009 PIEEE] Image Decomposition and Separation Using Sparse Representations An Overview\n[2010 PIEEE] Dictionaries for Sparse Representation Modeling\n[2010 PIEEE] It's All About the Data\n[2010 PIEEE] Matrix Completion With Noise\n[2010 PIEEE] On the Role of Sparse and Redundant Representations in Image Processing\n[2010 PIEEE] Sparse Representation for Computer Vision and Pattern Recognition\n[2011 SPM] Directionary Learning\n18. Support Vector Machines\n[1998] A Tutorial on Support Vector Machines for Pattern Recognition\n[2004] LIBSVM A Library for Support Vector Machines\n19. Wavelet\n在小波变换之前，时频分析的工具只有傅立叶变换。众所周知，傅立叶变换在时域没有分辨率，不能捕捉局部频域信息。虽然短时傅立叶变换克服了这个缺点，但只能刻画恒定窗口的频率特性，并且不能很好的扩展到二维。小波变换的出现很好的解决了时频分析的问题，作为一种多分辨率分析工具，在图像处理中得到了极大的发展和应用。在小波变换的发展过程中，有几个人是不得不提的，Mallat， Daubechies，Vetteri， M.N.Do， Swelden，Donoho。Mallat和Daubechies奠定了第一代小波的框架，他们的著作更是小波变换的必读之作，相对来说，小波十讲太偏数学了，比较难懂。而Mallat的信号处理的小波导引更偏应用一点。Swelden提出了第二代小波，使小波变换能够快速方便的实现，他的功劳有点类似于FFT。而Donoho，Vetteri，Mallat及其学生们提出了Ridgelet, Curvelet, Bandelet,Contourlet等几何小波变换，让小波变换有了方向性，更便于压缩，去噪等任务。尤其要提的是M.N.Do，他是一个越南人，得过IMO的银牌，在这个领域著作颇丰。我们国家每年都有5个左右的IMO金牌，希望也有一两个进入这个领域，能够也让我等也敬仰一下。而不是一股脑的都进入金融，管理这种跟数学没有多大关系的行业，呵呵。很希望能看到中国的陶哲轩，中国的M.N.Do。\n说到小波，就不得不提JPEG2000。在JPEG2000中使用了Swelden和Daubechies提出的用提升算法实现的9/7小波和5/3小波。如果对比JPEG和JPEG2000，就会发现JPEG2000比JPEG在性能方面有太多的提升。本来我以为JPEG2000的普及只是时间的问题。但现在看来，这个想法太Naive了。现在已经过去十几年了，JPEG2000依然没有任何出头的迹象。不得不说，工业界的惯性力量太强大了。如果以前的东西没有什么硬伤的话，想改变太难了。不巧的是，JPEG2000的种种优点在最近的硬件上已经有了很大的提升。压缩率？现在动辄1T，2T的硬盘，没人太在意压缩率。渐进传输？现在的网速包括无线传输的速度已经相当快了，渐进传输也不是什么优势。感觉现在做图像压缩越来越没有前途了，从最近的会议和期刊文档也可以看出这个趋势。不管怎么说，JPEG2000的Overview还是可以看看的。\n[1989 PAMI] A theory for multiresolution signal decomposition__the wavelet representation\n[1996 PAMI] Image Representation using 2D Gabor Wavelet\n[1998 ] FACTORING WAVELET TRANSFORMS INTO LIFTING STEPS\n[1998] The Lifting Scheme_ A Construction Of Second Generation Wavelets\n[2000 TCE] The JPEG2000 still image coding system_ an overview\n[2002 TIP] The curvelet transform for image denoising\n[2003 TIP] Gray and color image contrast enhancement by the curvelet transform\n[2003 TIP] Mathematical Properties of the jpeg2000 wavelet filters\n[2003 TIP] The finite ridgelet transform for image representation\n[2005 TIP] Sparse Geometric Image Representations With Bandelets\n[2005 TIP] The Contourlet Transform_ An Efficient Directional Multiresolution Image Representation\n[2010 SPM] The Curvelet Transform\n四、 图像处理与分析\n本章主要讨论图像处理与分析。虽然后面计算机视觉部分的有些内容比如特征提取等也可以归结到图像分析中来，但鉴于它们与计算机视觉的紧密联系，以及它们的出处，没有把它们纳入到图像处理与分析中来。同样，这里面也有一些也可以划归到计算机视觉中去。这都不重要，只要知道有这么个方法，能为自己所用，或者从中得到灵感，这就够了。\n1. Bilateral Filter\nBilateral Filter俗称双边滤波器是一种简单实用的具有保持边缘作用的平缓滤波器，由Tomasi等在1998年提出。它现在已经发挥着重大作用，尤其是在HDR领域。\n[1998 ICCV] Bilateral Filtering for Gray and Color Images\n[2008 TIP] Adaptive Bilateral Filter for Sharpness Enhancement and Noise Removal\n2. Color\n如果对颜色的形成有一定的了解，能比较深刻的理解一些算法。这方面推荐冈萨雷斯的数字图像处理中的相关章节以及Sharma在Digital Color Imaging Handbook中的第一章“Color fundamentals for digital imaging”。跟颜色相关的知识包括Gamma，颜色空间转换，颜色索引以及肤色模型等，这其中也包括著名的EMD。\n[1991 IJCV] Color Indexing\n[2000 IJCV] The Earth Mover's Distance as a Metric for Image Retrieval\n[2001 PAMI] Color invariance\n[2002 IJCV] Statistical Color Models with Application to Skin Detection\n[2003] A review of RGB color spaces\n[2007 PR]A survey of skin-color modeling and detection methods\nGamma.pdf\nGammaFAQ.pdf\n3. Compression and Encoding\n个人以为图像压缩编码并不是当前很热的一个话题，原因前面已经提到过。这里可以看看一篇对编码方面的展望文章\n[2005 IEEE] Trends and perspectives in image and video coding\n4. Contrast Enhancement\n对比度增强一直是图像处理中的一个恒久话题，一般来说都是基于直方图的，比如直方图均衡化。冈萨雷斯的书里面对这个话题讲的比较透彻。这里推荐几篇个人认为不错的文章。\n[2002 IJCV] Vision and the Atmosphere\n[2003 TIP] Gray and color image contrast enhancement by the curvelet transform\n[2006 TIP] Gray-level grouping (GLG) an automatic method for optimized image contrast enhancement-part II\n[2006 TIP] Gray-level grouping (GLG) an automatic method for optimized image contrast Enhancement-part I\n[2007 TIP] Transform Coefficient Histogram-Based Image Enhancement Algorithms Using Contrast Entropy\n[2009 TIP] A Histogram Modification Framework and Its Application for Image Contrast Enhancement\n5. Deblur (Restoration)\n图像恢复或者图像去模糊一直是一个非常难的问题，尤其是盲图像恢复。港中文的jiaya jia老师在这方面做的不错，他在主页也给出了可执行文件。这方面的内容也建议看冈萨雷斯的书。这里列出了几篇口碑比较好的文献，包括古老的Richardson-Lucy方法，几篇盲图像恢复的综述以及最近的几篇文章，尤以Fergus和Jiaya Jia的为经典。\n[1972] Bayesian-Based Iterative Method of Image Restoration\n[1974] an iterative technique for the rectification of observed distributions\n[1990 IEEE] Iterative methods for image deblurring\n[1996 SPM] Blind Image Deconvolution\n[1997 SPM] Digital image restoration\n[2005] Digital Image Reconstruction - Deblurring and Denoising\n[2006 Siggraph] Removing Camera Shake from a Single Photograph\n[2008 Siggraph] High-quality Motion Deblurring from a Single Image\n[2011 PAMI] Richardson-Lucy Deblurring for Scenes under a Projective Motion Path\n6. Dehazing and Defog\n严格来说去雾化也算是图像对比度增强的一种。这方面最近比较好的工作就是He kaiming等提出的Dark Channel方法。这篇论文也获得了2009的CVPR 最佳论文奖。2这位003年的广东高考状元已经于2011年从港中文博士毕业加入MSRA（估计当时也就二十五六岁吧），相当了不起。\n[2008 Siggraph] Single Image Dehazing\n[2009 CVPR] Single Image Haze Removal Using Dark Channel Prior\n[2011 PAMI] Single Image Haze Removal Using Dark Channel Prior\n7. Denoising\n图像去噪也是图像处理中的一个经典问题，在数码摄影中尤其重要。主要的方法有基于小波的方法和基于偏微分方程的方法。\n[1992 SIAM] Image selective smoothing and edge detection by nonlinear diffusion. II\n[1992 SIAM] Image selective smoothing and edge detection by nonlinear diffusion\n[1992] Nonlinear total variation based noise removal algorithms\n[1994 SIAM] Signal and image restoration using shock filters and anisotropic diffusion\n[1995 TIT] De-noising by soft-thresholding\n[1998 TIP] Orientation diffusions\n[2000 TIP] Adaptive wavelet thresholding for image denoising and compression\n[2000 TIP] Fourth-order partial differential equations for noise removal\n[2001] Denoising through wavelet shrinkage\n[2002 TIP] The Curvelet Transform for Image Denoising\n[2003 TIP] Noise removal using fourth-order partial differential equation with applications to medical magnetic resonance images in space and time\n[2008 PAMI] Automatic Estimation and Removal of Noise from a Single Image\n[2009 TIP] Is Denoising Dead\n8. Edge Detection\n边缘检测也是图像处理中的一个基本任务。传统的边缘检测方法有基于梯度算子，尤其是Sobel算子，以及经典的Canny边缘检测。到现在，Canny边缘检测及其思想仍在广泛使用。关于Canny算法的具体细节可以在Sonka的书以及canny自己的论文中找到，网上也可以搜到。最快最直接的方法就是看OpenCV的源代码，非常好懂。在边缘检测方面，Berkeley的大牛J Malik和他的学生在2004年的PAMI提出的方法效果非常好，当然也比较复杂。在复杂度要求不高的情况下，还是值得一试的。MIT的Bill Freeman早期的代表作Steerable Filter在边缘检测方面效果也非常好，并且便于实现。这里给出了几篇比较好的文献，包括一篇最新的综述。边缘检测是图像处理和计算机视觉中任何方向都无法逃避的一个问题，这方面研究多深都不为过。\n[1980] theory of edge detection\n[1983 Canny Thesis] find edge\n[1986 PAMI] A Computational Approach to Edge Detection\n[1990 PAMI] Scale-space and edge detection using anisotropic diffusion\n[1991 PAMI] The design and use of steerable filters\n[1995 PR] Multiresolution edge detection techniques\n[1996 TIP] Optimal edge detection in two-dimensional images\n[1998 PAMI] Local Scale Control for Edge Detection and Blur Estimation\n[2003 PAMI] Statistical edge detection_ learning and evaluating edge cues\n[2004 IEEE] Edge Detection Revisited\n[2004 PAMI] Design of steerable filters for feature detection using canny-like criteria\n[2004 PAMI] Learning to Detect Natural Image Boundaries Using Local Brightness, Color, and Texture Cues\n[2011 IVC] Edge and line oriented contour detection State of the art\n9. Graph Cut\n基于图割的图像分割算法。在这方面没有研究，仅仅列出几篇引用比较高的文献。这里又见J Malik，当然还有华人杰出学者Jianbo Shi，他的主页非常搞笑，在醒目的位置标注Do not fly China Eastern Airlines ... 看来是被坑过，而且坑的比较厉害。这个领域，俄罗斯人比较厉害。\n[2000 PAMI] Normalized cuts and image segmentation\n[2001 PAMI] Fast approximate energy minimization via graph cuts\n[2004 PAMI] What energy functions can be minimized via graph cuts\n10. Hough Transform\n虽然霍夫变换可以扩展到广义霍夫变换，但最常用的还是检测圆和直线。这方面同样推荐看OpenCV的源代码，一目了然。Matas在2000年提出的PPHT已经集成到OpenCV中去了。\n[1986 CVGIU] A Survey of the Hough Transform\n[1989] A Comparative study of Hough transform methods for circle finding\n[1992 PAMI] Shapes recognition using the straight line Hough transform_ theory and generalization\n[1997 PR] Extraction of line features in a noisy image\n[2000 CVIU] Robust Detection of Lines Using the Progressive Probabilistic Hough Transform\n11. Image Interpolation\n图像插值，偶尔也用得上。一般来说，双三次也就够了\n[2000 TMI] Interpolation revisited\n12. Image Matting\n也就是最近，我才知道这个词翻译成中文是抠图，比较难听，不知道是谁开始这么翻译的。没有研究，请看文章以及Richard Szeliski的相关章节。以色列美女Levin在这方面有两篇PAMI。\n[2008 Fnd] Image and Video Matting A Survey\n[2008 PAMI] A Closed-Form Solution to Natural Image Matting\n[2008 PAMI] Spectral Matting\n13. Image Modeling\n图像的统计模型。这方面有一本专门的著作Natural Image Statistics\n[1994] The statistics of natural images\n[2003 JMIV] On Advances in Statistical Modeling of Natural Images\n[2009 IJCV] Fields of Experts\n[2009 PAMI] Modeling multiscale subbands of photographic images with fields of Gaussian scale mixtures\n14. Image Quality Assessment\n在图像质量评价方面，Bovik是首屈一指的。这位老师也很有意思，作为编辑出版了很多书。他也是IEEE的Fellow\n[2004 TIP] Image quality assessment from error visibility to structural similarity\n[2011 TIP] blind image quality assessment From Natural Scene Statistics to Perceptual Quality\n15. Image Registration\n图像配准最早的应用在医学图像上，在图像融合之前需要对图像进行配准。在现在的计算机视觉中，配准也是一个需要理解的概念，比如跟踪，拼接等。在KLT中，也会涉及到配准。这里主要是综述文献。\n[1992 MIA] Image matching as a diffusion process\n[1992 PAMI] A Method for Registration of 3-D shapes\n[1992] a survey of image registration techniques\n[1998 MIA] A survey of medical image registration\n[2003 IVC] Image registration methods a survey\n[2003 TMI] Mutual-Information-Based Registration of Medical Survey\n[2011 TIP] Hairis registration\n16. Image Retrieval\n图像检索曾经很热，在2000年之后似乎消停了一段时间。最近各种图像的不变性特征提出来之后，再加上互联网搜索的商业需求，这个方向似乎又要火起来了，尤其是在商业界，比如淘淘搜。这仍然是一个非常值得关注的方面。而且图像检索与目标识别具有相通之处，比如特征提取和特征降维。这方面的文章值得一读。在最后给出了两篇Book chapter，其中一篇还是中文的。\n[2000 PAMI] Content-based image retrieval at the end of the early years\n[2000 TIP] PicToSeek Combining Color and Shape Invariant Features for Image Retrieval\n[2002] Content-Based Image Retrieval Systems A Survey\n[2008] Content-Based Image Retrieval-Literature Survey\n[2010] Plant Image Retrieval Using Color,Shape and Texture Features\n[2012 PAMI] A Multimedia Retrieval Framework Based on Semi-Supervised Ranking and Relevance Feedback\nCBIR Chinese\nfundament of cbir\n17. Image Segmentation\n图像分割，非常基本但又非常难的一个问题。建议看Sonka和冈萨雷斯的书。这里给出几篇比较好的文章，再次看到了J Malik。他们给出了源代码和测试集，有兴趣的话可以试试。\n[2004 IJCV] Efficient Graph-Based Image Segmentation\n[2008 CVIU] Image segmentation evaluation A survey of unsupervised methods\n[2011 PAMI] Contour Detection and Hierarchical Image Segmentation\n18. Level Set\n大名鼎鼎的水平集，解决了Snake固有的缺点。Level set的两位提出者Sethian和Osher最后反目，实在让人遗憾。个人以为，这种方法除了迭代比较费时，在真实场景中的表现让人生疑。不过，2008年ECCV上的PWP方法在结果上很吸引人。在重初始化方面，Chunming Li给出了比较好的解决方案\n[1995 PAMI] Shape modeling with front propagation_ a level set approach\n[2001 JCP] Level Set Methods_ An Overview and Some Recent Results\n[2005 CVIU] Geodesic active regions and level set methods for motion estimation and tracking\n[2007 IJCV] A Review of Statistical Approaches to Level Set Segmentation\n[2008 ECCV] Robust Real-Time Visual Tracking using Pixel-Wise Posteriors\n[2010 TIP] Distance Regularized Level Set Evolution and its Application to Image Segmentation\n19. Pyramid\n其实小波变换就是一种金字塔分解算法，而且具有无失真重构和非冗余的优点。Adelson在1983年提出的Pyramid优点是比较简单，实现起来比较方便。\n[1983] The Laplacian Pyramid as a Compact Image Code\n20. Radon Transform\nRadon变换也是一种很重要的变换，它构成了图像重建的基础。关于图像重建和radon变换，可以参考章毓晋老师的书，讲的比较清楚。\n[1993 PAMI] Image representation via a finite Radon transform\n[1993 TIP] The fast discrete radon transform I theory\n[2007 IVC] Generalised finite radon transform for N×N images\n21. Scale Space\n尺度空间滤波在现代不变特征中是一个非常重要的概念，有人说SIFT的提出者Lowe是不变特征之父，而Linderburg是不变特征之母。虽然尺度空间滤波是Witkin最早提出的，但其理论体系的完善和应用还是Linderburg的功劳。其在1998年IJCV上的两篇文章值得一读，不管是特征提取方面还是边缘检测方面。\n[1987] Scale-space filtering\n[1990 PAMI] Scale-Space for Discrete Signals\n[1994] Scale-space theory A basic tool for analysing structures at different scales\n[1998 IJCV] Edge Detection and Ridge Detection with Automatic Scale Selection\n[1998 IJCV] Feature Detection with Automatic Scale Selection\n22. Snake\n活动轮廓模型，改变了传统的图像分割的方法，用能量收缩的方法得到一个统计意义上的能量最小（最大）的边缘。\n[1987 IJCV] Snakes Active Contour Models\n[1996 ] deformable model in medical image A Survey\n[1997 IJCV] geodesic active contour\n[1998 TIP] Snakes, shapes, and gradient vector flow\n[2000 PAMI] Geodesic active contours and level sets for the detection and tracking of moving objects\n[2001 TIP] Active contours without edges\n23. Super Resolution\n超分辨率分析。对这个方向没有研究，简单列几篇文章。其中Yang Jianchao的那篇在IEEE上的下载率一直居高不下。\n[2002] Example-Based Super-Resolution\n[2009 ICCV] Super-Resolution from a Single Image\n[2010 TIP] Image Super-Resolution Via Sparse Representation\n24. Thresholding\n阈值分割是一种简单有效的图像分割算法。这个topic在冈萨雷斯的书里面讲的比较多。这里列出OTSU的原始文章以及一篇不错的综述。\n[1979 IEEE] OTSU A threshold selection method from gray-level histograms\n[2001 JISE] A Fast Algorithm for Multilevel Thresholding\n[2004 JEI] Survey over image thresholding techniques and quantitative performance evaluation\n25. Watershed\n分水岭算法是一种非常有效的图像分割算法，它克服了传统的阈值分割方法的缺点，尤其是Marker-Controlled Watershed，值得关注。Watershed在冈萨雷斯的书里面讲的比较详细。\n[1991 PAMI] Watersheds in digital spaces an efficient algorithm based on immersion simulations\n[2001]The Watershed Transform Definitions, Algorithms and Parallelizat on Strategies\n五、 计算机视觉\n这一章是计算机视觉部分，主要侧重在底层特征提取，视频分析，跟踪，目标检测和识别方面等方面。对于自己不太熟悉的领域比如摄像机标定和立体视觉，仅仅列出上google上引用次数比较多的文献。有一些刚刚出版的文章，个人非常喜欢，也列出来了。\n1. Active Appearance Models\n活动表观模型和活动轮廓模型基本思想来源Snake，现在在人脸三维建模方面得到了很成功的应用，这里列出了三篇最早最经典的文章。对这个领域有兴趣的可以从这三篇文章开始入手。\n[1998 ECCV] Active Appearance Models\n[2001 PAMI] Active Appearance Models\n2. Active Shape Models\n[1995 CVIU]Active Shape Models-Their Training and Application\n3. Background modeling and subtraction\n背景建模一直是视频分析尤其是目标检测中的一项关键技术。虽然最近一直有一些新技术的产生，demo效果也很好，比如基于dynamical texture的方法。但最经典的还是Stauffer等在1999年和2000年提出的GMM方法，他们最大的贡献在于不用EM去做高斯拟合，而是采用了一种迭代的算法，这样就不需要保存很多帧的数据，节省了buffer。Zivkovic在2004年的ICPR和PAMI上提出了动态确定高斯数目的方法，把混合高斯模型做到了极致。这种方法效果也很好，而且易于实现。在OpenCV中有现成的函数可以调用。在背景建模大家族里，无参数方法（2000 ECCV）和Vibe方法也值得关注。\n[1997 PAMI] Pfinder Real-Time Tracking of the Human Body\n[1999 CVPR] Adaptive background mixture models for real-time tracking\n[1999 ICCV] Wallflower Principles and Practice of Background Maintenance\n[2000 ECCV] Non-parametric Model for Background Subtraction\n[2000 PAMI] Learning Patterns of Activity Using Real-Time Tracking\n[2002 PIEEE] Background and foreground modeling using nonparametric\nkernel density estimation for visual surveillance\n[2004 ICPR] Improved adaptive Gaussian mixture model for background subtraction\n[2004 PAMI] Recursive unsupervised learning of finite mixture models\n[2006 PRL] Efficient adaptive density estimation per image pixel for the task of background subtraction\n[2011 TIP] ViBe A Universal Background Subtraction Algorithm for Video Sequences\n4. Bag of Words\n词袋，在这方面暂时没有什么研究。列出三篇引用率很高的文章，以后逐步解剖之。\n[2003 ICCV] Video Google A Text Retrieval Approach to Object Matching in Videos\n[2004 ECCV] Visual Categorization with Bags of Keypoints\n[2006 CVPR] Beyond bags of features Spatial pyramid matching for recognizing natural scene categories\n5. BRIEF\nBRIEF是Binary Robust Independent Elementary Features的简称，是近年来比较受关注的特征描述的方法。ORB也是基于BRIEF的。\n[2010 ECCV] BRIEF Binary Robust Independent Elementary Features\n[2011 ICCV] ORB an efficient alternative to SIFT or SURF\n[2012 PAMI] BRIEF Computing a Local Binary Descriptor Very Fast\n6. Camera Calibration and Stereo Vision\n非常不熟悉的领域。仅仅列出了十来篇重要的文献，供以后学习。\n[1979 Marr] A Computational Theory of Human Stereo Vision\n[1985] Computational vision and regularization theory\n[1987 IEEE] A versatile camera calibration technique for\nhigh-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses\n[1987] Probabilistic Solution of Ill-Posed Problems in Computational Vision\n[1988 PIEEE] Ill-Posed Problems in Early Vision\n[1989 IJCV] Kalman Filter-based Algorithms for Estimating Depth from Image Sequences\n[1990 IJCV] Relative Orientation\n[1990 IJCV] Using vanishing points for camera calibration\n[1992 ECCV] Camera self-calibration Theory and experiments\n[1992 IJCV] A theory of self-calibration of a moving camera\n[1992 PAMI] Camera calibration with distortion models and accuracy evaluation\n[1994 IJCV] The Fundamental Matrix Theory, Algorithms, and Stability Analysis\n[1994 PAMI] a stereo matching algorithm with an adaptive window theory and experiment\n[1999 ICCV] Flexible camera calibration by viewing a plane from unknown orientations\n[1999 IWAR] Marker tracking and hmd calibration for a video-based augmented reality conferencing system\n[2000 PAMI] A flexible new technique for camera calibration\n7. Color and Histogram Feature\n这里面主要来源于图像检索，早期的图像检测基本基于全局的特征，其中最显著的就是颜色特征。这一部分可以和前面的Color知识放在一起的。\n[1995 SPIE] Similarity of color images\n[1996 PR] IMAGE RETRIEVAL USING COLOR AND SHAPE\n[1996] comparing images using color coherence vectors\n[1997 ] Image Indexing Using Color Correlograms\n[2001 TIP] An Efficient Color Representation for Image Retrieval\n[2009 CVIU] Performance evaluation of local colour invariants\n8. Deformable Part Model\n大红大热的DPM，在OpenCV中有一个专门的topic讲DPM和latent svm\n[2008 CVPR] A Discriminatively Trained, Multiscale, Deformable Part Model\n[2010 CVPR] Cascade Object Detection with Deformable Part Models\n[2010 PAMI] Object Detection with Discriminatively Trained Part-Based Models\n9. Distance Transformations\n距离变换，在OpenCV中也有实现。用来在二值图像中寻找种子点非常方便。\n[1986 CVGIP] Distance Transformations in Digital Images\n[2008 ACM] 2D Euclidean Distance Transform Algorithms A Comparative Survey\n10. Face Detection\n最成熟最有名的当属Haar+Adaboost\n[1998 PAMI] Neural Network-Based Face Detection\n[2002 PAMI] Detecting faces in images a survey\n[2002 PAMI] Face Detection in Color Images\n[2004 IJCV] Robust Real-Time Face Detection\n11. Face Recognition\n不熟悉，简单罗列之。\n[1991] Face Recognition Using Eigenfaces\n[2000 PAMI] Automatic Analysis of Facial Expressions The State of the Art\n[2000] Face Recognition A Literature Survey\n[2006 PR] Face recognition from a single image per person A survey\n[2009 PAMI] Robust Face Recognition via Sparse Representation\n12. FAST\n用机器学习的方法来提取角点，号称很快很好。\n[2006 ECCV] Machine learning for high-speed corner detection\n[2010 PAMI] Faster and Better A Machine Learning Approach to Corner Detection\n13. Feature Extraction\n这里的特征主要都是各种不变性特征，SIFT，Harris，MSER等也属于这一类。把它们单独列出来是因为这些方法更流行一点。关于不变性特征，王永明与王贵锦合著的《图像局部不变性特征与描述》写的还不错。Mikolajczyk在2005年的PAMI上的文章以及2007年的综述是不错的学习材料。\n[1989 PAMI] On the detection of dominant points on digital curves\n[1997 IJCV] SUSAN—A New Approach to Low Level Image Processing\n[2004 IJCV] Matching Widely Separated Views Based on Affine Invariant Regions\n[2004 IJCV] Scale & Affine Invariant Interest Point Detectors\n[2005 PAMI] A performance evaluation of local descriptors\n[2006 IJCV] A Comparison of Affine Region Detectors\n[2007 FAT] Local Invariant Feature Detectors - A Survey\n[2011 IJCV] Evaluation of Interest Point Detectors and Feature Descriptors\n14. Feature Matching\nFua课题组在今年PAMI上的一篇文章，感觉还不错\n[2012 PAMI] LDAHash Improved Matching with Smaller Descriptors\n15. Harris\n虽然过去了很多年，Harris角点检测仍然广泛使用，而且基于它有很多变形。如果仔细看了这种方法，从直观也可以感觉到这是一种很稳健的方法。\n[1988 Harris] A combined corner and edge detector\n16. Histograms of Oriented Gradients\nHoG方法也在OpenCV中实现了：HOGDescriptor。\n[2005 CVPR] Histograms of Oriented Gradients for Human Detection\nNavneetDalalThesis.pdf\n17. Image Distance\n[1993 PAMI] Comparing Images Using the Hausdorff Distance\n18. Image Stitching\n图像拼接，另一个相关的词是Panoramic。在Computer Vision: Algorithms and Applications一书中，有专门一章是讨论这个问题。这里的两面文章一篇是综述，一篇是这方面很经典的文章。\n[2006 Fnd] Image Alignment and Stitching A Tutorial\n[2007 IJCV] Automatic Panoramic Image Stitching using Invariant Features\n19. KLT\nKLT跟踪算法，基于Lucas-Kanade提出的配准算法。除了三篇很经典的文章，最后一篇给出了OpenCV实现KLT的细节。\n[1981] An Iterative Image Registration Technique with an Application to Stereo Vision full version\n[1994 CVPR] Good Features to Track\n[2004 IJCV] Lucas-Kanade 20 Years On A Unifying Framework\nPyramidal Implementation of the Lucas Kanade Feature Tracker OpenCV\n20. Local Binary Pattern\nLBP。OpenCV的Cascade分类器也支持LBP，用来取代Haar特征。\n[2002 PAMI] Multiresolution gray-scale and rotation Invariant Texture Classification with Local Binary Patterns\n[2004 ECCV] Face Recognition with Local Binary Patterns\n[2006 PAMI] Face Description with Local Binary Patterns\n[2011 TIP] Rotation-Invariant Image and Video Description With Local Binary Pattern Features\n21. Low-Level Vision\n关于Low level vision的两篇很不错的文章\n[1998 TIP] A general framework for low level vision\n[2000 IJCV] Learning Low-Level Vision\n22. Mean Shift\n均值漂移算法，在跟踪中非常流行的方法。Comaniciu在这个方面做出了重要的贡献。最后三篇，一篇是CVIU上的top download文章，一篇是最新的PAMI上关于Mean Shift的文章，一篇是OpenCV实现的文章。\n[1995 PAMI] Mean shift, mode seeking, and clustering\n[2002 PAMI] Mean shift a robust approach toward feature space analysis\n[2003 CVPR] Mean-shift blob tracking through scale space\n[2009 CVIU] Object tracking using SIFT features and mean shift\n[2012 PAMI] Mean Shift Trackers with Cross-Bin Metrics\nOpenCV Computer Vision Face Tracking For Use in a Perceptual User Interface\n23. MSER\n这篇文章发表在2002年的BMVC上，后来直接录用到2004年的IVC上，内容差不多。MSER在Sonka的书里面也有提到。\n[2002 BMVC] Robust Wide Baseline Stereo from Maximally Stable Extremal Regions\n[2003] MSER Author Presentation\n[2004 IVC] Robust wide-baseline stereo from maximally stable extremal regions\n[2011 PAMI] Are MSER Features Really Interesting\n24. Object Detection\n首先要说的是第一篇文章的作者，Kah-Kay Sung。他是MIT的博士，后来到新加坡国立任教，极具潜力的一个老师。不幸的是，他和他的妻子都在2000年的新加坡空难中遇难，让人唏嘘不已。\nhttp://en.wikipedia.org/wiki/Singapore_Airlines_Flight_006\n最后一篇文章也是Fua课题组的，作者给出的demo效果相当好。\n[1998 PAMI] Example-based learning for view-based human face detection\n[2003 IJCV] Learning the Statistics of People in Images and Video\n[2011 PAMI] Learning to Detect a Salient Object\n[2012 PAMI] A Real-Time Deformable Detector\n25. Object Tracking\n跟踪也是计算机视觉中的经典问题。粒子滤波，卡尔曼滤波，KLT，mean shift，光流都跟它有关系。这里列出的是传统意义上的跟踪，尤其值得一看的是2008的Survey和2003年的Kernel based tracking。\n[2003 PAMI] Kernel-based object tracking\n[2007 PAMI] Tracking People by Learning Their Appearance\n[2008 ACM] Object Tracking A Survey\n[2008 PAMI] Segmentation and Tracking of Multiple Humans in Crowded Environments\n[2011 PAMI] Hough Forests for Object Detection, Tracking, and Action Recognition\n[2011 PAMI] Robust Object Tracking with Online Multiple Instance Learning\n[2012 IJCV] PWP3D Real-Time Segmentation and Tracking of 3D Objects\n26. OCR\n一个非常成熟的领域，已经很好的商业化了。\n[1992 IEEE] Historical review of OCR research and development\nVideo OCR A Survey and Practitioner's Guide\n27. Optical Flow\n光流法，视频分析所必需掌握的一种算法。\n[1981 AI] Determine Optical Flow\n[1994 IJCV] Performance of optical flow techniques\n[1995 ACM] The Computation of Optical Flow\n[2004 TR] Tutorial Computing 2D and 3D Optical Flow\n[2005 BOOK] Optical Flow Estimation\n[2008 ECCV] Learning Optical Flow\n[2011 IJCV] A Database and Evaluation Methodology for Optical Flow\n28. Particle Filter\n粒子滤波，主要给出的是综述以及1998 IJCV上的关于粒子滤波发展早期的经典文章。\n[1998 IJCV] CONDENSATION—Conditional Density Propagation for Visual Tracking\n[2002 TSP] A tutorial on particle filters for online nonlinear non-Gaussian Bayesian tracking\n[2002 TSP] Particle filters for positioning, navigation, and tracking\n[2003 SPM] particle filter\n29. Pedestrian and Human detection\n仍然是综述类，关于行人和人体的运动检测和动作识别。\n[1999 CVIU] Visual analysis of human movement_ A survey\n[2001 CVIU] A Survey of Computer Vision-Based Human Motion Capture\n[2005 TIP] Image change detection algorithms a systematic survey\n[2006 CVIU] a survey of avdances in vision based human motion capture\n[2007 CVIU] Vision-based human motion analysis An overview\n[2007 IJCV] Pedestrian Detection via Periodic Motion Analysis\n[2007 PR] A survey of skin-color modeling and detection methods\n[2010 IVC] A survey on vision-based human action recognition\n[2012 PAMI] Pedestrian Detection An Evaluation of the State of the Art\n30. Scene Classification\n当相机越来越傻瓜化的时候，自动场景识别就非常重要。这是比拼谁家的Auto功能做的比较好的时候了。\n[2001 IJCV] Modeling the Shape of the Scene A Holistic Representation of the Spatial Envelope\n[2001 PAMI] Visual Word Ambiguity\n[2007 PAMI] A Thousand Words in a Scene\n[2010 PAMI] Evaluating Color Descriptors for Object and Scene Recognition\n[2011 PAMI] CENTRIST A Visual Descriptor for Scene Categorization\n31. Shadow Detection\n[2003 PAMI] Detecting moving shadows-- algorithms and evaluation\n32. Shape\n关于形状，主要是两个方面：形状的表示和形状的识别。形状的表示主要是从边缘或者区域当中提取不变性特征，用来做检索或者识别。这方面Sonka的书讲的比较系统。2008年的那篇综述在这方面也讲的不错。至于形状识别，最牛的当属J Malik等提出的Shape Context。\n[1993 PR] IMPROVED MOMENT INVARIANTS FOR SHAPE DISCRIMINATION\n[1993 PR] Pattern Recognition by Affine Moment Invariants\n[1996 PR] IMAGE RETRIEVAL USING COLOR AND SHAPE\n[2001 SMI] Shape matching similarity measures and algorithms\n[2002 PAMI] Shape matching and object recognition using shape contexts\n[2004 PR] Review of shape representation and description techniques\n[2006 PAMI] Integral Invariants for Shape Matching\n[2008] A Survey of Shape Feature Extraction Techniques\n33. SIFT\n关于SIFT，实在不需要介绍太多，一万多次的引用已经说明问题了。SURF和PCA-SIFT也是属于这个系列。后面列出了几篇跟SIFT有关的问题。\n[1999 ICCV] Object recognition from local scale-invariant features\n[2000 IJCV] Evaluation of Interest Point Detectors\n[2003 CVIU] Speeded-Up Robust Features (SURF)\n[2004 CVPR] PCA-SIFT A More Distinctive Representation for Local Image Descriptors\n[2004 IJCV] Distinctive Image Features from Scale-Invariant Keypoints\n[2010 IJCV] Improving Bag-of-Features for Large Scale Image Search\n[2011 PAMI] SIFTflow Dense Correspondence across Scenes and its Applications\n34. SLAM\nSimultaneous Localization and Mapping, 同步定位与建图。\nSLAM问题可以描述为: 机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置估计和地图进行自身定位,同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。\n[2002 PAMI] Simultaneous Localization and Map-Building Using Active Vision\n[2007 PAMI] MonoSLAM Real-Time Single Camera SLAM\n35. Texture Feature\n纹理特征也是物体识别和检索的一个重要特征集。\n[1973] Textural features for image classification\n[1979 ] Statistical and structural approaches to texture\n[1996 PAMI] Texture features for browsing and retrieval of image data\n[2002 PR] Brief review of invariant texture analysis methods\n[2012 TIP] Color Local Texture Features for Color Face Recognition\n36. TLD\nKadal创立了TLD，跟踪学习检测同步进行，达到稳健跟踪的目的。他的两个导师也是大名鼎鼎，一个是发明MSER的Matas，一个是Mikolajczyk。他还创立了一个公司TLD Vision s.r.o. 这里给出了他的系列文章，最后一篇是刚出来的PAMI。\n[2009] Online learning of robust object detectors during unstable tracking\n[2010 CVPR] P-N Learning Bootstrapping Binary Classifiers by Structural Constraints\n[2010 ICIP] FACE-TLD TRACKING-LEARNING-DETECTION APPLIED TO FACES\n[2012 PAMI] Tracking-Learning-Detection\n37. Video Surveillance\n前两篇是两个很有名的视频监控系统，里面包含了很丰富的信息量，比如CMU的那个系统里面的背景建模算法也是相当简单有效的。最后一篇是比较近的综述。\n[2000 CMU TR] A System for Video Surveillance and Monitoring\n[2000 PAMI] W4-- real-time surveillance of people and their activities\n[2008 MVA] The evolution of video surveillance an overview\n38. Viola-Jones\nHaar+Adaboost的弱弱联手，组成了最强大的利器。在OpenCV里面有它的实现，也可以选择用LBP来代替Haar特征。\n[2001 CVPR] Rapid object detection using a boosted cascade of simple features\n[2004 IJCV] Robust Real-time Face Detection\n六、 结束语\n历时一个多月，终于用业余时间把这些资料整理出来了，总算了却了一块心病，也不至于再看着一堆资料发愁了。以后可能会有些小修小补，但不会有太大的变化了。万里长征走完了第一步，剩下的就是理解和消化了。借新浪ishare共享出来，希望能够对你的科研也有一定的帮助。最后简单统计一下各个年份出现的频率。\n文章总数：372\n2012年： 10\n2011年： 20\n2010年： 20\n2009年： 14\n2008年： 18\n2007年： 13\n2006年： 14\n2005年： 9\n2004年： 24\n2003年： 22\n2002年： 21\n2001年： 21\n2000年： 23\n1999年： 10\n1998年： 22\n1997年： 8\n1996年： 9\n1995年： 9\n1994年： 7\n1993年： 5\n1992年： 11\n1991年： 5\n1990年： 6\n1980-1989： 22\n1960-1979： 9"}
{"content2":"计算机视觉库包括FastCV、OpenCV、JavaCV等。\n一些关于机器视觉的概念(转)- https://blog.csdn.net/zx3517288/article/details/51603958\n计算机视觉牛人博客和代码汇总- https://blog.csdn.net/han____shuai/article/details/50762921\n> javaCV+javaCPP+FFmpeg的使用？？\njavacv的项目最早是为java平台封装了机器视觉领域的开源库，后提供Android支持。其中的库包含OpenCV, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, and ARToolKitPlus等等。在命名空间com.googlecode.javacv.cpp下包含了所有的类对象。并暴露出了他们完整的API:opencv_legacy, opencv_ml, opencv_contrib, avutil, avcodec, avformat, avdevice, avfilter, postproc, swscale, dc1394, PGRFlyCapture, freenect, videoInputLib, and ARToolKitPlus.工具类的使用使得我们可以非常容易的使用他的功能。\nJavaCPP 简介- https://www.ibm.com/developerworks/cn/java/j-lo-cpp/\nJavaCPP 是一个开源库，它提供了在 Java 中高效访问本地 C++的方法。采用 JNI 技术实现，所以支持所有 Java 实现包括 Android 系统，Avian 和 RoboVM。采用 JavaCPP 方式在编程上较 JNI 方式简单很多，另外，效率也比 JNI 高，所以建议多采用 JavaCPP 技术。当然，如果是开源项目，也可以通过 JavaCPP presets 子项目来分享自己做的库文件，让其他人快速使用。\n> Android RenderScript，计算机视觉\n计算机视觉是什么？计算机视觉研究如何让计算机从图像和视频中获取高级和抽象信息。从工程角度来讲，计算机视觉可以使模仿视觉任务自动化。\n朋友圈爆款背后的计算机视觉技术与应用- https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/80562076\nRenderScript是Android系统中能高效处理大量计算任务的框架，特别适用一些需要处理图片和加载图片以及计算机视觉的方面应用。\nOpenCV的英文全称是Open Source Computer Vision Library。它是一个开源的计算机视觉库，它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 实现颜色直方图。颜色直方图是在许多图像检索系统中被广泛采用的颜色特征。它所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置，即无法描述图像中的对象或物体。颜色直方图特别适于描述那些难以进行自动分割的图像。OpenCV 可以使用光流法检测物体运动。使用OpenCV可以对图像的轮廓进行检测。\nOpenCV的应用领域：(1)人机互动；(2)物体识别；(3)图象分割；(4)人脸识别；(5)动作识别；(6)运动跟踪；(7)机器人；(8)运动分析；(9)机器视觉；(10)结构分析\n基于内容的图像检索，即CBIR(Content-based image retrieval)，是计算机视觉领域中关注大规模数字图像内容检索的研究分支。典型的CBIR系统，允许用户输入一张图片，以查找具有相同或相似内容的其他图片。而传统的图像检索是基于文本的，即通过图片的名称、文字信息和索引关系来实现查询功能。"}
{"content2":"计算机视觉是人工智能技术的一个重要领域，打个比方（不一定恰当），我认为计算机视觉是人工智能时代的眼睛，可见其重要程度。计算机视觉其实是一个很宏大的概念，下图是有人总结的计算机视觉所需要的技能树。\n如果你是一个对计算机视觉一无所知的小白，千万不要被这棵技能树吓到。没有哪个人能够同时掌握以上所有的技能，这棵树只是让你对计算机视觉有个粗浅的认识。\n以下是我站在一个小白的视角给出一个入门计算机视觉的相对轻松的姿势。\n一、宏观认识\n小白通常看到这么多的细分方向大脑一片茫然，到底是学习人脸识别、物体跟踪，又或者是计算摄影，三维重建呢？不知道该怎么下手。其实这些细分方向有很多共通的知识，我的建议是心急吃不了热豆腐，只有对计算机视觉这个领域有了一个初步的全面了解，你才能够结合实际问题找到自己感兴趣的研究方向，而兴趣能够支持一个自学的小白克服困难持续走下去。\n1、入门书籍\n既然说是入门，这里就不推荐类似《 Multiple View Geometry in Computer Vision》这种虽然经典但是小白看了容易放弃的书了。\n像素级的图像处理知识是计算机视觉的底层基础知识。不管你以后从事计算机视觉的哪个细分领域，这些基础知识都是必须要了解的。即使一个急切入门的小白，这一关也必须走的踏实。看到网上有人说直接从某个项目开始，边做边学，这样学的快。对此我表示部分赞成，原因是他忽略了基础知识的重要性，脑子里没有基本的术语概念知识打底，很多问题他根本不知道如何恰当的表达，遇到问题也没有思路，不知道如何搜索，这会严重拖慢进度，也无法做较深入的研究，欲速则不达。\n入门图像处理的基础知识也不是直接去啃死书，否则几个公式和术语可能就会把小白打翻在地。这里推荐两条途径，都是从实践出发并与理论结合：一个是OpenCV，一个是MATLAB。\nOpenCV以C++为基础，需要具备一定的编程基础，可移植性强，运行速度比较快，比较适合实际的工程项目，在公司里用的较多；MATLAB只需要非常简单的编程基础就可以很快上手，实现方便，代码比较简洁，可参考的资料非常丰富，方便快速尝试某个算法效果，适合做学术研究。当然两者搭配起来用更好啦。下面分别介绍一下。\n用MATLAB学习图像处理\n推荐使用冈萨雷斯的《数字图像处理（MATLAB版）》（英文原版2001年出版，中译版2005年）。不需要一上来就全部过一遍，只需要结合MATLAB学习一下基本原理、图像变换、形态学处理、图像分割，以上章节强烈建议按照书上手动敲一遍代码（和看一遍的效果完全不同），其他章节可快速扫描一遍即可。但这本书比较注重实践，对理论的解释不多，理论部分不明白的可以在配套的冈萨雷斯的《数字图像处理（第二版）》这本书里查找，这本书主要是作为工具书使用，以后遇到相关术语知道去哪里查就好。\n用OpenCV学习图像处理\nOpenCV（Open Source Computer Vision Library）是一个开源跨平台计算机视觉程序库，主要有C++预研编写，包含了500多个用于图像/视频处理和计算机视觉的通用算法。\n学习OpenCV参考《学习OpenCV》或者《OpenCV 2 计算机视觉编程手册》都可以。这两本都是偏实践的书，理论知识较少，按照书上的步骤敲代码，可以快速了解到OpenCV的强大，想要实现某个功能，只要学会查函数（在https://www.docs.opencv.org/查询对应版本），调函数就可以轻松搞定。由于每个例子都有非常直观的可视化图像输出，所以学起来比较轻松有趣。\n2、进阶书籍\n经过前面对图像处理的基本学习，小白已经了解了图像处理的基础知识，并且会使用OpenCV或MATLAB来实现某个简单的功能。但是这些知识太单薄了，并且比较陈旧，计算机视觉领域还有大量的新知识在等你。\n同样给你两种选择，当然两个都选更佳。一本书是2010年出版的美国华盛顿大学Richard Szeliski写的《Computer Vision: Algorithms and Application》；一本是2012年出版的，加拿大多伦多大学Simon J.D. Prince写的《Computer Vision: Models, Learning, and Inference》。两本书侧重点不同，前者侧重视觉和几何知识，后者侧重机器学习模型。当然两本书也有互相交叉的部分。虽然都有中文版，但是如果有一定的英语阅读基础，推荐看英文原版（见文末获取方式）。老外写的书，图和示例还是挺丰富的，比较利于 理解。\n《Computer Vision: Algorithms and Application》\n这本书图文并茂地介绍了计算机视觉这门学科的诸多大方向，有了前面《数字图像处理》的基础，这本书里有些内容你已经熟悉了，没有那么强的畏惧感。相对前面的图像处理基础本书增加了许多新的内容，比如特征检测匹配、运动恢复结构、稠密运动估计、图像拼接、计算摄影、立体匹配、三维重建等，这些都是目前比较火非常实用的方向。如果有时间可以全书浏览，如果时间不够，你可以根据兴趣，选择性的看一些感兴趣的方向。这本书的中文版翻译的不太好，可以结合英文原版看。\n《Computer Vision: Models, Learning, and Inference》\n该书从基础的概率模型讲起，涵盖了计算机视觉领域常用的概率模型、回归分类模型、图模型、优化方法等，以及偏底层的图像处理、多视角几何知识，图文并茂，并辅以非常多的例子和应用，非常适合入门。在其主页：\nhttp://www.computervisionmodels.com/\n上可以免费下载电子书。此外还有非常丰富的学习资源，包括给教师用的PPT、每章节对应的开源项目、代码、数据集链接等，非常有用。\n二、深入实践\n当你对计算机视觉领域有了比较宏观的了解，下一步就是选一个感兴趣的具体的领域去深耕。这个时期就是具体编程实践环节啦，实践过程中有疑问，根据相关术语去书里查找，结合Google，基本能够解决你大部分问题。\n那么具体选择什么方向呢？\n如果你实验室或者公司有实际的项目，最好选择当前项目方向深耕下去。如果没有具体方向，那么继续往下看。\n我个人认为计算机视觉可以分为两大方向：基于学习的方法和基于几何的方法。其中基于学习的方法最火的就是深度学习，而基于几何方法最火的就是视觉SLAM。下面就这两个方向给出一个相对轻松的入门姿势。\n1、深度学习\n深度学习（Deep Learning）的概念是Hinton等人于2006年提出的，最早最成功的应用领域就是计算机视觉，经典的卷积神经网络就是为专门处理图片数据而生。目前深度学习已经广泛应用在计算机视觉、语音识别、自然语言处理、智能推荐等领域。\n学习深度学习需要一定的数学基础，包括微积分、线性代数，很多小白一听到这些课程就想起了大学时的噩梦，其实只用了非常基础的概念，完全不用担心。不过如果一上来就啃书本，可能会有强烈的畏难情绪，很容易早早的放弃。\nAndrew Ng (吴恩达)的深度学习视频课程我觉得是一个非常好的入门资料。首先他本人就是斯坦福大学的教授，所以很了解学生，可以很清晰形象、深入浅出的从最基本的导数开始讲起，真的非常难得。\n该课程可以在网易云课程上免费观看，有中文字幕，但没有配套习题。也可以在吴恩达自己创办的在线教育平台Coursera上学习，有配套习题，限时免费，结业通过后有相应证书。\n该课程非常火爆，不用担心听不懂，网上有数不清的学习笔记可以参考。简直小白入门必备佳肴。\n2、视觉SLAM\nSLAM（Simultaneous Localization and Mapping）（详见《SLAM初识》），中文译作同时定位与地图创建。视觉SLAM就是用摄像头作为主传感器，用拍摄的视频流作为输入来实现SLAM。视觉SLAM广泛应用于VR/AR、自动驾驶、智能机器人、无人机等前沿领域。\n视觉SLAM最好的入门资料是高翔（清华博士，慕尼黑理工博后）的《视觉SLAM十四讲-从理论到实践》。该书每章节都涵盖了基础理论和代码示例，深入浅出，非常注重理论与实践结合，大大降低了小白的学习门槛。\n好了，入门介绍到此为止，你可以开始你的计算机视觉学习之旅了！\n温馨提示：本文提到的部分书籍资料，公众号：“计算机视觉life” 已经为你准备好了，公众号下方回复“入门”即可获取。"}
{"content2":"杨志宏   视觉求索公众号编辑\n朱松纯   加州大学洛杉矶分校UCLA统计学和计算机科学教授（Song-Chun Zhu；www.stat.ucla.edu/~sczhu）\n时  间    2016年10月\n\n杨: 朱教授，你在计算机视觉领域耕耘20余年，获得很多奖项，是很资深的研究人员。近年来你又涉足认知科学、机器人和人工智能。受《视觉求索公众号》编辑部委托，我想与你探讨一下计算机视觉的起源，这个学科是什么时候创建的，有哪些创始和代表人物。兼谈一下目前热门的人工智能。\n朱: 好，我们首先谈一下为什么需要讨论这个问题。然后，再来探讨一下计算机视觉的三个重要人物David Marr，King-Sun Fu，Ulf Grenander以及他们的学术思想。我认为他们是这个领域的主要创始人，或者叫有重要贡献的奠基人物。\n第一节： 为什么要追溯计算机视觉的源头， 这有什么现实意义?\n中国有句很有名的话：“一个民族如果忘记了历史，她也注定将失去未来。”我认为这句话对一个学科来讲，同样发人深省。我们先来看看现实的状况吧。\n首先，假设你当前是一个刚刚进入计算机视觉领域的研究生，很快你会有一种错觉，觉得这个领域好像就是5年前诞生的。跟踪最新发表的视觉的论文，很少有文章能够引用到5年之前的文献，大部分文献只是2-3年前的，甚至是1年之内的。现在的信息交换比较快，大家都在比一些 Benchmarks，把结果挂到arXiv 网上发布。很少有一些认真的讨论追溯到10年前、20年前或30年前的一些论文，提及当时的一些思想和框架性的东西。现在大家都用同样的方法，只是比拼，你昨天是18.3%的记录（错误率），我今天搞到17.9%了。大家都相当短视，比如研究生毕业以后变成了博士，可能也会带学生做研究，而他如果只知道这几年的历史和流行的方法的话，怎么可能去传承这个学科，让其长期健康发展呢？特别是等当前这一波方法退潮之后，这批人就慢慢失去了根基和源创力。这是一个客观的现象。\n其次，还有一个现象是，随着视觉与机器学习结合，再混合到人工智能的这么一个社会关注度很高的领域去以后，目前各种工业界，资本、投资界都往这里面来炒作。所以，你可以在互联网上看到各种推送的文字，什么这个大师，那个什么牛人、达人说得有声有色，一大堆封号。中国是有出“大师”的肥沃的土壤的，特别是在这个万众创新、浮躁的年代。这些文字在混淆公众的视听。也有的是一些中国的研究人员、研究生，半懂不懂，写出来一些，某某梳理机器学习、神经网络和人工智能的历史大事。说得神乎其神。我的大学同学把这种帖子转发给我，让我担忧。\n杨：这大多是以学术的名义写的软文，看起来像学术文章，实际上就是带广告性质的，一般都是说创投、创业公司里的人，带着资本的目的，带商业推广性质的。\n朱: 我甚至不排除有些教授，比如与硅谷结合很紧密的、在IT公司或者风投公司兼职的，有意识地参与、引领这种炒作。\n这对我们的年轻学生其实是很致命的，因为他们不了解这背后的动机， 缺乏免疫力。而且现在年轻人和公众都依赖短平快的社交媒体，很少去读专业文献。当公众的思想被这些文字占领了，得出错误的社会性的共识，变成了 false common sense， 对整个社会， 甚至对学术界，都会产生长久的负面冲击。\n这就形成了新时代的皇帝的新装。我们需要对这种现象发声， 做一些严肃的探讨。所以，正本清源有着重要的现实意义。\n第二节：计算机视觉和人工智能、机器学习的关系\n杨：谈到这里，我想先问一下计算机视觉和人工智能是什么关系？还有机器学习这三个东西。\n朱：人工智能是在60年代中后期起步的。一直到80年代，翻开它的教科书，就是一些启发式搜索，研究最多的是下棋，从国际象棋一直到最近的围棋，都是比较抽象的表达。棋盘的位置是有限的、下棋的动作也是有限的，没有感知和动作执行的不确定性。所有的问题都变成一个图搜索的问题，教科书上甚至出现了一个通用图搜索算法号称可以解决任何人工智能问题。当时视觉问题还没引起大家重视。我这里有一份1966年7月的MIT AI 实验室的第100号报告（备忘录memo 100），很短，题目叫做“The Summer Vision Project”。这个备忘录的基本意思就是暑假的时候找几个学生构造一个视觉系统。他们当时可能就觉得这个问题基本上是不需要做什么研究的。所以你就一个暑假找几个人一起写个程序，就把它干掉算了。现在说起来，当然是个笑话。\n人的大脑皮层的活动，大约70%是在处理视觉相关信息。视觉就相当于人脑的大门，其它如听觉、触觉、味觉那都是带宽较窄的通道。视觉相当于八车道的高速， 其它感觉是两旁的人行道。如果不能处理视觉信息的话，整个人工智能系统是个空架子，只能做符号推理，比如下棋、定理证明，没法进入现实世界。所以你刚才问到的人工智能和计算机视觉的关系，视觉就相当于说芝麻开门。大门就在这里面，这个门打不开,就没法研究真实世界的人工智能。\n到80年代，人工智能，连带机器人研究就跌入了低谷。那时候，很多实验室都改名字了，因为拿不到经费了。客观来说，80年代，一个微型计算机的内存只有640K字节，还不到一兆（1MB一百万字节；我们现在一张图像，随便就是几个兆的大小），根本无法读入一张图像，还谈什么理解呢？等到我做博士论文的时候（1992-1996年），我导师把当时哈佛机器人实验室最好的SUN工作站给我用，也就是32兆字节。我们实验室花了25万美元构建了一个图像采集系统，因为当时没有数字照相机——可以这么说，一直到90年代中期，我们基本上不具备研究视觉这个问题的硬件条件和数据基础。只能用一些特征点的对应关系做射影几何，用一些线条做形状分析。因为图像做不了，所以80年代计算机视觉的研究，很大部分是做几何。\n杨：90年代后，就是数字照相机大量生产了。\n朱：在90年代的末期，发生了一个叫做感知器的革命，带动了大数据和机器学习的蓬勃发展。\n杨：那机器学习与计算机视觉的关系呢？\n朱：计算机视觉是一个domain， 它有很多问题要研究，就像物理学。而机器学习基本是一个方法和工具，就像数学和统计学。这个名词的兴起应该还是最近的事情，在我看来，是来自于两股人马。 一是80年代人工智能走入低谷后，迎来了人工神经网络的一个高潮， 所谓的从符号主义到连接主义的过渡。在中国80年代与气功、人体科学一起走红，但这基本是昙花一现。到了90年代初退潮之后就开始搞 NIPS这个会议，引入统计的方法来做。二是做模式识别的一些工程人员EECS背景的。 按道理来说，这个领域应该叫做统计学习 （Statistical Learning），因为它的方法都是由概率统计领域拿来的。这些人中的领军人物很有商业头脑，把统计和物理的数理模型，改名叫做机器，比如**模型（model）就叫**机（machine），把一些层次模型（hierarchical model）说成是“网”（net）。这样，搞出了几个“机”和“网”之后，这个领域就有了地盘。另一方面，我那些做统计的同事们也都老实、图个清静，不与他们去争论，也大多无力去争。当然，统计学领域也有不少人参与了机器学习的浪潮。简单说，机器学习中的 “机器”就是统计模型，“学习”就是用数据来拟合模型，是由做计算机的人抢占了统计人的理论和方法，然后应用到视觉、语音语言等 domains。 我在计算机和统计两个系当教授，看得一清二楚。这个问题我以后可以专门讨论。\n这个机器学习的群体在2000年之后，加上大量数据的到来，很快就成长了，商业上取得很大的成功。机器学习和计算机视觉大概有百分之六七十是重合的。顺便说一句，2019年我们两个领域会在一起在洛杉矶开CVPR 和 ICML年会，我是CVPR19的大会主席。因为学习搞来搞去，最丰富的数据是在视觉（图像和视频）。现在这次机器学习的一些大的动作和工程上的推广工作，还是从计算机视觉这边开始的。\n杨：谢谢你讲述人工智能、计算机视觉和机器学习的关系。下面我们回到本次访谈的主题。刚才说了这个感知器革命是90年代以后，出了很多的数据要处理了。那为什么马尔（Marr）在70年代末思考的问题，在面对我们当今处理这个数据的时候还有意义？就是说马尔用了什么方法、什么思路框架，使它有生命力？\n朱：好，就回到1975-1980年这个时间段。我们今天的主题是想初步探讨一下计算机视觉的起源。我们这个领域也没有一个统一的教科书来谈这个事情。我认为视觉的起源，可以追溯到三个人，David Marr, King-Sun Fu 和Ulf Grenander。这三个人代表三个完全不同的方面，为计算机视觉这个领域奠定了基础。\n杨：好， 我们逐个来介绍吧。\n\n第三节：视觉的开创者之一：David Marr 的学术思想\n朱： David Marr 【1945-1980】，中文音译为马尔， 他奠定了Computational Vision计算视觉这个领域，这其实包含两个领域： 一个是计算机视觉（Computer Vision），一个是计算神经学（Computational Neuroscience）。他的工作对认知科学（Cognitive Science）也产生了很深远的影响。\n我们计算机视觉CV，第一届国际会议ICCV始于1987年，就以David Marr的名字来命名最佳论文奖，而且一直到2007年之前的20年间，是CV唯一的奖项和最高的荣誉，两年一次。认知科学年会 （CogSci）也有一个 Marr Prize给最佳的学生论文。这三个领域在80-90年代走得很近， 最近十多年交叉越来越少了。就是说，原来都是亲戚，表兄弟，现在很少有人在之间走动了。\nMarr1972年从剑桥大学毕业，博士论文是从理论的角度研究大脑功能，具体来说，是研究的小脑，主管运动的Cerebellum。1973年受MIT 人工智能实验室主任Minsky的邀请，开始是做访问学者（博士后）。1977年转为教职，可是1978年冬诊断得了急性白血病。1980年转为正教授不久就去世了， 时年35岁。他在得知来日无多后就赶紧整理了一本书，就叫 “Vision：A Computational Investigation into the HumanRepresentation and Processing of Visual Information”, 《视觉：从计算的视角研究人的视觉信息表达与处理》。他去世后由学生和同事修订，1982年出版。\n杨：“Vision”2010年再版了，再版了以后在亚马逊仍然是卖得很好。\n朱：它是个经典的东西。我是1989年冬天本科三年级从中科大认知科学实验室的老师那里读到这本书的中文译本。因为缺乏背景知识，我当时基本读不懂。因为是中文，每句话都明白，但是一段话就不知道是什么意思了。在过去的20多年中， 我每隔1-2年都会再翻一翻这本书。后来我和同事花了大约8年时间，将他的一些思路转化成数理模型，比如primal sketch。\n杨：这个人生故事是可以拍电影的。\n朱：的确。 很多年前我与他的大弟子Shimon Ullman饭桌上谈到这段历史，他说当时大家到处找药，就是救不过来。当年这是一个30多岁正值科学顶峰的、交叉学科的领军人物。顺便说一句， 当年中日友好，1984播放日本电视剧《血疑》， 那是万人空巷， 感人至深。里面的大岛幸子（山口百惠饰）得的就是同样的病。\n可惜， 目前计算机视觉这个领域，你如果去问学生，他们很多人都没听说过David Marr。“喔，想起来了，好像有个Marr奖吧。”可是你去问认知科学、神经科学的人，他们基本上对Marr非常清楚。这也是我所担心的：计算机视觉的发展太工程化、功利化了，逐步脱离了科学的范畴。这是短视和危险的。最近又受到机器学习的冲击。\n我这里顺便说一下Marr对我的另一个间接的影响。他1973年来到MIT， 就租住在JayantShah的房子里，Shah 与Minsky很熟，他当时是研究代数几何（Algebraic geometry）的。而我导师Mumford也是研究代数几何的，并获得1974年的菲尔兹奖。他们两人很熟，后来在Shah的影响下，Mumford转入计算机视觉， 他们从提取物体边缘开始 （boundary detection），也就是产生了著名的 Mumford-Shah 模型，搞图像处理的应用数学人员基本都是从这个模型开始做。这是后话。关于这段历史，我们以后可以展开谈。\n杨：好， 那么 Marr的学术贡献是什么呢？\n朱：在我看来，David Marr对我们这个学科最主要的贡献有三条，从而基本上可以说定义了这个学科的格局。\n第一条，在60年代开始之初，已经有很多人研究视觉神经生理学、心理学问题，也有人做一些边缘检测的工作。但关于视觉到底要解决哪些问题、是怎么实现的，大家莫衷一是，谈不清楚，David Marr的第一个贡献就是分出了三个层次。\n他说， 要解决这个问题，可以把它分成计算（其实应该说成是表达）、算法和实现这三个层次。首先，在表达的层次，我们问一下这是个什么问题，如何把它写成一个数学问题，任务是什么，输出是什么？这是独立于解决问题的方法的。其次，对这个数学问题去求解时可以选择不同的算法，可以并行或者串行。再次，一个算法如何在硬件上实现，可以用CPU，DSP，或者神经网络来实现。很多观察到的心理学和神经科学的现象都是跟系统硬件有关的东西，比如说人的一些注意机制、记忆力。这些应该从表达层面剔除。这样，视觉就可以从纯粹的理论、计算的角度来研究了。我们可以参考心理学和神经科学的结论，但这不是主要的。——打个比方，要造飞机可以参考鸟类的结构，但关键还是建立空气动力学才能从根本上解释这个现象，并创造各种飞行器，走得更远。\n杨：他这么一说，今天看来好像很自然地就可以理解了，但是在当时，可能没有多少人是把问题这样分解的。\n朱：当时分不开。因为当时站在像神经科学和认知科学角度，是拿一些实验现象来说事，但不知道这个现象是在哪一层出现的。\n比如神经网络和目前的深度神经网络的学习，他们的模型（表达）、算法和实现的结构这三层是混在一起的，就变成一个特用的计算设备，算法就是由这个结构来实现的。当它性能不好的时候，到底是因为表达不对，还是算法不对，还是实现不对，这就不好分析了，目前的神经网络，或者是机器学习、深度学习，它的本源存在这个问题。\n以前我们审稿的时候，会追问论文贡献是提出了一个新的模型还是一个新的算法、在哪一个层级上你有贡献，这必须说得清清楚楚。2012年，我担任国际计算机视觉和模式识别年会（CVPR）的大会主席， 就发生一个事件，收到神经网络和机器学习学派的一个领军人物 LeCun的抱怨信，原来他的论文报告了很好的实验结果，但审稿的三个人都认为论文说不清楚到底为什么有这个结果，就拒稿。他一气之下就说再也不给CVPR投稿了，把审稿意见挂在网上以示抗议。2012年是个转折点。\n现在呢？随着深度学习的红火，这三层就又混在一块去了。一般论文直接就报告结果，一堆表格、曲线图。我就是这么做，然后再这么做，我在某些个数据集上提高了两个百分点，那就行了。你审稿人也别问我这个东西里面有什么贡献，哪个节点代表是什么意思，你别问，我也不知道。那算法收敛了吗、是全局收敛还是一个局部收敛，我也不知道，但我就是提高了两个百分点。\n杨：或者要用多少数据来训练材料才能够呢？\n朱：对，这个也不用管，而且说不清。反正我这个数据集就提高是吧？所以从这个角度来讲，它就很难是一个科学的方法。可以认为它就是一个工程或者是一个经验的，有点像中医。那么要往前再发展的时候，你必须要理清楚这三层的事情。\n杨：对。\n朱：他的第二个贡献是理清视觉到底要计算什么。Marr提出了一个系列的表达，从primal sketch（首要简约图），到2 ½ D sketch（深度简约图），到3D sketch。这里面还包含了纹理、立体视觉、运动分析、表面形状等等。比如说我要估计一个物体的深度和形状，我就估计它的光照和物理材料特性；还有，三维几何形状怎么去表达？他试图去建立一个完整的体系。\n现在的视觉就基本上被很多人错误地看成一个分类问题，你给我一张图像，我就说这个图像里有一只狗或没有狗，但狗在哪儿都不知道，头在哪、脚在哪，不知道。Marr的框架是有秩序的，现在的秩序在做深度学习的人眼中还不存在，或者还没忙过来。各人做各人的分类问题，比如说有人算这个动物分类，有人算这个家具的分类。各种分类以后，它们之间怎么样的关系呢？对这个图像或场景要产生一个整体的语义解释。\n第三个贡献，Marr提出了一个非常重要的概念，到现在一直还没有一个完整的解答。他说，计算视觉是一个计算的“过程”——这是什么意思？ 我们以前用贝叶斯方法（以及现在的深度网络）认为视觉就是表达成为一个后验概率，寻求一个最优解。这个解就是图像的解释，这个求解过程就会终止。可是Marr说的这个事情，它不是单纯去求一个解，而是一个连续不断的计算过程：我给你一张图像，你越看、越琢磨，你可能看到的东西会越多。\n我给你一秒钟，你可能看到某些东西。我给你一分钟，你可能有另外一种理解，这两个理解可能是不一样的。还有一个重要的概念是你的任务决定了你怎么去看这个图像，比如说我在慌忙之中做饭，那么我对这个场景只看其中很小一部分，足够来完成我的任务就行了。里面好多东西改变你根本没注意到。\n杨：好像有些魔术就利用了这一点。\n朱：就是。很多心理学实验表明，你眼睛盯着这个图片看的时候，眼睛不眨，我告诉你这个图片在改变。你盯着看，结果它改了你都没看见。在让你看这个图片的时候，把你的注意力引到某个任务需要计算的关键要素上，其它部分你就视而不见。视觉是受任务驱动的，而任务是时刻在改变之中。比方说，视觉求解不是打一个固定的靶子， 而是打一个运动目标。\n杨：这听起来是一个耳目一新的概念。\n朱：回到人工智能这个问题，视觉，它最后的用途是要给机器人用，机器人目前面临一个什么任务，来决定它要计算什么。这第三个贡献是在算法的层面。就是说我根据我们目前面临的任务，我才决定要计算什么。而且人的任务是在不断变化的，在此时此刻我任务都在变化，那么计算的过程中是没完没了地在改变。这个理念到目前——我们目前在研究这个事情——还没有完全实现。就是说，这将是人工智能和机器人视觉的一个关键。\n杨：明白。\n朱：我们现在很多人研究这个智能，比如说分类问题。他都是从谷歌的一些应用，比如搜索图片、广告投放，变成分类问题，从而忽视了更大的本质问题。如果说人工智能往前发展机器人，要从机器人的角度来用视觉的话，那么它就有很多不同的任务——我现在做饭，我在打球，我在欣赏风景，这时候我看到的东西是完全不一样的。我怎样通过这千千万万的任务，而不是简单一个分类，来驱动我的计算的过程，来找到我的需求，来支持我目前的任务，这是一个巨大的研究的方向。David Marr的思想，到今天反而变得意义非常重大，因为大家现在一窝蜂去搞深度学习，把这些基本东西给忘掉了。但这才是人工智能和机器人视觉的长远发展方向。\n我前两年给过几个报告，说研究视觉要从一个agent（执行者）的角度，带着任务进来的这么一个人或机器人，主动地去激发视觉。\n目前计算机视觉的研究还有一大部分是由视频监控的应用来驱动的，比如说我检测一些异常现象，看这个人是男还是女？那也是一种被动，就是说它只是在看而没有去做。要去做就涉及到因果关系和更多的不确定性。所以现在的研究生觉得，他整天在做机器学习，就在调参数，就在跟别人比拼百分之几的性能。一些公司的研究所就报道，他们在某某问题（数据集）上国际领先了，排名第一了。他们自己也觉得这研究没多少意思。\n那是因为他们没有接触到这些基本的问题上来。\n杨：他们可能还没有发现这个问题本身是多么有趣。\n朱：因为作为一个科学来发展的话，就是要认认真真地来做，把这个理清楚。当前的火热来源于工业界，工业界没有多少耐心资助他们的研究人员去做科学研究，大家很现实。那么，David Marr先谈这么多好不好？以后我们可能还会继续深入谈的。\n杨：好。那我们第二个人就谈一下傅京孫。\n\n第四节：视觉的开创者之二：傅京孫（King-Sun Fu）的学术思想\n朱： David Marr是从这个神经科学和脑科学这个方向来的。傅京孫【1930-1985】，他当时代表的是计算机科学，搞人工智能的人。他是一个有领导才能的人物。他和其他人于1973年组织了第一届国际模式识别会议（ICPR），并担任主席。会议后来演变成国际模式识别学会IAPR，在1976年成立，他被选为主席。他重组了另外一个IEEE学会下面的模式识别委员会，并于1974年成为其第一任主席，创办了IEEE模式分析和机器智能（PAMI）会刊，并于1978年担任第一任总编。这是目前计算机视觉和相关领域最权威的一本期刊了。很多中国学生现在不知道，这个领域的老大本来是华人。目前，国际模式识别学会IAPR设立了一个傅京孫奖，作为终身成就奖，是模式识别的最高荣誉。\n杨：可惜他1985年去世了。听说去世前他每年都在中国举办讲座，并于1978年担任台湾的中央研究院院士。\n朱：我正要说到这一点。他去世的时候55岁，在普渡大学，据说他的实验室是一个Chinatown。1978年中国打开国门，中国最早的一批中科院的计算机人员都到他那里进修，在普渡。所以他对中国计算机的发展，可以说是一个贡献非常巨大的人。我也是受到他的恩惠，从大学一二年级就开始跟着科大陈国良老师学习，他之前去普渡进修。周末我有时就到陈老师家听他讲外面的一些研究人员和工作。你想想，计算机界那时候华人在美国站住脚的可能没几个人。\n杨：对，他对中国计算机发展真的是有历史性的贡献的。我在科学院上研究生的时候，我们那些老师是说他过世太早了，要不然对中国的研究还会更好，他多活10来年就会好很多。\n朱：他1985年拿到一个很大的国家项目，好像是开宴会的时候心脏病突发了。 他要是活着，华人在这个领域的话，不止是现在这个样子。不过在他之后，稍晚一点我们有另外一个杰出华人，黄煦涛（Tom Huang）。他当时也在普渡任教，培养了大量华人研究人员。我们以后会专门介绍。\n杨：傅京孫的故事也可以拍电影。\n朱：这是我们这个领域的不幸，两个奠基人很快就走了。他们刚刚把这个地基打起来，人就没了。\n杨：那傅的主要贡献是什么呢？\n朱：傅京孫的贡献， 我也谈三点。第一个贡献应该就是对这个学科和学会的建设，以及工程师的培养上面，他起到了开创性的作用。一般公认他是模式识别的开山鼻祖，模式识别与计算机视觉分不开的。第二个贡献，是关于他的这个句法结构性的表达与计算，就是句法模式识别，Syntactic Pattern Recognition这个词其实非常深刻。他在走之前，他那时候也没有多少数据，那么他只是画一些图，图表性的东西，来表达他的概念，他从计算机这边来的，你想很自然就会用到形式语言，因为计算机里面的几个基础之一是形式语言。逻辑、形式语言，对吧？\n杨：这好像是在编译原理里面学到过，因为编译的基础是形式语言。\n朱：我们这个世界的模式， 一个最基本的组织原则是composition。一张图像就像语言、句子符合语法结构，视频中的一个事件也有语法结构。寻找一个层次化、结构化的解释是计算视觉的核心问题。从傅京孫1985年丢下这个摊子后，基本很少有人去碰。差不多18年以后，我和我第一个博士生继续做图像解译Image Parsing这个方向，于2003年得了Marr马尔奖。然后我和我导师专门于2006年写了一本小书，总结了图像的随机语法。我刚才谈到了，在做识别、做分类的时候，只是单独在分类某一个东西，怎么去把各个识别器和分类器给它整合在一起，变成一个统一的表达，就必须产生一个结构上的表达。现在机器学习界把它换了另外名字，叫做结构化的输出，其实是一个东西。他们提出一个新的名词，把原创的图像解译名称覆盖住，这事现在经常发生。所以我说机器学习领域经常到别人那里偷概念，改头换面。数学界不允许这样做的。我还是坚持把它叫做解译、语法。\n因为语法就是一些规则，其实语法并不见得是一个确定性的，它可以跟统计连在一块，它也可以跟目前的一些神经网络结合，这都没问题。它表达了一个骨架或者支柱，形成一个统一表达。\n第三个，从算法的角度来讲，有一个层次化的表达以后，意义就不一样了，比如自底向上或自顶向下的计算的过程就可以在上面体现出来，就是马尔说的计算的过程，就可以在这里面体现出来。视觉的计算过程应该是由大量的自底向上（bottom-up）和自顶向下（top-down）过程交互和同时进行的。顺便再说一句，当前的深度神经网络就是一个feedforward的自底向上的计算，缺乏自顶向下的过程。而在人脑计算中，自顶向下的计算占据很大一部分。\n杨：那就是说，这个语法结构对计算过程有了规范和表达的途路。\n朱：对，你的搜索的过程，这个计算的过程是什么？马尔提出了第二个概念，说视觉是个计算的过程，那么这个计算过程你什么时候算哪个，这是个调度的问题，就像操作系统。David Marr计算的过程是没完没了的，随着你的任务不断改变，就有一个调度的问题。所以说我现在要去做饭，或者我要欣赏风景，或者说我要去走路、开车，那么它的不同的任务产生了不同的进程。这个进程，要在层次化的表达里面的统一起来调度——从这个意义看，感知是计算一个解译图（parse graph）， 认知是对这个parse graph进一步推理扩大， 而机器人的任务规划（task planning）也是一个同样结构的parse graph， 那就更别说语言是用parse graph来表达的。所以，人工智能的一个核心表达就是随机的语法和解译图。\n杨：对。\n朱：这个是绕不掉的，不管谁来做，都要做这个事情。当然，现在有人千方百计想绕过去，重新发明一套名词，让新来的学生忘记历史，这样他们就可以变成社会公认的大师。有些教授、研究人员在学术上没什么原创贡献，却在网上、社会上成了当红明星, 学科代言人，用社会上的知名度再给学术界施压。\n总结一下，傅京孫三点主要贡献：一是学科的人才和组织基础，二是他提出这么一个语法表达方法，三是这个表达支撑了自底向上或自顶向下的计算的过程。他去世后，这个方向一直处于一种休眠状态，我的研究有一条线是跟着这个方向做。2011年马里兰大学周少华的导师有一个演讲，题目叫“语法模式识别：从傅到朱 （From Fu to Zhu）”。我们在继承他的框架往前走。\n杨：真好！那么咱们下面就谈第三个人Ulf Grenander。\n朱：这个人，知道的人非常少。\n杨：我翻看了网上资料，他是这个领域里头真正是大神，但绝对是个小众人物。\n\n第五节：视觉的开创者之三：Ulf Grenander的学术思想\n朱：Ulf Grenander 【1923-2016】是很少有人知道的。感觉有点像金庸小说《天龙八部》里的在藏经阁扫地的灰衣老僧。武功和思想都出神入化，但他基本是世外高人，不参与江湖争斗，金庸也没交代他的名字。所以江湖上的人大多没听说过他。这样也好，他自自在在活了93岁，今年刚刚去世的。国际应用数学季刊邀请我和其他人写纪念文章，正准备出版专刊呢。\n杨：对，我读他的生平，他这个人简直就是把欧洲美洲的，还有俄国的所有的精华的人物都接触过。\n朱：那是。他出身在瑞典，他的导师叫Harald Cramér。概率论一个重要的定理，还有数论的一个猜想，是用他命名的。他也跟 Bohr（波尔）、Kolmogorov（科尔莫戈罗夫）他们走得比较近。他的起点就是做概率统计、时间序列、随机过程，概率论和统计学的一些重要应用，就是那个时候发力了。\n杨：从保险业开始了，北欧那边因为航海，保险业非常发达，所以这也有点道理。\n朱：关于概率和统计学对于科学、视觉以及人工智能的重要意义， Mumford 1999年写了一篇论文，是在一个大会的发言，叫做《随机性时代的曙光》（Dawning of the Age of Stochasticity）。\n杨：对，那是你们老师写的， 网上能找到。\n朱：他总结说，过去两千多年西方科学的发展建立在亚里士多德以来的数理逻辑基础之上。但后面一千年包括人工智能、人的思维这些东西是随机性过程。人的思维应该是建立在概率推理基础之上。其实我们看到现在的机器学习、人工智能完全就是从这个方向走了。\n杨：你的导师说，整个世界的数学可以用概率的这套思想重新写一遍，就像罗素和怀特海的写这个数学原理似的，可以把数学重新建立起来，用概率的这种思想。\n朱：这个工作已经有人做了。E. T. Jaynes就是发明最大熵原理的那个人，他写了一本很厚的书，《Probability Theory: The Logic of Science》， 他就是用这个原理去写。这也是一篇遗作。他没写完就过世了。这也是以后可以谈的话题。\nUlf Grenander就诞生在这么一个概率发源的中心的地带，跟几个大师学习，博士毕业后出来游历，做概率论随机过程的这些东西。到六、七十年代的时候，他就开始提出来，想用数学来把这个模式识别与智能的现象的问题定义清楚。我们前面谈到的David Marr 是从神经科学、认知科学来的。傅京孫是一个计算机科学与工程的人。这两者基本没有多少严格的数学定义，提出的框架是漂浮的。Ulf是从数学的角度，奠定基础。他提出来一个应用数学的分支， 叫做 Pattern Theory。他的出发点完全不同， 就是要给世界上的各种模式、现象，建立一个数学的框架来研究，格局就很宏伟，而不是急于去解决某种实际问题，后者叫做模式识别 （pattern recognition）。 他在90岁高龄出版了最后一本书，想用数学来研究人的思想是从哪里来的——你看我们脑袋里的念头、主意也往往是随机产生，像冒泡一样，所谓思如泉涌，到底怎么来的？\n杨：那太了不起了。这个事说起来，我想到当时我的老师让我读Geman and Geman 1984年的吉布斯采样算法，那就已经了不起了。\n朱：Grenander最后落脚在布朗大学应用数学系，Geman是他当年（70年代末80年代初）招到组里的年轻教员之一。这个吉布斯采样（Gibbs Sampler）的算法是一个里程碑的东西，在80年代初引起轰动。但那只是这个学派的诸多贡献的一个片段。\nGrenander的理论解释起来的确有点费劲，既然谈历史，我先从我个人的经历谈一下。\n他1994年出了一部总结性的书，900多页，叫做《General Pattern Theory》，广义模式理论。有点爱因斯坦做广义相对论的意思。但这本书很抽象，没多少人读。我1995年在哈佛研究纹理模型（texture models），因为我用的学习算法就是吉布斯采样，在训练的时候跑一遍要等两个星期才收敛，机器被占了，我就有时间，也是耐着性子把这本书读完了。我估计世界上不超过20人，能有耐心完整地读他的书。然后，我1996年1月答辩论文，我导师和我每周开车去布朗大学参加讨论。波士顿的冬天很冷，哈佛到布朗1个小时左右，漫天大雪，我们有时在高速上车被陷住，下来铲雪。到了6月，我导师从哈佛提前退休，带我一起加入布朗的应用数学系。那在当时是一个学术思想的中心。组会里有Grenander、Mumford，、Geman， 还有其他20来人，一坐就是2个多小时。这些人都明察秋毫，做报告的人无法含混过关， 一步一步都必须理清楚，说不清楚你就下去想，下次再来。\n我一直认为计算机视觉和模式识别领域亏欠Grenander, 因为统计建模和随机计算逐渐成为我们领域的核心理论基础，而大家并不知道，很多思想、算法都源于这个人或者他的学派。所以，2012年，我主持CVPR（国际计算机视觉和模式识别）大会，特意放到布朗大学附近召开，我和另外两个主席一说，大家立即就同意了。并特制了一个银质的大奖章，在大会上颁给他，表达我们的敬意。这里发生很多故事，我们以后再谈吧。\n杨：那你能简短总结一下Grenander对计算机视觉、甚至人工智能的主要贡献吗。\n朱：还是谈三点主要的吧。 首先，他提出了一个思想，叫做 analysis-by-synthesis，这是所谓产生式建模的核心理念。当你要去识别、分析一个模式，比如一个动物、人脸、 一个事件， 你首先要建立一个数理模型，这个模型通过数据来拟合，也就是当前的机器学习。那么，判断这个模型好坏，或者模型是否充分的一个依据是什么？产生式建模的方法就是对这个模型随机抽样，也就是合成（synthesis）。 我把这个过程直观叫做“计算机之梦”。计算机模型一开始初始化为空（完全随机），那它做的梦就是白噪声，或者一张白纸。通俗来说， 这个模型就是一个“白痴”。人脑有这个功能，我们把眼睛一闭，没有外界输入了，就能做梦，白日梦就是想象力的体现。一个好的模型采样产生的图片（模式），与真实观察的图片（模式），就应该是真假难辨。如果你能分辨，那说明这个模型不到位。现在很多机器学习的方法是没法去随机合成图片的。举个例子来说，我要检验你是不是真的听懂和理解中文，就看你能不能说流利的中文。如果你说话语法有错、词汇量不够，或者有口音，那就揭示你在哪方面还需要提高。\n杨：这个要求好像比光是听懂要更严格。\n朱：的确。我们当年考英语，多半是读，说和写都不行。我们考TOEFL、GRE Verbal的时候，就算没搞懂，也能蒙个60%-70%。新东方的题海战术也很奏效。当你做了大量考题，就算不懂，也能考好。当前大数据、机器学习就用题海战术。 这个方法强调在实战中检验，考什么就拼命复习什么，不考的东西就不学，这也很有道理，很直接，来得快。但是，因为你的模型没有真正理解，没有“真懂”，考试大纲外面的东西更不懂，那么后遗症就是，遇到新考题缺乏泛化能力，遇到新问题缺乏创造力。\n想一想，如果我的学生一步步考试都是靠题海战术这么学过来的，那多可怕，要让他们去搞研究、创新，那就基本不可能。很遗憾的是，现在中国学生从幼儿园开始，就是在题海中泡大的。机器人、人工智能，靠题海战术是可以演示不少功能的，但那还离真正的智能比较遥远。\n杨：好， 我明白这个analysis-by-synthesis 的意义了。他的第二贡献呢？\n朱：他提出了一整套建模的理论和方法。把代数、几何、概率整合起来。代数指的是一些结构，比如群论，记得在科大本科学过群、环、域这些概念吧？也就是说我有一些基本元素，叫 generator，连接成为图graph，然后是群group，在上面进行操作, 产生了各种各样的变化。还有很多几何，变换，在连续情况就产生形变。通过组合，语法，产生丰富的图模式。然后，再在这个图模式的空间上定义距离（测度）和概率。\n比如一个概率模型，是定义在一个什么样的结构上，它是个什么样的解空间？这个数理上你必须交代清楚，否则你的论文写不下去了。现在它的一个很大的应用在医疗图像上面，比如说一个病人，他的肝变形了，那么他的肝的形状和正常人的肝的形状之间怎么定义一个合理的距离？两张人脸，怎么定义这个距离的呢？这个距离定义在一个流型上，数学的流型（manifold）。\n杨：这些东西真用上了吗？\n朱：他有个Postdoc，名叫Michael Miller， 现在是Johns Hopkins 大学图像中心主任， 就用这一套方法来做医疗图像、脑科学（Brain Mapping）等方面的应用。\n杨：他的第三方面的贡献呢？\n朱：第三个方面主要是算法上面。当我们去做求解的时候，在一个解空间，这个求解空间肯定是一个非凸的，有千千万万的局部最优解local minimum 在里面。\n杨：对。这是当时八十年代的时候提出来一个很尖锐的问题，好像有什么模拟煺火方法。\n朱：很多蒙特卡洛算法都是他和这个学派的人提出来的。这个解空间是一个异构空间，空间里面非常复杂的，包含有很多子空间，子空间里面又包含又子空间，每个子空间维度又不一样，在它们之间，从一个解跳到另外一个解的时候，这跳转必须是可逆的。在计算机里面就叫可以回溯。从这个学派走出来的人，他们设计算法每一个步骤都是有章法的，要做到合规合矩。包括上面提到的吉布斯采样算法、可逆蒙特卡洛跳转法，还有变分法（variational methods）和偏微分方程式， 还有一些随机下降法（stochastic gradient）， 这后者是目前训练深度学习模型的主要办法。他也开创了非参数模型的学习方法。这里面东西太多，先谈到这里吧。\n正因为很多人没有接触过Grenander的理论，缺乏这方面的理论素养，造成我们学科发展的一个巨大的问题：很多教授、博士、研究生就是用别人的模型（机），拿来调试，基本缺乏自己发明新模型、新算法的能力。我们这个领域，很多美国名牌大学助理教授、副教授、教授，他们的论文中的公式错误百出。现在干脆大家在论文中都不写公式了，直接报告最后的实验结果，提高了几个百分点。这就“一俊掩百丑”了。 英文有个类似的说法叫做 “sweep the dirt under the carpet”，把污垢扫到地毯下。 这些人在大量培养博士、他们出来的人评审论文。 这样一来，学科的发展堪忧！\n\n第六节：结束语\n杨：听了你番谈话，我明白很多。记得我当时念研究生，包括念博士生的时候，实际上是很糊涂的。就是对这个领域到底做多少东西，没有信心。觉得很多研究像画鬼一样，原理不清楚。我觉得那样的话，与其那样做事情, 那不如干脆到工业界那更快乐。\n朱：正因为我们这个领域很多历史、框架性的东西，没有搞清楚，培养出来的博士就缺乏分析能力。大家被一些工程的任务和数据驱动，被一些性能的指标牵制，对科学的发展比较迷茫。\n杨：好，谈了很多，我们做个总结吧。\n朱：那我就说两点。\n首先，我在开场白中提到 “一个民族如果忘记了历史, 她也注定将失去未来”。一个学科要健康发展，需要研究人员、研究生们理解自己领域的历史和大的发展方向，建立文化的认同。否则，自己家的东西被别人偷取而浑然不知。就像日本打入中国，想把我们的地名改掉，大家开始说日语，把名字都改做山本太郎之类，感觉很酷吗？  或者是韩国人把中国的文化拿去申报世界文化遗产，这都是要制止的。否则，过了一代人，还真说不清楚了。我记得刚来美国的时候，美国同事把汉字叫做“Kang-ji”，说是日本字。  我们领域很多人对保护这个领域的文化和传统缺乏清醒认识。皮之不存，毛将焉附？\n其次，一个学科内部，大家互相不够了解，各自为政。特别现在会议审稿人很多是研究生，以自己的狭窄的眼光和标准去评判别人的方法，造成很多混乱。搞工程的看不到理论的重要性，反之亦然。大家又都疏远心理学和认知科学的研究。我提倡我们的研究人员、学生要提高理论修养、培养长远眼光，向相关学科取经，取长补短。\n我希望这个微信公众号，能够帮助大家正视问题，让计算机视觉这个领域健康、稳健、可持续地发展。"}
{"content2":"前言，AI知识图谱\n人工智能分为强人工智能和弱人工智能。\n强人工智能是通过计算机来构造复杂的、拥有与人类智慧同样本质特性的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考，也就是电影里面的机器人。\n弱人工智能 (ANI) 是指擅长于单个方面的人工智能。垃圾邮件的自动识别，iPhone的助手siri，Pinterest上的图像分类，Facebook的人脸识别都属于弱人工智能，也就是我们现在大多是在从事的领域。\n一，人工智能，机器学习，深度学习关系\n人工智能是追求目标，机器学习是实现手段，深度学习是其中一种方法。\n二，机器学习框架图\n三，机器学习应用领域\n,\n4，深度学习框架图：\n五，人工智能应用领域\n六：中国的人工智能发展\n人工智能企业可以在应用层、技术层、基础上进行区分。\n在应用层的中国人工智能公司按照领域划分包括：\n机器人：Geek+、 Rokid、图灵机器人、优必选。\n自动驾驶：百度、天瞳威视、地平线机器人、驭势科技。\n无人机：大疆、亿航、Hover Camera、零度智控。\n语音助手：百度、出门问问。\n商业智能：永洪科技、Data KM。\n消费者服务：AiKF。\n产业应用：碳云智能、Maxent、今日头条、学霸君。\n在技术层的中国人工智能公司按照领域划分包括：\n语音识别&自然语言处理：\n思必驰、百度、科大讯飞、出门问问、捷通华生、腾讯、三角兽、云知声。\n机器学习&深度学习：深鉴科技、中科视拓。\n人工智能平台：达闼科技、第四范式。\n计算机视觉：依图科技、格灵深瞳、旷视科技、商汤科技。\n在基础层的中国人工智能公司按照领域划分包括：\n传感器：ICE DRINK、LeiShen、SLAMTEC、北醒光子。\nAI 芯片：寒武纪科技、地平线机器人。\n七，人工智能未来企业排行榜\n**\n欢迎扫码关注我的微信公众号\nhttps://github.com/yeyujujishou19\n**"}
{"content2":"【计算机视觉】目标检测中的指标衡量Recall与Precision\n标签（空格分隔）： 【图像处理】\n说明：目标检测性能指标Recall与Precision的理解。\nRecall与Precision\n其实道理非常朴素：\nPrecision就是精度，以行人检测为例，精度就是检测出来的行人中确实是行人的所占的百分比，也就是所谓的检测精度，可以提供给客户看，我们的检测精度是100%，也就是没有虚景，没有false positive；\nRecall就是正确检出的行人数量占行人总数的百分比，Recall=100%表示没有漏检；\n所以，这两个常常是一对矛盾，客户总是需要既没有虚景也不会发生漏检的情况，也就是Precision和Recall均为100%的状况。太难了！\n通常在论文中还会有这样的曲线，Recall和Precision Score随阈值的变化曲线，以及Recall-Precision曲线。\n如果一个分类器的性能比较好，那么它应该有如下的表现：是让Recall值增长的同时保持Precision的值在一个很高的水平。而性能比较差的分类器可能会损失很多Precision值才能换来Recall值的提高。通常情况下，文章中都会使用Precision-recall曲线，来显示出分类器在Precision与Recall之间的权衡。\nAverage Precision\n相比较与曲线图，在某些时候还是一个具体的数值能更直观地表现出分类器的性能。通常情况下都是用 Average Precision来作为这一度量标准，它的公式为：\nThat is equal to taking the area under the curve\n在这一积分中，其中p代表Precision ，r代表Recall，p是一个以r为参数的函数，That is equal to taking the area under the curve.\n实际上这一积分极其接近于这一数值：对每一种阈值分别求（Precision值）乘以（Recall值的变化情况），再把所有阈值下求得的乘积值进行累加。公式如下：\n在这一公式中，N代表测试集中所有图片的个数，P(k)表示在能识别出k个图片的时候Precision的值，而 Delta r(k) 则表示识别图片个数从k-1变化到k时（通过调整阈值）Recall值的变化情况。\n在这一例子中，Approximated Average Precision的值\n=(1 * （0.2-0）) + (1 * (0.4-0.2)) + (0.66 * (0.4-0.4)) + (0.75 * (0.6-0.4)) + (0.6 * (0.6-0.6)) + (0.66 * (0.8-0.6)) + (0.57 * (0.8-0.8)) + (0.5 * (0.8-0.8)) + (0.44 * (0.8-0.8)) + (0.5 * (1-0.8)) = 0.782.\n=(1 * 0.2) + (1 * 0.2) + (0.66 * 0) + (0.75 * 0.2) + (0.6 * 0) + (0.66 * 0.2) + (0.57 * 0) + (0.5 * 0) + (0.44 * 0) + (0.5 * 0.2) = 0.782.\n通过计算可以看到，那些Recall值没有变化的地方（红色数值），对增加Average Precision值没有贡献。\nInterpolated average precision\n不同于Approximated Average Precision，一些作者选择另一种度量性能的标准：Interpolated Average Precision。这一新的算法不再使用P(k)，也就是说，不再使用当系统识别出k个图片的时候Precision的值与Recall变化值相乘。而是使用：\n也就是每次使用在所有阈值的Precision中，最大值的那个Precision值与Recall的变化值相乘。公式如下：\n下图的图片是Approximated Average Precision 与 Interpolated Average Precision相比较。\n需要注意的是，为了让特征更明显，图片中使用的参数与上面所说的例子无关。\n很明显 Approximated Average Precision与精度曲线挨的很近，而使用Interpolated Average Precision算出的Average Precision值明显要比Approximated Average Precision的方法算出的要高。\n一些很重要的文章都是用Interpolated Average Precision 作为度量方法，并且直接称算出的值为Average Precision 。PASCAL Visual Objects Challenge从2007年开始就是用这一度量制度，他们认为这一方法能有效地减少Precision-recall 曲线中的抖动。所以在比较文章中Average Precision 值的时候，最好先弄清楚它们使用的是那种度量方式。\nIoU\nIoU这一值，可以理解为系统预测出来的框与原来图片中标记的框的重合程度。\n算方法即检测结果Detection Result与 Ground Truth 的交集比上它们的并集，即为检测的准确率：\nIoU=DetectionResult⋂GroundTruthDetectionResult⋃GroundTruth\nIoU = \\frac{DetectionResult⋂GroundTruth }{DetectionResult⋃GroundTruth}\n如下图所示：\n蓝色的框是：GroundTruth\n黄色的框是：DetectionResult\n绿色的框是：DetectionResult ⋂ GroundTruth\n红色的框是：DetectionResult ⋃ GroundTruth\n2017年5月10日 19:07\n张朋艺 pyZhangBIT2010@126.com"}
{"content2":"一、国务院\n继2015年07月04日国务院发布《“互联网+”行动的指导意见》后，中国政府网持续更新了很多关于“互联网+”的政策和报道：\n1. 国务院关于积极推进“互联网+”行动的指导意见\n2. 图表：国务院印发《关于积极推进“互联网＋”\n行动的指导意见》\n3. 中国政府网：“互联网”搜索结果\n（十一）“互联网+”人工智能。\n依托互联网平台提供人工智能公共创新服务，加快人工智能核心技术突破，促进人工智能在智能家居、智能终端、智能汽车、机器人等领域的推广应用，培育若干引领全球人工智能发展的骨干企业和创新团队，形成创新活跃、开放合作、协同发展的产业生态。（发展改革委、科技部、工业和信息化部、网信办等负责）\n1.培育发展人工智能新兴产业。建设支撑超大规模深度学习的新型计算集群，构建包括语音、图像、视频、地图等数据的海量训练资源库，加强人工智能基础资源和公共服务等创新平台建设。进一步推进计算机视觉、智能语音处理、生物特征识别、自然语言理解、智能决策控制以及新型人机交互等关键技术的研发和产业化，推动人工智能在智能产品、工业制造等领域规模商用，为产业智能化升级夯实基础。\n2.推进重点领域智能产品创新。鼓励传统家居企业与互联网企业开展集成创新，不断提升家居产品的智能化水平和服务能力，创造新的消费市场空间。推动汽车企业与互联网企业设立跨界交叉的创新平台，加快智能辅助驾驶、复杂环境感知、车载智能设备等技术产品的研发与应用。支持安防企业与互联网企业开展合作，发展和推广图像精准识别等大数据分析技术，提升安防产品的智能化服务水平。\n3.提升终端产品智能化水平。着力做大高端移动智能终端产品和服务的市场规模，提高移动智能终端核心技术研发及产业化能力。鼓励企业积极开展差异化细分市场需求分析，大力丰富可穿戴设备的应用服务，提升用户体验。推动互联网技术以及智能感知、模式识别、智能分析、智能控制等智能技术在机器人领域的深入应用，大力提升机器人产品在传感、交互、控制等方面的性能和智能化水平，提高核心竞争力。\n二、国家发展和改革委员会\n此后，国家发改委在2016年相继发布了多篇关于“互联性+”发展的通知、实施意见和方案，其中有一篇专门针对人工智能：\n1. “互联网+”人工智能三年行动实施方案\n2. 国家发改委：“互联网”搜索结果\n三、人工智能专项发展规划\n时隔两年，国务院又于2017年7月8日发布了《人工智能发展规划》，对人工智能在国内的发展进一步作出了规划和指导：\n国务院关于印发新一代人工智能发展规划的通知\n四、结语\n关于人工智能的政策已经形成了一顶规模，政府计划到2020年人工智能总体技术和应用与世界先进水平同步，到2025年人工智能基础理论实现重大突破，到2030年人工智能理论、技术与应用总体达到世界领先水平。有了国家的大力支持，人工智能在中国的发展势不可挡。\n“种一棵树最好的时间是十年前，其次是现在。”现在正式进入人工智能的好时候，快上车吧，来不及解释了。"}
{"content2":"本文为《计算机视觉：算法与应用》第十四章:”识别“阅读笔记，欢迎交流学习。\n1.物体检测(Object regognition)\n2.人脸识别(Face recognition)\n3.实例识别（Instance recognition）\n4.类识别（Class recognition)\n问：为什么识别对计算机很难？\n答：现实世界是由混杂的物体组成，所有物体都会遮挡其它物体，而且会呈现不同姿态。识别很难简单的用样例数据库进行匹配。\n1.物体检测\n1.1人脸检测\n人脸检测方法分为基于特征、基于模板、基于表现的方法。\n-基于特征：尝试寻找有区分性的图像特征的位置，如眼睛、嘴巴，然后在合理的几何布局上验证这些特征是否存在。\n-基于模板：能够处理姿态和表情变化较大范围的变化，通常需要接近真正人脸的好的初始化，因此不适合快速人脸检测。\n-基于表现：扫描图像的小的有重叠的矩形区域寻找与人脸相似的候选区域，然后用一组更昂贵但具有选择性的检测算法求精。\n常用的人脸检测算法有SVM、K-means、NN、boosting等。"}
{"content2":"编者按\n目前，计算机视觉是深度学习领域最热门的研究领域之一。计算机视觉实际上是一个跨领域的交叉学科，包括计算机科学（图形、算法、理论、系统、体系结构），数学（信息检索、机器学习），工程学（机器人、语音、自然语言处理、图像处理），物理学（光学 ），生物学（神经科学）和心理学（认知科学）等等。许多科学家认为，计算机视觉为人工智能的发展开拓了道路。\n那么什么是计算机视觉呢？ 这里给出了几个比较严谨的定义：\n✦ “对图像中的客观对象构建明确而有意义的描述”（Ballard＆Brown，1982）\n✦ “从一个或多个数字图像中计算三维世界的特性”（Trucco＆Verri，1998）\n✦ “基于感知图像做出对客观对象和场景有用的决策”（Sockman＆Shapiro，2001）\n为什么要学习计算机视觉？\n一个显而易见的答案就是，这个研究领域已经衍生出了一大批快速成长的、有实际作用的应用，例如：\n人脸识别： Snapchat 和 Facebook 使用人脸检测算法来识别人脸。\n图像检索：Google Images 使用基于内容的查询来搜索相关图片，算法分析查询图像中的内容并根据最佳匹配内容返回结果。\n游戏和控制：使用立体视觉较为成功的游戏应用产品是：微软 Kinect。\n监测：用于监测可疑行为的监视摄像头遍布于各大公共场所中。\n生物识别技术：指纹、虹膜和人脸匹配仍然是生物识别领域的一些常用方法。\n智能汽车：计算机视觉仍然是检测交通标志、灯光和其他视觉特征的主要信息来源。\n视觉识别是计算机视觉的关键组成部分，如图像分类、定位和检测。神经网络和深度学习的最新进展极大地推动了这些最先进的视觉识别系统的发展。在本文中，我将分享 5 种主要的计算机视觉技术，并介绍几种基于计算机视觉技术的深度学习模型与应用。\n1、图像分类\n﻿\n给定一组各自被标记为单一类别的图像，我们对一组新的测试图像的类别进行预测，并测量预测的准确性结果，这就是图像分类问题。图像分类问题需要面临以下几个挑战☟☟☟：\n视点变化，尺度变化，类内变化，图像变形，图像遮挡，照明条件和背景杂斑\n我们怎样来编写一个图像分类算法呢？\n计算机视觉研究人员提出了一种基于数据驱动的方法。\n该算法并不是直接在代码中指定每个感兴趣的图像类别，而是为计算机每个图像类别都提供许多示例，然后设计一个学习算法，查看这些示例并学习每个类别的视觉外观。也就是说，首先积累一个带有标记图像的训练集，然后将其输入到计算机中，由计算机来处理这些数据。\n因此，可以按照下面的步骤来分解：\n输入是由 N 个图像组成的训练集，共有 K 个类别，每个图像都被标记为其中一个类别。\n然后，使用该训练集训练一个分类器，来学习每个类别的外部特征。\n最后，预测一组新图像的类标签，评估分类器的性能，我们用分类器预测的类别标签与其真实的类别标签进行比较。\n目前较为流行的图像分类架构是卷积神经网络（CNN）——将图像送入网络，然后网络对图像数据进行分类。卷积神经网络从输入“扫描仪”开始，该输入“扫描仪”也不会一次性解析所有的训练数据。比如输入一个大小为 100*100 的图像，你也不需要一个有 10,000 个节点的网络层。相反，你只需要创建一个大小为 10 *10 的扫描输入层，扫描图像的前 10*10 个像素。然后，扫描仪向右移动一个像素，再扫描下一个 10 *10 的像素，这就是滑动窗口。\n﻿\n输入数据被送入卷积层，而不是普通层。每个节点只需要处理离自己最近的邻近节点，卷积层也随着扫描的深入而趋于收缩。除了卷积层之外，通常还会有池化层。池化是过滤细节的一种方法，常见的池化技术是最大池化，它用大小为 2*2 的矩阵传递拥有最多特定属性的像素。\n现在，大部分图像分类技术都是在 ImageNet 数据集上训练的， ImageNet 数据集中包含了约 120 万张高分辨率训练图像。测试图像没有初始注释（即没有分割或标签），并且算法必须产生标签来指定图像中存在哪些对象。\n现存的很多计算机视觉算法，都是被来自牛津、 INRIA 和 XRCE 等顶级的计算机视觉团队在 ImageNet 数据集上实现的。通常来说，计算机视觉系统使用复杂的多级管道，并且，早期阶段的算法都是通过优化几个参数来手动微调的。\n第一届 ImageNet 竞赛的获奖者是 Alex Krizhevsky（NIPS 2012） ，他在 Yann LeCun 开创的神经网络类型基础上，设计了一个深度卷积神经网络。该网络架构除了一些最大池化层外，还包含 7 个隐藏层，前几层是卷积层，最后两层是全连接层。在每个隐藏层内，激活函数为线性的，要比逻辑单元的训练速度更快、性能更好。除此之外，当附近的单元有更强的活动时，它还使用竞争性标准化来压制隐藏活动，这有助于强度的变化。\n﻿\n﻿就硬件要求而言， Alex 在 2 个 Nvidia GTX 580 GPU （速度超过 1000 个快速的小内核）上实现了非常高效的卷积网络。 GPU 非常适合矩阵间的乘法且有非常高的内存带宽。这使他能在一周内完成训练，并在测试时快速的从 10 个块中组合出结果。如果我们能够以足够快的速度传输状态，就可以将网络分布在多个内核上。\n随着内核越来越便宜，数据集越来越大，大型神经网络的速度要比老式计算机视觉系统更快。在这之后，已经有很多种使用卷积神经网络作为核心，并取得优秀成果的模型，如 ZFNet（2013），GoogLeNet（2014）， VGGNet（2014）， RESNET（2015），DenseNet（2016）等。\n2、对象检测\n﻿\n识别图像中的对象这一任务，通常会涉及到为各个对象输出边界框和标签。这不同于分类/定位任务——对很多对象进行分类和定位，而不仅仅是对个主体对象进行分类和定位。在对象检测中，你只有 2 个对象分类类别，即对象边界框和非对象边界框。例如，在汽车检测中，你必须使用边界框检测所给定图像中的所有汽车。\n如果使用图像分类和定位图像这样的滑动窗口技术，我们则需要将卷积神经网络应用于图像上的很多不同物体上。由于卷积神经网络会将图像中的每个物体识别为对象或背景，因此我们需要在大量的位置和规模上使用卷积神经网络，但是这需要很大的计算量！\n为了解决这一问题，神经网络研究人员建议使用区域（region）这一概念，这样我们就会找到可能包含对象的“斑点”图像区域，这样运行速度就会大大提高。第一种模型是基于区域的卷积神经网络（ R-CNN ），其算法原理如下：\n在 R-CNN 中，首先使用选择性搜索算法扫描输入图像，寻找其中的可能对象，从而生成大约 2,000 个区域建议；\n然后，在这些区域建议上运行一个 卷积神网络；\n最后，将每个卷积神经网络的输出传给支持向量机（ SVM ），使用一个线性回归收紧对象的边界框。\n﻿\n实质上，我们将对象检测转换为一个图像分类问题。但是也存在这些问题：训练速度慢，需要大量的磁盘空间，推理速度也很慢。\nR-CNN 的第一个升级版本是 Fast R-CNN，通过使用了 2 次增强，大大提了检测速度：\n在建议区域之前进行特征提取，因此在整幅图像上只能运行一次卷积神经网络；\n用一个 softmax 层代替支持向量机，对用于预测的神经网络进行扩展，而不是创建一个新的模型。\n﻿\nFast R-CNN 的运行速度要比 R-CNN 快的多，因为在一幅图像上它只能训练一个 CNN 。 但是，择性搜索算法生成区域提议仍然要花费大量时间。\nFaster R-CNN 是基于深度学习对象检测的一个典型案例。\n该算法用一个快速神经网络代替了运算速度很慢的选择性搜索算法：通过插入区域提议网络（ RPN ），来预测来自特征的建议。 RPN 决定查看“哪里”，这样可以减少整个推理过程的计算量。\nRPN 快速且高效地扫描每一个位置，来评估在给定的区域内是否需要作进一步处理，其实现方式如下：通过输出 k 个边界框建议，每个边界框建议都有 2 个值——代表每个位置包含目标对象和不包含目标对象的概率。\n﻿一旦我们有了区域建议，就直接将它们送入 Fast R-CNN 。 并且，我们还添加了一个池化层、一些全连接层、一个 softmax 分类层以及一个边界框回归器。\n总之，Faster R-CNN 的速度和准确度更高。值得注意的是，虽然以后的模型在提高检测速度方面做了很多工作，但很少有模型能够大幅度的超越 Faster R-CNN 。换句话说， Faster R-CNN 可能不是最简单或最快速的目标检测方法，但仍然是性能最好的方法之一。\n近年来，主要的目标检测算法已经转向更快、更高效的检测系统。这种趋势在 You Only Look Once（YOLO），Single Shot MultiBox Detector（SSD）和基于区域的全卷积网络（ R-FCN ）算法中尤为明显，这三种算法转向在整个图像上共享计算。因此，这三种算法和上述的3种造价较高的R-CNN 技术有所不同。\n3、目标跟踪\n﻿目标跟踪，是指在特定场景跟踪某一个或多个特定感兴趣对象的过程。传统的应用就是视频和真实世界的交互，在检测到初始对象之后进行观察。现在，目标跟踪在无人驾驶领域也很重要，例如 Uber 和特斯拉等公司的无人驾驶。\n根据观察模型，目标跟踪算法可分成 2 类：生成算法和判别算法。\n生成算法使用生成模型来描述表观特征，并将重建误差最小化来搜索目标，如主成分分析算法（ PCA ）；\n判别算法用来区分物体和背景，其性能更稳健，并逐渐成为跟踪对象的主要手段（判别算法也称为 Tracking-by-Detection ，深度学习也属于这一范畴）。\n为了通过检测实现跟踪，我们检测所有帧的候选对象，并使用深度学习从候选对象中识别想要的对象。有两种可以使用的基本网络模型：堆叠自动编码器（ SAE ）和卷积神经网络（ CNN ）。\n目前，最流行的使用 SAE 进行目标跟踪的网络是 Deep Learning Tracker（DLT），它使用了离线预训练和在线微调。其过程如下：\n离线无监督预训练使用大规模自然图像数据集获得通用的目标对象表示，对堆叠去噪自动编码器进行预训练。堆叠去噪自动编码器在输入图像中添加噪声并重构原始图像，可以获得更强大的特征表述能力。\n将预训练网络的编码部分与分类器合并得到分类网络，然后使用从初始帧中获得的正负样本对网络进行微调，来区分当前的对象和背景。 DLT 使用粒子滤波作为意向模型（motion model），生成当前帧的候选块。 分类网络输出这些块的概率值，即分类的置信度，然后选择置信度最高的块作为对象。\n在模型更新中， DLT 使用有限阈值。\n﻿鉴于 CNN 在图像分类和目标检测方面的优势，它已成为计算机视觉和视觉跟踪的主流深度模型。 一般来说，大规模的卷积神经网络既可以作为分类器和跟踪器来训练。具有代表性的基于卷积神经网络的跟踪算法有全卷积网络跟踪器（ FCNT ）和多域卷积神经网络（ MD Net ）。\nFCNT 充分分析并利用了 VGG 模型中的特征映射，这是一种预先训练好的 ImageNet 数据集，并有如下效果：\n卷积神经网络特征映射可用于定位和跟踪。\n对于从背景中区分特定对象这一任务来说，很多卷积神经网络特征映射是噪音或不相关的。\n较高层捕获对象类别的语义概念，而较低层编码更多的具有区性的特征，来捕获类别内的变形。\n因此， FCNT 设计了特征选择网络，在 VGG 网络的卷积 4-3 和卷积 5-3 层上选择最相关的特征映射。 然后为避免噪音的过拟合， FCNT 还为这两个层的选择特征映射单独设计了两个额外的通道（即 SNet 和 GNet ）： GNet 捕获对象的类别信息； SNet 将该对象从具有相似外观的背景中区分出来。\n这两个网络的运作流程如下：都使用第一帧中给定的边界框进行初始化，以获取对象的映射。而对于新的帧，对其进行剪切并传输最后一帧中的感兴趣区域，该感兴趣区域是以目标对象为中心。最后，通过 SNet 和 GNet ，分类器得到两个预测热映射，而跟踪器根据是否存在干扰信息，来决定使用哪张热映射生成的跟踪结果。 FCNT 的图如下所示。\n与 FCNT 的思路不同， MD Net 使用视频的所有序列来跟踪对象的移动。上述网络使用不相关的图像数据来减少跟踪数据的训练需求，并且这种想法与跟踪有一些偏差。该视频中的一个类的对象可以是另一个视频中的背景，因此， MD Net 提出了“多域”这一概念，它能够在每个域中独立的区分对象和背景，而一个域表示一组包含相同类型对象的视频。\n如下图所示， MD Net 可分为两个部分，即 K 个特定目标分支层和共享层：每个分支包含一个具有 softmax 损失的二进制分类层，用于区分每个域中的对象和背景；共享层与所有域共享，以保证通用表示。\n﻿\n﻿近年来，深度学习研究人员尝试使用了不同的方法来适应视觉跟踪任务的特征，并且已经探索了很多方法：\n应用到诸如循环神经网络（ RNN ）和深度信念网络（DBN ）等其他网络模型；\n设计网络结构来适应视频处理和端到端学习，优化流程、结构和参数；\n或者将深度学习与传统的计算机视觉或其他领域的方法（如语言处理和语音识别）相结合。\n4、语义分割\n﻿\n计算机视觉的核心是分割，它将整个图像分成一个个像素组，然后对其进行标记和分类。特别地，语义分割试图在语义上理解图像中每个像素的角色（比如，识别它是汽车、摩托车还是其他的类别）。如上图所示，除了识别人、道路、汽车、树木等之外，我们还必须确定每个物体的边界。因此，与分类不同，我们需要用模型对密集的像素进行预测。\n与其他计算机视觉任务一样，卷积神经网络在分割任务上取得了巨大成功。最流行的原始方法之一是通过滑动窗口进行块分类，利用每个像素周围的图像块，对每个像素分别进行分类。但是其计算效率非常低，因为我们不能在重叠块之间重用共享特征。\n解决方案就是加州大学伯克利分校提出的全卷积网络（ FCN ），它提出了端到端的卷积神经网络体系结构，在没有任何全连接层的情况下进行密集预测。\n这种方法允许针对任何尺寸的图像生成分割映射，并且比块分类算法快得多，几乎后续所有的语义分割算法都采用了这种范式。\n﻿\n﻿但是，这也仍然存在一个问题：在原始图像分辨率上进行卷积运算非常昂贵。为了解决这个问题， FCN 在网络内部使用了下采样和上采样：下采样层被称为条纹卷积（ striped convolution ）；而上采样层被称为反卷积（ transposed convolution ）。\n尽管采用了上采样和下采样层，但由于池化期间的信息丢失， FCN 会生成比较粗糙的分割映射。SegNet 是一种比 FCN （使用最大池化和编码解码框架）更高效的内存架构。在 SegNet 解码技术中，从更高分辨率的特征映射中引入了 shortcut/skip connections ，以改善上采样和下采样后的粗糙分割映射。\n﻿\n﻿目前的语义分割研究都依赖于完全卷积网络，如空洞卷积 ( Dilated Convolutions ），DeepLab 和 RefineNet 。\n5、实例分割\n﻿除了语义分割之外，实例分割将不同类型的实例进行分类，比如用 5 种不同颜色来标记 5 辆汽车。分类任务通常来说就是识别出包含单个对象的图像是什么，但在分割实例时，我们需要执行更复杂的任务。我们会看到多个重叠物体和不同背景的复杂景象，我们不仅需要将这些不同的对象进行分类，而且还要确定对象的边界、差异和彼此之间的关系！\n到目前为止，我们已经看到了如何以多种有趣的方式使用卷积神经网络的特征，通过边界框有效定位图像中的不同对象。我们可以将这种技术进行扩展吗？也就是说，对每个对象的精确像素进行定位，而不仅仅是用边界框进行定位？ Facebook AI 则使用了 Mask R-CNN 架构对实例分割问题进行了探索。\n﻿\n﻿\n就像 Fast R-CNN 和 Faster R-CNN 一样， Mask R-CNN 的底层是鉴于 Faster R-CNN 在物体检测方面效果很好，我们是否可以将其扩展到像素级分割？\nMask R-CNN 通过向 Faster R-CNN 添加一个分支来进行像素级分割，该分支输出一个二进制掩码，该掩码表示给定像素是否为目标对象的一部分：该分支是基于卷积神经网络特征映射的全卷积网络。将给定的卷积神经网络特征映射作为输入，输出为一个矩阵，其中像素属于该对象的所有位置用 1 表示，其他位置则用 0 表示，这就是二进制掩码。\n另外，当在原始 Faster R-CNN 架构上运行且没有做任何修改时，感兴趣池化区域（ RoIPool ） 选择的特征映射区域或原始图像的区域稍微错开。由于图像分割具有像素级特性，这与边界框不同，自然会导致结果不准确。 Mas R-CNN 通过调整 RoIPool 来解决这个问题，使用感兴趣区域对齐（ Roialign ）方法使其变的更精确。本质上， RoIlign 使用双线性插值来避免舍入误差，这会导致检测和分割不准确。\n一旦生成这些掩码， Mask R-CNN 将 RoIAlign 与来自 Faster R-CNN 的分类和边界框相结合，以便进行精确的分割：\n结语\n上述这 5 种主要的计算机视觉技术可以协助计算机从单个或一系列图像中提取、分析和理解有用的信息。你还可以通过我的 GitHub 存储库（https://github.com/khanhnamle1994/computer-vision）获取所有的演讲幻灯片以及指南。\n原文链接 https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b"}
{"content2":"相关链接：【计算机视觉算法岗面经】“吐血”整理：2019秋招资料\n//2018/09/28\n当初开始面试时就想着，以后我一定要写一篇面经，现在是来还愿的时候了。\n首先，嗯，非常感谢牛客平台，提供了很多信息啊。而且去年11月曾报名左神的课程，非常感谢左神啊！课程相当值啊，你见过用1分钟吃完饭接着讲课的老师吗！你见过自己加班加点也要把所有内容都讲完的老师吗！左神就是啊！而且左神唱歌也好听啊！有次我第一个去了直播间，听了左神唱歌啊！而且自从听了左神讲的各种排序算法后，再也没有忘记过，墙裂推荐啊！（嗯，不用给我广告费）\n好了，不扯了，开始整理面试记录。（回归严肃脸）\n分为两部分内容，先写秋招正式面试记录，后面是春招实习面试记录。\n实习面了6家，秋招面了21家。还有很多被筛了简历或者笔试没过的，后面的面试也不怎么打算面了，就先这样吧。\n嗯，不要问我最后结果，大家看看面经就好，这样就避免了秀的嫌疑，毕竟我非常害怕批评，你批评我我也不敢还回去。当然，你要是非要问，那我只能唱一句暴露年龄的歌词“不要问我星星有几颗”。\n还有，整的一些秋招面试资料，放在我的博客了，放上链接（我就是那个链接），大家有想看的可以看看（欢迎大家点赞关注，啊哈哈哈哈哈）这个帖子只写面试记录了。\n一些其他的建议，给同届或者其他学弟学妹（一家之言，不喜勿喷，嗯，喷了我也不敢拿你怎么样）\n简历：不要写太多与应聘岗位无关的信息，比如个人信息、爱好写了“很多”的，这样一般是为了增加简历内容而写的，自己都认为比较水，何况面过这么多人的面试官呢。当然除了真大神（论文、项目、实习等言简意赅，亮点突出）\n简历项目实习：介绍了方法、工作，总要写你的输出吧，以结果为导向，总需要一个量化你的能力的点吧。\n海投吗？看个人精力了，毕竟是多一次机会。我个人是投了很多的，一方面是想积累经验，另一方面给自己多个机会，毕竟你不知道哪个机会恰好就是你的人生际遇。\n提前刷题，毕竟笔试有编程，面试也会手撕代码，即使是电话，也会让你讲思路，刷题才是硬道理，可以多关注动态规划的题目，大部分都是这种题。\n春招尽量去实习，尽量早去实习。一是可以转正，二是你秋招的一个资本，三是提升能力的话，好的实习远比自己闷头苦学要快很多。\n努力很重要，“大部分”原因都是不够努力，努力让自己变得幸运。\n没想好，就先这样。\n一、秋招\n以下排名不分先后，哼\n1、美团\n美团AI提前批专场面试\n1.1 美团平台\n16:15-16:50\n视频分类难点有哪些\n项目相关\n你在团队都负责什么？\n你细粒度分类调研情况如何？\n给你一个场景：超市，需要上万种物品目标定位检测、识别，需要细粒度分类，如何处理给你一个场景：\n如何划分一级标签、二级标签的？人工还是？\n论文你有吗\n你知道attention起源是用在哪里？pixel还是frame，是soft还是hard\n介绍下都有哪些优化？\n项目相关\n你毕设做的什么？\n那untrimmed和trimmed，你认为untrimmed的难点是什么？\n其他项目都是偏工程的，那还有算法吗？\n你对美团技术有什么了解\n你职业规划是什么？做研究？做业务？做基础平台？\n你还有什么要问的？\n1.2 无人配送\n15:17-15:27\n自我介绍\n那你这个项目咋么做的？\n你们做了哪些改进，有改进网络结构吗？\n有什么提高吗？\n数据集\n项目相关\n你做的工作有哪些？\n你对这边的了解有哪些？\n你想做哪些工作？\n2、大华\n2.1 一面\n13分钟\n自我介绍\n详细介绍项目怎么做\n项目相关\n网络用的损失函数是什么\n还有用什么损失函数吗？\n监督信息是什么\n2.2 二面\n37分钟\n自我介绍\n介绍项目、数据集\n讲一下毕设，什么时候开始\n效率问题\n比赛工作内容分配、几个人等\nsoftmax loss\n全连接的作用\nGD、SGD、mini batch GD的区别\n用什么语言，C++代码量有多少\nstatic\n结构体占多少字节\n函数中的局部变量在内存中如何申请\nS3D了解吗？\n你什么时候接触深度学习的？实验室有几张卡？老师做什么的？\n图像处理学过吗？\n边缘检测算子有哪些\n霍夫变换\n已经有毕业资格了吗？\n直方图是什么？\n做目标检测的话，数据集用什么\n你们与前面几名的差距是多少\n2.3 HR面\n13分钟\n自我介绍\n对这个岗，你有什么优势\n你自己技术方面的优势\n比赛的工作内容分配\n工作地点\n对大华有什么了解\n实验室情况\n3、CVTE\n3.1 一面\n23分钟\n项目\n项目巴拉巴拉\n传统图像处理、边缘检测\ncanny算子是怎么做的？\n传统机器学习有了解哪些？\n说一下SVM核函数\nPCA：SVD分解\n霍夫变换\n过拟合\nL1、L2范数，L1趋向于0，但L2不会，为什么？\n偏差、方差\ninception v2、v3区别\nresnet好处\n为什么用LR，不是0-1\nsigmoid优缺点\n讲一下LSTM\n你有什么问题\n3.2 二面\n32分钟\n讲一个你觉得最满意的项目\n介绍比赛是怎么做的\n你做了那些工作\n你认为你在团队中排第几\n你还有那些工作\n你们这个项目可以用在工业界什么地方？\n图像的特征提取有哪些算法\n那你知道他们的适用范围、优缺点吗？\nL1 L2范数了解吗？区别\n偏差是什么\n余弦相似度距离和欧氏距离的区别？\n你知道其他距离度量公式啊？\nPCA\n你认为深度学习是最好的方法吗？\n那你觉得哪些时候不能使用深度学习\n给你一个烤箱，你需要识别烤箱中的食材，怎么去做？\n没有数据怎么办\n如果数据不好怎么办，数据不均衡怎么处理、只有少量带标签怎么处理\n模型不好怎么办，效果不行\n烤箱设备升级，照明、摄像头位置等提高，但是原有的模型准确率却下降了。怎么办\n你有什么问题\n3.3 HR面\n35分钟\n之前两面打分，为什么\n找工作的情况，有没有offer\n意向公司、目标企业\n找工作的标准\n对实习公司的评价\n那为什么不留在实习公司\n最好、成功的一件事\n最大挑战的一件事\n失败的事情或者说没有达到期望\n自己有做过哪些分外之事\n家庭情况、工作、性格、身体、年纪\n父母谁对自己影响比较大\n父母对你的期望\n至今为止对你帮助最大的一个人\n至今为止对你影响最大的一个人\n哪个时间段压力比较大，身体、心情\n5-10年的人生规划，事业和家庭\n人生理想\n拿到offer，为什么不来\n来了，为什么后来又离职\n期望工资、地点、工作时间\n同一批同学工资比你高，你怎么看\n最后没拿到offer，你认为为什么\n自己做过哪些重大决定\n在哪个领域有深入的了解\n有哪些兴趣爱好\n4、顺丰\n4.1 一面\n25分钟\n自我介绍\n介绍怎么做的\n项目相关的一堆问题\n你还做什么\n讲一下目标检测都有哪些方法\n讲一下faster rcnn\nROI层是怎么实现的，怎么做的映射\nyolo和SSD区别\n训练不收敛的问题\n数据集不好\n4.2 HR面\n27分钟\n面试官有没有给你介绍公司\n介绍了项目\n实习经历\n比赛怎么做的，负责的工作\n如何选择一个公司\n工作地点\n项目有哪些应用\n兴趣爱好\n5、Momenta\n5.1 一面\n25分钟\n介绍科研吧，\n项目相关\n目标检测\n求感受野\n有没有自己写过层，反向传播之类的\nBN层的moving——mean怎么求得\nBN层反向传播，怎么求导\n还有做过哪些\n5.2 二面\n25分钟\n用什么语言\nC++的多态\n怎么实现\n最大连续子数组\npython传参会改变原值吗\n平时看论文吗\n以后打算做哪个方向\n有没有自己设计算法\n5.3 三面\n41分钟\n自我介绍\n实习时间\n介绍一下项目\n数据集\n权重怎么设置\n做了哪些改进，提升\n目标检测、分割了解吗\n有用过哪些小网络吗\nmobilenet怎么实现\npooling层怎么反向传播\n数据输入是怎么处理的\ndepth conv知道吗\n计算一层的参数量、计算量\n计算感受野\n你想做什么方向\n6、腾讯\n6.1 一面\n58分钟\n自我介绍\n介绍项目\n比赛成绩\n第一名第二名是谁\n具体项目内容\nBN层怎么实现\n基础网络模型用了哪些\nresnet第二个版本做了哪些改进\n有没有做过action proposal\n目标检测\nxx怎么做的\nxx实习经历\n还在实习吗\n实习公司有发offer吗\n编程题:数组有序，但是循环右移了几位，问新数组中原数组起始位子的下标是多少\n6.2 还是一面\n1小时28分钟\n自我介绍\n问了论文\n问了比赛，\n负责的工作内容排第几\n项目的一堆问题\n近几年的网络\nmobileNet、shuffleNet知道吗？\n检测了解吗\n讲一下faster rcnn\nyolo\nSSD\nbn层怎么做的\n撕代码：iou计算、k-means\n其他项目偏算法还是工程，后来有跟进算法吗\n工作地点\n博客花了多久时间\n7、深信服\n7.1 一面\n26分钟\n自我介绍，然后说介绍一下项目\n比赛\n你做的工作有哪些\n你们这个比赛的优势有哪些\n技术上的改进\n那之后又做什么？\ndropout\n给数组，找到加起来是100的一组数。\n一篇英文文章，统计单词频率，得到频率最高的那个单词\n给一组整数，无序，在不改动原数组的基础上，请找出1000个位置连续的数，使其和为10万。\n那如果是不限制个数，只要使其和为10万就可以的数。\n给10x10的棋盘，扫雷，随机放置10个点作为雷，如何保证随机放置？\n那你觉得你还有什么优势，我们没了解到的\n7.2 二面\n40分钟\n项目\n实习的工作内容是什么\n介绍一下比赛方法\n这个比赛考察什么？\n工作的话偏向于图像还是可以做文本，\n过拟合怎么做\n正则化怎么选择，l1范数、l2范数；\n介绍残差网络\n有没有发论文\n数组n个正整数，每个数值不超过n-1，有一个重复的数，找出那个重复的数\nn位字符串，循环右移m位，要求时间复杂度线性，空间复杂度为o(n)\n两个图像库，场景一一对应，一个有雨滴，一个没有雨滴，对有雨滴的图像去除雨滴，要不留痕迹。\n用什么语言\n写一个函数，只有一条语句，判断数n是不是2的幂\n7.3 三面\n20分钟\n自我介绍\n项目\n团队分工、负责的工作\n你有做哪些算法上的创新\nxx做的工作\n其他领域呢，传统机器学习了解过吗、nlp呢？\n工程怎么样\nC++代码量\n实习和实验室的差别，学术上的差别等\n8、360\n8.1 一面\n36分钟\n自我介绍\n相似度衡量尺度\n图像特征提取方式\n人脸识别，如何输出标签\nboost、Adaboost\n有其他offer吗、工作地点要求\n有上线业务吗\n筛子六个面，每个面的概率不一样，要求实现一个掷筛子的函数。\n在xx做的什么\n其他领域有做过吗\n8.2 二面\n18分钟\n自我介绍\n介绍论文\n介绍比赛\n应用背景\nC++内部实现机制\n用过什么指针\nC11特性了解吗\nmat申请一个图像矩阵后怎么释放内存\n在xx做的什么\n其他项目\n什么时候能来实习\n9、多益网络\n9.1 一面\n17分钟\n自我介绍\n介绍最好的一个项目\n比赛这个还有哪些可以拓展的\n什么是梯度消失和梯度爆炸\n怎么处理这种问题\n正则化有哪些方式\n线性回归和逻辑回归的区别\n插入和删除用链表还是数组\n54张牌，分3组，大王小王同在一组的概率\n什么是SVM\n有哪些offer，薪资多少，都是人工智能吗？\n有论文专利吗\n怎样看待互联网加班\n10、网易互联网\n10.1 一面\n33分钟\n自我介绍\n项目\n学习的项目\n比赛\n实习\n博客链接\n代码：实现卷积操作\n10.2 二面\n24分钟\n自我介绍\n介绍项目\n优化算法\n介绍一下momentum\n介绍inception网络，主要用来解决什么问题，如何减少参数量\nresnet网络\n如何避免梯度消失\n介绍一下BN\n激活函数\n权重初始化方法都有哪些\n代码：归并排序\n10.3 HR面\n27分钟\n这次面试感觉怎么样\n跟其他面试有什么区别吗\n实习工作\n比赛负责的工作，几个人\n创新点等\n调研工作\n投了哪些公司，offer，\n期望薪资，如何定的\n压力大的时候\n加班？\n11、招行网络科技\n11.1 一面\n12分钟\n自我介绍\n介绍项目\n机器学习算法了解原理吗，有做过项目吗\n讲一下CNN，每个层，及作用\n网络权重初始化\n梯度消失和梯度爆炸，LSTM中如何解决\n各种数据的channel是指什么意思\n如何评估模型\n如果接触一个新的东西，怎么学习\n用什么语言\n12、蘑菇街\n12.1 一面\n52分钟\n自我介绍\n介绍项目\n比赛介绍\n分类，既有河流，又有建筑，怎么做。多标签\n有个类别总是错分到其他类怎么办\n精确率高、召回率低是为什么\n图像处理了解吗\n有哪些特征子\n传统机器学习呢\n讲一下boosting\nBN层\n目标检测SSD等算法了解吗\nanchor设置的意义\n一个人有很多框，什么原因造成的\n用什么框架\n其他框架呢\n工作地点意向\n代码问题：手写直方图\n用什么语言\nSTL用过哪些\n12.2 二面\n34分钟\n什么时候毕业\n考研还是保研\n工作地点\n数学课学过哪些\n项目中有没有实际用到\n图像处理学过吗\n什么语言\n什么框架，看过源码吗\n介绍项目\n比赛几个人，你负责的工作是什么\n你比较倾向的工作内容，感兴趣的领域，视频还是图像\n深度学习发展，大家都用一样的东西，对自我的提升没多少，你怎么认为\n职业规划怎么考虑的\n12.3 HR面\n15分钟\n为什么投蘑菇街\n对蘑菇街的了解，你看重蘑菇街什么\n目前有哪些offer、面试中、等结果\n如果蘑菇街给你offer，怎么选择\n如何选择offer\n蘑菇街跟其他公司比\n13、旷视\n13.1 一面\n57分钟\n自我介绍\n项目相关\ndata argumentation怎么处理的？\n人脸识别paper有了解过吗\n多标签识别怎么做\n视觉其他领域做过吗？\nfaster rcnn和ssd区别\n其他loss有了解过吗\npca过程\n二维平面，一堆散点，找一个条拟合的直线\n二维平面，一堆散点，找分布函数\nSVM、核函数\n无监督学习了解哪些\n模式识别上过课还是自学的\n图像处理学过没\n图像旋转、旋转矩阵、像素点怎么填充\n反转链表\n最熟的框架\n13.2 二面\n42分钟\n介绍最厉害的一个项目\n第一名是谁\n数据集\nBN层怎么计算\niou和NMS怎么算，写代码\n工程方面的，怎么处理数据\npython多线程多进程\n你有哪些offer\n计划实习吗？这半年怎么安排的\n13.3 三面\n50分钟\n自我介绍\n比赛负责哪些工作，第一名是谁，哪个任务\nopenpose了解吗\n都用过哪些网络\ninception v1-v4的区别、改进\nVGG16和resnet152哪个参数量多\n论文发的哪个会议，讲一下论文\n1000个视频，无标签，怎么分类预测\n还做过哪些\nfaster RCNN介绍、yolo\nanchor的正负样本比是多少\n数据不均衡怎么处理\nonline hard 样本接触过吗\n求期望：设随机变量X1,X2,…Xn相互独立，且都服从（0,θ）上的均匀分布。求U=max{X1,X2,…Xn}数学期望\n反转字符串\n有一个数target，一个数组，数组中两个数的和是这个target，请找到这两个数\n13.4 HR面\n26分钟\n面试感觉，与其他面试有什么不同\n印象最深的面试官\n自我介绍\n实习工作\n比赛负责的工作\n工作任务怎么分组，业务导向\n对实习的感受，氛围，有什么业界知名人士吗，发表论文什么的每年多少CVPR、ICCV、ECCV等\n压力大的时候\n哪里人\n怎么选择工作\n期望企业\n有哪些offer，给了多少钱，期望薪资\n13.5 终面\n32分钟\n问比赛\n比赛的含金量\n有做改进吗\n与前几名相比呢\n与去年相比吗\n你负责的工作是\n介绍项目的方法\n当前还存在什么问题，你认为后续发展方向\n传统机器学习用过吗\n还了解哪些领域\n你用什么框架，各个框架的实现机制有什么不同\n过拟合是怎么造成的，如何解决\nbias variance\n用什么语言\nC++一些语法特性平时项目中会用到吗，比如多态、继承等\nset map低层实现的数据结构是什么\nmap，m[1]=“str”;这句低层是怎么实现的，发生了什么\n数学怎么样\n上次编程题反转字符串写出来了吗\n你希望工作偏研究还是工程\n对现在创业公司的看法，\n你自己的优缺点\n14、vivo\n14.1 一面\n26分钟\n介绍最得意的一个项目\n比赛的影响力\n一共几人参赛\n你负责的工作\n讲一个比赛用的算法\n一个1-n的数，少了一个，找出来\n1-n，少了两个数，找出来\n计算1-100的质数和\n你觉得你的项目可以用在vivo的哪些产品上\n14.2 HR面\n20分钟\n自我介绍\n本科是一本吗\n保研还是考研\n本科班级排名\n比赛、比赛规模\n为什么做这个比赛\n工作地点\n家庭情况\n其他公司有投吗\n意向薪资\n15、虹软\n15.1 一面\n24分钟\n自我介绍\n介绍项目\n用什么框架\n论文\n比赛\n网络结构\n多少队伍、团队人数\n你负责的内容\n数据集\n细粒度分类\n你以后想做哪方面内容\n有没有用在实际场景中，\n数据集上会不会过拟合\n用什么语言，熟吗\nopencv怎么样\n图像处理怎么样\n笔试成绩怎么样\n期望的工作地点\n15.2 二面\n43分钟\n自我介绍\n介绍比赛\n算法框架\n做了哪些改进\n我负责的工作\n博客里写的网络都用过吗\n博客里各种归一化层有实现吗\n感兴趣的领域\n反向传播求导，给了个例子链式求导，pool如何反向传播\n最长回文子串\n15.3 HR面\n38分钟\n工作地点\n实习，学到了什么\n对虹软的认识\n有哪些offer，意向公司等\n考研还是保研\n三个词描述自己，为什么\n对自己两三年的计划\n对女生做计算机有什么看法\n16、欢聚时代\n16.1 一面\n26分钟\n项目介绍\n继续介绍\n光流怎么计算\n数据标注怎么做？\n实习经历\n人脸识别也做过？\n除了做视频，还做什么？\n声音特征是怎么提取的？\n创新点\n16.2 HR沟通\n7分钟\n为什么会投YY\n之前对YY有了解过吗？\n有转正offer吗？\n最想去的三家公司\n看你之前在xx实习过，后来为什么又去了xx\n那你在实验室的项目都是什么？\n那你如何选择offer？\n你对地点有要求吗？你是哪里人？\n你薪资大概要求多少？\n你还有什么问题？\n那你对岗位工作内容怎么看，有兴趣吗？\n16.3 二面\n20分钟\n自我介绍\n具体介绍一下怎么做的\n介绍基础网络inception\n介绍resnet\nloss怎么设置\nSGD各个参数怎么设置\n权值衰减这个参数怎么设置，\n优化器选择\n16.4 三面\n20分钟\n自我介绍\n比赛具体怎么做的\n数据集多少\n训练一个网络要多久时间\n你负责哪些工作\n代码量\n与第一名的差距\n论文主要思想\n用什么框架\n低层代码用看过吗\n什么时候进的实验室\n这段实习，他们是做什么的\n用在哪些业务上\n工作地点\n16.5 HR面\n26分钟\n自我介绍\n被打断，说不要介绍技术，就说除了技术之外的\n在xx做什么，收获了什么 、最大的挑战\n个人优点和缺点\n对YY的看法\n如何选择工作，\n有哪些offer\n抗压能力\n家里几个孩子\n17、头条\n17.1 一面\n56分钟\n介绍最好的一个项目\n问项目里面的 loss\npython多进程 多线程，为什么多线程比较鸡肋\npython是解释语言还是编译语言\nxrange与range的区别\n迭代器，啥来着，忘了\nSTL中vector的低层实现\nSTL中插入的操作时间复杂度，要考虑内存复制扩充，\n如何实现一个栈，支持动态扩充\n如何用链表实现一个栈，\n如何实现一个栈，O(1)获取最小值，get_min\n如何节省空间，存放最小值，如果有多个，不想多次存放\n用map计数，但是需要O(logN)的复杂度查找；用数组计数，空间复杂度更大用数组计数，空间复杂度更大；设置结构体，除了value，还有一个count值，计数。即前面已经用了链表，节点结构体含有，value，count，next；\n好了，前面说了这么多，你把这些条件都实现了吧，写代码\n计算feature map的size，卷积层参数量\n18、图森\n18.1 HR面\n7分钟\n一句话介绍自己做什么\n实习都做什么工作\n有做哪些改进\n有哪些offer\n18.2 一面\n42分钟电话面试\n介绍项目\n边介绍项目边问问题\n那些网络？\n项目巴拉巴拉\n细粒度分类\n介绍BN\n项目巴拉巴拉\n19、触宝\n19.1 一面\n50分钟\n自我介绍\n介绍项目\n数据集 哪些类别 标签\n数据不均衡\n过拟合\n蒙哥特洛方法\n手写快排\n手写：给数字N，表示以后N对括号，求所有可能的合法括号\n判断括号是否合法\nshell\n查看文件大小命令\n查看文件多少行命令\n如何后台起一个服务\nsoftmax和logistic的关系，可以推导吗\n19.2 二面\n53分钟\n自我介绍\n介绍项目\n介绍算法导论都有哪些方法\n动态规划和分治的区别与联系，各自适应哪些情况\nC11特性有了解吗\nSTL用过哪些，低层实现\nset、map的查找复杂度、插入删除等\n手写vector实现\n机器学习算法了解哪些\nLR手写代码\n激活函数有哪些，各自区别\n梯度消失、梯度爆炸问题怎么处理\nBN层，先加BN还是激活，有什么区别\n损失函数有哪些\n优化器有哪些，怎么演进的，平时怎么用，如何调参数\n对工作内容有什么要求\n如果给你offer，你会来做语音识别、NLP吗\n19.3 三面\n31分钟\n写个代码：字符串转数字\n介绍项目\n你自己做吗，分工\n如果分类结果不好怎么办\n传统机器学习了解吗，我看你博客有写\nLR，给你数据，进行性别分类，有：身高、体重、兴趣，注意有些特征连续，有些是离散的，怎么用LR来做\n特征维度是多少维度\n20、阿里\n20.1 一面\n53分钟\n自我介绍\n介绍了项目\n实习经历\n什么时候开始做算法的？\n当时去xx的契机是什么？收获是什么？\n比赛、结果等，你学到了什么？\n学校的项目有哪些？\n有什么成果产出？\n讲一下论文大概思路，创新点是什么？整个网络架构？\n有没有考虑应用？\n还有其他项目吗？\n如果用在移动端，如何处理？有这方面的经验吗？\n有哪些应用场景？\n如何做到real-time？\n之后有什么拓展？\n写博客的目的？从什么时候开始写的？\n你打算来杭州工作吗？有投其他公司吗？\n20.2 二面\n25分钟\n先自我介绍一下\n项目\n数据集？\n业务中怎么用呢，有产品落地吗\n介绍一下你们的算法架构\n做的改进\n多少人参加，会公开技术方案吗？其他是怎么做的？\n你们还有做哪些尝试，或者踩过得坑\n你自己还做过哪些，\n20.3 三面\n1小时31分钟\n自我介绍\n介绍了项目\n比赛怎么做的\n论文思路\n创新点、贡献点\n在xx时的工作是做什么\n是什么平台？是什么工具？工作内容是什么？\nxx做的是什么？\n这个项目怎么做的\nMFC界面通信怎么实现\nmysql都有哪些操作\n博客链接在哪里\n编程：强盗抢东西那个题，优化了3次\n20.4 四面\n32分钟\n面试官介绍他们那边在做什么\n你有什么问我的\n最能展现你技术水平的一个项目\n你认为还有哪些优化的地方\n研究生学过算法设计没？学过什么课程\n字符串的全排列\n学过数理统计没?\n那假设检验学过没，实际中用到过吗？\n论文中，怎么跟baseline比较，你的算法好呢？\n噪音干扰怎么办\n你还有什么问题？\n20.5 一面（五面）\n34分钟\n自我介绍\n项目：怎么做、用在哪里、比赛算法框架、\n数据处理、分析等一些心得\n项目\n论文内容\n20.6 二面（六面）\n42分钟\n介绍自己工作、亮点\n图像库，给一张图像，找出图像库中最相似的那个\n快速找到一个数字的开五次方根\n一个数，0-1024，最多几次找到这个数\n52张牌，摸5张牌，求顺子的概率，，允许不同花色\n工作城市、什么时候毕业、哪一年的\n20.7 三面（七面）\n26分钟\n介绍xx这边的工作\n介绍比赛，比赛算法等\n介绍后续工作\n你认为这个比赛还有那些改进的\n你认为过程中比较有挑战性的问题\n如何加快迭代速度\n用的什么框架\n这个项目怎么做的\n你认为后续如何发展\n做这个比赛有用在哪些业务上吗\n你对视觉哪个方面比较感兴趣\n20.8 四面（八面）\n28分钟\n介绍自己的项目\n还有哪些改进，\n其他团队用的什么方法，第一名用的啥？\n你们这个最后有落到产品上吗\n项目巴拉巴拉\n适用范围\n介绍论文\n你还有其他项目吗，或者其他方面的研究\n那你介绍一下对目标检测的了解\n还做过什么？\n其他公司也实习过\n你对应聘的部门有了解吗，知道是做什么的吗\n你对阿里怎么看，offer怎么考虑\n20.9 HR面（九面）\n17分钟\n为什么想来阿里，项目实习经历，在阿里的规划\n你的一些竞赛经验\n那个比赛对你来说帮助比较大\n你的实习经历\n那个实习经历对你帮助比较大\n转正offer吗\n家庭\n你还有什么问题\n21、搜狗\n21.1 一面\n46分钟\n自我介绍\n介绍项目、比赛\n数据集\n怎么做的\n一秒多少数据\n什么框架\n团队多少人\n介绍负责的工作\n数据增强\n正则化\ndropout\nBN层、参数量\n卷积是怎么实现的，比如caffe中卷积的实现\n其他损失函数\ninception v1中的inception结构怎么设计的\n为什么使用1x1卷积核\nSTL中vector的resize函数、reserve函数\n多态\n引用和指针的区别\nC++中内存管理\n堆和栈的访问哪个更快\n如何求二叉树的深度\n图的遍历方式\n广度优先可以用来求二叉树的深度吗\nLinux下有用C++编程吗\n你还有什么问题\n二、春招\n1、腾讯\n1.1 内推一面\n11:13-12:17\n问了实习时间\n项目的相关问题项目的相关问题\n行为识别主要有哪些技术？行为识别主要有哪些技术？\n应用场景是什么？数据集怎么采集？应用场景是什么？数据集怎么采集？\n你这个项目是实验室上一届就有做，你在上面继续优化，还是你从头开始的？\n对CNN的理解\n如何用到CNN的？比如用CNN解决了一个原先算法无法解决的问题，还是说刚上来就用CNN？\n还了解过哪些网络？\n你对GAN的理解？用DCGAN来做什么？\n你的职业规划是什么？\n你的博客都写什么\n你都看过哪些论文\n二维矩阵，行、列皆有序，用O(M+N)查找一个数是否存在\n介绍他们部门与研发流程\n还给了一些指导和建议\n1.2 正式春招一面\n19:04-19:50\n介绍项目1，问了巴拉巴拉\n介绍项目2，问了巴拉巴拉\n介绍项目3，问了巴拉巴拉\n你们这都是用别人的网络，自己有做什么吗？\n你们都是堆网络，有想过别的什么提高吗，比如训练方式上\n那你们是做分类任务，那后面怎么应用呢？\n那你有了解嵌入式吗？怎么应用你的场景？\n那你们既然做应用，实时性这个怎么办？\n你有了解过mobilenet这种轻量级的网络吗，直接就可以在移动端用的\n那你有移动端的经验吗？\n那你们现在就是在数据集上做是吗，有做过实际场景处理吗？\n实习的工作内容是什么\n一道编程题：给定一个单链表，一个数x，然后你把这个单链表改成前面是比x小，后面是大于等于x的顺序，然后每个部分仍然保持原来链表的顺序。\n一道推理题：三个连续数，大于6，其中两个是质数，问第三个一定能被6整除。\n意向城市\n2、阿里\n2.1 一面\n面试28分钟\n自我介绍，介绍一下项目经验\n介绍一下项目，是怎么做的，数据集，效果，最优比较，应用背景\nCNN的经典模型\n介绍一下logistics regression\n随机梯度下降和梯度下降\n优化算法有哪些，了解过吗？原理是什么？\n图像处理的其他算法，比如提取特征\nopencv用过哪些？\n卷积操作\n2.2 二面\n面试14分钟\n介绍一下研究生期间的项目和你负责的工作\n数据集\n项目相关的一些问题\n那你都是怎么写这些网络的\nresnet，你有了解吗\n那你了解其他神经网络吗？比如Inception\n那你知道inception后来有哪些改进吗？\n那你知道一些细节的改进吗？比如你刚才说的BN就是inception提出的\n那你在训练时有用什么激活函数吗？\n那你知道为什么不用sigmoid而用relu吗？做出了哪些改进？\n那你在训练时有用什么优化器吗？\n那你知道他们是如何改进的吗？\n你现在电脑在旁边吗？方便做一下在线编程吗？半个小时，第二天做的\n你的意向工作城市是哪里？\n3、美图\n3.1 一面\n09:50-10:35\n自我介绍\n具体介绍一下项目\n有用过pre train吗？\n那你每个准确度是多少\n那你就是复现论文吗？有做什么改进吗？\n对数据集的处理是怎样的？\n还了解哪些网络模型？\n介绍resnet，残差网络\n梯度怎样计算的，为什么这里梯度不会消失，推导一下。\n你是哪里人啊\n你能来实习多久？\n目前有几个offer？\n3.2 二面\n10:37-11:00\n你还做过图像哈希，介绍一下\n你最想去哪个公司工作？\n你认为工作后那几年作重要？你有一个什么样的规划？\n对于大公司和创业公司，你选择哪个？\n对于大公司做一颗螺丝钉和小公司做一个大部件你是怎么认为的，倾向于什么？\n你期望在哪个城市工作？\n你对美图有什么了解？\n什么时候可以来实习？\n你在xx实习过4个月，为什么离职了呢？\n3.3 HR面\n11:00-11:18\n什么时候来实习？具体实习时间是多久。\n对美图的了解？\n你做过两次实习，工作怎么样？有意思吗？\n培养实习生，干了一两年就走了。然后希望能多做下去。\n问这周五能来报到吗？\n4、京东\n4.1 一面\n15:50-16:10\n自我介绍\n我们这个岗位其实不是做视觉的，是做分布式计算，多机多核并行计算的。你有过这方面的知识积累吗？\n计算经过卷积操作后的feature map大小。\n你对tensorflow了解多少\n那你知道它的dataset和？？？这个模块吗？\n那你知道现在是到版本几了吗？\n你知道python的？？？模块吗？\n你说阅读最新论文，你都读过什么论文？\n介绍一下resnet网络。\n那你们做这些项目，有什么应用吗？学术上只要有效果就行，那在工业上怎么用呢？\n介绍下什么是过拟合，怎样解决过拟合\n那你说一下dropout和batch normalization。\n你了解x吗？那你写了这么多博客，在写和参考别人的时，都会有意无意积累一些知识，怎么会不知道呢，那你博客都是原创的吗？写的都是什么内容？\n你在xx实习过，都是做哪些工作吗\n你能什么时候来实习，实习多久？\n你这个xx项目是怎么实现的，有什么功能？\n4.3 二面\n10:58–11:08\n不用自我介绍。你来说还是我来问？\n一面面试官有问过你什么？（因为岗位不符，但还是通过面试了）\n那你为什么还来面试？\n你在xx实习过，都做过什么工作，老师让出去实习吗？\n你还有什么想说的？\n你还看过GAN，是后来用的吗？\n4.3 HR面\n9分钟\n自我介绍\n考研还是保研\n那你为什么报考xx，通过什么努力考上的xx\n我不是问你怎么复习的，我是问你做了哪些努力？\n那你觉得你本科学校和研究生学校有什么不同？\n那你自己都是怎么学习的呢？\n你的优势是什么？\n我不是问你性格，我是问你因为你的优势获得哪些成就\n你有哪些缺点，\n你怎么保证高效的完成自己的事呢？\n你遇到的最大困难、挫折是什么？\n那你就是在学校只学习没有参加活动咯？\n我刚才问你的问题就是想知道你是否参加一些活动，你一直聊得都是学术上的我刚才问你的问题就是想知道你是否参加一些活动，你一直聊得都是学术上的\n那今天就面到这里吧。\n4.4 四面\n13:20-13:40\n其实不是面试，就是问，如果给我发offer，会不会来，\n聊了一下，问了问工作内容、地点、转正、时间什么的\n5、360\n5.1 一面\n10:32-11:08\n自我介绍\n线程和进程 区别\n那进程间可以通信啊，什么方式\n线程可以通信吗？\n线程有哪些状态\n你了解多线程、多进程吗？有写过吗\npython中怎么实现多线程、多进程的？\npython中的多线程是真的多线程吗？\n指针和引用的区别\nTCP和UDP的区别\n内存中堆和栈的区别\nC++中多态了解吗？\n项目\n你数据结构怎么样\n了解哪些数据结构\n那链表你知道？？\n手写代码：反转链表、快排\n5.2 二面\n14:30-15:14\n自我介绍\n多态你知道吗？怎么实现的？\n你写一下吧\nC为什么比C++快？\n介绍一下你这个xx项目吧，我们也不太了解\n那你们数据集哪来的？\n那你们应用场景是什么？\n你这个xx项目是做的什么\n那这个项目呢？\n你在xx的实习工作是什么，他们的工作是做什么的\n如果给你一个课题，让你去实现，怎么做？\n那你如果给你数据，模型，你的结果不好怎么办\n那你知道怎么结果过拟合吗？\n你知道导数在物理上的意思吗？\n那你知道二阶导吗？\n拐点怎么求？\n你网络了解多少？\n你这个CCF软件能力认证是什么？\nLinux用的多吗\n那你知道grep命令吗？\n那你在Linux怎么写代码？\n那你了解shell吗？\n写一个二叉树非递归的中序遍历\n你知道这个函数是什么意思吗？（我写在节点结构体中的初始化函数）\n你在xx为什么不继续实习了？\n那你们老师让实习吗\n那你们有汇报吗？\n你什么时候能来实习？\n那你愿意来我们这做NLP、数据挖掘什么的吗？\n5.3 HR面\n15:37-15:47\n自我介绍\nxx项目是用在哪里的？\n现在已经在用了吗\nxx这是什么项目，那你现在觉得当时还有什么改进吗？\n你在xx实习过，都做得什么工作？\n你在xx实习过，这是个什么公司？\n那你还没有过视觉方面的实习是吗\n你C++上过课是吗\n那你自学过其他语言吗？\n你有什么兴趣爱好\n你还有什么问题？\n5.4 四面\n14:00-14:30\n问了之前来面试的情况，然后说，上次面试部门是xx，那边hc不多，然后就推到他们这边了。HR说是基本可以发offer了，但是他之前也没有了解过，还是想再面谈一下。\n已经有什么offer了吗？\n你比较偏向于哪个？\n介绍了他们这边是做什么的？\n快排手写\n了解过hadoop吗？MapReduce呢？\n讲下MapReduce的理解；\nhql\n查询表A中有但表B中没有的？\n两个表做连接\n6、58同城\n6.1 一面\n17:07-17:27 20分钟\n自我介绍\n项目\n有什么策略，\n你做过哪些优化？\n你后面会做传统机器学习方法吗？\n那你了解哪些基础？\n那你介绍一下LR\n介绍一下SVM\nsigmoid函数公式\n你还有什么问题"}
{"content2":"﻿﻿\n微软创投加速器最新成果展示：人工智能技术杀入时尚界\n发表于23小时前| 843次阅读| 来源CSDN| 1 条评论| 作者周建丁\n人工智能计算机视觉深度学习数据分析微软创投加速器\nwidth=\"22\" height=\"16\" src=\"http://hits.sinajs.cn/A1/weiboshare.html?url=http%3A%2F%2Fwww.csdn.net%2Farticle%2F2015-08-09%2F2825417&type=3&count=&appkey=&title=%E5%9C%A88%E6%9C%888%E6%97%A5%E7%9A%84%E5%BE%AE%E8%BD%AF%E5%88%9B%E6%8A%95%E5%8A%A0%E9%80%9F%E5%99%A8%E7%AC%AC%E5%85%AD%E6%9C%9F%E5%B1%95%E7%A4%BA%E6%97%A5%E5%B1%95%E7%A4%BA%E7%9A%84%E6%88%90%E6%9E%9C%E6%98%BE%E7%A4%BA%EF%BC%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E4%BA%91%E5%B9%B3%E5%8F%B0%E4%BB%8D%E7%84%B6%E6%98%AF%E6%8A%80%E6%9C%AF%E5%88%9B%E4%B8%9A%E7%9A%84%E6%9C%80%E4%BD%B3%E6%96%B9%E5%90%91%EF%BC%8C%E8%80%8C%E4%B8%8E%E5%A4%A7%E4%BC%97%E7%94%9F%E6%B4%BB%E5%8F%8A%E5%AD%A6%E7%94%9F%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%9B%B4%E5%AE%B9%E6%98%93%E8%8E%B7%E5%BE%97%E6%8A%95%E8%B5%84%E4%BA%BA%E7%9A%84%E9%9D%92%E7%9D%90%E3%80%82&pic=&ralateUid=&language=zh_cn&rnd=1439212877348\" frameborder=\"0\" scrolling=\"no\" allowtransparency=\"true\">摘要：在8月8日的微软创投加速器第六期展示日展示的成果显示，人工智能、数据分析和云平台仍然是技术创业的最佳方向，而与大众生活及学生学习相关的项目更容易获得投资人的青睐。\n在8月8日的微软创投加速器第六期展示日展示的成果显示，人工智能、数据分析和云平台仍然是技术创业的最佳方向，而与大众生活及学生学习相关的项目更容易获得投资人的青睐。\n在第六期15家团队分享了各自的创新产品成果之后，投资人、媒体和历届创业团队代表的现场投票结果表明，用人工智能技术做时尚的码隆科技最受欢迎，做无人机的亿航和移动应用云服务提供商APICloud次之。此外，基于Docker技术打造的灵雀云也在互动中被投资人和媒体人屡屡提及。\n人工智能\n码隆科技创始人黄鼎隆介绍，码隆科技基于深度学习与计算机视觉打造“有时尚品味的视觉决策引擎”，其CTO是曾任微软高级研发主管的Matt Scott，麾下聚集来自Google、腾讯、百度的研发人才。借助CNN，其StyleAI系统能够“看懂”和“理解”任意照片里面（如商场模特、晚霞、宠物、美食）的时尚元素，对随手拍的照片进行颜色、花纹、材质的精准分析，随即通过时尚大数据获取相关时尚穿搭信息，提供实时决策建议。产品端的How-Fashion.net，则提供实时指数、五维时尚分析及社交功能，颇有微软How-Old.net的风格。\n码隆科技创始人黄鼎隆介绍深度学习与计算机视觉在时尚领域的应用\n据说，码隆科技未来还将用基于深度学习的自动化图片处理系统，完全取代人工来搜集、处理、管理和调用海量时尚数据。\n另外一家初创一搭科技，也有志于将人工智能引进到日常穿衣搭配，不过目前还需要更多人为干预。一搭科技创始人张扬正介绍，一搭通过技术算法以及时尚编辑的专业推荐，根据用户的身材、脸形、五官比例以及不同场合推荐适合的搭配。\n此前我们了解到的图片搜索引擎衣+（奇点大学中国区学员），同样也是面向时尚市场，通过无监督学习、有监督学习和梯度下降法的结合对服装商品属性进行自学习。可见时尚领域对人工智能技术的欢迎程度。\n记者还没有机会和码隆科技及衣+进行过详细的交流，不知道其深度学习和机器视觉的具体应用情况，但应用本身就是人工智能发展的一个重点，复杂背景下的人体检测和衣物分割，对光照的鲁棒性和对款式的识别，都不是简单的事情。\n正如中国科学院院士、中国人工智能学会副理事长谭铁牛在2015中国人工智能大会上预言，“人工智能技术的发展将对传统行业产生重大颠覆性影响，‘智能+X’将成为创新时尚”，相信“智能+时尚”的创业创新会推动人工智能技术的落地。\n如果说码隆科技及衣+的概念成分偏多，那么智齿科技通过大数据及自然语言处理技术打造的智慧客服，就已经是实在的应用，乐视、海尔、360都是他们的用户。通过智慧客服，这些企业可以把90%的问题交给机器客服解答，人工客服可以解答10%的最关键的问题，据说每年节省60%的客服成本，工作效率提升300%。智慧客服技术要点包括：采用谷歌算法进行语义分析，精准理解客户需求匹配最佳答案；机器学习自动分析用户问题，构建完整知识库，实现自我成长；以及多维度用户数据分析等。\n亿航公司做的是智能无人机，被问到“智能如何体现”时，亿航工作人员回答是“可以通过手机来控制”。亿航科技联合创始人熊逸放表示，亿航通过APP，有很多一键式的傻瓜操控，将操控飞机姿态的复杂操控变得非常简单，并研发了智能跟随等智能化的功能。其三款产品，分别可搭载便携相机、专业单反和行业拍摄器，后者可用于判断是否喷洒农药、森林失火急救。\n严格说来，亿航这种无人机其实距离人工智能还很遥远（当然亿航也没说自己是做人工智能的），甚至第三款产品也没有涉及图像处理。余凯也曾经表示，智能的本质是自我学习，通过遥控来操作的无人机不能算是人工智能。但在人工智能的初级阶段，即便是Google Project Wing，也还没有做到安全的自主飞行，所以目前来说APP操控未尝不是好的解决方案，可惜熊逸放没有谈到一个手机同时控制多个无人机的实现情况。\n此外，记者在展区看到了洋葱、数信互融和乐驾科技。洋葱做基于脸型、声音、指纹和二维码的身份认证，已经应用在UCloud、Worktile、美团云、UPYUN、明道、蘑菇街等。现场工作人员演示了嵌入第三方平台的刷新登录和支付，当被问及双胞胎、照片识别问题时，洋葱技术人员表示，目前动态形体识别的准确率还不够稳定，但多因素认证可以提升效果，如人脸+声纹。\n洋葱的人脸识别是内部研发的，声纹则采用了科大讯飞公司的声纹识别算法。人脸识别+声纹识别，这也是科大讯飞在2015年上半年极力推广的方案，可以跨平台。不过，洋葱技术人员表示，如果有别的声纹技术效果更好（目前看来可能性还很小），洋葱也会考虑更新，由此也可看到，人工智能领域竞争的十分激烈。\n数信互融借助大数据和机器学习做互联网金融风控，乐驾科技则研发了基于语音与手势操控的HUD智能车载机器人，语音交互技术来源于思必驰公司。期待半年之后可以看到他们更详细的应用介绍。\n数据分析\n数据分析方面想提日志易和IT桔子，面向运维监控和创业投资决策的很实用的项目。日志易针对的是来源和格式杂乱的机器数据，日志易创始人陈军表示，“日志易就是机器大数据的搜索引擎”，可以用于运维监控的实时分析、日志挖掘，通过日志分析发现APT攻击，对安全审计也有用。\n日志易的研发团队有来自BAT的成员，也有来自Splunk的工程师，其系统可以搜索、分析任何文本格式的日志，支持TB级的处理，已经申请12项技术发明专利，并且采用Spark大数据处理框架获得了Databricks的认证。\n日志易的实际应用速度也很快，陈军表示：\n日志易在去年10月份上线发布，已经有几十个大的企业用户，以及近千家中小企业。有陌陌、乐视、网宿这样的互联网企业，也有国家开发银行、平安集团这样的金融集团；另外日志易提供SaaS版，已经在小米云、京东云、金山云、腾讯云等国内主流云平台上面提供分析服务。\nIT桔子则是一个互联网创业投资信息数据服务平台，已收录和实时跟踪国内创业公司20,000多家，为创业者、投资人提供创业信息和数据支持服务。IT桔子创始人文飞翔表示，IT桔子的价值主要在2个方面：\n有最新的创业公司信息，包括团队、业务、竞争对手等等，让投资人可以快速的了解创业项目。\n可以帮助投资人挖掘细分行业的投资机会，IT桔子已经覆盖十多个大行业，通过大公司格局帮助大家快速了解投资重点行业。\n云平台\nAPICloud平台用户数已超过2000万，其创始人兼CEO刘鑫表示，APICloud的核心理念就是一个“快”字，支持移动应用快速迭代。APICloud采用的方式就是“云端一体”，为开发者从“云”和“端”两个方向提供API，简化移动应用开发，可以减少云应用开发过程中70%的工作量。APICloud提供了三个产品：跨平台应用引擎，移动平台，和模块STORE。\n跨平台应用引擎：将苹果和安卓应用大量复杂的、常用的功能进行平台化，将一个应用拆分成导航、菜单。\n移动PaaS平台：用来解决云平台和开发，包括数据存储、统一分析、版本管理以及一些创新性的云的功能。功能特色，比如数据存储，开发者不再需要关注代码，只需要在线设置数据库，数据表可以全自动帮助他生成服务器端，从而提升服务器的开发效率。\n模块STORE：APICloud第一款生态性的产品，可以让第三方的云服务在APP中落地更快更直接。比如即时通讯用到分享功能，APICloud都是一站式的提供。\n可见，中国云计算市场PaaS层企业过得相对艰难，或许面向移动和智能联网设备的PaaS是中国PaaS的福音。\n然而，PaaS还要面临新的CaaS（容器即服务）的挑战，那就是灵雀云在做的事情。灵雀云创始人兼CEO左玥表示，IaaS平台配置繁琐，PaaS平台有更多的限制，相对复杂的应用不能运行起来，灵雀云以Docker技术为核心，在这个平台上，开发人员只需要关注应用的服务器，以及连接的方式，彻底忘记虚拟机和配置服务，复杂应用也可以在灵雀云上使用。包括一键部署，负载均衡，自动报警等。灵雀云今年6月份上线，其客户包括微软、金山等世界级云平台，还有知名的O2O平台，以及千万用户的游戏交友平台。\n其他\n最后还想提一款社交型个人CRM——客脉，比较有意思的是，团队CEO是移动CRM公司纷享销客的创始人兼CEO罗旭，但客脉是面向个人的，围绕维护老客户和开发新客户的需求。通过客脉，销售人员可以随时在手机上查阅客户资料，维护客户信息，记录和客户沟通的过程，记录业绩和提成，查看费用。当然，更重要的是这些信息不会随着销售人员的工作变动而丢失。\n客脉还提供有数据查询功能，自动整合搜索引擎、招聘网站、工商数据网站等，帮助销售找到客户信息。如果这些信息不全，销售人员可以在“客脉圈”中通过匿名问答的形式进行沟通，在客脉圈中匹配客户，以及交流相同客户的需求与痛点。\n第七期入选初创企业名单公布\n与往常一样，微软创投加速器最后公布了第七期团队名单，从1000多家申请企业中杀出重围的20家企业，仍然不乏许多致力于将数据分析、人工智能、移动技术应用于金融、健康、环境、安全等不同领域的公司。\n现场公布微软创投加速器第七期入选初创企业\n20家初创企业名单及创业方向如下：\nGrowingIO：用数据驱动企业增长，提供全自动移动互联网数据分析决策系统\n禾赛科技：激光气体检测和智慧城市环境大数据平台\n云报销：解决企业报销、审批、费控等财务流程问题的 SaaS 应用\n助理“来也”：最贴心的私人助理，通过对话和推荐技术连接人和本地商业\n微猫：国内第一家服务APP的移动电商云平台\n众量网：众包式量化投资策略市场和基金\n万卡：下一代智能金融IC卡，移动支付的安全卫士\nVIPHRM：基于SaaS平台，免费为全国中小企业提供一站式HR云端服务\n欣兆阳科技：围绕企业的客户旅程，对营销活动进行自动化、个性化，跨渠道管理的营销云\n融云：开发者首选的即时通讯云服务\n乐驾科技：基于HUD+语音操控人机对话的智能车载机器人\nPing++：移动应用开发首选的支付 SDK\n洋葱：基于云和用户生物特征的身份验证服务\n赛福基因：基于人类全基因组大数据精准解析的疾病筛查、用药指导及个性化健康管理\nAbleCloud：支持智能联网设备开发与管理的的PaaS平台\n数信互融：为互联网金融行业提供精准的风险定价及资产量化服务，提升风控水平、促进资产流动\ne税客：全国中小企业的手机办税及发票服务\nMaxent：最准确最可信赖的设备指纹技术专家\n侯斯特：微信公众号应用商店\n玲珑沙龙：服务于城市女性的兴趣沙龙移动社区\n结语\n微软创投加速器自2012年7月在中国启动，到现在已是三周年，已经毕业的106家企业的产品及服务，覆盖5亿个人用户和过百万企业用户，整体估值达234亿人民币，可见上述技术方向的代表性"}
{"content2":"人工智能-数学基础视频课程—562人已学习\n课程介绍\n数据科学与人工智能数学基础课程旨在帮助同学们打下数学基础，通俗讲解其中每一个知识点。课程内容涉及高等数学，线性代数，概率论与统计学，同学们在学习过程中应当以理解为出发点并不需要死记每一个公式，掌握核心知识点。课程章节内容较多，初级同学按顺序学习即可，有基础的同学们可以按照自己的需求来有选择的学习！\n课程收益\n掌握人工智能必备数学知识点。\n讲师介绍\n唐宇迪更多讲师课程\n计算机博士，专注于机器学习与计算机视觉领域，深度学习领域一线实战讲师。在图像识别领域有着丰富经验，实现过包括人脸识别，物体识别，关键点检测等多种应用的新算法。 参与多个国家级计算机视觉项目，多年数据领域培训经验，丰富的教学讲解经验，出品多套机器学习与深度学习系列课程，课程生动形象，风格通俗易懂。\n课程大纲\n第1章:高等数学基础\n1.课程简介  4:00\n2.函数  5:27\n3.极限（PPT下载----->）  6:43\n4.无穷大与无穷小  6:33\n5.连续性与导数  8:58\n6.偏导数  7:01\n7.方向导数  8:24\n8.梯度  11:26\n第2章:微积分\n1.微积分基本想法  6:06\n2.微积分的解释  8:01\n3.定积分  8:32\n4.定积分的性质  5:28\n5.牛顿莱布尼茨公式  11:36\n第3章:泰勒公式与拉格朗日\n1.泰勒公式出发点  6:13\n2.一点一世界  9:58\n3.阶数的作用  8:07\n4.阶乘的作用  5:54\n5.拉格朗日乘子法  9:53\n6.求解拉格朗日乘子法  10:05\n第4章:线性代数基础\n1.行列式概述  5:43\n2.矩阵与数据的关系  9:25\n3.矩阵基本操作  12:11\n4.矩阵的几种变换  5:30\n5.矩阵的秩  12:40\n6.内积与正交  11:27\n第5章:特征值与矩阵分解\n1.特征值与特征向量  7:25\n2.特征空间与应用  4:31\n3.SVD要解决的问题  7:17\n4.特征值分解  5:45\n5.SVD矩阵分解  11:52\n第6章:随机变量与概率估计\n1.离散型随机变量  7:49\n2.连续型随机变量  9:32\n3.简单随机抽样  2:30\n4.似然函数  2:30\n5.极大似然估计  7:34\n第7章:概率论基础\n1.概率与频率  6:50\n2.古典概型  6:23\n3.条件概率  8:33\n4.条件概率小例子  5:35\n5.独立性  7:15\n6.二维离散型随机变量  8:03\n7.二维连续型随机变量  5:29\n8.边缘分布  9:36\n9.期望  4:20\n10.期望求解  8:38\n11.马尔科夫不等式  8:35\n12.切比雪夫不等式  11:14\n13.后验概率估计  10:04\n14.贝叶斯拼写纠错实例  11:46\n15.垃圾邮件过滤实例  14:09\n第8章:数据科学必备分布\n1.正态分布  19:23\n2.二项式分布  11:02\n3.泊松分布  15:55\n4.均匀分布  3:22\n5.卡方分布  5:35\n6.beta分布  14:54\n第9章:核函数变换\n1.核函数的目的  6:37\n2.线性核函数  5:43\n3.多项式核函数  4:34\n4.核函数实例  6:53\n5.高斯和函数  8:51\n6.参数的影响  8:36\n第10章:熵与激活函数\n1.熵的概念  4:50\n2.熵的大小意味着什么  12:09\n3.激活函数  6:30\n4.激活函数的问题  9:59\n第11章:回归分析\n1.回归分析概述  7:11\n2.回归方程定义  4:42\n3.误差项定义  7:48\n4.最小二乘法推导与求解  12:41\n5.回归方程求解小例子  6:32\n6.回归直线拟合优度  11:08\n7.多元与曲线回归分析  8:26\n8.python工具包介绍  5:01\n9.statsmodels回归分析  9:38\n10.高阶与分类变量实例  12:06\n11.汽车价格预测任务概述  9:19\n12.缺失值填充  13:36\n13.特征相关性  13:47\n14.预处理问题  7:05\n15.回归求解  13:23\n第12章:假设检验\n1.假设检验基本思想  12:28\n2.左右检验与双侧检验  14:20\n3.Z检验基本原理  7:03\n4.Z检验实例  14:06\n5.T检验基本原理  13:02\n6.T检验实例  6:17\n7.T检验应用条件  7:43\n8.卡方检验  11:28\n9.假设检验中的两类错误  10:01\n10.python假设检验实例  12:34\n11.python卡方检验实例  7:59\n第13章:相关分析\n1.相关分析概述  9:03\n2.皮尔森相关系数  8:16\n3.计算与检验  13:05\n4.斯皮尔曼等级相关  14:06\n5.肯德尔系数  6:48\n6.质量相关分析  13:33\n7.偏相关与复相关  7:34\n第14章:方差分析\n1.方差分析概述  6:48\n2.方差的比较  11:50\n3.方差分析计算方法  14:00\n4.方差分析中的多重比较  8:15\n5.多因素方差分析  9:25\n6.python方差分析实例  8:34\n第15章:聚类分析\n1.层次聚类概述  4:41\n2.层次聚类流程  12:10\n3.层次聚类实例  11:33\n4.KMEANS算法概述  11:33\n5.KMEANS工作流程  9:42\n6.KMEANS迭代可视化展示  8:19\n7.DBSCAN算法  11:03\n8.DBSCAN工作流程  15:03\n9.DBSCAN可视化展示  8:52\n10.多种聚类算法概述  4:34\n11.聚类案例实战  17:19\n第16章:贝叶斯分析\n1.贝叶斯分析概述  7:22\n2.概率的解释  6:06\n3.贝叶斯学派与统计学派的争论  5:49\n4.贝叶斯算法概述  6:58\n5.贝叶斯推导实例  7:37\n6.贝叶斯解释  10:50\n7.经典求解思路  8:16\n8.MCMC概述  11:03\n9.PYMC3概述  5:40\n10.模型诊断  9:53\n11.模型决策  10:48\n大家可以点击【查看详情】查看我的课程"}
{"content2":"计算机视觉方面的三大顶级国际会议：ICCV，CVPR和ECCV，统称为ICE。\nICCV（International Comference on Computer Vision）\n正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。\nCVPR（International Conference on Computer Vision and Pattern Recogintion）\n这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。\nECCV（Europeon Conference on Computer Vision）\n是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster挑自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。\nICCV/CVPR/ECCV三个顶级会议, 都在一流会议行列, 没有必要给个高下. 有些us的人认为ICCV/CVPR略好于ECCV,而欧洲人大都认为ICCV/ECCV略好于CVPR。\n笔者就个人经验浅谈三会异同, 以供大家参考和讨论. 三者乃cv领域的旗舰和风向标,其oral paper (包括best paper) 代表当年度cv的最高水准, 在此引用Harry Shum的一句话, 想知道某个领域在做些什么, 找最近几年此领域的proceeding看看就知道了. ICCV/CVPR由IEEE Computer Society牵头组织, ECCV好像没有专门负责的组织. CVPR每年(除2002年)都在美国开, ECCV每两年开一次,仅限欧洲, ICCV也是每两年一次, 各洲轮值. 基本可以保证每年有两个会议开, 这样研究者就有两次跻身牛会的机会.\n就录取率而言, 三会都有波动. 如ICCV2001录取率>30%, 且出现两个人(华人)各有三篇第一作者的paper的情况, 这在顶级牛会是不常见的 (灌水嫌疑). 但是, ICCV2003, 2005\n两次录取率都很低, 大约20%左右. ECCV也是类似规律, 在2004年以前都是>30%, 2006年降低到20%左右. CVPR的录取率近年来一直偏高, 从2004年开始一直都在[25%,30%].最近一次CVPR2006是28.1%, CVPR2007还不知道统计数据. 笔者猜测为了维持录取paper的绝对数量, 当submission少的时候录取率偏高, 反之偏低, 近几年三大会议的投稿数量全部超过1000, 相对2000年前, 三会录取率均大幅度降低, 最大幅度50%->20%. 对录取率走势感兴趣的朋友, 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的),http://www.adaptivebox.net/research/bookmark/CICON_stat.html .\n显然, 投入cv的人越来越多,这个领域也是越来越大, 这点颇不似machine learning一直奉行愚蠢的小圈子主义. 另外一点值得注意, ICCV/ECCV只收vision相关的topic, 而cvpr会收少量的pattern recognition paper, 如fingerprint等, 但是不收和image/video完全不占边的pr paper,如speech recognition等. 我一个朋友曾经review过一篇投往CVPR的speech的paper, 三个reviewer一致拒绝, 其中一个reviewer搞笑的指出, 你这篇paper应该是投ICASSP被据而转投CVPR的. 就topic而言, CVPR涵盖最广. 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会, 故CVPR会优先接收很多来自us的paper (让大家都happy).\n以上对三会的分析对我们投paper是很有指导作用的. 目前的research我想绝大部分还是纸上谈兵, 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程. 故了解投paper的一些基本技巧, 掌握领域的走向和热点, 是非常必要的. 避免做无用功,选择切合的topic, 改善presentation, 注意格式 (遵守规定的模板), 我想这是很多新手需要注意的问题. 如ICCV2007明文规定不写summary page直接reject, 但是仍然有人忽视, 这是相当不值得的.\nORALS>SPOTLIGHTS>POSTERS"}
{"content2":"在计算机视觉领域的文献里经常会见到李代数和李群的概念，在此说说我个人对李代数与李群的理解。\n本人第一次遇见李代数与李群的概念是在研一时必修课《非线性控制系统理论与应用》上，在该课程中是使用李群与李代数来解决非线性控制系统精确线性化这一任务的。之后在计算机视觉课程中深入学习了双视几何，期间了解到三维旋转的不同表示方法。三维旋转的自由度为3，可以使用旋转矩阵、欧拉角、四元数和旋转向量来表示。\n旋转矩阵即是行列式等于1的正交矩阵，行列式等于-1的正交矩阵为镜面反射变换；\n欧拉角就是绕三个坐标轴转动个角度，会存在万向锁的情况使得三维旋转的自由度降为2；\n四元数和复数概念比较像，有实部和虚部的概念，实部表示旋转的角度，虚部表示旋转轴；\n旋转向量与四元数也比较像，旋转向量是一个三维向量，其方向表示旋转方向，其模表示旋转角度。\n三维旋转比较紧凑的表示方法是使用旋转向量来表示，其次是四元数。\n旋转向量与旋转矩阵之间的相互转换是通过罗德里格斯公式建立起来的：\n其中r为旋转向量，R为旋转矩阵。其反过程可有下式确定：\n简单提下关于罗德里格斯公式的证明，由理论力学关于刚体的三维旋转矩阵R随时间的变化与角速度w之间满足欧拉公式：\n该矩阵微分方程的解为\n下标x表示向量对应的反对称阵。为了简化，假设角速度为常量，则积分号可以去掉；R0为单位阵，则旋转矩阵Rt为：\n从这可以看出旋转矩阵可以通过一个三维向量来确定，三维向量与旋转矩阵之间通过矩阵指数函数建立映射关系，不过该映射不是一一映射，即存在若干个旋转向量对应一个旋转矩阵，任何一个旋转矩阵都可以找到若干个旋转向量与之对应，但是只要把旋转角度限制在一个圆周内，则为一一映射。矩阵指数函数可以按照矩阵级数展开，而凯莱-哈密顿定理告诉我们可以把该矩阵级数化为有限项的线性组合，于是有了罗德里格斯公式。这就是一个典型的李群SO（3）及其对应的李代数so（3）的实例，且李群的自由度与其对应的李代数具有相同的自由度。李群SO（3）叫三维旋转群，李代数so（3）为其对应的正切向量空间，它们之间满足微分方程，该微分方程差分化后用于计算机迭代求解。\n从使用角度看，机器视觉里面用到的李代数与李群，归根结底就是罗德里格斯公式，用旋转向量来表示三维旋转。开源库sophus里的api基本上就是四元数、旋转向量和旋转矩阵的相互转化和对点施行变换的功能。\n除此之外，还有凯莱公式（Cayley formula）可以将旋转向量转化为旋转矩阵，该公式也有说是欧拉发现的：\n其中A为旋转向量对应的反对称阵。\n值得一提的是，同一个旋转向量用凯莱公式和罗德里格斯公式转化得到的旋转矩阵通常并不相等，但都是旋转矩阵。凯莱公式还可以化简为：\n，\n其中。\n凯莱公式的逆变换为：\n或者\n使用凯莱公式计算旋转矩阵，计算量会比罗德里格斯公式小很多。\n参考：http://researchopen.lsbu.ac.uk/92/1/CayleyMap(corrected).pdf\n小例子：\nhttps://github.com/jah10527/rot3d"}
{"content2":"这两年，计算机视觉似乎火了起来计算机视觉的黄金时代真的到来了吗？。生物医学、机械自动化、土木建筑等好多专业的学生都开始研究其在各自领域的应用，一个视觉交流群里三分之一以上都不是计算机相关专业的。当然，我也是其中一员。\n对于非计算机相关专业的学生而言，学习过程中往往缺少交流机会，不容易把握知识的全貌。这里仅根据个人经验谈一谈对于一名非计算机专业的学生而言，该如何学习计算机视觉。\n1.编程能力\n1.1 编程语言(C++, python)\n刚接触CV(computer vision)（注：本文偏向于图像学而非图形学）时，大家一般都会不假思索地选择使用C++：装个VS(Visual Studio)，配置下opencv，撸起袖子就上了。这样做非常合理，几乎所有人都是这么入门的。\n不过，当你知识面扩展开后，你会感觉到很多时候C++都显得有些力不从心。比如：当你要画一些图表或做一些分析，就还得把数据导入MATLAB里做进一步处理；当你要非常快捷方便地学习或测试一个算法，C++会是你最糟糕的选择；或者当你要学习深度学习时，你绝对不会再选择使用C++….总之，有太多理由会促使你再学习一门编程语言，最好的选择没有之一：python。\n1.1.1 简单介绍一下C++和python的各自特点：\nC++：偏底层，执行效率高，适合嵌入式等平台上使用；在视觉领域，C++生态好，用的人多，网上找资源很方便。\n缺点是开发效率实在太低了，关于这一点如果你只是专注于图像处理的话可能感受不是那么真切，因为opencv库做得足够好。但是当你做到机器学习后，opencv就显得有些力不从心了，虽然它也包含一些SVM、神经网络等的简单实现，但毕竟不擅长。\npython：全能语言，干啥都行，并且都相对擅长。图像处理，opencv支持有python接口；科学计算，其功能类似于matlab了：机器学习及深度学习，python是最好用的，没有之一；爬虫等网络应用，豆瓣就是用python写的；简而言之，方便，实在太方便了。\n当然python也有自己的另一面。执行效率不高，这一点做嵌入式开发的可能比较忌讳。但如今手机的内存都升到6G了，tensorflow都可以在移动端跑了，Python也都可以用来控制STM32了，未来很难说。\n顺便说一句也有人使用MATLAB等做图像方面的研究，如果你只是偶尔用图像处理辅助一下你的研究，可以这么做，一般情况下不建议使用。\n1.1.2 C++和python学习资源推荐\nC++：大家好像都买《C++ primer》或《C++ primer plus》这样的大块头书，我自己感觉倒不如《王道程序员求职宝典》这类书实用。大块头书优点在于全面，同时也往往导致了重点不突出。码代码时不熟悉的用法一般直接在cppreference上搜就可以了，超级方便；但有些不容易理解的地方确实需要系统的找资料学习一下。课程的话推荐coursera上北大的《程序设计与算法》，第3门课程是C++程序设计。看视频课程一般比较慢，如果没什么基础或者特别想把基础学好的话，强烈推荐。\npython：基础部分看廖雪峰的python教程就可以了，然后就是用哪一块学哪一块了。python学起来很简单，看别人代码的过程就是学习的过程。对于不熟悉的用法多搜下官方文档，如python, numpy, pandas, matplot, scikit-learn。这里有几张python各种库的小抄表其实直接在网上搜这几张表也都比较方便。课程的话，我之前上过一些七月算法的课程，讲得不好，多少会给你一些知识体系和各种学习资料，总体不推荐或跳着看。python的开发环境值得说一下，因为有太多选择，这里比较建议使用pycharm和jupyter notebook吧，具体参考python入门环境搭建。\n1.2 编程平台（windows, linux）\n新手肯定都用windows了，学习过程中发现在windows上搞不定了，先忍几次，然后掉头就去学linux了。一定是这样。\n哪些在windows上真的搞不定呢？比如：deeplearning，或最新论文中提出的视觉开源算法。\n不过对我们而言，linux并不需要了解太深。装个ubuntu系统，常用的文件操作、程序编译等知道就OK了。我完全是在使用的过程中现用现学，手边常备一本书《鸟哥的linux私房菜》。\n2.视觉知识\n计算机视觉实在很广了，这里仅针对我个人知识体系来说一说。\n现在比较热门的方向总体上分为两大块：一块是深度学习，一块做SLAM。它们的研究点区别在哪呢？深度学习这一群体侧重于解决识别感知（是什么）问题，SLAM侧重于解决几何测量（在哪里）问题ICCV研讨会：实时SLAM的未来以及深度学习与SLAM的比较。拿机器人来说，如果你想要它走到你的冰箱面前而不撞到墙壁，那就需要使用 SLAM；如果你想要它能识别并拿起冰箱中的物品，那就需要用到深度学习机器人抓取时怎么定位的？用什么传感器来检测？。当然这两方面在research上也有互相交叉融合的趋势。\n不过在学习这些之前，一般都会先掌握下传统的计算机视觉知识，也就是图像处理这一部分了。我之前大致总结过一次：\n计算机视觉初级部分知识体系。这些基础知识的理解还是挺有必要的，有助于你理解更高层知识的本质，比如为什么会出现deeplearning等这些新的理论知识（感觉有点像读史了，给你智慧和自由）。这一部分学习资料的话还是挺推荐浅墨的《OpenCV3编程入门》 也可以看他的博客。当然他的书有一个问题就是涉及理论知识太少，所以推荐自己再另备一本偏理论一点的图像处理相关的书，我手边放的是《数字图像处理：原理与实践》，差强人意吧。个人之前看浅墨书的时候做了一份《OpenCV3编程入门》学习笔记，里边包含一些理论知识和个人见解。\n下面说一下两个大的方向：基于深度学习的视觉和SLAM技术。\n基于深度学习的视觉：机器学习包括深度学习里的大部分算法本质上都是用来做“分类”的。具体到计算机视觉领域一般就是物体分类（Object Classification）、目标检测（Object Detection）、语义分割（Image Semantic Segmentation）等，当然也有一些很酷又好玩的东西比如edges2cats、deepart。本人主要做一些Object Detection相关的东西。其实一般是直接跑别人的代码了，稍微做一些修改和参数调整，前期的预处理才是主要工作。这些程序基本都是在linux下跑的。好，深度学习为什么这么强？它主要解决了什么问题呢？我比较认同以下三点：学习特征的能力很强，通用性强，开发优化维护成本低 参见为什么深度学习几乎成了计算机视觉研究的标配？。\n关于这一部分的学习，主要就是deeplearning了。关于deeplearning，漫天飞的各种资源。可以看一看李宏毅的一天搞懂深度学习课件 youtube上有一个一天搞懂深度學習–學習心得；李飞飞的CS231n课程，网易云课堂有大数据文摘翻译的中文字幕版课程，知乎专栏智能单元有CS231N课程翻译（非常好）；三巨头之一Yoshua Bengio的新作《DEEP LEARNING》，目前已有中译版本 。\nSLAM技术：这一部分我了解不多，只是听过一些讲座。可以关注下泡泡机器人 公众号吧，他们公开课出得挺多的；听说高博的新书快出了，我也想赶紧入手偷偷学一下。\n3.机器学习\n计算机视觉中使用的机器学习方法个人感觉不算多，早期的时候会用SVM做分类，现在基本都用深度学习选特征+分类。原因在于统计机器学习这一块虽然方法不少，但是基本都无法应对图像这么大的数据量。\n不过大家在学习过程中很容易接触到各种机器学习方法的名字因为现在大数据分析、机器学习、语音识别、计算机视觉等这些其实分得不是很开，然后不自觉地就会去了解和学习。这样我感觉总体来说是好的。不过在学习一些暂时用不着的算法时，个人感觉没必要做的太深：重在理解其思想，抓住问题本质，了解其应用方向。\n下面分开介绍一下传统机器学习算法和深度神经网络。\n传统机器学习一般也就决策树、神经网络、支持向量机、boosting、贝叶斯网等等吧。方法挺多的，同一类方法不同的变形更多。除了这些监督式学习，还有非监督学习、半监督学习、强化学习。当然还有一些降维算法（如PCA）等。对这些个人整体把握的也不是特别好，太多了。\n学习资料，吴恩达的coursera课程《Machine Learning》，他正在出一本新书《MACHINE LEARNING YEARNING》，说好陆续更新的，刚更新一点就没了，本来想翻译学习一下。个人比较喜欢他的课程风格话说今天中午传出新闻，吴恩达从百度离职了。——执笔于2017.03.22，简单易懂。还有李航的《统计学习方法》和周志华的《机器学习》，两本在国内机器学习界成为经典的书。\n深度学习说着感觉有点心虚，哈哈总共就这几年就那些东西，资料上面视觉知识部分已经说过了，听听课程、看看那些出名的模型框架，基本上也就了解了《一天搞懂深度学习》其实就已经把大部分都给说了，不过个人感觉还是挺难理解的。主要的发展也就CNN、RNN；从去年起GAN火起来了，现在如日中天；增强学习现在发展也非常快，有些名校如CMU都开这方面课程了。\n资料上面说过就不说了喜欢高雅的人也可以看看这个深度学习论文阅读路线图 ，说说在使用deeplearning时用哪个库吧。目前为止还没有大一统的趋势，连各个大公司都是自己用自己开发的，一块大肥肉大家都不舍得放弃。我只用过keras和tensorflow，感觉在这方面没必要太计较，用相对简单的和大家都用的（生态好） 。\n4.数学\n一切工程问题归根结底都是数学问题，这里说说计算机视觉和机器学习所涉及的数学问题。\n微积分：比如图像找边缘即求微分在数字图像里是做差分（离散化）啦，光流算法里用到泰勒级数啦，空间域转频域的傅立叶变换啦，还有牛顿法、梯度下降、最小二乘等等这些都用的特别普遍了。其实个人感觉CV所涉及的微积分知识相对简单，积分很少，微分也不是特别复杂。也可能是本科那会儿力学学怕了吧。\n我好像没备微积分的资料，如果需要的话，同济大学出的本科教材应该也够用了吧。\n概率论与统计：这个比较高深，是应用在机器学习领域里最重要的数序分支。应用比如：条件概率、相关系数、最大似然、大数定律、马尔可夫链等等。\n浙大的《概率论与数理统计》我感觉还行，够用。\n线性代数与矩阵：数字图像本身就是以矩阵的形式呈现的，多个向量组成的样本也是矩阵这种形式非常常见，大多机器学习算法里每个样本都是以向量的形式存在的，多个矩阵叠加则是以张量(tensor)的形式存在google深度学习库tensorflow的字面意思之一。具体应用，比如：世界坐标系->相机坐标系->图像坐标系之间的转换，特征值、特征向量，范数等。\n推荐本书，国外的上课教材《线性代数》。因为浙大的那本教材感觉实在不太行，买过之后还是又买了这本。\n凸优化：这个需要单独拎出来说一下。因为太多问题（尤其机器学习领域）都是优化问题（求最优），凸优化是里面最简单的形式，所以大家都在想办法怎么把一般的优化问题转化为凸优化问题。至于单纯的凸优化理论，好像已经比较成熟了。在机器学习里，经常会看到什么求对偶问题、KKT条件等，潜下心花两天学一学。\n建议备一份高校关于凸优化的教学课件，大家对这一块毕竟比较生，缺乏系统感。比如北大的《凸优化》课程。\n这些数学知识没必要系统学习，效率低又耗时。毕竟大家都有本科的基础，够了。一般用到的时候学，学完之后总结一下。如果真想学习的话，七月在线有个课程《机器学习中的数学》，讲的一般，倒不妨看一看。\n介绍个小trick，之前学习好多数学知识或算法时，看不懂教材上晦涩死板的讲解，一般都会搜索“XXX 形象解释”，往往都会搜到些相对通俗易懂的解释也往往都是在知乎上搜到的这些解答，比如拉格朗日乘子法如何理解？, 如何通俗并尽可能详细解释卡尔曼滤波？ 。\n5.授之以鱼不如授之以渔\n编程能力->计算机视觉->机器学习->数学知识，前文已经把所要学习的知识基本都介绍完了。不知道你有没有冒出疑问：你怎么知道的这些？你平时怎么学习的？\n先说第一条：时间，时间的积累。讲个故事，去年暑期在华东师大参加一个关于ROS（Robot Operate System, 机器人操作系统）的Summer School。顺便提一句，主办者张新宇老师人特别nice。第一天上午的speaker叫Dinesh Manocha，Canny的学生。对，就是Canny边缘检测算法的Canny。Dinesh教授有一个保持了几十年的习惯：（平均）每天只睡4个多小时。用张新宇老师的一句总结就是：智力超群、体力超群、习惯超群。他还提到，未来中国要跟国外竞争，一定程度上就是体力的竞争。因为相比老外目前中国人在这方面不太重视。呃，，，反正我是弱的不行。应该加强的。\n当然，在具体学习方法也有一些trick，不然怎么解释有的人效率高呢。当然聪明和底子能够解释部分原因。现在我就说一说自己学习过程中的小trick。\ngoogle搜索。时代变了，一百年前的人类绝对想像不出自己有了困惑不是去翻书或请教他人而是告诉身旁的一台机器。如今，小学生做道算术题或小女生来个大姨妈都要问问电脑：这是怎么回事。但这些与学视觉又有什么关系呢？——答：没有。好像跑偏的有点多了，再扯远一点吧。跨越时间维度来思考一些新事物的发生及其与旧事物的联系，也许会给你一种想象的自由。比如电报、电话、视频聊天和全息通话用 HoloLens 通话 ，只是举例，我可没说以后这种技术真会普遍应用。，马车、汽车、火车、飞机和火箭太空旅行，蒸汽机、电、互联网和AI。\n百度搜索太烂了（当然，它本地化搜索做得不错。并且我也没说完全是技术原因），有多烂？我认为它跟google搜索的差距不是1:2，是1:10。这一点好像不应该说这么多，大家都公认的。问题根源在于“中国特色”不允许我们使用google搜索，这里介绍一个非常方便的科学上网工具lantern（链接是它的github地址，官网墙内好像登不了。）。下载完安装之后直接运行即可。\n还有一点，多使用英文搜索，这样呈现在你眼前的才是完整的世界。英文世界里优秀、原创资源多，浏览网页时不经意间也会遇到些好网站。比如曾经surf到一个计算机视觉方面的博客Learn OpenCV，通俗易懂，不频繁更新，几乎每篇文章必看。\n交流。这里特制人与人之间的交流，最好是面对面聊天。这样的好处是随意性大，随便一句话就可能指出你长期存在某个误区。对于我们（非计算机专业学生）而言，最缺的就是这种交流环境。所以大家只能尽量弥补了，比如通过各种途径认识点计算机专业或视觉方向的同学（蹭学校计算机视觉的课程）；多加点相关的公众号，QQ、微信群不好的再删，当然自己也要主动参与这些社区。\n书。好书基本上都是公认的，并且适合大部分人。有些人买书可能会有选择恐惧症，这一点，，，摆正心态吧，很多时候买书本来就不是为了读完，只要能给你一两次惊喜或节约你几小时宝贵时间，它的使命就已经完成了，值！！！当然买书也讲究个度，这个就如人饮水、冷暖自知了。\nPPT。PPT的出现在一定程度上对传统教材产生了冲击，方便，重点突出，体验舒服。个人几乎会把学习的所有课件都保存在ipad里推荐使用非常出名的备注记录软件Notability来保存和编辑你的PPT，听课时可以在上边做笔记，课后如果需要随时温故而知新。\n“一句话”抓住问题本质。算法太多，学过就忘。这可能是所有人遇到的问题。尤其对于那些学的不是特别深入的算法，倘或跟人聊起都不知道如何解释。“一句话”解释，就是用简单的几句话把一件事说清楚。比如《统计学习方法》里李航就提出统计机器学习的三要素：模型、策略和算法，针对某种机器学习方法根据这三要素梳理一下，你就已经把握到整体了，即使其中有些细节不理解也无伤大雅。想象一下如果有同学指着你桌上的书问你“机器学习是什么？”，你会不会一脸懵逼？我会，O(∩_∩)O。说一下个人理解，至少听起来是句人话：机器学习就是让机器学会自学，对已有信息进行归纳和识别，并自主获得新技能的能力。相比于传统计算机编程里直接告诉计算机“什么时候做什么”，机器学习通过“不显式编程”赋予计算机能力，即提供一些案例（训练数据）,让计算机通过案例自己学习什么时候应该做什么。\nA4纸学习法。平常的一个个人习惯吧，感觉对自己比较有用，分享一下。对于某些算法，有时候可以自己花半天、一天或者两天动手推导一下，然后A4纸总结整理一下放文件夹里，备日后翻阅。这样有助于提升你的数学能力，加深对算法的理解。\n学习新技能，讲究效率。在大家智力、体力水平都相当的情况下，怎么比别人学得更快更好？这里介绍一个自己快速学习一项新技能的方法：花两周时间把两本书看两遍。具体解释是：单位时间内，把两本书看一遍不如把一本书看两遍，在不确定哪本书具有绝对优势时最好两本书都看（不要把鸡蛋放进一个篮子里）。当然，一定要快！！！对于写代码而言，看书的同时实践也是非常重要。\n6.工作\n这一点好像跟学习本身关系不大，但跟大多数学习者本身（比如我）关系很大。\n花开两朵，各表一枝。\n不少人可能跟我一样都是冲着现在计算机视觉很火、有前景又比较感兴趣，所以选择学计算机视觉，并且以后想要从事计算机视觉这方面的工作。。一定要摆正心态，找工作时可能就要跟那些计算机专业的学生们竞争了；最好从现在起，就把自己当一名程序员看待。当然你也有自己的优势，你拥有自己专业的领域知识这对某些公司来说很重要，你找工作时基本上也都应该重点考虑这些公司。，你对视觉的具体应用本身也比较了解；劣势是你缺乏计算机专业的基本素养，具体到笔试或面试中就是你基础编程能力不行。\n说到这里，大家应该都听说过“刷题”这回事。程序员应聘的特点之一就是首先面试者会考查一些基础的算法题，借此评估一下你的基本编程能力。其实计算机专业的学生在工作季前也要在leetcode等平台上刷刷题练练手，不然他们也过不了第一关。不过，对于我们非计算机专业学生而言，刷题前最好系统学习下数据结构和算法这两门课。程序=数据结构+算法，前面提到的北大《程序设计与算法》专项课程里就有这两门课。然后就是苦练刷题技能了，刷题过程中注意多总结吧。（目前我也刚走到这一阶段，所以不好多说。）\n当然我相信也有一部分人毕业之后就再也不会接触这些破玩意儿，挺好的。三十而立之年，如果我还在整天苦逼地码代码，，，呃，不敢想象，那一定不是我想要的生活。对于这些人而言，计算机视觉可能会成为你人生中的一项常识——五年后的某一天，当你坐上无人车时，一点都不会感到惊讶。当然，也祝愿它会给你的人生带来更多改变，你所学的专业对你思维上最大的影响是什么？\n说完了，有用或没用的、该说或不该说的、跟视觉相关或不相关的都说了好多，收个尾：管理好自己，。\n还有，，，如果你诚心正意把计算机视觉作为个人事业并严肃认真对待的话，可以看下这篇文章《初探计算机视觉的三个源头、兼谈人工智能｜正本清源》，知道计算机视觉不是只有现在的深度学习。\n转自：http://blog.csdn.net/NNNNNNNNNNNNY/article/details/64240575?locationNum=13&fps=1。"}
{"content2":"﻿﻿\n人工智能自降生以来就仿佛自带光环、备受瞩目。尤其是今年，国务院印发的《新一代人工智能发展规划》中提出，到2030年使中国人工智能理论、技术与应用总体达到世界领先水平，成为世界主要人工智能创新中心，更使得人工智能将引爆智能投资革命成为行业及大众议论的焦点。\n作为人工智能（AI）的三大领域之一的计算机视觉近期也之间越来越受到关注，在今年7月，商汤科技更是以B轮4.1亿美元的融资创造了AI圈单笔最高融资记录。那么国内有哪些计算机视觉公司值得关注呢？我们来盘点下。\n计算机视觉与机器视觉\n计算机视觉是一门研究如何使机器“看”的科学，间的单说，就是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉也可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学。它的最终研究目标就是使计算机能象人那样通过视觉观察和理解世界，具有自主适应环境的能力。\n机器视觉就是用机器代替人眼来做测量和判断。机器视觉系统是通过机器视觉产品（即图像摄取装置，分CMOS和CCD两种）将被摄取目标转换成图像信号，传送给专用的图像处理系统，得到被摄目标的形态信息，根据像素分布和亮度、颜色等信息，转变成数字化信号;图像系统对这些信号进行各种运算来抽取目标的特征，进而根据判别的结果来控制现场的设备动作。\n从学科分类上，二者都被认为是AI下属科目，不过计算机视觉偏软件，通过算法对图像进行识别分析，而机器视觉软硬件都包括（采集设备，光源，镜头，控制，机构，算法等），指的是系统，更偏实际应用。简单的说，我们可以认为计算机视觉是研究“让机器怎么看”的科学，而机器视觉是研究“看了之后怎么用”的科学。\n随着硬件、算法及大数据的不断发展，整个人工智能领域面临前所未有的规模增长，也促使了国外的许多创业公司被大公司收购。　同时图像识别的能力越来越强，错误率越来越低，国内也陆续爆发了大批优秀的计算机视觉（ComputerVision）创业公司。\n1、SenseTime商汤科技：教会计算机看懂这个世界\n提到计算机视觉，最火的莫过于商汤科技，该公司在今年7月宣布完成4.1亿美元B轮融资，创下公开报道中全球人工智能领域单轮融资最高纪录。\n罗马不是一日建成的，人工智能也不可能横空出世。商汤在深度学习领域经历了一个长期艰苦积累的过程。\n80后徐立是商汤联合创始人、CEO，也是一位计算机视觉科学家。2010年，徐立于香港中文大学攻读博士学位，与深度学习视觉领域应用的先驱——汤晓鸥教授以及其带领的香港中文大学多媒体实验室的师兄弟联系紧密。\n2011年，汤晓鸥、徐立所在实验室的几十个博士、教师开始研究深度学习。这是学术界最早涉猎深度学习的华人团队。2011至2013年间，在CVPR和ICCV两大全球计算机视觉世界顶级学术会议上，29篇涉及深度学习的文章中，有14篇出自该实验室。这个团队，成为了后来商汤的中坚力量。\n2014年，商汤首次出征人工智能领域权威竞赛ImageNet，在大规模物体检测比赛中就以40.7%的成绩荣获世界亚军，战胜微软、百度等企业，仅次于谷歌。\n2015年，在ImageNet竞赛新增的视频物体检测任务中，商汤联合香港中文大学多媒体实验室组成的团队，在30个类别的物体识别准确率PK中获得28个胜利，以压倒性优势夺冠。\n2016年，商汤更是在ImageNet的五项竞赛里取得了三项冠军。同年，由商汤参与的人工智能团队与麻省理工、斯坦福等著名大学一道，入选世界十大人工智能先锋实验室。\n据悉，商汤致力于引领人工智能核心“深度学习”技术突破，构建人工智能、大数据分析行业解决方案。商汤成功聚集了当下华人中较有影响力的深度学习、计算机视觉科学家。在人工智能产业兴起的大背景下，商汤集团凭借在技术、人才、专利上超过十年的积累，迅速成为了人工智能行业领军企业之一。\n2、旷视科技：让机器看懂世界\n旷视科技是国内计算机视觉公司的另一个明星。\n资料显示，北京旷视科技有限公司（Face++）成立于2012年11月，公司专注于人脸识别技术和相关产品应用研究，面向开发者提供服务，能提供一整套人脸检测、人脸识别、人脸分析以及人脸3D技术的视觉技术服务，主要通过提供云端API、离线SDK、以及面向用户的自主研发产品形式，将人脸识别技术广泛应用到互联网及移动应用场景中。\nFace++通过和众多互联网公司合作，并通过“脱敏”技术掌握到了500万张人脸图片数据库，在互联网图片人脸识别LFW的准确率达到99.6%，合作伙伴包括阿里、360等一批大型的图片、社交、设备类企业。\n2014年成了旷视的分水岭，在“摸着石头过河”中，他们发现用人工智能为传统行业赋能具有商机，从而认定了toB（对企业）的商业模式，在互联网、金融、安防、智能楼宇、智能零售等领域铺开业务。\n2016年7月，前微软亚洲研究院首席研究员孙剑加入旷视，引起业内热议。\n旷视的发展也获得了其他行业的高度认可。作为国内最先发布的两款人脸解锁手机，无论是vivo还是小米都选择了与旷视（Face++）进行深度合作，看重的就是旷视科技的雄厚技术实力。两款手机也均可实现500毫秒级别的高速刷脸解锁，明显超越了国际品牌的同档机型。旷视（Face++）云事业副总裁吴文昊表示，刷脸和手机的结合也不会止步于解锁，旷视Face++目前正在打造一套适配整个手机平台的智能视觉解决方案，以解决不同手机厂商在图像增强、相机增强、智能图像和视频处理上的需求。\n毫不夸张的说，从智能手机到智慧手机，旷视（Face++）正在用AI助力手机产业升级！\n3、云从科技：源自计算机视觉之父的人脸识别技术\n2015年4月份，周曦在重庆成立云从科技，作为国内众多的人脸识别初创公司之一，这是一家从“出生”就贴上“国家队”标签的公司，也是唯一一家同时受邀制定人脸识别国家标准、公安部标准、行业标准的企业。团队的创始人员基本来自中科院，也是中科院唯一人脸识别代表团队，参与国家战略性先导科技A类专项，负责人脸识别研究和应用。\n资料显示，广州云从信息科技有限公司即云从科技是一家专注于计算机视觉与人工智能的高科技企业，核心技术源于四院院士、计算机视觉之父——ThomasS.Huang黄煦涛教授。核心团队曾于2007年到2011年6次斩获智能识别世界冠军，得到上市公司佳都科技与香港杰翱资本的战略投资。\n该公司主要技术团队来自中国科学院重庆分院，是中科院研发实力最雄厚的人脸识别团队，并作为中科院战略性先导科技专项的唯一人脸识别团队，代表参与了新疆喀什等地安防布控。\n凭借核心算法、大数据等资源，云从科技已经居于国内人工智能行业的领军地位。\n据报道，目前国内有50多家银行、80%以上的民航枢纽机场在使用云从科技的远程身份认证、人脸与证件照比对识别等产品，22个省（直辖市）采用云从科技的安防技术。2016年，公司实现产值1.5亿元。\n此前，国家发改委确定云从科技与百度、腾讯、科大讯飞共同承担国家人工智能重大工程——“人工智能基础服务资源公共服务平台”建设。其相关负责人透露，下一步，云从科技还将进军教育、医疗、智慧社区等领域，让技术为民生服务。\n4、依图科技：与您一起构建计算机视觉的未来\n成立于2012年9月，并获得了真格基金创始人徐小平100万美元天使投资，在2015年1月获得了高榕资本的数百万美元A轮融资。目前致力于计算机视觉、图像视频智能理解和分布式系统及大数据应用的研究，为用户提供基于图像视频理解的计算机视觉产品。\n依图在成立之初是做车辆识别的，主要与江苏、福建和成都等公安系统合作，之后开始在人脸识别应用上，开发了静态人像对比系统，并将产品的重点之一转向金融，与车辆物体识别相比，人脸识别的应用范围和场景更加广泛，当然难度也会高很多，今年上海金融展上，依图与招商银行推出刷脸取款支付功能。\n2017年5月15日，依图科技宣布得到高瓴资本领投，云锋基金、红杉资本、高榕资本以及真格基金等跟投的3.8亿元C轮融资，最新一轮融资将主要用于人工智能技术在医疗行业的推进。\n与知名度颇高的商汤科技、旷视科技等公司相比，2012年成立的依图科技称得上低调。有人曾建议依图科技创始人兼CEO朱珑加强宣传，但他觉得使公司“全民皆知不一定有意义”。\n5、超多维：让冰冷的机器看懂多彩的世界\n与其他计算机视觉公司相比，超多维的成立时间则要早很多。\n资料显示，深圳超多维科技集团成立于2004年，是全球领先的智能计算视觉生态体系的开创者和引领者。它是目前国内唯一一家以“智能3D识别技术”为创新基础，涵盖“专利授权、3D屏幕制造、3D摄像头制造、3D手机等个人消费电子产品及3D智能图像计算” 的国际创新型企业。\n超多维在智能计算视觉技术领域具有持续的研发能力，凭借十余年的深厚技术沉淀，走出了一条独有的中国企业自主创新之路。超多维拥有覆盖计算视觉关键技术领域——3D显示产业链的完整专利布局。迄今，该公司已在全球范围内申请专利超过1000余项，82%以上为发明专利，专利授权率高达91%。其中，3D显示技术专利数量为中国第一，世界第二。2013年，超多维跟踪式裸眼3D显示技术获得“中国国家技术发明一等奖”，是深圳首家获此国家级最高技术殊荣的企业。\n2016年12月，超多维完成对ivvi手机的并购，进一步完善了其在个人消费电子终端硬件体系的搭建和战略布局，逐步搭建起以核心技术优势为基石，完善的智能计算视觉生态体系（包括裸眼3D、VR、AR、视觉信息模糊计算、AI等）。\n其相关负责人介绍，超多维将加速开发和布局以深度学习、图形图像计算、模糊逻辑计算等核心技术为支撑，具有创新性的个人消费类市场互联网应用及行业垂直领域的专业应用。运用超多维计算视觉综合解决方案，提供与人类视觉感知习惯更加融合的智能化、自然化、人性化、娱乐化的全新极致应用体验，并且在医疗、教育、体验商城、设计等垂直应用领域综合布局。\n据悉，超多维将充分发挥其在信息采集、图形图像运算、智能3D及虚拟显示等方面的技术优势，形成以核心技术为支撑，具备3D及虚拟显示技术的智能硬件为载体，新一代计算视觉互联网及专业应用为基石的产业生态闭环。并以此为牵引，透过与多层次资本市场的互动，与产业链上下游企业进行全方位开放式深度合作和提供扶植，进一步完善布局全球领先的智能计算视觉生态体系，带动整个计算视觉产业链的升级与发展。\n6、格林深瞳：让计算机看懂世界\n格灵深瞳是成立于2013年4月，由创始人赵勇在北京创立，格灵深瞳是一家同时具备计算机视觉和深度学习技术以及嵌入式硬件研发能力的人工智能公司，作为一家视频大数据产品和方案提供商，自主研发的深瞳技术在人和车的检测、跟踪与识别方面居于世界领先水平，公司主要关注的领域包括公共安全、智能交通、金融安防等，同时公司在无人驾驶、机器人和智能医疗方面也进行了深入的布局。\n其创始人赵勇毕业于美国布朗大学计算机工程系，2013年创办北京格灵深瞳信息技术有限公司，在格灵深瞳之前，赵勇主要的工作经历是谷歌总部研究院任资深研究员。\n格灵深瞳已经推出威目视图大数据分析平台，以及威目车辆特征识别系统、威目视频结构化系统、威目人脸识别系统，能够辨识超过4000种车辆，支持车辆和人体的细分特征识别，同时具备人脸识别功能。此外，格灵深瞳还推出了皓目人体行为分析系统；在去年下半年，格灵深瞳人眼摄像机研发成功，它采用独创的像素动态瞬时分配技术，在距离人体50米外，可以达到数亿级等效像素，展现清晰人脸。\n7、陌上花科技（衣+）：人工智能计算机视觉引擎\nYi+成立于2014年，是国内领先的计算机视觉服务商，为企业提供视觉内容智能化和商业化解决方案。Yi+致力于挖掘视觉信息的商业价值。曾获阿里优酷土豆领投A轮融资。公司旗下品牌Yi+是人工智能计算机视觉引擎，衣+是时尚商品搜索引擎。在国际顶级计算机视觉竞赛ImageNet2015中，获得五项世界第一。Yi+致力于人工智能中的感知和认知智能，在图像视频中对场景、通用物体、商品、人脸的检测、识别、理解、搜索及推荐均达到领先水平。\n该公司目前和阿里云、华为、优酷土豆、微博、360、华数、京东、天猫魔盒、CIBN、英伟达、趣拍、花椒、来疯等多家顶级机构和产品深度合作，通过提供边看边买引擎、图像视频内容分析引擎、人脸属性分析引擎服务海量用户，同时帮助内容方实现场景营销、智能分析和内容互动。\n团队成员来自于CMU、新加坡国大、南洋理工、清华、北大、中科院、上海交大、谷歌、微软、IBM、Intel、阿里巴巴、腾讯、百度、华为、360、聚美等。获得阿里巴巴优酷的A轮投资。2017年获1亿元人民币B轮融资，投资方为海通证券、百融骏集团、北京银行旗下基金。\n近3年时间的发展，迅速积累了丰富的行业资源。尤其在传媒和广电方面，YI+拥有其他创业公司无法比拟的资源。除与7个牌照方关系密切外，目前Yi+是帮助广电总局制定TVOS行业标准的唯一视觉AI公司。"}
{"content2":"来源：极市平台\n摘要：计算机视觉领域同样精彩纷呈，与四年前相比GAN生成的假脸逼真到让人不敢相信；新工具、新框架的出现，也让这个领域的明天特别让人期待……\n2018，仍是AI领域激动人心的一年。\n计算机视觉领域同样精彩纷呈，与四年前相比GAN生成的假脸逼真到让人不敢相信；新工具、新框架的出现，也让这个领域的明天特别让人期待……\n近日，Analytics Vidhya发布了一份2018人工智能技术总结与2019趋势预测报告，原文作者PRANAV DAR。这份报告总结和梳理了全年主要AI技术领域的重大进展，同时也给出了相关的资源地址，以便大家更好的使用、查询。\n重点为大家介绍这份报告中的两个部分：\n计算机视觉\n工具和库\n下面，我们就逐一来盘点和展望。\n计算机视觉\n今年，无论是图像还是视频方向都有大量新研究问世，有三大研究曾在CV圈掀起了集体波澜。\nBigGAN\n今年9月，当搭载BigGAN的双盲评审中的ICLR 2019论文现身，行家们就沸腾了：简直看不出这是GAN自己生成的。\n在计算机图像研究史上，BigGAN的效果比前人进步了一大截。比如在ImageNet上进行128×128分辨率的训练后，它的Inception Score（IS）得分166.3，是之前最佳得分52.52分3倍。\n除了搞定128×128小图之外，BigGAN还能直接在256×256、512×512的ImageNet数据上训练，生成更让人信服的样本。\n在论文中研究人员揭秘，BigGAN的惊人效果背后，真的付出了金钱的代价，最多要用512个TPU训练，费用可达11万美元，合人民币76万元。\n不止是模型参数多，训练规模也是有GAN以来最大的。它的参数是前人的2-4倍，批次大小是前人的8倍。\n研究论文：\nhttps://openreview.net/pdf?id=B1xsqj09Fm\nFast.ai 18分钟训练整个ImageNet\n在完整的ImageNet上训练一个模型需要多久？各大公司不断下血本刷新着记录。\n不过，也有不那么烧计算资源的平民版。\n今年8月，在线深度学习课程Fast.ai的创始人Jeremy Howard和自己的学生，用租来的亚马逊AWS的云计算资源，18分钟在ImageNet上将图像分类模型训练到了93%的准确率。\n前前后后，Fast.ai团队只用了16个AWS云实例，每个实例搭载8块英伟达V100 GPU，结果比Google用TPU Pod在斯坦福DAWNBench测试上达到的速度还要快40%。\n这样拔群的成绩，成本价只需要40美元，Fast.ai在博客中将其称作人人可实现。\nFast.ai博客介绍：\nhttps://www.fast.ai/2018/08/10/fastai-diu-imagenet/\nvid2vid技术\n今年8月，英伟达和MIT的研究团队高出一个超逼真高清视频生成AI。\n只要一幅动态的语义地图，就可获得和真实世界几乎一模一样的视频。换句话说，只要把你心中的场景勾勒出来，无需实拍，电影级的视频就可以自动P出来：\n除了街景，人脸也可生成：\n这背后的vid2vid技术，是一种在生成对抗性学习框架下的新方法：精心设计的生成器和鉴别器架构，再加上时空对抗目标。\n这种方法可以在分割蒙版、素描草图、人体姿势等多种输入格式上，实现高分辨率、逼真、时间相干的视频效果。\n好消息，vid2vid现已被英伟达开源。\n研究论文：\nhttps://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf\nGitHub地址：\nhttps://github.com/NVIDIA/vid2vid\n2019趋势展望\nAnalytics Vidhya预计，明年在计算机视觉领域，对现有方法的改进和增强的研究可能多于创造新方法。\n在美国，政府对无人机的限令可能会稍微“松绑”，开放程度可能增加。而今年大火的自监督学习明年可能会应用到更多研究中。\nAnalytics Vidhya对视觉领域也有一些期待，目前来看，在CVPR和ICML等国际顶会上公布最新研究成果，在工业界的应用情况还不乐观。他希望在2019年，能看到更多的研究在实际场景中落地。\nAnalytics Vidhya预计，视觉问答（Visual Question Answering，VQA）技术和视觉对话系统可能会在各种实际应用中首次亮相。\n工具和框架\n哪种工具最好？哪个框架代表了未来？这都是一个个能永远争论下去的话题。\n没有异议的是，不管争辩的结果是什么，我们都需要掌握和了解最新的工具，否则就有可能被行业所抛弃。\n今年，机器学习领域的工具和框架仍在快速的发展，下面就是这方面的总结和展望。\nPyTorch 1.0\n根据10月GitHub发布的2018年度报告，PyTorch在增长最快的开源项目排行上，名列第二。也是唯一入围的深度学习框架。\n作为谷歌TensorFlow最大的“劲敌”，PyTorch其实是一个新兵，2017年1月19日才正式发布。2018年5月，PyTorch和Caffe2整合，成为新一代PyTorch 1.0，竞争力更进一步。\n相较而言，PyTorch速度快而且非常灵活，在GitHub上有越来越多的开码都采用了PyTorch框架。可以预见，明年PyTorch会更加普及。\n至于PyTorch和TensorFlow怎么选择？在我们之前发过的一篇报道里，不少大佬站PyTorch。\n实际上，两个框架越来越像。前Google Brain深度学习研究员，Denny Britz认为，大多数情况下，选择哪一个深度学习框架，其实影响没那么大。\nPyTorch官网：\nhttps://pytorch.org/\nAutoML\n很多人将AutoML称为深度学习的新方式，认为它改变了整个系统。有了AutoML，我们就不再需要设计复杂的深度学习网络。\n今年1月17日，谷歌推出Cloud AutoML服务，把自家的AutoML技术通过云平台对外发布，即便你不懂机器学习，也能训练出一个定制化的机器学习模型。\n不过AutoML并不是谷歌的专利。过去几年，很多公司都在涉足这个领域，比方国外有RapidMiner、KNIME、DataRobot和H2O.ai等等。\n除了这些公司的产品，还有一个开源库要介绍给大家：\nAuto Keras！\n这是一个用于执行AutoML任务的开源库，意在让更多人即便没有人工智能的专家背景，也能搞定机器学习这件事。\n这个库的作者是美国德州农工大学（Texas A&M University）助理教授胡侠和他的两名博士生：金海峰、Qingquan Song。Auto Keras直击谷歌AutoML的三大缺陷：\n第一，还得付钱。\n第二，因为在云上，还得配置Docker容器和Kubernetes。\n第三，服务商(Google)保证不了你数据安全和隐私。\n官网：\nhttps://autokeras.com/\nGitHub：\nhttps://github.com/jhfjhfj1/autokeras\nTensorFlow.js\n今年3月底的TensorFlow开发者会峰会2018上，TensorFlow.js正式发布。\n这是一个面向JavaScript开发者的机器学习框架，可以完全在浏览器中定义和训练模型，也能导入离线训练的TensorFlow和Keras模型进行预测，还对WebGL实现无缝支持。\n在浏览器中使用TensorFlow.js可以扩展更多的应用场景，包括展开交互式的机器学习、所有数据都保存在客户端的情况等。\n实际上，这个新发布的TensorFlow.js，就是基于之前的deeplearn.js，只不过被整合进TensorFlow之中。\n谷歌还给了几个TensorFlow.js的应用案例。比如借用你的摄像头，来玩经典游戏：吃豆人（Pac-Man）。\n官网：\nhttps://js.tensorflow.org/\n2019趋势展望\n在工具这个主题中，最受关注的就是AutoML。因为这是一个真正会改变游戏规则的核心技术。在此，引用H2O.ai的大神Marios Michailidis（KazAnova）对明年AutoML领域的展望。\n以智能可视化、提供洞见等方式，帮助描述和理解数据\n为数据集发现、构建、提取更好的特征\n快速构建更强大、更智能的预测模型\n通过机器学习可解释性，弥补黑盒建模带来的差距\n推动这些模型的产生\n未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。\n未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。\n如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”"}
{"content2":"以下是computer vision：algorithm and application计算机视觉算法与应用这本书中附录里的关于计算机视觉的一些测试数据集和源码站点，我整理了下，加了点中文注解。\nComputerVision:\nAlgorithms and Applications\nRichard Szeliski\n在本书的最好附录中，我总结了一些对学生，教授和研究者有用的附加材料。这本书的网址http://szeliski.org/Book包含了更新的数据集和软件，请同样访问他。\nC.1 数据集\n一个关键就是用富有挑战和典型的数据集来测试你算法的可靠性。当有背景或者他人的结果是可行的,这种测试可能甚至包含更多的信息(和质量更好)。\n经过这些年，大量的数据集已经被提出来用于测试和评估计算机视觉算法。许多这些数据集和软件被编入了计算机视觉的主页。一些更新的网址，像CVonline\n(http://homepages.inf.ed.ac.uk/rbf/CVonline ), VisionBib.Com (http://datasets.visionbib.com/ ), and Computer Vision online (http://computervisiononline.com/ ), 有更多最新的数据集和软件。\n下面，我列出了一些用的最多的数据集，我将它们让章节排列以便它们联系更紧密。\n第二章：图像信息\nCUReT: Columbia-Utrecht 反射率和纹理数据库Reﬂectance and TextureDatabase, http://www1.cs.columbia.edu/CAVE/software/curet/  (Dana, van Ginneken, Nayaret al. 1999).\nMiddlebury Color Datasets:不同摄像机拍摄的图像，注册后用于研究不同的摄像机怎么改变色域和彩色registeredcolor images taken by different cameras to study how they transform gamuts andcolors,http://vision.middlebury.edu/color/data/ Chakrabarti, Scharstein, and Zickler 2009).\n第三章：图像处理\nMiddlebury test datasets forevaluating MRF minimization/inference algorithms评估隐马尔科夫随机场最小化和推断算法,\nhttp://vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharstein et al. 2008).\n第四章：特征检测和匹配\nAfﬁne Covariant Featuresdatabase（反射协变的特征数据集） for evaluating feature detector and descriptor matching quality andrepeatability（评估特征检测和描述匹配的质量和定位精度）,   http://www.robots.ox.ac.uk/~vgg/research/affine/\n(Miko-lajczyk and Schmid 2005;Mikolajczyk, Tuytelaars, Schmid et al. 2005).\nDatabase of matched imagepatches for learning （图像斑块匹配学习数据库）and feature descriptor evaluation（特征描述评估数据库）,\nhttp://cvlab.epfl.ch/~brown/patchdata/patchdata.html\n(Winder and Brown 2007;Hua,Brown, and Winder 2007).\n第五章;分割\nBerkeleySegmentation Dataset（分割数据库） and Benchmark of 1000 images labeled by 30 humans,（30个人标记的1000副基准图像）along with an evaluation,http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/  (Martin, Fowlkes, Tal et al.2001).\nWeizmann segmentationevaluation database of 100 grayscale images with ground truth segmentations,\nhttp://www.wisdom.weizmann.ac.il/~vision/Seg EvaluationDB/index.html\n(Alpert, Galun, Basri et al. 2007).\n第八章：稠密运动估计\nTheMiddlebury optic ﬂow evaluation（光流评估） Web site, http://vision.middlebury.edu/flow/data/\n(Baker,Scharstein, Lewis et al. 2009).\nThe Human-Assisted MotionAnnotation database,（人类辅助运动数据库）\nhttp://people.csail.mit.edu/celiu/motionAnnotation/  (Liu, Freeman, Adelson etal. 2008)\n第十章：计算机摄像学\nHigh DynamicRange radiance（辐射）maps, http://www.debevec.org/Research/HDR/\n(De-bevecand Malik 1997).\nAlpha matting evaluation Website, http://alphamatting.com/ (Rhemann, Rother, Wang\net al. 2009).\n第十一章：Stereo correspondence立体对应\nMiddlebury Stereo Datasets andEvaluation, http://vision.middlebury.edu/stereo/  (Scharstein\nand Szeliski 2002).\nStereoClassiﬁcation（立体分类） and Performance Evaluation（性能评估） of different aggregation（聚类） costs for stereo matching（立体匹配）,http://www.vision.deis.unibo.it/spe/SPEHome.aspx  (Tombari, Mat-\ntoccia, Di Stefano et al.2008).\nMiddlebury Multi-View StereoDatasets,\nhttp://vision.middlebury.edu/mview/data/  (Seitz,Curless, Diebel etal. 2006).\nMulti-view and Oxford Collegesbuilding reconstructions,\nhttp://www.robots.ox.ac.uk/~vgg/data/data-mview.html .\nMulti-View Stereo Datasets, http://cvlab.epfl.ch/data/strechamvs/  (Strecha, Fransens,\nand Van Gool 2006).\nMulti-View Evaluation,  http://cvlab.epfl.ch/~strecha/multiview/ (Strecha, von Hansen,\nVan Gool et al. 2008).\n第十二章：3D重建\nHumanEva: synchronized video（同步视频） and motion capture （动作捕捉）dataset for evaluation ofarticulated human motion,http://vision.cs.brown.edu/humaneva/  Sigal, Balan, and Black 2010).\n第十三章：图像渲染\nThe (New) Stanford Light FieldArchive, http://lightfield.stanford.edu/\n(Wilburn, Joshi,Vaish et al.2005).\nVirtual Viewpoint Video:multi-viewpoint video with per-frame depth maps,\nhttp://research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/  (Zitnick, Kang, Uytten-\ndaele et al. 2004).\n第十四章：识别\n查找一系列的视觉识别数据库，在表14.1–14.2.除了那些，这里还有：\nBuffy pose classes, http://www.robots.ox.ac.uk/~vgg/data/  buffy pose classes/ andBuffy\nstickmen V2.1, http://www.robots.ox.ac.uk/~vgg/data/stickmen/index.html  (Ferrari,Marin-\nJimenez, and Zisserman 2009;Eichner and Ferrari 2009).\nH3D database of pose/jointannotated photographs of humans,\nhttp://www.eecs.berkeley.edu/~lbourdev/h3d/   (Bourdev and Malik 2009).\nAction Recognition Datasets,http://www.cs.berkeley.edu/projects/vision/action, has point-\ners toseveral datasets for action and activity recognition, as well as some papers.（有一些关于人活动和运动的数据库和论文） The humanaction database athttp://www.nada.kth.se/cvap/actions/  包含更多的行动序列。\nC.2 软件资源\n一个对于计算机视觉算法最好的资源就是开源视觉图像库（opencv）(http://opencv.willowgarage.com/wiki/),他有在intel的Gary Bradski和他的同事开发，现在由Willow Garage (Bradsky and Kaehler 2008)维护和扩展。一部分可利用的函数在http://opencv.willowgarage.com/documentation/cpp/中：\n图像处理和变换 (滤波，形态学，金字塔);\n图像几何学的变换 (旋转，改变大小);\n混合图像变换 (傅里叶变换，距离变换);\n直方图;\n分割 (分水岭, mean shift);\n特征检测 (Canny, Harris, Hough, MSER, SURF);\n运动分析和物体分析 (Lucas–Kanade, mean shift);\n相机矫正和3D重建\n机器学习 (k nearest neighbors, 支持向量机, 决策树, boost-\ning, 随机树, expectation-maximization, 和神经网络).\nIntel的Performance Primitives (IPP)library, http://software.intel.com/en-us/intel-ipp/，包含\n各种各样的图像处理任务的最佳优化代码，许多opencv中的例子利用了这个库，加入他安装了，程序运行得更快。依据功能，他和Opencv有很多相同的运算处理，并且加上了额外的库针对图像视频压缩，信号语音处理和矩阵代数。\nMTALAB中的Image Processing Toolbox图像处理工具，http://www.mathworks.com/products/image/，包含常规的处理，空域变换（旋转，改变大小），常规正交，图像分析和统计学（变边缘，哈弗变换），图像增强（自适应直方图均衡，中值滤波），图像恢复（去模糊），线性滤波（卷积），图像变换（傅里叶，离散余弦变换）和形态学操作（连通域和距离变换）\n两个比较旧的库，它们没有被发展，但是包含了一些的有用的常规操作：\nVXL (C++Libraries for Computer Vision Research and Implemen-tation,http://vxl.sourceforge.net/)\nLTI-Lib 2 (http://www.ie.itcr.ac.cr/palvarado/ltilib-2/homepage/ ).\n图像编辑和视图包，例如Windows Live Photo Gallery, iPhoto, Picasa,GIMP, 和 IrfanView，它们对执行这些处理非常有用：常规处理任务，格式转换，观测你的结果。它们同样可以用于对图像处理算法有趣的实现参考，例如色调调整和去噪。\n这里他也有一些软件包和基础框架对你建一个实时视频处理的DEMOS很有用，Vision on Tap(http://www.visionontap.com/ )提供一个可以实时处理你的网络摄像头的网页服务(Chiu and Raskar 2009）。Video-Man (VideoManager,http://videomanlib.sourceforge.net/处理实时的基于视频的DEMOS和应用非常有用，你也可以用MATLAB中的imread直接从任何URl（例如网络摄像头）中读取视频。\n下面，我列出了一些额外的网络资源，让章节排列以便它们看起来联系更紧密：\n第三章:图像处理\nmatlabPyrTools—MATLAB 下的源码对于拉普拉斯变换，金字塔, QMF/小波, 和\nsteerable pyramids, http://www.cns.nyu.edu/~lcv/software.php  (Simoncelli and Adel-\nson 1990a; Simoncelli,Freeman, Adelson et al. 1992).\nBLS-GSM 图像去噪, http://decsai.ugr.es/~javier/denoise/  (Portilla, Strela,Wain-\nwright et al. 2003).\nFast bilateral ﬁltering code（快速双边滤波）, http://people.csail.mit.edu/jiawen/#code (Chen, Paris, and Durand 2007).\nC++ implementation of the fastdistance transform algorithm,\nhttp://people.cs.uchicago.edu/~pff/dt/  (Felzenszwalb andHuttenlocher 2004a).\nGREYC’s Magic Image Converter,including image restoration software using regularization and anisotropicdiffusion,http://gmic.sourceforge.net/gimp.shtml (Tschumperl´ e and Deriche 2005).\n第四章：图像特征检测和匹配\nVLFeat, 一个开放便捷的计算机视觉算法库\nhttp://vlfeat.org/ (Vedaldi and Fulkerson 2008).\nSiftGPU: A GPU Implementationof Scale Invariant Feature Transform (SIFT),\nGPU实现的尺度特征性变换\nhttp://www.cs.unc.edu/~ccwu/siftgpu/  (Wu 2010).\nSURF: Speeded Up RobustFeatures, http://www.vision.ee.ethz.ch/~surf/\n(Bay, Tuyte-laars, and VanGool 2006).\nFAST corner detection, http://mi.eng.cam.ac.uk/~er258/work/fast.html\n(Rosten and Drum-mond 2005, 2006).\nLinux binaries for afﬁneregion detectors and descriptors, as well as MATLAB ﬁles to\ncompute repeatability andmatching scores,\nhttp://www.robots.ox.ac.uk/~vgg/research/affine/\nKanade–Lucas–Tomasi featuretrackers: KLT, http://www.ces.clemson.edu/~stb/klt/ (Shi and Tomasi 1994);\nGPU-KLT, http://cs.unc.edu/~cmzach/opensource.html  (Zach,Gallup, and Frahm2008); Lucas–Kanade 20 Years On,http://www.ri.cmu.edu/projects/project 515.html  (Baker and Matthews 2004).\n第五章：分割\n高效的基于图形的分割http://people.cs.uchicago.edu/~pff/segment\n(Felzenszwalb and Huttenlocher2004b).\nEDISON, 边缘检测和图像追踪,\nhttp://coewww.rutgers.edu/riul/research/code/EDISON/\n(Meer and Georgescu 2001; Comaniciu and Meer2002).\nNormalized cuts segmentationincluding intervening contours,\nhttp://www.cis.upenn.edu/~jshi/software/\n(Shi and Malik 2000; Malik,Belongie, Leung et al. 2001).\nSegmentation by weightedaggregation (SWA),利用加权集合的分割\nhttp://www.cs.weizmann.ac.il/~vision/SWA  (Alpert, Galun, Basri et al.2007).\n第六章：基于特征的对齐和校准\nNon-iterative PnP algorithm,（非迭代PnP算法）\nhttp://cvlab.epﬂ.ch/software/EPnP  (Moreno-Noguer, Lep-etit, and Fua 2007).\nTsai Camera Calibration（相机矫正） Software,\nhttp://www-2.cs.cmu.edu/~rgw/TsaiCode.html  (Tsai 1987).\nEasy CameraCalibration Toolkit,（简易相机校准工具包） http://research.microsoft.com/en-us/um/people/zhang/ Calib/ (Zhang 2000).\nCamera Calibration Toolbox forMATLAB,\nhttp://www.vision.caltech.edu/bouguetj/calib doc/ ; a C version is included in OpenCV.\nMATLAB functions for multipleview geometry,\nhttp://www.robots.ox.ac.uk/~vgg/hzbook/code/  (Hartley and Zisserman2004).\n第七章：运动重建\nSBA: A generic sparse bundle(稀疏束) adjustment C/C++ package basedon the Levenberg–\nMarquardt algorithm, http://www.ics.forth.gr/~lourakis/sba/  (Lourakis and Argyros 2009).\nSimple sparse bundleadjustment (SSBA), http://cs.unc.edu/~cmzach/opensource.html .\nBundler, structure from motionfor unordered image collections(无序图像集),\nhttp://phototour.cs.washington.edu/bundler/   (Snavely, Seitz, and Szeliski 2006).\n第八章:稠密运动估计\n光流, http://www.cs.brown.edu/~black/code.html (Black and Anan-\ndan 1996).\nOptical ﬂow（光流） using total variation（全变量差） and conjugate gradientdescent（共轭梯度下降）,http://people.csail.mit.edu/celiu/OpticalFlow/  (Liu 2009).\nTV-L1 optical ﬂow on the GPU, http://cs.unc.edu/~cmzach/opensource.html\n(Zach,Pock, and Bischof2007a).\nelastix: atoolbox for rigid（刚性） and nonrigid（非刚性） registration of images（配准图像）,http://elastix.isi.uu.nl/ (Klein, Staring, and Pluim 2007).\nDeformable image registration（可变形的配准图像） using discreteoptimization（离散最优化）, http://www.mrf-registration.net/deformable/index.html\n(Glocker, Komodakis, Tziritas et al. 2008).\n第九章：图像缝合\nMicrosoft Research ImageCompositing Editor for stitching images,（图像拼接，图像合成）\nhttp://research.microsoft.com/en-us/um/redmond/groups/ivm/ice/ .\n第十章：计算机摄影学\nHDRShop software for combiningbracketed exposures（包围式曝光） into high-dynamic range radiance images,http://projects.ict.usc.edu/graphics/HDRShop/.\nSuper-resolution（超分辨率） code,\nhttp://www.robots.ox.ac.uk/~vgg/software/SR/  (Pickup 2007;Pickup, Capel,Roberts et al. 2007, 2009).\n第十一章：立体对应\nStereoMatcher, standalone C++stereo matching code,\nhttp://vision.middlebury.edu/stereo/code/  (Scharstein and Szeliski2002).\nPatch-based multi-view stereosoftware (PMVS Version 2),\nhttp://grail.cs.washington.edu/software/pmvs/  (Furukawa and Ponce 2011).\n第十二章：3D重建\nScanalyze: a system foraligning and merging range data,\nhttp://graphics.stanford.edu/software/scanalyze/  (Curless and Levoy 1996).\nMeshLab: software forprocessing, editing, and visualizing unstructured 3D triangular\nmeshes, http://meshlab.sourceforge.net/.\nVRML viewers (various) arealso a good way to visualize texture-mapped 3D models.\n节 12.6.4: Whole body modeling andtracking（全身建模和追踪）\nBayesian 3D person tracking（贝叶斯3D人体追踪）, http://www.cs.brown.edu/~black/code.html  (Sidenbladh,Black, and Fleet2000; Sidenbladh and Black 2003).\nHumanEva: baseline code forthe tracking of articulated human motion,\nhttp://vision.cs.brown.edu/humaneva/   (Sigal, Balan, and Black 2010).\n节 14.1.1: Face detection（人脸检测）\nSample face detection code andevaluation tools,\nhttp://vision.ai.uiuc.edu/mhyang/face-detection-survey.html.\n节 14.1.2: Pedestrian detection（行人追踪）\nA simple object detector withboosting,\nhttp://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html\n(Hastie, Tibshirani, and Friedman 2001;Torralba, Murphy, and Freeman 2007).\nDiscriminatively（有区别） trained deformable（可变形） part models, http://people.cs.uchicago.edu/~pff/latent/  (Felzenszwalb, Girshick,McAllester et al. 2010).\nUpper-body detector（上身检测）,\nhttp://www.robots.ox.ac.uk/~vgg/software/UpperBody/  (Ferrari,Marin-Jimenez, andZisserman 2008).\n2D articulated human poseestimation software,\nhttp://www.vision.ee.ethz.ch/~calvin/articulated_human_pose_estimation_code/  (Eichner and Ferrari 2009).\n节 14.2.2: Active appearance and 3Dshape models\nAAMtools: An active appearancemodeling toolbox,\nhttp://cvsp.cs.ntua.gr/software/AAMtools/  (Papandreou and Maragos2008).\n节 14.3: Instance recognition\nFASTANN and FASTCLUSTER forapproximate k-means (AKM),\nhttp://www.robots.ox.ac.uk/~vgg/software/ (Philbin, Chum, Isard et al. 2007).\nFeature matching using fastapproximate nearest neighbors,\nhttp://people.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN  (Muja and Lowe 2009).\n节 14.4.1: Bag of words(词袋)\nTwo bag of words classiﬁers, http://people.csail.mit.edu/fergus/iccv2005/bagwords.html\n(Fei-Fei and Perona 2005;Sivic, Russell, Efros et al. 2005).\nBag of features andhierarchical（分层） k-means,http://www.vlfeat.org/  (Nist´ er and Stew´enius2006; Nowak, Jurie, and Triggs 2006).\n节 14.4.2: Part-based models\nA simple parts and structureobject detector,\nhttp://people.csail.mit.edu/fergus/iccv2005/partsstructure.html\n(Fischler and Elschlager 1973; Felzenszwalband Huttenlocher 2005).\n节 14.5.1: Machine learning software\nSupport vector machines (SVM)software (\nhttp://www.support-vector-machines.org/SVM soft.html )\n包含很多支持向量机的库,\nSVMlight http://svmlight.joachims.org/ ;\nLIBSVM, http://www.csie.ntu.edu.tw/~cjlin/libsvm/(Fan, Chen,and Lin 2005);\nLIBLINEAR, http://www.csie.ntu.edu.tw/~cjlin/liblinear/  (Fan,Chang, Hsieh et al.2008).\nKernel Machines: links to SVM,Gaussian processes, boosting, and other machine\nlearning algorithms, http://www.kernel-machines.org/software .\nMultiple kernels for imageclassiﬁcation,\nhttp://www.robots.ox.ac.uk/~vgg/software/MKL\n(Varma and Ray 2007; Vedaldi, Gulshan, Varmaet al. 2009).\n附录 A.1–A.2: Matrix decompositions（矩阵分解） and linear least squares（线性最小乘）\nBLAS (BasicLinear Algebra Subprograms基本线性代数子程序),\nhttp://www.netlib.org/blas/  (Blackford,Demmel, Dongarraet al. 2002).\nLAPACK (Linear Algebra（线性代数） PACKage),\nhttp://www.netlib.org/lapack/  (Anderson, Bai,Bischof etal. 1999).\nGotoBLAS, http://www.tacc.utexas.edu/tacc-projects/.\nATLAS (Automatically TunedLinear Algebra Software),\nhttp://math-atlas.sourceforge.net/  (Demmel, Dongarra, Eijkhoutet al. 2005).\nIntel Math Kernel Library(MKL), http://software.intel.com/en-us/intel-mkl/.\nAMD CoreMath Library (ACML),\nhttp://developer.amd.com/cpu/Libraries/acml/Pages/default.aspx .\nRobust PCA code（鲁棒主成分分析）, http://www.salle.url.edu/~ftorre/papers/rpca2.html\n(De la Torre and Black 2003).\nAppendix A.3: Non-linear leastsquares非线性最小二乘\nMINPACK, http://www.netlib.org/minpack/.\nlevmar: Levenberg–Marquardtnonlinear least squares algorithms, 非线性最小二乘\nhttp://www.ics.forth.gr/~lourakis/levmar/  (Madsen, Nielsen, andTingleff 2004).\n附录 A.4–A.5: Direct（直接） and iterative（迭代） sparse matrix（稀疏矩阵） solvers\nSuiteSparse (variousreordering algorithms, 各种各样的重排算法CHOLMOD) and SuiteSparse QR,http://www.cise.ufl.edu/research/sparse/SuiteSparse/  (Davis 2006, 2008).\nPARDISO (iterative and sparsedirect solution),  http://www.pardiso-project.org/.\nTAUCS (sparse direct,iterative, out of core, preconditioners),\nhttp://www.tau.ac.il/~stoledo/taucs/ .\nHSL Mathematical SoftwareLibrary,  http://www.hsl.rl.ac.uk/index.html .\nTemplatesfor the solution of linear systems（线性系统解决问题的模板）, http://www.netlib.org/linalg/html templates/Templates.html  (Barrett, Berry, Chan et al.1994). Download the PDF for instructions（说明） on how to get the software.\nITSOL,MIQR, and other sparsesolvers,\nhttp://www-users.cs.umn.edu/~saad/software/  (Saad 2003).\nILUPACK, http://www-public.tu-bs.de/~bolle/ilupack/ .\n附录 B: Bayesian modeling and inference（贝叶斯建模和推断）\nMiddleburysource code for MRF minimization（隐马尔科夫随机场最小化）, http://vision.middlebury.edu/MRF/code/  (Szeliski, Zabih, Scharsteinet al. 2008).\nC++ code for efﬁcient beliefpropagation for early vision,\nhttp://people.cs.uchicago.edu/~pff/bp/  (Felzenszwalb andHuttenlocher 2006).\nFastPD MRF optimization（最优化） code,\nhttp://www.csd.uoc.gr/~komod/FastPD  (Komodakisand Tziritas2007a; Komodakis, Tziritas, and Paragios 2008)\n算法 C.1   Calgorithm for Gaussian random noise generation, using the Box–Mullertransform.\nC描述的利用Box–Muller 变换产生高斯随机噪声\ndouble urand()\n{\nreturn ((double)rand()) / ((double) RAND MAX);\n}\nvoid grand(double& g1, double& g2)\n{\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif // M_PI\ndouble n1 = urand();\ndouble n2 = urand();\ndouble x1 = n1 + (n1 == 0); /* guardagainst log(0) */\ndouble sqlogn1 = sqrt(-2.0 * log (x1));\ndouble angl = (2.0 * M PI) * n2;\ng1 = sqlogn1 * cos(angl);\ng2 = sqlogn1 * sin(angl);\n}\n高斯噪声的产生。许多基本的软件包产生一些不同的随机的噪声(例如 运行在unix上的rand())，但是并不是所有的都有高斯随机噪声发生器。计算一个离散随机常量，你可以用Box–Mullertransform (Box and Muller 1958)，他的c代码在算法C.1中给出了，注意这个运行结果是返回一对随机变量。相关的产生高斯随机变量的方由Thomas, Luk, Leong et al. (2007)提出。\n伪彩色产生。在很多应用中，很方便给图像加上标记（或者给图像特征比如线）。一个最简单的方式就是给不同的标记不同的颜色。在我的工作中，我发现用RGB立体色彩系给不同的标记赋予标准均匀的色彩是很方便的。\n对于每一个（非消极）标记值，considerthe bits as being split among the three color channel，例如对于一个比特值为9的值，\n这个值可以被标记为RGBRGBRGB，获得三基色中的每一种颜色值后，颠倒比特值，结果是低位的比特值变化的最快。\n实际上，对于一个八比特的颜色通道，这个比特值的颠倒可以被存在一个表或者一个存储提前计算好的记录有由标记值向伪彩色的改变的完整表。\n图 8.16 显示了这样一个伪彩色绘制的例子.\nGPU实现\nGPU的出现，可以处理像素着色和计算着色，导致了实时应用的快速计算机视觉算法的发展，例如，分割，追踪，立体和运动估计（(Pock, Unger, Cremerset al. 2008; Vineet and Narayanan 2008; Zach,Gallup, and Frahm 2008）。一个好的资源来学习这些算法就是CVPR 2008 上关于Visual Computer Visionon GPUs的workshop。\nhttp://www.cs.unc.edu/~jmf/Workshop_on_Computer_Vision_on_GPU.html他的论文可以在CVPR2008的会议集的DVD中找到。额外的关于GPU算法资源包括GPGPU网址和小组讨论http://gpgpu.org/还有OpenVIDIAWeb site, http://openvidia.sourceforge.net/index.php/OpenVIDIA\nC.3 PPT和讲稿\n正如我在前言中提到的，我希望提供和书中材料相一致的PPT，直到这些全部准备好，你最好的方式去看我在华盛顿大学上课时的PPT，和一写相关课程中用到的教案。\n这里是一些这样的课程列表：\nUW 455:Undergraduate Computer Vision,\nhttp://www.cs.washington.edu/education/courses/455/.\nUW576:Graduate Computer Vision,\nhttp://www.cs.washington.edu/education/courses/576.\nStanfordCS233B: Introduction to Computer Vision,\nhttp://vision.stanford.edu/teaching/cs223b/.\nMIT6.869: Advances in Computer Vision,\nhttp://people.csail.mit.edu/torralba/courses/6.869/6.869.computervision.htm.\nBerkeley CS 280: Computer Vision, http://www.eecs.berkeley.edu/~trevor/CS280.html\nUNC COMP776: Computer Vision, http://www.cs.unc.edu/~lazebnik/spring10.\nMiddlebury CS 453: Computer Vision,\nhttp://www.cs.middlebury.edu/~schar/courses/cs453-s10/.\nRelated courses have also been taught onthe topic of Computational Photography, e.g.,\nCMU 15-463: Computational Photography, http://graphics.cs.cmu.edu/courses/15-463/.\nMIT 6.815/6.865: Advanced ComputationalPhotography,\nhttp://stellar.mit.edu/S/course/6/sp09/6.815\nStanford CS 448A: Computational photographyon cell phones,\nhttp://graphics.stanford.edu/courses/cs448a-10/.\nSIGGRAPH courses on ComputationalPhotography,\nhttp://web.media.mit.edu/~raskar/photo/.\n这里还有一些最好的关于各种计算机视觉主题的在线讲稿，例如：belief propagation and graph cuts，它们在UW-MSR Course of Vision Algo-rithmshttp://www.cs.washington.edu/education/courses/577/04sp/\nC.4 参考文献：\n这本的所有参考文献在这本书的网站上，一个几乎所有的计算机视觉的出版物都引用的更全面的部分注解书目由Keith Price维http://iris.usc.edu/Vision-Notes/bibliography/contents.html.\n这里还有一个可搜索的计算机图形学的参考书目http://www.siggraph.org/publications/bibliography/另外技术论文比较好的资源是GoogleScholar 和 CiteSeerX。"}
{"content2":"《从CVPR 2014看计算机视觉领域的最新热点》提供了一个了解CVPR的窗口，阅读后结合自身工作有些随想记录在此。\n1、尚未被深度学习渗透的Low-level Vision ；\n2、Depth Sensor（深度传感器）及深度图像；\n这两点都可以作为对非深度学习值得深入的方向 。\n其中包括3A算法在内的去模糊、稳像、图像抠图、去涂鸦适合以技术技能在图像处理方面的EG，这个方向一直在发展，受到机器学习算法的影响较小，对于美图类应用、手机及芯片制造商来说，还是热门技术\n计算机视觉一直在发展，热像仪、车载夜视仪、安防的指纹人脸门禁考勤机，视频行为分析摄像机、ADAS防碰撞电子后视镜等，到微软的kinect、灵格深瞳的RGB-D相机，再到360全景相机，作为前段的采集分析设备一直在革新，嵌入式软件工程师在创业型公司（如DGI）是越来越火，源于智能硬件的兴起 。\n附1.\nFace Alignment\nNew Project: An Empirical Study of Recent Face Alignment Methods\nRelated Papers [the bib file is attached at the end]\n2015\nXi Peng, Yu Yang, Shaoting Zhang, Dimitris Metaxas, PIEFA: Personalized Incremental and Ensemble Face Alignment in the Wild, ICCV 2015.Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen, Leveraging Datasets with Varying Annotations for Face Alignment via Deep Regression Network, ICCV, 2015.Amin Jourabloo, Xiaoming Liu, Pose-Invariant 3D Face Alignment, ICCV, 2015.Georgios Tzimiropoulos, Project-Out Cascaded Regression With an Application to Face Alignment, CVPR, 2015.\nE. Antonakos, J. Alabort-i-Medina, S. Zafeiriou, Active Pictorial Structures, CVPR, 2015. [CODE]Donghoon Lee, Hyunsin Park, Chang D. Yoo, Face Alignment Using Cascade Gaussian Process Regression Trees, CVPR, 2015. S. Zhu, C. Li, C. C. Loy, X. Tang, Face Alignment by Coarse-to-Fine Shape Searching, CVPR, 2015. X. Xiong and F. De la Torre, Global Supervised Descent Method, CVPR 2015.J. Alabort-i-Medina, S. Zafeiriou, \"Unifying Holistic and Parts-Based Deformable Model Fitting\", CVPR, 2015.Pei-Lun Hsieh, Chongyang Ma, Jihun Yu, Hao Li, UNCONSTRAINED REALTIME FACIAL PERFORMANCE CAPTURE, CVPR, 2015. Heng Yang, Ioannis Patras, \"Mirror mirror on the wall, tell me, is the error small? \", CVPR 2015.Heng Yang et al. \"Face alignment assisted by head pose estimation\", BMVC 2015. Golnaz Ghiasi and Charless C. Fowlkes. \"Using Segmentation to Predict the Absence of Occluded Parts\", BMVC 2015.\n2014\nPedersoli, Marco and Timofte, Radu and Tuytelaars, Tinne and Gool, Luc Van: \"Using a deformation field model for localizing faces and facial points under weak supervision\", CVPR,2014.J. Alabort-i-medina, E. Antonakos, J. Booth, P. T. Snape, S. Zafeiriou, Menpo: A Comprehensive Platform for Parametric Image Alignment and Visual Deformable Models, ACM Multimedia, Open Source Software Competition, 2014. [CODE]Baltrušaitis, Tadas and Robinson, Peter and  Morency, Louis-Philippe: \"Continuous Conditional Neural Fields for Structured Regression\", ECCV, 2014. [CODE]Wu, Yue and Wang, Ziheng and Ji, Qiang: \"A Hierarchical Probabilistic Model for Facial Feature Detection\", CVPR,2014.Xing, Junliang and Niu, Zhiheng and Huang, Junshi and Hu, Weiming and Yan, Shuicheng: \"Towards Multi-view and Partially-Occluded Face Alignment\", CVPR,2014.Fowlkes, Golnaz Ghiasi Charless C: \"Occlusion Coherence: Localizing Occluded Faces with a Hierarchical Deformable Part Model\", CVPR,2014.Smith, Brandon M and Zhang, Li: \"Collaborative Facial Landmark Localization for Transferring Annotations Across Datasets\", ECCV,2014.Yu, Xiang and Lin, Zhe and Brandt, Jonathan and Metaxas, Dimitris N: \"Consensus of Regression for Occlusion-Robust Facial Feature Localization\", ECCV,2014.Chen, Dong and Ren, Shaoqing and Wei, Yichen and Cao, Xudong and Sun, Jian: \"Joint Cascade Face Detection and Alignment\",ECCV,2014.Zafeiriou, Lazaros and Antonakos, Epameinondas and Zafeiriou, Stefanos and Pantic, Maja: \"Joint Unsupervised Face Alignment and Behaviour Analysis\", ECCV,2014.Zhang, Zhanpeng and Luo, Ping and Loy, Chen Change and Tang, Xiaoou: \"Facial Landmark Detection by Deep Multi-task Learning\",ECCV,2014.[CODE and DATASET]Kazemi, Vahid and Sullivan, Josephine: \"One Millisecond Face Alignment with an Ensemble of Regression Trees\",CVPR,2014.[CODE]Zhang, Jie and Shan, Shiguang and Kan, Meina and Chen, Xilin: \"Coarse-to-Fine Auto-Encoder Networks (CFAN) for Real-Time Face Alignment\", ECCV,2014.[CODE]Ren, Shaoqing and Cao, Xudong and Wei, Yichen and Sun, Jian: \"Face Alignment at 3000 FPS via Regressing Local Binary Features\", CVPR,2014.Tzimiropoulos, Georgios and Pantic, Maja: \"Gauss-newton deformable part models for face alignment in-the-wild\", CVPR,2014. [CODE]Asthana, Akshay and Zafeiriou, Stefanos and Cheng, Shiyang and Pantic, Maja: \"Incremental Face Alignment in the Wild\", CVPR,2014.[CODE]Smith, Brandon M and Brandt, Jonathan and Lin, Zhe and Zhang, Li: \"Nonparametric Context Modeling of Local Appearance for Pose-and Expression-Robust Facial Landmark Localization\", CVPR,2014.Yang, Heng and Zou, Changqing and Patras, Ioannis: \"Face sketch landmarks localization in the wild\", IEEE Signal Processing Letters,2014. [CODE]Jia, Xuhui and Yang, Heng and Lin, Angran and Chan, Kwok-Ping and Patras, Ioannis: \"Structured Semi-supervised Forest for Facial Landmarks Localization with Face Mask Reasoning\", BMVC,2014.\n2013\nSagonas, Christos and Tzimiropoulos, Georgios and Zafeiriou, Stefanos and Pantic, Maja: \"300 Faces in-the-Wild Challenge: The first facial landmark localization Challenge\", ICCV,2013. [DATASET]Sagonas, C. and Tzimiropoulos, G. and Zafeiriou, S. and Pantic, M.: \"A Semi-automatic Methodology for Facial Landmark Annotation\", CVPRW,2013.Zhao, Xiaowei and Shan, Shiguang and Chai, Xiujuan and Chen, Xilin: \"Cascaded Shape Space Pruning for Robust Facial Landmark Detection\", ICCV,2013.Cox, M and Nuevo-Chiquero, J and Saragih, JM and Lucey, S: \"CSIRO Face Analysis SDK\", AFGR,2013. [CODE]Sun, Yi and Wang, Xiaogang and Tang, Xiaoou: \"Deep Convolutional Network Cascade for Facial Point Detection\", CVPR,2013. [CODE]Cheng, Xin and Fookes, Clinton B and Sridharan, Sridha and Saragih, Jason and Lucey, Simon: \"Deformable face ensemble alignment with robust grouped-L1 anchors\", AFGR,2013.Shen, Xiaohui and Lin, Zhe and Brandt, Jonathan and Wu, Ying: \"Detecting and aligning faces by image retrieval\", CVPR,2013.Zhou, Feng and Brandt, Jonathan and Lin, Zhe: \"Exemplar-Based Graph Matching for Robust Facial Landmark Localization\", ICCV,2013.Yu, Xiang and Yang, Fei and Huang, Junzhou and Metaxas, DN: \"Explicit occlusion detection based deformable fitting for facial landmark localization\", AFGRW,2013.Wu, Yue and Wang, Zuoguan and Ji, Qiang: \"Facial Feature Tracking under Varying Facial Expressions and Face Poses based on Restricted Boltzmann Machines\", CVPR,2013.Zhou, E. and Fan, H. and Cao, Z. and Y., Jiang and Yin, Q.: \"Facial landmark localization with coarse-to-fine convolutional network cascade\", ICCVW,2013.Yan, Junjie and Lei, Zhen and Yi, Dong and Li, Stan Z: \"Learn to Combine Multiple Hypotheses for Accurate Face Alignment\", ICCVW,2013.Yu, Xiang and Huang, Junzhou and Zhang, Shaoting and Yan, Wang and Metaxas, Dimitris N.: \"Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model\", ICCV,2013. [CODE]Yang, Heng and Patras, Ioannis: \"Privileged Information-based Conditional Regression Forests for Facial Feature Detection\", AFGR,2013.Fanelli, Gabriele and Dantone, Matthias and Gall, Juergen and Fossati, Andrea and Van Gool, Luc: \"Random forests for real time 3d face analysis\", IJCV,2013.Burgos-Artizzu, Xavier P and Perona, Pietro and Doll{\\'a}r, Piotr: \"Robust face landmark estimation under occlusion\", ICCV,2013. [CODE]Yang, H and Patras, I.: \"Sieving regression forests votes for facial feature detection in the wild\", ICCV,2013. [CODE]Xiong, Xuehan and De la Torre, Fernando: \"Supervised Descent Method and its Applications to Face Alignment\", CVPR,2013. [CODE]\n2012\nCao, X. and Wei, Y. and Wen, F. and Sun, J.: \"Face alignment by explicit shape regression\", CVPR,2012.Zhu, X. and Ramanan, D: \"Face detection, pose estimation and landmark localization in the wild\", CVPR,2012. [CODE]Yang, H. and Patras, I.: \"Face Parts Localization Using Structured-output Regression Forests\", ACCV,2012. [CODE]Le, Vuong and Brandt, Jonathan and Lin, Zhe and Bourdev, Lubomir and Huang, Thomas S: \"Interactive facial feature localization\", ECCV,2012.Martinez, B and Valstar, M and Binefa, X and Pantic, M: \"Local Evidence Aggregation for Regression Based Facial Point Detection\", T-PAMI,2012.Dantone, M. and Gall, J. and Fanelli, G. and {Van Gool}, L.: \"Real-time facial feature detection using conditional regression forests\", CVPR,2012.[CODE]\n2011\nCocsar, S and \\c{C}etin, M: \"A graphical model based solution to the facial feature point tracking problem\", Image and Vision Computing,2011.Kostinger, M and Wohlhart, P and Roth, P M and Bischof, H: \"Annotated Facial Landmarks in the Wild: A Large-scale, Real-world Database for Facial Landmark Localization\", ICCVW,2011.Roh, Myung-Cheol and Oguri, Takaharu and Kanade, Takeo: \"Face alignment robust to occlusion\", AFGR,2011.Efraty, B and Huang, C and Shah, S K and Kakadiaris, I A: \"Facial landmark detection in uncontrolled conditions\", IJCB,2011.Belhumeur, P N and Jacobs, D W and Kriegman, D J and Kumar, N: \"Localizing parts of faces using a consensus of exemplars\", CVPR,2011.Rapp, V and Senechal, T and Bailly, K and Prevost, L: \"Multiple kernel learning svm and statistical validation for facial landmark detection\", AFGR,2011.Yang, Fei and Huang, Junzhou and Metaxas, Dimitris: \"Sparse shape registration for occluded facial feature localization\", AFGR,2011.Valstar, M F and Jiang, B and Mehu, M and Pantic, M and Scherer, K: \"The first facial expression recognition and analysis challenge\", AFGR,2011.\n2010\nValstar, M and Martinez, B and Binefa, X and Pantic, M: \"Facial point detection using boosted regression and graph models\", CVPR,2010.Milborrow, S and Morkel, J and Nicolls, F: \"The MUCT landmarked face database\", Journal of Pattern Recognition Association of South Africa,2010.Saragih, J M and Lucey, S and Cohn, J F: \"Face alignment through subspace constrained mean-shifts\", ICCV,2009.Ekenel, Haz{\\i}m and Stiefelhagen, Rainer: \"Why is facial occlusion a challenging problem?\", Advances in Biometrics,2009.Liang, L and Xiao, R and Wen, F and Sun, J: \"Face alignment via component-based discriminative search\", ECCV,2008.Milborrow, S and Nicolls, F: \"Locating facial features with an extended active shape model\", ECCV,2008.Kasinski, A and Florek, A and Schmidt, A: \"The put face database\", Image Processing and Communications,2008.Huang, Gary B and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik: \"Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments\", ,2007.Liang, L and Wen, F and Xu, Y Q and Tang, X and Shum, H Y: \"Accurate face alignment using shape constrained Markov network\", CVPR,2006.Vukadinovic, D and Pantic, M: \"Fully automatic facial feature point detection using Gabor feature based boosted classifiers\", SMC,2005.Liao, C T and Wu, Y K and Lai, S H: \"Locating facial feature points using support vector machines\", International Workshop on Cellular Neural Networks and Their Applications,2005.Viola, P and Jones, M J: \"Robust real-time face detection\", International journal of computer vision,2004.Feris, R S and Gemmell, J and Toyama, K and Kruger, V: \"Hierarchical wavelet networks for facial feature localization\", AFGR,2002.Jesorsky, O and Kirchberg, K and Frischholz, R: \"Robust face detection using the hausdorff distance\", Audio-and Video-Based Biometric Person Authentication,2001.Cootes, Timothy F and Edwards, Gareth J and Taylor, Christopher J: \"Active appearance models\", ECCV, 1998. (AAM)Cristinacce, David and Cootes, Timothy F: \"Feature Detection and Tracking with Constrained Local Models\", BMVC, 2006. (CLM)Cootes, Timothy F and Taylor, Christopher J and Cooper, David H and Graham, Jim: \"Active shape models-their training and application\", Computer vision and image understanding, 1995. (ASM) 参考文献：\n1、http://msra.cn/zh-cn/research/academic-conferences/cvpr-2014.aspx\n2、https://sites.google.com/site/yanghengcv/face-alignment"}
{"content2":"计算机视觉是人工智能（AI）中的热门研究课题，它已经存在多年。然而，计算机视觉仍然是人工智能面临的最大挑战之一。在本文中，我们将探讨使用深度神经网络来解决计算机视觉的一些基本挑战。特别是，我们将研究神经网络压缩，细粒度图像分类，纹理合成，图像搜索和对象跟踪等应用。\n1、神经网络压缩\n尽管深度神经网络具有令人难以置信的性能，但它们对计算能力和存储的需求对其在实际应用中的部署提出了重大挑战。研究表明，神经网络中使用的参数可能非常多余。因此，在提高精度的同时还需要投入大量的工作来降低了网络的复杂性。\n低秩近似用于接近原始权重矩阵。例如，SVD可用于获得矩阵的最佳低秩近似，或者Toeplitz矩阵可与Krylov分析结合使用以近似的原始矩阵。\n1.1：修剪\n一旦训练完成，一些不相关的神经元连接（可以在损失算法中加权值平衡和稀疏约束）或者将所有这些连接过滤掉，然后执行几轮微调。在实际应用中，修剪神经元连接的级别将使结果稀疏，难以缓存，并且难以从存储器访问。有时，我们需要特别设计一个合作运营数据库。\n相比之下，过滤级修剪可以直接在已经存在的操作数据库上运行，过滤级修剪的关键是确定如何平衡过滤器的重要性。例如，我们可以使用卷积结果的稀疏性、滤波器对损失算法的影响或者卷积对下一层结果的影响进行平衡。\n1.2：量化\n我们可以将权重值分成组，然后使用组中的中值来替换原始权重，并通过霍夫曼编码运行它。但是，如果我们只考虑权重本身，则可以减少量化过程的误差偏差。随后，分类操作的误差偏差将显着增加。因此，量化CNN的优化目标是重构以最小化误差偏差。此外，我们可以使用哈希编码并投影相同的哈希权重（hash bucket weights）来共享相同的值。\n1.3：减少数据值的范围\n在默认情况下，数据由单精度浮点组成，占32位。研究人员发现，使用半精度浮点（16位）对性能的影响几乎为零。谷歌的TPU使用8位整数来表示数据，这种情况是值的范围是两个或三个值（0/1或-1/0/1）。仅使用位进行操作可以使我们快速完成各种计算，但是训练两个或三个价值网络是一个至关重要的问题。\n传统方法是使用两个或三个值作为前馈过程并在更新过程中传递实数。此外，研究人员认为两个值的表达能力是有限的，因此可以使用额外的浮点缩放二进制卷积结果来改善网络表示。\n1.4：简化的结构设计\n研究人员一直致力于创建简化的网络结构，例如：\n1. 1x1卷积：这种设计理念已经在Inception和ResNet系列网络设计中得到了广泛应用；\n2. 分组卷积；\n3. 扩展卷积：只要值不变，使用扩展卷积就可以扩展感知域。\n知识蒸馏（Knowledge distillation）训练小网络接近广泛的网络。但是，目前还不清楚如何正确地接近庞大的网络。\n1.5：硬件-软件协议设计\n常用硬件：\n1. 常见硬件，如CPU（低延迟，复杂操作）和GPU（高吞吐量，适合并发，简单过程）；\n2. 专用硬件，包括ASIC（专用集成电路，例如Google的TPU）和FPGA（现场可编程门阵列，灵活但效率较低）。\n2、细粒度图像分类\n与（普通）图像分类相比，细粒度图像分类在确定图像类别时需要更高的精度。例如，我们可能需要确定目标鸟的确切种类、汽车的品牌和型号、飞机的型号。通常，这些类之间的差异很小。例如，波音737-300和波音737-400之间唯一明显不同的区别就是窗户的数量。因此，细粒度图像分类比标准图像分类更具挑战性。\n细粒度图像分类的经典方法是首先在图像上定义不同的位置，例如，鸟的头部、脚部或翅膀。然后我们必须从这些位置提取特征，最后，组合这些特征并使用它们来完成分类。这种方法具有非常高的准确性，但它需要大量的数据集和手动标记位置信息。细粒度分类的一个主要趋势是没有额外监督信息的训练，而不是仅使用图像笔记，该方法由双线性CNN方法表示。\n2.1：双线性（Bilinear）CNN\n首先计算卷积描述符的外积，以找出不同维度之间的相互关系。因为不同描述符的维度对应于卷积特征的不同通道，并且不同的通道提取不同的语义特征，所以使用双线性操作允许我们捕获输入图像上的不同语义元素之间的关系。\n2.2:流线型双线性汇合（Streamlined Bilinear Confluence）\n双线性汇合的结果是非常高维的，这需要大量的计算和存储资源，也明显增加了下一个完全连接层上的参数数量。后续研究旨在制定简化双线性汇合的战略，其结果包括以下内容：\n1. PCA维数减少：在双线性汇合出现之前，我们会在深度描述符上使用PCA投影维数减少，但这会影响影响性能的每个维度。一个折中的方案是仅将PCA降维应用于一条线。\n2. 近似核估计：证明在双线性收敛之后使用线性SVM分类与在描述符上使用多项式核一样有价值。因为两个描述符的向外投影等于两个独立描述符的卷积投影，所以一些研究集中于使用随机矩阵来近似描述符投影。此外，通过近似核估计，我们可以捕获超过二阶信息（见下图）。\n3. 低秩近似：使用来自全连接层的参数矩阵进行低秩近似使得不必明确地计算双线性汇合的结果。\n3、图像描述\n图像描述是生成图像的一个或两个句子描述的过程。这是一项涉及计算机视觉和自然语言处理的跨学科任务。\n3.1：编码器-解码器网络\n设计图像字幕网络背后的基本思想基于自然语言处理领域中机器翻译的概念。在具有图像CNN编码网络的机器翻译器中替换源语言编码网络并提取图像的特征之后，我们可以使用解码器网络作为目标语言来创建文本描述。\n3.2:Show Attend and Tell\n注意力机制是机器翻译器用来捕获远程依赖关系的标准技术，也可以用于图像字幕。在解码器网络中，除了预测下一个单词之外，在每个时刻，我们还需要输出二维注意力图像并将其用于深度卷积特征的加权收敛。使用注意力机制的另一个好处是网络可以被可视化，这样我们就可以轻松地看到网络在生成每个单词时所看到的图像部分。\n3.3:Adaptive Attention（自适应注意力机制）\n先前的注意力机制将为每个预测的单词产生二维注意图像（图像（a））。但是，对于一些停止词，我们不需要使用图像中的线索。相反，某些单词可以根据上下文生成，完全独立于图像本身。这项工作在LSTM上进行了扩展，并产生了“视觉哨兵”机制，该机制确定是否应根据上下文或图像信息（图像（b））预测当前单词。\n此外，与先前在根据隐藏层的状态计算注意图像的先前方法不同，该方法根据隐藏层的当前状态执行计算。\n4、视觉问答\n给定图像和与该图像相关的问题，视觉问答旨在从选择的候选答案中回答该问题。从本质上讲，这是一个分类任务，有时它使用递归神经网络解码来产生文本答案。视觉问答也是一项涉及视觉和自然语言处理的跨学科任务。\n4.1:基本思想过程\n问题概念是使用CNN从图像中提取特征，RNN从文本问题中提取文本特征，然后组合视觉和文本特征，最后使用完全连接后进行分类。这项任务的关键是弄清楚如何连接这两种类型的功能。直接组合这些特征的方法将它们转换为矢量，或者通过添加或乘以元素来添加或生成视觉和文本矢量。\n注意使用注意力机制的图像字幕系统可提高视觉问答的性能。注意力机制包括视觉注意（“我在哪里看”）和文本注意力（“我在看哪个词？”）HieCoAtten可以同时或依次创建视觉和文本注意力。DAN在同一空间内投射视觉和文本注意力的结果; 然后它同时产生视觉和文本注意力的下一步。\n4.2:双线性整合\n该方法使用视觉特征向量和文本特征向量的外积来捕获每个维度上这些状态的特征之间的关系。为了避免明确地计算双线性汇合的高维度结果，我们可以将在细粒度识别中发现的流线型双线性汇合背后的思想应用于视觉问题回答。例如，MFB使用低速率近似背后的概念以及视觉和文本注意机制。\n5、神经网络可视化与神经网络理解\n下文提供了许多可视化方法，以帮助理解卷积和神经网络。\n5.1:直接可视化第一个过滤器\n由于第一个卷积层上的滤镜在输入图像上滑动，我们可以直接在第一层上显示滤镜。我们可以看到第一层权重集中在特定方向的边缘和指定的颜色组合，这类似于视觉生物学机制。但是，由于高级过滤器不直接用于输入图像，因此直接可视化只能应用于第一层上的过滤器。\n5.2:T-SNE\n此方法在图像的fc7和pool5特征上使用低维嵌入。例如，将它们缩小到2维并允许它们在2维平面上绘制，具有类似语义信息的图像应该产生与t-SNE类似的结果。该方法与PCA之间的区别在于t-SNE是一种非线性减少方法，可以保留地点之间的距离。我们得到以下图像，该图像是将t-SNE应用于原始MNIST图像的结果。我们可以看到MNIST是一个相对简单的数据集，其中不同分类的图像之间的差异是显而易见的。\n5.3:可视化中间层激活值\n我们可以看到，即使ImageNet没有人脸类别，网络仍将学会区分这种语义信息并捕获未来的分类。\n5.4:最大化响应图像区域\n为了阻止这种情况，在中间层选择一个指定的神经元，然后将多个不同的图像输入网络，以找到导致神经元最大响应的图像区域。这允许我们观察神经元对应的语义特征，我们使用“图像区域”而不是“完整图像”的原因是中间层神经元的感受野受限并且不能覆盖整个图像。\n5.5:梯度上升优化\n该方法选择特定的神经元，然后计算该神经元对输入图像的反应产生的偏导数，然后使用梯度上升优化图像直到收敛。此外，我们需要一些标准化的项目来使生成的图像更接近自然模型。除了优化输入图像，我们还可以优化fc6功能并创建所需的图像。\n6、对抗性的例子\n选择图像和不正确的分类。然后，系统计算该分类对图像的偏导数，然后对图像应用梯度上升优化。实验表明，在使用小的，几乎察觉不到的变化之后，我们可以使网络以高可信度对模型实现不正确的类。\n在实际应用中，对抗性示例在金融和安全领域非常有用。研究人员发现，这是因为图像空间的维度非常高。即使有大量的训练数据，我们也只能覆盖该空间的一小部分。如果输入图像从该不同空间稍微变化，那么网络将难以做出合理的决定。"}
{"content2":"本文通过案例引入计算机视觉基本知识，并浅析其基本任务中的图像分类、图像分割进展及应用。\n历史文章回顾：HBase Replication详解\nForeword前言\n先上几个计算机视觉应用的案例：\n6月6日至8日，在第23届圣彼得堡国际经济论坛上，新华社、俄罗斯塔斯社和搜狗公司联合推出了全球首个俄语AI合成主播，未来它将被应用于塔斯社的新闻报道中。塔斯社是俄罗斯的国家通讯社，作为全球五大通讯社之一，对外向115个国家和地区提供新闻信息，在全球范围都具有广泛影响力。\nMAGIC短视频智能生产平台由新华社和阿里巴巴联合成立的新华智云科技有限公司独立研发。在世界杯期间，通过MAGIC生产的短视频达到了37581 条，平均一条视频耗时50.7秒，全网实现了116604975次播放！而其中制作速度 快的一段视频《俄罗斯2:0领先埃及》，仅耗时6秒！\n上面的人脸都是AI生成的假脸，这些人都是不存在的，是英伟达的利用GAN模型（生成对抗网络）生成的数据。\n计算机视觉\n计算机视觉的研究目标是使计算机程序能够解读和理解图片，不仅是理解图片的颜色，而且是更高层地理解图片的语义及特征，通俗的点说就是让计算机睁眼“看”世界。人类大脑皮层的70%活动都在处理视觉信息，所以从感知的角度来讲，视觉是重要的信息感知功能。\n如下几件事推动了视觉的发展：\n1、深度学习领域的突破，深度学习搭建在神经网络之上，而神经网络的概念则脱胎于上世纪50年代科研人员对人类脑神经系统的研究和模拟。神经网络的理论在上世纪50年代就有了，但是一直处于浅层的应用状态，人们没有想到多层会带来什么新的变化。\n2、英伟达研发了GPU，持续的提高了算力，由于天然的并行计算和矩阵处理能力，大大加速了图像处理的过程和神经网络的计算过程。至今，在2012 年训练AlexNet模型需要使用两块GPU，花费6天时间，到今天做同样的事情只需要一块 新GPU，十几分钟就能搞定。\n3、斯坦福大学教授李飞飞创建了ImageNet，她把数百万张照片发到了网络上并发动群众做了标注。真正引起大家注意的就是2012年斯坦福的实验，过去实验的图像样本数多是“万”这个级别，斯坦福用了1000万，用多层神经网络来做，结果发现在人脸、人体、猫脸三个图像类别中，这个模型的识别率大概有7%-10%的提高。这给大家非常大的震动，因为通常识别率要提高1%要做好多努力，现在只是把层数增加了，竟然发生两大变化，一个是识别率提高这么多；第二个是能处理这么大数据。这两个变化给大家非常大的鼓舞，何况在2012年之前，人工智能没有解决过实际问题。2015 年12月，微软通过152层的深度网络，将图像识别错误率在ImageNet上降至3.57%，低于人类的误识率5.1%。\n下图是ImageNet上图片分类的进展情况，柱状图越低代表错误率越低：\n深度学习\n我们现在处于人工智能的第三次崛起。前两波发生在1950-1960年代和1980-1990 年代，当时都产生了相当大的影响，却也都慢慢冷却下来。这是因为当时的神经网络既没有实现本该达成的性能提升，也没有帮助我们理解生物的视觉系统。第三次浪潮21世纪初至今，此次与前两次不同，深度学习在很多基准测试和现实应用上已经大幅超越了生物的能力。\n深度学习一般是指的深度神经网络，也称DNN（Deep Neural Networks），神经网络在20世纪50年代就被提出来了，但是由于他本身固有的梯度消失，大量参数导致过拟合和计算量太大等问题，导致实际应用效果一直不好。因此在此之前机器学习几乎一直是SVM一统天下的局面。\n深度学习是在2006年Hinton等人被提出以来的，但是其真正的兴起，或者说重大影响工作的出现，是在2012年之后，比如，Krizhevsky等用深度学习大幅度提高了图片分类的准确率，也就是Alex Net的工作。\n深度学习在图像领域应用技术主要是卷积神经网络CNN（Convolutional Neural Network）。之所以CNN在计算机视觉应用的非常成功，传统机器学习方法基本被弃之不用。其中 大的一个原因就是，图像数据的特征设计，即特征描述，一直是计算机视觉头痛的问题，在深度学习突破之前10多年， 成功的图像特征设计（hand crafted feature）是SIFT，还有著名的BOW（bag ofvisual words），这些都是花了很长时间，需要非常专业的领域知识才设计出来的，这些高成本的模型迭代，使得的过去视觉算法的发展非常缓慢。可以参考如下的流程图，上面是传统的机器学习流程：\n关于深度学习的热门的应用领域可以参考下图（2018年根据paperswithcode的统计结果）\n现在深度学习应该比较成功的领域是计算机视觉、语音识别和自研语言处理，随着AlphaGo和OpenAI的成功，增强学习也慢慢在兴起。\n基本任务\n计算机视觉领域包含很多任务，但是基本的任务是图像分类、图像检测/定位，图像关键点定位，图像分割，这些任务已经发展多年，而且因为其基础的地位，会深刻影响其他领域（比如人脸识别、OCR）的发展，下面分别简单的介绍下各个任务的新进展情况。\n图像分类进展\n图像分类是指给定一张输入图像，判断该图像所属类别，通俗点说就是让机器理解这个图像是什么或者说有什么（猫、狗等）。图像分类是计算机视觉中基础的一个任务，也是几乎所有的基准模型进行比较的任务。从 开始比较简单的10分类的灰度图像手写数字识别任务mnist，到后来更大一点的10 分类的cifar10和100分类的cifar100 任务，到后来的imagenet 任务，图像分类模型伴随着数据集的增长，一步一步提升到了今天的水平。现在，在 imagenet 这样的超过1000万图像，超过2万类的数据集中，计算机的图像分类水准已经超过了人类。\n根据图片内容的不同，可以分为物体分类、场景分类和行为事件分类。\n根据分类的精细程度，可以分为粗粒度分类和细粒度分类。\n根据分类标签的相关性，可以分为单标签分类和多标签分类。\n图像分类问题的困难和挑战：刚体&非刚体的变化、多视角、尺度、遮挡、光照条件、类内差异，参考下图：\n单标签分类\n单标签分类是 简单的分类任务，图片的内容相对简单，只包含一个物体或者场景。ImageNet就属于单标签分类的数据集。下面通过ImageNet比赛的时间脉络，介绍下单标签分类的进展情况。\nAlexNet：2012年提出的AlexNet网络结构模型引爆了神经网络的应用热潮，并赢得了2012届图像识别大赛的冠军，使得CNN成为在图像分类上的核心算法模型。\nZFNet：2013年ILSVRC分类任务冠军网络是Clarifai，不过更为我们熟知的是ZFNet。Hinton的学生Zeiler和Fergus在研究中利用反卷积技术引入了神经网络的可视化，对网络的中间特征层进行了可视化，为研究人员检验不同特征激活及其与输入空间的关系成为了可能。在这个指导下对AlexNet网络进行了简单改进，包括使用了更小的卷积核和步长，将11x11的卷积核变成7x7的卷积核，将stride从4变成了2，性能超过了原始的AlexNet网络。\nVGGNet：2014年的亚军，VGGNet包括16层和19层两个版本，共包含参数约为550M。全部使用3×3的卷积核和2×2的 大池化核，简化了卷积神经网络的结构。VGGNet很好的展示了如何在先前网络架构的基础上通过简单地增加网络层数和深度就可以提高网络的性能。虽然简单，但是却异常的有效，在今天，VGGNet仍然被很多的任务选为基准模型。\nGoogLeNet：来自于Google的Christian Szegedy等人提出的22层的网络，其top-5分类错误率只有6.7%。GoogleNet的核心是Inception Module，它采用并行的方式。一个经典的inception结构，包括有四个成分。1×1卷积，3×3卷积，5×5卷积，3×3大池化，后对四个成分运算结果进行通道上组合。这就是Inception Module的核心思想。通过多个卷积核提取图像不同尺度的信息然后进行融合，可以得到图像更好的表征。自此，深度学习模型的分类准确率已经达到了人类的水平(5%~10%)。\nResNet：2015年获得了分类任务冠军。它以3.57%的错误率表现超过了人类的识别水平，并以152层的网络架构创造了新的模型记录。由于ResNet 采用了跨层连接的方式，它成功的缓解了深层神经网络中的梯度消散问题，为上千层的网络训练提供了可能。\nResNeXt：2016年依旧诞生了许多经典的模型，包括赢得分类比赛第二名的ResNeXt，101层的ResNeXt可以达到ResNet152的精确度，却在复杂度上只有后者的一半，核心思想为分组卷积。即首先将输入通道进行分组，经过若干并行分支的非线性变换，后合并。\nDenseNet：在ResNet基础上，密集连接的DenseNet在前馈过程中将每一层与其他的层都连接起来。对于每一层网络来说，前面所有网络的特征图都被作为输入，同时其特征图也都被后面的网络层作为输入所利用。DenseNet中的密集连接还可以缓解梯度消失的问题，同时相比ResNet，可以更强化特征传播和特征的复用，并减少了参数的数目。DenseNet相较于ResNet所需的内存和计算资源更少，并达到更好的性能。\nSeNet：2017年也是ILSVRC图像分类比赛的 后一年，SeNet获得了冠军。这个结构，仅仅使用了“特征重标定”的策略来对特征进行处理，通过学习获取每个特征通道的重要程度，根据重要性去降低或者提升相应的特征通道的权重。\n至此，图像分类的比赛基本落幕，也接近算法的极限。但是，在实际的应用中，却面临着比比赛中更加复杂和现实的问题，需要大家不断积累经验。\n目前，随着NASNet（Neural Architecture Search Network）的崛起，效果好的基本都是这些网络比如：NASNet、PNasNet、AmoebaNet，尤其是近Google新出的EfficientNet，更是对其他网络有碾压式的提升，下面的图片一目了然：\n细粒度图像分类\n细粒度图像分类 (Fine-Grained Image Categorization)，是对属于同一基础类别的图像（汽车、狗、花、鸟等）进行更加细致的子类划分（比如：区分狗的种类萨摩还是哈士奇）。细粒度分类有很多实际的应用场景，比如区分在交通监控中，识别不同的车型。\n由于分类的粒度很小，子类之间差异细微，只在某个局部上有细微差异（如狗的眼睛），甚至在某些类别上甚至专家都难以区分，再加上子类内部差异巨大，如姿态、背景带来的差异，还有受视角、背景、遮挡的干扰等，所以细粒度图像分类比粗粒度分类要困难，也因此还是目前比较热门的研究领域。\n由于深度卷积网络能够学习到非常鲁棒的图像特征表示，对图像进行细粒度分类的方法，大多都是以深度卷积网络为基础的，这些方法大致可以分为以下四个方向：\n1、基于常规图像分类网络的微调方法\n这一类方法大多直接采用常见的深度卷积网络来直接进行图像细粒度分类，比如ResNet、DenseNet、SENet等。由于这些分类网络具有较强的特征表示能力，因此在常规图像分类中能取得较好的效果。然而在细粒度分类中，不同物种之间的差异其实十分细微，因此，直接将常规的图像分类网络用于对细粒度图像的分类，效果并不理想。受迁移学习理论启发，一种方法是将大规模数据上训练好的网络迁移到细粒度分类识别任务中来。常用的解决方法是采用在ImageNet上预训练过的网络权值作为初始权值，然后再通过在细粒度分类数据集上对网络的权值进行微调（FineTune），得到 终的分类网络。\n2、基于基于网络集成的方法\n比较有代表性的是双线性卷积神经网络模型（Bilinear CNN），该方法使用VGG-D和VGG-M两个网络作为基准网络，通过Bilinear Pooling得到两个特征融合后的向量，进而用来分类。在不使用Bounding Box （边框）标注信息的情况下，在CUB200-2011数据集上到达了84.1%的分类精度，而使用\nBoundingBox时，其分类精度高达85.1%。\n3、基于目标块的检测(part detection)和对齐(alignment)的方法\n基于目标块（object part）检测的方法思路是：先在图像中检测出目标所在的位置，然后再检测出目标中有区分性区域的位置，然后将目标图像（即前景）以及具有区分性的目标区域块同时送入深度卷积网络进行分类。但是，基于目标块检测的方法，往往在训练过程中需要用到目标的Bounding box标注信息，甚至是目标图像中的关键特征点信息，而在实际应用中，要想获取到这些标注信息是非常困难的。比较有代表性的是2014年ECCV中提出来的Part-RCNN方法。\n4、基于视觉注意机制(visual attention)的方法\n视觉注意机制(Vision Attention Mechanism)是人类视觉所特有的信号处理机制。具体表现为视觉系统在看东西的时候，先通过快速扫描全局图像获得需要关注的目标区域，而后抑制其他无用信息以获取感兴趣的目标。在深度卷积网络中，同样能够利用注意模型来寻找图像中的感兴趣区域或区分性区域，并且对于不同的任务，卷积网络关注的感兴趣区域是不同的。由于基于视觉注意模型（Vision Attention Model）的方法可以在不需要额外标注信息（比如目标位置标注框和重要部件的位置标注信息）的情况下，定位出图像中有区分性的区域，近年来被广泛应用于图像的细粒度分类领域。代表性的工作是17年CVPR中提出的循环注意卷积神经网络(Recurrent Attention Convolutional Neural Network, RA-CNN)。\n目前所有细粒度图像识别任务均需借助大量、甚至海量的标注数据。对于细粒度图像而言，其图像收集和标注成本巨大。如此便限制了细粒度研究相关的发展及其在现实场景下的应用。反观人类，我们则具备在极少监督信息的条件下学习新概念的能力，例如，对于一个普通成年人可仅借助几张图像便学会识别鸟类的一个新物种。为了使细粒度级别图像识别模型也能像人类一样拥有少量训练样本下的学习能力，研究人员也在研究细粒度级别图像识别的少量样本学习任务，这可能也是将来的发展趋势。\n多标签分类\n前面所说的分类，全部都是单标签分类问题，即每一个图只对应一个类别，而很多的任务，其实是多标签分类问题，一张图可以对应多个标签，相比于多类别图像分类，多标签任务的难度更大，因为其输出空间随着类别数目呈指数增大。多标签分类问题通常有如下的策略：一阶策略：朴素的方法，忽略和其它标签的相关性，分离地看待各个目标，比如把多标签分解成多个独立的二分类问题（简单高效）。\n二阶策略：考虑标签之间的成对关联，比如为相关标签和不相关标签排序。\n高阶策略：考虑多个标签之间的关联，比如对每个标签考虑所有其它标签的影响（效果优）。\n稍微展开讲讲高阶策略：由于现实世界中很多物体通常会同时出现，因此对标签之间的相关性进行建模就成了多标签图像识别的关键，如下图所示：\n大体上有两个方向，可以对多个角度探索标签之间相关性进行建模。一个是基于概率图模型或循环神经网络（RNN），显式地对标签依赖性进行建模。另一个是通过注意力机制来对标签相关性进行隐式建模。该方法考虑的是图像中被注意区域之间的关系（可视为局部相关性）。不过即便如此，该方法还是忽略了图像中标签之间的全局相关性（全局相关性需要通过单张图像之外的知识才能推断出来）。\n例如：ML-GCN使用图（Graph）来对标签之间的相互依赖关系进行建模。能够灵活地获取标签空间中的拓扑结构，在MS-COCO和VOC2007测试集上都取得了 有的结果。\n目标检测进展\n目标检测任务的目标是给定一张图像或是一个视频帧，让计算机找出其中所有目标的位置，并给出每个目标的具体类别，它结合了目标分类和定位两个任务，通俗的说就是要机器告诉图片中有什么同时告诉在哪里。检测是很多计算机视觉应用的基础，比如实例分割、人体关键点提取、人脸识别等。现代大多数目标检测器的框架是Two-Stage，其中目标检测被定义为一个多任务学习问题：\n（1）区分前景物体框与背景并为它们分配适当的类别标签；\n（2）回归一组系数使得大化检测框和目标框之间的交并比（IoU）或其它指标。后，通过一个 NMS 过程移除冗余的边界框（对同一目标的重复检测）。\nAnchor-Based方法\n传统Anchor-Based方法，都是用策略提出一些候选框（prior box or anchor box），然后对这些候选框做分类和位置的归回。方法是对这些框所对应的 featuremap向量作分类（softmax）或者回归（线性回归），得到box的位置和类别。\nOneStage算法是直接在网络中提取特征来预测物体分类和位置，Two Stage算法是指首先生成proposal，然后进行细粒度的物体检测。\n现代大多数目标检测器的框架是两步进行：\n（1）RPN：区分前景物体框与背景并为它们分配适当的类别标签；\n（2）回归一组系数使得大化检测框和目标框之间的交并比（IoU）或其它指标后，通过一个 NMS 过程移除冗余的边界框（对同一目标的重复检测）。\n目标检测的重要技术路线图如下图描述的很清晰：\n图中的里程碑检测器: VJ Det， HOG Det，DPM， RCNN， SPPNet， Fast RCNN， Faster RCNN， YOLO， SSD， PyramidNetworks，RetinaNet。\n如下是各个检测模型在VOC07、VOC12和MS-COCO数据集上的检测结果图：\n篇幅所限，以后有机会再对每个具体的检测器做具体的展开讲解。\nAnchor-Free方法\n自从去年8月CornerNet开始，Anchor-Free的目标检测模型层出不穷，近开始热门起来。所谓Anchor-Free是指检测时不用现预设一些参考的Anchor-\nBox，而是直接通过模型预测目标的位置和类别，比如通过关键点的方式。\n其实 Anchor-Free并不是一个新概念了， 早可以追溯的百度的DenseBox模型（此模型2015年提出，比Fast-RCNN还要早），大火的YOLO也算是目标检测领域的Anchor-Free模型，而 近的Anchor-Free模型如FASF、FCOS、FoveaBox都能看到DenseBox的影子。比较有代表性的Anchor-Free模型有：DenseBox、YOLO、CornerNet、ExtremeNet、FSAF、FCOS、FoveaBox。\n虽然目前Anchor-Free的方法还没有完全胜过传统的Anchor-Based方法，但是确实提供一种可行新的检测流程，主要的是对于BoundingBox是否是检测的 合理的表达，提出了挑战，后面随着Anchor-Free模型的演进，可能会产生出搞好的目标表达方式。\n图像分割进展\n图像分割就是把图像分成若干个特定的、具有独特性质的区域并提出感兴趣目标的技术和过程，可以被看作是一个逐像素的图像分类问题。分割任务主要分为语义分割（semantic segmentation）、实例分割（instancesegmentation）以及今年刚兴起的新领域全景分割（panoptic segmentation），上图展示了不同分割的区别。\n稍微展开说明一下不同分割任务：\n语义分割：语义分割更注重「类别之间的区分」，语义分割会重点将前景里的人群和背景里树木、天空和草地分割开，但是它不区分人群的单独个体，如图中的人全部标记为红色，导致右边黄色框中的人无法辨别是一个人还是不同的人。主要模型有U-Net、SegNet、DeepLab系列、FCN、ENet、ICNet、ShelfNet、BiseNet、DFN和CCNet等网络。\n实例分割：更注重「个体之间的区分」，实例分割这个问题近几年的发展在很大程度上是由 COCO 数据集和比赛推动的。从 MNC，FCIS 到PANet，都是在 COCO instance segmentation track 上拿第一名的方法。主要模型有FCIS、DeepMask、MaskR-CNN 、Hybrid Task Cascade（HTC）、PANet 等网络。\n全景分割：新的子任务， 先由FAIR与德国海德堡大学联合提出，可以说是语义分割和实例分割的结合，全景分割任务下，图像内的每个像素点都有其对应的语义标签和实例标签，从而能够大程度上地理解整幅图像。主要模型有JSIS-Net、TASCNet等。\n图像分割模型\n图像分割大体框架或者说流程如下：\n下采样+上采样：Convlution + Deconvlution／Resize。\n多尺度特征融合：特征逐点相加／特征channel维度拼接。\n获得像素级别的segement map：对每一个像素点进行判断类别。\n下图展示了图像分割进展的技术图谱：\n1、FullyConvolutional Networks (FCN)：这是神经网络做语义分割的开山之作，提出了全卷积网络。将全连接网络替换成了卷积网络，使得网络可以接受任意大小的图片，并输出和原图一样大小的分割图。只有这样，才能为每个像素做分类。使用了反卷积层（Deconvolution），特征图进行上采样。\n2、SegNet在FCN的基础上增加了解码器，形成目前分割任务中 流行的编解码结构，并给出了不同解码器对效果的影响和原因。\n3、DeepLabv1/v2/v3：引入了带洞卷积（Dilated Convolution or Atrous Convolution），使得视野更大了。\n4、PSPNet：核心贡献是全局金字塔池化（Global Pyramid Pooling），将特征图缩放到几个不同的尺寸，使得特征具有更好地全局和多尺度信息。\n5、MaskR-CNN：将Object Detection与SemanticSegmentation合在了一起做，提出了RoiAlign用来替换RoiPooling，消除了取整导致的偏移问题，提高了检测精度。\n6、U-Net：采用了编解码结构，编码部分，每经过一个池化层就构造一个新的尺度，包括原图尺度一共有5个尺度。解码部分，每上采样一次，就和特征提取部分对应的通道数相同尺度融合。这样就获得了更丰富的上下文信息，在Decode的过程中通过多尺度的融合丰富了细节信息，提高分割的精度。\n抠图（Image Matting）\nMatting也是一类前背景分割问题，但是matting不是硬分割，而是软分割（Soft Segmentation），像玻璃、头发这类前景，对应像素点的颜色不只是由前景本身的颜色决定，而是前背景颜色融合的结果，matting问题的目标就是，找出前背景颜色，以及它们之间的融合程度。\n抠图（ImageMatting）只将图片分成前景和背景两块，目的是拿到前景，好的抠图算法会对于头发等细节的处理效果比较精确。抠图和分割的重要区别是分割是返回像素分类结果，分类结果是整型；而抠图返回的是属于前景或背景的概率p，在前景与背景交互区域会产生渐变的效果，使得抠图更加自然。\n抠图技术的核心问题是解公式：I = αF + (1-α)B，其中I是图像当前可观察的像素，为已知量；α是透明度，F是前景像素，B是背景像素，这三个变量为未知量。对于这个公式的理解，可以把原始图像看做是前景和背景按照一定权重（α透明度）叠加组成的。对于完全确定是前景的像素，α = 1；对于完全确定是背景的像素，α = 0；对于不确定是前景还是背景的像素，α是介于0到1之间的浮点数。\n优秀的抠图算法是有能力将前景中非常细小的毛发细节都能提取出的好算法，这点是传统图像分割技术做不到的。\n现在深度学习也慢慢引入了Image Matting，基本上用的方法也是Encoder-Decoder框架，只是训练数据的GroundTruth变成了trimap。比较有代表性的是adobe搞的end to end的方案DeepImage Matting。\n由于应用场景没有其他分割广泛，再加上数据集和benchmark的欠缺，导致Matting技术没有其他的分割技术热度高。\n后续\n当然计算机视觉不止这些任务，分类、检测和分割只是计算机视觉的最基础的任务，而这些任务因为其基础性和通用性，在其他任务中都会用的到。比如人脸领域，也会用到检测和分类，在做特效的时候也会用到分割。而文中介绍的基础的网络结构，比如ResNet、GoogleNet等，在其他任务中也会用到。\n深度视觉领域，除了这些还有很多没有涉及到，比如关键点检测、视频分类、视频检测和追踪、生成对抗网络（GAN）、自动学习（AutoML），垂直领域的人脸识别、光学字符识别（OCR）、行人再识别，包括常用的深度学习框架tensorflow、pytorch等，还有一直在研究的无/弱监督学习、自监督学习，增强学习等，每个子领域展开讲都需要很大的篇幅，后面会再介绍这些方向的进展。\n参考资料：\n1、https://blog.csdn.net/xys430381_1/article/details/89640699\n2、https://medium.com/atlas-ml/state-of-deep-learning-h2-2018-review-cc3e490f1679\n3、https://zhuanlan.zhihu.com/p/57643009\n4、https://zhuanlan.zhihu.com/p/62212910\n5、https://cloud.tencent.com/developer/article/1428956\n本文首发于公众号“小米云技术”，转载请注明出处。"}
{"content2":"现在去xxx创业大街向天上扔一沓美金，砸到10个路人，有9个都自称投资人，其中8个投人工智能。\n满嘴的Artificial Intelligence、摩尔定律、大数据、新算法、认知技术、计算机视觉、机器学习、自然语言处理、机器人技术、语音识别……要是不懂点Deep Learning基本原理，没看过雷·库兹韦尔的《奇点临近》都不好意思和别人打招呼。\n事实真的是这样吗？23年后人工智能会统治地球吗？说实话，这些暂时和你没有太大关系，你只需要知道，人工智能的第一波红利已来临！\n在2016 Techcrunch论坛上，李开复曾经说过：“当人工智能识别人脸，超过人的时候，保安的工作至少一部分就没有了；当人工智能能够听懂语音的时候，客服和打电话卖东西人的工作就没有了；当人工智能能够更聪明的炒股的时候，很多人的工作就没有了。”\n今年是人工智能发展的第61个年头，所有的投资机构都在谈论和布局该领域，但人工智能将在哪些产业爆发？传统企业又该如何挖掘第一桶金？要知道，每个风口都只有2%的人能够成为赢家。\n黑马学院特邀CSDN&极客帮基金创始人蒋涛（黑马连营第5期连长），今晚9点-10点，将在103个黑马社群同步直播，讲讲人工智能的第一波红利！\nWHY 人工智能为什么会火？\n谈到科技革命，时下最火的莫过于人工智能。\n我自己做技术社区，做了十几年，看到过一波一波的技术浪潮过来。当年移动互联网大潮过来的时候，我们原来在PC端做的事情，都可以用移动互联网的理念把它重新做一遍。简单来说，现在的人工智能就相当于原来移动互联网的概念，原来移动端做过的事情，现在又可以结合人工智能的方式再做一遍，而且会比之前更具颠覆性的效果。毫不夸张地说，我认为人工智能所带给我们的冲击，将会像工业革命一样。\n今年是「人工智能」诞生的61周年（注：1956年夏“人工智能之父”麦卡锡首次提出这个概念），同时也是它的第三次浪潮。这次浪潮和以往的前两次都不一样，这次有了实质性的突破。以前，相当于你想到对面去，但是面前有一堵墙；现在这堵墙被凿开了，之所以这么讲，是因为我有以下三点的观察思考：\n第一点：人工智能虽然还处于技术创新期，但到人工智能的普及期，我认为也就需要十到二十年的时间。现在相当于移动时代的2005年，虽然第一部3G手机2007年才出现，但2005年我们已经很清楚2G是要到3G的。虽然人工智能还没有找到突破口，不知道会怎么商业化，但是大方向是有的。所以，现在只要你在这个领域冲到第一名，就会持续得到投资。\n第二点：人工智能的基础已经充实，它是一个逻辑上的发展，这个发展可以分为三个阶段。1）云计算，把信息基础云化，云计算基础设施的完善使得人工智能响应速度更快。2）大数据，计算的过程中累积了数据，数据的极大丰富，使得基于大数据做出行为分析及短期预判成为可能，各个行业的信息化也为此奠定了良好的基础。3）判断决策，对大数据的判断从而产生了更好的决策，决策实际上就是人工智能的进展。\n现在我们的生活中就有很多计算机技术在做决定，坐车是滴滴在帮你调度；去餐馆是大众点评用算法把离你最近、人气最高的餐馆选出来等等，所以你的生活已经和人工智能相关了。\n第三点：人工智能之所以取得重大的突破，除了前两个阶段的铺垫外，深度学习的发展也贡献了非常重要的力量。\n未来是“AIR”的世界\n未来你看到的是物理世界和虚拟世界的叠加，这就是VR和AR，也就是R时代；I时代是物联网时代；A时代也就是人工智能时代。\n其实你看到的世界很可能不是真的。为什么这么说？因为这是从视神经系统处理出来的。现在我们可以用计算机处理掉，叠加到视网膜上。比如：我不用递名片，只要念头一转，你的名片就应该自动出来了。那怎么能做到这样呢？就是把所有的信息都连接到网络上，这样物理世界和虚拟世界才能叠加在一起。人之所以和其他生物不一样是因为人会做思考、决策，比动物要高一个级别，具有抽象的能力，这是未来20年的大趋势。\nAI技术体现在图像识别的突破上，更重要的可能是智能语言的突破，他能理解你讲的话，写出来的文字，甚至能理解照片，当做到这些的时候，行业就会产生变革。\n各个行业基本可分为4个阶段：数字化、数据化、自动化和智能化。越到后面它的武器越强。原来是长枪、大矛，练的是武功，后面就变成机关枪了，扫射的时候你会发现不一样。\n今年我们做了AI100，也叫人工智能100年。今年是人工智能的第61年，可能再过20年到40年，这个世界90%的人就不用工作了。在未来，我们要培养200万名数据分析师，因为决策和运营都是用数据驱动的。同时我们也会与投资相结合，帮助中国30万家企业走向智能化阶段。\nWhere 第一波红利的三个产业\n人工智能究竟会改变哪些领域？如何改变呢？\n1． 自动驾驶。\n任何领域有非常大的数据量，人工智能都可以用上。全世界和运输价值相关的公司，都已经相信无人驾驶的发展是必然的。所以在无人驾驶、电动车的框架之下，未来的司机基本上会被无人驾驶取代。\n单车智能与智慧交通是无人驾驶技术发展的两个阶段。其中，单车智能是无人驾驶技术的基础，是实现无人驾驶终极形态的根本路径；车联网与智能交通则是推进无人驾驶技术发展的强力催化剂，将助力无人驾驶技术的普及。无人驾驶技术的成熟将最终构建城市智能驾驶生态圈，为未来出行提供新的解决方案。\n2. 客服行业。\n做金融服务的宜信，有1万多个客服，携程大概有7、8千个客服，每天负责就接各种投诉电话，每次都是被用户狂骂、抱怨，未来将有更多工业机器人替代这些低效率的人力。 人工智能客服系统主要是整合邮件、电话、微博、微信、网页、API接口、移动SDK等渠道在内的服务渠道，并统一自动分配工单，同时留存用户信息便于下次咨询时识别。\n基本能做到：1）24小时机器人客服在线，随时响应客户的相关资讯和需求；2）建立客服机器人的内容库，用深度学习的方式自动回复重复问题；3）接入人工时机器人给予部分回复建议，加快反馈速度；4）接入内部办公系统，推动多部门协作反馈以及用户精准营销；5）后台实时数据统计汇总，管理用户评价，进行数据挖掘和数据分析；\n3. 医疗领域。\n医疗人员医院里有大量的临床病历数据，而且不断的产出数据。医疗方面的人工智能主要分为两部分：一是图像识别，应用于感知环节，其主要目的是将影像这类非结构化数据进行分析，获取一些有意义的信息；二是深度学习，应用于学习和分析环节，是AI应用的最核心环节，通过大量的影像数据和诊断数据，不断对神经元网络进行深度学习训练，促使其掌握“诊断”的能力。不管是什么病，图像只是其中一个参数，而治疗疾病则需要多个参数。此外，80%的数据属于非结构化数据，亦即报告+影像。未来，人工智能的使用将大大提高治疗效率。\nHow 传统企业如何获得红利？\n简单来说，现在的人工智能就相当于原来移动互联网的概念，原来移动端做过的事情，现在又可以结合人工智能的方式再做一遍，而且会比之前更具颠覆性的效果。毫不夸张地说，我认为人工智能所带给我们的冲击，将会像工业革命一样。\n传统制造业智能化设想\n传统企业的未来有两个方向：第一，你有没有用户的服务和连接。第二，在这个基础上，有没有做智能化决策和分析。后来就是本身在生产线上的提升，生产线也面临着一个问题，你的控制有没有数据化。\n在一些制造业工厂，升级之后有没有更好的成本上的控制。极客帮有个合作的LP，专门给小米做代工，他就给我看传统的生产线，就是一条生产线，都是自动化的设备。所以它能够做到手环40元钱，还有利润挣。就卖40元钱还能挣钱，而且挣得还不少。总的来说，我们看到的一个方向的趋势。\n对制造业来说，意义最大的是把它的制造过程，原来的控制系统做成一定的数据化，再看看能不能做规划。那些大规模生产的时代已经结束，一定要往前端走，差异化可能在产品设计上、包装上、服务上，从大生产时代到个性化时代，未来到一个智能化时代。\n“创业者埋头苦干的同时也要抬头看天。欢迎加入我的黑马连营战队，和我一起探寻人工智能趋势背后的行业商机！”\n尽管人类是科技的创造者，却无法掌控其发展。我们所能做的就是掌握科技的发展趋势。人工智能浪潮来临，它将如何颠覆我们的工作和生活呢？\n近日，黑马营7期营员、黑马连营第5期连长蒋涛与我们分享了他的看法。他认为，未来是AIR的世界，各个行业将会进入数字化、数据化、自动化和智能化4个阶段。\n嘉宾介绍：\n蒋涛：CSDN创始人，极客帮基金创始合伙人；黑马营7期营员，黑马连营第5期连长；有24年软件开发经验，其创立的CSDN是全球最大的开发者社区；同时具备投资人身份，先后投资了聚合数据、IT桔子等90余家高科技创业公司。\n行业：人工智能\n关键词：趋势洞察家、技术达人、极客范儿\n报名请微信扫描下方二维码：\n本文来自CSDN创始人、极客帮基金创始合伙人、黑马营7期营员、黑马连营第5期连长蒋涛的分享，陈雪娇整理。"}
{"content2":"http://blog.sina.com.cn/s/blog_7136439f0102v7u3.html\n以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/首页\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）清华大学程明明博士：http://mmcheng.net/ 图像分割、检索\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/\n（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；\n（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、\n（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库\n（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理\n（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、\n（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n（213）北京航空航天大学周付根教授：http://www.sa.buaa.edu.cn/html/yhxy/szll/jsfc/txcl/1521.html  医学图像处理\n（214）北京航空航天大学姜志国教授：http://www.sa.buaa.edu.cn/html/yhxy/xygk/zyld/1595.html 医学图像处理，遥感图像分割；"}
{"content2":"▼\n点击上方蓝字 关注网易智能\n为你解读AI领域大公司大事件，新观点新应用\n\n【网易智能讯3月18日消息】2017年，人工智能获得了120亿美元的风险投资。而我们才刚刚开始发现AI应用程序的有用性。亚马逊最近推出了一个实体杂货店，并成功地用计算机视觉、传感器和深度学习等技术取代了收银员和收银台。在投资、新闻报道和重大创新领域，“人工智能（AI）”已经成为热门话题。但到目前为止，它是否真的存在呢？\n在世界经济论坛上，李开复博士表示：“我认为每个企业家都希望把他或她的公司打包成一家人工智能公司，每个风险投资者也希望把自己描述成‘AI投资者’。”他还观察到，这些AI泡沫中的一些可能会在2018年底破灭，具体指的是“那些编造了一个无法实现的美好故事的创业公司，以及那些被欺骗的风投公司，因为他们并不了解具体情况。”\n然而，李开复坚信人工智能将继续取得进步，并将从工人身上夺走许多就业机会。那么，合法的、有优点也有缺点的人工智能和那些虚构故事的区别是什么？\n如果仅仅通过一些涉及人工智能的故事进行解析，那么你很快就会发现不同的人对它的定义有很大的区别，仿真智能和机器学习应用程序之间的界限也很模糊。我与人工智能领域的专家进行了交流，试图找到一个共识，但这个问题带来了更多的问题。例如，什么时候必须遵循一个术语的原始定义？哪些时候定义的准确性并没有那么重要？这不是显而易见的，而关于人工智能的大肆宣传中并不在意这些细节。此外，现在在这种宣传中已经有了既得利益——准确地说，是120亿美元。\n世界知名的思想领袖们已经公开辩论了人工智能造成的危险。Facebook首席执行官马克·扎克伯格建议，反对者试图“鼓吹这些世界末日的情景”的行为是消极和不负责任的。在Twitter上，商业巨头及OpenAI联合创始人埃隆·马斯克反驳说，扎克伯格对这个问题的理解是片面的。今年2月，埃隆·马斯克再次与哈佛大学教授Steven Pinker进行了类似的交流。埃隆·马斯克在推特上称，Steven Pinker并不了解功能性AI（狭义AI）和普通AI之间的区别。\n鉴于围绕这项技术产生的担忧，公众必须清楚了解不同层次的人工智能之间的区别，以便他们能够真实地评估潜在的威胁和收益。\n像人一样智能？\n自然语言处理领域的专家Erik Cambria告诉我：“今天其实没有人在做AI，但是每个人都说他们在做AI，因为这是一个很酷而且很性感的流行语，与几年前的‘大数据’一样。”\nCambria提到，人工智能这个术语，最初的来源是人类智能。“而到今天甚至都没有什么能像地球上最愚蠢的人类一样聪明。因此，从严格意义上说，没有人在做AI，因为我们不知道人类大脑是如何工作的这个简单事实，“他说。\n他补充说，术语“人工智能”通常指的是强大的数据分类工具。这些工具令人印象深刻，但它们与人类认知完全不同。此外，Cambria已经注意到，人们声称神经网络是人工智能新浪潮的一部分。他认为这很奇怪，因为这项技术在五十年前就已经存在了。\n但是，技术人员现在已经不需要人工进行特征提取了。他们还获得了更强大的计算能力。所有这些进步都受到欢迎，但是“机器模拟了人类复杂的认知过程”的说法可能是不准确的。\n“各个公司只是在寻找技巧来创造看起来像智能的行为，但这不是真正的智能，它只是智能的一面镜子。这些专家系统在特定领域可能非常好，而在其他领域会非常愚蠢，”他说。\n这种对智力的模仿激发了公众的想象力，特定领域的系统也已经在众多行业中实现了其价值。但这些并不能改变人工智能的真正定义。\n辅助智能，增强智能与自主智能\n当提及科学完整性的问题时，准确的定义是很重要的。在1974年加州理工学院开学典礼上，理查德·费曼说了一句话，后来广为流传：“首要原则是你不能欺骗自己——而你是最容易被欺骗的人。”在同一篇演讲中，费曼也说过：“你当你作为一个科学家说话时，不应该欺骗外行。”他认为科学家应该向后弯腰，以表明他们可能是错的。“如果你把自己视为一名科学家，那么你应该向外行解释你在做什么——如果在这种情况下他们还不想支持你，那么这就是他们的决定。”\n就人工智能而言，这可能意味着专业科学家有义务清楚地声明，他们正在开发极其强大的、有争议的、有利可图甚至危险的工具，而这些工具不构成任何人们熟悉的或者全面意义上的智能。\n“人工智能”这个术语可能已经被过度宣传从而造成了人们的迷惑，但已经有一些努力使它变得清晰。普华永道最近的一份报告提出了“辅助智能”，“增强智能”和“自主智能”之间的区别。辅助智能的代表是当今汽车普遍使用的GPS导航程序。增强智能可以“使人们和组织做他们不能做的事情。”自主智能则“建立自主行动的机器”，比如自动驾驶汽车。\nRoman Yampolskiy是一位人工智能安全研究员，他撰写了“Artificial Superintelligence: A Futuristic Approach.”一书。我问他，人工智能的不同含义是否会给试图调节人工智能的立法者带来困难。Yampolskiy解释说：“（人工或自然）智能是一个连续统一体，所以这些技术可能存在潜在的问题。我们通常将人工智能（AI）称为强人工智能（AGI），以避免一些混淆。除此之外的情况，它就是超级智能。我们今天讨论的以及经常用于商业的是狭隘的AI。规范任何事情都很难，技术也不例外。问题不在于术语，而在于目前的这种系统的复杂性。“\n当被问及人们是否应该担心人工智能系统时，Yampolskiy博士评论说：“由于能力是连续的，所以与每个能力等级相关的问题都是如此。”他提到，已经有事故发生在了使用人工智能的产品上，而随着技术进一步发展，这种影响可能蔓延到隐私问题或技术失业之外。这些关于人工智能现实世界效应的担忧可能会超过人们对于人工智能固定行为模式的担忧。然而，这个问题也关乎诚实与欺骗的问题。\n“人工智能”已然成为流行语？\n最后，我将我的问题转向了一家正在积极推销“AI虚拟助手”的公司。“Conversica的首席营销官Carl Landers承认，对于人工智能是什么、不是什么的问题存在多种解释。\n他说：“我对AI的定义是有助于解决业务问题的技术创新。我真的不想从理论上谈论，‘我们能让机器像人类一样思考吗？’这是一个很好的问题，但我试图解决的是一个实际的业务问题。”\n我问他“人工智能”是否是用于宣传并吸引客户的流行词。Landers说，在三年前确实是这样的，但这些影响已经开始减弱。现在许多公司声称在他们的产品中都有AI，所以它并不是区别这些产品的关键。但是，这个词背后还有一个特定的意图。Landers希望传达的是，以前不可能的事情现在是可能的。“这里有一些你以前没有见过的新东西，你以前没有听说过，”他说。\n根据EncomLab的创始人Brian Decker的说法，机器学习算法只能满足他们先前存在的编程需求，而不是为了更好地理解内部驱动。因此，他认为人工智能是一个完全的语义论证。\nBrian Decker表示：“一位营销执行官会声称光电二极管控制的门廊灯使用了人工智能，因为‘它知道外面什么时候变黑暗’，而一位好的硬件工程师会指出，在整个过程中廊灯的工作系统只会根据事先编程好的系统做出改变。”\n虽然每个人都必须关注具体内容和潜在意义，但人工智能驱动的产品已经通过为人类创造直接价值而推动了这些争论。最终，人类会更关心价值，而不关心语义上的区别。在接受Quartz采访时，李开复透露，算法交易系统已经让他的私人银行投资回报8倍。“我不再与人类交易了，”他说。\n选自：SingularityHub\n编译：网易智能\n参与：李擎\n网易智能\n聚焦AI 读懂下一个大时代\n长按扫码关注我们\n加入社群\nAI社群（AI专家群、AI黑板报）火热招募中，欢迎对AI感兴趣的小伙伴，添加小助手微信kaiwu_club，说明身份即可加入。\n往期精华\n人物专访  沈向洋 | 于尔根 | 洪小文 | 李德毅 | 尤瓦尔 | 哈萨比斯 | 宋继强 | 杨强 | 余凯 | 邓志东 | 芮勇 | 戴文渊 | 石博盟 | 韦东 | 黄学东\n行业特稿  抢滩智能音箱（上） | 抢滩智能音箱（下） | 自动驾驶行业素描（上） | 自动驾驶行业素描（下） | 机器翻译技术与应用\n重磅报告  麦肯锡（二） | AlphaBeta & ABC | 埃森哲 |  英国政府 | Internet Society | Forrester | VertoAnalytics | 麦肯锡（一） | 苹果公司 | 耶鲁大学"}
{"content2":"与所有其它学术领域都不同，计算机科学使用会议而不是期刊作为发表研究成果的主要方式。目前国外计算机界评价学术水平主要看在顶级学术会议上发表的论文。特别是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。（但中国目前的国情不同于国外，我国主要看在学术期刊上发表的SCI论文。这种“一切以SCI期刊为评价标准”的做法已有不少批评。）\n会议论文比期刊论文更重要的原因是：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n作为刚入门的CV新人，有必要记住计算机视觉方面的三大顶级国际会议：ICCV，CVPR和ECCV，统称为ICE。\nICCV的全称是International Comference on Computer Vision（上一篇文章介绍我自己的id的时候介绍过，呵呵），正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。\nCVPR的全称是International Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster挑自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。\nICCV/CVPR/ECCV三个顶级会议, 都在一流会议行列, 没有必要给个高下. 有些us的人认为ICCV/CVPR略好于ECCV,而欧洲人大都认为ICCV/ECCV略好于CVPR。\n笔者就个人经验浅谈三会异同, 以供大家参考和讨论. 三者乃cv领域的旗舰和风向标,其oral paper (包括best paper) 代表当年度cv的最高水准, 在此引用Harry Shum的一句话, 想知道某个领域在做些什么, 找最近几年此领域的proceeding看看就知道了. ICCV/CVPR由IEEE Computer Society牵头组织, ECCV好像没有专门负责的组织. CVPR每年(除2002年)都在美国开, ECCV每两年开一次,仅限欧洲, ICCV也是每两年一次, 各洲轮值. 基本可以保证每年有两个会议开, 这样研究者就有两次跻身牛会的机会.\n就录取率而言, 三会都有波动. 如ICCV2001录取率>30%, 且出现两个人(华人)各有三篇第一作者的paper的情况, 这在顶级牛会是不常见的 (灌水嫌疑). 但是, ICCV2003, 2005\n两次录取率都很低, 大约20%左右. ECCV也是类似规律, 在2004年以前都是>30%, 2006年降低到20%左右. CVPR的录取率近年来一直偏高, 从2004年开始一直都在[25%,30%].最近一次CVPR2006是28.1%, CVPR2007还不知道统计数据. 笔者猜测为了维持录取paper的绝对数量, 当submission少的时候录取率偏高, 反之偏低, 近几年三大会议的投稿数量全部超过1000, 相对2000年前, 三会录取率均大幅度降低, 最大幅度50%->20%. 对录取率走势感兴趣的朋友, 可参考http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的),http://www.adaptivebox.net/research/bookmark/CICON_stat.html .\n显然, 投入cv的人越来越多,这个领域也是越来越大, 这点颇不似machine learning一直奉行愚蠢的小圈子主义. 另外一点值得注意, ICCV/ECCV只收vision相关的topic, 而cvpr会收少量的pattern recognition paper, 如fingerprint等, 但是不收和image/video完全不占边的pr paper,如speech recognition等. 我一个朋友曾经review过一篇投往CVPR的speech的paper, 三个reviewer一致拒绝, 其中一个reviewer搞笑的指出, 你这篇paper应该是投ICASSP被据而转投CVPR的. 就topic而言, CVPR涵盖最广. 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会, 故CVPR会优先接收很多来自us的paper (让大家都happy).\n以上对三会的分析对我们投paper是很有指导作用的. 目前的research我想绝大部分还是纸上谈兵, 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程. 故了解投paper的一些基本技巧, 掌握领域的走向和热点, 是非常必要的. 避免做无用功,选择切合的topic, 改善presentation, 注意格式 (遵守规定的模板), 我想这是很多新手需要注意的问题. 如ICCV2007明文规定不写summary page直接reject, 但是仍然有人忽视, 这是相当不值得的.\nhttp://blog.csdn.net/monanzyq/article/details/51495679"}
{"content2":"来源：科技行者\n概要：计算机算法可以从面相判断一个人的性取向，引发了对隐私、道德、伦理问题的争议。然而回过头去看，在人工智能领域，它是图像识别和机器人视觉的核心部分。\n最近斯坦福大学一篇论文《Deep neural networks are more accurate than humans at detecting sexual orientation from facial images》一出，舆论哗然，该论文研究指出，计算机算法可以从面相判断一个人的性取向，引发了对隐私、道德、伦理问题的争议。然而回过头去看，这原本是一个卷积神经网络应用的技术文章，在人工智能领域，它是图像识别和机器人视觉的核心部分。\n图像识别技术，是人工智能道路上的一座高峰，如今你可以看到包括个人相册图片管理、刷脸解锁手机、刷脸上班打卡等广泛应用。你一定好奇，图像识别是什么？如何让机器理解一张图甚至一个动态的生物？背后又用到了哪些技术？\n今天，我们就从源头挖一挖图像识别的概念、技术和应用。\n什么是“图像识别”？\n从概念来看，图像识别是指利用计算机对图像进行处理、分析和理解，以识别不同模式的目标和对像（人物、场景、位置、物体、动作等）的技术。\n而图像识别算法一般采用机器学习方法，模拟人脑看图，随后计算机依靠大量的数据，理解图像，最后建立相关的标签和类别。整个识别过程的核心，就是神经网络，经过优胜劣汰，目前已经发展到卷积神经网络（CNN或ConvNets）。\n据不完全统计，科学家们从神经网络研究到卷积神经网络，就花了从20世纪60年代末到20世纪80年代末的时间。\n让计算机看见，经历了一个剥茧抽丝的神经网络演进过程\n我们先来看，人如何辨识物体。人脑的神经细胞（神经元）包括很多彼此相邻并相连的层，层数越多，网络越“深”。单个神经元从其他神经元接收信号——可能高达10万个，当其他神经元被触发时，它们会对相连的神经元施加兴奋或抑制作用，如果我们的第一个神经元输入加起来达到一定阈值电压（threshold voltage）时，它也会被触发。\n也就是说，人不但可以用眼看字，当别人在他背上写字时，他也认得出这个字来。就好比下图，人一眼看过去，就能感知到图片中存在某种层级（hierarchy）或者概念结构（conceptual structure），一层一层的：\n地面是由草和水泥组成，图中有一个小孩，小孩在骑弹簧木马，弹簧木马在草地上。\n关键点是，我们知道这是小孩，无论小孩在哪种环境都认识，因此人类不需要重新学习小孩这个概念。\n但机器不同，它需要经过多次反复的学习过程。我们再来看，机器如何辨识物体。在人工神经网络中，信号也在“神经元”之间传播，但是，神经网络不是发射电信号，而是为各种神经元分配权重。 和权重较小的神经元相比，权重更大的神经元会对下一层神经元产生更多的作用，最后一层将这些加权输入放在一起，以得出答案。\n比如，要想让一个计算机认出“猫”，需要建立一组数据库，包含数千张猫的图像和数千张不含猫的图像，分别标记“猫”和“不是猫”，然后，将图像数据提供给神经网络，最终输出层将所有信息——尖耳朵、圆脸、胡须、黑鼻子、长尾巴——放在一起，并给出一个答案：猫。这种训练技术被称为监督学习（supervised learning）。\n还有一种技术叫做无监督学习（Unsupervised learning），就是使用未标记的数据，计算机必须自己看图识物，比如从“尖耳朵”辨别这是一只猫而不是其他动物。然而这些方法容易误导机器，误把“尖耳朵”猫识别成狗，或者把浣熊猫误认为暹罗猫。\n但是，如果图片是这样的呢？\n一个3 岁小孩都能识别出猫的照片，计算机科学家们却花了多年时间教会计算机看图识物。关键就是自主训练量。\n直到20世纪80年代，来自加拿大多伦多大学的“神经网络先驱”Geoff Hinton领导的小组，提出了一种训练神经网络的方法，叫做卷积神经网络，意味着它不会陷入局部陷阱。\n于是强大的图形处理单元或GPU出现了，研究人员因此可以在台式机上运行、操纵和处理图像，而不用超级计算机了。\n同时大数据的加持，让卷积神经网络应用越来越广泛。2007年，美国斯坦福大学计算机科学系副教授李飞飞推出了ImageNet——一个来自互联网的数百万带有标签图像的数据库。ImageNet为神经网络提供了约1000万张图像和1000个不同的标签。\n一直到现在，神经网络成为机器人视觉的核心工具。尽管现代神经网络包含许多层次——Google Photos有大约30层——但卷积神经网络的出现，仍然是前进了一大步。\n当你教会计算机认图，它需要反复学习\n与传统神经网络一样，卷积神经网络也是由加权神经元层组成。但是，它们不仅仅是模仿人脑的运作，而是非常恰到好处地从视觉系统本身获得了灵感。\n卷积神经网络中的每个层，都在图像上使用过滤器拾取特定的图案或特征。前几层检测到较大的特征，例如下图斜线，而后面的层拾取更细的细节，并将其组织成诸如“耳朵”的复杂特征。\n图：典型的卷积神经网络架构\n最终输出层像普通神经网络一样是完全连接的（也就是说，该层中的所有神经元都连接到上一层的所有神经元）。它集合高度具体的特征——其中可能包括猫的狭缝状瞳孔、杏仁形眼睛、眼睛到鼻子的距离——并产生超精确的分类：猫。\n在2012年，谷歌用数千个未标记的YouTube剪辑缩略图培训了一个卷积神经网络，看看会出现什么。毫不奇怪，它变得擅长寻找猫视频。\n卷积神经网络如何进行图片处理？基本上有三个步骤，卷积层、池化层、采用下采样阵列作为常规全连接神经网络的输入。\n譬如，从刚刚那张“小孩骑马图”可以分解出，卷积神经网络辨识物体的五个步骤：\n第一步：把图片分解成部分重合的小图块\n于是图片被分解成了 77 块同样大小的小图块。\n第二步：把每个小图块输入到小型神经网络中\n重复这个步骤 77 次，每次判断一张小图块\n然而，有一个非常重要的不同：对于每个小图块，我们会使用同样的神经网络权重，也就是说，如果哪个小图块不一样，我们就认为这个图块是“异常”（interesting）的。\n第三步：把每一个小图块的结果都保存到一个新的数组当中\n我们不想并不想打乱小图块的顺序，所以就把每个小图块按照图片上的顺序输入并保存结果，就像这样：\n第四步：缩减像素采样\n第三步的结果是一个数组，这个数组对应着原始图片中最异常的部分。但是这个数组依然很大:\n为了减小这个数组的大小，我们利用一种叫做最大池化（max pooling）的函数来降采样（downsample）。但这依然不够！\n让我们先来看每个 2×2 的方阵数组，并且留下最大的数：\n这里，一旦我们找到组成 2×2 方阵的 4 个输入中任何异常的部分，但我们就只保留这一个数。这样一来我们的数组大小就缩减了，同时最重要的部分也保留住了。\n最后一步：作出预测\n到现在为止，我们已经把一个很大的图片缩减到了一个相对较小的数组。\n数组就是一串数字而已，所以我们我们可以把这个数组输入到另外一个神经网络里面去。最后的这个神经网络会决定这个图片是否匹配。为了区分它和卷积的不同，我们把它称作“全连接”网络（“Fully Connected” Network）。\n所以从开始到结束，我们的五步就像管道一样被连接了起来：\n整个过程中，你可以把这些步骤任意组合、堆叠多次，卷积层越多，网络就越能识别出复杂的特征。当你想要缩小数据大小时，也随时可以调用最大池化函数。而深层卷积网络（Convolutional Neural Networks）就是使用了多次卷积、最大池化和多个全连接层。为了实现卷积神经网络应用，机器学习需要反复学习测试。\n如何构建卷积神经网络？这里有一些API\n从零开始构建卷积神经网络，费钱又耗时，业内开放了一些API（Application Programming Interface，应用程序编程接口），使开发者无需自己研究机器学习或计算机视觉专业知识。\n谷歌 Cloud Vision\nGoogleCloud Vision是谷歌的视觉识别API，使用REST API。它基于开源的TensorFlow框架。它检测单个面部和物体，并包含一个相当全面的标签集。\n另外，谷歌图像搜索可以说是一个巨大的图像数据库，基本上改变了我们处理图像的方式。\n这里有一张谷歌图像搜索的时间表。\nIBM沃森视觉识别\nIBM沃森视觉识别是沃森开发者云（Watson Developer Cloud）的一部分，并附带了一大批内置的类别，但实际上是为根据你提供的图像来训练自定义定制类而构建的。它还支持一些很棒的功能，包括NSFW和OCR检测，如Google Cloud Vision。\nFacebook的MultiPathNet 3\nFacebook AI Research（FAIR）认为，深度卷积神经网络让我们已经看到图像分类（图像中有什么）以及对象检测（对象在哪里？）上的巨大进步（见下图a和b），但这只是一个开始，目标是设计一种识别和分割图像中每个对象的技术，如下图c。\n\n于是Facebook想将机器视觉推向下一个阶段——在像素级别上理解图像和对象。推动的主要新算法是DeepMask 1分段框架以及SharpMask 2细分模块。它们共同使FAIR的机器视觉系统，能够检测并精确地描绘图像中的每个物体。识别流水线的最后阶段使用一个专门的卷积网络，称之为MultiPathNet 3，以其包含的对象类型（例如人，狗，羊），为每个对象掩码标记。\nClarif.ai\nClarif.ai是一个新兴的图像识别服务，也使用REST API。关于Clarif.ai的一个有趣的方面是它附带了一些模块，有助于将其算法定制到特定主题，如食物、旅行和婚礼。\n尽管上述API适用于少数一般应用程序，但你可能仍然需要为特定任务开发自定义解决方案。幸运的是，许多库可以通过处理优化和计算方面来使开发人员和数据科学家的生活变得更加容易，从而使他们专注于训练模型。有许多库，包括Theano、Torch、DeepLearning4J和TensorFlow已经成功应用于各种应用。\n-END-"}
{"content2":"人工智能是目前最火热的技术领域，也是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，数学、心理学，甚至哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任通常需要人类智能才能完成的复杂工作。那人工智能知识体系有哪些内容呢？\n下面是新一代人工智能知识体系大全图谱:\n中国人工智能发展现状与未来\n对于中国而言，人工智能的发展是一个历史性的战略机遇，对缓解未来人口老龄化压力、应对可持续发展挑战以及促进经济结构转型升级至关重要。\n虽然“人工智能”（AI）已经成为一个几乎人人皆知的概念，但对人工智能的定义还没有达成 普遍共识。传统的人工智能发展思路是研究人类如何产生智能，然后让机器学习人的思考方式和行为。现代人工智能概念的提出者约翰·麦卡锡认为，机器不一定需 要像人一样思考才能获得智能，重点是让机器能够解决人脑所能解决的问题。\n第四次工业革命正在来临，而人工智能已经从科幻逐步走入现实。从1956年人工智能这个概念被首次提出以来，人工智能的发展几经沉浮。随着核心算法的突破、计算能力的迅速提高、以及海量互联网数据的支撑，人工智能终于在21世纪的第二个十年里迎来质的飞跃，成为全球瞩目的科技焦点。自从2016年AIphaGo战胜李世石之后，全球对于人工智能发展的兴奋与担忧交织难分。\n即使如此，世界各国已经认识到人工智能是未来国家之间竞争的关键赛场，因而纷纷开始部署人工智能发展战略，以期占领新一轮科技革命的历史高点。对于中国而言，人工智能的发展是一个历史性的战略机遇，对缓解未来人口老龄化压力、应对可持续发展挑战以及促进经济结构转型升级至关重要。\n本文从科技产出与人才投入、产业发展和市场应用、发展战略和政策环境等方面描绘中国人工智能的发展面貌。\n看点01\n科技产出与人才投入\n1、论文产出：中国人工智能论文总量和高被引论文数量都是世界第一。中国在人工智能领域论文的全球占比从1997年4.26%增长至2017年的27.68%，遥遥领先其他国家。高校是人工智能论文产出的绝对主力，在全球论文产出百强机构中，87家为高校。中国顶尖高校的人工智能论文产出在全球范围内都表现得十分出众。不仅如此，中国的高被引论文呈现出快速增长的趋势，并在2013年超过美国成为世界第一。但在全球企业论文产出排行中，中国只有国家电网公司的排名进入全球前20位。从学科分布看，计算机科学、工程和自动控制系统是人工智能论文分布最多的学科。国际合作对人工智能论文产出的影响十分明显，高水平论文里中国通过国际合作而发表的占比高达42.64%。\n2、专利申请：中国专利数量略微领先于美国和日本，国家电网表现突出。中国已经成为全球人工智能专利布局最多的国家，数量略微领先于美国和日本，而中美日三国占全球总体专利公开数量的74%。全球专利申请主要集中在语音识别、图像识别、机器人以及机器学习等细分方向。中国人工智能专利持有数量前30名的机构中，科研院所与大学和企业的表现相当，其技术发明数量占比分别为52%和48%。企业中的主要专利权人表现差异巨大，尤其是中国国家电网近五年的人工智能相关技术发展迅速，在国内布局专利技术量远高于其他专利权人，而且在全球企业排名中位列第四。中国的专利技术集中在数据处理系统和数字信息传输等领域，其中图像处理分析的相关专利占总发明件数的16%。电力工程也已成为中国人工智能专利布局的重要领域。\n3、人才投入：中国人工智能人才总量居世界第二，但是杰出人才占比偏低。截至2017年，中国的人工智能人才拥有量达到18232人，占世界总量的8.9%，仅次于美国（13.9%） 。高校和科研机构是人工智能人才的主要载体，清华大学和中国科学院系统成为全球人工智能人才投入量最大的机构。然而，按高H因子（又称H指数，用于评价科学家的科研绩效）衡量的中国杰出人才只有977人，不及美国的五分之一，排名世界第六。企业人才投入量相对较少，高强度人才投入的企业集中在美国，中国仅有华为一家企业进入全球前20。中国人工智能人才集中在东部和中部，但个别西部城市如西安和成都也表现十分突出。国际人工智能人才集中在机器学习、数据挖掘和模式识别等领域，而中国的人工智能人才研究领域则比较分散。\n看点02\n产业发展和市场应用\n1、企业规模：中国人工智能企业数量为全球第二，北京是全球人工智能企业最集中的城市。截至2018年6月，全球共监测到人工智能企业总数达4925家，其中美国人工智能企业数2028家，位列全球第一。中国（不含港澳台地区）人工智能企业总数1011家，位列全球第二，其后分别是英国、加拿大和印度（）。\n从城市尺度看（），全球人工智能企业数量排名前20的城市中，美国占9个，中国占4个，加拿大占3个，英国、德国、法国和以色列各占1个。其中，北京成为全球人工智能企业数量最多的城市，其次是旧金山和伦敦。上海、深圳和杭州的人工智能企业数量也进入全球前20。\n从成立时间看（），中国人工智能创业企业的涌现集中在2012-2016年，在2015年达到顶峰，新增初创企业数量达到228家。从2016年开始，创业企业的增速有所放缓。\n中国人工智能企业的平均年龄为5.5年。其中，北京、上海和天津等地初创企业云集，企业平均年龄相较于全国平均水平更年轻，平均年龄在5.5年以下。山东和辽宁等地老牌工业机器人和自动化企业转型较多，企业年龄相对较大。\n人工智能的应用技术主要包括语音类技术（包括语音识别、语音合成等）、视觉类技术（包括生物识别、图像识别、视频识别等）和自然语言处理类技术（包括机器翻译、文本挖掘、情感分析等）。将基础硬件考虑在内，国内外人工智能企业应用技术分布如所示。相比国外，中国人工智能企业的应用技术更集中于视觉和语音，而基础硬件占比偏小。\n人工智能在行业应用上包括智能机器人、智能驾驶、无人机、AR/VR、大数据及数据服务、各类垂直领域应用（本文中定义为“AI+\"）等。国内外人工智能企业的行业应用分布如所示。可以看出，相比于国外，国内企业更看重智能机器人、无人机和智能驾驶等终端产品的市场，而国外企业更注重AI在各类垂直行业的应用。\n2.风险投资：中国已成为全球人工智能投融资规模最大的国家。自2013年以来，全球和中国人工智能行业投融资规模都呈上涨趋势（）。2017年全球人工智能投融资总规模达395亿美元，融资事件1208笔，其中中国的投融资总额达到277.1亿美元，融资事件369笔。中国AI企业融资总额占全球融资总额的70%，融资笔数达31%。\n根据2013年到2018年第一季度全球的投融资数据，中国已在人工智能融资规模上超越美国成为全球最“吸金”国家，但是在投融资笔数上，美国仍然在全球处于领先地位。\n国内融资金额和融资笔数最高的省市是北京，且遥遥领先其他各省。上海、浙江、江苏和广东等省市的表现也比较突出。值得注意的是，广东省虽然投融资总额相对较少，但活跃度很高，融资笔数仅次于北京和上海。各省融资金额和笔数如所示。\n从投融资轮次分布看，从2015年开始，国内投融资活动早期投资（包括种子轮、天使轮和A轮）的占比逐渐下降，这意味着国内投融资活动愈加趋于理性，产业也逐渐走向成熟。国内投融资活动各个轮次占比如所示。\n3.市场规模：中国人工智能市场增长迅速，计算机视觉市场规模最大。 2017年中国人工智能市场规模达到237.4亿元，相较于2016年增长67%。其中以生物识别、图像识别、视频识别等技术为核心的计算机视觉市场规模最大，占比34.9%，达到82.8亿元（）。\n中国人工智能创业热潮与投融资热情在2017年回归理性，但随着人工智能各项技术的不断成熟以及各类应用场景的落地，预计在2018年，人工智能市场增速将达到75%，整体规模将达到415.5亿元（）。\n2018年，机器学习、深度学习等算法能力的增强将促进计算机视觉、语音等技术不断突破。核心计算芯片也成为巨头们战略布局的一环，谷歌升级TPU3.0，英伟达发布最大GPU，国内寒武纪推出首款云端智能芯片MLU100，阿里、华为、小米纷纷推出自己的AI芯片产品，并将进入大批量商用上市阶段，人工智能产业将继续增长并与垂直行业加深融合。\n4.产品应用：应用范围广泛，语音和视觉类产品最为成熟。伴随着算法、算力的不断演进和提升，基于语音、自然语言处理和视觉技术，有越来越多的应用和产品落地。比较典型的包括语音交互类产品（如智能音箱、智能语音助理、智能车载系统等）、智能机器人、无人机、无人驾驶汽车等。在行业解决方案方面，人工智能的应用范围则更加广泛，目前已经在医疗健康、金融、教育、安防、商业、智能家居等多个垂直领域得到应用。\n1）人工智能终端产品。目前，由于人工智能技术尚处于发展阶段，且以机器学习、深度学习为代表的新一代人工智能技术主要体现在算法层面，而成熟的实体终端产品并不多。下面主要对发展较为成熟，且已初具市场规模的三款终端产品予以介绍，分别是智能音箱、智能机器人和无人机。\n①智能音箱。搭载了人工智能语音交互系统的联网智能音箱近几年年均复合增长率超过30%，全球总市场规模将从2017年的11.5亿美元增至2021年的35.2亿美元，超过普通智能音箱市场。\n研究公司Canalys今年5月发布的最新数据显示，谷歌已经超过亚马逊成为全球智能音箱市场的第一巨头。谷歌在2018年第一季度售出了320万台智能音箱，市场份额达36.2%。相比之下，亚马逊售出了250万台Echo智能音箱，市场份额为27.7%。中国两大品牌阿里巴巴天猫和小米在第一季度分列全球智能音箱市场第三和第四位，市场份额分别为11.8%和7.0%。\n②智能机器人。智能机器人的关键技术包括视觉、传感、人机交互和机电一体化等。从应用角度分，智能机器人可以分为工业机器人和服务机器人。其中，工业机器人一般包括搬运机器人、码垛机器人、喷涂机器人和协作机器人等。服务机器人可以分为行业应用机器人和个人/家用机器人。其中，行业应用机器人包括智能客服、医疗机器人、物流机器人、引领和迎宾机器人等;个人/家用机器人包括个人虚拟助理、家庭作业机器人（如扫地机器人）、儿童教育机器人、老人看护机器人和情感陪伴机器人等。\n根据IFR 2018年6月最新发布的数据，2017年全球机器人市场规模已达500亿美元。2017年全球工业机器人的总销量达38万台，同比增长29%。中国自2013年以后一直是全球最大的工业机器人市场。2017年，中国的工业机器人销量达13.8万台，其次是韩国，约4万台，日本销量约有3.8万台。在美洲，美国是最大的单一市场，销售了约3.3万台工业机器人。在欧洲，德国售出了约2.2万台。中、韩、日、美、德五国2017年工业机器人销量占全球总销量的71%（）。\n③无人机。目前无人机市场主要由个人消费级无人机和商用无人机构成。消费级无人机主要用于航拍、跟拍等娱乐场景。商用无人机的应用范围则非常广泛，可以用于农林植保、物流、安保、巡防等多个领域。\n消费级无人机售价基本保持在5000美元以下，续航能力不超过1小时。商用无人机相比于个人无人机，拥有更大的有效载荷和更长的飞行时间，目前在工业领域应用最为成功。商用无人机市场出货量虽小，但售价较高，其收入占据了无人机市场的三分之二。Gartner预测2018年全球无人机市场产量将达313万台，市场规模将达到73亿美元，预计较2017年同比增长28%。\n目前国内最有影响力的无人机企业是大疆创新（DJI）。大疆主要开发制造消费级无人机，同时在民用领域也有渗透。在消费级无人机市场，大疆在全球占有绝对领先的市场地位。\n除大疆外，国内还有一些发展较快、比较有影响力的无人机企业，如亿航、零零无限、零度智控和极飞科技等。\n2）人工智能的行业应用。相较于终端产品，人工智能在相关行业的应用则更为丰富。\n①智能医疗。随着人工智能技术的不断落地，已有不少应用人工智能提高医疗服务水平的成功案例。人工智能已深入医疗健康领域的方方面面，包括智能诊疗、医学影像分析、医学数据治理、健康管理、精准医疗、新药研发等场景中都可以看到人工智能的身影。\n过去，医生以自己的医疗知识和临床经验为基础，根据病人的症状和检查结果判定病症及病程。如今，人们将人工智能应用于医疗辅助诊断，让计算机“学习”专业的医疗知识、“记忆”海量历史病例、识别医学影像，构建智能诊疗系统，为医生提供一个“超级助手”，帮助医生完成诊断。IBM的Watson是智能诊疗应用中的一个著名案例，Watson可以在17秒内阅读3469本医学专著、248000篇论文、69种治疗方案、61540次试验数据、106000份临床报告。2012年Watson通过了美国职业医师资格考试，并部署在美国多家医院提供辅助诊疗的服务。目前Watson提供诊治服务的病种包括乳腺癌、肺癌、结肠癌、前列腺癌、膀肤癌、卵巢癌、子宫癌等多种癌症。\n②智能金融。智能金融是人工智能技术与金融体系的全面融合。人工智能在金融领域的应用主要包括“智能投顾”和金融欺诈检测等。\n“智能投顾”，即智能投资顾问，是金融科技中非常常见的一类应用场景。“智能投顾”通过机器学习算法，根据客户设定的收益目标、年龄、收入、当前资产及风险承受能力自动调整金融投资组合，以实现客户的收益目标。不仅如此，算法还能根据客户收益目标的变动和市场行情的变化实时自动调整投资策略，始终围绕客户的收益目标为客户提供最佳投资组合。目前美国的一些大中型投资公司（如Betterment和WeaIthFront）已经通过“智能投顾”为客户提供服务，并且价格低廉，获得了年轻一代的喜爱和认可。\n以往金融欺诈检测系统非常依赖复杂和呆板的规则，由于缺乏有效的科技手段，已无法应对日益演进的欺诈模式和欺诈技术。伪造、冒充身份等欺诈事件常有发生，给金融企业和用户造成很大经济损失。国内以猛犸反欺诈为代表的金融科技公司，应用人工智能技术构建自动、智能的反欺诈技术和系统，可以帮助企业风控系统打造用户行为追踪与分析能力，建立异常特征的自动识别能力，逐步达到自主、实时发现新欺诈模式的目标。\n③智能安防。安防是人工智能落地较好的应用领域。安防以图像、视频数据为核心，海量的数据来源满足了算法和模型训练的需求，同时人工智能技术也为安防行业事前预警、事中响应和事后处理提供了技术保障。\n目前，人工智能在安防领域的应用主要包括警用和民用两个方向。警用方向，人工智能在公安行业的应用最具有代表性。利用人工智能技术实时分析图像和视频内容，可以识别人员、车辆信息、追踪犯罪嫌疑人，也可以通过视频检索从海量图片和视频库中对犯罪嫌疑人进行检索比对，为各类案件侦查节省宝贵时间。在民用方向，利用人工智能可以实现智能楼宇和工业园区的智能监控。智能楼宇包括门禁管理、通过摄像头实现“人脸打卡”、人员进出管理、发现盗窃和违规探访的行为。在工业园区，固定摄像头和巡防机器人配合，可实现对园区内各个场所的实时监控，并对潜在的危险进行预警。除此之外，民用安防方向还有一个非常重要的应用场景，就是家用安防。当检测到家庭中没有人员时，家庭安防摄像机可自动进入布防模式，有异常时，给予闯入人员声音警告，并远程通知家庭主人。而当家庭成员回家后，又能自动撤防，保护用户隐私。\n安防领域作为人工智能成功落地的一个应用，国内很多安防企业也开始从技术、产品等不同角度涉足人工智能。大华、海康威视、东方网力等传统企业在不断加大安防产品的智能化;另外，像商汤科技、旷视科技、云从科技和依图科技等以算法见长的企业正将技术重点聚焦于人脸识别、行为分析等图像智能领域。\n④智能家居。智能家居基于物联网技术，以住宅为平台，由硬件、软件、云平台构成家居生态圈。智能家居可以实现远程设备控制、人机交互、设备互联互通、用户行为分析和用户画像等，为用户提供个性化生活服务，使家居生活更便捷、舒适和安全。\n例如，借助语音和自然语言处理技术，用户通过说话即可实现对智能家居产品的控制，如语音控制开关窗帘（窗户）、照明系统、调节音量、切换电视节目等操作;借助机器学习和深度学习技术，智能电视、智能音箱等可以根据用户订阅或者收看的历史数据对用户进行画像，并将用户可能感兴趣的内容推荐给用户。在家居安防方面，可以利用面部识别、指纹识别等生物识别技术对智能家居产品进行解锁，通过智能摄像头实时监控住宅安全，对非法入侵者进行监测等。\n在国内，小米打造的智能家居生态链在经历了几年的积累后，已经形成了一套自研、自产、自销的完整体系，接入生态链的硬件已经高达6000万台。另外，以美的、海尔、格力为代表的传统家电企业依托本身庞大的产品线及市场占有率，也在积极向智能家居转型，推进自己的智能战略。\n⑤智能电网。伴随着电网规模日趋庞大，未来人工智能将成为智能电网的核心部分。在需求方面，人工智能技术能持续监控家庭和企业的智能电表和传感器的供需情况，实时调整电网的电力流量，实现电网的可靠、安全、经济、高效。\n在供应方面，人工智能技术能协助电力网络营运商或者政府改变能源组合，调整化石能源使用量，增加可再生资源的产量，并且将可再生能源的自然间歇性破坏降到最低。生产者将能够对多个来源产生的能源输出进行管理，以便实时匹配社会、空间和时间的需求变化。\n在线路的巡视巡检方面，借助智能巡检机器人和无人机实现规模化、智能化作业，提高效率和安全性。智能巡检机器人搭载多种检测仪，能够近距离观察设备，运检准确性高。在数据诊断方面，相比人眼和各类手持仪器，机器人巡检也更精确，而且全天候全自主，大大提高了设备缺陷和故障查找的准确性和及时性。同时，可以对机器人巡检的每个点位的历史数据进行趋势分析，提前预警设备潜在的劣化信息，为制定精准检修策略提供科学依据。无人机搭载高清摄像仪，具有高精度定位和自动检测识别功能，可以飞到几十米高的输电铁塔顶端，利用高清变焦相机对输电设备进行拍照，即便非常细小的零件发生松脱现象，也可通过镜头得到清晰精准的呈现。来自广东电网的资料显示，广东电网在变电站的机巡，年作业量超18万公里，相当于绕地球4圈半，其中无人机巡视占85%，作业量全球第一，综合效率提升了2.6倍。\n1发展战略和政策环境\n1.国际比较：各国人工智能战略与政策各有着重点。 2013年以来，美、德、英、法、日、中等国都纷纷出台了人工智能战略和政策。各国人工智能战略各有侧重，美国重视人工智能对经济发展、科技领先和国家安全的影响;欧盟国家关注人工智能带来的安全、隐私、尊严等方面的伦理风险;日本希望人工智能推进其超智能社会的建设;而中国人工智能政策聚焦于实现人工智能领域的产业化，助力中国的制造强国战略。各国政策在研发重点和重点应用领域也存在着较大差异。\n2.国家政策：从物联网，到大数据，再到人工智能。从2009至今，中国人工智能政策的演变可以分为五个阶段，其核心主题词也不断变化，体现了各阶段发展重点的不同。国家层面政策早期关注物联网、信息安全、数据库等基础科研，中期关注大数据和基础设施，而2017年后人工智能成为最核心的主题，知识产权保护也成为重要主题。综合来看，中国人工智能政策主要关注以下六个方面：中国制造、创新驱动、物联网、互联网+、大数据、科技研发。\n3.地方政策：响应国家战略，地方政策主题因地而异。地方政府积极响应国家人工智能发展战略，其中，《中国制造2025》处于人工智能政策应用网络的核心，在地方人工智能政策制定过程中发挥着纲领性的作用。通过政策发布数量来看，目前中国人工智能发展活跃的区域主要集中在京津冀、长三角和粤港澳地区。各省的政策主题也大有不同，比如江苏省关注基础设施、物联网和云计算等基础研发领域，广东省关注制造和机器人等人工智能应用，而福建省关注物联网、大数据、创新平台和知识产权，各地政策与地方发展条件密切相关。\n2对社会的综合影响\n随着人工智能的充分发展，劳动生产率和生产力水平的提升，人们的生活体验将更加丰富多彩，将更多地将人们从体力劳动乃至常规性的脑力劳动中解放出来，更多地投入到创造性活动当中，使人类自身与社会得到更充分的发展。当前，人工智能技术的突飞猛进正不断改变着零售、农业、物流、教育、医疗、金融、商务等领域的发展模式，重构生产、分配、交换、消费等各环节。根据IDC数据显示，在未来5年内，人工智能技术应用到多个行业，将极大提高这些行业的运转效率，具体提升的效率为教育行业82%、零售业71%、制造业64%、金融业58%。\n1.人工智能对教育和就业的影响。发展人工智能的最终目的不是用来替代人类，而是帮助人类变得更加智慧，而教育将在这个过程中起到关键性作用。人工智能技术提升经济活动中的产能，使得人们逐渐从机械的重复性的或危险的劳动中抽离出来，从而增加了思考、欣赏等闲暇时间，更专注于创新能力、思考能力、审美与想象力的潜能开发与提升。\n目前，人工智能在教育领域的应用主要集中在以下几方面：自适应（个性化）学习、虚拟导师、教育机器人、基于编程和机器人的科技教育、基于虚拟现实/增强现实的场景式教育。用适合自己的方式去学习，不仅效率会提高，而且会保持更长时间的学习兴趣。\n在教育领域深度发展人工智能的意义并不是取代教师，而是协助教师使教学变得更加高效和有趣。另外，在人工智能技术所影响的教育体系中，对人才的信息输入与输出能力、自主学习能力等的要求骤然提高，创新能力的培养也成为重要方向。\n随着技术的发展逐步替代人类从事大部分繁琐重复的工作或体力劳动，在给人们带来福利的同时也带来前所未有的挑战。今天已经有越来越多的人担忧是否自己的工作会被人工智能技术所取代，或者只能在人工智能所留下的“夹缝”中生存。有专家对中国的就业岗位被人工智能取代的概率进行了估算，结果显示，未来20年中，约占总就业人口76%的劳动力会受到来自人工智能技术的冲击，若只考虑非农业人口，这一比例为65%。但同时，人工智能技术对就业的创造效应也已有所显现。调查显示，中国科技公司目前人工智能团队规模平均扩张20%，而且这种需求还会增长。另外国家工业和信息化部教育考试中心专家称，在未来几年中国对AI领域的人才需求可能增至500万。\n可以判断，在人工智能重塑产业格局和消费需求的情境下，一部分工作岗位终将被历史淘汰，但是也会伴随着人工智能技术孵化出一系列新的岗位。另一方面，新型的人机关系正在构建，非程序化的认知类工作会变得愈发难以替代，其对人的创新、思考与想象力提出更高的要求。\n机械化和智能化塑造着新的就业格局，但也要警惕新格局下有可能发生的衍生问题，比如由于失业率上升而引起的贫富差距和社会稳定问题。人工智能所带来的“冲击”是持续性的，对教育和就业的多重影响也是持续性的，因此也需要不断积极探索与技术革命相匹配、相适应的教育与就业机制。\n2.人工智能对隐私与安全的影响。今天，在许多生活消费场景中，人们对个性化体验的需求不断增加，个性化、场景化服务也逐渐成为人工智能驱动创新的主要方向。服务供应方在信息获取社交化、时间碎片化的情境下，着力建立更灵活便捷的消费场景，给人们带来更加友好的用户体验。与此同时，随着语音识别、人脸识别、机器学习算法的发展和日趋成熟，企业可以通过分析客户画像真正理解客户，精准、差异化的服务使得客户的被重视被满足感进一步增强。但是在蕴藏着巨大商业价值的同时，也对现有法律秩序与公共安全构成了一定的挑战。\n网络空间的虚拟性，使得个人数据更易于被收集与分享，极大地便利了身份信息编号、健康状态、信用记录、位置活动踪迹等信息的存储、分析和交易过程，与此同时，人们却很难追踪个人数据隐私的泄露途径与程度。例如，以人工智能技术为支撑的智慧医疗，病人的电子病例、私人数据归属权如何界定，医院获得及使用私人数据的权限界限如何规范。再比如人工智能技术生成作品的著作权问题等。开放的产业生态使得监管机构难以确定监管对象，也令法律的边界变得越来越模糊。\n人工智能的普遍使用使得“人机关系”发生了趋势性的改变，人机频繁互动，可以说已形成互为嵌入式的新型关系。时间与空间的界限被打破、虚拟与真实也被随意切换，这种趋势下的不可预测性与不可逆性很有可能会触发一系列潜在风险。与人们容易忽略的“信息泄露”不同，人工智能技术也可能被少数别有用心的人有目的地用于欺诈等犯罪行为。如基于不当手段获取的个人信息形成“数据画像”，并通过社交软件等冒充熟人进行诈骗。再比如，使用人工智能技术进行学习与模拟，生成包括图像、视频、音频、生物特征在内的信息，突破安防屏障。去年曾有报道，新款苹果手机“刷脸”开机功能被破解即是这类例子。而从潜在风险来看，无人机、无人车、智能机器人等都存在遭到非法侵入与控制，造成财产损失或被用于犯罪目的的可能。\n3.人工智能对社会公平的影响。随着人工智能研发与应用的突飞猛进，一系列价值难题也正逐渐显现在人们面前。目前还有大量不会上网、由于客观条件无法使用互联网及不愿触碰互联网的人群，已经被定义为人工智能时代的“边缘人”，而人工智能对人们的文化水平、信息流的掌握程度又有了更高的要求。人工智能技术越发达，信息鸿沟就越深，进而演变为服务鸿沟、福利鸿沟，而在人工智能时代，“边缘人”将越来越难享受到便捷的智能信息服务，也更不易获得紧缺的服务资源。\n在人类社会，按照公正原则，人工智能技术应该使尽可能多的人群获益，技术所带来的福利和便捷应让尽可能多的人群共享。2017年初在美国阿西洛马召开的BeneficialAI会议上提出的“阿西洛马人工智能原则”强调，应以安全、透明、负责、可解释、为人类做贡献和多数人受益等方式开发人工智能。实实在在的公共服务将极大限度地促进和谐良好的人机关系，使均等的智能服务惠及各地区、不同行业和不同群体。因此人工智能技术突飞猛进的同时，要积极思考与研究如何利用其提高基本公共服务平台的建设水平，不断缩小信息鸿沟，建设高效、发达、宜居的智能社会，推动社会包容与可持续发展，让全体公民能共享科技创造的美好未来。\n3初步判断和反思\n结合既有研究，我们可以得到以下几点对中国人工智能发展的初步判断和反思。\n1.从国际比较来看，中国人工智能发展已经进入国际领先集团。中国在历次工业革命里一直处于落后追赶的状态，而在第四次工业革命兴起之际，中国已经和其他国家一起坐在头班车上。在人工智能领域，中国在技术发展与市场应用方面已经进入了国际领先集团，呈现中美“双雄并立”的竞争格局。\n2.从发展质量来看，中国的人工智能发展还远未达到十分乐观的地步。中国的优势领域主要体现应用方面，而在人工智能核心技术领域，如硬件和算法上，力量依然十分薄弱，这使得中国人工智能发展的基础不够牢固。中国的人工智能技术发展缺乏顶尖人才，与发达国家特别是美国的差距还十分明显。\n3.从参与主体来看，中国人工智能企业的知识生产能力亟待提升。科研机构和大学是目前中国人工智能知识生产的主要力量。相比国外领先企业，中国企业作为一个群体的技术表现还比较逊色，在人工智能专利申请上落后于国内高校和科研院所。即使是被公认为人工智能巨头的百度、阿里巴巴、腾讯（BAT）等企业，在人才、论文和专利方面也还没有突出的表现，而它们的美国对手IBM、微软、谷歌等企业在每项指标的全球企业排名中均名列前茅。\n4.从应用领域来看，人工智能与能源系统的结合是一个被忽视的重要领域。电力工程已成为中国人工智能专利布局的重要领域，而国家电网公司在人工智能科研论文和专利申请上都是中国表现最抢眼的企业。这个事实在以往的人工智能研究中都未被提及或重视，说明人工智能与能源系统的结合很可能是一个之前被忽视的领域，而这可能为中国人工智能技术应用开拓新的方向，并为能源低碳转型做出有益的贡献。\n5.从发展方式来看，中国需要加强产学研合作，促进知识应用和转化。 国际合作和产学研合作是人工智能技术发展的重要途径。目前中国人工智能知识生产大量停留在大学和科研机构中，在产学研合作促进知识应用和转化方面仍然存在显著“短板”。展望未来，中国不但需要大力推进产学研融合创新，还需要更加鲜明地支持企业利用数据、算力等优势从事人工智能基础研究。\n6.从政策环境来看，各地方政府积极支持，但也存在盲目跟风倾向。中国社会对人工智能的发展总体上是积极乐观的，为人工智能产业的发展提供了非常有利的政策、舆论、金融、市场和人才供给等发展环境，但各地在人工智能发展政策方面仍然存在“跟风中央”、“追逐热点”的倾向。目前中国在人工智能发展政策上主要强调促进技术进步和产业应用，而对道德伦理、安全规制等问题还没有予以足够重视。"}
{"content2":"自认为从事机器学习已经有些时日了，常常听到统计学、人工智能、机器学习、数据挖掘、机器视觉吧啦吧啦….自己也曾迷茫，自己究竟算哪个方向的呢？因此起意整理一套系列文章将这些概念描述清楚，旨在理清这些错综复杂的概念，促使我们正对性学习我们关注的方向，我想这对我们搞算法也是相当必要的。\n统计学\n统计学（英语：Statistics）是在资料分析的基础上，自17世纪中叶产生比逐步发展起来的一门学科，它是研究如何测定、收集、整理、归纳和分析反应数据资料，一边给出正确信息的科学。统计学广泛的应用在各门科学，从自然科学、社会科学到人文科学，甚至被用来做工商业和政府的情报决策。随着大数据时代的来临，统计的面貌也逐步被改变，与信息、计算机等领域密切结合，是数据科学（Data Science)中的重要主轴之一。\n譬如在一组数据中，可以摘要并描述这部分数据的集中和离散情况，该用法被称作描述统计学。另外，观察者已数据的形态，建立一个用来解释随机性和不确定性的数学模型，以之来推理研究中的步骤和母体，这种用法被称作推理统计学。这两种用法都可以被称作应用统计学。梳理统计学则是讨论背后的数论基础的学科。\n人工智能\n人工智能(英语：Artificial Intelligence, AI）也称为机器智能，是指由人工制造出来的系统表现出来的智能。通常人工智能是指通过普通电脑实现的智能。\n人工智能的研究是高度技术和专业的，各分支领域都是深入且不相通的，因而设计范围极其广泛。\n人工智能的研究课分为几个技术问题。其分支领域主要集中在解决具体恩特，其一是，如何使用各种不同的工具完成特定的应用程序。AI的核心问题包括推理、知识、规划、学习、交流、感知、移动和操作物体的能力等。强人工智能目前仍然是该领域的长远目标。不签比较流行的方法包括统计方法，技术智能和传统意义上的AI。目前大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于仿生学、认知心理学，以及概率论和经济学的推演算法等等也在逐步探索当中。\n机器学习\n机器学习（英语：Machine Learning, ML）是近20多年兴起的一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。\n机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。\n数据挖掘\n数据挖掘（英语：Data Mining）又称资料勘探、资料挖掘、资料采矿。图是资料库知识发现（英语：Knowledge-Discovery in Databases,KDD）中的一个步骤。数据挖掘是从大量的资料中自动搜索隐藏于其中的有着特殊关联性（属于Association rule learning）的信息的过程。资料挖掘通常与电脑科学有关，并通过统计、在线分析处理、情报检索、机器学习、专家系统（依靠过去的经验法则）和模式识别等诸多方法来实现上述目标。"}
{"content2":"计算机视觉技术无疑是AI浪潮中最火热的议题之一。视觉技术的渗透，既可以对传统商业进行改造使之看到新的商业机会，还可以创造全新的商业需求和市场。无论在电商、安防、娱乐，还是在工业、医疗、自动驾驶领域，计算机视觉技术都扮演着越发重要的角色。\n《阿里巴巴机器智能计算机视觉技术精选》，收录了顶级会议 CVPR 2018 阿里论文。比如，拍立淘利用图像搜索和识别技术，帮助用户在移动端通过拍照就能找到相似商品；线下新零售领域，阿里用空间定位、货架商品SKU识别技术推动“人货场”数字化，并做进一步的商业分析；城市大脑项目中，阿里研发了大规模视频高效处理技术，帮助城市交通事故识别、人流轨迹判断、交通数据样本汇总。\n资源地址：https://download.csdn.net/download/dreamfarwhb/10879624"}
{"content2":"人工智能都包括什么？\n人工智能包括现在非常流行的一些网络词语：人工智能->机器学习->深度学习。\n图片来自于网络。\n怎样学习？\n路径一：一步一个脚印，扎扎实实从基础学起，逐步提高学习难度\nStep1：了解行业资讯，先来一波科普\n所以在学习人工智能之前，你先了解一下行业得相关资讯，对这个行业有一个基本的认识，那么接下来你要准备学习了\nStep2：务实基础—高数+Python来当道\n机器学习里面涉及了很多算法，而这些算法又是数学推导出来，所以你要理解算法，就需要先学习一部分高数知识。不管是你在机器几面编辑一个算法还是应用算法，你都需要通过写程序来和机器进行对话，那么你需要编程，假如你的造诣比较高，可以用C语言，如果你是转行过来或者以前没有编程基础，那么学习Python会不错，因为Python语言相对比较简单。\nStep3：机器学习算法+实践\n掌握以上基础以后，就要开始学习完机器学习的算法，并通过案例实践来加深理解和掌握。还有很多机器学习的小案例等着你来挑战，前面掌握的好，后面当然轻松很多，步入深度学习\nStep4：深度学习\n深度学习需要机器大量的经过标注的数据来训练模型，所以你的掌握一些数据挖掘和数据分析的技能，然后你再用来训练模式。在这里你可能会有疑问，据说深度学习，好像有很多神经网络，看着好复杂，编辑这些神经网络那不是太难了，你大可放心，谷歌、亚马逊、微软等大公司已经把这些神经网络模型封装在他们各自的框架里面了，你只需要调用就可以了。\nStep5：行业大型项目实践\n当你学习完深度学习，此时你就可以自己动手训练一个小模型了。有条件的话，从一个项目的前期数据挖掘，到中间模型训练，并做出一个有意思的原型，能把一整套的流程跑通，那么恭喜你，你已经具备一名人工智能初级工程师的水准了\n为了方便让你理解，我给你列举了学习课程的大纲：\n1、人工智能基础 — 高等数学必知必会\n数据分析（就是高数）\n常数e\n导数\n梯度\nTaylor\ngini系数\n信息熵与组合数\n梯度下降\n牛顿法\n2.概率论（大一大二学过有木有）\n微积分与逼近论\n极限、微分、积分基本概念\n利用逼近的思想理解微分，利用积分的方式理解概率\n概率论基础\n古典模型\n常见概率分布\n大数定理和中心极限定理\n协方差(矩阵)和相关系数\n最大似然估计和最大后验估计\n3.线性代数及矩阵（大一大二学过有木有）\n线性空间及线性变换\n矩阵的基本概念\n状态转移矩阵\n特征向量\n矩阵的相关乘法\n矩阵的QR分解\n对称矩阵、正交矩阵、正定矩阵\n矩阵的SVD分解\n矩阵的求导\n矩阵映射/投影\n4.凸优化（看不懂不要紧，掌握基础即可）\n凸优化基本概念\n凸集\n凸函数\n凸优化问题标准形式\n凸优化之Lagerange对偶化\n凸优化之牛顿法、梯度下降法求解\n2、人工智能基础-Python入门及实践课程\nPython快速入门\n科学计算库Numpy\n数据分析处理库Pandas\n可视化库Matplotlib\n更简单的可视化Seaborn\n3、人工智能提升 — Python项目\nPython爬虫项目\n4、机器学习基础入门-算法讲解\n线性回归算法\n梯度下降原理\n逻辑回归算法\n案例实战：Python实现逻辑回归\n案例实战：对比不同梯度下降策略\n案例实战：Python分析科比生涯数据\n案例实战：信用卡欺诈检测\n决策树构造原理\n案例实战：决策树构造实例\n随机森林与集成算法\n案例实战：泰坦尼克号获救预测\n贝叶斯算法推导\n案例实战：新闻分类任务\nKmeans聚类及其可视化展示\nDBSCAN聚类及其可视化展示\n案例实战：聚类实践\n降维算法：线性判别分析\n案例实战：Python实现线性判别分析\n降维算法：PCA主成分分析\n案例实战：Python实现PCA算法\n5、机器学习进阶提升-项目演练\nEM算法原理推导\nGMM聚类实践\n推荐系统\n案例实战：Python实战推荐系统\n支持向量机原理推导\n案例实战：SVM实例\n时间序列ARIMA模型\n案例实战：时间序列预测任务\nXgbooost提升算法\n案例实战：Xgboost调参实战\n计算机视觉挑战\n神经网络必备基础\n神经网络整体架构\n案例实战：CIFAR图像分类任务\n语言模型\n自然语言处理-word2vec\n案例实战：Gensim词向量模型\n案例实战：word2vec分类任务\n探索性数据分析：赛事数据集\n探索性数据分析：农粮组织数据集\n6、深度学习基础\n计算机视觉-卷积神经网络\n三代物体检测框架\n卷积神经网络基本原理\n卷积参数详解\n案例实战CNN网络\n网络模型训练技巧\n经典网络架构与物体检测任务\n深度学习框架Tensorflow基本操作\nTensorflow框架构造回归模型\nTensorflow神经网络模型\nTensorflow构建CNN网络\nTensorflow构建RNN网络\nTensorflow加载训练好的模型\n深度学习项目实战-验证码识别\n深度学习框架Caffe网络配置\nCaffe制作数据源\nCaffe框架小技巧\nCaffe框架常用工具\n7、深度学习项目演练\n项目演练：人脸检测数据源制作与网络训练（基于Caffe）\n项目演练：实现人脸检测（基于Caffe）\n项目演练：关键点检测第一阶段网络训练（基于Caffe）\n项目演练：关键点检测第二阶段模型实现（基于Caffe）\n项目演练：对抗生成网络（基于Tensorflow）\n项目演练：LSTM情感分析（基于Tensorflow）\n项目演练：机器人写唐诗（基于Tensorflow）\n项目演练：文本分类任务解读与环境配置\n项目演练：文本分类实战（基于Tensorflow）\n项目演练：强化学习基础（基于Tensorflow）\n项目演练：DQN让AI自己玩游戏（基于Tensorflow）\n8、人工智能综合项目实战\n语音识别、人脸识别、\n电商网站数据挖掘及推荐算法\n金融P2P平台的智能投资顾问\n自动驾驶技术\n医疗行业疾病诊断监测\n教育行业智能学习系统\n路径二、如果你希望快速学习完进行项目实践，请直接学习深度学习（哪里不懂，单独学习不懂得地方就可以了）\n只了解以上的东西就够了么？\n我们只知道了学什么？别忘了，人都有惰性，很多人是没办法坚持学习完的，而且就算是你坚持了，学习方法不对，你的效率依然很低，浪费大量时间，那么我给你列举了如下学习方法（大师级别的学习方法）\n路径二、如果你希望快速学习完进行项目实践，请直接学习深度学习（哪里不懂，单独学习不懂得地方就可以了）\n只了解以上的东西就够了么？\n我们只知道了学什么？别忘了，人都有惰性，很多人是没办法坚持学习完的，而且就算是你坚持了，学习方法不对，你的效率依然很低，浪费大量时间，那么我给你列举了如下学习方法（大师级别的学习方法）\n适合自己——学习方法——短时间——注意力——解决难题——设定目标\n首先找到适合自己的学习方法\n学习方法里面有两种，一个是自然主义，一个是结构主义，自然主义注重模仿，结构注意注重创造\n比如英语学习：英语你更多的是需要模仿别人说话，模仿得多了，自己也就会说了，就像是小孩子学习汉语，都是模仿父母学习一个道理，这是典型的自然主义，注重文本本身，就是只是表面看起来的样子。\n高等数学则需要扎实的基本功，一步一步来，就像是你高中数学很差，学高等数学就很吃力，学习高等数学，你做了大量的习题练习，你对于概念的理解就会深刻，本身像数学物理这些学科，让你记得东西本来就不多。\n学习这些的目的就是为了让你举一反三，让你学会创造。像英语这样完全是不需要我门创造了。\n人工智能课程属于技能课程学习，那就要遵循技能课程的学习规律，这是典型的结构注意学习方法\n所以你要多练，一定要把案例逐个实践一遍，然后去想，如果自己去做一个案例，或实现某个应用，你该怎么去做\n1、任务驱动\n学习本身就是反人性的事情，就算是你的学习资料再好，我没有足够的动力学啊，或者说我没法坚持学下去，常常半途而废。\n有人说我学习方法对了，为什么还是不管用，一种是学不会，另一种是学不好\n学不会是指连入门都没有办法，学不好往往指的是达不到某种专业程度\n学不会，可能跟天赋有关，比如身高就是不够打篮球，这种情况就不讨论了，直接说学不好。学不好这种情况是可以被优化，我们经常听到一句话，兴趣是最好的老师。这句话是对的，但是我在这里苦口婆心说你要多培养自己兴趣啊，这样的话，你累，我比你还要累，明明没有兴趣，非要让你喜欢上一件东西，这叫包办。很多事做成，都不是基于兴趣的。那是基于什么呢？你要知道任何人做任何事，都是要回报的。这是一个很浅显的道理，就算是一个人带着很大的痛苦去做一件事也是因为做成了这件事能获得更大的利益。\n人做成事来自两个驱动力\n内部驱动，兴趣这就是很强的内部驱动，此外还有虚荣心等\n外部驱动，也就是完成任务带来的奖励\n既然你干脆没有兴趣，我们干脆就把兴趣学习这件事给否定了，我要说的是，成年人学习不需要培养兴趣，应该用任务来驱动，我没有任务，我找不到任务啊，你会说，我没有任务啊，生活中的任务驱动，工作中的职业强迫，以教为学这些都是任务驱动。其中工作的职业强迫就是一种非常强的任务驱动，比如你可以通过学习人工智能技能，获得了更好的职业收入。你说我实在没有公司去聘请我，那你可以出教程啊，你出完教程然后去学习网站去卖。为了给别人教东西，而去学习。比如我一个朋友，他说他会说英语和日语。我说你啥时候会的日语啊，他说有一次他去英语辅导上课，那天正好日语课的讲师没有来，他自告奋勇说老师，我会教日语。然后他赶快报了一个日语班，结果一边学一边教，他的学生还教的特别好。因为我的这个朋友，他的目标比较明确，他是为了学完去教别人，假如你和他一起报名去学习日语，他一定比你学的好。所以我们是不是也可以抱着为了教别人的心态去学习。\n我在上本科的时候，就想学习PS，可是八年时间过去了，PS还没学会，偶尔学两天，最后也没坚持下去，直到有一次因为要给老妈开一个淘宝店。为了省钱，自己拍照，自己修图，但是不会PS怎么办，就在网上找学习如何做淘宝美工得教程，结果三天把教程里面的案例实践了一遍，愣是自己把淘宝店得图片给修好了，你还别说，图片处理得有模有样。别人都看不出来是新手。\n这就是典型得任务驱动，你啊，赶快给自己找一个任务\n2、拖延症的确诊与治疗\n通过上面一段，我是尽我所能帮你制造了学习的动机，但是光有动机，后面还会遇到别的问题。\n我们接下来就会谈在学习过程中会遇到的其他问题。你说我现在已经有了要教别人的动机，但是有拖延症啊，我就笑一下，你咋得了这么时髦的病啊。最近不少人得了这个病，拖延症中拖延两个字是一个表象。拖延症有个学名叫注意力缺陷多动障碍，在拖延得表象之下，其实本质是注意力无法集中。如果你有这个病，在下一段我尝试给你解决这个问题，我还是要强调，只有任务，只有严峻得任务才能解决你的拖延症。\n关于拖延症我希望你清楚得认识两点：\n任何人都有拖延症，只不过有些事拖延，有些事不拖延，因为一个人总不可能什么都不干嘛\n第二就算是自认为或者别人认为有拖延症得人，也没有被拖死\n据我了解，医学上得了这个注意力缺陷多动障碍得人，连初中学业都完成不了，高中学业得少之又少，能考上大学的简直是寥寥无几。所以你说你考上了大学，说明你是没有拖延症。想要解决这个拖延症是任务还是任务。主要是你没有给自己设置严峻得任务。比如你要做一个报告是给你自己朋友得，你可能会拖延，但是假如你要是给你的老板报告，你敢拖延么，假如你是给上千人做报告你敢拖延么？你想想你有给自己设置过这么严峻得任务么？定期像上千人交代，最好是隔天交一份报告的，最好是隔天交一份报告这样严峻，你可能需要这样一次得经验。这里你肯定又要问了，我去哪里找上千人报告去。在这里我要告诉你一个方法，你要把你任务告诉你的身边的朋友，越多越好，相当于给他们吹个牛逼，这样你为了不被别人笑话你，你就必须的努力啊。有的人说，我连一件小事都坚持不下了，比如我要减掉5斤肉，几乎大部分女孩子都成天喊着减肥，但是一直没有执行起来，这里面可能是你得目标设置得有问题。你总想着我有我有一个完美得身材，是的，我还想者当亿万富翁呢，这种目标设置是有问题得，王健林不是给我们提过小目标吗，我觉得他得方法是对的，练习一定伴随着一定程度的痛苦。只不过有些人他的长期目标非常明确，有的人觉得他很苦，他自己不觉得苦，他就能坚持下来，长期目标太长了，太宏大了会增加这个过程当中的痛苦，那你不妨先不要想着最后的结果，什么马甲线啊，什么翘臀啊，你可以先设置个小游戏，做仰卧起做，每天增加一下，今天做5下，明天做6下，这是设定小得目标，把你的大的目标细化成可以每天完成事情，完不成总觉得不行。如果你总觉得舒适，那就不是学习，是练习，如果每天只比头一天多做一个仰卧起坐，那没那么苦吧，三个月之后你就可以秀你得身材了，这件事难道听起来还不够过瘾么。所以说了这么多，我再尝试解决你的拖延问题，这个问题是不太容易解决，要解决它之前，你要先认清一个事实，就是\n任何人都有拖延得表现，给自己设置一个正真严峻的任务\n用小的目标去解决拖延的问题\n我也为你指出了，从整治自己的身材开始\n设定小目标，通过做成这件事来突破，然后形成一种惯性，逐渐去学习一些技巧性比较强的东西，先来增强自己信心，最后再去做哪些需要有量变完成质变得事，那整个这个过程算是高效得了。当然拖延症这种表象前提下事注意力无法集中，接下来会为你好好讲讲怎么改善注意力不集中得问题。\n3、通过衣食住行训练专注力\n方法一：\n如果你要学习一样东西，需要工具，你要买贵的，比如像乐器，人们学习乐器是为了什么呢？就是为了听到好的声音啊。有人说你是有钱啊，才能买贵的，其实我之前都是买便宜的。关于买工具这件事其实只要你稍微让自己心疼一点儿就达到效果了，刻意练习这本书强调大量练习，可是你知道怎么心甘情愿得大量练习么？就是要买自己让自己心疼的工具。\n方法二：\n吃上怎么训练自己专注力，就是吃点儿好的，少吃素食，少吃外卖，少吃自助餐，就算是人均500得自助餐也少吃，吃一次也就够了。无论是买东西还是吃饭，还是后面提到的其他方面，要的是什么，是一种仪式感，就自助餐就是缺乏仪式感得活动，这个吃一点儿那个吃一点儿，自助餐往往不会是你吃好，但是会把你吃撑，好几天都消化不了，我会建议你少去吃自助餐，吃那些素食、外卖弄几个盒饭在那儿吃，这些都缺乏仪式感，哪怕每次都只有你一个人，也要学会好好吃饭，有人说你能想象出比一个人吃火锅更孤独，更寂寞得状态么，要我说，你们每个人都应该认认真真一个人吃一顿火锅，最好能学会做一顿饭给自己吃，这是营造仪式感非常好得行为活动，就像上一讲给你提到过的，让自己得身体变得更好，可以是我们重塑自己的开端，一个对感受自己身体走样都感受不到，把饭吃好，都做不到，那其他方面是很难做到的，\n方法三：\n我的第三个建议就是在工作学习过程中，尽量使用射灯，就是这个灯得光只会局部打亮，就是光线会聚集在一个范围得那种灯，这种仪式感得建立可以让你的注意力稍微集中一点儿，因为其他环境都是黑的，有点儿像舞台得那种感觉，你把自己放在舞台上，应该会把自己放在观众席上面更加专注一些，有的人是无法专注，有的人是过分专注，也就是所谓的沉溺。\n方法四：\n最后我要说一个平常生活中非常影响大家专注的事就是睡眠，睡眠不够，人很难做到专注，如果你是一个长期缺觉得人，你想专注那真是太难了，所以我在这里告诉你，困了就睡，不要困得不行了，还要硬撑者做事，这个效率是非常低的，你不如立即进入到高质量的睡眠，你说怎么进入高质量的睡眠，只有当你累坏了的时候才能进入高质量的睡眠，也就是感觉非常困了，你要知道小孩子睡眠经常伴随很多次得夜醒，这种睡眠质量算是非常低的。\n4、大师\n要有明确的目标，直奔大师，不必从基础开始。你找一些乐器课程，都是让你基础开始学习，这是学生得学习路径，但是我们是成年人，如果是从基础开始，是没办法坚持下来的。要去到哪里，从哪里开始\n比如你要学习吉他，正好你有一首特别喜欢的歌，那么就练那一首歌，练三个月。你看你能不能学会。\n辅导机构为啥让你从基础开始啊，因为不从基础开始，他们怎么挣钱呢？\n这里面即涵盖了兴趣，也包含了任务\n在这里你肯定很好奇，为啥说吉他，这与大师有啥关系，因为让我直接去弹自己喜欢的歌曲，这个方法就是大师教的\n那么对应得就是要学习人工智能，你就要去找，这个里面教学水品最好的，尽管会稍微贵一点儿（当然也有教学效果好，还不贵得，有的话我一定在下面推荐），但是效果好呀，节省下来的时间，和让自己提升得层次水平，让你在工作里面不知道能多赚多少钱。\n5、制造反馈\n如何自己给自己制造反馈，写一点儿东西出来，录制一点儿东西出来，以前我给学生讲物理题的时候，我都让学生给我讲一遍，给我讲清楚算学懂。缺少反馈的是经常是就是平时看书，看完了啥都没记下。\n看书的时候，写摘要。 Summary 需要看着文本写，，其中的例子，就不需要出现了。Summary需要用自己的话来写，找出文本当中重要得事实，观点和论据 ，要用自己的话来解释他的话。通过大量的应用和不断的重复，你没想背也把它记住了，要的是这种效果，当我们学习英语的时候。尤其是那种你经常犯错的地方，你需要及时反馈，不然你会在错误当中不停打转，比如学习英语过程中，英语语音掌握就是比较容易犯错误的情况，接着给英语发音给你讲一讲  印度口音   日本口音。练习出了问题，精度不够，重新回炉，要给自己制造反馈，将自己录一下听一听，不录下来就不知道自己丑态百出，你能想象舞蹈演员为啥要对着镜子练习么？\n人工智能学习，怎么制造反馈呢，你把你学习心得，学习过程中遇到的问题，可以当经验分享出来，让别人帮你看哪里有问题，或者自己找问题。\n6、突破学习瓶颈\n涉猎范围不够广，影响了你发挥，\n专注力不够，影响了你发挥。\n遇到瓶颈不是方法可能是心态。怎么通过调整心态突破瓶颈，打游戏可以培养自信心。技术层面做出调整，突破瓶颈，也就是调整方法，这可能就是心态的问题了。心态调整的好，做好多事就会容易很多，这跟训练专注力，使用射灯的效果是一样的。遇到瓶颈的第二个瓶颈可能是涉猎范围不够广，比如你要学乐器，你发现你都没有听世界级小提请演奏家的的演奏，突破美感的东西就不能靠重复练习了，如果要突破自己瓶颈就需要涉猎范围广，听一些世界级演奏家的演奏，功夫在世外，如果涉猎范围也够了，可你还是没办法突破，可能是你的专注力不够。一通百通，一个拿下来，你去学其他的，就会比较快\n所以学习人工智能得过程中，我们不能只闷头学习，应该也要多听听大师讲座，一些相关的资讯，来加深自己理解。甚至你看看人工智能其他流派的学术著作，都有助于你理解人工智能。\n（本文来源：知乎。著作权归作者所有。）\n上面怎样学习是来自于https://blog.csdn.net/za8KFnpo2/article/details/80028009这个转载的博客地址。"}
{"content2":"第二届人工智能竞赛——题目一、人脸识别\n文章目录\n第二届人工智能竞赛——题目一、人脸识别\n一、简介\n二、题目要求\n三、参考资料\n一、简介\n人脸识别是计算机视觉领域中十分常见的技术，同时网上有大量公开数据集可以用于人脸识别模型的训练，不仅如此，网上也公开了很多优秀的模型给开发者直接使用。\n二、题目要求\n利用现有的深度学习模型实现人脸识别，正确识别出视频流中的人的姓名。\n设计一个可展示的UI界面，界面应简洁、友好、功能完善。\n通过P-R曲线、ROC 等曲线来评估模型的效果。\n三、参考资料\nhttps://github.com/Tonyfy/LCNN_TRAIN\nhttps://github.com/eglxiang/vgg_face\nhttps://github.com/jmu201521121021/FaceDetector-Base-Yolov3-spp\nUI界面可用QT编写"}
{"content2":"深度学习代码专栏\nCSDN技术主题月----“深度学习”代码\n注：下面有project网站的大部分都有paper和相应的code。Code一般是C/C++或者Matlab代码。\n最近一次更新：2013-3-17\n一、特征提取Feature Extraction：\n·         SIFT [1] [Demo program][SIFT Library] [VLFeat]\n·         PCA-SIFT [2] [Project]\n·         Affine-SIFT [3] [Project]\n·         SURF [4] [OpenSURF] [Matlab Wrapper]\n·         Affine Covariant Features [5] [Oxford project]\n·         MSER [6] [Oxford project] [VLFeat]\n·         Geometric Blur [7] [Code]\n·         Local Self-Similarity Descriptor [8] [Oxford implementation]\n·         Global and Efficient Self-Similarity [9] [Code]\n·         Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]\n·         GIST [11] [Project]\n·         Shape Context [12] [Project]\n·         Color Descriptor [13] [Project]\n·         Pyramids of Histograms of Oriented Gradients [Code]\n·         Space-Time Interest Points (STIP) [14][Project] [Code]\n·         Boundary Preserving Dense Local Regions [15][Project]\n·         Weighted Histogram[Code]\n·         Histogram-based Interest Points Detectors[Paper][Code]\n·         An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]\n·         Fast Sparse Representation with Prototypes[Project]\n·         Corner Detection [Project]\n·         AGAST Corner Detector: faster than FAST and even FAST-ER[Project]\n·         Real-time Facial Feature Detection using Conditional Regression Forests[Project]\n·         Global and Efficient Self-Similarity for Object Classification and Detection[code]\n·         WαSH: Weighted α-Shapes for Local Feature Detection[Project]\n·         HOG[Project]\n·         Online Selection of Discriminative Tracking Features[Project]\n二、图像分割Image Segmentation：\n·           Normalized Cut [1] [Matlab code]\n·           Gerg Mori’ Superpixel code [2] [Matlab code]\n·           Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]\n·           Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]\n·           OWT-UCM Hierarchical Segmentation [5] [Resources]\n·           Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]\n·           Quick-Shift [7] [VLFeat]\n·           SLIC Superpixels [8] [Project]\n·           Segmentation by Minimum Code Length [9] [Project]\n·           Biased Normalized Cut [10] [Project]\n·           Segmentation Tree [11-12] [Project]\n·           Entropy Rate Superpixel Segmentation [13] [Code]\n·           Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]\n·           Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]\n·           Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]\n·           Random Walks for Image Segmentation[Paper][Code]\n·           Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]\n·           An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]\n·           Geodesic Star Convexity for Interactive Image Segmentation[Project]\n·           Contour Detection and Image Segmentation Resources[Project][Code]\n·           Biased Normalized Cuts[Project]\n·           Max-flow/min-cut[Project]\n·           Chan-Vese Segmentation using Level Set[Project]\n·           A Toolbox of Level Set Methods[Project]\n·           Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]\n·           Improved C-V active contour model[Paper][Code]\n·           A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]\n·          Level Set Method Research by Chunming Li[Project]\n·          ClassCut for Unsupervised Class Segmentation[code]\n·         SEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]\n三、目标检测Object Detection：\n·           A simple object detector with boosting [Project]\n·           INRIA Object Detection and Localization Toolkit [1] [Project]\n·           Discriminatively Trained Deformable Part Models [2] [Project]\n·           Cascade Object Detection with Deformable Part Models [3] [Project]\n·           Poselet [4] [Project]\n·           Implicit Shape Model [5] [Project]\n·           Viola and Jones’s Face Detection [6] [Project]\n·           Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]\n·           Hand detection using multiple proposals[Project]\n·           Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]\n·           Discriminatively trained deformable part models[Project]\n·           Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]\n·           Image Processing On Line[Project]\n·           Robust Optical Flow Estimation[Project]\n·           Where's Waldo: Matching People in Images of Crowds[Project]\n·           Scalable Multi-class Object Detection[Project]\n·           Class-Specific Hough Forests for Object Detection[Project]\n·         Deformed Lattice Detection In Real-World Images[Project]\n·         Discriminatively trained deformable part models[Project]\n四、显著性检测Saliency Detection：\n·           Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]\n·           Frequency-tuned salient region detection [2] [Project]\n·           Saliency detection using maximum symmetric surround [3] [Project]\n·           Attention via Information Maximization [4] [Matlab code]\n·           Context-aware saliency detection [5] [Matlab code]\n·           Graph-based visual saliency [6] [Matlab code]\n·           Saliency detection: A spectral residual approach. [7] [Matlab code]\n·           Segmenting salient objects from images and videos. [8] [Matlab code]\n·           Saliency Using Natural statistics. [9] [Matlab code]\n·           Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]\n·           Learning to Predict Where Humans Look [11] [Project]\n·           Global Contrast based Salient Region Detection [12] [Project]\n·           Bayesian Saliency via Low and Mid Level Cues[Project]\n·           Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]\n·         Saliency Detection: A Spectral Residual Approach[Code]\n五、图像分类、聚类Image Classification, Clustering\n·           Pyramid Match [1] [Project]\n·           Spatial Pyramid Matching [2] [Code]\n·           Locality-constrained Linear Coding [3] [Project] [Matlab code]\n·           Sparse Coding [4] [Project] [Matlab code]\n·           Texture Classification [5] [Project]\n·           Multiple Kernels for Image Classification [6] [Project]\n·           Feature Combination [7] [Project]\n·           SuperParsing [Code]\n·           Large Scale Correlation Clustering Optimization[Matlab code]\n·           Detecting and Sketching the Common[Project]\n·           Self-Tuning Spectral Clustering[Project][Code]\n·           User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]\n·           Filters for Texture Classification[Project]\n·           Multiple Kernel Learning for Image Classification[Project]\n·          SLIC Superpixels[Project]\n六、抠图Image Matting\n·           A Closed Form Solution to Natural Image Matting [Code]\n·           Spectral Matting [Project]\n·           Learning-based Matting [Code]\n七、目标跟踪Object Tracking：\n·           A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]\n·           Object Tracking via Partial Least Squares Analysis[Paper][Code]\n·           Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]\n·           Online Visual Tracking with Histograms and Articulating Blocks[Project]\n·           Incremental Learning for Robust Visual Tracking[Project]\n·           Real-time Compressive Tracking[Project]\n·           Robust Object Tracking via Sparsity-based Collaborative Model[Project]\n·           Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]\n·           Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]\n·           Superpixel Tracking[Project]\n·           Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]\n·           Online Multiple Support Instance Tracking [Paper][Code]\n·           Visual Tracking with Online Multiple Instance Learning[Project]\n·           Object detection and recognition[Project]\n·           Compressive Sensing Resources[Project]\n·           Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]\n·           Tracking-Learning-Detection[Project][OpenTLD/C++ Code]\n·           the HandVu：vision-based hand gesture interface[Project]\n·           Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]\n八、Kinect：\n·           Kinect toolbox[Project]\n·           OpenNI[Project]\n·           zouxy09 CSDN Blog[Resource]\n·           FingerTracker 手指跟踪[code]\n九、3D相关：\n·           3D Reconstruction of a Moving Object[Paper] [Code]\n·           Shape From Shading Using Linear Approximation[Code]\n·           Combining Shape from Shading and Stereo Depth Maps[Project][Code]\n·           Shape from Shading: A Survey[Paper][Code]\n·           A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]\n·           Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]\n·           A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]\n·           Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]\n·           Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]\n·           Learning 3-D Scene Structure from a Single Still Image[Project]\n十、机器学习算法：\n·           Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]\n·           Random Sampling[code]\n·           Probabilistic Latent Semantic Analysis (pLSA)[Code]\n·           FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]\n·           Fast Intersection / Additive Kernel SVMs[Project]\n·           SVM[Code]\n·           Ensemble learning[Project]\n·           Deep Learning[Net]\n·           Deep Learning Methods for Vision[Project]\n·           Neural Network for Recognition of Handwritten Digits[Project]\n·           Training a deep autoencoder or a classifier on MNIST digits[Project]\n·          THE MNIST DATABASE of handwritten digits[Project]\n·          Ersatz：deep neural networks in the cloud[Project]\n·          Deep Learning [Project]\n·          sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]\n·          Weka 3: Data Mining Software in Java[Project]\n·          Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]\n·          CNN - Convolutional neural network class[Matlab Tool]\n·          Yann LeCun's Publications[Wedsite]\n·          LeNet-5, convolutional neural networks[Project]\n·          Training a deep autoencoder or a classifier on MNIST digits[Project]\n·          Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]\n·         Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]\n·         Sparse coding simulation software[Project]\n·         Visual Recognition and Machine Learning Summer School[Software]\n十一、目标、行为识别Object, Action Recognition：\n·           Action Recognition by Dense Trajectories[Project][Code]\n·           Action Recognition Using a Distributed Representation of Pose and Appearance[Project]\n·           Recognition Using Regions[Paper][Code]\n·           2D Articulated Human Pose Estimation[Project]\n·           Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]\n·           Estimating Human Pose from Occluded Images[Paper][Code]\n·           Quasi-dense wide baseline matching[Project]\n·           ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]\n·           Real Time Head Pose Estimation with Random Regression Forests[Project]\n·           2D Action Recognition Serves 3D Human Pose Estimation[Project]\n·           A Hough Transform-Based Voting Framework for Action Recognition[Project]\n·           Motion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]\n·         2D articulated human pose estimation software[Project]\n·         Learning and detecting shape models [code]\n·         Progressive Search Space Reduction for Human Pose Estimation[Project]\n·         Learning Non-Rigid 3D Shape from 2D Motion[Project]\n十二、图像处理：\n·         Distance Transforms of Sampled Functions[Project]\n·         The Computer Vision Homepage[Project]\n·         Efficient appearance distances between windows[code]\n·         Image Exploration algorithm[code]\n·         Motion Magnification 运动放大 [Project]\n·         Bilateral Filtering for Gray and Color Images 双边滤波器 [Project]\n·         A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]\n十三、一些实用工具：\n·           EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]\n·           a development kit of matlab mex functions for OpenCV library[Project]\n·           Fast Artificial Neural Network Library[Project]\n十四、人手及指尖检测与识别：\n·           finger-detection-and-gesture-recognition [Code]\n·           Hand and Finger Detection using JavaCV[Project]\n·           Hand and fingers detection[Code]\n十五、场景解释：\n·           Nonparametric Scene Parsing via Label Transfer [Project]\n十六、光流Optical flow：\n·         High accuracy optical flow using a theory for warping [Project]\n·         Dense Trajectories Video Description [Project]\n·         SIFT Flow: Dense Correspondence across Scenes and its Applications[Project]\n·         KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]\n·         Tracking Cars Using Optical Flow[Project]\n·         Secrets of optical flow estimation and their principles[Project]\n·         implmentation of the Black and Anandan dense optical flow method[Project]\n·         Optical Flow Computation[Project]\n·         Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]\n·         A Database and Evaluation Methodology for Optical Flow[Project]\n·         optical flow relative[Project]\n·         Robust Optical Flow Estimation [Project]\n·         optical flow[Project]\n十七、图像检索Image Retrieval：\n·           Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]\n十八、马尔科夫随机场Markov Random Fields：\n·         Markov Random Fields for Super-Resolution [Project]\n·         A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]\n十九、运动检测Motion detection：\n·         Moving Object Extraction, Using Models or Analysis of Regions [Project]\n·         Background Subtraction: Experiments and Improvements for ViBe [Project]\n·         A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]\n·         changedetection.net: A new change detection benchmark dataset[Project]\n·         ViBe - a powerful technique for background detection and subtraction in video sequences[Project]\n·         Background Subtraction Program[Project]\n·         Motion Detection Algorithms[Project]\n·         Stuttgart Artificial Background Subtraction Dataset[Project]\n·         Object Detection, Motion Estimation, and Tracking[Project]"}
{"content2":"前情提要\n上期结束前我们经过一些形态学处理得到了一幅这样的图（根据大家用的方法和参数设置可能会有出入）。\n可以看到即使经过一些腐蚀膨胀滤波的处理，图像依然有不少噪声，做计算机视觉就是这样的，没有银弹，只能不断的利用已有的信息逐步逼近我们想要的结果。\n本期内容\n本期介绍一些轮廓检测的方法，结合一些骚皮操作就能得到我们想要的车牌区域。\n一、矩形检测\n在OpenCV中检测矩形是用cv2.boundRect，接受的参数是一个由多个点组成的list，返回的是一个tuple，共有4个元素，分别表示矩形的左上角x坐标、左上角y坐标、宽度、高度，像这样。\n(x,y,w,h) = cv2.boundingRect([(10,10),(20,25),(30,30),(60,10),(2,15)])\n我准备了一个程序可以测试这个效果，运行程序以后点击图像可以在上面画点，按S键画矩形，按Q键退出。\n# coding: utf-8 import cv2 import numpy as np # 点集 points = [] # 窗口 window_name = 'DEMO' window = cv2.namedWindow(window_name) # 底图 img = np.zeros((300,300,3), dtype=np.uint8) # 鼠标回调 def on_mouse(event, x, y, flags, param): if event == cv2.EVENT_LBUTTONDOWN: print 'Click at (%d,%d)' % (x,y) points.append((x,y)) cv2.circle(img, (x,y), 3, (255,255,255), -1) # 程序入口 def main(): global points, img # 设置回调 cv2.setMouseCallback(window_name, on_mouse) # 画图 while True: cv2.imshow(window_name, img) k = cv2.waitKey(1) if k == ord('q'): # 退出 print 'EXIT' return elif k == ord('s'): # 绘制矩形 (x,y,w,h) = cv2.boundingRect(np.array(points)) cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2) elif k == ord('c'): # 清空 img[...] = 0 points = [] if __name__ == '__main__': main()\n由于我们得到的是掩码图而不是一堆点集，因此还要用一个函数检测出边界点——cv2.findContours。findContours可以找出各个连通域的内外边界点和结构化表示，在这里我们只需要外边界点而且不需要结构化信息。示例代码如下：\n# coding: utf-8 import cv2 import numpy as np # 程序入口 def main(): img = cv2.imread('mask.jpg') gray = img[..., 0] # 寻找点集 _, contours, _ = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # 画点 for cnt in contours: for point in cnt: point = (point[0][0], point[0][1]) cv2.circle(img, point, 1, (255,0,0), -1) # 画图 cv2.imshow('DEMO', img) cv2.imwrite('contours.jpg', img) cv2.waitKey(0) if __name__ == '__main__': main()\n检测到的点用蓝色标出，效果如下：\n接下来就可以对每个点集求出外接矩形了：\n# 在每个点集上求外接矩形 for cnt in contours: (x,y,w,h) = cv2.boundingRect(cnt) cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n效果如下：\n可以看到有很多个不同大小的矩形被检测出来了，但我们只想要车牌区域那一个，宽高比例信息是比较容易想到的用于筛选的信息之一，除此以外我还使用了白色区域占比的信息，代码如下：\n# 在每个点集上求外接矩形 for cnt in contours: (x,y,w,h) = cv2.boundingRect(cnt) # 白色区域占比 nonZeroRatio = float(cv2.countNonZero(gray[y:y+h, x:x+w])) / (w*h) # 宽高比 whRatio = float(w) / h # 高度大于20，宽高比大于3，白色区域占比大于0.7 if h > 20 and whRatio > 3.0 and nonZeroRatio > 0.7: cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n结果如下：\n放到原图上看看效果："}
{"content2":"计算机视觉—DoG和LoG算子\nbrycezou@163.com\n阅读本文，需要有一定的数字图像处理基础，否则不太容易明白数学公式想要传达的物理意义。希望通过仅此一篇文章就能让你理解图像处理中的高斯滤波（也叫高斯平滑、高斯模糊、高斯卷积）、DoG算子、LoG算子，以及它们之间的关系。下面先讲理论，再讲实际应用。在理论部分，一切语言都显得过于苍白，因此我只给出了最核心的、最简单的、最优美的公式，当然包括一些必要的推导过程。\n理 论 篇\n1、高斯函数\n在图像处理中，常用的二维高斯函数为\nG(x,y,σ)=12πσ2e−(x2+y2)/2σ2\nG(x,y,\\sigma)=\\frac{1}{2\\pi\\sigma^2}e^{-(x^2+y^2)/2\\sigma^2}\n2、DoG算子\nDoG\nDoG（\nDifference\nDifference\nof\nof\nGaussian\nGaussian ）算子定义为\nDoG=G(x,y,σ1)−G(x,y,σ2)\nDoG=G(x,y,\\sigma_1)−G(x,y,\\sigma_2)\n3、LoG算子\n拉普拉斯算子为\n∇2f=∂2f∂x2+∂2f∂y2\n\\nabla^2f=\\frac{\\partial^2f}{\\partial{x^2}}+\\frac{\\partial^2f}{\\partial{y^2}}\n对二维高斯函数应用拉普拉斯算子得\n∇2G=∂2G∂x2+∂2G∂y2=−2σ2+x2+y22πσ6e−(x2+y2)/2σ2\n\\nabla^2G=\\frac{\\partial^2G}{\\partial{x^2}}+\\frac{\\partial^2G}{\\partial{y^2}}= \\frac{-2\\sigma^2+x^2+y^2}{2\\pi\\sigma^6}e^{-(x^2+y^2)/2\\sigma^2}\nLoG\nLoG（\nLaplacian\nLaplacian\nof\nof\nGaussian\nGaussian ）算子定义为\nLoG=σ2∇2G\nLoG=\\sigma^2\\nabla^2G\n4、DoG和LoG的关系\n对二维高斯函数关于\nσ\n\\sigma 求一阶偏导数得\n∂G∂σ=−2σ2+x2+y22πσ5e−(x2+y2)/2σ2\n\\frac{\\partial G}{\\partial \\sigma}= \\frac{-2\\sigma^2+x^2+y^2}{2\\pi\\sigma^5}e^{-(x^2+y^2)/2\\sigma^2}\n不难发现\n∂G∂σ=σ∇2G\n\\frac{\\partial G}{\\partial \\sigma}=\\sigma\\nabla^2G\n在\nDoG\nDoG 算子中，令\nσ1=kσ2=kσ\n\\sigma_1=k\\sigma_2=k\\sigma，则\nDoG=G(x,y,kσ)−G(x,y,σ)\nDoG=G(x,y,k\\sigma)−G(x,y,\\sigma)\n进一步地\n∂G∂σ=limΔσ→0G(x,y,σ+Δσ)−G(x,y,σ)(σ+Δσ)−σ≈G(x,y,kσ)−G(x,y,σ)kσ−σ\n\\frac{\\partial G}{\\partial \\sigma}= \\lim_{\\Delta\\sigma\\to0}\\frac{G(x,y,\\sigma+\\Delta\\sigma)-G(x,y,\\sigma)}{(\\sigma+\\Delta\\sigma)-\\sigma}\\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma-\\sigma}\n因此\nσ∇2G=∂G∂σ≈G(x,y,kσ)−G(x,y,σ)kσ−σ\n\\sigma\\nabla^2G=\\frac{\\partial G}{\\partial\\sigma}\\approx \\frac{G(x,y,k\\sigma)-G(x,y,\\sigma)}{k\\sigma-\\sigma}\n即\nG(x,y,kσ)−G(x,y,σ)≈(k−1)σ2∇2G\nG(x,y,k\\sigma)-G(x,y,\\sigma)\\approx(k-1)\\sigma^2\\nabla^2G\n这表明\n可以用\nDoG\nDoG 算子来近似\nLoG\nLoG 算子\n。\n应 用 篇\n1、计算高斯卷积模板\n#include <iostream> #include <math.h> using namespace std; #define PI 3.1415926 int main(int argc, char *argv[]) { double sigma = 1; int N = 2*ceil(3*sigma)+1; double *val = new double[N*N]; int R = N/2; for(int i = 0; i < N; i++) { for(int j = 0; j < N; j++) { double r = (i-R)*(i-R)+(j-R)*(j-R); double res = exp(-r/(2*sigma*sigma)); res = res/(2*PI*sigma*sigma); val[i*N+j] = res; } } delete []val; return 0; }\n下图就是该段代码生成的\n5x5\n5x5 的高斯模板\n在图像处理中，为了提升计算速度，通常会牺牲少部分计算精度，使用整数模板代替浮点数模板。常见的\n3x3\n3x3 和\n5x5\n5x5 整数模板为\n116⎡⎣⎢⎢121242121⎤⎦⎥⎥,1273⎡⎣⎢⎢⎢⎢⎢1474141626164726412674162616414741⎤⎦⎥⎥⎥⎥⎥\n\\frac{1}{16} \\left[\\begin{matrix}1 & 2 & 1\\\\2 & 4 & 2\\\\1 & 2 & 1\\end{matrix}\\right], \\frac{1}{273} \\left[\\begin{matrix}1 & 4 & 7 & 4 & 1\\\\4 & 16 & 26 & 16 & 4\\\\7 & 26 & 41 & 26 & 7\\\\4 & 16 & 26 & 16 & 4\\\\1 & 4 & 7 & 4 & 1\\end{matrix}\\right]\n2、快速计算高斯卷积\n由于高斯函数可以写成可分离的形式，因此可以采用可分离滤波器来实现加速。\n可分离滤波器，就是可以把一个多维的卷积转化成多个一维的卷积。\n具体到二维的高斯滤波，就是指先对行做一维卷积，再对列做一维卷积。这样就可以将计算复杂度从O(M*M*N*N)降到O(2*M*M*N)，M、N分别是图像和滤波器的窗口大小。\n3、DoG算子应用\n在理论上\nDoG(x,y,σ1,σ2)∗I(x,y)=G(x,y,σ1)∗I(x,y)−G(x,y,σ2)∗I(x,y)\nDoG(x,y,\\sigma_1,\\sigma_2)*I(x,y)=G(x,y,\\sigma_1)*I(x,y)-G(x,y,\\sigma_2)*I(x,y)\n因此，实际计算中，只需要先对输入图像作2个不同尺度的高斯平滑，然后将两幅图像相减，非常简单！\n4、公式推导——符号计算\n在理解算法原理的时候，难免要进行公式推导。事实上，本文涉及的公式，不全是作者自己推导出来的，例如，求偏导数的公式就是计算机辅助完成的，虽然偷了个懒，但可以快速验证自己的理解是否正确，效率挺高。常见的符号计算工具有MATLAB，但本人使用的是Mac，没装MATLAB，无意间发现有一个Python包\nSymPy\n可以进行符号计算，而且可以将结果导出为LaTeX格式，非常赞，强烈推荐！下面是一段示例代码\nfrom sympy import * x, y, s, pi, k = symbols('x,y,s,pi,k') G_0 = 1/(2*pi*s**2) * exp(-(x**2+y**2)/(2*s**2)) ff = diff(G_0, s, 1) gg = diff(G_0, x, 2) + diff(G_0, y, 2) print latex(together(ff)) print latex(together(gg))"}
{"content2":"一、什么是计算机视觉？\n1. 计算机视觉的定义\n计算机视觉是一门研究如何使机器“看”的科学，更进一步的说，就是指用摄影机和计算机代替人眼对目标进行识别、跟踪和测量等机器视觉的应用。主要用于模拟人类视觉的优越能力和弥补人类视觉的缺陷。\n模拟人类视觉的优越能力：\n识别人、物体、场景\n估计立体空间、距离\n躲避障碍物进行导航\n想象并描述故事\n理解并讲解图片\n弥补人类视觉的缺陷：\n关注显著内容、容易忽略很多细节，不擅长精细感知\n描述主观、模棱两可\n不擅长长时间稳定的执行同一任务\n2. 计算机视觉的两个主要研究维度\n语义感知(Semantic)\n几何属性(Geometry)\n3. 计算机视觉的主要目标\n什么是人工智能（AI）？\n二、计算机视觉的基础及其应用(待细分扩展…….)\n1. 计算机视觉的基础\n数字图像处理\n空域分析及变换（Sobel，拉普拉斯，高斯，中值等）\n频域分析及变换（Fourier & Wavelet Transform）\n模板匹配，金字塔，滤波器组\n特征数据操作：主成分分析/PCA，奇异值分解/SVD，聚类/Cluster\n图像特征及描述\n颜色特征（RBG，HSV，Lab等）\n几何特征（Edge，Corner，Blob等）\n纹理特征（HOG，LBP，Gabor等）\n局部特征（SIFT，SURF，FAST等）\n2. 深度学习在计算机视觉中的应用\n图像分类(Image Classification)\n卷积神经网络CNN\n对应有没有问题？，有的话，给出属于某类概率的多少？\n图像检测(Image Detection)\n区域卷积神经网络R-CNN\n对应目标在哪儿问题？，用矩形框框出目标\n图像分割(Image Segmentation)\n全卷积神经网络FCN\n对应每个像素的类别问题？，用不同颜色画出图像中所有类别的区域轮廓\n图像识别(Image Identification)\n人脸识别、车牌识别、字符识别、行为识别等\n对应内容是什么问题？\n注意它和Image Verification的区别？\n图像描述(Image Captioning)\n迭代神经网络(Vanilla-RNN，LSTM，GRU)\n图像问答(Image Question Answering)\n迭代神经网络RNN\n图像生成(Image Generation)\n生成对抗网络GAN\n3. 图像检索(Content-based Image Retrieval)\n以文搜图、以图搜图、图文联搜，找出语义或图像相似的图片\n三、计算机视觉的主要研究挑战\n视角变化、光照变化、尺度变化、形态变化\n背景混淆干扰、遮挡、类内物体的外观差异\n四、实战环境配置\nwin 10 下使用 win_anaconda3-4.2.0 百度云下载(python3.5) 安装 py-opencv\n从上面云盘链接下载并安装 Anaconda3-4.2.0\n打开 cmd，然后执行conda install -c https://conda.anaconda.org/menpo opencv3\n打开 ipython 测试一下：import cv2 print(cv2.__version__)\nwin 10 下使用 win_anaconda3-4.2.0 百度云下载(python3.5) 安装 TensorFlow\n从上面云盘链接下载并安装 Anaconda3-4.2.0\n打开 cmd，然后执行pip install tensorflow # CPU 版 或 pip install tensorflow-gpu # GPU 版\nUbuntu 下使用 linux_anaconda2-4.3.1 百度云下载(python2.7) 安装 TensorFlow\n按照此教程安装CUDA8.0 和 cuDNN v5.1\n安装 libcupti-dev library，sudo apt-get install libcupti-dev\n使用 Anaconda 创建虚拟环境，激活环境后，根据 Anaconda 中的 Python 版本选择 TF_PYTHON_URL 安装 TensorFLow\n# 创建虚拟环境 conda create -n tensorflow # 激活环境 source activate tensorflow # 根据 Anaconda 中的 Python 版本选择 TF_PYTHON_URL(即 upgrade 后面的链接)，然后安装 TensorFLow pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.1.0-cp27-none-linux_x86_64.whl # 安装完成后，可以打开 ipython 进行验证 >>> import tensorflow as tf >>> hello = tf.constant('Hello, TensorFlow!') >>> sess = tf.Session() >>> print sess.run(hello) Hello, TensorFlow! # 至此，TF 安装成功，当你想离开此环境时，使用以下命令即可 source deactivate\n使用jupyter notebook调用远程服务器，参考此链接中的第三点详细介绍"}
{"content2":"HN上有人贴了Andrej Karpathy 2012年写的旧文，引起了热议。\n文章对人工智能发展并不太乐观，与今天大众媒体对人工智能的炒作恰成鲜明对照。Karpathy以上图为例，说明很多信息人类瞬间就能理解并发出会心微笑，我们只需要处理RGB的二维数组，用这些简单的冰山一角，利用已有知识，推导出巨大的整个冰山，这种能力是人工智能系统面临的最难任务。\n计算机系统要处理其中很多微妙（三面镜子，很多人及其映射，整个场面的很多含义，包括奥巴马动作的三维意义，那个称重的人如果发现俺自己超重的心情变化，其中的物理学等等）极其困难。先不说推断算法是否能完成这么复杂的任务，怎么收集所需的数据呢？怎么着手呢？\n就拿计算机视觉领域代表最新成果的Imagenet和Pascal VOC比赛中的研究水平来说，姿势估计、动作识别方面有很多工作，但都是很具体、孤立的场景，而且都未完成。总结成一句话，就是“路漫漫其修远兮”。前路漫长，充满不确定性。\nAndrej Karpathy是斯坦福大学人工智能方面的研究生，师从华裔女教授李飞飞，方向就是深度学习。Imagenet比赛，斯坦福大学李飞飞组是重要发起者、组织者。作为是圈内人，他的看法很典型。\n最近人工智能似乎很热很乐观，以至于霍金、盖茨和Elon Musk都开始担心发展太快，人类有风险了。但有意思的是，真正在一线做研究的人反而大多比较悲观。\n这篇文章写于2012年，那么3年后作者的态度如何呢？从文后的评论来看，至少5个月前，Karpathy的态度并没有什么变化。"}
{"content2":"计算机视觉 是一门研究如何使机器“看”的科学，更进一步的说，就是指用摄影机和计算机代替人眼对目标进行识别、跟踪和测量等，并进一步做图像处理，用计算机处理成为更适合人眼观察或传送给仪器检测的图像。\n计算机图形学和计算机视觉的区别：\n简单的来说，计算机视觉侧重于对图像的识别，得到一些结论，而计算机图形学侧重于如何从其他数据产生到图像。具体来说：计算机图形学（Computer Graphics）讲的是图形，也就是图形的构造方式，是一种从无到有的概念，从数据得到图像。是给定关于景象结构、表面反射特性、光源配置及相机模型的信息，生成图像。\n计算机视觉（Computer Vision） 是给定图象，从图象提取信息，包括景象的三维结构，运动检测，识别物体等。\n计算机的主要任务\n图像分类\n：分类任务是基础任务，而图像分类问题就是给输入图像分配标签类别的任务，这是计算机视觉的核心问题之一。一般说来，经典的图像分类算法是通过手工特征或者特征学习方法对整个图像进行全局描述，然后使用分类器判断是否存在某类物体。现在更多的是用端到端的深度学习技术。\n物体检测\n：物体检测是视觉感知的第一步，也是计算机视觉的一个重要分支。物体检测的目标，就是用框去标出物体的位置，并给出物体的类别。物体检测和图像分类不一样，检测侧重于物体的搜索，而且物体检测的目标必须要有固定的形状和轮廓。图像分类可以是任意的目标，这个目标可能是物体，也可能是一些属性或者场景。\n物体定位\n：如果说图像识别解决的是what，那么，物体定位解决的则是where的问题。利用计算视觉技术找到图像中某一目标物体在图像中的位置，即定位。目标物体的定位对于计算机视觉在安防、自动驾驶等领域的应用有着至关重要的意义。\n另外，物体定位的延伸目标跟踪，是指在给定场景中跟踪感兴趣的具体对象或多个对象的过程。简单来说，给出目标在跟踪视频第一帧中的初始状态（如位置、尺寸），自动估计目标物体在后续帧中的状态。该技术对自动驾驶汽车等领域显得至关重要。\n图像分割\n：图像分割指的是将数字图像细分为多个图像子区域（像素的集合，也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。更精确地说，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。另外，“图像语义分割”是一个像素级别的物体识别，即每个像素点都要判断它的类别。它和检测的区别是，物体检测是一个物体级别的，他只需要一个框，去框住物体的位置，而通常分割是比检测要更难的问题。\n图像标注\n：图像标注是一项引人注目的研究领域，它的研究目的是给出一张图片，你给我用一段文字描述它，近几年，工业界的百度，谷歌和微软 以及学术界的加大伯克利，深度学习研究重地多伦多大学都在做相应的研究。\n图像生成–文字转图像\n：图片标注任务本来是一个半圆，既然我们可以从图片产生描述文字，那么我们也能从文字来生成图片。这个任务也是非常有趣的，特别是在深度学习模型GAN被研发出来之后，这个任务也有更多的方法来解决.\n计算机视觉主要模型技术\n图像分类设计主要的模型-卷积神经网络（CNN）\n目标检测涉及主要模型是Fast R-CNN算法、YOLO、SSD以及R-FCN等等\n基于CNN完成目标跟踪的典型算法是FCNT和MD Net。\n图像分割的经典模型包括FCN模型，Mask R-CNN。\n图像标注的话可以看下谷歌开源的“Show and Tell”\n图像生成在今年来主流的方式是GAN模型和VAE模型，这两个生成模型都值得好好研究"}
{"content2":"这三者之间联系和区别可以通过下图表示， 左边的图片表示实际景物，右边图片表示实际景物对应的图片。\n1、计算机图形\n计算机图形技术常用于计算机生成图形。该技术常用的领域有：\na.动漫\nb.游戏\nc.计算机辅助设计（CAD）\n2、计算机视觉\n计算机视觉技术在于分析图片得出有用信息。该技术常用领域为：\na.人脸识别\nb.自动驾驶\nc.指纹识别\n3、图像处理\n图像处理顾名思义当然是处理图片了，由原图片得到新的图片。该技术常用的产品有：\na.美图秀秀\nb.PS"}
{"content2":"尽管人们对计算机视觉这门学科的起始时间和发展历史有不同的看法，但应该说， 1982年马尔( David Marr )《视觉》（Marr, 1982）一书的问世，标志着计算机视觉成为了一门独立学科。\n计算机视觉40多年的发展中，尽管人们提出了大量的理论和方法，但总体上说，计算机视觉经历了4个主要历程。即： 马尔计算视觉、主动和目的视觉、多视几何与分层三维重建和基于学习的视觉。\n马尔计算视觉（Computational Vision）\n马尔计算视觉理论包含两个主要观点：首先，马尔认为人类视觉的主要功能是复原三维场景的可见几何表面，即三维重建问题；其次，马尔认为这种从二维图像到三维几何结构的复原过程是可以通过计算完成的，并提出了一套完整的计算理论和方法。所以，马尔视觉计算理论在一些文献中也被称为三维重建理论。\n马尔认为，从二维图像复原物体的三维结构，涉及三个不同的层次。首先是计算理论层次，也就是说，需要使用何种类型的约束来完成这一过程。马尔认为合理的约束是场景固有的性质在成像过程中对图像形成的约束。其次是表达和算法层次，也就是说如何来具体计算。最后是实现层次。马尔对表达和算法层次进行了详细讨论。\n他认为从二维图像恢复三维物体，经历了三个主要步骤，即图像初始略图（sketch）—>物体2.5维描述—>物体3维描述。其中，初始略图是指高斯拉普拉斯滤波图像中的过零点（zero-crossing）、短线段、端点等基元特征。物体2.5维描述是指在观测者坐标系下对物体形状的一些粗略描述，如物体的法向量等。物体3维描述是指在物体自身坐标系下对物体的描述，如球体以球心为坐标原点的表述。\n马尔视觉计算理论是上世纪八十年代初提出的，之后三十多年的研究中，人们发现马尔理论的基本假设：\"人类视觉的主要功能是复原三维场景的可见几何表面\"——基本上是不正确的，\"物体识别中的三维表达的假设\"——也基本与人类物体识别的神经生理机理不相符。尽管如此，马尔计算视觉理论在计算机视觉领域的影响是深远的，他所提出的层次化三维重建框架，至今是计算机视觉中的主流方法。尽管文献中很多人对马尔理论提出了质疑、批评和改进，但就目前的研究状况看，还没有任何一种理论可以取代马尔理论，或与其相提并论。\n昙花一现的主动和目的视觉\n很多人介绍计算机视觉时，将这部分内容不作为一个单独部分加以介绍，主要是因为“主动视觉和目的视觉”并没有对计算机视觉后续研究形成持续影响。\n20世纪80年代初马尔视觉计算理论提出后，学术界兴起了“计算机视觉”的热潮。人们想到的这种理论的一种直接应用就是给工业机器人赋予视觉能力，典型的系统就是所谓的“基于部件的系统”（parts-based system）。然而，10多年的研究，使人们认识到，尽管马尔计算视觉理论非常优美，但“鲁棒性”（Robustness）不够，很难想人们预想的那样在工业界得到广泛应用。这样，人们开始质疑这种理论的合理性，甚至提出了尖锐的批评。\n对马尔计算视觉理论提出批评最多的有二点：一是认为这种三维重建过程是\"纯粹自底向上的过程\"（pure bottom-up process），缺乏高层反馈（top-down feedback）；二是\"重建\"缺乏\"目的性和主动性\"。由于不同的用途，要求重建的精度不同，而不考虑具体任务，仅仅\"盲目地重建一个适合任何任务的三维模型\"似乎不合理。\n对马尔视觉计算理论提出批评的代表性人物有：马里兰大学的 J. Y. Aloimonos;宾夕法尼亚大学的R. Bajcsy和密西根州立大学的A. K. Jaini。 Bajcsy 认为，视觉过程必然存在人与环境的交互，提出了主动视觉的概念（active vision）. Aloimonos认为视觉要有目的性，且在很多应用，不需要严格三维重建，提出了\"目的和定性视觉\"（purpose and qualitative vision） 的概念。 Jain 认为应该重点强调应用，提出了\"应用视觉\"（ practicing vision）的概念。上世纪80年代末到90年代初，可以说是计算机视觉领域的\"彷徨\"阶段。真有点\"批评之声不绝，视觉之路茫茫\"之势。\n值得指出的是，\"主动视觉\"应该是一个非常好的概念，但困难在于\"如何计算\"。 主动视觉往往需要\"视觉注视\"（visual attention），需要研究脑皮层（cerebral cortex）高层区域到低层区域的反馈机制，这些问题，即使脑科学和神经科学已经较20年前取得了巨大进展的今天，仍缺乏\"计算层次上的进展\"可为计算机视觉研究人员提供实质性的参考和借鉴。\n多视几何和分层三维重建（Multiple View Geometry and Stratified 3D Reconstruction）\n20世纪90年代初计算机视觉从\"萧条\"走向重新\"繁荣\"，主要得益于以下二方面的因素：首先，瞄准的应用领域从精度和鲁棒性要求太高的\"工业应用\"转到要求不太高，特别是仅仅需要\"视觉效果\"的应用领域，如远程视频会议（teleconference），考古，虚拟现实，视频监控等。另一方面，人们发现，多视几何理论下的分层三维重建能有效提高三维重建的鲁棒性和精度。\n2000 年Hartley 和Zisserman 合著的书 (Hartley & Zisserman 2000) 对这方面的内容给出了比较系统的总结，而后这方面的工作主要集中在如何提高\"大数据下鲁棒性重建的计算效率\"。大数据需要全自动重建，而全自动重建需要反复优化，而反复优化需要花费大量计算资源。所以，如何在保证鲁棒性的前提下快速进行大场景的三维重建是后期研究的重点。\n举一个简单例子，假如要三维重建北京中关村地区，为了保证重建的完整性，需要获取大量的地面和无人机图像。假如获取了1万幅地面高分辨率图像（4000×3000），5 千幅高分辨率无人机图像（8000×7000）（这样的图像规模是当前的典型规模），三维重建要匹配这些图像，从中选取合适的图像集，然后对相机位置信息进行标定并重建出场景的三维结构，如此大的数据量，人工干预是不可能的，所以整个三维重建流程必须全自动进行。这样需要重建算法和系统具有非常高的鲁棒性，否则根本无法全自动三维重建。在鲁棒性保证的情况下，三维重建效率也是一个巨大的挑战。所以，目前在这方面的研究重点是如何快速、鲁棒地重建大场景。\n基于学习的视觉（Learning based vision）\n基于学习的视觉，是指以机器学习为主要技术手段的计算机视觉研究。基于学习的视觉研究，文献中大体上分为二个阶段：本世纪初的以流形学习( manifold Learning)为代表的子空间法( subspace method)和目前以深度神经网络和深度学习（deep neural networks and deep learning）为代表的视觉方法。\n1、流形学习（Manifold Learning）\n正像前面所指出的，物体表达是物体识别的核心问题。给定图像物体，如人脸图像，不同的表达，物体的分类和识别率不同。另外，直接将图像像素作为表达是一种\"过表达\"，也不是一种好的表达。流形学习理论认为，一种图像物体存在其\"内在流形\"（intrinsic manifold）, 这种内在流形是该物体的一种优质表达。所以，流形学习就是从图像表达学习其内在流形表达的过程，这种内在流形的学习过程一般是一种非线性优化过程。\n流形学习始于2000年在Science 上发表的二篇文章（ Tenenbaum et al., 2000） (Roweis & Lawrence 2000)。流形学习一个困难的问题是没有严格的理论来确定内在流形的维度。人们发现，很多情况下流形学习的结果还不如传统的PCA （Principal Component Analysis），LDA（ linear DiscriminantAnalysis ）， MDS（ Multidimensional Scaling）等。流形学习的代表方法有：LLE（Locally Linear Embedding ）(Roweis & Lawrence 2000)，Isomap （ Tenenbaum et al., 2000）， Laplacian Eigenmaps (Belkin & Niyogi, 2001)等。\n2、深度学习（Deep Learning）\n深度学习的成功，主要得益于数据积累和计算能力的提高。深度网络的概念20世纪80年代就已提出来了，只是因为当时发现\"深度网络\"性能还不如\"浅层网络\"，所以没有得到大的发展。目前似乎有点计算机视觉就是深度学习的应用之势，这可以从计算机视觉的三大国际会议：国际计算机视觉会议（ICCV），欧洲计算机视觉会议（ECCV）和计算机视觉和模式识别会议（CVPR）上近年来发表的论文就可以看出。\n目前的基本状况是，人们都在利用深度学习来\"取代\"计算机视觉中的传统方法。\"研究人员\"成了\"调程序的机器\"，这实在是一种不正常的\"群众式运动\"。\n关于深度网络和深度学习，详细内容可参阅相关文献，这里仅仅强调以下几点：\n（1）深度学习在物体视觉方面较传统方法体现了巨大优势，但在空间视觉，如三维重建，物体定位方面，仍无法与基于几何的方法相媲美。这主要是因为深度学习很难处理图像特征之间的误匹配现象。在基于几何的三维重建中，RANSAC （Random Sample Consensus）等鲁棒外点（误匹配点）剔除模块可以反复调用，而在深度学习中，目前还很难集成诸如RANSAC等外点剔除机制。如果深度网络不能很好地集成外点剔除模块，深度学习在三维重建中将很难与基于几何的方法相媲美，甚至很难在空间视觉中得到有效应用；\n（2） 深度学习在静态图像物体识别方面已经成熟，这也是为什么在ImageNet上的物体分类竞赛已不再举行的缘故；\n（3） 目前的深度网络，基本上是前馈网络。不同网络主要体现在使用的代价函数不同。下一步预计要探索具有\"反馈机制\"的层次化网络。反馈机制，需要借鉴脑神经网络机制，特别是连接组学的成果。\n（4） 目前对视频的处理，人们提出了RCNN (Recurrent Neural Networks)。循环是一种有效的同层作用机制，但不能代替反馈。大脑皮层远距离的反馈可能是形成大脑皮层不同区域具有不同特定功能的神经基础。所以，研究反馈机制，特别具有\"长距离反馈\"（跨多层之间）的深度网络, 将是今后研究图像理解的一个重要方向；\n（5）尽管深度学习和深度网络在图像物体识别方面取得了\"变革性\"成果，但为什么\"深度学习\"会取得如此好的结果目前仍然缺乏坚实的理论基础。目前已有一些这方面的研究，但仍缺乏系统性的理论。事实上，\"层次化\"是本质，不仅深度网络，其它层次化模型，如Hmax 模型（Riesenhuber & Poggio,1999） HTM （Hierarchical Temporal memory）模型（George & Hawkins, 2009）存在同样的理论困惑。为什么\"层次化结构\"（ hierarchical structure ）具有优势仍是一个巨大的迷。\n本文部分内容摘自中国科学院自动化研究所胡占义研究员撰写的《计算机视觉简介：历史、现状和发展趋势》，重新编排"}
{"content2":"光流法理论背景\n1.什么是光流\n光流（optical flow）是空间运动物体在观察成像平面上的像素运动的\n瞬时速度\n。\n光流法是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。\n通常将二维图像平面特定坐标点上的\n灰度瞬时变化率定义为光流矢量\n。\n一言以概之：所谓光流就是瞬时速率，在时间间隔很小（比如视频的连续前后两帧之间）时，也等同于目标点的位移\n2.光流的物理意义\n一般而言，光流是由于场景中前景目标本身的移动、相机的运动，或者两者的共同运动所产生的。\n当人的眼睛观察运动物体时，物体的景象在人眼的视网膜上形成一系列连续变化的图像，这一系列连续变化的信息不断“流过”视网膜（即图像平面），好像一种光的“流”，故称之为光流。光流表达了图像的变化，由于它包含了目标运动的信息，因此可被观察者用来确定目标的运动情况。\n图（1）展示的便是三维空间内物体的运动在二维成像平面上的投影。得到的是一个描述位置变化的二维矢量，但在运动间隔极小的情况下，我们通常将其视为一个描述该点瞬时速度的二维矢量u=(u,v)，称为光流矢量。\n图（1） 三维运动在二维平面内的投影\n3.光流场\n在空间中，运动可以用运动场描述，而在一个图像平面上，物体的运动往往是通过图像序列中不同图像灰度分布的不同体现的，从而，空间中的运动场转移到图像上就表示为光流场（optical flow field）。\n光流场是一个二维矢量场，它反映了图像上每一点灰度的变化趋势，可看成是带有灰度的像素点在图像平面上运动而产生的瞬时速度场。它包含的信息即是各像点的瞬时运动速度矢量信息。\n研究光流场的目的就是为了从序列图像中近似计算不能直接得到的运动场。光流场在理想情况下，光流场对应于运动场。\n图（2）三维空间的矢量场及其在二维平面内的投影\n图（3）现实场景的可视化光流场\n三言以概之：所谓光流场就是很多光流的集合。\n当我们计算出了一幅图片中每个图像的光流，就能形成光流场。\n构建光流场是试图重现现实世界中的运动场，用以运动分析。\n光流法基本原理\n1.基本假设条件\n（1）亮度恒定不变。即同一目标在不同帧间运动时，其亮度不会发生改变。这是基本光流法的假定（所有光流法变种都必须满足），用于得到光流法基本方程；\n（2）时间连续或运动是“小运动”。即时间的变化不会引起目标位置的剧烈变化，相邻帧之间位移要比较小。同样也是光流法不可或缺的假定。\n2.基本约束方程\n考虑一个像素I(x,y,t)在第一帧的光强度（其中t代表其所在的时间维度）。它移动了 (dx,dy)的距离到下一帧，用了dt时间。因为是同一个像素点，依据上文提到的第一个假设我们认为该像素在运动前后的光强度是不变的，即：\n将(1)式右端进行泰勒展开，得：\n其中ε代表二阶无穷小项，可忽略不计。再将(2)代人(1)后同除dt，可得：\n设u,v分别为光流分别为沿X轴与Y轴的速度矢量，得：\n令分别表示图像中像素点的灰度沿X,Y,T方向的偏导数。\n综上，式(3)可以写为：\n其中，Ix,Iy,It均可由图像数据求得，而(u,v)即为所求光流矢量。\n约束方程只有一个，而方程的未知量有两个，这种情况下无法求得u和v的确切值。这种不确定性称为“孔径问题”。此时需要引入另外的约束条件，从不同的角度引入约束条件，导致了不同光流场计算方法。按照理论基础与数学方法的区别把它们分成四种：基于梯度（微分）的方法、基于匹配的方法、基于能量（频率）的方法、基于相位的方法和神经动力学方法。\n3.几种光流估计算法的简介\n1） 基于梯度的方法\n基于梯度的方法又称为微分法，它是利用时变图像灰度（或其滤波形式）的时空微分（即时空梯度函数）来计算像素的速度矢量。\n由于计算简单和较好的结果，该方法得到了广泛应用和研究。典型的代表是Horn-Schunck算法与Lucas-Kanade(LK)算法。\nHorn-Schunck算法在光流基本约束方程的基础上附加了全局平滑假设，假设在整个图像上光流的变化是光滑的，即物体运动矢量是平滑的或只是缓慢变化的。\n基于此思想，大量的改进算法不断提出。Nagel采用有条件的平滑约束，即通过加权矩阵的控制对梯度进行不同平滑处理；Black和Anandan针对多运动的估计问题，提出了分段平滑的方法。\n2) 基于匹配的方法\n基于匹配的光流计算方法包括基于特征和区域的两种。\n基于特征的方法不断地对目标主要特征进行定位和跟踪，对目标大的运动和亮度变化具有鲁棒性。存在的问题是光流通常很稀疏，而且特征提取和精确匹配也十分困难。\n基于区域的方法先对类似的区域进行定位，然后通过相似区域的位移计算光流。这种方法在视频编码中得到了广泛的应用。然而，它计算的光流仍不稠密。另外，这两种方法估计亚像素精度的光流也有困难，计算量很大。\n3)基于能量的方法\n基于能量的方法又称为基于频率的方法，在使用该类方法的过程中，要获得均匀流场的准确的速度估计，就必须对输入的图像进行时空滤波处理，即对时间和空间的整合，但是这样会降低光流的时间和空间分辨率。基于频率的方法往往会涉及大量的计算，另外，要进行可靠性评价也比较困难。\n4)基于相位的方法\n基于相位的方法是由Fleet和Jepson提出的，Fleet和Jepson最先提出将相位信息用于光流计算的思想。当我们计算光流的时候，相比亮度信息，图像的相位信息更加可靠，所以利用相位信息获得的光流场具有更好的鲁棒性。基于相位的光流算法的优点是：对图像序列的适用范围较宽，而且速度估计比较精确，但也存在着一些问题：第一，基于相位的模型有一定的合理性，但是有较高的时间复杂性；第二，基于相位的方法通过两帧图像就可以计算出光流，但如果要提高估计精度，就需要花费一定的时间；第三，基于相位的光流计算法对图像序列的时间混叠是比较敏感的。\n5）神经动力学方法\n神经动力学方法是利用神经网络建立的视觉运动感知的神经动力学模型，它是对生物视觉系统功能与结构比较直接的模拟。\n尽管光流计算的神经动力学方法还很不成熟，然而对它的研究却具有极其深远的意义。随着生物视觉研究的不断深入，神经方法无疑会不断完善，也许光流计算乃至计算机视觉的根本出路就在于神经机制的引入。神经网络方法是光流技术的一个发展方向。\n3.稠密光流与稀疏光流\n除了根据原理的不同来区分光流法外，还可以根据所形成的光流场中二维矢量的疏密程度将光流法分为稠密光流与稀疏光流两种。\n稠密光流\n稠密光流是一种针对图像或指定的某一片区域进行逐点匹配的图像配准方法，它计算图像上所有的点的偏移量，从而形成一个稠密的光流场。通过这个稠密的光流场，可以进行像素级别的图像配准。\nHorn-Schunck算法以及基于区域匹配的大多数光流法都属于稠密光流的范畴。\n图(4) 基于区域匹配方法生成稠密光流场图例\n由于光流矢量稠密，所以其配准后的效果也明显优于稀疏光流配准的效果。但是其副作用也是明显的，由于要计算每个点的偏移量，其计算量也明显较大，时效性较差。\n稀疏光流\n与稠密光流相反，稀疏光流并不对图像的每个像素点进行逐点计算。它通常需要指定一组点进行跟踪，这组点最好具有某种明显的特性，例如Harris角点等，那么跟踪就会相对稳定和可靠。稀疏跟踪的计算开销比稠密跟踪小得多。\n上文提到的基于特征的匹配方法是典型的属于稀疏光流的算法。\n图(5) 基于特征匹配方法生成稀疏光流场图例\nLucas-Kanade(LK)光流法\nLK光流法于1981年提出，最初是用于求稠密光流的，由于算法易于应用在输入图像的一组点上，而成为求稀疏光流的一种重要方法。\nLK光流法在原先的光流法两个基本假设的基础上，增加了一个“空间一致”的假设，即所有的相邻像素有相似的行动。也即在目标像素周围m×m的区域内，每个像素均拥有相同的光流矢量。以此假设解决式  无法求解的问题。\nLK光流法约束方程\n在一个小邻域内，LK光流法通过对下式的加权平方和最小化来估计光流矢量\n上式中是一个窗口权重函数，该函数使得邻域中心的加权比周围的大。对于Ω内的n个点X1⋯Xn，设\n故上面方程的解可由最小二乘法得到：\n最后得：\n通过结合几个邻近像素点的信息，LK光流法通常能够消除光流方程里的多义性。而且，与逐点计算的方法相比，LK方法对图像噪声不敏感。\n金字塔LK光流法\nLK算法的约束条件即：小速度，亮度不变以及区域一致性都是较强的假设，并不很容易得到满足。如当物体运动速度较快时，假设不成立，那么后续的假设就会有较大的偏差，使得最终求出的光流值有较大的误差。图像金字塔可以解决这个问题。\n考虑物体的运动速度较大时，算法会出现较大的误差。那么就希望能减少图像中物体的运动速度。一个直观的方法就是，缩小图像的尺寸。假设当图像为400×400时，物体速度为[16 16],那么图像缩小为200×200时，速度变为[8,8]。缩小为100*100时，速度减少到[4,4]。所以在源图像缩放了很多以后，原算法又变得适用了。所以光流可以通过生成 原图像的金字塔图像，逐层求解，不断精确来求得。简单来说上层金字塔（低分辨率）中的一个像素可以代表下层的四个。\n一段以补充\n：我灵魂画手在画下图时表达稍微不恰当Σ(っ°Д°;)っ，黑色方块实际上应该为方框。表示的是所选区域的\n大小\n在金字塔各层中的不变，而并不代表该区域内像素点保持怎样的状态或是有什么联系。没有！在图像缩放过程中每个区域的像素灰度值情况都会在变化。只是说我们基于LK算法假设的“\n空间一致\n”让“\n一片区域具有相同的运动状态\n”。这个所谓的区域并\n不是\n指某一个有相同特征的像素集合，\n仅仅只是一个框，一个范围的概念，一个大小的概念而已\n。\n一段以解释：黑色方块代表两个连续的帧内同一目标的不同位置。为了观察方便我才将其放在一张图里。\n面对“大运动”时我光流法的局限是什么？显然是运动距离大原算法不适用。（hhh）那么其实只要通过图片尺寸的不断缩小，而且目标选框大小保持不变，让运动前后两个物体的位置看上去“不断靠近”，直到变成小运动就可以使用光流法了。读者可能会存在很多疑问：1.为什么图片整体尺寸缩减时假定的目标其“尺寸”可以保持不变？2.只在分辨率很小的尺度内计算光流，有什么用？能替代真实的光流矢量的值吗？\n我先粗略解释一下，然后下文会展示详细的算法过程，让读者能有更深刻的认识。\n1.不论是在什么尺寸（尺度，缩小图片其实是模拟观察者的远近）的情况下，我们都坚持LK光流法的假设，即“空间一致”，不管我们看到图像是1000x1000还是10x10，我们都认为一个像素和它周围的一片区域内的若干个像素具有相同的速度。这是你使用LK光流法就必须承认的一个假设。这一片区域大小不一定是50x50，但始终是存在且不随客观情况改变只由你自己的判断而确定。\n2.在低尺度下只计算一次当然不准，为什么？你把图片弄得很模糊又很小，丢失了很多信息，还想检测准确？但是在低尺度下的光流矢量的测量能给我们一个指示，就像是低尺度下它综观全局，告诉你“运动大概是5点钟方向，大小大概是【32,57】”之类的信息。这条信息够粗糙，但是大体上是有用的。它使得你在进入金字塔的下一层时有一个大概的头绪，你由此指示可以在下一层金字塔再一次顺利“靠近”真实目标，强行符合“小运动”。这里可能说得很抽象，大家细细看下面的具体算法再回来看这一段或许会有所启示。\n（下面这一部分具体参照CSDN博客：https://blog.csdn.net/sgfmby1994/article/details/68489944 ，写得非常棒，受益匪浅）\n1)算法步骤简介\na.首先，对每一帧建立一个高斯金字塔，最低分辨率图像在最顶层,原始图片在底层。\nb.计算光流。从顶层(Lm层)开始，通过最小化每个点的邻域范围内的匹配误差和，得到顶层图像中每个点的光流。\n假设图像的尺寸每次缩放为原来的一半，共缩放了Lm层，则第0层为原图像。设已知原图的位移为d，则每层的位移为:\nc.顶层的光流计算结果(位移情况)反馈到第Lm-1层，作为该层初始时的光流值的估计g。\nd.这样沿着金字塔向下反馈，重复估计动作，直到到达金字塔的底层(即原图像)。\n(准确值=估计值+残差) “残差”即本算法的关键对于每一层L，每个点的光流的计算都是基于邻域内所有点的匹配误差和最小化。\n为了更好地理解金字塔LK光流法，下图简单展示了算法的实现过程：\n一段以解释：低尺度下找到光流矢量d0，将其扔到下一层去指引我们前行。扔下去后首先要放大两倍，此时你开始抱怨：“上一层找到的d0根本不靠谱，差距好大”（中蓝方块与右下角黑色方块的差距）但是你也要庆幸，正是“不靠谱”的d0让你到达了蓝色方块的位置，让你离真实区域前进了很多，否则你还在左上角苦于“小运动”寸步难行呢。在蓝色位置你就满足“小运动”了，继续计算光流，得到d1，然后把（2d0+d1）扔到下一层指导我们继续前行，如此往复。\n2)一些实现细节\n金字塔的构建\n利用低通滤波器平滑图像\n对平滑图像进行间隔采样，生成金字塔图像，每一层图像的高度与宽度均是下一层的二分之一。\n金字塔跟踪\n首先，从顶层开始计算金字塔最顶层图像上的光流。然后，根据最顶层(Lm-1) 光流的计算结果估计次顶层光流的初始值，再计算次顶层图像上光流的精确值。最后，根据次上层光流的计算结果估计下一层(Lm-2) 光流的初始值，计算其精确值后再反馈到下一 层，直至计算出最底层 的原始图像的光流。\n初始化顶层图像光流矢量估计值为0.\n邻域内所有像素点的匹配误差和记为：\n对其进行求导，得：\n对B(x+vx,y+vy)进行泰勒展开：\n导数公式可转化为：\n带入得：\n定义：\n有\n最终得光流矢量最优解为：\n观察G与b的组成，可知δI其实是两帧同层图像灰度之间的差值。而Ix,Iy分别是图像在该点处梯度的x方向分量与y方向分量。\n根据δI与Ix,Iy求得空间梯度 矩阵G和b。以此得到Lm层图像的最佳光流dLm。\n解释一下：前面说得好像很轻松，什么“然后就计算一下光流矢量”。但是具体要怎么做呢？我这里只粗略讲讲思路，具体结合上面的公式再来看或许才能有所感悟。\n很简单，我不是知道前一帧目标的具体位置(x,y)吗？我还给它画了个假定范围内速度一致对吧？好，现在我来个遍历，我让光流矢量（速度）取[-100,100]到[100,100]一个一个代（比如取[10,10]），然后到后一帧里让原来的目标x,y加上[10,10]，看看得到的这一个点（x+10,y+10）和上一帧的(x,y)点的灰度值差多少（别忘了我们最基础的假设一：同一目标运动过程中灰度值（光强）大小保持恒定，灰度值差当然越小我们就越认为找的没错啦），范围w内的其他像素点小伙伴们也一样加上[10,10]进行比较，看看误差有多大。如果觉得[10,10]误差太大了接受不了，就取[11,10]看看。直到取到一个（例如[73,29]）使得误差最小，那么我们就说[73,29]是（x,y）点的光流矢量了。\n这种遍历看上去是不是好low？的确如此。所以我们用的并不是这个方法。Σ(っ°Д°;)っΣ(っ°Д°;)っΣ(っ°Д°;)っ\n上文中 的其实就像是这样的一个函数：自变量是速度矢量，函数值是误差的大小。我们要求在什么速度矢量的输入下得到的误差最小，直接求导嘛！！！！！就不用一个一个v傻傻代入了呀。求导让导函数等于零，此时的v正是所求的光流矢量。此方法与上一段的“傻方法”效果一样，但是计算十分简便了。思路就是这么个思路，具体一些细节还是看上面。\n迭代过程\n将上层图像得到的光流矢量累加值传递到下一层做为初始值，即：\n可以看出，最终光流值就是所有层光流矢量的叠加。\n最后我们可以总结一下使用金字塔图像计算光流的好处，它对于每一次光流的都会保持很小，但是最终计算出来的光流可以进行放大然后累计。所以利用相对较小的邻域窗口就可以处理较大的像素运动。\n基于光流的运动目标检测（前景检测）算法\n基于光流运动目标检测是在对摄像机采集到的图像序列进行重采样和去噪预处理后，利用光流法计算出各点的光流值，得出各点的光流场。然后对光流场进行阈值分割，区分出前景与背景，得到清运动目标区域。一般还会再采用形态学滤波中的开、闭运算滤除孤立噪声点，最后经过区域连通便可识别出目标区域并统计其特征信息。流程图如下：\n实现原理\n使用光流法进行前景检测是是基于这样一个认识：\n如果图像中没有运动目标，则光流矢量在整个图像区域是连续变化的。当图像中有运动物体时，目标和背景存在着相对运动。运动物体所形成的速度矢量必然和背景的速度矢量有所不同，如此便可以计算出运动物体的位置。\n图（3）现实场景的可视化光流场\n通过观察上图我们可以看到，发生运动的物体的光流矢量与背景光流矢量之间存在差异。使用阀值分割可以将整幅图片的光流矢量分成两个部分，即区分出背景与前景。阀值的选取可以使用最大类间方差法（大津算法）来确定。它是按图像的灰度特性,将图像分成背景和目标两部分。背景和目标之间的类间方差越大,说明构成图像的2部分的差别越大,当部分目标错分为背景或部分背景错分为目标都会导致两部分差别变小。因此,使类间方差最大的分割意味着错分概率最小。\n光流场经过阈值分割后，有一些独立的点或者有凹区域，影响了运动目标的提取。可先利用开运算，去除那些光流值与结构元素不相吻合的凹区域，同时保留那些相吻合的凹区域。然后，利用形态学滤波的闭运算，填充凹区域。\n通过前面的处理，一帧图像中可能的目标区域已经成为一个可以连成一体的区域，采用合理的区域连通合并和分割技术来找出最终的目标区域。\n光流法的优缺点\n优点\n光流法的优点在于它无须了解场景的信息,就可以准确地检测识别运动日标位置,且在摄像机处于运动的情况下仍然适用。\n而且光流不仅携带了运动物体的运动信息，而且还携带了有关景物三维结构的丰富信息，它能够在不知道场景的任何信息的情况下，检测出运动对象。\n缺点\n光流法的适用条件，即两个基本假设，在现实情况下均不容易满足。\n假设一：亮度恒定不变。\n但是实际情况是光流场并不一定反映了目标的实际运动情况,如图,所示。图中,光源不动,而物体表面均一,且产生了自传运动,却并没有产生光流图中,物体并没有运动,但是光源与物体发生相对运动,却有光流产生。因此可以说光流法法对光线敏感, 光线变化极易影响识别效果。\n假设二：小运动。\n前文也有提到，现实情况下较大距离的运动也是普遍存在的。因此当需要检测的目标运动速度过快是，传统光流法也不适用。\n孔径问题\n观察上图(a)我们可以看到目标是在向右移动，但是由于“观察窗口”过小我们无法观测到边缘也在下降。LK算法中选区的小邻域就如同上图的观察窗口，邻域大小的选取会影响到最终的效果。当然，这是针对于一部分稀疏光流算法而言，属于稠密光流范畴的算法一般不存在这个问题。\n但是稠密光流法的显著缺点主要体现在,计算量大,耗时长,在对实时性要求苛刻的情况下并不适用。\n总结\n对于光流法来说，时效性与精确性难以兼得。\n在对于运动场景和目标无有效认知，运动模型难以预测、目标特征无法确定的情况下，若使用光流法构建稠密光流场，并且使用金字塔算法补充对于大幅度运动的检测精度，无论是应用于前景检测还是目标跟踪都能起到不错的效果，能够较高的检测精度。但是显而易见，这样的算法复杂度很高，计算量十分巨大，时效性极差。\n相反，若结合特征检测算法，针对特征点构建稀疏光流场，能够极大提高算法的执行效率。但是相对的，由于稀疏光流场所能获得的场景运动信息过少，检测精度与准确性难以保证。\n此外，光流法理论的基础建立在同一物体亮度恒定的假设上，现实中较难完全满足，这也是光流法的一大不足之处。\n参考资料\nhttps://blog.csdn.net/zhonghuan1992/article/details/38508185\nhttp://blog.csdn.net/zouxy09/article/details/8683859\nhttps://blog.csdn.net/conanlrj/article/details/5102481\nhttps://blog.csdn.net/gwplovekimi/article/details/80545274\n《基于光流法的运动目标检测与跟踪技术》裴巧娜《北方工业大学》 , 2009\n《基于改进光流法的运动目标检测》杨叶梅 《计算机与数字工程》 , 2011\n《光流法简述》孔令上，重庆邮电大学，2015\n（注：本文仅限于学习交流使用，若使用以上参考资料对原作者造成困扰，请私信我٩(๑òωó๑)۶）"}
{"content2":"论文地址\n从摄影测量到计算机视觉\n龚健雅\n摄影测量：透视几何、成像设备、摄影平台、测量法和测量工具\n几何角度：计算机视觉和摄影测量之间的紧密联系。\n语义方面  分析了遥感学科的发展，与机器学习和计算机视觉之间的关系， 深度学习和连接主义\n计算机视觉  人工智能\n1.     摄影测量\n2.     摄影测量与计算机视觉在几何上的联系\n用计算机代替人眼，从图片中重建和解译世界\n主要讨论几何上，也就是测量法理论上的差别，在我看来还是在计算机视觉的看部分，而不在处理图像部分，\n传统的摄影测量集中于航空和航天平台，处理航空图像和卫星遥感影像。\nSlam\n3.     遥感、机器学习与未来\n摄影测量的延伸是遥感，摄影测量已经解决了大部分几何问题，遥感的工作重点就集中在解译上，解译就是是什么和为什么的问题。\n在遥感中，相对于摄影测量的可见光，光谱段被大大扩充至多光谱并细化至高光谱。\n解决问题：土地覆盖物分类，农作物趋势分析，大气的长期变化监测，泥石流和洪水等自然灾害的评估与预测。\n抛开数据源不谈，   符号主义流派，基于统计学习思想\n连接主义流派，神经元网络模型和感知机，深度学习\n传统算法，几何领域，深度学习更关注于解译，但不代表在几何领域传统算法丧失有效性。\n深度学习，黑盒效应，导致传统，优雅的理论工作被简单调参所代替。"}
{"content2":"计算机视觉与图像：八大热点公司以及九大应用场景\nIT桔子 • 2017-05-11 • 人工智能\n人工智能系列图谱：八大热点公司以及九大应用场景\n编者按：本文来自微信公众号“IT桔子”（ID：itjuzi521），作者IT桔子数据分析部；36氪经授权发布。\n根据 IT 桔子数据，国内人工智能产业中「计算机视觉与图像」领域的公司数量已达 111 家，仅次于「自然语言处理」类公司，位居第二。\n我们将那些核心技术为「将深度学习技术应用于视觉与图像领域」的公司归到「计算机视觉与图像」类。同时，「无人机」和「机器人」相关公司并未被统计在内，这部分公司虽然也会涉及到计算机视觉技术，但以后将会单独在「无人机」和「机器人」专题中有所体现。\n我们通过对这 111 家公司进行观察，按公司的业务对企业进行统计如下：\n总融资额过亿的第一梯队公司盘点\n在这 111 家人工智能「视觉与图像」企业中，IT 桔子根据其历史融资数据进行了分析，并挑选出总融资额过亿元（人民币）的八家该领域创业公司，进行盘点：\n注：以下所有公司图谱来自桔子雷达 radar.itjuzi.com\nTOP 1：商汤科技融资总额 13.65 亿元人民币\n商汤科技专注于打造人工智能视觉引擎，基于自主研发的深度学习平台，输出全套人工智能视觉技术，包括成像处理、感知、识别，目前服务于金融、平安城市、机器人、无人驾驶等多个行业。\n4 月 20 日，商汤科技获得赛领资本 6000 万美元战略投资。本次融资后，商汤科技可能将重点精力放在商业化运作上，据商汤科技 CEO 徐立则透露，赛领的加盟将带来更多重量级的资源，帮助商汤的商业化布局。\nTOP 2：旷视科技融资总额 9.65 亿元人民币\n2016 年 12 月，Face++旷视科技融资一亿美元，成为了国内计算机视觉领域第一家完成 C 轮融资的创业公司，本轮投资方是建银国际和富士康。\n旷视目前的主营方向是互联网金融，这次建银国际的投资后，可能会进一步帮助让旷视在银行业务有所进展；另外，富士康的投资可能跟旷视在仓储机器人领域新成立的公司（艾瑞思机器人）有关。\nTOP 3：深醒科技融资总额 3 亿元人民币\n深醒科技成立于 2016 年初，以人脸识别技术切入 AI 战场，在识别动态模糊拍摄和暗光拍摄的人像上有技术优势。目前已面向安防监控、金融、地产、学校、医院等领域提供多种解决方案。\n2017 年 1 月最新一轮融资后，深醒科技表示会进一步在综合实战环境算法优化及数据训练等方面加大投入，并在中国公安领域加大市场投入，全面提升高科技在技防中的实战成效。此外，团队还在争取与中航工业、中石油、中铁建等有较高安防需求的企业选择与深醒合作。\nTOP 4：依图科技融资总额 2.18 亿元人民币\n依图科技也是人脸识别领域的佼佼者，2015 年获得「公安部科技进步奖」，并且与阿里云合作搭建「贵州公安交警大数据实时作战云平台」。同年，依图在与银行合作方面，招商银行将「依图人脸识别技术」推广到全国 1500 家网点。浦发银行「人像识别平台及直销银行远程开户视频认证建设项目」投产，实现 VTM 和手机银行的人脸身份认证。\n2016 年 3 月份，依图科技获得 B 轮数千万美元融资，本轮投资方云锋基金创始人马云、虞锋的企业管理经验可能将对依图有所帮助。依图已经开始在医疗健康，城市数据大脑等领域的探索和实践，并且为海关总署及中国边检提供人像比对系统。\nTOP 5：图普科技融资总额 2.15 亿元人民币\n图普科技是为数不多的技术紧密结合了盈利的商用场景的 AI 创业公司，在识别色情、暴恐、时政敏感信息、小广告等违规图片和视频方面市场挖掘出了国内市场的刚需获取了大量客户。随着移动直播的热潮，图普科技借势也获得了较大提升，已与国内大多数直播公司建立了合作。\n据之前图普科技数据显示，网络图像内容审查业务服务费约为 25 元／万张图片，提供月付费或包年方式。由此推测，图普科技的现金流和营收都很可观。但是，拿下了国内绝大多数直播公司的订单后，在该领域可能会很快迎来增长的瓶颈。\n2016 年 9 月份，图普科技拿到新一轮融资后，已经开始拓展新的业务方向。在人脸识别、物体及场景识别、文字识别等方向都已经推出服务。依图已经成为旷视（Face++）、商汤等公司的有力竞争对手。\nTOP 6：格灵深瞳融资总额 1.98 亿元人民币\n格灵深瞳致力于计算机视觉和深度学习，现主攻安防领域。格灵深瞳早期曾组建过汽车计算机视项目组，该项目组已与驭视科技合并，格灵深瞳现作为驭视科技的股东之一。\n2014 年 6 月，格灵深瞳获红杉中国数千万美元 A 轮投资。有尚未证实的消息称，格灵深瞳现拿到了新一轮战略投资，领投方为安防领域大佬，将来业务将可能继续深耕安防领域。\nTOP 7：云天励飞融资总额 1.95 亿元人民币\n云天励飞是一家专注于视觉智能领域，以深度学习和新型处理器技术为核心的创业公司，致力于实现视觉识别的「端智能」与大数据分析的「云智能」。曾服务过 2016 年的杭州 G20 峰会、双创周主会场、乌镇互联网大会等，并协助深圳公安破获各类案件 500 余起，找回多名失踪走失儿童。\n据云天励飞创始人陈宁称：「截止到 2016 年底，云天励飞已经在平安城市、智慧商业、无人机船车、机器人与智能制造等行业的 1000 家企业提供视觉芯片和智能解决方案。公司第一年即实现过亿元订单，2017 年预计销售额过 10 亿元。」\n今年 3 月，云天励飞获得数千万美元的 A 轮融资，投资方包括山水从容传媒投资有限公司、松禾资本、深投控、投控东海、红秀盈信等多家投资机构。云天励飞创始人陈宁透露，在获得本轮融资后，公司将会致力于软硬件的一体化建设。\nTOP 8：纵目科技融资总额 1.4 亿元人民币\n纵目科技成立于 2013 年，主要研发 2D 和 3D 全景视觉辅助驾驶系统，及自动停车和驾驶记录系统等。产品已被中国许多汽车制造商广泛使用，包括吉利汽车和上汽集团，以及野马等汽车配件供应商。\n今年 2 月 21 日，纵目科技正式挂牌新三板。两天后，纵目科技又完成了 1 亿元定增融资，认购方主要为北京君联成业股权投资合伙企业 (有限合伙)、厦门德丰嘉润股权投资基金合伙企业 (有限合伙)，两者分别认购 7000 万元、3000 万元。\n纵目科技透露，此轮融资将主要用于公司运营以及自动驾驶研发。公司计划在厦门新设子公司，开展 ADAS 系统相关传感器的研发和生产，向产业链前端延展布局。本次募集的部分资金将用于新设子公司的实缴出资。\n九大黄金应用场景盘点\n从以上 8 家公司的盘点我们可以看到的一点是，大家将人工智能「视觉与图像」技术的应用在各个领域、场景都有所应用与布局。那么在全部的 111 家企业中，哪些应用场景的热度与成熟度较高？\nIT 桔子详细梳理出了主要的九个不同场景下的应用，并根据企业分布数量排序进行盘点：\nTOP 1：人脸识别\nIT 桔子数据显示，「人脸识别」是人工智能「视觉与图像」领域中最热门的应用，在本次筛选出的 111 家公司中有接近三分之一的企业提供「人脸识别」产品或服务。\n今年 2 月，《麻省理工科技评论》发布「2017 全球十大突破性技术」榜单，来自中国的技术「刷脸支付」位列其中。这是该榜单创建 16 年来首个来自中国的技术突破。\n代表企业：Face++旷视科技、依图科技、商汤科技、深醒科技、云从科技等。\n人脸识别技术目前已经广泛应用于金融、司法、军队、公安、边检、政府、航天、电力、工厂、教育、医疗等行业。据业内人士分析，我国的人脸识别产业的需求旺盛，需求推动导致企业敢于投入资金。目前，该技术已具备大规模商用的条件，未来三到五年将高速增长。而今年，这一技术有望在金融与安防领域迎来大爆发。\nTOP 2：视频/监控分析\n在企业数量统计中，「视频/监控分析」是人工智能「视觉与图像」领域中第二大热门应用。\n代表企业：SenseTime 商汤科技、DeepGlint 格灵深瞳、依图科技、云天励飞、深网视界等。\n人工智能技术可以对结构化的人、车、物等视频内容信息进行快速检索、查询。这项应用使得让公安系统在繁杂的监控视频中搜寻到罪犯的有了可能。在大量人群流动的交通枢纽，该技术也被广泛用于人群分析、防控预警等。\n视频/监控领域盈利空间广阔，商业模式多种多样，既可以提供行业整体解决方案，也可以销售集成硬件设备。将技术应用于视频及监控领域在人工智能公司中正在形成一种趋势，这项技术应用将率先在安防、交通甚至零售等行业掀起应用热潮。\nTOP 3：图片识别分析\n「静态图片识别」应用热度在视觉与图像领域中排名第三。将人工智能技术单纯用于图片识别分析的应用企业数量并不如预想的多，可能有一下几个方面原因：\n1、目前视频监控方向的盈利空间大，众多企业的注意力都放在了视频监控领域；\n2、人脸识别属于图片识别的一个应用场景，做人脸识别的大多数企业同时也在提供图片识别服务，但是销售效果不佳，主要赢利点还在于人脸识别；\n3、图片识别大多商用场景还属于蓝海，潜力有待开发；\n4、图片数据大多被大型互联网企业所掌握，创业公司数据资源稀少。\n代表企业：Face++旷视科技、图普科技、码隆科技、酒咔嚓、YI+陌上花科技等。\nTOP 4：驾驶辅助/智能驾驶\n随着汽车的普及，汽车已经成为人工智能技术非常大的应用投放方向，但就目前来说，想要完全实现自动驾驶/无人驾驶，距离技术成熟还有一段路要走。\n不过利用人工智能技术，汽车的驾驶辅助的功能及应用越来越多，这些应用多半是基于计算机视觉和图像处理技术来实现。\n代表企业：纵目科技、TuSimple 图森科技、驭势科技、MINIEYE 佑驾创新、中天安驰等。\nMobileye 是一家以色列计算机视觉公司，是公认的全球一流驾驶辅助公司。在今年 3 月份，英特尔以 153 亿美元高价收购了 Mobileye，以布局自动驾驶。此事件成为以色列历史上最贵的一次科技收购案，也引起了市场的广泛关注。\nTOP 5：三维图像视觉\n三维图像视觉主要是对于三维物体的识别，应用于三维视觉建模，三维测绘等领域。\n代表企业：DeepGlint 格灵深瞳、拓视觉、非白三维科技、图漾科技等。\nTOP 6：工业视觉检测\n机器视觉可以快速获取大量信息，并进行自动处理。在自动化生产过程中，人们将机器视觉系统广泛地用于工况监视、成品检验和质量控制等领域。\n机器视觉系统的特点是提高生产的柔性和自动化程度。运用在一些危险工作环境或人工视觉难以满足要求的场合；此外，在大批量工业生产过程中，机器视觉检测可以大大提高生产效率和生产的自动化程度。\n代表企业：创想智控、凯视佳、拓视觉、帆声图像、华睿科技等。\nTOP 7：医疗影像诊断\n医疗数据中有超过 90% 的数据来自医疗影像。医疗影像领域拥有孕育深度学习的海量数据，医疗影像诊断可以辅助医生，提升医生的诊断的效率。\n2015 年 4 月，IBM 成立了 Watson Health 部门，开始进军医疗行业。2015 年 8 月 6 日，IBM 宣布以 10 亿美元的价格收购医疗影像公司 MergeHealthcare，并将其与新成立的 WatsonHealth 合并。2016 年 2 月，IBM 又斥资 26 亿美元收购医疗数据公司 TruvenHealthAnalytics。今年 2 月份，在 HIMSS17 大会上 Watson Health 公布了 IBM 的第一个认知影像产品 Watson Clinical Imaging Review，该产品可检查包括图像在内的医疗数据，帮助医疗服务提供商识别需要关注的最危急情况。\n代表企业：推想科技、雅森科技、汇医慧影、12Sigma 图玛深维、DeepCare 等。\nTOP 8：文字识别\n计算机文字识别，俗称光学字符识别，它是利用光学技术和计算机技术把印在或写在纸上的文字读取出来，并转换成一种计算机能够接受、人又可以理解的格式。这是实现文字高速录入的一项关键技术。\n今年三月份，海康威视研究院预研团队基于深度学习技术的 OCR（Optical Character Recognition，图像中文字识别）技术，刷新了 ICDAR Robust Reading 竞赛数据集的全球最好成绩，并在「互联网图像文字」、「对焦自然场景文字」和「随拍自然场景文字」三项挑战的文字识别（Word Recognition）任务中取得第一。同期参赛的有来自 82 个国家的 2367 支队伍参加，其中包括 Google、微软、百度、三星、旷视等团队。\n代表企业：海康威视、合合信息、鼎识科技、易道博识等。\nTOP 9：图像及视频编辑\n2016 年，Google 举行了一场「人工智能作家」的画展。通过一个名叫「DeepDream」的艺术生成器，谷歌可以将神经网络由内部传送到外部。不是识别图像，而是创作图像。有人称这些机器做的画为「机器之梦」。\n目前市场上也出现了很多运用及机器学习算法对图像进行处理，可以实现对图片的自动修复、美化、变换效果等操作。并且越来越受到用户青睐。\n近日，全球知名的数字媒体编辑软件供应商 Adobe，也加入了人工智能的大潮，发布了旗下首个基于深度学习和机器学习的底层技术开发平台——Adobe Sensei。\n代表公司：美图秀秀、泼辣熊、SenseTime 商汤科技、微禾迅科技等。\n总结\n在分析了「计算机视觉与图像」领域的公司中总融资额过亿元（人民币）的八家吸金大户及九大应用场景后，我们可以得到一些结论及推测。\n1、2016 年「人脸识别」成为了小风口，大量资本涌入，而 2017 年可能将是人脸识别产业应用产生突破性进展的一年。\n2、人脸识别和视频监控两大方向最受资本青睐，同时技术也在寻找其他方向的突破。\n3、「计算机视觉与图像」技术在互联网金融、银行业、安防、交通等行业应用最为广泛。\n4、八家吸金大户创业公司业务全为 toB 的，人工智能技术的普及确实非常需要借助行业的力量。\n5、目前「计算机视觉与图像」泡沫问题并不是特别突出，人工智能概念确实炒的过热，但大多数资本还是趋于理性。\n6、人脸识别、视频监控、互联网图像内容审查等应用，已经成为「计算机视觉与图像」技术的产业切入口，未来市场的认可将加速人工智能的产业化进程。\n7、我们在梳理企业的时候发现，目前互联网创业公司蹭人工智能热点的现象非常普遍，确实存在「人工智能泛化」的问题，但是多数真正拥有人工智能核心技术的公司，成长状况都比较良好。\n8、技术固然重要，但是，产业的切入点、应用场景、行业服务也是重中之重。最重要的是解决实际问题。"}
{"content2":"之前一直觉得自己学的杂而不专，在之后的学习和干活历程中也不断发现自己确实需要静下心来钻研，让自己像身边的师兄一样能有一技之长，能在社会上有自己的立足之地。\n讲真在计算机这个庞大的体系下挑选出一个方向来确实蛮难，自己挑来挑去，有种乱花渐欲迷人眼的感觉，不过就目前自己的兴趣、性格和学习能力，远景规划来看，计算机视觉这个方向目前来看对我自己来说确实是一个十分不错的选择。\n通过这篇文章希望可以在整理自己搜集的信息的同时，理清自己的思路，搞清楚计算机视觉大致是个什么东西，入坑后要学，要做些什么东西，它的前（钱）景怎么样？（这个说明一下啊，做任何事没有钱是万万不能的，钱代表了它的价值，如果研究出来的东西一文不值，那.....）\n目录：\n一、计算机视觉是什么\n二、计算机视觉里的一些应用方向现在认识有限以后遇到了再添加\n2-1. 物体识别和检测\n2-2. 语义分割\n2-3. 运动和跟踪\n2-4. 视觉问答\n2-5. 三维重建\n三、图像处理和计算机视觉的分类\n四、图像处理与计算机视觉涉及的知识和相关的书籍太深入的就不说了这里只浅显的介绍一下欢迎批评指正\n4-1. 数学知识\n4-2. 信号处理\n4-3. 模式识别\n4-4.  图像处理与计算机视觉的书籍推荐\n4-5. 小结\n五、图像处理绕不开的工具--OpenCV\n六、结语\n一、计算机视觉是什么：\n计算机视觉（Computer Vision）又称为机器视觉（Machine Vision），顾名思义是一门“教”会计算机如何去“看”世界的学科。在机器学习大热的前景之下，计算机视觉与自然语言处理（Natural Language Process， NLP）及语音识别（Speech Recognition）并列为机器学习方向的三大热点方向。而计算机视觉也由诸如梯度方向直方图（Histogram of Gradient， HOG）以及尺度不变特征变换（Scale-Invariant Feature Transform， SIFT）等传统的手办特征（Hand-Crafted Feature）与浅层模型的组合逐渐转向了以卷积神经网络（Convolutional Neural Network， CNN）为代表的深度学习模型。计算机视觉的理念其实与很多概念有部分重叠，包括：人工智能、数字图像处理、机器学习、深度学习、模式识别、概率图模型、科学计算以及一系列的数学计算等。\n二、计算机视觉里的一些应用方向（现在认识有限，以后遇到了再添加）：\n（一）、物体识别和检测：\n物体检测一直是计算机视觉中非常基础且重要的一个研究方向，物体识别和检测，顾名思义，即给定一张输入图片，算法能够自动找出图片中的常见物体，并将其所属类别及位置输出出来。当然也就衍生出了诸如人脸检测（Face Detection），车辆检测（Viechle Detection）等细分类的检测算法。\n（二）语义分割：\n图像语义分割（semantic segmentation），从字面意思上理解就是让计算机根据图像的语义来进行分割，语义在语音识别中指的是语音的意思，在图像领域，语义指的是图像的内容，对图片意思的理解。\n目前语义分割的应用领域主要有：地理信息系统、无人车驾驶、医疗影像分析、机器人等领域，详细见：计算机视觉之语义分割\n（三）运动和跟踪：\n跟踪也属于计算机视觉领域内的基础问题之一，在近年来也得到了非常充足的发展，方法也由过去的非深度算法跨越向了深度学习算法，精度也越来越高，不过实时的深度学习跟踪算法精度一直难以提升，而精度非常高的跟踪算法的速度又十分之慢，因此在实际应用中也很难派上用场。 视觉跟踪是指对图像序列中的运动目标进行检测、提取、识别和跟踪，获得运动目标的运动参数，如位置、速度、加速度和运动轨迹等，从而进行下一步的处理与分析，实现对运动目标的行为理解，以完成更高一级的检测任务。跟踪算法需要从视频中去寻找到被跟踪物体的位置，并适应各类光照变换，运动模糊以及表观的变化等。但实际上跟踪是一个不适定问题（ill posed problem），比如跟踪一辆车，如果从车的尾部开始跟踪，若是车辆在行进过程中表观发生了非常大的变化，如旋转了180度变成了侧面，那么现有的跟踪算法很大的可能性是跟踪不到的，因为它们的模型大多基于第一帧的学习，虽然在随后的跟踪过程中也会更新，但受限于训练样本过少，所以难以得到一个良好的跟踪模型，在被跟踪物体的表观发生巨大变化时，就难以适应了。所以，就目前而言，跟踪算不上是计算机视觉内特别热门的一个研究方向，很多算法都改进自检测或识别算法。\n\n（四）视觉问答：\n视觉问答也简称VQA（Visual Question Answering），是近年来非常热门的一个方向，一般来说，VQA系统需要将图片和问题作为输入，结合这两部分信息，产生一条人类语言作为输出。针对一张特定的图片，如果想要机器以自然语言处理（NLP）来回答关于该图片的某一个特定问题，我们需要让机器对图片的内容、问题的含义和意图以及相关的常识有一定的理解。就其本性而言，这是一个多学科研究问题。\n（五）三维重建：\n基于视觉的三维重建，指的是通过摄像机获取场景物体的数据图像，并对此图像进行分析处理，再结合计算机视觉知识推导出现实环境中物体的三维信息。三维重建技术的重点在于如何获取目标场景或物体的深度信息。在景物深度信息已知的条件下，只需要经过点云数据[4]的配准及融合，即可实现景物的三维重建。基于三维重建模型的深层次应用研究也可以随即展开。学习图像处理的人会接触到更广泛更多元的技术，而三维重建背景的会非常专注于细分的算法，因为三维重建本身还有更细分的技术，所以在做研究生阶段的学习的时候，会有很具体的专业方向，比如说就是做航拍地形的三维重建，或者是佛像的三维重建，这里面因为场景的区别运用到的拍摄技术和重建技术都是不一样的，而且有一些不同技术之间也没有关系（当然三维重建本身的概念是相同的）。关于三维重建未来的热点和难度，这个领域可以做的很专，场景也有很多，每个场景都有不同的挑战，深入的我也不懂就不说了。\n三、图像处理和计算机视觉的分类：\n按照当前流行的分类方法，可以分为以下三部分：\nA.图像处理：对输入的图像做某种变换，输出仍然是图像，基本不涉及或者很少涉及图像内容的分析。比较典型的有图像变换，图像增强，图像去噪，图像压      缩，图像恢复，二值图像处理等等。基于阈值的图像分割也属于图像处理的范畴。一般处理的是单幅图像。\nB.图像分析：对图像的内容进行分析，提取有意义的特征，以便于后续的处理。处理的仍然是单幅图像。\nC.计算机视觉：对图像分析得到的特征进行分析，提取场景的语义表示，让计算机具有人眼和人脑的能力。这时处理的是多幅图像或者序列图像，当然也包括部分单幅图像。\n关于图像处理，图像分析和计算机视觉的划分并没有一个很统一的标准。一般的来说，图像处理的书籍总会或多或少的介绍一些图像分析和计算机视觉的知识，比如冈萨雷斯的数字图像处理。而计算机视觉的书籍基本上都会包括图像处理和图像分析，只是不会介绍的太详细。其实图像处理，图像分析和计算机视觉都可以纳入到计算机视觉的范畴：图像处理->低层视觉（low level vision），图像分析->中间层视觉（middle level vision），计算机视觉->高层视觉（high level vision）。这是一般的计算机视觉或者机器视觉的划分方法。在本文中，仍然按照传统的方法把这个领域划分为图像处理，图像分析和计算机视觉。\n四、图像处理与计算机视觉涉及的知识和相关的书籍（太深入的就不说了，这里只浅显的介绍一下（欢迎批评指正~））：\n（一）、数学知识：\n我们所说的图像处理实际上就是数字图像处理，是把真实世界中的连续三维随机信号投影到传感器的二维平面上，采样并量化后得到二维矩阵。数字图像处理就是二维矩阵的处理，而从二维图像中恢复出三维场景就是计算机视觉的主要任务之一。这里面就涉及到了图像处理所涉及到的三个重要属性：连续性，二维矩阵，随机性。所对应的数学知识是高等数学（微积分），线性代数（矩阵论），概率论和随机过程。这三门课也是考研数学的三个组成部分，构成了图像处理和计算机视觉最基础的数学基础。如果想要更进一步，就要到网上搜搜林达华推荐的数学书目了。\nCV是一个涉及面非常广的学科，目前主流的依据视觉的学习，涉及到概率统计，各类优化方法，图论；一些研究方向（比如涉及到物体运动的）还会涉及拓扑学，群论，矩阵优化；一些图像分割算法，比如level-set，会涉及到微分方程等等。这些也都不是绝对区分的，现在的state-of-art的问题各方面可能都会有所涉及，依据问题本身而已。涉及面太广但是计算机的研究大多只是涉及，并不一定需要像数学系那样严密的推导。\n（二）、信号处理\n图像处理其实就是二维和三维信号处理，而处理的信号又有一定的随机性，因此经典信号处理和随机信号处理都是图像处理和计算机视觉中必备的理论基础。\n2.1经典信号处理\n信号与系统(第2版) Alan V.Oppenheim等著 刘树棠译\n离散时间信号处理(第2版) A.V.奥本海姆等著 刘树棠译\n数字信号处理:理论算法与实现 胡广书 (编者)\n2.2随机信号处理\n现代信号处理 张贤达著\n统计信号处理基础:估计与检测理论 Steven M.Kay等著 罗鹏飞等译\n自适应滤波器原理(第4版) Simon Haykin著 郑宝玉等译\n2.3 小波变换\n信号处理的小波导引:稀疏方法(原书第3版) tephane Malla著, 戴道清等译\n2.4 信息论\n信息论基础(原书第2版) Thomas M.Cover等著 阮吉寿等译\n（三）、模式识别\nPattern Recognition and Machine Learning Bishop, Christopher M. Springer\n模式识别(英文版)(第4版) 西奥多里德斯著\nPattern Classification (2nd Edition) Richard O. Duda等著\nStatistical Pattern Recognition, 3rd Edition Andrew R. Webb等著\n模式识别(第3版) 张学工著\n（四）、 图像处理与计算机视觉的书籍推荐\n图像处理，分析与机器视觉 第三版 Sonka等著 艾海舟等译\nImage Processing, Analysis and Machine Vision\n( 附：这本书是图像处理与计算机视觉里面比较全的一本书了，几乎涵盖了图像视觉领域的各个方面。中文版的个人感觉也还可以，值得一看。)\n数字图像处理 第三版 冈萨雷斯等著\nDigital Image Processing\n(附：数字图像处理永远的经典，现在已经出到了第三版，相当给力。我的导师曾经说过，这本书写的很优美，对写英文论文也很有帮助，建议购买英文版的。)\n计算机视觉：理论与算法 Richard Szeliski著\nComputer Vision: Theory and Algorithm\n(附：微软的Szeliski写的一本最新的计算机视觉著作。内容非常丰富，尤其包括了作者的研究兴趣，比如一般的书里面都没有的Image Stitching和                       Image Matting等。这也从另一个侧面说明这本书的通用性不如Sonka的那本。不过作者开放了这本书的电子版，可以有选择性的阅读。\nhttp://szeliski.org/Book/\nMultiple View Geometry in Computer Vision 第二版Harley等著\n引用达一万多次的经典书籍了。第二版到处都有电子版的。第一版曾出过中文版的，后来绝版了。网上也可以找到中英文版的电子版。)\n计算机视觉：一种现代方法 DA Forsyth等著\nComputer Vision: A Modern Approach\nMIT的经典教材。虽然已经过去十年了，还是值得一读。期待第二版\nMachine vision: theory, algorithms, practicalities 第三版 Davies著\n(附：为数不多的英国人写的书，偏向于工业应用。)\n数字图像处理 第四版 Pratt著\nDigital Image Processing\n(附：写作风格独树一帜，也是图像处理领域很不错的一本书。网上也可以找到非常清晰的电子版。)\n（五）、小结\n罗嗦了这么多，实际上就是几个建议：\n（1）基础书千万不可以扔，也不能低价处理给同学或者师弟师妹。不然到时候还得一本本从书店再买回来的。钱是一方面的问题，对着全新的书看完全没有看自己当年上过的课本有感觉。\n（2）遇到有相关的课，果断选修或者蹭之，比如随机过程，小波分析，模式识别，机器学习，数据挖掘，现代信号处理甚至泛函。多一些理论积累对将来科研和工作都有好处。\n（3）资金允许的话可以多囤一些经典的书，有的时候从牙缝里面省一点都可以买一本好书。不过千万不要像我一样只囤不看。\n五、图像处理绕不开的工具--OpenCV：\nOpenCV的全称，是Open source Computer Vision Library,开放源代码计算机视觉库。也就是说，它是一套关于计算机视觉的开放源代码的API函数库。这也就意味着，(1)不管是科学研究，还是商业应用，都可以利用它来作开发;(2)所有API函数的源代码都是公开的，你可以看到其内部实现的程序步骤；(3)你可以修改OpenCV的源代码，编译生成你需要的特定API函数。但是，作为一个库，它所提供的，仅仅是一些常用的，经典的，大众化的算法的API。一个典型的计算机视觉算法，应该包含以下一些步骤：(1)数据获取（对OpenCV来说，就是图片）；(2)预处理;(3)特征提取;(4)特征选择;(5)分类器设计与训练;(6)分类判别;而OpenCV对这六个部分，分别（记住这个词）提供了API。\n你可以将它理解为幼儿园小朋友过家家玩的积木，而OpenCV中的函数，则可以理解为一个一个的积木块，利用所有或者部分积木块，你可以快速的搭建起来具体的计算机视觉方面的应用（比如，字符识别，车牌识别，遗留物检测）。想必你也已经发现，在利用OpenCV这个积木来搭建具体的计算机视觉应用的时候，真正核心的，应该是这些积木块，如果你明白了积木块的工作原理，那么，是不是就可以不用这些积木块了呢？完全正确！不过，一般部分情况下，我们不需要这么做，因为，OpenCV已经帮你做好了一些工作（已经帮你做好了一些积木块，直接拿来用就是了）。但是，诸如前面提到的特征提取模块，很多情况下，OpenCV就无能为力了。这个时候，你就需要翻阅计算机视觉、模式识别、机器学习领域顶级会议、期刊、杂志上面发表的文章了。然后，根据这些文章中阐述的原理和方法，来编程实现你要的东西。实际上，也就等于搭建一个属于你私有的积木块。其实，OpenCV中的每一个API函数，也就是这么来的。\n如今，来自世界各地的各大公司、科研机构的研究人员，共同维护支持着opencv的开源库开发。这些公司和机构包括：微软，IBM，索尼、西门子、google、intel、斯坦福、MIT、CMU、剑桥........\n六、结语：\n随着深度学习的大举侵入，现在几乎所有人工智能方向的研究论文几乎都被深度学习占领了，传统方法已经很难见到了。有时候在深度网络上改进一个非常小的地方，就可以发一篇还不错的论文。并且，随着深度学习的发展，很多领域的现有数据集内的记录都在不断刷新，已经向人类记录步步紧逼，有的方面甚至已经超越了人类的识别能力。\n目前来看计算机视觉的研究处在一个非常好的时期，有很多我们原来解不了的问题现在能够解得比较好了，像人脸识别，尽管我们其实还没有从真正意义上达到人类视觉系统对人脸识别的鲁棒程度。但我们离真正让计算机能够像人看和感知这个世界还有很远的距离。在我们达到这个目标之前，深度学习的方法可能是这个过程中一个重要的垫脚石，同时我们还要将更多的新的方法和工具带入这个领域来进一步推动这个领域的发展。\n人的精力是有限的，这就意味着我们不可能把很多事情同时做好，所以在你选好方向之后，就要把我们的精力集中在你感兴趣的一个问题上， 努力成为这个方面的专家。研究是一项长跑，很多时候，我们在一个方向上比别人坚持久一点， 就有机会超越他而成为某个方面的专家。\n参考文档：\nhttps://www.zhihu.com/question/26836846\nhttps://blog.csdn.net/carson2005/article/details/6979806\nhttps://blog.csdn.net/wangss9566/article/details/54618507\nhttps://blog.csdn.net/qq_26499769/article/details/78989088\nhttp://blog.csdn.net/dcraw\n附：计算机视觉领域的一些牛人博客，超有实力的研究机构等的网站链接"}
{"content2":"计算机视觉会议\nA类\nICCV: International Conference on Computer Vision，两年一届\nCVPR: International Conference on Computer Vision and Pattern Recognition， 一年一届\nB类\nECCV: European Conference on Computer Vision，两年一届\nC类\nACCV: Asian Conference on Computer Vision\nICPR: International Conference on Pattern Recognition\nBMVC: British Machine Vision Conference\n其他\nSSVM,NIPS等\n计算机视觉刊物\nA类\nTPAMI: IEEE Trans on Pattern Analysis and Machine Intelligence\nIJCV: International Journal of Computer Vision\nSIAM Journal image sciences\nB类\nCVIU: Computer Vision and Image UnderstandingPattern Recognition\nC类\nIET-CVI: IET Computer Vision\nIVC: Image and Vision Computing\nIJPRAI: International Journal of Pattern Recognition and Artificial Intelligence\nMachine Vision and Applications\nPRL: Pattern Recognition Letters\n其他\nTIP，Civil等\n作者：sjming\n链接：https://www.zhihu.com/question/37687006/answer/188681083\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n作者：sjming\n链接：https://www.zhihu.com/question/37687006/answer/188681083\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}
{"content2":"﻿﻿\n开源生物特征识别库 OpenBR\nOpenBR 是一个用来从照片中识别人脸的工具。还支持推算性别与年龄。 使用方法：$ br -algorithm FaceRecognition -compare me.jpg you.jpg更多OpenBR信息\n最近更新： OpenBR —— 开源的生物识别工具 发布于 13天前\n计算机视觉库 OpenCV\nOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...更多OpenCV信息\n最近更新： OpenCV 2.4.5 发布，开源计算机视觉库 发布于 2个月前\n人脸识别 faceservice.cgi\nfaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。更多faceservice.cgi信息\nJava视觉处理库 JavaCV\nJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...更多JavaCV信息\n视频监控系统 OpenVSS\nOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。更多OpenVSS信息\nOpenCV的.NET版 OpenCVDotNet\nOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。更多OpenCVDotNet信息\n人脸检测算法 jViolajones\njViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033更多jViolajones信息\n手势识别 hand-gesture-detection\n手势识别，用OpenCV实现更多hand-gesture-detection信息\n人脸检测与跟踪库 asmlibrary\nActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。更多asmlibrary信息\n开放模式识别项目 OpenPR\nPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。更多OpenPR信息\n运动检测程序 QMotion\nQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。更多QMotion信息\n图像特征提取 cvBlob\ncvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.更多cvBlob信息\nOpenCV的.Net封装 OpenCVSharp\nOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。更多OpenCVSharp信息\n人脸检测识别 mcvai-tracking\n提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...更多mcvai-tracking信息\n视频捕获 API VideoMan\nVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。更多VideoMan信息\n基于QT的计算机视觉库 QVision\n基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。更多QVision信息\n开源视线跟踪软件 ITU Gaze Tracker\n哥本哈根大学开源视线跟踪软件 The ITU Gaze Tracker is an open-source eye tracker that aims to provide a low-cost alternative to commercial gaze tracking systems and to make this technology more accessible. It is developed by the Gaze Grou...更多ITU Gaze Tracker信息\n图像处理和计算机视觉常用算法库 LTI-Lib\nLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具更多LTI-Lib信息\n实时图像/视频处理滤波开发包 GShow\nGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...更多GShow信息\nC++计算机视觉库 Integrating Vision Toolkit\nIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV\nOpenCV的Python封装 pyopencv\nOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...更多pyopencv信息\n模式识别和视觉库 RAVL\nRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。更多RAVL信息\nOpenSURF\n利用OpenCV和C++编写的SURF算法，作者Christopher Evans是首个利用OpenCV和C++结合的方法实现SURF算法。更多OpenSURF信息\n人脸识别库 rpflex\nrpflex 是一个 Flex 开发的库，用来识别照片中的人脸、眼镜和脖子。更多rpflex信息\nOpenCV优化 opencv-dsp-acceleration\n优化了OpenCV库在DSP上的速度。更多opencv-dsp-acceleration信息\nJava 计算机视觉库 BoofCV\nBoofCV 是一个 Java 的全新实时的计算机视觉库，BoofCV 易于使用而且具有非常高的性能。它提供了一系列从低层次的图像处理、小波去噪功能以及更高层次的三维几何视野。使用 BSD 许可证可在商业应用中使用。 这里有篇英文文章用来介绍 BoofCV 的使用。...更多BoofCV信息\n计算机视觉库 SimpleCV\nSimpleCV 将很多强大的开源计算机视觉库包含在一个便捷的Python包中。使用SimpleCV，你可以在统一的框架下使用高级算法，例如特征检测、滤波和模式识别。使用者不用清楚一些细节，比如图像比特深度、文件格式、颜色空间、缓冲区管理、特征值还有矩阵和图像...更多SimpleCV信息\n3D视觉库 fvision2010\n基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...更多fvision2010信息\n视觉快速开发平台 qcv\n计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。更多qcv信息\n计算机视觉算法 OpenVIDIA\nOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API'...更多OpenVIDIA信息\nC++计算机视觉库 ICL\nICL (Image Component Library) 是一种新型的C + +计算机视觉库，由比勒费尔德大学神经信息学组和CITEC开发。它兼顾了性能和用户友好性。 ICL提供了一个易于使用的类和函数的集合，可以开发复杂的计算机视觉应用。 在不到15行的C + +代码（见例子）可以写成...更多ICL信息\nMatlab计算机视觉包 mVision\nMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。更多mVision信息\nLua视觉开发库 libecv\nECV 是 lua 的计算机视觉开发库(目前只提供linux支持)更多libecv信息\nOpenCV的扩展库 ImageNets\nImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。更多ImageNets信息\n图像捕获 libv4l2cam\n对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出更多libv4l2cam信息\n高斯模型点集配准算法 gmmreg\n实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...更多gmmreg信息\nScilab的计算机视觉库 SIP\nSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。更多SIP信息\n计算机视觉和机器人技术的工具包 EGT\nThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...更多EGT信息\n计算机视觉库 BazAR\nBazAR 是基于特征点检测和匹配的计算机视觉库。 它能够快速检测和匹配图像中的已知物体，并且能够用于增强现实，它是计算机视觉研究的先进成果。更多BazAR信息\n计算机视觉库 VLFeat\n一个开源的计算机视觉库，实现了 SIFT,MSER, k-means, hierarchical k-means, agglomerative information bottleneck, quick shift等算法。由C语言编写,提供MATLAB接口，文档详细。支持跨平台。...更多VLFeat信息\nSTAIR Vision Library\nSTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。更多STAIR Vision Library信息\nScilab Image Processing Toolbox\nSIP 提供了图像处理、模式识别以及计算机视觉处理。 SIP is able to read/write images in almost 90 major formats, including JPEG, PNG, BMP, GIF, FITS, and TIFF. It includes routines for filtering, segmentation, edge detection, morphology, cu...更多Scilab Image Processing Toolbox信息\n3D计算机视觉库 openvis3d\n这个项目的目的是提供一个高效的3D计算机视觉库，用于图像和视频处理。它包括深度立体匹配、光流（运动）估计、遮挡检测和运动平台估计更多openvis3d信息\nlibvideogfx\n视频处理、计算机视觉和计算机图形学的快速开发库。更多libvideogfx信息\ngo-opencv\nGo-OpenCV 是 Go 语言版的 OpenCV 封装。更多go-opencv信息\nJavaScript图形绘制库 Toxiclibs.js\nToxiclibs.js 是一个开源的计算机图形设计库，无需外部依赖，使用 <canvas> 元素进行图形绘制。更多Toxiclibs.js信息\nOpenCL 封装库 CLOGS\nCLOGS 是 OpenCL C++ API 的高级封装库，其设计目的是集成其他 OpenCL 代码，包括同步 OpenCL 事件，当前支持两个操作：基数排序和独立扫描。更多CLOGS信息\n最近更新： CLOGS 1.2.0 发布，OpenCL 的封装库 发布于 2个月前\nopenvgr\nOpenVGR 包含以下几个实时处理模块 (基于 OpenRTM-1.0): 立体相机采集 (对于 IEEE 1394b 相机), 立体图像浏览器, 3-D 点云重建 (使用 OpenCV),  基于边缘的 3-D 物体检测 包含以下几个命令行工具: 模型建立, 多相机标定....更多openvgr信息\nsparse-stereo-vision\n使用 OpenCV 函数, 这个项目能从成对的立体图像中重建场景。更多sparse-stereo-vision信息\nPIV图形软件包 Fluere\nFluere是粒子图像测速（PIV）的图形软件包。 Fluere是高度优化的并行处理，并在多个平台上运行。该项目的目标是提供高质量的测速软件，采用PIV技术处理的最新进展的研究人员和教育工作者，而所使用的算法的完整的知识。更多Fluere信息\nstereoview\nstereoview 是一个立体可视化和标定工具更多stereoview信息"}
{"content2":"2016-11-24 视觉求索\n谈话人：\n杨志宏   视觉求索公众号编辑\n朱松纯   加州大学洛杉矶分校UCLA统计学和计算机科学教授\nSong-Chun Zhu\nwww.stat.ucla.edu/~sczhu\n时间: 2016年10月\n杨: 朱教授，你在计算机视觉领域耕耘20余年，获得很多奖项， 是很资深的研究人员。近年来你又涉足认知科学、机器人和人工智能。受 《视觉求索公众号》编辑部委托，我想与你探讨一下计算机视觉的起源，这个学科是什么时候创建的， 有哪些创始和代表人物。兼谈一下目前热门的人工智能。\n朱: 好， 我们首先谈一下为什么需要讨论这个问题。 然后， 再来探讨一下计算机视觉的三个重要人物David Marr， King-Sun Fu， Ulf Grenander以及他们的学术思想。我认为他们是这个领域的主要创始人、或者叫有重要贡献的奠基人物。\n第一节： 为什么要追溯计算机视觉的源头， 这有什么现实意义?\n中国有句很有名的话：“一个民族如果忘记了历史,她也注定将失去未来。”  我认为这句话对一个学科来讲，同样发人深省。我们先来看看现实的状况吧。\n首先，假设你当前是一个刚刚进入计算机视觉领域的研究生，很快你会有一种错觉，觉得这个领域好像就是5年前诞生的。 跟踪最新发表的视觉的论文，很少有文章能够引用到5年之前的文献，大部分文献只是2-3年前的，甚至是1年之内的。现在的信息交换比较快，大家都在比一些 Benchmarks,把结果挂到arXiv 网上发布。 很少有一些认真的讨论追溯到10年前，20年前， 或30年前的一些论文，提及当时的一些思想和框架性的东西。现在大家都用同样的方法，只是比拼，你昨天是18.3%的记录（错误率），我今天搞到17.9%了。大家都相当短视，那么研究生毕业以后变成了博士，可能也会带学生做研究，他只知道这几年的历史和流行的方法的话，怎么可能去传承这个学科，让其长期健康发展呢？特别是等当前这一波方法退潮之后，这批人就慢慢失去了根基和源创力。这是一个客观的现象。\n其次，还有一个现象是，随着视觉与机器学习结合，再混合到人工智能的这么一个社会关注度很高的领域去以后，目前各种工业界，资本、投资界都往这里面来炒作。所以，你可以在互联网上看到各种推送的文字，什么这个大师，那个什么牛人、达人说得有声有色，一大堆封号。中国是有出“大师”的肥沃的土壤的，特别是在这个万众创新、浮躁的年代。 这些文字在混淆公众的视听。也有的是一些中国的研究人员、研究生， 半懂不懂，写出来一些， 某某梳理机器学习、神经网络和人工智能的历史大事。说得神乎其神。我的大学同学把这种帖子转发给我，让我担忧。\n杨：这大多是以学术的名义写的软文，看起来像学术文章，实际上就是带广告性质的，一般都是说创投、创业公司里的人，带着资本的目的，带商业推广性质的。\n朱: 我甚至不排除有些教授，比如与硅谷结合很紧密的、在IT公司或者风投公司兼职的，有意识地参与、引领这种炒作。\n这对我们的年轻学生其实是很致命的，因为他们不了解这背后的动机， 缺乏免疫力。而且现在年轻人和公众都依赖短平快的社交媒体，很少去读专业文献。当公众的思想被这些文字占领了，得出错误的社会性的共识，变成了 false common sense， 对整个社会， 甚至对学术界，都会产生长久的负面冲击。\n这就形成了新时代的皇帝的新装。我们需要对这种现象发声， 做一些严肃的探讨。所以，正本清源有着重要的现实意义。\n第二节：计算机视觉和人工智能、机器学习的关系\n杨：谈到这里，我想先问一下计算机视觉和人工智能是什么关系？还有机器学习这三个东西。\n朱：人工智能是在60年代中后期起步的。一直到80年代，翻开它的教科书，就是一些启发式搜索，研究最多的是下棋， 从国际象棋一直到最近的围棋，都是比较抽象的表达。棋盘的位置是有限的、下棋的动作也是有限的， 没有感知和动作执行的不确定性。 所有的问题都变成一个图搜索的问题，教科书上甚至出现了一个通用图搜索算法号称可以解决任何人工智能问题。当时视觉问题还没引起大家重视。我这里有一份1966  年7月 的  MIT AI 实验室的第100号报告（备忘录memo 100），很短，题目叫做“The Summer Vision Project”。这个备忘录的基本意思就是暑假的时候找几个学生构造一个视觉系统。他们当时可能就觉得这个问题基本上是不需要做什么研究的。所以你就一个暑假，几个人一起写个程序，就把它干掉算了。现在说起来，当然是个笑话。\n人的大脑皮层的活动， 大约70%是在处理视觉相关信息。视觉就相当于人脑的大门，其它如听觉、触觉、味觉那都是带宽较窄的通道。视觉相当于八车道的高速， 其它感觉是两旁的人行道。如果不能处理视觉信息的话，整个人工智能系统是个空架子，只能做符号推理，比如下棋、定理证明， 没法进入现实世界。所以你刚才问到的人工智能和计算机视觉的关系，视觉，它相当于说芝麻开门。大门就在这里面，这个门打不开, 就没法研究真实世界的人工智能。\n到80年代，人工智能， 连带机器人研究就跌入了低谷， 所谓的冬天。那个时候，很多实验室都改名字了， 因为拿不到经费了。 客观来说，80年代， 一个微型计算机的它的内存只有640K字节，还不到一兆（1MB一百万字节），我们现在一张图像，随便就是几个兆的大小，它根本无法读入一张图像，还谈什么理解呢？等到我做博士论文的时候（1992-1996），我导师把当时哈佛机器人实验室最好的SUN工作站给我用，也就是32兆字节。我们实验室花了25万美元构建了一个图像采集系统，因为当时没有数字照相机。可以这么说，一直到90年代中期的时候，我们基本上不具备研究视觉这个问题的硬件条件和数据基础。只能用一些特征点的对应关系做射影几何，用一些线条做形状分析。因为图像做不了，所以80年代计算机视觉的研究，很大部分是做几何。\n杨：90 年代后，就是数字照相机大量生产了。\n朱：在90年代的末期的时候，发生了一个叫做感知器的革命。带动了大数据和机器学习的蓬勃发展。\n杨：那机器学习与计算机视觉的关系呢？\n朱：计算机视觉是一个domain， 它有很多问题要研究， 就像物理学。 而机器学习基本是一个方法和工具，就像数学和统计学。 这个名词的兴起应该还是最近的事情， 在我看来，是来自于两股人马。 80年代人工智能走入低谷后，迎来了人工神经网络的一个高潮， 所谓的从符号主义到连接主义的过渡。在中国80年代与气功、人体科学一起走红，但这基本是昙花一现。到了90年代初， 退潮之后，就开始搞 NIPS这个会议， 引入统计的方法来做。还有一股就是做模式识别的一些工程人员EECS 背景的。 按道理来说， 这个领域应该叫做 统计学习 （Statistical Learning），因为它的方法都是由概率统计领域拿来的。这些人中的领军人物很有商业头脑， 把统计和物理的数理模型， 改名叫做机器， 比如**模型（model）就叫**机（machine），把一些层次模型（hierarchical model）说成是“网”（net）。这样，搞出了几个“机”和“网”之后， 这个领域就有了地盘。另一方面，我的那些做统计的同事们也都老实、图个清静，不与他们去争论， 也大多无力去争。当然，统计学领域也有不少人参与了机器学习的浪潮。简单说，机器学习中的 “机器”就是统计模型，“学习”就是用数据来拟合模型。 是由做计算机的人抢占了统计人的理论和方法，然后，应用到视觉、语音语言等 domains。 我在计算机和统计两个系当教授， 看得一清二楚。 这个问题我以后可以专门讨论。\n这个机器学习的群体在2000年之后，加上大量数据的到来，很快就成长了， 商业上取得很大的成功。机器学习和计算机视觉大概有百分之六七十是重合的。顺便说一句，2019年我们两个领域会在一起在洛杉矶开CVPR 和 ICML年会， 我是CVPR19的大会主席。因为学习搞来搞去，最丰富的数据是在视觉（图像和视频）。现在这次机器学习的一些大的动作和工程上的推广工作，还是从计算机视觉这边开始的。\n杨：谢谢你讲述人工智能,计算机视觉和机器学习的关系。下面我们回到本次访谈的主题。刚才说了这个感知器革命是90年代以后，出了很多的数据要处理了。那么为什么马尔（Marr）在70年代末思考的问题，在面对我们当今处理这个数据的时候, 还有意义？就是说马尔用了什么方法？什么思路框架？使它有生命力？\n朱：好，就回到1975-1980年这个时间段。我们今天的主题是想初步探讨一下计算机视觉的起源。我们这个领域也没有一个统一的教科书来谈这个事情。我认为视觉的起源，可以追溯到三个人，David Marr, King-Sun Fu 和Ulf Grenander。这三个人代表三个完全不同的方面，为计算机视觉这个领域奠定了基础。\n杨：好， 我们逐个来介绍吧。\n第三节：视觉的开创者之一：David Marr 的学术思想\n朱： David Marr 【1945-1980】，中文音译为马尔， 他奠定了这个领域叫做Computational Vision计算视觉，这包含了两个领域： 一个就是计算机视觉（Computer Vision），一个是计算神经学（Computational Neuroscience）。他的工作对认知科学（CognitiveScience）也产生了很深远的影响。\n我们计算机视觉CV，第一届国际会议ICCV 1987年就以David Marr的名字来命名最佳论文奖， 而且一直到2007年之前的20年间， 是CV唯一的奖项和最高的荣誉，两年一次。认知科学年会 （CogSci）也设有一个 Marr Prize给最佳的学生论文。这三个领域在80-90年代走得很近， 最近十多年交叉越来越少了。就是说，原来都是亲戚，表兄弟， 现在很少有人在之间走动了。\nMarr 1972年从剑桥大学毕业，博士论文是从理论的角度研究大脑功能，具体来说，是研究的小脑， 主管运动的Cerebellum。1973年受MIT 人工智能实验室主任Minsky的邀请， 开始是做访问学者（博士后）。 1977年转为教职。 可是， 1978年冬诊断得了急性白血病。1980年转为正教授不久就去世了， 时年35岁。他在得知来日无多后，就赶紧整理了一本书，就叫 “Vision：A Computational Investigation into the HumanRepresentation and Processing of Visual Information”, 《视觉：从计算的视角研究人的视觉信息表达与处理》。他去世后由学生和同事修订，1982年出版。\n杨：“Vision”2010年再版了，再版了以后在亚马逊仍然是卖得很好。\n朱：它是个经典的东西。我是1989年冬天本科三年级从中科大认知科学实验室的老师那里，读到这本书的中文译本。因为缺乏背景知识，我当时基本读不懂。因为是中文，每句话都明白，但是一段话就不知道是什么意思了。在过去的20多年中， 我每隔1-2年都会再翻一翻这本书。后来我和同事花了大约8年时间，将他的一些思路转化成数理模型，比如primal sketch。\n杨：这个人生故事是可以拍电影的。\n朱：的确。 很多年前我与他的大弟子 Shimon Ullman饭桌上谈到这段历史， 他说当时大家到处找药，就是救不过来。当年这是一个30多岁正值科学顶峰的、交叉学科的领军人物。顺便说一句， 当年中日友好，1984播放日本电视剧《血疑》， 那是万人空巷， 感人至深。里面的大岛幸子（三口百惠饰）得的就是同样的病。\n可惜， 目前计算机视觉这个领域，你如果去问学生的话，他们很多人都没听说过David Marr。“喔，想起来了，好像有个Marr奖吧”。可是你去问认知科学、神经科学的人，他们基本上对Marr非常的清楚。这也是我所担心的， 计算机视觉的发展太工程化、功利化了，逐步脱离了科学的范畴。这是短视和危险的。最近又受到机器学习来的冲击。\n我这里顺便说一句， Marr 对我的另外一个间接的影响。他1973年来到MIT， 就租住在JayantShah的房子里， Shah 与 Minsky很熟， 他当时是研究代数几何（Algebraic geometry）的。 而我导师Mumford也是研究代数几何的， 并获得1974年的菲尔兹奖。他们两人很熟，后来在Shah的影响下，Mumford转入计算机视觉， 他们从提取物体边缘开始 （boundarydetection），也就是产生了著名的 Mumford-Shah 模型，搞图像处理的应用数学人员基本都是从这个模型开始做。这是后话。关于这段历史，我们以后可以展开谈。\n杨：好， 那么 Marr的学术贡献是什么呢？\n朱：在我看来，David Marr对我们这个学科最主要的贡献有三条。从而基本上可以说，定义了这个学科的格局。\n第一条，就是说在那个时代，60年代开始的时候大家已经很多人研究视觉神经生理学、心理学问题。也有人做一些边缘检测的工作。但是，视觉到底要解决哪些问题？是怎么实现的？大家莫衷一是，谈不清楚，那么David Marr的第一个贡献就是分出了三个层次。他说， 要解决这个问题，可以把它分成计算（其实应该说成是表达）、算法、和实现三层次。首先，在表达的层次，我们问一下这是个什么问题呢？如何把它写成一个数学问题。任务是什么？输出是什么？这是独立于解决问题的方法的。其次，对这个数学问题去求解时，可以选择不同的算法， 可以并行或者串行。再次，一个算法如何在硬件上实现，可以用CPU，DSP， 或者神经网络来实现。 很多观察到的心理学和神经科学的现象都是跟系统硬件有关的东西，比如说人的一些注意机制，记忆力。这些应该从表达层面剔除。这样， 视觉就可以从纯粹的理论、计算的角度来研究了。我们可以参考心理学和神经科学的结论， 但这不是主要的。 打个比方，要造飞机， 可以参考鸟类的结构， 但关键还是建立空气动力学，才能从根本上解释这个现象， 并创造各种飞行器， 走得更远。\n杨：他这么一说，今天看来好像很自然的可以理解了，但是在当时，可能没有多少人，是把问题这样分解的。\n朱：当时分不开。因为当时站在像神经科学和认知科学角度，是拿一些实验现象来说事，但是不知道这个现象是在哪一层出现的。\n比如神经网络和目前的深度神经网络的学习，他们的模型（表达）、算法、和实现的结构三层 是混在一起的。就变成一个特用的计算设备， 算法就是由这个结构来实现的。当它性能不好的时候，到底是因为表达不对，还是算法不对，还是实现不对？ 这个不好分析了，目前的神经网络，或者是机器学习，深度学习，它的本源存在这个问题。\n以前我们审稿的时候，会追问论文贡献是提出了一个新的模型？还是一个新的算法？在哪一个层级上你有贡献，必须说得清清楚楚。2012年，我作为国际计算机视觉和模式识别年会（CVPR）的大会主席， 就发生一个事件。收到神经网络和机器学习学派的一个领军人物 LeCun的抱怨信，他的论文报告了很好的实验结果， 但是审稿的三个人都认为论文说不清楚到底为什么有这个结果， 就拒稿。他一气之下就说再也不给CVPR投稿了，把审稿意见挂在网上以示抗议。2012 年是个转折点。\n现在呢？随着深度学习的红火， 这三层就又混在一块去了。 一般论文直接就报告结果， 一堆表格、曲线图。我就是这么做，然后再这么做，我在某些个数据集上提高了两个百分点，那就行了。你审稿人也别问我这个东西里面有什么贡献，哪个节点代表是什么意思，你别问，我也不知道。那算法收敛了吗？是全局收敛还是一个局部收敛？我也不知道，但是我就提高了两个百分点。\n杨：或者要用多少数据来训练材料才能够呢？\n朱：对，这个也不用管，而且说不清。反正我这个数据集就提高是吧？所以从这个角度来讲，它就很难是一个科学的方法。可以认为它就是一个工程或者是一个经验的，有点像中医。那么要往前再发展的时候，你必须要理清楚这三层的事情。\n杨：对。\n朱：那么他第二个贡献的话，是理清视觉到底要计算什么。Marr提出了一个系列的表达，从primal sketch（首要简约图）， 到2 ? D sketch（深度简约图）， 到3D sketch。 这里面还包含了纹理、立体视觉、运动分析、表面形状、等等。比如说我要估计一个物体的深度和形状，我就估计它的光照，和物理材料特性；还有，三维几何形状怎么去表达？ 他试图去建立一个完整的体系。\n现在的视觉就基本上被很多人错误地看成一个分类问题，你给我一张图像，我说这个图像里有一只狗或者没有狗，狗在哪儿都不知道。头在哪？脚在哪？不知道。Marr框架是有秩序的，现在的秩序在做深度学习的人眼中还不存在，或者没有忙过来。各人做各人的分类问题，比如说有人算这个动物分类，有的人算这个家具的分类。各种分类以后，他们之间怎么样的关系呢？要对这个图像或者场景要产生一个整体的语义解释。\n第三个贡献，Marr提出了一个非常重要的概念，到现在一直还没有一个完整的解答。他说，计算视觉是一个计算的“过程”。这是什么意思？ 我们以前用贝叶斯方法（以及现在的深度网络）认为视觉就是表达成为一个后验概率，寻求一个最优解。这个解就是图像的解释。这个求解过程就会终止。可是Marr说的这个事情，它不是单纯去求一个解，而是一个连续不断的计算过程。我给你一张图像，你越看、越琢磨，你可能看到的东西会越多。\n我给你一秒钟，你可能看到某些东西。我给你一分钟，你可能有另外一种理解，这两个理解可能是不一样的。还有一个重要的概念是你的任务决定了你怎么去看这个图像，比如说我在慌忙之中在做饭，那么我对这个场景，只看其中的很小一部分，足够来完成我的任务就行了。里面好多东西改变你根本没注意到。\n杨：好像有些魔术就利用了这一点。\n朱：就是， 很多心理学实验表明，你眼睛盯着这个图片看的时候，眼睛不眨，我告诉你这个图片在改变。你盯着看，结果它改了你都没看见。在让你看这个图片的时候，把你的注意力引到某个任务所需要计算的关键要素上，其它部分你就视而不见。视觉是受任务驱动的。而任务是时刻在改变之中。 比方说， 视觉求解不是打一个固定的靶子， 而是打一个运动目标。\n杨：这听起来是一个耳目一新的概念。\n朱：回到人工智能这个问题，视觉，它最后的用途，要给机器人用，机器人目前面临一个什么任务，来决定它要计算什么。这第三个贡献是在算法的层面。就是说我根据我们目前面临的任务，我才决定要计算什么。而且人的任务是在不断变化的，在此时此刻我任务都在变化，那么计算的过程中是没完没了地在改变。这个理念到目前，我们目前在研究这个事情，还没有完全实现。就是说，这将是人工智能和机器人视觉的一个关键。\n杨：明白。\n朱：我们现在很多人研究这个智能，比如说分类问题。他都是从谷歌的一些应用，比如搜索图片、广告投放，变成分类问题。 从而忽视了更大的本质问题。如果说人工智能往前发展机器人，要从机器人的角度来用视觉的话，那么它就有很多不同的任务。我现在做饭，我在打球，我在欣赏风景，这个时候我看到的东西是完全不一样的。我怎么样通过这千千万万的任务，而不是简单一个分类，来驱动我的计算的过程，来找到我的需求，来支持我目前的任务，这是一个巨大的研究的方向。David Marr的思想，到今天，反而意义非常重大，因为大家现在一窝蜂的去搞深度学习，把这些基本东西给忘掉了。但是这才是人工智能和机器人视觉的长远发展方向。\n我前两年给过几个谈话，说研究视觉要从一个agent（执行者）的角度，带着任务进来的这么一个人或机器人，主动地去激发视觉。\n目前的计算机视觉的研究还有一大部分是由视频监控的应用来驱动的，比如说我检测一些异常现象，看这个人是男还是女？那这也是一种被动的，就是说它只是在看，没有去做。要去做的话，就涉及到因果关系和更多的不确定性。所以现在的研究生觉得，他整天在做机器学习， 就在调参数，就在跟别人比拼百分之几的性能。 一些公司的研究所就报道， 他们在某某问题（数据集）上国际领先了，排名第一了。他们自己也觉得这个研究没多少意思。那是因为他们没有接触到这些基本的问题上来。\n杨：他们可能还没有发现这个问题本身是多么有趣。\n朱：因为作为一个科学来发展的话，那它就是要认认真真的来做，把这个理清楚。当前的火热来源于工业界， 工业界没有多少耐心资助他们的研究人员去做科学研究，大家很现实。 那么，David Marr先谈这么多好不好？以后我们可能还会继续深入谈的。\n杨：好。那我们第二个人就谈一下傅京孫。\n第四节：视觉的开创者之二：傅京孫（King-Sun Fu）的学术思想\n朱： David Marr是从这个神经科学和脑科学这个方向来的。傅京孫【1930-1985】，他当时代表的是计算机科学，搞人工智能的人。他是一个有领导才能的人物。他和其他人于1973年组织了第一届国际模式识别会议（ICPR），并担任主席。会议后来演变成国际模式识别学会IAPR，在1976年成立，并被选为其主席。他重组了另外一个IEEE学会下面的模式识别委员会，并于1974年成为其第一任主席，创办了IEEE模式分析和机器智能（PAMI）会刊，并于1978年担任第一任总编。这是目前计算机视觉和相关领域最权威的一本期刊了。很多中国学生现在不知道，这个领域的老大本来是华人。目前， 国际模式识别学会IAPR设立了一个傅京孫奖， 作为终身成就奖， 是模式识别的最高荣誉。\n杨：可惜他1985年去世了。听说去世前他每年都在中国举办讲座，并于1978年担任台湾的中央研究院院士。\n朱：我正要说的这一点。他去世的时候55岁，在普渡大学，据说他的实验室是一个Chinatown。1978年中国打开国门，中国最早的一批中科院的计算机人员都到他那里进修，在普渡。所以他对中国计算机的发展，可以说是一个贡献非常巨大的人。我也是受到他的恩惠，我大学一二年级就开始跟着科大陈国良老师学习，他之前去普度进修。周末我有时就到陈老师家听他讲外面的一些研究人员和工作。你想想，计算机界那时候华人在美国站住脚的可能没几个人。\n杨：对，他对中国计算机发展真的是有历史性的贡献的。我在科学院上研究生的时候，我们那些老师是说他过世太早了，要不然对中国的研究还会更好，他多活10来年就会好很多。\n朱：他1985年拿到一个很大的国家项目，好像是开宴会的时候心脏病突发了。 他要是活着，华人在这个领域的话，不止是现在这个样子。不过在他之后， 稍晚一点我们有另外一个杰出华人，黄煦涛（Tom Huang）。他当时也在普渡任教，培养了大量华人研究人员。 我们以后会专门介绍。\n杨：傅京孫的故事也可以拍电影。\n朱：这是我们这个领域的不幸，两个奠基人很快就走了。他们刚刚把这个地基打起来，人就没了。\n杨：那傅的主要贡献是什么呢？\n朱：傅京孫的贡献， 我也谈三点。第一个贡献应该就是对这个学科和学会的建设，以及工程师的培养上面，他起到了开创性的作用。一般公认他是模式识别的开山鼻祖，模式识别与计算机视觉分不开的。第二个作用，就是关于他的这个句法结构性的表达与计算，就是句法模式识别，Syntactic Pattern Recognition这个词，这个词其实非常深刻。他在走之前，他那个时候也没有多少数据，那么他只是画一些图，图表性的东西，来表达他的概念，他从计算机这边来的，你想很自然就会用到形式语言，因为计算机里面的几个基础之一是形式语言。逻辑、形式语言，对吧？\n杨：这好像是在编译原理里面学到过，因为编译的基础是形式语言。\n朱：我们这个世界的模式， 一个最基本的组织原则是composition。一张图像就像语言、句子符合语法结构， 视频中的一个事件也有语法结构。寻找一个层次化、结构化的解释是计算视觉的核心问题。从傅京孫1985年丢下来这个摊子后，基本很少有人去碰。差不多18年以后，我和我第一个博士生继续做图像解译Image Parsing这个方向，于2003年得了Marr马尔奖。然后我和我导师专门于2006年写了一本小书，总结了图像的随机语法。我刚才谈到了，在做识别，做分类的时候，只是单独在分类某一个东西，怎么去把各个识别器和分类器给它整合在一起，变成一个统一的表达？就必须产生一个结构上的表达。现在机器学习界把它换了另外名字，叫做结构化的输出，其实是一个东西。他们提出一个新的名词，把原创的图像解译名称覆盖住，这事现在经常发生。所以我说机器学习领域经常到别人那里偷概念，改头换面。数学界不允许这样做的。我还是坚持把它叫做解译、语法。\n因为语法，它就是一些规则，其实语法并不见得是一个确定性的，它可以跟统计连在一块，它也可以跟目前的一些神经网络结合，这个都没问题。它表达了一个骨架或者支柱，形成一个统一表达。\n第三点，从算法的角度来讲，有一个层次化的表达以后，意义就不一样了，比如自底向上或自顶向下的计算的过程就可以在上面体现出来，就是马尔说的计算的过程，就可以在这里面体现出来。视觉的计算过程应该是由大量的自底向上（bottom-up）和 自顶向下（top-down）过程交互和同时进行的。顺便再说一句，当前的深度神经网络就是一个feedforward的自底向上的计算， 缺乏自顶向下的过程。而在人脑计算中，自顶向下的计算占据很大一部分。\n杨：那就是说， 这个语法结构对计算过程有了规范和表达的途路。\n朱：对，你的搜索的过程，这个计算的过程是什么？马尔他提出了第二个概念，说视觉是个计算的过程，那么这个计算过程你什么时候算哪个，这是个调度的问题，就像操作系统。那么David Marr计算的过程，没完没了的，随着你的任务不断改变，那么它就有一个调度的问题。所以说我现在要去做饭，或者我要欣赏风景，或者说我要去走路，开车，那么它的不同的任务产生了不同的进程。这个进程，要在层次化的表达里面的统一起来调度。从这个意义看，感知是计算一个解译图（parse graph）， 认知是对这个parse graph进一步推理扩大， 而机器人的任务规划（task planning）也是一个同样结构的parse graph， 那就更别说语言是用parse graph来表达的。所以，人工智能的一个核心表达就是随机的语法和解译图。\n杨：对。\n朱：这个是绕不掉的，不管谁来做，都要做这个事情。当然，现在有人千方百计想绕过去，重新发明一套名词， 让新来的学生忘记历史， 这样他们就可以变成社会公认的大师。有些教授、研究人员在学术上没什么原创贡献， 却在网上、社会上成了当红明星, 学科代言人。用社会上的知名度再给学术界施压。\n总结一下，傅京孫三点主要贡献：一是学科的人才和组织基础，二是他提出这么一个的语法表达方法， 三是这个表达支撑了自底向上或自顶向下的计算的过程。他去世后， 这个方向一直处于一种休眠状态，我的研究有一条线是跟着这个方向做。2011年马里兰大学周少华他的导师有一个演讲，题目叫：语法模式识别--从傅到朱 （From Fu to Zhu）。我们在继承他的框架往前走。\n杨：真好！那么咱们下面就谈第三个人Ulf Grenander。\n朱：这个人的话，知道的人非常少。\n杨：我翻看了网上资料，他是这个领域里头真正的是大神了，但绝对是个小众人物。\n第五节：视觉的开创者之三：Ulf Grenander的学术思想\n朱：Ulf Grenander 【1923-2016】是很少有人知道的。感觉有点像金庸小说《天龙八部》里的在藏经阁扫地的灰衣老僧。武功和思想都出神入化，但是，他基本是世外高人，不参与江湖争斗， 金庸也没有交代他的名字。所以江湖上的人大多没听说过他。 这样也好， 他自自在在活了93岁， 今年刚刚去世的。国际应用数学季刊邀请我和其他人写纪念文章，正准备出版专刊呢。\n杨：对，我读他的生平，他这个人简直就是把欧洲美洲的，还有俄国的所有的精华的人物都接触过。\n朱：那是，他出身在瑞典，他的导师叫Harald Cramér。概率论里面的一个重要的定理，还有数论里的一个猜想是用他命名的。然后，他也跟 Bohr（波尔），Kolmogorov（科尔莫戈罗夫）他们走得比较近。他的起点就是做概率统计， 时间序列， 随机过程，因为你现在想概率论和统计学的一些重要应用，就是那个时候发力了。\n杨：从保险业开始了，北欧那边因为航海，保险业非常发达，所以这也有点道理。\n朱：关于概率和统计学对于科学、视觉、以及人工智能的重要意义， Mumford 1999年写了一篇论文，是在一个大会的发言，叫做《随机性时代的曙光》（Dawning of the Age of Stochasticity）。\n杨：对，那是你们老师写的， 网上能找到。\n朱：他总结说，过去两千多年的西方科学的发展是建立在亚里士多德以来的数理逻辑基础之上的。但是，后面一千年包括人工智能、人的思维这些东西是随机性过程。人的思维应该是建立在概率推理基础之上。其实， 我们看到现在的机器学习， 人工智能完全就是从这个方向走了。\n杨：你的导师说，整个世界的数学可以用概率的这套思想重新写一遍，就像罗素和怀特海的写这个数学原理似的，可以把数学重新建立起来，用概率的这种思想。\n朱：这个工作已经有人做了。E. T. Jaynes就是发明最大熵原理的那个人，他写了一本很厚的书，《Probability Theory: The Logic of Science》， 他就是用这个原理去写。这也是一篇遗作。他没写完就过世了。这也是以后可以谈的话题。\n朱： Ulf Grenander就诞生在这么一个概率发源的中心的地带，跟几个大师学习，博士毕业后出来游历，做概率论随机过程的这些东西。到六、七十年代的时候，他就开始提出来，想用数学来把这个模式识别与智能的现象的问题定义清楚。我们前面谈到的David Marr 是从神经科学、认知科学来的。傅京孫是一个计算机科学与工程的人。这两者基本没有多少严格的数学定义，提出的框架是漂浮的。Ulf是从数学的角度，奠定基础。他提出来一个应用数学的分支， 叫做 Pattern Theory。他的出发点完全不同， 就是要给世界上的各种模式、现象， 建立一个数学的框架来研究。 格局就很宏伟。而不是急于去解决某种实际问题， 后者叫做模式识别 （pattern recognition）。 他在90岁高龄出版了最后一本书， 想用数学来研究人的思想是从哪里来的。 你看我们脑袋里的念头、主意也往往是随机产生，像冒泡一样， 所谓思如泉涌。到底怎么来的？\n杨：那太了不起了。这个事说起来，我想到当时我的老师是让我读Geman and Geman 1984年的吉布斯采样算法，那就已经了不起了。\n朱：Grenander最后落脚在布朗大学应用数学系，Geman是他当年（70年代末80年代初）招到组里的年轻教员之一。这个吉布斯采样（Gibbs Sampler）的算法是一个里程碑的东西，在80年代初引起轰动。但那只是这个学派的诸多贡献的一个片段。\nGrenander的理论解释起来的确有点费劲，既然谈历史，我先从我个人的经历谈一下。\n他1994年出了一部总结性的书，900多页，叫做《General Pattern Theory》，广义模式理论。有点爱因斯坦做广义相对论的意思。但这本书很抽象， 没多少人读。我1995年在哈佛研究纹理模型（texture models），因为我用的学习算法就是吉布斯采样，在训练的时候，跑一遍要等两个星期才收敛，机器被占了，我就有时间，也是耐着性子把这本书读完了。我估计世界上不超过20人，能有耐心完整地读他的书。然后，我1996年1月答辩论文，我导师和我每周开车去布朗大学参加讨论。波士顿的冬天很冷， 哈佛到布朗1个小时左右，漫天大雪， 我们有时在高速上车被陷住， 下来铲雪。到了6月， 我导师从哈佛提前退休，带着我一起加入布朗的应用数学系。那在当时是一个学术思想的中心。组会里有Grenander，Mumford， Geman 还有其他20来人， 一坐就是2个多小时。这些人都明察秋毫， 做报告的人无法含混过去的， 一步一步都必须理清楚，说不清楚你就下去想， 下次再来。\n我一直认为计算机视觉和模式识别领域亏欠Grenander, 因为统计建模和随机计算逐渐成为我们领域的核心理论基础，而大家并不知道，很多思想、算法都源于这个人或者他的学派。所以，2012年， 我主持CVPR（国际计算机视觉和模式识别）大会， 特意放到布朗大学附近召开，我和另外两个主席一说，大家立即就同意了。并特制了一个银质的大奖章， 在大会上颁给他，表达我们的敬意。这里发生很多故事，我们以后再谈吧。\n杨：那你能简短总结一下Grenander对计算机视觉、甚至人工智能的主要贡献吗。\n朱：还是谈三点主要的吧。 首先，他提出了一个思想， 叫做 analysis-by-synthesis， 这是所谓 产生式建模的核心理念。当你要去识别、分析一个模式，比如一个动物，人脸， 一个事件， 你首先要建立一个数理模型， 这个模型通过数据来拟合， 也就是当前的机器学习。 那么， 判断这个模型好坏， 或者模型是否充分，的一个依据是什么呢？产生式建模的方法就是对这个模型随机抽样，也就是，合成（synthesis）。 我把这个过程直观叫做“计算机之梦”。计算机模型一开始初始化为空（完全随机）， 那它做的梦就是白噪声， 或者一张白纸。通俗来说， 这个模型就是一个“白痴”。人脑有这个功能，我们把眼睛一闭，没有外界输入了，就能做梦， 白日梦就是想象力的体现。一个好的模型采样产生的图片（模式）， 与真实观察的图片（模式）， 就应该是真假难辨。如果你能分辨，那说明这个模型不到位。  现在很多机器学习的方法是没法去随机合成图片的。  举个例子来说，我要检验你是不是真的听懂和理解中文，就看你能不能说流利的中文。如果你说话语法有错，词汇量不够，或者有口音，那就揭示你在哪方面还需要提高。\n杨：这个要求好像比光是听懂 要更严格。\n朱：的确。我们当年考英语， 多半是读，说和写都不行。我们考TOEFL， GRE Verbal的时候， 就算没搞懂， 也能蒙个60%-70%。 新东方的题海战术也很奏效。当你做了大量考题， 就算不懂， 也能考好。当前大数据、机器学习就用题海战术。 这个方法强调在实战中检验，考什么就拼命复习什么，不考的东西就不学，这也很有道理，很直接， 来得快。 但是， 因为你的模型没有真正理解， 没有“真懂”，考试大纲外面的东西更不懂， 那么后遗症就是， 遇到新考题， 缺乏泛化能力，遇到新问题，缺乏创造力。\n想一想， 如果我的学生一步步考试都是靠题海战术这么学过来的， 那多可怕，要让他们去搞研究、创新，那就基本不可能。很遗憾的是，现在中国学生从幼儿园开始，就是在题海中泡大的。机器人、人工智能，靠题海战术是可以演示不少功能的， 但是， 那还离真正的智能比较遥远。\n杨：好， 我明白这个analysis-by-synthesis 的意义了。他的第二贡献呢？\n朱：他提出了一整套建模的理论和方法。把代数、几何、概率整合起来。 代数指的是一些结构，比如群论， 记得在科大本科我学过 群、环、域这些概念吧？也就是说我有一些基本元素，叫 generator，连接成为图graph，然后是群group，在上面进行操作, 产生了各种各样的变化。还有很多几何， 变换， 在连续情况就产生形变。通过组合，语法、产生丰富的图模式。然后，再在这个图模式的空间上定义距离（测度）和概率。\n朱：比如一个概率模型， 是定义在一个什么样的结构上，它是个什么样的解空间？这个数理上你必须交代清楚，否则你的论文写不下去了。现在它的一个很大的应用在医疗图像上面，比如说一个病人，他的肝变形了，那么他的肝的形状和正常人的肝的形状之间怎么定义一个合理的距离？两张人脸，怎么定义这个距离的呢？这个距离定义在一个流型上，数学的流型（manifold）。\n杨：这些东西真用上了吗？\n朱：他有个Postdoc，名叫Michael Miller， 现在是Johns Hopkins 大学图像中心主任， 就用这一套方法来做医疗图像、脑科学（Brain Mapping）等方面的应用。\n杨：他的第三方面的贡献呢？\n朱：第三个方面主要是算法上面。当我们去做求解的时候，在一个解空间，这个求解空间肯定是一个非凸的，他有千千万万的局部最优解local minimum 在里面。\n杨：对。这是当时八十年代的时候提出来一个很尖锐的问题，好像有什么模拟煺火方法。\n朱：很多蒙特卡洛算法都是他和这个学派的人提出来的。这个解空间是一个异构空间，空间里面非常复杂的，包含有很多子空间，子空间里面又包含又子空间，每个子空间维度又不一样，他们之间，从一个解跳到另外一个解的时候，这跳转必须是可逆的。在计算机里面就叫可以回溯。从这个学派走出来的人，他们设计算法每一个步骤都是有章法的，要做到合规合矩。包括上面提到的吉布斯采样算法、可逆蒙特卡洛跳转法，还有变分法（variational methods）和偏微分方程式， 还有一些随机下降法（stochastic gradient）， 这后者是目前训练深度学习模型的主要办法。他也开创了非参数模型的学习方法。这里面东西太多，先谈到这里吧。\n正因为很多人没有接触过Grenander的理论， 缺乏这方面的理论素养， 造成我们学科发展的一个巨大的问题：很多教授、博士、研究生就是用别人的模型（机），拿来调试，基本缺乏自己发明新模型、新算法的能力。我们这个领域，很多美国名牌大学助理教授、副教授、教授， 他们的论文中的公式错误百出。现在干脆大家在论文中都不写公式了， 直接报告最后的实验结果，提高了几个百分点。这就“一俊掩百丑”了。 英文有个类似的说法叫做 “sweep the dirt under the carpet把污垢扫到地毯下”。 这些人在大量培养博士、他们出来的人评审论文。 这样一来，学科的发展堪忧！\n第六节：结束语\n杨：听了你番谈话，我明白很多。记得我当时念研究生，包括念博士生的时候，实际上是很糊涂的。就是对这个领域到底做多少东西，没有信心。觉得很多研究像画鬼一样，原理不清楚。我觉得那样的话，与其那样做事情, 那不如干脆到工业界那更快乐。\n朱：正因为我们这个领域很多历史、框架性的东西，没有搞清楚，培养出来的博士，缺乏分析能力。大家被一些工程的任务和数据驱动，被一些性能的指标牵制，对科学的发展比较迷茫。\n杨：好， 谈了很多， 我们做个总结吧。\n朱：那我就说两点。\n首先， 我在开场白中提到 “一个民族如果忘记了历史, 她也注定将失去未来。”一个学科要健康发展，需要研究人员、研究生们理解自己领域的历史和大的发展方向，建立文化的认同。否则，自己家的东西，被别人偷取，浑然不知。就像日本打入中国，想把我们的地名改掉，大家开始说日语，把名字都改做山本太郎之类，感觉很酷吗？  或者是韩国人把中国的文化拿去申报世界文化遗产，这都是要制止的。否则，过了一代人，还真说不清楚了。我记得刚来美国的时候，美国同事把汉字叫做“Kang-ji”，说是日本字。  我们领域很多人对保护这个领域的文化和传统缺乏清醒认识。皮之不存，毛将焉附？\n其次，一个学科内部，大家互相不够了解，各自为政。特别现在会议审稿人很多是研究生，以自己的狭窄的眼光和标准去评判别人的方法，造成很多混乱。搞工程的看不到理论的重要性，反之亦然。大家又都疏远心理学和认知科学的研究。我提倡我们的研究人员、学生要提高理论修养、培养长远眼光，向相关学科取经，取长补短。\n我希望这个微信公众号，能够帮助大家正视问题，让计算机视觉这个领域健康、稳健、可持续地发展。"}
{"content2":"计算机视觉\n工具：opencv+caffe+tensorflow+python+c++\n代码：OpenCV算法精解：基于Python与C++\n关注网址：\n斯坦福李飞飞-深度学习计算机视觉\n计算机视觉领域的一些牛人博客，超有实力的研究机构等的网站链接\nhttps://arxiv.org/list/cs.AI/recent\nAnnotated Computer Vision Bibliography: Table of Contents\n(这是一个神奇的网站：\nKeith Price老爷子从1994年开始做了这个索引，涵盖了所有计算机视觉里面所有topic，所有subtopic的著作，包括论文，教材，还对各类主题的关键词。这个网站在Szeliski等人的书应该都有提及。Price坚持了23年，频繁更新（最近一次是三月10号），收录每个方向重要期刊，会议文献和书籍，并且保证了所有链接不失效\n作者：Yiming Lin\n链接：https://www.zhihu.com/question/28813777/answer/151253988\n来源：知乎)\n关注期刊和会议：计算机视觉的顶级期刊有两个PAMI和IJCV，顶级会议有ICCV和CVPR或者CVPR+ICCV+ECCV+ICML+NIPS+PAMI+JMLR\n书籍和链接：\n《Computer Vision: A Modern Approach》\n《Computer Vision: Algorithms andApplications》\n《Computer Vision:  Models, Learning, and Inference》\n看文章的时候注意上网搜索，作者可能在网上公布了代码，你可以把代码下载下来深入研究而不用自己重新发明轮子\n公众号：《视觉求索》、《人工智能学家》、《新智元》、《量子位》、《机器之心》\nQQ群：深度学习Tensorflow社区、kaggle机器学习数据挖掘\n培训机构：七月在线\n总结：少看书，多练习，少用CPU多用GPU，多看论文和博客，先中文再英文，掌握数学基础，掌握经典算法，推演深度学习网络，找开源框架，选择一个应用场景深入进去！"}
{"content2":"文章首发于微信公众号《有三AI》\n【AI白身境】一文览尽计算机视觉研究方向\n今天是新专栏《AI白身境》的第10篇，所谓白身，就是什么都不会，还没有进入角色。\n相信看了前面的几篇文章后很多朋友已经等不及快速入行了，今天就来介绍一下计算机视觉的各大研究方向及其特点。\n所谓计算机视觉，即compute vision，就是通过用计算机来模拟人的视觉工作原理，来获取和完成一系列图像信息处理的机器。计算机视觉属于机器学习在视觉领域的应用，是一个多学科交叉的研究领域，涉及数学，物理，生物，计算机工程等多个学科，由此也可以想象到计算机视觉的研究范围非常广，也是图像，语音，自然语言处理领域中从业人数最多的。\n作者 | 言有三\n编辑 | 言有三\n01 图像分类\n1.1 基本概念\n图像分类是计算机视觉中最基础的一个任务，也是几乎所有的基准模型进行比较的任务，从最开始比较简单的10分类的灰度图像手写数字识别mnist，到后来更大一点的10分类的cifar10和100分类的cifar100，到后来的imagenet，图像分类任务伴随着数据库的增长，一步一步提升到了今天的水平。\n现在在imagenet这样的超过1000万图像，2万类的数据集中，计算机的图像分类水准已经超过了人类。\n图像分类，顾名思义，就是一个模式分类问题，它的目标是将不同的图像，划分到不同的类别，实现最小的分类误差。\n总体来说，对于二分类的问题，图像分类可以分为跨物种语义级图像分类，子类细粒度图像分类，以及实例级图像分类三大类别。\n传统机器学习方法：\n通过各种经典的特征算子+经典分类器组合学习，比如HoG+SVM。\n深度学习方法：\n各种分类网络，最为大家熟知的就是ImageNet竞赛了。\n2012年Alexnet诞生，意味着GPU训练时代的来临。\nAlexnet是第一个真正意义上的深度网络，与LeNet5的5层相比，它的层数增加了3 层，网络的参数量也大大增加，输入也从32变成了224。\n2014年VGG诞生，它共包含参数约为550M。全部使用3*3*的卷积核*和2*2的最大池化核，简化了卷积神经网络的结构。VGG很好的展示了如何在先前网络架构的基础上通过增加网络层数和深度来提高网络的性能，网络虽然简单，但是却异常的有效，在今天VGG仍然被很多的任务选为基准模型。\n同一年GoogleNet诞生，也被成为Inception Model，它的核心是Inception Module。一个经典的inception 结构，包括有四个成分，1*1卷积，3*3 卷积， 5*5 卷积，3*3 最大池化，最后对运算结果进行通道上组合，可以得到图像更好的表征。自此，深度学习模型的分类准确率已经达到了人类的水平(5%~10%)。\n2015年，ResNet被提出。ResNet以 3.57%的错误率表现超过了人类的识别水平，并以152层的网络架构创造了新的模型记录。由于resnet采用了跨层连接的方式，它成功的缓解了深层神经网络中的梯度消散问题，为上千层的网络训练提供了可能。\n2016年ResNeXt诞生，101层的ResNeXt可以达到ResNet152 的精确度，却在复杂度上只有后者的一半，核心思想为分组卷积。即首先将输入通道进行分组，经过若干并行分支的非线性变换，最后合并。\n在resnet基础上，密集连接的densenet将前馈过程中将每一层与其他的层都连接起来。对于每一层网络来说，前面所有网络的特征图都被作为输入，同时其特征图也都被其他网络层作为输入所利用。\n2017年，也是imagenet图像分类比赛的最后一年，senet获得了冠军。这个结构，仅仅使用了“特征重标定”的策略来对特征进行处理，也就是通过学习获取每个特征通道的重要程度，根据重要性去抑制或者提升相应的特征。\n1.2 方向特点\n图像分类的比赛基本落幕，也接近算法的极限。但是在实际的应用中却面临着比比赛中更加复杂，比如样本不均衡，分类界面模糊，未知类别等。如果想了解更多，请查看往期文章。\n【技术综述】你真的了解图像分类吗？\n02 目标检测\n2.1 基本概念\n分类任务给出的是整张图片的内容描述，而目标检测任务则关注图片中特定的目标。\n检测任务包含两个子任务，其一是这一目标的类别信息和概率，它是一个分类任务。其二是目标的具体位置信息，这是一个定位任务。\n与计算机视觉领域里大部分的算法一样，目标检测也经历了从传统的人工设计特征和浅层分类器的思路（以），到大数据时代使用深度神经网络进行特征学习的思路。\n在传统方法时代，很多的任务不是一次性解决，而是需要多个步骤的。而深度学习时代，很多的任务都是采用End-To-End的方案，即输入一张图，输出最终想要的结果，算法细节和学习过程全部丢给了神经网络，这一点在物体检测这个领域，体现得尤为明显。\n不管是清晰地分步骤处理，还是深度学习的end-to-end的方法，目标检测算法一定会有3个模块。第一个是检测窗口的选择，第二个是图像特征的提取，第三个是分类器的设计。\n2.2 方法分类\n传统机器学习方法：\n以保罗·维奥拉和迈克尔·琼斯于2001年提出的维奥拉-琼斯目标检测框架为代表，这是第一篇基于Haar+Adaboost的检测方法，也是首次把检测做到实时的框架，此方法在opencv中被实现为cvHaarDetectObjects()，是opencv中最为人熟知的目标检测方法。速度非常快，检测召回率相对如今的算法较低。\n深度学习方法：\n仍然要解决区域选择、提取特征、分类回归三个问题。但是在演变过程中，却发展出了multi-stage和one-stage的方法。其中multi-stage方法，是分步骤完成上面的任务，甚至可能需要单独训练各个网络。而one-stage则是一步到位。\nRCNN的框架是multi-stage方法的典型代表。它使用了Selective search先生成候选区域再检测，候选窗口的数量被控制在了2000个左右。选择了这些图像框之后，就可以将对应的框进行resize操作，然后送入CNN中进行训练。由于CNN非常强大的非线性表征能力，可以对每一个区域进行很好的特征表达，CNN最后的输出，使用多个分类器进行分类判断。该方法将PASCAL VOC上的检测率从 35.1% 提升到了53.7%，其意义与Alexnet在2012年取得分类任务的大突破是相当的，对目标检测领域影响深远。\n随后Fast R-CNN提出RoIPooling从整图对应的卷积特征图选取区域特征，解决了重复提取特征的问题。Faster R-CNN则提出Region Proposal, anchors把一张图片划分成n*n个区域，每个区域给出9个不同ratio和scale的proposal，解决了重复提取候选proposal的问题。 RCNN系列在工业届应用非常广泛，因此从事目标检测的同学必须掌握。\n除了multi-stage方法，还有one-stage方法。以YOLO为代表的方法，没有显式的候选框提取过程。它首先将图片resize到固定尺寸，将输入图片划分成一个7x7的网格，每个网格预测2个边框，对每一个网络进行分类和定位。YOLO方法也经过了许多版本的发展，从YOLO v2到YOLO v3。YOLO的做法是速度快，但是会有许多漏检，尤其是小的目标。所以SSD就在 YOLO的基础上添加了Faster R-CNN的Anchor 概念，并融合不同卷积层的特征做出预测。虽然YOLO和SSD系列的方法没有了region proposal的提取，速度更快，但是必定会损失信息和精度。\n如果想了解更多，可以去阅读我们的往期文章。\n【技术综述】一文道尽R-CNN系列目标检测\n【技术综述】万字长文详解Faster&nbsp;RCNN源代码\n2.3 方向特点\n目标检测方向有一些固有的难题，比如小脸，遮挡，大姿态。\n而在方法上，多尺度与级联网络的设计，难样本的挖掘，多任务loss等都是比较大的研究小方向，咱们也写过一些文章，感兴趣的朋友可以去翻。\n03 图像分割\n3.1 基础概念\n图像分割属于图像处理领域最高层次的图像理解范畴。所谓图像分割就是把图像分割成具有相似的颜色或纹理特性的若干子区域，并使它们对应不同的物体或物体的不同部分的技术。这些子区域，组成图像的完备子集，又相互之间不重叠。\n在图像处理中，研究者往往只对图像中的某些区域感兴趣，在此基础上才有可能对目标进行更深层次的处理与分析，包括对象的数学模型表示、几何形状参数提取、统计特征提取、目标识别等。\n传统方法：\n图像分割问题最早来自于一些文本的分割，医学图像分割。在文本图像分割中，我们需要切割出字符，常见的问题包括指纹识别，车牌识别；由于这一类问题比较简单，因为基于阈值和聚类的方法被经常使用。\n基于阈值和聚类的方法虽然简单，但因此也经常失效。以graphcut为代表的方法，是传统图像分割里面鲁棒性最好的方法。Graphcut的基本思路，就是建立一张图，其中以图像像素或者超像素作为图像顶点，然后移除一些边，使得各个子图不相连从而实现分割。图割方法优化的目标是找到一个切割，使得移除边的和权重最小。\n深度学习方法：\n全卷积神经网络(Fully connected Network)是第一个将卷积神经网络正式用于图像分割问题的网络。\n一个用于分类任务的深度神经网络通过卷积来不断抽象学习，实现分辨率的降低，最后从一个较小的featuremap或者最后的特征向量，这个featuremap通常为5*5或者7*7等大小。而图像分割任务需要恢复与原尺度大小一样的图片，所以，需要从这个featuremap恢复原始图片尺寸，这是一个上采样的过程。由于这个过程与反卷积是正好对应的逆操作，所以我们通常称其为反卷积。\n实际上并没有反卷积这样的操作，在现在的深度学习框架中，反卷积通常有几种实现方式，一个是双线性插值为代表的插值法，一个是转置卷积。\n3.2 方向特点\n在基于深度学习的图像分割中，有一些比较关键的技术，包括反卷积的使用，多尺度特征融合，crf等后处理方法。\n多尺度与上下文信息：\n多尺度的信息融合可以从特征图，还可以直接采用多尺度的输入图像，不过这两者本质上没有太多的差异。使用金字塔的池化方案可实现不同尺度的感受野，它能够起到将局部区域上下文信息与全局上下文信息结合的效果。对于图像分割任务，全局上下文信息通常是与整体轮廓相关的信息，而局部上下文信息则是图像的细节纹理，要想对多尺度的目标很好的完成分割，这两部分信息都是必须的。\nCRF：\n由于经典的cnn是局部的方法，即感受野是局部而不是整个图像。另一方面，cnn具有空间变换不变性，这也降低了分割的边缘定位精度。针对cnn的这两个缺陷，crf可以进行很好的弥补。crf是一种非局部的方法，它可以融合context信息，Deeplab系列就使用了cnn加上全连接的crf的方式。\n另一方面，前面我们说的图像分割，是属于硬分割，即每一个像素都以绝对的概率属于某一类，最终概率最大的那一类，就是我们所要的类别。但是，这样的分割会带来一些问题，就是边缘不够细腻，当后期要进行融合时，边缘过渡不自然。此时，就需要用到image matting技术。\n更多请查看往期文章：\n【技术综述】闲聊图像分割这件事儿\n04 目标跟踪\n4.1 基本概念\n目标跟踪，指的其实就是视频中运动目标的跟踪，跟踪的结果通常就是一个框。目标跟踪是视频监控系统中不可缺少的环节。\n根据目标跟踪方法建模方式的不同，可以分为生成式模型方法与判别式模型方法。\n生成式模型跟踪算法以均值漂移目标跟踪方法和粒子滤波目标跟踪方法为代表，判别式模型跟踪算法以相关滤波目标跟踪方法和深度学习目标跟踪方法为代表。\n生成类方法：\n在原始影像帧中对目标按指定的方法建立目标模型，然后在跟踪处理帧中搜索对比与目标模型相似度最高的区域作为目标区域进行跟踪。算法主要对目标本身特征进行描述，对目标特征刻画较为细致，但忽略背景信息的影响。在目标发生变化或者遮挡等情况下易导致失跟现象。\n判别类方法：\n通过对原始影像帧，对目标及背景信息进行区分建立判别模型，通过对后续影像帧搜索目标进行判别是目标或背景信息进而完成目标跟踪。\n判别类方法与生成类方法的根本不同在于判别类方法考虑背景信息与目标信息区分来进行判别模型的建立，由于判别类方法将背景与目标进行区分，因此该类方法在目标跟踪时的表现通常更为鲁棒，目前已经成为目标跟踪的主流跟踪方式。判别类方法包括相关滤波，深度学习方法。\n4.2 方向特点\n目标跟踪有一些难点：\n(1) 目标表征表达问题，虽然深度学习方法具有很强的目标表征能力，但是仍然容易受相似环境的干扰。\n(2) 目标快速运动，由于很多跟踪的物体都是高速运动，因此既要考虑较大的搜索空间，也要在保持实时性的前提下减小计算量。\n(3) 变形，多尺度以及遮挡问题，当目标发生很大的形变或者临时被遮挡如何保持跟踪并且在目标重新出现时恢复跟踪。\n05 图像滤波与降噪\n5.1 基本概念\n现实中的数字图像在数字化和传输过程中常受到成像设备与外部环境噪声干扰等影响，称为含噪图像或噪声图像。减少数字图像中噪声的过程称为图像降噪，有时候又称为图像去噪。\n降噪可以应用于图像增强和美颜等领域。\n传统方法：\n传统降噪算法根据降噪的原理不同可分为基于邻域像素特征的方法，基于频域变换的方法，和基于特定模型的方法。\n基于空域像素特征的方法，是通过分析在一定大小的窗口内，中心像素与其他相邻像素之间在灰度空间的直接联系，来获取新的中心像素值的方法，因此往往都会存在一个典型的输入参数，即滤波半径r。此滤波半径可能被用于在该局部窗口内计算像素的相似性，也可能是一些高斯或拉普拉斯算子的计算窗口。在邻域滤波方法里面，最具有代表性的滤波方法有以下几种：算术均值滤波与高斯滤波，统计中值滤波，双边滤波，非局部平均滤波方法，BM3D算法。\n深度学习方法：\n在2012年，随着Alexnet的出现，深度学习做去噪的工作取得了一些进展，可以达到和BM3D差不多的水平。对于仿真的噪声和固定的噪声，深度学习已经可以很好的去除，达到或超过传统领域里最好的算法。\n利用卷积神经网络去除噪声的原理很简单，输入是一张有噪声的图，标签是一张无噪声的图，输出是一张降噪后的图，损失函数是无噪声groundtruth与网络输出的L2距离，网络通常就是与图像分割算法一样的网络，卷积+与之对称的反卷积。\n5.2 方向特点\n降噪的研究聚焦在真实数据的去噪声，因为真实世界的噪声不符合高斯加性噪声的假设，而且是依赖于信息本身的。不过，真实噪声图像和相应的无噪声图像获取是非常困难，慢慢的也有了一些benchmark，大家以后关注我们就知道了。\n06 图像增强\n6.1 基本概念\n图像增强，即增强图像中的有用信息，改善图像的视觉效果。\n图像增强实际上包含了很多的内容，上面的降噪也属于其中，只是因为降噪多了美颜这一个应用单独拿出来说一下。\n对比度增强，用于扩大图像中不同物体特征之间的差别，抑制不感兴趣的特征，可用于改善图像的识别效果，满足某些特殊分析。\n超分辨，使图像变得更加清晰，可以用于视频的传输先进行降采样，再进行升采样，即降低了传输成本，又增加了视觉效果。\n图像修复，重建图像和视频中丢失或损坏的部分，也被称为图像插值或视频插值，主要是替换一些小区域和瑕疵，如photoshop中的印章工具。随着发展，已经从原先针对划痕、污点等的修复到现在对图像、视频中文字、物体等的移除，比如水印等。\n传统方法：\n传统的方法就是一个预定义好的非线性变换，主要有三大类方法，一类是点操作，一类是直方图操作，一类是Retinex理论。\n点操作也被称为直接对比度增强，将每个像素独立操作，包括对数变化，指数变化，负图像，阈值化等。我们熟知的gamma变换如下，可以进行不同形状的映射。\n直方图操作也被称为间接对比度增强，包括直方图均衡，直方图匹配等。直方图均衡化通常用来增加图像的全局对比度，尤其是当图像中主体和背景对比度相当接近的时候。直方图均衡化的效果就是让直方图更均衡的分布，这种方法对于背景和前景都太亮或者太暗的图像非常有用，通常是曝光过度或者曝光不足的图片。\nRetinex理论，即颜色恒常知觉的计算理论，Retinex是一个合成词，它的构成是retina（视网膜）+cortex（皮层），它将图像认为是reflectance和illumination的点乘，理论基础是在不同的照明条件下，物体的色彩不受光照非均性的影响是恒定的，而物体的颜色是由物体对长波、中波和短波光线的反射能力决定的而不是由反射光强度的绝对值决定。\n深度学习方法：\n以增强对比度为例，深度学习方法使用了CNN来进行非线性变换的学习，而且通常不仅仅局限在对比度增强，经常会同时学习到降噪。深度学习的方法有两种，一种是采用成对的图片训练，比如pix2pix，learning in the dark，缺点是没有普适性，只能对所实验的数据集有用。一种是不需要成对图片训练，只需要好图，比如WESPE，常配合GAN使用。\n6.2 方向特点\n一个图像增强任务，传统方法需要分别进行降噪，颜色校正，对比度增强等各种操作，而深度学习算法的好处就是end-to-end输出，将整个流程丢给了网络。目前图像增强相对于前面的一些方向还是一个蓝海，覆盖的方向和应用非常广，有精力的朋友可以好好研究。\n07 风格化\n7.1 基本概念\n图像风格化之所以引起我们的注意，完全是因为2015年的一个研究，可以将任意的图像转换为梵高的画作风格。 也是得益于深度学习技术的发展，传统的方法做不到这么好的效果。而随着美图秀秀，天天P图等app层出不穷的滤镜，风格化已经成为了单独的一个研究领域。\n图像风格化是一个综述性的技术应用，为了简单起见，就理解为艺术类滤镜把，它指通过算法，将数码相机拍摄的照片，变成绘画、素描等艺术类的非数码相机效果，是后期程度最深的操作，将彻底改变相片的风格。\n深度学习方法：\n以A Neural Algorithm of Artistic Style 论文发表为起始，Prisma滤镜为典型代表。虽然风格迁移技术的发展日新月异，但是最革命性的还是该文章的方法，这是德国图宾根大学的研究，它通过分析某种风格的艺术图片，能将图片内容进行分离重组，形成任意风格的艺术作品，最开始的时候需要将近一个小时来处理。\n就是把一幅图作为底图，从另外一幅画抽取艺术风格，重新合成新的艺术画，可以参考上面的图。\n研究者认为，图片可以由内容层（Content）与风格层（Style）两个图层描述，相互分离开。在图像处理中经常将图像分为粗糙层与细节层，即前者描述图像的整体信息，后者描述图像的细节信息，具体可以通过高斯金字塔来得到。\n卷积神经网络的各个神经元可以看做是一个图像滤波器，而输出层是由输入图像的不同滤波器的组合，深度由浅到深，内容越来越抽象。\n底层信息重建，则可以得到细节，而从高层信息重建，则得到图像的”风格“。因此，可以选择两幅图像，一幅构建内容信息，一幅构建风格信息，分别进行Content重建与Style 重建。通过将内容与风格组合，可以得到新的视觉信息更加有意思的图像，如计算机油画，这就是它的基本原理。方法的核心在于损失函数的设计，包括内容损失和风格损失。\n内容损失在像素空间，要求风格化后的图能够保证内容的完整性。风格损失使用vgg特征空间的gram矩阵，这样就有了较高的抽象层级，实践结果表明可以很好的捕捉风格。\n7.2 方向特点\n如今风格化方法在很多地方都有应用，比如大家熟悉的变脸等。方法也演变成了几个方向；\n（1）单模型单风格，即一个网络只能做一种风格化。\n（2）单模型多风格，即一个网络可以实现多种风格，比（1）实用的多。\n（3）单模型任意风格，即一个网络可以任意风格，视输入图像而定，这是最好的，更多的研究我们以后会开专题。\n08 三维重建\n8.1 基本概念\n什么是三维重建呢？广义上来说，是建立真实世界的三维模型。随着软硬件的成熟，在电影，游戏，安防，地图等领域，三维重建技术的应用越来越多。目前获取三维模型的方法主要包括三种，手工建模，仪器采集与基于图像的建模。\n(1) 手工建模作为最早的三维建模手段，现在仍然是最广泛地在电影，动漫行业中应用。顶顶大名的3DMax就是典型代表，当然了，它需要专业人士来完成。\n(2) 由于手工建模耗费大量的人力，三维成像仪器也得到了长期的研究和发展。基于结构光（structured light）和激光扫描技术的三维成像仪是其中的典型代表。这些基于仪器采集的三维模型，精度可达毫米级，是物体的真实三维数据，也正好用来为基于图像的建模方法提供评价数据库。由于仪器的成本太高，一般的用户是用不上了。\n(3) 基于图像的建模技术（image based modeling），顾名思义，是指通过若干幅二维图像，来恢复图像或场景的三维结构，这些年得到了广泛的研究。\n我们这里说的三维重建，就特指基于图像的三维重建方法，而且为了缩小范围，只说人脸图像，并简单介绍其中核心的3DMM模型。\n3DMM模型：\n人脸三维重建方法非常多，有基于一个通用的人脸模型，然后在此基础上进行变形优化，会牵涉到一些模板匹配，插值等技术。有基于立体匹配（各种基于双目，多目立体视觉匹配）的方法，通过照相机模型与配准多幅图像，坐标系转换，获取真实的三维坐标，然后进行渲染。有采用一系列的人脸作为基，将人脸用这些基进行线性组合的方法，即Morphable models方法。\n其中，能够融会贯通不同传统方法和深度学习方法的，就是3D Morphable Models系列方法，从传统方法研究到深度学习。\n它的思想就是一幅人脸可以由其他许多幅人脸加权相加而来，学过线性代数的就很容易理解这个正交基的概念。我们所处的三维空间，每一点(x,y,z)，实际上都是由三维空间三个方向的基量，(1,0,0)，(0,1,0)，(0,0,1)加权相加所得，只是权重分别为x,y,z。\n转换到三维空间，道理也一样。每一个三维的人脸，可以由一个数据库中的所有人脸组成的基向量空间中进行表示，而求解任意三维人脸的模型，实际上等价于求解各个基向量的系数的问题。\n每一张人脸可以表示为：\n形状向量Shape Vector：S=(X1,Y1,Z1,X2,Y2,Z2,...,Yn,Zn)\n纹理向量Texture Vector：T=(R1,G1,B1,R2,G2,B2,...,Rn,Bn)\n而一张任意的人脸，其等价的描述如下：\n其中第一项Si，Ti是形状和纹理的平均值，而si，ti则都是Si，Ti减去各自平均值后的协方差矩阵的特征向量。 基于3DMM的方法，都是在求解α，β这一些系数，当然现在还会有表情，光照等系数，但是原理都是通用的。\n原理就说到这里，我们以后会专门讲述。\n8.2 方向特点\n人脸的三维建模有一些独特的特点。\n（1）预处理技术非常多，人脸检测与特征点定位，人脸配准等都是现在研究已经比较成熟的方法。利用现有的人脸识别与分割技术，可以缩小三维人脸重建过程中需要处理的图像区域，而在有了可靠的关键点位置信息的前提下，可以建立稀疏的匹配，大大提升模型处理的速度。\n（2）人脸共性多。正常人脸都是一个鼻子两只眼睛一个嘴巴两只耳朵，从上到下从左到右顺序都不变，所以可以首先建立人脸的参数化模型，实际上这也是很多方法所采用的思路。\n人脸三维重建也有一些困难。\n（1）人脸生理结构和几何形状非常复杂，没有简单的数学曲面模型来拟合。\n（2）光照变化大。同一张脸放到不同的光照条件下，获取的图像灰度值可能大不一样的，这些都会影响深度信息的重建。\n（3）特征点和纹理不明显。图像处理最需要的就是明显的特征，而光滑的人脸除了特征关键点，很难在脸部提取稠密的有代表性的角点特征。这个特点，使得那些采用人脸配准然后求取三维坐标的方法面临着巨大的困难。\n09 图像检索\n9.1 基本概念\n图像检索的研究从20世纪70年代就已经开始，在早期是基于文本的图像检索技术（Text-based Image Retrieval，简称TBIR），利用文本来描述图像的特征，如绘画作品的作者、年代、流派、尺寸等。随着计算机视觉技术的发展，90年代开始出现了对图像的内容语义，如图像的颜色、纹理、布局等进行分析和检索的图像检索技术，也就是基于内容的图像检索（Content-based Image Retrieval，简称CBIR）技术，本小节的图像检索就特指基于内容的图像检索。\n基于内容的图像检索也经历了传统方法和深度学习方法两个主要阶段，传统的基于内容的图像检索通常包括以下流程：\n预处理，通常包括一些图像归一化，图像增强等操作。特征提取，即提取一些非常鲁棒的图像特征，比如SIFT，HoG等特征。特征库就是要查询的库，库中不存储图像而是存储特征，每一次检索图像完成特征提取之后，就在特征库中进行匹配和相似度计算。索引就是在某种相似性度量准则下计算查询向量到特征库中各个特征的相似性大小，最后按相似性大小进行高效的排序并顺序输出对应的图片。\n图像检索的中最复杂的一步就是检索，在这一步完成验证过程。\n最简单的方法就是暴力(brute-force) 搜索方法(又称线性扫描)，即逐个与数据库中的每个点进行相似性计算然后进行排序，这种简单粗暴的方式虽然很容易实现，但是会随着数据库的大小以及特征维度的增加其搜索代价也会逐步的增加，从而限制在数据量小的小规模图像数据库，在大规模图像库上这种暴力搜索的方式不仅消耗巨大的计算资源，而且单次查询的响应时间会随着数据样本的增加以及特征维度的增加而增加，为了降低搜索的空间的空间复杂度与时间复杂度，研究者们提出了很多高效的检索技术，其中最成功的大家也最熟悉到方法是基于哈希的图像检索方法。\n深度学习在图像检索里面的作用就是把表征样本的特征学习好，就够了。\n9.2 方向特点\n图像检索系统具有非常大的商业价值，从搜索引擎的以图搜图，到人脸验证和识别系统，到一些搜索排序系统(比如基于美学的摄影图库)。由于图像特征的学习是一个通用的研究方向，因此更多的在于设计高效的检索系统。\n10 GAN\n10.1 基本概念\nGAN，即Generative adversarial net，被誉为新的深度学习，涉及的研究非常多，可以单列为一个方向，一个经典的网络结构如下。\nGAN的原理很简单，它包括两个网络，一个生成网络，不断生成数据分布。一个判别网络，判断生成的数据是否为真实数据。\n上图是原理展示，黑色虚线是真实分布，绿色实线是生成模型的学习过程，蓝色虚线是判别模型的学习过程，两者相互对抗，共同学习到最优状态。\n关于GAN的基础，我们以前已经写过相关的内容，大家去看就可以了。\n【技术综述】有三说GANs（上）\n10.2 方向特点\n作为新兴和热门方向，GAN包含的研究方向非常的广，包括GAN的应用，GAN的优化目标，GAN的模型发展，GAN的训练技巧，GAN的理论分析，GAN的可视化等等，以后等着我们的分享即可。\n最后发个通知，2019年有三AI学习季划之“春季计划”开始了，目标就是掌握计算机视觉的各项技能，欢迎参与。\n2019年有三AI“春季”划，给我一个荣耀，还你一生荣耀\n总结\n深度学习彻底点燃和推进了计算机视觉各大领域的研究，这是个可以投以终身的行业，希望你会喜欢，别忘了持续关注我们噢。\n下期预告：下一期我们讲AI在当前工业界的应用。\n转载文章请后台联系\n侵权必究\nAI白身境系列完整阅读：\n第一期：【AI白身境】深度学习从弃用windows开始\n第二期：【AI白身境】Linux干活三板斧，shell、vim和git\n第三期：【AI白身境】学AI必备的python基础\n第四期：【AI白身境】深度学习必备图像基础\n第五期：【AI白身境】搞计算机视觉必备的OpenCV入门基础\n第六期：【AI白身境】只会用Python？g++，CMake和Makefile了解一下\n第七期：【AI白身境】学深度学习你不得不知的爬虫基础\n第八期： 【AI白身境】深度学习中的数据可视化\n第九期：【AI白身境】入行AI需要什么数学基础：左手矩阵论，右手微积分\n第十期：【AI白身境】一文览尽计算机视觉研究方向\n第十一期：【AI白身境】AI+，都加在哪些应用领域了\n第十二期：【AI白身境】究竟谁是paper之王，全球前10的计算机科学家\nAI初识境系列完整阅读\n第一期：【AI初识境】从3次人工智能潮起潮落说起\n第二期：【AI初识境】从头理解神经网络-内行与外行的分水岭\n第三期：【AI初识境】近20年深度学习在图像领域的重要进展节点\n第四期：【AI初识境】激活函数：从人工设计到自动搜索\n第五期：【AI初识境】什么是深度学习成功的开始？参数初始化\n第六期：【AI初识境】深度学习模型中的Normalization，你懂了多少？\n第七期：【AI初识境】为了围剿SGD大家这些年想过的那十几招\n第八期：【AI初识境】被Hinton，DeepMind和斯坦福嫌弃的池化，到底是什么？\n第九期：【AI初识境】如何增加深度学习模型的泛化能力\n第十期：【AI初识境】深度学习模型评估，从图像分类到生成模型\n第十一期：【AI初识境】深度学习中常用的损失函数有哪些？\n第十二期：【AI初识境】给深度学习新手开始项目时的10条建议\n感谢各位看官的耐心阅读，不足之处希望多多指教。后续内容将会不定期奉上，欢迎大家关注有三公众号 有三AI！"}
{"content2":"转自论坛http://www.ieee.org.cn/dispbbs.asp?BoardID=62&replyID=31567&id=29962&star=1&skin=0\n作者好像是南大周志华老师\n我知道的几个人工智能会议(一流)\n下面同分的按字母序排列:\nIJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个领域就没几篇了，象machine learning、computer vision这么大的领域每次大概也就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司\"IJCAI Inc.\"主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer& Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外，IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找3位reviewer, second PC member 找一位.\nAAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可以给到1+, 也可以给到1-或者2+, 总的来说我给它\"1\". 这是因为它的开法完全受IJCAI制约: 每年开, 但如果这一年的IJCAI在北美举行, 那么就停开. 所以, 偶数年里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些,特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了.\nCOLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉, 所以这个会被一些人看成是理论计算机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: \"一小群数学家在开会\". 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的会议, 例如COLT.\nCVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. IEEE一直有个倾向, 要把会办成\"盛会\", 历史上已经有些会被它从quality很好的会办成\"盛会\"了. CVPR搞不好也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了.\nICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办.  ICCV逢奇数年开，开会地点以往是北美，欧洲和亚洲轮流，本来2003年定在北京，后来因Sars和原定05年的法国换了一下。ICCV'07年将首次在南美(巴西)举行.\nCVPR原则上每年在北美开, 如果那年正好ICCV在北美,则该年没有CVPR.\nICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的介绍.\nNIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS'05的论文集是06年出. 会议的名字是\"Advances in Neural Inxxxxation Processing Systems\", 所以, 与ICMLECML这样的\"标准的\"机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在MichaelJordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给\"外人\"的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说,ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选理事, 有资格提名的人包括近三年在ICMLECMLCOLT发过文章的人, NIPS则被排除在外了. 无论如何, 这是一个非常好的会.\nACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of\nComputational Linguistics) 主办, 每年开.\nKR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI)最好的会议之一. KR Inc.主办, 现在是偶数年开.\nSIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至有点机器学习应用会议的味道了, 所以把它也列进来.\nSIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列在tier-1里面, 一方面是名声远不及其他的top conference响亮, 另一方面是相对容易被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. 这几年来KDD的质量都很高. SIGKDD从2000年来full paper的录取率都在10%-12%之间，远远低于IJCAI和ICML.\n经常听人说，KDD要比IJICAI和ICML都要困难。IJICAI才6页，而KDD要10页。没有扎实系统的工作，很难不留下漏洞。有不少IJICAI的常客也每年都投KDD，可难得几个能经常中。\nUAI (1-): 名字叫\"人工智能中的不确定性\", 涉及表示推理学习等很多方面, AUAI(Association of UAI) 主办, 每年开.\n我知道的几个人工智能会议(二三流)\n(原创为lilybbs.us上的daniel)\n纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全.\n同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的.\ntier 2: tier-2的会议列得不全, 我熟悉的领域比较全一些.\nAAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,\n几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显.\nECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能升级到1-去.\nECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显.\nICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了.\nSDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚,但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的.\nICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了.\nICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上.\nCOLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多.\nECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,\n很难往上升.\nALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容.\nEMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点.\nILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了.\nPKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被PKDD接受).\ntier 3: 列得很不全. 另外, 因为AI的相关会议非常多, 所以能列在tier-3也算不错了, 基本上能进到所有AI会议中的前30%吧\nACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了.\nDS (3+): 日本人发起的一个接近数据挖掘的会议.\nECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议.\nICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了.\nPAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5.\nICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN.\nAJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.\nCAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.\nCEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC/FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI  (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作.\nFUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍.\nGECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型.\nICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议.\nICIP (3): 图像处理方面最著名的会议之一, 盛会型.\nICPR (3): 模式识别方面最著名的会议之一, 盛会型.\nIEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹.\nIJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍.\nIJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议.\nPRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升."}
{"content2":"VLFeat\nMexOpenCV\nfacedetect\nMachine Vision Toolbox\nPiotrs Image Video Matlab Toolbox\nMATLAB Functions for Multiple View Geometry\n1. VLFeat\n项目网站：http://www.vlfeat.org\n许可证：BSD\n著名的计算机视觉/图像处理开源项目，知名度应该不必OpenCV低太多，曾获ACM Open Source Software Competition 2010一等奖。使用C语言编写，提供C语言和Matlab两种接口。\nVLFeat完整的功能列表：http://www.vlfeat.org/matlab/matlab.html\n2. MexOpenCV\n让Matlab支持调用的OpenCV\n项目网站：http://www.cs.sunysb.edu/~kyamagu/mexopencv/\n3. facedetect\nPeter Kovesi的工具箱：轻量好用，侧重图像处理\n项目网站：http://www.csse.uwa.edu.au/~pk/research/matlabfns/\n4.Machine Vision Toolbox\n侧重机器视觉、三维视觉\n项目网站：http://www.petercorke.com/Machine_Vision_Toolbox.html\n许可证：LGPL\n侧重机器视觉，作者是另一个Peter，Peter Corke在机器人界很有名，他在2011年写了一本书《Robotics, Vision & Control》介绍了机器视觉相关的颜色、相机模型、三维视觉、控制等研究，并配套这个工具箱。算法包括了大量常用的视觉和图像处理小函数，，这些就不提了，提几个别的工具箱一般没有的功能\n5. Piotr’s Image & Video Matlab Toolbox\n侧重物体识别\n项目网站：http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html\n许可证：Simple BSD\n6. MATLAB Functions for Multiple View Geometry\n《计算机视觉中的多图几何》（Multiple View Geometry in Computer Vision），中算法的Matlab实现\n项目网站：http://www.robots.ox.ac.uk/~vgg/hzbook/code/\n许可证：MIT\nreference\n[1] http://blog.csdn.net/huilingwu/article/details/51352055"}
{"content2":"第 34 集：机器学习与人工智能\n01:23 分类 Classification\n01:25 分类器 Classifier\n01:34 特征 Feature\n02:03 标记数据 Labeled data\n02:38 决策边界 Decision boundaries\n03:00 混淆矩阵 Confusion matrix\n03:39 未标签数据 Unlabeled data\n03:49 决策树 Decision tree\n04:25 支持向量机 Support Vector Machines\n05:52 人工神经网络 Artificial Neural Network\n08:34 深度学习 Deep learning\n09:21 弱AI, 窄AI Weak AI, Narrow AI\n09:43 强AI Strong AI\n10:42 强化学习 Reinforcement Learning\n分类器：做分类的算法叫做分类器。“特征”是用来帮助“分类”的值。\n标记数据不仅要记录特征值，还要记录种类\n机器学习算法的目的：是最大化正确分类+最小化错误分类\n决策树：生成决策树的机器学习算法\n多个决策树组成的算法叫“决策森林”\n支持向量机：本质上是用任意线段来切分“决策空间”，而且线段不一定是直线，可以是多项式或其他数学函数\n人工神经网络：“决策树” 和 “支持向量机”这样的技术发源自统计学。但也有不用统计学的算法，比如人工神经网络\n神经元常见处理流程：加权、求和、偏置(加或减一个固定值)、激活函数\n激活函数，也叫传递函数。应用与输出，对结果执行最后一次数学修改\n做神经网络时，这些偏差和权重，一开始会设计成随机值，然后将最后算出的结果跟样本数据进行对比，不断调整和【训练】，直到获得 让结果最接近真实数据的 偏差值和权重值。\n输入层：主要用于样本数据输入\n隐藏层：可以有很多层，用于对数据进行加权、求和等各种处理\n输出层：输出最后处理的结果\n弱AI：只能做特定任务\n强AI：像人一样聪明的AI\n强化学习：通过反复试错，自己发现成功的策略\n人工智能的处理逻辑：(个人总结的认识，仅供参考)\n1.有庞大的样本数据(正确性，真实性)\n2.根据推测的关联因素作为数据【特征】\n3.然后利用神经网络算法得出的结果，与样本数据进行对比。得到最优算法的各种值(如某个特征的权重)\n4.然后输入真实的数据，根据最优算法进行事实的提前预测\n第 35 集：计算机视觉\n02:41 检测垂直边缘的算法\n03:26 核/过滤器 kernel or filter\n03:56 卷积 convolution\n04:23 Prewitt 算子 Prewitt Operators\n05:34 维奥拉·琼斯 人脸检测 Viola-Jones Face Detection\n05:35 卷积神经网络 Convolutional Neural Networks\n07:33 识别出脸之后，可以进一步用其他算法定位面部标志，如眼睛和眉毛具体位置，从而判断心情等信息\n08:52 跟踪全身的标记点，如肩部，手臂等\nRGB：三原色\n颜色跟踪算法—最简单的计算机视觉算法：跟踪一个颜色物体，比如一个白色的球。\n1.颜色跟踪算法是一个个像素搜索，因为颜色是在一个像素里。 首先记下球的颜色，保存最中心像素的RGB值。然后让程序在图像中找最接近这个颜色的像素。可以在视频的每一帧图片跑这个算法，跟踪球的位置。\n卷积算法：核  、  2个边缘增强的核\n卷积神经网络：\n第 36 集：自然语言处理\n01:50 词性 Parts of speech\n02:15 短语结构规则 Phrase structure rules\n02:32 分析树 Parse tree\n05:30 语音识别 Speech recognition\n07:26 谱图 Spectrogram\n07:44 快速傅立叶变换 Fast Fourier Transform\n08:42 音素 Phonemes\n09:29 语音合成 Speech Synthesis\nNLP 自然语言处理(Natural Language Processing)\n快速傅利叶变换(FFT)：一种将声音的波形转换成频率图形的算法\n语音识别：声音频率的”共振峰“特征—音素识别—组成单词—识别句首和句尾—语言模型修正口音和发音错误—转换成文字\n语音合成技术：人机交互，正向反馈"}
{"content2":"这两年，计算机视觉似乎火了起来计算机视觉的黄金时代真的到来了吗？。生物医学、机械自动化、土木建筑等好多专业的学生都开始研究其在各自领域的应用，一个视觉交流群里三分之一以上都不是计算机相关专业的。当然，我也是其中一员。\n对于非计算机相关专业的学生而言，学习过程中往往缺少交流机会，不容易把握知识的全貌。这里仅根据个人经验谈一谈对于一名非计算机专业的学生而言，该如何学习计算机视觉。\n1.编程能力\n1.1 编程语言(C++, python)\n刚接触CV(computer vision)（注：本文偏向于图像学而非图形学）时，大家一般都会不假思索地选择使用C++：装个VS(Visual Studio)，配置下opencv，撸起袖子就上了。这样做非常合理，几乎所有人都是这么入门的。\n不过，当你知识面扩展开后，你会感觉到很多时候C++都显得有些力不从心。比如：当你要画一些图表或做一些分析，就还得把数据导入MATLAB里做进一步处理；当你要非常快捷方便地学习或测试一个算法，C++会是你最糟糕的选择；或者当你要学习深度学习时，你绝对不会再选择使用C++….总之，有太多理由会促使你再学习一门编程语言，最好的选择没有之一：python。\n1.1.1 简单介绍一下C++和python的各自特点：\nC++：偏底层，执行效率高，适合嵌入式等平台上使用；在视觉领域，C++生态好，用的人多，网上找资源很方便。\n缺点是开发效率实在太低了，关于这一点如果你只是专注于图像处理的话可能感受不是那么真切，因为opencv库做得足够好。但是当你做到机器学习后，opencv就显得有些力不从心了，虽然它也包含一些SVM、神经网络等的简单实现，但毕竟不擅长。\npython：全能语言，干啥都行，并且都相对擅长。图像处理，opencv支持有python接口；科学计算，其功能类似于matlab了：机器学习及深度学习，python是最好用的，没有之一；爬虫等网络应用，豆瓣就是用python写的；简而言之，方便，实在太方便了。\n当然python也有自己的另一面。执行效率不高，这一点做嵌入式开发的可能比较忌讳。但如今手机的内存都升到6G了，tensorflow都可以在移动端跑了，Python也都可以用来控制STM32了，未来很难说。\n顺便说一句也有人使用MATLAB等做图像方面的研究，如果你只是偶尔用图像处理辅助一下你的研究，可以这么做，一般情况下不建议使用。\n1.1.2 C++和python学习资源推荐\nC++：大家好像都买《C++ primer》或《C++ primer plus》这样的大块头书，我自己感觉倒不如《王道程序员求职宝典》这类书实用。大块头书优点在于全面，同时也往往导致了重点不突出。码代码时不熟悉的用法一般直接在cppreference上搜就可以了，超级方便；但有些不容易理解的地方确实需要系统的找资料学习一下。课程的话推荐coursera上北大的《程序设计与算法》，第3门课程是C++程序设计。看视频课程一般比较慢，如果没什么基础或者特别想把基础学好的话，强烈推荐。\npython：基础部分看廖雪峰的python教程就可以了，然后就是用哪一块学哪一块了。python学起来很简单，看别人代码的过程就是学习的过程。对于不熟悉的用法多搜下官方文档，如python, numpy,pandas, matplot, scikit-learn。这里有几张python各种库的小抄表其实直接在网上搜这几张表也都比较方便。课程的话，我之前上过一些七月算法的课程，讲得不好，多少会给你一些知识体系和各种学习资料，总体不推荐或跳着看。python的开发环境值得说一下，因为有太多选择，这里比较建议使用pycharm和jupyter notebook吧，具体参考python入门环境搭建。\n1.2 编程平台（windows, linux）\n新手肯定都用windows了，学习过程中发现在windows上搞不定了，先忍几次，然后掉头就去学linux了。一定是这样。\n哪些在windows上真的搞不定呢？比如：deeplearning，或最新论文中提出的视觉开源算法。\n不过对我们而言，linux并不需要了解太深。装个ubuntu系统，常用的文件操作、程序编译等知道就OK了。我完全是在使用的过程中现用现学，手边常备一本书《鸟哥的linux私房菜》。\n2.视觉知识\n计算机视觉实在很广了，这里仅针对我个人知识体系来说一说。\n现在比较热门的方向总体上分为两大块：一块是深度学习，一块做SLAM。它们的研究点区别在哪呢？深度学习这一群体侧重于解决识别感知（是什么）问题，SLAM侧重于解决几何测量（在哪里）问题ICCV研讨会：实时SLAM的未来以及深度学习与SLAM的比较。拿机器人来说，如果你想要它走到你的冰箱面前而不撞到墙壁，那就需要使用 SLAM；如果你想要它能识别并拿起冰箱中的物品，那就需要用到深度学习机器人抓取时怎么定位的？用什么传感器来检测？。当然这两方面在research上也有互相交叉融合的趋势。\n不过在学习这些之前，一般都会先掌握下传统的计算机视觉知识，也就是图像处理这一部分了。我之前大致总结过一次：\n计算机视觉初级部分知识体系。这些基础知识的理解还是挺有必要的，有助于你理解更高层知识的本质，比如为什么会出现deeplearning等这些新的理论知识（感觉有点像读史了，给你智慧和自由）。这一部分学习资料的话还是挺推荐浅墨的《OpenCV3编程入门》 也可以看他的博客。当然他的书有一个问题就是涉及理论知识太少，所以推荐自己再另备一本偏理论一点的图像处理相关的书，我手边放的是《数字图像处理：原理与实践》，差强人意吧。个人之前看浅墨书的时候做了一份《OpenCV3编程入门》学习笔记，里边包含一些理论知识和个人见解。\n下面说一下两个大的方向：基于深度学习的视觉和SLAM技术。\n基于深度学习的视觉：机器学习包括深度学习里的大部分算法本质上都是用来做“分类”的。具体到计算机视觉领域一般就是物体分类（Object Classification）、目标检测（Object Detection）、语义分割（Image Semantic Segmentation）等，当然也有一些很酷又好玩的东西比如edges2cats、deepart。本人主要做一些Object Detection相关的东西。其实一般是直接跑别人的代码了，稍微做一些修改和参数调整，前期的预处理才是主要工作。这些程序基本都是在linux下跑的。好，深度学习为什么这么强？它主要解决了什么问题呢？我比较认同以下三点：学习特征的能力很强，通用性强，开发优化维护成本低 参见为什么深度学习几乎成了计算机视觉研究的标配？。\n关于这一部分的学习，主要就是deeplearning了。关于deeplearning，漫天飞的各种资源。可以看一看李宏毅的一天搞懂深度学习课件 youtube上有一个一天搞懂深度學習–學習心得；李飞飞的CS231n课程，网易云课堂有大数据文摘翻译的中文字幕版课程，知乎专栏智能单元有CS231N课程翻译（非常好）；三巨头之一Yoshua Bengio的新作《DEEP LEARNING》，目前已有中译版本 。\nSLAM技术：这一部分我了解不多，只是听过一些讲座。可以关注下泡泡机器人 公众号吧，他们公开课出得挺多的；听说高博的新书快出了，我也想赶紧入手偷偷学一下。\n3.机器学习\n计算机视觉中使用的机器学习方法个人感觉不算多，早期的时候会用SVM做分类，现在基本都用深度学习选特征+分类。原因在于统计机器学习这一块虽然方法不少，但是基本都无法应对图像这么大的数据量。\n不过大家在学习过程中很容易接触到各种机器学习方法的名字因为现在大数据分析、机器学习、语音识别、计算机视觉等这些其实分得不是很开，然后不自觉地就会去了解和学习。这样我感觉总体来说是好的。不过在学习一些暂时用不着的算法时，个人感觉没必要做的太深：重在理解其思想，抓住问题本质，了解其应用方向。\n下面分开介绍一下传统机器学习算法和深度神经网络。\n传统机器学习一般也就决策树、神经网络、支持向量机、boosting、贝叶斯网等等吧。方法挺多的，同一类方法不同的变形更多。除了这些监督式学习，还有非监督学习、半监督学习、强化学习。当然还有一些降维算法（如PCA）等。对这些个人整体把握的也不是特别好，太多了。\n学习资料，吴恩达的coursera课程《Machine Learning》，他正在出一本新书《MACHINE LEARNING YEARNING》，说好陆续更新的，刚更新一点就没了，本来想翻译学习一下。个人比较喜欢他的课程风格话说今天中午传出新闻，吴恩达从百度离职了。——执笔于2017.03.22，简单易懂。还有李航的《统计学习方法》和周志华的《机器学习》，两本在国内机器学习界成为经典的书。\n深度学习说着感觉有点心虚，哈哈总共就这几年就那些东西，资料上面视觉知识部分已经说过了，听听课程、看看那些出名的模型框架，基本上也就了解了《一天搞懂深度学习》其实就已经把大部分都给说了，不过个人感觉还是挺难理解的。主要的发展也就CNN、RNN；从去年起GAN火起来了，现在如日中天；增强学习现在发展也非常快，有些名校如CMU都开这方面课程了。\n资料上面说过就不说了喜欢高雅的人也可以看看这个深度学习论文阅读路线图 ，说说在使用deeplearning时用哪个库吧。目前为止还没有大一统的趋势，连各个大公司都是自己用自己开发的，一块大肥肉大家都不舍得放弃。我只用过keras和tensorflow，感觉在这方面没必要太计较，用相对简单的和大家都用的（生态好） 。\n4.数学\n一切工程问题归根结底都是数学问题，这里说说计算机视觉和机器学习所涉及的数学问题。\n微积分：比如图像找边缘即求微分在数字图像里是做差分（离散化）啦，光流算法里用到泰勒级数啦，空间域转频域的傅立叶变换啦，还有牛顿法、梯度下降、最小二乘等等这些都用的特别普遍了。其实个人感觉CV所涉及的微积分知识相对简单，积分很少，微分也不是特别复杂。也可能是本科那会儿力学学怕了吧。\n我好像没备微积分的资料，如果需要的话，同济大学出的本科教材应该也够用了吧。\n概率论与统计：这个比较高深，是应用在机器学习领域里最重要的数序分支。应用比如：条件概率、相关系数、最大似然、大数定律、马尔可夫链等等。\n浙大的《概率论与数理统计》我感觉还行，够用。\n线性代数与矩阵：数字图像本身就是以矩阵的形式呈现的，多个向量组成的样本也是矩阵这种形式非常常见，大多机器学习算法里每个样本都是以向量的形式存在的，多个矩阵叠加则是以张量(tensor)的形式存在google深度学习库tensorflow的字面意思之一。具体应用，比如：世界坐标系->相机坐标系->图像坐标系之间的转换，特征值、特征向量，范数等。\n推荐本书，国外的上课教材《线性代数》。因为浙大的那本教材感觉实在不太行，买过之后还是又买了这本。\n凸优化：这个需要单独拎出来说一下。因为太多问题（尤其机器学习领域）都是优化问题（求最优），凸优化是里面最简单的形式，所以大家都在想办法怎么把一般的优化问题转化为凸优化问题。至于单纯的凸优化理论，好像已经比较成熟了。在机器学习里，经常会看到什么求对偶问题、KKT条件等，潜下心花两天学一学。\n建议备一份高校关于凸优化的教学课件，大家对这一块毕竟比较生，缺乏系统感。比如北大的《凸优化》课程。\n这些数学知识没必要系统学习，效率低又耗时。毕竟大家都有本科的基础，够了。一般用到的时候学，学完之后总结一下。如果真想学习的话，七月在线有个课程《机器学习中的数学》，讲的一般，倒不妨看一看。\n介绍个小trick，之前学习好多数学知识或算法时，看不懂教材上晦涩死板的讲解，一般都会搜索“XXX 形象解释”，往往都会搜到些相对通俗易懂的解释也往往都是在知乎上搜到的这些解答，比如拉格朗日乘子法如何理解？, 如何通俗并尽可能详细解释卡尔曼滤波？ 。\n5.授之以鱼不如授之以渔\n编程能力->计算机视觉->机器学习->数学知识，前文已经把所要学习的知识基本都介绍完了。不知道你有没有冒出疑问：你怎么知道的这些？你平时怎么学习的？\n先说第一条：时间，时间的积累。讲个故事，去年暑期在华东师大参加一个关于ROS（Robot Operate System, 机器人操作系统）的Summer School。顺便提一句，主办者张新宇老师人特别nice。第一天上午的speaker叫Dinesh Manocha，Canny的学生。对，就是Canny边缘检测算法的Canny。Dinesh教授有一个保持了几十年的习惯：（平均）每天只睡4个多小时。用张新宇老师的一句总结就是：智力超群、体力超群、习惯超群。他还提到，未来中国要跟国外竞争，一定程度上就是体力的竞争。因为相比老外目前中国人在这方面不太重视。呃，，，反正我是弱的不行。应该加强的。\n当然，在具体学习方法也有一些trick，不然怎么解释有的人效率高呢。当然聪明和底子能够解释部分原因。现在我就说一说自己学习过程中的小trick。\ngoogle搜索。时代变了，一百年前的人类绝对想像不出自己有了困惑不是去翻书或请教他人而是告诉身旁的一台机器。如今，小学生做道算术题或小女生来个大姨妈都要问问电脑：这是怎么回事。但这些与学视觉又有什么关系呢？——答：没有。好像跑偏的有点多了，再扯远一点吧。跨越时间维度来思考一些新事物的发生及其与旧事物的联系，也许会给你一种想象的自由。比如电报、电话、视频聊天和全息通话用 HoloLens 通话 ，只是举例，我可没说以后这种技术真会普遍应用。，马车、汽车、火车、飞机和火箭太空旅行，蒸汽机、电、互联网和AI。\n百度搜索太烂了（当然，它本地化搜索做得不错。并且我也没说完全是技术原因），有多烂？我认为它跟google搜索的差距不是1:2，是1:10。这一点好像不应该说这么多，大家都公认的。问题根源在于“中国特色”不允许我们使用google搜索，这里介绍一个非常方便的科学上网工具lantern（链接是它的github地址，官网墙内好像登不了。）。下载完安装之后直接运行即可。\n还有一点，多使用英文搜索，这样呈现在你眼前的才是完整的世界。英文世界里优秀、原创资源多，浏览网页时不经意间也会遇到些好网站。比如曾经surf到一个计算机视觉方面的博客Learn OpenCV，通俗易懂，不频繁更新，几乎每篇文章必看。\n交流。这里特制人与人之间的交流，最好是面对面聊天。这样的好处是随意性大，随便一句话就可能指出你长期存在某个误区。对于我们（非计算机专业学生）而言，最缺的就是这种交流环境。所以大家只能尽量弥补了，比如通过各种途径认识点计算机专业或视觉方向的同学（蹭学校计算机视觉的课程）；多加点相关的公众号，QQ、微信群不好的再删，当然自己也要主动参与这些社区。\n书。好书基本上都是公认的，并且适合大部分人。有些人买书可能会有选择恐惧症，这一点，，，摆正心态吧，很多时候买书本来就不是为了读完，只要能给你一两次惊喜或节约你几小时宝贵时间，它的使命就已经完成了，值！！！当然买书也讲究个度，这个就如人饮水、冷暖自知了。\nPPT。PPT的出现在一定程度上对传统教材产生了冲击，方便，重点突出，体验舒服。个人几乎会把学习的所有课件都保存在ipad里推荐使用非常出名的备注记录软件Notability来保存和编辑你的PPT，听课时可以在上边做笔记，课后如果需要随时温故而知新。\n“一句话”抓住问题本质。算法太多，学过就忘。这可能是所有人遇到的问题。尤其对于那些学的不是特别深入的算法，倘或跟人聊起都不知道如何解释。“一句话”解释，就是用简单的几句话把一件事说清楚。比如《统计学习方法》里李航就提出统计机器学习的三要素：模型、策略和算法，针对某种机器学习方法根据这三要素梳理一下，你就已经把握到整体了，即使其中有些细节不理解也无伤大雅。想象一下如果有同学指着你桌上的书问你“机器学习是什么？”，你会不会一脸懵逼？我会，O(∩_∩)O。说一下个人理解，至少听起来是句人话：机器学习就是让机器学会自学，对已有信息进行归纳和识别，并自主获得新技能的能力。相比于传统计算机编程里直接告诉计算机“什么时候做什么”，机器学习通过“不显式编程”赋予计算机能力，即提供一些案例（训练数据）,让计算机通过案例自己学习什么时候应该做什么。\nA4纸学习法。平常的一个个人习惯吧，感觉对自己比较有用，分享一下。对于某些算法，有时候可以自己花半天、一天或者两天动手推导一下，然后A4纸总结整理一下放文件夹里，备日后翻阅。这样有助于提升你的数学能力，加深对算法的理解。\n学习新技能，讲究效率。在大家智力、体力水平都相当的情况下，怎么比别人学得更快更好？这里介绍一个自己快速学习一项新技能的方法：花两周时间把两本书看两遍。具体解释是：单位时间内，把两本书看一遍不如把一本书看两遍，在不确定哪本书具有绝对优势时最好两本书都看（不要把鸡蛋放进一个篮子里）。当然，一定要快！！！对于写代码而言，看书的同时实践也是非常重要。\n6.工作\n这一点好像跟学习本身关系不大，但跟大多数学习者本身（比如我）关系很大。\n花开两朵，各表一枝。\n不少人可能跟我一样都是冲着现在计算机视觉很火、有前景又比较感兴趣，所以选择学计算机视觉，并且以后想要从事计算机视觉这方面的工作。。一定要摆正心态，找工作时可能就要跟那些计算机专业的学生们竞争了；最好从现在起，就把自己当一名程序员看待。当然你也有自己的优势，你拥有自己专业的领域知识这对某些公司来说很重要，你找工作时基本上也都应该重点考虑这些公司。，你对视觉的具体应用本身也比较了解；劣势是你缺乏计算机专业的基本素养，具体到笔试或面试中就是你基础编程能力不行。\n说到这里，大家应该都听说过“刷题”这回事。程序员应聘的特点之一就是首先面试者会考查一些基础的算法题，借此评估一下你的基本编程能力。其实计算机专业的学生在工作季前也要在leetcode等平台上刷刷题练练手，不然他们也过不了第一关。不过，对于我们非计算机专业学生而言，刷题前最好系统学习下数据结构和算法这两门课。程序=数据结构+算法，前面提到的北大《程序设计与算法》专项课程里就有这两门课。然后就是苦练刷题技能了，刷题过程中注意多总结吧。（目前我也刚走到这一阶段，所以不好多说。）\n当然我相信也有一部分人毕业之后就再也不会接触这些破玩意儿，挺好的。三十而立之年，如果我还在整天苦逼地码代码，，，呃，不敢想象，那一定不是我想要的生活。对于这些人而言，计算机视觉可能会成为你人生中的一项常识——五年后的某一天，当你坐上无人车时，一点都不会感到惊讶。当然，也祝愿它会给你的人生带来更多改变，你所学的专业对你思维上最大的影响是什么？\n说完了，有用或没用的、该说或不该说的、跟视觉相关或不相关的都说了好多，收个尾：管理好自己，。\n还有，，，如果你诚心正意把计算机视觉作为个人事业并严肃认真对待的话，可以看下这篇文章《初探计算机视觉的三个源头、兼谈人工智能｜正本清源》，知道计算机视觉不是只有现在的深度学习。"}
{"content2":"本文转载自：http://blog.csdn.net/qq_14845119/article/details/51913171\nImageNet\nImageNet是一个计算机视觉系统识别项目，是目前世界上图像识别最大的数据库。是美国斯坦福的计算机科学家李飞飞模拟人类的识别系统建立的。能够从图片识别物体。目前已经包含14197122张图像，是已知的最大的图像数据库。每年的ImageNet大赛更是魂萦梦牵着国内外各个名校和大型IT公司以及网络巨头的心。图像如下图所示，需要注册ImageNet帐号才可以下载，下载链接为http://www.image-net.org/\nPASCAL VOC\nPASCALVOC 数据集是视觉对象的分类识别和检测的一个基准测试，提供了检测算法和学习性能的标准图像注释数据集和标准的评估系统。图像如下图所示，包含VOC2007（430M），VOC2012（1.9G）两个下载版本。下载链接为http://pjreddie.com/projects/pascal-voc-dataset-mirror/\nLabelme\nLabelme是斯坦福一个学生的母亲利用休息时间帮儿子做的标注，后来便发展为一个数据集。该数据集的主要特点包括\n（1）专门为物体分类识别设计，而非仅仅是实例识别\n（2）专门为学习嵌入在一个场景中的对象而设计\n（3）高质量的像素级别标注，包括多边形框（polygons）和背景标注（segmentation masks）\n（4）物体类别多样性大，每种物体的差异性，多样性也大。\n（5）所有图像都是自己通过相机拍摄，而非copy\n（6）公开的，免费的\n图像如下图所示，需要通过matlab来下载，一种奇特的下载方式，下载链接为http://labelme2.csail.mit.edu/Release3.0/index.php\nCOCO\nCOCO是一种新的图像识别，分割和加字幕标注的数据集。主要由Tsung-Yi Lin（Cornell Tech），Genevieve Patterson （Brown），MatteoRuggero Ronchi （Caltech），Yin Cui （Cornell Tech），Michael Maire （TTI Chicago），Serge Belongie （Cornell Tech），Lubomir Bourdev （UC Berkeley），Ross Girshick （Facebook AI), James Hays (Georgia Tech),PietroPerona (Caltech)，Deva Ramanan (CMU），Larry Zitnick （Facebook AI）， Piotr Dollár （Facebook AI）等人收集而成。其主要特征如下\n（1）目标分割\n（2）通过上下文进行识别\n（3）每个图像包含多个目标对象\n（4）超过300000个图像\n（5）超过2000000个实例\n（6）80种对象\n（7）每个图像包含5个字幕\n（8）包含100000个人的关键点\n图像如下图所示，支持Matlab和Python两种下载方式，下载链接为http://mscoco.org/\nSUN\nSUN数据集包含131067个图像，由908个场景类别和4479个物体类别组成，其中背景标注的物体有313884个。图像如下图所示，下载链接为http://groups.csail.mit.edu/vision/SUN/\nCaltech\nCaltech是加州理工学院的图像数据库，包含Caltech101和Caltech256两个数据集。该数据集是由Fei-FeiLi, Marco Andreetto, Marc 'Aurelio Ranzato在2003年9月收集而成的。Caltech101包含101种类别的物体，每种类别大约40到800个图像，大部分的类别有大约50个图像。Caltech256包含256种类别的物体，大约30607张图像。图像如下图所示，下载链接为http://www.vision.caltech.edu/Image_Datasets/Caltech101/\nCorel5k\n这是Corel5K图像集，共包含科雷尔（Corel）公司收集整理的5000幅图片，故名：Corel5K，可以用于科学图像实验：分类、检索等。Corel5k数据集是图像实验的事实标准数据集。请勿用于商业用途。私底下学习交流使用。Corel图像库涵盖多个主题，由若干个CD组成，每个CD包含100张大小相等的图像，可以转换成多种格式。每张CD代表一个语义主题，例如有公共汽车、恐龙、海滩等。Corel5k自从被提出用于图像标注实验后，已经成为图像实验的标准数据集，被广泛应用于标注算法性能的比较。Corel5k由50张CD组成，包含50个语义主题。\nCorel5k图像库通常被分成三个部分：4000张图像作为训练集，500张图像作为验证集用来估计模型参数，其余500张作为测试集评价算法性能。使用验证集寻找到最优模型参数后4000张训练集和500张验证集混合起来组成新的训练集。\n该图像库中的每张图片被标注1~5个标注词，训练集中总共有374个标注词，在测试集中总共使用了263个标注词。图像如下图所示，很遗憾本人也未找到官方下载路径，于是github上传了一份，下载链接为https://github.com/watersink/Corel5K\nCIFAR（Canada Institude For Advanced Research）\nCIFAR是由加拿大先进技术研究院的AlexKrizhevsky, Vinod Nair和Geoffrey Hinton收集而成的80百万小图片数据集。包含CIFAR-10和CIFAR-100两个数据集。 Cifar-10由60000张32*32的RGB彩色图片构成，共10个分类。50000张训练，10000张测试（交叉验证）。这个数据集最大的特点在于将识别迁移到了普适物体，而且应用于多分类。CIFAR-100由60000张图像构成，包含100个类别，每个类别600张图像，其中500张用于训练，100张用于测试。其中这100个类别又组成了20个大的类别，每个图像包含小类别和大类别两个标签。官网提供了Matlab,C，python三个版本的数据格式。图像如下图所示，下载链接为http://www.cs.toronto.edu/~kriz/cifar.html\n人脸数据库:\nAFLW（Annotated Facial Landmarks in the Wild）\nAFLW人脸数据库是一个包括多姿态、多视角的大规模人脸数据库，而且每个人脸都被标注了21个特征点。此数据库信息量非常大，包括了各种姿态、表情、光照、种族等因素影响的图片。AFLW人脸数据库大约包括25000万已手工标注的人脸图片，其中59%为女性，41%为男性，大部分的图片都是彩色，只有少部分是灰色图片。该数据库非常适合用于人脸识别、人脸检测、人脸对齐等方面的研究，具有很高的研究价值。图像如下图所示，需要申请帐号才可以下载，下载链接为http://lrs.icg.tugraz.at/research/aflw/\nLFW（Labeled Faces in the Wild）\nLFW是一个用于研究无约束的人脸识别的数据库。该数据集包含了从网络收集的13000张人脸图像，每张图像都以被拍摄的人名命名。其中，有1680个人有两个或两个以上不同的照片。这些数据集唯一的限制就是它们可以被经典的Viola-Jones检测器检测到（a hummor）。图像如下图所示，下载链接为http://vis-www.cs.umass.edu/lfw/index.html#download\nAFW（Annotated Faces in the Wild）\nAFW数据集是使用Flickr（雅虎旗下图片分享网站）图像建立的人脸图像库，包含205个图像，其中有473个标记的人脸。对于每一个人脸都包含一个长方形边界框，6个地标和相关的姿势角度。数据库虽然不大，额外的好处是作者给出了其2012 CVPR的论文和程序以及训练好的模型。图像如下图所示，下载链接为http://www.ics.uci.edu/~xzhu/face/\nFDDB（Face Detection Data Set and Benchmark）\nFDDB数据集主要用于约束人脸检测研究，该数据集选取野外环境中拍摄的2845个图像，从中选择5171个人脸图像。是一个被广泛使用的权威的人脸检测平台。图像如下图所示，下载链接为http://vis-www.cs.umass.edu/fddb/\nWIDER FACE\nWIDER FACE是香港中文大学的一个提供更广泛人脸数据的人脸检测基准数据集，由YangShuo， Luo Ping ，Loy ，Chen Change ，Tang Xiaoou收集。它包含32203个图像和393703个人脸图像，在尺度，姿势，闭塞，表达，装扮，关照等方面表现出了大的变化。WIDER FACE是基于61个事件类别组织的，对于每一个事件类别，选取其中的40%作为训练集，10%用于交叉验证（cross validation），50%作为测试集。和PASCAL VOC数据集一样，该数据集也采用相同的指标。和MALF和Caltech数据集一样，对于测试图像并没有提供相应的背景边界框。图像如下图所示，下载链接为http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/\nCMU-MIT\nCMU-MIT是由卡内基梅隆大学和麻省理工学院一起收集的数据集，所有图片都是黑白的gif格式。里面包含511个闭合的人脸图像，其中130个是正面的人脸图像。图像如下图所示，没有找到官方链接，Github下载链接为https://github.com/watersink/CMU-MIT\nGENKI\nGENKI数据集是由加利福尼亚大学的机器概念实验室收集。该数据集包含GENKI-R2009a,GENKI-4K,GENKI-SZSL三个部分。GENKI-R2009a包含11159个图像，GENKI-4K包含4000个图像，分为“笑”和“不笑”两种，每个图片的人脸的尺度大小，姿势，光照变化，头的转动等都不一样，专门用于做笑脸识别。GENKI-SZSL包含3500个图像，这些图像包括广泛的背景，光照条件，地理位置，个人身份和种族等。图像如下图所示，下载链接为http://mplab.ucsd.edu，如果进不去可以，同样可以去下面的github下载，链接https://github.com/watersink/GENKI\nIJB-A (IARPA JanusBenchmark A)\nIJB-A是一个用于人脸检测和识别的数据库，包含24327个图像和49759个人脸。图像如下图所示，需要邮箱申请相应帐号才可以下载，下载链接为http://www.nist.gov/itl/iad/ig/ijba_request.cfm\nMALF (Multi-Attribute Labelled Faces)\nMALF是为了细粒度的评估野外环境中人脸检测模型而设计的数据库。数据主要来源于Internet，包含5250个图像，11931个人脸。每一幅图像包含正方形边界框，俯仰、蜷缩等姿势等。该数据集忽略了小于20*20的人脸，大约838个人脸，占该数据集的7%。同时，该数据集还提供了性别，是否带眼镜，是否遮挡，是否是夸张的表情等信息。图像如下图所示，需要申请才可以得到官方的下载链接，链接为http://www.cbsr.ia.ac.cn/faceevaluation/\nMegaFace\nMegaFace资料集包含一百万张图片，代表690000个独特的人。所有数据都是华盛顿大学从Flickr（雅虎旗下图片分享网站）组织收集的。这是第一个在一百万规模级别的面部识别算法测试基准。 现有脸部识别系统仍难以准确识别超过百万的数据量。为了比较现有公开脸部识别算法的准确度，华盛顿大学在去年年底开展了一个名为“MegaFace Challenge”的公开竞赛。这个项目旨在研究当数据库规模提升数个量级时，现有的脸部识别系统能否维持可靠的准确率。图像如下图所示，需要邮箱申请才可以下载，下载链接为http://megaface.cs.washington.edu/dataset/download.html\n300W\n300W数据集是由AFLW，AFW，Helen，IBUG，LFPW，LFW等数据集组成的数据库。图像如下图所示，需要邮箱申请才可以下载，下载链接为http://ibug.doc.ic.ac.uk/resources/300-W/\nIMM Data Sets\nIMM人脸数据库包括了240张人脸图片和240个asf格式文件（可以用UltraEdit打开，记录了58个点的地标），共40个人（7女33男），每人6张人脸图片，每张人脸图片被标记了58个特征点。所有人都未戴眼镜,图像如下图所示，下载链接为http://www2.imm.dtu.dk/~aam/datasets/datasets.html\nMUCT Data Sets\nMUCT人脸数据库由3755个人脸图像组成，每个人脸图像有76个点的地标（landmark），图片为jpg格式，地标文件包含csv,rda,shape三种格式。该图像库在种族、关照、年龄等方面表现出更大的多样性。具体图像如下图所示，下载链接为http://www.milbo.org/muct/\nORL  (AT&T Dataset)\nORL数据集是剑桥大学AT&T实验室收集的一个人脸数据集。包含了从1992.4到1994.4该实验室的成员。该数据集中图像分为40个不同的主题，每个主题包含10幅图像。对于其中的某些主题，图像是在不同的时间拍摄的。在关照，面部表情（张开眼睛，闭合眼睛，笑，非笑），面部细节（眼镜）等方面都变现出了差异性。所有图像都是以黑色均匀背景，并且从正面向上方向拍摄。\n其中图片都是PGM格式，图像大小为92*102，包含256个灰色通道。具体图像如下图所示，下载链接为http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\nVGG Face dataset\n该数据集包含了2622个不同的人，每个人包含1000张图片，是一个训练人脸识别的大的数据集，官网提供了每个图片的URL，需要自己解析下载，当然有些链接是需要翻墙的，要不可能下载不全哦。\n下载链接：http://www.robots.ox.ac.uk/~vgg/data/vgg_face/\nCASIA WebFace Database\n该数据集为中科院自动化所，李子青老师组开源的数据集，包含了10575类人，一共494414张图片，其中有3类人和lfw中的一样。该数据集主要用于人脸识别。图像都是著名电影中crop而出的，每个图片的大小都是250*250，每个类下面都有3张以上的图片，非常适合做人脸识别的训练。现在发paper比较一致的做法都是在该数据集上训练下，再在lfw数据集做个测试。需要邮箱申请，下载链接：http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html\nCelebA（Large-scale CelebFaces Attributes dataset）\n该数据集为香港中文大学汤晓鸥老师组开源的数据集，主要包含了5个关键点，40个属性值等，包含了202599张图片，图片都是高清的名人图片，可以用于人脸检测，5点训练，人脸头部姿势的训练等。下载链接：http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\nYouTuBe Faces DB\n该数据集主要用于非约束条件下的视频中人脸识别，姿势判定等。该数据集包含1595个不同人的3425个视频，平均每个人的类别包含了2.15个视频，每个类别最少包含48帧，最多包含6070帧，平均包含181.3帧。下载链接：http://www.cslab.openu.ac.il/agas/，或者，http://www.cslab.openu.ac.il/download/，如果没有效果，可以尝试filezilla下载，\nserver:agas.openu.ac.il\nPath: /v/data9/cslab/wolftau/\nfilezilla模式设置为\"Transfer mode\"\n行人检测数据库\nINRIA Person Dataset\nInria数据集是最常使用的行人检测数据集。其中正样本（行人）为png格式，负样本为jpg格式。里面的图片分为只有车，只有人，有车有人，无车无人四个类别。图片像素为70*134，96*160，64*128等。具体图像如下图所示，下载链接为http://pascal.inrialpes.fr/data/human/\nCaltechPedestrian Detection Benchmark\n加州理工学院的步行数据集包含大约包含10个小时640x480 30Hz的视频。其主要是在一个在行驶在乡村街道的小车上拍摄。视频大约250000帧（在137个约分钟的长段），共有350000个边界框和2300个独特的行人进行了注释。注释包括包围盒和详细的闭塞标签之间的时间对应关系。更多信息可在其PAMI 2012 CVPR 2009标杆的论文获得。具体图像如下图所示，下载链接为http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\nMIT cbcl (center for biological and computational learning)Pedestrian Data\n该数据集主要包含2个部分，一部分为128*64的包含924个图片的ppm格式的图片，另一部分为从打图中分别切割而出的小图，主要包含胳膊，脑袋，脚，腿，头肩，身体等。具体图像如下图所示，下载链接为http://cbcl.mit.edu/software-datasets/PedestrianData.html，需要翻墙才可以。\n年龄，性别数据库\nAdience\n该数据集来源为Flickr相册，由用户使用iPhone5或者其它智能手机设备拍摄，同时具有相应的公众许可。该数据集主要用于进行年龄和性别的未经过滤的面孔估计。同时，里面还进行了相应的landmark的标注。是做性别年龄估计和人脸对齐的一个数据集。图片包含2284个类别和26580张图片。具体图像如下图所示，下载链接为http://www.openu.ac.il/home/hassner/Adience/data.html#agegender\n车辆数据库\nKITTI（Karlsruhe Institute ofTechnology and Toyota Technological Institute）\nKITTI包含7481个训练图片和7518个测试图片。所有图片都是真彩色png格式。该数据集中标注了车辆的类型，是否截断，遮挡情况，角度值，2维和3维box框，位置，旋转角度，分数等重要的信息，绝对是做车载导航的不可多得的数据集。具体图像如下图所示，下载链接为http://www.cvlibs.net/datasets/kitti/\n字符数据库\nMNIST（Mixed National Instituteof Standards and Technology）\nMNIST是一个大型的手写数字数据库，广泛用于机器学习领域的训练和测试，由纽约大学的Yann LeCun整理。MNIST包含60000个训练集，10000个测试集，每张图都进行了尺度归一化和数字居中处理，固定尺寸大小为28*28。具体图像如下图所示，下载链接为http://yann.lecun.com/exdb/mnist/\n人群密度估计数据库\nUCSD\n该数据集分为，UCSD Pedestrain ,people annotation，people counting三个部分，下载链接为：http://visal.cs.cityu.edu.hk/downloads/\nPETS\n该数据集包含S0，S1，S2，S3四个子集，S0为训练数据，S1为行人计数和密度估计，S2为行人跟踪，S3为流分析和事件识别，下载链接为：http://www.cvg.reading.ac.uk/PETS2009/a.html\n\nMall dataset\n下载链接为：http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html\nShanghaiTech_Crowd_Counting_Dataset:\n该数据集为上海科技大学研究生张营营，在其2016cvpr中所使用的数据集，数据集分为A,B两部分，每一部分都分好了train和test，下载链接为:https://pan.baidu.com/s/1gfyNBTh\nUCF_CC_50：\n官方的我也没找到，自己传一个自己的，下载链接为:http://download.csdn.net/detail/qq_14845119/9800218\n人头检测数据库\nHollywoodHeads dataset\n该数据集为从视频中截取的图片，包含224740张jpeg格式图片，还有xml格式的标注，和VOC的标注方式一样。下载链接为:http://www.di.ens.fr/willow/research/headdetection/release/HollywoodHeads.zip\n车型识别数据库\nCompCars\n该数据集包含208826个车辆图片工1716种最新款的车辆型号，是由实际场景和网上图片组成的数据集。包含了车辆的，\ncar hierarchy（car make ,car model,year of manufacture），\ncar attribute（maximum speed, displacement, num of doors, num of seats, type of car），\nviewpoints（front(F), rear(R), side(S), front-side(FS), rear-side(RS)），\ncar parts（headlight ,taillight, fog light, air intake, console, steering wheel, dashboard, gear lever ）\n等属性。下载链接为，http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html\n\n持续跟新中……"}
{"content2":"转载 ：https://blog.csdn.net/ksws0292756/article/details/78881839\n感觉写的很明白，留着看看。\n以下内容整理自 2017 年 6 月 29 日由“趣直播–知识直播平台”邀请的嘉宾实录。\n分享嘉宾: 罗韵\n目前，人工智能，机器学习，深度学习，计算机视觉等已经成为新时代的风向标。这篇文章主要介绍了下面几点：\n第一点，如果说你要入门计算机视觉，需要了解哪一些基础知识？\n第二点，既然你要往这方面学习，你要了解的参考书籍，可以学习的一些公开课有哪些？\n第三点，可能是大家都比较感兴趣的，就是计算机视觉作为人工智能的一个分支，它不可避免的要跟深度学习做结合，而深度学习也可以说是融合到了计算机视觉、图像处理，包括我们说的自然语言处理，所以本文也会简单介绍一下计算机视觉与深度学习的结合。\n第四点，身处计算机领域，我们不可避免的会去做开源的工作，所以本文会给大家介绍一些开源的软件。\n第五点，要学习或者研究计算机视觉，肯定是需要去阅读一些文献的，那么我们如何开始阅读文献，以及慢慢的找到自己在这个领域的方向，这些都会在本文理进行简单的介绍。\n1.基础知识\n接下来要介绍的，第一点是计算机视觉是什么意思，其次是图像、视频的一些基础知识。包括摄像机的硬件，以及 CPU 和 GPU 的运算。\n在计算机视觉里面，我们也不可避免的会涉及到考虑去使用 CPU 还是使用 GPU 去做运算。然后就是它跟其他学科的交叉，因为计算机视觉可以和很多的学科做交叉，而且在做学科交叉的时候，能够发挥的意义和使用价值也会更大。另外，对于以前并不是做人工智能的朋友，可能是做软件开发的，想去转型做计算机视觉，该如何转型？需要学习哪些编程语言以及数学基础？这些都会在第一小节给大家介绍。\n1.0 什么是计算机视觉\n计算机视觉是一门研究如何使机器“看”的科学。\n更进一步的说，就是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给一起检测的图像\n作为一个科学学科，计算机视觉研究相关的理论和技术，视图建立能够从图像或者多维数据中获取“信息”的人工智能系统。\n目前，非常火的VR、AR，3D处理等方向，都是计算机视觉的一部分。\n计算机视觉的应用\n无人驾驶\n无人安防\n人脸识别\n车辆车牌识别\n以图搜图\nVR/AR\n3D重构\n医学图像分析\n无人机\n其他\n了解了计算机视觉是什么之后，给大家列了一下当前计算机视觉领域的一些应用，几乎可以说是无处不在，而且当前最火的所有创业的方向都涵盖在里面了。其中包括我们经常提到的无人驾驶、无人安防、人脸识别。人脸识别相对来说已经是一个最成熟的应用领域了，然后还有文字识别、车辆车牌识别，还有以图搜图、 VR/AR，还包括 3D 重构，以及当下很有前景的领域–医学图像分析。\n医学图像分析他在很早就被提出来了，已经研究了很久，但是现在得到了一个重新的发展，更多的研究人员包括无论是做图像的研究人员，还是本身就在医疗领域的研究人员，都越来越关注计算机视觉、人工智能跟医学图像的分析。而且在当下，医学图像分析也孕育了不少的创业公司，这个方向的未来前景还是很值得期待的。然后除此之外还包括无人机，无人驾驶等，都应用到了计算机视觉的技术。\n1.1图像和视频，你要知道的概念\n图像\n一张图片包含了：维数、高度、宽度、深度、通道数、颜色格式、数据首地址、结束地址、数据量等等。\n图像深度：存储每个像素所用的位数（bits）\n当一个像素占用的位数越多时，它所能表现的颜色就更多，更丰富。\n举例：一张400*400的8位图，这张图的原始数据量是多少？像素值如果是整型的话，取值范围是多少？\n1，原始数据量计算：400 * 400 * ( 8/8 )=160,000Bytes\n(约为160K)\n2，取值范围：2的8次方，0~255\n图片格式与压缩：常见的图片格式JPEG，PNG，BMP等本质上都是图片的一种压缩编码方式\n举例：JPEG压缩\n1，将原始图像分为8*8的小块，每个block里有64pixels。\n2，将图像中每个8*8的block进行DCT变换（越是复杂的图像，越不容易被压缩）\n3，不同的图像被分割后，每个小块的复杂度不一样，所以最终的压缩结果也不一样\n视频\n原始视频=图片序列。\n视频中的每张有序图片称为“帧（frame）”。压缩后的视频，会采取各种算法减少数据的容量，其中IPB就是最常见的。\nI帧：表示关键帧，可以理解为这一幅画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）\nP帧：表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧画面差别的数据）\nB帧表示双向差别帧，记录的本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，要通过前后画面与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码比较麻烦。\n码率：码率越大，体积越大；码率越小，体积越小。\n码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。也就是取样率（并不等同于采样率，采样率用的单位是Hz，表示每秒采样的次数），单位时间内取样率越大，精度就越高，处理出来的文件就越接近原始文件，但是文件体积与取样率是成正比的，所以几乎所有的编码格式重视的都是如何用最低的码率达到最少的失真，围绕这个核心衍生出来cbr（固定码率）与vbr（可变码率），码率越高越清晰，反之则画面粗糙而且多马赛克。\n帧率\n影响画面流畅度，与画面流畅度成正比：帧率越大，画面越流畅；帧率越小，画面越有跳动感。如果码率为变量，则帧率也会影响体积，帧率越高，每秒钟经过的画面就越多，需要的码率也越高，体积也越大。\n帧率就是在一秒钟时间里传输的图片的帧数，也可以理解为图形处理器每秒钟刷新的次数。\n分辨率\n影响图像大小，与图像大小成正比；分辨率越高，图像越大；分辨率越低，图像越小。\n清晰度\n在码率一定的情况下，分辨率与清晰度成反比关系：分辨率越高，图像越不清晰，分辨率越低，图像越清晰\n在分辨率一定的情况下，码率与清晰度成正比关系：码率越高，图像越清晰；码率越低，图像越不清晰\n带宽、帧率\n例如在ADSL线路上传输图像，上行带宽只有512Kbps，但要传输4路CIF分辨率的图像。按照常规，CIF分辨率建议码率是512Kbps，那么照此计算就只能传一路，降低码率势必会影响图像质量。那么为了确保图像质量，就必须降低帧率，这样一来，即便降低码率也不会影响图像质量，但在图像的连贯性上会有影响。\n1.2摄像机\n摄像机的分类：\n监控摄像机（网络摄像机和摸你摄像机）\n不同行业需求的摄像机（超宽动态摄像机、红外摄像机、热成像摄像机等）\n智能摄像机\n工业摄像机\n当前的摄像机硬件我们可以分为监控摄像机、专业行业应用的摄像机、智能摄像机和工业摄像机。而在监控摄像机里面，当前用的比较多的两个类型一个叫做网络摄像机，一个叫做模拟摄相机，他们主要是成像的原理不太一样。\n网络摄像机一般比传统模拟摄相机的清晰度要高一些，模拟摄像机当前应该说是慢慢处于一个淘汰的状态，它可以理解为是上一代的监控摄像机，而网络摄像机是当前的一个主流的摄相机，大概在 13 年的时候，可能市场上 70% 到 80% 多都是模拟摄像机，而现在可能 60% 到 70% 都是的网络摄像机。\n除此之外，不同的行业其时会有特定的相机，想超宽动态摄像机以及红外摄像机、热成像摄像机，都是在专用的特定的领域里面可能用到的，而且他获得的画面跟图像是完全不一样的。如果我们要做图像处理跟计算机视觉分析，什么样的相机对你更有利，我们要学会利用硬件的优势。\n如果是做研究的话一般是可以控制我们用什么样的摄相机，但如果是在实际的应用场景，这个把控的可能性会稍微小一点，但是在这里你要知道，有些问题可能你换一种硬件，它就能够很好的被解决，这是一个思路。\n还有些问题你可能用算法弄了很久也没能解决，甚至是你的效率非常差，成本非常高，但是稍稍换一换硬件，你会发现原来的问题都不存在了，都被很好的解决了，这个就是硬件对你的一个新的处境了。\n包括现在还有智能摄像机、工业摄像机，工业摄像机一般的价格也会比较贵，因为他专用于各种工业领域，或者是做一些精密仪器，高精度高清晰度要求的摄像机。\n1.3 CPU和GPU\n接下来给大家讲一下 CPU 跟 GPU，如果说你要做计算机视觉跟图像处理，那么肯定跳不过 GPU 运算，GPU 运算这一块可能也是接下来需要学习或者自学的一个知识点。\n因为可以看到，当前大部分关于计算机视觉的论文，很多实现起来都是用 GPU 去实现的，但是在应用领域，因为 GPU 的价格比较昂贵，所以 CPU 的应用场景相对来说还是占大部分。\n而 CPU 跟 GPU 的差别主要在哪里呢？ 它们的差别主要可以在两个方面去对比，第一个叫性能，第二个叫做吞吐量。\n性能，换言之，性能会换成另外一个单词叫做 Latency（低延时性）。低延时性就是当你的性能越好，你处理分析的效率越高，相当于你的延时性就越低，这个是性能。另外一个叫做吞吐量，吞吐量的意思就是你同时能够处理的数据量。\n而 CPU 跟 GPU 的差别在哪里呢？主要就在于这两个地方，CPU 它是一个高性能，就是超低延时性的，他能够快速的去做复杂运算，并且能达到一个很好的性能要求。而 GPU是以一个叫做运算单元为格式的，所以他的优点不在于低延时性，因为他确实不善于做复杂运算，他每一个处理器都非常的小，相对来说会很弱，但是它可以让它所有的弱处理器，同时去做处理，那相当于他就能够同时处理大量的数据，那这个就意味着它的吞吐量非常大，所以 CPU重视的是性能，GPU重视的是吞吐量。\n所以大部分时候，GPU 他会跟另外一个词语联系在一起，叫做并行计算，意思就是它可以同时做大量的线程运算，为什么图像会特别适合用 GPU 运算呢？这是因为 GPU 它最开始的设计就是叫做图形处理单元，它的意思就是我可以把每一个像素，分割为一个线程去运算，每一个像素只做一些简单的运算，这个就是最开始图形处理器出现的原理。\n它要做图形渲染的时候，要计算的是每一个像素的变换。所以每一个像素变换的计算量是很小很小的，可能就是一个公式的计算，计算量很少，它可以放在一个简单的计算单元里面去做计算，那这个就是 CPU 跟 GPU 的差别。\n基于这样的差别，我们才会去设计什么时候用 CPU，什么时候用 GPU。如果你当前设计的算法，它的并行能力不是很强，从头到尾从上到下都是一个复杂的计算，没有太多可并性的地方，那么即使你用了 GPU，也不能帮助你很好提升计算性能。\n所以，不要说别人都在用 GPU 那你就用 GPU，我们要了解的是为什么要用 GPU ，以及什么样的情况下用 GPU，它效果能够发挥出来最好。\n1.4计算机视觉与其他学科的关系\n计算机视觉目前跟其他学科的关系非常的多，包括机器人，以及刚才提到的医疗、物理、图像、卫星图片的处理，这些都会经常使用到计算机视觉，那这里呢，最常问到的问题无非就是有三个概念，一个叫做计算机视觉，一个叫做机器视觉，一个叫做图像处理，那这三个东西有什么区别呢？\n这三个东西的区别还是挺因人而异的，每一个研究人员对它的理解都不一样。\n首先，Image Processing更多的是图形图像的一些处理，图像像素级别的一些处理，包括 3D 的处理，更多的会理解为是一个图像的处理；而机器视觉呢，更多的是它还结合到了硬件层面的处理，就是软硬件结合的图形计算的能力，跟图形智能化的能力，我们一般会理解为他就是所谓的机器视觉。\n而我们今天所说的计算机视觉，更多的是偏向于软件层面的计算机处理，而且不是说做图像的识别这么简单，更多的还包括了对图像的理解，甚至是对图像的一些变换处理，当前我们涉及到的一些图像的生成，也是可以归类到这个计算机视觉领域里面的。\n所以说计算机视觉它本身的也是一个很基础的学科，可以跟各个学科做交叉，同时，它自己内部也会分的比较细，包括机器视觉、图像处理。\n1.5 编程语言AND数学基础\n这一部分的内容可以参见《非计算机专业，如何学习计算机视觉》\n2.参考书籍和公开课\n参考书\n第一本叫《Computer Vision：Models, Learning and Inference》written by Simon J.D. prince，这个主要讲的更适合入门级别的，因为这本书里面配套了非常多的代码，Matlab 代码，C 的代码都有，配套了非常多的学习代码，以及参考资料、文献，都配得非常详细，所以它很适合入门级别的同学去看。\n第二本《Computer Vision：Algorithms and Applications》written by Richard Szeliski，这是一本非常经典，非常权威的参考资料，这本书不是用来看的，是用来查的，类似于一本工具书，它是涵盖面最广的一本参考书籍，所以一般会可以当成工具书去看，去查阅。\n第三本《OpenCV3编程入门》作者：毛星云，冷雪飞 ，如果想快速的上手去实现一些项目，可以看看这本书，它可以教你动手实现一些例子，并且学习到 OpenCV 最经典、最广泛的计算机视觉开源库。\n公开课：\nStanford CS223B\n比较适合基础，适合刚刚入门的同学，跟深度学习的结合相对来说会少一点，不会整门课讲深度学习，而是主要讲计算机视觉，方方面面都会讲到。\nStanford CS231N\n这个应该不用介绍了，一般很多人都知道，这个是计算机视觉和深度学习结合的一门课，我们上 YouTube 就能够看到，这门课的授课老师就是李飞飞老师，如果说不知道的话可以查一下，做计算机视觉的话，此人算是业界和学术界的“执牛耳”了。\n3.需要了解的深度学习知识\n深度学习没有太多的要讲的，不是说内容不多，是非常多，这里只推荐一本书给大家，这本书是去年年底才出的，是最新的一本深度学习的书，它讲得非常全面，从基础的数学，到刚才说的概率学、统计学、机器学习以及微积分、线性几何的知识点，非常的全面。\n4.需要了解和学习的开源软件\nOpenCV\n它是一个很经典的计算机视觉库，实现了很多计算机视觉的常用算法。可以帮助大家快速上手。\nCaffe\n如果是做计算机视觉的话，比较建议 Caffe。Caffe 更擅长做的是卷积神经网络，卷积神经网络在计算机视觉里面用的是最多的。\n所以无论你后面学什么样其它的开源软件， Caffe 是必不可免的，因为学完 Caffe 之后你会发现，如果你理解了 Caffe，会用 Caffe，甚至是有能力去改它的源代码，你就会发现你对深度学习有了一个质的飞跃的理解。\nTensorFlow\nTensorFlow 最近很火，但是它的入门门槛不低，你要学会使用它需要的时间远比其他所有的软件都要多，其次就是它当前还不是特别的成熟稳定，所以版本之间的更新迭代非常的多，兼容性并不好，运行效率还有非常大的提升空间。\n5.如何阅读相关的文献\n先熟悉所在方向的发展历程，然后精读历程中的里程碑式的文献。\n例如：深度学习做目标检测，RCNN，Fast RCNN，Faster RCNN，SPPNET，SSD和YOLO这些模型肯定是要知道的。又例如，深度学习做目标跟踪，DLT，SO-DLT等。\n计算机视觉的顶会：\nICCV：International Conference on Computer Vision，国际计算机视觉大会\nCVPR：International Conference on Computer Vision and Pattern Recognition，国际计算机视觉与模式识别大会\nECCV：European Conference on Computer Vision，欧洲计算机视觉大会\n除了顶会之外呢，还有顶刊。像 PAMI、IJCV，这些都是顶刊，它代表着这个领域里面最尖端最前沿以及当下的研究方向。"}
{"content2":"与所有其它学术领域都不同，计算机科学使用会议而不是期刊作为发表研究成果的主要方式。目前国外计算机界评价学术水平主要看在顶级学术会议上发表的论文。特别是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。（但中国目前的国情不同于国外，我国主要看在学术期刊上发表的SCI论文。这种“一切以SCI期刊为评价标准”的做法已有不少批评。）\n会议论文比期刊论文更重要的原因是：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n作为刚入门的CV新人，有必要记住计算机视觉方面的三大顶级国际会议：ICCV，CVPR和ECCV，统称为ICE。\nICCV的全称是International Comference on Computer Vision（上一篇文章介绍我自己的id的时候介绍过，呵呵），正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。\nCVPR的全称是International Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster挑自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。\nICCV/CVPR/ECCV三个顶级会议, 都在一流会议行列, 没有必要给个高下. 有些us的人认为ICCV/CVPR略好于ECCV,而欧洲人大都认为ICCV/ECCV略好于CVPR。\n笔者就个人经验浅谈三会异同, 以供大家参考和讨论. 三者乃cv领域的旗舰和风向标,其oral paper (包括best paper) 代表当年度cv的最高水准, 在此引用Harry Shum的一句话, 想知道某个领域在做些什么, 找最近几年此领域的proceeding看看就知道了. ICCV/CVPR由IEEE Computer Society牵头组织, ECCV好像没有专门负责的组织. CVPR每年(除2002年)都在美国开, ECCV每两年开一次,仅限欧洲, ICCV也是每两年一次, 各洲轮值. 基本可以保证每年有两个会议开, 这样研究者就有两次跻身牛会的机会.\n就录取率而言, 三会都有波动. 如ICCV2001录取率>30%, 且出现两个人(华人)各有三篇第一作者的paper的情况, 这在顶级牛会是不常见的 (灌水嫌疑). 但是, ICCV2003, 2005\n两次录取率都很低, 大约20%左右. ECCV也是类似规律, 在2004年以前都是>30%, 2006年降低到20%左右. CVPR的录取率近年来一直偏高, 从2004年开始一直都在[25%,30%].最近一次CVPR2006是28.1%, CVPR2007还不知道统计数据. 笔者猜测为了维持录取paper的绝对数量, 当submission少的时候录取率偏高, 反之偏低, 近几年三大会议的投稿数量全部超过1000, 相对2000年前, 三会录取率均大幅度降低, 最大幅度50%->20%. 对录取率走势感兴趣的朋友, 可参考http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的),http://www.adaptivebox.net/research/bookmark/CICON_stat.html .\n显然, 投入cv的人越来越多,这个领域也是越来越大, 这点颇不似machine learning一直奉行愚蠢的小圈子主义. 另外一点值得注意, ICCV/ECCV只收vision相关的topic, 而cvpr会收少量的pattern recognition paper, 如fingerprint等, 但是不收和image/video完全不占边的pr paper,如speech recognition等. 我一个朋友曾经review过一篇投往CVPR的speech的paper, 三个reviewer一致拒绝, 其中一个reviewer搞笑的指出, 你这篇paper应该是投ICASSP被据而转投CVPR的. 就topic而言, CVPR涵盖最广. 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会, 故CVPR会优先接收很多来自us的paper (让大家都happy).\n以上对三会的分析对我们投paper是很有指导作用的. 目前的research我想绝大部分还是纸上谈兵, 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程. 故了解投paper的一些基本技巧, 掌握领域的走向和热点, 是非常必要的. 避免做无用功,选择切合的topic, 改善presentation, 注意格式 (遵守规定的模板), 我想这是很多新手需要注意的问题. 如ICCV2007明文规定不写summary page直接reject, 但是仍然有人忽视, 这是相当不值得的."}
{"content2":"来源：机器之心\n本文长度为5000字，建议阅读7分钟\n为你分享一份2016至2017年计算机视觉领域的研究成果。\nThe M Tank编辑了一份报告《A Year in Computer Vision》，记录了2016至2017年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。该材料共包括四大部分，在本文中我们对第一部分做了编译介绍：\n内容目录\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率、风格迁移、着色\n动作识别\n第三部分\n3D 目标\n人体姿势估计\n3D 重建\n其他未分类 3D\n总结\n第四部分\n卷积架构\n数据集\n不可分类的其他材料与有趣趋势\n结论\n完整PDF地址：\nhttp://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf\n简介\n计算机视觉是关于研究机器视觉能力的学科，或者说是使机器能对环境和其中的刺激进行可视化分析的学科。机器视觉通常涉及对图像或视频的评估，英国机器视觉协会（BMVA）将机器视觉定义为「对单张图像或一系列图像的有用信息进行自动提取、分析和理解」。\n对我们环境的真正理解不是仅通过视觉表征就可以达成的。更准确地说，是视觉线索通过视觉神经传输到主视觉皮层，然后由大脑以高度特征化的形式进行分析的过程。从这种感觉信息中提取解释几乎包含了我们所有的自然演化和主体经验，即进化如何令我们生存下来，以及我们如何在一生中对世界进行学习和理解。\n从这方面来说，视觉过程仅仅是传输图像并进行解释的过程，然而从计算的角度看，图像其实更接近思想或认知，涉及大脑的大量功能。因此，由于跨领域特性很显著，很多人认为计算机视觉是对视觉环境和其中语境的真实理解，并将引领我们实现强人工智能。\n不过，我们目前仍然处于这个领域发展的胚胎期。这篇文章的目的在于阐明 2016 至 2017 年计算机视觉最主要的进步，以及这些进步对实际应用的促进。\n为简单起见，这篇文章将仅限于基本的定义，并会省略很多内容，特别是关于各种卷积神经网络的设计架构等方面。\n这里推荐一些学习资料，其中前两个适用与初学者快速打好基础，后两个可以作为进阶学习：\nAndrej Karpathy:「What a Deep Neural Network thinks about your #selfie」，这是理解 CNN 的应用和设计功能的最好文章 [4]。\nQuora:「what is a convolutional neural network?」，解释清晰明了，尤其适合初学者 [5]。\nCS231n: Convolutional Neural Networks for Visual Recognition，斯坦福大学课程，是进阶学习的绝佳资源 [6]。\nDeep Learning(Goodfellow,Bengio&Courville,2016)，这本书在第 9 章提供了对 CNN 的特征和架构设计等详尽解释，网上有免费资源 [7]。\n对于还想进一步了解神经网络和深度学习的，我们推荐：\nNeural Networks and Deep Learning(Nielsen,2017)，这是一本免费在线书籍，可为读者提供对神经网络和深度学习的复杂性的直观理解。即使只阅读了第 1 章也可以帮助初学者透彻地理解这篇文章。\n下面我们先简介本文的第一部分，这一部分主要叙述了目标分类与定位、目标检测与目标追踪等十分基础与流行的计算机视觉任务。而后机器之心将陆续分享 Benjamin F. Duffy 和 Daniel R. Flynn 后面 3 部分对计算机视觉论述，包括第二部分的语义分割、超分辨率、风格迁移和动作识别，第三部分三维目标识别与重建、和第四部分卷积网络的架构与数据集等内容。\n基础的计算机视觉任务\n分类/定位\n图像分类任务通常是指为整张图像分配特定的标签，如下左图整张图像的标签为 CAT。而定位是指找到识别目标在图像中出现的位置，通常这种位置信息将由对象周围的一些边界框表示出来。目前 ImageNet [9] 上的分类/定位的准确度已经超过了一组训练有素的人类 [10]。因此相对于前一部分的基础，我们会着重介绍后面如语义分割、3D 重建等内容。\n图 1：计算机视觉任务，来源 cs231n 课程资料。\n然而随着目标类别 [11] 的增加，引入大型数据集将为近期的研究进展提供新的度量标准。在这一方面，Keras [12] 创始人 Francois Chollet 将包括 Xception 等架构和新技术应用到谷歌内部的大型数据集中，该数据集包含 1.7 万个目标类别，共计 350M（Million）的多类别图像。\n图 2：ILSVRC 竞赛中，分类/定位的逐年错误率，来源 Jia Deng (2016)，ILSVRC2016。\nImageNet LSVRC（2016）亮点：\n场景分类是指用「温室」、「体育场」和「大教堂」等特定场景对图像进行分类。ImageNet 去年举办了基于 Places2[15] 子数据的场景分类挑战赛，该数据集有 365 个场景共计 8 百万 训练图像。海康威视 [16] 选择了深度类 Inception 的网络和并不太深的 ResNet，并利用它们的集成实现 9% 的 Top-5 误差率以赢得竞赛。\nTrimps-Soushen 以 2.99% 的 Top-5 分类误差率和 7.71% 的定位误差率赢得了 ImageNet 分类任务的胜利。该团队使用了分类模型的集成（即 Inception、Inception-ResNet、ResNet 和宽度残差网络模块 [17] 的平均结果）和基于标注的定位模型 Faster R-CNN [18] 来完成任务。训练数据集有 1000 个类别共计 120 万的图像数据，分割的测试集还包括训练未见过的 10 万张测试图像。\nFacebook 的 ResNeXt 通过使用从原始 ResNet [19] 扩展出来的新架构而实现了 3.03% 的 Top-5 分类误差率。\n目标检测\n目标检测（Object Detection）即如字面所说的检测图像中包含的物体或目标。ILSVRC 2016 [20] 对目标检测的定义为输出单个物体或对象的边界框与标签。这与分类/定位任务不同，目标检测将分类和定位技术应用到一张图像的多个目标而不是一个主要的目标。\n图 3：仅有人脸一个类别的目标检测。图为人脸检测的一个示例，作者表示目标识别的一个问题是小物体检测，检测图中较小的人脸有助于挖掘模型的尺度不变性、图像分辨率和情景推理的能力，来源 Hu and Ramanan (2016, p. 1)[21]。\n目标识别领域在 2016 年主要的趋势之一是转向更快、更高效的检测系统。这一特性在 YOLO、SSD 和 R-FCN 方法上非常显著，它们都倾向于在整张图像上共享计算。因此可以将它们与 Fast/Faster R-CNN 等成本较高的子网络技术区分开开来，这些更快和高效的检测系统通常可以指代「端到端的训练或学习」。\n这种共享计算的基本原理通常是避免将独立的算法聚焦在各自的子问题上，因为这样可以避免训练时长的增加和网络准确度的降低。也就是说这种端到端的适应性网络通常发生在子网络解决方案的初始之后，因此是一种可回溯的优化（retrospective optimisation）。然而，Fast/Faster R-CNN 技术仍然非常有效，仍然广泛用于目标检测任务。\nSSD：Single Shot MultiBox Detector[22] 利用封装了所有必要计算并消除了高成本通信的单一神经网络，以实现了 75.1% mAP 和超过 Faster R-CNN 模型的性能（Liu et al. 2016）。\n我们在 2016 年看到最引人注目的系统是「YOLO9000: Better, Faster, Stronger」[23]，它引入了 YOLOv2 和 YOLO9000 检测系统 [24]。YOLOv2 很大程度上提升了 2015 年提出的 YOLO 模型 [25] 性能，它能以非常高的 FPS（使用原版 GTX Titan X 在低分辨率图像上达到 90FPS）实现更好的结果。除了完成的速度外，系统在特定目标检测数据集上准确度要优于带有 ReNet 和 SSD 的 Faster RCNN。\nYOLO9000 实现了检测和分类的联合训练，并将其预测泛化能力扩展到未知的检测数据上，即它能检测从未见过的目标或物体。YOLO9000 模型提供了 9000 多个类别的实时目标检测，缩小了分类和检测数据集间的鸿沟。该模型其它详细的信息和预训练模型请查看：http://pjreddie.com/darknet/yolo/。\nFeature Pyramid Networks for Object Detection [27] 是 FAIR [28] 实验室提出的，它能利用「深度卷积网络的内部多尺度、金字塔型的层级结构构建具有边际额外成本的特征金字塔」，这意味着表征能更强大和快速。Lin et al. (2016) 在 COCO[29] 数据集上实现了顶尖的单模型结果。若与基础的 Faster R-CNN 相结合，将超过 2016 年最好的结果。\nR-FCN：Object Detection via Region-based Fully Convolutional Networks [30]，这是另一种在图像上避免应用数百次高成本的各区域子网络方法，它通过使基于区域的检测器在整张图像上进行全卷积和共享计算。「我们每张图像的测试时间只需要 170ms，要比 Faster R-CNN 快 2.5 到 20 倍」(Dai et al., 2016)。\n图 4：目标检测中的准确率权衡，来源 Huang et al. (2016, p. 9)[31]。\n注意：Y 轴表示的是平均准确率（mAP），X 轴表示不同元架构（meta-architecture）的各种特征提取器（VGG、MobileNet...Inception ResNet V2）。此外，mAP small、medium 和 large 分别表示对小型、中型和大型目标的检测平均准确率。即准确率是按「目标尺寸、元架构和特征提取器」进行分层的，并且图像的分辨率固定为 300。虽然 Faster R-CNN 在上述样本中表现得更好，但是这并没有什么价值，因为该元架构相比 R-FCN 来说慢得多。\nHuang et al. (2016)[32] 的论文提供了 R-FCN、SSD 和 Faster R-CNN 的深度性能对比。由于机器学习准确率对比中存在的问题，这里使用的是一种标准化的方法。这些架构被视为元架构，因为它们可以组合不同的特征提取器，比如 ResNet 或 Inception。\n论文的作者通过改变元架构、特征提取器和图像分辨率研究准确率和速度之间的权衡。例如，对不同特征提取器的选择可以造成元架构对比的非常大的变化。\n实时商业应用中需要低功耗和高效同时能保持准确率的目标检测方法，尤其是自动驾驶应用，SqueezeDet[33] 和 PVANet[34] 在论文中描述了这种发展趋势。\nCOCO[36] 是另一个常用的图像数据集。然而，它相对于 ImageNet 来说更小，更常被用作备选数据集。ImageNet 聚焦于目标识别，拥有情景理解的更广泛的语境。组织者主办了一场包括目标检测、分割和关键点标注的年度挑战赛。在 ILSVRC[37] 和 COCO[38] 上进行的目标检测挑战赛的结果如下：\nImageNet LSVRC 图像目标检测（DET）：CUImage 66% 平均准确率，在 200 个类别中有 109 个胜出。\nImageNet LSVRC 视频目标检测（VID）：NUIST 80.8% 平均准确率。\nImageNet LSVRC 视频追踪目标检测：CUvideo 55.8% 平均准确率。\nCOCO 2016 目标检测挑战赛（边界框）：G-RMI（谷歌）41.5% 平均准确率（比 2015 的胜者 MSRAVC 高出 4.2% 绝对百分点）。\n从以上结果可以看出，在 ImageNet 上的结果表明「MSRAVC 2015 的结果为『引入 ResNet』设置了很高的标准。在整个项目中对所有的类别的目标检测性能都有所提升。在两个挑战赛中，定位任务的性能都得到较大的提升。关于小型目标实例的大幅性能提升结果详见参考文献」（ImageNet,2016）。[39]\n图 5.ILSVRC 的图像目标检测结果（2013-2016），来源 ImageNet. 2016. [Online] Workshop\n目标追踪\n目标追踪即在给定的场景中追踪感兴趣的一个或多个特定目标的过程，在视频和现实世界的交互中（通常是从追踪初始的目标检测开始的）有很多应用，且对于自动驾驶而言非常重要。\nFully-Convolutional Siamese Networks for Object Tracking[40]，将一个连体网络（Siamese network）结合一个基础的追踪算法，使用端到端的训练方法，达到了当前最佳，图框显示率超过了实时应用的需求。这篇论文利用传统在线学习方法构建追踪模型。\nLearning to Track at 100 FPS with Deep Regression Networks[41]，该论文试图改善在线训练方法中存在的缺陷。他们构建了一个使用前馈网络学习目标运动、外观和方向中的普遍关系的追踪器，从而可以在没有在线训练的情况下有效地追踪到新的目标。该算法在一个标准的追踪基准测试中达到了当前最佳，同时可以 100FPS 的帧数追踪所有的目标（Held et al.,2016）。\nDeep Motion Features for Visual Tracking[43] 结合了手工设计的特征、深度外观特征（利用 CNN）和深度运动特征（在光流图像上训练），并取得了当前最佳的结果。虽然深度运动特征在动作识别和视频分类中很常见，但作者声称这是其首次被应用于视觉追踪上。该论文获得了 ICPR2016 的「计算机视觉和机器人视觉」的最佳论文。\n「本论文展示了深度运动特征（motion features）对检测和追踪框架的影响。我们还进一步说明了手工制作的特征、深度 RGB 和深度运用特征包含互补信息。据我们所知，这是第一个提出融合外表信息和深度运动特征，并用于视觉追踪的研究。我们全面的实验表明融合方法具有深度运动特征，并超过了单纯依赖外表信息的方法。」\nVirtual Worlds as Proxy for Multi-Object Tracking Analysis [44] 方法解决了现有虚拟世界中缺乏真实可变性视频追踪基准和数据集。该论文提出了一种新的真实世界复制方法，该方法从头开始生成丰富、虚拟、合成和照片逼真的环境。此外，该方法还能克服现有数据集中存在的一些内容匮乏问题。生成的图像能自动通过正确的真值进行标注，并允许应用于除目标检测/追踪外其它如光流等任务。\nGlobally Optimal Object Tracking with Fully Convolutional Networks [45] 专注处理目标变化和遮挡，并将它们作为目标追踪的两个根本限制。「我们提出的方法通过使用全卷积网络解决物体或目标外表的变化，还通过动态规划的方法解决遮挡情况」(Lee et al., 2016)。\n参考文献：\n[1] British Machine Vision Association (BMVA). 2016. What is computer vision? [Online] Available at: http://www.bmva.org/visionoverview [Accessed 21/12/2016]\n[2] Krizhevsky, A., Sutskever, I. and Hinton, G. E. 2012. ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada. Available: http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf\n[3] Kuhn, T. S. 1962. The Structure of Scientific Revolutions. 4th ed. United States: The University of Chicago Press.\n[4] Karpathy, A. 2015. What a Deep Neural Network thinks about your #selfie. [Blog] Andrej Karpathy Blog. Available: http://karpathy.github.io/2015/10/25/selfie/ [Accessed: 21/12/2016]\n[5] Quora. 2016. What is a convolutional neural network? [Online] Available: https://www.quora.com/What-is-a-convolutional-neural-network [Accessed: 21/12/2016]\n[6] Stanford University. 2016. Convolutional Neural Networks for Visual Recognition. [Online] CS231n. Available: http://cs231n.stanford.edu/ [Accessed 21/12/2016]\n[7] Goodfellow et al. 2016. Deep Learning. MIT Press. [Online] http://www.deeplearningbook.org/ [Accessed: 21/12/2016] Note: Chapter 9, Convolutional Networks [Available: http://www.deeplearningbook.org/contents/convnets.html]\n[8] Nielsen, M. 2017. Neural Networks and Deep Learning. [Online] EBook. Available: http://neuralnetworksanddeeplearning.com/index.html [Accessed: 06/03/2017].\n[9] ImageNet refers to a popular image dataset for Computer Vision. Each year entrants compete in a series of different tasks called the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Available: http://image-net.org/challenges/LSVRC/2016/index\n[10] See「What I learned from competing against a ConvNet on ImageNet」by Andrej Karpathy. The blog post details the author』s journey to provide a human benchmark against the ILSVRC 2014 dataset. The error rate was approximately 5.1% versus a then state-of-the-art GoogLeNet classification error of 6.8%. Available: http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\n[11] See new datasets later in this piece.\n[12] Keras is a popular neural network-based deep learning library: https://keras.io/\n[13] Chollet, F. 2016. Information-theoretical label embeddings for large-scale image classification. [Online] arXiv: 1607.05691. Available: arXiv:1607.05691v1\n[14] Chollet, F. 2016. Xception: Deep Learning with Depthwise Separable Convolutions. [Online] arXiv:1610.02357. Available: arXiv:1610.02357v2\n[15] Places2 dataset, details available: http://places2.csail.mit.edu/. See also new datasets section.\n[16] Hikvision. 2016. Hikvision ranked No.1 in Scene Classification at ImageNet 2016 challenge. [Online] Security News Desk. Available: http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/ [Accessed: 20/03/2017].\n[17] See Residual Networks in Part Four of this publication for more details.\n[18] Details available under team information Trimps-Soushen from: http://image-net.org/challenges/LSVRC/2016/results\n[19] Xie, S., Girshick, R., Dollar, P., Tu, Z. & He, K. 2016. Aggregated Residual Transformations for Deep Neural Networks. [Online] arXiv: 1611.05431. Available: arXiv:1611.05431v1\n[20] ImageNet Large Scale Visual Recognition Challenge (2016), Part II, Available: http://image-net.org/challenges/LSVRC/2016/ [Accessed: 22/11/2016]\n[21] Hu and Ramanan. 2016. Finding Tiny Faces. [Online] arXiv: 1612.04402. Available: arXiv:1612.04402v1\n[22] Liu et al. 2016. SSD: Single Shot MultiBox Detector. [Online] arXiv: 1512.02325v5. Available: arXiv:1512.02325v5\n[23] Redmon, J. Farhadi, A. 2016. YOLO9000: Better, Faster, Stronger. [Online] arXiv: 1612.08242v1. Available: arXiv:1612.08242v1\n[24] YOLO stands for「You Only Look Once」.\n[25] Redmon et al. 2016. You Only Look Once: Unified, Real-Time Object Detection. [Online] arXiv: 1506.02640. Available: arXiv:1506.02640v5\n[26]Redmon. 2017. YOLO: Real-Time Object Detection. [Website] pjreddie.com. Available: https://pjreddie.com/darknet/yolo/ [Accessed: 01/03/2017].\n[27] Lin et al. 2016. Feature Pyramid Networks for Object Detection. [Online] arXiv: 1612.03144. Available: arXiv:1612.03144v1\n[28] Facebook's Artificial Intelligence Research\n[29] Common Objects in Context (COCO) image dataset\n[30] Dai et al. 2016. R-FCN: Object Detection via Region-based Fully Convolutional Networks. [Online] arXiv: 1605.06409. Available: arXiv:1605.06409v2\n[31] Huang et al. 2016. Speed/accuracy trade-offs for modern convolutional object detectors. [Online] arXiv: 1611.10012. Available: arXiv:1611.10012v1\n[32] ibid\n[33] Wu et al. 2016. SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving. [Online] arXiv: 1612.01051. Available: arXiv:1612.01051v2\n[34] Hong et al. 2016. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection. [Online] arXiv: 1611.08588v2. Available: arXiv:1611.08588v2\n[35] DeepGlint Official. 2016. DeepGlint CVPR2016. [Online] Youtube.com. Available: https://www.youtube.com/watch?v=xhp47v5OBXQ [Accessed: 01/03/2017].\n[36] COCO - Common Objects in Common. 2016. [Website] Available: http://mscoco.org/ [Accessed: 04/01/2017].\n[37] ILSRVC results taken from: ImageNet. 2016. Large Scale Visual Recognition Challenge 2016.\n[Website] Object Detection. Available: http://image-net.org/challenges/LSVRC/2016/results [Accessed: 04/01/2017].\n[38] COCO Detection Challenge results taken from: COCO - Common Objects in Common. 2016. Detections Leaderboard [Website] mscoco.org. Available: http://mscoco.org/dataset/#detections-leaderboard [Accessed: 05/01/2017].\n[39] ImageNet. 2016. [Online] Workshop Presentation, Slide 31. Available: http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf [Accessed: 06/01/2017].\n[40] Bertinetto et al. 2016. Fully-Convolutional Siamese Networks for Object Tracking. [Online] arXiv: 1606.09549. Available: https://arxiv.org/abs/1606.09549v2\n[41] Held et al. 2016. Learning to Track at 100 FPS with Deep Regression Networks. [Online] arXiv: 1604.01802. Available: https://arxiv.org/abs/1604.01802v2\n[42] David Held. 2016. GOTURN - a neural network tracker. [Online] YouTube.com. Available: https://www.youtube.com/watch?v=kMhwXnLgT_I [Accessed: 03/03/2017].\n[43] Gladh et al. 2016. Deep Motion Features for Visual Tracking. [Online] arXiv: 1612.06615. Available: arXiv:1612.06615v1\n[44] Gaidon et al. 2016. Virtual Worlds as Proxy for Multi-Object Tracking Analysis. [Online] arXiv: 1605.06457. Available: arXiv:1605.06457v1\n[45] Lee et al. 2016. Globally Optimal Object Tracking with Fully Convolutional Networks. [Online] arXiv: 1612.08274. Available: arXiv:1612.08274v1\n原报告地址：\nhttp://www.themtank.org/a-year-in-computer-vision\n校对：王红玉\n为保证发文质量、树立口碑，数据派现设立“错别字基金”，鼓励读者积极纠错。\n若您在阅读文章过程中发现任何错误，请在文末留言，或到后台反馈，经小编确认后，数据派将向检举读者发8.8元红包。\n同一位读者指出同一篇文章多处错误，奖金不变。不同读者指出同一处错误，奖励第一位读者。\n感谢一直以来您的关注和支持，希望您能够监督数据派产出更加高质的内容。"}
{"content2":"2019年上半年收集到的AI计算机视觉方向干货文章\n时光飞逝，一晃上半年快要结束了。对人工智能高度感兴趣的笔者，每天都要看不少人工智能方面的文章，很多是干货文章，受益匪浅，所以整理成这个系列的文章。\n这是第一篇，聚焦计算机视觉方向。\n笔者打算以后人工智能研习侧重点就放在计算机视觉方面尤其是人脸识别领域。\n【计算机视觉】\n机器视觉系统的一般构架与组成\n干货｜如何利用CNN建立计算机视觉模型？\n【人脸识别】\n深入浅出人脸识别原理\nhttp://blog.itpub.net/29829936/viewspace-2646568/\n简述几种人脸识别的主要方法\nhttp://blog.itpub.net/29829936/viewspace-2646570/\n目前人脸识别中存在的主要技术难题\nhttp://blog.itpub.net/29829936/viewspace-2646572/\n深度学习下的人脸识别技术：从“后真相”到“无隐私”\n人脸识别技术在道路交通管理中的应用探究\n盘点国内八家初创计算机视觉领域优质企业\n人脸识别的“生意经”\n人脸识别技术在金融领域应用广泛\n人脸识别应用领域以及未来的行业发展趋势\n人脸识别技术成“金苹果”？安全与隐私不可偏废\n人脸识别争议再起 实际应用利弊几何？\n一种新的计算机视觉技术？将手机的摄像头变成了一个搜索引擎\n人脸算法拼杀中，深醒科技的自我迭代\n人脸识别之Python DLib库进行人脸关键点识别\n【语音识别】\n语音识别技术竟然发展如此迅速\n语音识别技术发展渐入佳境 AI企业奋力前行\n微软利用AI技术使文本转语音只需20分钟\n2019-06-20 写于苏州市"}
{"content2":"来源：本文由电姬翻译自nextplatform，作者Nick Tausanovitch，谢谢。\n美国投资公司Jefferies 最近一份报告称第四波计算浪潮已经开始，而且正受到物联网和并行处理方案的发展的推动。自 20 世纪 60 年代以来，计算领域的结构性转变一直都是由这一领域的主要力量导致的。\n在每次转变中，都会涌现出新的解决方案提供商，并成为主要供应商。在这第四波计算浪潮中，最新的力量是英伟达及其用于高性能计算（HPC）和人工智能（AI）的并行处理平台，即 GPU 和 CUDA 编程平台。英伟达业务中数据中心部分的增长（从 2016 财年的 3.39 亿美元增长到 2017 财年的 8.3 亿美元）是这样的结构性转变的一个证明。人工智能和自动驾驶汽车的技术和产品需求是英伟达增长的关键推动力，而且这在广义上都和物联网有关。但是，物联网还有其它一些同样关键的安全和网络要求，不容忽视。\n人工智能和并行处理的增长\n让我们先深入了解一下现在正在人工智能领域发生的结构性转变。机器学习训练、推理算法和相关的技术是人工智能的基础，而这些算法已经存在了几十年了。而为英伟达等公司创造了巨量机会的转折点是：\n有了跨多个行业的大量有用的训练数据集；\n芯片设计和工艺尺寸的进展让与机器学习相关的并行处理的成本和功耗特性达到了可以接受的程度。\n随着各种不同行业中许多不同类型的设备都越来越多地与互联网相连（换句话说就是 IoT 现象），生成的有用数据的量以及机器学习使用这些数据来改善这些行业中用户体验的能力都将受到广泛的影响。作为 x86 CPU 的协处理器，GPU 可以为机器学习带来大量所需的并行处理。GPU 原本是为游戏和图形处理应用设计的。配合 CUDA 等多线程编程环境，人们发现 GPU 是最有效执行机器学习算法的最优选择。\n第四波计算浪潮是由并行处理和 IoT 驱动的\nGPU 中的多线程处理让我们可以并行地执行类似的任务，而这对最有效地执行机器学习算法而言是至关重要的。这种处理方式非常不同于 x86 和 ARM 等通用型 CPU——这些处理器是为常见软件应用所需的单线程处理优化的，比如网页服务器和数据库处理。机器学习算法也需要处理大量训练数据，所以现代 GPU 也提供了高速高效的内存存取。\n带有基于 GPU 的多线程处理的英伟达可编程图形适配器，可以加速图形和人工智能计算处理（来自英伟达 2017 年度投资者日的演讲）\n尽管通用型 CPU 也可以被用于处理机器学习算法，但却无法提供必需的大规模计算性能。再加上随着硅芯片工艺几何尺寸的演进（也被称为摩尔定律），单位晶体管的成本也在上涨，而 GPU 等为机器学习优化过的协处理器芯片就成了一种必需品。\n安全性和 5G 将驱动第四波浪潮\n至于物联网和第四波计算浪潮，在使用协处理器的并行处理的重要性上，人工智能和网络安全之间存在很大的相似之处。\n我们生活方方面面对普遍安全的需求只会被物联网进一步放大。如果我们看看近来的分布式拒绝服务（DDoS）攻击以及当今的设备（笔记本电脑和平板作为攻击点）可以如何被人侵入从而发动这样的攻击，你就可以想见当使用 IoT 作为攻击点发动攻击时，攻击效果将指数式地猛增。预防 DDoS 攻击的机制将不得不超越以往的范围，并一直延伸到数据中心服务器领域，以便解决转移 DDoS 所需的规模和速度需求。随着数据中心中数据流量的增长，这种需求还会进一步放大。\n这一范式中的另一个关键转变是对流量可见性的需求，以便在网络流量上执行远程测量或屏蔽流氓访问流量。我们现在就需要这种东西，而且随着 5G 网络（带宽将增长 10 多倍）向新的行业敞开大门以在电信服务提供商网络上提供创新服务，这种需求还会加剧。比如，各种类型的 IoT 传感器和自动驾驶汽车将会在智能手机等移动设备生成的数据之上增加更多数据。为了确保不同类型的流量得到不同水平的保护，将网络分成“片”的能力将变得至关重要。这将需要高速的流量分类和可见性。\nGartner 预测到 2019 年时所有网络流量中的 80% 都将被加密。用于加密这些网络流量的关键相关技术是安全套接层（Secure Sockets Layer/SSL）和安全传输层（Transport Layer Security /TLS）。当使用这些技术加密流量时，我们就不可能获得所需的流量可见性。NSS Labs 的一项研究称在防火墙设备上解密 SSL 流量（以便实现流量的可见性）会让吞吐量损失 74%，每秒钟的交易量会减少 87.8%。当密钥更长时，SSL 解密引擎所需的工作负载中对复杂流量的处理也将增长。这将对延迟性能和服务水平产生显著的影响。最佳的解决方案是不要在数据中心网络的设备中实现这样的功能（因为流量在这里聚合，它的瓶颈会有很大影响），而是将 SSL 加密引擎工作负载分布到所有服务器上。\n使用SmartNIC扩展安全应用\n为了让数据中心服务器中的网络安全应用实现规模化、高性能和高效率，SmartNIC 平台使用了一种优化过的协处理器 NFP，即网络流处理器（Network Flow Processor）。和 GPU 类似，NFP 是多线程的——单块芯片上有多达 960 个线程。类似于 GPU 上基于 CUDA 的多线程编程，SmartNIC 中的 NFP 芯片支持使用 C 或更高级的与供应商无关的编程方法（比如 P4 和 eBPF）进行多线程编程。和 GPU 类型，NFP 也可以并行执行多个任务。和 GPU 只能并行执行相似的任务不同，NFP 还能并行执行多个不同的任务——这是网络安全所需要的功能。\n带有基于 NFP 的多线程处理的可编程 SmartNIC，可以加速网络数据包和安全性处理\n使用多线程内存存取引擎，NFP 可以在大量内存上实现高速且低延迟的存取，从而可实现对大量复杂流量的并行处理。最后，在人工智能领域，Caffe 等深度学习框架可以使用在 GPU 上运行的代码库加速。类似地，在网络安全领域，分布式虚拟交换、路由、防火墙、DOS、负载平衡以及其它安全和可见性框架都可以使用运行在 NFP 上的代码库加速。\n数据中心使用协同处理层和多线程编程环境实现的可重配置结构\n总而言之，对安全性和可见性的需求将愈发普遍，而且 DDoS 保护和 SSL 或 TLS 解密等技术也需要使用 COTS 和数据中心服务器以分布式的方式实现。在这种范式中，为了确保网络性能相关的服务的水平以及服务器的效率，使用协处理器的并行处理是至关重要的。和人工智能使用 GPU 等优化过的协处理器类似，为网络安全优化的协处理器将会成为实现物联网世界的第四波计算浪潮的一大主要力量。\n为了实现可重配置的结构，我们相信未来的数据中心服务器将具有两个协同处理平面——一个用于机器学习和人工智能，另一个用于网络连接和安全。这些协同处理平面将使用协同处理芯片中针对应用优化的功能（这些功能是为实现最佳性价比指标的服务器而设计的），从而为创新的多线程编程环境提供支持。\n作者简介\nNick Tausanovitch 是 Netronome 解决方案架构副总裁，负责该公司 SmartNIC 产品的云数据中心应用。Tausanovitch 希望帮助电信和云服务提供商实现软件定义网络（SDN）和网络功能虚拟化（NFV）的愿景，同时实现它们的基础设施的效率的最大化。在加入 Netronome 之前，他负责过博通公司的高端网络处理器产品线，也担任过 IDT 的电子设计总监（在这里开发过网络搜索引擎）和 Nortel 的系统架构师（在这里开发过交换机、路由器和网络处理器）。\n原文链接：https://www.nextplatform.com/2017/08/28/rise-fourth-wave-computing/\n高通：终端侧人工智能需要异构计算\n来源：本文作者Qualcomm Technologies, Inc.工程技术执行副总裁，Matt Grob，谢谢。\n在我们预想中的世界里，人工智能将使终端、机器、汽车和万物都变得更加智能，简化并丰富我们的日常生活。它们将能够基于场景认知，进行感知、推理并采取直观行动，改善目前我们提供给用户的所有体验，并解决我们目前更多交给常规算法所去处理的相关问题。\n人工智能（AI）正是驱动这次革命的技术。你可能听说过这一愿景，或认为人工智能只和大数据、云端有关，但Qualcomm的解决方案已具备合适的功耗、散热和处理效率，让强大的人工智能算法在实际的终端上运行，而这将带来诸多优势。\n得益于现代终端设备对大量数据的掌握，以及在算法和处理能力方面的提升，人工智能成为了快速增长的普遍趋势。新技术似乎总是出其不意地出现，但在时机成熟并取得关键进展之前，研究人员和工程师们通常需要辛苦钻研很多年。\n在Qualcomm，创新是我们的企业文化。我们为研发出大规模改变世界的基础技术而深感自豪。在人工智能方面也不例外。我们于十年前就开始了基础研究，目前我们的现有产品支持了许多人工智能用例：从计算机视觉和自然语言处理，到各种终端，如智能手机和汽车上的恶意软件侦测。同时，我们正在研究更广泛的课题，例如面向无线连接、电源管理和摄影的人工智能。\n\n我们在机器学习方面有着深厚积累\n\n我们对机器学习的投入有着悠久的历史。自2007年，Qualcomm开始探索面向计算机视觉和运动控制应用的机器学习脉冲神经方法，随后还将其研究范围从仿生方法拓展到了人工神经网络——主要是深度学习领域（这是机器学习的一个子范畴）。我们多次见证了基于深度学习的网络在模式匹配任务中展现出一流的成果。一个令人瞩目的例子就是，2012年AlexNet利用深度学习技术（而非传统手作计算机视觉）赢得ImageNet比赛。我们自己也在ImageNet挑战赛中利用深度学习技术获得成功，在物体定位、物体侦测和场景分类比赛中名列前三名。\n我们还将自主研究和与外界人工智能团体合作的领域扩展到诸如递归神经网络、物体跟踪、自然语言处理和手写识别等其他前景广阔的领域和机器学习应用等。2014年9月，我们在阿姆斯特丹开设了Qualcomm Research荷兰分支，作为机器学习研究的基地。我们通过Qualcomm创新奖学金计划与博士研究生紧密合作，开展前瞻性的理念研究。2015年9月，我们与阿姆斯特丹大学（QUVA）建立联合研究实验室，专注于推动面向移动计算机视觉的、最先进的机器学习技术发展。通过收购位于阿姆斯特丹的领先人工智能公司Scyfer，我们进一步深化与阿姆斯特丹人工智能业界的合作关系。Scyfer的创始人Max Welling是阿姆斯特丹大学知名教授，主攻机器学习、计算统计学和人工智能基础研究。Scyfer专注于应用广泛的机器学习方法以解决实际问题。Scyfer团队将加入Qualcomm Research机器学习团队。\n\n支持终端侧机器学习的出色功耗和性能\n\n为了实现我们的智能终端愿景，我们也意识到基于机器学习的解决方案需要在终端上运行，无论终端是智能手机、汽车、机器人、无人机、机器或是其他设备。与在云端运行的人工智能相比，在终端侧运行人工智能算法——亦称推理，具有诸多优势，例如即时响应、可靠性提升、隐私保护增强，以及高效利用网络带宽。\n当然，云端仍然十分重要，并作为终端侧处理的补充而存在。云端对汇集大数据以及在终端上运行的许多人工智能推理算法的训练（现阶段）是必要的。但是，在很多情况下，完全基于云端运行的推理在自动驾驶等时延敏感和关键型任务的实时应用中会遇到问题。此类应用无法负担数据传输往返的时间，或在无线覆盖变化时依靠关键功能运行。进一步讲，终端侧推理从本质来说更加私密。\n我们不想把自己仅仅局限在运行终端侧推理。我们也与云端协同合作，面向手势识别、连续认证、个性化用户界面和面向自动驾驶的精密地图构建等使用场景进行终端侧人工智能训练。实际上，得益于高速连接和高性能本地处理，我们有独特的能力去探索未来架构，实现最佳的总体系统性能。\n\n高效运行终端侧人工智能需要异构计算\n\n十多年来，Qualcomm一直专注于在移动终端的功耗、散热和尺寸限制之内，高效地处理多种计算工作负载。Qualcomm骁龙移动平台是最高性能移动终端的首选系统级芯片（SoC）。人工智能工作负载在这方面提出了另一个挑战。通过在适宜的计算引擎上运行各种机器学习任务（如CPU、GPU和DSP等），我们能提供最高效的解决方案。这已经集成在了我们的SoC中。Qualcomm Hexagon DSP就是一个典型范例，它最初是面向其他向量数学密集型工作而设计，但已通过进一步增强用来解决人工智能的工作负载。实际上，在骁龙835上支持Qualcomm Hexagon向量扩展的Hexagon DSP，与Qualcomm Kryo CPU相比，在运行相同工作负载时（GoogleNet Inception网络）能够实现25倍能效提升和8倍性能提升。\n架构的多样性是至关重要的，你不能仅依赖某一类引擎处理所有工作。我们将持续演进面向机器学习工作负载的现有引擎，保持我们在性能表现最大化上的领先优势。利用我们对新兴神经网络的研究，我们在专注提升性能表现，以扩展异构计算能力，应对未来人工智能工作负载上已具备了优势。实际上早在2012年，我们已预见了通过专用硬件高效运行人工智能的构想。\n\n我们正大规模普及人工智能\n\n让开发者能简单利用异构计算并非易事，仅有优良硬件还不够。为了弥补这一差距，我们发布了骁龙神经处理引擎（NPE）软件开发包（SDK）。它能缩短终端侧卷积神经网络（CNN）和递归神经网络（RNN）在合适的骁龙引擎（例如Kryo CPU、Qualcomm Adreno GPU和Hexagon DSP）上的运行时间，对图形识别和自然语言处理分别都有着重要作用。相同的开发者API给每个引擎都提供接入口，从而使开发者能够方便地无缝切换人工智能任务。\n该神经处理引擎还支持通用深度学习模型框架，例如Caffe/Caffe2和TensorFlow。该SDK是利用骁龙技术提供最佳性能和功耗的轻量灵活平台，旨在帮助从医疗健康到安全的广泛行业内的开发者和OEM厂商，在便携式终端上运行它们自己的专有神经网络模型。例如，今年的F8大会上，Facebook和Qualcomm Technologies宣布合作，支持优化Facebook开源深度学习框架Caffe2，以及NPE框架。\n\n通过持续研究扩展人工智能范围，并带来效率提升\n\n我们正处于机器学习发展征程的最初期，深度学习也仅是具备改变计算潜力的多项机器学习技术之一。\n为了实现更复杂的应用，我们在多个领域持续前进：\n专门的硬件架构：持续关注低功耗硬件（无论增强型、专用型还是定制型），以处理这些机器学习工作负载；\n神经网络技术的提升：针对半监督和无监督训练进行相关研究，如生成式对抗网络（GANs）、分布式学习和隐私保护；\n面向终端侧应用的网络优化：进行压缩、层间优化、稀疏优化，以及更好地利用内存和空间/时间复杂度的其他技术的相关研究；\n在终端侧完成全部或大部分思考的、“始终开启”的智能终端中蕴藏着巨大的机遇，我们期待通过研究和产品化推动先进机器学习的发展。目前，Qualcomm人工智能平台可通过高效的终端侧机器学习，提供高度响应、高度安全且直观的用户体验。未来还有更多可能。\n请关注我们未来发布的关于神经处理架构研究和机器学习应用的博客文章，我们所探讨的话题将包括增强型连接、电源管理和更佳的拍摄。\n原文链接：https://www.qualcomm.com/news/onq/2017/08/16/we-are-making-device-ai-ubiquitous\n新华社：人工智能需要专门的AI芯片\n来源：内容来自新华社，谢谢。\n去年“阿尔法狗”战胜韩国棋手李世石，需要耗电数万瓦、依赖体积巨大的云服务器。一年多后，一个小小的人工智能芯片，就可让手机、手表甚至摄像头都能和“阿尔法狗”一样“聪明”。\n随着中国企业率先推出市场化的人工智能手机芯片，这样的手机之“芯”正掀起全球热潮。它将带来怎样的影响，传统芯片命运几何？\n\n专“芯”专用\n\n２０１７年柏林国际消费电子展上，华为推出麒麟９７０人工智能手机芯片，内置神经元网络单元（ＮＰＵ），通过人工智能深度学习，让手机的运行更加高效。\n芯片又叫集成电路，按照功能可分为很多种，有的负责电源电压输出控制，有的负责音频视频处理，还有的负责复杂运算处理。目前市场上的手机芯片有指纹识别芯片、图像识别芯片、基带芯片、射频芯片等近百种。\n现有芯片种类繁多，为何还要人工智能芯片？\n随着手机智能应用越来越多，传统芯片要么性能不够，要么效率不足，难以支撑人工智能所需的大规模神经网络运转。\n例如，“谷歌大脑”用了上万个通用处理器“跑”了数天来学习如何识别猫脸；“阿尔法狗”和李世石下棋时使用了上千个中央处理器（ＣＰＵ）和数百个图形处理器（ＧＰＵ），平均每局电费近３０００美元。对于绝大多数智能需求来说，基于通用处理器的传统计算机成本高、功耗高、体积大、速度慢，难以接受。\n与传统的４核芯片相比，在处理同样的人工智能应用任务时，麒麟９７０拥有大约５０倍能效和２５倍性能优势。\n术业有专攻。专业人士指出，普通的处理器就好比瑞士军刀，虽然通用，但不专业。厨师要做出像样的菜肴，就必须使用专业的菜刀，而专门的深度学习处理器就是这把更高效、更快捷的“菜刀”。\n\n“芯”够强　才能走得远\n\n目前迅猛发展的人工智能，上层的应用都依赖于底层核心能力，而这个核心能力就是人工智能处理器。如果在芯片上不能突破，人工智能应用就不可能真正成功。可以说核心芯片是人工智能时代的战略制高点。\n人工智能目前采用的深度学习算法，有海量的数据运算需求，对传统架构和系统提出了极大挑战。\n深度学习，就是通过算法给机器设计一个神经网络。这个网络的基本特点，是模仿大脑神经元之间传递、处理信息的模式，从多个角度和层次来观察、学习、判断、决策。近年来，这种方法已应用于许多领域，比如人脸识别、语音识别等，是人工智能领域的热点研究方向之一。\n用于图像处理的ＧＰＵ芯片因海量数据(40.670, -0.95, -2.28%)并行运算能力，被最先引入深度学习。２０１１年，当时在谷歌就职的吴恩达将英伟达的ＧＰＵ应用于“谷歌大脑”中，结果表明１２个ＧＰＵ可达到相当于２０００个ＣＰＵ的深度学习性能。之后多家研究机构都基于ＧＰＵ来加速其深度学习神经网络。\n然而，随着近两年人工智能技术的迅速发展，ＧＰＵ在三个方面显露出局限性：无法充分发挥并行计算优势，硬件结构固定不具备可编程性，运行深度学习算法能效不足。\n全球科研界和企业于是竞相开发更加适用的人工智能芯片，尤其是适用于移动通信时代的芯片。\n华为公司与中国科学院计算技术研究所“寒武纪”项目团队共同开发的麒麟９７０人工智能手机芯片，首次集成ＮＰＵ，将通常由多个芯片完成的传统计算、图形、图像以及数字（数位）信号处理功能集成在一块芯片内，节省空间、节约能耗，同时极大提高了运算效率。\n据预测，类脑计算芯片市场将在２０２２年前达到千亿美元规模，其中消费终端将是最大市场，占据９８．１７％，其他需求包括工业检测、航空、军事与国防等领域。\n在新的计算时代，核心芯片将决定基础架构和未来生态。因此，谷歌、微软、超威等全球信息技术和通信制造巨头都投入巨资，加速人工智能芯片的研发。\n人工智能赛博物理操作系统\nAI-CPS OS\n“人工智能赛博物理操作系统”（新一代技术+商业操作系统“AI-CPS OS”：云计算+大数据+物联网+区块链+人工智能）分支用来的今天，企业领导者必须了解如何将“技术”全面渗入整个公司、产品等“商业”场景中，利用AI-CPS OS形成数字化+智能化力量，实现行业的重新布局、企业的重新构建和自我的焕然新生。\nAI-CPS OS的真正价值并不来自构成技术或功能，而是要以一种传递独特竞争优势的方式将自动化+信息化、智造+产品+服务和数据+分析一体化，这种整合方式能够释放新的业务和运营模式。如果不能实现跨功能的更大规模融合，没有颠覆现状的意愿，这些将不可能实现。\n领导者无法依靠某种单一战略方法来应对多维度的数字化变革。面对新一代技术+商业操作系统AI-CPS OS颠覆性的数字化+智能化力量，领导者必须在行业、企业与个人这三个层面都保持领先地位：\n重新行业布局：你的世界观要怎样改变才算足够？你必须对行业典范进行怎样的反思？\n重新构建企业：你的企业需要做出什么样的变化？你准备如何重新定义你的公司？\n重新打造自己：你需要成为怎样的人？要重塑自己并在数字化+智能化时代保有领先地位，你必须如何去做？\nAI-CPS OS是数字化智能化创新平台，设计思路是将大数据、物联网、区块链和人工智能等无缝整合在云端，可以帮助企业将创新成果融入自身业务体系，实现各个前沿技术在云端的优势协同。AI-CPS OS形成的数字化+智能化力量与行业、企业及个人三个层面的交叉，形成了领导力模式，使数字化融入到领导者所在企业与领导方式的核心位置：\n精细：这种力量能够使人在更加真实、细致的层面观察与感知现实世界和数字化世界正在发生的一切，进而理解和更加精细地进行产品个性化控制、微观业务场景事件和结果控制。\n智能：模型随着时间（数据）的变化而变化，整个系统就具备了智能（自学习）的能力。\n高效：企业需要建立实时或者准实时的数据采集传输、模型预测和响应决策能力，这样智能就从批量性、阶段性的行为变成一个可以实时触达的行为。\n不确定性：数字化变更颠覆和改变了领导者曾经仰仗的思维方式、结构和实践经验，其结果就是形成了复合不确定性这种颠覆性力量。主要的不确定性蕴含于三个领域：技术、文化、制度。\n边界模糊：数字世界与现实世界的不断融合成CPS不仅让人们所知行业的核心产品、经济学定理和可能性都产生了变化，还模糊了不同行业间的界限。这种效应正在向生态系统、企业、客户、产品快速蔓延。\nAI-CPS OS形成的数字化+智能化力量通过三个方式激发经济增长：\n创造虚拟劳动力，承担需要适应性和敏捷性的复杂任务，即“智能自动化”，以区别于传统的自动化解决方案；\n对现有劳动力和实物资产进行有利的补充和提升，提高资本效率；\n人工智能的普及，将推动多行业的相关创新，开辟崭新的经济增长空间。\n给决策制定者和商业领袖的建议：\n超越自动化，开启新创新模式：利用具有自主学习和自我控制能力的动态机器智能，为企业创造新商机；\n迎接新一代信息技术，迎接人工智能：无缝整合人类智慧与机器智能，重新\n评估未来的知识和技能类型；\n制定道德规范：切实为人工智能生态系统制定道德准则，并在智能机器的开\n发过程中确定更加明晰的标准和最佳实践；\n重视再分配效应：对人工智能可能带来的冲击做好准备，制定战略帮助面临\n较高失业风险的人群；\n开发数字化+智能化企业所需新能力：员工团队需要积极掌握判断、沟通及想象力和创造力等人类所特有的重要能力。对于中国企业来说，创造兼具包容性和多样性的文化也非常重要。\n子曰：“君子和而不同，小人同而不和。”  《论语·子路》云计算、大数据、物联网、区块链和 人工智能，像君子一般融合，一起体现科技就是生产力。\n如果说上一次哥伦布地理大发现，拓展的是人类的物理空间。那么这一次地理大发现，拓展的就是人们的数字空间。在数学空间，建立新的商业文明，从而发现新的创富模式，为人类社会带来新的财富空间。云计算，大数据、物联网和区块链，是进入这个数字空间的船，而人工智能就是那船上的帆，哥伦布之帆！\n新一代技术+商业的人工智能赛博物理操作系统AI-CPS OS作为新一轮产业变革的核心驱动力，将进一步释放历次科技革命和产业变革积蓄的巨大能量，并创造新的强大引擎。重构生产、分配、交换、消费等经济活动各环节，形成从宏观到微观各领域的智能化新需求，催生新技术、新产品、新产业、新业态、新模式。引发经济结构重大变革，深刻改变人类生产生活方式和思维模式，实现社会生产力的整体跃升。\n\n产业智能官  AI-CPS\n用“人工智能赛博物理操作系统”（新一代技术+商业操作系统“AI-CPS OS”：云计算+大数据+物联网+区块链+人工智能），在场景中构建状态感知-实时分析-自主决策-精准执行-学习提升的认知计算和机器智能；实现产业转型升级、DT驱动业务、价值创新创造的产业互联生态链。\n\n\n长按上方二维码关注微信公众号： AI-CPS，更多信息回复：\n新技术：“云计算”、“大数据”、“物联网”、“区块链”、“人工智能”；新产业：“智能制造”、“智能农业”、“智能金融”、“智能零售”、“智能城市”、“智能驾驶”；新模式：“财富空间”、“特色小镇”、“赛博物理”、“供应链金融”。\n点击“阅读原文”，访问AI-CPS OS官网\n\n本文系“产业智能官”（公众号ID：AI-CPS）收集整理，转载请注明出处！\n版权声明：由产业智能官（公众号ID：AI-CPS）推荐的文章，除非确实无法确认，我们都会注明作者和来源。部分文章推送时未能与原作者取得联系。若涉及版权问题，烦请原作者联系我们，与您共同协商解决。联系、投稿邮箱：erp_vip@hotmail.com"}
{"content2":"计算机视觉学习资料整理\n1、理论\n1.1、图像处理\n1.2、机器学习\n1.3、深度学习\n1.4、计算机视觉\n2、实践\n2.1、Python编程\n2.2、多维数组运算Numpy\n2.3、绘图Matplotlib\n2.4、数据分析Pandas\n2.5、SciPy\n2.6、Scikits_Learn\n2.7、Tensorflow\n2.8、Keras\n1、理论\n1.1、图像处理\nOpencv—Python\nScikit-Image\n1.2、机器学习\n吴恩达机器学习\n林轩田机器学习基石\n林轩田机器学习技法\n《统计学习方法》\n《机器学习西瓜书》\n《南瓜书》\n1.3、深度学习\n吴恩达深度学习\n《深度学习》花书\n1.4、计算机视觉\n李飞飞视觉课\n2、实践\n2.1、Python编程\n菜鸟教程\n廖雪峰\n官网\n中文教程\n2.2、多维数组运算Numpy\nNumpy 中文学习网址\nNumpy 官方\nNumpy 源码\n2.3、绘图Matplotlib\nMatplotlib 中文\nMatplotlib 官网\nMatplotlib 源码\n2.4、数据分析Pandas\nPandas 中文学习网址\nPandas 英文\nPandas 官网\nPandas 源码\n2.5、SciPy\nSciPy 官网\nSciPy 源码\n2.6、Scikits_Learn\n官方网站\n源码\n2.7、Tensorflow\n中文网站\n官网\n源码\n2.8、Keras\n中文网站\n英文网站\n源码"}
{"content2":"阶段1、人工智能基础 －　高等数学必知必会\n本阶段主要从数据分析、概率论和线性代数及矩阵和凸优化这四大块讲解基础，旨在训练大家逻辑能力，分析能力。拥有良好的数学基础，有利于大家在后续课程的学习中更好的理解机器学习和深度学习的相关算法内容。同时对于AI研究尤为重要，例如人工智能中的智能很大一部分依托“概率论”实现的。\n一、数据分析\n1）常数e\n2）导数\n3）梯度\n4）Taylor\n5）gini系数\n6）信息熵与组合数\n7）梯度下降\n8）牛顿法\n二、概率论\n1）微积分与逼近论\n2）极限、微分、积分基本概念\n3）利用逼近的思想理解微分，利用积分的方式理解概率\n4）概率论基础\n5）古典模型\n6）常见概率分布\n7）大数定理和中心极限定理\n8）协方差(矩阵)和相关系数\n9）最大似然估计和最大后验估计\n三、线性代数及矩阵\n1）线性空间及线性变换\n2）矩阵的基本概念\n3）状态转移矩阵\n4）特征向量\n5）矩阵的相关乘法\n6）矩阵的QR分解\n7）对称矩阵、正交矩阵、正定矩阵\n8）矩阵的SVD分解\n9）矩阵的求导\n10）矩阵映射/投影\n四、凸优化\n1）凸优化基本概念\n2）凸集\n3）凸函数\n4）凸优化问题标准形式\n5）凸优化之Lagerange对偶化\n6）凸优化之牛顿法、梯度下降法求解\n阶段2、人工智能提升 － Python高级应用\n随着AI时代的到来以及其日益蓬勃的发展，Python作为AI时代的头牌语言地位基本确定，机器学习是着实令人兴奋，但其复杂度及难度较大，通常会涉及组装工作流和管道、设置数据源及内部和云部署之间的分流而有了Python库后，可帮助加快数据管道，且Python库也在不断更新发布中，所以本阶段旨在为大家学习后续的机器学习减负。\n一、容器\n1）列表:list\n2）元组:tuple\n3）字典: dict\n4）数组: Array\n5）切片\n6）列表推导式\n7）浅拷贝和深拷贝\n二、函数\n1）lambda表达式\n2）递归函数及尾递归优化\n3）常用内置函数/高阶函数\n4）项目案例：约瑟夫环问题\n三、常用库\n1）时间库\n2）并发库\n3）科学计算库\n4）Matplotlib可视化绘图库\n5）锁和线程\n6）多线程编程\n阶段3、人工智能实用 － 机器学习篇\n机器学习利用算法去分析数据、学习数据，随后对现实世界情况作出判断和预测。因此，与预先编写好、只能按照特定逻辑去执行指令的软件不同，机器实际上是在用大量数据和算法去“自我训练”，从而学会如何完成一项任务。\n所以本阶段主要从机器学习概述、数据清洗和特征选择、回归算法、决策树、随机森林和提升算法、SVM、聚类算、EM算法、贝叶斯算法、隐马尔科夫模型、LDA主题模型等方面讲解一些机器学习的相关算法以及这些算法的优化过程，这些算法也就是监督算法或者无监督算法。\n一、机器学习\n1）机器学习概述\n二、监督学习\n1）逻辑回归\n2）softmax分类\n3）条件随机场\n4）支持向量机svm\n5）决策树\n6）随机森林\n7）GBDT\n8）集成学习\n三、非监督学习\n1）高斯混合模型\n2）聚类\n3）PCA\n4）密度估计\n5）LSI\n6）LDA\n7）双聚类\n四、数据处理与模型调优\n1）特征提取\n2）数据预处理\n3）数据降维\n4）模型参数调优\n5）模型持久化\n6）模型可视化\n阶段4、人工智能前沿 －　深度学习篇\n深度学习是实现机器学习的技术，同时深度学习也带来了机器学习的许多实际应用，拓展了AI的使用领域，本阶段主要从TensorFlow、BP神经网络、深度学习概述、CNN卷积神经网络、递归神经网、自动编码机，序列到序列网络、生成对抗网络，孪生网络，小样本学习技术等方面讲解深度学习相关算法以，掌握深度学习前沿技术，并根据不同项目选择不同的技术解决方案。针对公司样本不足，采用小样本技术和深度学习技术结合，是项目落地的解决方案。\n1）TensorFlow基本应用\n2）BP神经网络\n3）深度学习概述\n4）卷积神经网络(CNN)\n5）图像分类(vgg,resnet)\n6）目标检测(rcnn,fast-rcnn,faster-rcnn,ssd)\n7）递归神经网络(RNN)\n8）lstm,bi-lstm,多层LSTM\n9）无监督学习之AutoEncoder自动编码器\n10）Seq2Seq\n11）Seq2Seq with Attension\n12）生成对抗网络\n13）irgan\n14）finetune及迁移学习\n15）孪生网络\n16）小样本学习\n阶段5、人工智能进阶 － 图像处理篇\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。广泛的应用于农牧业、林业、环境、军事、工业和医学等方面，是人工智能和深度学习的重要研究方向。深度学习作为当前机器学习领域最热门的技术之一，已经在图像处理领域获得了应用，并且展现出巨大的前景。本阶段学习了数字图像的基本数据结构和处理技术，到前沿的深度学习处理方法。掌握前沿的ResNet,SSD,Faster RCNN等深度学习模型，对图像分类，目标检测和模式识别等图像处理主要领域达到先进水平。实际工作中很多项目都可以转化为本课程的所学的知识去解决，如行人检测，人脸识别和数字识别。\n一、图像基础\n图像读，写，保存，画图（线，圆，多边形，添加文字）\n二、图像操作及算数运算\n图像像素读取，算数运算，ROI区域提取\n三、图像颜色空间运算\n图像颜色空间相互转化\n四、图像几何变换\n平移，旋转，仿射变换，透视变换等\n五、图像形态学\n腐蚀，膨胀，开/闭运算等\n六、图像轮廓\n长宽，面积，周长，外接圆，方向，平均颜色，层次轮廓等\n七、图像统计学\n图像直方图\n八、图像滤波\n高斯滤波，均值滤波，双边滤波，拉普拉斯滤波等\n阶段八、人工智能终极实战 － 项目应用\n本阶段重点以项目为导向，通过公安系统人脸识别、图像识别以及图像检索、今日头条CTR广告点击量预估、序列分析系统、聊天机器人等多个项目的讲解，结合实际来进行AI的综合运用。\n项目一：公安系统人脸识别、图像识别\n使用深度学习框架从零开始完成人脸检测的核心技术图像类别识别的操作，从数据预处理开始一步步构建网络模型并展开分析与评估，方便大家快速动手进行项目实践！识别上千种人靓，返回层次化结构的每个人的标签。\n项目二：公安系统图像检索\n本项目基于卷积神经网在训练过程中学习出对应的『二值检索向量』，对全部图先做了一个分桶操作，每次检索的时候只取本桶和临近桶的图片作比对，而不是在全域做比对，使用这样的方式提高检索速度，使用Tensorflow框架建立基于ImageNet的卷积神经网络，并完成模型训练以及验证。\n项目九：手工数字识别\n人认知世界的开始就是从认识数字开始的，深度学习也一样，数字识别是深度学习的一个很好的切入口，是一个非常经典的原型问题，通过对手写数字识别功能的实现，可以帮助我们后续对神经网络的理解和应用。选取手写数字识别的主要原因是手写数字具有一定的挑战性，要求对编程能力及神经网络思维能力有一定的要求，但同时手写数字问题的复杂度不高，不需要大量的运算，而且手写数字也可以作为其它技术的一个基础，所以以手写数字识别为基础，贯穿始终，从而理解深度学习相关的应用知识。\n项目十：癌症筛选检测\n技术可以改变癌症患者的命运吗，对于患有乳腺癌患者来说，复发还是痊愈影响这患者的生命，那么怎么来预测患者的患病结果呢，机器学习算法可以帮助我们解决这一难题，本项目应用机器学习logistic回归模型，来预测乳腺癌患者复发还是正常，有效的预测出医学难题。\n项目十一：葡萄酒质量检测系统\n随着信息科技的快速发展,计算机中的经典算法在葡萄酒产业中得到了广泛的研究与应用。其中机器学习算法的特点是运用了人工智能技术,在大量的样本集训练和学习后可以自动地找出运算所需要的参数和模型。\n项目十三：手工实现梯度下降回归算法\n梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。\n项目十四：基于TensorFlow实现回归算法\n回归算法是业界比较常用的一种机器学习算法，通过应用于各种不同的业务场景，是一种成熟而稳定的算法种类；TensorFlow是一种常用于深度学习相关领域的算法工具；随着深度学习热度的高涨，TensorFlow的使用也会越来越多，从而使用TensorFlow来实现一个不存在的算法，会加深对TensorFlow的理解和使用；基于TensorFlow的回归算法的实现有助于后续的TensorFlow框架的理解和应用，并可以促进深度学习相关知识的掌握。\n项目十六：行人检测\n行人检测是利用图像处理技术和深度学习技术对图像或者视频序列中是否存在行人并给予精确定位。学习完行人检测技术后，对类似的工业缺陷检测，外观检测和医疗影像检测等目标检测范畴类的项目可以一通百通。该技术可与行人跟踪，行人重识别等技术结合，应用于人工智能系统、车辆辅助驾驶系统、智能机器人、智能视频监控、人体行为分析、智能交通等领域。由于行人兼具刚性和柔性物体的特性 ，外观易受穿着、尺度、遮挡、姿态和视角等影响，使得行人检测成为计算机视觉领域中一个既具有研究价值同时又极具挑战性的热门课题。\n阶段九、人工智能实战 － 企业项目实战\n课程一、基于Python数据分析与机器学习案例实战教程\n课程风格通俗易懂，基于真实数据集案例实战。主体课程分成三个大模块(1)python数据分析，(2)机器学习经典算法原理详解,(3)十大经典案例实战。通过python数据科学库numpy,pandas,matplot结合机器学习库scikit-learn完成一些列的机器学习案例。算法课程注重于原理推导与流程解释，结合实例通俗讲解复杂的机器学习算法，并以实战为主，所有课时都结合代码演示。算法与项目相结合，选择经典kaggle项目，从数据预处理开始一步步代码实战带大家快速入门机器学习。旨在帮助同学们快速上手如何使用python库来完整机器学习案例。选择经典案例基于真实数据集，从数据预处理开始到建立机器学习模型以及效果评估，完整的讲解如何使用python及其常用库进行数据的分析和模型的建立。对于每一个面对的挑战，分析解决问题思路以及如何构造合适的模型并且给出合适评估方法。在每一个案例中，同学们可以快速掌握如何使用pandas进行数据的预处理和分析，使用matplotlib进行可视化的展示以及基于scikit-learn库的机器学习模型的建立。\n1）Python数据分析与机器学习实战课程简介\n2）Python快速入门\n3）Python科学计算库Numpy\n4）Python数据分析处理库Pandas\n5）Python可视化库Matplotlib\n6）回归算法\n7）模型评估\n8）K近邻算法\n9）决策树与随机森林算法\n10）支持向量机\n11）贝叶斯算法\n12）神经网络\n13）Adaboost算法\n14）SVD与推荐\n15）聚类算法\n19）案例实战：鸢尾花数据集分析\n20）案例实战：级联结构的机器学习模型\n21）案例实战：员工离职预测\n22）案例实战：使用神经网络进行手写字体识别\n课程二、人工智能与深度学习实战\n课程风格通俗易懂，必备原理，形象解读，项目实战缺一不可！主体课程分成四个大模块(1)神经网络必备基础知识点，(2)深度学习模型，(3)深度学习框架Caffe与Tensorflow，(4)深度学习项目实战。 课程首先概述讲解深度学习应用与挑战，由计算机视觉中图像分类任务开始讲解深度学习的常规套路。对于复杂的神经网络，将其展开成多个小模块进行逐一攻破，再挑战整体神经网络架构。对于深度学习模型形象解读卷积神经网络原理，详解其中涉及的每一个参数，对卷积网络架构展开分析与评估，对于现阶段火爆的对抗生成网络以及强化学习给出形象解读，并配合项目实战实际演示效果。 基于框架实战，选择两款深度学习最火框架，Caffe与Tensorflow，首先讲解其基本使用方法，并结合案例演示如何应用框架构造神经网络模型并完成案例任务。 选择经典深度学习项目实战，使用深度学习框架从零开始完成人脸检测，验证码识别，人脸关键点定位，垃圾邮件分类，图像风格转换，AI自己玩游戏等。对于每一个项目实战，从数据预处理开始一步步构建网络模型并展开分析与评估。 课程提供所涉及的所有数据，代码以及PPT，方便大家快速动手进行项目实践！\n1）深度学习概述与挑战\n2）图像分类基本原理门\n3）深度学习必备基础知识点\n4）神经网络反向传播原理\n5）神经网络整体架构\n6）神经网络案例实战图像分类任务\n7）卷积神经网络基本原理\n8）卷积参数详解\n9）卷积神经网络案例实战\n10）经典网络架构分析\n11）分类与回归任务\n12）三代物体检测算法分析\n13）数据增强策略\n14）TransferLearning\n15）网络架构设计\n16） 深度学习框架Caffe网络结构配置\n17）Caffe\n18）深度学习项目实战人脸检测\n19）人脸正负样本数据源制作\n20）人脸检测网络架构配置习模型\n21）人脸检测代码实战\n22）人脸关键点定位项目实战\n23）人脸关键点定位网络模型\n24）人脸关键点定位构建级联网络\n25）人脸关键点定位测试效果与分析\n26）Tensorflow框架实战\n27）Tensorflow构建回归模型\n28）Tensorflow构建神经网络模型\n29）Tensorflow深度学习模型\n30）Tensorflow打造RNN网络模型\n31）Tensorflow项目实战验证识别\n32）项目实战图像风格转换\n33）QLearning算法原理\n34）DQN网络架构\n35）项目实战DQN网络让AI自己玩游戏\n36）项目实战对抗生成网络等"}
{"content2":"以下链接是本人整理的关于计算机视觉（Computer Vision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\nAllan+ Jepson http://www.cs.toronto.edu/~jepson/University of Toronto 主要研究方向：计算机视觉，主要包括图像重组，图像分割，运动估计，图像建模等。\nQiang Jihttp://www.ecse.rpi.edu/~qji/ Rensselaer Polytechnic Institute 主要研究方向：计算机视觉，概率图模型，模式识别，决策分析，人机交互等。\nShree K. Nayar http://www.cs.columbia.edu/~nayar/index.html Columbia University 主要研究方向：数字成像，计算机视觉，人机交互，机器人等。\nRama Chellappa http://www.cfar.umd.edu/~rama/University of Maryland, College Park 主要研究方向：计算机视觉，数字图象处理与分析，多维信号处理与随机分析。\nJianbo Shi http://www.cis.upenn.edu/~jshi/ University of Pennsylvania 主要研究方向：Human Recognition(主要包括人的关节识别，姿势识别等，最终目的是实现视频中人的动作理解)，图像分割和目标识别等。\nLarry S. Davis http://www.umiacs.umd.edu/~lsd/ University of Maryland 主要研究方向：目标识别，场景分析，human detection，人类运动跟踪与建模等。\nRichard Hartley http://users.cecs.anu.edu.au/~hartley/ Australian National University 主要研究方向：计算机视觉，包括：场景理解与分析（建模），3d目标识别与跟踪，特别的，从2d视频中产生3d视频序列。\nSVETLANA LAZEBNIK http://www.cs.illinois.edu/homes/slazebni/University of Illinois at Urbana-Champaign 主要研究方向：计算机视觉和机器学习，包括：目标识别，场景理解和建模，特征提取，聚类等\nHonglak Lee http://web.eecs.umich.edu/~honglak/ University of Michigan, Ann Arbor 主要研究方向：机器学习，特别热衷于研究从无标记数据中自动学习特征表示的算法，概率图形模型，凸优化，高维数据分析等。\nDavid J. Fleet http://www.cs.toronto.edu/~fleet/University of Toronto相关文章：Cartesian k-means等（这篇文章看过，有点没看懂，晕死！），主要研究方向计算机视觉，图像处理等（偏向于算法相关数学基础研究），具体包括：视觉运动分析，跟踪，估计，感知等\nEric Roberts http://www-cs-faculty.stanford.edu/~eroberts/ Stanford University\nDr.J.Parker http://people.ucalgary.ca/~jparker/cv.html  Research Area:Computer simulation,Pattern analysis and recognition,Multiple classifiers and ensemble methods\nLEI　Zhang(HK-PolyU) http://www4.comp.polyu.edu.hk/~cslzhang/ The Hong Kong Polytechnic University . Research Area:ImageSegmentation and Image Segmentation Evaluation,ImageQuality Assessment.\nPablo Alcantarilla http://www.robesafe.com/personal/pablo.alcantarilla/Georgia Institute of Technology, Atlanta, USA. Resarch Area: visual SLAM (Simultaneous Localization and Mapping) and 3D reconstruction problems.\nSong-Chun Zhu http://www.stat.ucla.edu/~sczhu/ University of California, Los Angeles,Los Angeles, CA 90095. Research Area: Computer Vision, Statistical Modeling and Computing, Machine Learning, Cognition and AI, and Visual Arts.\nDavid Lowe (SIFT) http://www.cs.ubc.ca/~lowe/ University of British Columbia.Research Area: computer vision, object recognition, and computational models of human vision.\nAndrea Vedaldi (SIFT) http://www.robots.ox.ac.uk/~vedaldi/index.html University Lecturer in Engineering Science, Research Area:machine learning and invariant visual representations with applications to the classification and detection of object categories\nPedro F. Felzenszwalb http://cs.brown.edu/~pff/ Brown University, Research Area: computer vision, AI, machine learning and discrete algorithms\nNing Zhange http://aesop.rutgers.edu/~plantbiopath/faculty/zhang/zhang.htm Rutgers University, Research Area: Fungal systematics and evolution,Fungal biodiversity and their functional role in the ecosystem and Development of novel molecular methods for rapid diagnosis of pathogenic fungi.\nDougla Dlanman http://mesh.brown.edu/dlanman/courses.html (在其主页上搜集了大量算法教程)\nWallflower TestImageshttp://research.microsoft.com/en-us/um/people/jckrumm/WallFlower/TestImages.htm (用于评价背景好坏)\nActive Vision Group http://www.robots.ox.ac.uk/ActiveVision/index.html (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)\nCBCL StreetScenes Databasehttp://cbcl.mit.edu/software-datasets/streetscenes/ (是一个图像、注释、软件和性能检测的对象集[cars, pedestrians, bicycles, buildings, trees, skies, roads, sidewalks, and stores])\nLabelMe dataSet http://labelme.csail.mit.edu/Release3.0/browserTools/php/dataset.php (超过150,000已经标注的照片).\nMuHavi:Multicamera Human Action Video Data.http://dipersec.king.ac.uk/MuHAVi-MAS/ A large body of human action video data using 8 cameras. Includes manually annotated silhouette data. (用于测试人行为的数据集).\nINRIA Datasets http://lear.inrialpes.fr/data 车辆, 人, 马, 人类行为等 .\nJuyang Weng http://www.cse.msu.edu/~weng/ Michigan StateUniversity(机器学习的专家，Autonomous Mental Development 是其物色), a member of the MSU Cognitive Science Program and a member of the MSU Neuroscience Program .His Research interests lie in the intersection of the general fields of computer science and engineering, brain science, and cognitive science. He and his coworkers study mental architectures; computational models of autonomous development for vision, audition,touch, behaviors and motivational systems in biological and engineered systems; and techniques that enable and facilitate autonomous development of mental skills. Engineering applications include pattern recognition, computer vision, speech recognition, language processing, robotics, sequential decision-making, and human-computer interactions. He is a co-founderof the Embodied IntelligenceLaboratory and amember of the PRIPLaboratory.\nCVonline:Vision Related Books including Online Books and Book Support Siteshttp://homepages.inf.ed.ac.uk/rbf/CVonline/books.htm\nQINGSHAN Liu http://www.research.rutgers.edu/~qsliu/ The State university of New Jersy,Research Area:Image and Vision Analysisincluding Face Image Analysis, Graph & Hyper-graph based Image and Videounderstanding, Medical Image Analysis, Event-based Video Analysis.\nAnil K. Jain http://www.cse.msu.edu/rgroups/biometrics/Pattern Recognition and Image Processing (PRIP) Lab, Research Area:pattern recognition and biometrics .\nHenry Schneidermanhttp://www.cs.cmu.edu/~hws/ Carnegie Mellon University,Research Area: Computer vision for object detection and recognition.(目标检测和识别；最有影响力的研究成果：目标检测；2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购).\nWilliam Freeman http://people.csail.mit.edu/billf/ Massachusetts Institute of Technology主要研究方向:应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成.\nComputer Vision Software http://www.cs.cmu.edu/~cil/v-source.html\nRechard Szeliski http://research.microsoft.com/~szeliski/  Research Area:computer vision and computer graphics.the Book:Computer Vision Algorithms and Application\nJiri Matas http://cmp.felk.cvut.cz/~matas/ Czech Technical University.Research Area:Visual recognition, Tracking, Image Retrieval, Sequential decision-making, Pattern recognition, Wide-baseline Matching, RANSAC, Face detection and recognition, Biometric authentication, Colour-based recognition, Hough Transform.\nPer-Erik Forssen http://www.cs.ubc.ca/~perfo/University of British Columbia,Research Area:Computer Vision,Active Perception and Learning,Developmental Robotics.\nAnat Levin http://www.wisdom.weizmann.ac.il/~levina/Department of Mathematics and Computer Science Ziskined The Weizmann Institute of Science，Research Area:Computer Vision, Computer Graphics and Machine Learning.\nRob Fegus http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php Department of Computer Science,Courant Institute of Mathematical Sciences,New York University ,Research Area: Machine Learning and Computer Vision. I am particularly interested in applying Deep Learning methods to object recognition. I also work on low-level vision problems, with applications to computational photography and astronomy.\nAnat Levin http://www.wisdom.weizmann.ac.il/~levina/Department of Mathematics and Computer Science Ziskined Building, room 224 The Weizmann Institute of Science.Research Area:Computer Vision, Computer Graphics and Machine Learning. In particular I worked on computational photography, low and mid level vision. For more details about my work, please have a look at my publication list.\ngoogleResearch； http://research.google.com/index.html\nU.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\ncomputer vision software; http://peipa.essex.ac.uk/info/software.html\nComputer Vision Resource; http://www.cvpapers.com/\ncomputer vision research groups;http://peipa.essex.ac.uk/info/groups.html\ncomputer vision center; http://computervisioncentral.com/cvcnews\nMIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\nMIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\nStanford大学vision实验室； http://vision.stanford.edu/research.html\nStanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\nMIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\nCMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes\n美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\nUniversity of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n微软剑桥研究院研究员John Winn: http://johnwinn.org/\n佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\nCMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\nCMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\nMIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\nCMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\nCMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\nCMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\nGE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n行人检测主页：http://www.pedestrian-detection.com/\n法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n计算机视觉分类信息导航：http://www.visionbib.com/\n西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\n康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索\n北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重\n澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学\n美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\nCMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\nCMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\nMIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\nMIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\ngoogle瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\nAdobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\nDeep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建\nWest Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\nhttp://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:http://www.image-net.org/challenges/LSVRC/2013/\n加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、\n苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、\n耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n中科院自动化所医学影像研究室：http://www.3dmed.net/\n中科院田捷研究员：http://www.3dmed.net/tian/\n凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理\n中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；\n中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n清华大学程明明博士：http://mmcheng.net/ 图像分割、检索\n武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n微软学术搜索：http://libra.msra.cn/\n香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n整理的内容不见得完善，也不见得能满足所有朋友的需要。如果您有更好的网站资源，欢迎推荐给我。另外，如果你不想记录所有这些链接，也可以去我的博客，所有这些网址在我的博客都有链接。而且，以后我还会不断更新！\n关于Image Engineering & Computer Vision的更多讨论与交流，敬请关注本博和新浪微博songzi_tea."}
{"content2":"相关视频资料下载见：https://blog.csdn.net/qwxwaty/article/details/80800701\n阶段一、人工智能基础 －　高等数学必知必会\n本阶段主要从数据分析、概率论和线性代数及矩阵和凸优化这四大块讲解基础，旨在训练大家逻辑能力，分析能力。拥有良好的数学基础，有利于大家在后续课程的学习中更好的理解机器学习和深度学习的相关算法内容。同时对于AI研究尤为重要，例如人工智能中的智能很大一部分依托“概率论”实现的。\n一、数据分析\n1）常数e\n2）导数\n3）梯度\n4）Taylor\n5）gini系数\n6）信息熵与组合数\n7）梯度下降\n8）牛顿法\n二、概率论\n1）微积分与逼近论\n2）极限、微分、积分基本概念\n3）利用逼近的思想理解微分，利用积分的方式理解概率\n4）概率论基础\n5）古典模型\n6）常见概率分布\n7）大数定理和中心极限定理\n8）协方差(矩阵)和相关系数\n9）最大似然估计和最大后验估计\n三、线性代数及矩阵\n1）线性空间及线性变换\n2）矩阵的基本概念\n3）状态转移矩阵\n4）特征向量\n5）矩阵的相关乘法\n6）矩阵的QR分解\n7）对称矩阵、正交矩阵、正定矩阵\n8）矩阵的SVD分解\n9）矩阵的求导\n10）矩阵映射/投影\n四、凸优化\n1）凸优化基本概念\n2）凸集\n3）凸函数\n4）凸优化问题标准形式\n5）凸优化之Lagerange对偶化\n6）凸优化之牛顿法、梯度下降法求解\n阶段二、人工智能提升 － Python高级应用\n随着AI时代的到来以及其日益蓬勃的发展，Python作为AI时代的头牌语言地位基本确定，机器学习是着实令人兴奋，但其复杂度及难度较大，通常会涉及组装工作流和管道、设置数据源及内部和云部署之间的分流而有了Python库后，可帮助加快数据管道，且Python库也在不断更新发布中，所以本阶段旨在为大家学习后续的机器学习减负。\n一、容器\n1）列表:list\n2）元组:tuple\n3）字典: dict\n4）数组: Array\n5）切片\n6）列表推导式\n7）浅拷贝和深拷贝\n二、函数\n1）lambda表达式\n2）递归函数及尾递归优化\n3）常用内置函数/高阶函数\n4）项目案例：约瑟夫环问题\n三、常用库\n1）时间库\n2）并发库\n3）科学计算库\n4）Matplotlib可视化绘图库\n5）锁和线程\n6）多线程编程\n阶段三、人工智能实用 － 机器学习篇\n机器学习利用算法去分析数据、学习数据，随后对现实世界情况作出判断和预测。因此，与预先编写好、只能按照特定逻辑去执行指令的软件不同，机器实际上是在用大量数据和算法去“自我训练”，从而学会如何完成一项任务。\n所以本阶段主要从机器学习概述、数据清洗和特征选择、回归算法、决策树、随机森林和提升算法、SVM、聚类算、EM算法、贝叶斯算法、隐马尔科夫模型、LDA主题模型等方面讲解一些机器学习的相关算法以及这些算法的优化过程，这些算法也就是监督算法或者无监督算法。\n一、机器学习\n1）机器学习概述\n二、监督学习\n1）逻辑回归\n2）softmax分类\n3）条件随机场\n4）支持向量机svm\n5）决策树\n6）随机森林\n7）GBDT\n8）集成学习\n三、非监督学习\n1）高斯混合模型\n2）聚类\n3）PCA\n4）密度估计\n5）LSI\n6）LDA\n7）双聚类\n四、数据处理与模型调优\n1）特征提取\n2）数据预处理\n3）数据降维\n4）模型参数调优\n5）模型持久化\n6）模型可视化\n阶段四、人工智能实用 － 数据挖掘篇\n本阶段主要通过音乐文件分类和金融反欺诈模型训练等项目，帮助大家对于上阶段的机器学习做更深入的巩固，为后续深度学习及数据挖掘提供项目支撑。\n项目一：百度音乐系统文件分类\n音乐推荐系统就是利用音乐网站上的音乐信息，向用户提供音乐信息或者建议，帮助用户决定应该听什么歌曲。而个人化推荐则是基于音乐信息及用户的兴趣特征、听歌历史行为，向用户推荐用户可能会感兴趣的音乐或者歌手。推荐算法主要分为以下几种：基于内容的推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐等；推荐系统常用于各个互联网行业中，比如音乐、电商、旅游、金融等。\n项目二：千万级P2P金融系统反欺诈模型训练\n目前比较火的互联网金融领域，实质是小额信贷，小额信贷风险管理，本质上是事前对风险的主动把控，尽可能预测和防范可能出现的风险。本项目应用GBDT、Randomforest等机器学习算法做信贷反欺诈模型，通过数据挖掘技术，机器学习模型对用户进行模型化综合度量，确定一个合理的风险范围，使风险和盈利达到一个平衡的状态。\n阶段五、人工智能前沿 －　深度学习篇\n深度学习是实现机器学习的技术，同时深度学习也带来了机器学习的许多实际应用，拓展了AI的使用领域，本阶段主要从TensorFlow、BP神经网络、深度学习概述、CNN卷积神经网络、递归神经网、自动编码机，序列到序列网络、生成对抗网络，孪生网络，小样本学习技术等方面讲解深度学习相关算法以，掌握深度学习前沿技术，并根据不同项目选择不同的技术解决方案。针对公司样本不足，采用小样本技术和深度学习技术结合，是项目落地的解决方案。\n1）TensorFlow基本应用\n2）BP神经网络\n3）深度学习概述\n4）卷积神经网络(CNN)\n5）图像分类(vgg,resnet)\n6）目标检测(rcnn,fast-rcnn,faster-rcnn,ssd)\n7）递归神经网络(RNN)\n8）lstm,bi-lstm,多层LSTM\n9）无监督学习之AutoEncoder自动编码器\n10）Seq2Seq\n11）Seq2Seq with Attension\n12）生成对抗网络\n13）irgan\n14）finetune及迁移学习\n15）孪生网络\n16）小样本学习\n阶段六、人工智能进阶 － 自然语言处理篇\n自然语言处理（NLP）是计算机科学领域与人工智能领域中的一个重要方向。它已成为人工智能的核心领域。自然语言处理解决的是“让机器可以理解自然语言”这一到目前为止都还只是人类独有的特权，被誉为人工智能皇冠上的明珠，被广泛应用。本阶段从NLP的字、词和句子全方位多角度的学习NLP，作为NLP的基础核心技术，对NLP为核心的项目，如聊天机器人，合理用药系统，写诗机器人和知识图谱等提供底层技术。通过学习NLP和深度学习技术，掌握NLP具有代表性的前沿技术。\n1）词（分词，词性标注）代码实战\n2）词（深度学习之词向量，字向量）代码实战\n3）词（深度学习之实体识别和关系抽取）代码实战\n4）词（关键词提取，无用词过滤）代码实战\n5）句（句法分析，语义分析）代码实战\n6）句（自然语言理解,一阶逻辑）代码实战\n7）句（深度学习之文本相似度）代码实战\n阶段七、人工智能进阶 － 图像处理篇\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。广泛的应用于农牧业、林业、环境、军事、工业和医学等方面，是人工智能和深度学习的重要研究方向。深度学习作为当前机器学习领域最热门的技术之一，已经在图像处理领域获得了应用，并且展现出巨大的前景。本阶段学习了数字图像的基本数据结构和处理技术，到前沿的深度学习处理方法。掌握前沿的ResNet,SSD,Faster RCNN等深度学习模型，对图像分类，目标检测和模式识别等图像处理主要领域达到先进水平。实际工作中很多项目都可以转化为本课程的所学的知识去解决，如行人检测，人脸识别和数字识别。\n一、图像基础\n图像读，写，保存，画图（线，圆，多边形，添加文字）\n二、图像操作及算数运算\n图像像素读取，算数运算，ROI区域提取\n三、图像颜色空间运算\n图像颜色空间相互转化\n四、图像几何变换\n平移，旋转，仿射变换，透视变换等\n五、图像形态学\n腐蚀，膨胀，开/闭运算等\n六、图像轮廓\n长宽，面积，周长，外接圆，方向，平均颜色，层次轮廓等\n七、图像统计学\n图像直方图\n八、图像滤波\n高斯滤波，均值滤波，双边滤波，拉普拉斯滤波等\n阶段八、人工智能终极实战 － 项目应用\n本阶段重点以项目为导向，通过公安系统人脸识别、图像识别以及图像检索、今日头条CTR广告点击量预估、序列分析系统、聊天机器人等多个项目的讲解，结合实际来进行AI的综合运用。\n项目一：公安系统人脸识别、图像识别\n使用深度学习框架从零开始完成人脸检测的核心技术图像类别识别的操作，从数据预处理开始一步步构建网络模型并展开分析与评估，方便大家快速动手进行项目实践！识别上千种人靓，返回层次化结构的每个人的标签。\n项目二：公安系统图像检索\n本项目基于卷积神经网在训练过程中学习出对应的『二值检索向量』，对全部图先做了一个分桶操作，每次检索的时候只取本桶和临近桶的图片作比对，而不是在全域做比对，使用这样的方式提高检索速度，使用Tensorflow框架建立基于ImageNet的卷积神经网络，并完成模型训练以及验证。\n项目三：今日头条CTR广告点击量预估\n点击率预估是广告技术的核心算法之一，它是很多广告算法工程师喜爱的战场。广告的价值就在于宣传效果,点击率是其中最直接的考核方式之一,点击率越大,证明广告的潜在客户越多,价值就越大,因此才会出现了刷点击率的工具和技术。通过对于点击量的评估，完成对于潜在用户的价值挖掘。\n项目四：序列分析系统\n时间序列分析(Time Series Analysis)是一种动态数据处理的统计方法，主要基于随机过程理论和数理统计方法，研究随机数据序列所遵从的统计规律以便用于解决实际问题。主要包括自相关分析等一般的统计分析方法，构建模型从而进行业务推断。经典的统计分析是假定数据序列具有独立性，而时间序列分析则侧重于研究数据样本序列之间的依赖关系。时间序列预测一般反应了三种实际变化规律：趋势变化、周期性变化和随机性变化。时间序列预测常应用于国民经济宏观控制、企业经营管理、市场潜力量预测、天气预报、水文预报等方面，是应用于金融行业的一种核心算法之一。\n项目五：京东聊天机器人/智能客服\n聊天机器人/智能客服是一个用来模拟人类对话或者聊天的一个系统，利用深度学习和机器学习等NLP相关算法构建出问题和答案之间的匹配模型，然后可以将其应用到客服等需要在线服务的行业领域中，聊天机器人可以降低公司客服成本，还能够提高客户的体验友好性。 在一个完整的聊天机器人实现过程中，主要包含了一些核心技术，包括但不限于：爬虫技术、机器学习算法、深度学习算法、NLP领域相关算法。通过实现一个聊天机器人可以帮助我们队AI整体知识的一个掌握。\n项目六：机器人写诗歌\n机器人写诗歌/小说是一种基于NLP自然语言相关技术的一种应用，在实现过程中可以基于机器学习相关算法或者深度学习相关算法来进行小说/诗歌构建过程。人工智能的一个终极目标就是让机器人能够像人类一样理解文字，并运用文字进行创作，而这个目标大致上主要分为两个部分，也就是自然语言理解和自然语言生成，其中现阶段的主要自然语言生成的运用，自然语言生成主要有两种不同的方式，分别为基于规则和基于统计，基于规则是指首先了解词性及语法等规则，再依据这样的规则写出文章；而基于统计的本质是根据先前的字句和统计的结果，进而判断下一个子的生成，例如马尔科夫模型就是一种常用的基于统计的方法。\n项目七：机器翻译系统\n机器翻译又称自动翻译，是指利用计算机将一种自然语言转换为另外一种自然语言的过程，机器翻译是人工智能的终极目标之一，具有很高的研究价值，同时机器翻译也具有比较重要的实用价值，机器翻译技术在促进政治、经济、文化交流等方面起到了越来越重要的作用；机器翻译主要分为以下三个过程：原文分析、原文译文转换和译文生成；机器翻译的方式有很多种，但是随着深度学习研究取得比较大的进展，基于人工网络的机器翻译也逐渐兴起，特别是基于长短时记忆(LSTM)的循环神经网络(RDD)的应用，为机器翻译添了一把火。\n项目八：垃圾邮件过滤系统\n邮件主要可以分为有效邮件和垃圾邮件两大类，有效邮件指的邮件接收者有意义的邮件，而垃圾邮件转指那些没有任何意义的邮件，其内容主要包含赚钱信息、成人广告、商业或者个人网站广告、电子杂志等，其中垃圾邮件又可以发为良性垃圾邮件和恶性垃圾邮件，良性垃圾邮件指的就是对收件人影响不大的信息邮件，而恶性垃圾邮件指具有破坏性的电子邮件，比如包含病毒、木马等恶意程序的邮件。垃圾邮件过滤主要使用使用机器学习、深度学习等相关算法，比如贝叶斯算法、CNN等，识别出所接收到的邮件中那些是垃圾邮件。\n项目九：手工数字识别\n人认知世界的开始就是从认识数字开始的，深度学习也一样，数字识别是深度学习的一个很好的切入口，是一个非常经典的原型问题，通过对手写数字识别功能的实现，可以帮助我们后续对神经网络的理解和应用。选取手写数字识别的主要原因是手写数字具有一定的挑战性，要求对编程能力及神经网络思维能力有一定的要求，但同时手写数字问题的复杂度不高，不需要大量的运算，而且手写数字也可以作为其它技术的一个基础，所以以手写数字识别为基础，贯穿始终，从而理解深度学习相关的应用知识。\n项目十：癌症筛选检测\n技术可以改变癌症患者的命运吗，对于患有乳腺癌患者来说，复发还是痊愈影响这患者的生命，那么怎么来预测患者的患病结果呢，机器学习算法可以帮助我们解决这一难题，本项目应用机器学习logistic回归模型，来预测乳腺癌患者复发还是正常，有效的预测出医学难题。\n项目十一：葡萄酒质量检测系统\n随着信息科技的快速发展,计算机中的经典算法在葡萄酒产业中得到了广泛的研究与应用。其中机器学习算法的特点是运用了人工智能技术,在大量的样本集训练和学习后可以自动地找出运算所需要的参数和模型。\n项目十二：淘宝网购物篮分析推荐算法\n购物篮分析(Market Basket Analysis)即非常有名的啤酒尿布故事的一个反应，是通过对购物篮中的商品信息进行分析研究，得出顾客的购买行为，主要目的是找出什么样的物品会经常出现在一起，也就是那些商品之间是有很大的关联性的。通过购物篮分析挖掘出来的信息可以用于指导交叉销售、追加销售、商品促销、顾客忠诚度管理、库存管理和折扣计划等业务；购物篮分析的最常用应用场景是电商行业，但除此之外，该算法还被应用于信用卡商城、电信与金融服务业、保险业以及医疗行业等。\n项目十三：手工实现梯度下降回归算法\n梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。\n项目十四：基于TensorFlow实现回归算法\n回归算法是业界比较常用的一种机器学习算法，通过应用于各种不同的业务场景，是一种成熟而稳定的算法种类；TensorFlow是一种常用于深度学习相关领域的算法工具；随着深度学习热度的高涨，TensorFlow的使用也会越来越多，从而使用TensorFlow来实现一个不存在的算法，会加深对TensorFlow的理解和使用；基于TensorFlow的回归算法的实现有助于后续的TensorFlow框架的理解和应用，并可以促进深度学习相关知识的掌握。\n项目十五：合理用药系统\n合理用药系统，是根据临床合理用药专业工作的基本特点和要求，运用NLP和深度学习技术对药品说明书，临床路径等医学知识进行标准化，结构化处理。如自动提取药品说明书文本里面的关键信息如：药品相互作用，禁忌，用法用量，适用人群等，实现医嘱自动审查，及时发现不合理用药问题，帮助医生、药师等临床专业人员在用药过程中及时有效地掌握和利用医药知识，预防药物不良事件的发生、促进临床合理用药工作。\n项目十六：行人检测\n行人检测是利用图像处理技术和深度学习技术对图像或者视频序列中是否存在行人并给予精确定位。学习完行人检测技术后，对类似的工业缺陷检测，外观检测和医疗影像检测等目标检测范畴类的项目可以一通百通。该技术可与行人跟踪，行人重识别等技术结合，应用于人工智能系统、车辆辅助驾驶系统、智能机器人、智能视频监控、人体行为分析、智能交通等领域。由于行人兼具刚性和柔性物体的特性 ，外观易受穿着、尺度、遮挡、姿态和视角等影响，使得行人检测成为计算机视觉领域中一个既具有研究价值同时又极具挑战性的热门课题。\n阶段九、人工智能实战 － 企业项目实战\n课程一、基于Python数据分析与机器学习案例实战教程\n课程风格通俗易懂，基于真实数据集案例实战。主体课程分成三个大模块(1)python数据分析，(2)机器学习经典算法原理详解,(3)十大经典案例实战。通过python数据科学库numpy,pandas,matplot结合机器学习库scikit-learn完成一些列的机器学习案例。算法课程注重于原理推导与流程解释，结合实例通俗讲解复杂的机器学习算法，并以实战为主，所有课时都结合代码演示。算法与项目相结合，选择经典kaggle项目，从数据预处理开始一步步代码实战带大家快速入门机器学习。旨在帮助同学们快速上手如何使用python库来完整机器学习案例。选择经典案例基于真实数据集，从数据预处理开始到建立机器学习模型以及效果评估，完整的讲解如何使用python及其常用库进行数据的分析和模型的建立。对于每一个面对的挑战，分析解决问题思路以及如何构造合适的模型并且给出合适评估方法。在每一个案例中，同学们可以快速掌握如何使用pandas进行数据的预处理和分析，使用matplotlib进行可视化的展示以及基于scikit-learn库的机器学习模型的建立。\n1）Python数据分析与机器学习实战课程简介\n2）Python快速入门\n3）Python科学计算库Numpy\n4）Python数据分析处理库Pandas\n5）Python可视化库Matplotlib\n6）回归算法\n7）模型评估\n8）K近邻算法\n9）决策树与随机森林算法\n10）支持向量机\n11）贝叶斯算法\n12）神经网络\n13）Adaboost算法\n14）SVD与推荐\n15）聚类算法\n16）案例实战：使用Python库分析处理Kobe Bryan职业生涯数据\n17）案例实战：信用卡欺诈行为检测\n18）案例实战：泰坦尼克号获救预测\n19）案例实战：鸢尾花数据集分析\n20）案例实战：级联结构的机器学习模型\n21）案例实战：员工离职预测\n22）案例实战：使用神经网络进行手写字体识别\n23）案例实战：主成分分析\n24）案例实战：基于NLP的股价预测\n25）案例实战：借贷公司数据分析\n课程二、人工智能与深度学习实战\n课程风格通俗易懂，必备原理，形象解读，项目实战缺一不可！主体课程分成四个大模块(1)神经网络必备基础知识点，(2)深度学习模型，(3)深度学习框架Caffe与Tensorflow，(4)深度学习项目实战。 课程首先概述讲解深度学习应用与挑战，由计算机视觉中图像分类任务开始讲解深度学习的常规套路。对于复杂的神经网络，将其展开成多个小模块进行逐一攻破，再挑战整体神经网络架构。对于深度学习模型形象解读卷积神经网络原理，详解其中涉及的每一个参数，对卷积网络架构展开分析与评估，对于现阶段火爆的对抗生成网络以及强化学习给出形象解读，并配合项目实战实际演示效果。 基于框架实战，选择两款深度学习最火框架，Caffe与Tensorflow，首先讲解其基本使用方法，并结合案例演示如何应用框架构造神经网络模型并完成案例任务。 选择经典深度学习项目实战，使用深度学习框架从零开始完成人脸检测，验证码识别，人脸关键点定位，垃圾邮件分类，图像风格转换，AI自己玩游戏等。对于每一个项目实战，从数据预处理开始一步步构建网络模型并展开分析与评估。 课程提供所涉及的所有数据，代码以及PPT，方便大家快速动手进行项目实践！\n1）深度学习概述与挑战\n2）图像分类基本原理门\n3）深度学习必备基础知识点\n4）神经网络反向传播原理\n5）神经网络整体架构\n6）神经网络案例实战图像分类任务\n7）卷积神经网络基本原理\n8）卷积参数详解\n9）卷积神经网络案例实战\n10）经典网络架构分析\n11）分类与回归任务\n12）三代物体检测算法分析\n13）数据增强策略\n14）TransferLearning\n15）网络架构设计\n16） 深度学习框架Caffe网络结构配置\n17）Caffe\n18）深度学习项目实战人脸检测\n19）人脸正负样本数据源制作\n20）人脸检测网络架构配置习模型\n21）人脸检测代码实战\n22）人脸关键点定位项目实战\n23）人脸关键点定位网络模型\n24）人脸关键点定位构建级联网络\n25）人脸关键点定位测试效果与分析\n26）Tensorflow框架实战\n27）Tensorflow构建回归模型\n28）Tensorflow构建神经网络模型\n29）Tensorflow深度学习模型\n30）Tensorflow打造RNN网络模型\n31）Tensorflow项目实战验证识别\n32）项目实战图像风格转换\n33）QLearning算法原理\n34）DQN网络架构\n35）项目实战DQN网络让AI自己玩游戏\n36）项目实战对抗生成网络等\n项目一、AI大数据互联网电影智能推荐（第一季）\n随着科技的发展，现在视频的来源和类型多样性，互联网视频内容充斥着整个网络，如果仅仅是通过翻页的方法来寻找自己想看的视频必然会感到疲劳，现在急需一种能智能推荐的工具，推荐系统通过分析用户对视频的评分分析，对用户的兴趣进行建模，从而预测用户的兴趣并给用户进行推荐。\nPython是一种面向对象的解释型计算机程序设计语言，Python具有丰富和强大的库。它常被昵称为胶水语言，而大数据是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，企业面临海量数据的到来，大多选择把数据从本地迁移至云端，云端将成为最大的非结构化数据存储场所。本项目主要以客户咨询为载体，分析客户的群体，分布，旨在挖掘客户的内在需求，帮助企业实现更有价值的营销。\n一、教务管理系统业务介绍\n1）教务管理系统框架讲解\n2）系统业务逻辑介绍\n二、大数据需求分析\n1）明确数据需求\n2）大数据分析过程\n3）分析难点和解决方案\n4）大数据相关技术选型\n三、构建分布式大数据框架\n1）Hadoop分布式集群配置\n2）ZooKeeper高可用\n3）SQOOP数据转移\n4）ETL数据清洗\n5）HIVE数据分析\n6）HBase数据存储\n四、基于教务管理系统大数据分析\n1）业务数据分析指标设定\n2）操作MapReduce分而治之\n3）使用Hive进行数据整合抽离\n4）使用HBase存储非结构话数据\n五、大数据可视化\n1）可视化技术选型\n2）Echarts代码展示炫酷视图\n3）使用Tableau进行数据可视化展示\n项目二、电商大数据情感分析与AI推断实战项目（第一季）\n本项目从开发的角度以大数据、PHP技术栈为基础，使用真实商用表结构和脱敏数据，分三步构建商用系统、真实大数据环境、进行推断分析以及呈现结果。 项目课程的完整性、商业性，可以使学者尽可能完整地体会真实的商业需求和业务逻辑。完整的项目过程，使PHP技术栈的同学得以窥见和学到一个完整商业平台项目的搭建方法；真实大数据环境的搭建，使呈现、建立大数据的工具应用技术概念储备；基于大数据平台的分析需求的实现、呈现，将完整的一次大数据技术栈到分析结果的中线，平铺直述，为想要学习大数据并有开发基础的同学点亮新的能力。\n一、实践项目研发\n1）开发环境的安装配置\n2）表与数据\n3）LARAVEL的快速开发实践\n4）批量创建模型\n5）万能控制器与表配置\n6）统一视图的创建\n二、数据分析需求设立\n1）定义数据需求\n2）分析计算过程\n3）分析难点和解决方案\n4）大数据技术选型\n三、大数据平台搭建\n1）分布式环境的模拟建立\n2）网络环境的调通\n3）身份验证与集群控制\n4）Hadoop环境搭建和要点说明\n5）MapReduce与Yarn的搭建和说明\n四、大数据分析脚本编写\n1）MapReduce脚本编写\n2）拆解数据需求\n3）Map逻辑详写\n4）Reduce逻辑详写\n5）结果整理与输出\n五、结果可视化\n1）可视化需求和技术选型\n2）展示页面的快速铺设\n3）可视化JS上手\n4）使用可视化JS展示结果\n项目三、AI法律咨询大数据分析与服务智能推荐实战项目(第一季)\n本项目结合目前流行的大数据框架，在原有成熟业务的前提下，进行大数据分析处理，真实还原企业应用，让学员身临其境的感受企业大数据开发的整个流程。\n项目的业务系统底层主要采用JAVA架构，大数据分析主要采用Hadoop框架，其中包括Kettle实现ETL、SQOOP、Hive、Kibana、HBASE、Spark以及人工智能算法等框架技术；采用真实大数据集群环境的搭建，让学员切身感受企业项目的从0到1的过程。\n一、系统业务介绍\n1）底层业务实现框架讲解\n2）功能模块讲解\n二、系统架构设计\n1）总体架构分析\n2）数据流向\n3）各技术选型承载作用\n4）部署方案\n三、详尽实现\n1）原始数据处理\n2）ETL数据导入\n3）MR数据计算\n4）Hive数据分析\n四、数据可视化\n1）采用Highcharts插件展示客户偏好曲线图\n2）使用Tableau进行数据分析可视化展示\n五、项目优化\n1）ZooKeeper实现HA\n2）集群监控的整体联调\n项目四、AI大数据基站定位智能推荐商圈分析项目实战（第一季）\n随着当今个人手机终端的普及、出行人群中手机拥有率和使用率已达到相当高的比例，根据手机信号在真实地理空间的覆盖情况，将手机用户时间序列的手机定位数据，映射至现实地理位置空间位置，即可完整、客观地还原出手机用户的现实活动轨迹，从而挖掘出人口空间分布与活动联系特征信息。\n商圈是现代市场中企业市场活动的空间，同时也是商品和服务享用者的区域。商圈划分为目的之一是研究潜在顾客分布，以制定适宜的商业对策。\n本项目以实战为基础结合大数据技术Hadoop、.Net技术全栈为基础，采用真实商业数据，分不同环节构建商用系统、真实大数据环境、进行推断分析及呈现数据。\n一、分析系统业务逻辑讲解\n1）大数据基站定位智能推荐商圈分析系统介绍\n2）数据前期清洗和数据分析目标指标的设定等\n二、大数据导入与存储\n1）关系型数据库基础知识\n2）hive的基本语法\n3）hive的架构及设计原理\n4）hive安装部署与案例等\n5）Sqoop安装及使用\n6）Sqoop与关系型数据库进行交互等\n7）动手实践\n三、Hbase理论及实战\n1）Hbase简介、安装及配置\n2）Hbase的数据存储与数据模型\n3）Hbase Shell\n4）Hbase 访问接口\n5）Hbase数据备份与恢复方法等\n6）动手实践（数据转储与备份）\n四、基站数据分析与统计推断\n1）背景与分析推断目标\n2）分析方法与过程推断\n3）动手实践（分析既定指标数据）\n五、数据分析与统计推断结果的展示（大数据可视化）\n1）使用Tableau展示数据分析结果\n2）使用HighCharts、ECharts展示数据分析结果\n阶段十、阿里云认证\n课程一、云计算 - 网站建设：部署与发布\n阿里云网站建设认证课程教你如何掌握将一个本地已经设计好的静态网站发布到Internet公共互联网，绑定域名，完成工信部的ICP备案。\n课程二、云计算 - 网站建设：简单动态网站搭建\n阿里云简单动态网站搭建课程教你掌握如何快速搭建一个WordPress动态网站，并会对网站进行个性化定制，以满足不同的场景需求。\n课程三、云计算 - 云服务器管理维护\n阿里云服务器运维管理课程教你掌握快速开通一台云服务器，并通过管理控制台方便地进行服务器的管理、服务器配置的变更和升级、数据的备份，并保证其可以正常运转并按业务需求随时进行配置的变更。\n课程四、云计算 - 云数据库管理与数据迁移\n阿里云云数据库管理与数据迁移认证课程掌握云数据库的概念，如何在云端创建数据库、将自建数据库迁移至云数据库MySQL版、数据导入导出，以及云数据库运维的常用操作。\n课程五、云计算 - 云存储：对象存储管理与安全\n阿里云云储存认证课程教你掌握安全、高可靠的云存储的使用，以及在云端存储下载文件，处理图片，以及如何保护数据的安全。\n课程六、云计算 - 超大流量网站的负载均衡\n掌握如何为网站实现负载均衡，以轻松应对超大流量和高负载。\n课程七、大数据 - MOOC网站日志分析\n本课程可以帮助学员掌握如何收集用户访问日志，如何对访问日志进行分析，如何利用大数据计算服务对数据进行处理，如何以图表化的形式展示分析后的数据。\n课程八、大数据 - 搭建企业级数据分析平台\n模拟电商场景，搭建企业级的数据分析平台，用来分析商品数据、销售数据以及用户行为等。\n课程九、大数据 - 基于LBS的热点店铺搜索\n本课程可以帮助学员掌握如何在分布式计算框架下开发一个类似于手机地图查找周边热点（POI）的功能，掌握GeoHash编码原理，以及在地理位置中的应用，并能将其应用在其他基于LBS的定位场景中。\n课程中完整的演示了整个开发步骤，学员在学完此课程之后，掌握其原理，可以在各种分布式计算框架下完成此功能的开发，比如MapReduce、Spark。\n课程十、大数据 - 基于机器学习PAI实现精细化营销\n本课程通过一个简单案例了解、掌握企业营销中常见的、也是必需的精准营销数据处理过程，了解机器学习PAI的具体应用，指导学员掌握大数据时代营销的利器---通过机器学习实现营销。\n课程十一、大数据 - 基于机器学习的客户流失预警分析\n本课程讲解了客户流失的分析方法、流程，同时详细介绍了机器学习中常用的分类算法、集成学习模型等通用技能，并使用阿里云机器学习PAI实现流失预警分析。可以帮助企业快速、准确识别流失客户，辅助制定策略进行客户关怀，达到挽留客户的目的。\n课程十二、大数据 - 使用DataV制作实时销售数据可视化大屏\n帮助非专业工程师通过图形化的界面轻松搭建专业水准的实时可视化数据大屏，以满足业务展示、业务监控、风险预警等多种业务的展示需求。\n课程十三、大数据 - 使用MaxCompute进行数据质量核查\n通过本案例，学员可了解影响数据质量的因素，出现数据质量问题的类型，掌握通过MaxCompute（DateIDE）设计数据质量监控的方法，最终独立解决常见的数据质量监控需求。\n课程十四、大数据 - 使用Quick BI制作图形化报表\n阿里云Quick BI制作图形化报表认证课程教你掌握将电商运营过程中的数据进行图表化展现，掌握通过Quick BI将数据制作成各种图形化报表的方法，同时还将掌握搭建企业级报表门户的方法。\n课程十五、大数据 - 使用时间序列分解模型预测商品销量\n使用时间序列分解模型预测商品销量教你掌握商品销量预测方法、时间序列分解以及熟悉相关产品的操作演示和项目介绍。\n课程十六、云安全 - 云平台使用安全\n阿里云云平台使用安全认证课程教你了解由传统IT到云计算架构的变迁过程、当前信息安全的现状和形势，以及在云计算时代不同系统架构中应该从哪些方面利用云平台的优势使用安全风险快速降低90%。\n课程十七、云安全 - 云上服务器安全\n阿里云云上服务器安全认证课程教你了解在互联网上提供计算功能的服务器主要面临哪些安全风险，并针对这些风险提供了切实可行的、免费的防护方案。\n课程十八、云安全 - 云上网络安全\n了解网络安全的原理和解决办法，以及应对DDoS攻击的方法和防护措施，确保云上网络的安全。\n课程十九、云安全 - 云上数据安全\n了解云上数据的安全隐患，掌握数据备份、数据加密、数据传输安全的解决方法。\n课程二十、云安全 - 云上应用安全\n了解常见的应用安全风险，SQL注入原理及防护，网站防篡改的解决方案等，确保云上应用的安全。\n课程二十一、云安全 - 云上安全管理\n了解云上的安全监控方法，学会使用监控大屏来监控安全风险，并能够自定义报警规则，确保随时掌握云上应用的安全情况。"}
{"content2":"第二届人工智能竞赛——题目五、FaceRank\n文章目录\n第二届人工智能竞赛——题目五、FaceRank\n一、简介\n二、题目要求\n三、参考资料\n一、简介\n我们常看到用机器学习识别字体，自动驾驶等项目，但是这题有个非常有趣的项目：FaceRank，它基于 TensorFlow CNN 模型，提供了一些图片处理的工具集，能够利用模型，为你的颜值打分。\n二、题目要求\n利用现有的深度学习模型实现“颜值打分”。\n利用 QT 设计 UI 界面，界面要求:友好、功能完善、简介。\n三、参考资料\nhttps://github.com/fendouai/FaceRank/\nhttps://github.com/pwfee/FaceRank\nUI界面可用QT编写"}
{"content2":"温馨提示：文末附福利领取地址，先到先得\n本月消息，继百度、腾讯之后，阿里也加入到无人驾驶领域。据《财新》报道该技术由AI实验室首席科学家王刚率领研究，并且进展很快。\n据悉，无人驾驶的市场预计可达7万亿美金，这么大的一块蛋糕，身为Python从业者，该如何下口？\n春光正好，莫负韶光，潭州Python学院创始人强子老师联袂新加坡国立大学教授高智博士、丰田汽车自动驾驶研究室负责人Sky老师等一众行业内大咖名师打造专属Python学习圈。\n恰逢五一，Python学习圈打造三重大礼助力学习，号召更多小伙伴加入我们，一同在苦短的人生中享受Python！\n一重礼\n新加坡国立大学博士在线分享AI精要\n演讲主题：计算机视觉和人工智能\n计算机视觉相当于是人工智能的大门，如果这个门不打开，就没有办法真的研究真实世界的人工智能。\n4月30日晚八点，新加坡国立大学高智博士将在潭州Python大咖讲堂展开一堂AI知识直播分享课。亚洲顶尖学者为你带来国际先进的理念和技术，与你展望AI的未来，在线答疑解惑，真正给你一个与大咖零距离接触的机会。\n新加坡国立大学教授\n高智老师\n高智博士在武汉大学获得本、硕、博学位，并在美国伊利诺伊大学香槟分校和牛津大学进行交流学习。高智博士在计算机视觉、遥感等人工智能核心领域具有坚实的专业基础和突出的研究成果，已发表学术论文近50篇，其中近20篇发表于世界著名杂志和会议，在全球最顶尖的人工智能领域期刊 IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI, 影响因子8. 329) 和 Springer International Journal of Computer Vision (IJCV, 影响因子8. 222) 上发表长篇论文。高智博士负责及参与了多项中国、新加坡人工智能领域的科研项目，包括复杂场景理解、异常检测等，取得一项技术专利。高智博士已指导多国学生，包括5名博士研究生，12名硕士研究生。指导学生在国际无人飞行系统大赛中获得过冠、亚军荣誉。高智博士担任30余种世界著名期刊的审稿人，编辑，及国际会议的专项主席。\n✲听课名额有限，请添加老师开通听课权限并领取资料\n二重礼\nPython技术直播课+专属学习助理\n行业名师在线授课，分享实用有趣的技术干货，从零基础到项目实战，无论你对Python掌握到了什么层次，总有适合你的那一堂。在线直播，名师互动，只为引领你推开Python的神秘大门！\n因为自制力不够而无法将学习沉淀？因为无人督促而无法将学习继续？不用担心，我们将为你配备一位专属学习助理，她会提醒你上课、写作业、及时发放你需要的学习资料，联系老师为你答疑解惑，还能陪你日常聊天，她就是你Python学习之路上最好的伙伴~\n✲下载潭州课堂视听效果更佳，随时随地随心学习\n福利三\n全套Python实战资料包\n不管是学生党还是上班族，想必都体验过抢票的疯狂吧，那是否能用Python实现自动抢票呢？\n由于12306官网没有提供相应的接口(也不可能提供)，我们只能通过自己寻找12306的数据包和买票流程来模拟浏览器行为实现自动化操作。\n价值¥1680的「通过Python实现12306自动抢票」全套实战资料包（附教学视频）可扫描文末二维码免费领取，仅限前500位有缘人，先到先得！\n以下是Python代码片段\n抓包查看浏览器与服务器的数据交互情况\n用Python对进行数据分析\n通过输入时间,城市获取目标的车次信息数据\n实现自动购票代码\n全套实战资料\n★独家资料，仅供学习使用，未经许可严禁挪作商用。\nAI直播课听课权限+\n12306实战资料领取方式\n长按或扫描识别下方二维码\n添加老师个人微信号即可免费领取\n添加老师的粉丝数量太多，部分小伙伴的资料包发放可能有一定延迟，希望大家能够谅解。\n老师介绍 · 强子\nPyCon China成员，前豆瓣Python开发技术总监\n8年软件开发经验， 擅长网站开发，网站运营（从0--日IP2W），软件开发，商业软件开发后自营\n现为潭州Python学院创始人，院长兼任潭州大数据中心数据挖掘部项目负责人！\n本次活动为免费活动，同时我们也提供付费课程服务，如有需要系统学习的同学，也可以咨询报名系统班级学习，完成从零基础到Python大咖的完美蜕变。"}
{"content2":"随着深度学习的不断发展，人工智能在未来几年将会出现井喷式的发展，而计算机视觉则是其重要的一个分支，计算机视觉是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。\n预计中国2018年计算机视觉的市场将会达到几百亿美元的市场，下面是最火的中国计算机视觉Top16公司，而且也是目前中国最受资本欢迎的公司。\n计算机视觉公司Top16\n公司名字\n成立时间\n公司地址\n是否融资\n1\n图普科技\n2013\n广州\n是\n2\n诺亦腾科技\n2012\n北京\n是\n3\n速感科技\n2014\n北京\n是\n4\nSenseTime\n2015\n北京\n是\n5\n码隆科技\n2015\n深圳\n是\n6\n格林深瞳\n2014\n北京\n是\n7\n依图科技\n2012\n上海\n是\n8\n旷视科技\n2013\n北京\n是\n9\n云从科技\n2015\n广州\n是\n10\n拍医拍\n2014\n北京\n是\n11\n北京陌上花科技有限公司\n2015\n北京\n是\n12\n图漾科技\n2015\n上海\n是\n13\n极视角\n2013\n深圳\n是\n14\nLinkface\n2014\n北京\n是\n15\n骏聿科技\n2010\n上海\n否\n16\n灏泷智能\n2011\n上海\n否\nNO.1 图普科技\n广州图普网络科技有限公司是一家站在人工智能前沿的创业科技公司，专注于图像识别整体解决方案。致力于打造新一代的计算机视觉理解和人工智能引擎，让计算机可以识人、识物，教会计算机看懂这个世界。\n公司主要为企业提供智能审核、图片增值、图像搜索、深度定制服务。产品在2015年4月上线后，仅半年已经和迅雷、酷狗、唱吧、58同城、秒拍、小咖秀、秀色互娱、21CN等国内知名公司正式合作，此外，通过入驻七牛云，融云，UCloud，我们的产品间接服务了数百个客户，业务规模在快速发展中。\nNO.2诺亦腾科技\n北京诺亦腾科技有限公司（Noitom Technology Ltd.）是一家在动作捕捉领域具有国际竞争力的公司，成立于2012年。公司核心团队由一群具有世界顶尖水准的工程师组成，公司主要创始人均具有海外留学与工作背景。研究领域涉及传感器、模态识别、运动科学、有限元分析、生物力学以及虚拟现实等。诺亦腾开发了具有国际领先水平的“基于MEMS惯性传感器的动作捕捉技术”，并在此基础上形成了一系列具有完全自主知识产权的低成本高精度动作捕捉产品。已经成功应用于动画与游戏制作、体育训练、医疗诊断、虚拟现实以及机器人等领域，并得到全球业内的高度认可。“Noitom”是英文“运动”（Motion）单词的倒序拼写，代表了公司目标：颠覆运动捕捉行业格局。\n目前Perception拥有两款产品：面向高端客户的Perception Legacy与面向个人开发者的Perception Neuron。在未来，公司将继续完善动作捕捉技术平台；诺亦腾还开发了mySwing，一套基于惯性原理动作捕捉的高精度高尔夫训练系统，采用当今世界领先的无线高速动作捕捉技术研发。这套系统将会为高尔夫训练带来革命性的变化。\nNO.3速感科技\n速感科技是一家以机器视觉为核心的人工智能创业公司。公司成立于2014年7月，并已先后完成两轮融资。团队主创人员为清华大学计算机系、信息设计系，北京航空航天大学计算机学院及美国宾夕法尼亚大学研究生与博士背景，并参与微软亚洲研究院、国家重点基础项目的技术合作。目前公司的主要产品为以视觉跟随为核心的智能跟随机器人，面向智能无人机、机器人的系统化操作解决方案，并以机器视觉为主要核心模块面向不同应用用户进行定制化的系统设计。\nNO.4 商汤科技\n商汤集团是一家科技创新公司，致力于引领人工智能核心“深度学习”技术突破，构建人工智能、大数据分析行业解决方案。在人工智能产业兴起的大背景下，商汤集团凭借在技术、人才、专利上超过十年的积累，迅速成为了人工智能行业领军企业。\n应用性技术上，基于深度学习的人脸识别、文字识别、人体识别、车辆识别、物体识别、图像处理等技术在业界遥遥领先；业务上，商汤集团深耕金融、移动互联网、安防监控三大行业，与银联、京东、拉卡拉、华为、小米、新浪微博、科大讯飞、东方网力、英伟达等各行业巨头深度合作，推动行业产品智能化升级。\nNO.5 码隆科技\n码隆科技（Malong Technologies）是一家专注于引领深度学习与计算机视觉技术突破的人工智能公司，致力于打造全球领先的视觉决策引擎，并为企业提供国际领先、定制化的计算机视觉解决方案。\nNO.6 格林深瞳\n格灵深瞳是一家将计算机视觉和深度学习技术应用于商业领域的科技公司，自主研发的深瞳技术在人和车的检测、跟踪与识别方面居于世界领先水平。公司借助海量数据，让计算机像人一样看懂这个世界，实时获取自然世界正在发生的一切，打造自然世界的搜索引擎。\nNO.7 旷视科技\n北京旷视科技有限公司是一家专注机器视觉和人工智能的极客技术公司，打造领先的人脸识别，图像识别，和深度学习技术服务云平台。旷视科技将致力于“先让机器看懂世界，再让机器真正思考”。\nNO.8 依图科技\n上海依图网络科技有限公司创立于2012年，是目前全球唯一能对人脸以及车辆都进行精确识别的公司。公司的愿景在于“变革人机视觉交互”，公司目前主要致力于计算机视觉、图像视频智能理解和分布式系统及大数据应用的研究。\n我们认为计算机视觉的突破需要三个要素：理论框架、真实场景数据、大规模计算平台。前者是学术界的擅长，但后两者的学术界局限大大限制了看问题的视角并阻碍了开拓的速度。\nNO.9云从科技\n广州云从信息科技有限公司（简称云从科技）是一家专注于计算机视觉与人工智能的高科技企业，核心技术源于四院院士、计算机视觉之父——Thomas S. Huang黄煦涛教授。核心团队曾于2007年到2011年6次斩获智能识别世界冠军，得到上市公司佳都科技与香港杰翱资本的战略投资。公司主要技术团队来自中国科学院重庆分院，是中科院研发实力最雄厚的人脸识别团队，并作为中科院战略性先导科技专项的唯一人脸识别团队，代表参与了新疆喀什等地安防布控。\nNO.10 拍医拍\n拍医拍是一家专注于大数据的移动医疗初创企业，通过机器视觉交互获取用户的诊疗数据，勾画用户真实有效的“医疗图谱”，通过大数据挖掘，构建以患者为中心的医疗健康生态圈。\n拍医拍的开发愿景是基于图像内容分析的医学影像管理；基于移动视觉的移动外伤治疗；基于机器学习的皮肤疾病智能诊断；基于谱分析的咽喉炎症分析；以及基于大数据分析的个体和群体健康感知与预测分析......\nNO.11 北京陌上花科技有限公司\n“衣+”是计算机视觉搜索引擎，团队来自斯坦福、新加坡国立大学、南洋理工大学、北大、上海交大、微软、IBM、阿里巴巴、百度等学校与企业。衣+目前已经和多家顶级企业合作，提供边看边买API、图像视频内容分析API、人脸属性识别API等服务海量用户。\nNO.12 图漾科技\n图漾科技是一家专业从事计算视觉的技术公司，专注于 三维深度信息测量的基础性技术研究。我们不断追求更 完善的传感器感知技术和方案，旨在为我们的客户提供 各种高效可靠的三维测量解决方案，增强产品对三维世 界的感知能力，帮助客户开发更加智能的产品。\nNO.13 极视角科技\n深圳极视角科技有限公司（Extreme Vision）是专业的计算机视觉与云端服务提供商，目前已为零售、通讯、地产、工业、餐饮、交通、公安等众多领域提供智能监控与视频分析服务，备受客户认可。公司总部设于中国深圳，并在澳门、北京、首尔等国内外城市设有服务支持团队，为客户及合作伙伴提供及时服务\n极视角拥有国际领先的视频分析技术，汇集了来自腾讯、华为、香港科技大学、北京大学等顶尖企业及实验室的博士研发团队，更邀请到全球首位跻身AAAI（国际人工智能协会）Fellow的华人科学家杨强教授担任首席技术顾问，让极视角的技术研发与世界接轨。\n目前极视角已为中国电信、恒大集团等客户提供服务，并获得众多客户的认可。未来极视角将致力于推出更丰富的视觉识别算法，为各领域、多样化的客户提供高质量的解决方案，与客户、合作商共赢发展。\nNO.14 Linkface\nLinkface以在线API和离线SDK的产品形式装载最新的人脸技术，提供给开发者和企业级用户。\n我们的人脸云平台搭载稳定版的人脸测试和识别模型，目前提供免费测试，并提供高并发无调用限额的企业级人脸\n云服务。企业客户可选择调用由我们维护的云服务，我们也帮助有需要的客户搭建其自有的私有人脸云服务。\nNO.15 骏聿科技\n骏聿科技的主要技术人员在物体跟踪识别领域均有超过 10 年的研究开发经验，有成功的商品应用；拥有 3 项在美国、日本的发明专利，8 项国内专利授权；获得 2008 年国务院颁发 的国家科技进步二等奖；在国际最顶尖的机器学习，人工智能，计算机视觉等学术会议杂志 等发表多篇文章。 骏聿科技管理团队成员分别拥有清华大学博士学位、 英国兰卡斯特大学硕士学位和复旦大学 MBA 学位；有先进的管理经验并有严格的质量控制流程和标准。\n在图像识别领域最前沿的人脸识别、目标对象识别和行为识别位于全球前列，其中人脸识别技术在全球一些评测中位列前茅，行人流量统计（拥挤度分析）居世界领先行列。\n公司的人脸识别产品在金融、银行、社保、公交、航空、ATM 等行业进行了成功的推广应用。公司是国内在人脸识别领域的技术创新及产业化领先者。\nNO.16 灏泷智能\n公司核心团队以原中国人民解放军某部，军用图形图像技术转业专家发起组建，公司所研发的视觉识别技术，通过日本、欧洲多家跨国电子企业及国内车厂的综合测试要求。\n灏泷智能科技，在车联网“主动安全驾驶“领域，作为国内外开发高级驾驶辅助系统的先行者，中国领先的“车辆视觉识别预警系统“技术供应商。\n灏泷智能科技的客户包括，前装市场：整车制造商、一级集成供应商；以及后装市场用户：客运、货运、危险运输、出租车、校车、高铁机车、巴士公交等交通部强制监管的“两客一危“运输企业等。"}
{"content2":"【前言】\n本文首发于：泰泰博客——Python计算机视觉编程，大数据、人工智能学习书籍。\n今天给大家推荐一本好书——Python计算机视觉编程。如果你在寻找关于大数据相关的学习书籍或准备“进军”大数据、人工智能的朋友，那么你可以看一下今天泰泰分享的这一本书（电子书）。\n该书是计算机视觉编程的权威实践指南，依赖 Python 语言讲解了基础理论与算法，并通过大量示例细致分析了对象识别、基于内容的图像搜索、光学字符识别、光流法、跟踪、三维重建、立体成像、增强现实、姿态估计、全景创建、图像分割、降噪、图像分组等技术。另外，书中附带的练习还能让读者巩固并学会应用编程知识。\n这本书适合的读者是 ：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。\n章节目录\n第 1 章　基本的图像操作和处理 ............................................................................................ 1\n1.1　PIL：Python 图像处理类库 .......................................................................................... 1\n1.1.1　转换图像格式 ................................................................................................................ 2\n1.1.2　创建缩略图 .................................................................................................................... 3\n1.1.3　复制和粘贴图像区域 .................................................................................................... 3\n1.1.4　调整尺寸和旋转 ............................................................................................................ 3\n1.2　 Matplotlib ....................................................................................................................... 4\n1.2.1　绘制图像、点和线 ........................................................................................................ 4\n1.2.2　图像轮廓和直方图 ........................................................................................................ 6\n1.2.3　交互式标注 .................................................................................................................... 7\n1.3　 NumPy ............................................................................................................................ 8\n1.3.1　图像数组表示 ................................................................................................................ 8\n1.3.2　灰度变换 ........................................................................................................................ 9\n1.3.3　图像缩放 ...................................................................................................................... 11\n1.3.4　直方图均衡化 .............................................................................................................. 11\n1.3.5　图像平均 ...................................................................................................................... 13\n1.3.6　图像的主成分分析（PCA）..................................................................................... 14\n1.3.7　使用 pickle 模块 ......................................................................................................... 16\n1.4　 SciPy .............................................................................................................................. 17\n1.4.1　图像模糊 ...................................................................................................................... 18\n1.4.2　图像导数 ...................................................................................................................... 19\n1.4.3　形态学：对象计数 ..................................................................................................... 22\n1.4.4　一些有用的 SciPy 模块 ........................................................................................... 23\n1.5　高级示例：图像去噪 .................................................................................................... 24\n练习 .......................................................................................................................................... 28\n代码示例约定 ......................................................................................................................... 29\n第 2 章　局部图像描述子 ......................................................................................................... 31\n2.1　Harris 角点检测器 ............................................................................................................ 31\n2.2　SIFT（尺度不变特征变换）..............................................................................................39\n2.2.1　兴趣点 .......................................................................................................................... 39\n2.2.2　描述子 .......................................................................................................................... 39\n2.2.3　检测兴趣点 .................................................................................................................. 40\n2.2.4　匹配描述子 .................................................................................................................. 43\n2.3　匹配地理标记图像 ......................................................................................................... 47\n2.3.1　从 Panoramio 下载地理标记图像 ......................................................................... 47\n2.3.2　使用局部描述子匹配 .................................................................................................. 50\n2.3.3　可视化连接的图像 ...................................................................................................... 52\n练习 ............................................................................................................................................. 54\n第 3 章　图像到图像的映射 ..................................................................................................57\n3.1　单应性变换 .......................................................................................................................57\n3.1.1　直接线性变换算法 ...................................................................................................... 59\n3.1.2　仿射变换 ...................................................................................................................... 60\n3.2　图像扭曲 ...........................................................................................................................61\n3.2.1　图像中的图像 .............................................................................................................. 63\n3.2.2　分段仿射扭曲 .............................................................................................................. 67\n3.2.3　图像配准 ...................................................................................................................... 70\n3.3　创建全景图 ...................................................................................................................... 76\n3.3.1　RANSAC ...................................................................................................................... 77\n3.3.2　稳健的单应性矩阵估计 .............................................................................................. 78\n3.3.3　拼接图像 ...................................................................................................................... 81\n练习 ..............................................................................................................................................84\n第 4 章　照相机模型与增强现实 ..........................................................................................85\n4.1　针孔照相机模型 ...............................................................................................................85\n4.1.1　照相机矩阵 .................................................................................................................. 86\n4.1.2　三维点的投影 .............................................................................................................. 87\n4.1.3　照相机矩阵的分解 ...................................................................................................... 89\n4.1.4　计算照相机中心 .......................................................................................................... 90\n4.2　照相机标定 ...................................................................................................................... 91\n目录 ｜ VII\n4.3　以平面和标记物进行姿态估计 .......................................................................................93\n4.4　增强现实 ........................................................................................................................... 97\n4.4.1　PyGame 和 PyOpenGL ............................................................................................ 97\n4.4.2　从照相机矩阵到 OpenGL 格式 ................................................................................98\n4.4.3　在图像中放置虚拟物体 ............................................................................................ 100\n4.4.4　综合集成 .................................................................................................................... 102\n4.4.5　载入模型 .................................................................................................................... 104\n练习 .................................................................................................................................................. 106\n第 5 章　多视图几何 ....................................................................................................................... 107\n5.1　外极几何 ................................................................................................................................. 107\n5.1.1　一个简单的数据集 .................................................................................................... 109\n5.1.2　用 Matplotlib 绘制三维数据 ................................................................................... 111\n5.1.3　计算 F：八点法 ......................................................................................................... 112\n5.1.4　外极点和外极线 ........................................................................................................ 113\n5.2　照相机和三维结构的计算 ............................................................................................ 116\n5.2.1　三角剖分 .................................................................................................................... 116\n5.2.2　由三维点计算照相机矩阵 ........................................................................................ 118\n5.2.3　由基础矩阵计算照相机矩阵 .................................................................................... 120\n5.3　多视图重建 ...................................................................................................................... 122\n5.3.1　稳健估计基础矩阵 .................................................................................................... 123\n5.3.2　三维重建示例 ............................................................................................................ 125\n5.3.3　多视图的扩展示例 .................................................................................................... 129\n5.4　立体图像 ...........................................................................................................................130\n练习 .............................................................................................................................................. 135\n第 6 章　图像聚类 ....................................................................................................................137\n6.1　K-means 聚类 .................................................................................................................137\n6.1.1　 SciPy 聚类包 .............................................................................................................. 138\n6.1.2　图像聚类 .................................................................................................................... 139\n6.1.3　在主成分上可视化图像 ............................................................................................ 140\n6.1.4　像素聚类 .................................................................................................................... 142\n6.2　层次聚类 ................................................................................................................................. 144\n6.3　谱聚类 ..................................................................................................................................... 152\n练习 .................................................................................................................................................. 157\n第 7 章　图像搜索 ............................................................................................................................ 159\n7.1　基于内容的图像检索 ............................................................................................................. 159\n7.2　视觉单词 ................................................................................................................................. 160\n7.3　图像索引 ................................................................................................................................. 164\n7.3.1　建立数据库 ................................................................................................................ 164\n7.3.2　添加图像 .................................................................................................................... 165\n7.4　在数据库中搜索图像 .................................................................................................... 167\n7.4.1　利用索引获取候选图像 ............................................................................................ 168\n7.4.2　用一幅图像进行查询 ................................................................................................ 169\n7.4.3　确定对比基准并绘制结果 ........................................................................................ 171\n7.5　使用几何特性对结果排序 ............................................................................................... 172\n7.6　建立演示程序及 Web 应用 ............................................................................................. 176\n7.6.1　用 CherryPy 创建 Web 应用 ...................................................................................... 176\n7.6.2　图像搜索演示程序 .................................................................................................... 176\n练习 .................................................................................................................................................. 179\n第 8 章　图像内容分类 ................................................................................................................... 181\n8.1　K 邻近分类法（KNN）......................................................................................................... 181\n8.1.1　一个简单的二维示例 ................................................................................................ 182\n8.1.2　用稠密 SIFT 作为图像特征 ...................................................................................... 185\n8.1.3　图像分类：手势识别 ................................................................................................ 187\n8.2　贝叶斯分类器 .................................................................................................................190\n8.3　支持向量机 .......................................................................................................................195\n8.3.1　使用 LibSVM ............................................................................................................. 196\n8.3.2　再论手势识别 ............................................................................................................ 198\n8.4　光学字符识别 ................................................................................................................. 199\n8.4.1　训练分类器 ................................................................................................................ 200\n8.4.2　选取特征 .................................................................................................................... 200\n8.4.3　多类支持向量机 ........................................................................................................ 201\n8.4.4　提取单元格并识别字符 ............................................................................................ 202\n8.4.5　图像校正 .................................................................................................................... 205\n练习 .......................................................................................................................................... 206\n第 9 章　图像分割 ................................................................................................................. 209\n9.1　图割（Graph Cut）......................................................................................................209\n9.1.1　从图像创建图 ............................................................................................................ 211\n9.1.2　用户交互式分割 ........................................................................................................ 216\n9.2　利用聚类进行分割 ................................................................................................................. 218\n9.3　变分法 ..................................................................................................................................... 224\n练习 .................................................................................................................................................. 226\n第 10 章　OpenCV .......................................................................................................................... 227\n10.1　OpenCV 的 Python 接口 ...................................................................................................... 227\n下载地址\n网盘链接: https://pan.baidu.com/s/1n43-n0U7d1N9gNSlSylflw 密码: cfsi\n声明：本电子书是从网络搜集过来，然后再次分享给广大的学者，意在帮助到大家。书籍所有权归作者所有，如果本站侵权，请联系联系站长，谢谢！"}
{"content2":"The M Tank 编辑了一份报告《A Year in Computer Vision》，记录了 2016 至 2017 年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。该材料共包括四大部分，在本文中机器之心对第一部分做了编译介绍，后续会放出其他部分内容。\n内容目录\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率、风格迁移、着色\n动作识别\n第三部分\n3D 目标\n人体姿势估计\n3D 重建\n其他未分类 3D\n总结\n第四部分\n卷积架构\n数据集\n不可分类的其他材料与有趣趋势\n结论\n完整 PDF 地址：http://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf\n简介\n计算机视觉是关于研究机器视觉能力的学科，或者说是使机器能对环境和其中的刺激进行可视化分析的学科。机器视觉通常涉及对图像或视频的评估，英国机器视觉协会（BMVA）将机器视觉定义为「对单张图像或一系列图像的有用信息进行自动提取、分析和理解」。\n对我们环境的真正理解不是仅通过视觉表征就可以达成的。更准确地说，是视觉线索通过视觉神经传输到主视觉皮层，然后由大脑以高度特征化的形式进行分析的过程。从这种感觉信息中提取解释几乎包含了我们所有的自然演化和主体经验，即进化如何令我们生存下来，以及我们如何在一生中对世界进行学习和理解。\n从这方面来说，视觉过程仅仅是传输图像并进行解释的过程，然而从计算的角度看，图像其实更接近思想或认知，涉及大脑的大量功能。因此，由于跨领域特性很显著，很多人认为计算机视觉是对视觉环境和其中语境的真实理解，并将引领我们实现强人工智能。\n不过，我们目前仍然处于这个领域发展的胚胎期。这篇文章的目的在于阐明 2016 至 2017 年计算机视觉最主要的进步，以及这些进步对实际应用的促进。\n为简单起见，这篇文章将仅限于基本的定义，并会省略很多内容，特别是关于各种卷积神经网络的设计架构等方面。\n这里推荐一些学习资料，其中前两个适用与初学者快速打好基础，后两个可以作为进阶学习：\nAndrej Karpathy:「What a Deep Neural Network thinks about your #selfie」，这是理解 CNN 的应用和设计功能的最好文章 [4]。\nQuora:「what is a convolutional neural network?」，解释清晰明了，尤其适合初学者 [5]。\nCS231n: Convolutional Neural Networks for Visual Recognition，斯坦福大学课程，是进阶学习的绝佳资源 [6]。\nDeep Learning(Goodfellow,Bengio&Courville,2016)，这本书在第 9 章提供了对 CNN 的特征和架构设计等详尽解释，网上有免费资源 [7]。\n对于还想进一步了解神经网络和深度学习的，我们推荐：\nNeural Networks and Deep Learning(Nielsen,2017)，这是一本免费在线书籍，可为读者提供对神经网络和深度学习的复杂性的直观理解。即使只阅读了第 1 章也可以帮助初学者透彻地理解这篇文章。\n下面我们先简介本文的第一部分，这一部分主要叙述了目标分类与定位、目标检测与目标追踪等十分基础与流行的计算机视觉任务。而后机器之心将陆续分享 Benjamin F. Duffy 和 Daniel R. Flynn 后面 3 部分对计算机视觉论述，包括第二部分的语义分割、超分辨率、风格迁移和动作识别，第三部分三维目标识别与重建、和第四部分卷积网络的架构与数据集等内容。\n基础的计算机视觉任务\n分类/定位\n图像分类任务通常是指为整张图像分配特定的标签，如下左图整张图像的标签为 CAT。而定位是指找到识别目标在图像中出现的位置，通常这种位置信息将由对象周围的一些边界框表示出来。目前 ImageNet [9] 上的分类/定位的准确度已经超过了一组训练有素的人类 [10]。因此相对于前一部分的基础，我们会着重介绍后面如语义分割、3D 重建等内容。\n图 1：计算机视觉任务，来源 cs231n 课程资料。\n然而随着目标类别 [11] 的增加，引入大型数据集将为近期的研究进展提供新的度量标准。在这一方面，Keras [12] 创始人 Francois Chollet 将包括 Xception 等架构和新技术应用到谷歌内部的大型数据集中，该数据集包含 1.7 万个目标类别，共计 350M（Million）的多类别图像。\n图 2：ILSVRC 竞赛中，分类/定位的逐年错误率，来源 Jia Deng (2016)，ILSVRC2016。\nImageNet LSVRC（2016）亮点：\n场景分类是指用「温室」、「体育场」和「大教堂」等特定场景对图像进行分类。ImageNet 去年举办了基于 Places2[15] 子数据的场景分类挑战赛，该数据集有 365 个场景共计 8 百万 训练图像。海康威视 [16] 选择了深度类 Inception 的网络和并不太深的 ResNet，并利用它们的集成实现 9% 的 Top-5 误差率以赢得竞赛。\nTrimps-Soushen 以 2.99% 的 Top-5 分类误差率和 7.71% 的定位误差率赢得了 ImageNet 分类任务的胜利。该团队使用了分类模型的集成（即 Inception、Inception-ResNet、ResNet 和宽度残差网络模块 [17] 的平均结果）和基于标注的定位模型 Faster R-CNN [18] 来完成任务。训练数据集有 1000 个类别共计 120 万的图像数据，分割的测试集还包括训练未见过的 10 万张测试图像。\nFacebook 的 ResNeXt 通过使用从原始 ResNet [19] 扩展出来的新架构而实现了 3.03% 的 Top-5 分类误差率。\n目标检测\n目标检测（Object Detection）即如字面所说的检测图像中包含的物体或目标。ILSVRC 2016 [20] 对目标检测的定义为输出单个物体或对象的边界框与标签。这与分类/定位任务不同，目标检测将分类和定位技术应用到一张图像的多个目标而不是一个主要的目标。\n图 3：仅有人脸一个类别的目标检测。图为人脸检测的一个示例，作者表示目标识别的一个问题是小物体检测，检测图中较小的人脸有助于挖掘模型的尺度不变性、图像分辨率和情景推理的能力，来源 Hu and Ramanan (2016, p. 1)[21]。\n目标识别领域在 2016 年主要的趋势之一是转向更快、更高效的检测系统。这一特性在 YOLO、SSD 和 R-FCN 方法上非常显著，它们都倾向于在整张图像上共享计算。因此可以将它们与 Fast/Faster R-CNN 等成本较高的子网络技术区分开开来，这些更快和高效的检测系统通常可以指代「端到端的训练或学习」。\n这种共享计算的基本原理通常是避免将独立的算法聚焦在各自的子问题上，因为这样可以避免训练时长的增加和网络准确度的降低。也就是说这种端到端的适应性网络通常发生在子网络解决方案的初始之后，因此是一种可回溯的优化（retrospective optimisation）。然而，Fast/Faster R-CNN 技术仍然非常有效，仍然广泛用于目标检测任务。\nSSD：Single Shot MultiBox Detector[22] 利用封装了所有必要计算并消除了高成本通信的单一神经网络，以实现了 75.1% mAP 和超过 Faster R-CNN 模型的性能（Liu et al. 2016）。\n我们在 2016 年看到最引人注目的系统是「YOLO9000: Better, Faster, Stronger」[23]，它引入了 YOLOv2 和 YOLO9000 检测系统 [24]。YOLOv2 很大程度上提升了 2015 年提出的 YOLO 模型 [25] 性能，它能以非常高的 FPS（使用原版 GTX Titan X 在低分辨率图像上达到 90FPS）实现更好的结果。除了完成的速度外，系统在特定目标检测数据集上准确度要优于带有 ReNet 和 SSD 的 Faster RCNN。\nYOLO9000 实现了检测和分类的联合训练，并将其预测泛化能力扩展到未知的检测数据上，即它能检测从未见过的目标或物体。YOLO9000 模型提供了 9000 多个类别的实时目标检测，缩小了分类和检测数据集间的鸿沟。该模型其它详细的信息和预训练模型请查看：http://pjreddie.com/darknet/yolo/。\nFeature Pyramid Networks for Object Detection [27] 是 FAIR [28] 实验室提出的，它能利用「深度卷积网络的内部多尺度、金字塔型的层级结构构建具有边际额外成本的特征金字塔」，这意味着表征能更强大和快速。Lin et al. (2016) 在 COCO[29] 数据集上实现了顶尖的单模型结果。若与基础的 Faster R-CNN 相结合，将超过 2016 年最好的结果。\nR-FCN：Object Detection via Region-based Fully Convolutional Networks [30]，这是另一种在图像上避免应用数百次高成本的各区域子网络方法，它通过使基于区域的检测器在整张图像上进行全卷积和共享计算。「我们每张图像的测试时间只需要 170ms，要比 Faster R-CNN 快 2.5 到 20 倍」(Dai et al., 2016)。\n图 4：目标检测中的准确率权衡，来源 Huang et al. (2016, p. 9)[31]。\n注意：Y 轴表示的是平均准确率（mAP），X 轴表示不同元架构（meta-architecture）的各种特征提取器（VGG、MobileNet...Inception ResNet V2）。此外，mAP small、medium 和 large 分别表示对小型、中型和大型目标的检测平均准确率。即准确率是按「目标尺寸、元架构和特征提取器」进行分层的，并且图像的分辨率固定为 300。虽然 Faster R-CNN 在上述样本中表现得更好，但是这并没有什么价值，因为该元架构相比 R-FCN 来说慢得多。\nHuang et al. (2016)[32] 的论文提供了 R-FCN、SSD 和 Faster R-CNN 的深度性能对比。由于机器学习准确率对比中存在的问题，这里使用的是一种标准化的方法。这些架构被视为元架构，因为它们可以组合不同的特征提取器，比如 ResNet 或 Inception。\n论文的作者通过改变元架构、特征提取器和图像分辨率研究准确率和速度之间的权衡。例如，对不同特征提取器的选择可以造成元架构对比的非常大的变化。\n实时商业应用中需要低功耗和高效同时能保持准确率的目标检测方法，尤其是自动驾驶应用，SqueezeDet[33] 和 PVANet[34] 在论文中描述了这种发展趋势。\nCOCO[36] 是另一个常用的图像数据集。然而，它相对于 ImageNet 来说更小，更常被用作备选数据集。ImageNet 聚焦于目标识别，拥有情景理解的更广泛的语境。组织者主办了一场包括目标检测、分割和关键点标注的年度挑战赛。在 ILSVRC[37] 和 COCO[38] 上进行的目标检测挑战赛的结果如下：\nImageNet LSVRC 图像目标检测（DET）：CUImage 66% 平均准确率，在 200 个类别中有 109 个胜出。\nImageNet LSVRC 视频目标检测（VID）：NUIST 80.8% 平均准确率。\nImageNet LSVRC 视频追踪目标检测：CUvideo 55.8% 平均准确率。\nCOCO 2016 目标检测挑战赛（边界框）：G-RMI（谷歌）41.5% 平均准确率（比 2015 的胜者 MSRAVC 高出 4.2% 绝对百分点）。\n从以上结果可以看出，在 ImageNet 上的结果表明「MSRAVC 2015 的结果为『引入 ResNet』设置了很高的标准。在整个项目中对所有的类别的目标检测性能都有所提升。在两个挑战赛中，定位任务的性能都得到较大的提升。关于小型目标实例的大幅性能提升结果详见参考文献」（ImageNet,2016）。[39]\n图 5.ILSVRC 的图像目标检测结果（2013-2016），来源 ImageNet. 2016. [Online] Workshop\n目标追踪\n目标追踪即在给定的场景中追踪感兴趣的一个或多个特定目标的过程，在视频和现实世界的交互中（通常是从追踪初始的目标检测开始的）有很多应用，且对于自动驾驶而言非常重要。\nFully-Convolutional Siamese Networks for Object Tracking[40]，将一个连体网络（Siamese network）结合一个基础的追踪算法，使用端到端的训练方法，达到了当前最佳，图框显示率超过了实时应用的需求。这篇论文利用传统在线学习方法构建追踪模型。\nLearning to Track at 100 FPS with Deep Regression Networks[41]，该论文试图改善在线训练方法中存在的缺陷。他们构建了一个使用前馈网络学习目标运动、外观和方向中的普遍关系的追踪器，从而可以在没有在线训练的情况下有效地追踪到新的目标。该算法在一个标准的追踪基准测试中达到了当前最佳，同时可以 100FPS 的帧数追踪所有的目标（Held et al.,2016）。\nDeep Motion Features for Visual Tracking[43] 结合了手工设计的特征、深度外观特征（利用 CNN）和深度运动特征（在光流图像上训练），并取得了当前最佳的结果。虽然深度运动特征在动作识别和视频分类中很常见，但作者声称这是其首次被应用于视觉追踪上。该论文获得了 ICPR2016 的「计算机视觉和机器人视觉」的最佳论文。\n「本论文展示了深度运动特征（motion features）对检测和追踪框架的影响。我们还进一步说明了手工制作的特征、深度 RGB 和深度运用特征包含互补信息。据我们所知，这是第一个提出融合外表信息和深度运动特征，并用于视觉追踪的研究。我们全面的实验表明融合方法具有深度运动特征，并超过了单纯依赖外表信息的方法。」\nVirtual Worlds as Proxy for Multi-Object Tracking Analysis [44] 方法解决了现有虚拟世界中缺乏真实可变性视频追踪基准和数据集。该论文提出了一种新的真实世界复制方法，该方法从头开始生成丰富、虚拟、合成和照片逼真的环境。此外，该方法还能克服现有数据集中存在的一些内容匮乏问题。生成的图像能自动通过正确的真值进行标注，并允许应用于除目标检测/追踪外其它如光流等任务。\nGlobally Optimal Object Tracking with Fully Convolutional Networks [45] 专注处理目标变化和遮挡，并将它们作为目标追踪的两个根本限制。「我们提出的方法通过使用全卷积网络解决物体或目标外表的变化，还通过动态规划的方法解决遮挡情况」(Lee et al., 2016)。\n参考文献：\n[1] British Machine Vision Association (BMVA). 2016. What is computer vision? [Online] Available at: http://www.bmva.org/visionoverview [Accessed 21/12/2016]\n[2] Krizhevsky, A., Sutskever, I. and Hinton, G. E. 2012. ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada. Available: http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf\n[3] Kuhn, T. S. 1962. The Structure of Scientific Revolutions. 4th ed. United States: The University of Chicago Press.\n[4] Karpathy, A. 2015. What a Deep Neural Network thinks about your #selfie. [Blog] Andrej Karpathy Blog. Available: http://karpathy.github.io/2015/10/25/selfie/ [Accessed: 21/12/2016]\n[5] Quora. 2016. What is a convolutional neural network? [Online] Available: https://www.quora.com/What-is-a-convolutional-neural-network [Accessed: 21/12/2016]\n[6] Stanford University. 2016. Convolutional Neural Networks for Visual Recognition. [Online] CS231n. Available: http://cs231n.stanford.edu/ [Accessed 21/12/2016]\n[7] Goodfellow et al. 2016. Deep Learning. MIT Press. [Online] http://www.deeplearningbook.org/ [Accessed: 21/12/2016] Note: Chapter 9, Convolutional Networks [Available: http://www.deeplearningbook.org/contents/convnets.html]\n[8] Nielsen, M. 2017. Neural Networks and Deep Learning. [Online] EBook. Available: http://neuralnetworksanddeeplearning.com/index.html [Accessed: 06/03/2017].\n[9] ImageNet refers to a popular image dataset for Computer Vision. Each year entrants compete in a series of different tasks called the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Available: http://image-net.org/challenges/LSVRC/2016/index\n[10] See「What I learned from competing against a ConvNet on ImageNet」by Andrej Karpathy. The blog post details the author』s journey to provide a human benchmark against the ILSVRC 2014 dataset. The error rate was approximately 5.1% versus a then state-of-the-art GoogLeNet classification error of 6.8%. Available: http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\n[11] See new datasets later in this piece.\n[12] Keras is a popular neural network-based deep learning library: https://keras.io/\n[13] Chollet, F. 2016. Information-theoretical label embeddings for large-scale image classification. [Online] arXiv: 1607.05691. Available: arXiv:1607.05691v1\n[14] Chollet, F. 2016. Xception: Deep Learning with Depthwise Separable Convolutions. [Online] arXiv:1610.02357. Available: arXiv:1610.02357v2\n[15] Places2 dataset, details available: http://places2.csail.mit.edu/. See also new datasets section.\n[16] Hikvision. 2016. Hikvision ranked No.1 in Scene Classification at ImageNet 2016 challenge. [Online] Security News Desk. Available: http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/ [Accessed: 20/03/2017].\n[17] See Residual Networks in Part Four of this publication for more details.\n[18] Details available under team information Trimps-Soushen from: http://image-net.org/challenges/LSVRC/2016/results\n[19] Xie, S., Girshick, R., Dollar, P., Tu, Z. & He, K. 2016. Aggregated Residual Transformations for Deep Neural Networks. [Online] arXiv: 1611.05431. Available: arXiv:1611.05431v1\n[20] ImageNet Large Scale Visual Recognition Challenge (2016), Part II, Available: http://image-net.org/challenges/LSVRC/2016/ [Accessed: 22/11/2016]\n[21] Hu and Ramanan. 2016. Finding Tiny Faces. [Online] arXiv: 1612.04402. Available: arXiv:1612.04402v1\n[22] Liu et al. 2016. SSD: Single Shot MultiBox Detector. [Online] arXiv: 1512.02325v5. Available: arXiv:1512.02325v5\n[23] Redmon, J. Farhadi, A. 2016. YOLO9000: Better, Faster, Stronger. [Online] arXiv: 1612.08242v1. Available: arXiv:1612.08242v1\n[24] YOLO stands for「You Only Look Once」.\n[25] Redmon et al. 2016. You Only Look Once: Unified, Real-Time Object Detection. [Online] arXiv: 1506.02640. Available: arXiv:1506.02640v5\n[26]Redmon. 2017. YOLO: Real-Time Object Detection. [Website] pjreddie.com. Available: https://pjreddie.com/darknet/yolo/ [Accessed: 01/03/2017].\n[27] Lin et al. 2016. Feature Pyramid Networks for Object Detection. [Online] arXiv: 1612.03144. Available: arXiv:1612.03144v1\n[28] Facebook's Artificial Intelligence Research\n[29] Common Objects in Context (COCO) image dataset\n[30] Dai et al. 2016. R-FCN: Object Detection via Region-based Fully Convolutional Networks. [Online] arXiv: 1605.06409. Available: arXiv:1605.06409v2\n[31] Huang et al. 2016. Speed/accuracy trade-offs for modern convolutional object detectors. [Online] arXiv: 1611.10012. Available: arXiv:1611.10012v1\n[32] ibid\n[33] Wu et al. 2016. SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving. [Online] arXiv: 1612.01051. Available: arXiv:1612.01051v2\n[34] Hong et al. 2016. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection. [Online] arXiv: 1611.08588v2. Available: arXiv:1611.08588v2\n[35] DeepGlint Official. 2016. DeepGlint CVPR2016. [Online] Youtube.com. Available: https://www.youtube.com/watch?v=xhp47v5OBXQ [Accessed: 01/03/2017].\n[36] COCO - Common Objects in Common. 2016. [Website] Available: http://mscoco.org/ [Accessed: 04/01/2017].\n[37] ILSRVC results taken from: ImageNet. 2016. Large Scale Visual Recognition Challenge 2016.\n[Website] Object Detection. Available: http://image-net.org/challenges/LSVRC/2016/results [Accessed: 04/01/2017].\n[38] COCO Detection Challenge results taken from: COCO - Common Objects in Common. 2016. Detections Leaderboard [Website] mscoco.org. Available: http://mscoco.org/dataset/#detections-leaderboard [Accessed: 05/01/2017].\n[39] ImageNet. 2016. [Online] Workshop Presentation, Slide 31. Available: http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf [Accessed: 06/01/2017].\n[40] Bertinetto et al. 2016. Fully-Convolutional Siamese Networks for Object Tracking. [Online] arXiv: 1606.09549. Available: https://arxiv.org/abs/1606.09549v2\n[41] Held et al. 2016. Learning to Track at 100 FPS with Deep Regression Networks. [Online] arXiv: 1604.01802. Available: https://arxiv.org/abs/1604.01802v2\n[42] David Held. 2016. GOTURN - a neural network tracker. [Online] YouTube.com. Available: https://www.youtube.com/watch?v=kMhwXnLgT_I [Accessed: 03/03/2017].\n[43] Gladh et al. 2016. Deep Motion Features for Visual Tracking. [Online] arXiv: 1612.06615. Available: arXiv:1612.06615v1\n[44] Gaidon et al. 2016. Virtual Worlds as Proxy for Multi-Object Tracking Analysis. [Online] arXiv: 1605.06457. Available: arXiv:1605.06457v1\n[45] Lee et al. 2016. Globally Optimal Object Tracking with Fully Convolutional Networks. [Online] arXiv: 1612.08274. Available: arXiv:1612.08274v1\n原报告地址：http://www.themtank.org/a-year-in-computer-vision"}
{"content2":"计算机视觉\n赋予机器人“看”的功能正是“机器视觉”这个学科所研究的问题之一。这一领域十分广阔，不仅包括通用技术，而且也包括为数众多的专用技术——如NLP、指纹识别、相片解释和机器人控制等等。这里仅介绍一些计算机视觉的概念。\n引言\n计算机视觉首先是在一组感光性原件上，生成一个场景的图像。这个图像是摄像机通过镜头对在视野中的场景进行一个透视投影，然后光电元件将其转化为一个二维的、随时间变化的亮度矩阵图像I(x,y,t)，其中x，y为光电元件在数组中的位置，t为时间（对于有色视觉则需要三个这样的矩阵来代表三原色）。一个由视觉引导的响应agent必须通过处理这个矩阵来产生这个场景的图标模型或者一组特征，从而使他能直接计算一个动作。\n希望获取信息的种类取决于agent的目的和任务。若要让一个agent平安地通过一个混乱的环境，这个agent必须了解其中物体的位置、边界、通路以及它所经路径表面的特性。agent也许还应具备根据每隔一段时间所有以上信息的变化来预测将来了能的变化。从一个或多个图像中获取此类信息将及其困难，所以，只能给出这类技术的一个概况。\n操纵一辆汽车\n在S-R agent的一些应用中，神经网络可用来把图像亮度矩阵直接转化为动作。其中一个突出的例子就是用来驾驶一辆汽车的ALVINN系统。\n网络第一层有5个隐藏单元，第二层有30个输出单元，以上所有单元均为sigmoid单元。输出单元通过线性排列来控制汽车高度。若此输出单元队列顶端附近的一个输出单元比其他大多数输出单元高，则车向左行驶；若此输出单元队列底端附近的一个输出单元比其他大多数输出单元高，则车向右行驶。\n此系统由改进过的“在空中（on-the-fly）”训练方式来传播，真人驾驶员开车，实际的驾驶角度被作为相应输入的正确标志。网络以反向传播的方式递增训练，从而使他能用驾驶员所指定的驾驶角度来响应实际驾驶车辆时出现的每一个视觉模式。\n机器视觉的两个阶段\n图像处理阶段把原始图像转换成更适合于景物分析的图像。\n图像处理包括降噪、增强边缘和寻找图像区域等不同的滤波操作。\n景物分析主要试图从已处理的图像中产生一个对原始场景的图标描述或基于特征的描述，并提供agent所处环境中与特定任务有关的信息。\n图像处理\n1. 平均法\n假设初始图像可表达为一个m*n的数组I(x,y),我们称之为“图像亮度数组”。他把图像平面分成许多被称为“像素（pixel）”的单元。这些数字表示这幅图相中某点的光亮度，图像中一些不规则之处可通过求平均数的方法得以平滑。这种滑动并求和的操作称为“卷积”。如果我们的得到的数组十二进制（1或0），那么就必须把这些加权总和和一个阈值比较。平均法不仅将压缩孤立的噪音点，而且将减小图像的卷曲度（crispness），放弃那些微不足道的图像元素。\n有时，我们把加权函数W(x,y)的值在x和y构成的长方形内看做1，长方形之外看做0.长方形大小决定平滑度，长方形越大平滑度越高。下图展示了一个求平均数操作是如何让对一个二进制图像先用一个长方形平滑函数平滑，然后将其与阈值比较来进行操作的。\n我们发现这个平滑操作加粗了宽线，去除了窄线和微小细节。\n2. 边缘增强\n如前所述，计算机视觉常常设计图像边缘的提取，然后用这些边缘来把图像转换成某种线条图形。获取轮廓的方法之一是先增强图像中的边界和边缘，边缘可以是图像个部分之间的任意边界。\n我们可以通过在以为图像上卷积一个位于垂直线上的、一半为负一半为正的窗口来增强这些图像的边缘强度。\n3. 边缘增强和平均法的结合\n还有其他的变化比拉普拉斯变换更好，其中突出的有：Canny变换、Sobel变化、Hueckel变换等。\n4. 区域查找\n首先，我们必须定义什么是图像的一个区域。一个区域就是满足一下特性的相互连接的像素：\n下图运用了亮度差别不超过1个单元这个同质的特性。当无需再进行分割时，可以合并那些满足此同质特性的相邻的候选区域。\n场景分析\n在用以上技术对图像进行处理后，我们力图从中获取所需有关场景的信息。计算机视觉的这个阶段被称为“场景分析”。\n1. 解释图像中的线条和曲线\n对已知的包含直线物体的场景进行分析时，其中关键的一步就是图像中线条的假定。可以通过采用把直线段与边缘或区域的边界拟合的技术来生成直线。下图就是对一个房间的解释。\n2.基于模型的视觉"}
{"content2":"智能硬件\n谷歌和纸壳产品（纸壳VR、纸壳音响）\nAIY Vision Kit\n树莓派\n卡片电脑\n终端智能\n魔镜项目\n智能音箱\n简易人脸识别\n简易人脸识别\n树莓派2 + Pi Camera（USB摄像头兼容性更好）\n简易物体检测\n树莓派 + yolo"}
{"content2":"人工智能赋予了哪些以“未来之眼”？\n5月11日，由张江高科技园区管委会主办的张江发布“人工智能，慧聚张江”系列活动之“未来之眼”专场在创智空间成功举办。本次活动聚焦人工智能计算机视觉领域，邀请业内知名企业亮风台、阅面科技、图漾科技、禾赛科技、纵目科技和叠境科技，大咖新秀齐聚一堂，纵论人工智能新“视”力。\n众所周知，人类 80%以上的信息都是来源于视觉，而机器对视觉的依赖则更大，达到了 90%以上。计算机视觉作为人工智能行业关键的细分领域，正在快速发展。今年3月，英特尔斥资150 亿美元收购以色列计算机视觉公司Mobileye，在AI领域引起广泛关注，“未来之眼”的时代是否已经到来？\n何为“未来之眼”？人工智能赋予人类的AR/VR即是“未来之眼”，人工智能赋予汽车的无人驾驶/自动驾驶即是“未来之眼”，人工智能赋予机器的视觉即是“未来之眼”。\nVR/AR——人工智能赋予人类的“未来之眼”\n亮风台联合创始人 凌海滨\n亮风台是一家专注于AR领域的企业，致力于帮助人类增强对现实的感知，公司在AR领域深耕多年后在软硬件方面均取得可喜成果，软件产品已经覆盖到7亿用户，其自主研发的AR智能眼镜HiAR Glasses获得有“设计界奥斯卡”之称的德国红点奖。\n在张江，亮风台是一家不折不扣的网红公司，不管是之前和腾讯QQ合作里约奥运火炬传递活动，还是4月份和支付宝达成上线AR入口的战略合作，都引起了一阵媒体的轰动。这次活动，亮风台联合创始人、首席科学家凌海滨教授给我们带来了“AR之路，未来已来”的主题演讲，给我们带了不小震动。他指出，下一代的计算机平台一定是让用户突破屏幕，touch到虚拟之物，形成人与物的自然交互。而这就是亮风台一直在实践和努力的方向。\n如果说亮风台的AR技术增强了人类对现实世界的虚拟体验，但眼睛在看世界的时候，离不开光亮，只有透过光，才能更真实的感受世界，而这，正是叠境科技正在做的事情——通往虚拟现实的光场之路。\n叠境数字科技科学家 高盛华\n叠境科技建立了一整套以光场采集、捕捉、显示为核心的VR/AR解决方案，将全球最顶尖的光场技术引入虚拟现实领域，给人们带来最真实、自然的虚拟体验，大大解决了虚拟现实体验差的问题，也给虚拟现实带来了巨大的变革。\n叠境科技科学家高盛华出席本次活动并做主题分享：“Giving V/AR Eyes and Brain”。他说，\n叠境利用光场技术，直接在三维空间里拍下战马照片，将拍到的多张照片先进行图形渲染，然后再通过计算机视觉建立战马模型，最终在虚拟现实中，呈现出一个立体有色彩的唐三彩战马，提高人类眼睛的真实观感。\n值得一提的是，VR直播风起云涌的今年，在伊利新品“酪艺”的推介会上，“宁泽涛”+“VR直播”这对高颜值和高科技CP组合依托叠境科技独有的360 3D VR直播解决方案，使直播真实可感，在天猫直播平台引起了不小轰动。这次合作令叠境科技在VR直播界一战成名。\n自动驾驶/无人驾驶——人工智能赋予汽车车的“未来之眼”\n自动驾驶/无人驾驶作为人工智能的重点发展方向，它的核心痛点在于环境识别和激光雷达传感器。在全球各大车企和芯片企业都在激烈角逐时，中国企业已经取得出了傲人的战绩，它们利用各自的技术优势，能够给汽车装上一双“慧眼”。\n纵目科技创始人 唐锐\n纵目科技着力解决用户在低速场景和最后100米驾驶过程中的痛点，公司研发的环视ADAS（高级驾驶辅助系统）系统，通过设计一个庞大的数据系统，对路况进行采集和标注，实现全景泊车、信息报警、车道偏移、盲区监测、行人检测、障碍物识别等功能。\n纵目科技创始人唐锐在“适应中国市场的纵目自动驾驶之路”的主题演讲中指出，公司选择从环视ADAS（高级驾驶辅助系统）切入自动驾驶领域，未来会增强ADAS技术对环境的感知和道路地图构建与定位，解决用户最后100米、500米、1000米的停车问题以及帮助车主实现在停车场下车，汽车能自主寻找到车位停车与自动接人的功能。\n禾赛科技CEO 李一帆\n自动驾驶的终极目标是无人驾驶。禾赛科技则专注研发无人车的“眼睛”——激光雷达。熟悉汽车行业的人都知道，激光雷达传感器成本高，货源紧张，是无人车大战中名副其实的兵家必争之地。禾赛科技最新发布的用于自动驾驶的40线混合固态激光雷达Pandar40，改变了自动驾驶激光雷达行业被国际巨头垄断而且需要半年以上交货周期的现状，因此受到国内外无人车行业的密切关注。公司昨日宣布获得了1.1亿元人民币的A轮融资，这是我国本土激光雷达所获得的单笔最大金额融资。\n同时，禾赛科技的创始人李一帆也是一位不折不扣的网红CEO，它的主题演讲 “激光雷达—无人车的‘眼睛’”也受到现场观众的热烈欢呼。他指出，禾赛科技研发的激光雷达，通过多线扫描能够投射到道路上任何物体，然后将数据反馈给激光传感器。汽车可以规避周围任何人、车辆与其它障碍物，最终实现无人驾驶。\n机器视觉——人工智能赋予机器的“未来之眼”\n机器视觉是人工智能正在快速发展的一个分支，简单来说，机器视觉就是用机器代替人眼来做测量和判断，但其功能范围不仅包括人眼对信息的接收，同时还延伸至大脑对信息的处理与判断。\n图漾科技创始人费浙平\n图漾科技创始人费浙平在活动中发表“3D视觉的兴起”主题演讲，他说：黑夜给了我黑色的眼睛，我却用它研究机器的眼睛。图漾科技研发的深度摄像头，基于XYZ坐标，要给机器装一双3D的眼睛。\n目前，深度摄像头不仅应用在机器人、安防、无人机、工业等领域，实现室内机器人走路不撞墙、监控无死角、无人机导航避障、检测包裹大小和仓库空间等场景，在未来还将应用到VR/AR行业，与真实环境完美融合，实现脸部识别、手势识别和3D头像生成等新功能，从而引发VR内容应用革命。\n视觉有三个层次，与生俱来、后天习得和专业学习。从算法到视觉模块，阅面科技要开启本能化的机器视界。\n阅面科技创始人赵京雷\n阅面科技创始人赵京雷在“开启机器新视界”的主题演讲中表示，阅面科技为用户提供以智能算法为核心的嵌入式人工智能平台，通过一体化解决方案解决智能机器人脸识别、手势识别以及视觉追踪等的视觉识别问题，使开发者更加便捷地通过该平台接入人机交互的技术。目前，阅面科技已和一些比较有代表性的服务性机器人企业展开深度合作，在智能家居、智能车载和智能手机领域均有客户合作。\n人工智能企业扎根上海，张江优势何在？\n不仅有精彩的演讲，还有热烈的讨论。围绕着AI行业的发展，6位嘉宾唇枪舌剑，好不热闹。当嘉宾主持高盛华博士提问“人工智能企业扎根上海，张江有何优势”时，各位嘉宾各抒己见，又有共识。在人工智能领域，张江有丰厚的高校资源和相当人才的积累，特别是在人工智能亟需的芯片领域，张江有完善的产业链条和丰富的人才梯队，这构成了张江后发制人的一个潜在优势，假以时日，张江一定能够成为人工智能产业高地。\n值得一提的是，本次活动的协办方上海科技大学虚拟现实和视觉计算中心即将于7月2日在上海科技大学举办一场国际AI行业盛会。届时，创新工场、360、今日头条、商汤科技、地平线机器人等诸多国内知名企业将齐聚一堂，共同探讨AI行业的发展。\n原文发布时间：2017-05-12 13:37\n本文作者：张江\n本文来自云栖社区合作伙伴镁客网，了解相关信息可以关注镁客网。"}
{"content2":"﻿﻿\n来源：http://www.leiphone.com/news/201605/zZqsZiVpcBBPqcGG.html#rd\n人工智能是人类一个非常美好的梦想，跟星际漫游和长生不老一样。我们想制造出一种机器，使得它跟人一样具有一定的对外界事物感知能力，比如看见世界。\n在上世纪50年代，数学家图灵提出判断机器是否具有人工智能的标准：图灵测试。即把机器放在一个房间，人类测试员在另一个房间，人跟机器聊天，测试员事先不知道另一房间里是人还是机器 。经过聊天，如果测试员不能确定跟他聊天的是人还是机器的话，那么图灵测试就通过了，也就是说这个机器具有与人一样的感知能力。\n但是从图灵测试提出来开始到本世纪初，50多年时间有无数科学家提出很多机器学习的算法，试图让计算机具有与人一样的智力水平，但直到2006年深度学习算法的成功，才带来了一丝解决的希望。\n众星捧月的深度学习\n深度学习在很多学术领域，比非深度学习算法往往有20-30%成绩的提高。很多大公司也逐渐开始出手投资这种算法，并成立自己的深度学习团队，其中投入最大的就是谷歌，2008年6月披露了谷歌脑项目。2014年1月谷歌收购DeepMind，然后2016年3月其开发的Alphago算法在围棋挑战赛中，战胜了韩国九段棋手李世石，证明深度学习设计出的算法可以战胜这个世界上最强的选手。\n在硬件方面，Nvidia最开始做显示芯片，但从2006及2007年开始主推用GPU芯片进行通用计算，它特别适合深度学习中大量简单重复的计算量。目前很多人选择Nvidia的CUDA工具包进行深度学习软件的开发。\n微软从2012年开始，利用深度学习进行机器翻译和中文语音合成工作，其人工智能小娜背后就是一套自然语言处理和语音识别的数据算法。\n百度在2013年宣布成立百度研究院，其中最重要的就是百度深度学习研究所，当时招募了著名科学家余凯博士。不过后来余凯离开百度，创立了另一家从事深度学习算法开发的公司地平线。\nFacebook和Twitter也都各自进行了深度学习研究，其中前者携手纽约大学教授Yann Lecun，建立了自己的深度学习算法实验室；2015年10月，Facebook宣布开源其深度学习算法框架，即Torch框架。Twitter在2014年7月收购了Madbits，为用户提供高精度的图像检索服务。\n前深度学习时代的计算机视觉\n互联网巨头看重深度学习当然不是为了学术，主要是它能带来巨大的市场。那为什么在深度学习出来之前，传统算法为什么没有达到深度学习的精度？\n在深度学习算法出来之前，对于视觉算法来说，大致可以分为以下5个步骤：特征感知，图像预处理，特征提取，特征筛选，推理预测与识别。早期的机器学习中，占优势的统计机器学习群体中，对特征是不大关心的。\n我认为，计算机视觉可以说是机器学习在视觉领域的应用，所以计算机视觉在采用这些机器学习方法的时候，不得不自己设计前面4个部分。\n但对任何人来说这都是一个比较难的任务。传统的计算机识别方法把特征提取和分类器设计分开来做，然后在应用时再合在一起，比如如果输入是一个摩托车图像的话，首先要有一个特征表达或者特征提取的过程，然后把表达出来的特征放到学习算法中进行分类的学习。\n过去20年中出现了不少优秀的特征算子，比如最著名的SIFT算子，即所谓的对尺度旋转保持不变的算子。它被广泛地应用在图像比对，特别是所谓的structure from motion这些应用中，有一些成功的应用例子。另一个是HoG算子，它可以提取物体，比较鲁棒的物体边缘，在物体检测中扮演着重要的角色。\n这些算子还包括Textons，Spin image，RIFT和GLOH，都是在深度学习诞生之前或者深度学习真正的流行起来之前，占领视觉算法的主流。\n几个（半）成功例子\n这些特征和一些特定的分类器组合取得了一些成功或半成功的例子，基本达到了商业化的要求但还没有完全商业化。\n一是八九十年代的指纹识别算法，它已经非常成熟，一般是在指纹的图案上面去寻找一些关键点，寻找具有特殊几何特征的点，然后把两个指纹的关键点进行比对，判断是否匹配。\n然后是2001年基于Haar的人脸检测算法，在当时的硬件条件下已经能够达到实时人脸检测，我们现在所有手机相机里的人脸检测，都是基于它或者它的变种。\n第三个是基于HoG特征的物体检测，它和所对应的SVM分类器组合起来的就是著名的DPM算法。DPM算法在物体检测上超过了所有的算法，取得了比较不错的成绩。\n但这种成功例子太少了，因为手工设计特征需要大量的经验，需要你对这个领域和数据特别了解，然后设计出来特征还需要大量的调试工作。说白了就是需要一点运气。\n另一个难点在于，你不只需要手工设计特征，还要在此基础上有一个比较合适的分类器算法。同时设计特征然后选择一个分类器，这两者合并达到最优的效果，几乎是不可能完成的任务。\n仿生学角度看深度学习\n如果不手动设计特征，不挑选分类器，有没有别的方案呢？能不能同时学习特征和分类器？即输入某一个模型的时候，输入只是图片，输出就是它自己的标签。比如输入一个明星的头像，出来的标签就是一个50维的向量（如果要在50个人里识别的话），其中对应明星的向量是1，其他的位置是0。\n这种设定符合人类脑科学的研究成果。\n1981年诺贝尔医学生理学奖颁发给了David Hubel，一位神经生物学家。他的主要研究成果是发现了视觉系统信息处理机制，证明大脑的可视皮层是分级的。他的贡献主要有两个，一是他认为人的视觉功能一个是抽象，一个是迭代。抽象就是把非常具体的形象的元素，即原始的光线像素等信息，抽象出来形成有意义的概念。这些有意义的概念又会往上迭代，变成更加抽象，人可以感知到的抽象概念。\n像素是没有抽象意义的，但人脑可以把这些像素连接成边缘，边缘相对像素来说就变成了比较抽象的概念；边缘进而形成球形，球形然后到气球，又是一个抽象的过程，大脑最终就知道看到的是一个气球。\n模拟人脑识别人脸，也是抽象迭代的过程，从最开始的像素到第二层的边缘，再到人脸的部分，然后到整张人脸，是一个抽象迭代的过程。\n再比如看到图片中的摩托车，我们可能在脑子里就几微秒的时间，但是经过了大量的神经元抽象迭代。对计算机来说最开始看到的根本也不是摩托车，而是RGB图像三个通道上不同的数字。\n所谓的特征或者视觉特征，就是把这些数值给综合起来用统计或非统计的形式，把摩托车的部件或者整辆摩托车表现出来。深度学习的流行之前，大部分的设计图像特征就是基于此，即把一个区域内的像素级别的信息综合表现出来，利于后面的分类学习。\n如果要完全模拟人脑，我们也要模拟抽象和递归迭代的过程，把信息从最细琐的像素级别，抽象到“种类”的概念，让人能够接受。\n卷积的概念\n计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。\n什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。实际上在计算机视觉里面，可以把卷积当做一个抽象的过程，就是把小区域内的信息统计抽象出来。\n比如，对于一张爱因斯坦的照片，我可以学习n个不同的卷积和函数，然后对这个区域进行统计。可以用不同的方法统计，比如着重统计中央，也可以着重统计周围，这就导致统计的和函数的种类多种多样，为了达到可以同时学习多个统计的累积和。\n上图中是，如何从输入图像怎么到最后的卷积，生成的响应map。首先用学习好的卷积和对图像进行扫描，然后每一个卷积和会生成一个扫描的响应图，我们叫response map，或者叫feature map。如果有多个卷积和，就有多个feature map。也就说从一个最开始的输入图像（RGB三个通道）可以得到256个通道的feature map，因为有256个卷积和，每个卷积和代表一种统计抽象的方式。\n在卷积神经网络中，除了卷积层，还有一种叫池化的操作。池化操作在统计上的概念更明确，就是一个对一个小区域内求平均值或者求最大值的统计操作。\n带来的结果是，如果之前我输入有两个通道的，或者256通道的卷积的响应feature map，每一个feature map都经过一个求最大的一个池化层，会得到一个比原来feature map更小的256的feature map。\n在上面这个例子里，池化层对每一个2X2的区域求最大值，然后把最大值赋给生成的feature map的对应位置。如果输入图像是100×100的话，那输出图像就会变成50×50，feature map变成了一半。同时保留的信息是原来2X2区域里面最大的信息。\n操作的实例：LeNet网络\nLe顾名思义就是指人工智能领域的大牛Lecun。这个网络是深度学习网络的最初原型，因为之前的网络都比较浅，它较深的。LeNet在98年就发明出来了，当时Lecun在AT&T的实验室，他用这一网络进行字母识别，达到了非常好的效果。\n怎么构成呢？输入图像是32×32的灰度图，第一层经过了一组卷积和，生成了6个28X28的feature map，然后经过一个池化层，得到得到6个14X14的feature map，然后再经过一个卷积层，生成了16个10X10的卷积层，再经过池化层生成16个5×5的feature map。\n从最后16个5X5的feature map开始，经过了3个全连接层，达到最后的输出，输出就是标签空间的输出。由于设计的是只要对0到9进行识别，所以输出空间是10，如果要对10个数字再加上26个大小字母进行识别的话，输出空间就是62。62维向量里，如果某一个维度上的值最大，它对应的那个字母和数字就是就是预测结果。\n压在骆驼身上的最后一根稻草\n从98年到本世纪初，深度学习兴盛起来用了15年，但当时成果泛善可陈，一度被边缘化。到2012年，深度学习算法在部分领域取得不错的成绩，而压在骆驼身上最后一根稻草就是AlexNet。\nAlexNet由多伦多大学几个科学家开发，在ImageNet比赛上做到了非常好的效果。当时AlexNet识别效果超过了所有浅层的方法。此后，大家认识到深度学习的时代终于来了，并有人用它做其它的应用，同时也有些人开始开发新的网络结构。\n其实AlexNet的结构也很简单，只是LeNet的放大版。输入是一个224X224的图片，是经过了若干个卷积层，若干个池化层，最后连接了两个全连接层，达到了最后的标签空间。\n去年，有些人研究出来怎么样可视化深度学习出来的特征。那么，AlexNet学习出的特征是什么样子？在第一层，都是一些填充的块状物和边界等特征；中间的层开始学习一些纹理特征；更高接近分类器的层级，则可以明显看到的物体形状的特征。\n最后的一层，即分类层，完全是物体的不同的姿态，根据不同的物体展现出不同姿态的特征了。\n可以说，不论是对人脸，车辆，大象或椅子进行识别，最开始学到的东西都是边缘，继而就是物体的部分，然后在更高层层级才能抽象到物体的整体。整个卷积神经网络在模拟人的抽象和迭代的过程。\n为什么时隔20年卷土重来？\n我们不禁要问：似乎卷积神经网络设计也不是很复杂，98年就已经有一个比较像样的雏形了。自由换算法和理论证明也没有太多进展。那为什么时隔20年，卷积神经网络才能卷土重来，占领主流？\n这一问题与卷积神经网络本身的技术关系不太大，我个人认为与其他一些客观因素有关。\n首先，卷积神经网络的深度太浅的话，识别能力往往不如一般的浅层模型，比如SVM或者boosting。但如果做得很深，就需要大量数据进行训练，否则机器学习中的过拟合将不可避免。而2006及2007年开始，正好是互联网开始大量产生各种各样的图片数据的时候。\n另外一个条件是运算能力。卷积神经网络对计算机的运算要求比较高，需要大量重复可并行化的计算，在当时CPU只有单核且运算能力比较低的情况下，不可能进行个很深的卷积神经网络的训练。随着GPU计算能力的增长，卷积神经网络结合大数据的训练才成为可能。\n最后一点就是人和。卷积神经网络有一批一直在坚持的科学家（如Lecun）才没有被沉默，才没有被海量的浅层方法淹没。然后最后终于看到卷积神经网络占领主流的曙光。\n深度学习在视觉上的应用\n计算机视觉中比较成功的深度学习的应用，包括人脸识别，图像问答，物体检测，物体跟踪。\n人脸识别\n这里说人脸识别中的人脸比对，即得到一张人脸，与数据库里的人脸进行比对；或同时给两张人脸，判断是不是同一个人。\n这方面比较超前的是汤晓鸥教授，他们提出的DeepID算法在LWF上做得比较好。他们也是用卷积神经网络，但在做比对时，两张人脸分别提取了不同位置特征，然后再进行互相比对，得到最后的比对结果。最新的DeepID-3算法，在LWF达到了99.53%准确度，与肉眼识别结果相差无几。\n图片问答问题\n这是2014年左右兴起的课题，即给张图片同时问个问题，然后让计算机回答。比如有一个办公室靠海的图片，然后问“桌子后面有什么”，神经网络输出应该是“椅子和窗户”。\n这一应用引入了LSTM网络，这是一个专门设计出来具有一定记忆能力的神经单元。特点是，会把某一个时刻的输出当作下一个时刻的输入。可以认为它比较适合语言等，有时间序列关系的场景。因为我们在读一篇文章和句子的时候，对句子后面的理解是基于前面对词语的记忆。\n图像问答问题是基于卷积神经网络和LSTM单元的结合，来实现图像问答。LSTM输出就应该是想要的答案，而输入的就是上一个时刻的输入，以及图像的特征，及问句的每个词语。\n物体检测问题\nRegion CNN\n深度学习在物体检测方面也取得了非常好的成果。2014年的Region CNN算法，基本思想是首先用一个非深度的方法，在图像中提取可能是物体的图形块，然后深度学习算法根据这些图像块，判断属性和一个具体物体的位置。\n为什么要用非深度的方法先提取可能的图像块？因为在做物体检测的时候，如果你用扫描窗的方法进行物体监测，要考虑到扫描窗大小的不一样，长宽比和位置不一样，如果每一个图像块都要过一遍深度网络的话，这种时间是你无法接受的。\n所以用了一个折中的方法，叫Selective Search。先把完全不可能是物体的图像块去除，只剩2000左右的图像块放到深度网络里面判断。那么取得的成绩是AP是58.5，比以往几乎翻了一倍。有一点不尽如人意的是，region CNN的速度非常慢，需要10到45秒处理一张图片。\nFaster R-CNN方法\n而且我在去年NIPS上，我们看到的有Faster R-CNN方法，一个超级加速版R-CNN方法。它的速度达到了每秒七帧，即一秒钟可以处理七张图片。技巧在于，不是用图像块来判断是物体还是背景，而把整张图像一起扔进深度网络里，让深度网络自行判断哪里有物体，物体的方块在哪里，种类是什么？\n经过深度网络运算的次数从原来的2000次降到一次，速度大大提高了。\nFaster R-CNN提出了让深度学习自己生成可能的物体块，再用同样深度网络来判断物体块是否是背景？同时进行分类，还要把边界和给估计出来。\nFaster R-CNN可以做到又快又好，在VOC2007上检测AP达到73.2，速度也提高了两三百倍。\nYOLO\n去年FACEBOOK提出来的YOLO网络，也是进行物体检测，最快达到每秒钟155帧，达到了完全实时。它让一整张图像进入到神经网络，让神经网络自己判断这物体可能在哪里，可能是什么。但它缩减了可能图像块的个数，从原来Faster R-CNN的2000多个缩减缩减到了98个。\n同时取消了Faster R-CNN里面的RPN结构，代替Selective Search结构。YOLO里面没有RPN这一步，而是直接预测物体的种类和位置。\nYOLO的代价就是精度下降，在155帧的速度下精度只有52.7，45帧每秒时的精度是63.4。\nSSD\n在arXiv上出现的最新算法叫Single Shot MultiBox Detector，即SSD。\n它是YOLO的超级改进版，吸取了YOLO的精度下降的教训，同时保留速度快的特点。它能达到58帧每秒，精度有72.1。速度超过Faster R-CNN 有8倍，但达到类似的精度。\n物体跟踪\n所谓跟踪，就是在视频里面第一帧时锁定感兴趣的物体，让计算机跟着走，不管怎么旋转晃动，甚至躲在树丛后面也要跟踪。\n深度学习对跟踪问题有很显著的效果。DeepTrack算法是我在澳大利亚信息科技研究院时和同事提出的，是第一在线用深度学习进行跟踪的文章，当时超过了其它所有的浅层算法。\n今年有越来越多深度学习跟踪算法提出。去年十二月ICCV 2015上面，马超提出的Hierarchical Convolutional Feature算法，在数据上达到最新的记录。它不是在线更新一个深度学习网络，而是用一个大网络进行预训练，然后让大网络知道什么是物体什么不是物体。\n将大网络放在跟踪视频上面，然后再分析网络在视频上产生的不同特征，用比较成熟的浅层跟踪算法来进行跟踪，这样利用了深度学习特征学习比较好的好处，同时又利用了浅层方法速度较快的优点。效果是每秒钟10帧，同时精度破了记录。\n最新的跟踪成果是基于Hierarchical Convolutional Feature，由一个韩国的科研组提出的MDnet。它集合了前面两种深度算法的集大成，首先离线的时候有学习，学习的不是一般的物体检测，也不是ImageNet，学习的是跟踪视频，然后在学习视频结束后，在真正在使用网络的时候更新网络的一部分。这样既在离线的时候得到了大量的训练，在线的时候又能够很灵活改变自己的网络。\n基于嵌入式系统的深度学习\n回到ADAS问题（慧眼科技的主业），它完全可以用深度学习算法，但对硬件平台有比较高的要求。在汽车上不太可能把一台电脑放上去，因为功率是个问题，很难被市场所接受。\n现在的深度学习计算主要是在云端进行，前端拍摄照片，传给后端的云平台处理。但对于ADAS而言，无法接受长时间的数据传输的，或许发生事故后，云端的数据还没传回来。\n那是否可以考虑NVIDIA推出的嵌入式平台？NVIDIA推出的嵌入式平台，其运算能力远远强过了所有主流的嵌入式平台，运算能力接近主流的顶级CPU，如台式机的i7。那么慧眼科技在做工作就是要使得深度学习算法，在嵌入式平台有限的资源情况下能够达到实时效果，而且精度几乎没有减少。\n具体做法是，首先对网络进行缩减，可能是对网络的结构缩减，由于识别场景不同，也要进行相应的功能性缩减；另外要用最快的深度检测算法，结合最快的深度跟踪算法，同时自己研发出一些场景分析算法。三者结合在一起，目的是减少运算量，减少检测空间的大小。在这种情况下，在有限资源上实现了使用深度学习算法，但精度减少的非常少。"}
{"content2":"训练与调优\n激活函数\n1、sigmoid函数不再适用的原因：1）在某些情况下梯度会消失，不利于反向传播，例如，当输如的值过大或过小时，根据函数的图像，返回的梯度都会是0，阻断了梯度的反向传播。2）不是以0为中心，梯度更新低效\n拿sigmoid举例，不是以零为中心，会造成其反向传播的导数不是正就是负，假设w是一个二维向量，其减少的方向只会是第一象限或第三象限，对于下面的情况就比较低效。\n2、tanh函数：第二个问题可以避免，但第一个避免不了\n3、relu：又快又简单，最接近神经元的工作过程，但仍有缺点，例如不以0为中心，负半轴容易出现梯度的消失\n4、leaky relu：\nmax\n⁡\n(\n0.01\nx\n,\nx\n)\n\\max (0.01x,x)\nmax(0.01x,x)\nprelu:\nmax\n⁡\n(\nα\nx\n,\nx\n)\n\\max (\\alpha x,x)\nmax(αx,x)\n这样可以消除梯度消失的影响。\n5、erelu：\n6、maxout neural：\nmax\n⁡\n(\nw\n1\nT\nx\n+\nb\n1\n,\nw\n2\nT\nx\n+\nb\n2\n)\n\\max (w_1^Tx + {b_1},w_2^Tx + {b_2})\nmax(w1T x+b1 ,w2T x+b2 )\n会避免前面许多缺点，但参数加倍\n数据预处理\n1、在图像处理中，我们一般会做零均值化，但一般不会做归一化，这跟大多数的机器学习问题不同，不需要将所有的特征都投影到相同的范围内，在图像中，我们还是想针对图像本身进行操作。\n优化算法\n1、SGD：当在某些方向梯度变化敏感，某些方向不敏感，梯度可能在敏感的方向做之字形运动，如下：\n这在高维上可能会更加敏感；而且，SGD容易卡在鞍点。\n2、Nesterov momentum:\n由于这种形式不能同时计算梯度跟损失，故进行改写为：\n3、adagrad：每一步都累加梯度的平方，更新参数时再除以这个平方的开方\n但随着训练的进行，步长会越来越小，因此出现了其改进：\nAdam算法进一步综合了二者的优点，一般也是现在默认的算法\n正则化\n1、dropconnect：不是随机将激活函数置零，而是随机将部分权值置零。\n2、随机最大池化：在池化过程中，随机进行区域池化（不常用，也不太懂）\n3、随机深度：训练时随机丢掉部分层，测试时使用全部层（不常用）"}
{"content2":"中国计算机学会推荐国际学术会议\n(人工智能)\nC类会议\nICANN 2018 : International Conference on Artificial Neural Networks\nhttps://e-nns.org/icann2018/\n截稿日期:\n2018-05-02\n通知日期:\n2018-06-01\n会议日期:\n2018-10-05\n会议地点:\nRhodes, Greece\nICTAI：IEEE International Conference on Tools with Artificial Intelligence\nhttp://ictai2018.org/index.htm\nPaper Submission: June 1, 2018 [Time: 11:59pm PST (i.e., UCT-08)]\nNotification of Acceptance/Rejection: July 16, 2018\nCamera Ready Paper: August 10, 2018\nICONIP 2018 : International Conference on Neural Information Processing\nhttps://conference.cs.cityu.edu.hk/iconip/\n截稿日期:\n2018-06-01\n通知日期:\n2018-08-01\n会议日期:\n2018-12-14\n会议地点:\nSiem Reap, Cambodia\nACCV 2018: Asian Conference on Computer Vision\nhttp://accv2018.net/\n截稿日期:\n2018-07-05\n通知日期:\n2018-09-21\n会议日期:\n2018-11-04\n会议地点:\nPerth, Australia\n～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～\n～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～"}
{"content2":"以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事 这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为， 知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.PHP/首页\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）南开大学程明明助教：http://mmcheng.net/ 图像分割、检索, bing特征快速目标（行人）检测；\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://www.robots.ox.ac.uk/~phst/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/\n（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；\n（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、\n（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库\n（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理\n（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、\n（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n（213）北京航空航天大学周付根教授：http://www.sa.buaa.edu.cn/html/yhxy/szll/jsfc/txcl/1521.html  医学图像处理\n（214）北京航空航天大学姜志国教授：http://www.sa.buaa.edu.cn/html/yhxy/xygk/zyld/1595.html 医学图像处理，遥感图像分割；\n（215）德国萨尔布吕肯大学博士后R. Benenson： http://rodrigob.github.io/# 行人检测，无人驾驶汽车\n（216）西南财经大学段江教授：http://it.swufe.edu.cn/2011-09/25/201109251002096701.html 高动态范围图像处理\n（217）中科院沈阳自动化所华春生研究员：http://people.ucas.ac.cn/~huacs 行人检测、目标跟踪、聚类分析\n（218）华中科技大学自动化学院张天序教授：http://auto.hust.edu.cn/viewnews-1978 红外图像处理，医学图像处理，武器装备图像处理\n（219）普林斯顿大学Jianxiong Xiao助理教授：http://vision.princeton.edu/people/xj/ 3D重建、3D识别、深度学习"}
{"content2":"计算机视觉入门书籍总结，包括书名和部分作者名字，。\n作者名字有漏写和错写。。"}
{"content2":"基于双目计算机视觉的自适应识别算法及其监控应用\n摘要：双时时彩奖金1800，凤凰平台，十年信誉全网第一\n平台总代QQ：2317365898\n目计算机视觉是利用仿生学原理,通过标定后的双摄像头来得到同步曝光图像,然后计算获取的2维图像像素点的第3维深度信息。为了对不同环境场景进行监控提出了一种新的基于双目计算机视觉的自适应识别算法。该算法首先利用像素点的深度信息对场景进行识别判断,然后采用统计的方法为场景建模,并通过时间滤波克服光照渐变,以及通过深度算法特性克服光照突变。与单摄像头监控系统相比,利用该算法实现的视频监控原型系统,可应用于更多场合,并利用深度信息设置报警级别,来降低误检率。\n关键词：双目计算机视觉　深度信息　自适应　光照变化　视频监控\n1　引　言\n面对日益复杂的社会和政治环境,国家安全、社会安全、个人人生安全和财产安全等都面临着不同程度的威胁,都需要各种安全保护措施,在众多场所建立切实有效的安保措施,成为一个迫切的课题。本文提出了一种基于双目计算机视觉的自适应识别算法,将该算法应用于现有的监控系统,并赋予监控终端智能性,不仅使其脱离人而具有独立智能、自主判断的能力,而且使得视频监控系统在安防方面的作用大大提高。\n在现有的背景建模方法中,大多对于背景象素点的亮度值,例如最小亮度值、最大亮度值和最大亮度差值[ 1 ] ,或是对颜色信息进行建模[ 2 ] 。对于背景的更新,一般使用自适应滤波器对像素的统计特性进行递归更新,为了考虑到噪声的影响,文献[ 3 ]提出了Kalman滤波器的方法,该文认为系统的最优信息可通过估计获得。考虑到环境的动态缓慢改变,文献[ 4 ]利用统计模型给背景建模,即由一个时域滤波器保留着一个序列均值和一个标准偏差,并通过滤波过程统计值随时间改变来反映环境的动态特性。另外有一些方法解决了光照渐变等影响[ 5～7 ] ,但计算较复杂。\n2双目计算机视觉深度算法\n基于实际应用考虑,摄像头的数量关系着成本和计算量,所以选择支持双摄像头(双目视觉)的算法是最合适的。在支持双目视觉的算法中,Princeton NEC research institute 基于最大流算法(maximum2flow)的计算机视觉算法( Stereo2MF)在深度效果平滑性上做得较好[ 8, 9 ] ,适用于监控区域深度计算的应用背景。但原有算法所需的计算量和计算过程中的暂存数据量是较大的,虽然支持计算量的削减,但只是机械地在一块区域中选择中心点来进行计算,这样计算的结果会因选择的机械性,而出现大量的“伪点”,这些伪点错误地表现了该区域的平均深度信息。本文采用统计平均值选取计算点,通过距离因子的Gauss分布将块内其他点的值融合计算,从而使得计算出的值较准确的代表了这一块内的大致深度分布。\nm, n分别是图像的长和宽所包含的像素点个数,M、N 表示像素点的横纵坐标, .d 是块内深度统计平均值, dM, N为计算点的深度值, q为距离因子, dB是计算所得的块深度代表值。为改进后双目视觉深度算法与原算法识别效果比较。由可以明显看出,修改后的算法效果在细节表现、平滑性、伪点减少上均有明显改善,而且深度计算精确度能够完全满足视频\n　改进后双目视觉深度算法与原算法识别效果比较Fig. 1Effect comparison after algorithm modification\n度计算精确度能够完全满足视频监控应用的需要。\n3　自适应识别算法\n对于一个固定的场景,场景各像素点的深度值是符合一个随机概率分布。以某一均值为基线,在其附近做不超过某一偏差的随机振荡,这种情况下的场景称之为背景。而场景环境往往是动态变化的,如环境自然光的缓变,灯光的突然熄灭或点亮,以及运动对象的出现、运动和消失等。如果能识别出场景中的动态变化,就能自适应的更新背景值,将光照的改变融合到背景值中。本文采取了用统计模型的方式给每个像素点建模,而以像素点变化的分布情况来确定光照突变引起的深度突变,并结合深度计算本身特性,解决光照缓变突变引起的误判问题,以及判别场景中对象的主次性。\n3. 1　背景象素点的深度值建模\n由于双目计算机视觉算法得到的深度值,已经是块融合的,可以根据精度要求,来加大块面积, 减少数据量。本文获得的数据量只有原像素点的( k, l分别是块的长和宽所包含的像素点个数) 。以统计的方法给每个像素点的深度值建模, 设为第u帧图像的某个像素点的深度值, 其中u代表第u帧图像, i, j分别代表像素点的横坐标和纵坐标。由一个时间滤波器来保持该像素点深度值的序列均值和时间偏差\n其中,α是一个可调增益参数, 其与采样频率有关。通过滤波过程,来得到每个像素点的深度值基于时间的统计特性,由于这些统计特性反映了环境的动态特性,据此可以了解到是环境的光照发生了突变,还是有运动对象的运动。\n3. 2　背景更新与场景识别\n通过上述滤波过程,就可以将光照缓变融入到背景中去,实现背景的自适应更新。而对于光照突变,此时几乎所有的象素点的亮度值会同时增大或减小,但根据最大流算法的特性,同方向的变化对流量差不会引起太大变化, 而对深度计算结果只会引起较小的同方向变化。这种全局的等量变化, 可以认为是光照突变引起的。\n其中, a、b和c是3个可调节系数,他们的取值可依据场景的情况及检测光照突变的速度与误差来进行选取。s, t分别是深度图像的长和宽所包含的像素点个数。Q是符合式( 9)的像素点个数。一旦检测到环境光照发生了突变, 就把背景点像素的深度序列均值,全部以当前帧像素点的深度值的测量\n值代替,而j以0取代,从而实现背景的及时更新。\n如果式(10)式(12)中任意一个不成立的话,则认为像素块深度值的变化并非由光照突变引起, 而是场景中有运动对象出现。\n4　算法分析与实验\n4. 1　算法复杂度\n对于光照突变检测,若有突变的话,则会立即检测出来,当有运动对象出现时, 并且式( 10)式( 12)都接近满足时,处理会较慢,因为需要处理突变检测和运动对象两个过程。当b取25% s ×t时的处理速度与变化点比例关系如所示。\n相对于一般的光强、灰度的识别检测算法,本算法的优势在于不仅可以利用深度特性更容易地检测到光照的渐变与突变, 而且可以判断出现的多个运动对象的主次性。\n4. 2　算法误检率\n由于光照直接对于像素点的光强、灰度等产生影响,所以深度算法的噪声容限更大,这样可降低了误检率,多组实验后得到的误检率对比图如所示。但是由于深度算法本身对于反光或者阴暗面会产生伪点,所以,某些时候由于光照突变中光源的位置变化而会误检为运动对象,为此算法还需进一步改进能判别伪点的出现, 除去它在光照突变检测中的影响。另外,公式中可调系数a, b, c的选取也会对不同场景产生影响。\n笔者在实验室环境下做了不同光照角度、不同环境光强度、不同运动物体的多组实验,发现在反光面或是阴暗面较多的情况下,光照突变检测不是很灵敏,而且会出错,但是在增加系数a, c的值后, 误检率有所降低(如所示) 。\n5　基于算法的监控系统\n我们利用该算法实现了视频监控原型系统。计算机视觉算法对于摄像头的同步曝光要求很高,所以本系统终端用一块单独的MCU (micro control unit)控制同步曝光。核心算法用DSP处理。系统结构如所示。实际系统原型图如所示。\n6　结　论\n利用深度信息做智能场景识别,是一种新的尝试,有其优势。将这种方法应用到智能视频监控中,能起到很好的效果,克服了其他方法较难处理的光照渐变和突变等问题。对比单摄像头监控系统,该系统可应用于更多场合。后续研发准备在系统上加上更多功能,以适用于更多的环境,并与其他保安类监控系统互联,以组成一整套功能强大、达到国内外一流水平的安防系统。\n参考文献( References)\n1Ude A, Riley M. Prediction of body configurations and appearance for model-based estimation of articulated human motions [A ]. In: IEEE SMC’99 Conference Proceeding [ C ] , Tokyo, Japan, 1999: 687～691.\n2Ricquebourg Y, Bouthemy P. Real-time tracking of moving persons by exp loiting spatio2tempp ral image slices[ J ]. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2000, 22 (8) : 797～808.\n3Tsap L V, Goldof D B, Sarkar S. Nonrigid motion analysis based on dynamic refinement of finite elementmodelsp [ J ]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 ( 5 ) : 526～543.\n4Haritaoglu I, Harwood D, Davis L. A real time system for detecting and tracking peop le [ A ]. In: Third International Conference on Automatic Face and Gesture[ C ] , Nara, Japan, Ap ril 1998.\n5Wren C, AzarbayejaniA, Darrell T. Real-time tracking of the human body [ J ]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1997, 19 (7) : 780～785.\n6Ridder C, Munkelt O, Kirchner H. Adap tive background estimation and foreground detection using Kalman filtering[A ]. In: Proceedings International Conference. Recent Advances in Mechatronics, ICRAM’95, Istanbul, Turkey, 1995: 193～199.\n7Fujiyoshi H, Lip ton A J. Real-time human motion analysis by image skeletonization[A ]. In: Proceedings of theWorkshop on App lication of ComputerVision, Freiburg, Germany, October 1998.\n8Sebastien Roy, Ingemar J Cox. A maximum-flow formulation of the n-camera stereo correspondence p roblem [ A ]. In: International Conference on Computer Vision ( ICCV’98 ) [ C ] , Bombay, India, January 1998: 492～499.\n9Cox I J, Hingorani S, Maggs B M. A maximum likelihood stereo algorithm[ J ]. Computer Vision and Image Understanding, 1996, 63 (3) : 542～567."}
{"content2":"【 声明：版权所有，欢迎转载，请勿用于商业用途。 联系信箱：feixiaoxing @163.com】\n这几年，机器学习非常火。自从alpha go战胜了韩国的李世石之后，深度学习又将机器学习推向了新的高度。不可否认，机器学习是非常有用的。不管是模式识别、自然语言处理、计算机视觉，还是推荐系统、机器发掘、语音识别等方面，机器学习都发挥了巨大的作用。机器学习作为基础学科，它为其他学科的发展作出了很大的贡献，不断贡献着新的理论和算法，推动技术不断发展。\n从另一方面，学习机器学习也是有一定困难的。因为，对于希望学好这门课程的同学来说，除了掌握必要的编程方法和编程技巧之外，你还需要了解基本的数学方法，比如优化、概率、矩阵、随机数学等等。很多同学看到这些数学公式就一下子手足无措，不知道如何下手。\n下面，我就从自己个人的角度推荐一下，作为一个it工程师应该如何学习好机器学习？\n1）机器学习应该选用什么编程语言？\n如果大家平时留意的话，会发现大家用来编写算法的时候什么语言都有，比如c、c++、java、matlab、python、R等等。我建议大家如果是自己编写算法，最好使用python脚本语言，编写起来很快，语法和c差不多，基本不需要调试。如果是别人写的算法，那么只需要自己会编译会修改就可以了。比如说，机器学习中svm这块，大家用的比较多的都是libsvm库，最早的库是用c++库，大家只要能看懂会使用就可以了。还有一些公开发表的论文，这些论文大部分是用matlab编写的，大家也只要能看懂就可以了，如果需要的话，可以自己转换成python或者c代码。\n2）python常用的库有哪些？\n只需要numpy、pandas和matplotlib就可以了，前者主要处理矩阵使用，后者画图使用。\n3）怎么编写算法？\n可以先看懂机器算法的基本原理，然后编写代码，最后再看推倒的数学公式。it工程师比较擅长的是看代码，所以可以从自己擅长的领域着手。大部分的机器学习代码都在100～200行之间，我指的是python代码，所以理解上不会有太大的困难。即使自己不会写，先看看别人的也可以。\n4）有没有什么好的机器学习库？\nsklearn，python下面最简单的机器学习库。所有的代码基本上都分成三个步骤，建模、训练、预测。\n5）常用的机器学习的书有哪些？\n我自己经常看的书就三本，《机器学习实战》、《统计学习方法》和周志华的《机器学习》。大家在看书的时候，尽量找适合自己的书籍。大牛的书籍不一定适合自己，反而是一些实战的书特别适合入门。\n6）机器学习最重要的是什么？\n比算法和测试数据集本身更重要的，是希望大家寻找更多机器学习的应用场景，让这门学科更好地服务于我们的生活。\n7）掌握机器学习的步骤是什么？\n先利用现成的库（比如sklearn）用好算法，后面可以根据算法定理实现好算法，最后再根据使用经验发觉算法的不足，改进好算法，让更多的人从你的工作中收益。这是一个持续的过程，没有办法一蹴而就。\n8）公司用的机器学习和学术界的机器学习有什么区别？\n公司用的算法往往都是稳定、简洁、可靠的算法，比如说logistic回归、决策树、bayes等等，而学术中的机器学习更偏向于性能和结果，在效率和成本和算法可理解性方面考虑的不多。比如大家学习中遇到的svm、pca、lda、神经网络、深度学习，在生产中可能没有大家想象的那么多。当然也不能说，学术的算法就没有意义，也许随着计算机处理效率的提高和大数据时代的到来，学术中的某些算法会成为新的主流趋势，这也说不定。\n9）有没有什么比较好的视频教程？\n斯坦福大学Andrew NG的《机器学习》课程。\nps:\n一般机器学习最好和一个领域联系在一起。比如ml和图像，或者dnn和语音。如果想学习语音的同学，利用kaldi来学习语音也是不错的选择，语音识别里面的声学模型、语言模型也大量使用了机器学习的知识。"}
{"content2":"自从上了研究生才让我对人工智能领域有了些许了解，然而也让我对其下一个热门领域——计算机视觉产生了浓厚的兴趣。然而目前已经快接触有一年的时间了，但还是有许多要提升的地方。现在就自己的学习路径作个总结。\n计算机视觉可以分三步走我个人觉得比较适合自己，这是一些浅见。第一个阶段——图像处理(数字图像处理)，第二阶段——图像识别(机器学习)，第三阶段——图像语义的理解(深度学习)。\n这期间自己接触过一些书籍也感觉是比较好的，同时也是我一直都需要看的。\n第一阶段：图像处理\n环境：win10+VS2012+Opencv2.4.10\n环境的搭建可以在网络上搜，现在已经有很多教程了，同时推荐安装Opencv3.2.0及以上的，环境配置及其方便，基本都集成在一个库里面，所以也只需要添加一个就好。前几天Opencv官网又出了新的版本——Opencv3.3.0这刚好迎合了深度学习的热潮，其中将DNN放入主要模块中的确是巩固Opencv市场根基的又一大体现。言归正传，《数字图像处理(第三版)》冈萨雷斯 著，这本书可以说是图像处理中的一股清流，书中对图像处理的原理深入浅出的讲解了；哈哈，看书是不是没能给你直接的感触，此时，《Opencv3编程入门》就闪亮登场啦，在这之前也有一本圣经《learning Opencv》这本书是基于Opencv1版本的，我当时没有看这本。《Opencv3编程入门》此书是博客专家浅墨著作的，这本书可以理解为是《数字图像处理》这本书中的大部分代码的实现，很直观，同时书中对Opencv几个常用的模块都有代码详细说明，本书的代码大部分都是C风格的，些许用到了C++模板库的函数，比如：vector等，要想深入学习以及用好图像处理的话还是有必要把C++学好，其实后期还会接触到Python编程语言（这个有空就看下吧，作为视觉领域或图像处理领域这两门语言算是必会了）。哈哈，把图像处理和Opencv中的例程都实现一遍后(这一阶段就感觉是把PS中实现的功能，自己给通过实实在在的代码给实现了，衰，当然自己还是很菜的，不能跟PS神器进行比拟)， 你会更加迫不及待要做些“大事”的，但发现除了将图像增强啊，什么开操作，闭操作，图像物体简单检测，目标简单跟踪，人脸检测下之外，要识别物体就衰了。所以这阶段能把图像处理这本书以及Opencv学好这对后期的学习还是有很大的帮助的，不要着急去学习其他的所谓“切勿浮沙筑高台”嘛。基础还是很重要的。\n第二阶段：图像识别\n环境：Python2.7+scikit-learn\n这一阶段呢，需要学的也是很多的，首先就是从原理上了解。对于机器学习，无疑斯坦福大学的《机器学习》课程，吴恩达这位人工智能大牛主讲，帮助了无数人学习机器学习，从理论上讲解机器学习的数学原理，同时李航博士著的《统计学习方法》也深入浅出的道出了机器学习算法的原理，提出了方法 = 模型 + 策略 + 算法。本书也是按着这样的思路给我们娓娓道来的，在Opencv中也有对应的模块ml，这里面包含了常用的机器学习算法，同时Python版的scikit-learn也是学习机器学习的好库，同时也把Python这门语言提升一下，对于最终要学习的深度学习用Python实现相应功能还是很快捷方便的，毕竟Python的宗旨就是智能简洁；要学会常用算法的使用以及学会其中几个编写(还有好多都是用的库，比如SVM，这个自己写就很费时，利用前人造的轮子我们拿来用就好了)，其中SVM（支持向量机）的数学原理必须要明白，从线性可分到线性不可分是如何加入核函数的，以及如何定义loss function函数的，其中的证明过程也是要会的。ＳＶＭ还有一本书专门写它的，叫《支持向量机导论》里面都是关于它的数学证明（是不是感觉机器学习就是在搞数学，泪崩），虽然过程是痛苦的，但实现像人脸识别啊，车牌识别之类的识别工作通过机器学习进行分类效果还是很不错的，当然了这里面也是有很多关于图像上的知识要学，比如，你要实现人脸的识别，首先你得提取人脸的特征，让机器知道这是一张脸，之后才是识别的工作，这其中就牵扯到特征的提取，在人脸中利用的haar特征，又比如之前很火的ＨＯＧ＋SVM组合进行行人检测的，其中，HOG就是提取特征，SVM进行分类。这在Opencv中也有专门的模块讲解特征检测的。但是近年来随着神经网络的惊人的效果以及精确度，迅速占领了像CVPR、ICCV等这些顶级会议（计算机视觉界比较牛X的学术会议，代表了你在这一领域的学术水平，哈哈，小菜我如果能发一篇，好吧，回归现实），中间几乎所有的paper都用到了神经网络，所有学习它自然也是必然水到渠成的过程啦。\n第三部分：图像语义的理解\n终于来到这模块了，还很激动，毕竟小菜最近才刚把Caffe配置好，之前一直在看理论的神经网络知识。在小试牛刀之前呢，我们还得学习一些课程：其中要属李菲菲教授教的cs231n深度学习与视觉识别课程了，在这之前你需要了解什么是神经网络，以及它的结构组成，可以参考UFLDL课程，这里面详细的介绍了神经网络的结构。对于神经网络，其实这个在机器学习中就会接触到的，就是感知机，只不过之所以叫网络其实就是它组成的网络，还有重要经典的BP反馈神经网络，并学会它们的推导。这有利于你的理解，当然啦，在你学会了以上的课程，对CNN也有了一些理解了，此时就可以读一些经典的paper来了解常用的神经网络结构了，比如AlexNet，GoogleNet，LeNet，VGGNet等。在了解了常见的网络模型后，就可以配置跑自己的数据集啦。\n总结\n写的很粗糙，算是对自己学习路线的一个总结，同时对以后要做的事情作一个规划吧，那就是接下来花更多的时间来学习神经网络的知识，同时提升自己Python的编程能力，研二即将到来，要学的还有很多，向这一行业的大牛们虚心学习，望自己能早日入门。\n仅以此文记自己研一的生活。"}
{"content2":"2013-2017：中国 CV（计算机视觉）公司恩仇录\n36氪的朋友们 • 2017-09-26 • 人工智能\n这是属于中国 CV（计算机视觉）公司的五年，这是属于他们的时代。\n编者按：本文来自KnowingAI知智（Knowing_AI），作者 虞喵喵，36氪经授权发布。\n2013 年新广告法还没实行，旷视科技为 Face++ 写下文案：「最好的人脸识别云计算平台」。\n2014 年是「格灵深瞳」的天下，纸媒、门户、科技媒体争相邀其登上头版。据说徐小平、冯波和沈南鹏曾就其市值激烈争论，「最后妥协在了3000亿美元这个『中间数』上」。\n2015 年，偏居重庆的周曦悄悄创立云从科技，将目标定为「只做人脸，深耕金融和安防」。这一年商汤开始经历从学术到商业的激烈转型，还与几家「小巨头」深度绑定，成立了合资公司。\n2016 年，旷视拿到建银国际和富士康集团的 1 亿美金融资。一向低调的依图同年将业务从车辆、安防，向医疗、城市数据拓展。\n如今，云从的初心仍未改变，业务已渗透到四大国有银行；依图拿到 3.8 亿 C 轮融资，商汤则破天荒融到 4.1 亿美元，距上市更进一步。\n这是属于中国 CV（计算机视觉）公司的五年，这是属于他们的时代。\n一、2011-2013：开始时从四方赶来，我们都有光明的前途\n2011 年 8 月 iOS 平台第一款体感游戏《Crow Coming》发布时，市面上最新款苹果还是巴掌大的 iPhone4 。开发这款游戏的团队叫「VisionHacker」，成员只有三个人：印奇、唐文斌和杨沐。\n谁也没想到，正是这款游戏拉开了中国 CV 时代的序幕。两个月后，三人正式成立旷视科技——这大概是中国最早定位于计算机视觉的创业公司。再过两个月，他们便会顺利拿到联想之星的天使投资。\n通过摇晃头部赶走乌鸦的《Crow Coming》，正是一份用来展示人脸技术的敲门答卷。据当时的报道称，这款游戏不仅获得清华大学第三十届「挑战杯」特等奖，还在短期累积 40 万用户，「一度冲到中国区 App Store 排行榜的前五名」。\n分任 CEO、CTO 的印奇和唐文斌是 2006 届清华姚班同期，大二时又共同在微软亚研（MSRA）实习，只不过一个在人脸识别组，一个在图像搜索组。至于担任工程副总裁、曾经的国际信息学奥赛金奖得主杨沐，则是两人的学弟。\n三人早早便制定了旷视的「三步战略」：第一步，搭建 Face++ 人脸识别云服务平台，目标是识别人脸；第二步，搭建 Image++，目标是识别万物；最后，则是实现「所见即所得」的机器之眼。\n与之类似，赵勇也早早确定了格灵深瞳的方向。为了补上硬件短板而去哥大 CAVE 实验室进修、学成归来的印奇发现，站在计算机视觉风口浪尖的人，已经变成在美国呆了十几年、顶着 Google Glass 核心成员光环回国的赵勇。\n赵勇比印奇大上十几岁，是复旦电子工程系 95 级校友。自第一次在 CSDN 与媒体见面，赵勇就被打上成熟稳重的「工程师」标签。2013 年的互联网 VC，笃信硅谷，笃信 Google，笃信未经商业社会洗礼的实验室研究员，能代表全世界最先进的科技生产力。\n格灵深瞳的目标很简单：通过传感器实现三维图像，即通过激光发射器和接收器，以结构光的方式获取空间中物体的深度信息。基于此，计算机就可以对图像中人的行为进行识别和分析。\n与此同时，赵勇也在谷歌内部寻找合伙人，先后曾有两名工程师承诺加入，又在最后时刻反悔。这年 6 月，赵勇等来了联合创始人何博飞。这位由徐小平介绍的合伙人简历相当耀眼，斯坦福商学院、新光天地、总裁等关键词点缀其间。所有人都相信这是一次商业与技术人才的完美结合，必将迸射出 CV 界、甚至是科技界最绚烂的火花。\n站在两者之间的朱珑，说自己选择创业是「一种感觉」。2012 年，正在 MIT 实验室担任博士后研究员的朱珑，在 UCLA 视觉识别与机器学习中心主任、导师 Alan Yuille 教授的支持下毅然回国。他拉上自己的童年好友、ACM 全球大学生程序设计竞赛冠军林晨曦，共同创办了依图科技。\n这种感觉叫「计算机视觉离产业化非常近、非常近了」。\n二、2013-2015：共同踏上一条看上去笔直的道路\n安防是朱珑定位的第一个产业化场景。\n由于需要从海量视频监控数据中精准、快速找到要找的目标，安防领域有大量的视频、图像比对需求，是图像识别、特别是人脸识别技术的天然土壤。同时安防产品亟待智能化，这个市场既有需求，也有购买力。\n有技术、没客户，创业之初的朱珑和林晨曦坐在办公室里，罗列团队所有人的资源人脉。\n与大部分白手起家故事类似，最开始总有重重困难，奋力解决后会成为撬动未来的支点。当委托朋友的朋友、辗转找到一位公安局副局长时，朱珑只得到三分钟时间和一句话：「我们现在套牌车的识别率不到 30%，如果能将识别率提升到 70%，就考虑用。」\n每天上街拍车辆、见警察、了解业务流程。两个月后接受测试时，系统的号牌识别率和车辆品牌识别率都达到 90%。这套系统的秘密是「车脸」——同时识别「车牌」和「车脸」，更容易发现套牌车辆。\n甫一上线，正赶上一起入室抢劫案。通过过滤车辆品牌，警方十分钟便锁定了驾车逃离的犯罪嫌疑人。自此，依图便敲开公安系统大门。2015 年，这套名为蜻蜓眼系统还获得了「公安部科技进步奖」。\n车辆识别之后，朱珑开始把精力放到人脸识别上。据称某省公安厅曾用依图的系统比对常住人口、暂住人口与通缉犯库，当天比中 17 个通缉犯，抓到 3 人。后来还传奇般的抓到一名背负 3 条人命，流亡 16 年的九华山住持。\n「计算机视觉离产业化非常近了」的感觉，周曦也有。\n2006-2010 年，在跟随「计算机视觉之父」黄煦涛教授学习的日子里，周曦拿了不少图像识别比赛的冠军。「拿了这么多冠军我就想，不管是检查零件还是挽救生命，总要做点儿有意义的事儿吧。」恰好中国科学院重庆研究院筹建，院长袁家虎三次专程赴美邀请，黄煦涛教授便推出这位得意门生。\n2011 年，周曦以中国科学院「百人计划」专家身份被引进回国，拉上大学好友李继伟和温浩，组建了当时中科院最大的人脸识别研究团队。为了拿到中科院战略性先导科技专项，几个人重新设计产品、更新算法、换感光设备，在新疆和重庆之间来回奔波了好几个月。最终周曦小组击败其他团队，其人脸识别系统也被应用到新疆安防项目中。\n四年之后，意识到「一定要有个公司、有能力做商务推广让更多人使用这项技术」的周曦，正式成立了云从科技。他反复强调专注的重要性，并明确了云从的目标：「一是研究内容要集中，虽然什么都能做，但现在还是做好人脸；第二是行业上要集中，各行各业都能做，我们只做金融和安防。」\n在 2014-2015 年间针对格灵深瞳的采访中，时任 CEO 的何博飞也屡次提到「安防是比手机更大的一块市场」。对硬件和深度信息的执着，使得格灵深瞳天然适合安防场景，「拿到了天安门广场的全部订单，其它广场还是得一个一个谈」。\n事实上安防是一条看上去笔直，却无比曲折的道路。\n三、2015-2016：从学术空间，到商业世界\n原因很简单，人脸识别对场境要求非常强。产品能否达到使用要求，核心并不在于算法本身，而是对场景的深耕。\n2014 年，旷视曾拿下过FDDB评测（人脸检测）、300-W 评测（人脸关键点定位）和 LFW 评测（人脸识别）冠军；同年 7 月，商汤则以三种人脸识别算法占据了 LFW 测评前三名。随后，排行榜上「识别率」的最高数值一度被推高到 99.65%。\n这诚然是算法水平的有力证明，但也仅仅是停留在训练集与测试集之间，存在于实验室的「理论数值」。\nLFW 测试的图片数据多来自网络，现实生活中人脸的获取过程有大量不可控因素。光的方向、强度，是否有胡须、发型的变化，是否有表情都会影响识别效果。多种因素叠加后，真实环境下测得的准确率可能只有 75% 左右，甚至更低。\n场景自身同样包含着不同需求。如「证照对比」，二代身份证照片分辨率较低，或者拍摄时间跨度较大，都会影响识别效果。嫌疑人排查、有配合的一比一识别、无配合的一比一识别，都有不同的场景特点。\n这就需要针对场景的不同特点收集大量场景数据，不断调试参数、组合算法、方法，甚至使用外围硬件辅助以提升效果，不断迭代以实现产品化。除此之外，还要在工程上满足计算量、延迟、可维护性等需求。\n还有价格。2010 年时便有四十多家安防企业集体上市，2014 年的安防市场正在经历大规模洗牌，龙头企业已经在使用下压价格的方式打压中小公司。这一年 720P 的 IPC（网络摄像机）价格，已从两年前的上千元跌至 200 元。\n虽然正在进入智能时代，安防本质上仍是一门传统生意。在满足需求、找到应对方法的同时降低成本，抵抗市场对渠道、品牌的依赖，绝不是刚刚踏入安防领域的 CV 公司就能解决的问题。学术背景深厚的各位掌舵人顿时发现，与实验室里的算法参数相比，现实世界既复杂又不可控。\n在 2015 年 9 月的一次采访中，赵勇承认安防这类软硬结合的产品复杂度超出预计，「我们低估了这种产品在品控上的难度……对于未来，我们觉得更大的挑战是在商业上，如何在商务上发挥更大的价值」。打从一开始就定位在安防的格灵深瞳，选择的解决方案是从某上市安防公司挖来一名 VP，负责销售和商务。\n商汤的选择，是与安防老牌捆绑成立合资公司。这家由香港中文大学多媒体实验室带头人汤晓鸥与得意门生徐立成立的公司，是汤教授前半生学术积累面向商业世界的一次野心实践。\n2015 年 7 月，商汤与东方网力共同成立「深网视界」：商汤以其人群智能分析、人体Re-ID（检索）两项自有技术作价出资，持股 49%；东方网力出资 5000 万，持股 51%。该公司主要发展智能安防产品业务，致力成为「拥有计算机视觉和深度学习原创技术的领先安防产品提供商」。次年 4 月，商汤还并购安防黑马「新舟锐视」，以弥补其抢球联动产品的短板。\n与之类似，依图于 2016 年 5 月与传统身份识别解决方案商神思电子成立「深思依图」，由神思电子提供终端设备及嵌入式软件，依图提供所需软件，双方各持股 49%、51%；云从自正式成立时便绑定了智慧城市解决方案提供商、上市公司佳都科技；旷视则成立子品牌「旷视智安」，专注算法产品化。\n四、2016-2017：金融、医疗、自动驾驶……明天的风啊，吹向哪儿？\n2016 年，商汤与东方网力共同成立的深网视界营收为 4393 万，安防巨头海康威视则为 319 亿。\n诚然这是一块广阔到不容放弃的市场，但仍需要奋力耕耘和一点运气。\n与安防类似，金融领域也有大量的人脸需求。ATM 机刷脸取款、支付，活体识别，人证合一……其应用场景的丰富程度，和以技术手段代替重复人力劳动的迫切需求，使得 CV 公司早早就在该领域布局。\n2015 年 3 月，在德国汉诺威消费电子、信息及通信博览会（CeBIT）上，马云对着手机摄像头微微一笑，买下一枚 1948 年的汉诺威纪念邮票。这次面向世界的人脸支付场景展示，既宣告着人脸识别技术开始走向商业场景，同时是旷视一次面向世界的技术展示：作为合作伙伴，其人脸支付认证技术已得到蚂蚁金服认可。\n商汤也不甘示弱。2015 年 10 月借贷宝上线不久，商汤便与其达成深度合作，注册、大额转账、出借等环节会大几率触发人脸识别环节。二者不仅成立合资公司人商鼎诚，随后该公司又入股商汤子公司今始科技——更为人知的名字是「LinkFace」。借贷宝疯狂膨胀的超亿规模的用户，成就了当时世界最广泛的人脸识别技术应用。\n云从的目光一直紧盯着银行。工商银行、农业银行、建设银行、中国银行均使用其提供的人脸识别软件，应用于柜台、直销银行、手机银行、网银等场景。大型银行之外，西安银行、重庆银行、贵阳银行，海南银行等中小银行，广电运通、信雅达等大型银行系统供应商同样选择云从作为人脸识别供应商。\n依图也做了自己的尝试，通过与招商银行 VTM （Virtual Teller Machine，远程视频柜员机）合作，已经在 106 各城市近千台 ATM 机上实现「刷脸取现」。\n与安防类似，金融也有传统行业的一面，同样需要时间和运气。到底该将精力放到哪儿，CV 公司各有自己的想法。\n曾经为媒体追捧、如今深陷泥沼的格灵深瞳，赌对了自动驾驶。经赵勇劝说投身自动驾驶的吴甘沙，在 2015 年底与其共同成立驭势科技。2016 年 4 月，格灵深瞳作为投资方入股驭势科技，两者成为「兄弟公司」。\n今年 3 月，驭势在广州白云机场完成无人驾驶场地车试运营，明年下半年将展开小批量试产。据称何博飞出走后，由赵勇带领的格灵深瞳将延续此做法，很快将会有第二家类似方式运作的公司。\n依图则自 2016 年下半年开始发力医疗，其开发的胸部 CT 影响辅助诊断产品已经在浙江深人民医院、复旦大学附属肿瘤医院等数十家三甲医院部署，报告采纳率超过 90％。就在今日，依图宣布与浙江省儿童医院结成战略合作伙伴，并发布了一款基于儿童骨龄的智能辅助诊断系统。\nCV 之外，依图还在 NLP （自然语言处理）领域展开尝试，开发了基于海量病例的儿科辅助诊断系统「咪姆熊」，以及病例智能搜索引擎。\n继去年招募 MSRA（微软亚洲研究院）首席研究员孙剑加入后，今年 8 月旷视引入 Adobe 首席科学家王珏，负责旷视美国研究院。该研究院与孙剑带领的旷视北京研究院互补，在独立承担部分研发工作外，还要开拓新市场。在涉足过身份认证平台、智能园区、安防硬件 MegEye 等几乎所有人脸应用方向后，旷视也在做智能服务机器人等尝试。\n7 月获得 4.1 亿美元巨额融资的商汤，正在加速实现技术和产品落地，原本以算法、研发为重的思路正向商务倾斜。据透露，商汤如今有 5 位销售总监，每位手下 10 人，共计五十多名销售。\n说是恩仇录，计算机视觉的应用蛋糕还未做大，谈什么恩仇？\n他们的时代，才刚刚开始。\n据近日红杉、真格联合发布的《全球人工智能专利资源发展概况》报告中称，中国在计算机视觉方面的专利已跃居世界第一，占当前全球专利公开数量的55%。\n「如果在有生之年能够看到人工智能全面影响生活，哪怕下一秒就死掉，我也值了。」\n「如果我活到 70 岁，无论经历多少失败，我所做的事情一定会改变这个世界。」\n虽说 CV 的现实是抛去形容词后留下的部分，但这些都是 CV 公司掌舵者们曾流露过的真实心声。正是这种理想支撑他们挨过 AlphaGo 前的 AI 寒冬，走向下一个 CV 的五年。\n如今还有一个疑问，旷视和商汤，到底谁会先上市？"}
{"content2":"计算机视觉—卡尔曼滤波\nbrycezou@163.com\n整篇文章参考微信公众号【电子搬砖师】，有兴趣的读者可以关注一下，感谢作者！\n最近发现，只有理论推导而没有贴实用代码的博客阅读量都比较少，由此可见，技术博客也是快餐文化的一种。本人后面的博客尽量也贴一些典型代码，来满足更读者的胃口。卡尔曼滤波算法相关的代码后续补上\\^_\\^\n0、卡尔曼滤波的核心内容\n假设测量值和估计值都不是100%可信的，通过如下两步主要操作来估计真实值\n根据前一时刻的最优估计\nx(k−1|k−1)\nx(k-1|k-1) 得到当前时刻的普通估计\nx(k|k−1)\nx(k|k-1)\n根据当前时刻的普通估计\nx(k|k−1)\nx(k|k-1) 和当前时刻的观测\nz(k)\nz(k)，得到当前时刻的最优估计\nx(k|k)\nx(k|k)\n1、问题描述\n假设我们要测量房间的温度，用温度计进行了10分钟的测量，每隔1分钟测量一次，这10次的测量结果如下。我们还明白一个事实：由于测量设备（温度计）自身存在误差，测量过程也会引入干扰，因此，工程上通常不能直接使用测量设备的数据，而是先要对数据进行处理。那么问题来了，该如何进行处理？\n25.3 26.5 24.0 35.0（温度计受到干扰） 22.9 25.6 23.5（开启暖气） 28.8 30.2 29.5\n2、方案一：均值滤波\n均值滤波可简单描述为：连续采样 N 次，求均值后输出一次，再连续采样 N 次，求均值后输出一次，如此反复。对温度测量过程进行均值滤波，取 N=5，结果为：\nT1=(25.3+26.5+24.0+35.0+22.9)/5=26.7 T2=(25.6+23.5+28.8+30.2+29.5)/5=27.5\n均值滤波确实减缓了数据的跳变，但其更新频率太低，而且真实的变化过程（开启暖气）容易被淹没。\n3、方案二：一阶滞后滤波\n为了提高更新频率，同时避免淹没真实的变化过程，可以采用一阶滞后滤波算法，\n本次滤波结果=α∗本次采样值+(1−α)∗上次滤波结果\n本次滤波结果=\\alpha*本次采样值+(1-\\alpha)*上次滤波结果\n公式中的\nα\n\\alpha 表示对本次采样值的信任程度。设\nα=0.6\n\\alpha=0.6，由于第1分钟时没有上次滤波结果，可以根据经验初始化为26.0，则结果为：\n0.6*25.3+0.4*26.0=25.6 0.6*26.5+0.4*25.6=26.1 0.6*24.0+0.4*26.1=24.8 0.6*35.0+0.4*24.8=30.9 0.6*22.9+0.4*30.9=26.1 0.6*25.6+0.4*26.1=25.8 0.6*23.5+0.4*25.8=24.4 0.6*28.8+0.4*24.4=27.0 0.6*30.2+0.4*27.0=28.9 0.6*29.5+0.4*25.6=29.3\n一阶滞后滤波算法既减缓了温度的跳变，又提高了更新频率。但是它仍然没能排除掉干扰，而且在房间开启暖气后温度变化有点滞后。\n4、方案三：卡尔曼滤波\n一阶滞后滤波算法的权重\nα\n\\alpha 在计算过程中是保持不变的，而卡尔曼滤波算法的核心在于，可以根据测量值的变化实时更新权重。使用卡尔曼滤波需要知道的几个初始条件：\n原始值：用于启动迭代，通常取传感器前 N 组测量数据的均值。本例中，原始温度取26.0度。\n原始值误差：原始值通常是根据经验得到的，存在误差。本例中，假定原始温度误差是2度。\n测量系统误差：假定温度计的测量误差为是3度。\n下一次的最大偏差：由于房间温度相对恒定，即使开启暖气下一次最多也只能变化5度。\n使用卡尔曼滤波算法进行迭代：首先，用26.0作为原始温度，由于认为房间温度恒定，因此下一分钟的估计温度也是26.0度。其次，下一分钟的测量温度是25.3度。此时，我们有两个可以参考的温度值，26.0度和25.3度，究竟该如何权衡？定义测量值的信任比例为\nkg=原始值误差2+下一次的最大偏差2原始值误差2+下一次的最大偏差2+测量系统误差2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾⎷\nkg=\\sqrt{\\frac{原始值误差^2+下一次的最大偏差^2}{原始值误差^2+下一次的最大偏差^2+测量系统误差^2}}\n代入本例中可得，测量值25.3的信任比例为\nkg=22+5222+52+32‾‾‾‾‾‾‾‾‾‾‾‾‾√=0.874\nkg=\\sqrt{\\frac{2^2+5^2}{2^2+5^2+3^2}}=0.874\n因此，第一分钟的最优温度估计为\nT1=26.0*(1-0.874)+25.3*0.874=25.4\n这个25.4度将会成为下一轮迭代的原始值。此外，由于卡尔曼滤波的结果更加接近真实世界的数值，因此下一轮迭代的原始值误差也会被更新\n下一轮原始值误差=(1−kg)∗(本轮原始值误差2+下一次的最大偏差2)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√\n下一轮原始值误差=\\sqrt{(1-kg)*(本轮原始值误差^2+下一次的最大偏差^2)}\n代入本例中可得\n原始值误差=(1−0.874)∗(22+52)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√=1.9\n原始值误差=\\sqrt{(1-0.874)*(2^2+5^2)}=1.9\n下面进行第二轮迭代。首先，计算测量值26.5的信任比例\nkg=1.92+521.92+52+32‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√=0.872\nkg=\\sqrt{\\frac{1.9^2+5^2}{1.9^2+5^2+3^2}}=0.872\n其次，计算第二分钟的最优温度估计\nT2=25.4*(1-0.872)+26.5*0.872=26.4\n最后，更新下一轮迭代的原始值误差\n原始值误差=(1−0.872)∗(1.92+52)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√=1.9\n原始值误差=\\sqrt{(1-0.872)*(1.9^2+5^2)}=1.9\n之后进行第三轮迭代\n⋯\n\\cdots\n卡尔曼滤波经过多次迭代后会逐渐缩小原始值误差，输出结果也更加接近真实值。如果将本例中的下一次最大偏差从5度改为2度，那么传感器测量值的信任比例将会变小，滤波器输出的温度值跳动也会变小，对干扰的抑制作用会更加明显，但对于真实环境的响应也会变得更加滞后。\n5、单变量形式推导\n参考：http://www.tina-vision.net/docs/memos/2003-003.pdf\n设温度的估计值服从正态分布\nf(x)=12πσ2估‾‾‾‾‾√exp⎛⎝⎜⎜−(x−μ估)22σ2估⎞⎠⎟⎟\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma_估^2}}\\exp\\left(-\\frac{(x-\\mu_估)^2}{2\\sigma_估^2}\\right)\n设温度的测量值也服从正态分布\ng(x)=12πσ2测‾‾‾‾‾√exp⎛⎝⎜⎜−(x−μ测)22σ2测⎞⎠⎟⎟\ng(x)=\\frac{1}{\\sqrt{2\\pi\\sigma_测^2}}\\exp\\left(-\\frac{(x-\\mu_测)^2}{2\\sigma_测^2}\\right)\n融合两个正态分布，可得\nh(x)=f(x)g(x)=12πσ测σ估exp⎛⎝⎜⎜−(x−μ测)22σ2测−(x−μ估)22σ2估⎞⎠⎟⎟\nh(x)=f(x)g(x)=\\frac{1}{2\\pi\\sigma_测\\sigma_估}\\exp\\left(-\\frac{(x-\\mu_测)^2}{2\\sigma_测^2}-\\frac{(x-\\mu_估)^2}{2\\sigma_估^2}\\right)\n令指数部分\nβ=(x−μ测)22σ2测+(x−μ估)22σ2估=(σ2测+σ2估)x2−2(μ测σ2估+μ估σ2测)x+μ2测σ2估+μ2估σ2测2σ2测σ2估\n\\beta=\\frac{(x-\\mu_测)^2}{2\\sigma_测^2}+\\frac{(x-\\mu_估)^2}{2\\sigma_估^2}= \\frac{(\\sigma_测^2+\\sigma_估^2)x^2-2(\\mu_测\\sigma_估^2+\\mu_估\\sigma_测^2)x+\\mu_测^2\\sigma_估^2+\\mu_估^2\\sigma_测^2}{2\\sigma_测^2\\sigma_估^2}\\\\\n化简可得\nβ=(x−μ测σ2估+μ估σ2测σ2测+σ2估)22σ2测σ2估σ2测+σ2估+(μ测−μ估)22(σ2测+σ2估)\n\\beta=\\frac{\\left(x-\\frac{\\mu_测\\sigma_估^2+\\mu_估\\sigma_测^2}{\\sigma_测^2+\\sigma_估^2}\\right)^2}{2\\frac{\\sigma_测^2\\sigma_估^2}{\\sigma_测^2+\\sigma_估^2}}+\\frac{(\\mu_测-\\mu_估)^2}{2(\\sigma_测^2+\\sigma_估^2)}\n于是，两个正态分布的乘积也服从正态分布\nμ新=μ测σ2估+μ估σ2测σ2测+σ2估=kg∗μ测+(1−kg)∗μ估σ新=σ2测σ2估σ2测+σ2估‾‾‾‾‾‾‾‾‾⎷=(1−kg)∗σ2估‾‾‾‾‾‾‾‾‾‾‾‾‾√\n\\mu_新=\\frac{\\mu_测\\sigma_估^2+\\mu_估\\sigma_测^2}{\\sigma_测^2+\\sigma_估^2}=kg*\\mu_测+(1-kg)*\\mu_估\\\\ \\sigma_新=\\sqrt{\\frac{\\sigma_测^2\\sigma_估^2}{\\sigma_测^2+\\sigma_估^2}}=\\sqrt{(1-kg)*\\sigma_估^2}\n其中，\nkg\nkg 就是前文提到的信任比例，即卡尔曼增益\nkg=σ2估σ2测+σ2估\nkg=\\frac{\\sigma_估^2}{\\sigma_测^2+\\sigma_估^2}\n6、矩阵形式推导\n卡尔曼滤波其实就5个公式\nX(k|k−1)=AX(k−1|k−1)+BU(k−1)P(k|k−1)=AP(k−1|k−1)AT+QKg(k)=P(k|k−1)HT[HP(k|k−1)HT+R]−1X(k|k)=X(k|k−1)+Kg(k)[Z(k)−HX(k|k−1)]P(k|k)=[I−Kg(k)H]P(k|k−1)\n\\begin{eqnarray*} & & X(k|k-1)=AX(k-1|k-1)+BU(k-1)\\\\ & & P(k|k-1)=AP(k-1|k-1)A^T+Q\\\\ & & Kg(k)=P(k|k-1)H^T[HP(k|k-1)H^T+R]^{-1}\\\\ & & X(k|k)=X(k|k-1)+Kg(k)[Z(k)-HX(k|k-1)]\\\\ & & P(k|k)=[I-Kg(k)H]P(k|k-1) \\end{eqnarray*}\n其中，\nX(k−1|k−1)\nX(k-1|k-1) 是上一时刻的最优估计；\nX(k|k−1)\nX(k|k-1) 是当前时刻的基本估计，由上一时刻的最优估计得来；\nA\nA 是状态转移矩阵；\nB\nB 是控制量矩阵；\nU(k−1)\nU(k-1) 是上一时刻的控制量。\n6.1 第二个公式\n设真实世界中，\nk\nk 时刻的状态为\nX(k)=AX(k−1)+BU(k−1)+W(k−1)\nX(k)=AX(k-1)+BU(k-1)+W(k-1)\n则\nk\nk 时刻的真实值\nX(k)\nX(k) 和基本估计\nX(k|k−1)\nX(k|k-1) 之间的误差为\ne(k|k−1)===X(k)−X(k|k−1)AX(k−1)+BU(k−1)+W(k−1)−[AX(k−1|k−1)+BU(k−1)]AX(k−1)−AX(k−1|k−1)+W(k−1)\n\\begin{eqnarray*} e(k|k-1) & = & X(k)-X(k|k-1)\\\\ & = & AX(k-1)+BU(k-1)+W(k-1)-[AX(k-1|k-1)+BU(k-1)]\\\\ & = & AX(k-1)-AX(k-1|k-1)+W(k-1) \\end{eqnarray*}\n该误差的方差为\nD[e(k|k−1)]======D[X(k)−X(k|k−1)]D[AX(k−1)−AX(k−1|k−1)+W(k−1)]D[AX(k−1)−AX(k−1|k−1)]+D[W(k−1)]+2cov(AX(k−1)−AX(k−1|k−1),W(k−1))D[AX(k−1)−AX(k−1|k−1)]+D[W(k−1)][AX(k−1)−AX(k−1|k−1)][AX(k−1)−AX(k−1|k−1)]T+W(k−1)W(k−1)TA[X(k−1)−X(k−1|k−1)][X(k−1)−X(k−1|k−1)]TAT+W(k−1)W(k−1)T\n\\begin{eqnarray*} D[e(k|k-1)] & = & D[X(k)-X(k|k-1)]\\\\ & = & D[AX(k-1)-AX(k-1|k-1)+W(k-1)]\\\\ & = & D[AX(k-1)-AX(k-1|k-1)]+D[W(k-1)]+2cov(AX(k-1)-AX(k-1|k-1),W(k-1))\\\\ & = & D[AX(k-1)-AX(k-1|k-1)]+D[W(k-1)]\\\\ & = & [AX(k-1)-AX(k-1|k-1)][AX(k-1)-AX(k-1|k-1)]^T+W(k-1)W(k-1)^T\\\\ & = & A[X(k-1)-X(k-1|k-1)][X(k-1)-X(k-1|k-1)]^TA^T+W(k-1)W(k-1)^T \\end{eqnarray*}\n在数学形式上有\nD[e(k−1|k−1)]=D[X(k−1)−X(k−1|k−1)]=[X(k−1)−X(k−1|k−1)][X(k−1)−X(k−1|k−1)]T\nD[e(k-1|k-1)]=D[X(k-1)-X(k-1|k-1)]=[X(k-1)-X(k-1|k-1)][X(k-1)-X(k-1|k-1)]^T\n因而\nD[e(k|k−1)]=AD[e(k−1|k−1)]AT+W(k−1)W(k−1)T\nD[e(k|k-1)]=AD[e(k-1|k-1)]A^T+W(k-1)W(k-1)^T\n令\nP(k|k−1)=D[e(k|k−1)]Q(k−1)=W(k−1)W(k−1)T\n\\begin{eqnarray*} & & P(k|k-1)=D[e(k|k-1)]\\\\ & & Q(k-1)=W(k-1)W(k-1)^T\\\\ \\end{eqnarray*}\n则\nP(k|k−1)=AP(k−1|k−1)AT+Q(k−1)\nP(k|k-1)=AP(k-1|k-1)A^T+Q(k-1)\n其中，\nP(k−1|k−1)\nP(k-1|k-1) 为\nk−1\nk-1 时刻的真实值\nX(k−1)\nX(k-1) 和最优估计\nX(k−1|k−1)\nX(k-1|k-1) 之间的\n误差的方差\n，卡尔曼滤波的本质就是减小误差的方差。\n6.2 第四个公式\n设当前时刻传感器的测量数据为\nZ(k)\nZ(k)，将其定义为\nZ(k)=HX(k)+V(k)\nZ(k)=HX(k)+V(k)\n需要说明的是，有时\nX(k)\nX(k) 中的部分数据是无法用传感器直接测量的，但这些数据又可以通过其它测量值推算出来，\nH\nH 矩阵就用来表示\nX(k)\nX(k) 中数据的可测量情况。\nV(k)\nV(k) 表示传感器的测量误差。至此，第四个公式便很容易理解：\n当前时刻的残差，乘以当前时刻的卡尔曼增益，再加上当前时刻的基本估计，便得到当前时刻的最优估计\n。最优估计其实是传感器测量值和基本估计的加权平均。\n6.3 第三个公式\n上文中提到，\nP(k|k−1)\nP(k|k-1) 是\nk\nk 时刻的真实值\nX(k)\nX(k) 和基本估计\nX(k|k−1)\nX(k|k-1) 之间的误差的方差。那么，\nP(k|k)\nP(k|k) 就是\nk\nk 时刻的真实值\nX(k)\nX(k) 和最优估计\nX(k|k)\nX(k|k) 之间的误差的方差，\nP(k|k)==============D[X(k)−X(k|k)]D{X(k)−X(k|k−1)−Kg(k)[Z(k)−HX(k|k−1)]}D{X(k)−X(k|k−1)−Kg(k)Z(k)+Kg(k)HX(k|k−1)}D{X(k)−X(k|k−1)−Kg(k)[HX(k)+V(k)]+Kg(k)HX(k|k−1)}D{X(k)−X(k|k−1)−Kg(k)HX(k)−Kg(k)V(k)+Kg(k)HX(k|k−1)}D{[I−Kg(k)H][X(k)−X(k|k−1)]−Kg(k)V(k)}D{[I−Kg(k)H][X(k)−X(k|k−1)]}+D{Kg(k)V(k)}+2cov(first,second)D{[I−Kg(k)H][X(k)−X(k|k−1)]}+D{Kg(k)V(k)}[I−Kg(k)H][X(k)−X(k|k−1)][X(k)−X(k|k−1)]T[I−Kg(k)H]T+Kg(k)V(k)V(k)TKg(k)T[I−Kg(k)H]P(k|k−1)[I−Kg(k)H]T+Kg(k)R(k)Kg(k)T[P(k|k−1)−Kg(k)HP(k|k−1)][I−Kg(k)H]T+Kg(k)R(k)Kg(k)TP(k|k−1)−P(k|k−1)HTKg(k)T−Kg(k)HP(k|k−1)+Kg(k)HP(k|k−1)HTKg(k)T+Kg(k)R(k)Kg(k)TP(k|k−1)−P(k|k−1)THTKg(k)T−Kg(k)HP(k|k−1)+Kg(k)[HP(k|k−1)HT+R(k)]Kg(k)TP(k|k−1)−[Kg(k)HP(k|k−1)]T−Kg(k)HP(k|k−1)+Kg(k)[HP(k|k−1)HT+R(k)]Kg(k)T\n\\begin{eqnarray*} P(k|k) & = & D[X(k)-X(k|k)]\\\\ & = & D\\{X(k)-X(k|k-1)-Kg(k)[Z(k)-HX(k|k-1)]\\}\\\\ & = & D\\{X(k)-X(k|k-1)-Kg(k)Z(k)+Kg(k)HX(k|k-1)\\}\\\\ & = & D\\{X(k)-X(k|k-1)-Kg(k)[HX(k)+V(k)]+Kg(k)HX(k|k-1)\\}\\\\ & = & D\\{X(k)-X(k|k-1)-Kg(k)HX(k)-Kg(k)V(k)+Kg(k)HX(k|k-1)\\}\\\\ & = & D\\{[I-Kg(k)H][X(k)-X(k|k-1)]-Kg(k)V(k)\\}\\\\ & = & D\\{[I-Kg(k)H][X(k)-X(k|k-1)]\\}+D\\{Kg(k)V(k)\\}+2cov(first,second)\\\\ & = & D\\{[I-Kg(k)H][X(k)-X(k|k-1)]\\}+D\\{Kg(k)V(k)\\}\\\\ & = & [I-Kg(k)H][X(k)-X(k|k-1)][X(k)-X(k|k-1)]^T[I-Kg(k)H]^T+Kg(k)V(k)V(k)^TKg(k)^T\\\\ & = & [I-Kg(k)H]P(k|k-1)[I-Kg(k)H]^T+Kg(k)R(k)Kg(k)^T\\\\ & = & [P(k|k-1)-Kg(k)HP(k|k-1)][I-Kg(k)H]^T+Kg(k)R(k)Kg(k)^T\\\\ & = & P(k|k-1)-P(k|k-1)H^TKg(k)^T-Kg(k)HP(k|k-1)+Kg(k)HP(k|k-1)H^TKg(k)^T+Kg(k)R(k)Kg(k)^T\\\\ & = & P(k|k-1)-P(k|k-1)^TH^TKg(k)^T-Kg(k)HP(k|k-1)+Kg(k)[HP(k|k-1)H^T+R(k)]Kg(k)^T\\\\ & = & P(k|k-1)-[Kg(k)HP(k|k-1)]^T-Kg(k)HP(k|k-1)+Kg(k)[HP(k|k-1)H^T+R(k)]Kg(k)^T\\\\ \\end{eqnarray*}\n其中\nR(k)=V(k)V(k)T\nR(k)=V(k)V(k)^T\n卡尔曼滤波本质上是一个不断减小（最优估计和真实值之间的）误差的方差的过程。\nP(k|k)\nP(k|k) 是误差的方差矩阵，其对角线元素就是各个状态变量的误差的方差，最小化各个方差就等价于最小化方差的和，也即最小化方差矩阵的迹\n∂tr{P(k|k)}∂Kg(k)======∂tr{P(k|k−1)}∂Kg(k)−∂tr{[Kg(k)HP(k|k−1)]T}∂Kg(k)−∂tr{Kg(k)HP(k|k−1)}∂Kg(k)+∂tr{Kg(k)[HP(k|k−1)HT+R(k)]Kg(k)T}∂Kg(k)0−[HP(k|k−1)T]T−[HP(k|k−1)]T+∂tr{Kg(k)[HP(k|k−1)HT+R(k)]Kg(k)T}∂Kg(k)−2P(k|k−1)HT+Kg(k)[HP(k|k−1)HT+R(k)]+Kg(k)[HP(k|k−1)HT+R(k)]T−2P(k|k−1)HT+Kg(k)[HP(k|k−1)HT+R(k)]+Kg(k)[HP(k|k−1)HT]T+Kg(k)R(k)T−2P(k|k−1)HT+Kg(k)[HP(k|k−1)HT+R(k)]+Kg(k)[HP(k|k−1)HT]+Kg(k)R(k)−2P(k|k−1)HT+2Kg(k)[HP(k|k−1)HT+R(k)]\n\\begin{eqnarray*} \\frac{\\partial tr\\{P(k|k)\\}}{\\partial Kg(k)} & = & \\frac{\\partial tr\\{P(k|k-1)\\}}{\\partial Kg(k)}- \\frac{\\partial tr\\{[Kg(k)HP(k|k-1)]^T\\}}{\\partial Kg(k)}- \\frac{\\partial tr\\{Kg(k)HP(k|k-1)\\}}{\\partial Kg(k)}+ \\frac{\\partial tr\\{Kg(k)[HP(k|k-1)H^T+R(k)]Kg(k)^T\\}}{\\partial Kg(k)}\\\\ & = & 0- [HP(k|k-1)^T]^T- [HP(k|k-1)]^T+ \\frac{\\partial tr\\{Kg(k)[HP(k|k-1)H^T+R(k)]Kg(k)^T\\}}{\\partial Kg(k)}\\\\ & = & -2P(k|k-1)H^T+Kg(k)[HP(k|k-1)H^T+R(k)]+Kg(k)[HP(k|k-1)H^T+R(k)]^T\\\\ & = & -2P(k|k-1)H^T+Kg(k)[HP(k|k-1)H^T+R(k)]+Kg(k)[HP(k|k-1)H^T]^T+Kg(k)R(k)^T\\\\ & = & -2P(k|k-1)H^T+Kg(k)[HP(k|k-1)H^T+R(k)]+Kg(k)[HP(k|k-1)H^T]+Kg(k)R(k)\\\\ & = & -2P(k|k-1)H^T+2Kg(k)[HP(k|k-1)H^T+R(k)]\\\\ \\end{eqnarray*}\n令\n∂tr{P(k|k)}∂Kg(k)=0\n\\frac{\\partial tr\\{P(k|k)\\}}{\\partial Kg(k)}=0\n可得\nKg(k)=P(k|k−1)HT[HP(k|k−1)HT+R(k)]−1\nKg(k)=P(k|k-1)H^T[HP(k|k-1)H^T+R(k)]^{-1}\n6.4 第五个公式\nP(k|k)=====D[X(k)−X(k|k)]⋯P(k|k−1)−P(k|k−1)THTKg(k)T−Kg(k)HP(k|k−1)+Kg(k)[HP(k|k−1)HT+R(k)]Kg(k)TP(k|k−1)−P(k|k−1)THTKg(k)T−Kg(k)HP(k|k−1)+P(k|k−1)HT[HP(k|k−1)HT+R(k)]−1[HP(k|k−1)HT+R(k)]Kg(k)T[I−Kg(k)H]P(k|k−1)\n\\begin{eqnarray*} P(k|k) & = & D[X(k)-X(k|k)]\\\\ & = & \\cdots\\\\ & = & P(k|k-1)-P(k|k-1)^TH^TKg(k)^T-Kg(k)HP(k|k-1)+Kg(k)[HP(k|k-1)H^T+R(k)]Kg(k)^T\\\\ & = & P(k|k-1)-P(k|k-1)^TH^TKg(k)^T-Kg(k)HP(k|k-1)+P(k|k-1)H^T[HP(k|k-1)H^T+R(k)]^{-1}[HP(k|k-1)H^T+R(k)]Kg(k)^T\\\\ & = & [I-Kg(k)H]P(k|k-1) \\end{eqnarray*}"}
{"content2":"本文已经首发在个人微信公共号：计算机视觉life（微信号CV_life），欢迎关注！\n国内/外每年都会举办很多计算机视觉（Computer Vision，CV）、 机器学习（Machine Learning，ML）、人工智能（Artificial Intelligence ，AI）领域相关的很多学术会议和研讨会等。在此把我所知道的2018年国内/外即将举办的CV和ML领域几个会议和研讨会列出来，希望对读者有用。如有其他我遗漏的，还请各位读者留言提醒，我会及时更新的。\nAAAI 2018（美国新奥尔良）\n全称：the Association for the Advancement of Artificial Intelligence\n时间：2018.02.02-07\n地点：Hilton New Orleans Riverside, New Orleans, Lousiana, USA\n介绍：人工智能领域顶级会议\n官网：https://aaai.org/Conferences/AAAI-18/\nMMM2018 (泰国曼谷)\n全称：the 24rd International Conference on MultiMedia Modeling\n时间：2018.02.05-08\n地点：Dusit Thani Hotel, Bangkok, Thailand\n介绍：多媒体建模及应用领域国际权威会议\n官网：http://mmm2018.chula.ac.th/\nICIGP 2018（中国香港）\n全称：INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING\n时间：2018.02.24-26\n地点：Regal Oriental Hotel，Hong Kong\n介绍：促进学术界和工业界交流的图形图像处理会议\n官网：http://icigp.org/\nACM - ICIAI 2018（中国上海）\n全称：2018 2nd International Conference on Innovation in Artificial Intelligence\n时间：2018.03.09-12\n地点：Shanghai, China\n网址：http://www.iciai.org/\nWACV 2018 (美国内华达州)\n全称：IEEE Winter Conference on Applications of Computer Vision\n时间：2018.03.12-15\n地点：Harvey’s Casino in Lake Tahoe, Nevada，U.S.\n介绍：侧重计算机视觉应用的国际知名会议\n网址：http://wacv18.uccs.us/\nICASSP 2018（加拿大卡尔加里）\n全称：International Conference on Acoustics, Speech and Signal Processing\n时间：2018.04.15-20\n地点：ELUS Convention Center, Calgary, Alberta, Canada\n介绍：声学、语音与信号处理及其应用国际顶级会议\n官网：https://2018.ieeeicassp.org/\nCVM2018（中国上海）\n全称：Computational Visual Media conference\n时间：2018.4.18-20\n地点：上海，上海交通大学\n介绍：计算机视觉相关的基础研究和应用会议\n官网：http://iccvm.org/2018/\nICLR 2018（加拿大温哥华）\n全称：6th International Conference on Learning Representations\n时间：2018.04.30-05.03\n地点：Vancouver Convention Center, Vancouver, BC, Canada\n主题：deep learning，compositional modeling, structured prediction, reinforcement learning, large-scale learning，non-convex optimization\n官网：http://www.iclr.cc/doku.php?id=ICLR2018:main&redirect=1\nICRA 2018（澳大利亚昆士兰）\n全称：IEEE International Conference on Robotics and Automation\n时间：2018.05.21-25\n地点：The Brisbane Convention & Exhibition Venue,, Brisbane, Queensland Australia\n介绍：机器人及自动化领域国际顶级会议\n官网：http://icra2018.org/\nCVPR 2018（美国盐湖城）\n全称：IEEE Conference on Computer Vision and Pattern Recognition\n时间：2018.06.18-22\n地点：SALT LAKE CITY, UTAH，USA\n介绍：计算机视觉及模式识别领域国际三大顶级会议之一\n官网： http://cvpr2018.thecvf.com/\nICML 2018（瑞典斯德哥尔摩）\n全称：Thirty-fifth International Conference on Machine Learning\n时间：2018.07.10-15\n地点：Stockholm Sweden\n介绍：机器学习国际著名会议\n官网：https://2017.icml.cc/\nIJCAI-ECAI 2018（瑞典斯德哥尔摩）\n全称： 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence\n时间：2018.07.13-19\n地点：Stockholm, Sweden\n介绍：国际顶级人工智能联合大会\n官网：https://www.ijcai-18.org/\nSIGGRAPH 2018（加拿大温哥华）\n时间：2018.08.12-16\n地点：Vancouver, Canada\n介绍：计算机图形学领域最权威、影响力最大的国际会议\n官网：http://s2018.siggraph.org/conference/conference-overview/\nICPR 2018(中国北京)\n全称：24th International Conference on Pattern Recognition\n时间：2018.08.20-24\n地点：Beijing, China\n介绍：模式识别国际著名会议\n官网：http://www.icpr2018.org/\nBMVC 2018（英国纽卡斯尔）\n全称： 29TH BRITISH MACHINE VISION CONFERENCE\n时间：2018.09.03-06\n地点：Northumbria University， North East of England\n介绍：计算机视觉领域著名国际会议\n官网：http://bmvc2018.org/\n3D VISON 2018(意大利维罗那)\n全称：International Conference on 3D Vision\n时间：2018.09.05-08\n地点： Verona, Italy\n主题：3D视觉研究及应用的著名会议\n官网：http://3dv18.uniud.it/\nECCV 2018（德国慕尼黑）\n全称：European Conference on Computer Vision\n时间：2018.09.08-14\n地点：Munich, Germany\n介绍：计算机视觉国际三大顶级会议之一\n官网：https://eccv2018.org/\nICCVG 2018（波兰华沙）\n全称：International Conference on Computer Vision and Graphics\n时间：2018.09.17-19\n地点：Warsaw, Poland\n官网：http://iccvg.wzim.sggw.pl/\nIROS 2018（西班牙马德里）\n全称：IEEE/RSJ International Conference on Intelligent Robots and Systems\n时间：2018.10.01-05\n地点：Madrid, Spain\n介绍：智能机器人系统领域国际顶级会议\n官网：http://www.iros2018.org/\nICIP 2018 （希腊雅典）\n全称：2017 IEEE International Conference on Image Processing\n时间：2018.10.07-10\n地点：Athens, Greece\n介绍：图像处理领域国际著名会议\n官网：https://2018.ieeeicip.org/\nNIPS 2018（加拿大蒙特利尔）\n全称：The Conference and Workshop on Neural Information Processing Systems\n时间：2018.12.03-08\n地点：Palais des Congrès de Montréal, Montréal CANADA\n介绍：Machine learning , computational neuroscience领域国际顶级会议\n官网：https://nips.cc/Conferences/2018\nJCRAI 2018 （埃及开罗）\n全称：International Joint Conference on Robotics and Artificial Intelligence\n时间：2018.12.21-23\n地点：Cairo Egypt\n介绍：图像处理领域国际会议\n官网：http://www.jcrai.org/\n持续更新。。\n欢迎志同道合的朋友关注 微信公共号：计算机视觉life。\n坚持原创。分享计算机视觉、机器学习、人工智能及相关领域前沿资讯、技术干货、产业理解。"}
{"content2":"本文首发于公众号：计算机视觉life。原文链接点这里\n有什么用？\n室内场景的稠密三维重建目前是一个非常热的研究领域，其目的是使用消费级相机（本文特指深度相机）对室内场景进行扫描，自动生成一个精确完整的三维模型，这里所说的室内可以是一个区域，一个房间，甚至是一整栋房屋。此外，该领域注重(一般是GPU上)实时重建，也就是一边扫描就可以一边查看当前重建的结果。如下所示。\n主要的应用包括室内的增强现实游戏、机器人室内导航、AR家具展示等。\n什么原理？\n在介绍原理前，先简单了解一下历史发展。\n1、发展历史\n在消费级深度相机出现之前，想要采用普通相机实现实时稠密三维重建比较困难。微软2010年发布了Kinect之后，基于深度相机的稠密三维重建掀起了研究热潮。早期比较有代表性的工作是2011年微软的Newcombe（单目稠密重建算法DTAM 的作者）、Davison等大牛发表在SIGGRAPH上的KinectFusion算法，算是该领域的开山之作。KinectFusion算法首次实现了基于廉价消费类相机的实时刚体重建，在当时是非常有影响力的工作，它极大的推动了实时稠密三维重建的商业化进程。下图所示是几款消费级深度相机。\nKinectFusion之后，陆续出现了Kintinuous，ElasticFusion，InfiniTAM，BundleFusion等非常优秀的工作。其中2017年斯坦福大学提出的BundleFusion算法，可以说是目前基于RGB-D相机进行稠密三维重建效果最好的方法了。本文主要以该算法为基础进行介绍。值得一提的是，目前室内三维重建和语意理解结合的研究越来越多，这方面暂不做介绍。\n2、基本原理\nBundleFusion是目前效果最好的开源算法框架，其概览图如下所示。\n我们先来看一下输入输出是什么。\n输入：RGB-D相机采集的对齐好的color+depth的数据流，这里使用的是structure\nsensor+iPad输出的30Hz，分辨率为640x480的序列图像。\n输出：重建好的三维场景。\n输入的color+depth的数据流首先需要做帧与帧之间的对应关系匹配，然后做全局位姿优化，将整体的漂移校正过来（上图下方所示），整个重建过程中模型是在不断动态更新的。\n上面只是一个概括的介绍，我们深入看一下。算法具体流程图如下：\n虽然看起来很复杂，其实思想还是非常直观的。下面分别介绍一下。\n在匹配方面，这里使用的是一种sparse-then-dense的并行全局优化方法。也就是说，先使用稀疏的SIFT特征点来进行比较粗糙的配准，因为稀疏特征点本身就可以用来做loop closure检测和relocalization。然后使用稠密的几何和光度连续性进行更加细致的配准。下图展示了sparse+dense这种方式和单纯sparse的对比结果。\n在位姿优化方面。这里使用了一种分层的 local-to-global 的优化方法，如下图所示。总共分为两层，在最低的第一层，每连续10帧组成一个chunk，第一帧作为关键帧，然后对这个chunk内所有帧做一个局部位姿优化。在第二层，只使用所有的chunk的关键帧进行互相关联然后进行全局优化。为什么要分层这么麻烦呢？或者说这样分层有什么好处呢？因为可以剥离出关键帧，减少存储和待处理的数据。并且这种分层优化方法减少了每次优化时的未知量，保证该方法可扩展到大场景而漂移很小。\n在稠密场景重建方面。一个关键的点在于模型的对称型更新：若要增加更新的一帧估计，需先去掉旧的一帧，然后在新的位姿处重新整合。这样理论上来说，随着帧越来越多，模型会越来越精确。\n作为纯视觉的算法，其鲁棒性非常重要，因此该算法在特征匹配对筛选方面还是做了不少工作。特征点对需要经历如下三种考验才能通关：\n第一种是直接对关键点本身分布的一致性和稳定性进行考验。如下图所示，绿色部分才是符合一致性的对应关系。\n第二道关卡是对特征匹配对跨越的表面面积进行考验，去掉特别小的，因为跨越面积较小的的话很容易产生歧义。第三道关卡是进行稠密的双边几何和光度验证，去掉重投影误差较大的匹配对。如下图所示两个同样的显示屏导致了左右的错误匹配，是通不过这道关卡的。\n此外在chunk内外如何进行局部和全局的处理，以及具体的位姿优化能量函数方面的细节就不介绍了，可以参考文末给出的论文和代码。\n效果怎么样？\n1、算法优点\n先来总结下算法的优点：\n使用持续的local to global分层优化，去除了时域跟踪的依赖。\n不需要任何显示的loop closure检测。因为每一帧都和历史帧相关，所以其实包含了持续的隐式的loop closure。\n支持在GPU上实时鲁棒跟踪，可以在跟踪失败时移动到重建成功的地方进行relocalization，匹配上后继续跟踪。\n2、重建效果\n实验表明该方法确实是目前该领域效果最好的方法，口说无凭，看图说话。下图是和其他方法的对比，方框内是细节的对比，重建优势还是很明显的。\n具体看一下重建细节如下图所示，还是挺精细的。\n另外在loop closure方面该算法做的也不错，在纹理比较丰富的情况下误差基本都能校正回来。如下图所示。\nhttps://img-blog.csdn.net/20180609230212827?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2VsZWN0ZWNoNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70” alt=”这里写图片描述” title=”” />fill/I0JBQkFCMA==/dissolve/70)\n3、运行速度\n关于运行速度，目前可以做到在两块GPU上实时。在演示视频中，structure sensor是卡在iPad上，将采集到的数据通过无线网络传给台式机（带GPU），匹配、优化和重建工作都是在台式机上运行，重建的结果再通过无线网络传到iPad上显示。下图是使用两块GPU（GTX Titan X + GTX Titan Black）的运行时间分配。\n下图是使用一块GPU的运行时间分配。\n4、问题及改进方向\n由于成像传感器存在噪音，稀疏关键点匹配可能产生小的局部误匹配。这些误匹配可能会在全局优化中传播，导致误差累积。\n上述效果图片都是在作者提供的公开数据集上的效果，该数据集采集的场景纹理比较丰富，光照也比较适中。而实际重建时效果和所使用深度相机的性能、待重建场景的纹理丰富程度关系很大。对于办公室这种简洁风格的场景效果会下降很多，还有很多可改进的地方。\n目前算法需要两块GPU才能实时运行，算法的优化和加速非常有意义。\n有什么参考资料？\n项目网址：\nhttp://graphics.stanford.edu/projects/bundlefusion/\ncode：\nhttps://github.com/niessner/BundleFusion\n放一个彩蛋。最近在中国科技大学举办的GAMES（计算机图形学与混合现实研讨会）2018大会上，MSRA的童欣研究员系统梳理了室内三维重建和理解方面的研究现状，感兴趣的可以点击这个视频学习。\n彩蛋2：温馨提示：本文中提到的论文我都给你准备好了，在“计算机视觉life”公众号菜单栏回复“室内重建”即可获取。\n彩蛋3：另外，计算机视觉life公众号关联的知识星球上线了，完全免费，微信一键登录！星球内所有成员都可以发布问题、解答问题，支持上传文件、点赞、留言、赞赏、收藏、匿名等功能，互动交流更方便！所有内容都可以沉淀，能分类可检索，详情见“计算机视觉life”公众号菜单栏《知识星球》，长按下图识别二维码加入吧！"}
{"content2":"找到了一个很好的博客，作者很详尽的总结了一系列有深刻影响的计算机视觉方面的论文，希望有更多的人能够看过这些经典的论文。在此转载改博客，在此向水木上表示深深的敬意，只有有更多像这样善于总结和分享的人才能给我们更好的网络学习环境。\n----------------------------------------------------------------------分割线----------------------------------------------------------------------------------------------------------\n前言：最近由于工作的关系，接触到了很多篇以前都没有听说过的经典文章，在感叹这些文章伟大的同时，也顿感自己视野的狭小。  想在网上找找计算机视觉界的经典文章汇总，一直没有找到。失望之余，我决定自己总结一篇，希望对 CV 领域的童鞋们有所帮助。由于自己的视野比较狭窄，肯定也有很多疏漏，权当抛砖引玉了，如果你觉得哪篇文章是非常经典的，也可以把相关信息连带你的昵称发给我，我好补上。我的信箱 xdyang.ustc@gmail.com\n文章主要来源：PAMI, IJCV, TIP, CVIU, PR, IVC, CVGIU, CVPR, ICCV, ECCV, NIPS, SIGGRAPH, BMVC等\n主要参考网站: Google scholar, citeseer, cvpapers, opencv 中英文官方网站\n主要参考书籍：\n数字图像处理  第三版  冈萨雷斯等\n图像处理，分析和机器视觉  第三版  Sonka等（非常非常好的一本书）\n学习OpenCV\n计算机视觉：算法与应用\n文章按时间排序，排名不分先后，^_^。每一行最后一栏是我自己加的注释，如果不喜欢可以无视之，如果有不对的地方还请告诉我，免得继续出丑。 给出的文章有些是从google scholar或者citeseer上拷贝下来的，所以有链接。所有的文章在网上都很容易找到。有空的时候我会把它们全部整理出来，逐步上传到ishare.iask.sina.com\n由于整理的很仓促，时间也很短，还有很多不完善的地方。我会不断改进，并不时上传新版本。\n上传地址为http://iask.sina.com.cn/u/2252291285/ish?folderid=775855\n最后更新：2012/3/14\n1990 年之前\nPeter Burt, Edward Adelson\nThe  Laplacian  Pyramid  as  A Compact Image Code\n虽说这个Laplacian Pyramid是有冗余的，但使用起来非常简单方便，对理解小波变换也非常有帮助。这位Adelson是W.T.Freeman的老板，都是大牛.\nJ Canny\nA Computational Approach to Edge Detection\n经典不需要解释。在 Sonka的书里面对这个算法也有比较详细的描述。\nS Mallat.\nA  theory  for  multiresolution  signal decomposition:  The  wavelet representation\nMallat的代表作\nM Kass, A Witkin, D  Terzopoulos.\nSnakes: active contour models\nDeformable model的开山鼻祖。\nRM HARALICK\nTextural Features for Image Classiﬁcation\n这三篇都是关于纹理特征的，虽然过去这么多年了，现在在检索和识别中依然很有用。\nRM HARALICK\nStatistical and structural approaches\nTamura等\nTexture features corresponding to visual perception\nA P Dempster, N M Laird, D B Rubin. 1977\nMaximum  likelihood  from  incomplete data via the EM algorithm\nEM 算法在计算机视觉中有着非常重要的作用\nL Rabiner. 1989\nA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\nHMM 同样是计算机视觉必须掌握的一项工具\nB D Lucas, T Kanade\nAn  iterative  image  registration technique  with  an  application  to stereo- vision\nLucas 光流法\nJ R Quinlan\nInduction of decision trees\n偏模式识别和机器学习一点\n1990 年\nP Perona, J Malik. PAMI\nScale-space and edge detection using anisotropic diffusion\n关于 scale space 最早的一篇论文之一，引用率很高\nT Lindeberg\nScale-space for discrete signals.\nLindeberg 关于 scale space 比较早的一篇，后续还有好几篇\nanzad, A.; Hong, Y.H.\nInvariant image recognition by Zernike moments\nZernike moment,做过模式识别或者检索的应该都知道这个东东\n1991 年\nW Freeman, E Adelson.\nThe design and use of steerable filters\nFreeman最早的一篇力作，也是我读的第一篇学术论文。现在Freeman在 MIT 风生水起，早已是IEEE Fellow了\nMichael J. Swain , Dana H. Ballard\nColor Indexing.\ngoogle scholar 上引用将近五千次\nMA Turk CVPR\nFace recognition using eigenfaces\n1992 年\nL G Brown.\nA survey of image registration techniques.\n比较早的一篇关于配准的综述了\n1993 年\nS G Mallat, Z Zhang.\nMatching pursuits with time-frequency dictionaries\nMallat另一篇关于小波的文章，不研究小波的可以无视之\nL Vincent.\nMorphological grayscale reconstruction in image analysis: Applications and efficient algorithms\nDP Huttenlocher\nComparing images using the Hausdorff distance\nGoogle scolar 上引用2200多次\n1994 年\nJ Shi, C Tomasi.\nGood feature to track.\nTomasi这个名字还会出现好几次，真的很牛\nLinderberg\nScale-space theory in computer vision\nJ L Barron, D J Fleet, S S  Beauchemin.\nPerformance  of  optical  flow techniques.\n1995 年\nR Malladi, J Sethian, B Vemuri.\nShape Modeling with Front Propagation: A Level Set Approach\nLevel set的经典文章\nTF COOTES\nActive Shape Models-Their Training and Application\nASM\nMA Stricker\nSimilarity of color images\n颜色检索相关\nC Cortes, V Vapnik.\nSupport-vector networks.\nSVM 在计算机视觉中也有着非常重要的地位\n1996 年\nT MCINERNEY.\nDeformable models in medical image analysis: A survey\n活动模型的一篇较早的综述\nTai Sing Lee\nImage Representation Using 2D Gabor Wavelets\nGoogle引用也有近千次\nAmir Said,  A. Pearlman\nA New, Fast, and Efﬁcient  Image Codec Based on Set Partitioning in Hierarchical Tree\nSPIHT。图像压缩领域与 EBCOT齐名的经典算法。\nL P Kaelbling, M L Littman, A W Moore.\nReinforcement learning: A survey\n机器学习里面的一篇综述，引用率比较高，就列在这了。\nB. S. Manjunath and W. Y. Ma\nTexture features for browsing and retrieval of image data\n检索的文章比较多，其实它们的应用不仅仅是检索。只要是需要提取特征的地方，检索里面的方法都可以用到\ncomparing images using color coherence vectors\n检索中的CCV方法\nImage retrieval using color and shape\n关于形状特征后面有一篇综述\n1997 年\nV Caselles, R Kimmel, G Sapiro.\nGeodesic active contours\n活动轮廓模型的一个小分支\nR E Schapire, Y Freund, P Bartlett, W S Lee.\nBoosting the Margin: A New Explanation for the Effectiveness of Voting Methods.\nSchapire 和 Freund 发 明 了Adaboost，给计算机视觉带来了不少经典算法\nF Maes, D Vandermeulen, G Marchal, P Suetens.\nMultimodality  image registration by maximization of mutual information\n互信息量配准\nE Osuna, R Freund, F Girosi.\nTraining support vector machines: An application to face detection.\nSVM在人脸检测中的应用。不过人脸检测最经典的方法应 该是Viola-Jones\nJ Huang, S Kumar, M Mitra, W-J Zhu, R Zabih.\nImage indexing using color correlogram\nColor correlogram，检索中的又一个颜色特征。和前面的 CCV 以及颜色矩特征基本上覆盖了所有的颜色特征。\nY Freund, R Schapire.\nA  decisiontheoretic  generalization  of on-line learning and an application to boosting.\nAdaboost的经典文章\n1998 年\n1998 年是图像处理和计算机视觉经典文章井喷的一年。大概从这一年开始，开始有了新的趋势。由于竞争的加剧，一些好的算法都先发在会议上了，先占个坑，等过一两年之后再扩展到会议上。\nT Lindeberg\nFeature detection with automatic scale selection\nLinderberg的 scale space到此为止基本结束了。在一些边缘提取，道路或者血管检测中，scale space 确实是一种很不错的工具\nC J C Burges.\nA tutorial on support vector machines for pattern recognition.\n使用 svm的话，这篇文章应该是必读的了。比 95 年那篇原始文章引用率还高\nM Isard, A Blake.\nCONDENSATION  –  Conditional TrackingDensity Propagation for Visual\nTracking中的经典文章了\nL Page, S Brin, R Motwani, T  Winograd\nThe PageRank citation ranking: bringing order to the web\n这篇文章应该不属于 CV 的范畴，鉴于作者的大名鼎鼎，暂且列在这\nC Tomasi, R Manduchi.\nBilateral filtering for gray and color images.\n做过图像滤波平滑去噪或者 HDR的应该都知道Bilateral filter。原理非常非常简单，简单到一个公式就可以概括这篇文章，简单到实在无法扩充到期刊。这也是 Tomasi 第二次出现了。一直很纳闷，这个很直观的思想在这之前怎么就从来没人提呢。\nC  Xu, J L Prince.\nSnakes, shapes and gradient vector flow.\n终于碰到中国人写的文章了，很荣幸还是校友。GVF是 snake和levelset领域的重要分支和方法\nWim Sweldens.\nThe lifting scheme: A construction of second generation wavelets.\n第二代小波。真正让小波有了实用价值，在 JPEG2000 中就采用的提升小波。个人更喜欢的是下一篇，简单易懂，字体也大\nDaubechies Wim Sweldens\nFactoring wavelet transforms into lifting steps\n另一位作者也很牛，小波十讲的作者\nH A Rowley, S Baluja, T Kanade.\nNeural Network-based Face Detection.\n做人脸的应该是必看的了。不做人脸的话应该可以不用看吧\nJ B A Maintz, M A Viergever.\nA survey of medical image registration.\n关于图像配准的另一篇综述\nT F Cootes, G J Edwards, C J Taylor.\nActive Appearance Models\nAAM\n1999 年\nD Lowe.\nObject Recognition from Local Scale-invariant Features\n大名鼎鼎的SIFT，后面有一篇IJCV上的 Journal版本，更全面一点。\nR E Schapire.\nA brief Introduction to Boosting\n还是 boosting\nD M Gavrila.\nThe visual analysis of human movements: a survey\n综述文章的引用一般都比较高\nY Rui, T S Huang, S F Change.\nImage retrieval: current techniques, promising directions, and open issues\nTSHuang小组对检索的一个总结\nJ K Aggarwal, Q Cai.\nHuman motion analysis: a review\n人体运动分析的一个综述\n2000 年\n世纪之交，各种综述都出来了\nJ Shi, J Malik.\nNormalized Cuts and Image Segmentation\nNCuts的引用率相当高，Jianbo Shi也因为这篇文章成为计算机视觉界引用率最高的作者之一\nZ Zhang.\nA Flexible New Technique for Camera Calibration\n张正友的关于摄像机标定的经典短文\nA K Jain, R P W Duin, J C Mao.\nStatistical pattern  recognition: a review.\n统计模式识别综述，这一年 pami上两篇很有名的综述之一。 在这里推荐 Web 写的 Statistical Pattern Recognition第三版，相当不错，网上有电子版。\nC Stauffe\nLearning Patterns of Activity Using Real-Time Tracking\n搜 TLD 的时候发现这篇文章引用率也很高，两千多次。还没来得及读。\nD Taubman.\nHigh performance Scalable Image Compression With EBCOT\nEBCOT，JPEG2000 中的算法\nA W M Smeulders, M Worring, S Santini, A Gupta, R Jain.\nContent-based image retrieval at the end of the early years\n在世纪之交对图像检索的一篇很权威的综述。感觉在这之后检索的研究也没那么热了。不过在工业界热度依旧，各大网上购物平台，比如淘宝，  亚马逊，京东等都在做这方面的研发，衣服检索是一个很不错的应用点。\nM Pantic, L J M Rothkrantz.\nAutomatic analysis of facial expressions: the state of the art.\nN Paragios, R Deriche.\nGeodesic active contours and  level sets for the detection and tracking of moving objects\n使用 level set做跟踪\nY Rubner, C Tomasi, L Guibas.\nTThe earth mover’s distance as a metric for image retrieval.\nEMD算法。Tomasi再次出现\nPicToSeek Combining Color and Shape Invariant Features for Image Retrieval\n依然是检索特征\n2001 年\nPaul Viola, Michael J Jones.\nRobust real-time object detection\n这是一篇很牛的文章，在人脸检测上几乎成了标准。比较坑爹的是，号称发在IJCV2001 上，但怎么找也找不到。应该是 IJCV2004年的那篇“Robust real-time face detection”吧。 他们在这一年另一篇比较出名的文章是在CVPR上的“Rapid ObjectDetection using a Boosted Cascade of Simple Features”这篇才是04年那篇著名文章的会议版。\nY Boykov, Kolmogorov.\nAn experimental comparison of min-cut/max-flow algorithms for energy minimization in vision.\n俄罗斯人在 graph cut 领域开始发力了\nT Moeslund, E Granum.\nA Survey of Computer Vision Based Human Motion Capture\n人体运动综述\nT F Chan, L Vese.\nActive contours without edges.\nSnake 和 level set领域的经典文章\nA M Martinez, A C Kak.\nPCA versus LDA\nPCA 也是计算机视觉中非掌握不可的工具。LDA在模式识别中有很重要的地位\nBS Manjunath\nColor and texture descriptors\n颜色和纹理的描述子，在识别中很有用\n2002 年\nD Comaniciu, P Meer.\nMean  shift:  A  robust  approach toward feature space analysis.\nMean shift的经典文章。前两天发现 Comaniciu 已经是 IEEE Fellow了\nMing-Husan Yang, David J Kriegman, Narendra Ahuja.\nDetecting  Faces  in  Images:  A Survey.\n人脸检测综述，引用率想不高都难\nR Hsu, M Abdel-Mottaleb.\nFace Detection in Color Images.\n依然是人脸检测，名字都起得这么霸气\nJ-L Starck, E J Candès, D L Donoho.\nThe  curvelet  transform  for  image denoising.\nGeometrical wavelet 中的一篇代表 作 。 其 他 的 如 ridgelet, contourlet, bandelet 等在这里就不赘述了。研究这方面的很容易找到这方面的经典文献。个人以为不研究这方面的看了后对自己的研究也不会有多大启发。曾经以为这个方向会很火，到最后还是没火起来。  我觉得原因可能是现在存储和传输能力的大大提高，使得对压缩的需求没有那么大了，这方面的研究自然就停滞了，就如同JPEG2000没有成气候\nShape matching and object recognition using shape contexts\nShape context。用形状匹配达到目标识别目的。这方面最经典的文章了。随后后续也有一些这方面的文章，但基本都是很小的改进或者应用。作者提供了原码，可以在 matlab上运行看看效果。\nN Paragios, R Deriche.\nGeodesic  active  regions  and  level set methods  for  supervised  texture segmentation\nStatistical Color Models with Application to Skin Detection\nA tutorial on particle filters for online nonlinear non-Gaussian Bayesian tracking\nparticle filter 的一个综述\n2003 年\nW Zhao, R Chellappa, P J Phillips, A Rosenfeld.\nFace recognition: A literature survey.\n人脸检测的综述\nJ Sivic, A Zisserman.\nVideo  Google:  A  text  retrieval approach  to  object  matching  in videos.\n好像是Visual words的起源文章。引用率很高，先列出来再看。\nD Comaniciu, V Ramesch, P Meer.\nKernel-Based Object Tracking.\n基于核的跟踪。\nB Zitová, J Flusser.\nImage  registration  methods:  A survey.\n又一篇图像配准的综述。做图像配准的比较有福气，综述很多\nK Mikolajczyk, C Schmid.\nA  performance  evaluation  of  local descriptors.\n比较各种描述子的，包括SIFT\nM J Wainwright, M I Jordan.\nGraphical  models,  exponential families, and variational inference.\n乔丹的名气太大，不露露脸说不过去\nJ Portilla, V Strela, M Wainwright, E Simoncelli.\nImage  denoising  using  scale mixtures of gaussians  in  the wavelet domain.\n图像去噪，小波变换，混合高斯\nRobert E. Schapire\nThe Boosting Approach to Machine Learning  An Overview\nboosting作者自己写的综述，自然值得一看。\n2004 年\nLucas-Kanade 20 Years On A Unifying Framework\n引用文章摘要的第一句话Since the Lucas-Kanade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. Applications range from optical flow and tracking to layered motion,  mosaic construction, and face coding.\nD G Lowe.\nDistinctive  image  features  from scale-invariant keypoints.\nSIFT，不解释\nChih-ChungChang,Chih-Jen Lin.\nLIBSVM: A  library  for support vectormachines\n我实在怀疑引用这篇文章的人是否都看过这篇文章。貌似不看这篇文章也可以使用 LIBSVM\nZ Wang, A C Bovik, H R Sheikh, E P Simoncelli.\nImage  quality  assessment:  From error visibility to structural similarity\n图像质量评价，最近 Bovik 还有一篇类似的文章也刊登在 TIP上\nY Ke, R Sukthankar.\nPca-sift:  a  more  distinctive representation  for  local  image descriptors\nSIFT 的变形\nReview of shape representation and description techniques\nEfficient Graph-Based Image Segmentation\n2005 年\nN Dalal, B Triggs.\nHistograms  of  oriented  gradients  for human detection.\nHOG  虽然很新，但很经典\nA C Berg, T L Berg, J Malik.\nShape  matching  and  object recognition  using  low  distortion correspondences.\n还是 shape matching\nS Roth, M Black.\nFields  of  experts:  A  framework  for learning image priors.\n这篇应该要归结到图像统计特性的范畴吧\nZ Tu, X Chen,A L Yuille, S C Zhu.\nImage  parsing:  Unifying segmentation,  detection,  and recognition.\nGeodesic active regions and level set methods for motion  estimation and tracking\nChunming Li, Chenyang Xu, Changfeng Gui, and  Martin D. Fox\nLevel Set Evolution Without Re-initialization: A New Variational Formulation\n这篇文章解决了level set中需要不停的重初始化的问题。在 2010 年的 TIP上有一篇 Journal版本 Distance Regularized Level Set Evolution and its Application to Image Segmentation\nA Performance Evaluation of Local Descriptors\n前面那篇是会议的，这篇是 PAMI上的。比较各种描述子的，包括SIFT\n2006 年\nD Donoho.\nCompressed sensing.\nCS  压缩感知  最近很火的一个名词\nGreg Welch, Gary Bishop.\nAn introduction to the Kalman Filter.\nkalman滤波\nS Lazebnik, C Schmid, J Ponce.\nBeyond  bags  of  features:  spatial pyramid  matching  for  recognizing natural scene categories.\nVisual words\nXiaojin Zhu.\nSemi-supervised  learning  literature survey.\nA Yilmaz, O Javed, M Shah.\nObject Tracking: A survey.\ntracking的一篇综述\nImage Alignment and Stitching: A Tutorial\n2007 年\nA Review of Statistical Approaches to Level Set Segmentation: Integrating Color, Texture, Motion and Shape\nThe Appearance of Human Skin: A Survey\nLocal Invariant Feature Detectors: A Survey\n2008 年\nH Bay, A Ess, T Tuytelaars, L V Gool.\nSURF:  Speeded  Up  Robust Features.\nK E A van de Sande, T Gevers, C G M Snoek.\nEvaluation  of  Color  Descriptors  for Object and Scene Recognition\nM Yang\nA Survey of Shape Feature Extraction Techniques\n虽然这篇文章的引用率目前来看并不高,但个人认为这是一篇在shape feature方面很不错的文章\nP.Felzenszwalb, D. McAllester, D. Ramanan\nA Discriminatively Trained, Multiscale, Deformable Part Model\n2008 年的 CVPR，到现在引用已有四百多次，潜力巨大。rosepink提供\n2009 年\nJ Wright, A Y Yang, A Ganesh, S S Sastry, Ma.\nRobust Face Recognition via Sparse Representation.\nB Settles.\nActive learning literature survey\n2010 年\n2011 年\nHough Forests for Object Detection, Tracking, and Action Recognition\nRobust Principal Component Analysis?\nCandes  和 UIUC 的Ma Yi等人\n2012 年\nZdenek Kalal, Krystian Mikolajczyk,and Jiri Matas,\nTracking-Learning-Detection\nPAMI上的，虽然还没有正式发表，但肯定会火。在作者的主页上有几篇相关的会议文章， demo和code。用到了 Lucas-Kanade方法"}
{"content2":"本排名是根据CCF公布的最新排名情况。"}
{"content2":"在当下的人工智能浪潮之中，中国和美国作为牌桌上两位手握最多筹码的玩家，彼此之间有许多相似，也有许多不同。尤其在人工智能技术从学术到产业再到商业的这一步上，美国的高校起到了更强大的背书作用。\n李飞飞、LeCun这些名字的影响力离不开他们的学术背景，科学家们在实验室项目和企业上的建树也会加强高校自身的影响力。可不管从世界范围，还是中国本土来看，即使中国的计算机视觉、自然语言处理技术已经在一些竞赛上名列前茅，中国高校却没能“沾上光”，提到人工智能的主导者，人们直觉中还想到的是百度这类企业，或是李开复这类资本界明星，而不是哪一所高校的实验室。虽然现在很多学校都在抓紧开设人工智能、机器学习相关专业，可从现状来看，比起学术驱动，中国人工智能更接近于资本驱动和大企业驱动。\n斯坦福为什么会成为斯坦福？\n那么为什么一提到人工智能，我们就会想到斯坦福、MIT、伯克利这些美国高校呢？\n第一点是办学制度上的根本差异。斯坦福这类老牌私立高校学费高昂，校中的学生不光脑子好，而且还有钱，其中很多人还是已经占据社会优质资源的Old Money。综合其他部分原因，在斯坦福这类私立高校中创业氛围和学术氛围都可以更加浓厚和纯粹，所以才会有“斯坦福一所学校撑起整个硅谷”的说法。这些学校中也出来了很多明星企业家和投资人，像来自斯坦福的Peter Thiel等。\n也就是说在人工智能浪潮出现之前，美国就有很多高校和企业、VC联系十分紧密了，当人工智能受到关注后，更方便了他们把来自高校的研究成果贩卖给企业。\n第二点则是，我们不得不承认在整体办学水平和学术能力上，美国高校还是高出中国不少的。这里的能力不仅仅指的是论文和实验室研究成果，也包括一些社会各界都可以应用的开源项目。比如李飞飞的ImageNet和最近斯坦福公布的医疗图像大数据Medical ImageNet。这些开源项目可以清晰的告诉企业和社会，高校在做什么和能做到什么。\n强大的学术能力以及和社会、企业间的紧密联系，让这些高校中的实验室、教授和研究成果更受关注，最后这些高校的名字甚至变成了一种认证标志，印上了这个标志，人们就会相信企业会有足够和持续的技术能力供给。\n科技博客和“荣誉园地”\n除了以上几点之外，中国人工智能产业还存在一个最大的问题，就是高校方面缺乏对技术成果的基本宣传意识。\n以下是伯克利人工智能研究的官方博客：\n可以看到博客中的大部分内容是对伯克利人工智能方面研究最新成果的介绍，读者可以通过简单的语言了解到伯克利的种种新发现和新尝试，如果感兴趣的话，还能通过链接跳转看完整篇论文。\n而以下则是中科大计算机技术学院主页上有关科研信息的部分：\n先不说整个内容更新频率和页面设计的美观程度，我们可以清晰的看到中科大在科研宣传方面还停留在“XX专家来访”、“我校师生获得XX荣誉”等等方面。不知道是因为校方认为奖项是比学术成果更值得宣传的东西，还是认为没人看得懂学术介绍。但结果就是，象牙塔越盖越严，人们可以模糊的理解到高校在学术上已经达到了很高的水平，但不知道他们的研究成果已经可以解决什么问题。所谓的产学结合也只能靠高薪聘请相关专业的教授博士，到企业内部再进行。\n即使不在国际上比较，从学科上来看人工智能的技术宣传手法也相当落后。《我在故宫修文物》、《国家宝藏》等等节目都在用新的传播形式向普罗大众介绍考古、历史方面的知识，人工智能却只能靠企业推出的机器人在问答节目中刷存在感。\n总之中国高校几乎不会去主动向外介绍自己的学术研究成果，而当外界主动向他们寻找学术研究成果时，也会困难重重。\n走穴的教授或许才是中国AI的守护者\n想解决这种现状无非依靠几种办法：第一是加强校企合作，通过产业方面的深入合作让企业率先了解高校的学术成果，再通过企业的宣传口径传达出去；第二是校方自己开辟学术展示窗口，不管是像伯克利那样简简单单的推出一个记录性质的博客，还是像《国家宝藏》那样贴近大众的节目，都会是不错的选择；第三是由政府介入，帮助高校走进企业和孵化器，进行研究成果变现。在法国、以色列等国家都有类似政策，政府或者为高校的研究成果买单，或者帮助教授、博士走入创业企业。\n不过由于种种不可抗力影响，目前看来只有通过校企合作来提高高校学术成果知名度是最靠谱的。在众多高校中，中科大算是走到了学术驱动的前列，走出了科大讯飞和寒武纪两家堪称现象级的AI企业。同时清华、浙大等高校因为地缘优势，未来也更有可能和企业进行密切的合作。可即使是这样，中国高校在人工智能行业中的存在感还是远远不够。\n或许文人风骨已经深深烙印在了中国人的心中，我们总把学术二字和“清贫”、“俭朴”等等关键词联系在一起，仿佛教授们涉足了商业，就会玷污了学术的纯洁性。要是有哪个教授上了几个电视节目，或多几个地方开开讲座，就会被酸溜溜的冠上“走穴”二字。这何尝不是一种偏见呢？\n我倒是希望，中国能多几个四处走穴的教授，真真切切的告诉大家中国AI不仅仅意味着高薪和大笔融资，也真实的发生在实验室中，并且在一点点的进步着，足以支撑整个行业在未来五年、十年甚至三十年发展。中国AI已经有了太多鼓风人，我们需要的或许是从象牙塔中走出来的守护者。"}
{"content2":"首先，本文从计算机视觉领域介绍attention机制，主要是在图像上的应用，会从总体motivation，具体的文章，以及相应的代码实现进行描述。\n本文背景部分，大量参考了 https://mp.weixin.qq.com/s/KKlmYOduXWqR74W03Kl-9A, 特此感谢。\n一、背景\n1.1 直观理解\n引用一下示例，从感知上理解一下计算机视觉中的注意力机制。 比如天空一只鸟飞过去的时候，往往你的注意力会追随着鸟儿，天空在你的视觉系统中，自然成为了一个背景（background）信息。\n而当我们在用深度神经网络处理计算视觉问题时，步骤首先是对图像中的特征进行提取，这些特征在神经网络“眼里”没有差异，神经网络并不会过多关注某个“区域”，但我们人在处理时，人类注意力是会集中在这张图片的一个区域内，而其他的信息受关注度会相应降低。\n计算机视觉（computer vision）中的注意力机制（attention）的基本思想就是想让系统学会注意力——能够忽略无关信息而关注重点信息。\n1.2 理论研究基础\n注意力机制最早的应用在机器翻译上。之所以它这么受欢迎，是因为Attention给模型赋予了区分辨别的能力，例如，在机器翻译、语音识别应用中，为句子中的每个词赋予不同的权重，使神经网络模型的学习变得更加灵活（soft），同时Attention本身可以做为一种对齐关系，解释翻译输入/输出句子之间的对齐关系，解释模型到底学到了什么知识，为我们打开深度学习的黑箱，提供了一个窗口。\n想从根本上理解它的提出，可以看张俊林的专栏 https://zhuanlan.zhihu.com/p/37601161，写的非常具体，另外，引用一下https://www.jianshu.com/p/c94909b835d6 的一个直观例子。\nA是encoder， B是decoder，A网络接收了一个四个字的句子，对每个字都产生了一个输出向量，v1，v2，v3，v4。B网络，在第一个B产生的hidden state（称其为h1）除了传给下一个cell外，还传到了A网络，这里就是Attention发挥作用的地方。首先，h1 分别与v1，v2，v3，v4做点积，产生了四个数，称其为m1，m2，m3，m4，由于是点积，这四个数均为标量。然后传到softmax层，产生a1，a2，a3， a4概率分布。然后将a1，a2，a3， a4 与 v1，v2，v3，v4分别相乘，再相加，得到得到一个vector，称其为Attention vector。Attention vector 将作为输入传到B网络的第二个cell中，参与预测。\n以上就是Attention机制的基本思想了。我们看到，Attention vector 实际上融合了v1，v2，v3，v4的信息，具体的融合是用一个概率分布来达到的，而这个概率分布又是通过B网络上一个cell的hidden state与v1，v2，v3，v4进行点乘得到的。\nAttention vector实际上达到了让B网络聚焦于A网络输出的某一部分的作用。这个概率分布，实际上就是一个权重，对输入进行一定的筛选。\n同样的，在计算机视觉领域，也是类似。注意力机制被引入来进行视觉信息处理。注意力是一种机制，或者方法论，并没有严格的数学定义。比如，传统的局部图像特征提取、显著性检测、滑动窗口方法等都可以看作一种注意力机制。在神经网络中，注意力模块通常是一个额外的神经网络，能够硬性选择输入的某些部分，或者给输入的不同部分分配不同的权重。\n从分类来看，可分为hard-attention和soft-attention。\n1. Hard-attention，就是0/1问题，哪些区域是被attentioned，哪些区域不关注\n2. Soft-attention，[0,1]间连续分布问题，每个区域被关注的程度高低，用0~1的score表示\n由于两者特性不同，在解决方法上也不同。hard-attention 主要利用强化学习，而 soft-attention则利用梯度下降即可求解。因而计算机视觉领域，soft-attention 使用的相对较多。本文中剩下的文章也都基于soft-attention。\n二、主要文章及其代码实现\n在主流的注意力机制应用中，有从channel 特征尺度上加上注意力，有从spatial 空间加上注意力，更多的是将两者进行结合。本人是研究医学图像语义分割，因而将视觉注意力和经典的Unet进行结合，在2D图像上，探究其有效性。以下几篇文章都很经典，基本都在顶会上，这里将其attention 部分进行提炼解析。文章更为详细的内容晚些补上。\n[1] Squeeze-and-Excitation Network: CVPR 2017\n作者主要在channel上增加了attention，作者称其为“Squeeze-and-Excitation”网络块。作者的定位是通过精确的建模卷积特征各个通道之间的作用关系来改善网络模型的表达能力。为了达到这个期望，作者提出了一种能够让网络模型对特征进行校准的机制，使网络从全局信息出发来选择性的放大有价值的特征通道并且抑制无用的特征通道。个人观点，这篇文章利用了注意力机制，对无效信息进行抑制，强调在信息筛选上，对传统的卷积操作进行了改进。这样设计带来的有效性还是非常玄妙的，但是结果不错，能在很多应用上有所提高。\n下面，具体介绍一下：\n1. 对于任意给定的信息进入网络模块后进行操作为：，，，因此 X--> U 可表示一次或多次卷积操作）。\n2. 对于第一个Squeeze操作，其实就是做了一个global average pooling (GAP)，将各个特征图全局感受野的空间信息置入特征图。称为descriptor，后面的操作可根据这个 descriptor 获取全局的感受野，避免了由于卷积核尺寸造成的信息不足。为了加强理解GAP, 给出tensorflow的代码，实际上就是将width and height 信息全部进行平均，从而得到一个全局的信息：\n\"\"\" input: net, shape=[b,h,w,c],b is batch_size, h is height, w is width and c is channel \"\"\" x = tf.reduce_mean(x, [1, 2], name='average_pooling', keep_dims=True)\n3. 随后再接着进行excitation操作，在excitation操作中，每层卷积操作之后都接着一个样例特化（sample-specific）激活函数，基于通道之间的依赖关系对每各个通道过一种筛选机制（self-gating mechanism）操作，以此来对各个通道进行权值评比（excitation） 。这里采用了bottleneck，具有更多非线性，并减少参数来和计算量，可以比单层的性能更优秀。\n4. 最后将得到的权值和卷积后的 U 相乘，得到输出，作为下一层的输入。\n优点很明显，整个个SE网络模型通过不断堆叠SE网络模块进行构造，SE网络模块能够在一个网络模型中的任意深度位置进行插入替换 。SE网络模块会不断的特化，并且以一个高度特化类别的方式对所在不同深度的SE网络模块的输入进行相应，因此在整个网络模型中，特征组图的调整的优点能够通过SE网络模块不断地累计。\n下面是 keras 写的se模块代码。\ndef activation(x,func='relu'): return Activation(func)(x) def squeeze_excitation_layer(x, out_dim, ratio=4): squeeze = GlobalAveragePooling2D()(x) excitation = Dense(units=out_dim // ratio)(squeeze) excitation = activation(excitation) excitation = Dense(units=out_dim)(excitation) excitation = activation(excitation, 'sigmoid') excitation = Reshape((1, 1, out_dim))(excitation) scale = multiply([x, excitation]) return scale\n完整实验请看github/下一篇。\n[2] CBAM: Convolutional Block Attention Module ECCV 2018\n这个ECCV2018的一篇文章。针对SE的一个补充和改进。\ndef cbam(x,ratio = 4): x = channel_attention(x,ratio) x = spatial_attention(x) return x\n这是整体的框架图，motivation 很直观，和SE相比，channel attention部分基本类似，主要增加了spatial attention, 作者认为这样可以更好的将信息进行整合。主要亮点将attention同时运用在channel 和 spatial两个维度上，CBAM与SE Module一样，可以嵌入了目前大部分主流网络中，在不显著增加计算量和参数量的前提下能提升网络模型的特征提取能力。主要attention的作用，仍旧是对信息进行一个筛选与整合。\n首先，先看一下channel attention部分。在SE中，仅仅使用了一个AvgPool，这里进行了改进，同时用了MaxPool 和 AvgPool, 两者通过同一个MLP（多层感知机）共享参数。这样做的原因是通过两个pooling函数以后总共可以得到两个一维矢量。global average pooling对feature map上的每一个像素点都有反馈，而global max pooling在进行梯度反向传播计算只有feature map中响应最大的地方有梯度的反馈，能作为一个补充。\n其实客观来看，除了增加MaxPool 作为补充，它和SE基本是一样的，MLP在代码中也可以看到，取了bottelneck形状。下面一起写一下keras代码。\ndef channel_attention(x,ratio): channel = x._keras_shape[-1] shared_layer_one = Dense(channel//ratio,activation='relu',kernel_initializer='he_normal',use_bias=True, bias_initializer='zeros') shared_layer_two = Dense(channel,kernel_initializer='he_normal',use_bias=True,bias_initializer='zeros') avg_pool = GlobalAveragePooling2D()(x) avg_pool = shared_layer_one(avg_pool) avg_pool = shared_layer_two(avg_pool) max_pool = GlobalMaxPooling2D()(x) max_pool = shared_layer_one(max_pool) max_pool = shared_layer_two(max_pool) a= Add()([avg_pool,max_pool]) a = activation(a,'sigmoid') return multiply([x,a])\n接着看一下第二个spatial attention 部分。\n其实操作上更为简单一些，使用average pooling和max pooling对输入feature map 在通道层面上进行压缩操作，对输入特征分别在通道维度上做了mean和max操作。最后得到了两个二维的 feature，将其按通道维度拼接在一起得到一个通道数为2的feature map，之后使用一个包含单个卷积核的隐藏层对其进行卷积操作，要保证最后得到的feature在spatial 维度上与输入的feature map一致。先上代码。\ndef spatial_attention(x): avg_pool = Lambda(lambda x:K.mean(x,axis=3,keepdims=True))(x) max_pool = Lambda(lambda x:K.max(x,axis=3,keepdims=True))(x) concat = Concatenate(axis=3)([avg_pool,max_pool]) o = Conv2D(filters=1,kernel_size=7,strides=1,padding='same',activation='sigmoid',kernel_initializer='he_normal', use_bias=False)(concat) return multiply([x,o])\n总体来说，文章中的一句原话说的很好，作者认为通道注意力关注的是：what，然而空间注意力关注的是：Where。这样两方面结合，能更好地指明信息丰富的区域。另外，文章中这种对称的结构设计以及效果，还是让人觉得很不错的。文章中其实还有一些对整体结构的具体分析值得学习。篇幅限制，就不展开了。 完整实验请看github/下一篇。\n[3] Dual Attention Network for Scene Segmentation:AAAI 2019\n这篇文章是AAAI 刚录取的文章。但是整体想法其实和self-attention GAN 以及 Non-local 的操作十分类似。它通过self attention 机制来捕获上下文依赖。这样一个结构可以自适应地整合局部特征和全局依赖。self-attention GAN 和Non-local 有兴趣的可以自己看看论文。这里主要介绍这一篇文章的主要做法和意义。\n主要有两个 attention 模块。其中，position attention module 选择性地通过所有位置的加权求和聚集每个特征的位置。 channel attention module 通过所以 channel 的 feature map中的特征选择地强调某个特征图。最后将两者模块的输出求和，两个分支并联，得到最后的特征表达。\n在这篇文章中加入attention的motivation有一些不同。\n它们是对空间和通道维度的语义相互关联进行建模。相对于前面的强调信息流，对有效信息/区域的提取，这里更加强调特征或者是语义之间的关联。\nposition注意力模块通过对所有位置的特征加权总和选择性地聚集每个位置的特征。无论距离远近，相似的特征都会相互关联。通道注意力模块通过整合所有通道图中的相关特征，有选择地强调相互关联的通道图。\n首先先看PAM，特征图A(C*H*W)分别通过三个卷积层得到3个特征图B,C,D，然后reshape为C*N，其中N=H*W,之后将reshape 之后的B的转置与reshape之后的C相乘，再通过softmax得到spatial attention map S(N*N)，接着把S的转置与D做乘积再乘以尺度系数α，再reshape回原来的形状，最后与A相加得到最后的输出E。其中α初始化为0，是一个可学习的变量，逐渐学习分配到更大的权重。其中我们可以看出E的每个位置的值是原始特征每个位置的加权求和得到。\n这一部分实际上和 self attention gan 一模一样。通过 (H*W)*(H*W)，建立起了每个位置的像素点之间的联系。\ndef hw_flatten(x): return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[3]]) def pam(x): f = K.conv2d(x, kernel= kernel_f, strides=(1, 1), padding='same') # [bs, h, w, c'] g = K.conv2d(x, kernel= kernel_g, strides=(1, 1), padding='same') # [bs, h, w, c'] h = K.conv2d(x, kernel= kernel_h, strides=(1, 1), padding='same') # [bs, h, w, c] s = K.batch_dot(hw_flatten(g), K.permute_dimensions(hw_flatten(f), (0, 2, 1))) #[bs, N, N] beta = K.softmax(s, axis=-1) # attention map o = K.batch_dot(beta, hw_flatten(h)) # [bs, N, C] o = K.reshape(o, shape=K.shape(x)) # [bs, h, w, C] x = gamma * o + x return x\n在CAM 模块中，分别对A做reshape和(reshape 与 transpose)，将得到的两个特征图相乘，再通过softmax得到 channel attention map X(C*C)，接着把X与A做乘积再乘以尺度系数 β，再reshape回原来的形状，最后与A相加得到最后的输出E。其中，β初始化为0，并逐渐的学习分配到更大的权重。这里的β和之前一样，是可学习的参数。\ndef hw_flatten(x): return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[3]]) def cam(x): f = hw_flatten(x) # [bs, h*w, c] g = hw_flatten(x) # [bs, h*w, c] h = hw_flatten(x) # [bs, h*w, c] s = K.batch_dot(K.permute_dimensions(hw_flatten(g), (0, 2, 1)), hw_flatten(f)) beta = K.softmax(s, axis=-1) # attention map o = K.batch_dot(hw_flatten(h),beta) # [bs, N, C] o = K.reshape(o, shape=K.shape(x)) # [bs, h, w, C] x = gamma * o + x return x\n通过看代码，有一点值得注意，作者加了这一部分，防止训练时梯度爆炸。我们的代码没加。\n\"\"\"官方pytorch版本\"\"\"\" energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n最后，将两个模块的输出执行 element-wise sum 进行特征融合。\n这一个整个模块在原文中放在ResNet之后，作为额外的模块更好地辅助恢复原文信息。作为实验，还没想好怎么和unet结合，后续有进一步的实验会补充上来，也希望有兴趣的可以和我交流！完整版请见github。\n[4] Attention U-Net: Learning Where to Look for the Pancreas: MIDL 2018\n最后介绍一下Attention Unet, 它是加了一个gated 模块，对信息流进行一个筛选。文章的意思是，它是用于医学成像的新型注意门（AG）模型，该模型自动学习聚焦于\n不同形状和大小的目标结构\n。用AG训练的模型隐含地学习抑制输入图像中的不相关区域，同时突出显示对特定任务有用的显著特征。在医学图像尤其是病灶中很直观，可以用 AG 避免级联的定位模块。\n具有AG的CNN模型可以通过类似于FCN模型的训练的方式训练，并且AG自动学习专注于目标结构而无需额外的监督。在测试时，这些门会动态地隐式生成软区域提议，并突出显示对特定任务有用的显着特征。\n为了减少附加的级联定位模块，作者增加了注意系数，，识别显着图像区域和修剪特征响应，以仅保留与特定任务相关的激活。这个是最终目的，接下来看看是如何实现的。\n输入包括两个方面，一是针对每个像素矢量的 ，对其计算单个标量注意值，其中对应于层 l 中的特征图的数量。另一个是门控矢量用于每个像素 i 以确定聚焦区域。门控向量包含上下文信息，以减少较低级别的特征响应。这里使用加性注意来获得门控系数。核心公式如下，第一个Φ 代表 relu，第二个 σ 代表了 sigmoid 函数。最后resample到原始尺寸。\n根据pytorch所改写keras代码如下。\ndef attention_block(x, gating, inter_shape): shape_x = K.int_shape(x) shape_g = K.int_shape(gating) theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x) # 16 shape_theta_x = K.int_shape(theta_x) phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating) upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same')(phi_g) # 16 concat_xg = layers.add([upsample_g, theta_x]) act_xg = layers.Activation('relu')(concat_xg) psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg) sigmoid_xg = layers.Activation('sigmoid')(psi) shape_sigmoid = K.int_shape(sigmoid_xg) upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg) # 32 upsample_psi = expend_as(upsample_psi, shape_x[3]) y = layers.multiply([upsample_psi, x]) result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y) result_bn = layers.BatchNormalization()(result) return result_bn\n整篇文章的立意还是很清楚的，实现起来不难，但是整个结构的出处，或者来源点还没有完全弄清，弄清后补充。\n最后的最后，总结一下，这几篇文章各有立意，各有改进，精读一下很有必要。\n代码还会不断完善改进，一旦完成会更新博客。如果有同样感兴趣的，欢迎与我联系。如有理解不到位，也欢迎指出。"}
{"content2":"这本书的第一版在2002年出来时轰动一时，是一本不可多得计算机视觉的教材。计算机视觉领域发展非常快，10年前的书在现在看来多少有些过时。当然偏理论的除外，比如《Multiple View Geometry in Computer Vision》。前一段时间我还在憧憬这本书什么时候可以出第二版。后来在浏览David Forsyth的个人主页时，兴奋得发现我的愿望实现了。更兴奋的是在iask上可以下到非常清晰的版本，这里就不贴出链接了，很容易找到，找不到的可以mail我。Copyright@2012，引用的参考文献最新的到了2011年，可以说是最新的计算机视觉方面的书籍了。在原有的基础上添加了很多计算机视觉的最新的发展，增加了几章全新的章节。至此，我最喜欢的三本书最近都出了最新版，包括冈萨雷斯的《数字图像处理》和Sonka等的《图像处理，分析与机器视觉》。已经果断入手了冈萨雷斯的英文版和Sonka的中文版（也只能找到中文版），都很不错。再配上Richard Szeliszi的《Computer Vision:Algorithms and Applications》，这四本书都很新，各有侧重，互相补充，基本涵盖了图像处理和计算机视觉的各个方面。希望Forsyth的这本书也能尽快出影印版。还是喜欢在一个安静的角落，没有电脑的干扰和诱惑，闻着书香，边看边思考边做笔记。真希望能有一个长假，没有任何压力，也没有任何杂事，带上几本好书，挑选一些好的论文，能够静下心来好好的研读，同时配合着OpenCV去实现领悟其中的一些经典算法。It is my dream。\n擦，刚说完就出版了，在卓越和当当上都可以找到，折后也要70多元。\nhttp://luthuli.cs.uiuc.edu/~daf/CV2E-site/cv2eindex.html\nFull Table of Contents\nSlides not ready yet, but coming soon\nChapter List:\nImage Formation\nGeometric Camera Models\nLight and Shading\nColor\nEarly Vision: Just One Image\nLinear Filters\nLocal Image Features\nTexture\nEarly Vision: Multiple Images\nStereopsis\nStructure from Motion\nMid-Level Vision\nSegmentation by Clustering\nGrouping and Model Fitting\nTracking\nHigh-Level Vision\nRegistration\nSmooth Surfaces and Their Outlines\nRange Data\nLearning to Classify\nClassifying Images\nDetecting Objects in Images\nTopics in Object Recognition\nApplications\nImage-Based Modeling and Rendering\nLooking at People\nImage Search and Retrieval\nOptimization Techniques\nBibliography\nIndex\nList of Algorithms"}
{"content2":"1. 数学\n我们所说的图像处理实际上就是数字图像处理，是把真实世界中的连续三维随机信号投影到传感器的二维平面上，采样并量化后得到二维矩阵。数字图像处理就是二维矩阵的处理，而从二维图像中恢复出三维场景就是计算机视觉的主要任务之一。这里面就涉及到了图像处理所涉及到的三个重要属性：连续性，二维矩阵，随机性。所对应的数学知识是高等数学（微积分），线性代数（矩阵论），概率论和随机过程。这三门课也是考研的三门课，构成了图像处理和计算机视觉最基础的数学基础。如果想要更进一步，就要到网上搜搜林达华推荐的数学数目了。\n2. 信号处理\n图像处理其实就是二维和三维信号处理，而处理的信号又有一定的随机性，因此经典信号处理和随机信号处理都是图像处理和计算机视觉中必备的理论基础。\n2.1经典信号处理\n信号与系统(第2版)  Alan V.Oppenheim等著 刘树棠译\n离散时间信号处理(第2版)  A.V.奥本海姆等著 刘树棠译\n数字信号处理:理论算法与实现胡广书 (编者)\n2.2随机信号处理\n现代信号处理 张贤达著\n统计信号处理基础:估计与检测理论Steven M.Kay等著 罗鹏飞等译\n自适应滤波器原理(第4版) Simon Haykin著 郑宝玉等译\n2.3 小波变换\n信号处理的小波导引:稀疏方法(原书第3版)  tephane Malla著, 戴道清等译\n2.4 信息论\n信息论基础(原书第2版) Thomas M.Cover等著 阮吉寿等译\n3. 模式识别\nPattern Recognition and Machine Learning Bishop, Christopher M. Springer\n模式识别(英文版)(第4版) 西奥多里德斯著\nPattern Classification (2nd Edition) Richard O. Duda等著\nStatistical Pattern Recognition, 3rd Edition Andrew R. Webb等著\n模式识别(第3版) 张学工著\n4. 图像处理与计算机视觉的书籍推荐\n图像处理，分析与机器视觉 第三版Sonka等著 艾海舟等译\nImage Processing, Analysis and Machine Vision\n这本书是图像处理与计算机视觉里面比较全的一本书了，几乎涵盖了图像视觉领域的各个方面。中文版的个人感觉也还可以，值得一看。\n数字图像处理 第三版 冈萨雷斯等著\nDigital Image Processing\n数字图像处理永远的经典，现在已经出到了第三版，相当给力。我的导师曾经说过，这本书写的很优美，对写英文论文也很有帮助，建议购买英文版的。\n计算机视觉：理论与算法 RichardSzeliski著\nComputer Vision: Theory and Algorithm\n微软的Szeliski写的一本最新的计算机视觉著作。内容非常丰富，尤其包括了作者的研究兴趣，比如一般的书里面都没有的Image Stitching和Image Matting等。这也从另一个侧面说明这本书的通用性不如Sonka的那本。不过作者开放了这本书的电子版，可以有选择性的阅读。\nMultiple View Geometry in Computer Vision 第二版Harley等著\n引用达一万多次的经典书籍了。第二版到处都有电子版的。第一版曾出过中文版的，后来绝版了。网上也可以找到电子版。\n计算机视觉：一种现代方法 DAForsyth等著\nComputer Vision: A Modern Approach\nMIT的经典教材。虽然已经过去十年了，还是值得一读。第二版已经在今年（2012年）出来了，在iask上可以找到非常清晰的版本，将近800页，补充了很多内容。期待影印版。\nMachine vision: theory,algorithms, practicalities 第三版 Davies著\n为数不多的英国人写的书，偏向于工业。\n数字图像处理 第四版 Pratt著\nDigital Image Processing\n写作风格独树一帜，也是图像处理领域很不错的一本书。网上也可以找到非常清晰的电子版。\n5 小结\n罗嗦了这么多，实际上就是几个建议：\n（1）基础书千万不可以扔，也不能低价处理给同学或者师弟师妹。不然到时候还得一本本从书店再买回来的。钱是一方面的问题，对着全新的书看完全没有看自己当年上过的课本有感觉。\n（2）遇到有相关的课，果断选修或者蹭之，比如随机过程，小波分析，模式识别，机器学习，数据挖掘，现代信号处理甚至泛函。多一些理论积累对将来科研和工作都有好处。\n（3）资金允许的话可以多囤一些经典的书，有的时候从牙缝里面省一点都可以买一本好书。不过千万不要像我一样只囤不看。"}
{"content2":"今天在看熊风光同学博士论文《三维点云配准技术研究》，觉得作者总结的对我这个入门小白很受用，于是就接着逐个学习三维计算机视觉的一些局部特征。\n看到了大佬的博客 https://blog.csdn.net/app_12062011/article/details/78168953 很详细，我也顺便搜到了参考文献中Andrew Edie Johnson的1997年的那篇博士原文《Spin-Images: A Representation for 3-D Surface Matching》。\n现记录一些我觉得上面博客没有点出的，对我困扰很大的问题。\n1、网格分辨率\n作者专门给出了网格分辨率的定义，应该是网格中所有边缘长度的中位数。对分辨率的定义与图像处理中使用的分辨率的定义具有相反的含义。 与图像处理一样，分辨率随着采样的添加而增加，采样减少了而减少。 但是，网格分辨率的度量----边长，随着采样的增加而减少，随着采样的减少而增加。\n2、生成 Spin Image 的步骤\n大写P------三维网格某顶点p的切面\nn-------p点单位法向量\nx-------p附近的三维网格上的另一个顶点\nα------x点在P上投影与p的距离\nβ------x点与P点的垂直距离\n其中p和n定义为一个定向点(Oriented point)。\n这的意思应该是说将点p和其法向量n作为基，然后这就变成了一个两个维度的坐标系。空间中的任一点x可以用它到n的距离a和到平面P的距离b来确定（才怪！因为明显还是缺少了一个角度），原文中也有提到说是一个缺少极坐标的圆柱坐标系。因为会导致在空间上不在一起的顶点变换之后有相同的（α，β）坐标。\n1.定义一个 Oriented point\n2.以Oriented point为轴生成一个圆柱坐标系\n3.定义Spin image的参数，Spin image是一个具有一定大小（行数列数）、分辨率（二维网格大小）的二维图像（或者说网格）。Spin image的三个关键参数将在后面讨论。\n4.将圆柱体内的三维坐标 投影到二维Spin image，这一过程可以理解为一个Spin image绕着法向量n旋转360度，Spin image扫到的三维空间的点会落到Spin image的网格中。就是如下的公式：\n﻿从三维空间投影到spin-image坐标\n5.根据spin image中的每个网格中落入的点不同，计算每个网格的强度I，显示spin image时以每个网格（也就是像素）I不同为依据。最直接的方法是直接计算每个网格中落入的点，然而为了降低对位置的敏感度降低噪音影响增加稳定性，Johnson论文中用双线性插值的方法将一个点分布到4个像素中。原理如下图：\n﻿双线性插值\n上图中，默认的网格（像素）边长是1（真实边长的选择会在稍后讨论），当一个点落入网格（i,j）中时会被双线性插值分散到（i,j）、（i,j+1）、（i+1,j）、（i+1,j+1）四个网格中。\n我觉得这里是说落在了Bin中更合适。\n这样就获得了spin image，如下图所示。\n总结一下：某一点p的Spin Image就是用点p和其法向量n为基，把所有点云做的一个3D->2D映射。点云中任一点x，映射为到n的距离和到P平面距离一个二维坐标。很明显还可能存在其他不同于x的点，但是和x的二维坐标相同。可以想象成，拿一个中国象棋棋盘，找一个帅的位置与点p重合，摆棋子的一排与n重合。棋格的距离就是bin size，棋盘的长宽相等，就是image width，而文中所说的support distance = bin size * image width\n然后你就开始转动棋盘，记录点云落在每个棋盘的个数（就是强度），转过的角度就是文中所说的support angle。\n初学小白，要是有理解不对的地方望大神指出！\n还是要看一下PCL中实现的代码，验证自己的理解"}
{"content2":"Python-计算机视觉编程（一）\n第一章 图像处理基础\n1.1 灰度图\n主要内容\n灰度化，在RGB模型中，如果R=G=B时，则彩色表示一种灰度颜色，其中R=G=B的值叫灰度值，因此，灰度图像每个像素只需一个字节存放灰度值（又称强度值、亮度值），灰度范围为0-255。以下是将彩色图做灰度处理转化为灰度图，利用的是pylab中的gray()。\n示例代码\n# -*- coding: utf-8 -*- from PIL import Image from pylab import * pil_im = Image.open(\"me.jpg\") gray() # 灰度处理 subplot(121) axis('off') imshow(pil_im) # 显示图片 pil_im = Image.open(\"me.jpg\").convert('L') subplot(122) axis('off') imshow(pil_im) show()\n运行结果\n1.2 调整尺寸及旋转\n主要内容\n要对一幅图像的尺寸进行调整，可以调用resize()方法，元组中放置的便是你要调整尺寸的大小。如果要对图像进行旋转变换的话，可以调用rotate()方法。以下是对图片进行灰度处理、拷贝原图粘贴到原图上、调整重新设置图片的尺寸和进行图片旋转\n示例代码\n# -*- coding: utf-8 -*- from PIL import Image from pylab import * from matplotlib.font_manager import FontProperties font = FontProperties(fname=r\"c:\\windows\\fonts\\SimSun.ttc\", size=14) figure() pil_im = Image.open(\"me.jpg\") print pil_im.mode, pil_im.size, pil_im.format subplot(231) title(u'原图', fontproperties=font) axis('off') imshow(pil_im) pil_im = Image.open(\"me.jpg\").convert('L') gray() subplot(232) title(u'灰度图', fontproperties=font) axis('off') imshow(pil_im) pil_im = Image.open(\"me.jpg\") box = (100,100,400,400) region = pil_im.crop(box) region = region.transpose(Image.ROTATE_180) pil_im.paste(region,box) subplot(233) title(u'拷贝粘贴区域', fontproperties=font) axis('off') imshow(pil_im) # 缩略图 pil_im = Image.open(\"me.jpg\") size = 128, 128 pil_im.thumbnail(size) print pil_im.size subplot(234) title(u'缩略图', fontproperties=font) axis('off') imshow(pil_im) pil_im = Image.open(\"me.jpg\") pil_im = pil_im.resize(size) print pil_im.size subplot(235) title(u'调整尺寸后的图像', fontproperties=font) axis('off') imshow(pil_im) pil_im = Image.open(\"me.jpg\") pil_im = pil_im.rotate(45) subplot(236) title(u'旋转45°后的图像', fontproperties=font) axis('off') imshow(pil_im) show()\n运行结果\n1.3 画图、描点和线\n主要内容\n给图片加上坐标轴进行绘图\n示例代码\n# -*- coding: utf-8 -*- from PIL import Image from pylab import * # 添加中文字体支持 from matplotlib.font_manager import FontProperties font = FontProperties(fname=r\"c:\\windows\\fonts\\SimSun.ttc\", size=14) im = array(Image.open(\"me.jpg\")) figure() # 画有坐标轴的 subplot(121) imshow(im) x = [100, 100, 400, 400] y = [200, 500, 200, 500] plot(x, y, 'r*') plot(x[:2], y[:2]) title(u'绘图: \"me.jpg\"', fontproperties=font) # 不显示坐标轴 subplot(122) imshow(im) x = [100, 100, 400, 400] y = [200, 500, 200, 500] plot(x, y, 'r*') plot(x[:2], y[:2]) axis('off') #显示坐标轴 title(u'绘图: \"me.jpg\"', fontproperties=font) show()\n运行结果\n1.4 图像轮廓和直方图\n主要内容\n对图片进行RGB转灰度处理、提取图像轮廓并绘制出图像直方图。contour()表示检测图像轮廓，hist()表示创建直方图\n示例代码\n# -*- coding: utf-8 -*- from PIL import Image from pylab import * # 添加中文字体支持 from matplotlib.font_manager import FontProperties font = FontProperties(fname=r\"c:\\windows\\fonts\\SimSun.ttc\", size=14) im = array(Image.open(\"C:\\\\Users\\\\yume4\\\\Pictures\\\\Saved Pictures\\\\me.jpg\").convert('L')) # 打开图像，并转成灰度图像 figure() # 使用颜色信息 subplot(121) gray() # 在原点的左上角显示轮廓图像 contour(im, origin='image') axis('equal') axis('off') title(u'图像轮廓', fontproperties=font) subplot(122) hist(im.flatten(), 128) title(u'图像直方图', fontproperties=font) plt.xlim([0,260]) plt.ylim([0,11000]) show()\n运行结果\n1.5 灰度变换\n主要内容\n以下对图片进行不同程度的灰度处理\n示例代码\n# -*- coding: utf-8 灰度变换-*- from PIL import Image from numpy import * from pylab import * im = array(Image.open(\"C:\\\\Users\\\\yume4\\\\Pictures\\\\Saved Pictures\\\\me.jpg\").convert('L')) print int(im.min()), int(im.max()) im2 = 255 - im # 反色 print int(im2.min()), int(im2.max()) im3 = (100.0/255) * im + 100 # clamp to interval 100...200 print int(im3.min()), int(im3.max()) im4 = 255.0 * (im/255.0)**2 # squared print int(im4.min()), int(im4.max()) figure() gray() subplot(1, 3, 1) imshow(im2) axis('off') title(r'$f(x)=255-x$') subplot(1, 3, 2) imshow(im3) axis('off') title(r'$f(x)=\\frac{100}{255}x+100$') subplot(1, 3, 3) imshow(im4) axis('off') title(r'$f(x)=255(\\frac{x}{255})^2$') show()\n运行结果\n1.6 直方图均衡化\n主要内容\n在图像处理中，直方图均衡化一般用来均衡图像的强度，或增加图像的对比度。示例是对图片进行灰度处理及进行直方图均衡化，显示处理后的图片和直方图。\n示例代码\n# -*- coding: utf-8 -*- from PIL import Image from pylab import * from numpy import * from PCV.tools import imtools # 添加中文字体支持 from matplotlib.font_manager import FontProperties font = FontProperties(fname=r\"c:\\windows\\fonts\\SimSun.ttc\", size=14) im = array(Image.open(\"C:\\\\Users\\\\yume4\\\\Pictures\\\\Saved Pictures\\\\me.jpg\").convert('L')) # 打开图像，并转成灰度图像 #im = array(Image.open('../data/AquaTermi_lowcontrast.JPG').convert('L')) im2, cdf = imtools.histeq(im) figure() subplot(2, 2, 1) axis('off') gray() title(u'原始图像', fontproperties=font) imshow(im) subplot(2, 2, 2) axis('off') title(u'直方图均衡化后的图像', fontproperties=font) imshow(im2) subplot(2, 2, 3) axis('off') title(u'原始直方图', fontproperties=font) #hist(im.flatten(), 128, cumulative=True, normed=True) hist(im.flatten(), 128, normed=True) subplot(2, 2, 4) axis('off') title(u'均衡化后的直方图', fontproperties=font) #hist(im2.flatten(), 128, cumulative=True, normed=True) hist(im2.flatten(), 128, normed=True) show()\n运行结果\n1.7 滤波\n主要内容\n滤波的意思就是对原图像的每个像素周围一定范围内的像素进行运算，运算的范围就称为掩膜。均值滤波是用相邻元素灰度值的平均值代替该元素的灰度值。高斯滤波一般针对的是高斯噪声，能够很好的抑制图像输入时随机引入的噪声，将像素点跟邻域像素看作是一种高斯分布的关系，它的操作是将图像和一个高斯核进行卷积操作。中值滤波是将将每一像素点的灰度值设置为该点某邻域窗口内的所有像素点灰度值的中值。双边滤波是结合图像的空间邻近度和像素值相似度的一种折中处理，同时考虑空域信息和灰度相似性，达到保边去噪的目的，具有简单，非迭代、局部的特点，示例分别对图片进行以上四种滤波处理\n示例代码\n# -*- coding: utf-8-*- import numpy as np import cv2 import matplotlib.pyplot as plt img = cv2.imread(\"C:\\\\Users\\\\yume4\\\\Pictures\\\\Saved Pictures\\\\me.jpg\") # 均值滤波 img_mean = cv2.blur(img, (5,5)) # 高斯滤波 img_Guassian = cv2.GaussianBlur(img,(5,5),0) # 中值滤波 img_median = cv2.medianBlur(img, 5) # 双边滤波 img_bilater = cv2.bilateralFilter(img,9,75,75) # 展示不同的图片 titles = ['srcImg','mean', 'Gaussian', 'median', 'bilateral'] imgs = [img, img_mean, img_Guassian, img_median, img_bilater] for i in range(5): plt.subplot(2,3,i+1)# 注意，这和matlab中类似，没有0，数组下标从1开始 plt.imshow(imgs[i]) plt.title(titles[i]) plt.show()\n运行结果"}
{"content2":"经典论文\nImageNet分类\n物体检测\n物体跟踪\n低级视觉\n边缘检测\n语义分割\n视觉注意力和显著性\n物体识别\n人体姿态估计\nCNN原理和性质（Understanding CNN）\n图像和语言\n图像解说\n视频解说\n图像生成\n微软ResNet\n论文：用于图像识别的深度残差网络\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1512.03385v1.pdf\n微软PRelu（随机纠正线性单元/权重初始化）\n论文：深入学习整流器：在ImageNet分类上超越人类水平\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1502.01852.pdf\n谷歌Batch Normalization\n论文：批量归一化：通过减少内部协变量来加速深度网络训练\n作者：Sergey Ioffe, Christian Szegedy\n链接：http://arxiv.org/pdf/1502.03167.pdf\n谷歌GoogLeNet\n论文：更深的卷积，CVPR 2015\n作者：Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich\n链接：http://arxiv.org/pdf/1409.4842.pdf\n牛津VGG-Net\n论文：大规模视觉识别中的极深卷积网络，ICLR 2015\n作者：Karen Simonyan & Andrew Zisserman\n链接：http://arxiv.org/pdf/1409.1556.pdf\nAlexNet\n论文：使用深度卷积神经网络进行ImageNet分类\n作者：Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n链接：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n物体检测\nPVANET\n论文：用于实时物体检测的深度轻量神经网络（PVANET：Deep but Lightweight Neural Networks for Real-time Object Detection）\n作者：Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park\n链接：http://arxiv.org/pdf/1608.08021\n纽约大学OverFeat\n论文：使用卷积网络进行识别、定位和检测（OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks），ICLR 2014\n作者：Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun\n链接：http://arxiv.org/pdf/1312.6229.pdf\n伯克利R-CNN\n论文：精确物体检测和语义分割的丰富特征层次结构（Rich feature hierarchies for accurate object detection and semantic segmentation），CVPR 2014\n作者：Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik\n链接：http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\n微软SPP\n论文：视觉识别深度卷积网络中的空间金字塔池化（Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition），ECCV 2014\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1406.4729.pdf\n微软Fast R-CNN\n论文：Fast R-CNN\n作者：Ross Girshick\n链接：http://arxiv.org/pdf/1504.08083.pdf\n微软Faster R-CNN\n论文：使用RPN走向实时物体检测（Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks）\n作者：任少卿、何恺明、Ross Girshick、孙剑\n链接：http://arxiv.org/pdf/1506.01497.pdf\n牛津大学R-CNN minus R\n论文：R-CNN minus R\n作者：Karel Lenc, Andrea Vedaldi\n链接：http://arxiv.org/pdf/1506.06981.pdf\n端到端行人检测\n论文：密集场景中端到端的行人检测（End-to-end People Detection in Crowded Scenes）\n作者：Russell Stewart, Mykhaylo Andriluka\n链接：http://arxiv.org/pdf/1506.04878.pdf\n实时物体检测\n论文：你只看一次：统一实时物体检测（You Only Look Once: Unified, Real-Time Object Detection）\n作者：Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi\n链接：http://arxiv.org/pdf/1506.02640.pdf\nInside-Outside Net\n论文：使用跳跃池化和RNN在场景中检测物体（Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks）\n作者：Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick\n链接：http://arxiv.org/abs/1512.04143.pdf\n微软ResNet\n论文：用于图像识别的深度残差网络\n作者：何恺明、张祥雨、任少卿和孙剑\n链接：http://arxiv.org/pdf/1512.03385v1.pdf\nR-FCN\n论文：通过区域全卷积网络进行物体识别（R-FCN: Object Detection via Region-based Fully Convolutional Networks）\n作者：代季峰，李益，何恺明，孙剑\n链接：http://arxiv.org/abs/1605.06409\nSSD\n论文：单次多框检测器（SSD: Single Shot MultiBox Detector）\n作者：Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg\n链接：http://arxiv.org/pdf/1512.02325v2.pdf\n速度/精度权衡\n论文：现代卷积物体检测器的速度/精度权衡（Speed/accuracy trade-offs for modern convolutional object detectors）\n作者：Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy\n链接：http://arxiv.org/pdf/1611.10012v1.pdf\n物体跟踪\n论文：用卷积神经网络通过学习可区分的显著性地图实现在线跟踪（Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network）\n作者：Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han\n地址：arXiv:1502.06796.\n论文：DeepTrack：通过视觉跟踪的卷积神经网络学习辨别特征表征（DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking）\n作者：Hanxi Li, Yi Li and Fatih Porikli\n发表： BMVC, 2014.\n论文：视觉跟踪中，学习深度紧凑图像表示（Learning a Deep Compact Image Representation for Visual Tracking）\n作者：N Wang, DY Yeung\n发表：NIPS, 2013.\n论文：视觉跟踪的分层卷积特征（Hierarchical Convolutional Features for Visual Tracking）\n作者：Chao Ma, Jia-Bin Huang, Xiaokang Yang and Ming-Hsuan Yang\n发表： ICCV 2015\n论文：完全卷积网络的视觉跟踪（Visual Tracking with fully Convolutional Networks）\n作者：Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu,\n发表：ICCV 2015\n论文：学习多域卷积神经网络进行视觉跟踪（Learning Multi-Domain Convolutional Neural Networks for Visual Tracking）\n作者：Hyeonseob Namand Bohyung Han\n对象识别（Object Recognition）\n论文：卷积神经网络弱监督学习（Weakly-supervised learning with convolutional neural networks）\n作者：Maxime Oquab，Leon Bottou，Ivan Laptev，Josef Sivic，CVPR，2015\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf\nFV-CNN\n论文：深度滤波器组用于纹理识别和分割（Deep Filter Banks for Texture Recognition and Segmentation）\n作者：Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Cimpoi_Deep_Filter_Banks_2015_CVPR_paper.pdf\n人体姿态估计（Human Pose Estimation）\n论文：使用 Part Affinity Field的实时多人2D姿态估计（Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields）\n作者：Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh, CVPR, 2017.\n论文：Deepcut：多人姿态估计的联合子集分割和标签（Deepcut: Joint subset partition and labeling for multi person pose estimation）\n作者：Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele, CVPR, 2016.\n论文：Convolutional pose machines\n作者：Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh, CVPR, 2016.\n论文：人体姿态估计的 Stacked hourglass networks（Stacked hourglass networks for human pose estimation）\n作者：Alejandro Newell, Kaiyu Yang, and Jia Deng, ECCV, 2016.\n论文：用于视频中人体姿态估计的Flowing convnets（Flowing convnets for human pose estimation in videos）\n作者：Tomas Pfister, James Charles, and Andrew Zisserman, ICCV, 2015.\n论文：卷积网络和人类姿态估计图模型的联合训练（Joint training of a convolutional network and a graphical model for human pose estimation）\n作者：Jonathan J. Tompson, Arjun Jain, Yann LeCun, Christoph Bregler, NIPS, 2014.\n理解CNN\n论文：通过测量同变性和等价性来理解图像表示(Understanding image representations by measuring their equivariance and equivalence)\n作者：Karel Lenc, Andrea Vedaldi, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf\n论文：深度神经网络容易被愚弄：无法识别的图像的高置信度预测（Deep Neural Networks are Easily Fooled:High Confidence Predictions for Unrecognizable Images）\n作者：Anh Nguyen, Jason Yosinski, Jeff Clune, CVPR, 2015.\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf\n论文：通过反演理解深度图像表示（Understanding Deep Image Representations by Inverting Them）\n作者：Aravindh Mahendran, Andrea Vedaldi, CVPR, 2015\n链接：\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf\n论文：深度场景CNN中的对象检测器（Object Detectors Emerge in Deep Scene CNNs）\n作者：Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, ICLR, 2015.\n链接：http://arxiv.org/abs/1412.6856\n论文：用卷积网络反演视觉表示（Inverting Visual Representations with Convolutional Networks）\n作者：Alexey Dosovitskiy, Thomas Brox, arXiv, 2015.\n链接：http://arxiv.org/abs/1506.02753\n论文：可视化和理解卷积网络（Visualizing and Understanding Convolutional Networks）\n作者：Matthrew Zeiler, Rob Fergus, ECCV, 2014.\n链接：http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\n图像与语言\n图像说明（Image Captioning）\nUCLA / Baidu\n用多模型循环神经网络解释图像（Explain Images with Multimodal Recurrent Neural Networks）\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille, arXiv:1410.1090\nhttp://arxiv.org/pdf/1410.1090\nToronto\n使用多模型神经语言模型统一视觉语义嵌入（Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models）\nRyan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, arXiv:1411.2539.\nhttp://arxiv.org/pdf/1411.2539\nBerkeley\n用于视觉识别和描述的长期循环卷积网络（Long-term Recurrent Convolutional Networks for Visual Recognition and Description）\nJeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, arXiv:1411.4389.\nhttp://arxiv.org/pdf/1411.4389\nGoogle\n看图写字：神经图像说明生成器（Show and Tell: A Neural Image Caption Generator）\nOriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, arXiv:1411.4555.\nhttp://arxiv.org/pdf/1411.4555\nStanford\n用于生成图像描述的深度视觉语义对齐（Deep Visual-Semantic Alignments for Generating Image Description）\nAndrej Karpathy, Li Fei-Fei, CVPR, 2015.\nWeb：http://cs.stanford.edu/people/karpathy/deepimagesent/\nPaper：http://cs.stanford.edu/people/karpathy/cvpr2015.pdf\nUML / UT\n使用深度循环神经网络将视频转换为自然语言（Translating Videos to Natural Language Using Deep Recurrent Neural Networks）\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, NAACL-HLT, 2015.\nhttp://arxiv.org/pdf/1412.4729\nCMU / Microsoft\n学习图像说明生成的循环视觉表示（Learning a Recurrent Visual Representation for Image Caption Generation）\nXinlei Chen, C. Lawrence Zitnick, arXiv:1411.5654.\nXinlei Chen, C. Lawrence Zitnick, Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation, CVPR 2015\nhttp://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf\nMicrosoft\n从图像说明到视觉概念（From Captions to Visual Concepts and Back）\nHao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig, CVPR, 2015.\nhttp://arxiv.org/pdf/1411.4952\nUniv. Montreal / Univ. Toronto\nShow, Attend, and Tell：视觉注意力与神经图像标题生成（Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention）\nKelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, arXiv:1502.03044 / ICML 2015\nhttp://www.cs.toronto.edu/~zemel/documents/captionAttn.pdf\nIdiap / EPFL / Facebook\n基于短语的图像说明（Phrase-based Image Captioning）\nRemi Lebret, Pedro O. Pinheiro, Ronan Collobert, arXiv:1502.03671 / ICML 2015\nhttp://arxiv.org/pdf/1502.03671\nUCLA / Baidu\n像孩子一样学习：从图像句子描述快速学习视觉的新概念（Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images）\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L. Yuille, arXiv:1504.06692\nhttp://arxiv.org/pdf/1504.06692\nMS + Berkeley\n探索图像说明的最近邻方法（ Exploring Nearest Neighbor Approaches for Image Captioning）\nJacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, arXiv:1505.04467\nhttp://arxiv.org/pdf/1505.04467.pdf\n图像说明的语言模型（Language Models for Image Captioning: The Quirks and What Works）\nJacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, arXiv:1505.01809\nhttp://arxiv.org/pdf/1505.01809.pdf\n阿德莱德\n具有中间属性层的图像说明（ Image Captioning with an Intermediate Attributes Layer）\nQi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, arXiv:1506.01144\n蒂尔堡\n通过图片学习语言(Learning language through pictures)\nGrzegorz Chrupala, Akos Kadar, Afra Alishahi, arXiv:1506.03694\n蒙特利尔大学\n使用基于注意力的编码器-解码器网络描述多媒体内容（Describing Multimedia Content using Attention-based Encoder-Decoder Networks）\nKyunghyun Cho, Aaron Courville, Yoshua Bengio, arXiv:1507.01053\n康奈尔\n图像表示和神经图像说明的新领域（Image Representations and New Domains in Neural Image Captioning）\nJack Hessel, Nicolas Savva, Michael J. Wilber, arXiv:1508.02091\nMS + City Univ. of HongKong\nLearning Query and Image Similarities with Ranking Canonical Correlation Analysis\nTing Yao, Tao Mei, and Chong-Wah Ngo, ICCV, 2015\n视频字幕（Video Captioning）\n伯克利\nJeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR, 2015.\n犹他州/ UML / 伯克利\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729.\n微软\nYingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui, Joint Modeling Embedding and Translation to Bridge Video and Language, arXiv:1505.01861.\n犹他州/ UML / 伯克利\nSubhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence–Video to Text, arXiv:1505.00487.\n蒙特利尔大学/ 舍布鲁克\nLi Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029\nMPI / 伯克利\nAnna Rohrbach, Marcus Rohrbach, Bernt Schiele, The Long-Short Story of Movie Description, arXiv:1506.01698\n多伦多大学 / MIT\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books, arXiv:1506.06724\n蒙特利尔大学\nKyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053\nTAU / 美国南加州大学\nDotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf, Temporal Tessellation for Video Annotation and Summarization, arXiv:1612.06950.\n图像生成\n卷积/循环网络\n论文：Conditional Image Generation with PixelCNN Decoders”\n作者：Aäron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu\n论文：Learning to Generate Chairs with Convolutional Neural Networks\n作者：Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox\n发表：CVPR, 2015.\n论文：DRAW: A Recurrent Neural Network For Image Generation\n作者：Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra\n发表：ICML, 2015.\n对抗网络\n论文：生成对抗网络（Generative Adversarial Networks）\n作者：Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n发表：NIPS, 2014.\n论文：使用对抗网络Laplacian Pyramid 的深度生成图像模型（Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks）\n作者：Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus\n发表：NIPS, 2015.\n论文：生成模型演讲概述 （A note on the evaluation of generative models）\n作者：Lucas Theis, Aäron van den Oord, Matthias Bethge\n发表：ICLR 2016.\n论文：变分自动编码深度高斯过程（Variationally Auto-Encoded Deep Gaussian Processes）\n作者：Zhenwen Dai, Andreas Damianou, Javier Gonzalez, Neil Lawrence\n发表：ICLR 2016.\n论文：用注意力机制从字幕生成图像 （Generating Images from Captions with Attention）\n作者：Elman Mansimov, Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov\n发表： ICLR 2016\n论文：分类生成对抗网络的无监督和半监督学习（Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks）\n作者：Jost Tobias Springenberg\n发表：ICLR 2016\n论文：用一个对抗检测表征（Censoring Representations with an Adversary）\n作者：Harrison Edwards, Amos Storkey\n发表：ICLR 2016\n论文：虚拟对抗训练实现分布式顺滑 （Distributional Smoothing with Virtual Adversarial Training）\n作者：Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii\n发表：ICLR 2016\n论文：自然图像流形上的生成视觉操作（Generative Visual Manipulation on the Natural Image Manifold）\n作者：朱俊彦, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros\n发表： ECCV 2016.\n论文：深度卷积生成对抗网络的无监督表示学习（Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks）\n作者：Alec Radford, Luke Metz, Soumith Chintala\n发表： ICLR 2016\n问题回答\n弗吉尼亚大学 / 微软研究院\n论文：VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.\n作者：Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh\nMPI / 伯克利\n论文：Ask Your Neurons: A Neural-based Approach to Answering Questions about Images\n作者：Mateusz Malinowski, Marcus Rohrbach, Mario Fritz,\n发布 ： arXiv:1505.01121.\n多伦多\n论文： Image Question Answering: A Visual Semantic Embedding Model and a New Dataset\n作者：Mengye Ren, Ryan Kiros, Richard Zemel\n发表： arXiv:1505.02074 / ICML 2015 deep learning workshop.\n百度/ 加州大学洛杉矶分校\n作者：Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, 徐伟\n论文：Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering\n发表： arXiv:1505.05612.\nPOSTECH（韩国）\n论文：Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction\n作者：Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han\n发表： arXiv:1511.05765\nCMU / 微软研究院\n论文：Stacked Attention Networks for Image Question Answering\n作者：Yang, Z., He, X., Gao, J., Deng, L., & Smola, A. (2015)\n发表： arXiv:1511.02274.\nMetaMind\n论文：Dynamic Memory Networks for Visual and Textual Question Answering\n作者：Xiong, Caiming, Stephen Merity, and Richard Socher\n发表： arXiv:1603.01417 (2016).\n首尔国立大学 + NAVER\n论文：Multimodal Residual Learning for Visual QA\n作者：Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang\n发表：arXiv:1606:01455\nUC Berkeley + 索尼\n论文：Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\n作者：Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach\n发表：arXiv:1606.01847\nPostech\n论文：Training Recurrent Answering Units with Joint Loss Minimization for VQA\n作者：Hyeonwoo Noh and Bohyung Han\n发表： arXiv:1606.03647\n首尔国立大学 + NAVER\n论文： Hadamard Product for Low-rank Bilinear Pooling\n作者：Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhan\n发表：arXiv:1610.04325.\n视觉注意力和显著性\n论文：Predicting Eye Fixations using Convolutional Neural Networks\n作者：Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu\n发表：CVPR, 2015.\n学习地标的连续搜索\n作者：Learning a Sequential Search for Landmarks\n论文：Saurabh Singh, Derek Hoiem, David Forsyth\n发表：CVPR, 2015.\n视觉注意力机制实现多物体识别\n论文：Multiple Object Recognition with Visual Attention\n作者：Jimmy Lei Ba, Volodymyr Mnih, Koray Kavukcuoglu,\n发表：ICLR, 2015.\n视觉注意力机制的循环模型\n作者：Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu\n论文：Recurrent Models of Visual Attention\n发表：NIPS, 2014.\n低级视觉\n超分辨率\nIterative Image Reconstruction\nSven Behnke: Learning Iterative Image Reconstruction. IJCAI, 2001.\nSven Behnke: Learning Iterative Image Reconstruction in the Neural Abstraction Pyramid. International Journal of Computational Intelligence and Applications, vol. 1, no. 4, pp. 427-438, 2001.\nSuper-Resolution (SRCNN)\nChao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, ECCV, 2014.\nChao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Image Super-Resolution Using Deep Convolutional Networks, arXiv:1501.00092.\nVery Deep Super-Resolution\nJiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Accurate Image Super-Resolution Using Very Deep Convolutional Networks, arXiv:1511.04587, 2015.\nDeeply-Recursive Convolutional Network\nJiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Deeply-Recursive Convolutional Network for Image Super-Resolution, arXiv:1511.04491, 2015.\nCasade-Sparse-Coding-Network\nZhaowen Wang, Ding Liu, Wei Han, Jianchao Yang and Thomas S. Huang, Deep Networks for Image Super-Resolution with Sparse Prior. ICCV, 2015.\nPerceptual Losses for Super-Resolution\nJustin Johnson, Alexandre Alahi, Li Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, arXiv:1603.08155, 2016.\nSRGAN\nChristian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, arXiv:1609.04802v3, 2016.\n其他应用\nOptical Flow (FlowNet)\nPhilipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox, FlowNet: Learning Optical Flow with Convolutional Networks, arXiv:1504.06852.\nCompression Artifacts Reduction\nChao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, Compression Artifacts Reduction by a Deep Convolutional Network, arXiv:1504.06993.\nBlur Removal\nChristian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf, Learning to Deblur, arXiv:1406.7444\nJian Sun, Wenfei Cao, Zongben Xu, Jean Ponce, Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal, CVPR, 2015\nImage Deconvolution\nLi Xu, Jimmy SJ. Ren, Ce Liu, Jiaya Jia, Deep Convolutional Neural Network for Image Deconvolution, NIPS, 2014.\nDeep Edge-Aware Filter\nLi Xu, Jimmy SJ. Ren, Qiong Yan, Renjie Liao, Jiaya Jia, Deep Edge-Aware Filters, ICML, 2015.\nComputing the Stereo Matching Cost with a Convolutional Neural Network\nJure Žbontar, Yann LeCun, Computing the Stereo Matching Cost with a Convolutional Neural Network, CVPR, 2015.\nColorful Image Colorization Richard Zhang, Phillip Isola, Alexei A. Efros, ECCV, 2016\nFeature Learning by Inpainting\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros, Context Encoders: Feature Learning by Inpainting, CVPR, 2016\n边缘检测\nSaining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.\nDeepEdge\nGedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.\nDeepContour\nWei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.\n语义分割\nSEC: Seed, Expand and Constrain\nAlexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016.\nAdelaide\nGuosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. (1st ranked in VOC2012)\nGuosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. (4th ranked in VOC2012)\nDeep Parsing Network (DPN)\nZiwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 / ICCV 2015 (2nd ranked in VOC 2012)\nCentraleSuperBoundaries, INRIA\nIasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)\nBoxSup\nJifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)\nPOSTECH\nHyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. (7th ranked in VOC2012)\nSeunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924.\nSeunghoon Hong,Junhyuk Oh,Bohyung Han, andHonglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928\nConditional Random Fields as Recurrent Neural Networks\nShuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)\nDeepLab\nLiang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. (9th ranked in VOC2012)\nZoom-out\nMohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015\nJoint Calibration\nHolger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.\nFully Convolutional Networks for Semantic Segmentation\nJonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.\nHypercolumn\nBharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.\nDeep Hierarchical Parsing\nAbhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015.\nLearning Hierarchical Features for Scene Labeling\nClement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.\nClement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.\nUniversity of Cambridge\nVijay Badrinarayanan, Alex Kendall and Roberto Cipolla “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.” arXiv preprint arXiv:1511.00561, 2015.\nAlex Kendall, Vijay Badrinarayanan and Roberto Cipolla “Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.” arXiv preprint arXiv:1511.02680, 2015.\nPrinceton\nFisher Yu, Vladlen Koltun, “Multi-Scale Context Aggregation by Dilated Convolutions”, ICLR 2016\nUniv. of Washington, Allen AI\nHamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, “Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing”, ICCV, 2015\nINRIA\nIasonas Kokkinos, “Pusing the Boundaries of Boundary Detection Using deep Learning”, ICLR 2016\nUCSB\nNiloufar Pourian, S. Karthikeyan, and B.S. Manjunath, “Weakly supervised graph based semantic segmentation by learning communities of image-parts”, ICCV, 2015\n其他资源\n课程\n深度视觉\n[斯坦福] CS231n: Convolutional Neural Networks for Visual Recognition\n[香港中文大学] ELEG 5040: Advanced Topics in Signal Processing(Introduction to Deep Learning)\n· 更多深度课程推荐\n[斯坦福] CS224d: Deep Learning for Natural Language Processing\n[牛津 Deep Learning by Prof. Nando de Freitas\n[纽约大学] Deep Learning by Prof. Yann LeCun\n图书\n免费在线图书\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\nNeural Networks and Deep Learning by Michael Nielsen\nDeep Learning Tutorial by LISA lab, University of Montreal\n视频\n演讲\nDeep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng\nRecent Developments in Deep Learning By Geoff Hinton\nThe Unreasonable Effectiveness of Deep Learning by Yann LeCun\nDeep Learning of Representations by Yoshua bengio\n软件\n框架\nTensorflow: An open source software library for numerical computation using data flow graph by Google [Web]\nTorch7: Deep learning library in Lua, used by Facebook and Google Deepmind [Web]\nTorch-based deep learning libraries: [torchnet],\nCaffe: Deep learning framework by the BVLC [Web]\nTheano: Mathematical library in Python, maintained by LISA lab [Web]\nTheano-based deep learning libraries: [Pylearn2], [Blocks], [Keras], [Lasagne]\nMatConvNet: CNNs for MATLAB [Web]\nMXNet: A flexible and efficient deep learning library for heterogeneous distributed systems with multi-language support [Web]\nDeepgaze: A computer vision library for human-computer interaction based on CNNs [Web]\n应用\n对抗训练 Code and hyperparameters for the paper “Generative Adversarial Networks” [Web]\n理解与可视化 Source code for “Understanding Deep Image Representations by Inverting Them,” CVPR, 2015. [Web]\n词义分割 Source code for the paper “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR, 2014. [Web] ； Source code for the paper “Fully Convolutional Networks for Semantic Segmentation,” CVPR, 2015. [Web]\n超分辨率 Image Super-Resolution for Anime-Style-Art [Web]\n边缘检测 Source code for the paper “DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection,” CVPR, 2015. [Web]\nSource code for the paper “Holistically-Nested Edge Detection”, ICCV 2015. [Web]\n讲座\n[CVPR 2014] Tutorial on Deep Learning in Computer Vision\n[CVPR 2015] Applied Deep Learning for Computer Vision with Torch\n博客\nDeep down the rabbit hole: CVPR 2015 and beyond@Tombone’s Computer Vision Blog\nCVPR recap and where we’re going@Zoya Bylinskii (MIT PhD Student)’s Blog\nFacebook’s AI Painting@Wired\nInceptionism: Going Deeper into Neural Networks@Google Research\nImplementing Neural networks"}
{"content2":"指纹解锁、刷脸识别、语音转换文字、机器人看病、Alphago······我们已经深刻的感受到，人工智能在改变我们的工作方式和认知。\n通过 SAS 针对企业人工自能就绪调研的报告可以看到，大部分企业认为人工智能还处于初期阶段，“目前，我们正在部署的大量应用场景都包含 AI 板块”。 显而易见，我们必须学习新的技能来配合 AI 的发展，并且，未来也是属于那些意识到这一点，并开始及早发展这些技能的人。\n进入 AI 领域，方向选择很重要\n以 2017 年 AI 领域各赛道的投资数据来看，投资事件数最多的是计算机视觉方向，其次是自然语言处理、智能机器人及自动驾驶。\n能够获得如此多的投资，足以证明，计算机视觉是一个发展前景巨大的方向。\n这么火的计算机视觉到底该怎样学习呢？\n1、你可以先从看书学起。有很多关于计算机视觉的书籍，通过学习掌握计算机视觉的基本术语，了解计算机视觉的基本概念，同时也能够根据书中给到的代码及案例动手实操，一边看书一边实践；\n2、深入实践。这需要你具有一定的计算机视觉知识。你可以选择在实验室或者公司动手操作实际项目，最好选择当前项目方向深耕下去。实践过程中你可以和导师、上级随时沟通。\n3、系统专业的课程学习。这里说的课程并不是大学的专业课，而是将计算机视觉领域的重点研究问题、行业发展趋势及实际案例整理汇总，浓缩成精华，集中授课之后让你有一个质的飞跃。\n这里我向你推荐小象学院的课程：\n《计算机视觉的深度学习实践》\n上海交大机器学习方向博士后叶梓老师10年+人工智能研发经验倾囊相授：\n计算机视觉的领域的重点研究问题。由浅入深的讲解数字图像的存储、预处理、特征提取、以及在深度学习兴起之前计算机视觉领域所取得的成就；\n专门介绍深度学习的基础理论知识，包括神经网络的基本原理，以及深度学习对于传统神经网络的关键改进；\n重点介绍深度学者模型在计算机视觉领域的应用。具体设计在计算机领域如何引用卷积神经网络、区域神经网路、全卷积网络、循环神经网络、长短时记忆单元、生成对抗网络等解决图像应用的难点；\n课程将用 Python 等语言及 Tensoeflow、Keras 深度学习框架等进行案例实践教学；\n点击阅读原文，参团价更低！\n👇👇👇快上车！"}
{"content2":"引言\n目标定位是图像处理或计算机视觉系统（如目标检测与分类，证件识别等）的第一步。任何计算机视觉系统都显性或隐性的包含着目标定位的步骤。目标定位的方法有传统方法和基于卷积神经网络的深度学习方法，本文主要讨论后者。深度学习方法有更好的鲁棒性（robustness），对各种问题实现形式统一，无需人为设定参数，无需太多的图像处理知识等优点。\n问题的定义\n目标定位解决的是在一张图像中找到我们感兴趣的目标的位置，用的最多是（x，y，width，height）形式的定位，除此之外还有矩形目标的四个顶点形式以便后续通过透视变化矫正形变。\n解决方案\n卷积层抽特征\n全连层目标回归\n定义欧式距离损或IoU损失函数\n最小化损失函数\n优化\n在解决问题的前提下优化网络以实现最小的CPU消耗，最少的内存占用是至关重要的。\n参考\nhttps://github.com/tensorflow/models/tree/master/research/object_detection\nhttps://en.wikipedia.org/wiki/Object_detection\nhttps://www.quora.com/Convolutional-Neural-Networks-What-are-bounding-box-regressors-doing-in-Fast-RCNN\nhttps://www.quora.com/How-can-I-design-a-bounding-box-regressor-using-CNN"}
{"content2":"计算机视觉目前跟其他学科的关系非常的多，包括机器人，以及刚才提到的医疗、物理、图像、卫星图片的处理，这些都会经常使用到计算机视觉，那这里呢，最常问到的问题无非就是有三个概念，一个叫做计算机视觉，一个叫做机器视觉，一个叫做图像处理，那这三个东西有什么区别呢？\n这三个东西的区别还是挺因人而异的，每一个研究人员对它的理解都不一样。\n首先，ImageProcessing更多的是图形图像的一些处理，图像像素级别的一些处理，包括3D的处理，更多的会理解为是一个图像的处理；而机器视觉呢，更多的是它还结合到了硬件层面的处理，就是软硬件结合的图形计算的能力，跟图形智能化的能力，我们一般会理解为他就是所谓的机器视觉。\n而我们今天所说的计算机视觉，更多的是偏向于软件层面的计算机处理，而且不是说做图像的识别这么简单，更多的还包括了对图像的理解，甚至是对图像的一些变换处理，当前我们涉及到的一些图像的生成，也是可以归类到这个计算机视觉领域里面的。\n所以说计算机视觉它本身的也是一个很基础的学科，可以跟各个学科做交叉，同时，它自己内部也会分的比较细，包括机器视觉、图像处理。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n人工智能与计算机视觉\nhttp://www.duozhishidai.com/article-15129-1.html\n机器人视觉系统分为哪几种，主要包括哪些关键技术？\nhttp://www.duozhishidai.com/article-1753-1.html\n图像识别经历了哪几个阶段，主要应用在哪些领域？\nhttp://www.duozhishidai.com/article-6461-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"本篇文章为转载，转载地址http://blog.sina.com.cn/s/blog_9d3b84e30102wfz8.html\n以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事 这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为， 知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.PHP/首页\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/ 人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/ 人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/ http://yann.lecun.com/exdb/mnist/ 手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）南开大学程明明助教：http://mmcheng.net/ 图像分割、检索, bing特征快速目标（行人）检测；\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://www.robots.ox.ac.uk/~phst/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/\n（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n（205）中文语言资源联盟：http://www.chineseldc.org/index.html 内有很多语言识别、字符识别的训练，测试库；\n（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、\n（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库\n（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html# 烟雾检测、3D重建、医学图像处理\n（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/ 图像增强、\n（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n（213）北京航空航天大学周付根教授：http://www.sa.buaa.edu.cn/html/yhxy/szll/jsfc/txcl/1521.html 医学图像处理\n（214）北京航空航天大学姜志国教授：http://www.sa.buaa.edu.cn/html/yhxy/xygk/zyld/1595.html 医学图像处理，遥感图像分割；\n（215）德国萨尔布吕肯大学博士后R. Benenson： http://rodrigob.github.io/# 行人检测，无人驾驶汽车\n（216）西南财经大学段江教授：http://it.swufe.edu.cn/2011-09/25/201109251002096701.html 高动态范围图像处理\n（217）中科院沈阳自动化所华春生研究员：http://people.ucas.ac.cn/~huacs 行人检测、目标跟踪、聚类分析\n（218）华中科技大学自动化学院张天序教授：http://auto.hust.edu.cn/viewnews-1978 红外图像处理，医学图像处理，武器装备图像处理\n（219）普林斯顿大学Jianxiong Xiao助理教授：http://vision.princeton.edu/people/xj/ 3D重建、3D识别、深度学习"}
{"content2":"豆瓣关于计算机视觉的书评及介绍\n标签：\n计算机视觉 http://book.douban.com/tag/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89\n机器视觉 http://book.douban.com/tag/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89\n图像视觉 http://book.douban.com/tag/%E5%9B%BE%E5%83%8F%E8%A7%86%E8%A7%89\n图像处理 http://book.douban.com/tag/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86\n模式识别 http://book.douban.com/tag/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86\n智能信息处理 http://book.douban.com/tag/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86\n数据挖掘 http://book.douban.com/tag/datamining%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98\n人工智能与信息处理 http://book.douban.com/tag/datamining%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98\n机器学习 http://book.douban.com/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\n统计学习 http://book.douban.com/tag/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0\nCV http://book.douban.com/tag/CV"}
{"content2":"3D 图像介绍\n3D 图像会多包含一个维度，即深度。\n有两种最广泛使用的 3D 格式：RGB-D 和点云。\nRGB-D\nRGB-D 格式图像就像一堆单值图像，每个像素都有四个属性，红色，绿色，蓝色和深度。\n在普通的基于像素的图像中，我们可以通过（x，y）坐标定位任何像素，然后就可以分别获得三种颜色属性（R，G，B）。而在 RGB-D 图像中，每个（x，y）坐标将对应于四个属性（深度，R，G，B）。\nRGB-D 和点云之间的唯一区别在于，在点云中，（x，y）坐标反映了其在现实世界中的实际值，而不是简单的整数值。\n点云\n点云可以由 RGB-D 图像构建。\n如果你有已扫描的 RGB-D 的图像，并且还知道扫描相机的内在参数，那么你可以以 RGB-D 图像创建点云，方法是通过使用相机内在参数计算真实世界的点（x，y）。这个过程被称为相机校准。\n因此，到目前为止，你知道了 RGB-D 图像是网格对齐的图像，而点云是更稀疏的结构。"}
{"content2":"在计算机视觉领域，每年都会有很多顶级会议召开，如比较著名的CVPR，ICCV等，在会议上会有CV各个领域的新思想、新方法被提出来，推动着这个领域的发展，以下为2018年各个会议的时间地点，还有会议相关链接。\n1. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)\nLocation / Date: Salt Lake City, June 18-21, 2018\nPaper Submission Deadline: November 15, 2017\nAcceptance rate: 20% ~ 30%\nhttp://cvpr2018.thecvf.com/\n２. IEEE International Conference on Computer Vision (ICCV 2018)\nLocation / Date: Istanbul, Turkey; January 30-31, 2018\nPaper Submission Deadline: November 30, 2017\nAcceptance rate: 20% ~ 30%\nhttp://www.guide2research.com/conference/iccv-2018\nNB：CVPR和ICCV上的文章是被引用和下载最多的，这里附上这两个会议的文章下载链接，大家自行保存，对于其他会议的文章可以自己百度查找\nhttp://openaccess.thecvf.com/menu.py\n３.  The 15th European Conference on Computer Vision (ECCV 2018) (两年一次)\nLocation / Date: Munich, Germany; September 8-14, 2018\nPaper Submission Deadline: March 14, 2018 (23:59 CET)\nAcceptance rate: 20% ~ 30%\nhttps://eccv2018.org/\n4. Conference on Neural Information Processing Systems (NIPS 2018)\nLocation / Date: Montreal, Canada; December 03-08, 2018\nPaper Submission Deadline: May 18, 2018\nAcceptance rate: 20%~ 30%\nhttps://nips.cc/\n5. The 14th Asian Conference on Computer Vision (ACCV 2018)\nLocation / Date:Perth, Australia; December 02-06, 2018\nPaper Submission Deadline: July 05, 2018\nAcceptance rate: ~ 30%\nhttp://accv2018.net/\n6. The 29th British Machine Vision Conference (BMVC 2018)\nLocation / Date:Northumbria University; September 03-06, 2018\nPaper Submission Deadline: May 07, 2018\nAcceptance rate: ~ 40%\nhttp://bmvc2018.org/index.html\n7. IEEE International Conference on Image Processing (ICIP 2018)\nLocation / Date: Athens, Greece; October 7-10, 2018\nPaper Submission Deadline: February 07, 2018\nAcceptance rate: ~ 45%\nhttps://2018.ieeeicip.org/\n8. The 24th International Conference on Pattern Recognition (ICPR 2018)\nLocation / Date: Beijing, China; August 20-24, 2018\nPaper Submission Deadline: January 22, 2018\nAcceptance rate: ~ 55%\nhttp://www.icpr2018.org/\n9. The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)\nLocation / Date: Louisiana, USA; February 2-7, 2018\nPaper Submission Deadline: October 13, 2017\nAcceptance rate:\nhttps://aaai.org/Conferences/AAAI-18/\n原创文章，转载请注明出处：https://blog.csdn.net/hitzijiyingcai/article/details/81210498"}
{"content2":"让计算机'看'是一个不小的壮举。为了让机器像人或动物一样真正地观察世界，它依赖于计算机视觉和图像识别。\n计算机视觉是条形码扫描仪能够“看到”UPC中的一堆条纹的能力。这也是Apple的Face ID可以判断出它的相机正在看的脸是否是你的。基本上，只要机器处理原始视觉输入（例如JPEG文件或摄像机馈送），它就会使用计算机视觉来理解它所看到的内容。一般来讲计算机视觉视为处理眼睛接收到的信息的人类大脑的一部分 - 而不是眼睛本身。\n从人工智能的角度来看，计算机视觉最有趣的用途之一是图像识别，它使机器能够解释通过计算机视觉接收的输入并对其“看到”进行分类。\n以下是工作中图像识别的一些示例：\nEbay应用程序允许您使用相机搜索项目\n利用神经网络将漆黑的照片变成明亮的图像\nFacebook的AI对你的照片了解很多\n人工智能可以读懂你的想法怎么样？\n例如，还有一款应用程序使用智能手机相机来确定对象是否是热狗。它利用计算机视觉和图像识别来做出判断。这可能看起来并不令人印象深刻，毕竟一个小孩子也可以告诉你某个东西是否是热狗。但是，在人脑和计算机中，训练神经网络进行图像识别的过程非常复杂。\n在这一点上，AI就像一个小孩子。计算机视觉赋予它视觉感，但这并不是对物理宇宙的继承理解。为此，AI需要像孩子一样进行培训。如果您给孩子一个数字或字母足够的时间，它将学习识别该数字。\n令人惊讶的是，许多幼儿在他们正确地学习它们后，可以立即识别字母和数字。我们的生物神经网络非常擅长解释视觉信息，即使我们正在处理的图像看起来并不完全符合我们的预期。\n使计算机识别特定图像（如QR码）很容易，但是他们很难识别出他们不期望的状态 - 进入图像识别。\n通常，图像识别的工作方式涉及创建处理图像的各个像素的神经网络。研究人员尽可能多地为这些网络提供预先标记的图像，以“教导”他们如何识别相似的图像。\n在上面的热狗示例中，开发人员可以为AI提供数千张热狗照片。然后，人工智能会对热狗应该拥有的图片进行全面的了解。当你给它提供一些东西的图像时，它会将该图像的每个像素与它所见过的热狗的每张图像进行比较。如果输入满足类似像素的最小阈值，则AI将其声明为热狗。\n处理视觉信息的任何AI系统通常依赖于计算机视觉，并且能够识别特定对象或基于其内容对图像分类的那些系统正在执行图像识别。\n这对于需要快速准确地识别和分类环境中不同对象的机器人来说非常重要。例如，无人驾驶汽车使用计算机视觉和图像识别来识别行人，标志和其他车辆。（新图智 www.vision123.cn）"}
{"content2":"[转]http://www.leiphone.com/news/201605/zZqsZiVpcBBPqcGG.html\n今年夏天，雷锋网将在深圳举办“全球人工智能与机器人创新大会”（GAIR），在本次大会上，我们将发布“人工智能与机器人Top25创新企业榜“，慧眼科技是我们重点关注的公司之一。今天，我们邀请到慧眼科技研发总监李汉曦，为我们带来深度学习与计算机视觉方面的内容分享。\n嘉宾介绍：李汉曦，慧眼科技研发总监，澳大利亚国立大学博士；曾任澳大利亚国家信息通信公司(NICTA)任高级研究员；人脸识别，物体检测，物体跟踪、深度学习方面的专家，在TPAMI，TIP, TNNLS和Pattern Recognition等权威期刊，以及CVPR，ECCV，BMVC, ACCV等领域内重要会议发表过有影响力的论文；现为澳大利亚格里菲斯大学客座研究员，江西师范大学特聘教授。\n人工智能是人类一个非常美好的梦想，跟星际漫游和长生不老一样。我们想制造出一种机器，使得它跟人一样具有一定的对外界事物感知能力，比如看见世界。\n在上世纪50年代，数学家图灵提出判断机器是否具有人工智能的标准：图灵测试。即把机器放在一个房间，人类测试员在另一个房间，人跟机器聊天，测试员事先不知道另一房间里是人还是机器 。经过聊天，如果测试员不能确定跟他聊天的是人还是机器的话，那么图灵测试就通过了，也就是说这个机器具有与人一样的感知能力。\n但是从图灵测试提出来开始到本世纪初，50多年时间有无数科学家提出很多机器学习的算法，试图让计算机具有与人一样的智力水平，但直到2006年深度学习算法的成功，才带来了一丝解决的希望。\n众星捧月的深度学习\n深度学习在很多学术领域，比非深度学习算法往往有20-30%成绩的提高。很多大公司也逐渐开始出手投资这种算法，并成立自己的深度学习团队，其中投入最大的就是谷歌，2008年6月披露了谷歌脑项目。2014年1月谷歌收购DeepMind，然后2016年3月其开发的Alphago算法在围棋挑战赛中，战胜了韩国九段棋手李世石，证明深度学习设计出的算法可以战胜这个世界上最强的选手。\n在硬件方面，Nvidia最开始做显示芯片，但从2006及2007年开始主推用GPU芯片进行通用计算，它特别适合深度学习中大量简单重复的计算量。目前很多人选择Nvidia的CUDA工具包进行深度学习软件的开发。\n微软从2012年开始，利用深度学习进行机器翻译和中文语音合成工作，其人工智能小娜背后就是一套自然语言处理和语音识别的数据算法。\n百度在2013年宣布成立百度研究院，其中最重要的就是百度深度学习研究所，当时招募了著名科学家余凯博士。不过后来余凯离开百度，创立了另一家从事深度学习算法开发的公司地平线。\nFacebook和Twitter也都各自进行了深度学习研究，其中前者携手纽约大学教授Yann Lecun，建立了自己的深度学习算法实验室；2015年10月，Facebook宣布开源其深度学习算法框架，即Torch框架。Twitter在2014年7月收购了Madbits，为用户提供高精度的图像检索服务。\n前深度学习时代的计算机视觉\n互联网巨头看重深度学习当然不是为了学术，主要是它能带来巨大的市场。那为什么在深度学习出来之前，传统算法为什么没有达到深度学习的精度？\n在深度学习算法出来之前，对于视觉算法来说，大致可以分为以下5个步骤：特征感知，图像预处理，特征提取，特征筛选，推理预测与识别。早期的机器学习中，占优势的统计机器学习群体中，对特征是不大关心的。\n我认为，计算机视觉可以说是机器学习在视觉领域的应用，所以计算机视觉在采用这些机器学习方法的时候，不得不自己设计前面4个部分。\n但对任何人来说这都是一个比较难的任务。传统的计算机识别方法把特征提取和分类器设计分开来做，然后在应用时再合在一起，比如如果输入是一个摩托车图像的话，首先要有一个特征表达或者特征提取的过程，然后把表达出来的特征放到学习算法中进行分类的学习。\n过去20年中出现了不少优秀的特征算子，比如最著名的SIFT算子，即所谓的对尺度旋转保持不变的算子。它被广泛地应用在图像比对，特别是所谓的structure from motion这些应用中，有一些成功的应用例子。另一个是HoG算子，它可以提取物体，比较鲁棒的物体边缘，在物体检测中扮演着重要的角色。\n这些算子还包括Textons，Spin image，RIFT和GLOH，都是在深度学习诞生之前或者深度学习真正的流行起来之前，占领视觉算法的主流。\n几个（半）成功例子\n这些特征和一些特定的分类器组合取得了一些成功或半成功的例子，基本达到了商业化的要求但还没有完全商业化。\n一是八九十年代的指纹识别算法，它已经非常成熟，一般是在指纹的图案上面去寻找一些关键点，寻找具有特殊几何特征的点，然后把两个指纹的关键点进行比对，判断是否匹配。\n然后是2001年基于Haar的人脸检测算法，在当时的硬件条件下已经能够达到实时人脸检测，我们现在所有手机相机里的人脸检测，都是基于它或者它的变种。\n第三个是基于HoG特征的物体检测，它和所对应的SVM分类器组合起来的就是著名的DPM算法。DPM算法在物体检测上超过了所有的算法，取得了比较不错的成绩。\n但这种成功例子太少了，因为手工设计特征需要大量的经验，需要你对这个领域和数据特别了解，然后设计出来特征还需要大量的调试工作。说白了就是需要一点运气。\n另一个难点在于，你不只需要手工设计特征，还要在此基础上有一个比较合适的分类器算法。同时设计特征然后选择一个分类器，这两者合并达到最优的效果，几乎是不可能完成的任务。\n仿生学角度看深度学习\n如果不手动设计特征，不挑选分类器，有没有别的方案呢？能不能同时学习特征和分类器？即输入某一个模型的时候，输入只是图片，输出就是它自己的标签。比如输入一个明星的头像，出来的标签就是一个50维的向量（如果要在50个人里识别的话），其中对应明星的向量是1，其他的位置是0。\n这种设定符合人类脑科学的研究成果。\n1981年诺贝尔医学生理学奖颁发给了David Hubel，一位神经生物学家。他的主要研究成果是发现了视觉系统信息处理机制，证明大脑的可视皮层是分级的。他的贡献主要有两个，一是他认为人的视觉功能一个是抽象，一个是迭代。抽象就是把非常具体的形象的元素，即原始的光线像素等信息，抽象出来形成有意义的概念。这些有意义的概念又会往上迭代，变成更加抽象，人可以感知到的抽象概念。\n像素是没有抽象意义的，但人脑可以把这些像素连接成边缘，边缘相对像素来说就变成了比较抽象的概念；边缘进而形成球形，球形然后到气球，又是一个抽象的过程，大脑最终就知道看到的是一个气球。\n模拟人脑识别人脸，也是抽象迭代的过程，从最开始的像素到第二层的边缘，再到人脸的部分，然后到整张人脸，是一个抽象迭代的过程。\n再比如看到图片中的摩托车，我们可能在脑子里就几微秒的时间，但是经过了大量的神经元抽象迭代。对计算机来说最开始看到的根本也不是摩托车，而是RGB图像三个通道上不同的数字。\n所谓的特征或者视觉特征，就是把这些数值给综合起来用统计或非统计的形式，把摩托车的部件或者整辆摩托车表现出来。深度学习的流行之前，大部分的设计图像特征就是基于此，即把一个区域内的像素级别的信息综合表现出来，利于后面的分类学习。\n如果要完全模拟人脑，我们也要模拟抽象和递归迭代的过程，把信息从最细琐的像素级别，抽象到“种类”的概念，让人能够接受。\n卷积的概念\n计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。\n什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。实际上在计算机视觉里面，可以把卷积当做一个抽象的过程，就是把小区域内的信息统计抽象出来。\n比如，对于一张爱因斯坦的照片，我可以学习n个不同的卷积和函数，然后对这个区域进行统计。可以用不同的方法统计，比如着重统计中央，也可以着重统计周围，这就导致统计的和函数的种类多种多样，为了达到可以同时学习多个统计的累积和。\n上图中是，如何从输入图像怎么到最后的卷积，生成的响应map。首先用学习好的卷积和对图像进行扫描，然后每一个卷积和会生成一个扫描的响应图，我们叫response map，或者叫feature map。如果有多个卷积和，就有多个feature map。也就说从一个最开始的输入图像（RGB三个通道）可以得到256个通道的feature map，因为有256个卷积和，每个卷积和代表一种统计抽象的方式。\n在卷积神经网络中，除了卷积层，还有一种叫池化的操作。池化操作在统计上的概念更明确，就是一个对一个小区域内求平均值或者求最大值的统计操作。\n带来的结果是，如果之前我输入有两个通道的，或者256通道的卷积的响应feature map，每一个feature map都经过一个求最大的一个池化层，会得到一个比原来feature map更小的256的feature map。\n在上面这个例子里，池化层对每一个2X2的区域求最大值，然后把最大值赋给生成的feature map的对应位置。如果输入图像是100×100的话，那输出图像就会变成50×50，feature map变成了一半。同时保留的信息是原来2X2区域里面最大的信息。\n操作的实例：LeNet网络\nLe顾名思义就是指人工智能领域的大牛Lecun。这个网络是深度学习网络的最初原型，因为之前的网络都比较浅，它较深的。LeNet在98年就发明出来了，当时Lecun在AT&T的实验室，他用这一网络进行字母识别，达到了非常好的效果。\n怎么构成呢？输入图像是32×32的灰度图，第一层经过了一组卷积和，生成了6个28X28的feature map，然后经过一个池化层，得到得到6个14X14的feature map，然后再经过一个卷积层，生成了16个10X10的卷积层，再经过池化层生成16个5×5的feature map。\n从最后16个5X5的feature map开始，经过了3个全连接层，达到最后的输出，输出就是标签空间的输出。由于设计的是只要对0到9进行识别，所以输出空间是10，如果要对10个数字再加上26个大小字母进行识别的话，输出空间就是62。62维向量里，如果某一个维度上的值最大，它对应的那个字母和数字就是就是预测结果。\n压在骆驼身上的最后一根稻草\n从98年到本世纪初，深度学习兴盛起来用了15年，但当时成果泛善可陈，一度被边缘化。到2012年，深度学习算法在部分领域取得不错的成绩，而压在骆驼身上最后一根稻草就是AlexNet。\nAlexNet由多伦多大学几个科学家开发，在ImageNet比赛上做到了非常好的效果。当时AlexNet识别效果超过了所有浅层的方法。此后，大家认识到深度学习的时代终于来了，并有人用它做其它的应用，同时也有些人开始开发新的网络结构。\n其实AlexNet的结构也很简单，只是LeNet的放大版。输入是一个224X224的图片，是经过了若干个卷积层，若干个池化层，最后连接了两个全连接层，达到了最后的标签空间。\n去年，有些人研究出来怎么样可视化深度学习出来的特征。那么，AlexNet学习出的特征是什么样子？在第一层，都是一些填充的块状物和边界等特征；中间的层开始学习一些纹理特征；更高接近分类器的层级，则可以明显看到的物体形状的特征。\n最后的一层，即分类层，完全是物体的不同的姿态，根据不同的物体展现出不同姿态的特征了。\n可以说，不论是对人脸，车辆，大象或椅子进行识别，最开始学到的东西都是边缘，继而就是物体的部分，然后在更高层层级才能抽象到物体的整体。整个卷积神经网络在模拟人的抽象和迭代的过程。\n为什么时隔20年卷土重来？\n我们不禁要问：似乎卷积神经网络设计也不是很复杂，98年就已经有一个比较像样的雏形了。自由换算法和理论证明也没有太多进展。那为什么时隔20年，卷积神经网络才能卷土重来，占领主流？\n这一问题与卷积神经网络本身的技术关系不太大，我个人认为与其他一些客观因素有关。\n首先，卷积神经网络的深度太浅的话，识别能力往往不如一般的浅层模型，比如SVM或者boosting。但如果做得很深，就需要大量数据进行训练，否则机器学习中的过拟合将不可避免。而2006及2007年开始，正好是互联网开始大量产生各种各样的图片数据的时候。\n另外一个条件是运算能力。卷积神经网络对计算机的运算要求比较高，需要大量重复可并行化的计算，在当时CPU只有单核且运算能力比较低的情况下，不可能进行个很深的卷积神经网络的训练。随着GPU计算能力的增长，卷积神经网络结合大数据的训练才成为可能。\n最后一点就是人和。卷积神经网络有一批一直在坚持的科学家（如Lecun）才没有被沉默，才没有被海量的浅层方法淹没。然后最后终于看到卷积神经网络占领主流的曙光。\n深度学习在视觉上的应用\n计算机视觉中比较成功的深度学习的应用，包括人脸识别，图像问答，物体检测，物体跟踪。\n人脸识别\n这里说人脸识别中的人脸比对，即得到一张人脸，与数据库里的人脸进行比对；或同时给两张人脸，判断是不是同一个人。\n这方面比较超前的是汤晓鸥教授，他们提出的DeepID算法在LWF上做得比较好。他们也是用卷积神经网络，但在做比对时，两张人脸分别提取了不同位置特征，然后再进行互相比对，得到最后的比对结果。最新的DeepID-3算法，在LWF达到了99.53%准确度，与肉眼识别结果相差无几。\n图片问答问题\n这是2014年左右兴起的课题，即给张图片同时问个问题，然后让计算机回答。比如有一个办公室靠海的图片，然后问“桌子后面有什么”，神经网络输出应该是“椅子和窗户”。\n这一应用引入了LSTM网络，这是一个专门设计出来具有一定记忆能力的神经单元。特点是，会把某一个时刻的输出当作下一个时刻的输入。可以认为它比较适合语言等，有时间序列关系的场景。因为我们在读一篇文章和句子的时候，对句子后面的理解是基于前面对词语的记忆。\n图像问答问题是基于卷积神经网络和LSTM单元的结合，来实现图像问答。LSTM输出就应该是想要的答案，而输入的就是上一个时刻的输入，以及图像的特征，及问句的每个词语。\n物体检测问题\nRegion CNN\n深度学习在物体检测方面也取得了非常好的成果。2014年的Region CNN算法，基本思想是首先用一个非深度的方法，在图像中提取可能是物体的图形块，然后深度学习算法根据这些图像块，判断属性和一个具体物体的位置。\n为什么要用非深度的方法先提取可能的图像块？因为在做物体检测的时候，如果你用扫描窗的方法进行物体监测，要考虑到扫描窗大小的不一样，长宽比和位置不一样，如果每一个图像块都要过一遍深度网络的话，这种时间是你无法接受的。\n所以用了一个折中的方法，叫Selective Search。先把完全不可能是物体的图像块去除，只剩2000左右的图像块放到深度网络里面判断。那么取得的成绩是AP是58.5，比以往几乎翻了一倍。有一点不尽如人意的是，region CNN的速度非常慢，需要10到45秒处理一张图片。\nFaster R-CNN方法\n而且我在去年NIPS上，我们看到的有Faster R-CNN方法，一个超级加速版R-CNN方法。它的速度达到了每秒七帧，即一秒钟可以处理七张图片。技巧在于，不是用图像块来判断是物体还是背景，而把整张图像一起扔进深度网络里，让深度网络自行判断哪里有物体，物体的方块在哪里，种类是什么？\n经过深度网络运算的次数从原来的2000次降到一次，速度大大提高了。\nFaster R-CNN提出了让深度学习自己生成可能的物体块，再用同样深度网络来判断物体块是否是背景？同时进行分类，还要把边界和给估计出来。\nFaster R-CNN可以做到又快又好，在VOC2007上检测AP达到73.2，速度也提高了两三百倍。\nYOLO\n去年FACEBOOK提出来的YOLO网络，也是进行物体检测，最快达到每秒钟155帧，达到了完全实时。它让一整张图像进入到神经网络，让神经网络自己判断这物体可能在哪里，可能是什么。但它缩减了可能图像块的个数，从原来Faster R-CNN的2000多个缩减缩减到了98个。\n同时取消了Faster R-CNN里面的RPN结构，代替Selective Search结构。YOLO里面没有RPN这一步，而是直接预测物体的种类和位置。\nYOLO的代价就是精度下降，在155帧的速度下精度只有52.7，45帧每秒时的精度是63.4。\nSSD\n在arXiv上出现的最新算法叫Single Shot MultiBox Detector，即SSD。\n它是YOLO的超级改进版，吸取了YOLO的精度下降的教训，同时保留速度快的特点。它能达到58帧每秒，精度有72.1。速度超过Faster R-CNN 有8倍，但达到类似的精度。\n物体跟踪\n所谓跟踪，就是在视频里面第一帧时锁定感兴趣的物体，让计算机跟着走，不管怎么旋转晃动，甚至躲在树丛后面也要跟踪。\n深度学习对跟踪问题有很显著的效果。DeepTrack算法是我在澳大利亚信息科技研究院时和同事提出的，是第一在线用深度学习进行跟踪的文章，当时超过了其它所有的浅层算法。\n今年有越来越多深度学习跟踪算法提出。去年十二月ICCV 2015上面，马超提出的Hierarchical Convolutional Feature算法，在数据上达到最新的记录。它不是在线更新一个深度学习网络，而是用一个大网络进行预训练，然后让大网络知道什么是物体什么不是物体。\n将大网络放在跟踪视频上面，然后再分析网络在视频上产生的不同特征，用比较成熟的浅层跟踪算法来进行跟踪，这样利用了深度学习特征学习比较好的好处，同时又利用了浅层方法速度较快的优点。效果是每秒钟10帧，同时精度破了记录。\n最新的跟踪成果是基于Hierarchical Convolutional Feature，由一个韩国的科研组提出的MDnet。它集合了前面两种深度算法的集大成，首先离线的时候有学习，学习的不是一般的物体检测，也不是ImageNet，学习的是跟踪视频，然后在学习视频结束后，在真正在使用网络的时候更新网络的一部分。这样既在离线的时候得到了大量的训练，在线的时候又能够很灵活改变自己的网络。\n基于嵌入式系统的深度学习\n回到ADAS问题（慧眼科技的主业），它完全可以用深度学习算法，但对硬件平台有比较高的要求。在汽车上不太可能把一台电脑放上去，因为功率是个问题，很难被市场所接受。\n现在的深度学习计算主要是在云端进行，前端拍摄照片，传给后端的云平台处理。但对于ADAS而言，无法接受长时间的数据传输的，或许发生事故后，云端的数据还没传回来。\n那是否可以考虑NVIDIA推出的嵌入式平台？NVIDIA推出的嵌入式平台，其运算能力远远强过了所有主流的嵌入式平台，运算能力接近主流的顶级CPU，如台式机的i7。那么慧眼科技在做工作就是要使得深度学习算法，在嵌入式平台有限的资源情况下能够达到实时效果，而且精度几乎没有减少。\n具体做法是，首先对网络进行缩减，可能是对网络的结构缩减，由于识别场景不同，也要进行相应的功能性缩减；另外要用最快的深度检测算法，结合最快的深度跟踪算法，同时自己研发出一些场景分析算法。三者结合在一起，目的是减少运算量，减少检测空间的大小。在这种情况下，在有限资源上实现了使用深度学习算法，但精度减少的非常少。\n今年7月，雷锋网(公众号：雷锋网)将在深圳举办“全球人工智能与机器人创新大会”（简称：GAIR）。想了解下，您对人工智能的未来趋势怎么看？\n汉曦：我觉得未来人工智能的方向应该是机器服务人，机器辅助人。人始终是最后决策者。这在技术上和伦理上更合理。"}
{"content2":"在我的理解里，要实现计算机视觉必须有图像处理的帮助，而图像处理倚仗与模式识别的有效运用，而模式识别是人工智能领域的一个重要分支，人工智能与机器学习密不可分。纵观一切关系，发现计算机视觉的应用服务于机器学习。各个环节缺一不可，相辅相成。\n计算机视觉（computervision），用计算机来模拟人的视觉机理获取和处理信息的能力。就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，用电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。\n图像处理（imageprocessing），用计算机对图像进行分析，以达到所需结果的技术。又称影像处理。基本内容图像处理一般指数字图像处理。数字图像是指用数字摄像机、扫描仪等设备经过采样和数字化得到的一个大的二维数组，该数组的元素称为像素，其值为一整数，称为灰度值。图像处理技术的主要内容包括图像压缩，增强和复原，匹配、描述和识别3个部分。常见的处理有图像数字化、图像编码、图像增强、图像复原、图像分割和图像分析等。图像处理一般指数字图像处理。\n模式识别(PatternRecognition)是指对表征事物或现象的各种形式的(数值的、文字的和逻辑关系的)信息进行处理和分析,以对事物或现象进行描述、辨认、分类和解释的过程,是信息科学和人工智能的重要组成部分。模式识别又常称作模式分类，从处理问题的性质和解决问题的方法等角度，模式识别分为有监督的分类（Supervised Classification）和无监督的分类(UnsupervisedClassification)两种。模式还可分成抽象的和具体的两种形式。前者如意识、思想、议论等,属于概念识别研究的范畴,是人工智能的另一研究分支。我们所指的模式识别主要是对语音波形、地震波、心电图、脑电图、图片、照片、文字、符号、生物传感器等对象的具体模式进行辨识和分类。模式识别研究主要集中在两方面,一是研究生物体(包括人)是如何感知对象的，属于认识科学的范畴,二是在给定的任务下,如何用计算机实现模式识别的理论和方法。应用计算机对一组事件或过程进行辨识和分类，所识别的事件或过程可以是文字、声音、图像等具体对象，也可以是状态、程度等抽象对象。这些对象与数字形式的信息相区别，称为模式信息。模式识别与统计学、心理学、语言学、计算机科学 、生物学、控制论等都有关系。它与 人工智能 、 图像处理的研究有交叉关系。\n机器学习(MachineLearning)是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习在人工智能的研究中具有十分重要的地位。一个不具有学习能力的智能系统难以称得上是一个真正的智能系统，但是以往的智能系统都普遍缺少学习的能力。随着人工智能的深入发展，这些局限性表现得愈加突出。正是在这种情形下，机器学习逐渐成为人工智能研究的核心之一。它的应用已遍及人工智能的各个分支，如专家系统、自动推理、自然语言理解、模式识别、计算机视觉、智能机器人等领域。机器学习的研究是根据生理学、认知科学等对人类学习机理的了解，建立人类学习过程的计算模型或认识模型，发展各种学习理论和学习方法，研究通用的学习算法并进行理论上的分析，建立面向任务的具有特定应用的学习系统。这些研究目标相互影响相互促进。\n人类研究计算机的目的，是为了提高社会生产力水平，提高生活质量，把人从单调复杂甚至危险的工作中解救出来。今天的计算机在计算速度上已经远远超过了人，然而在很多方面，特别是在人类智能活动有关的方面例如在视觉功能、听觉功能、嗅觉功能、自然语言理解能力功能等等方面，还不如人。\n这种现状无法满足一些高级应用的要求。例如，我们希望计算机能够及早地发现路上的可疑情况并提醒汽车驾驶员以避免发生事故，我们更希望计算机能帮助我们进行自动驾驶，目前的技术还不足以满足诸如此类高级应用的要求，还需要更多的人工智能研究成果和系统实现的经验。\n什么是人工智能呢？人工智能，是由人类设计并在计算机环境下实现的模拟或再现某些人智能行为的技术。一般认为，人类智能活动可以分为两类：感知行为与思维活动。模拟感知行为的人工智能研究的一些例子包括语音识别、话者识别等与人类的听觉功能有关的“计算机听觉”，物体三维表现的形状知识、距离、速度感知等与人类视觉有关的“计算机视觉”，等等。模拟思维活动的人工智能研究的例子包括符号推理、模糊推理、定理证明等与人类思维有关的“计算机思维”，等等。\n从图像处理和模式识别发展起来的计算机视觉研究对象之一是如何利用二维投影图像恢复三维景物世界。计算机视觉使用的理论方法主要是基于几何、概率和运动学计算与三维重构的视觉计算理论，它的基础包括射影几何学、刚体运动力学、概率论与随机过程、图像处理、人工智能等理论。计算机视觉要达到的基本目的有以下几个：\n(1)      根据一幅或多幅二维投影图像计算出观察点到目标物体的距离；\n(2)      根据一幅或多幅二维投影图像计算出目标物体的运动参数；\n(3)      根据一幅或多幅二维投影图像计算出目标物体的表面物理特性；\n(4)      根据多幅二维投影图像恢复出更大空间区域的投影图像。\n计算机视觉要达到的最终目的是实现利用计算机对于三维景物世界的理解，即实现人的视觉系统的某些功能。\n在计算机视觉领域里，医学图像分析、光学文字识别对模式识别的要求需要提到一定高度。又如模式识别中的预处理和特征抽取环节应用图像处理的技术；图像处理中的图像分析也应用模式识别的技术。在计算机视觉的大多数实际应用当中，计算机被预设为解决特定的任务，然而基于机器学习的方法正日渐普及，一旦机器学习的研究进一步发展，未来“泛用型”的电脑视觉应用或许可以成真。\n人工智能所研究的一个主要问题是：如何让系统具备“计划”和“决策能力”？从而使之完成特定的技术动作（例如：移动一个机器人通过某种特定环境）。这一问题便与计算机视觉问题息息相关。在这里，计算机视觉系统作为一个感知器，为决策提供信息。另外一些研究方向包括模式识别和机器学习（这也隶属于人工智能领域，但与计算机视觉有着重要联系），也由此，计算机视觉时常被看作人工智能与计算机科学的一个分支。\n机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演译。\n为了达到计算机视觉的目的，有两种技术途径可以考虑。第一种是仿生学方法，即从分析人类视觉的过程入手，利用大自然提供给我们的最好参考系——人类视觉系统，建立起视觉过程的计算模型，然后用计算机系统实现之。第二种是工程方法，即脱离人类视觉系统框框的约束，利用一切可行和实用的技术手段实现视觉功能。此方法的一般做法是，将人类视觉系统作为一个黑盒子对待，实现时只关心对于某种输入，视觉系统将给出何种输出。这两种方法理论上都是可以使用的，但面临的困难是，人类视觉系统对应某种输入的输出到底是什么，这是无法直接测得的。而且由于人的智能活动是一个多功能系统综合作用的结果，即使是得到了一个输入输出对，也很难肯定它是仅由当前的输入视觉刺激所产生的响应，而不是一个与历史状态综合作用的结果。\n不难理解，计算机视觉的研究具有双重意义。其一，是为了满足人工智能应用的需要，即用计算机实现人工的视觉系统的需要。这些成果可以安装在计算机和各种机器上，使计算机和机器人能够具有“看”的能力。其二，视觉计算模型的研究结果反过来对于我们进一步认识和研究人类视觉系统本身的机理，甚至人脑的机理，也同样具有相当大的参考意义。"}
{"content2":"微软创始人比尔盖茨认为，IT界下一个大事件是计算机视觉，以及与深度学习的结合。不管是计算机视觉，还是深度学习，都属于人工智能（Artificial Intelligence）。人工智能应该是一个最老的术语了，同时也是最含糊的。它在过去50年里经历了几度兴衰。当你遇到一个说自己是做人工智能的人，你可以有两种选择：要么摆个嘲笑的表情，要么抽出一张纸，记录下他所说的一切。\n一、人工智能（AI）的基本内容\n（1）AI的一般解释：人工智能就是用人工的方法在机器（计算机）上实现的智能，或称机器智能。\n（2）智能的层次结构\n高层智能：以大脑皮层（抑制中枢）为主，主要完成记忆、思维等活动。\n中层智能：以丘脑（感觉中枢）为主，主要完成感知活动。\n低层智能：以小脑、脊髓为主，主要完成动作反应活动。\n1、人工智能的主要研究、应用领域\n机器感知：机器视觉；机器听觉；自然语言理解；机器翻译\n机器思维：机器推理\n机器学习：符号学习；连接学习\n机器行为：智能控制\n智能应用：智能机器：智能机器人；机器智能\n博弈；自动定理证明；自动程序设计\n专家系统；智能决策；智能检索；智能CAD；智能CAI\n智能交通；智能电力；智能产品；智能建筑；等\n2、人工智能新技术\n计算智能：神经计算；模糊计算；进化计算；自然计算\n人工生命：人工脑；细胞自动机\n分布智能：多Agent ,  群体智能\n数据挖掘：知识发现；数据挖掘\n二、智能模拟的方法和技术研究\n1、机器感知：就是要让计算机具有类似于人的感知能力，如视觉、听觉、触觉、嗅觉、味觉。\n（1）机器视觉（或叫计算机视觉）：就是给计算机配上能看的视觉器官，如摄像机等，使它可以识别并理解文                   字、图像、景物等。\n（2）机器听觉（或叫计算机听觉）：就是给计算配上能听的听觉器官，如话筒等，使计算机能够识别并理解语                   言、声音等。\n机器感知相当于智能系统的输入部分。\n机器感知的专门的研究领域：计算机视觉、模式识别、自然语言理解、语音识别。\n2、机器思维：让计算机能够对感知到的外界信息和自己产生的内部信息进行思维性加工。\n包括：推理、搜索、规划等方面的研究。\n推理的概念：推理是指按照某种策略从已知事实出发利用知识推出所需结论的过程。\n搜索的概念：是指为了达到某一目标，不断寻找推理线路，以引导和控制推理，使问题得以解决的过程。\n规划的概念：是指从某个特定问题状态出发，寻找并建立一个操作序列，直到求得目标状态为止的一个行动过                                  程的描述。\n3、机器学习：让计算机能够像人那样自动地获取新知识，并在实践中不断地完善自我和增强能力。\n机器学习是机器获取知识的根本途径，同时也是机器具有智能的重要标志。\n机器学习方法：机械学习、类比学习、归纳学习、发现学习、遗传学习和连接学习等。\n4、机器行为：让计算机能够具有像人那样地行动和表达能力，如走、跑、拿、说、唱、写画等。\n相当于智能系统的输出部分。\n（1）智能控制：是指那种无需或需要尽可能少的人工干预就能独立的驱动智能机器实现其目标的控制过程。                                         它是人工智能技术与传统自动控制技术相结合的产物。\n（2）智能检索：是指利用人工智能的方法从大量信息中尽快找到所需要的信息或知识。\n5、智能系统与智能机器\n无论是人工智能的近期目标还是远期目标，都需要建立智能系统或构造智能机器。\n需要开展对系统模型、构造技术、构造工具及语言环境等研究。\n三、AI研究中的不同学派\n1、符号主义：功能模拟\n构造能够模拟大脑功能的智能系统。相当于“鸟飞”\n2、连接主义：结构模拟\n构造模拟大脑结构的神经网络系统。相当于“飞鸟”\n3、行为主义：行为模拟\n构造具有进化能力的智能系统。相当于“由猿到人”\n以上就是对AI基本面的一个概述，然而AI所包含的、所涉及的，远远超过这些。互联网，大数据也会在AI的发展\n中起到推进的作用。送给大家一篇文章《斯坦福首席人工智能科学家:如何教计算机\"看懂一张图\"？》点击打开链接\n文章导读：一位三岁的小女孩对于这个世界也许还有很多东西要学习，但是在一个重要的任务上她已经是专家了：那就是理解并描述她所看到的东西。尽管我们目前的科技取得了前所未有的进步，但是，哪怕最先进的机器和电脑，依然会在这个问题上犯难。斯坦福大学首席人工智能科学家李菲菲认为，解决这个问题将是实现人工智能的第一步。"}
{"content2":"本篇文章中有计算机视觉，图像处理，人工智能的大作业，具体有哪些看下面图片\n大的功能有边缘检测，图像去噪，PCA降维，神经网路\n然后边缘检测，图像去噪中有各种算子，是用C写的。\n\nPCA降维\n神经网络 包含3个. 分别是手写字体识别，车牌识别，人脸识别。\n下载链接 :  https://download.csdn.net/download/qq_39263663/10438168"}
{"content2":"计算机视觉是一门研究如何使机器“看”的科学，更进一步的说，就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。作为一个科学学科，计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。这里所指的信息指Shannon定义的，可以用来帮助做一个“决定”的信息。因为感知可以看作是从感官信号中提取信息，所以计算机视觉也可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学。\n定义\n计算机视觉是使用计算机及相关设备对生物视觉的一种模拟。它的主要任务就是通过对采集的图片或视频进行处理以获得相应场景的三维信息，就像人类和许多其他类生物每天所做的那样。\n计算机视觉是一门关于如何运用照相机和计算机来获取我们所需的，被拍摄对象的数据与信息的学问。形象地说，就是给计算机安装上眼睛（照相机）和大脑（算法），让计算机能够感知环境。我们中国人的成语\"眼见为实\"和西方人常说的\"One picture is worth ten thousand words\"表达了视觉对人类的重要性。不难想象，具有视觉的机器的应用前景能有多么地宽广。\n计算机视觉既是工程领域，也是科学领域中的一个富有挑战性重要研究领域。计算机视觉是一门综合性的学科，它已经吸引了来自各个学科的研究者参加到对它的研究之中。其中包括计算机科学和工程、信号处理、物理学、应用数学和统计学，神经生理学和认知科学等。\n解析\n视觉是各个应用领域，如制造业、检验、文档分析、医疗诊断，和军事等领域中各种智能/自主系统中不可分割的一部分。由于它的重要性，一些先进国家，例如美国把对计算机视觉的\n计算机视觉与其他领域的关系\n研究列为对经济和科学有广泛影响的科学和工程中的重大基本问题，即所谓的重大挑战（grandchallenge）。计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。作为一门学科，计算机视觉开始于60年代初，但在计算机视觉的基本研究中的许多重要进展是在80年代取得的。计算机视觉与人类视觉密切相关，对人类视觉有一个正确的认识将对计算机视觉的研究非常有益。为此我们将先介绍人类视觉。\n原理\n计算机视觉就是用各种成象系统代替视觉器官作为输入敏感手段，由计算机来代替大脑完成处理和解释。计算机视觉的最终研究目标就是使计算机能象人那样通过视觉观察和理解世界，具有自主适应环境的能力。要经过长期的努力才能达到的目标。因此，在实现最终目标以前，人们努力的中期目标是建立一种视觉系统，这个系统能依据视觉敏感和反馈的某种程度的智能完成一定的任务。例如，计算机视觉的一个重要应用领域就是自主车辆的视觉导航，还没有条件实现象人那样能识别和理解任何环境，完成自主导航的系统。因此，人们努力的研究目标是实现在高速公路上具有道路跟踪能力，可避免与前方车辆碰撞的视觉辅助驾驶系统。这里要指出的一点是在计算机视觉系统中计算机起代替人脑的作用，但并不意味着计算机必须按人类视觉的方法完成视觉信息的处理。计算机视觉可以而且应该根据计算机系统的特点来进行视觉信息的处理。但是，人类视觉系统是迄今为止，人们所知道的功能最强大和完善的视觉系统。如在以下的章节中会看到的那样，对人类视觉处理机制的研究将给计算机视觉的研究提供启发和指导。因此，用计算机信息处理的方法研究人类视觉的机理，建立人类视觉的计算理论，也是一个非常重要和信人感兴趣的研究领域。这方面的研究被称为计算视觉（ComputationalVision）。计算视觉可被认为是计算机视觉中的一个研究领域。\n相关\n有不少学科的研究目标与计算机视觉相近或与此有关。这些学科中包括图像处理、模式识别或图像识别、景物分析、图象理解等。计算机视觉包括图像处理和模式识别，除此之外，它还包括空间形状的描述，几何建模以及认识过程。[1]  实现图像理解是计算机视觉的终极目标。[2]\n图像处理\n图像处理技术把输入图像转换成具有所希望特性的另一幅图像。例如，可通过处理使输出图象有较高的信-噪比，或通过增强处理突出图象的细节，以便于操作员的检验。在计算机视觉研究中经常利用图象处理技术进行预处理和特征抽取。\n模式识别\n模式识别技术根据从图象抽取的统计特性或结构信息，把图像分成予定的类别。例如，文字识别或指纹识别。在计算机视觉中模式识别技术经常用于对图象中的某些部分，例如分割区域的识别和分类。\n图像理解\n给定一幅图像，图象理解程序不仅描述图象本身，而且描述和解释图象所代表的景物，以便对图像代表的内容作出决定。在人工智能视觉研究的初期经常使用景物分析这个术语，以强调二维图象与三维景物之间的区别。图象理解除了需要复杂的图象处理以外还需要具有关于景物成象的物理规律的知识以及与景物内容有关的知识。\n在建立计算机视觉系统时需要用到上述学科中的有关技术，但计算机视觉研究的内容要比这些学科更为广泛。计算机视觉的研究与人类视觉的研究密切相关。为实现建立与人的视觉系统相类似的通用计算机视觉系统的目标需要建立人类视觉的计算机理论。\n现状\n计算机视觉领域的突出特点是其多样性与不完善性。这一领域的先驱可追溯到更早的时候，但是直到20世纪70年代后期，当计算机的性能提高到足以处理诸如图像这样的大规模数据时，计算机视觉才得到了正式的关注和发展。然而这些发展往往起源于其他不同领域的需要，因而何谓“计算机视觉问题”始终没有得到正式定义，很自然地，“计算机视觉问题”应当被如何解决也没有成型的公式。\n尽管如此，人们已开始掌握部分解决具体计算机视觉任务的方法，可惜这些方法通常都仅适用于一群狭隘的目标（如：脸孔、指纹、文字等），因而无法被广泛地应用于不同场合。\n对这些方法的应用通常作为某些解决复杂问题的大规模系统的一个组成部分（例如医学图像的处理，工业制造中的质量控制与测量）。在计算机视觉的大多数实际应用当中，计算机被预设为解决特定的任务，然而基于机器学习的方法正日渐普及，一旦机器学习的研究进一步发展，未来“泛用型”的电脑视觉应用或许可以成真。\n人工智能所研究的一个主要问题是：如何让系统具备“计划”和“决策能力”？从而使之完成特定的技术动作（例如：移动一个机器人通过某种特定环境）。这一问题便与计算机视觉问题息息相关。在这里，计算机视觉系统作为一个感知器，为决策提供信息。另外一些研究方向包括模式识别和机器学习（这也隶属于人工智能领域，但与计算机视觉有着重要联系），也由此，计算机视觉时常被看作人工智能与计算机科学的一个分支。\n物理是与计算机视觉有着重要联系的另一领域。\n计算机视觉关注的目标在于充分理解电磁波——主要是可见光与红外线部分——遇到物体表面被反射所形成的图像，而这一过程便是基于光学物理和固态物理，一些尖端的图像感知系统甚至会应用到量子力学理论，来解析影像所表示的真实世界。同时，物理学中的很多测量难题也可以通过计算机视觉得到解决，例如流体运动。也由此，计算机视觉同样可以被看作是物理学的拓展。\n另一个具有重要意义的领域是神经生物学，尤其是其中生物视觉系统的部分。\n在整个20世纪中，人类对各种动物的眼睛、神经元、以及与视觉刺激相关的脑部组织都进行了广泛研究，这些研究得出了一些有关“天然的”视觉系统如何运作的描述（尽管仍略嫌粗略），这也形成了计算机视觉中的一个子领域——人们试图建立人工系统，使之在不同的复杂程度上模拟生物的视觉运作。同时计算机视觉领域中，一些基于机器学习的方法也有参考部分生物机制。\n计算机视觉的另一个相关领域是信号处理。很多有关单元变量信号的处理方法，尤其是对时变信号的处理，都可以很自然的被扩展为计算机视觉中对二元变量信号或者多元变量信号的处理方法。但由于图像数据的特有属性，很多计算机视觉中发展起来的方法，在单元信号的处理方法中却找不到对应版本。这类方法的一个主要特征，便是他们的非线性以及图像信息的多维性，以上二点作为计算机视觉的一部分，在信号处理学中形成了一个特殊的研究方向。\n除了上面提到的领域，很多研究课题同样可被当作纯粹的数学问题。例如，计算机视觉中的很多问题，其理论基础便是统计学，最优化理论以及几何学。\n如何使既有方法通过各种软硬件实现，或说如何对这些方法加以修改，而使之获得合理的执行速度而又不损失足够精度，是现今电脑视觉领域的主要课题。\n应用\n人类正在进入信息时代，计算机将越来越广泛地进入几乎所有领域。一方面是更多未经计算机专业训练的人也需要应用计算机，而另一方面是计算机的功能越来越强，使用方法越来越复杂。这就使人在进行交谈和通讯时的灵活性与在使用计算机时所要求的严格和死板之间产生了尖锐的矛盾。人可通过视觉和听觉，语言与外界交换信息，并且可用不同的方式表示相同的含义，而计算机却要求严格按照各种程序语言来编写程序，只有这样计算机才能运行。为使更多的人能使用复杂的计算机，必须改变过去的那种让人来适应计算机，来死记硬背计算机的使用规则的情况。而是反过来让计算机来适应人的习惯和要求，以人所习惯的方式与人进行信息交换，也就是让计算机具有视觉、听觉和说话等能力。这时计算机必须具有逻辑推理和决策的能力。具有上述能力的计算机就是智能计算机。\n智能计算机不但使计算机更便于为人们所使用，同时如果用这样的计算机来控制各种自动化装置特别是智能机器人，就可以使这些自动化系统和智能机器人具有适应环境，和自主作出决策的能力。这就可以在各种场合取代人的繁重工作，或代替人到各种危险和恶劣环境中完成任务。\n应用范围从任务，比如工业机器视觉系统，比方说，检查瓶子上的生产线加速通过，研究为人工智能和计算机或机器人，可以理解他们周围的世界。计算机视觉和机器视觉领域有显著的重叠。计算机视觉涉及的被用于许多领域自动化图像分析的核心技术。机器视觉通常指的是结合自动图像分析与其他方法和技术，以提供自动检测和机器人指导在工业应用中的一个过程。在许多计算机视觉应用中，计算机被预编程，以解决特定的任务，但基于学习的方法现在正变得越来越普遍。计算机视觉应用的实例包括用于系统：\n（1）控制过程，比如，一个工业机器人；\n（2）导航，例如，通过自主汽车或移动机器人；\n（3）检测的事件，如，对视频监控和人数统计；\n（4）组织信息，例如，对于图像和图像序列的索引数据库；\n（5）造型对象或环境，如，医学图像分析系统或地形模型；\n（6）相互作用，例如，当输入到一个装置，用于计算机人的交互；\n（7）自动检测，例如，在制造业的应用程序。\n其中最突出的应用领域是医疗计算机视觉和医学图像处理。这个区域的特征的信息从图像数据中提取用于使患者的医疗诊断的目的。通常，图像数据是在形式显微镜图像，X射线图像，血管造影图像，超声图像和断层图像。的信息，可以从这样的图像数据中提取的一个例子是检测的肿瘤，动脉粥样硬化或其他恶性变化。它也可以是器官的尺寸，血流量等。这种应用领域还支持通过提供新的信息，医学研究的测量例如，对脑的结构，或约医学治疗的质量。计算机视觉在医疗领域的应用还包括增强是由人类的解释，例如超声图像或X射线图像，以降低噪声的影响的图像。\n第二个应用程序区域中的计算机视觉是在工业，有时也被称为机器视觉，在那里信息被提取为支撑的制造工序的目的。一个例子是质量控制，其中的信息或最终产品被以找到缺陷自动检测。另一个例子是，被拾取的位置和细节取向测量由机器人臂。机器视觉也被大量用于农业的过程，从散装材料，这个过程被称为去除不想要的东西，食物的光学分拣。\n军事上的应用很可能是计算机视觉最大的地区之一。最明显的例子是探测敌方士兵或车辆和导弹制导。更先进的系统为导弹制导发送导弹的区域，而不是一个特定的目标，并且当导弹到达基于本地获取的图像数据的区域的目标做出选择。现代军事概念，如“战场感知”，意味着各种传感器，包括图像传感器，提供了丰富的有关作战的场景，可用于支持战略决策的信息。在这种情况下，数据的自动处理，用于减少复杂性和融合来自多个传感器的信息，以提高可靠性。\n一个较新的应用领域是自主车，其中包括潜水，陆上车辆（带轮子，轿车或卡车的小机器人），高空作业车和无人机（UAV）。自主化水平，从完全独立的（无人）的车辆范围为汽车，其中基于计算机视觉的系统支持驱动程序或在不同情况下的试验。完全自主的汽车通常使用计算机视觉进行导航时，即知道它在哪里，或用于生产的环境（地图SLAM）和用于检测障碍物。它也可以被用于检测特定任务的特定事件，例如，一个UAV寻找森林火灾。支承系统的例子是障碍物警报系统中的汽车，以及用于飞行器的自主着陆系统。数家汽车制造商已经证明了系统的汽车自动驾驶，但该技术还没有达到一定的水平，就可以投放市场。有军事自主车型，从先进的导弹，无人机的侦察任务或导弹的制导充足的例子。太空探索已经正在使用计算机视觉，自主车比如，美国宇航局的火星探测漫游者和欧洲航天局的ExoMars火星漫游者。\n其他应用领域包括：\n（1）支持视觉特效制作的电影和广播，例如，摄像头跟踪（运动匹配）。\n（2）监视。\n异同\n计算机视觉，图象处理，图像分析，机器人视觉和机器视觉是彼此紧密关联的学科。如果你翻开带有上面这些名字的教材，你会发现在技术和应用领域上他们都有着相当大部分的重叠。这表明这些学科的基础理论大致是相同的，甚至让人怀疑他们是同一学科被冠以不同的名称。\n然而，各研究机构，学术期刊，会议及公司往往把自己特别的归为其中某一个领域，于是各种各样的用来区分这些学科的特征便被提了出来。下面将给出一种区分方法，尽管并不能说这一区分方法完全准确。\n计算机视觉的研究对象主要是映射到单幅或多幅图像上的三维场景，例如三维场景的重建。计算机视觉的研究很大程度上针对图像的内容。\n图象处理与图像分析的研究对象主要是二维图像，实现图像的转化，尤其针对像素级的操作，例如提高图像对比度，边缘提取，去噪声和几何变换如图像旋转。这一特征表明无论是图像处理还是图像分析其研究内容都和图像的具体内容无关。\n机器视觉主要是指工业领域的视觉研究，例如自主机器人的视觉，用于检测和测量的视觉。这表明在这一领域通过软件硬件，图像感知与控制理论往往与图像处理得到紧密结合来实现高效的机器人控制或各种实时操作。\n模式识别使用各种方法从信号中提取信息，主要运用统计学的理论。此领域的一个主要方向便是从图像数据中提取信息。\n还有一个领域被称为成像技术。这一领域最初的研究内容主要是制作图像，但有时也涉及到图像分析和处理。例如，医学成像就包含大量的医学领域的图像分析。\n对于所有这些领域，一个可能的过程是你在计算机视觉的实验室工作，工作中从事着图象处理，最终解决了机器视觉领域的问题，然后把自己的成果发表在了模式识别的会议上。\n问题\n几乎在每个计算机视觉技术的具体应用都要解决一系列相同的问题。这些经典的问题包括：\n识别\n一个计算机视觉，图像处理和机器视觉所共有的经典问题便是判定一组图像数据中是否包含某个特定的物体，图像特征或运动状态。这一问题通常可以通过机器自动解决，但是到目前为止，还没有某个单一的方法能够广泛的对各种情况进行判定：在任意环境中识别任意物体。现有技术能够也只能够很好地解决特定目标的识别，比如简单几何图形识别，人脸识别，印刷或手写文件识别或者车辆识别。而且这些识别需要在特定的环境中，具有指定的光照，背景和目标姿态要求。\n广义的识别在不同的场合又演化成了几个略有差异的概念：\n识别（狭义的）：对一个或多个经过预先定义或学习的物体或物类进行辨识，通常在辨识过程中还要提供他们的二维位置或三维姿态。\n鉴别：识别辨认单一物体本身。例如：某一人脸的识别，某一指纹的识别。\n监测：从图像中发现特定的情况内容。例如：医学中对细胞或组织不正常技能的发现，交通监视仪器对过往车辆的发现。监测往往是通过简单的图象处理发现图像中的特殊区域，为后继更复杂的操作提供起点。\n识别的几个具体应用方向：\n基于内容的图像提取：在巨大的图像集合中寻找包含指定内容的所有图片。被指定的内容可以是多种形式，比如一个红色的大致是圆形的图案，或者一辆自行车。在这里对后一种内容的寻找显然要比前一种更复杂，因为前一种描述的是一个低级直观的视觉特征，而后者则涉及一个抽象概念（也可以说是高级的视觉特征），即‘自行车’，显然的一点就是自行车的外观并不是固定的。\n姿态评估：对某一物体相对于摄像机的位置或者方向的评估。例如：对机器臂姿态和位置的评估。\n光学字符识别对图像中的印刷或手写文字进行识别鉴别，通常的输出是将之转化成易于编辑的文档形式。\n运动\n基于序列图像的对物体运动的监测包含多种类型，诸如：\n自体运动：监测摄像机的三维刚性运动。\n图像跟踪：跟踪运动的物体。\n场景重建\n给定一个场景的二或多幅图像或者一段录像，场景重建寻求为该场景建立一个计算机模型/三维模型。最简单的情况便是生成一组三维空间中的点。更复杂的情况下会建立起完整的三维表面模型。\n图像恢复\n图像恢复的目标在于移除图像中的噪声，例如仪器噪声，模糊等。\n系统\n计算机视觉系统的结构形式很大程度上依赖于其具体应用方向。有些是独立工作的，用于解决具体的测量或检测问题；也有些作为某个大型复杂系统的组成部分出现，比如和机械控制系统，数据库系统，人机接口设备协同工作。计算机视觉系统的具体实现方法同时也由其功能决定——是预先固定的抑或是在运行过程中自动学习调整。尽管如此，有些功能却几乎是每个计算机系统都需要具备的：\n图像获取\n一幅数字图像是由一个或多个图像感知器产生，这里的感知器可以是各种光敏摄像机，包括遥感设备，X射线断层摄影仪，雷达，超声波接收器等。取决于不同的感知器，产生的图片可以是普通的二维图像，三维图组或者一个图像序列。图片的像素值往往对应于光在一个或多个光谱段上的强度（灰度图或彩色图），但也可以是相关的各种物理数据，如声波，电磁波或核磁共振的深度，吸收度或反射度。\n预处理\n在对图像实施具体的计算机视觉方法来提取某种特定的信息前，一种或一些预处理往往被采用来使图像满足后继方法的要求。例如：\n二次取样保证图像坐标的正确；\n平滑去噪来滤除感知器引入的设备噪声；\n提高对比度来保证实现相关信息可以被检测到；\n调整尺度空间使图像结构适合局部应用。\n特征提取\n从图像中提取各种复杂度的特征。例如：\n线，边缘提取；\n局部化的特征点检测如边角检测，斑点检测；\n更复杂的特征可能与图像中的纹理形状或运动有关。\n检测分割\n在图像处理过程中，有时会需要对图像进行分割来提取有价值的用于后继处理的部分，例如\n筛选特征点；\n分割一或多幅图片中含有特定目标的部分。\n高级处理\n到了这一步，数据往往具有很小的数量，例如图像中经先前处理被认为含有目标物体的部分。这时的处理包括：\n验证得到的数据是否符合前提要求；\n估测特定系数，比如目标的姿态，体积；\n对目标进行分类。\n高级处理有理解图像内容的含义，是计算机视觉中的高阶处理，主要是在图像分割的基础上再经行对分割出的图像块进行理解，例如进行识别等操作。\n要件\n光源布局影响大需审慎考量。\n正确的选择镜组，考量倍率、空间、尺寸、失真…。\n选择合适的摄影机(CCD)，考量功能、规格、稳定性、耐用...。\n视觉软件开发需靠经验累积，多尝试、思考问题的解决途径。\n以创造精度的不断提升，缩短处理时间为最终目标。\nend。\n会议\n顶级\nICCV：InternationalConference on Computer Vision，国际计算机视觉大会\nCVPR：InternationalConference on Computer Vision and Pattern Recognition，国际计算机视觉与模式识别大会\nECCV：EuropeanConference on Computer Vision，欧洲计算机视觉大会\n较好\nICIP：InternationalConference on Image Processing，国际图像处理大会\nBMVC：BritishMachine Vision Conference，英国机器视觉大会\nICPR：InternationalConference on Pattern Recognition，国际模式识别大会\nACCV：Asian Conferenceon Computer Vision，亚洲计算机视觉大会\n期刊\n顶级\nPAMI：IEEETransactions on Pattern Analysis and Machine Intelligence，IEEE模式分析与机器智能杂志\nIJCV：InternationalJournal on Computer Vision，国际计算机视觉杂志\n较好\nTIP:IEEE Transactions on Image Processing，IEEE图像处理杂志\nCVIU：Computer Vision and ImageUnderstanding，计算机视觉与图像理解\nPR：Pattern Recognition，模式识别\nPRL：Pattern Recognition Letters，模式识别快报\n参考资料\n1.  D. Vernon．Machine vision-Automated visual inspection and robot vision：Englewood Cliffs, NJ (US); Prentice Hall，1991：2\n2.  T. F.Cootes, C. J. Taylor．Statistical models of appearance for computervision：World Wide Web Publication，2004"}
{"content2":"运动目标跟踪在军事制导，视觉导航，机器人，智能交通，公共安全等领域有着广泛的应用。例如，在车辆违章抓拍系统中，车辆的跟踪就是必不可少的。在入侵检测中，人、动物、车辆等大型运动目标的检测与跟踪也是整个系统运行的关键所在。所以，在计算机视觉领域目标跟踪是一个很重要的分支。\n运动目标检测是运动目标跟踪的前提；运动目标检测，依据目标与摄像机之间的关系可以分为静态背景下的运动检测与动态背景下的运动检测。\n一，静态背景下的运动检测：整个监控过程中只有目标在运动；主要包括以下几种方法。\n1，背景差分法；整个监控过程中，需要不停地维护一个“纯背景”。对于任意一帧监控画面而言，将其与纯背景进行差分，从而得到出现在当前画面中的运动目标。该方法对光照变化、天气、背景变化比较敏感。而且，需要不停进行地依靠学习来维护一个纯背景画面。此外，背景的维护和更新，阴影去除等对运动目标的检测至关重要。\n2，帧间差分法；通过相邻帧之间的差值计算，来获得运动目标的位置、形状等信息的方法。该方法对光照的适应能力很强，但由于运动目标像素上的相似性，从而不能完整地检测出运动目标。需要提醒的是，有研究人员将相邻帧间的差分进行改进，得到三帧差分方法。即，利用相邻三帧之间的差值计算，来进行运动目标的检测。该方法经很多研究人员和工程师的实际测试，证明了其在特定环境中优良的性能。\n3，光流法；在空间中，运动可以用运动场描述；而在一个图像平面上，物体的运动往往是通过图像序列中图像灰度分布的不同来体现, 从而使空间中的运动场转移到图像上就表示为光流场。光流场反映了图像上每一点灰度的变化趋势。它可看成是带有灰度的像素点在图像平面上运动而产生的瞬时速度场,也是一种对真实运动场的近似估计。在比较理想的情况下，它能够检测独立运动的对象而不需要预先知道场景的任何信息, 可以很精确地计算出运动物体的速度，并且可用于动态场景的情况。但是大多数光流方法的计算相当复杂, 对硬件要求比较高, 不适于实时处理, 而且对噪声比较敏感,抗噪性差。\n二，动态背景下的运动检测：监控过程中，目标和背景都在发生运动或变化；在运动目标检测的应用环境中，动态背景相比而言更加复杂。根据相机的运动形式，可以分为以下两种：\n1，相机支架固定；但相机可以随着云台的运动而发生旋转，倾斜等运动。另外，相机也可以根据远程计算机指令来控制镜头调焦，从而产生远景和近景缩放运动。\n2，相机置于移动设备之上（例如，车载相机）；\n对于以上两种相机运动形式的任意一种而言，在进行运动目标检测之前，都需要根据一定的方法进行全局运动估计与补偿。通常，可以利用块匹配法、特征点匹配法等进行运动量的估计。当然，也可以利用光流法建立光流场模型，利用光流方程求解图像像素点的运动速度。\n运动目标跟踪就是在一个连续视频序列中，在每一帧监控画面中找到感兴趣的运动目标（例如，车辆，行人，动物等）。跟踪可以大致分为以下几个步骤：\n（1）目标的有效描述；目标的跟踪过程跟目标检测一样，需要对其进行有效的描述，即，需要提取目标的特征，从而能够表达该目标；一般来说，我们可以通过图像的边缘、轮廓、形状、纹理、区域、直方图、矩特征、变换系数等来进行目标的特征描述；\n（2）相似性度量计算；常用的方法有：欧式距离、马氏距离、棋盘距离、加权距离、相似系数、相关系数等；\n（3）目标区域搜索匹配；如果对场景中出现的所有目标都进行特征提取、相似性计算，那么，系统运行所耗费的计算量是很大的。所以，我们通常采用一定的方式对运动目标可能出现的区域进行估计，从而减少冗余，加快目标跟踪的速度；常见的预测算法有：Kalman滤波、粒子滤波、均值漂移等；"}
{"content2":"计算机视觉主要分为四个步骤：图像获取、图像校正、立体匹配和三维重建。其中，立体匹配的目的是在两个或多个对应同一场景的图像中找到匹配点，生成视差图。视差图可以通过一些简单的几何关系转换成深度图，用于三维重建。立体匹配是计算机视觉领域一个瓶颈问题，其结果的好坏直接影响着三维重建的效果。\n立体匹配算法，主要分为特征匹配算法和区域匹配算法。特征匹配算法主要是提取图像特征进行匹配，生成视图。由于只提取局部特征，因此特征匹配算法速度快，但是得到的都是稀疏的视差图，而稀疏的视差图在很多应用中都不适用。\n基于特征的匹配主要提取图像特征点进行匹配，例如边缘、轮廓、直线、角点等，具体步骤如图所示。由于这些特征点不受光照、尺度、旋转等变化影响，因此基于特征的匹配对于图像畸变、遮挡等具有一定的鲁棒性。但是它得到的视差图为稀疏视差图，如果需要像素的视差信息没能在稀疏视差图中得以体现，则需要利用已得到的特征点的视差通过拟合、插值、局部生长法等方法将稀疏视差图转成稠密视差图，来得到需要像素的视差信息。这种通过周围特征点插值计算得到的稠密视差图往往不可靠，丧失了大部分表面细节，使得生成的视差图不可靠。\n区域匹配算法依据是否使用全局搜索可以分为全局匹配算法和局部匹配算法。全局匹配算法是基于像素的，通常将匹配运算构建在一个能量最小化的框架下，然后使用优化算法来最小化或最大化能量函数，得到视差图。比较经典的全局匹配算法有：置信度传播算法（belief propagation）、图割算法（graph cut）等。全局匹配算法得到的视差图较为准确，但运行时间长。局部匹配算法将像素代价聚集在一个支持窗中，然后选择与其相匹配的支持窗。然而，在这个过程中，选择合适的支持窗是一个困难的问题。\n全局匹配将立体匹配关系用一个能量函数表示，如下式所示。能量函数由匹配代价和平滑代价组成，然后使用不同的优化算法来迭代地得到视差图。经典的全局算法有置信度传播算法（belief  propagation）、图割算法(graph  cuts)、动态规划算法（dynamic  programming）等。这些算法虽然能得到比较正确的视差图，但都需要通过多次迭代来获得最终结果，其计算耗时大，效率低。\n2006 年 Yoon 提出了自适应权值算法，固定支持窗的大小，赋予支持窗中每个像素不同的权值。Yoon 算法的性能超过了一般的局部算法，甚至可以和全局算法相媲美，因此得到了很多的关注。\n计算机视觉的目标是从摄像机得到的二维图像中提取三维信息，从而重建三维世界模型。在这个过程中，获得场景中某一物体的深度，即场景中物体各点相对于摄像机的距离，无疑成为了计算机视觉的研究重点。获得深度图的方法可分为被动测距和主动测距。被动测距是指视觉系统接受来自场景发射或反射的光能量，形成有关场景的二维图像，然后在这些二维图像的基础上恢复场景的深度信息。具体实现方法可以使用两个或多个相隔一定距离的照相机同时获取场景图像，也可使用一台照相机在不同空间位置上分别获取两幅或两幅以上的图像。主动测距与被动测距的主要区别在于视觉系统是否是通过增收自身发射的能量来测距，雷达测距系统、激光测距系统则属于主动测距。主动测距的系统投资巨大，成本太高，而被动测距方法简单，并且容易实施，从而得到了广泛的应用。利用被动测距的计算机视觉主要分为四个步骤，如图所示。\n（1）图像获取。一般情况下，人类通过双眼来获得图像，双眼可近似为平行排列，在观察同一场景时，左眼获得左边的场景信息多一些，在左视网膜中的图像偏右；而右眼获得右边场景信息多一些，在右视网膜中的图像偏左。同一场景点在左视网膜上和右视网膜上的图像点位置差异即为视差，也是感知物体深度的重要信息。\n计算机视觉的获取图像的原理与人眼相似，是通过不同位置上的相机来获得不同的图像，左摄像机拍摄的图像称为左图像，右摄像机拍摄的图像称为右图像。左图像得到左边的场景信息多一些，右图像得到右边场景的信息多一些，\n（2）   图像校准。在图像获取过程中，有许多因素会导致图像失真，如成像系统的象差、畸变、带宽有限等造成的图像失真；由于成像器件拍摄姿态和扫描非线性引起的图像几何失真；由于运动模糊、辐射失真、引入噪声等造成的图像失真。\n（3）   立体匹配。在两幅或多幅不同位置下拍摄的且对应同一场景的图像中，建立匹配基元之间关系的过程称为立体匹配。例如，在双目立体匹配中，匹配基元选择像素，然后获得对应于同一个场景的两个图像中两个匹配像素的位置差别，即视差。并将视差按比例转换到0－255 之间，以灰度图的形式显示出来，即为视差图。\n（4）三维重建。根据立体匹配得到的像素的视差，如果已知照相机的内外参数，则根据摄像机几何关系得到得到场景中物体的深度信息，进而得到场景中物体的三维坐标。\n局部匹配算法主要有自适应支持窗算法（windowwith adaptive size）、自适应权值算法（adaptivesuppot-weight）和多窗口算法（multiple window）。自适应支持窗算法假设支持窗中的像素的视差值是一样的，然后在目标图像的搜索区域中寻找与参考图像支持窗差异最小的支持窗，两个支持窗中的中心像素即为匹配像素，.11 中左图像为参考图像，右图像为目标图像，和分别为左图像和右图像支持窗中的中心像素，由于采用水平极线校正的约束，因此，搜索区域为一维，即可能匹配像素的纵坐标y 相同。自适应权值的算法与自适应支持窗的算法不同，它用权值来代表支持窗中的像素对中心像素匹配影响的大小，权值越大，影响越大。多窗口匹配算法，主要是根据一定准则，在事先指定的多个窗口中选择最佳的窗口进行匹配计算。\n在国际方面，将现存的立体匹配算法进行了分类和总结，将算法分成四个步骤：匹配代价计算、代价聚集、视差计算、视差精化，但并不是所有匹配算法中都包括这四个步骤，需要哪一步，要根据具体情况而定。还提出了一个专供稠密视差图定量测试的平台（www.middlebury.edu/stereo），得到了广泛的应用。下面将通过这四个步骤分别对国外近些年的立体匹配算法进行总结。\n（1）匹配代价计算。若令参考图像中任一像素点为 p ，目标图像中可能匹配像素点为q ，d 为视差范围。最普通的匹配算法有灰度平方差异（squared intensity differences）和灰度绝对值差异（absoluteintensity differences）。\n还有一些匹配代价对相机偏移、噪声和光照鲁棒，如基于梯度的一些测量rank 变换和census变换等。rank 变换大致过程为以待匹配像素为中心作一个矩形窗口（rank 窗口），然后统计 rank 窗口中灰度值比中心像素灰度值小的像素的数目，并用这个数代替原来中心像素的灰度值，依次计算，直到被转换成一个整数矩阵，这个整数矩阵称为rank 图像。图 1.12 为 rank 变换的一个示例，红色圈内为待匹配元素，rank 窗口大小为3*3 ，rank 窗口内比中心像素灰度值小的元素有 4 个，因此，用 4 来代替中心像素的灰度值。\n而 census 变换则将窗口中心像素以外的像素变成一个比特串，如果窗口中一个像素的灰度值比中心像素大，则相应位置为1，反之为 0。文献匹配代价应用到相邻的半像素值，如下式所示，使得匹配代价对图像抽样具有一定的鲁棒性。\n（2）代价聚集。局部算法是基于窗口的聚集方法，将支持窗内像素的代价相加或平均作为中心像素的匹配代价。支持窗可以是二维的，也可以三维的。二维的支持窗先将视差固定，然后计算聚集代价。除上述介绍的多窗口法和自适应窗口法，常用的支持窗有移动窗口（shiftable  window）、常数视差的连通窗口（window based on connectd components of constant disparity）。三维支持窗包括有界的视差差分窗口（limited disparity difference）、有界的视差梯度窗口（limiteddisparity gradient）、Prazdny 的相关准则（coherence principle）等。\n（3）视差计算。局部算法主要强调匹配代价、代价聚集和视差计算三个步骤，在视差计算中，最小代价对应的视差即为所要求的视差，这也被称为WTA 算法。这个算法的缺点在于它的唯一性约束只在参考图像中有所体现，而目标图像中的像素点可能对应许多个参考图像的像素点。而全局算法则强调视差计算步骤，通常跳过代价聚集步骤。\n（4）视差精化。大多数立体匹配算法都是在一个离散的空间中进行运算的，得到的视差值也是整数的。对于许多应用，如机器人导航（robot navigation）和人类跟踪（people tracking），这已经足够了，但是对于图像绘制（image-based rendering）来说，这些量化的过程会导致视点综合的结果出现错误。为了应对这种情况，许多算法在初始视差计算后，应用子像素精化步骤。子像素精化方法有迭代梯度下降法（iterative  gradient  descent）和拟合曲线法（fitting  a curve）。对偶运算（cross-checking）也可以对视差进行后处理运算，对偶运算可以检测出遮挡区域，然后应用平面拟合算法（surface fitting）或邻域视差估计算法（neighboringdisparity estimates）对遮挡区域的视差进行填补。\n立体匹配面临的挑战\n目前，国际上存在很多算法，但这些匹配方法仍不能很好地解决立体匹配中存在的一些问题：噪声、遮挡匹配问题、弱纹理或重复匹配问题、深度不连续问题、光照变化引起的匹配问题、倾斜区域的匹配问题。除此之外，立体匹配算法都在实验室进行实验阶段，通常在实验过程中采用了许多假设，例如匹配图像对应具有相同的图像特征，包括像素RGB 值，灰度值等，即匹配图像对不受光照等元素的影响。这些挑战限制了立体匹配算法在实际中的应用。\n（1）噪声。图像获取过程中，受到光线变化，图像模糊，传感器噪声等因素的影响。这要求实际应用的立体匹配算法一定具有鲁棒性。\n（2）遮挡问题。遮挡是由于摄像机的空间位置不同，造成一些场景在一幅图中可以看到，但在另一幅图中不可见的现象。本论文将在第六章详细介绍遮挡问题以及解决方法。\n（3）弱纹理区域或纹理单调重复区域。图像对中存在大量重复区域，在寻找匹配点的过程中，容易存在二义性或多义性，使匹配不准确。如图所示，两个视点的图像由于重复性过多，对左图像中任一像素，在右图像中有许多像素和它相似，因此，在匹配中容易出现误匹配情况，影响了匹配质量。这个问题也叫“孔径问题”，即灰度一致性约束在无纹理区是无用的，所以，立体匹配时，消息需要从高纹理区传到低纹理区。\n（4）深度不连续问题。图像中物体上视差都是平滑过度的，但到了物体边缘处，则视差出现突变，对于局部匹配来说，如果在物体边缘处还认为其匹配窗内的像素视差相同的话，则会出现一些错误，对结果造成影响。如图所示，图中物体比较多，物体边缘也多，因此采用局部匹配很容易出现错误。在全局算法中，当使用空间平滑约束时，消息传递应该停止在物体边缘。\n（5）光照强度变化。照相机处于不同的空间位置，如果场景中存在一些镜面反射的现象，有可能使对应点在两幅图像中表现出不同的特征，导致匹配错误。如图所示，两个曝光不同的针对同一场景的图像有不同的RGB 值，而通常的匹配算法则是通过像素 RGB 值来判断两个可能匹配像素是否匹配，这样，光照条件差异无疑给立体匹配造成了许多困难。\n（6）倾斜区域。由于视差图中视差变化是以整数为单位的，即离散的；而场景中物体的深度变化是连续的，用离散的视差表示连续的地方，必定会出现一些不足。\n摄像机成像几何模型\n在立体视觉系统中，第一步应该建立摄像机成像系统，进行摄像机标定，然后获取图像对，最后进行立体匹配与三维重建等后续工作。如图所示，用摄像机拍摄建筑上的某一点，然后在照片中出现这个三维点的二维投影点，而摄像机标定主要是根据摄像机成像系统，来建立三维场景中的点和二维图像像素位置的对应关系。摄像机需要标定的模型参数分为内部参数和外部参数。摄像机模型也有许多，主要分为线性模型和非线性模型。线性模型又称针孔成像模型，是实验室中常用的一种模型，将着重介绍。\n针孔成像模型\n首先，介绍最简单的点光源成像模型，如.2 所示，A ，B ，C 代表实际空间中的线，O 点为光心， A’， B’和C’分别为 A ， B 和C 在图像平面中的成像，从图中可知，实际空间中的三维点和图像中的二维投影点确实存在一定的对应关系，且根据几何关系可求得。\n然后，引入小孔成像模型，如图 2.3 所示。所有光线都经过小孔，使得物体通过孔平面在图像平面上形成了一个倒立的实像。图像平面与孔平面的距离为f ，孔平面与物体的距离为 Z 。\n为了向摄像机模型靠近，将成像平面和景物放在一侧，如.4 所示。P(x,y, z)为实际空间中一点，p(x, y)为其对应的像平面上的点，为图像平面与摄像机光心O 的距离 ，一般称为摄像机的焦距，用 f 表示。根据几何关系，我们能得到如下式子：\n用齐次坐标和矩阵表示式（2－1）的关系如下：\n式中， s 为一比例因子， P 为透视投影矩阵。\n图像坐标系与摄像机坐标系\n摄像机拍摄的图像经高速图像采集系统变换为数字图像，并输入计算机，计算机内每幅数字图像显示为 M*N 数组，即有M 行 N 列个元素，其中每一个元素称为像素。\n如图 2.5 所示，每一个像素的坐标(u,v)是以像素为单位的图像坐标系坐标，代表该像素在计算机中的行数和列数。由于以像素为单位的图像坐标系没能显示像素在图像中的位置，因此又定义了一个以物理单位表示的图像坐标系。以图像中某一点为原点，作 X 、 Y 轴与u 、 v 轴平行，（X,Y）即为像素的图像坐标系。每一个像素在X 轴与Y 轴的物理尺寸为 dX 和 d Y ，图像中任一像素在两个图像坐标系的关系可表示为如下：\n用齐次坐标与矩阵的形式可表示为\n逆关系可写成\n摄像机坐标系与世界坐标系\n世界坐标系描述摄像机在环境中的任意位置，也用它来描述环境中任何物体的位置，由WX 、WY 和WZ 轴组成。摄像机坐标系和世界坐标系之间的关系可用旋转矩阵R 和平移向量t 来描述，用齐次坐标与矩阵的形式可表示为：\n图像坐标系与世界坐标系\n将式（2－8）代入式（2－6）中，得到图像坐标系与世界坐标系的关系，如下式所示：\n从式（2－9）可知，已知摄像机内外参数M1 和M2 ，对任意空间点 P ，如果知道它的世界坐标系坐标，就可以知道它的图像坐标系坐标；而已知它的图像坐标系坐标，并不能知道它的世界坐标系坐标，而只能确定它的空间点均在射线OP 上，如图 2.4 所示。因此，为了得到一空间点准确的世界坐标系坐标，就必须有两个或更多摄像机构成的立体视觉系统模型才能实现。\n双目立体视觉\n双目立体视觉原理\n人用双眼去观察实际空间中物体，空间中的物体点在左右两眼视网膜上成像，左视网膜成像偏右，右视网膜成像偏左，这种差异称为双目视差(binocular  disparity)，人有深度感知就是因为双目视差，如.6 所示。两眼同时看实际空间中的 A 点、C点和 D 点，三点在左视网膜上成像为a2、c2和d2；三点在右视网膜中成像为a1、c1和d1，从图中可明显看出c1，c2的距离比d1，d2\n的距离短，即C 点的视差比D点小，而空间点C 点比 D 点离人眼的距离远。\n基于此理论，用两个摄像机代替人眼，对同一空间物体拍摄，获得两幅图片，称为立体图像对。然后，运用立体匹配算法找到对应同一物体点的图像点，计算出视差，根据简单的三角几何关系即可得到此物体点的深度信息。\n.7为简单的平视双目立体视觉系统示意图，和分别为左摄像机和右摄像机的光心，两个光心连线即为基线，基线距离为 B 。p 为实际空间中一点，它在左图像上成像点为，在右图像上成像点为。假设两个摄像机是平行的，在同一平面上，因此左右图像成像点的纵坐标相同，即。由三角几何关系得到\n从式（2－11）可以看出，对于简单平视的双目立体视觉系统，只要找到对应同一空间点的图像匹配点，知道焦距f 和基线 B 距离，就可以得到空间点在摄像机坐标系下的三维坐标。z 即为空间点 p 的深度，可以看出空间点视差和深度是成反比的。\n极线几何\n极线几何讨论的是两个摄像机图像平面的关系，下面介绍几个概念，如图 2.8。（1）基线：指左右两摄像机光心和的连线，用 B 表示。\n（2）极平面：指任一空间点和两摄像机光心三点确定的平面，如平面和平面都是极平面，所有的极平面相交于基线。\n（3）极点：指基线与左右两图像平面的交点，用和表示。\n（4）极线：指极平面与左右两图像平面的交线，左图像或右图像中所有极线相交于极点或。\n立体匹配中，为了降低可能匹配点对的数量，提出了极线约束，即左图像上的任一点，在右图像上的可能匹配点只可能位于极线上。假设和是空间中同一点 p在两个图像匹配点。左图像平面任一点，它在右图像上的可能匹配点不可能在整个平面上，而一定位于极线上，这样找到最终匹配点为；相似的，右图像平面点，在左图像上的可能匹配点一定位于极线上，和称为共轭极线。极线约束使得立体匹配搜索范围从二维平面降到一维直线，大大减少了搜索时间，在立体匹配中占据着重要的地位。\n如上一节分析，在平视双目立体视觉系统中，左右图像匹配点之间的纵坐标相同，只存在水平差异，即极线是水平的，这一特征对立体匹配算法的简化起了很大的作用。然而，在实际应用中，由于组装、成像等多方面原因，平视双目立体视觉系统很难构建，左右图像中可能匹配点之间不只存在水平差异，同时也存在垂直差异，这就需要通过极线校正来使左右两幅图像的极线相平行，消除或减少两个可能匹配点间的垂直差异，从而较小匹配搜索难度。极线校正示意图如图 2.9 所示，将汇聚立体匹配模型的两个图像平面进行了旋转，产生虚拟的平行立体系统，其中虚线为原图像平面，实线为经过校正后的图像平面。\n基本约束关系\n为了提高系统的去歧义匹配能力和计算效率，除了上一节所说的极线约束，立体匹配中还存在许多匹配约束，如唯一性约束(uniqueness  Constraint)、连续性约束(ContinueConstraint)、相似性约束(Feature Compatibility Constrain)、顺序一致性约束(ordering  Constraint)、互对应约束(Mutual  correspondence  constraint)、视差范围约束(DisparitvLimit constraint)。  下面将分别介绍这几种约束。\n（1）唯一性约束。对双目匹配来说，参考图像上的任一点在目标图像上只能有唯一的点与其相匹配。此约束可避免重复匹配问题。\n（2）连续性约束。空间中物体表面一般都是光滑的，因此摄像机将其拍摄在图像上的点是连续的。在计算视差时，参考图像上同一物体上相邻点的视差应该是连续的。对物体边界处的点，连续性约束并不成立。\n（3）相似性约束。在两幅或多幅图像对应同一实际空间上的点，在某些物体度量上（如灰度，梯度变化等几何形状上）具有一定的相似性。例如空间中某一点在物体的边缘，梯度变化较大，则它在另一幅图像上也位于物体的边缘，且梯度变化较大。\n（4）顺序一致性约束。对于参考图像上的两个像素和，在的左边，这两个像素在目标图像上的对应像素分别为和 ，则也在的左边。但此约束在视点方位变化很大时不满足。\n（5）互对应约束。左图像上的像素和右图像上的像素为一对匹配像素。如果根据立体匹配运算，以左图像为参考图像，找到像素 在右图像中的匹配像素，那么，以右图像为参考图像，同样也能找到像素 在左图像中的匹配像素为 。这个过程也被称为对偶运算，可以有效减少立体匹配过程中由于遮挡、高光或噪声原因而导致的误匹配问题。\n（6）视差范围约束。视差范围约束是指人类视觉系统只能融合视差比某个限度小的立体图像。此约束限制了寻找对应点时的搜索范围。例如从Middlebury 网站上得到的测试图像 cones 和 teddy 的视差范围为 0－59。\n同一物点Q在左右眼两幅视差图上的对应像素点分别称为左像素点L和右像素点R,两像素点间的水平距离称为该物点的水平视差。\n三维模型特征提取算法\n三维模型由于其结构属于三维子空间，因此能够表现的信息较二维图像更多、更复杂。基于内容的三维模型检索需要根据模型的形状信息提取特定的特征描述符，以特征描述符来代替三维模型的形状特征，然后根据特征描述符的形式来确定三维模型间的相似度，从而实现对三维模型的相似性检索。因此特征描述符也称为三维形状描述子。大多数三维模型的特征描述符都是基于三维模型的几何属性，包括顶点坐标、法向矢量、高斯曲率、三角形面积、法向矢量等等。根据描述机理和侧重特征的不同可将三维形状描述子分为：\nA. 基于全局特征（Global Feature-based）的方法\nB. 基于统计（Histogram-based）的方法\nC. 基于投影视图（Graph-based）的方法\nD. 基于拓扑结构（Topology-based）的方法\nE. 基于局部特征点的方法\nF. 综合利用上述方法的混合描述算子(HybridDescriptor)\n基于统计的特征提取算法\n由于三维模型的表面有任意的拓扑结构，因此一些在二维图像分析和检索中常用的一些算法无法直接在三维模型检索领域进行拓展和应用；同时对三维模型进行参数化表示也是很复杂的问题，导致获取有清晰几个或形状意义的三维模型特征十分困难。\n由于存在上述诸多的困难和不便，很多研究人员倾向于从统计学的观点出发，应用一些统计特征作为三维模型的特征。这方面的研究主要使用了模型顶点间的距离、角度、法向、曲率、面积等的几何数据分布以及各阶统计矩和变换特征系数等。\n在大多数基于统计的特征提取算法中，经常被引用的一个算法是Osada等根据顶点在模型表面上的相互关系统计出的分布特征，如此即将一个任意的三维模型中的复杂特征提取问题转化为一个简单形状分布的概率问题。算法使用了5 种在三维模型上采样得到的几何函数，统计这些几何函数的取值，然后对取值进行排序，构造成值序列，以该序列作为三维模型的特征描述符来描述三维模型的形状特征，最后，计算不同序列之间的欧式距离来确认2 组序列之间的相似度，以此来代替三维模型的相似度。该算法基于统计特征，在三维模型检索领域已经成为比较经典的算法之一。\nA1：基于角度的函数，在模型表面上采样任意三个顶点构成的两条线段之间的角度；\nD1：基于距离的函数，采样模型中心到任意顶点的距离；\nD2：基于距离的函数，采样任意两个顶点之间的距离；\nD3：基于面积的函数，采样任意三个顶点组成的三角形面积的平方根；\nD4：基于体积的函数，采样任意四个顶点组成的四面体体积的立方根；\n由于采样的数据都是基于顶点的，因此该方法具有平移、旋转、缩放不变性，同时抗噪音干扰效果也比较好。实验分析的结果表明，基于D2 距离的几何函数，在这 5 种几何函数中的检索效果最好，也就是-5 中的 D2 距离。然而，当三维模型形状比较复杂，包含较多细节信息时，D2 距离分布就会因为信息表征过于粗糙而造成差别较大的三维模型具有相似的形状分布。为了提高 D2距离形状描述符的检索能力， 为了提高 提出根据两点间连线是否经过模型内部、外部或都经过将 D2 分布分为内部、外部、混合三种类型，实现对 CAD 模型的相似性检索。\n除了前面的采样统计的方法以外，还有一些基于切分统计的方法，用球体或者用正立方体进行切分，其中 Ankerst提出了一种用球体对模型进行切分，然后进行统计的方法。该方法首先计算可以包围模型的最小球体，然后将球体按照 3 种规则分割成一系列的格子，分别是基于同心球、基于扇形和组合的切分规则，如图 1-6 所示。通过统计每个格子中的顶点数量，来构造三维模型的特征序列，作为三维模型的特征描述符使用。对于大多数以三角网格表示的三维模型来说，同样的形状可能存在不同的三角网格表示形式，因此模型上网格的细分和简化是影响此类算法准确性的一个问题。若要提高该方法对模型表示形式的兼容性，则需要在使用该方法之前，对三维模型的三角网格化表示形式进行统一的简化处理，增强该算法的稳定性。\n在图 1-6 中，(a)是基于同心球切分规则的示意图，(b)是基于扇形切分规则的示意图，(c)是基于组合切分规则的示意图。在这三种切分规则中，基于同心球的切分规则，其特征描述符具有旋转不变性；基于扇形的切分规则，模型的等比例变化对其特征描述符没有影响；基于组合的切分规则，特征描述符的维数比前两个更大，更能体现三维模型的形状细节，但是失去了旋转和比例不变性。-6 最右侧的图是将一个三维蛋白质分子模型应用上述三种方法获得的特征描述符的图形化表示。\n另一种用正立方体切分的方法由 Suzuki提出，顾名思义，该方法与用球体切分的方法类似，采用可以容纳模型的最小正立方体将模型进行包围，然后对正立方体进行切分，将切分后的立方体切块进行分类，然后统计切分后的每个立方体切块中的三维模型的顶点数量，据此构造三维模型的特征描述符，以此来描述三维模型的形状特征。由于切分后的切块都是小立方体形式，因此该方法也被称为“点密度方法”。-7 展示了点密度方法的大致思路。\n模型预处理\n随着计算机三维造型软件技术的发展以及互联网的普及，三维模型的制作软件和网上可共享的资源也越来越多，但是能够在互联网上获取到的三维模型在尺寸、位置和方向上都存在着或多或少的差异。因此使用的三维模型的来源的多样性导致三维模型可能具有不同的尺度、方位、旋转角度等。这种基本信息的不统一性在三维模型特征提取过程中会对特征结果产生严重影响，所以要在特征提取时，要使得提取出来的特征向量具有平移不变性、旋转不变性、缩放不变性。\n一种解决的办法是把要进行匹配的模型，相互对齐坐标方向，但是这种方案对每个库中的三维模型都要进行一次坐标对准，需要耗费大量的时间，因此很少被采用。另一种解决的办法是在提取三维模型特征之前对三维模型进行一系列的处理，该处理将模型进行变换，变换后的三维模型将会统一在标准的坐标尺度下，具有相同的方向、位置和角度。这一系列操作被称为模型标准化(Normaliztion)，也叫预处理。一般预处理过程中，以三维模型的质心作为坐标原点，对三维模型进行平移变换，保证模型具有平移不变性；采用主成分析法（Principal  Componment Analize）将三维模型旋转对齐，使之保证旋转不变性；对三维模型的顶点到质心距离进行计算，采用最大距离作为标准值对三维模型进行尺度归一化。、\n基于面积分布算子的检索算法的基本思想是三维模型 STL 文件作为入，首先对模型进行预处理，使得表达模型的点和面的集合达到最小化，接着通过计算各个顶点关联的三角形面积，并对点相关的面积进行归一化处理，然后对点关联的面积序列进行通傅里叶变换，得到特征向量，做出面积分布图，通过计算模型间特征向量的差异，得到模型间的差异，进而检索出相似的三维模型。\n模型的简化处理\nSTL 文件是以三角网格模型表示模型的，因此对于相同的模型可能存在着不同的表示形式，如图 2-4所示，就是同一个模型的两种网格表示形式。\n对于图 2-4 中的 2 个模型的表示形式，可以看出，第 1 个模型的表示方式其点和面积的集合最小，而第 2 个模型中存在着部分点并不能表示模型的凸凹信息，将这些点过滤后会得到与第 1 个模型相似的点和面的集合。\n为了避免相似模型的不同的点和面的表示形式对算法的影响，考虑对所有的模型先进行预处理过程，也就是对模型进行简化，使模型的表达简洁，同时更好的突出对象的关键特征，现有的网格简化策略中，几何元素删除法应用最为广泛,它包括顶点删除法、三角形折叠法、边折叠法等，采用基于顶点删除的三角形网格模型简化方法，对模型进行预处理，基本思想是判断每一顶点的类型，根据不同的类型选择不同的判据进行计算，若满足判据成立条件，则将顶点删除,对形成的空洞三角化，其操作示意如图 2-5所示：\n判断顶点是否可删除有 2 个准则，第一，如果一个顶点所有邻接三角形的法向量均相同，则删除该顶点，第二邻接三角形某2 个边在一条直线上，第一个是将平面中间的点去掉，第二个是将边缘上不必要的点去掉。根据该算法，对图 2-4 中第 2 个模型进行了简化处理，最终得到了一个与图 2-4 中第 1 个模型类似的模型化表示。\n匹配约束条件\n一般情况下，一幅图像中的某一特征基元在另一幅图像中可能会有很多候选匹配对象，可真正同名的结构基元只有一个，因此可能会出现歧义匹配。在这种情况下，就需要根据物体的先验知识和某些约束条件来消除误匹配，降低匹配工作量，提高匹配精度、准确度和速度。常用的约束原则如下：\n1）极线约束：一幅图像上的任一点，在另一幅图像的对应点只可能位于一条特定的被称为极线的直线上。这个约束极大的降低了待验证的可能匹配点对的数量，把一个点在另一幅图像上的可能匹配点的分布从二维降到了一维。若是已知目标与摄像机之间的距离在某一区间内，则搜索范围可以限制在极线上的一个很小区间内，这样可以大大的缩小对应点的搜索空间，既可以提高特征点搜索速度，也可以减少误匹配的数量。\n2）唯一性约束：一般情况下，一幅图像上的一个特征点只与另一幅图像的唯一特征点对应。\n一般情况下,当人观察一现实世界中某一物体的时候,每只眼睛的视网膜上各自形成一个独立的影像,左眼看到物体的左边多一些,右眼看到物体的右边多些,同一物体在两个视网膜上得到不同的影像,同一物体上某点落在左右两眼视网膜上的位置是不同的,这种位置差就称为双眼视差。人之所以能有深度感知,就是因为有了这个视差,本文的工作主要就是根据人类双眼对同一景物成像的视差原理来研究双目视觉图像的视差生成算法。\n按照marr的理论,视觉过程可以看成是成像过程的逆过程,在成像过程中,有如下三个重要的变化。\n（1）三维的场景被投影为两维的图像,深度和不可见部分的信息被丢失了,因为也产生了同一物体在不同视角下的图像会有极大的不同,以及后面的物体被前面的物体遮挡而丢失信息等问题。\n（2）场景中的诸多因素,包括照明和光源的情况、场景中的物体的几何形状和物理性质特别是物体表面的发射特性、摄像机的特性、以及光源于物体设摄像机之间的空间关系等,都被综合成单一的图像中的像素的灰度值了。\n（3）成像过程或多或少的带入了一些畸变和噪声。\n总之,由于成像过程中存在的投影、混合、畸变与噪声等原因,使得作为成像过程逆过程的视觉过程是个病态的问题。\nMarr的视觉理论框架\n70年代中期到80年代中期,Marr提出了第一个计算机视觉领域的理论框架,即视觉计算理论,极大地推动了计算机视觉的发展,并最终形成了这一领域的主导思想，Marr的视觉计算理论立足于计算机科学,系统地概括了心理学、物理学、神经生理学、临床神经病理学等方面已经取得的重要成果,是迄今为止比较系统的视觉理论,计算机视觉这一学科与此理论框架有着密切的关系,目前的基于视觉的方法都还没有脱离这个指导性的理论框架,如-1所示。\nMarr的视觉计算理论从信息处理的角度出发,认为视觉处理中存在三个不同的层次,即计算理论的层次、表示数据结构与算法的层次、硬件实现的层次。其中最重要的是计算理论层次,并根据Warrington临床神经学的研究结果,阐明视觉的目的是从图像中建立物体形状和位置的描述。在这一层次把视觉过程主要规定为从二维图像信息中定量地恢复出图像所反映的场景中的三维物体的形状和空间位置,即三维重建。在计算理论这一层次上,与三个层次对应,Marr将三维重建这一过程分为三个阶段。第一阶段是从图像中获得要素图,称为早期视觉。所谓要素图是只从图像中灰度变化剧烈处的位置及其几何分布和组织结构,如零交叉、边缘、边界等,早期视觉是提取原始二维图像中的有用信息而抛弃无关紧要的部分,第二阶段是由要素图获得2.5维图,称为中期视觉,2.5维图是指在以观察者为中心的坐标系中,景物表面的法向,深度及轮廓等,这些信息包含了深度信息,但不是真正的物体三维表示。第三阶段是由第一阶段和第二阶段的结果获得物体的三维表示,称为后期视觉,所谓物体的三维表示是指在物体坐标中心描述各物体之间的空间关系。\n所有的知识点均为从论文中截取的有效知识点。"}
{"content2":"关于人工智能研究思路的一点设想\n作者:牟牧云\n时间：2015/7/27\n（这篇文章作为我写的第一篇博文，有错误或不严谨之处欢迎指出，如有希望深入讨论的，请加QQ1273031371）\n如今，以谷歌百度的深度学习算法为代表的计算机视觉可谓是发展得如火如荼，人们似乎看到了人工智能的一线曙光。\n然而，用侯世达（《GEB》一书作者）的话来说：“想通过爬树到达月球的人，他能不断进步，直到树的尽头”。这个比喻恰到好处，残酷地否定了当代人工智能主流的研究思路。顺便提一下侯世达这个人，就个人而言，我对他是充满敬佩的。他选择了一条不同的道路，或者说他一直坚守这一条与众不同的道路（他的研究可是比现代机器学习深度学习等早了很多年）。遗憾的是，他除了早期的一些著作，很少有建设性的研究成果。分析其中的原因，他可能注重的是对人脑的研究而不是强人工智能的实现，只是研究中涉及到了人工智能。从这个角度来看，我们就不能说他有什么错。\n回到本文的话题，究竟哪一种研究思路是正确的呢？机器学习深度学习研究者可能会说当初莱特兄弟恰恰是摒弃了对鸟类仿生学上的研究才发明了飞机。但我认为这种类比毫不具有说服力，类比不能作为证据，这种类比更不可能得出机器学习深度学习是通往强人工智能的结论。\n按照由因溯果的方法论，也就是提出问题分析问题解决问题。\n问题很明了，在1956年的达特茅斯会议上就已提出：怎样实现强人工智能，让机器达到人的智力水平。\n人类有个很重要的特点：学习能力很强。人类从婴儿阶段到成人阶段智力的提高是很大的，婴儿阶段就具有了完整的硬件设施，就如一个编好的程序能够进行独立学习,自我完善。\n人脑是一个复杂的系统，不要希望从逻辑上来构建这样一个系统，也许其中存在某些简单的规则从而涌现出一些智能的特性。人脑还是一个分布式系统（顺便推荐一本书《谁说了算：自由意志的心理学解读》，有助于对大脑的理解）大脑的各个部分各司其职，每个部分是一个独立的单元，最后通过一个叫解释器的东西然你产生自己是一个统一体的错觉。\n现在来罗列一些人工智能涉及到的领域：\n1：概率论（既然强人工智能是一个复杂的系统，想要做到精确是不可能的。而且量子物理也从理论上终结了确定性的存在）\n2：认知科学（了解一些心理学的知识是很有必要的，这有助于我们对思维机制的理解，人类思维有的很重要的功能就是类比，类比是学习很重要的一部分）\n3：计算机科学（这个不用多说）\n至于究竟哪一种研究思路走得通，我想，既不会是机器学习，也不会是整体论者对大脑的构想。也许到了某一天人们对大脑的研究取得了突破性的进展，计算机的计算能力突飞猛进，再加上全新的算法思路，强人工智能的实现将不再遥不可及。我相信这一天不会很远。"}
{"content2":"EAIDK-610是专为AI开发者和学生精心打造，面向边缘计算的人工智能开发套件。学生或者开发者可以使用此套件了解人工智能的相关知识，了解计算机视觉、人脸检测、人脸识别、双目立体视觉等相关算法，并且可以使用此套件做一些扩展，做产品原型的验证。\nEAIDK-610硬件平台（EAI610-P0）使用高性能Arm SoC（瑞芯微rk3399），搭载OPEN AI LAB嵌入式AI开发平台AID（包含支持异构计算库HCL、嵌入式深度学习框架Tengine、以及轻量级嵌入式计算机视觉加速库BladeCV）。为AI应用提供简洁、高效、统一的API接口，加速终端AI产品的场景化应用落地实现。\n计算机视觉教学主要内容如下：\n01图像采集\n图像采集主要介绍如何对EAI610-P0的视频图像进行采集，以及介绍V4L2，RockchipISP和RockchipRGA 的概念及简单操作应用。\n02图像处理\n图像处理是指对图像进行分析、加工、和处理，使其满足视觉、心理或其他要求的技术。图像处理是信号处理在图像领域上的一个应用。图像处理通过对MIPI摄像头采集到的图像应用多种处理方法，介绍了数字图像处理领域的常用概念和基本方法。\n03视频编码\n视频编码教学案例的目的是介绍视频压缩的原理，并以EAIDK610为例展示一个实际的视频硬编码实例。主要介绍的视频压缩的概念和历史，同时着重分析了H.264标准，在此基础上采用EAIDK610上的硬件编码作为实例，希望达到理论和实践相结合的教学目标。\n04视频解码\n视频解码介绍视频压缩的原理，并以EAIDK610为例展示一个实际的视频硬解码实例。教学方案介绍的视频压缩的概念和历史，同时着重分析了H.264标准，在此基础上采用EAIDK610上的硬件解码作为实例，希望达到理论和实践相结合的教学目标。\n05视频分析之背景提取\n背景提取主要介绍在EAI610-P0的实时采集视频图像基础上的背景提取，着重介绍和实现基于混合高斯模型和CodeBook的背景提取算法，最后对结果简单分析，并对其他算法进行简要介绍。\n06人脸检测（传统机器学习方法）\n人脸检测是为了学习人脸检测的概念，了解传统机器学习人脸检测方法，展示传统机器学习算法及应用开发过程。本教学方案以haar+adaboost算法为例，从算法的原理分析开始，不仅介绍了算法训练的过程，也展示了算法在嵌入式平台（EAI610-P0）的部署。\n07人脸特征提取（深度学习方法）\n人脸特征提取是以人脸特征提取方案为例，展示完整的深度学习算法及应用开发过程，同时介绍人脸识别中的核心算法概念。此教学方案以Light CNN 为例，从算法的原理分析开始，不仅介绍了算法训练的过程，也展示了算法在嵌入式平台（EAI610-P0）的部署。\n08双目立体视觉\n双目立体视觉是学习立体视觉的概念，掌握用双目相机计算深度信息的方法。从双目相机标定及矫正原理分析开始，介绍双目立体视觉的基础理论方法，并且介绍了基于SGBM的深度计算算法，最终在嵌入式平台（EAI610-P0）上使用两个高清相机模组部署。\n09人脸识别系统\n人脸识别是以人脸特征提取方案为例，展示完整的深度学习算法及应用开发过程，以 Light CNN 1 为例 ，从算法的原理及特点开始，不仅介绍了算法训练的过程，也展示了算法在嵌入式平台（EAI610-P0）的部署 。\n010双路人脸门禁系统\n双路人脸门禁系统是以嵌入式人工智能开发套件（EAIDK-610）为基础平台，人员进门时，网络摄像机（IPC）连续抓取多张人脸信息，并毫秒级地比对抓取照片和底库信息，比对成功后触发开门动作，人脸抓取、比对结果和人员进门的信息在显示设备上实时展示。\n011目标检测（深度学习方法）\n目标检测方案是在嵌入式平台上进行机器视觉的目标检测，通过深度学习的方法实现对多目标物体进行检测并识别。使用Arm的嵌入式人工智能开发套件（EAIDK-610）为基础平台，通过网络摄像机进行视频数据采集，EAIDK-610作为边缘智能处理对视频进行分析，并实时显示分析结果。\nEAIDK-嵌入式人工智能开发套件（Embedded AI Development Kit 简称EAIDK），是全球首个采用Arm架构的人工智能开发平台，专为 AI 开发者精心打造，面向边缘计算的人工智能开发套件。\n硬件平台具备语音、视觉等传感器数据采集能力，及适用于多场景的运动控制接口；智能软件平台支持视觉处理与分析、语音识别、语义分析、SLAM 等应用和主流开源算法，满足AI 教育、算法应用开发、产品原型开发验证等需求。\n更多"}
{"content2":"1.基本概念\n从定义理解概念是最严谨的。所以首先搞清楚维基百科中这些概念的定义。\n计算机视觉(CV)：\nComputer vision is a field that includes\nmethods for acquiring, processing, analyzing, and understanding images and, in\ngeneral, high-dimensional data from the real world in order to produce\nnumerical or symbolic information, e.g., in the forms of decisions.[1]\n直译过来就是\n计算机视觉是一个学科/领域，它包括获取、处理、分析和理解图像或者更一般意义的真实世界的高维数据的方法；它的目的是产生决策形式的数字或者符号信息。\n计算机图像学(CG)：\nComputer graphics is a sub-field of\ncomputer science which studies methods for digitally synthesizing and\nmanipulating visual content. Although the term often refers to the study of\nthree-dimensional computer graphics, it also encompasses two-dimensional\ngraphics and image processing.[2]\n直译过来就是\n计算机图形学是计算机科学的一个子领域，它包括数字合成和操作可视内容（图像、视频）的方法。尽管这个术语通常指三维计算机图形学的研究，但它也包括二维图形学和图像处理。\n图像处理（IP）：\nIn imaging science, image processing is\nprocessing of images using mathematical operations by using any form of signal\nprocessing for which the input is an image, such as a photograph or video\nframe; the output of image processing may be either an image or a set of characteristics\nor parameters related to the image.[3]\n直译过来就是\n在图像科学中，图像处理是用任何信号处理等数学操作处理图像的过程，输入时图像（摄影图像或者视频帧），输出是图像或者与输入图像有关的特征、参数的集合。\n2.区别和联系[4]\n2.1 精简的概括\nComputer Graphics和Computer\nVision是同一过程的两个方向。Computer Graphics将抽象的语义信息转化成图像，Computer Vision从图像中提取抽象的语义信息。Image\nProcessing探索的是从一个图像或者一组图像之间的互相转化和关系，与语义信息无关。\n2.2 从输入输出角度看\n(1)区别\nComputer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb 颜色等。输出的是图像，即二维像素数组。\nComputer Vision，简称\nCV。输入的是图像或图像序列，通常来自相机、摄像头或视频文件。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。\nDigital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop 中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。\n(2)联系\nCG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是\nDIP，只是将计算量放在了显卡端。\nCV 更是大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理。\n最后还要提到近年来的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉\nDIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。\n(3)图解\n这里还有一张图，简明地表达了CV、CG、DIP和AI的区别和联系。\n2.3 从问题本身看\n(1)区别\n从问题本身来说，这三者主要以两类问题区分：是根据状态模拟观测环境，还是根据观测的环境来推测状态。假设观测是Z，状态是X：Computer\nGraphics是一个Forwad Problem (Z|X)： 给你光源的位置，物体形状，物体表面信息，你如何根据已有的变量的状态模拟出一个环境出来。\nComputer Vision正好相反，是一个Inverse Problem (X|Z)：你所有能得到的都是观测信息(measurements),\n根据得到的每一个Pixel的信息(颜色，深度)，我要来估计物体环境的特征和状态出来，比如物体运动(Tracking)，三维结构（SFM）,物体类别（Classification\nand Segmentation）等等。\n对于Image Processing来说，它恰好介于两者之间，两种问题都有。但对于State-of-art的研究来说，Image Processing更偏于Computer Vision, 或者看上去更像Computer Vision的子类。尽管这三类研究中，随着CV领域的不断进步，以及越来越高级相机传感器出现（Depth Camera, Event\nCamera），很多算法都被互相用到，但是从Motivation来看，并没有太大变化。\n(2)联系\n得益于这几个领域的共同进步，所以你能看到Graphics和Computer Vision现在出现越来越多的交集。如果根据观测量（图片），Computer\nVision可以越来越准确的估计出越来越多的变量，那么这些变量套到Graphics算法中，就可以模拟出一个跟真实环境一样的场景出来。\n与此同时，Graphics需要构建更真实的场景，也希望能够将变量更加接机与实际，或者通过算法估计出来，这就引入了Vision的动机。这也是近年来三维重建算法，同时大量发表在Graphics和Vision的会议的原因。随着CV从2D向3D发展，以后两者的交集会越来越大，除了learning以外的其他很多问题融合并到一个领域我也不会奇怪。\n附：\nVR 是电脑构建的3维立体；\nAR 是计算机找准一个基点，绘制的三维图形；\n混合现实，是基于计算机辅助生成一个现实环境，再基于这个现实环境，生成一个多视角的三维图形。\n参考文献\n[1]\nhttps://en.wikipedia.org/wiki/Computer_vision\n[2]https://en.wikipedia.org/wiki/Computer_graphics_(computer_science)\n[3]\nhttps://en.wikipedia.org/wiki/Image_processing\n[4] 张静, 知乎,\nhttp://www.zhihu.com/question/20672053/answer/15854031"}
{"content2":"【 导读】目前，计算机视觉是深度学习领域最热门的研究领域之一。计算机视觉实际上是一个跨领域的交叉学科，包括计算机科学（图形、算法、理论、系统、体系结构），数学（信息检索、机器学习），工程学（机器人、语音、自然语言处理、图像处理），物理学（光学 ），生物学（神经科学）和心理学（认知科学）等等。许多科学家认为，计算机视觉为人工智能的发展开拓了道路。\n那么什么是计算机视觉呢？ 这里给出了几个比较严谨的定义：\n✦ “对图像中的客观对象构建明确而有意义的描述”（Ballard＆Brown，1982）\n✦ “从一个或多个数字图像中计算三维世界的特性”（Trucco＆Verri，1998）\n✦ “基于感知图像做出对客观对象和场景有用的决策”（Sockman＆Shapiro，2001）\n▌为什么要学习计算机视觉？\n一个显而易见的答案就是，这个研究领域已经衍生出了一大批快速成长的、有实际作用的应用，例如：\n人脸识别： Snapchat 和 Facebook 使用人脸检测算法来识别人脸。\n图像检索：Google Images 使用基于内容的查询来搜索相关图片，算法分析查询图像中的内容并根据最佳匹配内容返回结果。\n游戏和控制：使用立体视觉较为成功的游戏应用产品是：微软 Kinect。\n监测：用于监测可疑行为的监视摄像头遍布于各大公共场所中。\n生物识别技术：指纹、虹膜和人脸匹配仍然是生物识别领域的一些常用方法。\n智能汽车：计算机视觉仍然是检测交通标志、灯光和其他视觉特征的主要信息来源。\n视觉识别是计算机视觉的关键组成部分，如图像分类、定位和检测。神经网络和深度学习的最新进展极大地推动了这些最先进的视觉识别系统的发展。在本文中，我将分享 5 种主要的计算机视觉技术，并介绍几种基于计算机视觉技术的深度学习模型与应用。\n▌1 、图像分类\n给定一组各自被标记为单一类别的图像，我们对一组新的测试图像的类别进行预测，并测量预测的准确性结果，这就是图像分类问题。图像分类问题需要面临以下几个挑战☟☟☟：\n视点变化，尺度变化，类内变化，图像变形，图像遮挡，照明条件和背景杂斑\n我们怎样来编写一个图像分类算法呢？\n计算机视觉研究人员提出了一种基于数据驱动的方法。\n该算法并不是直接在代码中指定每个感兴趣的图像类别，而是为计算机每个图像类别都提供许多示例，然后设计一个学习算法，查看这些示例并学习每个类别的视觉外观。也就是说，首先积累一个带有标记图像的训练集，然后将其输入到计算机中，由计算机来处理这些数据。\n因此，可以按照下面的步骤来分解：\n输入是由 N 个图像组成的训练集，共有 K 个类别，每个图像都被标记为其中一个类别。\n然后，使用该训练集训练一个分类器，来学习每个类别的外部特征。\n最后，预测一组新图像的类标签，评估分类器的性能，我们用分类器预测的类别标签与其真实的类别标签进行比较。\n目前较为流行的图像分类架构是卷积神经网络（CNN）——将图像送入网络，然后网络对图像数据进行分类。卷积神经网络从输入“扫描仪”开始，该输入“扫描仪”也不会一次性解析所有的训练数据。比如输入一个大小为 100*100 的图像，你也不需要一个有 10,000 个节点的网络层。相反，你只需要创建一个大小为 10 *10 的扫描输入层，扫描图像的前 10*10 个像素。然后，扫描仪向右移动一个像素，再扫描下一个 10 *10 的像素，这就是滑动窗口。\n输入数据被送入卷积层，而不是普通层。每个节点只需要处理离自己最近的邻近节点，卷积层也随着扫描的深入而趋于收缩。除了卷积层之外，通常还会有池化层。池化是过滤细节的一种方法，常见的池化技术是最大池化，它用大小为 2*2 的矩阵传递拥有最多特定属性的像素。\n现在，大部分图像分类技术都是在 ImageNet 数据集上训练的， ImageNet 数据集中包含了约 120 万张高分辨率训练图像。测试图像没有初始注释（即没有分割或标签），并且算法必须产生标签来指定图像中存在哪些对象。\n现存的很多计算机视觉算法，都是被来自牛津、 INRIA 和 XRCE 等顶级的计算机视觉团队在 ImageNet 数据集上实现的。通常来说，计算机视觉系统使用复杂的多级管道，并且，早期阶段的算法都是通过优化几个参数来手动微调的。\n第一届 ImageNet 竞赛的获奖者是 Alex Krizhevsky（NIPS 2012） ，他在 Yann LeCun 开创的神经网络类型基础上，设计了一个深度卷积神经网络。该网络架构除了一些最大池化层外，还包含 7 个隐藏层，前几层是卷积层，最后两层是全连接层。在每个隐藏层内，激活函数为线性的，要比逻辑单元的训练速度更快、性能更好。除此之外，当附近的单元有更强的活动时，它还使用竞争性标准化来压制隐藏活动，这有助于强度的变化。\n就硬件要求而言， Alex 在 2 个 Nvidia GTX 580 GPU （速度超过 1000 个快速的小内核）上实现了非常高效的卷积网络。 GPU 非常适合矩阵间的乘法且有非常高的内存带宽。这使他能在一周内完成训练，并在测试时快速的从 10 个块中组合出结果。如果我们能够以足够快的速度传输状态，就可以将网络分布在多个内核上。\n随着内核越来越便宜，数据集越来越大，大型神经网络的速度要比老式计算机视觉系统更快。在这之后，已经有很多种使用卷积神经网络作为核心，并取得优秀成果的模型，如 ZFNet（2013），GoogLeNet（2014）， VGGNet（2014）， RESNET（2015），DenseNet（2016）等。\n▌2 、对象检测\n识别图像中的对象这一任务，通常会涉及到为各个对象输出边界框和标签。这不同于分类/定位任务——对很多对象进行分类和定位，而不仅仅是对个主体对象进行分类和定位。在对象检测中，你只有 2 个对象分类类别，即对象边界框和非对象边界框。例如，在汽车检测中，你必须使用边界框检测所给定图像中的所有汽车。\n如果使用图像分类和定位图像这样的滑动窗口技术，我们则需要将卷积神经网络应用于图像上的很多不同物体上。由于卷积神经网络会将图像中的每个物体识别为对象或背景，因此我们需要在大量的位置和规模上使用卷积神经网络，但是这需要很大的计算量！\n为了解决这一问题，神经网络研究人员建议使用区域（region）这一概念，这样我们就会找到可能包含对象的“斑点”图像区域，这样运行速度就会大大提高。第一种模型是基于区域的卷积神经网络（ R-CNN ），其算法原理如下：\n在 R-CNN 中，首先使用选择性搜索算法扫描输入图像，寻找其中的可能对象，从而生成大约 2,000 个区域建议；\n然后，在这些区域建议上运行一个 卷积神网络；\n最后，将每个卷积神经网络的输出传给支持向量机（ SVM ），使用一个线性回归收紧对象的边界框。\n实质上，我们将对象检测转换为一个图像分类问题。但是也存在这些问题：训练速度慢，需要大量的磁盘空间，推理速度也很慢。\nR-CNN 的第一个升级版本是 Fast R-CNN，通过使用了 2 次增强，大大提了检测速度：\n在建议区域之前进行特征提取，因此在整幅图像上只能运行一次卷积神经网络；\n用一个 softmax 层代替支持向量机，对用于预测的神经网络进行扩展，而不是创建一个新的模型。\nFast R-CNN 的运行速度要比 R-CNN 快的多，因为在一幅图像上它只能训练一个 CNN 。 但是，择性搜索算法生成区域提议仍然要花费大量时间。\nFaster R-CNN 是基于深度学习对象检测的一个典型案例。\n该算法用一个快速神经网络代替了运算速度很慢的选择性搜索算法：通过插入区域提议网络（ RPN ），来预测来自特征的建议。 RPN 决定查看“哪里”，这样可以减少整个推理过程的计算量。\nRPN 快速且高效地扫描每一个位置，来评估在给定的区域内是否需要作进一步处理，其实现方式如下：通过输出 k 个边界框建议，每个边界框建议都有 2 个值——代表每个位置包含目标对象和不包含目标对象的概率。\n﻿一旦我们有了区域建议，就直接将它们送入 Fast R-CNN 。 并且，我们还添加了一个池化层、一些全连接层、一个 softmax 分类层以及一个边界框回归器。\n总之，Faster R-CNN 的速度和准确度更高。值得注意的是，虽然以后的模型在提高检测速度方面做了很多工作，但很少有模型能够大幅度的超越 Faster R-CNN 。换句话说， Faster R-CNN 可能不是最简单或最快速的目标检测方法，但仍然是性能最好的方法之一。\n近年来，主要的目标检测算法已经转向更快、更高效的检测系统。这种趋势在 You Only Look Once（YOLO），Single Shot MultiBox Detector（SSD）和基于区域的全卷积网络（ R-FCN ）算法中尤为明显，这三种算法转向在整个图像上共享计算。因此，这三种算法和上述的3种造价较高的R-CNN 技术有所不同。\n▌3 、 目标跟踪\n﻿目标跟踪，是指在特定场景跟踪某一个或多个特定感兴趣对象的过程。传统的应用就是视频和真实世界的交互，在检测到初始对象之后进行观察。现在，目标跟踪在无人驾驶领域也很重要，例如 Uber 和特斯拉等公司的无人驾驶。\n根据观察模型，目标跟踪算法可分成 2 类：生成算法和判别算法。\n生成算法使用生成模型来描述表观特征，并将重建误差最小化来搜索目标，如主成分分析算法（ PCA ）；\n判别算法用来区分物体和背景，其性能更稳健，并逐渐成为跟踪对象的主要手段（判别算法也称为 Tracking-by-Detection ，深度学习也属于这一范畴）。\n为了通过检测实现跟踪，我们检测所有帧的候选对象，并使用深度学习从候选对象中识别想要的对象。有两种可以使用的基本网络模型：堆叠自动编码器（ SAE ）和卷积神经网络（ CNN ）。\n目前，最流行的使用 SAE 进行目标跟踪的网络是 Deep Learning Tracker（DLT），它使用了离线预训练和在线微调。其过程如下：\n离线无监督预训练使用大规模自然图像数据集获得通用的目标对象表示，对堆叠去噪自动编码器进行预训练。堆叠去噪自动编码器在输入图像中添加噪声并重构原始图像，可以获得更强大的特征表述能力。\n将预训练网络的编码部分与分类器合并得到分类网络，然后使用从初始帧中获得的正负样本对网络进行微调，来区分当前的对象和背景。 DLT 使用粒子滤波作为意向模型（motion model），生成当前帧的候选块。 分类网络输出这些块的概率值，即分类的置信度，然后选择置信度最高的块作为对象。\n在模型更新中， DLT 使用有限阈值。\n鉴于 CNN 在图像分类和目标检测方面的优势，它已成为计算机视觉和视觉跟踪的主流深度模型。 一般来说，大规模的卷积神经网络既可以作为分类器和跟踪器来训练。具有代表性的基于卷积神经网络的跟踪算法有全卷积网络跟踪器（ FCNT ）和多域卷积神经网络（ MD Net ）。\nFCNT 充分分析并利用了 VGG 模型中的特征映射，这是一种预先训练好的 ImageNet 数据集，并有如下效果：\n卷积神经网络特征映射可用于定位和跟踪。\n对于从背景中区分特定对象这一任务来说，很多卷积神经网络特征映射是噪音或不相关的。\n较高层捕获对象类别的语义概念，而较低层编码更多的具有区性的特征，来捕获类别内的变形。\n因此， FCNT 设计了特征选择网络，在 VGG 网络的卷积 4-3 和卷积 5-3 层上选择最相关的特征映射。 然后为避免噪音的过拟合， FCNT 还为这两个层的选择特征映射单独设计了两个额外的通道（即 SNet 和 GNet ）： GNet 捕获对象的类别信息； SNet 将该对象从具有相似外观的背景中区分出来。\n这两个网络的运作流程如下：都使用第一帧中给定的边界框进行初始化，以获取对象的映射。而对于新的帧，对其进行剪切并传输最后一帧中的感兴趣区域，该感兴趣区域是以目标对象为中心。最后，通过 SNet 和 GNet ，分类器得到两个预测热映射，而跟踪器根据是否存在干扰信息，来决定使用哪张热映射生成的跟踪结果。 FCNT 的图如下所示。\n与 FCNT 的思路不同， MD Net 使用视频的所有序列来跟踪对象的移动。上述网络使用不相关的图像数据来减少跟踪数据的训练需求，并且这种想法与跟踪有一些偏差。该视频中的一个类的对象可以是另一个视频中的背景，因此， MD Net 提出了“多域”这一概念，它能够在每个域中独立的区分对象和背景，而一个域表示一组包含相同类型对象的视频。\n如下图所示， MD Net 可分为两个部分，即 K 个特定目标分支层和共享层：每个分支包含一个具有 softmax 损失的二进制分类层，用于区分每个域中的对象和背景；共享层与所有域共享，以保证通用表示。\n近年来，深度学习研究人员尝试使用了不同的方法来适应视觉跟踪任务的特征，并且已经探索了很多方法：\n应用到诸如循环神经网络（ RNN ）和深度信念网络（DBN ）等其他网络模型；\n设计网络结构来适应视频处理和端到端学习，优化流程、结构和参数；\n或者将深度学习与传统的计算机视觉或其他领域的方法（如语言处理和语音识别）相结合。\n▌4、语义分割\n计算机视觉的核心是分割，它将整个图像分成一个个像素组，然后对其进行标记和分类。特别地，语义分割试图在语义上理解图像中每个像素的角色（比如，识别它是汽车、摩托车还是其他的类别）。如上图所示，除了识别人、道路、汽车、树木等之外，我们还必须确定每个物体的边界。因此，与分类不同，我们需要用模型对密集的像素进行预测。\n与其他计算机视觉任务一样，卷积神经网络在分割任务上取得了巨大成功。最流行的原始方法之一是通过滑动窗口进行块分类，利用每个像素周围的图像块，对每个像素分别进行分类。但是其计算效率非常低，因为我们不能在重叠块之间重用共享特征。\n解决方案就是加州大学伯克利分校提出的全卷积网络（ FCN ），它提出了端到端的卷积神经网络体系结构，在没有任何全连接层的情况下进行密集预测。\n这种方法允许针对任何尺寸的图像生成分割映射，并且比块分类算法快得多，几乎后续所有的语义分割算法都采用了这种范式。\n但是，这也仍然存在一个问题：在原始图像分辨率上进行卷积运算非常昂贵。为了解决这个问题， FCN 在网络内部使用了下采样和上采样：下采样层被称为条纹卷积（ striped convolution ）；而上采样层被称为反卷积（ transposed convolution ）。\n尽管采用了上采样和下采样层，但由于池化期间的信息丢失， FCN 会生成比较粗糙的分割映射。 SegNet 是一种比 FCN （使用最大池化和编码解码框架）更高效的内存架构。在 SegNet  解码技术中，从更高分辨率的特征映射中引入了 shortcut/skip connections ，以改善上采样和下采样后的粗糙分割映射。\n目前的语义分割研究都依赖于完全卷积网络，如空洞卷积 ( Dilated Convolutions ），DeepLab 和 RefineNet 。\n▌5 、实例分割\n除了语义分割之外，实例分割将不同类型的实例进行分类，比如用 5 种不同颜色来标记 5 辆汽车。分类任务通常来说就是识别出包含单个对象的图像是什么，但在分割实例时，我们需要执行更复杂的任务。我们会看到多个重叠物体和不同背景的复杂景象，我们不仅需要将这些不同的对象进行分类，而且还要确定对象的边界、差异和彼此之间的关系！\n到目前为止，我们已经看到了如何以多种有趣的方式使用卷积神经网络的特征，通过边界框有效定位图像中的不同对象。我们可以将这种技术进行扩展吗？也就是说，对每个对象的精确像素进行定位，而不仅仅是用边界框进行定位？  Facebook AI 则使用了 Mask R-CNN 架构对实例分割问题进行了探索。\n就像 Fast R-CNN 和 Faster R-CNN 一样， Mask R-CNN 的底层是鉴于 Faster R-CNN 在物体检测方面效果很好，我们是否可以将其扩展到像素级分割？\nMask R-CNN 通过向 Faster R-CNN 添加一个分支来进行像素级分割，该分支输出一个二进制掩码，该掩码表示给定像素是否为目标对象的一部分：该分支是基于卷积神经网络特征映射的全卷积网络。将给定的卷积神经网络特征映射作为输入，输出为一个矩阵，其中像素属于该对象的所有位置用 1 表示，其他位置则用 0 表示，这就是二进制掩码。\n另外，当在原始 Faster R-CNN 架构上运行且没有做任何修改时，感兴趣池化区域（ RoIPool ） 选择的特征映射区域或原始图像的区域稍微错开。由于图像分割具有像素级特性，这与边界框不同，自然会导致结果不准确。 Mas R-CNN 通过调整 RoIPool 来解决这个问题，使用感兴趣区域对齐（ Roialign ）方法使其变的更精确。本质上， RoIlign 使用双线性插值来避免舍入误差，这会导致检测和分割不准确。\n一旦生成这些掩码， Mask R-CNN 将 RoIAlign 与来自 Faster R-CNN 的分类和边界框相结合，以便进行精确的分割：\n▌结语\n上述这 5 种主要的计算机视觉技术可以协助计算机从单个或一系列图像中提取、分析和理解有用的信息。你还可以通过我的 GitHub 存储库（https://github.com/khanhnamle1994/computer-vision）获取所有的演讲幻灯片以及指南。\n作者 | James Le\n原文链接\nhttps://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b"}
{"content2":"本篇的硬件环境一样，跟着官方的教程来深度体验下角蜂鸟的人工智能的实力。根据官方介绍，角蜂鸟内置的几种深度神经网络模型如下，包括数字识别、人脸识别和物体识别等。\nMNIST 数字识别模型\nMobilenet-SSD 人脸检测模型\nMobilenet-SSD VOC物体检测模型\nSqueezeNet 图像分类模型\nGoogleNet 图像识别模型（特征提取）\nFaceNet 人脸识别模型（特征提取）\nSketchGraph 手绘识别模型\nOCR 中文文字识别模型 （Work in progress）\nhttps://hornedsungem.github.io/Docs/cn/workflow/\n由于在第一讲中已经体验过入门的MNIST手写数字识别了，这里就不做介绍了，官网也有详细解释。\n1. 人脸检测\n使用角蜂鸟和Python调用内置部署的SSD-Mobilenet人脸检测卷积神经网络。检测器分析图片并找出目标的位置和尺寸。\ncd ~/hornedSungemSDK/examples/python/\nsudo python3 FaceDetector.py\n其中SSD [N]中N为检测人脸个数，下一行为人脸检测框Bounding Box的左上角和右下角坐标以及窗的宽高。比如最后一行的几个数表示检测框的左上角在图片坐标系的（175,28），右下角在（245,127），width为245-175=70个像素，height为127-28=99个像素。图片坐标系的原点在图片窗口左上角，横轴为x，纵轴为y。\n#! /usr/bin/env python3 # Copyright(c) 2018 Senscape Corporation. # License: Apache 2.0 import numpy as np, cv2, sys sys.path.append('../../api/') import hsapi as hs WEBCAM = False # Set to True if use Webcam net = hs.HS('FaceDetector', zoom = True, verbose = 2, threshSSD=0.55) if WEBCAM: video_capture = cv2.VideoCapture(0) try: while True: if WEBCAM: _, img = video_capture.read() else: img = None result = net.run(img) img = net.plotSSD(result) cv2.imshow(\"Face Detector\", img) cv2.waitKey(1) finally: net.quit()\n来看代码，加上注释，一共二十多行就实现了人脸识别，实时性也很不错，真的很出乎我的意料。这里要给角蜂鸟的程序猿们奖个大香蕉 O(∩_∩)O哈哈~\n再来细看一下\n人脸识别Python：hornedSungemSDK/examples/python/FaceDetector.py\n模型文件：hornedSungemSDK/examples/graphs/graph_face_SSD\n模型文件名称 Filename: graph_face_SSD\n描述 Description: Mobilenet + Single-shot detector 人脸检测模型。\n属性 Properties:\nChannel: 3 （RGB图）\nScale: 0.007843\nMean: [1.0, 1.0, 1.0]\nImage Size: [300,300]\nType: SSD检测\n为什么可以这么简洁呢？设置net参数，调用net.run传入图片，然后就是结果了。在hornedSungemSDK/api/hsapi.py可以看到run函数里确实是加载神经网络模型self.graph.LoadTensor\ndef run(self, img=None, **kwargs): if img is None: image = self.graph.GetImage(self.zoom) else: if self.isGray: image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY) else: image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) img2load = cv2.resize(image,self.netSize).astype(float) img2load *= self.scale img2load -= self.mean self.graph.LoadTensor(img2load.astype(numpy.float16), None) self.imgSize = image.shape[:2] output, _ = self.graph.GetResult() #print(output) for k,v in kwargs.items(): exec('self.'+k+'=v') if self.type in [1,7] : # Classification output = numpy.argmax(output) elif self.type is 2: # SSD Face output = self.getBoundingBoxFromSSDResult(output, self.imgSize)\n我们一起在hornedSungemSDK/api/hsapi.py中分析下源码，L485行\ndef plotSSD(self, result, labels=None): if labels is None: labels = self.labels display_image = result[0] boxes = result[1] source_image_width = display_image.shape[1] source_image_height = display_image.shape[0] self.msg_debug('SSD [%d]: Box values' % len(boxes),'*') for box in boxes: class_id = box[0] percentage = int(box[1] * 100) label_text = self.labels[int(class_id)] + \" (\" + str(percentage) + \"%)\" box_w = box[4]-box[2] box_h = box[5]-box[3] if (box_w > self.imgSize[0]*0.8) or (box_h > self.imgSize[1]*0.8): continue self.msg_debug('Box Name: %s' % self.labels[int(class_id)]) self.msg_debug('%d %d %d %d - w:%d h:%d' %(box[2],box[3],box[4],box[5],box_w,box_h)) . . .\n打印识别结果。\n2. 物体检测\n介绍如何使用角蜂鸟在Python调用内置部署的SSD-Mobilenet物体检测卷积神经网络。检测器分析图片并找出目标的位置和尺寸。\ncd ~/hornedSungemSDK/examples/python/\nsudo python3 ObjectDetector.py\n文件名称 Filename: graph_object_SSD\n描述 Description: Mobilenet + Single-shot detector 物体检测模型，VOC数据集训练，共20种物体。\n实例 Example:\nMobilenet-SSD 物体检测\n属性 Properties:\nChannel: 3 （RGB图）\nScale: 0.007843\nMean: [1.0, 1.0, 1.0]\nImage Size: [300,300]\nType: SSD检测\n3. 情景记录器\n简单来说就是利用神经网络进行特征提取，分类器，目前最多支持五个。\ncd ~/hornedSungemSDK/examples/python/\nsudo python3 SceneRecorder.py\nGoogleNet 图像识别模型（特征提取）\n版本 Version: 0.1.0\n文件名称 Filename: graph_g\n描述 Description: GoogleNet 图像识别模型（输出层用于特征提取）。\n实例 Example:\n情景记录器\n属性 Properties:\nChannel: 3 （RGB图）\nScale: 0.007843\nMean: [1.0, 1.0, 1.0]\nImage Size: [224,224]\nType: 特征提取\n这里我是设置两个手势，伸出1根手指和伸出两根手指，分别为场景1和场景2。\n按 ‘r’ 将筛选去除每个存档中的冗余图像特征，并生成模型。\n按 ’s’ 将存档录入至 /misc/record.dat 文件中。按 ‘l’ 可读档\n按 ‘p’ 将重置删除所有存档。\n路径和文件\n情景记录器Python：hornedSungemSDK/examples/python/SceneRecorder.py\n模型文件：hornedSungemSDK/examples/graphs/graph_g - GoogleNet\n模型文件（备选）：hornedSungemSDK/examples/graphs/graph_fn - FaceNet\n4.图像识别器（底层API教程）\n图像识别器，比起检测器，识别器可从图片分析得到较细的分类类别，例如猫、狗的某个品种。\ncd ~/hornedSungemSDK/examples/python/\nsudo python3 ImageRecognition.py\n人脸识别Python：SungemSDK/examples/python/ImageRecognition.py\n模型文件：SungemSDK/examples/graphs/graph_sz\nSqueezeNet 图像分类模型\n版本 Version: 0.1.0\n文件名称 Filename: graph_sz\n描述 Description: SqueezeNet 图像分类模型，由ImageNet数据集训练，共1000种物体。\n实例 Example:\nSqueezeNet 图像识别器（底层API教程）\n属性 Properties:\nChannel: 3 (BGR)\nScale: 1\nMean: [104, 117, 123]\nImage Size: [227,227]\nType: 识别\n按W和S可以相应的增大或减小ROI区域，这点官网没有提哦。\n5. 你画我猜\ncd ~/hornedSungemSDK/examples/python/\nsudo python3 SketchGuess.py\nhttps://mp.weixin.qq.com/s?__biz=MzA4MzAwNzcyOA==&mid=2650049516&idx=1&sn=bac2233b5e6c8f04e1183e56fe1f1ad1&chksm=87fd2a73b08aa365e5c199926a299382198a9d3cb15c36c22c75b76e1a1a2ae1f9da4f0ef7c3&mpshare=1&scene=23&srcid=07278ZPfI7lINKxvWcwbzmZq#rd\n这篇博客详细介绍了整个实现过程\nhttps://blog.csdn.net/weixin_40986174/article/details/80223626"}
{"content2":"点击有惊喜\n目录\n人工智能/机器学习\n计算机视觉/模式识别\n自然语言处理/计算语言学\n体系结构\n数据挖掘/信息检索\n计算机图形学\n人工智能/机器学习\n1. AAAI 2018\n会议时间：2月2日~7日\n会议地点： 新奥尔良市，美国\nAAAI是人工智能领域的主要学术会议，由美国人工智能促进协会主办。AAAI 成立于 1979 年，最初名为 “美国人工智能协会” （American Association for Artificial Intelligence），2007 年才正式更名为 “人工智能促进协会” （Association for the Advancement of Artificial Intelligence ）。\n近年的AAAI会议不乏中国学者的身影，据统计AAAI 2017接收的700多篇论文中半数以上有华人参与。此外，南京大学教授周志华当选2019年AAAI程序主席，该年另一位程序主席是密歇根大学教授 Pascal Van Hentenryck。\n今年的AAAI Fellow包括：\nNancy Amato (TAMU),\nRegina Barzilay (MIT)\nMarie desJardins (UMBC)\nKevin Leyton-Brown (UBC)\nDinesh Manocha (UNC)\nJoelle Pineau (McGill)\nAmit Sheth (Wright State)\nGaurav Sukhatme (USC)\n今年的AAAI会议举办在即，新智元已经展开发布一系列重要论文解读文章，更多报道敬请关注。\n官网：https://aaai.org/Conferences/AAAI-18/\n2. AISTATS 2018\n会议时间：4月9日~11日\n会议地点：普拉亚布兰卡，加纳利群岛\nAISTATS（International Conference on Artificial Intelligence and Statistics）是人工智能与统计学的国际会议，始于1985年，关注人工智能、机器学习、统计学及相关领域。今年AISTATS将于ALT（算法学习理论）会议联合举办。\n官网：http://www.aistats.org/\n3. ICLR 2018\n决议公布日期：1月29日\n会议时间：4月30日~5月3日\n会议地址：温哥华，加拿大\nICLR（International Conference on Learning Representations）虽然是一个很年轻的会议，今年举办到第6届，但已经成为深度学习领域不容忽视的重要会议，甚至有深度学习顶会“无冕之王”之称。\nICLR由Yann LeCun 和 Yoshua Bengio 等大牛发起，会议开创了公开评议机制（open review），但在今年取消了公开评议，改为双盲评审。\n官网：http://www.iclr.cc\n4. COLT 2018\n截稿时间：2月16日\n会议时间：7月5日~9日\n会议地点：斯德哥尔摩，瑞典\nCOLT全称是计算学习理论年会（Annual Conference on Computational Learning Theory），这是计算学习理论最重要的会议，由ACM每年举办。会议关注学习理论的广泛主题，包括学习算法的设计和分析、学习的统计和计算复杂性、学习的优化方法、无监督、半监督、在线和主动学习等等。\n官网：http://www.learningtheory.org/colt2018/index.html\n5. ICML 2018\n截稿时间：2月9日\n会议时间：7月10日~15日\n会议地点：斯德哥尔摩，瑞典\nICML（国际机器学习会议）是机器学习领域最具影响力的学术会议之一，始于1980年，由国际机器学习学会（IMLS）每年举办一次。\n官网：https://icml.cc/\n6. IJCAI-ECAI-2018\n截稿时间：1月31日\n会议日期：7月13日~19日\n会议地点：斯德哥尔摩，瑞典\nIJCAI（国际人工智能联合会议）是人工智能领域最主要的综合性学术会议之一，由于领域热度上涨，从 2016 年起，IJCAI 从原来的每两年举办一次改为每年举办一次。今年来华人在IJCAI的参与度不断增加，尤其是南京大学的周志华教授将担任 IJCAI-21 的程序主席，成为 IJCAI 史上第一位华人大会程序主席。\nECAI（欧洲人工智能会议）是在欧洲举行的主要人工智能和机器学习会议，始于1974年，由欧洲人工智能协调委员会主办。ECAI通常与IJCAI和AAAI并称AI领域的三大顶会。\n今年IJCAI和ECAI两个会议将与7月13日~19日再瑞典首都斯德哥尔摩联合举办。\n官网：https://www.ijcai-18.org/\n7. UAI 2018\n截稿时间：3月9日\n会议时间：8月6日~10日\n会议地点：蒙特雷，美国\nUAI（Conference on Uncertainty in Artificial Intelligence）是不确定性人工智能领域的重要会议，涉及表示、推理、学习等领域，由AUAI（Association of UAI）每年举办。\n官网：http://www.auai.org/uai2018/index.php\n8. NIPS 2018\n截稿时间：5月31日\n会议时间：12月3日~8日\n会议地点：蒙特利尔，加拿大\nNIPS的全称是神经信息处理系统会议（Conference and Workshop on Neural Information Processing Systems），始于1987年，是神经计算和机器学习领域的顶级会议。早期NIPS论文涉及的主题范围很广，包括从解决纯工程问题到使用计算机模型了解生物神经系统等等。之后，生物系统和人工系统的研究发生了分化，近年的 NIPS 会议一直以机器学习、人工智能和统计学的论文为主。NIPS会议固定在每年的 12 月举行, 由 NIPS 基金会主办。\nNIPS近年的主要受邀演讲人包括：\nNIPS一直保持较低的论文录取率，2017年NIPS一共录取了678篇论文，录取率约为20.9%。下图显示了2006~2017年NIPS的录取论文数量。\nNIPS官网：https://nips.cc/\n计算机视觉/模式识别\n9. CVPR 2018\n会议时间：6月18日~22日\n会议地点：盐湖城，UTAH\n在计算机视觉领域，CVPR、ICCV和ECCV被称为“三大顶会”。其中ICCV和ECCV每两年召开，下一届ICCV将于2019年在韩国首尔举行。\nCVPR（IEEE Computer Vision and Pattern Recognition conference）是是由 IEEE 主办的计算机视觉和模式识别领域的顶级会议，始于1983年，每年举办。CVPR除主会外，还包括多个workshop和短期课程，为学生、学术界和工业界研究人员提供重要的交流和学习场所。CVPR颁布的主要奖项有最佳论文奖、最佳学生论文奖、Longuet-Higgins奖（十年时间检验奖）和PAMI青年科学家奖。\n官网：http://cvpr2018.thecvf.com/\n10. ECCV 2018\n截稿时间：3月14日\n会议时间：9月8日~14日\n会议地点：慕尼黑，德国\nECCV全称欧洲计算机视觉大会（European Conference on Computer Vision），每两年举办一次，与ICCV错开举办。像计算机视觉的其他会议一样，ECCV主会包含tutorial talks，technical sessions和poster sessions，此外也有tutorial和workshop。\n官网：https://eccv2018.org\n自然语言处理/计算语言学\n11. ACL 2018\n截稿时间：2月22日\n会议时间：7月15日~20日\n会议地点：墨尔本，澳大利亚\nACL（The Association for Computational Linguistics）是计算语言学领域的顶级会议，由由国际计算语言学协会在每年夏天举办。ACL有着悠久的历史，它成立于1962年，最初名为机器翻译和计算语言学协会（AMTCL），在1968年更名为ACL。\n官网：http://acl2018.org/\n12. COLING 2018\n截稿时间：5月16日\n会议时间：8月22日~25日\n会议地点：新墨西哥，USA\nCOLING也是计算语言学/自然语言处理的重要会议，全称是国际计算语言学大会（International Conference on Computational Linguistics），每两年举办一次。\n官网：http://coling2018.org/\n13. EMNLP 2018\n会议时间：10月31日~11月4日\n会议地点：布鲁塞尔，比利时\nEMNLP（Conference on Empirical Methods in Natural Language Processing）是自然语言处理领域的顶会，由ACL学会下的语言数据特殊兴趣小组SIGDAT组织，自1996年起每年举行。\n点击有惊喜"}
{"content2":"根据市场研究报告，2015年全球机器视觉市场规模为80.8亿美元，到2020年市场规模将达125亿美元，其中，2015年中国机器视觉市场规模61.2亿元，到2020年市场规模将达152亿元，同比增速为28.57%。伴随着人工智能产业升温，机器视觉行业有望迈向新的发展阶段,可以说，机器视觉市场迎来了行业发展的黄金期。\n机器视觉通过计算机或图像处理以及相关设备来模拟人类视觉功能，从客观事物的图像中提取信息进行处理，以让机器获得相关视觉信息并加以理解，最终用于实际检测和控制等领域，是未来能否真正实现万物互联和智能制造的关键，涉及人工智能、神经生物学、计算机科学、图像处理、模式识别等诸多领域，是跨学科的集成式创新领域。\n经常有人问，智能化与自动化的区别在哪里？答案就是感知能力，包括了视觉、听觉、触觉等。和人一样，目前机器人的视觉系统正在成为它们接受信息最多的“器官”。\n而新一代的技术使机器人拥有了一双“3D眼睛”，利用最新的激光(蓝光)投影扫描成像技术，它2900万像素的摄像头可以在一秒钟内连续拍摄30张图片，建立三维立体模型，并给每个零件建立坐标。通过坐标定位，它能够在杂乱无章堆放的零件中分辨出哪件产品在上面、哪件在下面，甚至可以按照要求抓取不规则零件的任何一个部位。比方拿一个扳手，就拿中间而不拿两头。\n此外，机器人的眼睛不但能建立3D模型，还具有射频感应或扫码技术。利用扫码技术，AGV小车(自动导引运输车)每到一个工位，就扫码获取信息指令，并完成相关操作。\n尽管我国机器视觉行业起步较晚，但发展速度极快，已成为继美国和日本之后的全球第三大机器视觉市场。面对竞争日益激烈的内地市场，机器视觉厂商推出产品的速度加快，产品技术发展迅猛，这必然对竞争者提出了更高的要求，以往简单的模仿复制以不再可行，自己的技术和产品特色才是厂商们需要考虑和挖掘的重点。\n如果在机器视觉领域掉队，今后我们将会受制于国外，只能沦为国外机器视觉上层二次应用开发商以及产品代理商，而且成本高昂，也无法完成国内制造业的转型升级，更莫谈智能制造。朗锐智科认为，随着机器视觉的成本逐渐降低，有更多的方案、更强大的硬件、更优的算法出现，再加上物联网连接所有装置，机器视觉将成为绝佳的数据收集工具。"}
{"content2":"K-means\n已知样本集，其中每一个观测都是d-维实向量 ，K-means要把这m个样本划分到k个集合中(k ≤ m)，使得组内平方和(WCSS)最小。\n标准算法\n最常用的算法使用了迭代优化的技术。它被称为K-means而广为使用，有时也被称为Lloyd算法（尤其在计算机科学领域）。已知初始的k个聚类质心点，算法按照下面两个步骤交替进行。\n分配(Assignment)：将每个样本i分配到聚类中，使得组内平方和（WCSS）达到最小。\n更新(Update)：对于上一步得到的每一个聚类，以聚类中样本值的图心，作为新的均值点。\nK-means面对的第一个问题是如何保证收敛，首先可以固定每个类的质心 ，调整每个样本的所属类别  来让WCSS减小；同样固定  ，调整每个类的质心  也可以让WCSS减小，这两个过程就是内循环中使J单调递减的过程。当WCSS递减到最小时，和c也同时收敛。（在理论上，可以有多组不同的和c值能够使得J取得最小值，但这种现象实际上很少见）。\niris\n我们用非常著名的iris数据集。\nfrom sklearn import datasets from matplotlib import pyplot as plt import numpy as np iris = datasets.load_iris() X, y = iris.data, iris.target\ndata = X[:,[1,3]] # 为了便于可视化，只取两个维度 fig, plt.scatter(data[:,0],data[:,1]) plt.show()\n欧式距离\ndef distance(p1,p2): tmp = np.sum((p1-p2)**2) return np.sqrt(tmp)\n随机质心\ndef rand_center(data, k): \"\"\"Generate k center within the range of data set.\"\"\" n = data.shape[1] centroids = np.zeros((k,n)) # k个质心，每个质心的维度和样本维度相同 for i in range(n): dmin, dmax = np.min(data[:, i]), np.max(data[:, i]) centroids[:, i] = dmin + (dmax - dmin) * np.random.rand(k) return centroids centroids = rand_center(data,2) centroids\narray([[ 2.15198267, 2.42476808], [ 2.77985426, 0.57839675]])\n完整代码\nkmeans有个缺点，就是可能陷入局部最小值，有改进的方法，比如二分k均值；也可以多计算几次，取效果好的结果。\ndef kmeans(data,k=2): def _distance(p1,p2): tmp = np.sum((p1-p2)**2) return np.sqrt(tmp) def _rand_center(data,k): \"\"\"Generate k center within the range of data set.\"\"\" n = data.shape[1] # features centroids = np.zeros((k,n)) for i in range(n): dmin, dmax = np.min(data[:,i]), np.max(data[:,i]) centroids[:,i] = dmin + (dmax - dmin) * np.random.rand(k) return centroids def _converged(centroids1, centroids2): # if centroids not changed, we say 'converged' set1 = set([tuple(c) for c in centroids1]) set2 = set([tuple(c) for c in centroids2]) return (set1 == set2) m = data.shape[0] # number of entries centroids = _rand_center(data,k) c = np.zeros(m, dtype=np.int) # track the nearest centroid assement = np.zeros(m) # for the assement of our model converged = False while not converged: old_centroids = np.copy(centroids) for i in range(m): # determine the nearest centroid and track it with label min_dist, min_index = np.inf, -1 for j in range(k): dist = _distance(data[i],centroids[j]) if dist < min_dist: min_dist, min_index = dist, j c[i] = j assement[i] = _distance(data[i], centroids[c[i]])**2 # update centroid for j in range(k): centroids[j] = np.mean(data[c==j], axis=0) converged = _converged(old_centroids, centroids) return centroids, c, np.sum(assement)\n由于算法可能局部收敛的问题，随机多运行几次，取最优值\nbest_assement = np.inf best_centroids = None best_c = None for i in range(10): centroids, c, assement = kmeans(data,2) if assement < best_assement: best_assement = assement best_centroids = centroids best_c = c data0 = data[best_c==0] data1 = data[best_c==1]\n如下图，我们把数据分为两簇：\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(12,5)) ax1.scatter(data[:,0],data[:,1],c='c',s=30,marker='o') ax2.scatter(data0[:,0],data0[:,1],c='r') ax2.scatter(data1[:,0],data1[:,1],c='c') ax2.scatter(centroids[:,0],centroids[:,1],c='b',s=120,marker='o') plt.show()\n最大似然估计\n最大似然估计是一种确定模型参数值的方法。确定参数值的过程，是找到能最大化模型产生真实观察数据可能性的那一组参数。同时我们将找到的参数值称为最大似然估计（maximum likelihood estimates，MLE）。\n最大似然估计总是能精确地得到解吗？\n简单来说，不能。更有可能的是，在真实的场景中，对数似然函数的导数仍然是难以解析的（也就是说，很难甚至不可能人工对函数求微分）。因此，一般采用期望最大化（EM）算法等迭代方法为参数估计找到数值解，但总体思路还是一样的。\n为什么叫「最大似然（最大可能性）」，而不是「最大概率」呢？\n我们先来定义 P(data; μ, σ) 它的意思是「在模型参数μ、σ条件下，观察到数据 data 的概率」。值得注意的是，我们可以将其推广到任意数量的参数和任何分布。\n另一方面，L(μ, σ; data) 的意思是「我们在观察到一组数据 data 之后，参数μ、σ取特定的值的似然度。」\n上面的公式表示，给定参数后数据的概率等于给定数据后参数的似然度。但是，尽管这两个值是相等的，但是似然度和概率从根本上是提出了两个不同的问题——一个是关于数据的，另一个是关于参数值的。这就是为什么这种方法被称为最大似然法（极大可能性），而不是最大概率。\n什么时候最小二乘参数估计和最大似然估计结果相同？\n结果表明，当模型被假设为高斯分布时，MLE 的估计等价于最小二乘法。\nKullback–Leibler divergence\n相对熵（relative entropy）又称为KL散度（Kullback–Leibler divergence，简称KLD），信息散度（information divergence），信息增益（information gain）。\nKL散度是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来 度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。\n定义\n对于离散随机变量，其概率分布P和Q的KL散度可按下式定义为\n对于连续随机变量，其概率分布P和Q可按积分方式定义为\n流形（manifolds）\n流形（英语：Manifolds），是局部具有欧几里得空间性质的空间，是欧几里得空间中的曲线、曲面等概念的推广。欧几里得空间就是最简单的流形的实例。地球表面这样的球面则是一个稍微复杂的例子。一般的流形可以通过把许多平直的片折弯并粘连而成。\n任何一个流形都可以嵌入到足够高维度的欧氏空间中 (Whitney嵌入定理)。\n流形的特殊性质：\n不满足平行公设：存在过空间中任意两点的平行直线(测地线)\n球面：任意两条测地线(大圆弧)都相交\n测地三角形的内角和不一定等于180度"}
{"content2":"计算机视觉领域正在从统计方法转向深度学习神经网络方法。\n计算机视觉中仍有许多具有挑战性的问题需要解决。然而，深度学习方法正在针对某些特定问题取得最新成果。\n在最基本的问题上，最有趣的不仅仅是深度学习模型的表现;事实上，单个模型可以从图像中学习意义并执行视觉任务，从而无需使用专门的手工制作方法。\n在这篇文章中，您将发现九个有趣的计算机视觉任务，其中深度学习方法取得了一些进展。\n让我们开始吧。\n概观\n在这篇文章中，我们将研究以下使用深度学习的计算机视觉问题：\n图像分类\n具有本地化的图像分类\n物体检测\n对象分割\n图像样式转移\n图像着色\n影像重建\n图像超分辨率\n图像合成\n其他问题\n注意，当涉及图像分类（识别）任务时，已采用ILSVRC的命名约定。虽然任务集中在图像上，但它们可以推广到视频帧。\n我试图关注您可能感兴趣的最终用户问题的类型，而不是深度学习能够做得更好的学术问题。\n每个示例都提供了问题的描述，示例以及对演示方法和结果的论文的引用。\n您是否有最喜欢的深度学习计算机视觉应用程序未列出？\n请在下面的评论中告诉我。\n图像分类\n图像分类涉及为整个图像或照片分配标签。\n该问题也被称为“对象分类”，并且可能更一般地称为“图像识别”，尽管后一任务可以应用于与分类图像内容相关的更广泛的任务集。\n图像分类的一些示例包括：\n1、将X射线标记为癌症与否（二元分类）。\n2、对手写数字进行分类（多类分类）。\n3、为脸部照片指定名称（多类别分类）。\n用作基准问题的图像分类的流行示例是MNIST数据集。\n分类数字照片的流行真实版本是街景房号（SVHN）数据集。\n有许多图像分类任务涉及对象的照片。 两个流行的例子包括CIFAR-10和CIFAR-100数据集，这些数据集的照片分别分为10类和100类。\n大规模视觉识别挑战赛（ILSVRC）是一项年度竞赛，其中团队在从ImageNet数据库中提取的数据上竞争一系列计算机视觉任务的最佳性能。图像分类方面的许多重要进步来自于发布在该挑战或来自该挑战的任务的论文，最值得注意的是关于图像分类任务的早期论文。例如：\n使用深度卷积神经网络的ImageNet分类，2012。\n用于大规模图像识别的非常深的卷积网络，2014。\n围绕卷积更深入，2015年。\n图像识别的深度残留学习，2015年。\n具有本地化的图像分类\n具有本地化的图像分类涉及为图像分配类标签并通过边界框（在对象周围绘制框）来显示图像中对象的位置。\n这是一个更具挑战性的图像分类版本。\n本地化图像分类的一些示例包括：\n1.将X射线标记为癌症或在癌症区域周围画一个盒子。\n2.在每个场景中对动物的照片进行分类并在动物周围画一个盒子。\n用于具有定位的图像分类的经典数据集是PASCAL视觉对象类数据集，或简称为PASCAL VOC（例如VOC 2012）。这些是多年来在计算机视觉挑战中使用的数据集。\n该任务可以涉及在图像中的同一对象的多个示例周围添加边界框。 因此，该任务有时可称为“对象检测”。\n用于本地化图像分类的ILSVRC2016数据集是一个流行的数据集，包含150,000张照片和1000种对象。\n关于本地化图像分类的论文的一些例子包括：\n选择性搜索对象识别，2013年。\n用于精确对象检测和语义分割的丰富特征层次结构，2014年。\n快速R-CNN，2015年。\n物体检测\n物体检测是具有定位的图像分类的任务，尽管图像可能包含需要定位和分类的多个对象。\n与简单的图像分类或具有定位的图像分类相比，这是一项更具挑战性的任务，因为在不同类型的图像中通常存在多个对象。\n通常，使用并展示用于具有定位的图像分类的技术用于对象检测。\n对象检测的一些示例包括：\n绘制边界框并标记街道场景中的每个对象。\n绘制边界框并在室内照片中标记每个对象。\n绘制边界框并在横向中标记每个对象。\nPASCAL Visual Object Classes数据集或简称PASCAL VOC（例如VOC 2012）是用于对象检测的常见数据集。\n用于多个计算机视觉任务的另一个数据集是Microsoft的上下文数据集中的公共对象，通常称为MS COCO。\n关于物体检测的论文的一些例子包括：\nOverFeat：使用卷积网络的集成识别，本地化和检测，2014年。\n更快的R-CNN：利用区域提案网络实现实时目标检测，2015年。\n您只看一次：统一，实时对象检测，2015年。\n对象分割\n对象分割或语义分割是对象检测的任务，其中在图像中检测到的每个对象周围绘制线。图像分割是将图像分成段的更普遍的问题。\n对象检测有时也称为对象分割。\n与涉及使用边界框来识别对象的对象检测不同，对象分割识别图像中属于对象的特定像素。这就像一个细粒度的本地化。\n更一般地，“图像分割”可以指将图像中的所有像素分割成不同类别的对象。\n同样，VOC 2012和MS COCO数据集可用于对象分割。\nKITTI Vision Benchmark Suite是另一种流行的对象分割数据集，提供用于自动驾驶车辆训练模型的街道图像。\n关于对象分割的一些示例论文包括：\n同步检测和分割，2014年。\n用于语义分割的完全卷积网络，2015。\n用于对象分割和细粒度本地化的超级列，2015。\nSegNet：用于图像分割的深度卷积编码器 - 解码器架构，2016。\nMask R-CNN，2017年。\n风格转移\n风格转移或神经风格转移是从一个或多个图像学习风格并将该风格应用于新图像的任务。\n该任务可以被认为是一种可能没有客观评价的照片滤波器或变换。\n例子包括将特定着名艺术品（例如Pablo Picasso或Vincent van Gogh）的风格应用于新照片。\n数据集通常涉及使用公共领域的着名艺术作品和标准计算机视觉数据集中的照片。\n一些论文包括：\n艺术风格的神经算法，2015。\n使用卷积神经网络的图像样式转移，2016。\n图像着色\n图像着色或神经着色涉及将灰度图像转换为全色图像。\n该任务可以被认为是一种可能没有客观评价的照片滤波器或变换。\n例子包括着色旧的黑白照片和电影。\n数据集通常涉及使用现有的照片数据集并创建模型必须学习着色的照片的灰度版本。\n一些论文包括：\n彩色图像着色，2016年。\n让我们有颜色：全球和本地图像的联合端到端学习，用于同步分类的自动图像着色，2016。\n深色着色，2016。\n影像重建\n图像重建和图像修复是填充图像的缺失或损坏部分的任务。\n该任务可以被认为是一种可能没有客观评价的照片滤波器或变换。\n示例包括重建旧的，损坏的黑白照片和电影（例如照片恢复）。\n数据集通常涉及使用现有的照片数据集并创建模型必须学会修复的损坏版本的照片。\n一些论文包括：\n像素回归神经网络，2016年。\n使用部分卷积的图像修复不规则孔，2018年。\n使用具有带通滤波的深度神经网络进行高度可扩展的图像重建，2018年。\n图像超分辨率\n图像超分辨率是生成具有比原始图像更高分辨率和细节的图像的新版本的任务。\n通常为图像超分辨率开发的模型可用于图像恢复和修复，因为它们解决了相关问题。\n数据集通常涉及使用现有的照片数据集并创建缩小版照片，模型必须学会创建超分辨率版本。\n一些论文包括：\n使用生成对抗网络的照片真实单图像超分辨率，2017。\n深拉普拉斯金字塔网络，快速准确的超分辨率，2017。\nDeep Image Prior，2017。\n图像合成\n图像合成是生成现有图像或全新图像的目标修改的任务。\n这是一个非常广泛的领域，正在迅速发展。\n它可能包括图像和视频的小修改（例如图像到图像的翻译），例如：\n更改场景中对象的样式。\n将对象添加到场景中。\n将面添加到场景中。\n它还可能包括生成全新的图像，例如：\n1、生成面孔。\n2、生成浴室。\n3、生成衣服。\n一些论文包括：\n用深度卷积生成对抗网络学习无监督表示，2015。 使用PixelCNN解码器生成条件图像，2016。\n使用周期一致的对抗网络进行不成对的图像到图像转换，2017。\n其他问题\n还有其他重要且有趣的问题我没有涉及，因为它们不是纯粹的计算机视觉任务。\n值得注意的例子是图像到文本和文本到图像：\n1、图像字幕：生成图像的文本描述。\nShow and Tell：神经图像标题生成器，2014。\n2、图像描述：生成图像中每个对象的文本描述。\n用于生成图像描述的深层视觉语义对齐，2015。\n3、文本到图像：基于文本描述合成图像。\nAttnGAN：使用注意生成对抗网络生成细粒度文本到图像，2017。\n据推测，人们学会在其他模态和图像之间进行映射，例如音频。\n总结\n在这篇文章中，您发现了九种深度学习应用于计算机视觉任务。\n您最喜欢的计算机视觉深度学习的例子是否错过了？\n请在评论中告诉我。\n你有任何问题吗？\n在下面的评论中提出您的问题。"}
{"content2":"计算机视觉-计算机视觉基础\n1、加载、显示、保存图像\nimport argparse import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required=True, help=\"Path to the image\")#读取指定指令，获取图片。参数1：输入指令的头字母，参数2：需要输入的指令 args = vars(ap.parse_args()) image = cv2.imread(args[\"image\"]) #读取指定参数的，读取照片 print \"width: %d pixels\" % (image.shape[1]) # 获取图像的宽度，横向尺寸，图像坐标系中，第二个参数 print \"height: %d pixels\" % (image.shape[0] )#获取图像的高度，竖向尺寸，图像坐标系中，第一个参数 注意：读取图像x,y互换 print \"channels: %d\" % (image.shape[2]) cv2.imshow(\"Image\", image)#显示图片 cv2.imwrite(\"newimage.jpg\", image) #将图片写入指定路径 cv2.waitKey(0)#等待程序结束\n2、图像基础\n(h, w) = image.shape[:2] #（x,y）像素中的显示，获取图片的高度(x)，获取图片的宽度(y) (b, g, r) = image[0, 0] #获取指定位置的像素，存储方式为bgr (cX, cY) = (w / 2, h / 2) #cX:图片的宽度，cY:图片高度 tl = image[0:cY, 0:cX] #获取图片的左上角部分[起始点x坐标：终点x坐标，起点的y坐标:终点y坐标]，得出的值分别指高度和宽度 　 #运用像素指定位置赋值方法，向Background图片上插入图片resized_left for left_x in xrange(0, 500, 1): 　　for left_y in xrange(0, 500, 1):\nBackground[400 + left_x, 20 + left_y] = resized_left[left_x, left_y]\n3、绘图\ncanvas = np.zeros((300, 300, 3), dtype=\"uint8\") #设置画布尺寸 green = (0, 255, 0)#设置线条颜色 cv2.line(canvas, (0, 0), (300, 300), green) #参数1：指定画布，参数2：线条的开始位置，参数3：线条终点位置，参数4：线条颜色 cv2.line(canvas, (300, 0), (0, 300), red, 3) #参数5：线条像素厚度 cv2.rectangle(canvas, (10, 10), (60, 60), green)#参数1：指定画布，参数2：矩形起点位置，参数3：矩形对角线点的位置，线条颜色 cv2.rectangle(canvas, (50, 200), (200, 225), red, 5)#参数5：线条宽度，负数表示填充矩形 (centerX, centerY) = (canvas.shape[1] / 2, canvas.shape[0] / 2)#设置圆心坐标 white = (255, 255, 255)#设置圆的线条颜色 cv2.circle(canvas, (centerX, centerY), r, white)#参数1：画布，参数2：圆心点，参数3：设置圆的半径，设置画圆的线条颜色 cv2.circle(canvas, tuple(pt), radius, color, -1)#参数4：设置画圆的线条粗细，如果为负数，表示填充圆\n4、图像处理\n4.1、翻译\nM = np.float32([[1, 0, -50], [0, 1, -90]]) #定义翻译矩阵 ：参数1：[1,0,x]:x表示像素向左或者向右移动个数，x为负值图像左移，正值为右移定。参数2：[0,1,y]：y表示像素上下移动，y为负值向上移动，正值向下移动。记忆口诀：前左负右正，后上负下正 shifted = cv2.warpAffine(image, M, (image.shape[1],image.shape[0]))#对矩阵进行翻译 参数1：翻译的目标图像，参数2：翻译的矩阵，参数3：翻译后图像大小 #imutils模块的翻译函数 shifted = imutils.translate(image, 0, 100)#参数1：移动的目标图像，参数2：左右移动的值，参数3：上下移动的值\n注：不会改变图像大小。\n4.2、旋转\n注：运用翻译将图片移到中心位置，四周留出黑色边框，在运用旋转（旋转角度为0），可将图片放大\n(h, w) = image.shape[:2] #获取图像的高和宽 (cX, cY) = (w / 2, h / 2) #获取图像的中心点 M = cv2.getRotationMatrix2D((cX, cY), 45, 1.0) #设置旋转矩阵，参数1：旋转点 参数2：旋转角度，正值逆时针旋转，负值顺时针选装，参数3：旋转后图像与原始图像的比例，图像原始大小不会发生变化，类似相机焦距变化 rotated = cv2.warpAffine(image, M, (w, h)) #参数1：目标图像，参数2：旋转矩阵，参数3#旋转后的图像尺寸 #imutils模块的旋转函数 rotated = imutils.rotate(image, 180) #参数1：旋转目标图片 参数2：旋转角度 center=(x,y)可以设置旋转点\n4.3、图像大小调整\n注：改变原始图片的实际大小\nr = 150.0 / image.shape[1] #新图像与旧图像的宽比例 注：方便图片按原比例缩放 dim = (150, int(image.shape[0] * r)) #设置新图像的像素尺寸 resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA) #调整图像大小，返回一个新图像 #运用imutils中的resize函数 resized = imutils.resize(image, width=100) #参数1：目标图像，参数2：设置新图像的宽度，可以为height，运用高度 #运用插值方法缩放大图片，，通过不同的像素填充方法放大图片。 resized = imutils.resize(image, width=image.shape[1] * 3, inter=method)#参数1：目标图片，参数2：处理后图片像素宽（整形），参数3：像素处理方法 \"\"\"method： cv2.INTER_NEAREST:最近邻内插值 cv2.INTER_LINEAR:双线性插值 cv2.INTER_AREA：区域插值 cv2.INTER_CUBIC：双三次插值 cv2.INTER_LANCZOS4 ：双三次插值\n4.4、图像翻转\nflipped = cv2.flip(image, 1)#水平翻转 flipped = cv2.flip(image, 0)#上下翻转 flipped = cv2.flip(image, -1)#水平翻转后上下翻转\n4.5、图像裁剪\nface = image[85:250, 85:220] #参数1：裁切高度 x开始位置：x结束位置，参数2：裁切宽度 y开始位置：y结束位置，返回新图片\n4.6、图像像素像素值操作\n注：修改图片的亮度\nM = np.ones(image.shape, dtype = \"uint8\") * 100 #设置与图片大下相同的矩阵，矩阵填充值为100 added = cv2.add(image, M)#将原始图片与新矩阵相加，像素亮度提高100 M = np.ones(image.shape, dtype = \"uint8\") * 50#设置与图片大下相同的矩阵，矩阵填充值为50 subtracted = cv2.subtract(image, M)#将原始图片与新矩阵相减，像素亮度降低50\n4.7、按位操作\n注：主要是黑白图像处理\nbitwiseAnd = cv2.bitwise_and(rectangle, circle)#当且仅当俩个相同位置的像素大于0，才返回真 bitwiseOr = cv2.bitwise_or(rectangle, circle)#俩个相同位置的像素有一个大于0，返回真 bitwiseXor = cv2.bitwise_xor(rectangle, circle)#当且仅当俩个相同位置的像素只有一个大于0，才返回真 bitwiseNot = cv2.bitwise_not(circle)#像素值取反\n4.8、掩蔽\n注：提取图像中感兴趣的部分，遮掩的必须用关键字mask\nmask = np.zeros(image.shape[:2], dtype=\"uint8\") #设置掩蔽画布的大小 cv2.rectangle(mask, (0, 90), (290, 450), 255, -1)#设置不掩蔽的地方 masked = cv2.bitwise_and(image, image, mask=mask)#图片显示的区域\n4.9、像素分割与合并\n(B, G, R) = cv2.split(image) #像素分离图像，B,G,R图像的值为整数，非BGR像素矩阵组成，其值是如何转变的？ merged = cv2.merge([B, G, R]) #三个色彩合并，还原为彩色图像，像素由BGR像素矩阵组成 zeros = np.zeros(image.shape[:2], dtype = \"uint8\")#建立一个二值画布，与各个色彩通道合并，像是各个色彩通道的影响 cv2.imshow(\"Red\", cv2.merge([zeros, zeros, R]))#只有红色的图片 cv2.imshow(\"Green\", cv2.merge([zeros, G, zeros]))#只有绿色的图片 cv2.imshow(\"Blue\", cv2.merge([B, zeros, zeros]))#只有蓝色的图片\n问：如何从BGR转换成整数值\n5、内核\n略\n6、形态操作\nimage = cv2.imread(args[\"image\"]) #打开一张图片 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)#将图片转为二值划 #侵蚀 将前景物体变小，理解成将图像断开裂缝变大（在图片上画上黑色印记，印记越来越大） for i in xrange(0, 3): eroded = cv2.erode(gray.copy(), None, iterations=i + 1)#参数1：需要侵蚀的图像，参数2：结构元素（） 参数3：迭代次数，值越大，侵蚀越严重 cv2.imshow(\"Eroded {} times\".format(i + 1), eroded) cv2.waitKey(0) #扩张 将前景物体变大，理解成将图像断开裂缝变小（在图片上画上黑色印记，印记越来越小） for i in xrange(0, 3): dilated = cv2.dilate(gray.copy(), None, iterations=i + 1)#参数1：需要侵蚀的图像，参数2：结构元素（） 参数3：迭代次数，值越大，扩张越大 cv2.imshow(\"Dilated {} times\".format(i + 1), dilated) cv2.waitKey(0) #开盘 应用侵蚀以去除小斑点，然后应用扩张以重新生成原始对象的大小，用于消除杂质 kernelSizes = [(3, 3), (5, 5), (7, 7)]#定义结构化元素的宽度和高度 for kernelSize in kernelSizes: kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize) #参数1：结构化元素的类型 参数2：构造元素的大小 opening = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)#参数1：形态学运算的图像 参数2：形态操作的实际 类型 参数3：内核/结构化元素 cv2.imshow(\"Opening: ({}, {})\".format(kernelSize[0], kernelSize[1]), opening) cv2.waitKey(0） \"\"\"形态操作实际类型： cv2.MORPH_CLOSE(闭幕):用于封闭对象内的孔或将组件连接在一起 cv2.MORPH_GRADIENT(形态梯度):用于确定图像的特定对象的轮廓 \"\"\" #顶帽/黑帽 显示黑色背景上的图像的 明亮区域。适合于灰度图像 rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (13, 5))#定义宽度为13 像素和高度为 5像素的 矩形 结构元素 blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, rectKernel)#参数1：形态学运算的图像 参数2：形态操作的实际 类型 参数3：内核/结构化元素 tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel) #参数1：形态学运算的图像 参数2：形态操作的实际 类型 参数3：内核/结构化元素\n7、平滑和模糊\nkernelSizes = [(3, 3), (9, 9), (15, 15)] #定义内核大小参数列表，内核越大，模糊越明显 # loop over the kernel sizes and apply an \"average\" blur to the image for (kX, kY) in kernelSizes: blurred = cv2.blur(image, (kX, kY))#使用平均模糊方法,参数1：模糊对象，参数2：矩阵大小 cv2.imshow(\"Average ({}, {})\".format(kX, kY), blurred) cv2.waitKey(0) \"\"\"模糊方法： 平均模糊：过度模糊图像并忽略重要的边缘 blurred =cv2.blur(image, (kX, kY)) 高斯：保留更多的图像边缘 blurred =cv2.GaussianBlur(image, (kX, kY), 0)参数1：模糊对象，参数2：矩阵大小 参数3：标准方差 中位数模糊： 图像中去除盐和胡椒，图像中的杂质点 blurred = cv2.medianBlur(image, k)参数1：模糊对象，参数2：中位数值，为整型数据，数据越大图像越模糊 双边模糊： 减少噪音同时仍然保持边缘，我们可以使用双边模糊。双边模糊通过引入两个高斯分布来实现 blurred =cv2.bilateralFilter(image, diameter, sigmaColor, sigmaSpace)参数1：想要模糊的图像。参数2：像素邻域的直径 - 这个直径越大，模糊计算中包含的像素越多。参数3：颜色标准差，模糊时将考虑邻域中的更多颜色，相似颜色的像素才能显\n着地影响模糊，参数4：空间标准偏差，更大的值意味着像素越远离中心像素直径 将影响模糊计算。后面3个参数都为整型参数 \"\"\"\n8、照明和色彩空间\n#RGB 红、黄、蓝组成的颜色矩阵，每个色度值范围[0,255] image = cv2.imread(args[\"image\"]) for (name, chan) in zip((\"B\", \"G\", \"R\"), cv2.split(image)): cv2.imshow(name, chan) #HSV 色调(H):们正在研究哪种“纯”的颜色。饱和度(S)：颜色如何“白，例如纯红，随着零饱和度的颜色是纯白色。价值(V)：该值允许我们控制我们的颜色的亮度，零值表示纯黑色 hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) for (name, chan) in zip((\"H\", \"S\", \"V\"), cv2.split(hsv)): cv2.imshow(name, chan) #L * a * b *表 L通道:像素的“亮度”。a通道：源于L通道的中心，在频谱的一端定义纯绿色，另一端定义纯红色。b通道： 也来自于L通道的中心，但是垂直于a通道。 lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB) for (name, chan) in zip((\"L*\", \"a*\", \"b*\"), cv2.split(lab)): cv2.imshow(name, chan) #灰度：转换成灰度级时，每个RGB通道 不是 均匀加权 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n9、阀值\n阀值：图像的二值化\nimage = cv2.imread(args[\"image\"]) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)#将图像二值化 blurred = cv2.GaussianBlur(gray, (7, 7), 0) (T,threshInv)=cv2.threshold(blurred,200,255,cv2.THRESH_BINARY_INV) #参数1：希望阈值的灰度图像。参数2：手动提供我们的T阈值。参数3：设置输出值。参数4：阈值方法（白色背景，黑色图），将像素值小于参数2的值换成参数3输出，大于参数2的值，输出值为0 cv2.imshow(\"Threshold Binary Inverse\", threshInv) (T, thresh) =cv2.threshold(blurred,200,255,cv2.THRESH_BINARY)#参数1：希望阈值的灰度图像。参数2：手动提供我们的T阈值。参数3：设置输出值。参数4：阈值方法（黑背景，白色图），将像素值大于参数2的值换成参数3输出，小于参数2的值，输出值为0。 cv2.imshow(\"Threshold Binary\", thresh) cv2.imshow(\"Output\", cv2.bitwise_and(image, image, mask=threshInv)) #大津方法 参数1：希望阈值的灰度图像。参数2：大津的方法将自动计算出我们的T的最优值。参数3：阈值的输出值，只要给定像素通过阈值测试。参数4：对应于Otsu的阈值法 (T, threshInv) = cv2.threshold(blurred, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU) #自适应阀值法：参数1：希望阈值的灰度图像。参数2：参数是输出阈值。参数3：自适应阈值法。参数4：阈值方法。参数5：是我们的像素邻域大小。参数6：微调 我们的阈值 thresh=cv2.adaptiveThreshold(blurred,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 25, 15) thresh =threshold_adaptive(blurred,29,offset=5).astype(\"uint8\")* 255 thresh = cv2.bitwise_not(thresh)#阈值适配函数执行自适应阈值\n10.1、图像渐变\n图像渐变：图像梯度主要应用与边缘检测\n#Sobel内核 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) gX = cv2.Sobel(gray, ddepth=cv2.CV_64F, dx=1, dy=0) #计算x方向的梯度 gY = cv2.Sobel(gray, ddepth=cv2.CV_64F, dx=0, dy=1) #计算y方向的梯度 gX = cv2.convertScaleAbs(gX) #转换回8位无符号整型 gY = cv2.convertScaleAbs(gY) #转换回8位无符号整型 sobelCombined = cv2.addWeighted(gX, 0.5, gY, 0.5, 0) #将俩个图像组合成单个图像 gX = cv2.Sobel(gray, cv2.CV_64F, 1, 0) #计算x梯度方向 gY = cv2.Sobel(gray, cv2.CV_64F, 0, 1) #计算y梯度方向 mag = np.sqrt((gX ** 2) + (gY ** 2)) #梯度幅度计算：平方梯度的平方根 X和 ÿ 相加 orientation = np.arctan2(gY, gX) * (180 / np.pi) % 180 #梯度方向计算：两个梯度的反正切 idxs = np.where(orientation >= args[\"lower_angle\"], orientation, -1)#手柄选择，参数1： 函数是我们要测试的条件，寻找大于最小提供角度的索引。参数2：要检查的阵列在哪里。参数3：特定值设置为-1。 idxs = np.where(orientation <= args[\"upper_angle\"], idxs, -1) mask = np.zeros(gray.shape, dtype=\"uint8\")#构造一个 掩码 - 所有具有相应idxs 值> -1的坐标 都设置为 255 （即前景）。否则，它们保留为 0（即背景） mask[idxs > -1] = 255\n10.2、边缘检测\n边缘类型：\n步边：阶跃边缘形式当存在来自不连续到另一的一侧的像素强度的突然变化\n斜坡边缘：斜坡边缘就像一个阶跃边缘，仅在像素强度的变化不是瞬时的。相反，像素值的变化发生短而有限的距离\n岭边：脊边缘是相似于两个结合  斜坡边缘，一个右对另一碰撞\n屋顶边：顶部有一个短而有限的高原的边缘不同\n边缘检测法：\n对图像应用高斯平滑来帮助减少噪点。\n使用Sobel内核计算和图像渐变。\n应用非最大值抑制来仅保持指向梯度方向的梯度幅度像素的局部最大值。\n定义和应用和阈值滞后阈值\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) blurred = cv2.GaussianBlur(gray, (5, 5), 0) wide = cv2.Canny(blurred, 10, 200)#参数1：想要检测边缘的图像。参数2和3：分别提供阈值下限和阈值上限 #自动调整边缘检测参数 #自动调整边缘检测参数函数 def auto_canny(image, sigma=0.33): v = np.median(image) lower = int(max(0, (1.0 - sigma) * v)) upper = int(min(255, (1.0 + sigma) * v)) edged = cv2.Canny(image, lower, upper) return edged #自动调整边缘检测参数函数运用 blurred = cv2.GaussianBlur(gray, (3, 3), 0) auto = imutils.auto_canny(blurred)\n11.1、查找和绘制轮廓\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) (cnts, _) = cv2.findContours(gray.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)#参数1：需要绘制轮廓的图像。参数2：返回的轮廓数量的标志。参数3：轮廓压缩类型。返回值：第一个值是轮廓本上。第二个值是要检查的轮廓层次结构 clone = image.copy() cv2.drawContours(clone, cnts, -1, (0, 255, 0), 2)#参数1：要绘制轮廓的图像。参数2：使用的轮廓列表。参数3：cnts列表中的轮廓索引，-1表示绘制所有轮廓，0是仅画第一个，1表示绘制第二个轮廓。参数3：绘制轮廓的颜色。参数4：绘制轮廓线的像素 #轮廓单个绘制 for (i, c) in enumerate(cnts): print \"Drawing contour #{}\".format(i + 1) cv2.drawContours(clone, [c], -1, (0, 255, 0), 2) cv2.imshow(\"Single Contour\", clone) cv2.waitKey(0) #返回所有轮廓外观： (cnts, _) = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #轮廓外观与掩码的一起使用 for c in cnts: # construct a mask by drawing only the current contour mask = np.zeros(gray.shape, dtype=\"uint8\") cv2.drawContours(mask, [c], -1, 255, -1) # show the images cv2.imshow(\"Image\", image) cv2.imshow(\"Mask\", mask) cv2.imshow(\"Image + Mask\", cv2.bitwise_and(image, image, mask=mask)) #补充：运用霍夫找圆心 gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #输出图像大小，方便根据图像大小调节minRadius和maxRadius print(img.shape) #霍夫变换圆检测 circles= cv2.HoughCircles(gray,cv2.HOUGH_GRADIENT,1,100,param1=100,param2=30,minRadius=5,maxRadius=300) #输出返回值，方便查看类型 print(circles) #输出检测到圆的个数 print(len(circles[0])) #根据检测到圆的信息，画出每一个圆 for circle in circles[0]: #圆的基本信息 print(circle[2]) #坐标行列 x=int(circle[0]) y=int(circle[1]) #半径 r=int(circle[2]) #在原图用指定颜色标记出圆的位置 img=cv2.circle(img,(x,y),r,(0,0,255),-1) #显示新图像 cv2.imshow('res',img)\n11.2、简单的轮廓属性\n质心：质心”或“质心” 是图像中物体的中心  （x，y）坐标\n#绘制轮廓质心 (cnts, _) = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) clone = image.copy() for c in cnts: # compute the moments of the contour which can be used to compute the # centroid or \"center of mass\" of the region M = cv2.moments(c) cX = int(M[\"m10\"] / M[\"m00\"]) cY = int(M[\"m01\"] / M[\"m00\"]) # draw the center of the contour on the image cv2.circle(clone, (cX, cY), 10, (0, 255, 0), -1)\n面积和周长：轮廓的面积是轮廓轮廓内部的像素数。类似地，  周长  （有时称为  弧长）是轮廓的长度\n#获取轮廓的面积与周长 for (i, c) in enumerate(cnts): area = cv2.contourArea(c) perimeter = cv2.arcLength(c, True) print \"Contour #%d -- area: %.2f, perimeter: %.2f\" % (i + 1, area, perimeter) cv2.drawContours(clone, [c], -1, (0, 255, 0), 2) #计算图像的质心，并在图像上显示轮廓数，以便我们可以将形状与终端输出相关联。 M = cv2.moments(c) cX = int(M[\"m10\"] / M[\"m00\"])#？ cY = int(M[\"m01\"] / M[\"m00\"])#？ cv2.putText(clone, \"#%d\" % (i + 1), (cX - 20, cY), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (255, 255, 255), 4)\n边框：边界”和“包含”整个图像的轮廓区域\nfor c in cnts: # fit a bounding box to the contour (x, y, w, h) = cv2.boundingRect(c) cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)\n旋转边框：\n#绘制轮廓的旋转边框 for c in cnts: # fit a rotated bounding box to the contour and draw a rotated bounding box box = cv2.minAreaRect(c)#参数：我们的轮廓。并返回一个包含3个值的元组。元组的第一个值是旋转的边界框的起始 （x，y）坐标。第二个值是边界框的宽度和高度。而最终的值就是我们形状或旋转的角度 box = np.int0(cv2.cv.BoxPoints(box))#宽度和高度以及旋转角转换为一组坐标点 cv2.drawContours(clone, [box], -1, (0, 255, 0), 2)\n最小封闭圆：\nfor c in cnts: ((x, y), radius) = cv2.minEnclosingCircle(c)#返回圆的中心的（x，y）坐标以及圆的 半径 cv2.circle(clone, (int(x), int(y)), int(radius), (0, 255, 0), 2)\n装配椭圆：将椭圆拟合到轮廓上很像将轮廓的矩形装配到轮廓上\nfor c in cnts: if len(c) >= 5： ellipse = cv2.fitEllipse(c) cv2.ellipse(clone, ellipse, (0, 255, 0), 2)\n11.3、高级轮廓\n长宽比：宽高比=图像宽度/图像宽度\n程度：边界框区域=边界框宽度X边框高度\n凸海鸥：欧氏空间中的一组  X点，凸包是包含这些X点的最小可能凸集\n密实度：坚固度=轮廓面积/凸包面积\n#识别‘X’与‘O’ (cnts, _) = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)#获取轮廓列表 for (i, c) in enumerate(cnts): area = cv2.contourArea(c)#获取轮廓面积 (x, y, w, h) = cv2.boundingRect(c)#获取轮廓的起始坐标，边界的宽度和高度 hull = cv2.convexHull(c) #获取形状的实际凸包 hullArea = cv2.contourArea(hull)#计算凸包面积 solidity = area / float(hullArea)#获取坚固度 char = \"?\" #依据坚固度，判断图像的形状 if solidity > 0.9: char = \"O\" elif solidity > 0.5: char = \"X\" #绘制轮廓 if char != \"?\": cv2.drawContours(image, [c], -1, (0, 255, 0), 3) cv2.putText(image, char, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 4) print \"%s (Contour #%d) -- solidity=%.2f\" % (char, i + 1, solidity) cv2.imshow(\"Output\", image) cv2.waitKey(0)\n#识别俄罗斯方块 (cnts, _) = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)#获取图片中的轮廓列表 hullImage = np.zeros(gray.shape[:2], dtype=\"uint8\") for (i, c) in enumerate(cnts): area = cv2.contourArea(c)#获取轮廓面积 (x, y, w, h) = cv2.boundingRect(c)#获取轮廓边界 aspectRatio = w / float(h)#获取宽高比 extent = area / float(w * h)#获取当前轮廓的范围 hull = cv2.convexHull(c) hullArea = cv2.contourArea(hull)# solidity = area / float(hullArea)#获取坚固度，依据坚固度，判断物体形状 cv2.drawContours(hullImage, [hull], -1, 255, -1) cv2.drawContours(image, [c], -1, (240, 0, 159), 3) shape = \"\"\n11.4、轮廓近似\n轮廓逼近：一种用减少的点集合减少曲线中的点数的算法，简单的称为分裂合并算法\n#检测图像中的正方形 import cv2 image = cv2.imread(\"images/circles_and_squares.png\") gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #获得图像轮廓列表 (cnts, _) = cv2.findContours(gray.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #循环每个轮廓 for c in cnts: #获取轮廓的周长 peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.01 * peri, True) if len(approx) == 4:#判断处理后的轮廓是否有4个顶点 # draw the outline of the contour and draw the text on the image cv2.drawContours(image, [c], -1, (0, 255, 255), 2) (x, y, w, h) = cv2.boundingRect(approx) cv2.putText(image, \"Rectangle\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2) cv2.imshow(\"Image\", image) cv2.waitKey(0)\n物体轮廓检测：\nimport cv2 image = cv2.imread(\"images/receipt.png\") gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) edged = cv2.Canny(gray, 75, 200) cv2.imshow(\"Original\", image) cv2.imshow(\"Edge Map\", edged) (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:7] # loop over the contours for c in cnts: peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.01 * peri, True) print \"original: {}, approx: {}\".format(len(c), len(approx)) if len(approx) == 4: cv2.drawContours(image, [approx], -1, (0, 255, 0), 2) cv2.imshow(\"Output\", image) cv2.waitKey(0)\n11.5、排列轮廓\nimport numpy as np import argparse import cv2 #参数1：轮廓列表 参数2：排列方法 def sort_contours(cnts, method=\"left-to-right\"): reverse = False i = 0 if method == \"right-to-left\" or method == \"bottom-to-top\": reverse = True if method == \"top-to-bottom\" or method == \"bottom-to-top\": i = 1 boundingBoxes = [cv2.boundingRect(c) for c in cnts] (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes), key=lambda b:b[1][i], reverse=reverse)) return (cnts, boundingBoxes) def draw_contour(image, c, i): M = cv2.moments(c) cX = int(M[\"m10\"] / M[\"m00\"]) cY = int(M[\"m01\"] / M[\"m00\"]) cv2.putText(image, \"#{}\".format(i + 1), (cX - 20, cY), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2) return image ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required=True, help=\"Path to the input image\") ap.add_argument(\"-m\", \"--method\", required=True, help=\"Sorting method\") args = vars(ap.parse_args()) image = cv2.imread(args[\"image\"]) accumEdged = np.zeros(image.shape[:2], dtype=\"uint8\") for chan in cv2.split(image): chan = cv2.medianBlur(chan, 11) edged = cv2.Canny(chan, 50, 200) accumEdged = cv2.bitwise_or(accumEdged, edged) cv2.imshow(\"Edge Map\", accumEdged) (cnts, _) = cv2.findContours(accumEdged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:5] orig = image.copy() for (i, c) in enumerate(cnts): orig = draw_contour(orig, c, i) cv2.imshow(\"Unsorted\", orig) (cnts, boundingBoxes) = sort_contours(cnts, method=args[\"method\"]) for (i, c) in enumerate(cnts): draw_contour(image, c, i) cv2.imshow(\"Sorted\", image) cv2.waitKey(0)\n12、直方图\n直方图：表示图像中的像素强度\n运用cv2.calcHist函数构建直方图\ncv2.calcHist(图像，通道，掩码，histSize,范围)\n参数详解：\n图像：我们要计算的直方图的图像\n通道：索引列表，其中指定要计算直方图的通道的索引。\n掩码：提供一个掩码，那么只对被掩盖的像素计算一个直方图\nhistSize:计算直方图时要使用的分组数\n范围：可能的像素值的范围\n#灰度直方图 from matplotlib import pyplot as plt import argparse import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required=True, help=\"Path to the image\") args = vars(ap.parse_args()) image = cv2.imread(args[\"image\"]) image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) cv2.imshow(\"Original\", image) hist = cv2.calcHist([image], [0], None, [256], [0, 256]) plt.figure() plt.title(\"Grayscale Histogram\") plt.xlabel(\"Bins\") plt.ylabel(\"# of Pixels\") plt.plot(hist) plt.xlim([0, 256]) hist /= hist.sum() plt.figure() plt.title(\"Grayscale Histogram (Normalized)\") plt.xlabel(\"Bins\") plt.ylabel(\"% of Pixels\") plt.plot(hist) plt.xlim([0, 256]) plt.show()\n#颜色直方图 from matplotlib import pyplot as plt import argparse import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required=True, help=\"Path to the image\") args = vars(ap.parse_args()) image = cv2.imread(args[\"image\"]) cv2.imshow(\"Original\", image) chans = cv2.split(image) colors = (\"b\", \"g\", \"r\") plt.figure() plt.title(\"'Flattened' Color Histogram\") plt.xlabel(\"Bins\") plt.ylabel(\"# of Pixels\") for (chan, color) in zip(chans, colors): hist = cv2.calcHist([chan], [0], None, [256], [0, 256]) plt.plot(hist, color = color) plt.xlim([0, 256]) fig = plt.figure() ax = fig.add_subplot(131) hist = cv2.calcHist([chans[1], chans[0]], [0, 1], None, [32, 32], [0, 256, 0, 256]) p = ax.imshow(hist, interpolation=\"nearest\") ax.set_title(\"2D Color Histogram for G and B\") plt.colorbar(p) ax = fig.add_subplot(132) hist = cv2.calcHist([chans[1], chans[2]], [0, 1], None, [32, 32], [0, 256, 0, 256]) p = ax.imshow(hist, interpolation=\"nearest\") ax.set_title(\"2D Color Histogram for G and R\") plt.colorbar(p) ax = fig.add_subplot(133) hist = cv2.calcHist([chans[0], chans[2]], [0, 1], None, [32, 32], [0, 256, 0, 256]) p = ax.imshow(hist, interpolation=\"nearest\") ax.set_title(\"2D Color Histogram for B and R\") plt.colorbar(p) print \"2D histogram shape: %s, with %d values\" % ( hist.shape, hist.flatten().shape[0]) hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]) print \"3D histogram shape: %s, with %d values\" % ( hist.shape, hist.flatten().shape[0]) plt.show()\n#直方图均衡 import argparse import cv2 # construct the argument parser and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required=True, help=\"Path to the image\") args = vars(ap.parse_args()) # load the image and convert it to grayscale image = cv2.imread(args[\"image\"]) image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # apply histogram equalization to stretch the constrast of our image eq = cv2.equalizeHist(image) # show our images -- notice how the constrast of the second image has # been stretched cv2.imshow(\"Original\", image) cv2.imshow(\"Histogram Equalization\", eq) cv2.waitKey(0)\n#直方图和面具 from matplotlib import pyplot as plt import numpy as np import cv2 def plot_histogram(image, title, mask=None): chans = cv2.split(image) colors = (\"b\", \"g\", \"r\") plt.figure() plt.title(title) plt.xlabel(\"Bins\") plt.ylabel(\"# of Pixels\") for (chan, color) in zip(chans, colors): hist = cv2.calcHist([chan], [0], mask, [256], [0, 256]) plt.plot(hist, color=color) plt.xlim([0, 256]) image = cv2.imread(\"beach.png\") cv2.imshow(\"Original\", image) plot_histogram(image, \"Histogram for Original Image\") mask = np.zeros(image.shape[:2], dtype=\"uint8\") cv2.rectangle(mask, (60, 290), (210, 390), 255, -1) cv2.imshow(\"Mask\", mask) masked = cv2.bitwise_and(image, image, mask=mask) cv2.imshow(\"Applying the Mask\", masked) plot_histogram(image, \"Histogram for Masked Image\", mask=mask) plt.show()\n13、连接分量标签\n略\n发表于 2018-06-28 15:51 天码丶行满 阅读(...) 评论(...) 编辑 收藏\n刷新评论刷新页面返回顶部"}
{"content2":"大家好，我是小木，没想到吧，我又回来了，啊哈哈哈。之前几天我肠胃感冒，所以我的博客就一直没有更新。但我小木是打不死的小强，这次继续回来讲解啦！\n本次课程我主讲的内容是计算机视觉。为什么小木我要开这次课程呢？很简单，因为我之前读了一本书，叫做《OPENCV3 计算机视觉 PYTHON语言实现》。读了这本书，不知道是翻译没翻译好，还是国外的作者写的不好，读起来语句不通，知识概念模糊，稍有常识的人都会知道，这是人能读的懂的么，一本全tmd是术语，能不能好好说话？\n本人最讨厌的是不好好说话的人，总说一些别人听不懂的之乎者也，显得自己像一个文化人，尤其是那些上了名校的本科，自以为了不起的人。我一般很喜欢用白话把他说的听起来很困难的东西变成简单的不行的，幼儿园小朋友都可以理解的东西，直接给他搞的下不了台。虽然本人的正直得罪的不少人，但是这并不影响本人在网络中进行知识的普及。\n废话不多说，开始今天的内容：\n首先我先说一下啥叫OPENCV，OPENCV是一个缩写，它的英文全称是Open source computer version，也就是免费的计算机视觉资源。我来一个个地解释一下，OPEN也就是open source，这个意思是开放资源，也就是免费资源。我们中国人一般有点东西都藏起来。而美国人不是这样的，他们是做出来什么，直接发到网上，毫无保留，为了就是告诉大家，我能！美国人很单纯，很实在，我们一定要学习。Computer version也就是CV，指的是计算机视觉，视觉我们最能联想到的是眼睛，那么计算机的眼睛是啥？它有眼睛？我的回答是有的，我们笔记本上面是不是都有一个摄像头？你坐在计算机的面前，它是不是能通过摄像的方式看到你呢？这个难道不是计算机的眼睛吗？所以计算机视觉也就是对计算机摄像头或者是已有的图片进行处理，让它能够辨识一些事物，比如我们上班打卡的人脸识别。\n我们名字已经介绍完成了，既然灯塔国的科学家为我们提供了这么好的东西，那么我们是不是必须拿来研究一下，才能对得起他们的心血呢？那么我们就开始吧！\n我们这次讲座使用的编程语言叫做PYTHON，这个语言的教程网上太多了，大家如果不太懂的话，随便找找教程看看就行了。\n（1）我们先来一个基础，来说一下如何运用OPENCV，读取一张图片，并在屏幕上输出。\n首先，我们导入两个库，一个是cv库，另外一个是numpy库。为啥要这两个库呢，第一个库是计算机视觉的类库，里面包含很多视觉算法。第二库是矩阵库，这个库里面包括很多用来计算矩阵的算法。\n其次，我们要做的是用imread类读取图片文件到内存中，假如小木我读取的文件名字是opencv1-1.png图片样式为\n接下来，我们要做的是把读取的图片在屏幕上显示，使用imshow类。\n最后，我们设定一个参数，按键盘任意键，可以关闭图像。\n代码如下所示：\n#导入cv与numpy库 import cv2 import numpy #读入图片opencv1-1.png到变量img中 img=cv2.imread('D:/小木/opencv1-1.png') #在屏幕上显示图片，图片窗口名称为dawawa，展示的图片为img cv2.imshow('dawawa',img) #等待键盘按键 cv2.waitKey() #关闭窗口并退出 cv2.destroyAllWindows()\n这样我们就在一个叫做dawawa的窗口下，显示了一张图片。但是我们有一点要注意，就是我们窗口名称一定要是英文，千万不要写中文，否则可能报错。\n最后的结果如图所示：\n如果大家对代码不是很熟悉的话，你们就直接拿过去用，把地址和窗口名一改就好了。（2）在我们讲完读入图片之后，接下来我们要讲解的是如何保存图片。\n首先，创造一张图片，为了方便，我们就用按照上面的代码，导入的图片img。\n其次，我们讲内存中的图片，保存到硬盘中，使用imwrite()类。\n最后，我们设定一个参数，按键盘任意键，可以关闭图像。\n代码如下所示：\n#导入cv与numpy库 import cv2 import numpy #读入图片opencv1-1.png到变量img中 img=cv2.imread('D:/小木/opencv1-1.png') #把img变量保存为图片dawawa.png cv2.imwrite('D:/小木/dawawa.png',img) #等待键盘按键 cv2.waitKey() #关闭窗口并退出 cv2.destroyAllWindows()\n我们的结果如图所示：\n（3）这样，我们的图像就保存完毕了，我们再（1）、（2）上面拓展一下，如何能够打开一个图片，再把它转换为黑白图片，并在屏幕中显示，之后再保存起来呢？很简单，我们仅仅把imread类稍微改动一下就好啦。\n#导入cv与numpy库 import cv2 import numpy #读入图片opencv1-1.png到变量img中 ###########修改之处########################## img=cv2.imread('D:/小木/opencv1-1.png',cv2.IMREAD_GRAYSCALE) ###########修改之处########################## #在屏幕上显示图片，图片窗口名称为dawawa，展示的图片为img cv2.imshow('dawawa',img) #把img变量保存为图片dawawa.png cv2.imwrite('D:/小木/dawawa1.png',img) #等待键盘按键 cv2.waitKey() #关闭窗口并退出 cv2.destroyAllWindows()\n结果如图所示：\n（4）图片的输入输出基础我已经讲完了，下面我要讲解的是如何进行视频的导入导出。首先我先讲解一下如何导入视频，并在屏幕中读取：\n首先，我们导入两个库，一个是cv库，另外一个是numpy库。\n其次，导入视频，应用VideoCapture类。\n接着，我们获取视频每秒钟的帧数。\n接下来VideoCapture类中的read()方法，读取一帧数据，我们用cv库自带的方法：cv2.CAP_PROP_FPS。\n接着，我们搞一个循环，首先在屏幕显示一下这帧图片，等待一段时间，一般是等待一帧的时间，计算方法是用1000/每秒帧数。然后接着读取下一帧数据。然后显示这帧，等待一帧的时间，然后读取下一帧。。。\n最后，当所有的帧数都读取完毕了，我们设定一个参数，按键盘任意键，可以关闭图像。\n代码如下所示：\n#导入cv与numpy库 import cv2 import numpy #导入视频testvideo.mp4 cameraCapture=cv2.VideoCapture('D:/小木/testvideo.mp4') #获取视频的帧数 fps=cameraCapture.get(cv2.CAP_PROP_FPS) #读取视频的第一帧 success,frame=cameraCapture.read() #按照顺序，循环读取视频中的每一帧，并显示到屏幕上 while success: cv2.imshow('video',frame) #显示到屏幕上 cv2.waitKey(int(1000/fps)) success,frame=cameraCapture.read() #读取视频的第一帧 #等待键盘按键 cv2.waitKey() #关闭窗口并退出 cv2.destroyAllWindows()\n结果如图所示：\n这样，我们的视频读取，并且播放就做完了。\n（5）假如我们用OPENCV创造了一个视频，我们想把它保存到硬盘上，怎么做呢，其实很简单：\n首先，我们制作视频，为了方便，我们就用（4）中读取的视频当作我们制作的视频。\n然后，给视频确定一个帧数和尺寸（为了方便，就用视频自带的帧数和尺寸代替自己设定了）并且我们建立一个视频输出流，用cv库中的VideoWriter类。\n其次，我们读这个视频的每一帧。\n接下来，我们把每一帧写入到输出流当中进行保存。并循环读取每一帧，并写入输出流，直到所有帧都保存为止。\n代码如下所示：\n#导入cv与numpy库 import cv2 import numpy #导入视频testvideo.mp4 cameraCapture=cv2.VideoCapture('D:/小木/testvideo.mp4') #获取视频的帧数 fps=cameraCapture.get(cv2.CAP_PROP_FPS) #获取视频的尺寸 size=(int(cameraCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cameraCapture.get(cv2.CAP_PROP_FRAME_HEIGHT))) #打开写入流 videowriter=cv2.VideoWriter('D:/小木/save1.avi',cv2.VideoWriter_fourcc('I','4','2','0'),fps,size) #读取视频的第一帧 success,frame=cameraCapture.read() #按照顺序，循环读取视频中的每一帧，并输出到写入流中 while success: videowriter.write(frame) #写入到硬盘 success,frame=cameraCapture.read() #读取视频的第一帧 #等待键盘按键 cv2.waitKey() #关闭写入流 videowriter.release() #关闭窗口并退出 cv2.destroyAllWindows()\n结果如图所示：\n这样我们的视频读入写入就讲完了。我们这（5）讲完成之后，我们对OPENCV多少有了一点点的了解了。然而计算机视觉是视觉，摄像头实验应该是最重要的了，所以我们必须要使用摄像头，但是怎么使用呢，很简单，就改改我们的（4）~（5）代码就好啦。\n（6）读取摄像头实验：\n我们用VideoCapture(0)读取摄像头，在括号里，我们写上索引号就行，比如你有2个摄像头，第一个摄像头索引就是0，第二个就是1。\n其它的都和上面一样，如果我们打开一个摄像头，并且在屏幕上显示摄像头的内容，然后把摄像头录制的东西保存，如果想关闭摄像头，按键盘任意按键即可。\n代码这样写就行：\n#导入cv与numpy库 import cv2 import numpy #导入摄像头 cameraCapture=cv2.VideoCapture(0) #设定摄像头的帧数 fps=30 #设定摄像头的尺寸 size=(int(cameraCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cameraCapture.get(cv2.CAP_PROP_FRAME_HEIGHT))) #打开写入流 videowriter=cv2.VideoWriter('D:/小木/save1.avi',cv2.VideoWriter_fourcc('I','4','2','0'),fps,size) #读取视频的第一帧 success,frame=cameraCapture.read() #按照顺序，循环读取视频中的每一帧，并显示到屏幕上、输出到写入流中 while success and cv2.waitKey(1)== -1: #一直读取，并且按任意键结束 cv2.imshow('video',frame) #显示到屏幕上 cv2.waitKey(int(1000/fps)) videowriter.write(frame) #写入到硬盘 success,frame=cameraCapture.read() #读取视频的第一帧 #等待键盘按键 cv2.waitKey() #关闭写入流 videowriter.release() #关闭摄像头 cameraCapture.release() #关闭窗口并退出 cv2.destroyAllWindows()\n结果：\n以上内容主要有：图片的读取，保存。视频的读取、保存。以及如何调用摄像头。然而不仅仅小木我喜欢编写这样的代码，好多人都编写了。美国一个天才编写了一个Cameo的类库，这里面包含我们所有的东西，而且还有不少新的功能，非常强大，我们只需要调用就可以。这个代码我们下节课讲解。\n————————————————\n如果对我的课程感兴趣的话，欢迎关注小木希望学园-微信公众号：\nmutianwei521\n也可以扫描二维码哦！"}
{"content2":"计算机视觉将改变物联网。\n计算机视觉的演变\n计算机视觉技术在日常产品中的应用非常广泛，从可以识别手势的游戏机到可以自动对焦的手机摄像头。计算机视觉技术影响着我们生活的方方面面。\n事实上，计算机视觉在政府方面以及商业领域已经应用多年。可以在各种光谱范围内感测光波的光学传感器被部署在许多应用中：比如制造业中的质量保证，用于环境管理的远程传感技术或者在战场上收集情报的高分辨率相机。其中有一些传感器是静态的，也有另外一些传感器是动态的，它们被连接到诸如卫星、无人机和车辆等移动物体上。\n在过去，许多计算机视觉应用程序仅限于某些封闭平台。但是随着与互联网连接技术的结合，他们创造了一套过去难以实现的新应用。计算机视觉加上互联网连接，高级数据分析和人工智能，它们将成为彼此的催化剂，在物联网（IoT）创新和应用方面带来革命性的飞跃。\n推动计算机视觉多领域的发展\n视觉环境设计\n视觉是人类五种感官中最为发达的。我们每天都用它来辨别我们的朋友，发现路上的障碍物，完成任务和学习新事物。我们为我们的视觉设计周边环境，比如：有路牌和信号灯帮助我们从一个地方到另一个地方；商店有标志牌帮助我们找到它们；电脑和电视屏幕显示各种资讯和娱乐节目。鉴于视觉的重要性，将其扩展到计算机和自动化系统显然是势在必行。\n计算机视觉\n计算机视觉首先捕获并存储图像，然后将这些图像转换成可以进一步执行的信息。它由多种技术组合而成，如下图所示。\n计算机视觉工程是一个跨学科领域，需要多种技术中跨职能和跨系统的专家。\n例如，Microsoft Kinect 使用3D计算机图形算法实现计算机视觉来分析和理解三维场景。它允许游戏开发人员将实时全身运动捕捉与人造3D环境结合起来。除了游戏，这在机器人、虚拟现实（VR）和增强现实（AR）应用等领域开辟了新的可能性。\n传感器技术的进步也在传统摄像机传感器以外的许多层面得到迅速发展。最近的一些例子包括：\n红外传感器和激光结合起来感测深度和距离，这是自动驾车和3D地图应用的关键推动因素之一\n非接触式传感器可以在无需身体接触的情况下跟踪患者的生命体征\n高频摄像机可以捕捉到人眼无法觉察的微妙动作，以帮助运动员分析其步态\n超低功耗和低成本的视觉传感器可部署在任何地方并长期使用\n计算机视觉更加智能\n早期应用\n监控行业是图像处理技术和视频分析技术的早期应用领域之一。视频分析是计算机视觉的一个特殊用例，它能够从长达数小时的视频中找出我们需要的片段。 自动检测和识别现实中预定义模式的能力能够应用到数百个场景中，带来巨大的市场机会。\n第一个视频分析工具使用手工算法来识别图像和视频中的特定功能。它们在实验室设置条件下和模拟环境中都是准确的。然而，当输入数据（如照明条件和摄像机视图）偏离设计假设时，性能急速下降。\n研究人员和工程师花费了多年开发和调优算法，或者用新的方法来处理不同的条件。然而，使用这些算法的摄像头或者录像机仍然不够稳健。尽管多年来取得了一些进步，但现实世界的糟糕表现限制了该技术的应用和推广。\n深度学习的兴起\n近年来深度学习算法的出现促进了计算机视觉的发展。深度学习使用人造神经网络（ANN）算法，模拟人脑神经元。\n从2010年初开始，由图形处理单元（GPU）加速的计算机性能已经越来越强大，足以使研究人员实现复杂 ANN 的功能。此外，在部分视频站点和 IoT 设备支持下，研究人员拥有大量不同的视频库和图像数据来训练其神经网络。\n在2012年，一种被称为卷积神经网络（CNN）的深度神经网络（DNN）在精度方面展示了巨大的飞跃。这一飞跃推动了计算机视觉工程领域的发展。现在，在需要图像分类和面部识别的应用中，深度学习算法甚至可以应用到人类以外。更重要的是，就像人类一样，这些算法具有学习和适应不同条件的能力。\n通过深度学习，我们正在进入一个认知技术的时代，计算机视觉和深度学习融合在一起，解决人类大脑层面的高层次的复杂问题。我们目前的研发成果还浮于表面。这些系统将会持续改进，使用更快的处理器，更先进的机器学习算法，更深入地集成到边缘设备。计算机视觉将改变物联网。\n更多应用案例\n其他有趣的用例包括：\n监测作物健康的农业无人机（http://www.slantrange.com/）\n交通设施管理（http://www.vivacitylabs.com/）\n无人机安全检测（http://industrialskyworks.com/drone-inspections-services/）\n下一代家庭安全摄像头（https://buddyguard.io/）\n这些只是计算机视觉大大提高许多领域生产力的一些小例子。我们正在进入物联网进化的下一个阶段。在第一阶段，我们专注于连接设备，搜集数据并建立大型数据平台。在第二阶段，重点将转移到通过计算机视觉和深度学习等技术使“事物”更加智能，从而产生更多可操作的数据。\n挑战\n增强该技术的实用性并进行商业化的过程中，还有很多问题需要克服：\n嵌入式平台需要集成深度神经设计。在能源消耗、成本、准确性、灵性方面都需要认真设计。\n行业需要标准化，以允许智能设备和系统相互通信并共享元数据。\n系统不再只是一个单纯的数据收集器。它们能够支持人们对这些数据的操作。系统能够自动学习和成长。整个软件、固件更新过程在机器学习时代具有新的意义。\n黑客可以利用计算机视觉和 AI 中新的安全漏洞。设计人员需要考虑到这一点。\n结论\n在这篇文章中，我们简要介绍了计算机视觉，以及它是如何成为许多连接设备和应用程序的重要组成部分。最重要的是，我们预测其即将爆发式增长，并列出了实际应用中的一些障碍。在接下来的系列文章中，我们将探索新的框架、最佳实践和设计方法，以克服一些挑战。"}
{"content2":"国内/外每年都会举办很多计算机视觉（Computer Vision，CV）、 机器学习（Machine Learning，ML）、人工智能（Artificial Intelligence ，AI）领域相关的很多学术会议和研讨会等。在此把我所知道的2017年国内/外即将举办的CV和ML领域几个会议和研讨会列出来，希望对读者有用。如有其他我遗漏的，还请各位读者留言提醒，我会及时更新的。\nMMM2017 (冰岛雷克雅维克)\n全称：the 23rd International Conference on MultiMedia Modeling\n时间：2017.01.04-06\n地点：Reykjavík, Iceland\n介绍：多媒体建模及应用领域国际权威会议\n官网：http://mmm2017.ru.is/\nAAAI 2017（美国旧金山）\n全称：the Association for the Advancement of Artificial Intelligence\n时间：2017.02.04-09\n地点：San Francisco, California USA\n介绍：人工智能领域顶级会议\n官网：http://www.aaai.org/Conferences/AAAI/aaai17.php\nICASSP 2017（美国洛杉矶）\n全称：International Conference on Acoustics, Speech and Signal Processing\n时间：2017.03.05-09\n地点：New Orleans, Los Angeles， USA\n介绍：全世界最大的的声学、语音与信号处理及其应用方面的顶级会议\n官网：http://www.ieee-icassp2017.org/\nWACV 2017 (美国加州)\n全称：IEEE Winter Conference on Applications of Computer Vision\n时间：2017.03.27-29\n地点：Santa Rosa, CA, USA\n介绍：不同于一般的学术会议，比较侧重计算机视觉的应用\n网址：http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=56568\nCVM2017（中国天津）\n全称：Computational Visual Media\n时间：2017.4.12-14\n地点：天津，南开大学\n会议语言：英语\n官网：http://iccvm.org/2017/\n会议涉及领域：\ncomputer graphics, computer vision, machine learning, image processing, video processing, visualization，geometric computing\nICLR 2017（法国）\n全称：5th International Conference on Learning Representations\n时间：2017.04.24-26\n地点：Palais des Congrès Neptune, Toulon, France\n主题：deep learning， feature learning, metric learning, compositional modeling, structured prediction, reinforcement learning, large-scale learning，non-convex optimization\n官网：http://www.iclr.cc/doku.php?id=ICLR2017:main&redirect=1\nVALSE2017（中国厦门）\nvalse是全球计算机视觉，模式识别，机器学习，多媒体技术等相关领域华人青年学者最具影响力的交流平台。\n全称：Vision And Learning SEminar\n时间：2017.04.21-23\n地点：厦门大学\n介绍：全球华人青年学者在计算机视觉，模式识别，机器学习，多媒体技术等相关领域最具影响力的交流平台。\n官网：http://mac.xmu.edu.cn/valse2017/index.html\nICRA 2017（新加坡）\n全称：IEEE International Conference on Robotics and Automation\n时间：2017.05.29-06.03\n地点：Sands Expo and Convention Centre, Marina Bay Sands in Singapore\n介绍：机器人及自动化领域国际顶级会议\n官网：http://www.icra2017.org/\nSSIST 2017 (中国上海)\n全称：ShanghaiTech Symposium on Information Science and Technology\n时间：2017.07.02-04\n地点：No.393, Huaxia Middle Road, Pudong District, Shanghai\n主题： Artificial Intelligence, Web Security, and Smart Energy\n官网：http://ssist.shanghaitech.edu.cn./\nICVS 2017（中国深圳）\n全称：The 11th International Conference on Computer Vision Systems\n时间：2017.07.10-13\n地点：深圳，香格里拉酒店\n介绍：关注计算机视觉系统的应用开发及以应用为驱动的研究\n官网：http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=59159\nCVPR 2017（美国夏威夷）\n全称：IEEE Conference on Computer Vision and Pattern Recognition\n时间：2017.07.21-07.26\n地点：Honolulu, Hawaii，USA\n介绍：计算机视觉领域三大顶会之一\n官网： http://cvpr2017.thecvf.com/\nSIGGRAPH 2017（美国加州）\n时间：2017.07.30-08.03\n地点：Los Angeles, California, USA\n介绍：计算机图形学领域最权威、影响力最大的国际会议\n官网：http://s2017.siggraph.org/\nIJCAI 2017（澳大利亚墨尔本）\n全称：International Joint Conference on Artificial Intelligence\n时间：2017.08.19-25\n地点：Melbourne, Australia\n介绍：国际顶级人工智能联合大会\n官网：http://www.leiphone.com/news/201607/MGrdIcAkBBDcCTS1.html\nICIG 2017 (中国上海)\n全称：The 9th International Conference on Image and Graphics\n时间：2017.09.14-15\n地点：上海亚特兰蒂斯酒店\n官网：http://icig2017.org/\n内容： image and graphics theory, techniques and algorithms\nICIP 2017 （中国北京）\n全称：2017 IEEE International Conference on Image Processing\n时间：2017.09.17-20\n地点：China National Convention Center in Beijing, China\n介绍：图像处理领域国际会议\n官网：http://2017.ieeeicip.org/\nIROS 2017（加拿大温哥华）\n全称：IEEE/RSJ International Conference on Intelligent Robots and Systems\n时间：2017.09.24-28\n地点：Vancouver, BC, Canada\n介绍：智能机器人系统领域国际顶级会议\n官网：http://iros2017.org/\n3D VISON 2017(中国青岛)\n全称：International Conference on 3D Vision\n时间：2017.10.10-12\n地点：山东青岛\n主题：3D research in computer vision and graphics, novel optical sensors, signal processing, geometric modeling, representation and transmission, visualization and interaction, and a variety of applications\n官网：http://irc.cs.sdu.edu.cn/3dv/index.html\nCCCV 2017 （中国天津）\n全称：中国计算机视觉大会（TheChinese Conference on Computer Vision）\n时间：2017.10.13-15\n地点：天津\n介绍：中国计算机学会（CCF）主办，是国内计算机视觉领域最主要的学术活动之一。\nICGIP（中国青岛）\n全称： the 9th International Conference on Graphic and Image Processing\n时间：2017.10.13-15\n地点：中国海洋大学\n官网：http://www.icgip.org/\n介绍：在过去的8年中已经先后在亚庇、马尼拉、开罗、新加坡、香港、北京、东京成功举办，旨在汇集图形与图像处理领域的研究人员，科学家，工程师，学者和学生，分享他们的经验，进行思想交流，分型研究成果，并讨论所遇到的实际挑战和采取的解决方案。\nMMSP 2017 （英国伦敦）\n全称：IEEE 19th International Workshop on Multimedia Signal Processing\n时间：2017.10.16-18\n地点：London-Luton, UK\n介绍：多媒体信号处理领域国际研讨会，今年增加了多媒体信号处理在医疗保健和老人养护方面应用的主题\n官网：http://mmsp2017.eee.strath.ac.uk/\nICCV 2017（意大利威尼斯）\n全称： IEEE International Conference on Computer Vision\n时间：2017.10.22-29\n地点：Venice, Italy\n官网：http://iccv2017.thecvf.com/\n介绍：计算机视觉领域三大顶会之一\nNIPS 2017（美国加州）\n全称：The Conference and Workshop on Neural Information Processing Systems\n时间：2017.12.04-09\n地点：Long Beach，California USA\n官网：https://nips.cc/Conferences/2017\n主题：machine learning , computational neuroscience\nSIGGRAPH AISA 2017（泰国曼谷）\n时间：2017.12.27-30\n地点：Bangkok International Trade and Exhibition Centre (BITEC)，Bangkok, Thailand\n主题：Computer Graphics and Interactive Techniques\n介绍：计算机图形学和交互技术顶级会议\n个人微信公众号：以后这类文章会在公众号里随时提供~\n目标检测与深度学习\n欢迎大家一起交流探讨~"}
{"content2":"记录下《Computer Vision：Algorithms and Applications》计算机视觉 算法与应用 这本书的学习吧，就先简单记录下目录，方便后续的学习。\n第1章 概述\n1.1 什么是计算机视觉？\n1.2 简史\n1.3 本书概述\n1.4 课程大纲样例\n1.5 标记法说明\n1.6 扩展阅读\n第2章 图像形成\n2.1 几何基元和变换\n2.1.1 几何基元\n2.1.2 2D变换\n2.1.3 3D变换\n2.1.4 3D旋转\n2.1.5 3D到2D投影\n2.1.6 镜头畸变\n2.2 光度测定学的图像形成\n2.2.1 照明\n2.2.2 反射和阴影\n2.2.3 光学\n2.3 数字摄像机\n2.3.1 采样与混叠\n2.3.2 色彩\n2.3.3 压缩\n2.4 补充阅读\n2.5 习题\n第3章 图像处理\n3.1 点算子\n3.1.1 像素变换\n3.1.2 彩色变换\n3.1.3 合成与抠图\n3.1.4 直方图均衡化\n3.1.5 应用：色调调整\n3.2 线性滤波\n3.2.1 可分离的滤波\n3.2.2 线性滤波示例\n3.2.3 带通和导向滤波器\n3.3 更多的邻域算子\n3.3.1 非线性滤波\n3.3.2 形态学\n3.3.3 距离变换\n3.3.4 连通量\n3.4 傅里叶变换\n3.4.1 傅里叶变换对\n3.4.2 二维傅里叶变换\n3.4.3 维纳滤波\n3.4.4 应用：锐化，模糊和去噪\n3.5 金字塔与小波\n3.5.1 插值\n3.5.2 降采样\n3.5.3 多分辨率表达\n3.5.4 小波\n3.5.5 应用：图像融合\n3.6 几何变换\n3.6.1 参数化变换\n3.6.2 基于网格的卷绕\n3.6.3 应用：基于特征的变形\n3.7 全局优化\n3.7.1 正则化\n3.7.2 马尔科夫随机场\n3.7.3 应用：图像的恢复\n3.8 补充阅读\n3.9 习题\n第4章 特征检测与匹配\n4.1 点和块\n4.1.1 特征检测器\n4.1.2 特征描述子\n4.1.3 特征匹配\n4.1.4 特征跟踪\n4.1.5 应用：表演驱动的动画\n4.2 边缘\n4.2.1 边缘检测\n4.2.2 边缘连接\n4.2.3 应用：边缘编辑和增强\n4.3 线条\n4.3.1 逐次近似\n4.3.2 Hough变换\n4.3.3 消失点\n4.3.4 应用：矩形检测\n4.4 扩展阅读\n4.5 习题\n第5章 分割\n5.1 活动轮廓\n5.1.1 蛇行\n5.1.2 动态蛇行和CONDENSATION\n5.1.3 剪刀\n5.1.4 水平集\n5.1.5 应用：轮廓跟踪和转描机\n5.2 分裂与归并\n5.2.1 分水岭\n5.2.2 区域分裂（区分式聚类）\n5.2.3 区域归并（凝聚式聚类）\n5.2.4 基于图的分割\n5.2.5 概率聚集\n5.3 均值移位和模态发现\n5.3.1 k-均值和高斯混合\n5.3.2 均值移位\n5.4 规范图割\n5.5 图割和基于能量的方法\n5.6 补充阅读\n5.7 习题\n第6章 基于特征的配准\n6.1 基于2D和3D特征的配准\n6.1.1 使用最小二乘的2D配准\n6.1.2 应用：全景图\n6.1.3 迭代算法\n6.1.4 鲁棒最小二乘和RANSAC\n6.1.5 3D配准\n6.2 姿态估计\n6.2.1 线性算法\n6.2.2 迭代算法\n6.2.3 应用：增强现实\n6.3 几何内参数标定\n6.3.1 标定模式\n6.3.2 消失点\n6.3.3 应用：单视图测量学\n6.3.4 旋转运动\n6.3.5 径向畸变\n6.4 补充阅读\n6.5 习题"}
{"content2":"opencv2计算机视觉编程手册(中文).pdf高清电子版，非常好的资料，需要的可以下载使用!\n下载地址：http://download.csdn.net/download/a5820736/7568253?utm_source=blogseo\nOpenCV2计算机视觉编程手册是一本循序渐进的计算机视觉指导手册，给予OpenCV2代码库中包含高级特性的C++接口。本书介绍了OpenCV2中众多的视觉算法。你将学会如何读、写、创建及操作图像，领略图像分析中常用的技术，并了解如何使用C++高效实现。\n本书以案例的形式介绍OpenCV 2.X的新特性和C++新接口，案例中包含具体的代码与详细的说明。本书很好地平衡了基础知识与进阶内容，要求读者具有基础的C++知识。\n本书既适合想要学习计算机视觉的C++初学者，也适合专业的软件开发人员。可作为高等院校计算机视觉课程的辅助教材，也可以作为图像处理和计算机视觉领域研究人员的参考手册。\nOpenCV2计算机视觉编程手册目录\n第 1 章接触图像\n第 2 章操作像素\n第 3 章基于类的图像处理\n第 4 章使用直方图统计像素\n第 5 章基于形态学运算的图像变换\n第 6 章图像滤波\n第 7 章提取直线、轮廓及连通区域\n第 8 章检测并匹配兴趣点\n第 9 章估算图像间的投影关系\n第 10 章处理视频序列"}
{"content2":"欢迎分享本文，转载请保留出处\n点击关注，获取最新AI干货\n一、安装库\n首先我们需要安装PIL和pytesseract库。\nPIL：（Python Imaging Library）是Python平台上的图像处理标准库，功能非常强大。\npytesseract：图像识别库。\n我这里使用的是python3.6，PIL不支持python3所以使用如下命令\npip install pytesseract pip install pillow\n如果是python2，则在命令行执行如下命令：\npip install pytesseract pip install PIL\n这时候我们去运行上面的代码会发现如下错误：\n错误提示的很明显：\nNo such file or directory ：”tesseract”\n这是因为我们没有安装tesseract-ocr引擎\n二、tesseract-ocr引擎\n光学字符识别(OCR,Optical Character Recognition)是指对文本资料进行扫描，然后对图像文件进行分析处理，获取文字及版面信息的过程。OCR技术非常专业，一般多是印刷、打印行业的从业人员使用，可以快速的将纸质资料转换为电子资料。关于中文OCR，目前国内水平较高的有清华文通、汉王、尚书，其产品各有千秋，价格不菲。国外OCR发展较早，像一些大公司，如IBM、微软、HP等，即使没有推出单独的OCR产品，但是他们的研发团队早已掌握核心技术，将OCR功能植入了自身的软件系统。对于我们程序员来说，一般用不到那么高级的，主要在开发中能够集成基本的OCR功能就可以了。这两天我查找了很多免费OCR软件、类库，特地整理一下，今天首先来谈谈Tesseract，下一次将讨论下Onenote 2010中的OCR API实现。可以在这里查看OCR技术的发展简史。\nTesseract的OCR引擎最先由HP实验室于1985年开始研发，至1995年时已经成为OCR业内最准确的三款识别引擎之一。然而，HP不久便决定放弃OCR业务，Tesseract也从此尘封。\n数年以后，HP意识到，与其将Tesseract束之高阁，不如贡献给开源软件业，让其重焕新生－－2005年，Tesseract由美国内华达州信息技术研究所获得，并求诸于Google对Tesseract进行改进、消除Bug、优化工作。\n安装tesseract-ocr引擎\nbrew install tesseract\n然后我们通过tesseract -v看一下是否安装成成功\ntesseract 3.05.01 leptonica-1.75.0 libjpeg 9b : libpng 1.6.34 : libtiff 4.0.9 : zlib 1.2.11\n这时候我们运行上面代码会出现乱码\n这是因为tesseract默认只有语言包中没有中文包，如下图：\n安装tesseract-ocr语言包\n我们去GitHub下载我们需要的语言包，这里我只下载了chi_tra.traineddata和chi_sim.traineddata\ngithub：tesseract-ocr/tessdata\n然后放到/usr/local/Cellar/tesseract/3.05.01/share/tessdata路径下面。\n可以通过tesseract --list-langs查看本地语言包：\n可以通过tesseract --help-psm 查看psm\n0：定向脚本监测（OSD）\n1： 使用OSD自动分页\n2 ：自动分页，但是不使用OSD或OCR（Optical Character Recognition，光学字符识别）\n3 ：全自动分页，但是没有使用OSD（默认）\n4 ：假设可变大小的一个文本列。\n5 ：假设垂直对齐文本的单个统一块。\n6 ：假设一个统一的文本块。\n7 ：将图像视为单个文本行。\n8 ：将图像视为单个词。\n9 ：将图像视为圆中的单个词。\n10 ：将图像视为单个字符。\n为什么这里要强调语言包和psm，因为我们在使用中会用到，\n比如多个语言包组合并且视为统一的文本块将使用如下参数：\npytesseract.image_to_string(image,lang=\"chi_sim+eng\",config=\"-psm 6\")\n这里我们通过+来合并使用多个语言包。\n接下来我们看一下配置好一切的正确结果。\nimport pytesseract from PIL import Image image = Image.open(\"../pic/c.png\") code = pytesseract.image_to_string(image,lang=\"chi_sim\",config=\"-psm 6\") print(code)"}
{"content2":"计算机视觉是一个相当新且发展十分迅速的研究领域，并成为计算机科学的重要研究领域之一。\n计算机视觉始于20世纪50年代的统计模式识别，当时的工作主要集中于二维图像分析和识别上，如光学字符识别、工件表面、显微图片和航空图片的分析和解释等。\n60年代，Roberts（1965）通过计算机程序从数字图像中提取出诸如立方体、楔形体、棱柱体等多面体的三维结构，并对物体形状及物体的空间关系进行描述[Roberts 1965]。Roberts的研究工作开创了以理解三维场景为目的的三维计算机视觉的研究。Roberts对积木世界的创造性研究给人们以极大的启发，许多人相信，一旦由白色积木玩具组成的三维世界可以被理解，则可以推广到理解更复杂的三维场景。于是，人们对积木世界进行了深入的研究，研究的范围从边缘的检测、角点特征的提取，到线条、平面、曲线等几何要素分析，一直到图像明暗、纹理、运动以及成像几何等，并建立了各种数据结构和推理规则。\n70年代中期，麻省理工学院（MIT）人工智能（AI）实验室正式开设“计算机视觉”（Machine Vision）课程，由著名学者B. K. P. Horn教授主讲。同时，MIT AI实验室吸引了国际上许多知名学者参与计算机视觉的理论、算法、系统设计的研究，David Marr教授就是其中的一位。他与1973年应邀在MIT AI实验室领导一个以博士生为主体的研究小组，1977年提出了不同于“积木世界”分析方法的计算视觉（computational vision）理论，该理论在80年代成为计算机视觉研究领域中的一个十分重要的理论框架[Marr 1982]。\n到了80年代中期，计算机视觉获得了迅速发展，主动视觉理论框架、基于感知特征群的物体识别理论框架等新概念、新方法、新理论不断涌现。\n90年代，计算机视觉开始在工业环境中得到广泛的应用，同时基于多视几何的视觉理论也得到迅速发展。\n进入21世纪，计算机视觉与计算机图形学的相互影响日益加深，基于图像的绘制成为研究热点。高效求解复杂全局优化问题的算法得到发展。"}
{"content2":"图像处理\n图像处理是指对获取的二维图像进行基本的处理。比如，滤波、边缘、角点提取等。通过对图像处理可以实现降噪、特征提取等。一般来说，图像处理被用作图像数据的预处理，其主要包括滤除噪声数据及增强图像中的关键数据。\n图像分类\n图像分类也称之为图像识别。图像分类是通过海量的图像数据来训练分类器，比如深度学习网络，并通过分类器来进行图像的分类。一般来说，图像分类可用于人脸识别，指纹识别等需要图像和人相对应的场景。\n计算机视觉\n计算机视觉，其更准确的应该称之为机器视觉，或者机器人视觉，因为其应用的算法主要是在机器人视觉上。计算机视觉主要是通过获取得到的2维图像来还原真实的3维场景。其最基本的应用就是双目测距。"}
{"content2":"计算机视觉与许多学科关系密切，在本门课中，我们将学习到跨学科的建模方式。\n本门课针对更加专业的领域，不只是讨论神经网络。但是在应用方面只是针对计算机视觉。\n卷积神经网络，解决了识别问题，识别问题有促进了深度学习的发展。\n搞科研要有耐心，足够细心。"}
{"content2":"来源：AI科技评论\n本文长度为4170字，建议阅读6分钟\n本文为你解读机器计算机视觉的进展与前景。\n9 月 26 日，机器人领域的顶级学术会议 IROS 2017 进入第二日。上午，著名华人计算机视觉专家、斯坦福副教授李飞飞，在温哥华会议中心面向全体与会专家学者作了长达一小时的专题报告。\n在报告中李飞飞与大家讨论了计算机视觉的目标：丰富场景理解，以及计算机视觉与语言结合和任务驱动的计算机视觉的进展和前景。场景理解和与语言结合的计算机视觉进一步搭起了人类和计算机之间沟通的桥梁，任务驱动的计算机视觉也会在机器人领域大放异彩。李飞飞介绍的自己团队工作也丰富多样、令人振奋。\n2015年，李飞飞也在同一个会场面向着大海和听众进行过一次 TED 演讲\n物体识别之后：丰富场景识别\n在物体识别问题已经很大程度上解决以后，我们的下一个目标是走出物体本身，关注更为广泛的对象之间的关系、语言等等。\n在Visual Genome数据集之后，我们做的另一项研究是重新认识场景识别。\n场景识别单独来看是一项简单的任务，在谷歌里搜索“穿西装的男人”或者“可爱的小狗”，都能直接得到理想的结果。但是当你搜索“穿西装的男人抱着可爱的小狗”的时候，就得不到什么好结果。它的表现在这里就变得糟糕了，这种物体间的关系是一件很难处理的事情。\n比如只关注了“长椅”和“人”的物体识别，就得不到“人坐在长椅上”的关系；即便训练网络识别“坐着的人”，也无法保证看清全局。我们有个想法是，把物体之外、场景之内的关系全都包含进来，然后再想办法提取精确的关系。\n如果我们有一张场景图（graph），其中包含了场景内各种复杂的语义信息，那我们的场景识别就能做得好得多。其中的细节可能难以全部用一个长句子描述，但是把一个长句子变成一个场景图之后，我们就可以用图相关的方法把它和图像做对比；场景图也可以编码为数据库的一部分，从数据库的角度进行查询。\n我们已经用场景图匹配技术在包含了许多语义信息的场景里得到了许多不错的量化结果，不过在座的各位可能边听就边觉得，这些场景图是谁来定义的呢？在Visual Genome数据集中，场景图都是人工定义的，里面的实体、结构、实体间的关系和到图像的匹配都是我们人工完成的，过程挺痛苦的，我们也不希望以后还要对每一个场景都做这样的工作。所以在这项工作之后，我们很可能会把注意力转向自动场景图生成。\n比如这项我和我的学生们共同完成的CVPR2017论文就是一个自动生成场景图的方案，对于一张输入图像，我们首先得到物体识别的备选结果，然后用图推理算法得到实体和实体之间的关系等等；这个过程都是自动完成的。\n这里涉及到了一些迭代信息传递算法，我先不详细解释了。但这个结果体现出的是，我们的模型的工作方式和人的做法已经有不少相似之处了。\n\n得到这样的结果我们非常开心，这代表着一组全新的可能性来到了我们面前。借助场景图，我们可以做信息提取、可以做关系预测、可以理解对应关系等等。\n当然了论文发表前我们也做了好看的数据出来。\n我们相信Visual Genome数据集也能够帮助很多的研究人员在研究关系和信息提取的算法和模型实验中施展拳脚。\n场景识别之后还有什么？\n\n刚才说过了物体识别、关系预测这两项场景理解难题之后，Jeremy 提到的最后一件事情就是，“场景中的gist的根本是三维空间中在物体间和物体表面上以一定形式扩散、重复出现的视觉元素”。不过由于我关注的并不是三维场景理解，我就只是简单介绍一下斯坦福的同事们近期的研究成果。\n左侧是从单张图片推测三维场景的布局，展现出其中物体的三维几何特征；右侧是空间三维结构的语意分割。除了斯坦福的这两项之外，三维场景理解还有很多的研究成果，包括使用图片的和点云的。我也觉得很兴奋，将来也不断地会有新东西来到我们面前，尤其是在机器人领域会非常有用。\n这样，我们就基本覆盖全了场景的gist，就是看到场景的前150毫秒中发生的事情。视觉智慧的研究当然并不会局限于这150毫秒，之后要考虑的、我们也在期待的还有两项任务。\n我的研究兴趣里，除了计算机科学和人工智能之外，认知神经科学也占了相当的位置。所以我想回过头去看看我在加州理工学院读博士的时候做的一个实验，我们就让人们观察一张照片，然后让他们尽可能地说出自己在照片中看到的东西。当时做实验的时候，我们在受试者面前的屏幕上快速闪过一张照片，然后用一个别的图像、墙纸一样的图像盖住它，它的作用是把他们视网膜暂留的信息清除掉。\n接下来我们就让他们尽可能多地写下自己看到的东西。从结果上看，有的照片好像比较容易，但是其实只是因为我们选择了不同长短的展示时间，最短的照片只显示了27毫秒，这已经达到了当时显示器的显示速度上限；有些照片显示了0.5秒的时间，对人类视觉理解来说可算是绰绰有余了。\n我们得到的结果大概是这样的，对于这张照片，时间很短的时候看清的内容也很有限，500毫秒的时候他们就能写下很长一段。进化给了我们这样的能力，只看到一张图片就可以讲出一个很长的故事。\n计算机视觉+语言\n我展示这个实验想说的是，在过去的3年里，CV领域的研究人员们就在研究如何把图像中的信息变成故事。\n他们首先研究了图像说明，比如借助CNN把图像中的内容表示到特征空间，然后用LSTM这样的RNN生成一系列文字。这类工作在2015年左右有很多成果，从此之后我们就可以让计算机给几乎任何东西配上一个句子。\n比如这两个例子，“一位穿着橙色马甲的工人正在铺路”和“穿着蓝色衬衫的男人正在弹吉他”。这让我想起来，2015年的时候我就是在这同一个房间里做过演讲。两年过去了，我们的算法也已经不是最先进的了，不过那时候我们的研究确实是是图像说明这个领域的开拓性工作之一。\n我们沿着这个方向继续做研究，迎来的下一个成果是稠密说明，就是在一幅图片中有很多个区域都会分配注意力，这样我们有可以有很多个不同的句子描述不同的区域，而不仅仅是用一个句子描述整个场景。在这里就用到了CNN模型和逻辑区域检测模型的结合，再加上一个语言模型，这样我们就可以对场景做稠密的标注。\n比如这张图里就可以生成，“有两个人坐在椅子上”、“有一头大象”、“有一棵树”等等；另一张我的学生们的室内照片也标出了丰富的内容。\n我们的稠密标注系统也比当时其它基于滑动窗口的方法表现好得多。\n在最近的CVPR2017的研究中，我们让表现迈上了一个新的台阶，不只是简单的说明句子，还要生成文字段落，把它们以具有空间意义的方式连接起来。\n这样我们就可以写出“一只长颈鹿站在树边，在它的右边有一个有叶子的杆子，在篱笆的后面有一个黑色和白色的砖垒起来的建筑”，等等。虽然里面有错误，而且也远比不上莎士比亚的作品，但我们已经迈出了视觉和语言结合的第一步。\n而且，视觉和语言的结合并没有停留在静止的图像上，刚才的只是我们的最新成果之一。在另外的研究中，我们把视频和语言结合起来，比如这个CVPR2017的研究，我们可以对一个说明性视频中不同的部分做联合推理、整理出文本结构。这里的难点是解析文本中的实体，比如第一步是“搅拌蔬菜”，然后“拿出混合物”。如果算法能够解析出“混合物”指的是前一步里混合的蔬菜，那就棒极了。我的学生和博士后们也都觉得这是让机器人进行学习的很重要的一步。\n这里的机会仍然是把视觉问题和语言结合起来，如果只用视觉的方法，就会造成视觉上的模糊性；如果只用语言学的方法，就会造成语言上的模糊性；把视觉和语言结合起来，我们就可以解决这些问题。\n太细节的还是不说了，我们主要用了图优化的方法在实体嵌入上解决这些模糊性。我们的结果表明，除了解决模糊性之外，我们还能对视频中的内容作出更广泛完善的推理。\n任务驱动的计算机视觉\n在语言之后，我想说的最后一个方向是任务驱动的视觉问题，它和机器人的联系也更紧密一些。对整个AI研究大家庭来说，任务驱动的AI是一个共同的长期梦想，从一开始人类就希望用语言给机器人下达指定，然后机器人用视觉方法观察世界、理解并完成任务。\n比如人类说：“蓝色的金字塔很好。我喜欢不是红色的立方体，但是我也不喜欢任何一个垫着5面体的东西。那我喜欢那个灰色的盒子吗？” 那么机器，或者机器人，或者智能体就会回答：“不，因为它垫着一个5面体”。它就是任务驱动的，对这个复杂的世界做理解和推理。\n最近，我们和Facebook合作重新研究这类问题，创造了带有各种几何体的场景，然后给人工智能提问，看它会如何理解、推理、解决这些问题。这其中会涉及到属性的辨别、计数、对比、空间关系等等。\n我们在这方面的第一篇论文用了CNN+LSTM+注意力模型，结果算不上差，人类能达到超过90%的正确率，机器虽然能做到接近70%了，但是仍然有巨大的差距。有这个差距就是因为人类能够组合推理，机器则做不到。\n在一个月后的ICCV我们就会介绍新一篇论文中的成果，我们把一个问题分解成带有功能的程序段，然后在程序段基础上训练一个能回答问题的执行引擎。这个方案在尝试推理真实世界问题的时候就具有高得多的组合能力。\n模型的实际表现当然不错，所以论文被ICCV接收了。比如这个例子里，我们提问某种颜色的东西是什么形状的，它就会回答“是一个立方体”这样，表明了它的推理是正确的。它还可以数出东西的数目。这都体现出了算法可以对场景做推理。\n我们也在尝试环境仿真，我们用三维渲染引擎建立执行任务的环境，让学习策略的机器人在其中学习动作，比如把篮球放进微波炉，也需要它把这个任务分解成许多步骤然后执行。\n我们采用了一种深度语意表征，然后用不同难度的任务测试它，中等难度的任务可以是从厨房里多个不同的地方拿取多个不同的物体，然后把它们放在指定的地方；难的任务可以是需要策略让它寻找之前从来没有见过的新物体。\n视觉相关的任务说了这么多，我想把它们组织成这三类:\n首先是除了物体识别之外的关系识别、复杂语意表征、场景图；\n在场景gist之外，我们需要用视觉+语言处理单句标注、段落生成、视频理解、联合推理；\n最后是任务驱动的视觉问题，这里还是一个刚刚起步的领域，我相信视觉和逻辑的组合会在这个领域真正携起手来。\n人类视觉已经发展了很久，计算机视觉虽然在出现后的60年里有了长足的进步，但也仍然只是一门新兴学科。我以前应该有提过我边工作边带孩子，这也就是一张我女儿二十个月大时候的照片。\n看着她一天天成长的过程，真的让我觉得还有许许多多的东西等着我们去研究。视觉能力也是她的日常生活里重要的一部分，读书、画画、观察情感等等，这些重大的进步都是这个领域未来的研究目标。\n谢谢大家！（完）\n编辑：文婧"}
{"content2":"计算机视觉、机器学习、自然语言处理、机器人和语音识别是人工智能的五大核心技术，它们均会成为独立的子产业。\n计算机视觉\n计算机视觉是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉技术运用由图像处理操作及其他技术所组成的序列，来将图像分析任务分解为便于管理的小块任务。比如，一些技术能够从图像中检测到物体的边缘及纹理，分类技术可被用作确定识别到的特征是否能够代表系统已知的一类物体。\n计算机视觉有着广泛的应用，其中包括：医疗成像分析被用来提高疾病预测、诊断和治疗；人脸识别被Facebook用来自动识别照片里的人物；在安防及监控领域被用来指认嫌疑人；在购物方面，消费者现在可以用智能手机拍摄下产品以获得更多购买选择。\n机器视觉作为相关学科，泛指在工业自动化领域的视觉应用。在这些应用里，计算机在高度受限的工厂环境里识别诸如生产零件一类的物体，因此相对于寻求在非受限环境里操作的计算机视觉来说目标更为简单。计算机视觉是一个正在进行中的研究，而机器视觉则是“已经解决的问题”，是系统工程方面的课题而非研究层面的课题。因为应用范围的持续扩大，某些计算机视觉领域的初创公司自2011年起已经吸引了数亿美元的风投资本。\n机器学习\n机器学习指的是计算机系统无须遵照显式的程序指令，而只依靠数据来提升自身性能的能力。其核心在于，机器学习是从数据中自动发现模式，模式一旦被发现便可用于预测。比如，给予机器学习系统一个关于交易时间、商家、地点、价格及交易是否正当等信用卡交易信息的数据库，系统就会学习到可用来预测信用卡欺诈的模式。处理的交易数据越多，预测就会越准确。\n机器学习的应用范围非常广泛，针对那些产生庞大数据的活动，它几乎拥有改进一切性能的潜力。除了欺诈甄别之外，这些活动还包括销售预测、库存管理、石油和天然气勘探，以及公共卫生等。机器学习技术在其他的认知技术领域也扮演着重要角色，比如计算机视觉，它能在海量图像中通过不断训练和改进视觉模型来提高其识别对象的能力。\n现如今，机器学习已经成为认知技术中最炙手可热的研究领域之一，在2011~2014年这段时间内就已吸引了近10亿美元的风险投资。谷歌也在2014年斥资4亿美元收购Deepmind这家研究机器学习技术的公司。\n自然语言处理\n自然语言处理是指计算机拥有的人类般的文本处理的能力。比如，从文本中提取意义，甚至从那些可读的、风格自然、语法正确的文本中自主解读出含义。一个自然语言处理系统并不了解人类处理文本的方式，但是它却可以用非常复杂与成熟的手段巧妙处理文本。例如，自动识别一份文档中所有被提及的人与地点；识别文档的核心议题；在一堆仅人类可读的合同中，将各种条款与条件提取出来并制作成表。以上这些任务通过传统的文本处理软件根本不可能完成，后者仅针对简单的文本匹配与模式就能进行操作。\n自然语言处理像计算机视觉技术一样，将各种有助于实现目标的多种技术进行了融合。建立语言模型来预测语言表达的概率分布，举例来说，就是某一串给定字符或单词表达某一特定语义的最大可能性。选定的特征可以和文中的某些元素结合来识别一段文字，通过识别这些元素可以把某类文字同其他文字区别开来，比如垃圾邮件同正常邮件。以机器学习为驱动的分类方法将成为筛选的标准，用来决定一封邮件是否属于垃圾邮件。\n因为语境对于理解“timeflies”（时光飞逝）和“fruitflies”（果蝇）的区别是如此重要，所以自然语言处理技术的实际应用领域相对较窄，这些领域包括分析顾客对某项特定产品和服务的反馈，自动发现民事诉讼或政府调查中的某些含义，自动书写诸如企业营收和体育运动的公式化范文，等等。\n机器人\n将机器视觉、自动规划等认知技术整合至极小却高性能的传感器、制动器以及设计巧妙的硬件中，这就催生了新一代的机器人，它有能力与人类一起工作，能在各种未知环境中灵活处理不同的任务。例如，无人机、可以在车间为人类分担工作的“cobots”等。\n语音识别\n语音识别主要是关注自动且准确地转录人类的语音技术。该技术必须面对一些与自然语言处理类似的问题，在不同口音的处理、背景噪声、区分同音异形/异义词（“buy”和“by”听起来是一样的）方面存在一些困难，同时还需要具有跟上正常语速的工作速度。语音识别系统使用一些与自然语言处理系统相同的技术，再辅以其他技术，比如描述声音和其出现在特定序列与语言中概率的声学模型等。语音识别的主要应用包括医疗听写、语音书写、电脑系统声控、电话客服等。比如Domino抯Pizza，最近推出了一个允许用户通过语音下单的移动APP。\n上述5项技术的产业化，是人工智能产业化的要素。人工智能将是一个万亿级的市场，甚至是10万亿级的市场，将会为我们带来一些全新且容量巨大的子产业，比如机器人、智能传感器、可穿戴设备等，其中最令人期待的是机器人子产业。\n机器人应用的分法有很多种，从应用层面可以粗略地分为以下几个类别。第一个类别是工业级机器人，像富士康这种公司已经运用得很好了，因为劳工成本越来越高，用工风险越来越高，而机器人则可以解决这些问题。第二个类别是监护级机器人，它可以在家里和医院里作为病人、老人或孩子的护理，帮助他们做一定复杂程度的事情。中国对监护级机器人需求其实更迫切一些，因为中国人口红利在下降，同时老龄化又不断地上升，这两个矛盾，机器人都可以帮助解决。因此，这个领域的需求在民用市场占比很大。第三个类别就是探险级机器人，用来采矿或者探险等，大大避免了人所要经历的危险。此外还有用来打仗的军事机器人等。\n网络媒体Business Insider预测，机器人将在许多岗位上取替人类：电话营销员、校对员、手工裁缝师、数学家、保险核保人、钟表修理师、货运代理商、报税员、图像处理人员、银行开户员、图书馆员、打字员等。因为它们的价格竞争力惊人。麦肯锡全球研究院的研究表明，当中国制造业工资每年增长10%~20%时，全球机器人的价格每年下调10%，一台最便宜的低阶机器人只需花费美国人年平均工资的一半。国际研究机构顾能预测：2020年机器人将导致全球新一波失业潮。\n同时，人工智能技术的发展还将让许多旧产业获得改头换面式的新生，其中最典型的是汽车产业。汽车产业已存在上百年了，其间的变革也是非常大的，但驾驶汽车的始终是人，可最近几年，随着谷歌等公司的大力投入，机器或者说某种自动化的系统已经有望取代人来驾驶汽车，从而形成一个市场容量巨大的新产业，即无人驾驶汽车产业。这个产业的规模也将是万亿级甚至是10万亿级的。而且，这个产业还将与新能源产业叠加、融合在一起，形成“车联网＋能联网＋互联网＋电动汽车”的复合产业——未来，我们会把插电式汽车和氢燃料汽车作为发电厂使用，从而使新能源汽车成为电网的一部分，成为新能源的供给者，与现在一些装有太阳能发电系统的房屋是太阳能的供给者一样。\n毫无疑问，与互联网一样，智能技术会向几乎所有旧产业渗透。华泰证券在一份人工智能产业的研究报告中提及了九大行业：生活服务O2O、医疗、零售业、金融业、数字营销业、农业、工业、商业和在线教育。实际上，将获得新生的旧产业还有许多，如军事、传媒、家居、医疗健康业、生命科学、能源、公共部门……甚至包括受VR/AR（虚拟现实与增强现实）技术发展影响而产生的虚拟产业。"}
{"content2":"Dr. Scott Ross\n有计算机专业，信息科学技术学院的，艺术学院的，数字艺术方向、电影方向\n陈宝权老师主持，Scott成立了两家电影公司，是BFA的高精尖的国际顾问\n“懂技术的艺术家”\n很fancy的视频，介绍电影的发展史\n特效技术：太空，骷髅头，拍一帧，停一下\ncamera motion control， robotic cameras\nno computer， no digital，靠相机移动拍出特效，1977\nanimation，全是hand draw，没有计算机特效，1986\ntoy story，pixar\nalways，fire plane，burning houses，effect by camera motion，real fire，1989\nwater creature，by computer graphics， creativity or technology，water simulation，visual effects，1989\nterminate two， judgement day (电脑生成人及运动)，track，特效都是电脑形成\nmotion tracking (外科医生)\n侏罗纪公园，真实的creature？都是计算机生成的，自动运动的恐龙\n计算机生成的恐龙\n卡通动画是handcraft，要演员模拟和不存在的任务对话很难\nCG模拟爆炸，烟雾，1992\n手动去掉人的腿脚\n第一个全电脑生成的动画，Hollywood，1984\nanimation -> Disney，有一个人预言到animation的未来将会是全电脑生成\n玩具总动员，character animation\ncreature development，传感器+生成mesh\n星球大战，异种生物的形成\nfeature变得更加真实\n星球大战，在blue screen里拍摄，然后将背景再修改\nWali，complex全电脑生成，virtual reality，living in the virtual world，motion blur，camera motion，computer animation\nVR\nVR in Entertainment，Still Virtual\nVR is like sex in High School… everybody is doing it. But no one is doing it well. -Chris Little/Strivr\n特效将去做VR，导演去讲故事\n全球做VR的企业越来越多\nVR眼镜，设备太笨重了，用户体验并不好，无法成为生活的一部分\n产品：Financing Budgets，Camera technical limitations，Post\nVR技术产品投产收入并不高\nVR环境生成成本太高\n360度影院？我只是个相机，没法交互，很无聊，没有回放，错过精彩\nSTORY TELLING TRANSFORMED\n用户在故事世界的体验没有现实世界中的好\nAI\n非常严肃的科技，中国如果AI发展太快，很多人会失业，他们又没有其他技能，很容易出问题\n提问环节：\n创造性可以被机器取代吗？\n人可以讲故事，人机结合，人的思想是无法取代的\n中美的影视特效质量为什么会有较大差距？\n美国有较长的历史，时间影响最大\nbudget，中国的成本太高\n陈：有gap，新的电影制作方式，本身是一种机会\nScott：China，领导形式，中国要成为影视行业的参与者，因为没有基础和各种条条框框，所以构建新的体系会容易得多\nAR技术\n希望successful，但是，现在技术还很不成熟\n但虚拟世界和现实世界是不一样的，虚拟世界依然难以脱离显示存在\n虚拟人物，虚拟明星，虚拟中的真实人物\n人还是喜欢真实的？学习一个真实人物的说话方式，思考方式，“复活”人"}
{"content2":"翻译 | 人工智能头条（ID：AI_Thinker）\n参与 | 林椿眄\n本文概述了 Facebook AI Research（FAIR）近期在计算机视觉领域的研究进展，内容主要包括基础结构模块的创新、卷积神经网络、one shot 检测模块等，以及一些在实例分割方面的创新方法，并介绍了弱半监督学习方式下实例分割的研究进展。下面将逐一介绍，文中的一些引用可在文末的参考文献中找到。\n▌Feature Pyramid Networks（ 特征金字塔网络）\n首先，我们要介绍的是著名的特征金字塔网络[1](这是发表在 CVPR 2017 上的一篇论文，以下简称FPN)。\n如果你在过去两年有一直跟进计算机视觉领域的最新进展的话，那你一定听说过这个网络的大名，并和其他人一样等待着作者开源这个项目。FPN 这篇论文提出的一种非常棒的思路。我们都知道，构建一个多任务、多子主题、多应用领域的基线模型是很困难的。\nFPN 可以视为是一种扩展的通用特征提取网络（如 ResNet、DenseNet），你可以从深度学习模型库中选择你想要的预训练的 FPN 模型并直接使用它！\n通常，图像目标有多个不同尺度和尺寸大小。一般的数据集无法捕捉所有的图像属性，因此人们使用图像金字塔的方式，对图像按多种分辨率进行降级，提取图像特征，以方便 CNN 处理。但是，这种方法最大弊端是网络处理的速度很慢，因此我们更喜欢使用单个图像尺度进行预测，也就导致大量图像特征的流失，如一部分研究者可能从特征空间的中间层获取预测结果。\n换句话说，以 ResNet 为例，对于分类任务而言，在几个 ResNet 模块后放置一个反卷积层，在有辅助信息和辅助损失的情况下获取分割输出（可能是 1x1 卷积和 GlobalPool），这就是大部分现有模型架构的工作流程。\n回到我们的主题，FPN 作者提出一种新颖的思想，能够有效改善现有的处理方式。他们不单单使用侧向连接，还使用自上而下的路径，并通过一个简单的 MergeLayer（mode=『addition』）将二者结合起来，这种方式对于特征的处理是非常有效！由于初始卷积层提取到的底层特征图（初始卷积层）的语义信息不够强，无法直接用于分类任务，而深层特征图的语义信息更强，FPN 正是利用了这一关键点从深层特征图中捕获到更强的语义信息。\n此外，FPN 通过自上而下的连接路径获得图像的 Fmaps(特征图)，从而能够到达网络的最深层。可以说，FPN 巧妙地将二者结合了起来，这种网络结构能够提取图像更深层的特征语义信息，从而避免了现有处理过程信息的流失。\n其他一些实现细节\n图像金字塔：认为同样大小的所有特征图属于同一个阶段。最后一层的输出是金字塔的 reference FMaps。如 ResNet 中的第 2、3、4、5 个模块的输出。你可以根据内存和特定使用情况来改变金字塔。\n侧向连接：1x1 卷积和自上而下的路径都经过 2× 的上采样过程。上层的特征以自上而下的方式生成粗粒度的图像特征，而侧向连接则通过自下而上的路径来添加更多细粒度的特征信息。在此我引用了论文中的一些图片来帮助你进一步理解这一过程。\n在 FPN 的论文中，作者还介绍了一个简单的 demo 来可视化这个想法的设计思路。\n如前所述，FPN 是一个能够在多任务情景中使用的基线模型，适用于如目标检测、分割、姿态估计、人脸检测及其他计算机视觉应用领域。这篇论文的题目是 FPNs for Object Detection，自 2017 年发表以来引用量已超过 100 次！\n此外，论文作者在随后的 RPN（区域建议网络）和 Faster-RCNN 网络研究中，仍使用 FPN 作为网络的基线模型，可见 FPN的强大之处。以下我将列出一些关键的实验细节，这些在论文中也都可以找到。\n﻿\n实验要点\nRPN：这篇论文中，作者用 FPN 来代替单个尺度 Fmap，并在每一级使用单尺度 anchor （由于使用了 FPN，因此没必要使用多尺度的 anchor）。此外，作者还展示了所有层级的特征金字塔共享类似的语义信息。\nFaster RCNN：这篇论文中，作者使用类似图像金字塔的输出方式处理这个特征金字塔，并使用以下公式将感兴趣域（RoI）分配到特定的层级中。\n﻿﻿，其中 w、h 分别表示宽度和高度，k 表示 RoI 所分配到的层级，k0 代表的是 w=224，h=224 时所映射到的层级。\nFaster RCNN 在 COCO 数据集上取得当前最先进的实验结果，没有任何冗余的结构。\n论文的作者对每个模块的功能进行了消融(ablation)研究，并论证了本文提出的想法。\n此外，还基于 DeepMask 和 SharpMask 论文,作者进一步展示了如何使用 FPN 生成分割的建议区域（segmentation proposal generation）。\n对于其他的实现细节、实验设置等内容，感兴趣的同学可以认真阅读下这篇论文。\n实现代码\n官方的Caffe2版本：\nhttps://github.com/facebookresearch/Detectron/tree/master/configs/12_2017_baselines\nCaffe版本：https://github.com/unsky/FPN\nPyTorch版本：https://github.com/kuangliu/pytorch-fpn (just the network)\nMXNet版本：https://github.com/unsky/FPN-mxnet\nTensorflow版本：https://github.com/yangxue0827/FPN_Tensorflow\n▌RetinaNet：Focal Loss 损失函数用于密集目标检测任务\n这个架构是由同一个团队所开发，这篇论文[2]发表在 ICCV 2017 上，论文的一作也是 FPN 论文的一作。该论文中提出有两个关键想法：通用损失函数Focal Loss(FL)和单阶段的目标检测器RetinaNet。两者组合成的RetinaNet在COCO的目标检测任务中表现得非常好，并超过了先前FPN所保持的结果。\nFocal Loss\nFocal Loss损失函数的提出来源于一个聪明又简单的想法。如果你熟悉加权函数的话，那么你应该对Focal Loss并不陌生。该损失函数其实就是巧妙地使用了加权的损失函数，让模型训练过程更聚焦于分类难度高的样本。其数学公式如下所示：\n\n其中，γ 是一个可改变的超参数，pt 表示分类器输出的样本概率。将 γ 设置为大于 0，将会减小分类结果较好的样本权重。α_t 表示标准加权损失函数中的类别权重，在论文中将其称为 α-balanced 损失。值得注意的是，这个是分类损失，RetinaNet 将其与 smooth L1 损失结合，用于目标检测任务。\nRetinaNet\nYOLO2 和 SSD 是当前处理目标场景最优的单阶段（one-stage）算法。相继的，FAIR 也开发了自己的单阶段检测器。作者指出，YOLO2 和 SSD 模型都无法接近当前最佳的结果，而RetinaNet 可以轻松地实现单阶段的最佳的检测结果，而且速度较快，他们将这归功于新型损失函数（Focal Loss）的应用，而不是简单的网络结构（其结构仍以 FPN 为基础网络）。\n作者认为，单阶段检测器将面临很多背景和正负类别样本数量不平衡的问题（而不仅仅的简单的正类别样本的不均衡问题），一般的加权损失函数仅仅是为了解决样本数量不均衡问题，而Focal Loss 函数主要是针对分类难度大/小的样本，而这正好能与 RetinaNet 很好地契合。\n注意点：\n两阶段（two-stage）目标检测器无需担心正、负样本的不均衡问题，因为在第一阶段就将绝大部分不均衡的样本都移除了。\nRetinaNet 由两部分组成：主干网络（即卷积特征提取器，如 FPN）和两个特定任务的子网络（分类器和边界框回归器）。\n采用不同的设计参数时，网络的性能不会发生太大的变化。\nAnchor 或 AnchorBoxes 是与 RPN 中相同的 Anchor[5]。Anchor 的坐标是滑动窗口的中心位置，其大小、横纵比（aspect ratio）与滑动窗口的长宽比有关，大小从 322 到 512 ，横纵比取值为{1:2, 1:1, 2:1}。\n用 FPN 来提取图像特征，在每一阶段都有 cls+bbox 子网络，用于给出 Anchor 中所有位置的对应输出。\n实现代码\n官方的Caffe2版本：\nhttps://github.com/facebookresearch/Detectron/tree/master/configs/12_2017_baselines\nPyTorch版本：https://github.com/kuangliu/pytorch-retinanet\nKeras版本：https://github.com/fizyr/keras-retinanet\nMXNet版本：https://github.com/unsky/RetinaNet\n▌Mask R-CNN\n正如上面所述，Mask R-CNN [3]也几乎是同一个团队开发的，并发表在 ICCV 2017 上，用于图像的实例分割任务。简单来说，图像的实例分割不过就是不使用边界框的目标检测任务，目的是给出检测目标准确的分割掩码。这项任务想法简单，实现起来也并不困难，但是要使模型正常运行并达到当前最佳的水准，或者使用预训练好的模型来加快分割任务的实现等，想要做到这些可并不容易。\nTL;DR：如果你了解 Faster R-CNN 的工作原理，那么 Mask R-CNN 模型对你来说是很简单的，只需要在 Faster R-CNN 的基础上添加一个用于分割的网络分支，其网络主体有 3 个分支，分别对应于 3 个不同的任务：分类、边界框回归和实例分割。\n值得注意的是，Mask R-CNN 的最大贡献在于，仅仅使用简单、基础的网络设计，不需要多么复杂的训练优化过程及参数设置，就能够实现当前最佳的实例分割效果，并有很高的运行效率。\n我很喜欢这篇论文，因为它的思想很简单。但是，那些看似简单的东西却伴有大量的解释。例如，多项式掩码与独立掩码的使用（softmax vs sigmoid）。\n此外，Mask R-CNN 并未假设大量先验知识，因此在论文中也没有需要论证的内容。如果你有兴趣，可以仔细查看这篇论文，你可能会发现一些有趣的细节。基于你对 Faster RCNN已有了基础了解，我总结了以下一些细节帮助你进一步理解 Mask R-CNN：\n首先，Mask R-CNN 与 Faster RCNN 类似，都是两阶段网络。第一阶段都是 RPN 网络。\nMask R-CNN 添加一个并行分割分支，用于预测分割的掩码，称之为 FCN。\nMask R-CNN 的损失函数由 L_cls、L_box、L_maskLcls、L_box、L_mask 四部分构成。\nMask R-CNN 中用 ROIAlign 层代替 ROIPool。这不像 ROIPool 中那样能将你的计算结果的分数部分（x/spatial_scale）四舍五入成整数，而是通过双线性内插值法来找出特定浮点值对应的像素。\n例如：假定 ROI 高度和宽度分别是 54、167。空间尺度，也称为 stride 是图像大小 size/Fmap 的值(H/h)，其值通常为 224/14=16 (H=224,h=14)。此外，还要注意的是：\nROIPool: 54/16, 167/16 = 3,10\nROIAlign: 54/16, 167/16 = 3.375, 10.4375\n现在，我们使用双线性内插值法对其进行上采样。\n根据 ROIAlign 输出的形状(如7x7)，我们可以用类似的操作将对应的区域分割成合适大小的子区域。\n使用 Chainer folks 检查 ROIPooling 的 Python 实现，并尝试自己实现 ROIAlign。\nROIAlign 的实现代码可在不同的库中获得，具体可查看下面给出的代码链接。\nMask R-CNN 的主干网络是 ResNet-FPN。\n此外，我还曾专门写过一篇文章介绍过Mask-RCNN的原理，博客地址是：https://coming.soon/。\n实现代码\n官方的Caffe2版本：\nhttps://github.com/facebookresearch/Detectron/tree/master/configs/12_2017_baselines\nKeras版本：https://github.com/matterport/Mask_RCNN/\nPyTorch版本：https://github.com/soeaver/Pytorch_Mask_RCNN/\nMXNet版本：https://github.com/TuSimple/mx-maskrcnn\n▌ Learning to Segment Everything\n正如题目 Learning to Segment Everything 那样，这篇论文是关于目标分割任务，具体来说是解决实例分割问题。计算机视觉领域中标准的分割数据集对于现实的应用而言，数据集的数量都太有限了，即使是当前最流行、最丰富的 COCO 数据集[7]，也仅有 80 种目标类别，这还远远无法达到实用的需求。\n相比之下，目标识别及检测的数据集，如 OpenImages[8]就有将近 6000 个分类类别和 545 个检测类别。此外，斯坦福大学的另一个数据集 Visual Genome 也拥有近 3000 个目标类别。但由于这个数据集中每个类别所包含的目标数量太少了，即使它的类别在实际应用中更加丰富、有用，深度神经网络也无法在这样的数据集上取得足够好的性能，因此研究者通常不喜欢选用这些数据集进行目标分类、检测问题的研究。值得注意的是，这个数据集仅有 3000 个目标检测（边界框）的标签类别，而没有包含任何目标分割的标注，即无法直接用于目标分割的研究。\n下面来介绍我们要讲的这篇论文[4]。\n就数据集而言，实际上边界框与分割标注之间并不存在太大的区别，区别仅在于后者比前者的标注信息更加精确。因此，本文的作者正是利用 Visual Genome[9]数据集中有 3000 个类别的目标边界框标签来解决目标分割任务。我们称这种方法为弱监督学习，即不需要相关任务的完整监督信息。如果他们使用的是 COCO + Visual Genome 的数据集，即同时使用分割标签和边界框标签，那么这同样可称为是半监督学习。\n让我们回到主题，这篇论文提出了一种非常棒的思想，其网络架构主要如下：\n网络结构建立在 Mask-RCNN 基础上。\n同时使用有掩码和无掩码的输入对模型进行训练。\n在分割掩码和边界框掩码之间添加了一个权重迁移函数。\n当使用一个无掩码的输入时，将  函数预测的权重与掩码特征相乘。当传递一个有掩码的输入时，则使用一个简单的 MLP 而不使用该函数。\n如下图所示：A 表示 COCO 数据集，B 表示 Visual Genome 数据集，对网络的不同输入使用不同的训练路径。\n将两个损失同时进行反向传播将导致  不一致的权重值：对 COCO 和 Visual Genome 之间的共有的目标类别，需要同时计算掩码损失和边界框损失；而对于二者各自独有的类别，则仅需要计算边界框损失。作者使用的改进方法是：\nFix：当反向传播掩码损失时，要计算预测掩码的权重 τ 关于权重迁移函数参数 θ 的梯度值，而对边界框的权重不做该计算。\n，其中 τ 表示预测掩码的权重值。﻿\n\n由于 Visual Genome 数据集没有分割标注，模型无法给出在该数据集上目标分割的准确率，因此作者在其他的数据集上展示模型的验证结果。PASCAL-VOC 数据集有 20 个目标类别，这些类别全部包含在 COCO 数据集中。因此，对于这 20 种类别，他们使用 PASCAL-VOC 数据集的分割标注及 COCO 数据集中相应类别的边界框标签对模型进行训练。\n论文展示了在 COCO 数据集中这 20 个类别上，模型实例分割的结果。此外由于两个数据集包含两种不同的真实标签，他们还对相反的情况进行了训练，实验结果如下图所示。\n﻿\n参考文献\n[1] Lin, Tsung-Yi, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath Hariharan and Serge J. Belongie. “Feature Pyramid Networks for Object Detection.” *2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)* (2017): 936-944.\n[2] Lin, Tsung-Yi, Priya Goyal, Ross B. Girshick, Kaiming He and Piotr Dollár. “Focal Loss for Dense Object Detection.” *2017 IEEE International Conference on Computer Vision (ICCV)* (2017): 2999-3007.\n[3] He, Kaiming, Georgia Gkioxari, Piotr Dollár and Ross B. Girshick. “Mask R-CNN.” *2017 IEEE International Conference on Computer Vision (ICCV)* (2017): 2980-2988.\n[4] Hu, Ronghang, Piotr Dollár, Kaiming He, Trevor Darrell and Ross B. Girshick. “Learning to Segment Every Thing.” *CoRR*abs/1711.10370 (2017): n. pag.\n[5] Ren, Shaoqing, Kaiming He, Ross B. Girshick and Jian Sun. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” *IEEE Transactions on Pattern Analysis and Machine Intelligence* 39 (2015): 1137-1149.\n[6] Chollet, François. “Xception: Deep Learning with Depthwise Separable Convolutions.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 1800-1807.\n[7] Lin, Tsung-Yi, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár and C. Lawrence Zitnick. “Microsoft COCO: Common Objects in Context.” ECCV (2014).\n[8] Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio et al. OpenImages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages\n[9] Krishna, Ranjay, Congcong Li, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, David A. Shamma, Michael S. Bernstein and Li Fei-Fei. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” International Journal of Computer Vision 123 (2016): 32-73.\n作者：krish\n原文链接：https://skrish13.github.io/articles/2018-03/fair-cv-saga"}
{"content2":"人工智能学习线路图\nPython教程\nPython 教程\nPython 简介\nPython 环境搭建\nPython 中文编码\nPython 基础语法\nPython 变量类型\nPython 运算符\nPython 条件语句\nPython 循环语句\nPython 数字\nPython 列表(List)\nPython 字符串\nPython 元组\nPython 字典(Dictionary)\nPython 日期和时间\nPython 函数\nPython 模块\nPython File及os模块\nPython文件IO\nPython 异常处理\nPython 面向对象\nPython正则表达式\nPython CGI编程\npython操作mysql数据库\nPython 网络编程\nPython使用SMTP发送邮件\nPython 多线程\nPython XML解析\npython GUI编程(Tkinter)\nPython2.x与3 .x版本区别\nPython IDE\nPython JSON\nPython 100例\nPython实例教程\nPython3 实例教程\nPython文本处理\n01 Python - 文本处理\n02 Python - 文本处理简介\n03 Python - 文本处理环境\n04 Python - 字符串不变性\n05 Python - 排序线\n06 Python - 重新格式化段落\n07 Python - 在段落中计算令牌\n08 Python - 转换二进制为ASCII码\n09 Python - 字符串作为文件\n10 Python - 向后文件阅读\n11 Python - 过滤重复的单词\n12 Python - 从文本中提取电子邮件\n13 Python - 从文本提取URL\n14 Python - 格式化打印数字\n15 Python - 文本处理状态机\n16 Python - 资本化和翻译\n17 Python - 标记化\n18 Python - 删除停用词\n19 Python - 同义词和反义词\n20 Python - 文本翻译\n21 Python - 单词替换\n22 Python - 拼写检查\n23 Python - WordNet界面\n24 Python - Corpora Access\n25 Python - 标记单词\n26 Python - Chunks和Chinks\n27 Python - 块分类\n28 Python - 文本分类\n29 Python - Bigrams\n30 Python - 处理PDF\n31 Python - 处理Word文档\n32 Python - 阅读RSS提要\n33 Python - 情感分析\n34 Python - 搜索和匹配\n35 Python - 文本Munging\n36 Python - 文本包装\n37 Python - 频率分布\n38 Python - 文本摘要\n39 Python - 词干算法\n40 Python - 约束搜索\nAI与Python\n人工智能的基本概念\nPython的人工智能入门\nPython机器学习\nPython数据准备\nPython监督学习：分类\nPython监督学习：回归\nPython逻辑编程\nPython无人监督学习：聚类\nPython自然语言处理\n自然语言工具包 NLTK\nPython分析时间序列数据\nPython语音识别\nPython启发式搜索\nPython游戏\n基于Python神经网络\nPython强化学习\nPython遗传算法\nPython计算机视觉\nPython深度学习\nNumpy教程\nNumPy教程\nNumPy介绍\nNumPy环境\nNumPy ndarray对象\nNumPy数据类型\nNumPy数组属性\nNumPy数组创建例程\n来自现有数据的NumPy数组\n数值范围的NumPy数组\nNumPy索引和切片\nNumPy高级索引\nNumPy广播\nNumPy遍历数组\nNumPy数组操作\nNumPy二元运算符\nNumPy字符串函数\nNumPy数学函数\nNumPy算术运算\nNumPy统计函数\nNumPy排序，搜索和计数功能\nNumPy字节交换\nNumPy副本和视图\nNumPy 矩阵库\nNumPy线性代数\nNumPy Matplotlib\n使用Matplotlib的NumPy直方图\nIO与NumPy\nScipy教程\nSciPy教程\nSciPy简介\nSciPy环境设置\nSciPy的基本功能\nSciPy Cluster\nSciPy常量\nSciPy FFTpack\nSciPy集成\nSciPy插值\nSciPy输入和输出\nSciPy Linalg\nSciPy Ndimage\nSciPy优化\nSciPy Stats\nSciPy CSGraph\nSciPy空间\nSciPy ODR\nSciPy特殊包装\nPandas教程\nPandas教程\nPandas简介\nPandas环境设置\nPandas数据结构简介\nPandas序列\nPandas DataFrame\nPandas面板\nPandas基本功能\nPandas描述性统计\nPandas函数应用程序\nPandas 重建索引\nPandas迭代\nPandas排序\nPandas处理文本数据\nPandas选项和自定义\nPandas索引和选择数据\nPandas统计函数\nPandas窗口函数\nPandas Aggregations\nPandas缺少数据\nPandas GroupBy\nPandas合并_加入\nPandas串联\nPandas日期功能\nPandas Timedelta\nPandas分类数据\nPandas可视化\nPandasIO工具\nPandas稀疏数据\nPandas Caveats & Gotchas\nPandas与SQL比较\nMatplotlib 教程\n入门\n图例、标题和标签\n条形图\n直方图\n散点图\n堆叠图\n饼图\n从文件加载数据\n使用numpy从文件加载数据\nSeaborn 教程\nSeaborn介绍\nSeaborn环境设置\nSeaborn导入数据集和库\nSeaborn Figure Aesthetic\nSeaborn调色板\nSeaborn直方图\nSeaborn核密度估计\nSeaborn可视化成对关系\nSeaborn绘制分类数据\nSeaborn观测分布\nSeaborn统计估计\nSeaborn绘制宽幅数据\nSeaborn多面板分类图\nSeaborn线性关系\nSeaborn Facet Grid\nSeaborn Pair Grid\nPython数据科学教程\nPython数据科学教程\nPython数据科学介绍\nPython数据科学环境设置\nPython Pandas\nPython Numpy\nPython SciPy\nPython Matplotlib\nPython - 数据清理\nPython数据操作\nPython数据清理\n处理CSV数据的Python\nPython处理JSON数据\nPython处理XLS数据\nPython关系数据库\nPython NoSQL数据库\nPython日期和时间\nPython 数据调整\nPython数据聚合\nPython阅读HTML页面\nPython处理非结构化数据\nPython Word Tokenization\nPython的词干化和词形化\nPython图表属性\nPython图表样式\nPython Box Plots\nPython热图\nPython散点图\nPython泡泡图\nPython 3D图表\nPython时间序列\nPython地理数据\nPython图形数据\nPython测量中心趋势\nPython 测量差异\nPython正态分布\nPython二项分布\nPython泊松分布\nPython伯努利分布\nPython的P值\nPython关联\nPython卡方检验\nPython线性回归\nPython深度学习教程\nPython深度学习教程\nPython深度学习介绍\nPython深度学习环境\nPython深度基础机器学习\nPython深度学习人工神经网络\nPython深度学习深度神经网络\nPython深度学习基础\nPython深度学习训练神经网络\nPython深度学习计算图\nPython深度学习应用程序\nPython深度学习库和框架\nPython深度学习实现"}
{"content2":"一、计算机视觉：\n三个层次：系统工程方案层、领域任务模块层、基础算法层。\n三方面知识点：图像处理、机器学习、基础数学与模型。\n视频的三个场景：近距离（手机、智能硬件、PC等），室内中距离（室内，如办公室、商场、家内；卡口，门口等），室外远距离（道路、公共场所等）\n二、系统工程方案层：WEB图像结构化；离线SDK图像结构化；视频关键帧获取、结构化、序列化行为分析、结果图流媒体；\n性能上，高并发；高可用；单张时间，尽量在200ms以内，尤其是视频；准确度。\n三、领域任务模块层：五大领域（人、车、文、物、事）\n人：人体（检测、关键点、属性分类、行为、识别或者以图搜人）；人脸（检测、关键点、属性分类、活体行为、识别）；\n车：车辆（检测、关键点、品牌细分类、属性分类、行为，识别或者以图搜车）；车牌（检测、样式分类、文字识别）；\n文：OCR（图像预处理、基于图像分类、全部文字检测、指定字段定位、文字识别、基于文字内容分类），领域：票据、证件（个人、企业）、证书、车牌、自然场景（内部系统图像、门牌、站牌、物体等）\n物：动物（检测、关键点、品种细分类、属性分类、行为、识别或以图搜动物）；物体（检测、关键点、品牌细分类、属性分类、识别或以图搜物）\n事：特定场景检测，如烟火、物体遗留、工业视觉等。\n四、基础算法层：三个方面（检测分割、分类识别、图像预处理）\n检测分割：定位目标位置、分出目标类别、提取目标关键点、从画面中分割出目标的像素。\n分类识别：分类包含三层，大类、属类、细类，检测到目标后，目标品牌或者品种细分类、目标属性分类（颜色、形状、属类等）、静态行为分类、序列行为分类；提取目标特征，结合类别进行以图搜图识别。\n图像预处理：图像增强、去雾、亮度调整、倾斜校正等。\n五、深入领域体会：\n人脸领域，检测敏感度(人脸倾斜检测)、误检测率，人脸特征提取速度，人脸识别准确度。\n车辆领域，车牌准确度、敏感度；颜色、类型、品牌准确度；车辆整体特征与内部局部特征的提取；车辆行为分析。\n文字领域，图像质量对文字检测与识别的影响，图像预处理，文字检测的准确度与漏检率，文字识别，文字语义的分析。\n视频领域，三大难题：（1）误检较多。（2）漏检问题，例如遮挡、逆光条件、倾斜姿态较大等情况无法检测到。（3）速度问题，检测算法速度无法完全达到实时性，在100ms以内就不错。解决速度问题方法：a.视频关键帧或间隔帧；b.图像压缩，坐标还原；c.耗时模块在关键时刻运行，其他时刻做数据关联。"}
{"content2":"学习《Python计算机视觉编程》笔记\n第43页上面代码运行不成功\n原因：sift.py文件中process_image()函数的\ncmmd = str(“sift \" + imagename + \" --output=” + resultname + \" \" + params)\nos.system(cmmd)\n执行不成功导致的\n解决办法：请参考一下内容：\n转载：Python计算机视觉编程练习1：Python 调用 SIFT\nhttp://www.itkeyword.com/doc/6844809060106985x122/python-sift"}
{"content2":"原理：\n直线：\n一条直线在图像中是一系列离散点的集合，通过一个直线的离散极坐标公式，可以表达出直线的离散点几何等式如下：\nX\n∗\nc\no\ns\n(\nt\nh\ne\nt\na\n)\n+\ny\n∗\ns\ni\nn\n(\nt\nh\ne\nt\na\n)\n=\nr\nX *cos(theta) + y * sin(theta) = r\nX∗cos(theta)+y∗sin(theta)=r\n其中角度theta指r与X轴之间的夹角，r为到直线几何垂直距离。任何在直线上点，x, y都可以表达，其中 r， theta是常量。我们接下来要绘制每个(r, theta)值根据像素点坐标P(x, y)值，那么图像就从图像笛卡尔坐标系统转换到极坐标霍夫空间系统，这种从点到曲线的变换称为直线的霍夫变换。变换通过量化霍夫参数空间为有限个值间隔等分或者累加格子。当霍夫变换算法开始，每个像素坐标点P(x, y)被转换到(r, theta)的曲线点上面，累加到对应的格子数据点，当一个波峰出现时候，说明有直线存在。\n圆\n同以上一样的原理，我们可以用来检测圆，只是对于圆的参数方程变为如下等式：\n(\nx\n–\na\n)\n2\n+\n(\ny\n−\nb\n)\n2\n=\nr\n2\n(x –a ) ^2 + (y-b) ^ 2 = r^2\n(x–a)2+(y−b)2=r2\n其中(a, b)为圆的中心点坐标，r圆的半径。这样霍夫的参数空间就变成一个三维参数空间。给定圆半径转为二维霍夫参数空间。\n实现过程：\n实验基于上一次代码的实现，用sobel算子先实现边缘检测，然后再在边缘检测的基础上通过调节高斯滤波的内核size、标准差以及双阈值检测的阈值大小来实现对图像检测的效果。\n效果基本实现，缺点是直线检测中的图片4侧面两条边因为过亮所以干扰比较强，没有实现很好的能直接检测四条边，而是通过检测出的直线后排除较短的两条直线剩下中间的纸张。还有一个问题就是圆检测是做不到智能地检测出图像中圆的数量，而是要一个预估数量值(其实也可以通过设置一个很大的检测值来通过设置投票值的大小实现数量检测，不过算法复杂度要很高)。\n没办法实现用一套边缘检测参数测出所有的图像，干扰程度大的图片需要调参来实现。\n直线：\n过程中用到的类成员：\nCImg<int> HoughImg; //直线的霍夫空间图像 vector<lineParameter> HoughLine; //直线检测得出的直线的参数 vector<lineParameter> Hough_Line; //临时存储直线参数 vector<int> nums; //用于筛选出最长的四条边 vector<parameter> HoughPoint; //直线检测的角点\n过程中用到的类成员函数：\n//将直线变换为极坐标 void Polar_Line(); //霍夫直线检测 CImg<unsigned char> LineDetect(int, int, int); //描绘角点 CImg<unsigned char> drawPoint(); //直线检测操作 void LineDetect_Image(string, int, double, int, int, int, int, int);\n**将直角坐标转到极坐标并统计“票数”： **\nvoid Hough::Polar_Line() { CImg<int> pic = thres; int rows = pic.width(); int cols = pic.height(); int maxLength = (int)sqrt(pow(rows, 2) + pow(cols, 2)); HoughImg.resize(360, maxLength, 1, 1, 0); HoughImg.fill(0); cimg_forXY(pic, x, y) { if (pic(x,y) != 0) { for (int i = 0; i < 360; i++) { int r = (int)x * cos(M_PI*i / 180) + y * sin(M_PI*i / 180); if (r >= 0 && r < maxLength) { HoughImg(i, r)++; } } } } }\n筛选出得票数最高的我们需求数量的直线，并剔除干扰直线：\n(其中的三个参数分别表示要检测出的直线数量（都是4），干扰直线的数量，以及认为是同一条直线需要剔除掉的检测距离)\nCImg<unsigned char> Hough::LineDetect(int lineNum, int linedisturb, int distance) { int linenum = lineNum; const int Num = lineNum; CImg<unsigned char> pic = thres; pic.resize(pic.width(),pic.height(),1,3); vector<parameter> Lines; vector<parameter> result; vector<int> lineweight; cimg_forXY(HoughImg, x, y) { parameter a; a.x = x; a.y = y; Lines.push_back(a); lineweight.push_back(HoughImg(x, y)); } vector<int> sortlineweight = lineweight; sort(sortlineweight.begin(), sortlineweight.end(), greater<int>()); int fnums = lineNum + linedisturb; for (int i = 0; i < fnums; i++) { int weight = sortlineweight[i], index; vector<int>::iterator iter = find(lineweight.begin(), lineweight.end(), weight); index = iter - lineweight.begin(); int x1 = Lines[index].x, y1 = Lines[index].y; //cout << x1 << \" \" << y1 << endl; bool flag = 1; for (int k = 0; k < i; k++) { int x0 = result[k].x; int y0 = result[k].y; if (sqrt(pow(x1-x0,2)+pow(y1-y0,2)) < distance) { flag = 0; break; } } if (flag == 1) { result.push_back(Lines[index]); } else { fnums++; } } const unsigned char red[] = { 255, 0, 0 }; //cout << result.size() << endl; for (int i = 0; i < result.size(); i++) { int a0 = 0; nums.push_back(a0); } for (int i = 0; i < result.size(); i++) { //cout << result[i].x << \" \" << result[i].y << endl; //此时sin角度值为0，k为无穷，单独讨论 if (result[i].x == 0 || result[i].x == 180) { int r = 0; if (result[i].x == 0) { r = result[i].y; //cout << \"Line \" << i+1 << \": x=\" << r << endl; } else { r -= (result[i].y); //cout << \"Line \" << i+1 << \": x=\" << r << endl; } //const int x_min = 0; //const int x_max = pic.width() - 1; const int y_min = 0; const int y_max = pic.height() - 1; lineParameter temp; temp.k = DBL_MAX; temp.b = r; Hough_Line.push_back(temp); for (int yi = y_min; yi < y_max; yi++) { if (thres.atXY(r, yi) != 0) { nums[i]++; } } //pic.draw_line(r , y_min, r, y_max, red); } else { double theta = double(result[i].x)*M_PI/180; int r = result[i].y; //cout << \"Line \" << i << \" \" << theta << \" \" << r << endl; double k = (double)(-cos(theta) / sin(theta)); double b = (double) r / sin(theta); lineParameter temp; temp.k = k; temp.b = b; Hough_Line.push_back(temp); const int x_min = 0; const int x_max = pic.width() - 1; const int y_min = 0; const int y_max = pic.height() - 1; const int x0 = (double)(y_min - b) / k; const int x1 = (double)(y_max - b) / k; const int y0 = x_min * k + b; const int y1 = x_max * k + b; if (abs(k) > 1) { for (int yi = y1; yi < y_max; yi++) { int xi = (double)(yi - b) / k; if (thres.atXY(xi,yi) != 0 || thres.atXY(xi+1,yi) != 0 || thres.atXY(xi,yi+1) != 0 || thres.atXY(xi+1,yi+1) != 0) nums[i]++; } //pic.draw_line(x0, y_min, x1, y_max, red); } else { for (int xi = x_min; xi < x_max; xi++) { int yi = k*xi + b; if (thres.atXY(xi,yi) != 0 || thres.atXY(xi+1,yi) != 0 || thres.atXY(xi,yi+1) != 0 || thres.atXY(xi+1,yi+1) != 0) nums[i]++; } //pic.draw_line(x_min, y0, x_max, y1, red); } } //cout << nums[i] << endl; } vector<int> sortnums = nums; sort(sortnums.begin(), sortnums.end(), greater<int>()); for (int i = 0; i < lineNum; i++) { int weight1 = sortnums[i], index1; vector<int>::iterator iter1 = find(nums.begin(), nums.end(), weight1); index1 = iter1 - nums.begin(); HoughLine.push_back(Hough_Line[index1]); } for (int i = 0; i < HoughLine.size(); i++) { const int x_min = 0; const int x_max = pic.width() - 1; const int y_min = 0; const int y_max = pic.height() - 1; double b = HoughLine[i].b; if (HoughLine[i].k < DBL_MAX) { double k = HoughLine[i].k; const int x0 = (double)(y_min - b) / k; const int x1 = (double)(y_max - b) / k; const int y0 = x_min * k + b; const int y1 = x_max * k + b; if (b > 0) cout << \"Line \" << i+1 << \": y=\" << k << \"*x+\" << b << endl; else if (b == 0) cout << \"Line \" << i+1 << \": y=\" << k << \"*x\" << endl; else cout << \"Line \" << i+1 << \": y=\" << k << \"*x\" << b << endl; if (abs(k) > 1) { pic.draw_line(x0, y_min, x1, y_max, red); } else { pic.draw_line(x_min, y0, x_max, y1, red); } } else { cout << \"Line \" << i+1 << \": x=\" << b << endl; pic.draw_line(b , y_min, b, y_max, red); } } //pic.display(); sort(HoughLine.begin(), HoughLine.end()); //得出四个角点的坐标 for (int i = 0; i < lineNum; i++) { double k0 = HoughLine[i].k; double b0 = HoughLine[i].b; for (int j = i+1; j < lineNum; j++) { int x = 0, y = 0; if (HoughLine[j].k < DBL_MAX) { x = (double)(HoughLine[j].b - b0) / (double)(k0 - HoughLine[j].k); y = (double)(k0*HoughLine[j].b - HoughLine[j].k*b0) / (double)(k0 - HoughLine[j].k); } //K无穷大时的取值 else { x = HoughLine[j].b; y = k0*x+b0; } parameter p; p.x = x; p.y = y; if (p.x > 0 && p.x < img.width() && p.y > 0 && p.y < img.height()) { bool flag1 = 1; for (int k = 0; k < HoughPoint.size();k++) { if (p.x == HoughPoint[k].x && p.y == HoughPoint[k].y) { flag1 = 0; break; } } if (flag1) { HoughPoint.push_back(p); } } } } return pic; }\n**绘制直线相交的角点： **\nCImg<unsigned char> Hough::drawPoint() { /*for (int i = 0; i < HoughLine.size(); i++) { cout << HoughLine[i].k << \" \" << HoughLine[i].b << endl; }*/ const unsigned char blue[] = { 0, 0, 255 }; CImg<unsigned char> pic = img; for (int i = 0; i < HoughPoint.size(); i++) { cout << HoughPoint[i].x << \" \" << HoughPoint[i].y << endl; pic.draw_circle(HoughPoint[i].x, HoughPoint[i].y, 50, blue); } return pic; }\n**整合到一起： **\nvoid Hough::LineDetect_Image(string name, int scale, double sigma, int thresh_low, int thresh_high, int lineNum, int linedisturb, int distance) { const char *names = name.c_str(); img.load_bmp(names); toGrayScale(); useFilter(scale, sigma); sobel(); nonMaxSupp(); threshold(thresh_low, thresh_high); Polar_Line(); CImg<unsigned char> pic = LineDetect(lineNum, linedisturb, distance); CImg<unsigned char> pic1 = drawPoint(); string path = \"result1/\" + name.substr(9,1) + \"_a.bmp\"; string path1 = \"result1/\" + name.substr(9,1) + \"_b.bmp\"; pic.save(path.c_str()); pic1.save(path1.c_str()); }\n检测效果：\n圆：\n用到的类成员：\nCImg<unsigned char> HoughImg_Circle; //霍夫空间圆图像 CImg<unsigned char> CircleImg; //在霍夫圆图像上生成的像素点 CImg<unsigned char> thres_img; //边缘检测图像的圆检测图\n用到的类成员函数：\n//霍夫圆检测 void CircleDetect(int, int, int); //查找圆对应的像素点 void findpixelCircle(); //圆检测操作 void CircleDetect_Image(string, int, double, int, int, int, int, int);\n将半径分梯度映射到极坐标并投票筛选出图中存在的半径值并进一步以该值映射到极坐标确定圆心：\nvoid Hough::CircleDetect(int Nums, int min_r, int max_r) { thres_img = thres; thres_img.resize(thres.width(), thres.height(), 1, 3); //thres_img.display(); vector<int> sortCircleWeight; vector<pair<int,int>> Circle; vector<int> Circleweight; vector<pair<int,int>> center; int max = 0; int width = thres_img.width(); int height = thres_img.height(); CImg<int> pic = thres; //pic.display(); vector<pair<int,int>> vote; for (int r = min_r; r < max_r; r+=5) { HoughImg_Circle.resize(width, height, 1, 1, 0); HoughImg_Circle.fill(0); max = 0; cimg_forXY(pic,x,y) { if (pic(x,y) != 0) { for (int i = 0; i < 360; i++) { int x0 = x - r*cos(i*M_PI/180); int y0 = y - r*sin(i*M_PI/180); if (x0 > 0 && x0 < width && y0 > 0 && y0 < height) HoughImg_Circle(x0,y0)++; } } } cimg_forXY(HoughImg_Circle,x,y) { if (HoughImg_Circle(x,y) > max) { max = HoughImg_Circle(x,y); } } vote.push_back(make_pair(max, r)); } sort(vote.begin(), vote.end(), [](const pair<int, int>& x, const pair<int, int>& y) -> int { return x.first > y.first; }); //int Nums = 1; int knums = 0; for (int num = 0; num < Nums; num++) { int i = 0; HoughImg_Circle.resize(width, height, 1, 1, 0); HoughImg_Circle.fill(0); int r = vote[num].second; cimg_forXY(pic,x,y) { if (pic(x,y) != 0) { for (int i = 0; i < 360; i++) { int x0 = x - r*cos(i*M_PI/180); int y0 = y - r*sin(i*M_PI/180); if (x0 > 0 && x0 < width && y0 > 0 && y0 < height) HoughImg_Circle(x0,y0)++; } } } Circle.clear(); Circleweight.clear(); cimg_forXY(HoughImg_Circle,x,y) { if (HoughImg_Circle(x,y) != 0) { Circle.push_back(make_pair(x,y)); Circleweight.push_back(HoughImg_Circle(x,y)); } } unsigned char blue[3] = { 0, 0, 255 }; sortCircleWeight = Circleweight; sort(sortCircleWeight.begin(), sortCircleWeight.end(), greater<int>()); // 将累加矩阵从大到小进行排序 //同个半径圆的检测 int count = 0; while (1) { int weight = sortCircleWeight[count], index; vector<int>::iterator iter = find(Circleweight.begin(), Circleweight.end(), weight); index = iter - Circleweight.begin(); int a = Circle[index].first, b = Circle[index].second; count++; int ii; for (ii = 0; ii < center.size(); ii++) { // 判断检测出来的圆心坐标是否跟已检测的圆心坐标的距离，如果距离过小，默认是同个圆 if (sqrt(pow((center[ii].first - a), 2) + pow((center[ii].second - b), 2)) < minRadius) { break; } } if (ii == center.size()) { center.push_back(make_pair(a, b)); thres_img.draw_circle(a, b, r, blue, 1, 1); knums++; break; } } } cout << \"the number of the coins is: \" << knums << endl; }\n寻找检测出的圆在原图中对应的像素点：\nvoid Hough::findpixelCircle() { CircleImg = img; CircleImg.resize(thres.width(), thres.height()); cimg_forXY(CircleImg, x, y) { if (thres_img(x,y,0) == 0 && thres_img(x,y,1) == 0 && thres_img(x,y,2) == 255 && thres(x,y) == 0) { CircleImg(x,y,0) = 255; CircleImg(x,y,1) = 0; CircleImg(x,y,2) = 0; } } //CircleImg.display(); }\n整合到一起：\nvoid Hough::CircleDetect_Image(string name, int scale, double sigma, int thresh_low, int thresh_high, int Nums, int min_r, int max_r) { const char *names = name.c_str(); img.load_bmp(names); toGrayScale(); useFilter(scale, sigma); sobel(); nonMaxSupp(); threshold(thresh_low, thresh_high); CircleDetect(Nums, min_r, max_r); findpixelCircle(); string path = \"result2/\" + name.substr(9,1) + \"_a.bmp\"; string path1 = \"result2/\" + name.substr(9,1) + \"_b.bmp\"; thres_img.save(path.c_str()); CircleImg.save(path1.c_str()); }\n检测效果：\n完整代码参见\nhttps://github.com/WangPerryWPY/Computer-Version/tree/master/Exp3"}
{"content2":"算法岗计算机视觉方向求职经验总结\n进入11月份，楼主找工作也基本进入尾声了，从7月份开始关注牛客网，在求职的过程中学到了不少，感谢牛客提供这样一个平台，让自己的求职历程不再孤单。\n先说一下楼主教育背景，本科西部末流985，研究生调剂到帝都某文科学校.专业都是CS专业，求职方向都是计算机视觉算法。有某外企以及二线互联网实习经历，本科虽然CS出身，但实际动手能力并不强。研究生的研究方向并不是计算机视觉方向。实习的时候开始接触计算机视觉，自己比较感兴趣，开始转CV方向。回想这几个月的求职经历，其中的辛苦只有自己知道。最终拿到了百度SP，京东SSP，美团无人驾驶SP，顺丰科技SP，拼多多SP，以及虹软SP，思科，中电29等offer。\n想把我学习与求职路上的一些心得告诉学弟学妹们。\n1.一定要有一门自己比较熟悉的语言。\n我由于使用C++比较多，所以简历上只写了C++。C++的特性要了解，C++11要了解一些，还有STL。面试中常遇到的一些问题，手写代码实现一个string类，手写代码实现智能指针类，以及STL中的容器的实现机制，多态和继承，构造函数， 析构函数等。推荐看一下网易云课堂翁恺老师的C++的视频以及经典的几本书。\n2.一定要刷题\n楼主主要刷了剑指offer以及leetcode上的easy,middle的题目。如果编程能力不是很强，推荐可以分类型进行刷题，按照tag来刷，对于某一类型的题目，可以先看一下该算法的核心思想，然后再刷题。楼主在求职的过程中，遇到好多跟leetcode上类似的题目，刷题的目的不是为了碰见原题，而是为了熟练算法。当然能够碰见原题最好不过啦。\n3.机器学习的一些建议\n推荐西瓜书，以及李航老师的统计学方法。另外熟悉一种深度学习框架。学习计算机，一定要实战，毕竟只有在实战的过程中，才能懂得更透彻。可以多参加一些比赛，比如kaggle,天池，滴滴的一些比赛。这对找工作的用处很大。\n4.能实习就尽量实习。\n如果导师是学术大牛，可以带你发顶会的论文，并且自己对方向比较感兴趣，那可以在实验室待着好好搞科研。如果你研究生的研究方向跟你以后的求职方向不一致，建议早点出来实习，找个对口的实习，实习才能发现，实际工作和在学校学习的东西差距比较大。\n\n楼主能不能分享下面试问了哪些视觉的问题啊\n问到的问题主要跟我实习做的东西有关，有关于视频拆分的一些算法，以及三维点云的一些问题，传统的图像处理的一些基本操作还是要了解的，比如滤波，边缘检测，以及常用的一些传统的特征，SIFT,SURF，HOG等。深度学习这方面，我主要做过目标检测，所以问到的就是rcnn,fast-rcnn,faster-rnn,yolo,ssd这些算法。另外，问过一些调参，正则化，Batch normalization,drop out,激活函数的选择。手动推导BP,LR,SVM，算法题主要有常规的排序，二分查找，BP相关的题目，还有一些就是关于二叉树的递归和非递归遍历，层次遍历，最近公共祖先等，其余的题目记得不太清楚了\n深度学习面经\n（1）代码题（leetcode类型），主要考察数据结构和基础算法，以及代码基本功\n虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。\n大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free. 并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。\n以下是我所遇到的一些需要当场写出完整代码的题目：\n<1> 二分查找。分别实现C++中的lower_bound和upper_bound.\n<2> 排序。 手写快速排序，归并排序，堆排序都被问到过。\n<3> 给你一个数组，求这个数组的最大子段积\n时间复杂度可以到O(n)\n<4> 给你一个数组，在这个数组中找出不重合的两段，让这两段的字段和的差的绝对值最大。\n时间复杂度可以到O(n)\n<5> 给你一个数组，求一个k值，使得前k个数的方差 + 后面n-k个数的方差最小\n时间复杂度可以到O(n)\n<6> 给你一个只由0和1组成的字符串，找一个最长的子串，要求这个子串里面0和1的数目相等。\n时间复杂度可以到O(n)\n<7> 给你一个数组以及一个数K， 从这个数组里面选择三个数，使得三个数的和小于等于K， 问有多少种选择的方法？\n时间复杂度可以到O(n^2)\n<8> 给你一个只由0和1组成的矩阵，找出一个最大的子矩阵，要求这个子矩阵是方阵，并且这个子矩阵的所有元素为1\n时间复杂度可以到O(n^2)\n<9> 求一个字符串的最长回文子串\n时间复杂度可以到O(n) (Manacher算法)\n<10> 在一个数轴上移动，初始在0点，现在要到给定的某一个x点， 每一步有三种选择，坐标加1，坐标减1，坐标乘以2，请问最少需要多少步从0点到x点。\n<11> 给你一个集合，输出这个集合的所有子集。\n<12> 给你一个长度为n的数组，以及一个k值（k < n) 求出这个数组中每k个相邻元素里面的最大值。其实也就是一个一维的max pooling\n时间复杂度可以到O(n)\n<13> 写一个程序，在单位球面上随机取点，也就是说保证随机取到的点是均匀的。\n<14> 给你一个长度为n的字符串s，以及m个短串（每个短串的长度小于10），每个字符串都是基因序列，也就是说只含有A,T,C,G这四个字母。在字符串中找出所有可以和任何一个短串模糊匹配的子串。模糊匹配的定义，两个字符串长度相等，并且至多有两个字符不一样，那么我们就可以说这两个字符串是模糊匹配的。\n<15> 其它一些描述很复杂的题这里就不列了。\n（2）数学题或者\"智力\"题。\n不会涉及特别高深的数学知识，一般就是工科数学（微积分，概率论，线性代数）和一些组合数学的问题。\n下面是我在面试中被问到过的问题：\n<1> 如果一个女生说她集齐了十二个星座的前男友，她前男友数量的期望是多少？\nps：这道题在知乎上有广泛的讨论，作为知乎重度用户我也看到过。如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？\n<2> 两个人玩游戏。有n堆石头，每堆分别有a1, a2, a3.... an个石头，每次一个游戏者可以从任意一堆石头里拿走至少一个石头，也可以整堆拿走，但不能从多堆石头里面拿。无法拿石头的游戏者输，请问这个游戏是否有先手必胜或者后手必胜的策略？如果有，请说出这个策略，并证明这个策略能保证必胜。\n<3> 一个一维数轴，起始点在原点。每次向左或者向右走一步，概率都是0.5. 请问回到原点的步数期望是多少？\n<4> 一条长度为1的线段，随机剪两刀，求有一根大于0.5的概率。\n<5> 讲一下你理解的矩阵的秩。低秩矩阵有什么特点？ 在图像处理领域，这些特点有什么应用？\n<6> 讲一下你理解的特征值和特征向量。\n<7> 为什么负梯度方向是使函数值下降最快的方向？简单数学推导一下\n（3）机器学习基础\n这部分建议参考周志华老师的《机器学习》。\n下面是我在面试中被问到过的问题：\n<1> 逻辑回归和线性回归对比有什么优点？\n<2> 逻辑回归可以处理非线性问题吗？\n<3> 分类问题有哪些评价指标？每种的适用场景。\n<4> 讲一下正则化，L1和L2正则化各自的特点和适用场景。\n<5> 讲一下常用的损失函数以及各自的适用场景。\n<6> 讲一下决策树和随机森林\n<7> 讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系\n<8> 手推softmax loss公式\n<9> 讲一下SVM, SVM与LR有什么联系。\n<10>讲一下PCA的步骤。PCA和SVD的区别和联系\n<11> 讲一下ensemble\n<12> 偏差和方差的区别。ensemble的方法中哪些是降低偏差，哪些是降低方差？\n...... 这部分问得太琐碎了，我能记起来的问题就这么多了。我的感觉，这部分问题大多数不是问得很深，所以不至于被问得哑口无言，总有得扯；但是要想给出一个特别深刻的回答，还是需要对机器学习的基础算法了解比较透彻。\n（4）深度学习基础\n这部分的准备，我推荐花书（Bengio的Deep learning）和@魏秀参 学长的《解析卷积神经网络-深度学习实践手册》\n下面是我在面试中被问到过的问题：\n<1> 手推BP\n<2> 手推RNN和LSTM结构\n<3> LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失\n<4> 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？\n<5> 梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？\n<6> CNN和RNN的梯度消失是一样的吗？\n<6> 有哪些防止过拟合的方法？\n<7> 讲一下激活函数sigmoid，tanh，relu. 各自的优点和适用场景？\n<8> relu的负半轴导数都是0，这部分产生的梯度消失怎么办？\n<9> batch size对收敛速度的影响。\n<10> 讲一下batch normalization\n<11> CNN做卷积运算的复杂度。如果一个CNN网络的输入channel数目和卷积核数目都减半，总的计算量变为原来的多少？\n<12> 讲一下AlexNet的具体结构，每层的作用\n<13> 讲一下你怎么理解dropout，分别从bagging和正则化的角度\n<14> data augmentation有哪些技巧？\n<15> 讲一下你了解的优化方法，sgd,momentum, rmsprop, adam的区别和联系\n<16> 如果训练的神经网络不收敛，可能有哪些原因？\n<17> 说一下你理解的卷积核， 1x1的卷积核有什么作用？\n........\n同上，这部分的很多问题也是每个人都或多或少能回答一点，但要答得很好还是需要功底的。\n（5）科研上的开放性问题\n这部分的问题没有固定答案，也没法很好地针对性准备。功在平时，多读paper多思考，注意培养自己的insight和intuition\n下面是我在面试中被问到过的问题：\n<1> 选一个计算机视觉、深度学习、机器学习的子领域，讲一下这个领域的发展脉络，重点讲出各种新方法提出时的motivation，以及谈谈这个领域以后会怎么发展。\n<2> 讲一下你最近看的印象比较深的paper\n<3> 讲一下经典的几种网络结构， AlexNet， VGG，GoogleNet，Residual Net等等，它们各自最重要的contribution\n<4> 你看过最近很火的XXX paper吗? 你对这个有什么看法？\n......\n（6） 编程语言、操作系统等方面的一些问题。\nC++， Python， 操作系统，Linux命令等等。这部分问得比较少，但还是有的，不具体列了\n（7）针对简历里项目/论文 / 实习的一些问题。\n这部分因人而异，我个人的对大家也没参考价值，也不列了。\n作者：牛客网\n链接：https://zhuanlan.zhihu.com/p/30212122\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n楼主本科，今年前前后后面了一些BAT的实习和校招，岗位主要是基础研究，机器学习方面。最后去T的技术研究岗。并不是大神，只是在这里总结一下自己的看法和面经。因为面试题主要是看面试官，并不是看公司，所以这里就不分开写了。\n这一类的岗位面试大多数是以简历为基础，会根据简历上所提到的一些工具(比如caffe，cuda)，算法(比如CNN,随机森林)和项目进行提问。所以准备面试最重要的就是要深刻的理解这些工具和算法的基础和优劣势，和实际工程上会遇到的问题并且要知道如何解决(比如，SVD在遇到数据特别大的时候会产生一定的问题？如何解决？)。如果自己不太熟或说不清楚的算法或记忆不太清楚的项目最好不要写在简历上。一般之前实习的具体内容不会被问到，面试官感兴趣的主要是学校课程和实验室所做的基础项目。\n这类岗位同时也经常会问到一些统计相关的问题，所以基本的统计模型，概率题，矩阵知识得熟悉。也经常会遇到一些很抽象的问题，比如某技术的前景，和一些技术的对比，所以建议大家多看看机器学习相关的公众号啊之类的。\n面试时楼主认为不仅是你能回答所有问题，态度等也很重要，要让面试官感受到你的谦虚和热情并且感受到你是一个将来可以融入他们团队的人。面试官更想看到你是个学习能力强并且积极阳光的人，毕竟校招不同于社招，并不会期待你能懂很多很多。\n以下是楼主遇到过的问题，这些问题只供参考，不同面试官问的问题差别会很大。所以重点还是放在精通简历内容上。\n有任何具体问题可以私信我～祝大家都拿到心仪的offer！\n算法,数据库,数据结构:\n1. 快排\n2. 归并排序(核心为一个merge函数，把两个sort好的list合并起来)\n3. 冒泡排序\n4. 两个单向链表相接的一系列问题\n5. reverse一个linked list，while和for两种方法\n6. dynamic programming (要会做基础的背包问题，longest substring等)\n7. mysql索引\n8. B+树(节点如何分裂的)\n9. 数据库底层所用到的grace hash join和merge join\n10. 死锁和解决办法\n11. n个数取k个最大\n数理统计:\n1. 如何求出一个圆的sin1度的大小(用taylor series)\n2. 有一个不规则硬币，如何用它做一个1到6的随机数生成器(核心为找六个概率相同的事件)\n3. 斗地主摸到王炸和四个二的概率是多少\n4. 有一个二位坐标轴，你从原点可以往上下左右走，请问100步后走回原点的可能性(题目大概是这样，学过随机过程的可以做做)\n5. confidence interval\n6. 统计中的P值和Alpha值\n7. 三门问题\n8. 还有一些其他的概率题不太记得了\n9. 矩阵的基本知识得牢记\n机器学习方面:\n(机器学习方面的问题一般会根据简历来问, 简历上写了的算法一定要很清楚如何底层实现，面试官一般都会让你给他讲)\n1. 随机森林(注意要了解随机树上的每个节点上的feature是怎么选择的)\n2. 线性回归(最优解表达式要牢记)\n3. logistic回归\n4. logistic函数和relu函数的图像是什么样的\n5. 反向传播神经网络的讲解\n6. PCA和SVD(数学方面的分解最好能比较精通)\n7. SVD在遇到数据特别多的时候会产生一定的问题？如何解决？\n8. cuda可以通过哪些方法对神经网络进行加速\n9. 有一张统计qq在线时间分布图，基于一些统计的test提问\n10. 推荐系统(简单的可以用PCA和SVD做)\n11. RNN和CNN的对比\n12. 如何做矩阵的卷积。\n语言：\n(C++,java的这里就不写了，现在这类岗位比较喜欢问python相关的问题）\n1. python2和python3有哪些区别\n2. 用哪些库(比较基础的是numpy和scikit-learn)\n3. python如何继承其他的class\n4. super()的用法\n5. python较java相比的优劣势\n6. AngularJS, ReactJS, html的比较(写过前端的同学可以看看)\n7. unix里的一些指令，比如cat，mkdir\n8. git的一些指令，比如fetch和pull的区别\n9. SQL一系列问题\n2017秋招面试总结-计算机视觉/深度学习算法\n作者：拾荒者000\n链接：https://www.nowcoder.com/discuss/66114\n来源：牛客网\n楼主秋招过程，也没面几家（8月低到9月中旬面试安排比较多），9月15号之前把公司定了。月底就把三分寄出去了，找的比较随意。国庆节之后，一家没面，把所有的面试都推了，进入二面的也推了。\n现在把我面试的公司的面试情况总结一下（以下仅代表个人经验和观点）：\n1、简历上的项目非常重要，对一些细节一定要熟悉，尤其是项目中用到的算法。\n2、基础很重要！！！\n图谱科技面试\n1、  梯度下降：为什么多元函数在负梯度方向下降最快？\n数学证明题，和专业无关-->设多元函数及多元函数的单位向量，再求多元函数在单位向量上的方向导数\n2、  Sigmoid激活函数为什么会出现梯度消失？Sigmoid函数导数的最大值出现在哪个值？-->（x=0处）\nReLU激活函数为什么能解决梯度消失问题？\n3、  Softmax是和什么loss function配合使用？-->多项式回归loss\n该loss function的公式？\n4、  （以xx loss function）推导BP算法？\n5、  CNN中，卷积层的输入为df*df*M（weight,height,channel），输出为df*df*N（或输出为df*df*M），卷积核大小为dk*dk时，请问由输入得到输出的计算量为多少？题中默认stride=1\n计算量-->浮点数计算量：49HWC^2，27HWC^2-->会把滤波器用到每个像素上，即为长x宽x可学习的参数个数\n6、  说一下dropout的原理？若在训练时，以p的概率关闭神经元，则在预测（测试）的时候概率1-p怎么使用？https://yq.aliyun.com/articles/68901\n测试时，被dropout作用的层，每个神经元的输出都要乘以（1-p）à使训练和测试时的输出匹配\n7、  传统机器学习是否了解？\n8、  说一下作项目时遇到的困难？\n9、  表达式为max(x,y)的激活函数，反向传播时，x、y上的梯度如何计算à\n答：较大的输入的梯度为1，较小输入的梯度为0；即较小的输入对输出没有影响；另一个值较大，它通过最大值运算门输出，所以最后只会得到较大输入值的梯度。à这也是最大值门是梯度路由的原因。\n前向传播时，最大值往前传播；反向传播时，会把梯度分配给输入值最大的线路，这就是一个梯度路由。\n地平线机器人面试\n1、  检测框架faster rcnn是怎样的一个框架？à这里回答了faster rcnn的过程\n2、  Faster rcnn中，ROI pooling具体如何工作（怎么把不同大小的框，pooling到同样的大小）？\nRoIPool首先将浮点数值的RoI量化成离散颗粒的特征图，然后将量化的RoI分成几个空间的小块（spatial bins），最后对每个小块进行max pooling操作生成最后的结果。\n3、优化代码的方法：多线程等à多线程比单线程快\n3、  深度学习那个项目做的方法没有创新点；深度学习项目，数据集要自己做，检测方法要创新à自己制作数据集并添加新层（新的激活函数maxout）\n4、  每个项目的衡量指标；如：（1）双目追踪能检测的目标最小是多大à能检测的最小目标是根据实时图像中最大的目标而定的，设定目标面积小于最大的目标的面积的1/5是不能检测的。\n（2）深度学习中的指标mAP等（衡量模型好坏的指标？）平均精度（mAP）如何计算的？http://blog.csdn.net/timeflyhigh/article/details/52015163\nhttp://blog.csdn.net/Relocy/article/details/51453950\n目标检测的指标：识别精度，识别速度，定位精度\nA、目标检测中衡量识别精度的指标是mAP（mean average precision）。多个类别物体检测中，每一个类别都可以根据recall和precision绘制一条曲线，AP就是该曲线下的面积，mAP是多个类别AP的平均值。\nB、  目标检测评价体系中衡量定位精度的指标是IoU,IoU就是算法预测的目标窗口和真实的目标窗口的交叠（两个窗口面积上的交集和并集比值），Pascal VOC中，这个值是0.5（已被证明相对宽松）。\n机器学习中评价指标： Accuracy、 Precision、Recall\n6、 熟悉基本的图像处理算法和图像处理方法（如图像矫正）\n5、  Caffe中具有哪些层，如data layer、image data layer、softmaxwithloss（还有其它loss）\n6、  训练网络时，如果要每个batch中每种类别的图象数固定（按自己定的取），则该怎么做？（训练时，每个batch都是随机从数据集中抽取一定数量的样本）。\n7、  立体匹配有哪些方法？收藏的链接\n8、  混合高斯模型（GMM）是怎么样的？à原理和公式\n混合高斯模型是无监督学习à可用于聚类\nhttp://www.cnblogs.com/mindpuzzle/archive/2013/04/24/3036447.html\nhttp://blog.csdn.net/wqvbjhc/article/details/5485242\n混合高斯模型使用K（基本为3到5个）个高斯模型来表征图像中各个像素点的特征,在新一帧图像获得后更新混合高斯模型, 用当前图像中的每个像素点与混合高斯模型匹配,如果成功则判定该点为背景点,将其归入该模型中，并对该模型根据新的像素值进行更新，若不匹配，则以该像素建立一个高斯模型，初始化参数，代理原有模型中最不可能的模型。最后选择前面几个最有可能的模型作为背景模型，为背景目标提取做铺垫。\n9、  光流法？\nhttp://blog.csdn.net/carson2005/article/details/7581642\nhttp://www.cnblogs.com/xingma0910/archive/2013/03/29/2989209.html\n光流法，通过求解偏微分方程求的图像序列的光流场，从而预测摄像机的运动状态\n10、              Kalman滤波器的原理？\nKalman滤波器最重要的两个过程：预测和校正\nhttp://blog.csdn.net/carson2005/article/details/7367135\n11、              需要熟悉简历上项目写的每个算法的具体过程甚至公式；；；以及是否对算法进行改进，即修改OpenCV的源码\n12、              编程：合并两个单调不减的链表，并保证合并后的链表也是单调不减的？\n好未来\n1、  LeetCode第一题“TwoSum”\n2、  通过简单示例，详细解释ROC曲线，要求给出必要的公式推导。\n3、  给出LR（逻辑回归）算法的cost function公式的推导过程。\n4、  目标检测时，输入的是视频时，如何进行检测？视频中有很多无用的帧（不包含要检测的目标等）-->人工分割视频、每隔一定数量的帧进行检测\n5、  项目。\n美团\n一面：\n1、  faster rcnn中ROI pooling 是不是第一次用？-->第一次用是在fast rcnn中\n2、  在检测物体时，物体只有少部分在图像中时，是否检测？\n系统检测的最小目标为16*16；当部分在图像中时，也对其进行检测（这里有一个阈值，当目标的面积占图像的面积比小于1/5时，不检测；否则就检测（这个思路是从单路分类那来的））\n3、  双目视觉中，立体校正如何进行？-->立体标定得出俩摄像机的旋转和平移矩阵，然后在对左右图像进行校正，使其行对齐。\n4、  Kalman滤波器是否有运动方程？没建立运动方程，直接将物体轮廓的外接矩形的中心点作为初始化追踪点进行后续追踪。\n5、  双目视觉中，光流法用的哪一种？L-K光流是稠密的还是稀疏的？\n金字塔l-k光流，其计算的是稀疏特征集的光流。\n非金字塔l-k光流(原始的l-k光流)计算的是稠密光流。\nhttp://blog.sina.com.cn/s/blog_15f0112800102wjai.html\n二面：\n1、  Fatser rcnn与rcnn的不同？-->fatser rcnn是端到端；rcnn不是端到端\n2、  Rcnn、fatse rcnn、fatser rcnn、mask rcnn的原理？\nmask rcnn-->在fatser rcnn的基础上对ROI添加一个分割的分支，预测ROI当中元素所属分类，使用FCN进行预测；\n具体步骤：使用fatser rcnn中的rpn网络产生region proposal（ROI），将ROI分两个分支：（1）fatser rcnn操作，即经过ROI pooling 输入fc进行分类和回归；（2）mask操作，即通过ROIAlign校正经过ROI Pooling之后的相同大小的ROI，然后在用fcn进行预测（分割）。\nROIAlign产生的原因：RoI Pooling就是实现从原图区域映射到卷积区域最后pooling到固定大小的功能，把该区域的尺寸归一化成卷积网络输入的尺寸。在归一化的过程当中，会存在ROI与提取到的特征不对准的现象出现，由于分类问题对平移问题比较鲁棒，所以影响比较小。但是这在预测像素级精度的掩模时会产生一个非常的大的负面影响。作者就提出了这个概念ROIAlign，使用ROIAlign层对提取的特征和输入之间进行校准。\nROIAlign方法：作者用用双线性插值（bilinear interpolation）在每个RoI块中4个采样位置上计算输入特征的精确值，并将结果聚合（使用max或者average）。\nLmask为平均二值交叉熵损失。\n实例分割的目的是区分每一个像素为不同的分类而不用区别不同的目标。实例分割就是要在每一个像素上都表示出来目标所属的具体类别。\n3、  介绍resnet和GoogLeNet中的inception module的结构？\nResNet 主要的创新在残差网络，其实这个网络的提出本质上还是要解决层次比较深的时候无法训练的问题。这种借鉴了Highway Network思想的网络相当于旁边专门开个通道使得输入可以直达输出，而优化的目标由原来的拟合输出H(x)变成输出和输入的差H(x)-x，其中H(X)是某一层原始的的期望映射输出，x是输入。\nà优化后\n优化后的结构新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量\nResnet  http://blog.csdn.net/mao_feng/article/details/52734438\nInception module:\n共有四个版本。网上搜inception v4就会出现v1-v4\nhttp://www.voidcn.com/article/p-zglerubc-ty.html   （重点）\nhttp://blog.csdn.net/u010025211/article/details/51206237\nhttp://blog.csdn.net/sunbaigui/article/details/50807418\ninception v1中用1*1的卷积à降维\nInception v2（BN-inception）在v1的基础上增加BN层，同时将5*5的卷积核替换成两个3*3的卷积核（降低参数数量，加速计算）\nInception v3见博客http://www.voidcn.com/article/p-zglerubc-ty.html\nv3一个最重要的改进是分解（Factorization），将7x7分解成两个一维的卷积（1x7,7x1），3x3也是一样（1x3,3x1），这第一个样的好处，既可以加速计算（多余的计算能力可以用来加深网络），又可以将1个conv拆成2个conv，使得网络深度进一步增加，增加了网络的非线性\nv3使用的是RMSProp优化方法\ninception v4-->16年imagenet分类的第一名\nhttp://blog.csdn.net/lynnandwei/article/details/53736235\n由链接中的图可以看出，v4包含v3和v2\n4、  yolo和ssd？\n5、  Fatser rcnn不能检测小目标的原因？\n6、  在训练好的模型数据里， 如何添加识别错误的数据，在进行训练呢？\n方法一：直接往lmdb数据里添加，再次重新训练；\n方法二：把你的proto里datalayer改成用image data layer 然后把需要添加的图像路径写到list文件里，然后fine tune你的网络\nRoKid机器人\n1、  Adaboost算法？\n2、  逻辑回归实现多分类？\n3、  Fatser rcnn中，如何处理背景候选框和含有目标的候选框不平衡问题？\n4、  SVM的核函数的作用？\n5、其他就是和项目相关的问题？\n数码视讯\n1、  Canny边缘检测算法的过程？\n2、  常用的局部特征和全局特征？\n3、LDA原理？\n除了上面的公司之外，还有顺丰科技、苏宁、恒润科技，这三个公司问项目相关的比较多，还是要了解自己的项目以及一些相关的基础知识。\n面试的水，总结的也水，觉得有用的就看看，不喜勿喷！！！"}
{"content2":"8月26日至27日，在中国科学技术协会、中国科学院的指导下，由中国人工智能学会发起主办、中科院自动化研究所与CSDN共同承办的2016中国人工智能大会（CCAI 2016）在北京辽宁大厦盛大召开，这也是本年度国内人工智能领域规模最大、规格最高的学术和技术盛会，对于我国人工智能领域的研究及应用发展有着极大的推进作用。大会由CSDN网站进行专题直播，并由百度开放云提供独家视频直播技术支持。\n大会邀请了40多位全球顶级人工智能专家共论前沿/热门技术与产业实践。麻省理工学院人工智能实验室（MIT CSAIL）教授、美国人文与科学院院士Tomaso Poggio在本次大会上发表了题为《The Science and the Engineering of Intelligence》的演讲，并接受CSDN记者专访，分享了自己对如何发展人工智能技术的看法。\n麻省理工学院人工智能实验室（MIT CSAIL）教授、美国人文与科学院院士Tomaso Poggio\n希望像爱因斯坦一样成为智慧的代表\nTomaso Poggio的研究兴趣不仅仅在于人工智能，还包括了人类本身的智能。\n在被问及因何被人工智能所吸引时，Tomaso Poggio谈到了儿时的梦想：他小时候特别迷恋物理，希望像爱因斯坦一样成为智慧的代表人物，成为解决智能问题的天才。\n“如果我们能懂得智慧是因何而来，智能又是怎样被创造出来的，那么其他问题就会被智能本身所解决。”他说。正式因为这个兴趣，Tomaso Poggio走上了研究人工智能的道路。\n看好智能驾驶、深度学习\nTomaso Poggio表示，人工智能的新技术发展很快，先是在美国的问答游戏，然后在围棋、括游戏等方面都有比较好的成果。\n而在现实当中，基于以色列创业公司Mobileye做的小型计算机视觉芯片，汽车可以像人一样根据视觉来做自动驾驶。这种芯片目前已经在沃尔沃、特斯拉的一些汽车上应用。Tomaso Poggio对这种芯片具有浓厚的兴趣，并表示汽车驾驶员终将会被机器在一定程度上取代，就像现在飞机的自动巡航，让飞机员能够得到很大程度的解放。\n智能驾驶的视觉智能主要得益于深度学习的进展。事实上，谈到当前人工智能领域的最大研究进展，Tomaso Poggio认为基于深度学习的方法机会比较大。\n机器学习的进展依赖神经科学\n但Tomaso Poggio更为关注的人类智能本身。他表示，深度学习采取的多层的机制，其实是来源于大脑的机制。深度学习的方式，其实是受到了十九世纪六七十年代哈佛大学的研究者在猴子大脑上做试验的启发。\n他认为，这是一个很好的印证：如果我们要在智能方面走得远，不能只靠计算机，还需要和人类本身的研究相互结合，才能碰撞出更多的东西。这就是说，需要智能科学的研究作为基础。\n然而，对于当下智能科学的发展，Tomaso Poggio持谨慎态度。他认为，目前智能的实现，还是很依赖于数据，但这不是真正的人类学习的过程——人类学习是通过与家长、监护人等的频繁互动达到学习的过程，而现在机器好像是比较笨地学习。他认为，接下来研究人员不应只是从机器角度去研究，而是要从认知科学、神经科学以及人类认识来解决这个问题。\n中国AI研究印象\n对于与中国AI学者或企业机构的交流，Tomaso Poggio表示，与中科院自动化所有合作，但并不是很紧密。但这次他来到CCAI大会后，发现中国学者在神经科学和认知科学领域也有比较多的研究，这种相似的研究应该多一些交流，以便可形成合力推动整个领域的发展。\n但他同时表示，中国人工智中国的研究进展不错，学术贡献比较多，不过现在还没有出现在人工智能领域有比较突出的贡献的商业公司。"}
{"content2":"转自：http://blog.csdn.net/zhubenfulovepoem/article/details/7191794\n作者：zhubenfulovepoem\n以下是computer vision：algorithm and application计算机视觉算法与应用这本书中附录里的关于计算机视觉的一些测试数据集和源码站点，我整理了下，加了点中文注解。\nComputerVision:\nAlgorithms and Applications\nRichard Szeliski\n在本书的最好附录中，我总结了一些对学生，教授和研究者有用的附加材料。这本书的网址http://szeliski.org/Book包含了更新的数据集和软件，请同样访问他。\nC.1 数据集\n一个关键就是用富有挑战和典型的数据集来测试你算法的可靠性。当有背景或者他人的结果是可行的,这种测试可能甚至包含更多的信息(和质量更好)。\n经过这些年，大量的数据集已经被提出来用于测试和评估计算机视觉算法。许多这些数据集和软件被编入了计算机视觉的主页。一些更新的网址，像CVonline\n(http://homepages.inf.ed.ac.uk/rbf/CVonline ), VisionBib.Com (http://datasets.visionbib.com/ ), and Computer Vision online (http://computervisiononline.com/ ), 有更多最新的数据集和软件。\n下面，我列出了一些用的最多的数据集，我将它们让章节排列以便它们联系更紧密。\n第二章：图像信息\nCUReT: Columbia-Utrecht 反射率和纹理数据库Reﬂectance and TextureDatabase, http://www1.cs.columbia.edu/CAVE/software/curet/  (Dana, van Ginneken, Nayaret al. 1999).\nMiddlebury Color Datasets:不同摄像机拍摄的图像，注册后用于研究不同的摄像机怎么改变色域和彩色registeredcolor images taken by different cameras to study how they transform gamuts andcolors,http://vision.middlebury.edu/color/data/ Chakrabarti, Scharstein, and Zickler 2009).\n第三章：图像处理\nMiddlebury test datasets forevaluating MRF minimization/inference algorithms评估隐马尔科夫随机场最小化和推断算法,\nhttp://vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharstein et al. 2008).\n第四章：特征检测和匹配\nAfﬁne Covariant Featuresdatabase（反射协变的特征数据集） for evaluating feature detector and descriptor matching quality andrepeatability（评估特征检测和描述匹配的质量和定位精度）,   http://www.robots.ox.ac.uk/~vgg/research/affine/\n(Miko-lajczyk and Schmid 2005;Mikolajczyk, Tuytelaars, Schmid et al. 2005).\nDatabase of matched imagepatches for learning （图像斑块匹配学习数据库）and feature descriptor evaluation（特征描述评估数据库）,\nhttp://cvlab.epfl.ch/~brown/patchdata/patchdata.html\n(Winder and Brown 2007;Hua,Brown, and Winder 2007).\n第五章;分割\nBerkeleySegmentation Dataset（分割数据库） and Benchmark of 1000 images labeled by 30 humans,（30个人标记的1000副基准图像）along with an evaluation,http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/  (Martin, Fowlkes, Tal et al.2001).\nWeizmann segmentationevaluation database of 100 grayscale images with ground truth segmentations,\nhttp://www.wisdom.weizmann.ac.il/~vision/Seg EvaluationDB/index.html\n(Alpert, Galun, Basri et al. 2007).\n第八章：稠密运动估计\nTheMiddlebury optic ﬂow evaluation（光流评估） Web site, http://vision.middlebury.edu/flow/data/\n(Baker,Scharstein, Lewis et al. 2009).\nThe Human-Assisted MotionAnnotation database,（人类辅助运动数据库）\nhttp://people.csail.mit.edu/celiu/motionAnnotation/  (Liu, Freeman, Adelson etal. 2008)\n第十章：计算机摄像学\nHigh DynamicRange radiance（辐射）maps, http://www.debevec.org/Research/HDR/\n(De-bevecand Malik 1997).\nAlpha matting evaluation Website, http://alphamatting.com/ (Rhemann, Rother, Wang\net al. 2009).\n第十一章：Stereo correspondence立体对应\nMiddlebury Stereo Datasets andEvaluation, http://vision.middlebury.edu/stereo/  (Scharstein\nand Szeliski 2002).\nStereoClassiﬁcation（立体分类） and Performance Evaluation（性能评估） of different aggregation（聚类） costs for stereo matching（立体匹配）, http://www.vision.deis.unibo.it/spe/SPEHome.aspx  (Tombari, Mat-\ntoccia, Di Stefano et al.2008).\nMiddlebury Multi-View StereoDatasets,\nhttp://vision.middlebury.edu/mview/data/  (Seitz,Curless, Diebel etal. 2006).\nMulti-view and Oxford Collegesbuilding reconstructions,\nhttp://www.robots.ox.ac.uk/~vgg/data/data-mview.html .\nMulti-View Stereo Datasets, http://cvlab.epfl.ch/data/strechamvs/  (Strecha, Fransens,\nand Van Gool 2006).\nMulti-View Evaluation,  http://cvlab.epfl.ch/~strecha/multiview/ (Strecha, von Hansen,\nVan Gool et al. 2008).\n第十二章：3D重建\nHumanEva: synchronized video（同步视频） and motion capture （动作捕捉）dataset for evaluation ofarticulated human motion, http://vision.cs.brown.edu/humaneva/  Sigal, Balan, and Black 2010).\n第十三章：图像渲染\nThe (New) Stanford Light FieldArchive, http://lightfield.stanford.edu/\n(Wilburn, Joshi,Vaish et al.2005).\nVirtual Viewpoint Video:multi-viewpoint video with per-frame depth maps,\nhttp://research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/  (Zitnick, Kang, Uytten-\ndaele et al. 2004).\n第十四章：识别\n查找一系列的视觉识别数据库，在表14.1–14.2.除了那些，这里还有：\nBuffy pose classes, http://www.robots.ox.ac.uk/~vgg/data/  buffy pose classes/ andBuffy\nstickmen V2.1, http://www.robots.ox.ac.uk/~vgg/data/stickmen/index.html  (Ferrari,Marin-\nJimenez, and Zisserman 2009;Eichner and Ferrari 2009).\nH3D database of pose/jointannotated photographs of humans,\nhttp://www.eecs.berkeley.edu/~lbourdev/h3d/   (Bourdev and Malik 2009).\nAction Recognition Datasets,http://www.cs.berkeley.edu/projects/vision/action, has point-\ners toseveral datasets for action and activity recognition, as well as some papers.（有一些关于人活动和运动的数据库和论文） The humanaction database at http://www.nada.kth.se/cvap/actions/  包含更多的行动序列。\nC.2 软件资源\n一个对于计算机视觉算法最好的资源就是开源视觉图像库（opencv）(http://opencv.willowgarage.com/wiki/),他有在intel的Gary Bradski和他的同事开发，现在由Willow Garage (Bradsky and Kaehler 2008)维护和扩展。一部分可利用的函数在http://opencv.willowgarage.com/documentation/cpp/中：\n图像处理和变换 (滤波，形态学，金字塔);\n图像几何学的变换 (旋转，改变大小);\n混合图像变换 (傅里叶变换，距离变换);\n直方图;\n分割 (分水岭, mean shift);\n特征检测 (Canny, Harris, Hough, MSER, SURF);\n运动分析和物体分析 (Lucas–Kanade, mean shift);\n相机矫正和3D重建\n机器学习 (k nearest neighbors, 支持向量机, 决策树, boost-\ning, 随机树, expectation-maximization, 和神经网络).\nIntel的Performance Primitives (IPP)library, http://software.intel.com/en-us/intel-ipp/，包含\n各种各样的图像处理任务的最佳优化代码，许多opencv中的例子利用了这个库，加入他安装了，程序运行得更快。依据功能，他和Opencv有很多相同的运算处理，并且加上了额外的库针对图像视频压缩，信号语音处理和矩阵代数。\nMTALAB中的Image Processing Toolbox图像处理工具，http://www.mathworks.com/products/image/，包含常规的处理，空域变换（旋转，改变大小），常规正交，图像分析和统计学（变边缘，哈弗变换），图像增强（自适应直方图均衡，中值滤波），图像恢复（去模糊），线性滤波（卷积），图像变换（傅里叶，离散余弦变换）和形态学操作（连通域和距离变换）\n两个比较旧的库，它们没有被发展，但是包含了一些的有用的常规操作：\nVXL (C++Libraries for Computer Vision Research and Implemen-tation, http://vxl.sourceforge.net/)\nLTI-Lib 2 (http://www.ie.itcr.ac.cr/palvarado/ltilib-2/homepage/ ).\n图像编辑和视图包，例如Windows Live Photo Gallery, iPhoto, Picasa,GIMP, 和 IrfanView，它们对执行这些处理非常有用：常规处理任务，格式转换，观测你的结果。它们同样可以用于对图像处理算法有趣的实现参考，例如色调调整和去噪。\n这里他也有一些软件包和基础框架对你建一个实时视频处理的DEMOS很有用，Vision on Tap(http://www.visionontap.com/ )提供一个可以实时处理你的网络摄像头的网页服务(Chiu and Raskar 2009）。Video-Man (VideoManager, http://videomanlib.sourceforge.net/处理实时的基于视频的DEMOS和应用非常有用，你也可以用MATLAB中的imread直接从任何URl（例如网络摄像头）中读取视频。\n下面，我列出了一些额外的网络资源，让章节排列以便它们看起来联系更紧密：\n第三章:图像处理\nmatlabPyrTools—MATLAB 下的源码对于拉普拉斯变换，金字塔, QMF/小波, 和\nsteerable pyramids, http://www.cns.nyu.edu/~lcv/software.php  (Simoncelli and Adel-\nson 1990a; Simoncelli,Freeman, Adelson et al. 1992).\nBLS-GSM 图像去噪, http://decsai.ugr.es/~javier/denoise/  (Portilla, Strela,Wain-\nwright et al. 2003).\nFast bilateral ﬁltering code（快速双边滤波）, http://people.csail.mit.edu/jiawen/#code (Chen, Paris, and Durand 2007).\nC++ implementation of the fastdistance transform algorithm,\nhttp://people.cs.uchicago.edu/~pff/dt/  (Felzenszwalb andHuttenlocher 2004a).\nGREYC’s Magic Image Converter,including image restoration software using regularization and anisotropicdiffusion, http://gmic.sourceforge.net/gimp.shtml (Tschumperl´ e and Deriche 2005).\n第四章：图像特征检测和匹配\nVLFeat, 一个开放便捷的计算机视觉算法库\nhttp://vlfeat.org/ (Vedaldi and Fulkerson 2008).\nSiftGPU: A GPU Implementationof Scale Invariant Feature Transform (SIFT),\nGPU实现的尺度特征性变换\nhttp://www.cs.unc.edu/~ccwu/siftgpu/  (Wu 2010).\nSURF: Speeded Up RobustFeatures, http://www.vision.ee.ethz.ch/~surf/\n(Bay, Tuyte-laars, and VanGool 2006).\nFAST corner detection, http://mi.eng.cam.ac.uk/~er258/work/fast.html\n(Rosten and Drum-mond 2005, 2006).\nLinux binaries for afﬁneregion detectors and descriptors, as well as MATLAB ﬁles to\ncompute repeatability andmatching scores,\nhttp://www.robots.ox.ac.uk/~vgg/research/affine/\nKanade–Lucas–Tomasi featuretrackers: KLT, http://www.ces.clemson.edu/~stb/klt/ (Shi and Tomasi 1994);\nGPU-KLT, http://cs.unc.edu/~cmzach/opensource.html  (Zach,Gallup, and Frahm2008); Lucas–Kanade 20 Years On, http://www.ri.cmu.edu/projects/project 515.html  (Baker and Matthews 2004).\n第五章：分割\n高效的基于图形的分割http://people.cs.uchicago.edu/~pff/segment\n(Felzenszwalb and Huttenlocher2004b).\nEDISON, 边缘检测和图像追踪,\nhttp://coewww.rutgers.edu/riul/research/code/EDISON/\n(Meer and Georgescu 2001; Comaniciu and Meer2002).\nNormalized cuts segmentationincluding intervening contours,\nhttp://www.cis.upenn.edu/~jshi/software/\n(Shi and Malik 2000; Malik,Belongie, Leung et al. 2001).\nSegmentation by weightedaggregation (SWA),利用加权集合的分割\nhttp://www.cs.weizmann.ac.il/~vision/SWA  (Alpert, Galun, Basri et al.2007).\n第六章：基于特征的对齐和校准\nNon-iterative PnP algorithm,（非迭代PnP算法）\nhttp://cvlab.epﬂ.ch/software/EPnP  (Moreno-Noguer, Lep-etit, and Fua 2007).\nTsai Camera Calibration（相机矫正） Software,\nhttp://www-2.cs.cmu.edu/~rgw/TsaiCode.html  (Tsai 1987).\nEasy CameraCalibration Toolkit,（简易相机校准工具包） http://research.microsoft.com/en-us/um/people/zhang/ Calib/ (Zhang 2000).\nCamera Calibration Toolbox forMATLAB,\nhttp://www.vision.caltech.edu/bouguetj/calib doc/ ; a C version is included in OpenCV.\nMATLAB functions for multipleview geometry,\nhttp://www.robots.ox.ac.uk/~vgg/hzbook/code/  (Hartley and Zisserman2004).\n第七章：运动重建\nSBA: A generic sparse bundle(稀疏束) adjustment C/C++ package basedon the Levenberg–\nMarquardt algorithm, http://www.ics.forth.gr/~lourakis/sba/  (Lourakis and Argyros 2009).\nSimple sparse bundleadjustment (SSBA), http://cs.unc.edu/~cmzach/opensource.html .\nBundler, structure from motionfor unordered image collections(无序图像集),\nhttp://phototour.cs.washington.edu/bundler/   (Snavely, Seitz, and Szeliski 2006).\n第八章:稠密运动估计\n光流, http://www.cs.brown.edu/~black/code.html (Black and Anan-\ndan 1996).\nOptical ﬂow（光流） using total variation（全变量差） and conjugate gradientdescent（共轭梯度下降）, http://people.csail.mit.edu/celiu/OpticalFlow/  (Liu 2009).\nTV-L1 optical ﬂow on the GPU, http://cs.unc.edu/~cmzach/opensource.html\n(Zach,Pock, and Bischof2007a).\nelastix: atoolbox for rigid（刚性） and nonrigid（非刚性） registration of images（配准图像）, http://elastix.isi.uu.nl/ (Klein, Staring, and Pluim 2007).\nDeformable image registration（可变形的配准图像） using discreteoptimization（离散最优化）, http://www.mrf-registration.net/deformable/index.html\n(Glocker, Komodakis, Tziritas et al. 2008).\n第九章：图像缝合\nMicrosoft Research ImageCompositing Editor for stitching images,（图像拼接，图像合成）\nhttp://research.microsoft.com/en-us/um/redmond/groups/ivm/ice/ .\n第十章：计算机摄影学\nHDRShop software for combiningbracketed exposures（包围式曝光） into high-dynamic range radiance images, http://projects.ict.usc.edu/graphics/HDRShop/.\nSuper-resolution（超分辨率） code,\nhttp://www.robots.ox.ac.uk/~vgg/software/SR/  (Pickup 2007;Pickup, Capel,Roberts et al. 2007, 2009).\n第十一章：立体对应\nStereoMatcher, standalone C++stereo matching code,\nhttp://vision.middlebury.edu/stereo/code/  (Scharstein and Szeliski2002).\nPatch-based multi-view stereosoftware (PMVS Version 2),\nhttp://grail.cs.washington.edu/software/pmvs/  (Furukawa and Ponce 2011).\n第十二章：3D重建\nScanalyze: a system foraligning and merging range data,\nhttp://graphics.stanford.edu/software/scanalyze/  (Curless and Levoy 1996).\nMeshLab: software forprocessing, editing, and visualizing unstructured 3D triangular\nmeshes, http://meshlab.sourceforge.net/.\nVRML viewers (various) arealso a good way to visualize texture-mapped 3D models.\n节 12.6.4: Whole body modeling andtracking（全身建模和追踪）\nBayesian 3D person tracking（贝叶斯3D人体追踪）, http://www.cs.brown.edu/~black/code.html  (Sidenbladh,Black, and Fleet2000; Sidenbladh and Black 2003).\nHumanEva: baseline code forthe tracking of articulated human motion,\nhttp://vision.cs.brown.edu/humaneva/   (Sigal, Balan, and Black 2010).\n节 14.1.1: Face detection（人脸检测）\nSample face detection code andevaluation tools,\nhttp://vision.ai.uiuc.edu/mhyang/face-detection-survey.html.\n节 14.1.2: Pedestrian detection（行人追踪）\nA simple object detector withboosting,\nhttp://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html\n(Hastie, Tibshirani, and Friedman 2001;Torralba, Murphy, and Freeman 2007).\nDiscriminatively（有区别） trained deformable（可变形） part models, http://people.cs.uchicago.edu/~pff/latent/  (Felzenszwalb, Girshick,McAllester et al. 2010).\nUpper-body detector（上身检测）,\nhttp://www.robots.ox.ac.uk/~vgg/software/UpperBody/  (Ferrari,Marin-Jimenez, andZisserman 2008).\n2D articulated human poseestimation software,\nhttp://www.vision.ee.ethz.ch/~calvin/articulated_human_pose_estimation_code/  (Eichner and Ferrari 2009).\n节 14.2.2: Active appearance and 3Dshape models\nAAMtools: An active appearancemodeling toolbox,\nhttp://cvsp.cs.ntua.gr/software/AAMtools/  (Papandreou and Maragos2008).\n节 14.3: Instance recognition\nFASTANN and FASTCLUSTER forapproximate k-means (AKM),\nhttp://www.robots.ox.ac.uk/~vgg/software/ (Philbin, Chum, Isard et al. 2007).\nFeature matching using fastapproximate nearest neighbors,\nhttp://people.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN  (Muja and Lowe 2009).\n节 14.4.1: Bag of words(词袋)\nTwo bag of words classiﬁers, http://people.csail.mit.edu/fergus/iccv2005/bagwords.html\n(Fei-Fei and Perona 2005;Sivic, Russell, Efros et al. 2005).\nBag of features andhierarchical（分层） k-means, http://www.vlfeat.org/  (Nist´ er and Stew´enius2006; Nowak, Jurie, and Triggs 2006).\n节 14.4.2: Part-based models\nA simple parts and structureobject detector,\nhttp://people.csail.mit.edu/fergus/iccv2005/partsstructure.html\n(Fischler and Elschlager 1973; Felzenszwalband Huttenlocher 2005).\n节 14.5.1: Machine learning software\nSupport vector machines (SVM)software (\nhttp://www.support-vector-machines.org/SVM soft.html )\n包含很多支持向量机的库,\nSVMlight http://svmlight.joachims.org/ ;\nLIBSVM, http://www.csie.ntu.edu.tw/~cjlin/libsvm/(Fan, Chen,and Lin 2005);\nLIBLINEAR, http://www.csie.ntu.edu.tw/~cjlin/liblinear/  (Fan,Chang, Hsieh et al.2008).\nKernel Machines: links to SVM,Gaussian processes, boosting, and other machine\nlearning algorithms, http://www.kernel-machines.org/software .\nMultiple kernels for imageclassiﬁcation,\nhttp://www.robots.ox.ac.uk/~vgg/software/MKL\n(Varma and Ray 2007; Vedaldi, Gulshan, Varmaet al. 2009).\n附录 A.1–A.2: Matrix decompositions（矩阵分解） and linear least squares（线性最小乘）\nBLAS (BasicLinear Algebra Subprograms基本线性代数子程序),\nhttp://www.netlib.org/blas/  (Blackford,Demmel, Dongarraet al. 2002).\nLAPACK (Linear Algebra（线性代数） PACKage),\nhttp://www.netlib.org/lapack/  (Anderson, Bai,Bischof etal. 1999).\nGotoBLAS, http://www.tacc.utexas.edu/tacc-projects/.\nATLAS (Automatically TunedLinear Algebra Software),\nhttp://math-atlas.sourceforge.net/  (Demmel, Dongarra, Eijkhoutet al. 2005).\nIntel Math Kernel Library(MKL), http://software.intel.com/en-us/intel-mkl/.\nAMD CoreMath Library (ACML),\nhttp://developer.amd.com/cpu/Libraries/acml/Pages/default.aspx .\nRobust PCA code（鲁棒主成分分析）, http://www.salle.url.edu/~ftorre/papers/rpca2.html\n(De la Torre and Black 2003).\nAppendix A.3: Non-linear leastsquares非线性最小二乘\nMINPACK, http://www.netlib.org/minpack/.\nlevmar: Levenberg–Marquardtnonlinear least squares algorithms, 非线性最小二乘\nhttp://www.ics.forth.gr/~lourakis/levmar/  (Madsen, Nielsen, andTingleff 2004).\n附录 A.4–A.5: Direct（直接） and iterative（迭代） sparse matrix（稀疏矩阵） solvers\nSuiteSparse (variousreordering algorithms, 各种各样的重排算法CHOLMOD) and SuiteSparse QR, http://www.cise.ufl.edu/research/sparse/SuiteSparse/  (Davis 2006, 2008).\nPARDISO (iterative and sparsedirect solution),  http://www.pardiso-project.org/.\nTAUCS (sparse direct,iterative, out of core, preconditioners),\nhttp://www.tau.ac.il/~stoledo/taucs/ .\nHSL Mathematical SoftwareLibrary,  http://www.hsl.rl.ac.uk/index.html .\nTemplatesfor the solution of linear systems（线性系统解决问题的模板）, http://www.netlib.org/linalg/html templates/Templates.html  (Barrett, Berry, Chan et al.1994). Download the PDF for instructions（说明） on how to get the software.\nITSOL,MIQR, and other sparsesolvers,\nhttp://www-users.cs.umn.edu/~saad/software/  (Saad 2003).\nILUPACK, http://www-public.tu-bs.de/~bolle/ilupack/ .\n附录 B: Bayesian modeling and inference（贝叶斯建模和推断）\nMiddleburysource code for MRF minimization（隐马尔科夫随机场最小化）, http://vision.middlebury.edu/MRF/code/  (Szeliski, Zabih, Scharsteinet al. 2008).\nC++ code for efﬁcient beliefpropagation for early vision,\nhttp://people.cs.uchicago.edu/~pff/bp/  (Felzenszwalb andHuttenlocher 2006).\nFastPD MRF optimization（最优化） code,\nhttp://www.csd.uoc.gr/~komod/FastPD  (Komodakisand Tziritas2007a; Komodakis, Tziritas, and Paragios 2008)\n算法 C.1   Calgorithm for Gaussian random noise generation, using the Box–Mullertransform.\nC描述的利用Box–Muller 变换产生高斯随机噪声\ndouble urand()\n{\nreturn ((double)rand()) / ((double) RAND MAX);\n}\nvoid grand(double& g1, double& g2)\n{\n#ifndef M_PI\n#define M_PI 3.14159265358979323846\n#endif // M_PI\ndouble n1 = urand();\ndouble n2 = urand();\ndouble x1 = n1 + (n1 == 0); /* guardagainst log(0) */\ndouble sqlogn1 = sqrt(-2.0 * log (x1));\ndouble angl = (2.0 * M PI) * n2;\ng1 = sqlogn1 * cos(angl);\ng2 = sqlogn1 * sin(angl);\n}\n高斯噪声的产生。许多基本的软件包产生一些不同的随机的噪声(例如 运行在unix上的rand())，但是并不是所有的都有高斯随机噪声发生器。计算一个离散随机常量，你可以用Box–Mullertransform (Box and Muller 1958)，他的c代码在算法C.1中给出了，注意这个运行结果是返回一对随机变量。相关的产生高斯随机变量的方由Thomas, Luk, Leong et al. (2007)提出。\n伪彩色产生。在很多应用中，很方便给图像加上标记（或者给图像特征比如线）。一个最简单的方式就是给不同的标记不同的颜色。在我的工作中，我发现用RGB立体色彩系给不同的标记赋予标准均匀的色彩是很方便的。\n对于每一个（非消极）标记值，considerthe bits as being split among the three color channel，例如对于一个比特值为9的值，\n这个值可以被标记为RGBRGBRGB，获得三基色中的每一种颜色值后，颠倒比特值，结果是低位的比特值变化的最快。\n实际上，对于一个八比特的颜色通道，这个比特值的颠倒可以被存在一个表或者一个存储提前计算好的记录有由标记值向伪彩色的改变的完整表。\n图 8.16 显示了这样一个伪彩色绘制的例子.\nGPU实现\nGPU的出现，可以处理像素着色和计算着色，导致了实时应用的快速计算机视觉算法的发展，例如，分割，追踪，立体和运动估计（(Pock, Unger, Cremerset al. 2008; Vineet and Narayanan 2008; Zach,Gallup, and Frahm 2008）。一个好的资源来学习这些算法就是CVPR 2008 上关于Visual Computer Visionon GPUs的workshop。\nhttp://www.cs.unc.edu/~jmf/Workshop_on_Computer_Vision_on_GPU.html他的论文可以在CVPR2008的会议集的DVD中找到。额外的关于GPU算法资源包括GPGPU网址和小组讨论http://gpgpu.org/还有OpenVIDIAWeb site, http://openvidia.sourceforge.net/index.php/OpenVIDIA\nC.3 PPT和讲稿\n正如我在前言中提到的，我希望提供和书中材料相一致的PPT，直到这些全部准备好，你最好的方式去看我在华盛顿大学上课时的PPT，和一写相关课程中用到的教案。\n这里是一些这样的课程列表：\nUW 455:Undergraduate Computer Vision,\nhttp://www.cs.washington.edu/education/courses/455/.\nUW576:Graduate Computer Vision,\nhttp://www.cs.washington.edu/education/courses/576.\nStanfordCS233B: Introduction to Computer Vision,\nhttp://vision.stanford.edu/teaching/cs223b/.\nMIT6.869: Advances in Computer Vision,\nhttp://people.csail.mit.edu/torralba/courses/6.869/6.869.computervision.htm.\nBerkeley CS 280: Computer Vision, http://www.eecs.berkeley.edu/~trevor/CS280.html\nUNC COMP776: Computer Vision, http://www.cs.unc.edu/~lazebnik/spring10.\nMiddlebury CS 453: Computer Vision,\nhttp://www.cs.middlebury.edu/~schar/courses/cs453-s10/.\nRelated courses have also been taught onthe topic of Computational Photography, e.g.,\nCMU 15-463: Computational Photography, http://graphics.cs.cmu.edu/courses/15-463/.\nMIT 6.815/6.865: Advanced ComputationalPhotography,\nhttp://stellar.mit.edu/S/course/6/sp09/6.815\nStanford CS 448A: Computational photographyon cell phones,\nhttp://graphics.stanford.edu/courses/cs448a-10/.\nSIGGRAPH courses on ComputationalPhotography,\nhttp://web.media.mit.edu/~raskar/photo/.\n这里还有一些最好的关于各种计算机视觉主题的在线讲稿，例如：belief propagation and graph cuts，它们在UW-MSR Course of Vision Algo-rithms http://www.cs.washington.edu/education/courses/577/04sp/\nC.4 参考文献：\n这本的所有参考文献在这本书的网站上，一个几乎所有的计算机视觉的出版物都引用的更全面的部分注解书目由Keith Price维http://iris.usc.edu/Vision-Notes/bibliography/contents.html.\n这里还有一个可搜索的计算机图形学的参考书目http://www.siggraph.org/publications/bibliography/另外技术论文比较好的资源是GoogleScholar 和 CiteSeerX。"}
{"content2":"资源1：\n16篇系列：https://www.cnblogs.com/ironstark/category/759418.html\n资源2：\n三维计算机视觉(一)--点云处理综述 - CSDN博客\n三维计算机视觉(二)--点云滤波 - CSDN博客\n三维计算机视觉(三)--点云分割 - CSDN博客\n三维计算机视觉(四)--关键点 - CSDN博客\n三维计算机视觉(五)--特征描述子 - CSDN博客\n三维计算机视觉(六)--3DSC(3D形状上下文特征) - CSDN博客\n三维计算机视觉(七)--Spin image - CSDN博客\n三维计算机视觉(八)--点云配准 - CSDN博客\n资源3：\nCS231A : https://blog.csdn.net/Julialove102123/article/details/79960934"}
{"content2":"本文转自：http://www.innobase.cn/?cat=11\n计算机视觉是一门研究如何使机器“看”的科学，更进一步的说，就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，用电脑处理成为更适合人眼观察或传送给仪器检测的图像。\n作为一个科学学科，计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。这里所 指的信息指Shannon定义的，可以用来帮助做一个“决定”的信息。因为感知可以看作是从感官信号中提 取信息，所以计算机视觉也可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学。\n为了帮助大家更好了解计算机视觉的最新技术进展，移动互联网实验室从互联网上找到一篇关于计算机视觉相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。\n打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）清华大学程明明博士：http://mmcheng.net/ 图像分割、检索\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/"}
{"content2":"应用领域：视频监控、人脸识别、机器视觉、医学图像分析、自动驾驶、机器人、AR、VR。\n主要技术：图像分类，目标检测（识别）、分割、目标追踪、边缘检测、姿势评估、深度学习、超分辨率重建、序列学习、特征检测与匹配、视频标定、图片生成（文本生成图片）、视觉关注性和显著性、人脸识别、3D重建、图像压缩。\n1.分类：\n主要解决问题：我（图片）是谁？ 主要用的是CNN。\n2.目标检测：\n主要解决问题：我（目标）在哪？基本步骤是区域生成，特征提取，分类，位置精修。\n3.图像分割：\n主要解决的问题是：我（像素）属于谁（实体）。主流方法是FCN。\n4.目标跟踪：\n主要解决的问题是：尽快跟上步伐，找到我（目标）。主要方法是相关滤波。\n5.边缘检测：\n主要解决的问题是：如何检测到目标的边缘。\n6.人体姿态识别：\n主要解决的问题是：通过我的姿势知道我在干什么。\n7.超分辨率重建：\n主要解决的问题是：如何从岛国画质变成美帝大片。\n8.序列学习：\n主要解决的问题是：让你（计算机）意淫（预测）下一帧视频画面是什么。\n9.特征检测与匹配：\n主要解决的问题是：检测图像的特征，判断相似度。\n10.图像描述：\n主要解决的问题是：（计算机）看图说话（描述），让计算机说出图像中有什么东西，在干什么。\n11.图片生成\n主要解决的问题是：如何根据给的信息生成对应的图片。\n12.3D重建\n主要解决的问题是：你能通过我给你的图片生成对应的高质量3D点云吗。\n如何以较少的比特有损或者无损的表示原来的图像"}
{"content2":"为什么不去读顶级会议上的论文？适应于机器学习、计算机视觉和人工智能?\n看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？\n我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。\n可以从以下几点说明：\n（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。\n（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。\n（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n(1)\n以下是不完整的列表，但基本覆盖。\n机器学习顶级会议：NIPS, ICML, UAI, AISTATS; （期刊：JMLR, ML, Trends in ML, IEEE T-NN）\n计算机视觉和图像识别：ICCV, CVPR, ECCV; （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）\n人工智能：IJCAI, AAAI; （期刊AI）\n另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。\n特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。\n(2)\n另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/; JMLR(期刊): http://jmlr.csail.mit.edu/papers/; COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。\n(3)\n说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。\n对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。"}
{"content2":"计算机视觉模型学习和原理\n绪论\n什么是计算机视觉\n计算机视觉和机器视觉\n绪论内容继续讲解\n计算机视觉简史\n所自学书籍的结构\n总结\n计算机视觉：模型、学习和原理\n本博文乃自学为主，转载需要声明，并且我也不知道能坚持编写多久，故而尽力为之，最近进行计算机视觉的学习的研究生阶段，时间上委实不多。\n学习的书采用了\n《计算机视觉：模型、学习和推理》\n,这本书的英文名为\n《Computer vision:models, learning and inference》\n,是一本不错的教材，但是可惜的是在习题上有点困难，就是没有提供自学者完整的答案，故而很是可惜。另外，所介绍的数学部分，相对有些难度，而且说明不是很详细和清楚，\n我的目的是通过博客进行学习，学习的过程中，加上一些自己的理解，而且我会适当加和减一些内容，还有就是本书一些配套的习题，想自己试着写写，如果有不对的地方，欢迎提出来让我进行修改。\n绪论\n什么是计算机视觉？\n计算机视觉是一门用计算机模拟生物视觉的学说，简而言之，让计算机代替人眼实现对目标的识别、分类、跟踪和场景理解。\n为此有人提出一个疑虑，就算机器视觉和计算机视觉，两者是否是同一个概念。现在不纠结这个问题，而是说明在《计算机视觉：模型、学习和推理》中，尤其是绪论第一段说到，\nThe goal of computer vision is to extract useful information from images. This has proved a surprisingly challenging task; it has occupied thousands of intelligent and creative minds over the last four decades, and despite this we are still far from being able to build a general-purpose ”seeing machine”.\n我觉得中文翻译得不对，computer vision，应该翻译为计算机视觉而不是机器视觉。所以我觉得应该翻译为，\n计算机视觉旨在从图形中提取有用的消息，已被证实是一个极具挑战性的任务。在过去四十年里，成千上万智慧和创造性的思维致力于这一任务，尽管如此，我们还远远没有能够建立一个通用的“视觉机器”\n计算机视觉和机器视觉\n计算机视觉（computer vision）和机器视觉（machine vision）在很多文献没有区分，但是这个两个术语是有区别又有联系。\n计算机视觉\n是采用图像处理、模式识别、人工智能技术相结合的手段，着重与一幅或多幅图像的计算机分析。图像可以由单个或者多个传感器获取，也可以是单个传感器在不同时刻获取的图像序列。分析是对目标物体的识别，确定目标物体的位置和姿态，对三维景物进行符号描述和解释。在计算机视觉的研究中，经常使用几何模型、复杂的知识表达，采用基于模型的匹配和搜索技术，搜索的策略常使用自底向上、自顶向下、分层和启发式控制策略。\n机器视觉\n偏重于计算机视觉技术工程化，能够自动获取和分析特定图像，以控制相应的行为。\n两者的联系在于\n，计算机视觉为机器视觉提供图像和景物分析的理论及算法基础，机器视觉为计算机视觉的实现提供传感器模型、系统构造和实现手段。\n因此可以认为，一个机器视觉系统就是一个能自动获取一幅或多幅目标物体图像，对所获取图像的各种特征量进行处理、分析和测量，并对测量结果做出定性分析和定量解释，从而得到有关目标物体的某种认识并做出相应决策的系统。机器视觉系统的功能包括:物体定位、特征检测、缺陷判断、目标识别、计数和运动跟踪。\n下面给出个图，是从维基百科搬来的。\n维基认为，计算机视觉的研究对象主要是映射到单幅或多幅图像上的三维场景，例如三维场景的重建。计算机视觉的研究很大程度上针对图像的内容。而机器视觉主要是指工业领域的视觉研究，例如自主机器人的视觉，用于检测和测量的视觉。这表明在这一领域通过软件硬件，图像感知与控制理论往往与图像处理得到紧密结合来实现高效的机器人控制或各种实时操作。\n总结来说，计算机视觉和机器视觉是有区别而且又有联系。两者用于地方不同，有交叉点。所学的书是以计算机视觉为主。\n绪论内容继续讲解\n回归主要内容，前面说过，本书绪论说，我们远远没有能够建立一个通用的“视觉机器”，那么这个原因是什么导致的？是\n可视视觉的复杂性所导致。\n考虑到下面一幅图像（），场景中有数百物体。这些物体没有呈现出“特定”的姿态，几乎所有物体都被部分遮挡。对于一个计算机视觉算法，很难确定某个物体的结束和另一个物体的开始。比如，背景中天空和白色建筑物之间的边界上，图像在亮度上几乎没有变化。然而，即使没有物体的边界或材质的变化，前景中SUV后窗上的亮度也有明显的亮度。\n若非这是一件事情：我们有具体的证据去证明计算机视觉是可研究的，因为我们自己的视觉系统能够毫不费力地处理复杂的图像，如所示，恐怕我们可能已经对开发有用的计算机视觉算法的可能性感到沮丧。如果要求你统计该图像中的树的总数或绘制街道布局的草图，你很容易做到这点。甚至于你可能通过提取微妙的视觉线索，比如人的种族、车和树的种类以及天气等，找出这张照片是在世界上哪个位置拍的。\n：一个视觉场景包含许多物体，而几乎所有物体都是部分遮挡的。红圈所示场景中几乎没有亮度的变化指示天空和建筑之间的边界。绿圈所示区域中有很大的亮度变化而这实际上跟亮度没关系，这里没有物体边界或物体材质的变化。\n可以得出结论，研究计算机视觉并非不可能，只是它具有挑战性。计算机视觉领域取得了长足进步，并在个人消费领域首次大规模部署，比如大多数数码相机已经嵌入人脸检测算法。那么计算机视觉迅速发展的原因，最明显的是计算机的处理能力、内存以及存储能力有巨大的提升。另一个原因是机器学习的广泛应用。机器学习提供许多有用的工具，有助于以新的视角理解已知算法及联系。\n计算机视觉简史\n计算机视觉过去30年主要发展如下图所示（）\n：从左到右依次是：（1）数字图像处理，（2）积木世界，线条标注，（3）广义圆锥，（4）图案结构，（5）立体视觉对应，（6）本征图像，（7）光流，（8）由运动到结构，（9）图像金字塔，（10）尺度空间处理，（11）由阴影，纹理，变焦到形状，（12）基于物理的建模，（13）正则化，（14）马尔科夫随机场（MRF），（15）卡尔曼滤波，（16）3D距离数据处理，（17）投影不变量，（18）因子分解，（19）基于物理的视觉，（20）图割，（21）粒子滤波，（22）基于能量的分割，（23）人脸识别和检测，（24）子空间方法，（25）基于图像的建模和绘制，（26）纹理合成与修图，（27）计算投影学，（28）基于特征的识别，（29）MRF推断算法，（30）类属识别，（31）学习\n1958年加拿大科学家大卫•休伯尔和瑞典科学家托斯坦•维厄瑟尔对猫视觉皮层的研究，提出在计算机模式识别中，和生物识别类似，边缘是用来描述物体形状的最关键信息。1963年，美国计算机科学家拉里•罗伯茨在MIT博士毕业论文《Machine perception of Three-Dimensional Solids》，对输入图像进行梯度操作，进一步提取边缘，然后从3D模型中提取出简单形状结构，然后利用这些结构像搭积木一样去描述场景中无疑的关系，最后获得从另一角度看图像物体的渲染图。这篇论文中，从二维图像恢复图像中物体的三维模型的尝试，正是计算机视觉和传统图像处理学科思想上最大的不同：计算机视觉的目的是让计算机理解图像的内容。这项研究也就成了计算机视觉相关最早的研究。之后MIT人工智能实验室的明斯基发起了“暑期视觉项目”，目的是集中暑假的闲散劳动力解决计算机视觉问题，力争产出模式识别研发的里程碑式的结果。而广为人知的描述中，可以获知，当时明斯基只是让组里的一个学本科生杰拉德•杰伊•萨斯曼将摄像机连接在计算机上，尝试利用暑假的时间让计算机描述它所看到的东西，这个项目当时没有成功，但是计算机视觉作为一个专门的课题出现在了历史上。\n将计算机视觉与已经存在的数字图像处理领域相区别的是期望从图像恢复世界的三维结构并以此为跳板得到完整场景理解。Winston（1975）和Hanson and Riseman（1978）提供了这个早期时代的两本较好的经典论文集。\n场景理解的早期尝试涉及物体即“积木世界”的边缘提取及随后的从2D线条的拓扑结构推断其3D结构（Roberts 1965）。那是提出了一些线条标注算法（Huffman 1971;Clowes 1971;Waltz 1975;Rosenfeld,Hummel,and Zucker 1976;Kanade 1980）。那时边缘检测也是很活跃的研究领域。此外，人们还对非多边形物体的三维建模进行了研究，流行做法是使用了广义椎，即旋转体和封闭曲线扫描体。20世纪70年代一般是关于图像内容建模，如三维模型、立体视觉等。很有代表性的是弹簧模型和广义圆柱体模型。David Marr（戴维•马尔）(1982)《视觉计算理论》（Vision:A computational investigation into the human representation and processing of visual information）总结那个时代的视觉原理工作。他将视觉信息处理分为三个层次：计算理论、表达和算法和硬件实现。在如今看来，或许有些不合理，但是却将计算机视觉作为了一门正式学科的研究。而且其方法论到今天仍然是表达和解决问题的好向导。值得一提的是，1987年成立的ICCV（国际计算机视觉大会）给计算机视觉领域做出重要贡献的人颁发奖项，奖项名字叫马尔奖。\n：计算机视觉算法早期例子：a)线条标注；b)图案结构；c)关节身体模型；d)本征图像；e）立体视觉对应；f)光流\n视觉计算理论提出后，计算机视觉进入20世纪80年代，一个蓬勃发展的年代，提出了主动视觉理论和定性视觉理论等，这些理论认为人类视觉重建过程不是马尔理论那样直接，而是主动的，有目的性和选择性的。这个时期很多研究关注于定量的图像和场景分析的更复杂的数学方法。比如图像金字塔开始广泛应用于完成诸如图像混合这样的任务和由粗到精的对应搜索。使用尺度空间处理的概念也建立起了金字塔的连续版本，到了20世纪80年代后期，小波在一些应用中开始取代或增强规范的图像金字塔。这个时期还提出了Canny边缘检测算法，图像分割和立体视觉，基于人工神经网络的计算机视觉研究尤其是模式识别也在这时候火了起来。首先说说，立体视觉，这时期使用定量的形状线索使用的立体视觉扩展到由X到形状的各种各样的方法，包括由阴影到形状、光度测定学立体视觉、由纹理到形状以及由聚集到形状。其次说边缘和轮廓检测，这时期Canny边缘检测提出，还包括动态演化轮廓跟踪器的引入，比如蛇行。还有基于三维物理量的模型的引入。这时期的研究人员发现，很多立体视觉、流、由X到形状以及边缘检测算法，如果作为变分优化问题来处理，可以用相同的数学框架来统一或至少来描述，且可以使用正则化方法使其更鲁棒（适定的）。与此同时。German(1984)指出这类问题同样可以通过离散马尔科夫随机场模型（MRF）来很好地表达，这样就能使用更好的（全局）搜索和优化算法，比如“模拟退火”。之后出现了卡尔曼滤波来对不确定性进行建模和更新的MRF算法的在线变形。人们也尝试了将正则化及MRF算法映射到并行硬件。这个时期活跃的研究领域还包括了三维距离数据处理（获取、归并、建模和识别）。\n：20世纪80年代计算机视觉算法例子：a)金字塔混合；b）由阴影到形状；c)边缘检测；d)基于物理量的模型；e)基于正则化的表面重建;f)距离数据获取和归并\n20世纪90年代，在识别中使用投影不变量的研究呈现爆发性增长，演变为解决从运行到结构问题的共同努力。最初很多研究是针对投影重建问题的。它不需要摄像机标定的结果。同时，提出了因子分解方法来高效地解决近似正交投影的问题，而后扩展到了透视投影的情况。最终，该领域开始使用完全的全局优化方法，这在后来被认为与摄像测量学中常用的“光束平差法”相同。使用这些方法建立了完全自动的（稀疏）3D建模系统。这个时期，从80年代开始的，使用颜色和亮度的精细测量，并与精确的辐射传输和形成彩色图像的物理模型相结合，构成一个称作“基于物理的视觉”的子领域。同时，光流方法得到了不断的改进，稠密立体视觉对应算法方面也取得了很多进展,其中最大的突破是使用“图割”（graph cut）方法的全局优化算法。另外，产生完整3D表面的多视角立体视觉算法到现在都依旧活跃。从二值的轮廓产生3D体描述的方法仍然还在研究中。\n因为伴随着计算机视觉在交通和医疗等工业领域的应用越来越多，其他一些基础视觉研究方向，比如跟踪算法、图像分割等这个时期有了一定的发展。比如基于跟踪的算法，诸如使用“活动轮廓”方法的轮廓跟踪（例，蛇行、例子滤波器和水平集方法还有基于亮度的（直接）方法，常用于跟踪人脸和整个物体），此外还有重建光滑的遮挡轮廓的研究。\n另外，图像分割，从计算机视觉早期开始一直是重要的方向和活跃的研究话题，90年代产生了基于最小能量的方法和最小描述长度方法，规范化割方法以及均值移位方法。统计学习的方法也是在这个时期流行起来，最初用于人脸识别的主分量本征脸分析和曲线跟踪的线性动态系统。从进入20世纪90年代，伴随着各种机器学习算法的全面开花。机器学习开始成为计算机视觉，尤其是识别、检测和分类等应用中一个不可分割的重要工具。各种识别和检测算法迎来了大发展。尤其是人脸识别在这个时期迎来研究的小高草。各种用描述图像特征的算子不断被发明出来，耳熟能详的SIFT算法也是在20实际90年代末被提出。\n：20世纪90年代计算机视觉算法的例子：a)基于因子分解的从运动到结构；b)稠密立体视觉匹配；c）多视觉重建;d)人脸跟踪；e)图像分割;f)人脸识别\n进入21世纪以后，计算机视觉俨然成为计算机领域一个大学科，国际计算机视觉与模式识别会议（CVPR）和ICCV已经成为人工智能领域甚至是整个计算机领域内的大型盛会。计算机视觉领域最显著的发展是与计算机图形学之间的交互增多了，尤其是基于图像的建模和绘制这个交叉学科领域。直接操作真实世界的影像来创建动画的想法最初是从图像变成方法开始变得显著起来，后来用于视角插值、全景图拼接和全光场绘制。同时也出现了从图像汇集自动创建具有真实感3D模型的基于图像的建模方法。随着计算机视觉和图形学的相互影响加深，最为明显的是，在基于图像绘制下，产生了图像拼接、光场获取和绘制以及通过曝光包围进行的高动态范围图像捕获，这些都被重新命名为计算摄影学。例如，通过曝光包围创建高动态范围图像得到快速采纳，使得色调映射算法发展成为必要，以便将这样的图像转变为可显示的结果。处理归并多个曝光之外，也出现了闪光图像和其对应的非闪光图像归并的方法以及交互地或自动地从交叠的图像中选择不同区域的方法。\n纹理合成、绗缝（quilting）和修图也被划入计算机摄影学的另外一些研究方向，因为它们把输入图像样例重新结合以产生新的照片。\n21世纪以来，显著趋势还有物体识别中基于特征方法（与学习方法相结合），该领域著名的论文有星群模型(Weakly Supervised Scale-Invariant Learning of Models for Visual Recognition,2007)和图案架构(Pictorial Structures for Object Recognition,2005)。基于特征的方法也主导了其他识别任务，例如场景识别和全景图以及位置识别。虽然兴趣点特征趋于主导当前研究，有些研究小组也在从事基于轮廓的识别研究和基于趋于分割的识别研究。\n纹理合成、绗缝（quilting）和修图也被划入计算机摄影学的另外一些研究方向，因为它们把输入图像样例重新结合以产生新的照片。\n21世纪以来，还有一个发展趋势是发展更高效求解全局优化问题的算法。这一趋势始于图割方面的工作，但在消息传递的算法也取得了许多进展，例如，带环的置信转播。\n21世纪以来，还有一个趋势是复杂的机器学习方法在计算机视觉问题中的应用和深度学习在计算机视觉的应用，有意思的是，2010开始举办的大规模视觉识别挑战比赛(ImageNet)，在2012年举办的那一届出现了一个使用神经网络AlexNet的研究生阿厉克斯•克里泽夫斯基，完胜第二名，此后基于深度学习的检测和识别、基于深度学习的图像分割、基于深度学习的立体视觉如雨后春笋般一夜之间全冒出来。\n：21世纪以来的计算机视觉例子：a)基于图像的绘制；b）基于图像的建模;c）交互色调映射；d)纹理合成;e）基于特征的识别;f)基于区域的识别\n所自学书籍的结构\n总结\n本次博文主要是以《计算机视觉：模型、学习和推理》为主，先介绍了计算机视觉的定义，而后介绍计算机视觉的简史，最后描述该书的架构，对计算机视觉有一个完整的认知和技术的发展。最后值得说明的是，本书在之后的概率论部分，有些委实难懂。所以需要一定的基础之外，还需要强大理解能力。"}
{"content2":"作为一个计算机视觉的菜鸟，但是又想在这个领域有所立足的人来说，开始总是很艰难。会有迷茫，会有艰难，还会有犹豫。\n于是我打算开一个博客， 把每天关于计算机视觉的工作和努力，记录下来。作为一个登山者在旅途中的记录。记录自己的心血，\n记录的自己的奋斗，也记录自己的每一步的攀登。\n计算机视觉， 我来了。。。。。。"}
{"content2":"三者之间既有区别，又有联系，不确切的描述：\n计算机图形学≈画图\n计算机视觉≈看图\n数字图像处理≈看图前沐浴更衣焚香做好各种仪式，然后再看图\n计算机图形学（Computer Graphics）讲的是图形，也就是图形的构造方式，是一种从无到有的概念，从数据得到图像。是给定关于景象结构、表面反射特性、光源配置及相机模型的信息，生成图像。\n计算机视觉（Computer Vision）是给定图象，从图象提取信息，包括景象的三维结构，运动检测，识别物体等。\n数字图像处理（Digital Image Processing）是对已有的图像进行变换、分析、重构，得到的仍是图像。\n模式识别（PR）本质就是分类，根据常识或样本或二者结合进行分类，可以对图像进行分类，从图像得到数据。\nComputer Graphics和Computer Vision是同一过程的两个方向。Computer Graphics将抽象的语义信息转化成图像，Computer Vision从图像中提取抽象的语义信息。Image Processing探索的是从一个图像或者一组图像之间的互相转化和关系，与语义信息无关。总之，计算机图形学是计算机视觉的逆问题，两者从最初相互独立的平行发展到最近的融合是一大趋势。图像模式的分类是计算机视觉中的一个重要问题，模式识别中的许多方法可以应用于计算机视觉中。\n区别：\nComputer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb颜色等。输出的是图像，即二维像素数组。\nComputer Vision，简称 CV。输入的是图像或图像序列，通常来自相机或usb摄像头。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。\nDigital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。\n联系：\nCG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。\nCV 更是大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理。\n最后还要提到近年来的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。\n简单点说：1 计算机视觉，里面人工智能的东西更多一些，不仅仅是图像处理的知识，还涵盖了人工智能，机器学习等领域知识；2，计算机图形学，主要涉及图形成像及游戏类开发，如opengl等，还有就是视频渲染等；3，图像处理，这个主要针对图像图像的基本处理，如图像检索或则图像识别，压缩，复原等等操作。\n计算机图形学和数字图像处理是比较老的技术。计算机视觉要迟几十年才提出。\n计算机图形学和数字图像处理的区别在于图形和图像。\n图形是矢量的、纯数字式的。图像常常由来自现实世界的信号产生，有时也包括图形。\n而图像和图形都是数据的简单堆积，图像是像素的叠加，图形则是基本图元的叠加。计算机视觉要从图像中整理出一些信息或统计数据，也就是说要对计算机图像作进一步的分析。计算机图形学的研究成果可以用于产生数字图像处理所需要的素材，计算机视觉需要以数字图像处理作为基础。计算机视觉与数字图像处理的这种关系类似于物理学和数学的关系。\n另外，如果不是浙江大学的或者中科院计算所的，不建议做计算机图形学这一方向，难度太大（图形比图像虽然表面上只高一维，但实际上工作量大了好多倍；其次，图像，国内外差距目前已经很小，好发重要期刊；图形，除上面两个单位和微软外，国内外差距很大，不好发重要期刊）\n数字图像处理主要是对已有的图像，比如说可见光的图像、红外图像、雷达成像进行噪声滤除、边缘检测、图像恢复等处理，就像用ps 处理照片一样的。人脸识别啊、指纹识别啊、运动物体跟踪啊，都属于图像处理。去噪有各种滤波算法；其他的有各种时频变化算法，如傅里叶变化，小波变换等，有很多这方面的书籍。\n图形学主要研究如何生成图形的，像用autoCAD作图，就是图形学中算法的应用。各种动漫软件中图形算法的生成等。"}
{"content2":"理解矩阵（一）\n（真是大学时候不好好学习给自己挖的坑，现在只能一点一点的补回来，这个介绍的矩阵简单易懂，非常牛）\n链接如下：\nhttp://blog.csdn.net/myan/article/details/647511\n（偶然在矩阵文章中发现算法的世外桃源－－《算法》，非常好的一个网站）\n链接如下：\nhttp://lib.csdn.net/base/datastructure\n零基础入门深度学习\n链接如下：\n（这个从模式识别感知器算法，梯度下降讲起，很神奇，深度好文～）\nhttp://blog.csdn.net/TS1130/article/details/53244576\n真正的干货集合（简直神了）\n图像处理、机器视觉、人工智能、机器学习：http://blog.csdn.net/zouxy09/article/details/14222605\nhttp://blog.csdn.net/carson2005/article/details/6601109\nhttp://www.cnblogs.com/tornadomeet/archive/2012/06/24/2560261.html\n神经网络入门：http://blog.csdn.net/zzwu/article/details/574931\n游戏开发：http://blog.csdn.net/leonwei?viewmode=contents"}
{"content2":"人工智能的本质：让计算机能够像生物体一样，具有思考和决定的能力来执行某些特定操作。\n人工智能研究的三个主大类领域：\n①　计算机视觉：\n从视觉输入源获取信息并对它们进行分析，以执行特定操作。（ex:脸部识别、对象识别、光学字符识别）。\n②　自然语言处理：\n让机器能够像我们平常那样阅读和理解语言的能力。通过网络上大量可获取的数据集合，研究人员对语言进行自动分析。（ex:人工小冰、微软Cortana）。\n③　常识推理：\n对一些常识问题进行推理。比如常识性知识我们人类可以通过理解某些问题再结合混合上下文、背景知识和语言能力让问题相互影响、相互作用得到答案，但是机器不行，而常识推理就是研究人员根据各种数据的处理并使机器综合分析然后推断答案的过程。\n游戏中为什么要有人工智能？完善提高游戏的品质。\n什么游戏有吸引力？具有挑战性的游戏一定好玩。\n意味着：一个游戏不应该太过困难，让玩家没有击败对手的可能，也不应该让玩家轻而易举取得胜利。难度合适\n所以，我们制作人工智能的目的不是复制人类或者其他动物的整个思维过程，而是通过让NPC对游戏世界不断变化的情形，产生对玩家来说足够合理、有意义的反应，让他们看起来更加智能。\n游戏中的人工智能：\n①　有限状态机\n状态机：一组数量有限的可以相互转换的状态。\n组成：状态（ex：站立、跳跃、奔跑、巡逻、攻击等）\n转移：改变状态\n规则：触发状态转移\n事件：触发检查规则\n通过if else和switch就可以轻松的实现状态机\n②　人工智能中的随机性和概率\n意义：防止出现完美或者愚蠢的对手降低游戏的乐趣\n随机的情况：\n1、非故意：做出何种决定无关紧要，简单的做个随机决定即可\n2、故意：防止完美或愚蠢的人工智能，所以用随机性增加可玩率\n③　感应器设定\n作用：了解其周围的环境，以及与它们互相影响的游戏世界，以便做出某个特殊的决定。\n1、轮寻：用if/else或switch在简单的人工智能角色的FixedUpdate方法中进行检查，然后采取行动。（适用于没有太多事情需要检查的角色）\n2、消息系统：决策响应世界中的事件。（对世界中的物体发出的所有信息都进行接收与检测，使监听者响应需要发生的事件）\n一般游戏中的事件接收都是轮寻和消息系统一起使用的。\n④　群组、蜂拥和羊群效应\n作用：实现整个组群的复杂的智能全局行为。\n方法：为每个个体单位设定三个规则\n分离：保持群体里每个个体的最短距离\n队列：保持每个个体与周围个体的平均方向一致\n凝聚：保证个体质量与组群的质量中心最短距离\n⑤　路径的跟随与引导\n作用：使人工智能角色跟随一个粗略的引导路径进行跟随（适用于赛车的人工智能等）\n包涵三个层次：\n1、行动选择：策略，目标，计划\n2、行为控制：路径确定\n3、动作：关节动画\n⑥　A*寻路算法\n作用：使跟随玩家的角色或者怪物避开障碍到达指定地点\n方法：把地图划分为N个网格，在网格中避开障碍物计算最短路径\n⑦　导航网格\n作用：减少计算成本而使用的网格导航\n方法：选出航点，并使人工智能角色经过航点到达目标点\n缺点：更新障碍物后必须重新更新航点\n⑧　行为树\n作用：把角色的所有活动划分为不同的任务，然后根据以下四个组件来使用\n1、选择节点任务：去判断是否执行某任务，执行成功，任务完成并返回到父节点，任务失败则尝试兄弟节点的任务\n2、顺序节点任务：从第一个节点任务开始，逐个执行，如果任务完成，继续执行下一个任务，失败返回上一级执行下一个任务\n3、并行节点任务：同时执行所有的子任务\n4、装饰节点任务：它只有一个子任务，但是可以改变其子任务的行为，比如是否运行这个子任务或者这个任务应该运行多少次等等。\n⑨　运动系统\n作用：使用动物的骨骼、肌肉、关节和其他组织做出各种姿势完成各种事。\nUnity自带的有一个运动扩展系统，这里不多做介绍。\n⑩　Dijkstra算法（最短路径算法）\n以其设计者的名字命名，是在不含权值为负的边的图中求最短路径的最著名的算法之一。\n作用：从一个开始点找到目标节点的最短路径。但是，当这个算法找到最短路径后，却会继续运行寻找其他的路径，这会造成资源的浪费，所以在大多数游戏中都不是特别有效。\n在人工智能需要对地图进行战术决策时，Dijkstra算法仍是一个重要算法。在其他领域也有很广泛的应用。\n总结：\n学术领域的人工智能能尝试解决真实世界中的问题，并需要在不消耗过多有限资源的情况下证明某个理论。\n游戏领域的人工智能致力于在资源有限的条件下，构建对于玩家来说看上去很智能的NPC。游戏人工智能的目标就是提供一个挑战性的对手，让游戏玩起来更加的有趣。"}
{"content2":"人工智能是人类一个非常美好的梦想，跟星际漫游和长生不老一样。我们想制造出一种机器，使得它跟人一样具有一定的对外界事物感知能力，比如看见世界。\n在上世纪50年代，数学家图灵提出判断机器是否具有人工智能的标准：图灵测试。即把机器放在一个房间，人类测试员在另一个房间，人跟机器聊天，测试员事先不知道另一房间里是人还是机器 。经过聊天，如果测试员不能确定跟他聊天的是人还是机器的话，那么图灵测试就通过了，也就是说这个机器具有与人一样的感知能力。\n但是从图灵测试提出来开始到本世纪初，50多年时间有无数科学家提出很多机器学习的算法，试图让计算机具有与人一样的智力水平，但直到2006年深度学习算法的成功，才带来了一丝解决的希望。\n众星捧月的深度学习\n深度学习在很多学术领域，比非深度学习算法往往有20-30%成绩的提高。很多大公司也逐渐开始出手投资这种算法，并成立自己的深度学习团队，其中投入最大的就是谷歌，2008年6月披露了谷歌脑项目。2014年1月谷歌收购DeepMind，然后2016年3月其开发的Alphago算法在围棋挑战赛中，战胜了韩国九段棋手李世石，证明深度学习设计出的算法可以战胜这个世界上最强的选手。\n在硬件方面，Nvidia最开始做显示芯片，但从2006及2007年开始主推用GPU芯片进行通用计算，它特别适合深度学习中大量简单重复的计算量。目前很多人选择Nvidia的CUDA工具包进行深度学习软件的开发。\n微软从2012年开始，利用深度学习进行机器翻译和中文语音合成工作，其人工智能小娜背后就是一套自然语言处理和语音识别的数据算法。\n百度在2013年宣布成立百度研究院，其中最重要的就是百度深度学习研究所，当时招募了著名科学家余凯博士。不过后来余凯离开百度，创立了另一家从事深度学习算法开发的公司地平线。\nFacebook和Twitter也都各自进行了深度学习研究，其中前者携手纽约大学教授Yann Lecun，建立了自己的深度学习算法实验室；2015年10月，Facebook宣布开源其深度学习算法框架，即Torch框架。Twitter在2014年7月收购了Madbits，为用户提供高精度的图像检索服务。\n前深度学习时代的计算机视觉\n互联网巨头看重深度学习当然不是为了学术，主要是它能带来巨大的市场。那为什么在深度学习出来之前，传统算法为什么没有达到深度学习的精度？\n在深度学习算法出来之前，对于视觉算法来说，大致可以分为以下5个步骤：特征感知，图像预处理，特征提取，特征筛选，推理预测与识别。早期的机器学习中，占优势的统计机器学习群体中，对特征是不大关心的。\n我认为，计算机视觉可以说是机器学习在视觉领域的应用，所以计算机视觉在采用这些机器学习方法的时候，不得不自己设计前面4个部分。\n但对任何人来说这都是一个比较难的任务。传统的计算机识别方法把特征提取和分类器设计分开来做，然后在应用时再合在一起，比如如果输入是一个摩托车图像的话，首先要有一个特征表达或者特征提取的过程，然后把表达出来的特征放到学习算法中进行分类的学习。\n过去20年中出现了不少优秀的特征算子，比如最著名的SIFT算子，即所谓的对尺度旋转保持不变的算子。它被广泛地应用在图像比对，特别是所谓的structure from motion这些应用中，有一些成功的应用例子。另一个是HoG算子，它可以提取物体，比较鲁棒的物体边缘，在物体检测中扮演着重要的角色。\n这些算子还包括Textons，Spin image，RIFT和GLOH，都是在深度学习诞生之前或者深度学习真正的流行起来之前，占领视觉算法的主流。\n几个（半）成功例子\n这些特征和一些特定的分类器组合取得了一些成功或半成功的例子，基本达到了商业化的要求但还没有完全商业化。\n一是八九十年代的指纹识别算法，它已经非常成熟，一般是在指纹的图案上面去寻找一些关键点，寻找具有特殊几何特征的点，然后把两个指纹的关键点进行比对，判断是否匹配。\n然后是2001年基于Haar的人脸检测算法，在当时的硬件条件下已经能够达到实时人脸检测，我们现在所有手机相机里的人脸检测，都是基于它或者它的变种。\n第三个是基于HoG特征的物体检测，它和所对应的SVM分类器组合起来的就是著名的DPM算法。DPM算法在物体检测上超过了所有的算法，取得了比较不错的成绩。\n但这种成功例子太少了，因为手工设计特征需要大量的经验，需要你对这个领域和数据特别了解，然后设计出来特征还需要大量的调试工作。说白了就是需要一点运气。\n另一个难点在于，你不只需要手工设计特征，还要在此基础上有一个比较合适的分类器算法。同时设计特征然后选择一个分类器，这两者合并达到最优的效果，几乎是不可能完成的任务。\n仿生学角度看深度学习\n如果不手动设计特征，不挑选分类器，有没有别的方案呢？能不能同时学习特征和分类器？即输入某一个模型的时候，输入只是图片，输出就是它自己的标签。比如输入一个明星的头像，出来的标签就是一个50维的向量（如果要在50个人里识别的话），其中对应明星的向量是1，其他的位置是0。\n这种设定符合人类脑科学的研究成果。\n1981年诺贝尔医学生理学奖颁发给了David Hubel，一位神经生物学家。他的主要研究成果是发现了视觉系统信息处理机制，证明大脑的可视皮层是分级的。他的贡献主要有两个，一是他认为人的视觉功能一个是抽象，一个是迭代。抽象就是把非常具体的形象的元素，即原始的光线像素等信息，抽象出来形成有意义的概念。这些有意义的概念又会往上迭代，变成更加抽象，人可以感知到的抽象概念。\n像素是没有抽象意义的，但人脑可以把这些像素连接成边缘，边缘相对像素来说就变成了比较抽象的概念；边缘进而形成球形，球形然后到气球，又是一个抽象的过程，大脑最终就知道看到的是一个气球。\n模拟人脑识别人脸，也是抽象迭代的过程，从最开始的像素到第二层的边缘，再到人脸的部分，然后到整张人脸，是一个抽象迭代的过程。\n再比如看到图片中的摩托车，我们可能在脑子里就几微秒的时间，但是经过了大量的神经元抽象迭代。对计算机来说最开始看到的根本也不是摩托车，而是RGB图像三个通道上不同的数字。\n所谓的特征或者视觉特征，就是把这些数值给综合起来用统计或非统计的形式，把摩托车的部件或者整辆摩托车表现出来。深度学习的流行之前，大部分的设计图像特征就是基于此，即把一个区域内的像素级别的信息综合表现出来，利于后面的分类学习。\n如果要完全模拟人脑，我们也要模拟抽象和递归迭代的过程，把信息从最细琐的像素级别，抽象到“种类”的概念，让人能够接受。\n卷积的概念\n计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。\n什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。实际上在计算机视觉里面，可以把卷积当做一个抽象的过程，就是把小区域内的信息统计抽象出来。\n比如，对于一张爱因斯坦的照片，我可以学习n个不同的卷积和函数，然后对这个区域进行统计。可以用不同的方法统计，比如着重统计中央，也可以着重统计周围，这就导致统计的和函数的种类多种多样，为了达到可以同时学习多个统计的累积和。\n上图中是，如何从输入图像怎么到最后的卷积，生成的响应map。首先用学习好的卷积和对图像进行扫描，然后每一个卷积和会生成一个扫描的响应图，我们叫response map，或者叫feature map。如果有多个卷积和，就有多个feature map。也就说从一个最开始的输入图像（RGB三个通道）可以得到256个通道的feature map，因为有256个卷积和，每个卷积和代表一种统计抽象的方式。\n在卷积神经网络中，除了卷积层，还有一种叫池化的操作。池化操作在统计上的概念更明确，就是一个对一个小区域内求平均值或者求最大值的统计操作。\n带来的结果是，如果之前我输入有两个通道的，或者256通道的卷积的响应feature map，每一个feature map都经过一个求最大的一个池化层，会得到一个比原来feature map更小的256的feature map。\n在上面这个例子里，池化层对每一个2X2的区域求最大值，然后把最大值赋给生成的feature map的对应位置。如果输入图像是100×100的话，那输出图像就会变成50×50，feature map变成了一半。同时保留的信息是原来2X2区域里面最大的信息。\n操作的实例：LeNet网络\nLe顾名思义就是指人工智能领域的大牛Lecun。这个网络是深度学习网络的最初原型，因为之前的网络都比较浅，它是比较深的。LeNet在98年就发明出来了，当时Lecun在AT&T的实验室，他用这一网络进行字母识别，达到了非常好的效果。\n怎么构成呢？输入图像是32×32的灰度图，第一层经过了一组卷积和，生成了6个28X28的feature map，然后经过一个池化层，得到得到6个14X14的feature map，然后再经过一个卷积层，生成了16个10X10的卷积层，再经过池化层生成16个5×5的feature map。\n从最后16个5X5的feature map开始，经过了3个全连接层，达到最后的输出，输出就是标签空间的输出。由于设计的是只要对0到9进行识别，所以输出空间是10，如果要对10个数字再加上26个大小字母进行识别的话，输出空间就是62（字母的大小写26*2+10）。62维向量里，如果某一个维度上的值最大，它对应的那个字母和数字就是就是预测结果。\n压在骆驼身上的最后一根稻草\n从98年到本世纪初，深度学习兴盛起来用了15年，但当时成果泛善可陈，一度被边缘化。到2012年，深度学习算法在部分领域取得不错的成绩，而压在骆驼身上最后一根稻草就是AlexNet。\nAlexNet由多伦多大学几个科学家开发，在ImageNet比赛上做到了非常好的效果。当时AlexNet识别效果超过了所有浅层的方法。此后，大家认识到深度学习的时代终于来了，并有人用它做其它的应用，同时也有些人开始开发新的网络结构。\n其实AlexNet的结构也很简单，只是LeNet的放大版。输入是一个224X224的图片，是经过了若干个卷积层，若干个池化层，最后连接了两个全连接层，达到了最后的标签空间。\n去年，有些人研究出来怎么样可视化深度学习出来的特征。那么，AlexNet学习出的特征是什么样子？在第一层，都是一些填充的块状物和边界等特征；中间的层开始学习一些纹理特征；更高接近分类器的层级，则可以明显看到的物体形状的特征。\n最后的一层，即分类层，完全是物体的不同的姿态，根据不同的物体展现出不同姿态的特征了。\n可以说，不论是对人脸，车辆，大象或椅子进行识别，最开始学到的东西都是边缘，继而就是物体的部分，然后在更高层层级才能抽象到物体的整体。整个卷积神经网络在模拟人的抽象和迭代的过程。\n为什么时隔20年卷土重来？\n我们不禁要问：似乎卷积神经网络设计也不是很复杂，98年就已经有一个比较像样的雏形了。自由换算法和理论证明也没有太多进展。那为什么时隔20年，卷积神经网络才能卷土重来，占领主流？\n这一问题与卷积神经网络本身的技术关系不太大，我个人认为与其他一些客观因素有关。\n首先，卷积神经网络的深度太浅的话，识别能力往往不如一般的浅层模型，比如SVM或者boosting。但如果做得很深，就需要大量数据进行训练，否则机器学习中的过拟合将不可避免。而2006及2007年开始，正好是互联网开始大量产生各种各样的图片数据的时候。\n另外一个条件是运算能力。卷积神经网络对计算机的运算要求比较高，需要大量重复可并行化的计算，在当时CPU只有单核且运算能力比较低的情况下，不可能进行个很深的卷积神经网络的训练。随着GPU计算能力的增长，卷积神经网络结合大数据的训练才成为可能。\n最后一点就是人和。卷积神经网络有一批一直在坚持的科学家（如Lecun）才没有被沉默，才没有被海量的浅层方法淹没。然后最后终于看到卷积神经网络占领主流的曙光。\n深度学习在视觉上的应用\n计算机视觉中比较成功的深度学习的应用，包括人脸识别，图像问答，物体检测，物体跟踪。\n人脸识别\n这里说人脸识别中的人脸比对，即得到一张人脸，与数据库里的人脸进行比对；或同时给两张人脸，判断是不是同一个人。\n这方面比较超前的是汤晓鸥教授，他们提出的DeepID算法在LWF上做得比较好。他们也是用卷积神经网络，但在做比对时，两张人脸分别提取了不同位置特征，然后再进行互相比对，得到最后的比对结果。最新的DeepID-3算法，在LWF达到了99.53%准确度，与肉眼识别结果相差无几。\n图片问答问题\n这是2014年左右兴起的课题，即给张图片同时问个问题，然后让计算机回答。比如有一个办公室靠海的图片，然后问“桌子后面有什么”，神经网络输出应该是“椅子和窗户”。\n这一应用引入了LSTM网络，这是一个专门设计出来具有一定记忆能力的神经单元。特点是，会把某一个时刻的输出当作下一个时刻的输入。可以认为它比较适合语言等，有时间序列关系的场景。因为我们在读一篇文章和句子的时候，对句子后面的理解是基于前面对词语的记忆。\n图像问答问题是基于卷积神经网络和LSTM单元的结合，来实现图像问答。LSTM输出就应该是想要的答案，而输入的就是上一个时刻的输入，以及图像的特征，及问句的每个词语。\n物体检测问题\nRegion CNN\n深度学习在物体检测方面也取得了非常好的成果。2014年的Region CNN算法，基本思想是首先用一个非深度的方法，在图像中提取可能是物体的图形块，然后深度学习算法根据这些图像块，判断属性和一个具体物体的位置。\n为什么要用非深度的方法先提取可能的图像块？因为在做物体检测的时候，如果你用扫描窗的方法进行物体监测，要考虑到扫描窗大小的不一样，长宽比和位置不一样，如果每一个图像块都要过一遍深度网络的话，这种时间是你无法接受的。\n所以用了一个折中的方法，叫Selective Search。先把完全不可能是物体的图像块去除，只剩2000左右的图像块放到深度网络里面判断。那么取得的成绩是AP是58.5，比以往几乎翻了一倍。有一点不尽如人意的是，region CNN的速度非常慢，需要10到45秒处理一张图片。\nFaster R-CNN方法\n而且我在去年NIPS上，我们看到的有Faster R-CNN方法，一个超级加速版R-CNN方法。它的速度达到了每秒七帧，即一秒钟可以处理七张图片。技巧在于，不是用图像块来判断是物体还是背景，而把整张图像一起扔进深度网络里，让深度网络自行判断哪里有物体，物体的方块在哪里，种类是什么？\n经过深度网络运算的次数从原来的2000次降到一次，速度大大提高了。\nFaster R-CNN提出了让深度学习自己生成可能的物体块，再用同样深度网络来判断物体块是否是背景？同时进行分类，还要把边界和给估计出来。\nFaster R-CNN可以做到又快又好，在VOC2007上检测AP达到73.2，速度也提高了两三百倍。\nYOLO\n去年FACEBOOK提出来的YOLO网络，也是进行物体检测，最快达到每秒钟155帧，达到了完全实时。它让一整张图像进入到神经网络，让神经网络自己判断这物体可能在哪里，可能是什么。但它缩减了可能图像块的个数，从原来Faster R-CNN的2000多个缩减缩减到了98个。\n同时取消了Faster R-CNN里面的RPN结构，代替Selective Search结构。YOLO里面没有RPN这一步，而是直接预测物体的种类和位置。\nYOLO的代价就是精度下降，在155帧的速度下精度只有52.7，45帧每秒时的精度是63.4。\nSSD\n在arXiv上出现的最新算法叫Single Shot MultiBox Detector，即SSD。\n它是YOLO的超级改进版，吸取了YOLO的精度下降的教训，同时保留速度快的特点。它能达到58帧每秒，精度有72.1。速度超过Faster R-CNN 有8倍，但达到类似的精度。\n物体跟踪\n所谓跟踪，就是在视频里面第一帧时锁定感兴趣的物体，让计算机跟着走，不管怎么旋转晃动，甚至躲在树丛后面也要跟踪。\n深度学习对跟踪问题有很显著的效果。DeepTrack算法是我在澳大利亚信息科技研究院时和同事提出的，是第一在线用深度学习进行跟踪的文章，当时超过了其它所有的浅层算法。\n今年有越来越多深度学习跟踪算法提出。去年十二月ICCV 2015上面，马超提出的Hierarchical Convolutional Feature算法，在数据上达到最新的记录。它不是在线更新一个深度学习网络，而是用一个大网络进行预训练，然后让大网络知道什么是物体什么不是物体。\n将大网络放在跟踪视频上面，然后再分析网络在视频上产生的不同特征，用比较成熟的浅层跟踪算法来进行跟踪，这样利用了深度学习特征学习比较好的好处，同时又利用了浅层方法速度较快的优点。效果是每秒钟10帧，同时精度破了记录。\n最新的跟踪成果是基于Hierarchical Convolutional Feature，由一个韩国的科研组提出的MDnet。它集合了前面两种深度算法的集大成，首先离线的时候有学习，学习的不是一般的物体检测，也不是ImageNet，学习的是跟踪视频，然后在学习视频结束后，在真正在使用网络的时候更新网络的一部分。这样既在离线的时候得到了大量的训练，在线的时候又能够很灵活改变自己的网络。\n基于嵌入式系统的深度学习\n回到ADAS问题（慧眼科技的主业），它完全可以用深度学习算法，但对硬件平台有比较高的要求。在汽车上不太可能把一台电脑放上去，因为功率是个问题，很难被市场所接受。\n现在的深度学习计算主要是在云端进行，前端拍摄照片，传给后端的云平台处理。但对于ADAS而言，无法接受长时间的数据传输的，或许发生事故后，云端的数据还没传回来。\n那是否可以考虑NVIDIA推出的嵌入式平台？NVIDIA推出的嵌入式平台，其运算能力远远强过了所有主流的嵌入式平台，运算能力接近主流的顶级CPU，如台式机的i7。那么慧眼科技在做工作就是要使得深度学习算法，在嵌入式平台有限的资源情况下能够达到实时效果，而且精度几乎没有减少。\n具体做法是，首先对网络进行缩减，可能是对网络的结构缩减，由于识别场景不同，也要进行相应的功能性缩减；另外要用最快的深度检测算法，结合最快的深度跟踪算法，同时自己研发出一些场景分析算法。三者结合在一起，目的是减少运算量，减少检测空间的大小。在这种情况下，在有限资源上实现了使用深度学习算法，但精度减少的非常少。"}
{"content2":"计算机视觉、图像处理、计算机图形学以及机器视觉名字上看起来很接近，其具体内容却又比较大的不同，之前从知乎上看到了一个介绍各个之间差异的图片，感觉很受启发，所以在这里记录一下。\n作为一个英语渣渣，只好借助有道翻译成了中文，大神勿喷"}
{"content2":"感知=触发器+感知器+事件管理器\n1、视觉感知：\n视锥体是模拟视觉地基本方法，以眼睛为中心，一定锥角范围内为检测区域\n除了判断物体是否在视锥体范围内外，还要进行视线测试（LOS），才能确定最终结果\n原则上以玩家为中心，限定AI角色的智能。\n对于AI角色看到需要做出响应的物体，添加Trigger派生类SightTrigger作为触发器\nusing System.Collections; using System.Collections.Generic; using UnityEngine; public class SightTrigger : Trigger { public override void Try(Sensor sensor) { //如果感知器能感觉到这个触发器，那么向感知器发出通知，感知体做出相应的决策或行动 if (isTouchingTrigger(sensor)) { sensor.Notify(this); } } //判断感知器是否能感知到这个触发器 protected override bool isTouchingTrigger(Sensor sensor) { GameObject g = sensor.gameObject; //如果这个感知器能够感知视觉信息 if (sensor.sensorType == Sensor.SensorType.sight) { RaycastHit hit; Vector3 rayDirection = transform.position - g.transform.position; rayDirection.y = 0; //判断感知体的向前方向与物体所在方向的夹角，是否在视域范围内 if((Vector3.Angle(rayDirection,g.transform.forward))<(sensor as SightSensor).fieldOfView) { //在视线距离内是否存在其他障碍物遮挡，如果没有障碍物，则返回true; if (Physics.Raycast(g.transform.position+new Vector3(0,1,0),rayDirection,out hit, (sensor as SightSensor).viewDistance)) { if (hit.collider.gameObject == this.gameObject) { return true; } } } } return false; } //更新触发器的内部信息，由于带有视觉触发器的AI角色可能是运动的，因此要不停的更新这个触发器的位置 public override void Updateme() { position = transform.position; } // Use this for initialization void Start () { //调用基类的Start()函数 base.Start(); //像管理器注册这个触发器，管理器会把它加入到当前触发器列表中 manager.RegisterTrigger(this); } // Update is called once per frame void Update () { } }\nusing System.Collections; using System.Collections.Generic; using UnityEngine; public class SightSensor : Sensor { //定义这个AI角色的视域范围 public float fieldOfView = 45; //定义这个AI角色最远能看到的距离 public float viewDistance = 100.0f; // Use this for initialization void Start () { sensorType = SensorType.sight; manager.RegisterSensor(this); } // Update is called once per frame void Update () { } public override void Notify(Trigger t) { //当感知器能够真正感觉到某个触发器的信息时被调用，产生相应的行为或做出某些决策 //此处行为：打印相关信息，感知体和触发器之间画一条红色连线 print(\"I see a \" + t.gameObject.name + \"!\"); Debug.DrawLine(transform.position, t.transform.position, Color.red); } private void OnDrawGizmos() { Vector3 frontRaypoint = transform.position + (transform.forward * viewDistance); float fieldOfViewinRadians = fieldOfView * 3.14f / 180.0f; Vector3 leftRaypoint = transform.TransformPoint(new Vector3(viewDistance * Mathf.Sin(fieldOfViewinRadians), 0, viewDistance * Mathf.Cos(fieldOfViewinRadians))); Vector3 rightRayPoint = transform.TransformPoint(new Vector3(-viewDistance * Mathf.Sin(fieldOfViewinRadians), 0, viewDistance * Mathf.Cos(fieldOfViewinRadians))); // } }"}
{"content2":"关键点又称为感兴趣的点，是低层次视觉通往高层次视觉的捷径，抑或是高层次感知对低层次处理手段的妥协。\nRangeImage\n1.关键点，线，面\n关键点=特征点；\n关键线=边缘；\n关键面=foreground；\n上述三个概念在信息学中几乎占据了统治地位。比如1维的函数（信号），有各种手段去得到某个所谓的关键点，有极值点，拐点...二维的图像，特征点提取算法是标定算法的核心（harris),边缘提取算法更是备受瞩目（canny,LOG.....)，当然，对二维的图像也有区域所谓的前景分割算法用于提取感兴趣的区域，但那属于较高层次的视觉，本文不讨论。 由此可以推断，三维视觉应该同时具备：关键点，关键线，关键面三种算法。本质上，关键面算法就是我们之前一文中讨论的分割算法（三维点云不是实心的）。\nok,在这里我们了解到了，要在n维信息中提取n-1维信息是简单的，但n-2维信息会比n-1维要不稳定或者复杂的多。很容易想象，图像的边缘处理算法所得到的结果一般大同小异，但关键点提取算法的结果可以是千差万别的。主要原因是降维过大后，特征的定义很模糊，很难描述清楚对一幅图像来说，到底怎样的点才是关键点。所以，对3维点云来说，关键点的描述就更难了。点云也有1维边缘检测算法，本文不做讨论。单说说关键点提取。\n2.来自点云的降维打击\n图像的Harris角点算子将图像的关键点定义为角点。角点也就是物体边缘的交点，harris算子利用角点在两个方向的灰度协方差矩阵响应都很大，来定义角点。既然关键点在二维图像中已经被成功定义且使用了，看来在三维点云中可以沿用二维图像的定义...不过今天要讲的是另外一种思路，简单粗暴，直接把三维的点云投射成二维的图像不就好了。这种投射方法叫做range_image.\n首先放上一张range_imge和点云图像的合照：\n看起来像个眼睛的那玩意就是range_image. 至于它为什么像个眼睛，就要从它的出生开始说起了。三维点云有多种采集方式，最为著名的是结构光，飞秒相机，双目视觉。简而言之，采集都离不开相机。用相机拍照当然就存在相机的光心坐标原点 Oc 以及主光轴方向 Z. 从这个点，有一种办法可以将三维数据映射到2维平面上。首先，将某点到光心Oc的距离映射成深度图的灰度或颜色（灰度只有256级但颜色却可接近连续变化）。除此之外，再定义一下怎样将点云映射到图像的横纵坐标上就可以了。\n任意一点都要和光心进行连线.....这么听起来很熟悉....好像有点像球坐标的意思。球坐标长下面这张图这样。在数学里，球坐标系（英语：Spherical coordinate system）是一种利用球坐标（r, θ，φ）表示一个点 p 在三维空间的位置的三维正交坐标系。右图显示了球坐标的几何意义：原点与点 P 之间的径向距离 r ，原点到点 P 的连线与正 z-轴之间的天顶角\nθ\n以及原点到点 P 的连线，在 xy-平面的投影线，与正 x-轴之间的方位角φ\n。\n深度图中的横，纵坐标实际上是θ和φ，如果要保证沿着场景中某条直线移动，φ线性变化θ却先增大后减小。这也就造成了深度图像一个眼睛一样。但这并不妨碍什么，φ没有定义的地方可以使用深度无限大来代替。\n将点云转成深度图，只需要确定一个直角坐标系，角分辨率，θ范围，φ范围即可。毕竟这只是一个直角坐标转球坐标的工作而已。\n这样做显然是有好处的，首先，这是一种除了八叉树，kd_tree之外，能够将点云的空间关系表达出来的手段。每个点云都有了横，纵，深，三个坐标，并且这种坐标原点的设定方式，在理论上是不会存在干涉的（从原点出发的一条线理论上不会遇到多余1个点）。于是点云的空间关系就自然的被编码与深度图中。\n显然，图像中的关键点检测算子就可以被移植到点云特征点求取中来了。\n3.基于PCL的点云-深度图转换\n//rangeImage也是PCL的基本数据结构 pcl::RangeImage rangeImage; //角分辨率 float angularResolution = (float) ( 1.0f * (M_PI/180.0f)); // 1.0 degree in radians //phi可以取360° float maxAngleWidth = (float) (360.0f * (M_PI/180.0f)); // 360.0 degree in radians //a取180° float maxAngleHeight = (float) (180.0f * (M_PI/180.0f)); // 180.0 degree in radians //半圆扫一圈就是整个图像了 //传感器朝向 Eigen::Affine3f sensorPose = (Eigen::Affine3f)Eigen::Translation3f(0.0f, 0.0f, 0.0f); //除了三维相机模式还可以选结构光模式 pcl::RangeImage::CoordinateFrame coordinate_frame = pcl::RangeImage::CAMERA_FRAME; //noise level表示的是容差率，因为1°X1°的空间内很可能不止一个点，noise level = 0则表示去最近点的距离作为像素值，如果=0.05则表示在最近点及其后5cm范围内求个平均距离 float noiseLevel=0.00; //minRange表示深度最小值，如果=0则表示取1°X1°的空间内最远点，近的都忽略 float minRange = 0.0f; //bordersieze表示图像周边点 int borderSize = 1; //基本数据结构直接打印是ok的 std::cout << rangeImage << \"\\n\";\nNARF描述子\n关键点检测本质上来说，并不是一个独立的部分，它往往和特征描述联系在一起，再将特征描述和识别、寻物联系在一起。关键点检测可以说是通往高层次视觉的重要基础。但本章节仅在低层次视觉上讨论点云处理问题，故所有讨论都在关键点检测上点到为止。NARF 算法实际上可以分成两个部分，第一个部分是关键点提取，第二个部分是关键点信息描述，本文仅涉及第一个部分。\n在文章开始之前，有非常重要的一点要说明，点云中任意一点，都有一定概率作为关键点。关键点也是来自原始点云中的一个元素。和图像的边缘提取或者关键点检测算法追求n次插值，最终求的亚像素坐标不同，点云的关键点只在乎找到那个点。\n1. 边缘提取\n首先声明本文所有思想算法公式均来自：Point Feature Extraction on 3D Range Scans Taking into Account Object Boundaries\n在正式开始关键点提取之前，有必要先进行边缘提取。原因是相对于其他点，边缘上的点更有可能是关键点。和图像的边缘不同（灰度明显变化），点云的边缘有更明确的物理意义。对点云而言，场景的边缘代表前景物体和背景物体的分界线。所以，点云的边缘又分为三种：前景边缘，背景边缘，阴影边缘。\nrangeImage 是一个天然适合用于边缘提取的框架。在这里需要做一些假设：每个rangeImage像素中假设都只有一个点（显然在生成rangeImage的时候点云是被压缩了的，压缩了多少和rangeImage的分辨率有关，分辨率不能太小，否则rangeImage上会有\"洞”，分辨率太大则丢失很多信息）。\n三维点云的边缘有个很重要的特征，就是点a 和点b 如果在 rangImage 上是相邻的，然而在三维距离上却很远，那么多半这里就有边缘。由于三维点云的规模和稀疏性，“很远”这个概念很难描述清楚。到底多远算远？这里引入一个横向的比较是合适的。这种比较方法可以自适应点云的稀疏性。所谓的横向比较就是和 某点周围的点相比较。 这个周围有多大？不管多大，反正就是在某点pi的rangeImage 上取一个方窗。假设像素边长为s. 那么一共就取了s^2个点。接下来分三种情况来讨论所谓的边缘：\n1.这个点在某个平面上，边长为 s 的方窗没有涉及到边缘\n2.这个点恰好在某条边缘上，边长 s 的方窗一半在边缘左边，一半在右边\n3.这个点恰好处于某个角点上，边长 s 的方窗可能只有 1/4 与 pi 处于同一个平面\n如果将 pi 与不同点距离进行排序，得到一系列的距离，d0 表示与 pi 距离最近的点，显然是 pi 自己。 ds^2 是与pi 最远的点，这就有可能是跨越边缘的点了。 选择一个dm，作为与m同平面，但距离最远的点。也就是说，如果d0~ds^2是一个连续递增的数列，那么dm可以取平均值。如果这个数列存在某个阶跃跳动（可能会形成类似阶跃信号）那么则发生阶跃的地方应该是有边缘存在，不妨取阶跃点为dm(距离较小的按个阶跃点）原文并未如此表述此段落，原文取s=5, m=9 作为m点的一个合理估计。\n对任意一个点，进行打分，来判断该点作为边缘点有多大可能性。首先，边缘可能会在某点的：上，下，左，右四个方向。\n所以只要把pi 和 pi 右边的点求相对距离。 并把这个相对距离和dm进行比较，就可以判断边缘是不是在该点右边。如果距离远大于dm，显然该点右边的邻点就和pi不是同一个平面了。\n为了增加对噪声的适应能力，取右边的点为右边几个点的平均数。接下来依据此信息对该点进行打分。\n其中deta 就是dm. dright = || pi pright ||.\n最后再取大于0.8的Sright，并进行非极大值抑制。就可以得到物体的边缘了\n2. 关键点提取\n在提取关键点时，边缘应该作为一个重要的参考依据。但一定不是唯一的依据。对于某个物体来说关键点应该是表达了某些特征的点，而不仅仅是边缘点。所以在设计关键点提取算法时，需要考虑到以下一些因素：\ni) it must take information about borders and the surface structure into account;\n边缘和曲面结构都要考虑进去\nii) it must select positions that can be reliably detected even if the object is observed from another perspective;\n关键点要能重复\niii) the points must be on positions that provide stable areas for normal estimation or the descriptor calculation in general.\n关键点最好落在比较稳定的区域，方便提取法线\n对于点云构成的曲面而言，某处的曲率无疑是一个非常重要的结构描述因素。某点的曲率越大，则该点处曲面变化越剧烈。在2D rangeImage 上，去 pi 点及其周边与之距离小于2deta的点，进行PCA主成分分析。可以得到一个 主方向v，以及曲率值 lamda. 注意， v 必然是一个三维向量。\n那么对于边缘点，可以取其 权重 w 为1 ， v 为边缘方向。\n对于其他点，取权重 w 为 1-(1-lamda)^3 ， 方向为 v 在平面 p上的投影。 平面 p 垂直于 pi 与原点连线。\n到此位置，每个点都有了两个量，一个权重，一个方向。\n将权重与方向带入下列式子 I 就是某点 为特征点的可能性。\n最后进行极大值抑制，就可以得到一些特征点了。\nHarris3D\n除去NARF这种和特征检测联系比较紧密的方法外，一般来说特征检测都会对曲率变化比较剧烈的点更敏感。Harris算法是图像检测识别算法中非常重要的一个算法，其对物体姿态变化鲁棒性好，对旋转不敏感，可以很好的检测出物体的角点。甚至对于标定算法而言，HARRIS角点检测是使之能成功进行的基础。\n1.Harris 算法\n其思想及数学推导大致如下：\n1.在图像中取一个窗 w\n2.获得在该窗下的灰度  I\n3.移动该窗，则灰度会发生变化，平坦区域灰度变化不大，边缘区域沿边缘方向灰度变化剧烈，角点处各个方向灰度变化均剧烈\n4.依据3中条件选出角点\n当然啦，如果Harris算子的实现也和它的思想这么平淡那我就不表扬他聪明了，Harris算子的具体实现方法，利用的是图像偏微分方程的思想。\n先给出抽象数学表达式：\n其中 w 代表窗函数，某个x,y为图像坐标，u,v是一个移动向量（既反应移动方向，也反应移动大小）。\nIx表示图像沿x方向的差分，Iy表示图像沿y方向的差分。\n显然，E(u,v)可以用另外一种形式来表示了。最终可以表达为协方差矩阵的形式。\nOK，在这里我们有了数学中最优雅的表达——Matrix，especially symmetric Matrix. Nothing is better than that.\n2.矩阵的方向性\n显然，E(u,v)的值和u,v有关。。。很有关。。\n1.可以取一组u,v，让E(u,v)的值最小。\n2.还可以取一组u,v，让E(u,v)的值最大。\n这些u,v怎么取，显然就和矩阵M的方向有关。\n平面内的一个矩阵乘以一个向量v，大概简单的写成   Mv\n它会使得这个向量发生一个作用：旋转，拉伸，平移.....总之，这种作用叫做  线性变换\n矩阵的左边好像也是一个向量，只不过是横着写的（[u v]），换而言之，那就是 vT（v的转置）。\nvT（Mv)......这是啥？\n意思好像是。。。。v先旋转+拉伸一下，然后再在它自己身上投影，最终的 E(u,v)本质上来说，就是这个投影的长度。。。嗯，对，投影的长度\n好了。我们现在明确了 E(u,v) 的数学几何意义，再回过头来想想，要怎样才能让这个投影的长度达到最大或者最小呢？\n显然，答案就是矩阵的特征值与特征向量，当[u v]T 取特征向量方向的时候，矩阵M只有拉伸作用，而没有旋转作用，这时的投影长度是最长的（如果反向投则是负的最长）。\n到此为止，我们已经知道了 E(u,v）的最大和最小值了（笨办法是求出特征向量方向再带进去，聪明的方法是直接看矩阵特征值，特征值就是放大倍数）。并且，分析可以知道，特征值越大，那么说明 E(u,v)越大。\n1.两个特征值都很大==========>角点（两个响应方向）\n2.一个特征值很大，一个很小=====>边缘（只有一个响应方向）\n3.两个特征值都小============>平原地区（响应都很微弱）\n基于上述特征，有很多人设计了角点的快速判据。\n有 det(M) - trace(M)^2\n有 det(M)/trace(M)\n.....等等很多，但是这不重要，思想都是一样的。\n3. 3DHarris\n在2DHarris里，我们使用了 图像梯度构成的 协方差矩阵。 图像梯度。。。嗯。。。。每个像素点都有一个梯度，在一阶信息量的情况下描述了两个相邻像素的关系。显然这个思想可以轻易的移植到点云上来。\nOOPS，糟糕，点云木有灰度的概念啊，一般的点云也木有强度的概念啊。。。这可如何是好？？？？？？\n别紧张，pcl 说这样能行，那就肯定能行咯，先定性的分析一下Harris3D的理念。\n想象一下，如果在 点云中存在一点p\n1、在p上建立一个局部坐标系：z方向是法线方向，x,y方向和z垂直。\n2、在p上建立一个小正方体，不要太大，大概像材料力学分析应力那种就行\n3、假设点云的密度是相同的，点云是一层蒙皮，不是实心的。\na、如果小正方体沿z方向移动，那小正方体里的点云数量应该不变\nb、如果小正方体位于边缘上，则沿边缘移动，点云数量几乎不变，沿垂直边缘方向移动，点云数量改变\nc、如果小正方体位于角点上，则有两个方向都会大幅改变点云数量\nOK，我们已经有了Harris3D的基本准则，接下来要思考的是怎样优雅的解决这个问题\n两个和z相互垂直的方向。。。。嗯。。。。perpendicular。。。。\n如果由法向量x,y,z构成协方差矩阵，那么它应该是一个对称矩阵。而且特征向量有一个方向是法线方向，另外两个方向和法线垂直。\n那么直接用协方差矩阵替换掉图像里的M矩阵，就得到了点云的Harris算法。\n其中，半径r可以用来控制角点的规模\nr小，则对应的角点越尖锐（对噪声更敏感）\nr大，则可能在平缓的区域也检测出角点\n4.PCL对Harris算法的实现\n根据以上分析，在PCL的API文档的帮助下，我尝试了一下 Harris3D 算法。感谢山大的毕同学提供的点云，该点云是场景点云而不是一般的物体点云。总体感觉是慢，因为针对每个点云，需要计算它的法线，算完之后又要针对每个点进行协方差矩阵的计算，总而言之，整个过程还是非常耗时的。并且说实话。。。算法的效果一般般。\n#include <iostream> #include <pcl\\io\\pcd_io.h> #include <pcl/point_cloud.h> #include <pcl/visualization/pcl_visualizer.h> #include <pcl/io/io.h> #include <pcl/keypoints/harris_keypoint3D.h> #include <cstdlib> #include <vector> using namespace std; int main() { pcl::PointCloud<pcl::PointXYZ>::Ptr cloud (new pcl::PointCloud<pcl::PointXYZ>); pcl::io::loadPCDFile (\"F:\\\\PCL\\\\PCD\\\\both.pcd\", *cloud); boost::shared_ptr<pcl::visualization::PCLVisualizer> viewer(new pcl::visualization::PCLVisualizer); viewer->addPointCloud(cloud,\"all_cloud\"); //注意Harris的输出点云必须是有强度(I)信息的，因为评估值保存在I分量里 pcl::PointCloud<pcl::PointXYZI>::Ptr cloud_out (new pcl::PointCloud<pcl::PointXYZI>); pcl::HarrisKeypoint3D<pcl::PointXYZ,pcl::PointXYZI,pcl::Normal> harris; harris.setInputCloud(cloud); cout<<\"input successful\"<<endl; harris.setNonMaxSupression(true); harris.setRadius(0.04f); harris.setThreshold(0.02f); cout<<\"parameter set successful\"<<endl; //新建的点云必须初始化，清零，否则指针会越界 cloud_out->height=1; cloud_out->width =100; cloud_out->resize(cloud_out->height*cloud->width); cloud_out->clear(); harris.compute(*cloud_out); int size = cloud_out->size(); pcl::PointCloud<pcl::PointXYZ>::Ptr cloud_harris (new pcl::PointCloud<pcl::PointXYZ>); cloud_harris->height=1; cloud_harris->width =100; cloud_harris->resize(cloud_out->height*cloud->width); cloud_harris->clear(); pcl::PointXYZ point; //可视化结果不支持XYZI格式点云，所有又要导回XYZ格式。。。。 for (int i = 0;i<size;i++) { point.x = cloud_out->at(i).x; point.y = cloud_out->at(i).y; point.z = cloud_out->at(i).z; cloud_harris->push_back(point); } pcl::visualization::PointCloudColorHandlerCustom<pcl::PointXYZ> harris_color_handler (cloud_harris, 0, 255, 0); viewer->addPointCloud(cloud_harris,harris_color_handler,\"harris\"); viewer->setPointCloudRenderingProperties (pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 5, \"harris\"); while (!viewer->wasStopped()) { viewer->spinOnce(100); } system(\"pause\"); }\n由于我选择的搜索半径比较大，所以找到的角点都不是太\"角”,关于参数设置大家可以多多探索，但我认为，特征点检测算法实在太慢，对实时机器人系统来说是远远达不到要求的。这种先算法线，再算协方差的形式真心上不起。。。。实际上这种基于领域法线的特征点检测算法有点类似基于 CRF的语义识别算法，都只使用了相邻信息而忽略了全局信息。也可能相邻信息包含的相关性比较大，是通往高层次感知的唯一路径吧，谁又知道呢？\nISS&Trajkovic\n关键点检测往往需要和特征提取联合在一起，关键点检测的一个重要性质就是旋转不变性，也就是说，物体旋转后还能够检测出对应的关键点。不过说实话我觉的这个要求对机器人视觉来说是比较鸡肋的。因为机器人采集到的三维点云并不是一个完整的物体，没哪个相机有透视功能。机器人采集到的点云也只是一层薄薄的蒙皮。所谓的特征点又往往在变化剧烈的曲面区域，那么从不同的视角来看，变化剧烈的曲面区域很难提取到同样的关键点。想象一下一个人的面部，正面的时候鼻尖可以作为关键点，但是侧面的时候呢？会有一部分面部在阴影中，模型和之前可能就完全不一样了。\n也就是说现在这些关键点检测算法针对场景中较远的物体，也就是物体旋转带来的影响被距离减弱的情况下，是好用的。一旦距离近了，旋转往往造成捕获的仅有模型的侧面，关键点检测算法就有可能失效。\n1.ISS算法\nISS算法的全程是Intrinsic Shape Signatures，第一个词叫做内部，这个词挺有讲究。说内部，那必然要有个范围，具体是什么东西的范围还暂定。如果说要描述一个点周围的局部特征，而且这个物体在全局坐标下还可能移动，那么有一个好方法就是在这个点周围建立一个局部坐标。只要保证这个局部坐标系也随着物体旋转就好。\n方法1.基于协方差矩阵\n协方差矩阵的思想其实很简单，实际上它是一种耦合，把两个步骤耦合在了一起\n1.把pi和周围点pj的坐标相减：本质上这生成了许多从pi->pj的向量，理想情况下pi的法线应该是垂直于这些向量的\n2.利用奇异值分解求这些向量的0空间，拟合出一个尽可能垂直的向量，作为法线的估计\n协方差矩阵本质是啥？就是奇异值分解中的一个步骤。。。。奇异值分解是需要矩阵乘以自身的转置从而得到对称矩阵的。\n当然，用协方差计算的好处是可以给不同距离的点附上不同的权重。\n方法2.基于齐次坐标\n1.把点的坐标转为齐次坐标\n2.对其次坐标进行奇异值分解\n3.最小奇异值对应的向量就是拟合平面的方程\n4.方程的系数就是法线的方向。\n显然，这种方法更加简单粗暴，省去了权重的概念，但是换来了运算速度，不需要反复做减法。其实本来也不需要反复做减法，做一个点之间向量的检索表就好。。。\n但是我要声明PCL的实现是利用反复减法的。\n不管使用了哪种方法，都会有三个相互垂直的向量，一个是法线方向，另外两个方向与之构成了在某点的局部坐标系。在此局部坐标系内进行建模，就可以达到点云特征旋转不变的目的了。\nISS特征点检测的思想也甚是简单：\n1.利用方法1建立模型\n2.其利用特征值之间关系来形容该点的特征程度。\n显然这种情况下的特征值是有几何意义的，特征值的大小实际上是椭球轴的长度。椭球的的形态则是对邻近点分布状态的抽象总结。试想，如果临近点沿某个方向分布致密则该方向会作为椭球的第一主方向，稀疏的方向则是第二主方向，法线方向当然是极度稀疏（只有一层），那么则作为第三主方向。\n如果某个点恰好处于角点，则第一主特征值，第二主特征值，第三主特征值大小相差不会太大。\n如果点云沿着某方向致密，而垂直方向系数则有可能是边界。\n总而言之，这种局部坐标系建模分析的方法是基于特征值分析的特征点提取。\n最后补充，Intrisic指的就是这个椭球的内部。\nPCL实现\npcl::PointCloud<pcl::PointXYZRGBA>::Ptr model (new pcl::PointCloud<pcl::PointXYZRGBA> ());; pcl::PointCloud<pcl::PointXYZRGBA>::Ptr model_keypoints (new pcl::PointCloud<pcl::PointXYZRGBA> ()); pcl::search::KdTree<pcl::PointXYZRGBA>::Ptr tree (new pcl::search::KdTree<pcl::PointXYZRGBA> ()); // Fill in the model cloud double model_resolution; // Compute model_resolution pcl::ISSKeypoint3D<pcl::PointXYZRGBA, pcl::PointXYZRGBA> iss_detector; iss_detector.setSearchMethod (tree); iss_detector.setSalientRadius (6 * model_resolution); iss_detector.setNonMaxRadius (4 * model_resolution); iss_detector.setThreshold21 (0.975); iss_detector.setThreshold32 (0.975); iss_detector.setMinNeighbors (5); iss_detector.setNumberOfThreads (4); iss_detector.setInputCloud (model); iss_detector.compute (*model_keypoints);\n2.Trajkovic关键点检测算法\n角点的一个重要特征就是法线方向和周围的点存在不同，而本算法的思想就是和相邻点的法线方向进行对比，判定法线方向差异的阈值，最终决定某点是否是角点。并且需要注意的是，本方法所针对的点云应该只是有序点云。\n本方法的优点是快，缺点是对噪声敏感。\n另外，在二维图像中的BRISK描述子，AGAST描述子，SIFT特征，SUAN特征，都 已经移植为对应的3D特征。这里不再啰嗦。"}
{"content2":"转载\n▼\n标签： 计算机视觉\n目前，公认的计算机视觉三大会议分别为ICCV,ECCV,CVPR。\n1、ICCV\nICCV的全称是 IEEE International Conference on Computer Vision，国际计算机视觉大会，是计算机视觉方向的三大顶级会议之一，通常每两年召开一次，2005 年 10 月曾经在北京召开。 会议收录论文的内容包括：底层视觉与感知，颜色、光照与纹理处理，分割与聚合，运动与跟踪，立体视觉与运动结构重构，基于图像的建模，基于物理的建模，视觉中的统计学习，视频监控，物体、事件和场景的识别，基于视觉的图形学，图片和视频的获取，性能评估，具体应用等。 ICCV是计算机视觉领域最高级别的会议，会议的论文集代表了计算机视觉领域最新的发展方向和水平。会议的收录率较低，以 2007 年为例，会议共收到论文1200余篇，接受的论文仅为244篇。会议的论文会被 EI 检索。\n2、ECCV\nECCV的全称是Europeon Conference on Computer Vision，两年一次，是计算机视觉三大会议（另外两个是ICCV和CVPR）之一。很明显，ECCV是一个欧洲会议，欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n3、CVPR\nCVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。"}
{"content2":"看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？\n我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。\n可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n(1)以下是不完整的列表，但基本覆盖。\n机器学习顶级会议：NIPS, ICML, UAI, AISTATS; （期刊：JMLR, ML, Trends in ML, IEEE T-NN）\n计算机视觉和图像识别：ICCV, CVPR, ECCV; （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）\n人工智能：IJCAI, AAAI; （期刊AI）\n另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。\n特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。\n(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如：\nCV方面：http://www.cvpapers.com/index.html;\nNIPS: http://books.nips.cc/;\nJMLR(期刊): http://jmlr.csail.mit.edu/papers/;\nCOLT和ICML(每年度的官网):http://www.cs.mcgill.ca/~colt2009/proceedings.html。\n(3)说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。\n注:\nNIPS = Neural Information Processing Systems https://nips.cc/\nICML = International Conference on Machine Learning https://icml.cc\nUAI(AUAI) =Association for Uncertainty in Artifical Intelligence http://www.auai.org/\nAISTATS = Artificial Intelligence and Statistics http://www.aistats.org/\nJMLR = Journal of Machine Learning Research http://jmlr.org/\nIJCAI = International Joint Conference on Artifical Intelligence http://ijcai.org/\nAAAI =Association for the Advancement of Aritifical Intelligence http://www.aaai.org/home.html\n世界计算机算法最权威会议SODA—全称ACM-SIAM Symposium on Discrete Algorithms。\n世界计算机科学领域最顶级期刊JACM—全称Journal of the Association for Computing Machinery，该期刊只发表世界计算机科学领域具有最重要意义的研究工作，每年仅收录30多篇。\n世界数据库领域最顶级的期刊ACM TODS—全称ACM Transactions on Database Systems，该期刊全年在全世界范围不过收录30篇高水平论文\n世界计算机存储领域顶尖期刊ACM Transactions on Storage—该期刊全年收录文章不超过20篇\n世界程序语言设计领域顶级学术会议PLDI2007—全称ACM SIGPLAN Conference on Programming Language Designand Implementation\n世界物理学最权威学术刊PRL—全称Physical Review Letter，国内大学计算机系目前只有清华计算机系发过两篇PRL\n世界理论计算机领域顶级会议STOC—全称ACM Symp on Theory of Computing\n世界人工智能方面最顶级会议IJCAI—全称International Joint Conferences on ArtificialIntelligence\n世界计算机视觉和模式识别领域顶级国际会CVPR—全称IEEE Conference on Computer Vision and PatternRecognition\n世界信息检索领域顶级会议SIGIR—全称ACM SIGIR Special Interest Group on InformationRetrieval\n世界数据挖掘领域最权威国际期刊IEEE TKDE—全称IEEE Transactions on Knowledge and Data Engineering\n世界数据库领域最顶级会议SIGMOD—全称ACM’s Special Interest Group on Management Of Data\n世界计算机图形学最权威国际会议ACM SIGGRAPH\n世界计算语言/自然语言处理领域最顶级会议ACL—全称Association for Computational Linguistics\n世界理论计算机科学顶级学术期刊Theoretical Computer Science\n世界计算复杂性领域顶级会议CCC—全称IEEE Conference on Computational Complexity\n世界计算机视觉和模式识别领域顶尖期刊IEEE PAMI—全称IEEE Transactions on Pattern Analysis and MachineIntelligence\n世界集成电路设计领域最顶级会议DAC—全称Design Automation Conference\n世界人工智能领域顶级学术会议AAAI—全称Association for the Advancement of ArtificialIntelligence\n世界互联网领域顶级会议WWW—全称World Wide Web Conference\n世界通信与计算机网络领域顶级学术会议Infocom—全称IEEE Conference on Computer Communications，\n世界信息科学理论顶级期刊IEEE Transactions on Information Theory\n世界数据挖掘领域一流会议SDM—全称SIAM International Conference on Data Mining\n世界声学与信号处理一流会议ICASSP—全称IEEE International Conference on Acoustics, Speech,and Signal Processing\n世界计算机算法与理论领域一流会议STACS—全称Symp on Theoretical Aspects of Computer Science\n世界计算机理论科学领域一流会议ICALP—全称International Colloquium on Automata, Languages andProgramming\n世界数据挖掘领域一流会议ICME—全称IEEE International Conference on Multimedia & Expo\n世界计算机图形学领域一流会议EuroGraphics\n世界集成电路领域一流会议ISVLS"}
{"content2":"这篇文章从一个刚刚开始计算机视觉研究的初学者的角度，详细探讨了这个领域的文献、专家学者、研究组、博客，并重点说明了如何开始研究，如何选择方向，如何看论文、实现代码、调试代码等，并详细说明了研究计算机视觉应该如何学习机器学习等。是初入该领域的博士、学者的非常值得详细考察和收藏的参考。（52CV.NET注释）\n由于微信公众号限制，文中很多超链接无法点击，访问https://www.52cv.net/?p=524 ，可以查看完整文章。\n顶级会议和期刊\n第一梯队顶级会议: CVPR, ECCV, ICCV, NIPS, IJCAI\n高声誉第二梯队的顶级会议: BMVC\n著名的第二梯队顶级会议: ICIP, ACCV, ICPR, SIGGRAPH\n顶级期刊: PAMI, IJCV\n著名期刊: CVIU, IVC\nMicrosoft Academic Research 列出的 顶级会议\nRanks from Core\nRanks from Arnetminer\nsource 列出了近几年的会议论文\njournal 列出了期刊的影响因子\n来自 EigenFactor 的期刊分数\n顶级专家作者\n微软学术Microsoft Academic authors list\n谷歌学术Google Scholar List\nHOG 特征作者 Navneet Dalal\nJitendra Malik.\nGary Bradski OpenCV创始缔造者\nDavid Lowe SIFT特征发明人\nList of vision people (but not necessarily top authors)\nComputer Vision: Algorithms and Applications by Richard Szeliski\n顶级研究组\nCheck them here\nCheck others here\nCMU: Robotics everywhere.\nLEAR\nImageLab Group\nMachine Vision Laboratory at UWE\nALCOR\nCentre for Image Processing and Analysis (CIPA)\nImageMetry\nVISILAB\nGRIMA – Machine Intelligence Group\nVision and Sensing Research Group – University of Canberra\nCAVE – Computer Vision Laboratory at Columbia University\nComputational Biomedicine Laboratory (CBL), University of Houston\nVision Lab – University of Antwerp.\nVisual Geometry Group, Oxford UK (Andrew  Zisserman’s group)\nLEAR, Grenoble, France (Cordelia Schmid’s group)\nWILLOW, Paris France (Jean Ponce’s group)\nCVLAB EPFL, Laussane Switzerland (Pascal Fua’s group)\nComputer vision group ETH, Zurich Switzerland (Luc Van Gool’s group)\nUCB (Malik, Darrel, Efros)\nUMD (Davis, Chellappa, Jacobs, Aloimonos, Doermann)\nUIUC (Forsyth, Hoiem, Ahuja, Lazebnik)\nUCSD (Kriegman)\nUT-Austin (Aggarwal, Grauman)\nStanford (Fei-Fei Li, Savarese)\nUSC (Nevatia, Medioni)\nBrown (Felzenszwalb, Hays, Sudderth)\nNYU (Rob Fergus)\nUC-Irvine (Ramanan, Fowlkes)\nUNC (Tamara Berg, Alex Berg, Jan-Michael Frahm)\nColumbia (Belhumeur, Shree Nayar, Shih-Fu Chang)\nLaboratory for Computational Intelligence, University of British Columbia, Vancouver (David Lowe’s group)\nComputer Science Department, University of Toronto, Toronto (Deep Learning fame Hilton, Srivastava, Salakhutdinov)\nCentre for Vision Research, York University, Toronto\n博客\nTomasz Malisiewicz blog\nThe Serious Computer Vision Blog\nResearch blog of Roman Shapovalov\nComputer Vision Talks\nSteves Computer Vision Blog\nThe Computer Vision\nComputer Vision Blog\nAndy’s Computer Vision and Machine Learning Blog\nComputer Vision Models\nsolem’s vision blog\nuncannyvision blog\nBlogs on Computer Vision, Machine Vision and Image Processing\nAll About Computer Vision\nOpen Computer Vision\nCV工业界的实验室和创业公司\nMicrosoft and Google\nIBM Research\nNEC Labs America\nAcute3D (Sophia Antipolis, France) was founded in 2011.\nBubbli\nShoppTag\nOculusai\nVideosurf (video search)\nWillow garage (robotics)\nSportvision (sports broadcast)\nIntelli-vision (surveillance)\nGauss Surgical\nAdobe’s Advanced Technology Labs\nDolby\n如何开始研究\n我喜欢把计算机视觉问题分为两种类型\n一些研究方向设计到人工智能基于学习的方法。比如图像分类，OCR，视频跟踪等\n大多数你所能看到的论文都是这种方向的.\n学习意味着我们有很多数据 (e.g. 比如ImageNet，100万图像和他们的标签）,然后学习这种模式  (e.g. 比如分类图像中的字符)\n对这种类型的方向，你必须学习很多机器学习的知识\n其他研究方向涉及到不需要学习的算法，比如3D重建，光流计算，全景拼接（52CV.Net评论：其实现在3D重建和光流估计已经有很多基于学习的算法了，可在本站搜索关键字获取相关信息）\n使用课本和课程\n一种直接的方法是从书本开始\n不要被困在书本里。请记住，你想开始研究。尝试了解基础知识并进行一些编码。保持你的眼睛定睛在对你来说最近有趣的工作上。\n尝试找出不同的研究视觉问题..看哪个更令你兴奋。\n然后你要进入下一个阶段: “从论文开始”\n从论文开始\n从顶级会议和期刊的论文开始。其他低级别的会议可能会有虚假结果并浪费您的时间。\nCVPR保留重要会议和许多论文的清单。\n使用文件知道什么是可用的轨道.. Wiki也会有帮助\n使用Google Scholar查找特定问题的综述。综述可以节省大部分时间。\n考虑最近过去3年的论文。假设我们在2014年，考虑2011年，然后是2012年，然后是2013年。不要从2014年开始。\n收集文件，使标题看起来相关。搜索他们找到是否有源代码。尝试从源代码文件开始。\n开始将是艰难的，因为你遇到了许多你不知道的术语和工具。耐心一点。谷歌搜索他们，在论坛上提问，如Quora或Stackoverflow。\n尝试找到一个特定研究方向（例如3D重建，点云，场景理解，物体识别，大图像数据，多目标跟踪，图像描述符理论等）。查看wiki或会议论文目录以查找您感兴趣的内容。\n使用会议来了解某方向论文或使用Google学术搜索\n关注那些研究工作更权威的的研究人员。关注高引用次数文献。\n首选从有运行软件的研究工作开始，节省你的时间。\n为了学习一些工程实现方向，请为您选择一个简单而漂亮的论文然后实现它。复现论文的结果。在这样做的时候，会有很多问题弹出，很多时候你将不得不做一些假设，因为你所看到的论文中通常并不是所有的都提到了。还有许多实现细节，比如如何有效地实现这一点不会被列出。您将了解诸如性能，实验等问题。可选择的论文比如：Viola Jones face detection, Christophe Lampert Efficient Subwindow Search, or Brian Fulkerson superpixel neighborhoods 等。实现具有完整代码的论文是一个非常好的主意，以便您可以检查自己的实现有什么问题。\n对于你自己的研究工作，要尝试使用现有开源代码，而不是一切都从头开始，不要重复造轮子！\n如果论文没有公开代码，你可以尝试联系作者是否可以得到代码。\n如果理解几次尝试理解一篇论文仍然很难，就转到另一片论文。或者换一个方向。（这是你在寻找研究方向的时候）\n这也许对你有用，最佳获奖论文集\n研究生研讨课程取决于论文。\n从代码开始\n从代码到纸张，是从一些可用的代码开始理解你所研究的问题\n找一个开源库，然后尝试它，比如OpenCV\n有很多不错的书关于OpenCV\nYoutube 上也有不少视频：\nhttps://www.youtube.com/playlist?v=MfnEtFAWooQ&list=PLo1wvPF7fMxQ_SXibg1azwBfmTFn02B9O\nhttps://www.youtube.com/playlist?v=xEnPZ78queI&list=PLDqunwM5dbtIbEuXv1rB7OFBoRzEF8GH6\nhttps://www.youtube.com/playlist?v=IwsHuSITs3c&list=PLTgRMOcmRb3PvUZpNTRsdkzVuZ4z_s444\nhttps://www.youtube.com/playlist?v=cgo0UitHfp8&list=PLvwB65U8V0HHCEyW2UTyOJym5FsdqfbHQ\n学习Matlab并使用它来编写初始解决方案原型（因为它往往比较快的能够开发出原型）\nHelpful: Join OpenCV yahoo group and read comments & messages.\n选择一个有意思的toy项目并实现它\n机器学习\n机器学习是从数据中学习的核心算法。\n对于计算机视觉而言，特别是初学者，最开始的时候你不需要学习太多机器学习。你可以像黑箱一样使用他们就够了\n顺便说一下，这是一个艰难的领域。要成为专家，你需要付出大量时间。\n你想要在这个领域成长够多，你就要关注更多的细节。\n最开始，您只需要学习一些基础知识+最近使用的算法。\n每4-5年，都有一些算法在文献中流行\n例如3年前（2012年之前，52CV.NET注），SVM非常受欢迎\n如今（2014年，2015年），深度学习往往有最好的表现。\n建立该领域的基础知识：\n在Coursera 上完成Andrew NG 机器学习课程。\n了解最近使用的算法是什么\n尝试阅读有关这些算法的更多信息\n尝试做一些编码。搜索流行的工具并使用它们\n例如对于SVM（libsvm），CNN（Caffe）\n要么询问一些专业人士\n或在您的问题中下载2-3年范围内的顶级会议论文。浏览它们并且知道他们使用了什么学习算法。\n总的来说应该是很少重复的人。多关注他们\n然后\n现在，您可以回到前面论文/书籍并继续阅读，当涉及ML时，您会发现主题更加容易。\n更加进深\n请参阅Andrew Nn Standford Machine Learning Course\n其他网络上的视频和书籍\n请参阅Mostafa博士的”Learning From Data“视频。\n学习Waleed博士的CS395: Pattern Recognition 。\n教科书：Pattern Recognition and Machine Learning\n要更多地了解学习如何发生？\n了解更多算法主题和背后的数学\n一些推荐论文\n很难说什么是好的论文。也许就是更好地确定问题并作为参考。\n视觉中的顶级刊物\nWhat are the must-read papers 什么是计算机视觉领域的必读文章？让学生在这个领域进行研究？\n非常有用的大学课程\nCS395T：视觉识别，2012年秋季\nCMPT888：2010年夏季人类活动识别\nCMPT882：计算机视觉中的识别问题，2009年夏季\n积累经验\n在获得博士学位时，您通常会学会处理所有这些问题\n您如何高效可靠地解决研究中的所有问题？为了了解所有这些问题，您基本上必须成为研究小组的成员几年。如果你在一个专注于物体检测的实验室里，你周围会有很多学生在解决相同的问题，在深夜与同学交谈是我知道你可以获得专业知识的唯一途径了解：多交流打听。\n您如何调试代码并有效调整参数？最佳实践是看更高级学生的优秀代码。在开始调试机器学习算法之前，您应该总体上熟悉调试。调试机器学习算法不像调试快速排序。如果你修正了所有的错误，你的算法可能仍然不起作用，可能是因为其他问题，比如缺乏数据，模型复杂度太低等等。坦率地说，调试视觉/学习算法更像是艺术而不是科学。\n调整您未编写的算法或软件库的参数并非易事。您应该学会如何正确使用验证数据，了解如何运行完整的训练/评估流程，并准备好进行交叉验证。\n你如何用个人电脑实现大规模的问题？（对于图像/视频分析，可能会有大量的数据超出你的内存，如何处理它？）一般来说，你不会实现一个大的在一台PC上出现问题。我在研究生院学到的最有价值的技能之一就是如何在群集中并行计算。没有群集的大学/实验室很难与拥有大中型集群的大学竞争。这也是许多教授加入Google和Facebook等组织的原因之一 —他们拥有数据和计算资源，可以让高级研究人员处理越来越多的大型问题。\n如果您无法访问大型集群，那么我会建议您在Google这样的地方申请实习。你会在那里学到很多东西（至少我是）。虽然你无法将自己编写的任何代码带回家，但是你会学到很多课程，这些课程会影响你作为学生的生活。如果你必须在一台机器上工作，你将不得不将数据集切割成更小的块，并逐渐将块加载到内存中。\n材料\n在线视频和会谈\n在线课程：离散推理和人工视觉学习\nUCF计算机视觉视频讲座：视频\nEGGN 512 – 计算机视觉视频\n视频讲座包括许多计算机视觉。\n技术会谈  对于一些会议，如ICML2011，他们主持视频中的大部分（全部）会谈。其他人，如CVPR2011，只有选定的视频。这是了解大量近期工作而不依赖阅读报告的好方法。\nCVPR2010，他们为会谈主持了很多视频。他们也有很多夏季学校的ML视频。\nWired，IEEE Spectrum，TechCrunch，TED，BigThink，Sixty Symbols，GISCIA，http://www.youtube.com/user/GoogleTechTalks，\n课程\n计算机视觉简介（斯坦福大学;李飞飞教授）相当标准的CV课程。\n计算机视觉（UIUC; Forsyth教授）相当标准的简历课程。\n视觉中的基于学习的方法（CMU; Alexei Efros教授）我学习了很多关于纹理（纹理）识别和一些使用花式ML技术的最先进的方法。\n基础物体识别和场景理解  （CMU; Antonio Torralba教授）这是一个持续不断的课程，侧重于更高层次的视觉。第一场讲座看起来很有前途，但我不确定班上的其他人会是什么样子。\n机器视觉MIT 课程\n计算机视觉麻省理工学院课程进展\n计算机视觉\n计算机视觉：模型，学习和推理  – 这是一个很好的（免费的！）预印本，主要倾向于机器学习。每个部分都提供了一套涉及的模型或机器学习工具的背景以及推理方法。开始是对必要概率和机器学习概念的深入概述。我刚开始阅读本书，但对于获取零件模型和形状模型等概述非常有用。\n计算机视觉：算法和应用  – Richard Szeliski。一本调查书。这是更传统的教科书，在许多目前的CV课程中都有引用，如李飞飞的上述内容以及我校目前的CV课程（JHU）。\n计算机视觉中的多视图几何 – Richard Hartley和Andrew Zisserman\n计算机视觉现代方法 – David Forsyth和Jean Ponce\n视觉对象识别：人工智能和机器学习综合讲座 – Kristen Grauman和Bastian Leibe\n由Trucco和Verri介绍3D计算机视觉\nDigital Image Processing 3rd Edition by Gonzales and Woods\n图像分析的实用算法\nhttp://www.computervisiononline.com/books\n计算机视觉和图像处理编码\n用Python编程计算机视觉 – Jan Erik Solem\n学习OpenCV – Gray Bradski和Adrian Kaehler\n数字图像处理基础：Matlab中的实例 – Chris Solomon和Toby Breckon\n人类视觉\n视觉：视觉信息的人类表现和处理的计算调查 – David Marr\n迈向视觉信息理论的步骤：主动感知，信号 – 符号转换以及传感与控制之间的相互作用 – Stefano Soatto\n基本视觉：视觉感知介绍 – 罗伯特斯诺登，彼得汤普森和汤姆Troscianko\n用Python编程计算机视觉\n其他\nCV论文是来自视觉会议的近期计算机视觉论文集。\n视觉识别和机器学习暑期学校，格勒诺布尔，2012\n我会参加一些机器学习课程，并参加信号处理/时频分析/小波分析的一些课程。\n精彩的应用程序\n永不停止图像学习（NEIL）\n这是一个计算机程序运行24X7浏览互联网从互联网数据提取视觉信息。它得到了谷歌和国防部海军研究办公室的支持。\n它目前识别对象 – 对象关系，对象 – 属性关系，场景 – 对象关系，场景 – 属性关系\n人脸检测\n网球追踪\n与深度相机的身体姿势估计\n微软展示的3D扫描技术，Heads Turn\n颜色变化显示人血流量\n只有公共Flickr照片才能在3D中重建整个城市\n自主物体，例如自驾车\nPredator对象跟踪\nKinect Fusion – 从移动Kinect实时3D模型构建\nVeebot，一个采集血液样本的机器人\nHarp：检测激光的中断以播放音符（简单，强大）。Piano。\nGoogle照片搜索\nPhysical security\nPTAM是AR的重要应用\n谷歌眼镜\n谷歌街景：在街道层面捕捉世界\nWord Lens：基于增强现实相机的语言翻译应用程序。手机摄像头可以识别一种语言的文本，并显示用另一种语言翻译的文字。我发现关于这个应用程序的最好的东西是翻译是在没有连接到互联网的情况下实时执行的！\nCarSafe：该应用程序使用计算机视觉和机器学习算法来监视和检测驾驶员是否疲倦或分心，同时使用两台独立的摄像机跟踪道路状况。本文提供了一些细节和结果：CarSafe：驾驶员安全应用程序，可在智能手机上使用双摄像头检测危险驾驶行为\niOnRoad：这是一款使用Qualcomm FastCV移动优化计算机视觉库的移动驾驶辅助系统应用程序。它使用智能手机的本机相机和传感器来执行各种功能。该应用程序具有先进的功能，如前方碰撞警告，车道偏离警告，车头监控和汽车定位器。\nJumio：用于在线和移动签出的实时信用卡扫描和验证应用程序。他们还在许多国家提供护照和执照的身份证明。\n令人兴奋的算法\nHOG特征+线性SVM对物体检测非常有用。\n基于部件的HOG + SVM\n基于范例的HOG + SVM\nRANSAC（RANdom SAmple Consensus） – 简单/强大/鲁棒\n高维数据是存在低维结构内。\n最优随机RANSAC\n与PROSAC匹配 – 渐进样本共识\n霍夫变换算法\n基于KD森林的近似最近邻算法\n马尔可夫随机场\n2D图像拼接，图像挖掘，带有SIFT算法的纹理对象的三维重建\nSURF\nViola-Jones：人脸检测\n形状上下文\n可变形零件模型\n同时定位和映射 Simultaneous localization and mapping\n其他\n工作机会\nCVPR 招聘职位\nhttp://www.computervisiononline.com/jobs\n加入LinkedIn并查看图像处理或计算机视觉兴趣小组。\nAdobe的高级技术实验室http://www.adobe.com/technology/ …\n数据集\n点击这里\n数据集汇总\n跟踪视频\n网络上有太多…… Google。\n软件\n我的清单\nhttp://www.computervisiononline.com/software\nhttp://www.computer-vision-software.com/blog/\n截止日期\n活动\n日历\n有用的网站\n谷歌学术\n顶级刊物\nGoogle学术搜索可以告诉你更多关于研究人员的信息。\nGoogle Scholar可以告诉你更多关于论文的信息\n微软学术研究\n您可以查看某个领域排序的顶级关键人物\n您可以在一个领域获得顶级会议和期刊\n你可以知道关于人的引用来了解工作质量。如果某人有100个和100个引用，看起来每个作品都被1个人使用。另一方面，如果引用10000，则平均被100个作品引用。第二个有更强大的参考价值。\nhttp://www.scopus.com/\nhttp://wokinfo.com/products_tools/analytical/jcr/\nhttp://www.computervisiononline.com\nhttp://www.computervisioncentral.com/\nhttp://computervision.wikia.com\nAd-hocks\nICCV Marr奖\n计算机视觉和商业应用\nImageNet挑战\nPASCAL挑战\nImageworld用于发布计算机视觉，图像分析和医学图像分析领域的全球事件和学术工作机会。\n机器人比赛\n什么是Deep Learning仍然无法解决的一些计算机视觉任务？\nAwesome Computer Vision\nAwesome Deep Vision\nEmails Digest in Vision\n链接\n学习计算机视觉需要了解哪些数学知识？"}
{"content2":"人工智能产业生态：应用层（服务机器人、智能家居、金融、安防、智能驾驶）、技术层（自然语言处理、计算机视觉、语音处理、图谱/os/平台）和基础层（AI芯片、传感器）。\n人工智能市场规模增速、“AI+金融”机遇与挑战并存（金融行业信息业建设起步较早，极其重视IT标准化规范化；金融业务有了大量数据积累，数据处理上急需自动化、智能化解放人力；金融普惠化、场景化需要人工智能驱动。技术的界限难以界定，市场也有较强不确定性，AI方面揉入面临较多不确定因素）。\n人工智能技术条件：1、运算速度足够快；2、数据足够丰富；3、算法突破性进展。\n目前，传统金融机构运用人工智能进行科技项目实施所面临的现实问题：\n（1）、人才储备落后：AI抢人大战，高薪、互联网企业流向。\n（2）、合作外包困境：大中型传统软件商AI能力疲倦；初创型公司有活力、但合作风险高；互联网巨头2B经验不足，甲方思维。\n（3）、技术与场景的结合能力堪忧：由于不了解技术，场景想象空间有限。\n常规的金融信息化三步走：电子化、在线化、智能化。电子化借助在线化向智能化实现跨越式发展。\n流程岗、操作岗、审批岗等环节被机器自动化技术替换或简化。未来打造“哑铃”状。面向客户专属服务、人与人情感的维系；强大的产品、数据、风控、研发等团队。\n哑铃状中间这一层，过去很多在于流程和操作，其中的问题包括审批流程太慢，这都要靠技术来解决，另一方面，这也代表业界必须回头探究金融科技的本质为何？定位于智慧型中台的建设，运用包括OCR识别、图像识别、自然语言理解、智能语音、人脸识别等大量机器自动化技术，对中台的审批、流程、操作等人工环节进行简化或替代。\n有温度的电子客服》听懂人话的电话客服》APP金融小助手。其中温度的电子客服：优化传统机械式问答、支持开放非业务问题、支持多轮交互会话、语境内主动营销/服务、丰富回复形式：表情、语言。听懂人话的电话客服：语言解析+语义理解，自然语言交互替代按键菜单，快读导航，业务服务集成。计算基础模型：机器学习ML、深度学习DL；语音+文字+生物认证；高频繁琐功能简易化、语境内主动营销、业务服务集成。连接营销、连接功能、连接产品流程。\n选址模型框架：建立模型》特征计算》协变量引入》回归分析》假设检验，当然假设检验之后的流程节点为：建立模型，环环相扣。\n设计分析模型包括：商业银行选址分析、区域金融潜力分析、城市区域竞争力分析、区域网点布局优化四个方面。\n智能验票功能改变人工录入方式（缺点：下载发票清单、拍照上传、人工差错处理、T+N日员工报账）升级为：图像智能识别->云端校验、自动录入->员工即可报账。\n文本分类QA问答信息抽取实现中文分词、词性标注、语法分析、语义分析。\n行为数据成为价值创造的最大洼地；数据安全是业务发展的新动力；智慧行动是价值创造的核心引擎；数据智能是业务转型的最大变量。\n智慧行动：   行动更智能、行动更便利，起关键作用，起桥梁作用，是实现数据智能，包括：行动更实时、行动更大规模（每个客群、每个产品、每个网点、每个渠道）。\n客户洞察--分析建模--决策智能化--客群策略--活动发起（审批等流程自动化）--分组对照策略制定--渠道接触策略制定（审批渠道发起自动化流程）--流程自动化--活动执行--跟踪评估（优化决策，返回前面节点），这些关键流程都是数据智能起了核心作用，其中包括：客户洞察、深度标签、预测模型、营销个性化、one2one产品推荐、AI驱动个性化服务、优化与学习、效果归因、冠军挑战者。\n目标：活动准备时间减少45%、活动评估优化时间减少60%、活动审批时间减少55%、一键式活动覆盖量70%、精准活动吞吐量增加15倍。\n规模个性化实时行动逐步成为金融机构价值创造的分水岭。实时决策平台包括算法链、策略选择。模型数量从数十到百万级；模型训练时间从数天到数分钟；模型上线部署从天到随时；从单一模型到算法链。\n空间、时间、人本、物体（数据连接、数据增强、数据智能）：线下业务如何重获新生？客户管理如何转型成功？产品创新如何胜人一筹？营销如何快人一步？"}
{"content2":"人工智能应用之客服机器人的打造\n人工智能技术是2018年互联网上谈论最多的话题之一。那么关于人工智能技术的应用话题层出不穷，比如大数据、计算机视觉、爬虫、自然语音处理、无人汽车等等。今天张佬狮想和大家谈一下关于自然语音处理方面的应用。\n自言语言处理是研究计算机如何能识别人类的语音的一种技术。这种技术广泛采用机械学习和深度学习的相关知识，小伙伴们可能觉得这些术语和技术离我有太遥远，不过没有关系，有一句话叫做：我们可以站在巨人的肩膀上，我承认，我们可以四个字是我加上去的。\n自言语言处理是研究计算机如何能识别人类的语音的一种技术。这种技术广泛采用机械学习和深度学习的相关知识，小伙伴们可能觉得这些术语和技术离我有太遥远，不过没有关系，有一句话叫做：我们可以站在巨人的肩膀上，我承认，我们可以四个字是我加上去的。\n我们今天的项目谈的是人工智能客服机器人的打造，我们采用了“欧拉蜜”公司的自然语言处理API，网址是：http://cn.olami.ai/open/website/home/home_show\n“欧拉蜜”公司的自然语言处理API提供了很多大众化的功能，比如，数学计算、查询天气、讲笑话、讲故事、查询日期、翻译、做菜等等很多模块，这些模块都是在后台利用大数据进行处理得来的。\n在上面的网址中，注册一个账号，申请成为开发者，然后你就会拿到两个数据，一个是App Key，另一个是App Secret，这两个数据就是我们在其他平台上调用“欧拉蜜”公司的自然语言处理API的身份证。\n有了这个身份证，我们就可以自用的调用相关的语义处理技术，这是我们业务的核心呀，接下来，我们要解决前端问题。前端的方案有很多种，比如有Web端、小程序和手机APP等，由于小程序近两年火的一塌糊涂，主要是由于小程序的不需要安装、点开即用，关闭即走的特性，所以我们就紧跟时代发展步伐，采用小程序作为机器人的前端，关于小程序的开发，这又是另一个坑了，这个帖子不详细说明了。\n有了这个身份证，我们就可以自用的调用相关的语义处理技术，这是我们业务的核心呀，接下来，我们要解决前端问题。前端的方案有很多种，比如有Web端、小程序和手机APP等，由于小程序近两年火的一塌糊涂，主要是由于小程序的不需要安装、点开即用，关闭即走的特性，所以我们就紧跟时代发展步伐，采用小程序作为机器人的前端，关于小程序的开发，这又是另一个坑了，这个帖子不详细说明了。\n感兴趣的小伙伴们可以扫描下面的二维码，体验一下这个机器人，当前，机器人的功能是API提供的如数学计算、查询天气、讲笑话、讲故事、查询日期、翻译、做菜等功能，后期，我会根据具体使用场景对机器人进行定制化，让她能够完成某些人工客服能做的工作，如果想学习下这个机器人具体是这么做出来的，和所有程序源码，也可以扫描小程序二维码，进入到小程序里，点击：商务合作，就可以联系到张佬狮了。\n下面是小程序二维码：\n!"}
{"content2":"目前，计算机视觉是深度学习领域最热门的研究领域之一。计算机视觉实际上是一个跨领域的交叉学科，包括计算机科学（图形、算法、理论、系统、体系结构），数学（信息检索、机器学习），工程学（机器人、语音、自然语言处理、图像处理），物理学（光学 ），生物学（神经科学）和心理学（认知科学）等等。许多科学家认为，计算机视觉为人工智能的发展开拓了道路。\n那么什么是计算机视觉呢？ 这里给出了几个比较严谨的定义：\n✦ “对图像中的客观对象构建明确而有意义的描述”（Ballard＆Brown，1982）\n✦ “从一个或多个数字图像中计算三维世界的特性”（Trucco＆Verri，1998）\n✦ “基于感知图像做出对客观对象和场景有用的决策”（Sockman＆Shapiro，2001）\n▌为什么要学习计算机视觉？\n一个显而易见的答案就是，这个研究领域已经衍生出了一大批快速成长的、有实际作用的应用，例如：\n人脸识别： Snapchat 和 Facebook 使用人脸检测算法来识别人脸。\n图像检索：Google Images 使用基于内容的查询来搜索相关图片，算法分析查询图像中的内容并根据最佳匹配内容返回结果。\n游戏和控制：使用立体视觉较为成功的游戏应用产品是：微软 Kinect。\n监测：用于监测可疑行为的监视摄像头遍布于各大公共场所中。\n生物识别技术：指纹、虹膜和人脸匹配仍然是生物识别领域的一些常用方法。\n智能汽车：计算机视觉仍然是检测交通标志、灯光和其他视觉特征的主要信息来源。\n视觉识别是计算机视觉的关键组成部分，如图像分类、定位和检测。神经网络和深度学习的最新进展极大地推动了这些最先进的视觉识别系统的发展。在本文中，我将分享 5 种主要的计算机视觉技术，并介绍几种基于计算机视觉技术的深度学习模型与应用。\n▌1 、图像分类\n﻿\n﻿\n给定一组各自被标记为单一类别的图像，我们对一组新的测试图像的类别进行预测，并测量预测的准确性结果，这就是图像分类问题。图像分类问题需要面临以下几个挑战☟☟☟：\n视点变化，尺度变化，类内变化，图像变形，图像遮挡，照明条件和背景杂斑\n我们怎样来编写一个图像分类算法呢？\n计算机视觉研究人员提出了一种基于数据驱动的方法。\n该算法并不是直接在代码中指定每个感兴趣的图像类别，而是为计算机每个图像类别都提供许多示例，然后设计一个学习算法，查看这些示例并学习每个类别的视觉外观。也就是说，首先积累一个带有标记图像的训练集，然后将其输入到计算机中，由计算机来处理这些数据。\n因此，可以按照下面的步骤来分解：\n输入是由 N 个图像组成的训练集，共有 K 个类别，每个图像都被标记为其中一个类别。\n然后，使用该训练集训练一个分类器，来学习每个类别的外部特征。\n最后，预测一组新图像的类标签，评估分类器的性能，我们用分类器预测的类别标签与其真实的类别标签进行比较。\n目前较为流行的图像分类架构是卷积神经网络（CNN）——将图像送入网络，然后网络对图像数据进行分类。卷积神经网络从输入“扫描仪”开始，该输入“扫描仪”也不会一次性解析所有的训练数据。比如输入一个大小为 100*100 的图像，你也不需要一个有 10,000 个节点的网络层。相反，你只需要创建一个大小为 10 *10 的扫描输入层，扫描图像的前 10*10 个像素。然后，扫描仪向右移动一个像素，再扫描下一个 10 *10 的像素，这就是滑动窗口。\n﻿\n输入数据被送入卷积层，而不是普通层。每个节点只需要处理离自己最近的邻近节点，卷积层也随着扫描的深入而趋于收缩。除了卷积层之外，通常还会有池化层。池化是过滤细节的一种方法，常见的池化技术是最大池化，它用大小为 2*2 的矩阵传递拥有最多特定属性的像素。\n现在，大部分图像分类技术都是在 ImageNet 数据集上训练的， ImageNet 数据集中包含了约 120 万张高分辨率训练图像。测试图像没有初始注释（即没有分割或标签），并且算法必须产生标签来指定图像中存在哪些对象。\n现存的很多计算机视觉算法，都是被来自牛津、 INRIA 和 XRCE 等顶级的计算机视觉团队在 ImageNet 数据集上实现的。通常来说，计算机视觉系统使用复杂的多级管道，并且，早期阶段的算法都是通过优化几个参数来手动微调的。\n第一届 ImageNet 竞赛的获奖者是 Alex Krizhevsky（NIPS 2012） ，他在 Yann LeCun 开创的神经网络类型基础上，设计了一个深度卷积神经网络。该网络架构除了一些最大池化层外，还包含 7 个隐藏层，前几层是卷积层，最后两层是全连接层。在每个隐藏层内，激活函数为线性的，要比逻辑单元的训练速度更快、性能更好。除此之外，当附近的单元有更强的活动时，它还使用竞争性标准化来压制隐藏活动，这有助于强度的变化。\n﻿\n﻿就硬件要求而言， Alex 在 2 个 Nvidia GTX 580 GPU （速度超过 1000 个快速的小内核）上实现了非常高效的卷积网络。 GPU 非常适合矩阵间的乘法且有非常高的内存带宽。这使他能在一周内完成训练，并在测试时快速的从 10 个块中组合出结果。如果我们能够以足够快的速度传输状态，就可以将网络分布在多个内核上。\n随着内核越来越便宜，数据集越来越大，大型神经网络的速度要比老式计算机视觉系统更快。在这之后，已经有很多种使用卷积神经网络作为核心，并取得优秀成果的模型，如ZFNet（2013），GoogLeNet（2014）， VGGNet（2014）， RESNET（2015），DenseNet（2016）等。\n▌2 、对象检测\n﻿\n﻿\n识别图像中的对象这一任务，通常会涉及到为各个对象输出边界框和标签。这不同于分类/定位任务——对很多对象进行分类和定位，而不仅仅是对个主体对象进行分类和定位。在对象检测中，你只有 2 个对象分类类别，即对象边界框和非对象边界框。例如，在汽车检测中，你必须使用边界框检测所给定图像中的所有汽车。\n如果使用图像分类和定位图像这样的滑动窗口技术，我们则需要将卷积神经网络应用于图像上的很多不同物体上。由于卷积神经网络会将图像中的每个物体识别为对象或背景，因此我们需要在大量的位置和规模上使用卷积神经网络，但是这需要很大的计算量！\n为了解决这一问题，神经网络研究人员建议使用区域（region）这一概念，这样我们就会找到可能包含对象的“斑点”图像区域，这样运行速度就会大大提高。第一种模型是基于区域的卷积神经网络（ R-CNN ），其算法原理如下：\n在 R-CNN 中，首先使用选择性搜索算法扫描输入图像，寻找其中的可能对象，从而生成大约 2,000 个区域建议；\n然后，在这些区域建议上运行一个 卷积神网络；\n最后，将每个卷积神经网络的输出传给支持向量机（ SVM ），使用一个线性回归收紧对象的边界框。\n﻿\n实质上，我们将对象检测转换为一个图像分类问题。但是也存在这些问题：训练速度慢，需要大量的磁盘空间，推理速度也很慢。\nR-CNN 的第一个升级版本是 Fast R-CNN，通过使用了 2 次增强，大大提了检测速度：\n在建议区域之前进行特征提取，因此在整幅图像上只能运行一次卷积神经网络；\n用一个 softmax 层代替支持向量机，对用于预测的神经网络进行扩展，而不是创建一个新的模型。\n﻿\nFast R-CNN 的运行速度要比 R-CNN 快的多，因为在一幅图像上它只能训练一个 CNN 。 但是，择性搜索算法生成区域提议仍然要花费大量时间。\nFaster R-CNN 是基于深度学习对象检测的一个典型案例。\n该算法用一个快速神经网络代替了运算速度很慢的选择性搜索算法：通过插入区域提议网络（ RPN ），来预测来自特征的建议。 RPN 决定查看“哪里”，这样可以减少整个推理过程的计算量。\nRPN 快速且高效地扫描每一个位置，来评估在给定的区域内是否需要作进一步处理，其实现方式如下：通过输出 k 个边界框建议，每个边界框建议都有 2 个值——代表每个位置包含目标对象和不包含目标对象的概率。\n﻿一旦我们有了区域建议，就直接将它们送入 Fast R-CNN 。 并且，我们还添加了一个池化层、一些全连接层、一个 softmax 分类层以及一个边界框回归器。\n总之，Faster R-CNN 的速度和准确度更高。值得注意的是，虽然以后的模型在提高检测速度方面做了很多工作，但很少有模型能够大幅度的超越 Faster R-CNN 。换句话说， Faster R-CNN 可能不是最简单或最快速的目标检测方法，但仍然是性能最好的方法之一。\n近年来，主要的目标检测算法已经转向更快、更高效的检测系统。这种趋势在 You Only Look Once（YOLO），Single Shot MultiBox Detector（SSD）和基于区域的全卷积网络（ R-FCN ）算法中尤为明显，这三种算法转向在整个图像上共享计算。因此，这三种算法和上述的3种造价较高的R-CNN 技术有所不同。\n▌3 、 目标跟踪\n﻿目标跟踪，是指在特定场景跟踪某一个或多个特定感兴趣对象的过程。传统的应用就是视频和真实世界的交互，在检测到初始对象之后进行观察。现在，目标跟踪在无人驾驶领域也很重要，例如 Uber 和特斯拉等公司的无人驾驶。\n根据观察模型，目标跟踪算法可分成 2 类：生成算法和判别算法。\n生成算法使用生成模型来描述表观特征，并将重建误差最小化来搜索目标，如主成分分析算法（ PCA ）；\n判别算法用来区分物体和背景，其性能更稳健，并逐渐成为跟踪对象的主要手段（判别算法也称为 Tracking-by-Detection ，深度学习也属于这一范畴）。\n为了通过检测实现跟踪，我们检测所有帧的候选对象，并使用深度学习从候选对象中识别想要的对象。有两种可以使用的基本网络模型：堆叠自动编码器（ SAE ）和卷积神经网络（ CNN ）。\n目前，最流行的使用 SAE 进行目标跟踪的网络是 Deep Learning Tracker（DLT），它使用了离线预训练和在线微调。其过程如下：\n离线无监督预训练使用大规模自然图像数据集获得通用的目标对象表示，对堆叠去噪自动编码器进行预训练。堆叠去噪自动编码器在输入图像中添加噪声并重构原始图像，可以获得更强大的特征表述能力。\n将预训练网络的编码部分与分类器合并得到分类网络，然后使用从初始帧中获得的正负样本对网络进行微调，来区分当前的对象和背景。 DLT 使用粒子滤波作为意向模型（motion model），生成当前帧的候选块。 分类网络输出这些块的概率值，即分类的置信度，然后选择置信度最高的块作为对象。\n在模型更新中， DLT 使用有限阈值。\n﻿鉴于 CNN 在图像分类和目标检测方面的优势，它已成为计算机视觉和视觉跟踪的主流深度模型。 一般来说，大规模的卷积神经网络既可以作为分类器和跟踪器来训练。具有代表性的基于卷积神经网络的跟踪算法有全卷积网络跟踪器（ FCNT ）和多域卷积神经网络（ MD Net ）。\nFCNT 充分分析并利用了 VGG 模型中的特征映射，这是一种预先训练好的 ImageNet 数据集，并有如下效果：\n卷积神经网络特征映射可用于定位和跟踪。\n对于从背景中区分特定对象这一任务来说，很多卷积神经网络特征映射是噪音或不相关的。\n较高层捕获对象类别的语义概念，而较低层编码更多的具有区性的特征，来捕获类别内的变形。\n因此， FCNT 设计了特征选择网络，在 VGG 网络的卷积 4-3 和卷积 5-3 层上选择最相关的特征映射。 然后为避免噪音的过拟合， FCNT 还为这两个层的选择特征映射单独设计了两个额外的通道（即 SNet 和 GNet ）： GNet 捕获对象的类别信息； SNet 将该对象从具有相似外观的背景中区分出来。\n这两个网络的运作流程如下：都使用第一帧中给定的边界框进行初始化，以获取对象的映射。而对于新的帧，对其进行剪切并传输最后一帧中的感兴趣区域，该感兴趣区域是以目标对象为中心。最后，通过 SNet 和 GNet ，分类器得到两个预测热映射，而跟踪器根据是否存在干扰信息，来决定使用哪张热映射生成的跟踪结果。 FCNT 的图如下所示。\n与 FCNT 的思路不同， MD Net 使用视频的所有序列来跟踪对象的移动。上述网络使用不相关的图像数据来减少跟踪数据的训练需求，并且这种想法与跟踪有一些偏差。该视频中的一个类的对象可以是另一个视频中的背景，因此， MD Net 提出了“多域”这一概念，它能够在每个域中独立的区分对象和背景，而一个域表示一组包含相同类型对象的视频。\n如下图所示， MD Net 可分为两个部分，即 K 个特定目标分支层和共享层：每个分支包含一个具有 softmax 损失的二进制分类层，用于区分每个域中的对象和背景；共享层与所有域共享，以保证通用表示。\n﻿\n﻿近年来，深度学习研究人员尝试使用了不同的方法来适应视觉跟踪任务的特征，并且已经探索了很多方法：\n应用到诸如循环神经网络（ RNN ）和深度信念网络（DBN ）等其他网络模型；\n设计网络结构来适应视频处理和端到端学习，优化流程、结构和参数；\n或者将深度学习与传统的计算机视觉或其他领域的方法（如语言处理和语音识别）相结合。\n▌4、语义分割\n﻿\n计算机视觉的核心是分割，它将整个图像分成一个个像素组，然后对其进行标记和分类。特别地，语义分割试图在语义上理解图像中每个像素的角色（比如，识别它是汽车、摩托车还是其他的类别）。如上图所示，除了识别人、道路、汽车、树木等之外，我们还必须确定每个物体的边界。因此，与分类不同，我们需要用模型对密集的像素进行预测。\n与其他计算机视觉任务一样，卷积神经网络在分割任务上取得了巨大成功。最流行的原始方法之一是通过滑动窗口进行块分类，利用每个像素周围的图像块，对每个像素分别进行分类。但是其计算效率非常低，因为我们不能在重叠块之间重用共享特征。\n解决方案就是加州大学伯克利分校提出的全卷积网络（ FCN ），它提出了端到端的卷积神经网络体系结构，在没有任何全连接层的情况下进行密集预测。\n这种方法允许针对任何尺寸的图像生成分割映射，并且比块分类算法快得多，几乎后续所有的语义分割算法都采用了这种范式。\n﻿\n﻿但是，这也仍然存在一个问题：在原始图像分辨率上进行卷积运算非常昂贵。为了解决这个问题， FCN 在网络内部使用了下采样和上采样：下采样层被称为条纹卷积（ striped convolution ）；而上采样层被称为反卷积（ transposed convolution ）。\n尽管采用了上采样和下采样层，但由于池化期间的信息丢失， FCN 会生成比较粗糙的分割映射。 SegNet 是一种比 FCN （使用最大池化和编码解码框架）更高效的内存架构。在 SegNet  解码技术中，从更高分辨率的特征映射中引入了 shortcut/skip connections ，以改善上采样和下采样后的粗糙分割映射。\n﻿\n﻿目前的语义分割研究都依赖于完全卷积网络，如空洞卷积 ( Dilated Convolutions ），DeepLab 和 RefineNet 。\n▌5 、实例分割\n﻿除了语义分割之外，实例分割将不同类型的实例进行分类，比如用 5 种不同颜色来标记 5 辆汽车。分类任务通常来说就是识别出包含单个对象的图像是什么，但在分割实例时，我们需要执行更复杂的任务。我们会看到多个重叠物体和不同背景的复杂景象，我们不仅需要将这些不同的对象进行分类，而且还要确定对象的边界、差异和彼此之间的关系！\n到目前为止，我们已经看到了如何以多种有趣的方式使用卷积神经网络的特征，通过边界框有效定位图像中的不同对象。我们可以将这种技术进行扩展吗？也就是说，对每个对象的精确像素进行定位，而不仅仅是用边界框进行定位？  Facebook AI 则使用了 Mask R-CNN 架构对实例分割问题进行了探索。\n﻿\n﻿\n就像 Fast R-CNN 和 Faster R-CNN 一样， Mask R-CNN 的底层是鉴于 Faster R-CNN 在物体检测方面效果很好，我们是否可以将其扩展到像素级分割？\nMask R-CNN 通过向 Faster R-CNN 添加一个分支来进行像素级分割，该分支输出一个二进制掩码，该掩码表示给定像素是否为目标对象的一部分：该分支是基于卷积神经网络特征映射的全卷积网络。将给定的卷积神经网络特征映射作为输入，输出为一个矩阵，其中像素属于该对象的所有位置用 1 表示，其他位置则用 0 表示，这就是二进制掩码。\n另外，当在原始 Faster R-CNN 架构上运行且没有做任何修改时，感兴趣池化区域（ RoIPool ） 选择的特征映射区域或原始图像的区域稍微错开。由于图像分割具有像素级特性，这与边界框不同，自然会导致结果不准确。 Mas R-CNN 通过调整 RoIPool 来解决这个问题，使用感兴趣区域对齐（ Roialign ）方法使其变的更精确。本质上， RoIlign 使用双线性插值来避免舍入误差，这会导致检测和分割不准确。\n一旦生成这些掩码， Mask R-CNN 将 RoIAlign 与来自 Faster R-CNN 的分类和边界框相结合，以便进行精确的分割：\n▌结语\n上述这 5 种主要的计算机视觉技术可以协助计算机从单个或一系列图像中提取、分析和理解有用的信息。你还可以通过我的 GitHub 存储库（https://github.com/khanhnamle1994/computer-vision）获取所有的演讲幻灯片以及指南。\n近期热文\n干货 | NLP中的self-attention【自-注意力】机制\n推荐 | 机器学习中的这12条经验，希望对你有所帮助\n图解机器学习的常见算法\n利用Python实现卷积神经网络的可视化\n干货｜浅谈强化学习的方法及学习路线\n广告、商业合作\n请添加微信：guodongwe1991\n（备注：商务合作）"}
{"content2":"9月10是一年一度的教师节，中国科学院大学选择了这一天为新设立的人工智能技术学院正式揭牌。首批研究生正式注册为中国科学院大学人工智能技术学院学生。\n今年的5月28日，中国科学院大学对外发文成立人工智能技术学院，这也是国内人工智能领域首个开展教学和科研工作的新型学院。短短的几个月时间，中国科学院大学建立了学院管理组织和规章制度，开展了课程体系与课程设置方案的设定，完成了开展教学、科研师资力量的配置等工作。\n据AI科技评论了解到，此次出席揭牌仪式的是国科学院院长、国科大名誉校长白春礼院士，中国科学院副院长、国科大党委书记张杰院士，中国科学院副院长、国科大校长丁仲礼院士，中科院自动化所智能感知与计算研究中心主任、人工智能学院学术委员会主任谭铁牛院士，以及中科院自动化所所长、国科大人工智能学院院长徐波。大会仪式主持人为国科大党委常务副书记、副校长董军社。\n中科院白春礼院长（右二）、张杰副院长（右一）、丁仲礼副院长（左一）、自动化所徐波所长（左二）共同揭牌\n此外还有中国科学院办公厅副主任黄从利，中国科学院前沿科学与教育局副局长王颖，自动化所副所长、人工智能学院副院长、教学委员会主任刘成林共同出席活动，学院共建单位相关部门负责人、部分导师代表以及2017级全体新生参加了会议。\n白春礼院长等院领导集体与人工智能学院老师、首届新生合影\n徐波所长主持人工智能学院首届新生开学典礼\n徐波所长在随后举行的人工智能学院首届新生开学典礼上，感谢中科院领导及各相关部门对学院建设的关心和大力支持，介绍了学院成立的相关背景与历程，并值此第33个教师节之际，祝福自动化所和人工智能学院全体教师节日快乐！\n中科院前沿科学与教育局副局长王颖致辞\n王颖副局长对人工智能学院成立表示祝贺，期望学院能够抓住人工智能领域学科发展的历史机遇，主动谋划、把握方向，把高端人才队伍建设作为重中之重，创新探索完善教育体系，打造人才培养新模式，培养具有前瞻科学思想、通晓技术创新的纵向贯通式的复合型科技人才。\n谭铁牛院士《人工智能的发展回顾与展望》专题报告\n原中科院副院长谭铁牛院士在学院新生开学典礼上做了《人工智能的发展回顾与展望》专题报告，为首届研究生讲授开学专业第一课。报告回顾了人工智能60多年来发展历程，展望人工智能领域的前沿问题和发展趋势，结合自身长期从事人工智能研究的深刻见解，引领大家探索前沿，客观分析，正确认识，拓展视野，启发同学们认真规划即将开启的学术生活，并寄望同学们珍惜恰逢其时的大好机遇，志存高远，刻苦学习，努力为加快建设创新型国家和世界科技强国、实现“两个一百年”奋斗目标和中华民族伟大复兴中国梦提供强大支撑。\n办学体制\n根据中国科学院大学“科教融合”的办学体制，人工智能学院由中科院自动化所承办，联合中科院计算所、软件所、声学所、沈阳自动化所、深圳先进院、数学与系统科学院、重庆绿色智能研究院等单位共同建设，旨在打造我国人工智能领域首个全面开展教学和科研工作的新型科教融合学院。\n刘成林副院长介绍学院教学与课程设计情况\n刘成林副院长代表学院教学委员会报告了学院教学与课程设计总体情况。他介绍说，人工智能学院课程体系结合该领域学科前沿和重大应用需求，涵盖了智能领域主要研究方向，体现基础理论、关键技术、重大应用相结合的特点。在设置上以控制科学与工程一级学科基础性知识为准设立核心课，以人工智能学科前沿领域系统理论和技术设置专业普及课，以前沿科学和技术设置专业研讨课。面向人工智能发展态势和人才现状，学院还将围绕产业态势，联合学术界、产业界开设前沿系列讲座、国际学术前沿暑期课程等。\n办学目标\n当前，人工智能发展进入新阶段，国务院发布了《新一代人工智能发展规划》，明确指出要建设人工智能学科，完善人工智能领域学科布局，鼓励高校、科研院所与企业等机构合作开展人工智能学科建设。\n丁仲礼校长指出：\n人工智能是引领中国未来的战略性技术，是新一轮产业变革的核心驱动力。人工智能的发展离不开人才的支撑。中科院作为我国在科学技术方面的最高学术机构和全国自然科学与高新技术的综合研究与发展中心，有责任和义务承担起培养人工智能人才的重大历史使命。\n具体的办学宗旨是：顺应时代发展，面向国际学科前沿与社会发展需求，聚焦人工智能领域核心科学和关键技术，汇聚国内人工智能领域的优质资源，以中国科学院大学为核心、研究所深度参与，建设国家级的智能科技教育平台，努力形成科研、教育、创业、产业深度融合，创新型人才培养与技术应用型人才培养互补，专业化培育与定制型培育结合的教育体系。\n乐投网评论：随着人工智能发展进入新阶段，国务院发布《新一代人工智能发展规划》之后，开设人工智能专业、培养越来越多的AI人才已是当前迫切所需。目前一些AI公司，为解决人才需求，已经开始在企业内部实行导师培养，像专注于计算机视觉领域的商汤科技，团队中就有18名教授，120多名博士，形成了导师带学生机制。但是还有很多企业没有这么多，这么好的人力资源，只能依靠学校输送人才。中国科学院大学开了个好头，相信不久会有越来越多高校开展类似专业。如果实施顺利的话，未来三至五年，AI人才紧缺的现状将会大大改善。"}
{"content2":"导语：本文内容来自图森互联科技首席科学家王乃岩在雷锋网硬创公开课的分享。\n编者按：本文内容来自图森互联科技首席科学家王乃岩在雷锋网(公众号：雷锋网)硬创公开课的分享，由雷锋网旗下栏目“新智驾”整理。\n本期公开课要点：\n1、在自动驾驶场景中使用到的具体计算机视觉技术；\n2、如何利用计算机视觉打造低成本自动驾驶方案。\n计算机视觉如何“观察”世界\n无人驾驶技术链非常长，基本分为三大阶段：感知、决策和控制。计算机视觉技术在无人驾驶的感知阶段，我个人总结下来，包括五方面的应用场景和技术。\n第一，使用双目视觉系统获取场景中的深度信息。它可以帮助我们进行后续的图像语义理解，在无人驾驶中可以帮我们探索可行驶区域和目标障碍物。\n上面两张图片分别对应双目成像系统中的左眼和右眼，图片下半部分是根据左眼和右眼的图像，估算出的一个深度信息。\n我们可以把双目成像系统看作是一个廉价但没有那么准确的激光雷达。它的作用是得到周围场景每个像素的深度信息，这与激光雷达获取的 3D 点云数据非常像。双目的优点是成本非常便宜，但缺点也明显，探测距离可能没有激光雷达远；其次是对计算能力，其次是对计算能力要求较高。\n第二，通过一段视频来估计每一个像素的运动方向和运动速度。下图是一个例子：\n如图，一个人在打网球，这是两帧图片联系在一起进行的一个运动估计，其中亮度代表运动激烈程度，颜色代表它不同运动方向。\n这样一个运动估计的任务，在无人驾驶中是可以较好地帮助我们去估计，首先是自身的运动，这与后面我们提到的定位息息相关。除此之外，我们还可以使用它来估计周围场景中其他我们感兴趣物体的运动方向，对于后续的决策模块、路径规划模块非常有意义。\n除此之外，我们还可以根据某个感兴趣物体历史的运动轨迹，来对它后续的运动方向和轨迹进行预测，这是非常重要的一点，也是我们常提到的，对路面上移动物体意图的估计去获取它的轨迹。\n第三，对物体的检测与追踪。在无人驾驶中主要是各种车辆、行人、非机动车。这可能是大家比较熟悉的应用，在这样一个问题中也分为多种设置，比如说我们最常熟悉的是 2D下的检测，但我们同样也很关注 3D 包围框的表示方式。\n上图中，上半部分是 2D 检测，下半部分是 3D 检测。2D 检测只需要画一个平面矩形框，把它框住就可以。但在 3D 检测中，我们需要报告每个物体的中心点的位置，包括 X、Y、Z 的位置、长宽高以及朝向，这对于后续的轨迹估计以及它的意图分析非常重要。\n这种基于深度学习的物体检测方法是目前效果最好、最主流的方法。它对于传统算法来讲，大大降低了物体检测的漏检率和误检率。\n比如下图，我们可以看到在一个非常拥挤的环境下，深度学习算法是可以很准确地检测出场景中的行人。\n第四，对于整个场景的理解，最重要的两个，第一是道路线检测，其次是在道路线检测下更进一步，需要把场景中的每一个像素打成标签，这叫做场景分割或者场景解析。\n道路线检测其实是相对容易的任务，在过去的十年也有很多研究，在绝大多数正常情况下做到的结果是相当不错的。而场景分割和场景解析，相对来说是比较新的概念。\n上图中，我们给整个场景中每个像素打了标签。比如深紫色是路面，粉色是人行道，红色是行人，绿色是树木。如果我们有了每个像素的标签图，就可以根据它得到的语义信息，对避障以及路径规划等决策模块做一个非常有价值的输入和参考。\n场景分割其实是在检测任务上对周围的环境更进一步的理解。例如，目标检测任务中，我们可能只关心路面上移动的物体。但是，在场景分割中，我们不仅仅对行人、车辆和非机动车等进行一个分割，同样对背景静态障碍物，也能进行很好地感知。\n第五，是同步的地图构建和定位技术，即 SLAM （Simultaneous Localization and Mapping）技术。这是最早在机器人领域中的一项研究，在这里我们主要说的是基于视觉的传感器，即摄像头的 SLAM 技术：\n通过摄像头和其他多种廉价传感器的融合，能够递增创建周围环境地图，同时利用多种传感器提供的位置信息去实现自身位置的精确定位。\n比如，无人驾驶对于定位的要求非常高，需要达到分米级别的定位。如果我们只使用加速传感器、GPS 等，得到的精度是远远不够的。但借助视觉传感器、视觉地图，再融合这些加速传感器和 GPS 等，我们可以得到一个非常准确的定位效果。\n上图是 SLAM 自身轨迹的绘制，一条蓝色轨迹以及 3D 点云地图。如果我们离线建立一个比较准确的 3D 地图，我们只需要根据视觉标定（Visual landmark），再辅以其他传感器的验证，就可以得到对无人车位置的一个准确估计，这是 SLAM 技术在无人驾驶中的作用和地位。\n目前计算机视觉在无人驾驶上的应用有两个难点。\n第一个难点可能是精度和可靠性达不到我们的预期，但是随着这几年深度学习的快速发展，在这方面的难点会被一一攻克；\n第二个难点主要源自于传感器本身，比如说过曝和欠曝，都是非常严重和常见的问题。解决这个问题的方法就是通过传感器融合，也就是说当视觉传感器失效的时候，我们融合其他传感器，包括毫米波雷达甚至激光雷达，对周围环境进行感知，来保证它的安全。\n从商用车切入\n和绝大多数自动驾驶公司不一样，我们服务的对象是大型的客运车辆，尤其是大型的物流公司。我们希望通过人工智能与计算机视觉技术，开发一个可商业化的物流自动化解决方案：在限定路段限定条件下，可以降低驾驶员的工作量，减少物流公司运营成本和大型车辆事故率，提升运输车辆的安全。\n目前自动驾驶落地主要有三个方向：\n小型乘用车辆，以特斯拉、各大传统车企为代表；\n特种车辆：特定厂房和园区里的固定路线和货物运输车辆；\n大型商用车：比如 Otto 这样的公司；\n我们在确定商用车方向前做了很多调研，最后结论是：对于自动驾驶这项技术，最有刚需的是商用车：\n1、企业会考虑成本\n在长途运输中，尤其是超过 800 公里，为保证时效性，一辆车上大概会有 2 到 3 名司机。如果我们在限定条件下通过自动驾驶，把人力降低 1 名甚至 2 名，这能为物流企业节省 50% 的人力成本；\n自动驾驶车辆能够对整个路线进行更好的规划，驾驶习惯比普通司机好，使车辆耗油量大大降低，减少 5%-7% 的油量，这对物流企业来讲有比较大的吸引力。\n2、增加车辆的安全性\n尤其在中国环境下，大型货运车辆的安全事故非常多，一旦出现事故大多是致命的。大型车辆事故主要有两个原因：一个是超载，一个是疲劳驾驶。超载我们不能控制，但在疲劳驾驶方面，我们能通过辅助驾驶和自动驾驶技术来减少驾驶员的工作压力；\n对物流车辆来讲，他们绝大多数仓库在高速路附近，我们可能只需要让驾驶员通过人工驾驶的方式把车辆开到高速路上。在下高速之前，我们再提醒驾驶员接管驾驶权。自动驾驶技术能在长途运输中绝大多数路段，比如高速公路相对封闭、固定、确定性的路段实现高度自动驾驶。在这种情况下，驾驶员可以适当休息，增加车辆的安全性。\n简单来说，就是路线相对固定，环境更加可控，而商用车对成本更加敏感。综合这几点，这是我们通过视觉技术能够发挥作用的地方。\n打造低成本自动驾驶方案\n在无人驾驶的体系中，我们采用的是一个双目摄像头，因为成本比较低。\n但是，双目在实际生产中的问题在于：双目系统的两个摄像头需要精确标定，由于硬件机械结构的不稳定，在车辆行进的过程中，比较小的颠簸和抖动都会对两个摄像头的外参造成扰动。所以我们要实现一个相对可靠的自标定，这是双目从实验环境走到实际生产比较大的挑战。\n我们的做法是研发了一套比较可靠的双目自标定算法，在车辆行驶过程中会以一个固定的频率对双目的外参不停地进行修正。在无人驾驶中，我们采用的解决方案是以计算机视觉为主导，然后辅以其他传感器，比如说毫米波雷达、GPS、IMU 等传感器的解决方案。\n降低成本最关键的一点是如何利用算法的优势来保障安全性，这其中包含两点：\n第一，在视觉传感器自身之间不同任务和算法之间做交叉验证。我们知道没有一个算法是 100% 可靠的，任何一个单一算法都会有失效的可能。但是，如果我们同时进行多个算法之间的交叉验证，那么安全性就会得到一个大的提升。\n比如前段时间特斯拉出的致命事故，就是在白色的大卡车在转弯的时候，Mobileye 的视觉感知芯片没有检测出卡车，这是它第一重传感器失效。但是想像一下，如果我们在这个时候去做一个道路可行驶区域的分割，特斯拉一定不会把白色物体当成是可以行驶的道路，直接以巡航的速度去撞上一辆大卡车。\n这是我们说的多个任务之间的交叉验证。\n第二，多个传感器之间的交叉验证，主要是视觉传感器的输出和毫米波雷达的交叉验证。\n所以降低成本的关键，并不意味着我们牺牲安全性，降低的关键就是提升单个算法性能极限，同时对多个算法的结果做交叉验证。\n精彩问答\n问：机器视觉对光线，进动态要求很高，计算机的运算速度要求更好，据我所知，目前连最基本的 ADAS 系统的准确度都很低，除了 Mobileye 有芯片，其他还没有成熟，出现虚报误报的情况很多，如何保证无人驾驶的安全？\n答：这个其实是基于上一代计算机算法，就是传统的视觉算法。使用深度学习之后，结果会有非常大的提升，以我们目前的测试结果来看，最起码在检测任务上，计算机视觉是完全可以达到产品的需求，尤其是对于 ADAS 这样的产品完全不成问题。\n至于芯片的选择，Mobileye 有自己专用的芯片，但目前我们也可以针对这种深度学习算法考虑英伟达嵌入式 GPU 这样的平台。我们的 ADAS 系统是基于英伟达 TX1 芯片，自动驾驶平台则会基于英伟达 Drive PX2 以及后续的 Drive PX 平台去做。在这样的支撑下，我们可以做到很好的结果。\n问：目前自动驾驶技术突破是从车场突破可能性大一点，例如特斯拉、丰田还是计算机算法公司如谷歌、百度？\n答：其实单就任何一个方面，车厂或者是计算机视觉公司，能实现突破的可能性都不大，汽车产业是一个非常长的产业链，车厂是偏向底层的控制，如车辆本身的控制；而上层是算法公司控制的。其实这两部分缺哪一部分都是不可以的。\n如果自动驾驶技术在乘用车上真正能够商用化，这两者一定需要一个非常好的合作关系，目前我们也看到了这样一些联盟。我相信在不远的将来，在乘用车市场的上游和下游产业链一定会有比较好的融合。\n雷锋网原创文章，网页转载请注明来自雷锋网，署名作者和原文链接。微信转载授权，请联系雷锋网公众号（微信ID：leiphone-sz），详情见转载须知。"}
{"content2":"这是很久以前在prfans论坛上有人总结的贴子，现翻出来，a tree stucture of cv guys.\nDavid Marr\n—–>Shimon Ullman (Weizmann)\n—–>Eric Grimson (MIT)\n—–>Daniel Huttenlocher (Cornell)\n—–> Pedro Felzenszwalb (Chicago)\nThomas Binford (Stanford)\n—–>David Lowe (UBC)\n—–>Jitendra Malik (UC Berkeley)\n—–> Pietro Perona (Caltech)\n—–>Stefano Soatto (UCLA)\n—–>Fei-Fei Li (Princeton)\n—–>Jianbo Shi (UPenn)\n—–>Yizhou Yu (UIUC)\n—–>Christoph Bregler (NYU)\n—–>Serge Belongie (UCSD)\n—–>Alyosha Efros (CMU)\nAndrew Blake (Microsoft Research Cambridge)\n—–>Andrew Zisserman (Oxford)\n—–>Andrew Fitzgibbon (Microsoft Research Cambridge)\n—–>Roberto Cipolla (Cambridge)\n—–>Alan Yuille (UCLA)\n(UK这个学派的师承关系不太清楚, 这是我听说加上自己猜测的. 事实上, 几个非常优秀的researcher如Vladimir Kolmogorov虽然不是Andrew Blake的学生, 但是也属于这个学派. )\nThomas Huang (UIUC)\n—–>Yong Rui (Microsoft Research Redmond)\n—–>Nebojsa Jojic (Microsoft Research Redmond)\n—–>Ying Wu (Northwestern University)\n—–>Hai Tao (UCSC)\n—–>Yuncai Liu (SJTU)\n(Huang这个系的人太多, 而且很怪的是, UIUC的web上信息不全, 在此仅列出我知道的.)\n此外, 还有Takeo Kanade等非常有名的大牛, 囿于篇幅, 不一一列举. 从上得知, 加州派基本占了cv的半壁江山. 最近几年, 特别活跃的cv guys是\nUSA\nJitendra Malik, UC Berkeley\nPietro Perona, Caltech\nSerge Belongie, UCSD\nJianbo Shi, UPenn\nStefano Soatto, UCLA\nFei-Fei Li, Princeton\nWilliam Freeman, MIT\nTrevor Darrell, MIT\nSimon Baker, CMU\nYanxi Liu, CMU\nSongchun Zhu, UCLA\nAlan Yuille, UCLA\nYi Ma, UIUC\nMichael Black, Brown\nCarlo Tomasi, Duke\nRamin Zabih, Cornell\nShree Nayar, Columbia\nRama Chellappa, Maryland\nSteve Seitz, University of Washington\nEurope\nAndrew Zisserman, Oxford, UK\nAndrew Fitzgibbon, Microsoft Research Cambridge, UK\nRoberto Cipolla, Cambridge, UK\nJean Ponce, INRIA, France\nCordelia Schmid, INRIA, France\nBill Triggs, LEAR, France\nYair Weiss, Hebrew University, Israel\nAnat Levin, Hebrew University, Israel\nMichal Irani, Weizmann, Israel\nLuc van Gool, University of Leuven/ETH Zurich, Czechic\nChina\nSongde Ma, NLPR CASIA,\nTieniu Tan, NLPR CASIA,\nStan Z. Li, NLPR CASIA,\nHarry Shum, MSRA\nXiaoou Tang, MSRA/CUHK\nJian Sun, MSRA\nSteve Lin, MSRA\nYasuyuki Matsushita, MSRA\nZhouchen Lin, MSRA\nLong Quan, HKUST\nChi-Keung Tang, HKUST\n就写到这, 希望这些信息对大家有用.\n补充一下那个tree，一个法国大牛\nOlivier Faugeras\n—-Ponce  UIUC\n—lazebnik\n—-Zhengyou Zhang    MSR\n—-Martial Hebert  CMU\n—Songde Ma, NLPR CASIA\nMit AI lab\npoggio\n—Oliva\n—serre\nFreeman 80年代还来太原理工扶贫了\n—Y. Weiss\n—– Levin\n—Antonio Torralba （research scientist）\nTrevor Darrell\n—Grauman\n补充一下\nZisserman还有一个不错的学生\nlifeifei的合作者Rob Fergus\n按研究方向分分， 应该更合理一些。\n现在计算机视觉， 计算机图形图像， 机器学习开始融合到一起了吧。\nJ. Malik，Zhu Songchu偏segmentation;\nD. Lowe, S. Ullman, Poggio 偏于从生物视觉的启发来研究视觉；\nZisserman, Schmid, Lowe研究局部特征；\nLuc Van Goo， Long Quanl三维重建；\nPerona, Li Feife， Freeman视觉i学习， 物体分类；\n还有运动分析， 视觉跟踪，纹理分析………….\nMIT的Brain & Cognitive Science Dept和CSAIL里面聚集了一帮人，有的作low level有的作mid level to high level的。他们的工作是值得关注的。\n当然说视觉还是要从伟大的David Marr开始。Tomaso Poggio, Richard Whitman是Marr的同事，传承了其理念，一直往下走。Poggio最近几年比较重点的工作放在他那个hierarchical model上。\nT. Poggio的第一个PhD学生是Christof Koch （kLab at Caltech）。哦，顺便说一下Koch的另外一个导师是Valentino Breitenberg——同样是影响了一个时代的大人物。Koch研究重点兴趣在consciousness上，在Nature上的很多文章体现了他的\n研究思想。不过他们也做不少初级的视觉问题，诸如attention。\nKoch比较知名的弟子比如Laurent Itti和Li Feifei。\nRichard Whitman 年纪比较大了，个人不是很关注他现在做的东西。不过他所在的Perceptual Science Group，是一个非常有影响力的地方，这个组其他大家比较熟悉的老师有Aude Oliva和EH Adelson。Adelson最著名的一个事儿是色彩恒常相关的视错觉，93年发在Science上的那篇。\n关于Oliva，前面的帖子错了，她不是Poggio的学生，这家伙和Torralba是老 乡，同在法国念书，主要从心理学那条路子开始做，成名之役是hybrid image，和Antonio Torrralba一起搞的。这个Hybrid Image 其实80年代就有了，但是最开始从心理学方向上探讨，没有非常有影响力的文章。后来开始靠谱作自然图像统计，得到Gist\ntheory，当然这个illusion本身后来转投SIGGRAPH，其影响是深远的。嗯，这个和CV关系不大。\nPerceptual Science Group出了不少牛人，他们的alumni list可谓超豪华阵容：Yair Weiss, Josh Tenenbaum, Pawan Sinha, Bill Freeman……\n这一派的工作跟我比较相关，大概八卦的关系也能总结出一些。\n希望对大家有所帮助\nVan 那个组也做局部特征和物体识别分类。现在Olivier Faugeras也从三维重建转到计算认知学了。\nctozlm, 现在不研究局部特征的很少啊，因为局部特征方法克服了以前方法的很多缺陷。\n局部特征方法本就是从三维重建发展出来的。所以他们研究局部特征也就不奇怪了。\nINRIA 的 Faugeras 是 IJCV 的 主编，在欧洲  Computer Vision 届是老大，他的弟子Ayach 现在 Medical Image Analysis\n领域有超过其师之势。\n以下是计算机视觉常用的网络资源，包括实验室网页，比上学校数据库资源更精、更有启发性。排名不分先后，如果有更好的请补充，呵呵~~~\n（1）微软公司的文献：http://research.microsoft.com/research/pubs\n（2）微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry\nShum, Jian Sun, Steven Lin, Long Quan(兼职HKUST)etc.\n（3）瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/\n感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。\n（4）澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/\n（5）美国北卡大学：http://www.cs.unc.edu/~marc/\n（6）加州大学伯克利分校David A. Forsyth：http://www.cs.berkeley.edu/~daf/\n（7）CMU的视觉组：http://www.cs.cmu.edu/~cil/vision.html\n著名的有Tomasi, Kanade等，CMU不愧是美国计算机牛校，仅视觉就好猛。\n（8）法国INRIA：http://www-sop.inria.fr/odyssee/team/\n由Olivier.Faugeras领衔的牛人众多。\n（9）英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/\n(10)比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/\n(11)中国科学院自动化研究所 模式识别国家重点实验室(CASIA NLPR)： http://www.nlpr.ia.ac.cn/\nfrom: http://www.sigvc.org/bbs/thread-60-1-1.html"}
{"content2":"编者注： 更多人工智能业务方面重要的发展请关注2018年4月10-13日人工智能北京大会。\n人工智能的奇妙之处在于，它能让机器像人类一样拥有理解能力，完成智能任务。而它的难解之处在于，如何让人工智能拥有理解力，甚至让机器可以像人一样思考。让人工智能“听懂人话”，是近几年数据科学家们一直在做的努力，也收获了很多欣喜：\n为用户提供实时应答服务；\n为用户提供精确的搜索、推荐等个性化服务；\n辅助医生对患者进行综合诊疗；\n在无人驾驶、新闻传媒、在线娱乐、金融、教育等领域还将有许多令人期待的美好应用前景。\n可这些离人工智能像人的“小目标”还远着呢！尽管机器学习、深度学习、神经网络的发展推动自然语言技术的进步，但想要让机器学会理解，首先需要攻克人工智能的核心领域——自然语言处理技术（NLP）。关于它的技术痛点，应用难题，你都会在AI Conference 2018北京站 “自然语言处理与语音技术”板块中茅塞顿开。\n这里汇集了传媒/新闻、机器翻译、电信和教育行业的应用案例，先进的模型与算法,这里有我们耳熟能详的小冰，还有百度、微软、Intel、谷歌这些人工智能巨头，分享他们这些年在人工智能上翻过的山，趟过的河。\n在进入NLP的世界探究其奥秘之前，我迫不及待的要分享给你一些很有价值的内部消息：\n1. 你将会成为少数深入了解微软小冰的一员\n方向：与人工智能交互\n主题：小冰从人类与AI之间的对话中学到的经验教训\n主讲人：周力（微软中国）\n语音识别是人机交互的入口，经过四年的探索，小冰已经成为科技史上最大规模的人工智能情感计算框架系统。她当过歌手、诗人、主持人、评论员、客服，与中国、日本和美国超过1亿用户进行互动，从中学习人类特有的情感。小冰的每一次演进都让我们对机器能做什么产生了非常多的联想，这次又是什么呢？\n4月12日，微软小冰首席架构师周力博士将亲自带着小冰来到AI Conference 2018北京站，与大家分享在过去四年中研究微软小冰的感悟。希望在不久的将来，人类的生活会因为与人工智能的直接交流，变得更加美好。\n❖\n2. 使用Intel AI技术的NLP企业案例让你醍醐灌顶\n方向：模型与方法\n主题：深度学习时代的数据科学和自然语言处理\n主讲人：Yinyin Liu（Intel Nervana）\n2016年起，Intel逐渐将自己的战略重心转移到了数据科学和人工智能领域，向业界提供 AI 解决方案。最近几年主要的AI推动力是由深度学习产生的，NLP利用深度学习最新算法发展例如文档理解之类的应用，使公司能够筛查海量文本，分类并找到相关信息。\nIntel人工智能产品事业部数据科学主任Yinyin Liu将会与你讨论深度学习最新发展如何影响处理文本、语言及基于对话应用，并启发了利用数据的新方向。另外，还毫不吝啬的为大家分享一些使用Intel® AI技术的NLP企业案例。\n❖\n3. NLP落地金融了\n方向：模型与方法\n主题：使用AI来分析财务新闻的影响\n主讲人：ZhefuShi (密苏里大学)\nAI的领域在不断地进展之中，越来越多的公司认识到NLP对于金融行业分析的重要作用。在金融领域，AI技术对分析金融新闻的影响是有帮助的，将非结构化数据结构化处理，从中探寻影响市场变动的线索。比如通过历史金融新闻预测价格趋势，评估市场风险；为监管人员提供企业监管、市场监管、舆情监控；应用知识图谱和图谱计算技术来进行风险管理、供应链金融管理和投融资管理等。\n但自然语言处理技术，目前是人工智能进行场景落地时的一大难点重点。密苏里大学的Zhefu Shi博士带着自己的知识宝库与你分享如何使用AI来分析财务新闻的影响，如何提取金融实体信息并将其用于分析业务影响，敬请关注。\n❖\n4. 提升深度学习的表现有技巧\n方向：企业人工智能, 实施人工智能, 模型与方法\n主题：深度学习在文本挖掘中的应用\n主讲人：Emmanuel Ameisen (Insight DataScience), Jeremy Karnowski (Insight Data Science)\n深度学习在自然语言处理中的应用非常广泛，可以说横扫自然语言处理的各个应用，从底层的分词、语言模型、句法分析等到高层的语义理解、对话管理、知识问答等方面都几乎都有深度学习的模型，并且取得了不错的效果。多数公司已经开始利用文本数据支持部分业务运营，但也遇到了一系列挑战，其中包括如何验证和解释模型性能，以及模型复杂性如何影响部署它们的简便性。\nEmmanuel Ameisen和Jeremy Karnowski通过对google，Facebook，Amazon，Twitter，Salesforce，Airbnb等超过75个团队的对话进行分析，得到了很有价值的研究成果。分享他们如何从传统的机器学习算法转变成更有表现力的深度学习模型，如卷积神经网络和回归神经网络。这些新技术使公司能够改进许多关键业务操作，您将学习不同模型在不同项目中的应用，并了解如何选择最适合您项目的模型。\nGartner认为，未来10年，人工智能将成为最具破坏性级别的技术，主要是因为卓越的计算能力、漫无边际的数据集、深度神经网络领域的超乎寻常的进步。与其停留观望不如赶快行动，跟着大咖学习人工智能领域NLP的新知识，借鉴他们在人工智能布局的新思路。\n自然语言处理技术与语音技术\n15个议题   6大方向\n企业人工智能、模型与方法\n实施人工智能、人工智能交互\n……\n4月10-13日，AI Conference 2018北京站已经准备好了，你呢？"}
{"content2":"4月16日，北京智源人工智能研究院与中国人工智能领军企业旷视召开“智源学者计划暨联合实验室发布会”。北京市科委副主任张光连，海淀区委常委、副区长李俊杰，以及来自科技部、北京市科委、海淀区人民政府、朝阳区人民政府、中关村管委会，北京脑科学与类脑研究中心、北京量子信息科学研究院、北京生命科学研究所等科研机构，北京亦庄国际投资发展有限公司等投资机构，以及智源研究院相关发起单位等领导及代表出席会议。\n北京智源人工智能研究院是在科技部和北京市委市政府的指导和支持下，由北京市科委和海淀区人民政府推动成立，依托北京大学、清华大学、中国科学院、旷视等北京人工智能领域优势单位共建的新型研发机构。\n北京智源人工智能研究院副院长刘江介绍，自2018年11月成立以来，北京智源人工智能研究院按照《北京市支持建设世界一流新型研发机构实施办法（试行）》精神，积极探索新型科研管理等机制体制创新，贯彻国家新一代人工智能发展规划总体部署，实施“智源学者计划”，支持科学家勇闯人工智能科技前沿“无人区”，同时与北京优势高校院所和骨干企业共建联合实验室，开展跨学科、大协同的创新攻关，引领未来人工智能基础研究方向。\n北京市科委副主任张光连在讲话中指出，北京市科委在2018和2019年两年给予智源研究院共计3.4亿元的资金支持，海淀区在办公空间、人才政策等方面提供了服务保障，今后还将支持智源研究院积极争取国家科技部等部门支持，承接国家科技创新2030—“新一代人工智能”重大项目等任务。今年，北京将开放一批人工智能应用场景，推动人工智能新技术、新产品、新模式在北京率先运用，促进人工智能和实体经济深度融合。同时，还将成立智源人工智能产业培育基金，超前布局，分担企业创新风险，积极培育一批国际领先的人工智能企业。\n智源学者计划发布，着力培养国际影响力的青年学术英才\n据北京智源人工智能研究院院长黄铁军介绍，此次发布的“智源学者计划”，是智源研究院打造高层次基础研究人才队伍的引领性工程，旨在面向当前和未来人工智能创新发展，选拔并培养一批德才兼备、具有国际影响力的学科领军人才和具有发展潜力的青年学术英才，构建富有创新能力、梯队合理的人工智能基础研究创新团队，建立面向未来人工智能发展的高端人才储备，引领未来人工智能基础研究方向，推动北京人工智能产业创新发展。\n“智源学者计划”为从事人工智能基础研究的科学家营造了良好的科研环境，给予科学家充分的尊重和信任，将技术路线决策权和人财物支配权全部交于科学家，试点科技经费“包干制”，使科学家可以心无旁骛、潜心研究、创新突破。“‘智源学者计划’的目标，就是要找到最好的人，给他自由支配的经费，提供他需要的资源，支持开展人工智能领域特定方向上的重大基础问题研究，或者开展前沿问题的自由探索。”黄铁军说。\n具体来说，“智源学者计划”将依托北京大学、清华大学、中科院等优势高校院所，以及旷视等骨干企业研究院，对四类人才进行重点支持，分别是：智源科学家首席（CS）、智源研究项目经理（PM）、智源研究员（PI）和智源青年科学家。其中，智源首席科学家（CS）由领域内顶尖专家学者担任，负责研究方向的整体规划布局，并推荐智源研究员（PI）人选；智源研究项目经理（PM）则兼备技术和管理才能，协助CS管理项目，并负责多方沟通；智源研究员（PI）是所属研究领域的领军人才，开展确定领域的科研活动；智源青年科学家是38岁以下拥有博士学位、在科研上具有无限发展潜力的青年学者，并在智源研究院对支持下开展开放性、探索性研究，勇闯人工智能科技前沿“无人区”，面向未来人工智能新思维和新体系的发展提出引领性的原创基础研究成果。\n目前，经过提名、初评、审议等流程，已经遴选出首批智源学者共21人，并即将启动公示程序。其中，首批智源学者“人工智能数理基础”重大研究方向首席科学家（CS）由中国科学院院士、北京大学数学科学学院张平文教授担任，智源研究项目经理（PM）由北京大学夏壁灿教授担任，智源研究员（PI）候选人由来自北京大学、清华大学、中国科学院的10名杰出中青年学者。首批“智源青年科学家”候选人共9人，其中“人工智能数理基础”5人，“机器学习”方向4名，分布来自北大、清华、北京应用物理与计算数学研究所和旷视。其中，入选首批“智源青年科学家”候选人的旷视研究院研发总监张祥雨博士仅28岁，也是首批智源学者候选人中年龄最小的一位。\n按照计划，首批智源学者将致力于打破基于计算机实验和神经科学的人工智能的惯用建模范式，以可解释性的新型人工智能模型、新型的机器学习算法、深度学习的基础研究为研究内容，解决人工智能面临的可计算性、可解释性、泛化性、稳定性等基础理论问题，建立以数学与统计理论为第一原理的新一代人工智能方法论。\n黄铁军表示，今年将计划遴选智源学者100人，其中青年科学家30-50人。2020年和2021年再分别增加100人，智源学者总体规模保持在300人左右。\n智源与旷视共建联合实验室，建设一体化开放创新平台\n会上海淀区委常委、副区长李俊杰、旷视首席科学家孙剑教授共同为“北京智源 - 旷视智能模型设计与图像感知联合实验室”揭牌。该联合实验室拟由旷视首席科学家兼旷视研究院院长孙剑任实验室主任，研究员周舒畅任实验室副主任。\n“目前有大量的高校、研究院所、创新创业人员和行业客户，想参与到人工智能创新中，但苦于没有数据，算力或算法等资源，无法施展。”在发布会现场，孙剑一针见血指出了当前人工智能行业创新所面临的问题。\n2017年7月，国务院发布《新一代人工智能发展规划》，人工智能上升为国家战略。建立开源开放的人工智能开放创新平台，服务企业应用技术开发成为当前该领域的难点。国家也希望龙头企业通过打造人工智能开源开放创新平台，调动产学研，社会各类创新创业主体参与共同打造我国自主知识产权的产业生态，提升国际影响力，促进行业整体发展。\n同时，北京市作为第一个国家新一代人工智能创新发展试验区，也希望能够充分调动北京技术、人才方面优势，通过组织机制创新，依托龙头企业牵引进行多主体协同创新，打造我国乃至全球技术高地。\n“北京智源 - 旷视智能模型设计与图像感知联合实验室”也因此孕育而生。孙剑表示，旷视与智源通过联手打造数据集和建设联合实验室，来推动整个行业协同创新发展，建设共性技术开放创新平台，构建自主可控产业生态。\n据孙剑介绍，“北京智源 - 旷视智能模型设计与图像感知联合实验室”将基于旷视在计算机视觉技术和应用的深厚积累，通过3年时间，建设一体化的数据共享、模型设计与场景测试的开放创新平台，突破大数据背景下模型架构设计、优化和部署等技术，探索突破视觉系统性能极限和模型设计及场景测试的自动化，推进大数据背景下模型架构设计、优化和部署等方面的研究。\n数据方面，实验室将立足旷视算法落地场景丰富、数据积累深厚的优势，以开源数据集、预抽取特征等形式，持续开放通用物体分类、物体检测追踪、人像人形识别属性等视觉问题数据，助力相关领域研究。针对高搜索效率、高准确率、高灵活性等关键指标，该实验室还将开展支持大计算量模型的新神经网络架构搜索算法（NAS）研究，构建一站式深度模型自动化设计平台等工作。\n模型设计优化方面，平台将提供自动化模型搜索、错误分析与可视化工具，加速大模型架构探索和优化，降低面向应用的模型设计门槛，来解决传统模型架构人工设计方法的经验性强，迭代周期长的问题。同时，实验室还将积极探索低位宽神经网络相关技术，研究实现对于大计算量模型的云上和终端两级的硬件支持，完成从模型搜索到实际应用的垂直整合。\n场景验证方面，实验室将通过建设人脸抓拍识别、结构化检测、大规模人像比对等验证场景和提供自动分析报告服务，为研究算法调优提供环境，并积极探索低位宽等网络压缩技术，研发高效硬件部署方案，推动模型研究成果迅速投入实用。针对计算机视觉算法与真实场景适配的问题，实验室还将建设可重现的实景测试环境，并提供自动化的量化错误分析报告，帮助算法迭代。\n孙剑表示，实验室平台建成后，将面向高校院所、创业企业等创新创业主体开放，致力推动图像感知与计算领域的协同创新。“通过以上措施，实验室将被打造成为一个开放、贴近实际场景的计算机视觉算法研究实验平台，形成涵盖数据、平台、场景的完整闭环，有利于打造我国自主可控的技术和产业生态，共同助力我国图像感知和计算领域的技术及应用达到国际先进水平。”\n北京智源人工智能研究副院长唐杰表示，北京智源联合实验室是智源研究院面向人工智能未来发展，围绕重大应用需求，联合各大高校、科研院所、优势企业等机构进行全方位科研合作的平台。其目标是高效整合大学、科研院所、企业的数据、平台、场景等优势资源，形成具有国际领先水平的人工智能研究团队，培育国际顶尖AI人才，推动人工智能产学研协同创新发展，产出系列国际领先水平的突破性成果，并将重点组织数据共享，支持算法开源，推动场景开放，实施协同创新。智源联合实验室分为两类，分别是与高校院所共建、与优势企业共建。共建单位将提供配套资金、场地、设备等条件保障，解决实验室建设与运行中的有关问题。\n在联合实验室的建设上，智源研究院遵循“成熟一个、启动一个”的原则，择优支持共建，最多可连续支持三年。唐杰强调，“北京智源 - 旷视智能模型设计与图像感知联合实验室” 已通过立项论证，也是智源联合实验室首个成功落地的项目。\n全球最大的物体检测数据集发布，首批开放超过60万张图像和1000万标注框\n会上，旷视研究院联合北京智源人工智能研究院发布全球最大的物体检测数据集——Objects365。旷视研究院院长孙剑说，该数据集也是新一代通用物体检测数据集，具有规模大、质量高、泛化能力强的特点。\n规模方面，Objects365定义了生活中常见的365个类别，第一批将开放63万张图像，1000万的标注框。开放图片数是COCO的5倍，标注框超过COCO的11倍。\n算法优化的上限严重依赖于基准数据集术的质量。为保证标注质量，在打造Objects365时，旷视设计出一套科学而严格的标注流程，每一张图片的背后至少会经过9名标注工人之手。\n此外，作为一个优秀的预训练数据集，Objects365预训练模型在使用过程中，可以轻松超越现有算法的精度，显著加速收敛过程，表现出极强的泛化能力。在执行COCO、VOO Det、CityPersons等检测任务时，在VOC Seg和ADE等分割任务上均有显著提升。\nDIW2019挑战赛即将启动，共同探讨检测问题的瓶颈及优化方向\nDetection In the Wild 2019（DIW 2019）挑战赛也发布会当天正式启动。据孙剑介绍，该挑战赛是基于2019年CVPR的研讨会，由旷视联合北京智源人工智能研究院举办，其目的是为了推动目标检测技术的发展，改善现有目标检测数据集的类别覆盖不全、标注精度不高、密集场景缺少等问题。挑战赛于5月10日开放测试集，6月10日结果提交截止，6月10日公布比赛结果，并邀请优胜队介绍比赛经验。主办方为每个赛道的冠军队伍准备了10000美元的奖励届时，优胜队将在CVPR现场的研讨会上介绍经验，共同探讨检测问题的瓶颈及优化方向。\nDIW2019挑战赛的比赛赛道分为三个赛道，即Objects365赛道、Objects365小赛道和CrowdHuman赛道。\n其中，为探索目标检测系统的瓶颈，参加Objects365赛道的选手将利用公开的365种类别，60万张图片超过1000万个框的完整训练集对检测模型进行训练；在3万张图片构成的验证集上调试算法，并在10万张图片构成的测试集上进行最终的挑战。\n为了降低参赛门槛，加快算法迭代速度，研究长尾类别检测问题，Objects365小赛道则从Objects365数据集中挑选出65个类别，选手可以用1万张图片进行模型的训练。\nCrowdHuman赛道则是为了解决现实生活中的遮挡问题，其算法的提升将会推动人体检测算法落地。届时，选手将基于专门为了密集场景人体检测设计的CrowdHuman数据集进行训练，数据集包含有丰富标注信息和多种场景。"}
{"content2":"这是\n阿拉灯神丁Vicky\n的第\n014\n篇文章\n计算机视觉是一门研究让机器看见世界的学科，就是让摄像头与电脑结合成为计算机的视觉系统，对目标物体进行识别，追踪，与推理。\n计算机视觉技术主要帮助计算机从一系列图像中感知，识别和理解有用信息。其应用领域非常广泛，包括但不限于人脸识别，图详检索，安防监控，生物识别，智能汽车等。\n首先，来看一下人眼的视觉过程，视觉从发光源开始，光的模式通过场景的物体反射进入视觉感受器官的左右眼睛并同时作用于视网膜上引起视觉感觉。\n图片来源于网络\n视网膜是含有光感受器官和神经阻滞网络的薄膜，光刺激在视网膜上经神经处理产生的神经冲动沿视神经纤维传出眼睛，通过视觉神经传出大脑皮层进行处理并最终引起知觉，或者说在大脑中刺激对光刺激产生影响，形成关于场景的表象，大脑皮层要完成一系列处理工作，从图像存储直到根据图像做出响应。\n图片来源于网络：计算机视觉整体框架\n在计算机视觉种主要研究的三维场景中的对象，包括人，物，环境三大块，针对三维场景中的三维信息（几何，表现，行为）处理可分为3个研究方向：\n第一：获取和建模：\n有效的从现实世界获取三维信息，构建，编辑和处理不同的三维信息在\n，既包含物体的三维信息的几何\n，也包含材质，关照，人体建模与动作捕捉等等。\n第二：理解与认知：\n。识别，分析，抽取三维信息中对应的语义与\n，机器视觉等方向，如三维物体识别，图像检索，场景识别，姿态识别，人脸识别等等\n第三：模拟＆交互：\n处理与模拟不同三维对象之间的\n，包含流体模拟，物理仿真，图像绘制，人体动画，人脸动画等\n图片来源：微软亚洲研究院\n计算机视觉主要有两种研究方式：\n1，仿生学方式：\n以人类视觉系统原理为模型，构建各个模块完成类似的功能和\n。\n2，工程学方式：\n也是以人类视觉系统原理为模型，但并不是模仿人类视觉系统，仅考虑系统的输入输出，系统内部以现有的任何可行的\n。\n用先简单易懂的方式\n讲述\n计算机视觉的构建步骤：\n1，图像分类\n根据现实世界对物体界定，将图像中的反应不同特征，不同类目的物体分解出来的图像处理方法。如下图，计算机视觉会对场景中的物体进行区分。要想让计算机识别图像中的物体，我们就要教会他如何去辨别，现在对于计算机视觉的训练完全依赖于数据驱动，为其提供大量人为标注图像样本，作为训练集让机器学习。\n图像分类的具体实操过程可分为：\n如所示，输入训练集，包含Ñ个图像，用ķ个不同的类别对头像进行标注;\n2，用训练集对分类器进行分类;\n3，让分类器预测测试集来评估分类器效果;\n图片来源于网络\n2，目标检测\n目标检测也叫目标提取，对复杂场景中的多个目标进行处理，提取识别并进行图像分割。通常涉及目标的编辑框与标签的输出，例如对下图中饮料，餐具与菜品的检测，必须使用边界框对图像中的所有物品进行界定以及名称标注。\n图片来源于网络\n3，目标追踪\n根据给定的初始图像或视频初始帧的目标大小和位置，预测或追踪该目标的大小和位置目标追踪有着广泛的应用如：视频监控，人机交互，无人驾驶等。\n图片来源于网络\n目标追踪的实操步骤可分为：\n1，初始化目标框;\n2，在下一帧中产生众多候选框，提取这些候选 框的特征;\n3，对候选框评分;\n如所示，找到评分最高的候选框作为预测目标，并对多个预测值融合找到更优的预测目标;\n根据上面的额实操步骤，把目标追踪划分为5项主要的研究内容\n如所示，运动模型：如何产生众多的候选样本;\n2，特征提取：如何利用何种特征找到目标;\n3，观测模型：如何为候选样本进行评分;\n如所示，模型更新：如何更新观测模型使其适应目标的变化;\n5，集成方法：如何融合多个决策样本获得一个更优的决策结果;\n4，语义分割\n在语义理解上对图像中每个像素进行分割，然后对其进行标记分类。语义分割分为标准语义分割和实例感知语义分割，标准语义分割也称全像素语义分割是将每个像素分类为属于对象类的过程，实例感知语义分割是标准语义分割或全像素语义分割的子类型，它将每个像素分类为属于对象类及该类的实体。\n图片来源于网络\n语义分割试图在语义上理解图像中每个像素的作用，上图中，除了要识别道路，汽车，房屋，路灯之外，还要描绘每个物体的边界。\n5，实例分割\n可以说实例分割是物体检测和语义分割的结合体，相对于物体检测其可以精确到物体的边缘;相对于语义分割其可以标注图上的同一类别中的不同个体。\n图片来源于网络\n6，图像描述\n图像描述是将自然语言处理与计算机视觉的结合体，生成最适合当前图像的文字描述。\n图片来源于网络\n再来梳理一下机器视觉相邻领域的关系\n视觉计算机\n主要研究三维场景的重建，主要针对场景图像上的内容;\n处理图像\n研究的的英文二维图像的转化，主要是像素级上的操作，例如提高图像的对比度，边缘提取，去噪声，几何变换及图像旋转，这些主要是图像的处理与图像\n不大;\n视觉机器\n主要指工业领域的视觉研究，例如机器人的视觉，常用于检测和研究。通过机器视觉与控制论处理紧密\n。\n模式识别\n的英文使用各种方法从信号中提取信息，在图像识别领域主要是从图像\n。\n计算机视觉是一项综合技术，包括图像处理技术，机械工程技术，控制技术，光源照明技术，光学成像技术，传感器技术，数字和模拟视频技术，计算机硬件技术，人机硬件结构技术等等，这些技术的相互协调，才能构成一个完整的计算机视觉系统。\n截止目前还没有，还不能在任意环境中识别任意物体，现有技术能很好的解决特定的目标领域的识别，比如简单的图像识别，人脸识别，印刷，手写文件识别，车辆识别，并需要在特定的环境中，具有指定的光照，背景和目标姿态要求的状态下。"}
{"content2":"部分归纳来自 @雨诺寒雪，@ZJU_fish1996，@lutos，感谢大佬们一起造福后人 (。・∀・)ノ゛\n· 计算机视觉\n计算机视觉的首要目标是用图像创建或恢复现实世界模型，然后认知现实世界\n中心任务就是对图像进行理解\n· 格式塔法则\n格式塔理论的出发点是形，任何形都是知觉进行了积极组织或构造的结果和功能，而不是客体本身就有的\n-\n1. Law of Proximity\n接近的物体容易被感知成同一组\n-\n2. Law of Similarity\n人们将相似的物体感知成同一组的部分\n-\n3. Law of Common Fate\n人们将以相同的速度和（或）方向运动的物体感知成一个组的部分\n-\n4. Law of Symmetry\n互相对称的元素容易被感知为统一的组\n-\n5. Law of Continuity\n人们会把一些离散的物体看成是连续的\n-\n6. Law of Closure\n人们会把不完整的个体看成是一个整体的形状\n· Marr视觉表示框架的三个阶段\nPrimal Sketch：\n将输入的原始图像进行处理，抽取图像中诸如角点，边缘，纹理，线条，边界等基本特征，这些特征的集合称为基元图\n-\n2.5D Sketch：\n在以观测者为中心的坐标系中，由输入图像和基元图恢复场景可见部分的深度，法线方向，轮廓等，这些信息称为二维半图\n-\n3D Model：\n在以物体为中心的坐标系中，由输入图像，基元图，二维半图来恢复，表示和识别三维物体\n· 二值图像\n· 几何特性\n-\n1. 尺寸和位置\n1.1 面积（零阶矩）：表示前景的像素点个数\n1.2 区域中心（一阶矩）：\n-\n2. 方向\n某些形状（如圆）是没有方向的。假定物体是长形的，长轴方向为物体的方向\n2.1 求方向：\n实际转化成一个最小化问题，需要找到一条满足以下条件的直线的方向：所有的前景像素点到该直线的距离之和最小\n2.2 最小二乘法拟合求解：\n-\n3. 伸长率，密度集，形态比\n3.1 伸长率：图像短轴方向度量与长轴方向度量之比\n3.2 密集度：\n给定周长，密集度越高则围成的面积就越大：\n圆>正方形>长方形\n3.3 形态比：区域的最小外接矩形的长与宽之比\n3.4 欧拉数\n又叫亏格数，定义为联通分量数减去洞数，例如：\n-\n· 投影计算\n定义：给定一条直线，用垂直该直线的一簇等间距直线将一幅二值图像分割成若干条，每一条内像素值为1的像素的数量\n-\n· 连通区域\n概念：四联通和八联通\n联通分量的定义：联通像素的集合\n联通分量标记算法：\n递归算法：\n1. 扫描图像，找到没有标记的一个前景点，分配标记L\n2. 递归分配标记L给该点的邻点\n3. 如果不存在没标记的点，则停止\n4. 返回第1步\n序贯算法（四联通）：\n1. 从左到右，从上到下扫描图像\n2. 如果像素点值为1，则分四种情况：\n- 2.1 如果上面点和左面点有且仅有一个标记，则复制这一标记\n- 2.2 如果两点有相同标记，则复制这一标记\n- 2.3 如果两点有不同标记，则复制上点的标记且将这两个标记输入等价表中作为等价标记\n- 2.4 否则给这一像素点分配一个新的标记并将这一标记输入等价表\n3. 如果需要考虑更多的点，则返回2\n4. 在等价表的每一等价集中找到最低的标记\n5. 扫描图像，用等价表中的最低标记取代每一标记\n区域边界跟踪算法：\n· 边缘\n· 模版卷积\n给一个图像与一个模板，会计算卷积结果\n· 四种最主要的不连续：\n1. 表面法向量的不连续\n2. 深度的不连续\n3. 表面颜色的不连续\n4. 光照的不连续\n· 边缘检测的基本思想：\n检测灰度不连续的地方\n-\n· 基于一阶的边缘检测：\n一阶导数的极大值为边界点\n实际运算中用差分近似微分：\n种类有：\n1. Roberts交叉算子\n2. Sobel算子\n3. Prewitt算子：运算较快\n-\n· 基于二阶的边缘检测：\n图像灰度二阶导数的过零点对应边缘点\n种类有：\n1. Laplacian算子\n2. LoG算子（Marr&Hildreth算子）：高斯滤波 + 拉普拉斯边缘检测\n- 为什么要加高斯滤波：平滑去噪和边缘检测是一对矛盾，应用高斯函数的一阶导数，可以在两者之间获得一个较好的平衡\n-\n· Canny边缘检测\n1. 用高斯滤波器平滑图像\n2. 用一阶偏导有限差分计算梯度幅值和方向\n3. 对梯度幅值进行非极大值抑制 NMS：\n先将所有梯度角离散为圆周的八个扇区之一，然后去掉不比梯度线方向上的两个相邻点幅值大的点\n4. 用双阈值算法检测和连接边缘\n为什么需要非极大值抑制：\n可以进一步消除非边缘的噪点，更重要的是可以细化边缘，将模糊的边界变得清晰\n双阈值算法：\n阈值太低：假边缘\n阈值太高：部分轮廓丢失\n· 局部特征 Local Feature\n· Harris角点检测\n原理：给定包围一个点的窗口，在任何方向上移动窗口，强度都会发生较大变化，即为角点\n公式推导：\nHarris算法是利用窗口内图像的灰度自相关性进行的，设定一个窗口并在图像中移动，计算移动前与移动后窗口所在的区域图像的自相关系数\nλ1和λ2是M的特征向量\nHarris角点有旋转不变性，但没有尺度不变性：旋转不变性是因为我们取的是局部最大。尺度放大以后一个原来的角点会被检测成edge\n设计一个尺度不变函数，不同尺度下的图片找到的区域是相同的：\n1. 在多尺度检测关键点\n2. 然后找上下不同尺度的局部最大值\n3. 消除低于阈值的点\n-\n· SIFT描述子的计算（得到128维的\n计算的基本步骤：\n1. 用16*16的窗口放在特征点附近 + 将16*16分成16个4*4的窗口\n2. 计算窗口中每个像素的边的方向（梯度角减去90°）\n3. 丢掉方向能量小的边（使用阈值）\n4. 用直方图描述结果 + 将每个小窗口中的所有的方向离散成8个方向，一共16*8=128个\n为什么只使用梯度信息：\n因为梯度信息可以表示边缘信息，并且在光照变化时有抵抗能力\n如何实现旋转不变的：\n因为我们找的是对应位置的参考方向而非绝对方向\n· 尺度不变的原理\n因为在使用高斯模糊的不同尺度（如图像金字塔）处重新采样图像，仅当在两个尺度之间观察到最大值时才将梯度存储为描述符\n· 曲线\n· Hough变换直线检测：\nHough变换是基于投票原理的参数估计方法，是一种重要的形状检测技术\n基本思想：图像中的每一点对参数组合进行表决，赢得多数票的参数组合为胜者（结果）\n步骤：\n1. 适当地量化参数空间\n2. 假定参数空间每一个单元都是一个累加器，把累加器初始化为0\n3. 对图像空间的每一点，在其所满足的参数方程对应的累加器上加1\n4. 累加器阵列最大值对应模型的参数\n· 图像频域\n· 图像的傅里叶变换\n理解变换的基本含义：\n傅里叶变换讲的是任何信号（如图像信号）都可以表示成一系列正弦信号的叠加，在图像领域就是将图像brightness variation 作为正弦变量。傅里叶变化储存每一个频率的magnitude和phase信息，前者代表这个频率上有多少信号，后者间接可代表空间信息。\n理解图像的低频成分与高频成分：\n图像的高频部分是像素值变化剧烈的地方，如图像的边缘和轮廓。低频部分是变化不剧烈的地方，对应大的色块。我们从近处看图像看到的是高频信息，而远处看到的是低频信息\n-\n· 怎么理解拉普拉斯金字塔的每一层是带通滤波？\n拉普拉斯金字塔是通过源图像减去先缩小再放大的一系列图像构成的。下采样的时候丢失了高频信息，而相邻金字塔相减的时候丢失了低频信息，因此只有中间频段的信息保留了下来\n· 相机模型\n· 理解：景深，光圈，焦距，视场\n景深：相机镜头能够取得清晰图像的成像所测定的被摄物体前后范围距离\n光圈：镜头中，用来控制光线透过镜头，进入机身内感光面光量的装置\n焦距：从镜片中心到底片等成像平面的距离\n视场：摄像头能够观察到的最大范围\n-\n光圈对景深的影响？\n较小的光圈对应较大的景深（但是较小的光圈限制了光线的进入，因此需要增加曝光）\n焦距对视场的影响？\n焦距越短，视场角越大，放大倍率越小，拍摄范围越大，拍摄画面中的人越小\n-\n· 理想的针孔相机模型\n基本投影：\n齐次坐标形式下的透视投影公式：\n负号表示倒立\n有哪几个内参：\n会写内参矩阵：\n对角线和竖着的分别代表什么含义？\n分别代表焦距对应的单位像素和中心点的偏移\n-\n· 畸变\n径向畸变与切向畸变各是什么原因引起的？\n径向畸变：\n不完美的镜头，镜片的不理想，镜头的几何形状，光圈位置\n↑ 概括成透镜几何性质和孔径位置\n远离透镜中心的地方比靠近中心的地方更加弯曲\n如何解决：\n切向畸变：\n由于CMOS等感光元件摆放倾斜，没有平行于图像平面\n越靠近中间，畸变越小\n如何解决：\n径向畸变常见的有哪两种？\n枕形畸变：\n桶形畸变：\n畸变参数：\n-\n· 外参有哪几个？分别代表什么含义？（每拍一张照片就有一个外参\n外参用于模型空间到相机空间的变化，三个参数是平移参数，三个参数是旋转参数\n-\n· 内参，外参，畸变参数在成像各阶段中的角色（三维物体到真实图像的过程\n1. 第一步是从世界坐标系转为相机坐标系，这一步是从三维点到三维点的转换，包括R，t等参数（相机外参）\n2. 第二步是从相机坐标系转为成像平面坐标系（像素坐标系），这一步是三维点到二维点的转换，包括K等参数（相机内参）\n3. 最后再用到畸变参数\n· 相机标定\n· 相机标定需要求解哪些参数？\n畸变参数，内参，外参\n· 基于Pattern/Reference Object的相机标定\n已知什么？求解什么？\n已知：给定标定物体的N个角点，K个视角（棋盘格子两个点可以得出四个等式）\n求解：所有的参数。N个点K个视角可以列出2NK个等式，会带来6*K+4个参数。每次会变的是外参，而内参和畸变参数是不变的，所以只需要2NK>6*K+4即可\n简述其基本过程哪几个步骤？\n1. 获取标定物体网格的角点在坐标系的位置\n2. 找到图片的角点\n3. 根据图像空间坐标系到世界坐标系列出等式\n4. 求解相机参数\n即：拍照 - 找特征点 - 特征点对应 - 计算公式和参数\n· 立体视觉\n· 立体视觉的三角测量基本原理\n假设有两个焦距相同的完美相机，假设我们知道3维空间点在2维图像上的具体位置\n会画视差disparity的那张图：\n推导上图公式：\n要知道：相机的位置，点的对应关系（真实环境中的点到平面上）\n视差：xl-xr，表示左右两摄像头成像的距离\nZ的结果误差主要在分母（视差）那里。视差小的时候，视差的误差会对Z产生很大的影响。此外T越小，误差越小\nT越大，看到的范围越小（因为是取两眼图像的交叉部分）\n· 立体视觉的基本步骤\n1. 恢复失真，消除畸变\n2. 矫正相机，使图像在同一个平面上\n3. 在两张图中找到对应的相同特征\n4. 三角测量\n· 三维数据获取\n· 结构光成像系统的构成：\n一个结构光三维成像系统主要由三个部分组成：结构光投影仪（一台或多台），CCD相机（一台或多台），以及深度信息重建系统\n· 利用结构光获取三维数据的基本原理\n公式可以由三角形相似推出来（公式不要弄混了！）\n· 三维获取：Kinect\nkinect的原理是单目结构光\n-\n· ICP算法的作用及其基本步骤\nICP：迭代最近点方法（用于多个摄像机的配准问题，即把多个扫描结果拼接在一起形成对扫描对象的完整描述）\n基本步骤：\n给定两个三维点集X与Y，将Y配准到X：\n1. 计算Y中的每一个点在X中的对应最近点\n2. 求使上述对应点对的平均距离最小的刚体变换，获得刚体变换参数（平移参数和旋转参数）\n3. 对Y应用上一步求得的刚体变换（平移和旋转），更新Y\n4. 如果X与Y的对应点对平均距离大于阈值，回到1，否则停止计算\n· 光流\n· 光流解决的是什么问题？\n给定两张图像H和I，评估从H到I的像素运动，解决的是像素对应的问题\n· 光流的三个基本假设是什么？\n亮度一致，空间一致，移动小\n· 一个点的约束公式会推导\n· 哪些位置的光流比较可靠，为什么\n纹理复杂区域，梯度比较大且方向不同，求出来的特征值比较大\n· 图像拼接\n· 图像拼接的基本步骤\n1. 检测关键点\n2. 计算SIFT描述子\n3. 匹配SIFT描述子\n4. 计算变换矩阵，进行仿射变换\n5. RANSAC\n6. 图像混合（边缘平滑过渡）\n-\n· RANSAC：随机抽样一致算法\n采用迭代的方式从一组包含outliers的数据中估算数学模型的参数。核心思想是随机性和假设性，随机性是根据正确数据出现的概率去随机选取抽样数据，假设性是假设选取出的抽样数据都是正确数据。\n优点：是大范围模型匹配问题的一个普遍意义上的方法，且运用简单，计算快。\n缺点：只能计算outliers不多的情况（投票机制可以解决outliers高的情况）\n基本步骤：\n1. 随机选择一些点作为样本\n2. 计算选出的样本应该使用的变换矩阵\n3. 把刚才没有选中的点代入建立的模型，判断有多少点符合，误差是否小于阈值\n4. 比较匹配数量是否为当前最优解，如果是，则更新当前最优集，并更新迭代次数\n5. 重复多次，如果迭代次数大于K（k由最优的inliers的点集计算得到），则退出，否则迭代次数+1\n· 人脸识别\n· 主成分分析PCA\n基本思想：选择一个新的坐标系统进行线性降维，第一轴是数据集的投影的方差最大的方向，第二轴是数据集投影的方差第二大方向\n作用：降维和去噪\n优化目标函数的推导：\n-\n· Eigenface\n是什么：所有人脸图像展开后构成的大矩阵的协方差矩阵的特征向量\n人脸识别方法的基本步骤：\n1. 获得人脸图像训练集\n2. 对人脸图像归一化处理\n3. PCA计算获取一组特征向量\n4. 将每幅图像投影到特征向量的空间中，得到新的空间的坐标\n5. 对输入的待测试图像，归一化后投影到特征人脸空间中，然后用距离度量来描述两幅人脸图像的相似性\n将重构用于人脸检测的原理：\n人脸投影到特征人脸空间中，保留了主要特征的信息，所以可以恢复人脸本来的样子\n· 物体识别\n· Visual Recognition\n基本任务大概可以分为哪几大类？\n1. 图片分类\n2. 检测和定位物体/图片分割\n3. 估计语义和几何属性\n4. 对人类活动和事件进行分类\n都有哪些挑战因素？\n视角变换\n光线变化\n尺度变化\n物体形变\n物体遮挡\n背景凌乱\n内部类别多样\n· 基于词袋的物体分类：\n图像的BoW（bag-of-words）是指什么意思？\n图像中的单词被定义为一个图像块的特征向量，图像的Bow模型即图像中所有图像块的特征向量得到的直方图\n几个基本步骤\n1. 特征提取和表示（grid），每个特征为一个质点\n2. 通过对质点聚类建立字典（k 聚类），得到k个聚类中心，聚类中心就是词袋中的单词，所有聚类中心就是特征直方图的基\n3. 将图片用直方图的基表示出来，这样就可以得到关于图片的特征直方图。该直方图与单词的顺序没有关系，而是每个单词在图片中出现的频率\n4. 将新的图片获取质点，然后映射到直方图上进行聚类\n· 物体识别-CNN\n· 卷积神经网络：\n卷积层：提取特征，用了共享参数\n池化层：缩小（如max pooling，每四个像素点只取最小的那个），让它只专注于关注某一小块\n缩小 - 卷积 - 缩小 - 卷积……\n-\n卷积神经网络和多层感知机最大的差别是什么？\n引入了卷积层和池化层\n要会计算卷积层的连接数和权重数：\n· 图像分割\n基于K means聚类的图像分割\n先表示，再聚类\n理解用聚类进行图像分割的基本原理\n给定一幅图像，能描述如何用K means进行分割的算法基本步骤\nmean shift图像分割的思路\n空间分布密集程度\n*· 最后再看看这门课的老师叫什么名字吧，万一考到了呢 ′∀`)"}
{"content2":"做工程第一步，确定任务，标注数据；\n下图很好的展示了图像识别的四种任务:\n1. 图像分类(Image classification)\n如图(a), 给定一张输入图像，图像分类的任务是判断该图像属于哪类, 如果是多任务分类, 可以用于分类该图像包含哪个类别。\n所以该类任务的标注非常简单, 只需要标注图片的种类即可. 如果是多任务的, 只需要多标注几种图片是否包含某类物品即可;\n2. 目标检测(Object localization)\n如图(b), 目标定位是在图像分类的基础上, 进一步判断图像中的目标具体在图像的什么位置, 通常是以包围目标的矩形框(bounding box)形式展示. 在目标定位中，通常只有一个或固定数目的目标，而目标检测更一般化，其图像中出现的目标种类和数目都不定。\n所以该任务的标注比图像分类多了一些, 首先需要标注图片中包含目标的bbox, 然后标注目标种类; 一张图片可能会有多个bbox.\n3. 语义分割(Semantic segmentation)\n如图(c), 语义分割是目标检测更进阶的任务，目标检测只需要框出每个目标的包围盒，语义分割需要进一步判断图像中哪些像素属于哪个目标。但是，语义分割不区分属于相同类别的不同实例。例如，当图像中有多只猫时，语义分割会将两只猫整体的所有像素预测为“猫”这个类别。\n所以该类任务的标注要求会比bbox的矩形框更精准一些, 需要沿着物体的外轮廓做外切多边形; 这种标注虽然比目标检测略微繁琐, 但是这种标注的数据也能用于目标检测, 因为可以根据外切多边形求得bbox, 然后用于目标检测; 所以现在的图像目标标注多是采用这种外切多边形的标注方法.\n4. 实例分割(Instance segmentation)\n如图(d), 与此不同的是，实例分割需要区分出哪些像素属于第一只猫、哪些像素属于第二只猫。此外，目标跟踪通常是用于视频数据，和目标检测有密切的联系，同时要利用帧之间的时序关系。\n所以该类任务的标注与语义分割要求繁琐一些, 需要标注同类样本的不同个体.\n5. 常用的图片标注工具\n常用的标注工具有\nlabelImg： VOC数据集采用的标注工具，github上有很多VOC数据集的解析代码，所以很好用；对于面向github编程的我们来说, 很好使;\nvia: 标注为csv文件；\nlabelme: python标注工具模块，为json文件；\n参考文章链接：\nhttps://www.zhihu.com/question/36500536/answer/304469552"}
{"content2":"会议：\n三大顶级\nICCV：International Conference on Computer Vision，国际计算机视觉大会\nCVPR：International Conference on Computer Vision and Pattern Recognition，国际计算机视觉与模式识别大会\nECCV：European Conference on Computer Vision，欧洲计算机视觉大会\n较好\nICIP：International Conference on Image Processing，国际图像处理大会\nBMVC：British Machine Vision Conference，英国机器视觉大会\nICPR：International Conference on Pattern Recognition，国际模式识别大会\nACCV：Asian Conference on Computer Vision，亚洲计算机视觉大会\nICCV的全称是International Comference on Computer Vision，正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂。\nCVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster调自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。\n期刊：\n顶级\nPAMI：IEEE TRANS.on Patt.Analysis and Machine Intelligence国际计算机视觉大会\nLICV:Inter.Jour.on Comp.Vision\n较好\nCVIU:Computer Vision and Image Understanding\nPR：Pattern Reco.\nA.I.\nAAAI: American Association for Artificial Intelligence\nACM/SigIR: 这个是IR方面的，可能DB/AI的人都有\nIJCAI: International Joint Conference on Artificial Intelligence\nNIPS: Neural Information Processing Systems\nICML: International Conference on Machine Learning\nJour.:\nMachine Learning\nNEURAL COMPUTATION: 这个的影响因子在AI里最高，2000年为1.921\nARTIFICIAL INTELLIGENCE: 1.683(2000年的数据，下同)\nPAMI: 1.668\nIEEE TRANSACTIONS ON FUZZY SYSTEMS: 1.597\nIEEE TRANSACTIONS ON NEURAL NETWORKS: 1.395\nAI MAGAZINE: 1.044\nNEURAL NETWORKS: 1.019\nPATTERN RECOGNITION: 0.781\nIMAGE AND VISION COMPUTING: 0.616\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING: 0.465\nAPPLIED INTELLIGENCE: 0.268\n部分ai刊物影响因子05 SCI IF\n2005 2004\nJMLR 4.027 5.952\nPAMI 3.810 4.352\nTOIS 4.529 4.097\nAIJ 2.638 3.570\nMLJ 3.108 3.258\nECJ 1.568 3.206\nTEvC 3.257 3.688\nIJCV 3.657 2.914\nDMKD 2.105 2.800\nNCJ 2.591 2.364\nTNN 2.205 2.178\nPR 2.153 2.176\nAlife 1.857 2.150\nJASIST 1.583 2.086\nJAIR 2.247 2.045\nCIJ 0.850 1.923\nNNJ 1.665 1.736\nTFS 1.701 1.373\nTKDE 1.758 1.243\nIRJ 2.036 1.231\nAIIM 1.882 1.124\nTSMCB 1.108 1.052\nAICom 0.612 0.738\nAIRev 0.868 0.562\n————————————————————————————————————————————————————————\n搜集了一些在计算机视觉领域比较有名的数据库、期刊、会议，分享给大家：（大牛们有补充的就在评论区补充一下，我会加上去。）\n1、Elsevier (ScienceDirect OnSite,SDOL)\nhttp://www.sciencedirect.com/\n2、IEEE/IEE Electronic Library\nhttp://ieeexplore.ieee.org/\n3、SpringerLink 科技期刊\nhttp://link.springer.com/\n4、ICCV（IEEE International Conference on Computer Vision）\nhttp://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000149\n5、ICCVR\nhttp://ieeexplore.ieee.org/xpl/conferences.jsp?queryText=ICCVW\n6、CVPR(IEEE Conference on Computer Vision and Pattern Recognition)\nhttp://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1001809\n7、CVPRW\nhttp://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6230822\n8、PAMI（IEEE Transactions on Pattern Analysis and Machine Intelligence）\nhttp://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34\n9、IJCV\nhttp://link.springer.com/journal/11263\n10、ECCV\nhttp://link.springer.com/search?query=ECCV\n11、TIP\nhttp://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83\n12、CVPapers：\nhttp://www.cvpapers.com/index.html关于ICCV，CVPR，ECCV的一个总结"}
{"content2":"计算机视觉分享（2017年 腊八）\n介绍一篇论文：旨在介绍深度学习在计算机视觉领域四大基本任务中的应用，包括分类(图a)、定位、检测(图b)、语义分割(图c)、和实例分割(图d)。\n作者简介\n张皓：南京大学计算机系机器学习与数据挖掘所（LAMDA）硕士生，研究方向为计算机视觉和机器学习，特别是视觉识别和深度学习。\n个人主页http://lamda.nju.edu.cn/zhangh/。\n论文下载：https://pan.baidu.com/s/1mjJi0YW\n图像分类(image classification)\n给定一张输入图像，图像分类任务旨在判断该图像所属类别。\n(1) 图像分类常用数据集\n以下是几种常用分类数据集，难度依次递增。http://rodrigob.github.io/are_we_there_yet/build/列举了各算法在各数据集上的性能排名。\n• MNIST 60k训练图像、10k测试图像、10个类别、图像大小1×28×28、内容是0-9手写数字。\n• CIFAR-10 50k训练图像、10k测试图像、10个类别、图像大小3×32×32。\n• CIFAR-100 50k训练图像、10k测试图像、100个类别、图像大小3×32×32。\n• ImageNet 1.2M训练图像、50k验证图像、1k个类别。2017年及之前，每年会举行基于ImageNet数据集的ILSVRC竞赛，这相当于计算机视觉界奥林匹克。\n(2) 图像分类经典网络结构\n基本架构 我们用conv代表卷积层、bn代表批量归一层、pool代表汇合层。最常见的网络结构顺序是conv -> bn -> relu -> pool，其中卷积层用于提取特征、汇合层用于减少空间大小。随着网络深度的进行，图像的空间大小将越来越小，而通道数会越来越大。\n针对你的任务，如何设计网络？ 当面对你的实际任务时，如果你的目标是解决该任务而不是发明新算法，那么不要试图自己设计全新的网络结构，也不要试图从零复现现有的网络结构。找已经公开的实现和预训练模型进行微调。去掉最后一个全连接层和对应softmax，加上对应你任务的全连接层和softmax，再固定住前面的层，只训练你加的部分。如果你的训练数据比较多，那么可以多微调几层，甚至微调所有层。\n• LeNet-5 60k参数。网络基本架构为：conv1 (6) -> pool1 -> conv2 (16) -> pool2 -> fc3 (120) -> fc4 (84) -> fc5 (10) -> softmax。括号中的数字代表通道数，网络名称中有5表示它有5层conv/fc层。当时，LeNet-5被成功用于ATM以对支票中的手写数字进行识别。LeNet取名源自其作者姓LeCun。\n• AlexNet 60M参数，ILSVRC 2012的冠军网络。网络基本架构为：conv1 (96) -> pool1 -> conv2 (256) -> pool2 -> conv3 (384) -> conv4 (384) -> conv5 (256) -> pool5 -> fc6 (4096) -> fc7 (4096) -> fc8 (1000) -> softmax。AlexNet有着和LeNet-5相似网络结构，但更深、有更多参数。conv1使用11×11的滤波器、步长为4使空间大小迅速减小(227×227 -> 55×55)。AlexNet的关键点是：(1). 使用了ReLU激活函数，使之有更好的梯度特性、训练更快。(2). 使用了随机失活(dropout)。(3). 大量使用数据扩充技术。AlexNet的意义在于它以高出第二名10%的性能取得了当年ILSVRC竞赛的冠军，这使人们意识到卷机神经网络的优势。此外，AlexNet也使人们意识到可以利用GPU加速卷积神经网络训练。AlexNet取名源自其作者名Alex。\n• VGG-16/VGG-19 138M参数，ILSVRC 2014的亚军网络。VGG-16的基本架构为：conv1^2 (64) -> pool1 -> conv2^2 (128) -> pool2 -> conv3^3 (256) -> pool3 -> conv4^3 (512) -> pool4 -> conv5^3 (512) -> pool5 -> fc6 (4096) -> fc7 (4096) -> fc8 (1000) -> softmax。 ^3代表重复3次。VGG网络的关键点是：(1). 结构简单，只有3×3卷积和2×2汇合两种配置，并且重复堆叠相同的模块组合。卷积层不改变空间大小，每经过一次汇合层，空间大小减半。(2). 参数量大，而且大部分的参数集中在全连接层中。网络名称中有16表示它有16层conv/fc层。(3). 合适的网络初始化和使用批量归一(batch normalization)层对训练深层网络很重要。VGG-19结构类似于VGG-16，有略好于VGG-16的性能，但VGG-19需要消耗更大的资源，因此实际中VGG-16使用得更多。由于VGG-16网络结构十分简单，并且很适合迁移学习，因此至今VGG-16仍在广泛使用。VGG-16和VGG-19取名源自作者所处研究组名(Visual Geometry Group)。\n• GoogLeNet 5M参数，ILSVRC 2014的冠军网络。GoogLeNet试图回答在设计网络时究竟应该选多大尺寸的卷积、或者应该选汇合层。其提出了Inception模块，同时用1×1、3×3、5×5卷积和3×3汇合，并保留所有结果。网络基本架构为：conv1 (64) -> pool1 -> conv2^2 (64, 192) -> pool2 -> inc3 (256, 480) -> pool3 -> inc4^5 (512, 512, 512, 528, 832) -> pool4 -> inc5^2 (832, 1024) -> pool5 -> fc (1000)。GoogLeNet的关键点是：(1). 多分支分别处理，并级联结果。(2). 为了降低计算量，用了1×1卷积降维。GoogLeNet使用了全局平均汇合替代全连接层，使网络参数大幅减少。GoogLeNet取名源自作者所处单位(Google)，其中L大写是为了向LeNet致敬，而Inception的名字来源于盗梦空间中的”we need to go deeper”梗。\n• Inception v3/v4 在GoogLeNet的基础上进一步降低参数。其和GoogLeNet有相似的Inception模块，但将7×7和5×5卷积分解成若干等效3×3卷积，并在网络中后部分把3×3卷积分解为1×3和3×1卷积。这使得在相似的网络参数下网络可以部署到42层。此外，Inception v3使用了批量归一层。Inception v3是GoogLeNet计算量的2.5倍，而错误率较后者下降了3%。Inception v4在Inception模块基础上结合了residual模块(见下文)，进一步降低了0.4%的错误率。\n• ResNet ILSVRC 2015的冠军网络。ResNet旨在解决网络加深后训练难度增大的现象。其提出了residual模块，包含两个3×3卷积和一个短路连接(左图)。短路连接可以有效缓解反向传播时由于深度过深导致的梯度消失现象，这使得网络加深之后性能不会变差。短路连接是深度学习又一重要思想，除计算机视觉外，短路连接也被用到了机器翻译、语音识别/合成领域。此外，具有短路连接的ResNet可以看作是许多不同深度而共享参数的网络的集成，网络数目随层数指数增加。ResNet的关键点是：(1). 使用短路连接，使训练深层网络更容易，并且重复堆叠相同的模块组合。(2). ResNet大量使用了批量归一层。(3). 对于很深的网络(超过50层)，ResNet使用了更高效的瓶颈(bottleneck)结构(下图右)。ResNet在ImageNet上取得了超过人的准确率。\n下表对比了上述几种网络结构。\n• preResNet ResNet的改进。preResNet整了residual模块中各层的顺序。相比经典residual模块(a)，(b)将BN共享会更加影响信息的短路传播，使网络更难训练、性能也更差；(c)直接将ReLU移到BN后会使该分支的输出始终非负，使网络表示能力下降；(d)将ReLU提前解决了(e)的非负问题，但ReLU无法享受BN的效果；(e)将ReLU和BN都提前解决了(d)的问题。preResNet的短路连接(e)能更加直接的传递信息，进而取得了比ResNet更好的性能。\n• ResNeXt ResNet的另一改进。传统的方法通常是靠加深或加宽网络来提升性能，但计算开销也会随之增加。ResNeXt旨在不改变模型复杂度的情况下提升性能。受精简而高效的Inception模块启发，ResNeXt将ResNet中非短路那一分支变为多个分支。和Inception不同的是，每个分支的结构都相同。ResNeXt的关键点是：(1). 沿用ResNet的短路连接，并且重复堆叠相同的模块组合。(2). 多分支分别处理。(3). 使用1×1卷积降低计算量。其综合了ResNet和Inception的优点。此外，ResNeXt巧妙地利用分组卷积进行实现。ResNeXt发现，增加分支数是比加深或加宽更有效地提升网络性能的方式。ResNeXt的命名旨在说明这是下一代(next)的ResNet。\n• 随机深度 ResNet的改进。旨在缓解梯度消失和加速训练。类似于随机失活(dropout)，其以一定概率随机将residual模块失活。失活的模块直接由短路分支输出，而不经过有参数的分支。在测试时，前馈经过全部模块。随机深度说明residual模块是有信息冗余的。\n• DenseNet 其目的也是避免梯度消失。和residual模块不同，dense模块中任意两层之间均有短路连接。也就是说，每一层的输入通过级联(concatenation)包含了之前所有层的结果，即包含由低到高所有层次的特征。和之前方法不同的是，DenseNet中卷积层的滤波器数很少。DenseNet只用ResNet一半的参数即可达到ResNet的性能。实现方面，作者在大会报告指出，直接将输出级联会占用很大GPU存储。后来，通过共享存储，可以在相同的GPU存储资源下训练更深的DenseNet。但由于有些中间结果需要重复计算，该实现会增加训练时间。\n目标定位(object localization)\n在图像分类的基础上，我们还想知道图像中的目标具体在图像的什么位置，通常是以包围盒的(bounding box)形式。\n基本思路\n多任务学习，网络带有两个输出分支。一个分支用于做图像分类，即全连接+softmax判断目标类别，和单纯图像分类区别在于这里还另外需要一个“背景”类。另一个分支用于判断目标位置，即完成回归任务输出四个数字标记包围盒位置(例如中心点横纵坐标和包围盒长宽)，该分支输出结果只有在分类分支判断不为“背景”时才使用。\n人体位姿定位/人脸定位\n目标定位的思路也可以用于人体位姿定位或人脸定位。这两者都需要我们对一系列的人体关节或人脸关键点进行回归。\n弱监督定位\n由于目标定位是相对比较简单的任务，近期的研究热点是在只有标记信息的条件下进行目标定位。其基本思路是从卷积结果中找到一些较高响应的显著性区域，认为这个区域对应图像中的目标。\n目标检测(object detection)\n在目标定位中，通常只有一个或固定数目的目标，而目标检测更一般化，其图像中出现的目标种类和数目都不定。因此，目标检测是比目标定位更具挑战性的任务。\n(1) 目标检测常用数据集\n• PASCAL VOC 包含20个类别。通常是用VOC07和VOC12的trainval并集作为训练，用VOC07的测试集作为测试。\n• MS COCO COCO比VOC更困难。COCO包含80k训练图像、40k验证图像、和20k没有公开标记的测试图像(test-dev)，80个类别，平均每张.2个目标。通常是用80k训练和35k验证图像的并集作为训练，其余5k图像作为验证，20k测试图像用于线上测试。\n• mAP (mean average precision) 目标检测中的常用评价指标，计算方法如下。当预测的包围盒和真实包围盒的交并比大于某一阈值(通常为0.5)，则认为该预测正确。对每个类别，我们画出它的查准率-查全率(precision-recall)曲线，平均准确率是曲线下的面积。之后再对所有类别的平均准确率求平均，即可得到mAP，其取值为[0, 100%]。\n• 交并比(intersection over union, IoU) 算法预测的包围盒和真实包围盒交集的面积除以这两个包围盒并集的面积，取值为[0, 1]。交并比度量了算法预测的包围盒和真实包围盒的接近程度，交并比越大，两个包围盒的重叠程度越高。\n(2) 基于候选区域的目标检测算法\n基本思路\n使用不同大小的窗口在图像上滑动，在每个区域，对窗口内的区域进行目标定位。即，将每个窗口内的区域前馈网络，其分类分支用于判断该区域的类别，回归分支用于输出包围盒。基于滑动窗的目标检测动机是，尽管原图中可能包含多个目标，但滑动窗对应的图像局部区域内通常只会有一个目标(或没有)。因此，我们可以沿用目标定位的思路对窗口内区域逐个进行处理。但是，由于该方法要把图像所有区域都滑动一遍，而且滑动窗大小不一，这会带来很大的计算开销。\nR-CNN\n先利用一些非深度学习的类别无关的无监督方法，在图像中找到一些可能包含目标的候选区域。之后，对每个候选区域前馈网络，进行目标定位，即两分支(分类+回归)输出。其中，我们仍然需要回归分支的原因是，候选区域只是对包含目标区域的一个粗略的估计，我们需要有监督地利用回归分支得到更精确的包围盒预测结果。R-CNN的重要性在于当时目标检测已接近瓶颈期，而R-CNN利于在ImageNet预训练模型微调的方法一举将VOC上mAP由35.1%提升至53.7%，确定了深度学习下目标检测的基本思路。一个有趣之处是R-CNN论文开篇第一句只有两个词”Features matter.” 这点明了深度学习方法的核心。\n候选区域(region proposal)\n候选区域生成算法通常基于图像的颜色、纹理、面积、位置等合并相似的像素，最终可以得到一系列的候选矩阵区域。这些算法，如selective search或EdgeBoxes，通常只需要几秒的CPU时间，而且，一个典型的候选区域数目是2k，相比于用滑动窗把图像所有区域都滑动一遍，基于候选区域的方法十分高效。另一方面，这些候选区域生成算法的查准率(precision)一般，但查全率(recall)通常比较高，这使得我们不容易遗漏图像中的目标。\nFast R-CNN\nR-CNN的弊端是需要多次前馈网络，这使得R-CNN的运行效率不高，预测一张图像需要47秒。Fast R-CNN同样基于候选区域进行目标检测，但受SPPNet启发，在Fast R-CNN中，不同候选区域的卷积特征提取部分是共享的。也就是说，我们先将整副图像前馈网络，并提取conv5卷积特征。之后，基于候选区域生成算法的结果在卷积特征上进行采样，这一步称为兴趣区域汇合。最后，对每个候选区域，进行目标定位，即两分支(分类+回归)输出。\n兴趣区域汇合(region of interest pooling, RoI pooling)\n兴趣区域汇合旨在由任意大小的候选区域对应的局部卷积特征提取得到固定大小的特征，这是因为下一步的两分支网络由于有全连接层，需要其输入大小固定。其做法是，先将候选区域投影到卷积特征上，再把对应的卷积特征区域空间上划分成固定数目的网格(数目根据下一步网络希望的输入大小确定，例如VGGNet需要7×7的网格)，最后在每个小的网格区域内进行最大汇合，以得到固定大小的汇合结果。和经典最大汇合一致，每个通道的兴趣区域汇合是独立的。\nFaster R-CNN\nFast R-CNN测试时每张图像前馈网络只需0.2秒，但瓶颈在于提取候选区域需要2秒。Faster R-CNN不再使用现有的无监督候选区域生成算法，而利用候选区域网络从conv5特征中产生候选区域，并且将候选区域网络集成到整个网络中端到端训练。Faster R-CNN的测试时间是0.2秒，接近实时。后来有研究发现，通过使用更少的候选区域，可以在性能损失不大的条件下进一步提速。\n候选区域网络(region proposal networks, RPN) 在卷积特征上的通过两层卷积(3×3和1×1卷积)，输出两个分支。其中，一个分支用于判断每个锚盒是否包含了目标，另一个分支对每个锚盒输出候选区域的4个坐标。候选区域网络实际上延续了基于滑动窗进行目标定位的思路，不同之处在于候选区域网络在卷积特征而不是在原图上进行滑动。由于卷积特征的空间大小很小而感受野很大，即使使用3×3的滑动窗，也能对应于很大的原图区域。Faster R-CNN实际使用了3组大小(128×128、256×256、512×512)、3组长宽比(1:1、1:2、2:1)，共计9个锚盒，这里锚盒的大小已经超过conv5特征感受野的大小。对一张1000×600的图像，可以得到20k个锚盒。\n为什么要使用锚盒(anchor box)\n锚盒是预先定义形状和大小的包围盒。使用锚盒的原因包括：(1). 图像中的候选区域大小和长宽比不同，直接回归比对锚盒坐标修正训练起来更困难。(2). conv5特征感受野很大，很可能该感受野内包含了不止一个目标，使用多个锚盒可以同时对感受野内出现的多个目标进行预测。(3). 使用锚盒也可以认为这是向神经网络引入先验知识的一种方式。我们可以根据数据中包围盒通常出现的形状和大小设定一组锚盒。锚盒之间是独立的，不同的锚盒对应不同的目标，比如高瘦的锚盒对应于人，而矮胖的锚盒对应于车辆。\nR-FCN\nFaster R-CNN在RoI pooling之后，需要对每个候选区域单独进行两分支预测。R-FCN旨在使几乎所有的计算共享，以进一步加快速度。由于图像分类任务不关心目标具体在图像的位置，网络具有平移不变性。但目标检测中由于要回归出目标的位置，所以网络输出应当受目标平移的影响。为了缓和这两者的矛盾，R-FCN显式地给予深度卷积特征各通道以位置关系。在RoI汇合时，先将候选区域划分成3×3的网格，之后将不同网格对应于候选卷积特征的不同通道，最后每个网格分别进行平均汇合。R-FCN同样采用了两分支(分类+回归)输出。\n小结\n基于候选区域的目标检测算法通常需要两步：第一步是从图像中提取深度特征，第二步是对每个候选区域进行定位(包括分类和回归)。其中，第一步是图像级别计算，一张图像只需要前馈该部分网络一次，而第二步是区域级别计算，每个候选区域都分别需要前馈该部分网络一次。因此，第二步占用了整体主要的计算开销。R-CNN, Fast R-CNN, Faster R-CNN, R-FCN这些算法的演进思路是逐渐提高网络中图像级别计算的比例，同时降低区域级别计算的比例。R-CNN中几乎所有的计算都是区域级别计算，而R-FCN中几乎所有的计算都是图像级别计算。\n(3) 基于直接回归的目标检测算法\n基本思路\n基于候选区域的方法由于有两步操作，虽然检测性能比较好，但速度上离实时仍有一些差距。基于直接回归的方法不需要候选区域，直接输出分类/回归结果。这类方法由于图像只需前馈网络一次，速度通常更快，可以达到实时。\nYOLO\n将图像划分成7×7的网格，其中图像中的真实目标被其划分到目标中心所在的网格及其最接近的锚盒。对每个网格区域，网络需要预测：每个锚盒包含目标的概率(不包含目标时应为0，否则为锚盒和真实包围盒的IoU)、每个锚盒的4个坐标、该网格的类别概率分布。每个锚盒的类别概率分布等于每个锚盒包含目标的概率乘以该网格的类别概率分布。相比基于候选区域的方法，YOLO需要预测包含目标的概率的原因是，图像中大部分的区域不包含目标，而训练时只有目标存在时才对坐标和类别概率分布进行更新。\nYOLO的优点在于：(1). 基于候选区域的方法的感受野是图像中的局部区域，而YOLO可以利用整张图像的信息。(2). 有更好的泛化能力。\nYOLO的局限在于：(1). 不能很好处理网格中目标数超过预设固定值，或网格中有多个目标同时属于一个锚盒的情况。(2). 对小目标的检测能力不够好。(3). 对不常见长宽比的包围盒的检测能力不强。(4). 计算损失时没有考虑包围盒大小。大的包围盒中的小偏移和小的包围盒中的小偏移应有不同的影响。\nSSD\n相比YOLO，SSD在卷积特征后加了若干卷积层以减小特征空间大小，并通过综合多层卷积层的检测结果以检测不同大小的目标。此外，类似于Faster R-CNN的RPN，SSD使用3×3卷积取代了YOLO中的全连接层，以对不同大小和长宽比的锚盒来进行分类/回归。SSD取得了比YOLO更快，接近Faster R-CNN的检测性能。后来有研究发现，相比其他方法，SSD受基础模型性能的影响相对较小。\nFPN\n之前的方法都是取高层卷积特征。但由于高层特征会损失一些细节信息，FPN融合多层特征，以综合高层、低分辨率、强语义信息和低层、高分辨率、弱语义信息来增强网络对小目标的处理能力。此外，和通常用多层融合的结果做预测的方法不同，FPN在不同层独立进行预测。FPN既可以与基于候选区域的方法结合，也可以与基于直接回归的方法结合。FPN在和Faster R-CNN结合后，在基本不增加原有模型计算量的情况下，大幅提高对小目标的检测性能。\nRetinaNet\nRetinaNet认为，基于直接回归的方法性能通常不如基于候选区域方法的原因是，前者会面临极端的类别不平衡现象。基于候选区域的方法可以通过候选区域过滤掉大部分的背景区域，但基于直接回归的方法需要直接面对类别不平衡。因此，RetinaNet通过改进经典的交叉熵损失以降低对已经分的很好的样例的损失值，提出了焦点(focal)损失函数，以使模型训练时更加关注到困难的样例上。RetinaNet取得了接近基于直接回归方法的速度，和超过基于候选区域的方法的性能。\n(4) 目标检测常用技巧\n非最大抑制(non-max suppression, NMS)\n目标检测可能会出现的一个问题是，模型会对同一目标做出多次预测，得到多个包围盒。NMS旨在保留最接近真实包围盒的那一个预测结果，而抑制其他的预测结果。NMS的做法是，首先，对每个类别，NMS先统计每个预测结果输出的属于该类别概率，并将预测结果按该概率由高至低排序。其次，NMS认为对应概率很小的预测结果并没有找到目标，所以将其抑制。然后，NMS在剩余的预测结果中，找到对应概率最大的预测结果，将其输出，并抑制和该包围盒有很大重叠(如IoU大于0.3)的其他包围盒。重复上一步，直到所有的预测结果均被处理。\n在线困难样例挖掘(online hard example mining, OHEM)\n目标检测的另一个问题是类别不平衡，图像中大部分的区域是不包含目标的，而只有小部分区域包含目标。此外，不同目标的检测难度也有很大差异，绝大部分的目标很容易被检测到，而有一小部分目标却十分困难。OHEM和Boosting的思路类似，其根据损失值将所有候选区域进行排序，并选择损失值最高的一部分候选区域进行优化，使网络更关注于图像中更困难的目标。此外，为了避免选到相互重叠很大的候选区域，OHEM对候选区域根据损失值进行NMS。\n在对数空间回归\n回归相比分类优化难度大了很多。L2\\ell_损失对异常值比较敏感，由于有平方，异常值会有大的损失值，同时会有很大的梯度，使训练时很容易发生梯度爆炸。而L1\\el损失的梯度不连续。在对数空间中，由于数值的动态范围小了很多，回归训练起来也会容易很多。此外，也有人用平滑的L1\\el损失进行优化。预先将回归目标规范化也会有助于训练\n语义分割(semantic segmentation)\n语义分割是目标检测更进阶的任务，目标检测只需要框出每个目标的包围盒，语义分割需要进一步判断图像中哪些像素属于哪个目标。\n(1) 语义分割常用数据集\nPASCAL VOC 2012 1.5k训练图像，1.5k验证图像，20个类别(包含背景)。\nMS COCO COCO比VOC更困难。有83k训练图像，41k验证图像，80k测试图像，80个类别。\n(2) 语义分割基本思路\n基本思路\n逐像素进行图像分类。我们将整张图像输入网络，使输出的空间大小和输入一致，通道数等于类别数，分别代表了各空间位置属于各类别的概率，即可以逐像素地进行分类。\n全卷积网络+反卷积网络\n为使得输出具有三维结构，全卷积网络中没有全连接层，只有卷积层和汇合层。但是随着卷积和汇合的进行，图像通道数越来越大，而空间大小越来越小。要想使输出和输入有相同的空间大小，全卷积网络需要使用反卷积和反汇合来增大空间大小。\n反卷积(deconvolution)/转置卷积(transpose convolution)\n标准卷积的滤波器在输入图像中进行滑动，每次和输入图像局部区域点乘得到一个输出，而反卷积的滤波器在输出图像中进行滑动，每个由一个输入神经元乘以滤波器得到一个输出局部区域。反卷积的前向过程和卷积的反向过程完成的是相同的数学运算。和标准卷积的滤波器一样，反卷积的滤波器也是从数据中学到的。\n反最大汇合(max-unpooling)\n通常全卷积网络是对称的结构，在最大汇合时需要记下最大值所处局部区域位置，在对应反最大汇合时将对应位置输出置为输入，其余位置补零。反最大汇合可以弥补最大汇合时丢失的空间信息。反最大汇合的前向过程和最大汇合的反向过程完成的是相同的数学运算。\n(3) 语义分割常用技巧\n扩张卷积(dilated convolution)\n经常用于分割任务以增大有效感受野的一个技巧。标准卷积操作中每个输出神经元对应的输入局部区域是连续的，而扩张卷积对应的输入局部区域在空间位置上不连续。扩张卷积保持卷积参数量不变，但有更大的有效感受野。\n条件随机场(conditional random field, CRF)\n条件随机场是一种概率图模型，常被用于微修全卷积网络的输出结果，使细节信息更好。其动机是距离相近的像素、或像素值相近的像素更可能属于相同的类别。此外，有研究工作用循环神经网络(recurrent neural networks)近似条件随机场。条件随机场的另一弊端是会考虑两两像素之间的关系，这使其运行效率不高。\n利用低层信息\n综合利用低层结果可以弥补随着网络加深丢失的细节和边缘信息。\n实例分割(instance segmentation)\n语义分割不区分属于相同类别的不同实例。例如，当图像中有多只猫时，语义分割会将两只猫整体的所有像素预测为“猫”这个类别。与此不同的是，实例分割需要区分出哪些像素属于第一只猫、哪些像素属于第二只猫。\n基本思路\n目标检测+语义分割。先用目标检测方法将图像中的不同实例框出，再用语义分割方法在不同包围盒内进行逐像素标记。\nMask R-CNN\n用FPN进行目标检测，并通过添加额外分支进行语义分割(额外分割分支和原检测分支不共享参数)，即Master R-CNN有三个输出分支(分类、坐标回归、和分割)。此外，Mask R-CNN的其他改进有：(1). 改进了RoI汇合，通过双线性差值使候选区域和卷积特征的对齐不因量化而损失信息。(2). 在分割时，Mask R-CNN将判断类别和输出模板(mask)这两个任务解耦合，用sigmoid配合对率(logistic)损失函数对每个类别的模板单独处理，取得了比经典分割方法用softmax让所有类别一起竞争更好的效果。"}
{"content2":"图A  图B\n图A和图B哪个更亮？哪个颜色更浓？哪个颜色更深？\n你能分辨吗？不能就先胡乱猜测一下吧！\n有时候我们要判断一个颜色和另一个颜色哪个更亮，哪个色彩更浓，哪个视觉上颜色更深。。。。\n各种比较，如果不知道方法，计算机是不会给出答案的！\n我们必须教会计算机一些个公式：（X是灰通道，SN是色彩浓度）\nPC（亮度）计算: X+SN/2\n视觉亮度（色深）计算：128+SN/2-X/2\n再来看看从视觉的角度，你很难判断哪个更什么，但是通过数据计算，我们可以知道，哪个图更亮，哪个颜色更深，\n图A  RGB(4，9，32)\n计算结果：色彩=篮青色；色相值=185 ；亮度值=18；色深值=140；色彩浓度值=28；\n图BRGB(17，11，36)\n计算结果：色彩=篮紫色；色相值=149 ；亮度值=23；色深值=135；色彩浓度值=25；\n明显图A的亮度小于图B，但色深图A大于图B。从数据的角度，图A的颜色要深与图B。\n算法原创，转载请注明作者是：郑斯彬"}
{"content2":"转自：https://me.csdn.net/ali_start （该博主也是转载的，很遗憾我没能找到这篇博文真正的作者，如果原文博主看到这篇文章，请您私信我，很想向您请教一些问题）\n这两年，计算机视觉似乎火了起来计算机视觉的黄金时代真的到来了吗？。生物医学、机械自动化、土木建筑等好多专业的学生都开始研究其在各自领域的应用，一个视觉交流群里三分之一以上都不是计算机相关专业的。当然，我也是其中一员。\n对于非计算机相关专业的学生而言，学习过程中往往缺少交流机会，不容易把握知识的全貌。这里仅根据个人经验谈一谈对于一名非计算机专业的学生而言，该如何学习计算机视觉。\n1.编程能力\n1.1 编程语言(C++, python)\n刚接触CV(computer vision)（注：本文偏向于图像学而非图形学）时，大家一般都会不假思索地选择使用C++：装个VS(Visual Studio)，配置下opencv，撸起袖子就上了。这样做非常合理，几乎所有人都是这么入门的。\n不过，当你知识面扩展开后，你会感觉到很多时候C++都显得有些力不从心。比如：当你要画一些图表或做一些分析，就还得把数据导入MATLAB里做进一步处理；当你要非常快捷方便地学习或测试一个算法，C++会是你最糟糕的选择；或者当你要学习深度学习时，你绝对不会再选择使用C++….总之，有太多理由会促使你再学习一门编程语言，最好的选择没有之一：python。\n1.1.1 简单介绍一下C++和python的各自特点：\nC++：偏底层，执行效率高，适合嵌入式等平台上使用；在视觉领域，C++生态好，用的人多，网上找资源很方便。\n缺点是开发效率实在太低了，关于这一点如果你只是专注于图像处理的话可能感受不是那么真切，因为opencv库做得足够好。但是当你做到机器学习后，opencv就显得有些力不从心了，虽然它也包含一些SVM、神经网络等的简单实现，但毕竟不擅长。\npython：全能语言，干啥都行，并且都相对擅长。图像处理，opencv支持有python接口；科学计算，其功能类似于matlab了：机器学习及深度学习，python是最好用的，没有之一；爬虫等网络应用，豆瓣就是用python写的；简而言之，方便，实在太方便了。\n当然python也有自己的另一面。执行效率不高，这一点做嵌入式开发的可能比较忌讳。但如今手机的内存都升到6G了，tensorflow都可以在移动端跑了，Python也都可以用来控制STM32了，未来很难说。\n顺便说一句也有人使用MATLAB等做图像方面的研究，如果你只是偶尔用图像处理辅助一下你的研究，可以这么做，一般情况下不建议使用。\n1.1.2 C++和python学习资源推荐\nC++：大家好像都买《C++ primer》或《C++ primer plus》这样的大块头书，我自己感觉倒不如《王道程序员求职宝典》这类书实用。大块头书优点在于全面，同时也往往导致了重点不突出。码代码时不熟悉的用法一般直接在cppreference上搜就可以了，超级方便；但有些不容易理解的地方确实需要系统的找资料学习一下。课程的话推荐coursera上北大的《程序设计与算法》，第3门课程是C++程序设计。看视频课程一般比较慢，如果没什么基础或者特别想把基础学好的话，强烈推荐。\npython：基础部分看廖雪峰的python教程就可以了，然后就是用哪一块学哪一块了。python学起来很简单，看别人代码的过程就是学习的过程。对于不熟悉的用法多搜下官方文档，如python, numpy,pandas, matplot, scikit-learn。这里有几张python各种库的小抄表其实直接在网上搜这几张表也都比较方便。课程的话，我之前上过一些七月算法的课程，讲得不好，多少会给你一些知识体系和各种学习资料，总体不推荐或跳着看。python的开发环境值得说一下，因为有太多选择，这里比较建议使用pycharm和jupyter notebook吧，具体参考python入门环境搭建。\n1.2 编程平台（windows, linux）\n新手肯定都用windows了，学习过程中发现在windows上搞不定了，先忍几次，然后掉头就去学linux了。一定是这样。\n哪些在windows上真的搞不定呢？比如：deeplearning，或最新论文中提出的视觉开源算法。\n不过对我们而言，linux并不需要了解太深。装个ubuntu系统，常用的文件操作、程序编译等知道就OK了。我完全是在使用的过程中现用现学，手边常备一本书《鸟哥的linux私房菜》。\n2.视觉知识\n计算机视觉实在很广了，这里仅针对我个人知识体系来说一说。\n现在比较热门的方向总体上分为两大块：一块是深度学习，一块做SLAM。它们的研究点区别在哪呢？深度学习这一群体侧重于解决识别感知（是什么）问题，SLAM侧重于解决几何测量（在哪里）问题ICCV研讨会：实时SLAM的未来以及深度学习与SLAM的比较。拿机器人来说，如果你想要它走到你的冰箱面前而不撞到墙壁，那就需要使用 SLAM；如果你想要它能识别并拿起冰箱中的物品，那就需要用到深度学习机器人抓取时怎么定位的？用什么传感器来检测？。当然这两方面在research上也有互相交叉融合的趋势。\n不过在学习这些之前，一般都会先掌握下传统的计算机视觉知识，也就是图像处理这一部分了。我之前大致总结过一次：\n计算机视觉初级部分知识体系。这些基础知识的理解还是挺有必要的，有助于你理解更高层知识的本质，比如为什么会出现deeplearning等这些新的理论知识（感觉有点像读史了，给你智慧和自由）。这一部分学习资料的话还是挺推荐浅墨的《OpenCV3编程入门》 也可以看他的博客。当然他的书有一个问题就是涉及理论知识太少，所以推荐自己再另备一本偏理论一点的图像处理相关的书，我手边放的是《数字图像处理：原理与实践》，差强人意吧。个人之前看浅墨书的时候做了一份《OpenCV3编程入门》学习笔记，里边包含一些理论知识和个人见解。\n下面说一下两个大的方向：基于深度学习的视觉和SLAM技术。\n基于深度学习的视觉：机器学习包括深度学习里的大部分算法本质上都是用来做“分类”的。具体到计算机视觉领域一般就是物体分类（Object Classification）、目标检测（Object Detection）、语义分割（Image Semantic Segmentation）等，当然也有一些很酷又好玩的东西比如edges2cats、deepart。本人主要做一些Object Detection相关的东西。其实一般是直接跑别人的代码了，稍微做一些修改和参数调整，前期的预处理才是主要工作。这些程序基本都是在linux下跑的。好，深度学习为什么这么强？它主要解决了什么问题呢？我比较认同以下三点：学习特征的能力很强，通用性强，开发优化维护成本低 参见为什么深度学习几乎成了计算机视觉研究的标配？。\n关于这一部分的学习，主要就是deeplearning了。关于deeplearning，漫天飞的各种资源。可以看一看李宏毅的一天搞懂深度学习课件 youtube上有一个一天搞懂深度學習–學習心得；李飞飞的CS231n课程，网易云课堂有大数据文摘翻译的中文字幕版课程，知乎专栏智能单元有CS231N课程翻译（非常好）；三巨头之一Yoshua Bengio的新作《DEEP LEARNING》，目前已有中译版本 。\nSLAM技术：这一部分我了解不多，只是听过一些讲座。可以关注下泡泡机器人 公众号吧，他们公开课出得挺多的；听说高博的新书快出了，我也想赶紧入手偷偷学一下。\n3.机器学习\n计算机视觉中使用的机器学习方法个人感觉不算多，早期的时候会用SVM做分类，现在基本都用深度学习选特征+分类。原因在于统计机器学习这一块虽然方法不少，但是基本都无法应对图像这么大的数据量。\n不过大家在学习过程中很容易接触到各种机器学习方法的名字因为现在大数据分析、机器学习、语音识别、计算机视觉等这些其实分得不是很开，然后不自觉地就会去了解和学习。这样我感觉总体来说是好的。不过在学习一些暂时用不着的算法时，个人感觉没必要做的太深：重在理解其思想，抓住问题本质，了解其应用方向。\n下面分开介绍一下传统机器学习算法和深度神经网络。\n传统机器学习一般也就决策树、神经网络、支持向量机、boosting、贝叶斯网等等吧。方法挺多的，同一类方法不同的变形更多。除了这些监督式学习，还有非监督学习、半监督学习、强化学习。当然还有一些降维算法（如PCA）等。对这些个人整体把握的也不是特别好，太多了。\n学习资料，吴恩达的coursera课程《Machine Learning》，他正在出一本新书《MACHINE LEARNING YEARNING》，说好陆续更新的，刚更新一点就没了，本来想翻译学习一下。个人比较喜欢他的课程风格话说今天中午传出新闻，吴恩达从百度离职了。——执笔于2017.03.22，简单易懂。还有李航的《统计学习方法》和周志华的《机器学习》，两本在国内机器学习界成为经典的书。\n深度学习说着感觉有点心虚，哈哈总共就这几年就那些东西，资料上面视觉知识部分已经说过了，听听课程、看看那些出名的模型框架，基本上也就了解了《一天搞懂深度学习》其实就已经把大部分都给说了，不过个人感觉还是挺难理解的。主要的发展也就CNN、RNN；从去年起GAN火起来了，现在如日中天；增强学习现在发展也非常快，有些名校如CMU都开这方面课程了。\n资料上面说过就不说了喜欢高雅的人也可以看看这个深度学习论文阅读路线图 ，说说在使用deeplearning时用哪个库吧。目前为止还没有大一统的趋势，连各个大公司都是自己用自己开发的，一块大肥肉大家都不舍得放弃。我只用过keras和tensorflow，感觉在这方面没必要太计较，用相对简单的和大家都用的（生态好） 。\n4.数学\n一切工程问题归根结底都是数学问题，这里说说计算机视觉和机器学习所涉及的数学问题。\n微积分：比如图像找边缘即求微分在数字图像里是做差分（离散化）啦，光流算法里用到泰勒级数啦，空间域转频域的傅立叶变换啦，还有牛顿法、梯度下降、最小二乘等等这些都用的特别普遍了。其实个人感觉CV所涉及的微积分知识相对简单，积分很少，微分也不是特别复杂。也可能是本科那会儿力学学怕了吧。\n我好像没备微积分的资料，如果需要的话，同济大学出的本科教材应该也够用了吧。\n概率论与统计：这个比较高深，是应用在机器学习领域里最重要的数序分支。应用比如：条件概率、相关系数、最大似然、大数定律、马尔可夫链等等。\n浙大的《概率论与数理统计》我感觉还行，够用。\n线性代数与矩阵：数字图像本身就是以矩阵的形式呈现的，多个向量组成的样本也是矩阵这种形式非常常见，大多机器学习算法里每个样本都是以向量的形式存在的，多个矩阵叠加则是以张量(tensor)的形式存在google深度学习库tensorflow的字面意思之一。具体应用，比如：世界坐标系->相机坐标系->图像坐标系之间的转换，特征值、特征向量，范数等。\n推荐本书，国外的上课教材《线性代数》。因为浙大的那本教材感觉实在不太行，买过之后还是又买了这本。\n凸优化：这个需要单独拎出来说一下。因为太多问题（尤其机器学习领域）都是优化问题（求最优），凸优化是里面最简单的形式，所以大家都在想办法怎么把一般的优化问题转化为凸优化问题。至于单纯的凸优化理论，好像已经比较成熟了。在机器学习里，经常会看到什么求对偶问题、KKT条件等，潜下心花两天学一学。\n建议备一份高校关于凸优化的教学课件，大家对这一块毕竟比较生，缺乏系统感。比如北大的《凸优化》课程。\n这些数学知识没必要系统学习，效率低又耗时。毕竟大家都有本科的基础，够了。一般用到的时候学，学完之后总结一下。如果真想学习的话，七月在线有个课程《机器学习中的数学》，讲的一般，倒不妨看一看。\n介绍个小trick，之前学习好多数学知识或算法时，看不懂教材上晦涩死板的讲解，一般都会搜索“XXX 形象解释”，往往都会搜到些相对通俗易懂的解释也往往都是在知乎上搜到的这些解答，比如拉格朗日乘子法如何理解？, 如何通俗并尽可能详细解释卡尔曼滤波？ 。\n5.授之以鱼不如授之以渔\n编程能力->计算机视觉->机器学习->数学知识，前文已经把所要学习的知识基本都介绍完了。不知道你有没有冒出疑问：你怎么知道的这些？你平时怎么学习的？\n先说第一条：时间，时间的积累。讲个故事，去年暑期在华东师大参加一个关于ROS（Robot Operate System, 机器人操作系统）的Summer School。顺便提一句，主办者张新宇老师人特别nice。第一天上午的speaker叫Dinesh Manocha，Canny的学生。对，就是Canny边缘检测算法的Canny。Dinesh教授有一个保持了几十年的习惯：（平均）每天只睡4个多小时。用张新宇老师的一句总结就是：智力超群、体力超群、习惯超群。他还提到，未来中国要跟国外竞争，一定程度上就是体力的竞争。因为相比老外目前中国人在这方面不太重视。呃，，，反正我是弱的不行。应该加强的。\n当然，在具体学习方法也有一些trick，不然怎么解释有的人效率高呢。当然聪明和底子能够解释部分原因。现在我就说一说自己学习过程中的小trick。\ngoogle搜索。时代变了，一百年前的人类绝对想像不出自己有了困惑不是去翻书或请教他人而是告诉身旁的一台机器。如今，小学生做道算术题或小女生来个大姨妈都要问问电脑：这是怎么回事。但这些与学视觉又有什么关系呢？——答：没有。好像跑偏的有点多了，再扯远一点吧。跨越时间维度来思考一些新事物的发生及其与旧事物的联系，也许会给你一种想象的自由。比如电报、电话、视频聊天和全息通话用 HoloLens 通话 ，只是举例，我可没说以后这种技术真会普遍应用。，马车、汽车、火车、飞机和火箭太空旅行，蒸汽机、电、互联网和AI。\n百度搜索太烂了（当然，它本地化搜索做得不错。并且我也没说完全是技术原因），有多烂？我认为它跟google搜索的差距不是1:2，是1:10。这一点好像不应该说这么多，大家都公认的。问题根源在于“中国特色”不允许我们使用google搜索，这里介绍一个非常方便的科学上网工具lantern（链接是它的github地址，官网墙内好像登不了。）。下载完安装之后直接运行即可。\n还有一点，多使用英文搜索，这样呈现在你眼前的才是完整的世界。英文世界里优秀、原创资源多，浏览网页时不经意间也会遇到些好网站。比如曾经surf到一个计算机视觉方面的博客Learn OpenCV，通俗易懂，不频繁更新，几乎每篇文章必看。\n交流。这里特制人与人之间的交流，最好是面对面聊天。这样的好处是随意性大，随便一句话就可能指出你长期存在某个误区。对于我们（非计算机专业学生）而言，最缺的就是这种交流环境。所以大家只能尽量弥补了，比如通过各种途径认识点计算机专业或视觉方向的同学（蹭学校计算机视觉的课程）；多加点相关的公众号，QQ、微信群不好的再删，当然自己也要主动参与这些社区。\n书。好书基本上都是公认的，并且适合大部分人。有些人买书可能会有选择恐惧症，这一点，，，摆正心态吧，很多时候买书本来就不是为了读完，只要能给你一两次惊喜或节约你几小时宝贵时间，它的使命就已经完成了，值！！！当然买书也讲究个度，这个就如人饮水、冷暖自知了。\nPPT。PPT的出现在一定程度上对传统教材产生了冲击，方便，重点突出，体验舒服。个人几乎会把学习的所有课件都保存在ipad里推荐使用非常出名的备注记录软件Notability来保存和编辑你的PPT，听课时可以在上边做笔记，课后如果需要随时温故而知新。\n“一句话”抓住问题本质。算法太多，学过就忘。这可能是所有人遇到的问题。尤其对于那些学的不是特别深入的算法，倘或跟人聊起都不知道如何解释。“一句话”解释，就是用简单的几句话把一件事说清楚。比如《统计学习方法》里李航就提出统计机器学习的三要素：模型、策略和算法，针对某种机器学习方法根据这三要素梳理一下，你就已经把握到整体了，即使其中有些细节不理解也无伤大雅。想象一下如果有同学指着你桌上的书问你“机器学习是什么？”，你会不会一脸懵逼？我会，O(∩_∩)O。说一下个人理解，至少听起来是句人话：机器学习就是让机器学会自学，对已有信息进行归纳和识别，并自主获得新技能的能力。相比于传统计算机编程里直接告诉计算机“什么时候做什么”，机器学习通过“不显式编程”赋予计算机能力，即提供一些案例（训练数据）,让计算机通过案例自己学习什么时候应该做什么。\nA4纸学习法。平常的一个个人习惯吧，感觉对自己比较有用，分享一下。对于某些算法，有时候可以自己花半天、一天或者两天动手推导一下，然后A4纸总结整理一下放文件夹里，备日后翻阅。这样有助于提升你的数学能力，加深对算法的理解。\n学习新技能，讲究效率。在大家智力、体力水平都相当的情况下，怎么比别人学得更快更好？这里介绍一个自己快速学习一项新技能的方法：花两周时间把两本书看两遍。具体解释是：单位时间内，把两本书看一遍不如把一本书看两遍，在不确定哪本书具有绝对优势时最好两本书都看（不要把鸡蛋放进一个篮子里）。当然，一定要快！！！对于写代码而言，看书的同时实践也是非常重要。\n6.工作\n这一点好像跟学习本身关系不大，但跟大多数学习者本身（比如我）关系很大。\n花开两朵，各表一枝。\n不少人可能跟我一样都是冲着现在计算机视觉很火、有前景又比较感兴趣，所以选择学计算机视觉，并且以后想要从事计算机视觉这方面的工作。。一定要摆正心态，找工作时可能就要跟那些计算机专业的学生们竞争了；最好从现在起，就把自己当一名程序员看待。当然你也有自己的优势，你拥有自己专业的领域知识这对某些公司来说很重要，你找工作时基本上也都应该重点考虑这些公司。，你对视觉的具体应用本身也比较了解；劣势是你缺乏计算机专业的基本素养，具体到笔试或面试中就是你基础编程能力不行。\n说到这里，大家应该都听说过“刷题”这回事。程序员应聘的特点之一就是首先面试者会考查一些基础的算法题，借此评估一下你的基本编程能力。其实计算机专业的学生在工作季前也要在leetcode等平台上刷刷题练练手，不然他们也过不了第一关。不过，对于我们非计算机专业学生而言，刷题前最好系统学习下数据结构和算法这两门课。程序=数据结构+算法，前面提到的北大《程序设计与算法》专项课程里就有这两门课。然后就是苦练刷题技能了，刷题过程中注意多总结吧。（目前我也刚走到这一阶段，所以不好多说。）\n当然我相信也有一部分人毕业之后就再也不会接触这些破玩意儿，挺好的。三十而立之年，如果我还在整天苦逼地码代码，，，呃，不敢想象，那一定不是我想要的生活。对于这些人而言，计算机视觉可能会成为你人生中的一项常识——五年后的某一天，当你坐上无人车时，一点都不会感到惊讶。当然，也祝愿它会给你的人生带来更多改变，你所学的专业对你思维上最大的影响是什么？\n说完了，有用或没用的、该说或不该说的、跟视觉相关或不相关的都说了好多，收个尾：管理好自己，。\n还有，，，如果你诚心正意把计算机视觉作为个人事业并严肃认真对待的话，可以看下这篇文章《初探计算机视觉的三个源头、兼谈人工智能｜正本清源》，知道计算机视觉不是只有现在的深度学习。"}
{"content2":"出于学习的需要，对人工智能领悟几个很重要的概念(人工智能、数据挖掘、模式识别、机器学习、深度学习)做了简单的总结。不一定很全，只是总结了几者主要的区别。\n人工智能是相对于人大脑智能以外，机器展示出来的智能，是一个模糊抽象的概念。和机器人领域结合比其他概念要更直接些。以下是wiki上的解释\n所以说，AI其实机器智能的最终目标。\n数据挖掘：\n强调在在海量数据（大数据）里发现知识，并试图描述数据内在的逻辑。具体方法不一定依托于机器学习，而只是依托于规则，来寻找数据的内在逻辑。只是数据挖掘用到机器学习方法时候，会产生1+1>2的效应。换句话说，数据挖掘方法可以是自动化或者非自动，而自动化方法可以借鉴机器学习方法，而非自动化则需要专门设定规则。\n模式识别:\n70年代至80年代提出，强调的是如何让一个计算机程序去做一些看起来很“智能”的事情，例如区分“3”和“B”或者“3”和“8”，很多时候需要专门手工设计一些分类规则，如滤波，边缘检测和形态学处理等技术。（智能程序的诞生）\n机器学习：\n90年代初提出，强调的是，在给计算机程序（或者机器）输入一些数据后，他必须学习这些数据，而这个学习的步骤是明确的，学习结果是对已有数据的分类和一个预测模型，预测模型可用于对未知数据的预测。（从样本中学习的智能程序，data-driven）\n典型机器学习的流程图\n深度学习：\n21世纪很热门，强调的是你使用的模型（例如深度卷积多层神经网络），模型中的参数可以从数据中学习获得。（一统江湖的框架）\n卷积神经网络框架\n参考资料\n整理：深度学习 vs 机器学习 vs 模式识别\nhttp://www.csdn.net/article/2015-03-24/2824301\n大数据与深度学习区别？\nhttps://www.zhihu.com/question/31814850\nWIKI-artificial intelligence\nhttps://en.wikipedia.org/wiki/Artificial_intelligence"}
{"content2":"牛人主页（主页有很多论文代码）\nSerge Belongie atUC San Diego\nAntonioTorralba at MIT\nAlexei Ffros atCMU\nCe Liu atMicrosoft Research New England\nVittorioFerrari at Univ.of Edinburgh\nKristenGrauman at UT Austin\nDevi Parikh at  TTI-Chicago (MarrPrize at ICCV2011)\nJohn Wright atColumbia Univ.\nPiotr Dollar atCalTech\nBoris Babenko atUC San Diego\nDavid Ross atGoogle/Youtube\n相关领域：\nTerence Tao at UCLA\nDavid Donoho atStanford Univ.\n大神们：\nWilliam T.Freeman at MIT\nRobertoCipolla at Cambridge\nDavid Lowe atUniv. of British Columbia\nMubarak Shah at Univ. of Central Florida\nYi Ma atMSRA\nTinneTuytelaars at K.U. Leuven\nTrevor Darrell atU.C. Berkeley\nMichael J.Black at Brown Univ.\n重要研究组：\nComputer Vision Group at UC Berkeley\nRoboticsResearch Group at Univ. of Oxford\nLEAR atINRIA\nComputerVision Lab at Stanford\nComputerVision Lab at EPFL\nComputerVision Lab at ETH Zurich\nComputer Vision Lab atSeoul National Univ.\nComputer Vision Lab atUC San Diego\nComputerVision Lab at UC Santa Cruz\nComputerVision Lab at Univ. of Southern California\nComputerVision Lab at Univ. of Central Florida\nComputerVision Lab at Columbia Univ.\nUCLA VisionLab\nMotion andShape Computing Group at George Mason Univ.\nRobust ImageUnderstanding Lab at Rutgers Univ.\nIntelligentVision Systems Group at Univ. of Bonn\nInstitute forComputer Graphics and Vision at Graz Univ. of Tech.\nComputerVision Lab. at Vienna Univ. of Tech.\nComputationalImage Analysis and Radiology at Medical Univ. of Vienna\nPersonal Robotics Lab atCMU\nVisualPerception Lab at Purdue Univ.\n潜力牛人：\nJuergen Gall at ETHZurich\nMatt Flagg atGeorgia Tech.\nMathieuSalzmann at TTI-Chicago\nGergShakhnarovich at TTI-Chicago\nTaeg Sang Cho atMIT\nJianchao Yang atUIUC\nStefan Roth at TU Darmstadt\nPeter Kontschieder at Graz Univ. of Tech.\nDominikAlexander Klein at Univ. of Bonn\nYinan Yu atCASIA (PASCAL VOC 2010 Detection Challenge Winner)\nZdenek Kalal atFPFL\nJulien Pilet atFPFL\nKenji Okuma\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/首页\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山；http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/indexCH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T.Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)Universityof Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋:http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan:http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar:http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh:http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际会议VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\naboutmulti-camera: http://server.cs.ucf.edu/~vision/projects.html\nabout 3D VoxelColoring   Rob Hess:\nhttp://blogs.oregonstate.edu/hess/code/voxels/\nAbout  theparticle filters--condensation filter:\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/ISARD1/condensation.html\nMachine LearningOpen Source Software：\nhttp://jmlr.csail.mit.edu/mloss/\n1、动作识别数据库：Recognition ofhuman actions：http://www.nada.kth.se/cvap/actions/\n2、Datasets for Computer Vision Research：http://www-cvr.ai.uiuc.edu/ponce_grp/data/\n3、Computer VisionDatasets:http://clickdamage.com/sourcecode/cv_datasets.php\n4、里面有好多基本算法 matlab：  http://www.mathworks.cn/index.html\n·        Matlab Codefor Graph EmbeddingDiscriminant Analysis on Grassmannian Manifolds for Improved Image Set Matching (CVPR),2011.\n·        Matlab Codefor Optimal Local Basis: AReinforcement Learning Approach for Face Recognition(IJCV), vol. 81,no. 2, pp. 191-204, 2009.\n牛人bolg：\n1、Hong KongPolytechnic University ：http://www4.comp.polyu.edu.hk/~cslzhang/\n2、Computer VisionResources：资源非常丰富，包含有基本算法。https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n3、源代码非常丰富~~  http://homepage.tudelft.nl/19j49/Publications.html\nCVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htm\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm\n李子青的大作：\nMarkov Random Field Modeling in Computer Vision\nhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.html\nHandbook of Face Recognition (PDF)\nhttp://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf\n张正友的有关参数鲁棒估计著作：\nParameter Estimation Techniques:A Tutorial with Application to Conic Fitting\nhttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.html\nAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Vision\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007\n有关马尔可夫蒙特卡罗方法的资料：\nAn introduction to Markov chain Monte Carlo\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.html\nMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05\nhttp://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm\n有关独立成分分析（Independent Component Analysis , ICA）的资料：\nAn ICA-Page\nhttp://www.cnl.salk.edu/~tony/ica.html\nFast ICA\nhttp://www.cis.hut.fi/projects/ica/fastica/\nThe Kalman Filter (介绍卡尔曼滤波器的终极网页)\nhttp://www.cs.unc.edu/~welch/kalman/index.html\nCached k-d tree search for ICP algorithms\nhttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html\n几个计算机视觉研究工具\nMachine Vision Toolbox for Matlab\nhttp://www.petercorke.com/MachineVision Toolbox.html\nMatlab and Octave Function for Computer Vision and Image Processing\nhttp://www.csse.uwa.edu.au/~pk/research/matlabfns/\nBayes Net Toolbox for Matlab\nhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.html\nOpenCV (Chinese)\nhttp://www.opencv.org.cn/index.php/首页\nGandalf (A Computer Vision and Numerical Algorithm Labrary)\nhttp://gandalf-library.sourceforge.net/\nCMU Computer Vision Home Page\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\nMachine Learning Resource Links\nhttp://www.cse.ust.hk/~ivor/resource.htm\nThe Bayesian Filtering Library\nhttp://www.orocos.org/bfl\nOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)\nhttp://of-eval.sourceforge.net/\nMATLAB code for ICP algorithm\nhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html\n牛人主页：\n朱松纯 （Song-Chun Zhu）\nhttp://www.stat.ucla.edu/~sczhu/\nDavid Lowe (SIFT) (很帅的一个老头哦 ^ ^)\nhttp://www.cs.ubc.ca/~lowe/\nAndrea Vedaldi (SIFT)\nhttp://vision.ucla.edu/~vedaldi/index.html\nPedro F. Felzenszwalb\nhttp://people.cs.uchicago.edu/~pff/\nDougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)\nhttp://mesh.brown.edu/dlanman/courses.html\nJianbo Shi (Ncuts 的始作俑者)\nhttp://www.cis.upenn.edu/~jshi/\nActive Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)\nhttp://www.robots.ox.ac.uk/ActiveVision/index.html\nJuyang Weng（机器学习的专家，\nAutonomous Mental Development\n是其特色）\nhttp://www.cse.msu.edu/~weng/\n测试图片或视频：\nMiddlebury College‘s Stereo Vision Data Set\nhttp://cat.middlebury.edu/stereo/data.html\nIntelligent Vehicle:\nIVSource\nwww.ivsoruce.net\nRobot Car\nhttp://www.plyojump.com/robot_cars.html\nHow to Build a Robot: The Computer Vision Part\nhttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml\n收集的一般牛人主页（带代码）:\nXiaofei He(machine learning code)\nhttp://people.cs.uchicago.edu/~xiaofei/\nYingNian Wu(active base model code)\nhttp://www.stat.ucla.edu/~ywu/research.html\n布朗大学计算机主页（可找到该校CS牛人博客）\nhttp://www.cs.brown.edu/research/areas.html\nNavneet Dalal(Histograms of Oriented Gradients for Human Detection )\nhttp://www.navneetdalal.com/software\nPaul Viola(Robust Real-timeObject Detection)\nhttp://research.microsoft.com/en-us/um/people/viola/\n人工智能与模式识别国际顶级期刊会议目录（包含该领域最权威的期刊和会议）\n--------------------- 本文来自 Solomon1588 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/solomon1558/article/details/51484873?utm_source=copy"}
{"content2":"视觉是自然智能不可思议的结晶。人脑中有关视觉的部分所占比重最大\n人类大脑对视觉进行层次化的处理，人类采用神经网络对视觉信息进行深层次的处理，与深度学习密切结合。\n计算机视觉、深度学习的发展历史\n起源：统计模式识别、二维图像分析\n诞生：Minsky->David Marr，人工智能“计算机视觉”专辑，Marr视觉计算理论得到了迅速的发展\n发展：80年代后\n（1）应用：计算能力增长，导致视觉计算成本极大降低\n（2）理论：以Marr理论为基础的视觉理论广泛研究\n在视频监控、工业分析等领域得到了广泛的应用\n2000后，特征提取和基于学习的视觉得到了迅速发展\n2006年，Hinton提出了深度学习\n2010年，微软使用深度学习在语音方面取得了突破进展\n2015年，深度学习在视觉各个领域取得突破（ImageNet上的识别准确率、Tesla56亿公里自动驾驶、iPhone X的高精度人脸识别、OpenAI 2:1战胜人类DOTO2高手队）\n计算机视觉应用：服务机器人、安防监控、自动驾驶、智能穿戴、无人机快递"}
{"content2":"PASCAL VOC和ImageNet ILSVRC竞赛关注于物体识别。\nCOCO\nLSUN（Large-scale Scene Understanding Challenge）比赛：在CVPR上举办的竞赛，主要针对场景识别问题，包括场景分类、显著性预测、房间布局估计、生成caption等。"}
{"content2":"The M Tank 编辑了一份报告《A Year in Computer Vision》，记录了 2016 至 2017 年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。该材料共包括四大部分，在本文中机器之心对第一部分做了编译介绍，后续会放出其他部分内容。\n内容目录\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率、风格迁移、着色\n动作识别\n第三部分\n3D 目标\n人体姿势估计\n3D 重建\n其他未分类 3D\n总结\n第四部分\n卷积架构\n数据集\n不可分类的其他材料与有趣趋势\n结论\n完整 PDF 地址：http://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf\n简介\n计算机视觉是关于研究机器视觉能力的学科，或者说是使机器能对环境和其中的刺激进行可视化分析的学科。机器视觉通常涉及对图像或视频的评估，英国机器视觉协会（BMVA）将机器视觉定义为「对单张图像或一系列图像的有用信息进行自动提取、分析和理解」。\n对我们环境的真正理解不是仅通过视觉表征就可以达成的。更准确地说，是视觉线索通过视觉神经传输到主视觉皮层，然后由大脑以高度特征化的形式进行分析的过程。从这种感觉信息中提取解释几乎包含了我们所有的自然演化和主体经验，即进化如何令我们生存下来，以及我们如何在一生中对世界进行学习和理解。\n从这方面来说，视觉过程仅仅是传输图像并进行解释的过程，然而从计算的角度看，图像其实更接近思想或认知，涉及大脑的大量功能。因此，由于跨领域特性很显著，很多人认为计算机视觉是对视觉环境和其中语境的真实理解，并将引领我们实现强人工智能。\n不过，我们目前仍然处于这个领域发展的胚胎期。这篇文章的目的在于阐明 2016 至 2017 年计算机视觉最主要的进步，以及这些进步对实际应用的促进。\n为简单起见，这篇文章将仅限于基本的定义，并会省略很多内容，特别是关于各种卷积神经网络的设计架构等方面。\n这里推荐一些学习资料，其中前两个适用与初学者快速打好基础，后两个可以作为进阶学习：\nAndrej Karpathy:「What a Deep Neural Network thinks about your #selfie」，这是理解 CNN 的应用和设计功能的最好文章 [4]。\nQuora:「what is a convolutional neural network?」，解释清晰明了，尤其适合初学者 [5]。\nCS231n: Convolutional Neural Networks for Visual Recognition，斯坦福大学课程，是进阶学习的绝佳资源 [6]。\nDeep Learning(Goodfellow,Bengio&Courville,2016)，这本书在第 9 章提供了对 CNN 的特征和架构设计等详尽解释，网上有免费资源 [7]。\n对于还想进一步了解神经网络和深度学习的，我们推荐：\nNeural Networks and Deep Learning(Nielsen,2017)，这是一本免费在线书籍，可为读者提供对神经网络和深度学习的复杂性的直观理解。即使只阅读了第 1 章也可以帮助初学者透彻地理解这篇文章。\n下面我们先简介本文的第一部分，这一部分主要叙述了目标分类与定位、目标检测与目标追踪等十分基础与流行的计算机视觉任务。而后机器之心将陆续分享 Benjamin F. Duffy 和 Daniel R. Flynn 后面 3 部分对计算机视觉论述，包括第二部分的语义分割、超分辨率、风格迁移和动作识别，第三部分三维目标识别与重建、和第四部分卷积网络的架构与数据集等内容。\n基础的计算机视觉任务\n分类/定位\n图像分类任务通常是指为整张图像分配特定的标签，如下左图整张图像的标签为 CAT。而定位是指找到识别目标在图像中出现的位置，通常这种位置信息将由对象周围的一些边界框表示出来。目前 ImageNet [9] 上的分类/定位的准确度已经超过了一组训练有素的人类 [10]。因此相对于前一部分的基础，我们会着重介绍后面如语义分割、3D 重建等内容。\n图 1：计算机视觉任务，来源 cs231n 课程资料。\n然而随着目标类别 [11] 的增加，引入大型数据集将为近期的研究进展提供新的度量标准。在这一方面，Keras [12] 创始人 Francois Chollet 将包括 Xception 等架构和新技术应用到谷歌内部的大型数据集中，该数据集包含 1.7 万个目标类别，共计 350M（Million）的多类别图像。\n图 2：ILSVRC 竞赛中，分类/定位的逐年错误率，来源 Jia Deng (2016)，ILSVRC2016。\nImageNet LSVRC（2016）亮点：\n场景分类是指用「温室」、「体育场」和「大教堂」等特定场景对图像进行分类。ImageNet 去年举办了基于 Places2[15] 子数据的场景分类挑战赛，该数据集有 365 个场景共计 8 百万 训练图像。海康威视 [16] 选择了深度类 Inception 的网络和并不太深的 ResNet，并利用它们的集成实现 9% 的 Top-5 误差率以赢得竞赛。\nTrimps-Soushen 以 2.99% 的 Top-5 分类误差率和 7.71% 的定位误差率赢得了 ImageNet 分类任务的胜利。该团队使用了分类模型的集成（即 Inception、Inception-ResNet、ResNet 和宽度残差网络模块 [17] 的平均结果）和基于标注的定位模型 Faster R-CNN [18] 来完成任务。训练数据集有 1000 个类别共计 120 万的图像数据，分割的测试集还包括训练未见过的 10 万张测试图像。\nFacebook 的 ResNeXt 通过使用从原始 ResNet [19] 扩展出来的新架构而实现了 3.03% 的 Top-5 分类误差率。\n目标检测\n目标检测（Object Detection）即如字面所说的检测图像中包含的物体或目标。ILSVRC 2016 [20] 对目标检测的定义为输出单个物体或对象的边界框与标签。这与分类/定位任务不同，目标检测将分类和定位技术应用到一张图像的多个目标而不是一个主要的目标。\n图 3：仅有人脸一个类别的目标检测。图为人脸检测的一个示例，作者表示目标识别的一个问题是小物体检测，检测图中较小的人脸有助于挖掘模型的尺度不变性、图像分辨率和情景推理的能力，来源 Hu and Ramanan (2016, p. 1)[21]。\n目标识别领域在 2016 年主要的趋势之一是转向更快、更高效的检测系统。这一特性在 YOLO、SSD 和 R-FCN 方法上非常显著，它们都倾向于在整张图像上共享计算。因此可以将它们与 Fast/Faster R-CNN 等成本较高的子网络技术区分开开来，这些更快和高效的检测系统通常可以指代「端到端的训练或学习」。\n这种共享计算的基本原理通常是避免将独立的算法聚焦在各自的子问题上，因为这样可以避免训练时长的增加和网络准确度的降低。也就是说这种端到端的适应性网络通常发生在子网络解决方案的初始之后，因此是一种可回溯的优化（retrospective optimisation）。然而，Fast/Faster R-CNN 技术仍然非常有效，仍然广泛用于目标检测任务。\nSSD：Single Shot MultiBox Detector[22] 利用封装了所有必要计算并消除了高成本通信的单一神经网络，以实现了 75.1% mAP 和超过 Faster R-CNN 模型的性能（Liu et al. 2016）。\n我们在 2016 年看到最引人注目的系统是「YOLO9000: Better, Faster, Stronger」[23]，它引入了 YOLOv2 和 YOLO9000 检测系统 [24]。YOLOv2 很大程度上提升了 2015 年提出的 YOLO 模型 [25] 性能，它能以非常高的 FPS（使用原版 GTX Titan X 在低分辨率图像上达到 90FPS）实现更好的结果。除了完成的速度外，系统在特定目标检测数据集上准确度要优于带有 ReNet 和 SSD 的 Faster RCNN。\nYOLO9000 实现了检测和分类的联合训练，并将其预测泛化能力扩展到未知的检测数据上，即它能检测从未见过的目标或物体。YOLO9000 模型提供了 9000 多个类别的实时目标检测，缩小了分类和检测数据集间的鸿沟。该模型其它详细的信息和预训练模型请查看：http://pjreddie.com/darknet/yolo/。\nFeature Pyramid Networks for Object Detection [27] 是 FAIR [28] 实验室提出的，它能利用「深度卷积网络的内部多尺度、金字塔型的层级结构构建具有边际额外成本的特征金字塔」，这意味着表征能更强大和快速。Lin et al. (2016) 在 COCO[29] 数据集上实现了顶尖的单模型结果。若与基础的 Faster R-CNN 相结合，将超过 2016 年最好的结果。\nR-FCN：Object Detection via Region-based Fully Convolutional Networks [30]，这是另一种在图像上避免应用数百次高成本的各区域子网络方法，它通过使基于区域的检测器在整张图像上进行全卷积和共享计算。「我们每张图像的测试时间只需要 170ms，要比 Faster R-CNN 快 2.5 到 20 倍」(Dai et al., 2016)。\n图 4：目标检测中的准确率权衡，来源 Huang et al. (2016, p. 9)[31]。\n注意：Y 轴表示的是平均准确率（mAP），X 轴表示不同元架构（meta-architecture）的各种特征提取器（VGG、MobileNet...Inception ResNet V2）。此外，mAP small、medium 和 large 分别表示对小型、中型和大型目标的检测平均准确率。即准确率是按「目标尺寸、元架构和特征提取器」进行分层的，并且图像的分辨率固定为 300。虽然 Faster R-CNN 在上述样本中表现得更好，但是这并没有什么价值，因为该元架构相比 R-FCN 来说慢得多。\nHuang et al. (2016)[32] 的论文提供了 R-FCN、SSD 和 Faster R-CNN 的深度性能对比。由于机器学习准确率对比中存在的问题，这里使用的是一种标准化的方法。这些架构被视为元架构，因为它们可以组合不同的特征提取器，比如 ResNet 或 Inception。\n论文的作者通过改变元架构、特征提取器和图像分辨率研究准确率和速度之间的权衡。例如，对不同特征提取器的选择可以造成元架构对比的非常大的变化。\n实时商业应用中需要低功耗和高效同时能保持准确率的目标检测方法，尤其是自动驾驶应用，SqueezeDet[33] 和 PVANet[34] 在论文中描述了这种发展趋势。\nCOCO[36] 是另一个常用的图像数据集。然而，它相对于 ImageNet 来说更小，更常被用作备选数据集。ImageNet 聚焦于目标识别，拥有情景理解的更广泛的语境。组织者主办了一场包括目标检测、分割和关键点标注的年度挑战赛。在 ILSVRC[37] 和 COCO[38] 上进行的目标检测挑战赛的结果如下：\nImageNet LSVRC 图像目标检测（DET）：CUImage 66% 平均准确率，在 200 个类别中有 109 个胜出。\nImageNet LSVRC 视频目标检测（VID）：NUIST 80.8% 平均准确率。\nImageNet LSVRC 视频追踪目标检测：CUvideo 55.8% 平均准确率。\nCOCO 2016 目标检测挑战赛（边界框）：G-RMI（谷歌）41.5% 平均准确率（比 2015 的胜者 MSRAVC 高出 4.2% 绝对百分点）。\n从以上结果可以看出，在 ImageNet 上的结果表明「MSRAVC 2015 的结果为『引入 ResNet』设置了很高的标准。在整个项目中对所有的类别的目标检测性能都有所提升。在两个挑战赛中，定位任务的性能都得到较大的提升。关于小型目标实例的大幅性能提升结果详见参考文献」（ImageNet,2016）。[39]\n图 5.ILSVRC 的图像目标检测结果（2013-2016），来源 ImageNet. 2016. [Online] Workshop\n目标追踪\n目标追踪即在给定的场景中追踪感兴趣的一个或多个特定目标的过程，在视频和现实世界的交互中（通常是从追踪初始的目标检测开始的）有很多应用，且对于自动驾驶而言非常重要。\nFully-Convolutional Siamese Networks for Object Tracking[40]，将一个连体网络（Siamese network）结合一个基础的追踪算法，使用端到端的训练方法，达到了当前最佳，图框显示率超过了实时应用的需求。这篇论文利用传统在线学习方法构建追踪模型。\nLearning to Track at 100 FPS with Deep Regression Networks[41]，该论文试图改善在线训练方法中存在的缺陷。他们构建了一个使用前馈网络学习目标运动、外观和方向中的普遍关系的追踪器，从而可以在没有在线训练的情况下有效地追踪到新的目标。该算法在一个标准的追踪基准测试中达到了当前最佳，同时可以 100FPS 的帧数追踪所有的目标（Held et al.,2016）。\nDeep Motion Features for Visual Tracking[43] 结合了手工设计的特征、深度外观特征（利用 CNN）和深度运动特征（在光流图像上训练），并取得了当前最佳的结果。虽然深度运动特征在动作识别和视频分类中很常见，但作者声称这是其首次被应用于视觉追踪上。该论文获得了 ICPR2016 的「计算机视觉和机器人视觉」的最佳论文。\n「本论文展示了深度运动特征（motion features）对检测和追踪框架的影响。我们还进一步说明了手工制作的特征、深度 RGB 和深度运用特征包含互补信息。据我们所知，这是第一个提出融合外表信息和深度运动特征，并用于视觉追踪的研究。我们全面的实验表明融合方法具有深度运动特征，并超过了单纯依赖外表信息的方法。」\nVirtual Worlds as Proxy for Multi-Object Tracking Analysis [44] 方法解决了现有虚拟世界中缺乏真实可变性视频追踪基准和数据集。该论文提出了一种新的真实世界复制方法，该方法从头开始生成丰富、虚拟、合成和照片逼真的环境。此外，该方法还能克服现有数据集中存在的一些内容匮乏问题。生成的图像能自动通过正确的真值进行标注，并允许应用于除目标检测/追踪外其它如光流等任务。\nGlobally Optimal Object Tracking with Fully Convolutional Networks [45] 专注处理目标变化和遮挡，并将它们作为目标追踪的两个根本限制。「我们提出的方法通过使用全卷积网络解决物体或目标外表的变化，还通过动态规划的方法解决遮挡情况」(Lee et al., 2016)。\n参考文献：\n[1] British Machine Vision Association (BMVA). 2016. What is computer vision? [Online] Available at: http://www.bmva.org/visionoverview [Accessed 21/12/2016]\n[2] Krizhevsky, A., Sutskever, I. and Hinton, G. E. 2012. ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada. Available: http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf\n[3] Kuhn, T. S. 1962. The Structure of Scientific Revolutions. 4th ed. United States: The University of Chicago Press.\n[4] Karpathy, A. 2015. What a Deep Neural Network thinks about your #selfie. [Blog] Andrej Karpathy Blog. Available: http://karpathy.github.io/2015/10/25/selfie/ [Accessed: 21/12/2016]\n[5] Quora. 2016. What is a convolutional neural network? [Online] Available: https://www.quora.com/What-is-a-convolutional-neural-network [Accessed: 21/12/2016]\n[6] Stanford University. 2016. Convolutional Neural Networks for Visual Recognition. [Online] CS231n. Available: http://cs231n.stanford.edu/ [Accessed 21/12/2016]\n[7] Goodfellow et al. 2016. Deep Learning. MIT Press. [Online] http://www.deeplearningbook.org/ [Accessed: 21/12/2016] Note: Chapter 9, Convolutional Networks [Available: http://www.deeplearningbook.org/contents/convnets.html]\n[8] Nielsen, M. 2017. Neural Networks and Deep Learning. [Online] EBook. Available: http://neuralnetworksanddeeplearning.com/index.html [Accessed: 06/03/2017].\n[9] ImageNet refers to a popular image dataset for Computer Vision. Each year entrants compete in a series of different tasks called the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Available: http://image-net.org/challenges/LSVRC/2016/index\n[10] See「What I learned from competing against a ConvNet on ImageNet」by Andrej Karpathy. The blog post details the author』s journey to provide a human benchmark against the ILSVRC 2014 dataset. The error rate was approximately 5.1% versus a then state-of-the-art GoogLeNet classification error of 6.8%. Available: http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\n[11] See new datasets later in this piece.\n[12] Keras is a popular neural network-based deep learning library: https://keras.io/\n[13] Chollet, F. 2016. Information-theoretical label embeddings for large-scale image classification. [Online] arXiv: 1607.05691. Available: arXiv:1607.05691v1\n[14] Chollet, F. 2016. Xception: Deep Learning with Depthwise Separable Convolutions. [Online] arXiv:1610.02357. Available: arXiv:1610.02357v2\n[15] Places2 dataset, details available: http://places2.csail.mit.edu/. See also new datasets section.\n[16] Hikvision. 2016. Hikvision ranked No.1 in Scene Classification at ImageNet 2016 challenge. [Online] Security News Desk. Available: http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/ [Accessed: 20/03/2017].\n[17] See Residual Networks in Part Four of this publication for more details.\n[18] Details available under team information Trimps-Soushen from: http://image-net.org/challenges/LSVRC/2016/results\n[19] Xie, S., Girshick, R., Dollar, P., Tu, Z. & He, K. 2016. Aggregated Residual Transformations for Deep Neural Networks. [Online] arXiv: 1611.05431. Available: arXiv:1611.05431v1\n[20] ImageNet Large Scale Visual Recognition Challenge (2016), Part II, Available: http://image-net.org/challenges/LSVRC/2016/ [Accessed: 22/11/2016]\n[21] Hu and Ramanan. 2016. Finding Tiny Faces. [Online] arXiv: 1612.04402. Available: arXiv:1612.04402v1\n[22] Liu et al. 2016. SSD: Single Shot MultiBox Detector. [Online] arXiv: 1512.02325v5. Available: arXiv:1512.02325v5\n[23] Redmon, J. Farhadi, A. 2016. YOLO9000: Better, Faster, Stronger. [Online] arXiv: 1612.08242v1. Available: arXiv:1612.08242v1\n[24] YOLO stands for「You Only Look Once」.\n[25] Redmon et al. 2016. You Only Look Once: Unified, Real-Time Object Detection. [Online] arXiv: 1506.02640. Available: arXiv:1506.02640v5\n[26]Redmon. 2017. YOLO: Real-Time Object Detection. [Website] pjreddie.com. Available: https://pjreddie.com/darknet/yolo/ [Accessed: 01/03/2017].\n[27] Lin et al. 2016. Feature Pyramid Networks for Object Detection. [Online] arXiv: 1612.03144. Available: arXiv:1612.03144v1\n[28] Facebook's Artificial Intelligence Research\n[29] Common Objects in Context (COCO) image dataset\n[30] Dai et al. 2016. R-FCN: Object Detection via Region-based Fully Convolutional Networks. [Online] arXiv: 1605.06409. Available: arXiv:1605.06409v2\n[31] Huang et al. 2016. Speed/accuracy trade-offs for modern convolutional object detectors. [Online] arXiv: 1611.10012. Available: arXiv:1611.10012v1\n[32] ibid\n[33] Wu et al. 2016. SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving. [Online] arXiv: 1612.01051. Available: arXiv:1612.01051v2\n[34] Hong et al. 2016. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection. [Online] arXiv: 1611.08588v2. Available: arXiv:1611.08588v2\n[35] DeepGlint Official. 2016. DeepGlint CVPR2016. [Online] Youtube.com. Available: https://www.youtube.com/watch?v=xhp47v5OBXQ [Accessed: 01/03/2017].\n[36] COCO - Common Objects in Common. 2016. [Website] Available: http://mscoco.org/ [Accessed: 04/01/2017].\n[37] ILSRVC results taken from: ImageNet. 2016. Large Scale Visual Recognition Challenge 2016.\n[Website] Object Detection. Available: http://image-net.org/challenges/LSVRC/2016/results [Accessed: 04/01/2017].\n[38] COCO Detection Challenge results taken from: COCO - Common Objects in Common. 2016. Detections Leaderboard [Website] mscoco.org. Available: http://mscoco.org/dataset/#detections-leaderboard [Accessed: 05/01/2017].\n[39] ImageNet. 2016. [Online] Workshop Presentation, Slide 31. Available: http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf [Accessed: 06/01/2017].\n[40] Bertinetto et al. 2016. Fully-Convolutional Siamese Networks for Object Tracking. [Online] arXiv: 1606.09549. Available: https://arxiv.org/abs/1606.09549v2\n[41] Held et al. 2016. Learning to Track at 100 FPS with Deep Regression Networks. [Online] arXiv: 1604.01802. Available: https://arxiv.org/abs/1604.01802v2\n[42] David Held. 2016. GOTURN - a neural network tracker. [Online] YouTube.com. Available: https://www.youtube.com/watch?v=kMhwXnLgT_I [Accessed: 03/03/2017].\n[43] Gladh et al. 2016. Deep Motion Features for Visual Tracking. [Online] arXiv: 1612.06615. Available: arXiv:1612.06615v1\n[44] Gaidon et al. 2016. Virtual Worlds as Proxy for Multi-Object Tracking Analysis. [Online] arXiv: 1605.06457. Available: arXiv:1605.06457v1\n[45] Lee et al. 2016. Globally Optimal Object Tracking with Fully Convolutional Networks. [Online] arXiv: 1612.08274. Available: arXiv:1612.08274v1\n原报告地址：http://www.themtank.org/a-year-in-computer-vision\n分享朋友圈 也是另一种赞赏\nThe more we share, The more we have\n欢迎加入数据君高效数据分析社区\n加我私人微信进入大数据干货群：tongyuannow\n\n\n目前100000+人已关注加入我们"}
{"content2":"中文名\n英文名\n简称\n所属学科\nInput\nOutput\n计算机视觉\nComputer Vision\nCV\nComputer Science/ Artificial Intelligence\n图像\n模型\n图像处理\nImage Processing\nIP\nElectrical Engineering/ Signal Processing/ Digital Signal Processing\n图像\n图像\n计算机图形学\nComputer Graphics\nCG\nComputer Science/ Computer Graphics and Visualization\n模型\n图像"}
{"content2":"多方搜寻，继续总结，考的范围不所谓不广。。。\n- 存储一张大小为1024*1024,512个灰度级的图像，需要（）bit A\nA. 8M\nB. 32M\nC. 16M\nD. 64M\n首先\n1024∗1024=220,512=29\n1024*1024=2^{20}, 512=2^9 ，那么存储这样一幅图需要\n1024∗1024∗9\n1024*1024*9 bit =9Mbit，近似计算得8Mbit\n金字塔分解融合法属于（）融合法 C\nA. 决策级\nB. 无法判断\nC. 数据级\nD. 特征级\n图像融合层次由低到高分为四个层次：信号级、数据级、特征级、决策级\n信号级融合，对未经处理的传感器输出在信号域进行混合，产生一个融合后的信号。\n数据级融合又称像素级融合，包括空间域算法和变换域算法，空间域算法中又有多种融合规则方法，如逻辑滤波法，灰度加权平均法，对比调制法；变换域中又有金字塔分解融合法，小波变换法。\n在特征级融合中，保证不同图像包含信息的特征，从源图像中将特征信息提取出来，这些特征信息是观察者对源图像中目标或感兴趣的区域，如边缘、人物、建筑或车辆等信息，然后对这些特征信息进行分析、处理与整合从而得到融合后的图像特征。\n决策级图像融合是以认知为基础的方法，它不仅是最高层次的图像融合方法，抽象等级也是最高的。\n有如下两个列表list1=[2,3,6,8], list2=[5,6,10]则执行list(list1+list2)的结果是： C\nA [2,3,5,6,6,8,10]\nB 编译错误\nC [2,3,6,8,5,6,10]\nD [2,3,5,6,8,10]\n直接是列表合并，元素直接合并\n一个有131个元素的顺序表，插入一个新元素并保持原来的顺序不变，平均要移动（）个元素，删除一个元素平均需要移动（）个元素 D\nA. 67,66\nB. 65,64\nC. 9,8\nD. 65.5, 65\n对于由n个元素组成的顺序表，插入新元素平均移动的个数可以如此计算：\n当插在首端时需要移动n个元素，当插在尾端时需要移动0个元素，因此平均移动个数为((n)/2)\n当删除首端元素时需要移动个数是（n-1）个，当删除尾端元素时需要移动个数是0个，因此平均移动个数为(n-1)/2\n下面的程序要求用户输入二进制数字0/1并显示之，请找出程序中的错误（） B\nbit=input(“Enter a binary digit:”)\nif bit=0 or 1:\nprint “Your input is:”,bit\nelse:\nprint “Your input is invalid”\nA. 1\nB. 2\nC. 3\nD. 4\n唉。。。没学过python的人，一看见python就吓尿了，用python跑了一看应该是bit==0 or 1\n一个待散列线性表位K=(27,18,66,57,36,79,94,41),散列函数为H(k)=k mode 13,与27发生冲突的元素有（）个 C\nA. 1\nB. 4\nC. 3\nD. 2\nH(27)=1\nH(18)=5\nH(66)=1\nH(57)=5\nH(36)=10\nH(79)=1\nH(94)=3\nH(41)=2\n下列程序的运行结果是（） C\n#include <stdio.h> void f(int *p) {p[0]=*(p+2);} main(){ int a[10]={1,2,3,4,5,6,7,8,9,0}; for(i=3;i>=0;i--)f(a+i); for(i=0;i<10;i++)printf(\"%d\",a[i]); printf(\"\\n\"); }\nA. 4 4 4 4 5 6 7 8 9 0\nB. 5 5 5 5 5 6 7 8 9 0\nC. 5 6 5 6 5 6 7 8 9 0\nD. 3 3 3 4 5 6 7 8 9 0\n耐心算一下就可以，但是没时间了。。。后面的题做的太烂了\n以下程序\n这里写代码片"}
{"content2":"计算机视觉入门系列（一） 综述\n自大二下学期以来，学习计算机视觉及机器学习方面的各种课程和论文，也亲身参与了一些项目，回想起来求学过程中难免走了不少弯路和坎坷，至今方才敢说堪堪入门。因此准备写一个计算机视觉方面的入门文章，一来是时间长了以后为了巩固和温习一下所学，另一方面也希望能给新入门的同学们介绍一些经验，还有自然是希望各位牛人能够批评指正不吝赐教。由于临近大四毕业，更新的时间难以保证，这个系列除了在理论上面会有一些介绍以外，也会提供几个小项目进行实践，我会尽可能不断更新下去。\n因诸多学术理论及概念的原始论文都发表在英文期刊上，因此在尽可能将专业术语翻译成中文的情况下，都会在括号内保留其原始的英文短语以供参考。\n目录\n简介\n方向\n热点\n简介\n计算机视觉（Computer Vision）又称为机器视觉（Machine Vision），顾名思义是一门“教”会计算机如何去“看”世界的学科。在机器学习大热的前景之下，计算机视觉与自然语言处理（Natural Language Process， NLP）及语音识别（Speech Recognition）并列为机器学习方向的三大热点方向。而计算机视觉也由诸如梯度方向直方图（Histogram of Gradient， HOG）以及尺度不变特征变换（Scale-Invariant Feature Transform， SIFT）等传统的手办特征（Hand-Crafted Feature）与浅层模型的组合逐渐转向了以卷积神经网络（Convolutional Neural Network， CNN）为代表的深度学习模型。\n方式\n特征提取\n决策模型\n传统方式\nSIFT，HOG， Raw Pixel …\nSVM， Random Forest， Linear Regression …\n深度学习\nCNN …\nCNN …\nsvm（Support Vector Machine） ： 支持向量机\nRandom Forest ： 随机森林\nLinear Regression ： 线性回归\nRaw Pixel ： 原始像素\n传统的计算机视觉对待问题的解决方案基本上都是遵循： 图像预处理 → 提取特征 → 建立模型（分类器/回归器） → 输出 的流程。 而在深度学习中，大多问题都会采用端到端（End to End）的解决思路，即从输入到输出一气呵成。本次计算机视觉的入门系列，将会从浅层学习入手，由浅入深过渡到深度学习方面。\n方向\n计算机视觉本身又包括了诸多不同的研究方向，比较基础和热门的几个方向主要包括了：物体识别和检测（Object Detection），语义分割（Semantic Segmentation），运动和跟踪（Motion & Tracking），三维重建（3D Reconstruction），视觉问答（Visual Question & Answering），动作识别（Action Recognition）等。\n物体识别和检测\n物体检测一直是计算机视觉中非常基础且重要的一个研究方向，大多数新的算法或深度学习网络结构都首先在物体检测中得以应用如VGG-net， GoogLeNet， ResNet等等，每年在imagenet数据集上面都不断有新的算法涌现，一次次突破历史，创下新的记录，而这些新的算法或网络结构很快就会成为这一年的热点，并被改进应用到计算机视觉中的其它应用中去，可以说很多灌水的文章也应运而生。\n物体识别和检测，顾名思义，即给定一张输入图片，算法能够自动找出图片中的常见物体，并将其所属类别及位置输出出来。当然也就衍生出了诸如人脸检测（Face Detection），车辆检测（Viechle Detection）等细分类的检测算法。\n近年代表论文\nHe, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nLiu, Wei, et al. “SSD: Single shot multibox detector.” European Conference on Computer Vision. Springer International Publishing, 2016.\nSzegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\nRen, Shaoqing, et al. “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems. 2015.\nSimonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.\n数据集\nIMAGENET\nPASCAL VOC\nMS COCO\nCaltech\n语义分割\n语义分割是近年来非常热门的方向，简单来说，它其实可以看做一种特殊的分类——将输入图像的每一个像素点进行归类，用一张图就可以很清晰地描述出来。\n很清楚地就可以看出，物体检测和识别通常是将物体在原图像上框出，可以说是“宏观”上的物体，而语义分割是从每一个像素上进行分类，图像中的每一个像素都有属于自己的类别。\n近年代表论文\nLong, Jonathan, Evan Shelhamer, and Trevor Darrell. “Fully convolutional networks for semantic segmentation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\nChen, Liang-Chieh, et al. “Semantic image segmentation with deep convolutional nets and fully connected crfs.” arXiv preprint arXiv:1412.7062 (2014).\nNoh, Hyeonwoo, Seunghoon Hong, and Bohyung Han. “Learning deconvolution network for semantic segmentation.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\nZheng, Shuai, et al. “Conditional random fields as recurrent neural networks.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\n数据集\nPASCAL VOC\nMS COCO\n运动和跟踪\n跟踪也属于计算机视觉领域内的基础问题之一，在近年来也得到了非常充足的发展，方法也由过去的非深度算法跨越向了深度学习算法，精度也越来越高，不过实时的深度学习跟踪算法精度一直难以提升，而精度非常高的跟踪算法的速度又十分之慢，因此在实际应用中也很难派上用场。\n那么什么是跟踪呢？就目前而言，学术界对待跟踪的评判标准主要是在一段给定的视频中，在第一帧给出被跟踪物体的位置及尺度大小，在后续的视频当中，跟踪算法需要从视频中去寻找到被跟踪物体的位置，并适应各类光照变换，运动模糊以及表观的变化等。但实际上跟踪是一个不适定问题（ill posed problem），比如跟踪一辆车，如果从车的尾部开始跟踪，若是车辆在行进过程中表观发生了非常大的变化，如旋转了180度变成了侧面，那么现有的跟踪算法很大的可能性是跟踪不到的，因为它们的模型大多基于第一帧的学习，虽然在随后的跟踪过程中也会更新，但受限于训练样本过少，所以难以得到一个良好的跟踪模型，在被跟踪物体的表观发生巨大变化时，就难以适应了。所以，就目前而言，跟踪算不上是计算机视觉内特别热门的一个研究方向，很多算法都改进自检测或识别算法。\n近年代表论文\nNam, Hyeonseob, and Bohyung Han. “Learning multi-domain convolutional neural networks for visual tracking.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nHeld, David, Sebastian Thrun, and Silvio Savarese. “Learning to track at 100 fps with deep regression networks.” European Conference on Computer Vision. Springer International Publishing, 2016.\nHenriques, João F., et al. “High-speed tracking with kernelized correlation filters.” IEEE Transactions on Pattern Analysis and Machine Intelligence 37.3 (2015): 583-596.\nMa, Chao, et al. “Hierarchical convolutional features for visual tracking.” Proceedings of the IEEE International Conference on Computer Vision. 2015.\nBertinetto, Luca, et al. “Fully-convolutional siamese networks for object tracking.” European Conference on Computer Vision. Springer International Publishing, 2016.\nDanelljan, Martin, et al. “Beyond correlation filters: Learning continuous convolution operators for visual tracking.” European Conference on Computer Vision. Springer International Publishing, 2016.\nLi, Hanxi, Yi Li, and Fatih Porikli. “Deeptrack: Learning discriminative feature representations online for robust visual tracking.” IEEE Transactions on Image Processing 25.4 (2016): 1834-1848.\n数据集\nOTB(Object Tracking Benchmark)\nVOT(Visual Object Tracking)\n视觉问答\n视觉问答也简称VQA（Visual Question Answering），是近年来非常热门的一个方向，其研究目的旨在根据输入图像，由用户进行提问，而算法自动根据提问内容进行回答。除了问答以外，还有一种算法被称为标题生成算法（Caption Generation），即计算机根据图像自动生成一段描述该图像的文本，而不进行问答。对于这类跨越两种数据形态（如文本和图像）的算法，有时候也可以称之为多模态，或跨模态问题。\n近年代表论文\nXiong, Caiming, Stephen Merity, and Richard Socher. “Dynamic memory networks for visual and textual question answering.” arXiv 1603 (2016).\nWu, Qi, et al. “Ask me anything: Free-form visual question answering based on knowledge from external sources.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\nZhu, Yuke, et al. “Visual7w: Grounded question answering in images.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n数据集\nVQA\n热点\n随着深度学习的大举侵入，现在几乎所有人工智能方向的研究论文几乎都被深度学习占领了，传统方法已经很难见到了。有时候在深度网络上改进一个非常小的地方，就可以发一篇还不错的论文。并且，随着深度学习的发展，很多领域的现有数据集内的记录都在不断刷新，已经向人类记录步步紧逼，有的方面甚至已经超越了人类的识别能力。那么，下一步的研究热点到底会在什么方向呢？就我个人的一些观点如下：\n多模态研究： 目前的许多领域还是仅仅停留在单一的模态上，如单一分物体检测，物体识别等，而众所周知的是现实世界就是有多模态数据构成的，语音，图像，文字等等。 VQA 在近年来兴起的趋势可见，未来几年内，多模态的研究方向还是比较有前景的，如语音和图像结合，图像和文字结合，文字和语音结合等等。\n数据生成： 现在机器学习领域的许多数据还是由现实世界拍摄的视频及图片经过人工标注后用作于训练或测试数据的，标注人员的职业素养和经验，以及多人标注下的规则统一难度在一定程度上也直接影响了模型的最终结果。而利用深度模型自动生成数据已经成为了一个新的研究热点方向，如何使用算法来自动生成数据相信在未来一段时间内都是不错的研究热点。\n无监督学习：人脑的在学习过程中有许多时间都是无监督（Un-supervised Learning）的，而现有的算法无论是检测也好识别也好，在训练上都是依赖于人工标注的有监督（Supervised Learning）。如何将机器学习从有监督学习转变向无监督学习，应该是一个比较有挑战性的研究方向，当然这里的无监督学习当然不是指简单的如聚类算法（Clustering）这样的无监督算法。而LeCun也曾说： 如果将人工智能比喻作一块蛋糕的话，有监督学习只能算是蛋糕上的糖霜，而增强学习（Reinforce Learning）则是蛋糕上的樱桃，无监督学习才是真正蛋糕的本体。\n最后，想要把握领域内最新的研究成果和动态，还需要多看论文，多写代码。\n计算机视觉领域内的三大顶级会议有：\nConference on Computer Vision and Pattern Recognition （CVPR）\nInternational Conference on Computer Vision （ICCV）\nEuropean Conference on Computer Vision （ECCV）\n较好的会议有以下几个：\nThe British Machine Vision Conference （BMVC）\nInternational Conference on Image Processing （ICIP）\nWinter Conference on Applications of Computer Vision （WACV）\nAsian Conference on Computer Vision (ACCV)\n当然，毕竟文章的发表需要历经审稿和出版的阶段，因此当会议论文集出版的时候很可能已经过了小半年了，如果想要了解最新的研究，建议每天都上ArXiv的cv板块看看，ArXiv上都是预出版的文章，并不一定最终会被各类会议和期刊接收，所以质量也就良莠不齐，对于没有分辨能力的入门新手而言，还是建议从顶会和顶级期刊上的经典论文入手。\n这是一篇对计算机视觉目前研究领域的几个热门方向的一个非常非常简单的介绍，希望能对想要入坑计算机视觉方向的同学有一定的帮助。由于个人水平十分有限，错误在所难免，欢迎大家对文中的错误进行批评和指正。"}
{"content2":"1.采集装置\n（1）基本概念\n对某个电磁能量谱波段敏感的物理器件，可以接受辐射并产生与所接收到的电磁辐射能量成正比的模拟电信号并数字化器件，它能将上述模拟电信号转化为数字离散的形式（模/数转换），以输入计算机。\n（2）常用装置\n<1>CCD:电荷耦合器件，其固态阵是由称为感光基元的离散硅成像元素构成的。这样的感光基于能产生与接收所输入光强成正比的输出电压。按照几何组织形式分为线扫描和平面扫描。线阵靠一行基元利用场景和检测器之间的相对运动获得2-D图像。平面可直接得到图像。\n<2>CMOS:互补性金属氧化物，其传感器包括传感器核心，模数转换器，输出寄存器，控制寄存器，增益放大器。感光像元电路分3种：①光敏二极管无源像素结构（其由一个反置的光敏二极管和一个开关管构成，当开关管与垂直的列线连通，位于列线末端的放大器读出放大器电压，当光敏二极管存储的信号被读取时，电压被复位，次数放大器将与输入的光信号成正比的电荷转为为电压输出）②光敏二极管无源像素结构（在无源像元后加上了放大器）③光栅型有源像素结构：信号电荷在光栅下积分，输出前，将扩散点复位，然后改变光栅脉冲，收集光栅下的信号电荷转移到扩散点，复位电压水平之差就是输出电压。\n<3>CID:电荷注入器件，其有一个和图像矩阵对应的电极矩阵，在每一个像素位置有两个隔离/绝缘的能产生电位阱的电极。其中一个电极与同一行的所有像素的对应电极连通，而另一个电极则与同一列的所有像素的对应电极连通。访问像素通常选择行和列实现。这两个电极的电压可分别为正负（包括零），有3种工作模式：①积分模式：此时两个电极的电压为正，光电子会累加，所有的行和列均保持正电压，整个芯片将给出一幅完整的图像。②非消除性模式：此时两个电极一个为正一个为负，为负的电极累加的光电子将会迁移到为正的电极下。这个迁移将会在与第2个电极连通的电路中激发出一个脉冲，脉冲的幅度反映了累加的光电子数。迁移来的光电子会留在电位阱中，这样就可以通过电荷往返迁移来对像素进行反复读出而不消除。③消除性模式：此时两个电极的电压均为负，累加的光电子将会流溢，或注射进电极间芯片硅层中，并在电路中激发处脉冲。同样，脉冲的幅度反映了累加的光电子数。但这个过程将迁移来的光电子排除处电位阱，所以用来“清零”。\n（3）性能指标\n<1>线性响应：指输入物理信号的强度与输出响应信号的强度之间的关系是否线性。\n<2>灵敏度：绝对灵敏度可用所能检测到的最小光子个数表示，相对灵敏度可用能使输出发生一个级别的变化所需的光子个数表示。\n<3>信噪比：指所采集的图像中有用信号与无用干扰的（强度或能量）比值。\n<4>阴影（不均匀度）:指输入物理信号为常数而输出的数字形式部位常数的现象。\n<5>快门速度：对应每次采集拍摄所需的时间。\n<6>读取速率：指信号数据从敏感单元读取信号（传输）的速率。\n2.采集模型\n（1）几何成像模型\n图像采集的过程从几何角度可看作是一个将客观世界的场景通过投影进行空间转化的过程，例如用照相机或摄像机进行图像采集时要将3-D客观场景投影到2-D图像平面，这个投影过程可以用投影变换（也称为成像变化或几何透视变换）描述。一般情况下，客观场景、摄像机和图像平面各有自己不同的坐标系统，所有投影成像涉及到在不同坐标系统之间的转换。\n<1>世界坐标系统:也称真实或现实世界坐标系统XYZ，它是客观时间的绝对坐标。一般的3-D场景都是用这个坐标系统来表示的。\n<2>摄像机坐标系统：是以摄像机为中心制定的坐标系统xyz，一般去摄像机的光学轴为z轴。\n<3>图像平面坐标系统：是在摄像机内形成的图像平面的坐标系统x'y'。一般取图像平面与摄像机坐标系统的xy平面平行，且x轴与x'轴、y轴与y'轴分别重合，这样图像平面的原点就在摄像机的光学轴上。\nPS：归一化摄像机指焦距为1的特定摄像机，也是一种简化的重合模型。\n实际上使用的焦距并不总是1，且在图像平面上是使用像素而不是物理距离来表示位置的。图像平面坐标与世界坐标的联系是(s是尺度因子)，焦距的改变和传感器中光子接受单元的间距变化都会影响平面坐标点与世界坐标点的联系。一个世界坐标系统中的点可用笛卡尔坐标矢量形式表示为,则该点对应的齐次坐标矢量实现为。\n当摄像机与世界坐标不重合时，可通过平移和旋转重合，其中平移矩阵为,xX轴夹角为,zZ轴夹角为\n（2）亮度模型\n图像灰度是由景物亮度转化而来的，成像时如何将景物亮度转化为图像灰度可以遵循一定规律。其由入射到可见景物上的光通量和景物对入射光反射的比率，分布用照度函数和反射函数表示，即。\n3.成像方式\n图像采集方式主要由光源、采集器和景物三者的相对关系所决定。根据光源、采集器和景物三者不同的相互位置和运动情况，可构成多种成像方式。①让光源固定，用一个采集器在一个固定位置对场景取一幅像，亦称单目成像；②如果用多个采集器各在一个位置对同一场景取像就是多目成像；③让采集器相对于景物固定而将光源绕景物运动，就是构成光移成像；如果保持光源固定而让采集器进行运动以跟踪场景是主动视觉成像；④让采集器和景物同时运动，为主动视觉自运动成像；⑤用可控光源照射景物，通过采集到的投影模式来解释景物的表面形状为结构光成像。\n(1)结构光法\n该方法是采集图像是直接获取深度信息的方法，基本思想是用照明中的几何信息来帮助提取景物自身的几何信息。结构光测距成像系统主要有光源和摄像机两部分组成，与被观察物体成一个三角形。光源产生一系列的点或线激光并照射到物体表面，再由摄像机将景物被照明的部分记录下来，最后通过三角计算来获得深度信息，也称主动三角测距法。其测距精度可达微米级别，深度场范围是测距精度的几百或者几万倍。\n其具体方式有很多：采用光条法、栅格法、圆形光条线、交叉线、厚光条、空间编码模版法、彩色编码条、密度比例等，由于投射光束的结构不同，拍摄方式和深度距离计算方法也不同，但共同点是利用了摄像机与光源之间的结构关系。\n在基本的光条法中，使用单个的光平面依次（移动或旋转）照射景物各部分，每次使景物上仅出现一个光条且仅让光条部分被检测到，每照射一次就会得到一张二维实体图，再通过摄像机视线与光平面的交点，就可以得到光条上可见图像点所对应的空间点第三维信息。\n,将代入，Z可表示为,其中成像高度包含了3-D深度信息，或者说深度是成像高度的函数。\n结构光成像不仅能给出空间点的距离Z，也能给出沿Y方向的物体厚度，这时可借助从摄像机顶部向下所观察到的yz平面上的成像金银珠宝产业，其中w为成像宽度：,式中镜头中心到W点在Z轴垂直投影的距离：,而,就可得到。"}
{"content2":"一、经典任务\n计算机视觉领域中和目标有关的经典任务有三种：分类、检测和分割。其中分类是为了告诉你「是什么」，后面两个任务的目标是为了告诉你「在哪里」，而分割任务将在像素级别上回答这个问题。\n二、几种专业名词的含义\n目标检测，搜索系统感兴趣的目标区域；\n目标跟踪，捕获感兴趣区域的运动轨迹；\n目标分类，將被跟踪目标分为人，汽车或其他移动物体；\n目标行为识别，对跟踪目标进行行为识别。\n立体视觉匹配，是一种从平面图像中恢复深度信息的技术。\n光流，是指图像灰度模式的表面运动，是三维运动场在二维图像平面上的投影。\n场景流，是空间中场景运动形成的三维运动场。两者在视频跟踪与监控，自主机器人导航，三维视频压缩与显示等领域有着广泛的应用。\n三、KITTI数据集（非常重要）\n对于KITTI数据集，该数据集用于评测立体图像(stereo)，光流(optical flow)，视觉测距(visual odometry)，3D物体检测(object detection)和3D跟踪(tracking)等计算机视觉技术在车载环境下的性能。\n四、几种深度学习算法含义\nFCNN：全卷积网络：并行，迭代\nCNN：FCNN基础上加入空间结构参数共享\nRNN：FCNN基础上加入时间结构参数共享\n五、论文汇总总结\n1、机器学习，深度学习在图像，语音等富媒体的分类和识别上取得了非常好的效果，所以各大研究机构和公司都投入了大量的人力做相关的研究和开发。值得我们学习。\n2、光流法，场景流法，立体视觉匹配对于视频处理有至关重要的功能。\n3、KITTI官网上有大量关于光流，场景流和立体视觉匹配算法以及对比，我们可以借鉴并加以引用。\n六、论文以及源码常用网站\nCVPR2017 http://openaccess.thecvf.com/CVPR2017.py\nICCV2017 http://openaccess.thecvf.com/ICCV2017.py\nKITTI算法 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php\n七、几个搜源码的国内网站：\n1、http://blog.sina.com.cn/s/blog_72d206b7010105v5.html\n2https://ymcn.org/search.php?keyword=%E5%85%89%E6%B5%81%E5%9C%BA&cateid=0&sort=&sortby=&sm=0&os=0&platform=0&language=0&file=0&page=4\n3、http://www.tk4479.net/yf0811240333/article/details/42076677\n4、http://blog.sciencenet.cn/blog-4099-638485.html\n5、https://lmb.informatik.uni-freiburg.de/resources/binaries/\n八、有关人工智能比较好的开源社区\n极市 http://cvmart.net/community\n机器之心 https://www.jiqizhixin.com/\nhttps://zhuanlan.zhihu.com/jiqizhixin\n无痛的机器学习 https://zhuanlan.zhihu.com/hsmyy\n深度学习整理笔记系列 http://blog.csdn.net/zouxy09/article/details/8775360\n廖雪峰Python教程：\nhttps://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000\n光流法：http://blog.csdn.net/zouxy09/article/details/8683859\nhttp://blog.csdn.net/crzy_sparrow/article/details/7407604\nhttp://bbs.elecfans.com/jishu_485979_1_1.html\n场景流：http://www.doc88.com/p-2344567483739.html\nhttp://paopaorobot.org/2017/04/30/%E7%AC%AC%E5%9B%9B%E5%8D%81%E4%B8%80%E8%AF%BE%EF%BC%9A%E8%A7%86%E8%A7%89%E5%9C%BA%E6%99%AF%E6%B5%81-%E9%99%88%E9%BE%99/\nhttp://www.rosclub.cn/post-963.html"}
{"content2":"近年来，我们随处可以听到一个词，“人工智能”。机器的智能化成为了现今的一大研究热点，而机器要变得更加智能，必然少不了对外界环境的感知。有研究表明，人对外界的环境的感知70%以上来自人类的视觉系统，机器也是如此，大多数的信息都包含在图像中，人工智能的实现少不了计算机视觉。那么计算机视觉具体有哪些应用呢？\n无人驾驶\n无人驾驶又称自动驾驶，是目前人工智能领域一个比较重要的研究方向，让汽车可以进行自主驾驶，或者辅助驾驶员驾驶，提升驾驶操作的安全性。目前已经有一些公司研发出了自动泊车等辅助驾驶功能并得以应用。目前这方面做得比较好的是谷歌的无人驾驶汽车。国内也有一些比较好的公司，如百度无人驾驶车已经在一些园区得以应用，还有图森未来的货运车也完成了多次路测，并已经投入市场使用。\n计算机视觉在无人驾驶中起到了非常关键的作用，比如道路的识别，路标的识别，红绿灯的识别，行人识别等等平常驾驶过程中需要注意的。另外还包括三维重建及自主导航，通过激光雷达或者视觉传感器可以重建三维模型，辅助汽车进行自主定位及导航，进行合理的路径规划和相关决策。\n2.人脸识别\n人脸识别技术目前已经研究得相对比较成熟，并在很多地方得到了应用，且人脸识别准确率目前已经高于人眼的识别准确率，很多高铁站及门禁的地方都用到了人脸识别，很多都有刷脸系统，有些城市甚至在银行取钱都可以直接刷脸。\n3.无人安防\n安防一直是我国比较重视的问题，也是人们特别重视的问题，在很多重要地点都安排有巡警巡查，在居民小区以及公司一般也都有保安巡查来确保安全。随着计算机视觉的发展，计算机视觉技术已经能够很好的应用到安防领域，目前很多智能摄像头都已经能够自动识别出异常行为以及可疑危险人物，及时提醒相关安防人员或者报警，加强安全防范。\n4.车辆车牌识别\n车辆车牌识别目前已经是一种非诚成熟的技术了，高速路上的违章检测，车流分析，安全带识别，智能红绿灯，还有停车场的车辆身份识别等都用到了车辆车牌识别，不仅能识别出车牌的号码，目前车辆识别技术已经能对道路上的车辆车型进行识别，通过识别摄像头获取的图像，能获取到车辆的型号及颜色等特征。\n5.智能识图\n智能识图是我们生活中比较常见的计算机视觉的应用了。看到一个纸质文档，想要把其转换成电子文档，直接把文档拍下来，用相关软件进行文字识别，就能把图像中的文字自动转换成电子文档，甚至还能自动翻译成其他语言。看到一件衣服或一个物品，想在网上找他的来源等其他相关信息，直接输入图片，以图搜图，很快就能找到很多该图片出现的地方以及很多类似的图片。甚至还有些能直接告诉你图片中的物体是个什么东西，或者大概判断图片中的人像的大概年龄等比较好玩的功能。\n6.3D重构\n3D重构之前在工业领域应用比较多，可以用于对三维物体进行建模，方便测量出物体的各种参数，或者对物体进行简单复制。最近也慢慢开始应用到民用领域了，比如新出的华为mate20系列手机，就已经可以对玩偶进行三维建模，并能够设置一些特定的动作，让玩偶“活”起来，甚至可以与人进行一些互动。当然这里与人互动还用到AR技术。\n7.VR/AR\nVR/AR技术相信大家都已经比较熟悉了。VR眼睛在前两年卖得特别火爆，还有一些9D游戏机，就是利用VR技术让人能够有一种身临其境的感觉。而AR技术目前比较常见的可能是双十一时候淘宝的天猫，还有之前比较火爆的宠物捕获游戏，可以在现实场景中加入一些其他元素，目前这个领域还在快速发展中。很多方便人们生活的应用也在不断推出，比如智能翻译，用手机对着需要翻译的字，在这个界面上就自动显示出相关的翻译，或者后面可能实现的虚拟试衣间等，将大大的方便人们的生活。\n8.智能拍照\n这个相信是大家很熟悉的一个名词了，基本每个智能手机都开始配有这个功能。最基础的功能包括自动曝光，自动白平衡，自动对焦等，还有一些去燥算法，能很好的提高手机拍照的图像质量。随着计算机视觉技术的进步，一些自动美颜算法，自动挂件，自动滤镜，场景切换等越来越多有趣的功能都被开发出来。还有一些图像处理软件，像专业的Photoshop，还有比较民用化的美图秀秀，美颜相机等，基本也都是利用计算机视觉的技术。\n9.医学图像处理\n常见的医学成像，比如B超，核磁共振，X光拍片等。随着AI技术的发展，还开始有一些AI诊断的功能，AI根据图像的特征对相关疾病的可能性进行分析。\n10.无人机\n随着无人机技术的发展，计算机视觉技术在无人机上的应用必不可少，军用无人机中，可以对目标进行自动识别并自主导航，精确制导等，民用的无人机也类似，例如大疆的无人机，能够跟踪人进行实时的拍照，还有一些手势控制等。还有一些特殊场景的应用，例如电力巡检，农作物分析等。\n11.工业检测\n工业领域计算机视觉也得到了充分应用，例如产品缺陷检测，工业机器人姿态控制，利用立体视觉来获得工件和机器人之间的相对位置姿态。\n12.其他\n计算机视觉还有很多应用，随着技术的发展，应用领域也会越来越多。在工业领域的应用，在机器人技术方面的应用，这里就不一一的赘述了。相信随着计算机视觉技术的不断发展，我们的生活能够越来越智能化，便捷化。\n因为机器视觉课程刚好布置了这门作业，所以大概总结了一下，自己之前一段时间也在找计算机视觉相关的工作，所以多数还是根据自己对这个行业的了解来写的，难免和其他相关的总结有点类似。转载请注明出处。\n参考文献：\nhttps://36kr.com/p/5074487.html\nhttp://wb.qdqss.cn/html/qdwb/20180226/qdwb303829.html"}
{"content2":"本文首发于GitChat，未经授权不得转载，转载需与GitChat联系。\n背景\n当前，人工智能是下一代信息技术的核心和焦点，而无人配送则是人工智能典型的落地场景，因为完成无人配送需要自动驾驶技术、机器人技术、视觉分析、自然语言理解、机器学习、运筹优化等一系列创新技术的高度集成。目前，美团的日订单数量已经超过 2000 万单，在人力有限的情况下，对无人配送就有着非常迫切的需求。美团无人配送团队已经自主研发两款适应不同业务场景的无人车产品，其中一款适用于室内外道路的小型低速无人车，还有一款长距离室外运输的中型无人车。\n目标检测和语义分割技术简介\n目标检测\n目标检测是在一幅图片中找到目标物体，给出目标的类别和位置，如下图所示：\n在 2014 年以前，目标检测通常采用比较传统的方法，先想办法生成一些候选框，然后提取出每个框的特征，例如 HOG，最后通过一个分类器来确认这个框是否是目标物体。而生成候选框的方式也有很多种，比如用不同大小的预选框在图片中滑动，或者像 Selective Search[1] 算法一样，可以根据图片本身的纹理等特征生成一些候选框。\n但是自 2013 年以来，随着深度学习相关技术的发展，不断有新的模型出现，可以实现端到端的训练检测网络，并且效果比传统方法有了显著的提高。目标检测的发展脉络如下图所示：\n目标检测方法分为 One-Stage 和 Two-Stage 两种。两步检测算法是把整个检测过程分为两个步骤，第一步提取一些可能包含目标的候选框，第二步再从这些候选框中找出具体的目标并微调候选框。而一步检测算法则是省略了这个过程，直接在原始图片中预测每个目标的类别和位置。两步检测最经典的就是 Faster-RCNN[4] 三部曲。R-CNN[2] 是比较早期提出用深度学习解决检测的模型。\n思路是先用 Selective Search 算法提取一定数量的候选区域，然后对于每个候选区域使用 CNN 提取特征，最后在提出的特征后面接一个回归和 SVM 分类，分别预测目标物体的位置和类别。\nR-CNN 的优点是使用了 CNN 提出的特征，效果比较好。当然，缺点也很明显，整个过程分成了好几步，无法完整的训练，另外由于每个候选框提特征是独立计算的，整个过程包含了大量的冗余计算。\nFast-RCNN[3] 是在此基础上的一个改进版本，主要解决了提取特征时冗余计算的问题。首先对整张图片做卷积，提取特征得到一层 Feature Map，然后再提取每个候选框的特征时直接在这个 Feature Map 上面提取特征。\n但是每个候选框的尺寸是不一样的，而后面做分类和回归时要求特征必须定长。为了解决这个问题 Fast-RCNN 中提出了 Roi-Pooling 层，可以对不同大小的区域提取出固定维度的特征，使得后面的分类和回归可以正常运行。这个模型整体减少了大量的冗余计算，提高了整个模型的运行速度。\nFaster-RCNN[4] 是三部曲的最后一步。Fast-RCNN 存在的问题是：提取候选区域仍然是使用的 Selective Search 算法，打乱了整个模型的连续性。\nFaster-RCNN 为了改进这点提出了 RPN 结构，RPN 可以在 Feature Map 的每个位置提取很多不同尺寸、不同形状的候选框，也叫 Anchor。每个 Anchor 后会跟一个二值分类，来判断这个 Anchor 是否是背景，并接一个回归对位置进行微调。具体的类别和位置在网络尾端还会进一步调整。至此，整个目标检测过程都可以实现端到端的进行训练。\n在一步检测中，比较经典的是 YOLO[5] 和 SSD[6] 模型。这里介绍下 YOLO 算法。首先将图片划分为 NxN 的方格，每个方格预测 C 个类别概率，表示某类目标中心落在这个方格的概率，并且预测 B 组位置信息，包含 4 个坐标和 1 个置信度。整个网络输出 NxNx(5xB+C) 的 Tensor。YOLO 的优点是：省略两步检测中提区域的步骤，所以速度会比较快，但是它对于密集小物体的识别很不好。后续的 YOLOv2[7] 和 YOLOv3[8] 都对此做出了很多的改进。\n分割\n分割，是一个对图片中的像素进行分类的问题。分割最初分为语义分割和实例分割。\n语义分割是对图片中每一个像素都要给出一个类别，例如地面、树、车、人等。而实例分割则和目标检测比较像，但是实例分割是要给出每个目标的所有像素，并且同一种类别不同目标要给出不同的 ID，即可以将每个目标清晰的区分开。今年，有人研究将语义分割和实例分割统一在一起，称为全景分割，如下图所示：\n在无人驾驶中应用比较多的是语义分割。例如路面分割、人行横道分割等等。语义分割比较早期和经典的模型是 FCN[9]。FCN 有几个比较经典的改进，首先是用全卷积层替换了全连接层，其次是卷积之后的小分辨率 Feature Map 经过上层采样，再得到原分辨率大小的结果，最后 FCN 使用了跨层连接的方式。跨层连接可以将高层的语义特征和底层的位置特征较好地结合在一起，使得分割的结果更为准确。FCN 结构图如下所示：\n目前很多主流的分割模型准确率都比较高，但是帧率会比较低。而无人驾驶的应用场景中模型必须实时，尤其是高速场景下，对模型的速度要求更高。目前美团使用的是改进版的 ICNet[10]，既保证了模型的运行速度，又保证了模型的准确率。下图是一些经典分割模型的时间和准确率对比图：\n无人驾驶相关介绍\n传感器\n在无人驾驶中，车辆在行驶时需要实时地去感知周围的环境，包括行驶在哪里、周围有什么障碍物、当前交通信号怎样等等。就像我们人类通过眼睛去观察世界，无人车也需要这样一种 “眼睛”，这就是传感器。传感器有很多种，例如激光雷达、摄像头、超声波等等。\n每种传感器都有自己的特点：摄像头可以包含丰富的颜色信息，可以识别各种精细的类别，但是在黑暗中无法使用；激光可以在黑暗或强光中使用，但是雨天无法正常工作。目前不存在一种传感器可以满足不同的使用场景，所以目前业界通常会通过传感器融合的方式来提高准确率，也能够弥补缺点。各种传感器的特点可以查看下图：\n不同的传感器的数据格式有很大差别，所以也会有专门针对某种传感器数据设计的算法。例如有专门针对激光点云设计的障碍物检测模型VoxelNet。\n目标检测\n由于摄像头数据（图片）包含丰富的颜色信息，所以对于精细的障碍物类别识别、信号灯检测、车道线检测、交通标志检测等问题就需要依赖计算机视觉技术。无人驾驶中的目标检测与学术界中标准的目标检测问题有一个很大的区别，就是距离。无人车在行驶时只知道前面有一个障碍物是没有意义的，还需要知道这个障碍物的距离，或者说需要知道这个障碍物的 3D 坐标，这样在做决策规划时，才可以知道要用怎样的行驶路线来避开这些障碍物。这个问题对于激光的障碍物检测来说很容易，因为激光本身就包含距离信息，但是想只凭借图片信息去计算距离难度比较高。\n分割\n分割技术在无人驾驶中比较主要的应用就是可行驶区域识别。可行驶区域可以定义成机动车行驶区域，或者当前车道区域等。由于这种区域通常是不规则多边形，所以分割是一种较好的解决办法。与检测相同的是，这里的分割同样需要计算这个区域的三维坐标。如果我们分割的目标都是地面的话，就可以使用“距离估计”中第5种方式获得精确的三维空间中的区域坐标，这种应用在未来对无人驾驶有着巨大的意义，因为现在的无人驾驶都是基于高精地图，而这种基于可行驶区域的方案是一种脱离高精地图的方案。当然这种方案目前也只能在限定场景下应用，还不是很成熟。\n距离估计\n对于距离信息的计算有多种计算方式：\n激光测距，原理是根据激光反射回的时间计算距离。这种方式计算出的距离是最准的，但是计算的输出频率依赖于激光本身的频率，一般激光是 10Hz。\n单目深度估计，原理是输入是单目相机的图片，然后用深度估计的 CNN 模型进行预测，输出每个像素点的深度。这种方式优点是频率可以较高，缺点是估出的深度误差比较大。\n结构光测距，原理是相机发出一种独特结构的结构光，根据返回的光的偏振等特点，计算每个像素点的距离。这种方式主要缺点是结构光受自然光影响较大，所以在室外难以使用。\n双目测距，原理是根据两个镜头看到的微小差别，根据两个镜头之间的距离，计算物体的距离。这种方式缺点是计算远处物体的距离误差较大。\n根据相机内参计算，原理跟小孔成像类似。图片中的每个点可以根据相机内参转化为空间中的一条线，所以对于固定高度的一个平面，可以求交点计算距离。通常应用时固定平面使用地面，即我们可以知道图片中每个地面上的点的精确距离。这种计算方式在相机内参准确的情况下精度极高，但是只能针对固定高度的平面。\n业界相关进展\n目前业界开源的解决方案中比较成熟的是百度的 Apollo[11]，包含了改进的 ROS 底层系统，以及无人驾驶中各个模块的实现。Apollo 中视觉方案的距离计算非常有意思，这里简单介绍一下。Apollo 使用一个模型去预测 2D 图片中物体的框，以及物体实际的在三维空间中的长宽高和朝向。当我们知道一个物体在三维空间中的位置和姿态（长宽高和朝向）时，我们可以根据相机内参，计算这个物体投影到图片中的所在区域。\n那么如果我们知道物体在图像中的区域和在三维空间中的姿态，我们如何计算三维位置呢？可以根据近大远小的特点，去二分物体离我们的距离。这就是 Apollo 中视觉方案的距离计算方法。\n除了 Apollo 之外，业界开源解决方案还有 Autoware[12]。虽然 Autoware 并没有 Apollo 火热，但是也给我们提供了一些解决问题的思路。Autoware 的视觉方案通过激光与摄像头联合标定的方式将每个激光点转换到图像之中，并进一步根据 2D 检测结果，知道哪些激光点打到了这个物体上，由于激光点的三维坐标是已知的，就可以计算出这个物体的距离。\n美团自研算法\n美团的自研算法参考了 Autoware 的这种解决思路，并做了很多改进。同样先将激光点转换到图片当中，这样我们就知道每个激光点打到了哪里。在得到每个 2D 框中的激光点之后，我们需要做一步聚类操作，这样可以过滤掉打到背景上的点，于是我们就得到了打到这个物体上的激光点（参看下图红点）。然后在三维空间中，我们可以拟合这些激光点，得到一个三维框，包含了物体准确的位置信息。\n这种方法计算出的三维框相对比较准确，但缺点是对于远处较小的物体，由于打到的激光点太少了，难以拟合出合适的结果。具体效果可以参看下图：\n总结\n本文从计算机视觉方向的检测和分割出发，介绍了相关算法原理，以及其在无人驾驶中的实际应用。之后介绍了距离估计的一些算法，以及业界的一些视觉解决方案。目前业界主流的的无人车障碍物感知是依赖于激光的，视觉方案相对还不是很成熟。但是我们仍然比较看好视觉方案，因为其成本低，并且可以减轻对高精地图的依赖。\n>>> 查看交流实录，请扫描二维码 <<<\n更多精彩信息，请点击阅读原文↓↓↓"}
{"content2":"人工智能 机器学习/深度学习 知识点混记\n计算机视觉模型 benchmark\n卷积层计算公式\n特征图尺寸\n参数量\n卷积动态过程示意图\n常用机器学习算法选择路线\n矩阵相乘的理解\n卷积公式理解\n卷积与傅里叶变换\n向量的点积\n计算机视觉模型 benchmark\n卷积层计算公式\n特征图尺寸\n参数量\nP\n=\nF\nw\n∗\nF\nh\n∗\nD\np\n∗\nD\nc\n+\nb\nP=F_w*F_h*D_p*D_c+b\nP=Fw ∗Fh ∗Dp ∗Dc +b\nP\n:\nP:\nP: 参数量\nF\n:\nF:\nF: 卷积核长宽尺寸\nD\nD\nD 输入维度和输出维度\nb\n:\nb:\nb: 偏置数量\n卷积动态过程示意图\n常用机器学习算法选择路线\n矩阵相乘的理解\n以 A 为输入矩阵 X，行为特征数，列为特征维度\n矩阵 A * B（A左乘B） 可以看作以 B 列向量为卷积核对 A 进行隔离卷积（卷积核尺寸为向量长度，类似于 Network In Network 的思想）\nA 的行为特征维度，代表特征个数，A 的列数为向量维度就是通道维数。B 中每一列都是一个滤波器。\n可以把 B 想象为一个单层全连接神经网络，A * B 就相当于使用B对 A中每行的向量进行神经网络变换。所以让 A 乘以一个矩阵，就相当于使用一个线性神经网络拓展 A 的列（通道）维度。\n矩阵 B * A（A右乘B） 可以看作对A的转置（以列为特征数，行为特征维度）进行隔离卷积\n卷积公式理解\n以卷积层的角度来理解，x 即卷积核的窗口面积（周围邻域数），t 为滑窗时的索引。卷积层中的卷积实际上为离散卷积。\n卷积与傅里叶变换\n卷积 = 特征与权重在频域相乘\n推导如下：\n参考\n向量的点积\n向量 a 点乘向量 b 可以看作把向量 a 的 x，y 轴基向量变换为基向量 b（同时投影后加和），降二维空间压缩至一维，获得 a 在基向量变换后在一维空间的值，即 a 在 b 上的投影 * b 的模长。\n参考"}
{"content2":"在很多文献中，计算机视觉与机器视觉是不加区分的，但其实这两个术语既有区别又有联系。计算机视觉是采用图像处理、模式识别、人工智能技术相结合的手段，着重于一幅或多幅图像的计算机分析。机器视觉则偏重于计算机视觉技术工程化，能够自动获取和分析特定图像，以控制相应的行为。\n1.计算机视觉\n计算机视觉是指用摄像机和电脑及其他相关设备，对生物视觉的一种模拟。它的主要任务是通过对采集的图片或视频进行处理以获得相应场景的三维信息，就像人类和许多其他类生物每天所做的那样。\n计算机视觉的最终目标是使计算机能像人那样通过视觉观察和理解世界，具有自主适应环境的能力。但能真正实现计算机能够通过摄像机感知这个世界却是非常之难，因为虽然摄像机拍摄的图像我们平时所见一样，但对于计算机来说，任何图像都只是如上图右半边所示的像素值排列，是一堆死板的数字。如何让计算机从这些死板的数字里面读取到有意义的视觉线索，是计算机视觉应该解决的问题。\n然而，计算机视觉发展多年，却依然存在着一系列难以解决的难题。目前人们掌握的具体计算机视觉任务的方法，也仅仅适用于狭隘的人脸识别、指纹识别等简单任务，无法广泛的应用于不同场合。不过，也有学者认为，随着机器学习方法的日渐普及以及大数据科技的应用，计算机视觉实现质的突破也是指日可待的。\n2.机器视觉\n机器视觉是人工智能正在快速发展的一个分支。简单说来，机器视觉就是用机器代替人眼来做测量和判断。机器视觉系统是通过机器视觉产品将被摄取目标转换成图像信号，传送给专用的图像处理系统，得到被摄目标的形态信息，根据像素分布和亮度、颜色等信息，转变成数字化信号;图像系统对这些信号进行各种运算来抽取目标的特征，进而根据判别的结果来控制现场的设备动作。\n机器视觉是一项综合技术，包括图像处理、机械工程技术、控制、电光源照明、光学成像、传 感器、模拟与数字视频技术、计算机软硬件技术(图像增强和分析算法、图像卡、 I/O卡等)。一个典型的机器视觉应用系统包括图像捕捉、光源系统、图像数字化模块、数字图像处理模块、智能判断决策模块和机械控制执行模块。\n机器视觉系统的特点是提高生产的柔性和自动化程度。在一些不适合于人工作业的危险工作环境或人工视觉难以满足要求的场合，常用机器视觉来替代人工视觉；同时在大批量工业生产过程中，用人工视觉检查产品质量效率低且精度不高，用机器视觉检测方法可以大大提高生产效率和生产的自动化程度。而且机器视觉易于实现信息集成，是实现计算机集成制造的基础技术。上图便是机器视觉的一个典型应用。\n3.计算机视觉与机器视觉的异同\n毋庸置疑，计算机视觉与机器视觉在技术和应用领域上都有相当大的重叠，这表明这两个学科的基础理论大致是相同的，但细究其机理，确实也有一些不同之处：\n计算机视觉的研究对象主要是映射到单幅多幅图像上的三维场景。计算机视觉的研究很大程度上是针对图像的内容。如下图所示，如何让计算机判断出图片中都是猫，才是计算机视觉研究的内容。\n机器视觉主要是指工业领域的视觉研究，例如自主机器人的视觉，用于检测和测量的视觉。这表明在这一领域通过软件硬件，图像感知与控制理论往往与图像处理得到紧密结合来实现高效的机器人控制或各种实时操作。举个不恰当的例子，还是上图中的猫，机器视觉是观察上图中成百上千个某一特定形态的猫，发现哪只猫缺只耳朵，然后把它剔除出去。\n原文地址：https://www.sohu.com/a/211810074_359183"}
{"content2":"新的开始！\n人工智能：计算机视觉、图像处理、模式识别、机器学习之间的关系\n什么是人工智能呢？人工智能，是由人类设计并在计算机环境下实现的模拟或再现某些人智能行为的技术。一般认为，人类智能活动可以分为两类：感知行为与思维活动。模拟感知行为的人工智能研究的一些例子包括语音识别、话者识别等与人类的听觉功能有关的“计算机听觉”，物体三维表现的形状知识、距离、速度感知等与人类视觉有关的“计算机视觉”，等等。模拟思维活动的人工智能研究的例子包括符号推理、模糊推理、定理证明等与人类思维有关的“计算机思维”，等等。\n人工智能领域：机器学习 深度学习 图像算法 图像处理 语音识别 图像识别 算法研究\n从图像处理和模式识别发展起来的计算机视觉研究对象之一是如何利用二维投影图像恢复三维景物世界。计算机视觉使用的理论方法主要是基于几何、概率和运动学计算与三维重构的视觉计算理论，它的基础包括射影几何学、刚体运动力学、概率论与随机过程、图像处理、人工智能等理论。计算机视觉要达到的基本目的有以下几个：\n(1) 根据一幅或多幅二维投影图像计算出观察点到目标物体的距离；\n(2) 根据一幅或多幅二维投影图像计算出目标物体的运动参数；\n(3) 根据一幅或多幅二维投影图像计算出目标物体的表面物理特性；\n(4) 根据多幅二维投影图像恢复出更大空间区域的投影图像。\n计算机视觉要达到的最终目的是实现利用计算机对于三维景物世界的理解，即实现人的视觉系统的某些功能。\n在计算机视觉领域里，医学图像分析、光学文字识别对模式识别的要求需要提到一定高度。又如模式识别中的预处理和特征抽取环节应用图像处理的技术；图像处理中的图像分析也应用模式识别的技术。在计算机视觉的大多数实际应用当中，计算机被预设为解决特定的任务，然而基于机器学习的方法正日渐普及，一旦机器学习的研究进一步发展，未来“泛用型”的电脑视觉应用或许可以成真。\n人工智能所研究的一个主要问题是：如何让系统具备“计划”和“决策能力”？从而使之完成特定的技术动作（例如：移动一个机器人通过某种特定环境）。这一问题便与计算机视觉问题息息相关。在这里，计算机视觉系统作为一个感知器，为决策提供信息。另外一些研究方向包括模式识别和机器学习（这也隶属于人工智能领域，但与计算机视觉有着重要联系），也由此，计算机视觉时常被看作人工智能与计算机科学的一个分支。\n机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演译。\n为了达到计算机视觉的目的，有两种技术途径可以考虑。第一种是仿生学方法，即从分析人类视觉的过程入手，利用大自然提供给我们的最好参考系——人类视觉系统，建立起视觉过程的计算模型，然后用计算机系统实现之。第二种是工程方法，即脱离人类视觉系统框框的约束，利用一切可行和实用的技术手段实现视觉功能。此方法的一般做法是，将人类视觉系统作为一个黑盒子对待，实现时只关心对于某种输入，视觉系统将给出何种输出。这两种方法理论上都是可以使用的，但面临的困难是，人类视觉系统对应某种输入的输出到底是什么，这是无法直接测得的。而且由于人的智能活动是一个多功能系统综合作用的结果，即使是得到了一个输入输出对，也很难肯定它是仅由当前的输入视觉刺激所产生的响应，而不是一个与历史状态综合作用的结果。\n不难理解，计算机视觉的研究具有双重意义。其一，是为了满足人工智能应用的需要，即用计算机实现人工的视觉系统的需要。这些成果可以安装在计算机和各种机器上，使计算机和机器人能够具有“看”的能力。其二，视觉计算模型的研究结果反过来对于我们进一步认识和研究人类视觉系统本身的机理，甚至人脑的机理，也同样具有相当大的参考意义。\n在我的理解里，要实现计算机视觉必须有图像处理的帮助，而图像处理倚仗与模式识别的有效运用，而模式识别是人工智能领域的一个重要分支，人工智能与机器学习密不可分。纵观一切关系，发现计算机视觉的应用服务于机器学习。各个环节缺一不可，相辅相成。\n计算机视觉（computer vision），用计算机来模拟人的视觉机理获取和处理信息的能力。就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，用电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。\n图像处理（image processing），用计算机对图像进行分析，以达到所需结果的技术。又称影像处理。基本内容图像处理一般指数字图像处理。数字图像是指用数字摄像机、扫描仪等设备经过采样和数字化得到的一个大的二维数组，该数组的元素称为像素，其值为一整数，称为灰度值。图像处理技术的主要内容包括图像压缩，增强和复原，匹配、描述和识别3个部分。常见的处理有图像数字化、图像编码、图像增强、图像复原、图像分割和图像分析等。图像处理一般指数字图像处理。\n模式识别(Pattern Recognition)是指对表征事物或现象的各种形式的(数值的、文字的和逻辑关系的)信息进行处理和分析,以对事物或现象进行描述、辨认、分类和解释的过程,是信息科学和人工智能的重要组成部分。模式识别又常称作模式分类，从处理问题的性质和解决问题的方法等角度，模式识别分为有监督的分类（Supervised Classification）和无监督的分类(Unsupervised Classification)两种。模式还可分成抽象的和具体的两种形式。前者如意识、思想、议论等,属于概念识别研究的范畴,是人工智能的另一研究分支。我们所指的模式识别主要是对语音波形、地震波、心电图、脑电图、图片、照片、文字、符号、生物传感器等对象的具体模式进行辨识和分类。模式识别研究主要集中在两方面,一是研究生物体(包括人)是如何感知对象的，属于认识科学的范畴,二是在给定的任务下,如何用计算机实现模式识别的理论和方法。应用计算机对一组事件或过程进行辨识和分类，所识别的事件或过程可以是文字、声音、图像等具体对象，也可以是状态、程度等抽象对象。这些对象与数字形式的信息相区别，称为模式信息。模式识别与统计学、心理学、语言学、计算机科学、生物学、控制论等都有关系。它与人工智能、图像处理的研究有交叉关系。\n机器学习(Machine Learning)是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习在人工智能的研究中具有十分重要的地位。一个不具有学习能力的智能系统难以称得上是一个真正的智能系统，但是以往的智能系统都普遍缺少学习的能力。随着人工智能的深入发展，这些局限性表现得愈加突出。正是在这种情形下，机器学习逐渐成为人工智能研究的核心之一。它的应用已遍及人工智能的各个分支，如专家系统、自动推理、自然语言理解、模式识别、计算机视觉、智能机器人等领域。机器学习的研究是根据生理学、认知科学等对人类学习机理的了解，建立人类学习过程的计算模型或认识模型，发展各种学习理论和学习方法，研究通用的学习算法并进行理论上的分析，建立面向任务的具有特定应用的学习系统。这些研究目标相互影响相互促进。\n人类研究计算机的目的，是为了提高社会生产力水平，提高生活质量，把人从单调复杂甚至危险的工作中解救出来。今天的计算机在计算速度上已经远远超过了人，然而在很多方面，特别是在人类智能活动有关的方面例如在视觉功能、听觉功能、嗅觉功能、自然语言理解能力功能等等方面，还不如人。\n这种现状无法满足一些高级应用的要求。例如，我们希望计算机能够及早地发现路上的可疑情况并提醒汽车驾驶员以避免发生事故，我们更希望计算机能帮助我们进行自动驾驶，目前的技术还不足以满足诸如此类高级应用的要求，还需要更多的人工智能研究成果和系统实现的经验。"}
{"content2":"技能简介\n1. Computer Science Fundamentals and Programming\n计算机科学基础和编程\n对机器学习工程师而言，计算机科学基础的重要性包括：\n1、数据结构(数据堆栈、队列、多位数组、树形以及图像等等)、\n2、算法(搜索、分类、优化、动态编程等)、\n3、计算性与复杂性(P对NP、NP完全问题、大O符号以及近似算法等)、\n4、计算机架构(存储、缓存、带宽、死锁和分布式处理等等)。\n当你在编程的时候必须能够对以上提到的这些基础知识进行应用、执行、修改或者处理。课后练习、编码竞赛还有黑客马拉松比赛都是你不可或缺的磨练技能的绝佳途径。\n2. Probability and Statistics\n概率论和数理统计\n1、概率的形式表征(条件概率、贝叶斯法则、可能性、独立性等)\n2、从其中衍生出的技术(贝叶斯网、马尔科夫决策过程、隐藏式马可夫模型等)是机器学习算法的核心，这些理论可以用来处理现实世界中存在的不确定性问题。\n3、统计学，这个学科提供了很多种衡量指标(平均值、中间值、方差等)、\n4、分布(均匀分布、正态分布、二项式分布、泊松分布等)\n5、分析方法(ANOVA、假设实验等)，\n这些理论对于观测数据模型的建立和验证非常必要。很多机器学习算法的统计建模程序都是可以扩展的。\n3. Data Modeling and Evaluation\n数据建模及评估\n数据建模就是对一个给定的数据库的基本结构进行评估的过程，目的就是发现其中所蕴含的\n1、有用模式(相互关系，聚合关系、特征矢量等)\n2、预测以前案例(分类，回归、异常检测等)的特征。\n3、评估过程的关键就是不断地对所给模型的优良性能进行评价。\n4、根据手中的任务，你需要选取一种恰当的精准/误差衡量指标(比如日志分类的损失、线性回归的误差平方和等等)\n5、求值策略(培训测试、连续Vs. 随机交叉验证等)。\n通过对算法的反复学习，我们可以发现其中会存在很多误差，而我们可以根据这些误差对模型(比如神经网络的反相传播算法)进行细微的调整，因此即使你想能够运用最基本的标准算法，也需要你对这些测量指标有所了解。\n4. Applying Machine Learning Algorithms and Libraries\n应用机器学习算法和库\n尽管通过程式库/软件包/API\n1、框架：(比如scikit-learn,Theano, Spark MLlib, H2O, TensorFlow等)\n可以广泛地实现机器学习算法的标准化执行，但是算法的应用还包括\n2、选取合适的模型\n(决策、树形结构、最近邻点、神经网络、支持向量机器、多模型集成等)、\n3、适用于数据的学习程序\n(线性回归、梯度下降法、基因遗传算法、袋翻法、模型特定性方法等)，\n4、同时还需要能够了解超参数对学习产生影响的方式。\n5、需要注意不同方式之间存在的优势和劣势，\n6、以及那些可能会让你受牵绊的大量陷阱\n(偏差和方差、高拟合度与低拟合度、数据缺失、数据丢失等)。\n对于数据科学和机器学习所带来的这些方面的挑战，大家可以去Kaggle网站获取很多学习参考，你可发现不同的问题当中存在的细微差别，从而可以让你更好的掌握机器学习的算法。\n5. Software Engineering and System Design\n软件工程和系统设计\n在每天工作结束的时候，机器学习工程师通常产生的成果或者应交付的产品就是一种软件。\n1、这种软件其实也是一种小型插件，它可以适用于相对更大型的产品或者服务的生态系统。\n2、你需要很好地掌握如何才能让这些彼此不同的小插件协同工作，并与彼此进行流畅的沟通(使用库函数调用、数据接口、数据库查询等)的方法，\n3、为了让其他的插件可以依附你的插件进行很好的工作，你也得需要为你的差价建立合适的接口。\n4、精心设计的系统可以避免以后可能出现的瓶颈问题，并让你的算法系统满足数据量激增时候的扩展性能。\n软件工程的最佳的实践经验\n(需求分析、系统设计、模块化、版本控制、测试以及归档等)\n5、对于产能、协作、质量和可维护性而言是不可获取的无价之宝。"}
{"content2":"第一讲 工欲善其事必先利其器——图像处理基础\n1, Opencv入门；\n2, 图像的基本操作（遍历图像，ROI选取等）\n（Python基础）\n应用：手写字符识别（KNN）\n第二讲 空域图像处理的洪荒之力\n1，线性非线性滤波器\n边缘检测方法：sobel，canny以及图像拉普拉斯\n应用：车牌识别（SVM+NN）\n第三讲：机器视觉中的特征提取与描述\n1，霍夫变换（普通，和概率）\n2，局部特征大汇总（GFTT，SIFT，SURF，FAST，ORB等等）\n第四讲：坐标变换与视觉测量 ###\n1，相机模型\n2，2D，3D，坐标变换\n3，相机标定\n应用：增强现实技术simple VR不神秘（第三讲第四讲综合演练）\n第五讲：数据处理并不难\n（ML&CV中的数据处理方法：PCA， SVD分解，聚类）\n应用：人脸识别之Eigenfaces\n第六讲：深度学习在图像识别中的应用\n分类：linear regression, neural networks\n检测：bounding box regression\n定位：localization\n应用：使用CNN进行图像识别\n第七讲：图像搜索\n图像搜索常用方法（聚类、降维等）\n大规模数据搜索（spark, data paralleling, etc）\n动作识别流程、方法\n第八讲：图像标注与问答\n语言模型介绍\nLSTM模型与标注问题\n应用：DenseCaption in Generating Captions in Images.\n第九讲：3D计算机视觉\n表面和外形重构\n基于模型的重构\n应用：人脸动画\n第十讲：机器视觉项目实战\nCV 实战：以鲸鱼识别为例，利用深度学习解决Kaggle竞赛中的图像分类问题"}
{"content2":"前情提要\n在【计算机视觉（一）图像数据表示】中，我介绍了RGB和灰度两种颜色空间，并且介绍了像素的概念以及在程序上如何访问。\n本期内容\n接下来介绍从RGB到灰度的转换，以及两种我常用的颜色空间HSV和二值空间（严格来说属于灰度，只是只有0和255两个值）。\n一、RGB转灰度\n假如先不谈原理，RGB转灰度你会怎么做？先从我们知道的信息入手，RGB是三通道的，灰度只有一个通道，很自然的会联想到怎么把三个通道“融合”成一个通道，最直接的想法，也许是对于同一个RGB像素值，我们把这三个通道值求一个平均值作为灰度值。用公式表示一下就是：Gray = R * 1/3 + G * 1/ 3 + B * 1/3 。\n好，那我们先来写这个程序看看效果。\n# coding: utf-8 import cv2 import numpy as np ''' 函数名：rgb2gray_mean 功能：通过求通道平均值得到灰度图 输入： img 输入的彩图 返回： result 灰度图 ''' def rgb2gray_mean(img): ratio = 1.0 / 3 # 转换类型 int_img = img.astype(np.int32) result = ratio * (int_img[...,0]+int_img[...,1]+int_img[...,2]) return result.astype(np.uint8) # 程序入口 def main(): # 读取lena图 color = cv2.imread('../lena.jpg') # 转灰度 gray = rgb2gray_mean(color) # 显示 cv2.imshow('color', color) cv2.imshow('gray', gray) cv2.waitKey(0) if __name__ == '__main__': main()\n注意：在rgb2gray_mean函数中我对img做了一个类型转换并存放在int_img中，img的类型是numpy.uint8，也就是8位无符号整数，直接对里面的值进行相加很可能造成数值溢出，也就是255+1会变成0，因此要先用一个能容纳更大的值范围的矩阵装起来，比如int32类型的。这是新手经常会遇到的坑，切记切记。\n好那我们来看看运行效果，再一次使用lena测试。\n运行的结果：\n看起来还行。\nOpenCV中也有自带的转换灰度的函数，cvtColor，代码也贴在这里：\ngray = cv2.cvtColor(color, cv2.COLOR_BGR2GRAY)\n注意第二个参数是BGR2GRAY，是的，OpenCV中默认的彩图通道排列是BGR（蓝绿红）而不是RGB，具体原因我也是道听途说，是因为一开始相机制造商从Sensor拿到的数据就是BGR的，这是他们制定的标准，尽管后来有很多软件也是默认采用RGB。\n那么这个cvtColor的效果怎么样呢，请看\n不知道你能否看出区别，不能的话请注意看头发的明暗交界处，cvtColor在本来该黑的地方更黑，该白的地方更白，就是对比度更强烈，还是看不出来的话用程序帮我们看看，数一下灰度值不相等的地方有多少个。\nprint np.sum(gray != cv_gray)\n结果是：244157， 原图的大小是512x512，也就是总共262144个像素点，那这么看下来绝大部分的像素值都不一样，虽然我们看起来也要费点力才能看出来不同。这就说明OpenCV转灰度的方法跟我们拍脑门想的是不一样的，也就是对应到三个通道各自乘的系数是不一样的，假设：\nGray = a * R + b * G + c * B (a + b + c = 1)\na、b、c三个值应该怎么取能让图像看起来比较舒服呢？其实人眼对三原色的“偏好”是不一样的，研究表明人眼对红绿蓝的权重接近3:6:1，更精确的，对于上面的公式，a=0.299，b=0.587，c=0.114，这就是通常所说的心理学模型。在OpenCV中，为了减少浮点运算（浮点运算在一般的CPU中很耗时），使用了类似下面的转换方式：\n''' 函数名：心理学模型转换灰度 输入： img 输入的彩图 返回： result 灰度图 ''' def rgb2gray_mental(img): # 转换类型 int_img = img.astype(np.int32) result = (int_img[...,2]*299 + int_img[...,1]*587 + int_img[...,0]*114 + 500) / 1000 return result.astype(np.uint8)\n经过测试，这跟OpenCV自带的转换函数cvtColor的效果是一模一样的。\n二、HSV空间\nHSV空间是由RGB空间演变过来的，RGB空间在几何上是一个正方体（详情请查阅【计算机视觉（一）图像数据表示】），而当你从正方体的一个顶点看向离它最远的另一个顶点时，就会看到一个六角锥体，这就是HSV空间的几何表达，如下图：\n通常，我们会把顶上的这个六边形近似成一个圆，就变成了一个圆锥，像这样：\nHSV三个轴的走动方向也在图上标出了，可以理解为一个有深度的极坐标系，我们只看Hue轴，沿着圆周方向走，可以看到每转动一定的角度，所表示的颜色就变了，而且不止红绿蓝，事实上这对应了我们认知上的所有颜色，因此，使用HSV空间的好处就是只要一个维度我们就能表示物体本来的颜色。那其他两个轴是表达什么信息的呢？具体可以参考百度百科，我这里只说些个人见解。S维度表示饱和度，更通俗的说，是打在物体上的白灯的亮度，比如你在黑夜中用手电筒照着个苹果，电量足够的话苹果看上去当然是正常的红色，电量不够也许看上去是暗红色，关掉手电也就啥都看不到黑漆漆一片了，这就是S维度的信息。V维度表示了一种物体自身的材质信息，是不是透明的，透明度有多少，同样拿照苹果这个例子，如果现在这个苹果换成了水晶苹果，灯光打上去以后看到的颜色也是不一样的，即使当初涂的颜色跟真实的苹果颜色一样，这是由于一些漫反射和折射造成的。\nHSV的值域跟RGB的并不一样，其中H维度的值域是0-180，其他两个维度都是0-255，H不取到0-360也许是因为超过了uchar的范围。\nHSV空间对颜色的描述是用户友好的，而RGB是硬件友好的。HSV空间在我日常学习中更多是作为颜色筛选的基础，人眼能区分的不同颜色的范围对应HSV的值都比较固定，下面是一个对照表：\n现在举个实际应用的案例，比如我要对下面的图片做车牌识别，我的第一步是要把车牌的区域抠出来，车牌的底色是蓝色的，当然宝马的标志也有蓝色，但我们可以不管三七二十一先把蓝色的东西都抠出来再做筛选，这时候就可以用HSV空间。\n图片刚读入的时候是BGR格式的，这时候又要用到cvtColor转换到HSV，示例代码如：\nhsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n显示HSV的效果如下：\n现在我们要的是蓝色区域，查找上面的对照表就可以知道，我们要的是H值在100到124，S值在43到255，V值在46到255之间的像素点。怎么表达这个“要”跟“不要”呢？我们可以这么考虑，对于每个像素点只有“要”和“不要”两种状态，就像一盏灯的开关一样，于是我们可以创建一幅等大的图，在上面，“要”的像素点设为一个值，“不要”的像素点设为另一个值，这就是接下来要说的二值图，我也喜欢称其为掩码图。\n三、二值图（掩码图）\n二值图本质上是灰度图，只是只用了0和255两个值，用0表示“不要”，255表示“要”，整幅图看起来就是符合条件的区域是白色的，不符合的是黑色的。对上面的HSV图作这种条件筛选处理，叫做二值化，OpenCV已经为我们准备了对应的函数cv2.inRange，示例代码如下：\nmask = cv2.inRange(hsv, np.array([100,43,46]), np.array([124,255,255]))\n处理完的结果如下图：\n可以看到，尽管有很多干扰的白色区域，车牌区域还是在里面，我们只要采取进一步的手段就可以把车牌区域筛选出来，这就要涉及一些形态学处理，敬请期待下一篇文章。\n个人简书：https://www.jianshu.com/u/9dae9c45f4a0"}
{"content2":"来源：商业电讯\n大家好，我叫赵勇，我们讨论一下人工智能。讲到人工智能，我们必须来说一下机器人。机器人其实在我们现在社会运用的比较广泛，先开始大规模应用在军事领域，然后在工业界。你们知道吗?现在机器人可以代替医生给人动手术，并且作得比人更好。接下来机器人可能会坐在驾驶员的位子上，帮我们来开车。在更遥远的未来我们可能会发现更多的机器人出现在街道上、出现在家里边，为我们服务。但是，如果这些机器人没有感知能力的话，它们就只是机器，不是机器人。所以我们必须赋予他们人工智能。\n实现这个目标我觉得必须要做好两件事，第一件事情是我们要给它们非常好的感知能力，简单的说，给它们非常好的耳朵和眼睛，让他们得到非常好的数据。在这方面其实过去这十年发生了非常大的进步，比如说在自动驾驶，以及人机交互领域，有一种特殊的传感器被成功应用，这种传感器可以探测三维的世界，通过这些传感器得到的数据不再是以前从传统相机上呈现的两维的数据格式。另外一件事情就是当数据被获取后，接下来就是去理解数据，识别数据中隐含的模式信息。那么这个就比较难了，所以我们来聊一下另外一个学科，machine learning，机器学习。\n机器学习不是一个新玩意儿，简单来说就是我们要设计一套算法机器，它可以帮我们去理解数据的意义。在过去做这个事大多都是用比较数学的方法，它包括两部分，第一部分是我们有一些非常聪明的人，他们看到这些数据的时候，能够发明一种数学的算法，把这种数据转换成高维度的数学表达，这种数学表达再结合某种统计工具，将这种结合精细地调教，把它完美地融合起来你就形成了一个体系。这个体系就可以去理解目标类型的数据。这张图片上显示了几种在图像处理领域常见的算法和统计方式。你可以看到，这些方式非常的解析。如果你要设计一个复杂的机器学习系统，就需要把很多这样的解析算法集成在一起共同实现一定的功能。这种系统基本上就是一个非常精密的仪器，它有很多零件，被我们发明出来然后集成在一起。可是这样的系统给设计者带来了很多的痛苦，因为它非常难设计，一方面这个过程需要非常聪明的算法设计者。可能今天某一个聪明的人发明了某种数学表达，用某种统计算法得到了一个结果;明天你想要超越他的话，需要某一个更聪明的人，找到了更好的算法，更好的统计工具，以及一个更好的组合，来完成超越。很多时候，这种超越需要将过去的方法推倒重来。那么这种创新的方式代价太高了，发展的速度也很慢。所以很多学者就忍不住想，人是怎么做到学习这件事情的?我们只有一个大脑，而且生物学家告诉我们，我们的大脑基本上在婴幼儿时期就已经发育得差不多了，等我们长大以后，在硬件层面上基本上没有太多空间提升我们的智能了。\n昨天有一个朋友告诉我说，他现在每天用左手刷牙，为什么呢?他听说通过这种方式可以来进一步开发他的右脑。我跟他说你来不及了，你这么作最可能的是进一步开发了你左手上的肌肉。事实是，我们用一个固定的大脑硬件可以学习很多东西。其实我们生下来大脑的硬件条件都差不多，谁也没有比谁聪明特别多。可是，我们的教育、我们的经历、我们见到的人、听到的故事，最终决定了我们变成了不同的人。\n人脑是怎样学习的?从一百多年前开始，就有科学家关注人脑。那个时候的科学家只能作到根据功能区分把人脑分成几个不同的区域，然后找到每一个区大致的功能，比如说听觉是哪儿做的，嗅觉是哪儿做的。也有科学家通过当时的显微技术去探索人脑的微观结构，尽管技术手段原始，在当时也是非常了不起的成就。然后科学家们发现，人脑里面没有CPU，也没有内存和硬盘，但是有很多微小的生物结构，我们把它叫神经元。这些神经元通过神经突触形成了复杂的网络连接。可是这个网络为什么可以工作呢?不清楚。所以我们对神经元做了一个最初级的假设。我们假设每个神经元可以进行最简单的运算，比如说一个线性运算。然后计算机科学家就根据这个原理设计了一个软件的神经网络模拟。在过去几年，科学家们惊奇地发现，当这个网络足够深，足够复杂的时候，它们产生了惊人的力量。这项技术，就是神经网络技术。在过去几年我们把这个网络做得越来越大、越来越深、越来越复杂，然后发现，他们展现了强大的学习能力。比如说，在图像识别领域，随着网络的深入，它们自动会提取出一些越来越复杂、越来越直观、越来越高级的图像特征。这些复杂的图像特征可以帮助我们把图像理解得更透彻。在过去，指望利用人脑发明的算法来稳定的检测这些高级特征的复杂度是无法想象的。然而现在一个足够深的神经网络就可以帮助我们轻松的打败最聪明的人发明的特征算法。这种奇迹不仅仅发生在图像分析领域，当我们把这项技术运用到其他领域的时候，发现它们都征服了曾经最先进的由人设计出来的特征算法。这是一个非常非常有趣的发现。\n我在这里要跟你们讲一个故事，在这张照片上显示了一个男孩，他的名字叫 Ben Underwood，他是美国加州的一个小孩。很不幸，在他两岁的时候，因为一种癌症，他的双眼不得不被摘除。从那之后他就完全没有视觉了。令人惊讶的是，从他四岁的时候，他的妈妈发现，他可以用耳朵来“看见”这个世界。他知道在什么地方可以行走;他可以通过听觉来感知桌子上的杯子具体放在哪里;他学会了骑自行车，自由地在街道上穿梭躲过那些汽车;他可以滑旱冰，很少发生事故，在滑冰的时候他可以精确地感受到路边停的车，两辆车中间有一个缝，他从这个缝钻过去。他去冲浪、他打跆拳道，他可以做很多其他盲人做不了的事情，他似乎可以真的看到这个世界。他还会打篮球，投篮还投的很准。当在一个安静环境里的时候，他的舌头就可以发出一种类似于海豚的声音，然后这种声音传出去以后，通过回波就可以做三维定位。所以科学家就觉得他非常奇怪，就请他去做研究。首先帮他检查耳朵的听力情况，发现他的听力很正常，他完全听不见正常人听不到的频率范围，他的耳朵跟正常人的耳朵是一样的。科学家们接着去观测他的脑子里发生了什么变化，他们发现当他做回声定位的时候，他大脑里原来去负责视觉的那部分神经元变得非常活跃，是这部分神经元帮他去做了回声定位这个事，这个大多数人类根本不存在的能力。回声定位，这个只存在于少数生物，比如说海豚、或者蝙蝠脑子里的功能，在Ben的大脑里原本负责视觉信号分析的那个区域实现了。也就是说，这个Ben其实他的脑子和我们正常人差不多的。同样的硬件，当失去一项功能的时候，那里面的部件可以去做另外一些全新的功能。\n在神经网络的领域里，现在就出现一个类似的现象：当我们把一个模拟的神经网络，它的规模加到一定程度的时候，这个结构就和你设计它的初衷关系不大了，只要你给它特殊的数据，只要把它教育好，它似乎可以把很多事情都做得很好。我们似乎把智能的硬件和软件分开来了。在过去，一个算法的设计需要对目标问题有极其深入的了解，需要设计针对性很强的算法和统计方法，这些模块之间必须有超强的相关性，因此开发这些方法的过程很长，人力成本也很高。深层次的神经网络技术，使得我们可以把把自己从一个设计完美算法体系的数学家，变成一个神经网络“教育者”。只要我们有足够多的数据，善于教育，然后过段时间之后，这个神经网络就会做一些事情了。就比如果我们想做一个无人驾驶汽车的项目，那我们就发明了一个神经网络，我们叫它“小王”，“小王”是一颗司机的大脑。那么我们就教他关于开车的所有的事情，经过一段时间的努力学习，“小王”就学会开车了。如果我们教小王一些证券交易知识的话，也许到时候他也会变成一个非常好的交易员。\n我曾经有一个老师，叫Hanspeter Pfister。他说我们人类对于神经元的理解太简单了，一百多年的那些照片给我们的启示太模糊。所以他就发明了一套机器，目标就是对大脑的神经网络进行高分辨率的扫描。你想象这么一个过程：给你一个大脑组织，先把脑子切成小“豆腐块”，然后呢，用非常精确的机器把它们切成非常非常薄的片。薄到什么程度呢?当科学家把这个切片放到电子显微镜下的时候，这些切片可以帮助科学家们看到神经元还有神经突触这些微观结构的连接。那么收集了很多这些图片之后呢，在三维上把这些图片垒起来，通过图像分析能够从三维空间里面重建这些生物体系的三维结构。而把这些数据扫出来之后，放在一个数据库里面，就可以供全世界的科学家们去研究人脑的奥秘。用这种方式扫描大脑产生了非常庞大的数据，以至于在一立方毫米的脑子上，扫描出来的图像数据就多达800个TB，如果你把这些数据存到DVD光盘上的话，需要20万张盘。请注意这些数据仅仅来自于一立方毫米的大脑组织，也就是大小类似于一粒盐那么丁点儿的大脑组织。当科学家们把这些数据重现出来的时候，不得不对它们进行着色，所以这些脑神经网络的图片变得五彩缤纷。下面我就要呈现给你们的照片，这些照片非常非常美丽，我建议我们现在屏住呼吸，来欣赏这些作品。\n这些照片非常漂亮，科学家们需要给这个项目起个名字。由于它们和大脑有关，所以想到了brain，又因为它们看上去五彩缤纷很像彩虹(rainbow)，所以科学家们把这个项目起名叫Brainbow。通过Brainbow这个项目，科学家们有一些很有意思的发现。首先，他们发现在一个普通人的大脑里至少拥有一千亿个神经元，每一个神经元平均拥有一千个以上的连接，通往其他的神经元。可是每一个神经元里面到底在做什么事情呢?我们现在拥有的技术还不能帮助我们在不损坏内部结构的前提下把这么微小的一个结构打开。所以这仍然是一个谜。但根据各种相关研究，科学家们发现之前人们对神经元功能的假设，例如线性运算假设，基本上都是错的。神经元完全有可能是一个非线性的操作甚至是复杂的矩阵操作。如果把每个神经元比做一个计算机的话，那么在我们这个大脑里面，有超过一千亿台计算机，这个数量超过了人类有史以来制造的计算机数目的总和。\n所以当看到这个人脑结构的时候，我被深深地震撼了。首先，它真的是个非常非常复杂的系统，我们现在最先进的科技，无论是在全世界的哪家公司里，我们无非是利用我们从脑神经科学里得到的最基本的知识，以及一些模糊的灵感去设计一套复杂程度远远低于真正人脑结构的模拟神经网络，利用大量的数据教育这些神经网络学习一些模式识别的功能。可是真正的人工智能、真正的人类智能是远远超越了模式识别这个层次的。比如说我们人脑除了重复执行一件已知的任务，我们还会创造，我们会造一系列东西，我们可以想一些过去历史中从未存在的事情，可以在一个想象的世界里的从未存在的时空中想象从未存发生过的故事。这些就是创造力和想象力。同时我们还拥有情感。我注意到，在这个世界上的生物里，越是高等的生物似乎就越有更丰富的情感。如果你问一个问题，这些情感在我们的智能体系里的功能是什么?它是一个优点还是弱点?我觉得答案是很明显的。但如果你问我另外一个问题：这些情感在人工智能里面，它帮助我们成为了这么高等的动物，在这个过程中，它是一个原因还是一个结果?或者两者都是?这个我答不上来。而且不只我们人有情感，很多动物也有情感，母子间的情感、恋人间的情感，这些情感甚至可以持续一辈子。这些层面的智能，我们现在全世界的科学家还知之甚少。所以我们今天的人工智能和我们的人类智能有着非常非常遥远的差距。\n如果让我预测singularity什么时候会发生，我无法像Ray Kurzwell先生那样做出一个精确的预测。但有一件事情我是确定的，我们现在距离这个目标还很遥远，必须仰望它。这种感觉非常非常像我们仰望宇宙。不用说整个宇宙，我们就来看一看我们的银河系吧。天文学家们现在有非常先进的射电天文望远镜，他们已经可以比较精确的观测说整个银河系大约有一千亿到四千亿颗恒星。这个规模和我们人脑的神经元规模是类似的。可是到现在，人类真正能够实地探测的范围还没有超越我们自己所在的太阳系，就好比在人脑的微观结构中我们没有能够进入到神经元微观结构中精确探索的能力。\n所以，在可以预见的未来，除了我们在探讨人工智能(AI)的奇点什么时候会到来，也许我们更应该把重点放在IA，也就是IntelligenceAugmentation，智能增强。如果我们能发明一个机器，这个机器虽然不像我们一样聪明，但他们可以忠诚地为我们服务，变成了我们的一个平行智慧：它看到我们能看到的东西，听到我们能听到的东西，他连到因特网网上，连接了人类所有的知识和智慧。我觉得这样的技术会给我们的生活带来巨大的变化和帮助。所以，奇点什么时候到来，我也说不上来，也许在很遥远的未来，但是人工智能的发展必然会给我们的世界带来非常有意义的变化。"}
{"content2":"A Year in Computer Vision 计算机视觉这一年 The M Tank 2017\nThe M Tank 编辑了一份报告《A Year in Computer Vision》，记录了 2016 至 2017 年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。\n原文网址 http://www.themtank.org/a-year-in-computer-vision\n内容目录\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率，风格转换，着色\n动作识别\n第三部分\n3D目标\n人体姿势评估\n3D重建\n其它未分类3D\n3D总结和SLAM（Simultaneous Localization and Mapping）\n第四部分\n卷积框架\n数据集\n发展趋势\n结论\n参考文献\nIntroduction\n计算机视觉通俗地说是一种研究机器视觉能力的学科，或者说是使机器能对周围环境和其中的刺激进行可视化分析的学科。机器视觉通常涉及对图像或视频的评估，英国机器视觉协会（BMVA）将机器视觉定义为*“对单张图像或一系列图像的有用信息进行自动提取、分析和理解“*。\n对我们环境的真正理解不是仅通过视觉表征就可以达成的。更准确地说，是视觉线索通过视觉神经传输到主视觉皮层，然后由大脑以高度特征化的形式进行分析的过程。从这种感觉信息中提取解释几乎包含了我们所有的自然演化和主体经验，即进化如何令我们生存下来，以及我们如何在一生中对世界进行学习和理解。\n从这方面来说，视觉过程仅仅是传输图像并进行解释的过程，然而从计算的角度看，图像其实更接近思想或认知，涉及大脑的大量功能。因此，由于跨领域特性很显著，很多人认为计算机视觉是对视觉环境和其中语境的真实理解，并将引领我们实现强人工智能。\n不过，我们目前仍然处于这个领域发展的胚胎期。这篇文章的目的在于阐明 2016 至 2017 年计算机视觉最主要的进步，以及这些进步对实际应用的促进。"}
{"content2":"深度学习之于传统计算机视觉"}
{"content2":"转自计算机视觉论坛。\nICCV的全称是International Comference on Computer Vision，正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。\n“ICCV”是“International Conference on Computer Vision”的简称。该会议由美国电气和电子工程师学会（IEEE，Institute of Electrical & Electronic Engineers）主办，通常是在北美、欧洲、亚洲的一些科研实力较强的国家举行。作为世界顶级的学术会议，首届国际计算机视觉大会于1987年在伦敦揭幕，其后两年举办一届。[1]\n计算机视觉是当前计算机科学研究的一个非常活跃的领域，该学科旨在为计算机和机器人开发出具有与人类水平相当的视觉能力。各国学者对于计算机视觉的研究始于20世纪60年代初，但相关基础研究的大部分重要进展则是在80年代以后取得的。近年来，全球学界愈来愈关注中国人在计算机视觉领域所取得的科研成就，这是因为由中国人主导的相关研究已取得了长足的进步——2007年大会共收到论文1200余篇，而获选论文仅为244篇，其中来自中国大陆，香港及台湾的论文有超过30篇，超过大会获选论文总数的12%。\n会议收录论文的内容包括：底层视觉与感知，颜色、光照与纹理处理，分割与聚合，运动与跟踪，立体视觉与运动结构重构，基于图像的建模，基于物理的建模，视觉中的统计学习，视频监控，物体、事件和场景的识别，基于视觉的图形学，图片和视频的获取，性能评估，具体应用等。\nICCV是计算机视觉领域最高级别的会议，会议的论文集代表了计算机视觉领域最新的发展方向和水平。会议的收录率较低，以 2007 年为例，会议共收到论文1200余篇，接受的论文仅为244篇。会议的论文会被 EI 检索。\nCVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。\n国际计算机视觉与模式识别会议（CVPR）是IEEE一年一度的学术性会议，会议的主要内容是计算机视觉与模式识别技术。CVPR是世界顶级的计算机视觉会议（三大顶会之一，另外两个是ICCV和ECCV），近年来每年有约1500名参加者，收录的论文数量一般300篇左右。第一届CVPR会议于1985年在旧金山举办，后面每年都在美国本土举行。\n近几年录取率25%左右，自2001年开始每年在会议上进行演讲的论文[oral]通过率锐减为10%以下，2006年这一数字以来更是低于5%（一个重要原因是由于论文数量过多，大部分的6~8页长篇论文在会议期间只要求做海报[poster]展示）。在各种学术会议统计中，cvpr被认为有着很强的影响因子和很高的排名。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\nECCV的全称是European Conference on Computer Vision(欧洲计算机视觉国际会议) ，两年一次，是计算机视觉三大会议（另外两个是ICCV和CVPR）之一。每次会议在全球范围录用论文300篇左右，主要的录用论文都来自美国、欧洲等顶尖实验室及研究所，中国大陆的论文数量一般在10-20篇之间。ECCV2010的论文录取率为27%。\nECCV是一个欧洲会议，欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster调自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。"}
{"content2":"2013春天一个普普通通的下午，在剑桥大学牛顿数学中心图书馆看书的时候，偶然发现了David Marr 的名著 Vision （1982版）原本，心情顿时极为复杂，想偷、想借而不还、想......不知为何，最后仅以拍照全文而结束了各种“狂想”，2015年12月18日谨以此文向David Marr 脱帽致敬：谢谢！\n在工作和生活中，作为主体的人，总是通过五官从周围环境中获取信息，并根据这些信息来指导自己的行动。其中视觉最为重要，1967年，特瑞东拉提供以下研究结论：人的认识与人的器官的关系是：味觉1%；触觉1.15%；嗅觉3.15%；听觉11%；视觉83%。听觉和视觉工占94%。从记忆角度来讲，人们能记住阅读的10%；自己听到的20%；自己看到的30%。看和听共占50%。以信息论研究人的信道特性表明，假定触觉信道宽为1时，则听觉信道宽为100，视觉信道宽为10000。因此，如何发挥各渠道信息交换能力，特别是研究利用视觉信息，是促进认知的十分重要的课题。此外，大多数人工控制也都主要依赖视觉显示来获取与控制有关的输入信息。由于视觉的重要性，有关视觉系统的研究始终是科学界关注的问题之一，其中有关生物视觉机制与计算机视觉早就引起了神经病学、眼科学、生理学、解剖学以及电子、计算机领域专家们的极大兴趣，特别是近年来，世界各国对视觉系统的研究越来越多：NASA、哈佛、麻省、剑桥、牛津、东京工业大学等著名科研机构或大学都设有专门的视觉系统研究部门。\n一、生物视觉的研究\n视觉系统是一种复杂的自动控制系统，可以从控制论的角度来研究它。但是它不仅涉及眼的活动，而且和脑干、小脑、大脑等神经活动有关，即使从医学上讲也是一个跨学科的复杂系统。有关视觉系统的研究从研究方法上讲，有许多不同的方法来研究眼的不同侧面，有生理生化方面，也有系统与信息方面以及临床应用方面。下面本文将从形态学的角度对生物的视觉系统进行探讨。\n1． 生物的视觉通路\n为人类视觉通路示意图，物体在可见光的照射下经眼的光学系统在眼底视网膜上形成物像。视网膜上的感光细胞又将视网膜上接受的光能转换成神经冲动，经过视交叉部分交换神经纤维后，形成视束，传到中枢的许多部位，其中包括脑的外膝状体或外膝状核（LGN）、四叠体上丘（SC）、顶盖前区（AP）和视皮层（VC）等。上丘与眼动等视反射有关，顶盖前区与调节反射、瞳孔反射有关，外膝状体和视皮层都直接与视觉知觉有关。\n神经节细胞轴突在外膝状体换神经元后，由外膝状体神经元直接经视放射到视皮层，这是视束的大部分纤维的去向，称为视觉第一通路。视束的小部分纤维走向内方，经上丘臂，到达上丘和顶盖前区。上丘浅层神经元再投射到丘脑枕（PT）换元后再投射到视皮层；上丘还有纤维直接投射到视皮层。由于这条通路不经外膝体，故称膝状体外通路或第二视通路。\n由眼、外膝状体和视皮层解剖学结构中的视通路可以看到，眼、外膝状体与视皮层构成了对视觉信息处理的三个基本层次。进一步的分析表明，外膝状体与视皮层，尤其是视皮层还有更为复杂的分块分层结构，分块表明了视觉信息处理的并行性质，不同区域的神经细胞具有不同的功能；分层表明了视觉信息处理的串行性质。因此，生物视觉系统是一个串行与并行处理相结合的复杂系统。\n2.感受野分层等级假设\n视通路上各层次的神经细胞，由简单到复杂，它们所处理的信息，分别对应于视网膜上的一个局部区域，层次越深入，该区域就越大，这就是著名的感受野（RF）与感受野等级假设，感受野是支持视觉信息分层串行处理的最重要的生理学基础。\n以信息处理的第一级为例，视网膜上的神经节细胞（GC），将感官细胞上接受到的光信号转换成电信号再由它的轴突传出，但每一个GC细胞只能接受视网膜上一个局部区域的信号，该区域就是GC的感受野，研究表明，GC感受野及其对光信号的转换作用可划分为以下几种：\n（1）对空间亮度变化敏感的感受野，形状可用两个同心圆表示。圆形状的感受野按其对光信号的转换作用又可分为中心兴奋区、周边抑制区组成的On-中心型以及中心抑制区、周边兴奋区组成的Off-中心型 [2]。表示当视网膜上光信号为一边亮一边暗的具有一定对比度的信号时，感受野位于不同空间位置的GC的输出，只有当亮暗边缘线过同心圆中心时，GC的输出与感受野受到均匀光照时一样，设为E，而当边缘线位于同心圆的其他位置时，GC的输出分别高于或低于该平均输出E。如将输出看作实际输出减去平均输出E，则当亮暗边缘线过感受野同心圆中心时，输出为零。可见，由GC的输出与感受野的位置可以检测亮暗边缘线。这也是计算机视觉中轮廓、边缘等特征抽取的生物视觉基础。\n\n（2）除具有上述响应特征及感受野的GC细胞外（这种GC细胞也称为x型细胞），还有一种y型细胞，它的响应不对亮度的空间变化敏感，而是对时间变化敏感，即当感受野圆形区域内的亮度随时间变化时，GC输出会增大或减少，y型细胞也可划分为On型或Off型，这种对局部亮度随时间变化敏感的性质是计算机视觉中物体运动分析的基础。\n除了视觉信息处理第一层次的感受野外，对于更高层次，包括外膝状体细胞及初级视皮层（即视皮层的前几层），也发现了类似的性质，即每一个单个细胞只接受视网膜上的局部信息，但层次越高感受野越大，即信息处理是从局部到更大的区域的。\n3． 视觉信息处理的多通道、多任务并行处理性质\n感受野的等级假设与局部性质主要支持视觉信息处理的自上而下的分层次串行处理性质。但视觉系统的任务不是单一的，它要识别物体的形状与颜色，要得到三维物体的深入信息，要检测物体的方位与运动参数。另外，物体的空间与时间频率性质也有很大差异，例如较细的表面纹理表现了物体表面较高的空间频率特征；较快的运动，表现为图像较高的时间频率特征。神经生理学的研究表明，视通路的各个层次上存在着基本相互独立的并行通道，分别完成不同的视觉任务，下面是不同的并行通道：\n（1）x, y 通道\n除了视网膜神经节细胞GC的感受野有x, y之分，更高层次细胞的感受野也有x, y之分，即某一层次的神经细胞（如外膝状体的神经细胞）只接受来自x（或y）型GC细胞的信号，使它也具有x（或y）型的感受野，由于x，y型感受野的不同，x通道传递高空间分辨率的、时间上变化慢速的信息与颜色信息，而y通道传递低空间分辨率的、高时间变化频率的信息。\n（2）空间与时间频率通道\n虽然x，y型细胞分别对具有空间频率或时间频率的信息敏感，心理物理学实验证明，外膝状体上的某一个神经细胞并不对所有频率的信息敏感，而是对某一频段的信息呈现较强的反应，在视觉皮层也发现了类似的证据，即皮层的某一部分只对某一定频率的信号敏感。这些实验证据表明，在视通路中存在处理不同频道信号的独立通道。\n（3）颜色信息通道\n首先在视网膜上就存在不同的光感受细胞，即锥状细胞与杆状细胞，其中杆状细胞对颜色不敏感，而锥状细胞又分为对不同光谱（红、绿、蓝）敏感的三种细胞，对外膝状体与视觉皮层17区的分析也表明，它们都有专门的区域从事颜色信息的处理与识别。\n（4）左右眼信息通道与立体视觉\n在的视觉信息通道简图中可见，眼睛、外膝状体与视觉皮层都有左右两侧，分析表明，左右两侧的神经细胞分别处理由左右两眼半侧来的信息，也就是说，每一个眼睛的左右侧视野的信息是交叉地分别投射到左右外膝状体与视皮层的。视皮层处理来自两眼的信息得到双眼视差信息是立体视觉的基础，即只有比较来自两眼的信息，才能使我们有深度感。近来的研究表明，直到视皮层的17，18，19区，还存在独立处理不同视差的并行通道。\n（5）空间方位信息通道\n早期对视觉信息处理的研究认为，视皮层细胞对空间几何元素（如直线）的方位敏感，阮迪云[Ruan 1992]等对外膝体细胞的研究表明，这种方位敏感性也存在于外膝状体细胞，即具有相似最优方位敏感的细胞在外膝体层次已经聚集在一起。可见在视皮层处理之前，外膝体已经对方位信息进行了组织，在视觉通路中存在着处理方位信息的通路。\n（6）视皮层对形状、颜色、运动与深度信息的并行处理\n对视皮层17至更高层次的研究表明，对物体的形状、颜色、运动与深度等不同视觉信息的处理已经明显分离开，最近的研究成果已经提供了两类确定的视信息处理流向，这两类信息处理流在V1区即已发出，且可以跟踪好几个等级。第一条信息通路包括MT和MST区，主要功能涉及视运动功能分析；第二条通路包括V4、VP和IT（下颞皮层），主要涉及颜色和形状信息处理。\n4.视觉皮层间的反馈和整合作用\n一般而言，至少存在着３５个以上的大脑皮层区域是直接地或紧密地与视觉信息处理有关，它们既平行又串行处理着各种不同视觉信息。视皮层所独有的，其它皮层细胞也是如此。通过研究，迄今为止尚未发现一个单独的皮层区域只接受所有的其他视皮层区域来的信息。以英国科学家Zeki为代表，提出了关于视觉皮层整合作用的“多级同步整合作用”假说，即在视皮层的整合作用包括三个不必在时间上连续的过程：（1）放大视觉感受野，并在整个视野内收集信息；（2）与前一个过程同时，产生更加复杂和特殊的性质；（3）将代表不同的视觉功能的视觉皮层的信息统一在一起，但并不要求信号都走到同一个皮层区来，而可能是空间上分离的，时间上却是同步的。\n二．计算机视觉的研究\n80年代初，David Marr [Marr 1982]首次从信息处理的角度综合了图像处理、心理物理学、神经生理学及临床神经病学的研究成果，提出了第一个较为完善的视觉系统框架，这一框架虽然在细节上甚至在主导思想上还存在不完备的方面，许多方面还有很多争议，但至今仍是目前计算机视觉研究的基本框架。计算机视觉的研究目标及任务是把输入的二维数字图像概括成抽象的符号描述或参数模型表示，不但能够真实地反映客观世界物体的空间几何信息，包括它的形状、位置、姿态、运动等，而且还能对它们进行存储、识别与处理。\n1． 计算机视觉系统研究的层次\nMarr从信息处理系统的角度出发，认为对此系统的研究应分为三个层次：即计算理论层次、表达与算法层次、硬件实现层次。\n目前计算机视觉的研究工作主要在计算理论、表达与算法这两个层次上，对于硬件实现，当前只有较成熟的部分，如低层次处理的去噪声，边缘抽取等；对简单的二维物体识别及简单场景下的视觉方法，已有专用芯片或其它并行处理体系结构方面的研究与试验产品；从系统上构造一般的视觉系统，虽有些尝试，但并不成功。\n2． 视觉信息处理的三个阶段表达与算法\nMarr从视觉计算理论出发，将系统分为自上而下的三个阶段，即视觉信息是从最初的原始数据（二维图像数据）到最终对三维表达经历了三个阶段的处理。第一阶段构成所谓“要素图”或“基元图”，基元图由二维图像中的边缘点、直线段、曲线、顶点、纹理等基本几何元素或特征组成；第二阶段，Marr称为环境的2.5维描述，即部分的、不完整的三维信息描述，用“计算”的语言来讲就是重建三维物体在观察者为中心的坐标系下的三维形状与位置。这一阶段中存在许多并行的相对独立的模块，如立体视觉、运动分析、由灰度恢复表面形状等不同处理单元。第三阶段，当观察者从不同角度去观察物体，其形状都是不完整的，不能设想，人脑中存有同一物体从所有可能的观察角度看到的物体形象，以用来与所谓的物体的2.5维描述进行匹配与比较，因此，2.5维描述必须进一步处理以得到物体的完整三维描述，而且必须是物体本身某一固定坐标系下的描述，这一阶段被称为第三阶段或三维阶段。\n第一阶段与第二阶段被称为视觉的低层次处理，也有人将第一阶段称为早期视觉，第二阶段称为中期阶段。\n3． 计算机视觉系统的基本体系结构与研究问题\n(1) 计算机视觉系统的基本体系结构\n计算机视觉系统的基本体系结构可由所示：\n其中Shape from X表示由某些图像特征（如边缘点、直线、曲线、纹理、物体轮廓线、序列图像对应特征、图像灰度、颜色等）恢复物体三维形状。\n(2) 计算机视觉系统研究存在的问题\n80年代至今，世界各国的研究者们按照Marr提出的基本理论框架，对计算机视觉系统的各个研究层次及各个阶段的各个模块，进行了大量的研究，并提出了相应的解决方法，但总的来讲，这些方法都存在着一些问题，或缺乏通用性，或抗干扰能力差，或存在多解性，究其因，不外乎两点：一是计算机视觉是一个逆问题，即输入图像为二维图像的灰度，它是三维物体几何特征、光照、物体材料表面性质、物体的颜色、摄象机参数等许多因素的函数，由灰度反推以上各种参数是逆问题，而这些问题大都是非线形的，问题的解不具有唯一性，而且对噪声或离散化引起的误差都极其敏感；另一个原因是Marr的视觉系统框架是一个自上而下的、模块的、单向的、数据驱动型的结构，神经生理学的深入研究表明，这种结构与人的视觉系统还有很大差距，人的视觉系统上下各层次之间、各模块之间存在着更为复杂的相互作用，并且由眼动等现象可知，生物视觉系统的认知过程是一种与外界交互作用的有目的、主动性过程，而不仅仅是一种被动式的反应，这些发展都突破了Marr的视觉理论框架。\n三. 生物视觉与计算机视觉的比较研究\n虽然计算机视觉的发展得益于神经生理学、心理学与认知科学对生物视觉的系统研究，但计算机视觉计算理论与算法的发展却相对独立，并不刻意去“仿真”生物视觉。主要原因是：1.目前，生物视觉在更高层次上的机理尚未搞清楚，对计算机视觉的发展指导意义不大；2.有不少有关学者认为，只要从信息转换的角度真正理解了视觉信息处理过程并发展出一套信息处理的计算理论，用哪种体系结构去实现它是无关紧要的；3.一般而言，计算机视觉系统要观察的环境相对比较简单。\n生物视觉系统在硬件实现层次上是神经网络，因而与计算机视觉有很大的差别，但如果两者在计算理论层次上是相同的，则本质上应没有太大区别。传统的思想是，计算机视觉信息处理的理论研究应侧重计算理论、表达与算法，而神经网络则属于实现层次，对视觉不具有本质上的重要性。然而有两个原因正促使这种看法的改变：一是硬件实现方式很可能反过来影响计算理论与表达；二是由于条件的限制，目前的人工神经网络也只是真正神经网络的一种“过分简化”，但确有一种新的自适应、自学习机制存在，需要进一步研究。\n比较人的视觉系统与计算机视觉系统的工作过程，在低层处理阶段，图像中物体受到退化、噪声、模糊、阴影等影响，现有低层处理算法很难把物体在图像中的区域截然分开。而人的视觉却具有相当强的区分能力。第二个困难是在平面图像中发现立体信息，人能很快地辨别出图像的前后关系，而要计算机做到这一点，光凭图像上的信息是很难甚至不可能做到的。\n计算机视觉系统现在一般分为两大类：工业视觉系统和通用型视觉系统。前者已在实际工作中得到大量的应用，各类系统的技术层次、指标差异很大，一般是功能相对专门化，使用条件和环境有一定限制。后者虽有实验室研究，但多侧重于图象识别和景物分析（包括三维运动）等，而复杂的生物视觉系统的行为功能要用工程技术加以实现，还面临许多困难，难度较大。\n从目前对生物视觉的研究现状来看，视觉信息处理是一个串行与并行相结合的复杂视觉信息处理过程，到目前为止搞清楚的仅仅是其中的极小一部分，而且，从信息处理的角度看，对大多数处理单元我们仅仅知道非常浅的知识，即只知道这个单元对某种信息“敏感”，而信息是如何表征的，如何变换的，则仍不清楚，尤其是在较高层信息的处理，关于串行与并行处理进入高级皮层后，信息是如何综合等方面。\n总之，由于当前生物视觉在更高层次上的机理研究尚未有突破性进展，生物视觉系统与计算机视觉系统的研究从总体上是处于“独立自主，各自为政”的发展阶段；但是从未来的发展趋势来看，一旦人们对于生物视觉系统中的信息转换及处理机制完成由定性分析到定量计算的跃进，那么计算机视觉系统的研究将会逐步地与生物视觉系统的研究融合在一起。\n结束语\n视觉是人类和一些动物的基本功能，也是人们认识世界、了解客观世界的主要感知手段，同时也是了解脑的认知功能的突破口。视觉系统的研究目的是感知视觉世界的空间存在，了解周围视觉世界的空间结构、特点、组成以及它们的空间运动变化规律；从信息处理角度来说，计算机视觉系统的任务是把输入的二维数字图像概括成抽象的符号描述或参数模型表示，以真实反应客观世界中物体的空间信息。从根本上说，生物视觉的研究是计算机视觉研究的基础，但由于对生物视觉的研究尚未出现突破性进展等原因，造成了计算机视觉远远没有达到期待的目标，其中一些关键性技术尚未得到解决。但随着神经生理学、认知心理学、物理学、数学、图像处理、模式识别、人工智能等相关学科的迅速发展，无论是生物视觉（尤其是人的视觉）还是计算机视觉的研究都将会出现实质性、革命性进展，进而将极大的改变人们的生活、工作方式。不过，这一切还有待时间来进行验证。\n左边是1982年原版 右边是MIT后出的新版"}
{"content2":"如果想要机器能够进行思考，我们需要先教会它们去看。\n李飞飞——Director of Stanford AI Lab and Stanford Vision Lab\n计算机视觉（Computer vision）是一门研究如何使机器“看”的科学，更进一步的说，就是指用摄影机和计算机代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图像处理，用计算机处理成更适合人眼观察或进行仪器检测的图像。\n学习和运算能让机器能够更好的理解图片环境，并且建立具有真正智能的视觉系统。当下环境中存在着大量的图片和视频内容，这些内容亟需学者们理解并在其中找出模式，来揭示那些我们以前不曾注意过的细节。 计算机视觉的实现基本过程为：\n计算机从图片中生成数学模型\n计算机图形在模型中对图像进行绘制，然后在图像处理过程中将其作为输入，另外给出处理图像作为输出\n计算机视觉的理念在某些方面其实与很多概念有部分重叠，包括：人工智能、数字图像处理、机器学习、深度学习、模式识别、概率图模型、科学计算以及一系列的数学计算等。因此，你可以将本文看成是深入这个领域研究的第一步。本文将尽量包涵到尽可能多的内容，但是可能仍然会存在一些较为复杂的主题，也有可能存在某些遗漏之处，敬请见谅。\n丨第一步——背景\n通常来说，你应该具有一点相关的学术背景，比如上过有关概率学、统计学、线性代数、微积分（微分与积分）等相关课程，对矩阵计算有一定了解更好。另外，从我的经验来看如果你对数字信号处理有了解的话，在以后对于概念的理解来说会更加容易。\n在实现层面来说，你最好能够会用MATLAB或者Python中的一种，一定要记住的是计算机视觉几乎全部与计算机编程有关。\n你也可以在Coursera上选修《概率绘图模型》一课，这门课程相对较难（讲得比较深入），你也可以在学习一段时间之后再对其进行了解。\n丨第二步——数字图像处理\n观看来自杜克大学的Guillermo Sapiro所教授的课程——《图像和视频处理：从火星到好莱坞Image and Video Processing: From Mars to Hollywood with a Stop at the Hospital》，该课程所提供的教学大纲每章都是独立的且包涵大量的练习，你可以在coursera和YouTube上找到相关的课程视频信息。另外你可以看下Gonzalez与Woods编写的《数字图像处理（Digital Image Processing）》一书，使用MATLAB来运行其中所提到的范例，相信一定会有所获。\n丨第三步——计算机视觉\n一旦学习完有关数字图像处理有关内容，接下来应该了解相关的数学模型在各种图像和视频内容中的应用方法。来自佛罗里达大学的Mubarak Shah教授在计算机视觉方面的课程可以作为一门很好的入门课程，其涵盖了几乎所有的基础概念。\n观看这些影片的同时，可以学习Gatech的James Hays教授的计算机视觉项目课程所使用的概念和算法，这些练习也都是基于MATLAB的。千万不要跳过这些练习，只有在真正的练习过程中才会对这些算法和公式有更深入的了解。\n丨第四步——高级计算机视觉\n如果你认真学习了前三步中的内容，现在可以进入到高级计算机视觉相关学习了。\n来自巴黎中央理工学院的Nikos Paragios和Pawan Kumar讲授了一门人工视觉中的离散推理（Discrete Inference in Artificial Vision）课程，它能提供相关的概率图形模型和计算机视觉相关的大量数学知识。\n到现在这一步来看就比较有趣了，这门课程一定能让你感受到用简单模型构筑机器视觉系统有多么复杂。学完这门课程的话，在接触学术论文之前又迈进一大步。\n丨第五步——引入Python和开源框架\n这一步我们要接触到Python编程语言。\n就Python而言有许多像 OpenCV、PIL、vlfeat这样的相关扩展包，现在就是将这些扩展包运用到你的项目中的最好时机。因为如果有其他的开源框架存在的话，没有必要从头开始来编写一切内容。\n如果需要参考资料的话可以考虑《使用Python对计算机视觉进行编程 Programming Computer Vision with Python》，使用这本书就够了。你可以动手去尝试下，看看MATLAB和Python结合的话如何来实现你的算法。\n丨第六步——机器学习与CovNets（卷积神经网络）\n有关如何从头开始机器学习的资料实在太多，你可以从在网上查找到大量相关教程。\n从现在开始最好一直使用Python进行编程，可以看下《使用Python建立机器学习系统——Building Machine Learning Systems with Python》和《Python机器学习——Python Machine Learning》这两本书。\n目前深度学习正大行其道，可以试着学习卷积神经网络在计算机视觉中的应用（ Computer Vision: the use of CovNets），在此推荐斯坦福的CS231n课程：针对视觉识别的卷积神经网络。\n丨第七步——如何才能更进一步\n行文至此，你可能会觉得已经讲了太多的内容，需要学的已经太多。但是，你还可以进一步进行探索研究。\n其中一个方法是看看由多伦多大学的Sanja Fidler和James Hays所举行的一系列研讨会课程，能帮助你对当下计算机视觉研究方向的最新概念有所理解。\n另一种即跟着 CVPR、ICCV、 ECCV、 BMVC这些顶级学术会议的相关学术论文（也可关注雷锋网的相关报道），通过会上的研讨会、主旨演讲以及tutorial等日程一定能学到不少知识。\n总结：如果你按照步骤一步步完成所有的学习任务，届时你将大概了解计算机视觉中有关滤波器、特征检测、描述、相机模型、追踪器的历史，另外还学习到分割和识别、神经网络和深度学习的最新进展。希望本文能帮助你在计算机视觉领域走得更远，学习得更加深入"}
{"content2":"计算机视觉中常用的评价标准\n1 召回率\nRecall，又称“查全率”——还是查全率好记，也更能体现其实质意义。\n2 准确率\nPrecision，又称“精度”、“正确率”。\n以检索为例，可以把搜索情况用下图表示：\n相关 不相关\n检索到 A B\n未检索到 C D\nA：检索到的，相关的 （搜到的也想要的）\nB：检索到的，但是不相关的\n（搜到的但没用的）\nC：未检索到的，但却是相关的\n（没搜到，然而实际上想要的）\nD：未检索到的，也不相关的\n（没搜到也没用的）\n如果我们希望：被检索到的内容越多越好，这是追求“查全率”，即\nA/(A+C)，越大越好。\n如果我们希望：检索到的文档中，真正想要的、也就是相关的越多越好，不相关的越少越好，这是追求“准确率”，即A/(A+B)，越大越好。\n“召回率”与“准确率”虽然没有必然的关系（从上面公式中可以看到），在实际应用中，是相互制约的。\n要根据实际需求，找到一个平衡点。\n3 举例\n假设原始样本中有两类，其中：\n1：总共有 P个类别为1的样本，假设类别1为正例。\n2：总共有N个类别为0 的样本，假设类别0为负例。\n经过分类后：\n3：有 TP个类别为1 的样本被系统正确判定为类别1，FN 个类别为1 的样本被系统误判定为类别 0，显然有P=TP+FN；\n4：有 FP 个类别为0 的样本被系统误判断定为类别1，TN 个类别为0 的样本被系统正确判为类别 0，显然有N=FP+TN；\n精确度（Precision）\nP = TP/(TP+FP) ; 反映了被分类器判定的正例中真正的正例样本的比重\n准确率（Accuracy）\nA = (TP + TN)/(P+N) = (TP + TN)/(TP + FN + FP + TN); 反映了分类器统对整个样本的判定能力——能将正的判定为正，负的判定为负\n召回率(Recall)，也称为 True Positive Rate\nR = TP/(TP+FN) = 1 - FN/T; 反映了被正确判定的正例占总的正例的比重\n转移性（Specificity，不知道这个翻译对不对，这个指标用的也不多），也称为 True NegativeRate\nS = TN/(TN + FP) = 1 – FP/N； 明显的这个和召回率是对应的指标，只是用它在衡量类别0 的判定能力。\nF-measure(综合评价指标) or balanced F-score\nF = 2 * 召回率 * 准确率/ (召回率+准确率)；这就是传统上通常说的F1 measure\ntrue positives (纳真) false positives（纳伪）\nfalse negatives（去真）true negatives (去伪)\n其中false positives（纳伪）也通常称作误报，false negatives也通常称作漏报！\n以上均参考网上大牛！感谢万能的互联网！"}
{"content2":"听了一个关于计算机视觉的报告，报告名字叫\"computer vision ++: Where do we go from here?\"，做点小结。我对计算机视觉了解的不多，连入门都算不上，但是近来对此产生了兴趣。报告主要讲的是一些应用。\n提到了David Marr (neuroscientist)好像是一个大牛，写过一本书：Vision-豆瓣，Vision-Amazon\n两个超过人的事情：1.人脸识别，LFW数据集；2.图像分类，ImageNet数据集。\n两个数据集：\n1.国际权威人脸识别公开测试集LFW(Labeled Faces in the Wild)，LFW由马萨诸塞大学于2007年建立，用于评测非约束条件下的人脸识别算法性能，是人脸识别领域使用最广泛的评测集合。\n2.ImageNet\n应用\n图像分类\nLiu J, Luo J, Shah M. Recognizing realistic actions from videos[C]// Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009:1996-2003.\nCao L, Luo J, Liang F, et al. Heterogeneous feature machines for visual recognition[C]// IEEE, International Conference on Computer Vision. IEEE, 2010:1095-1102.\n行为识别\nLi Q, Qiu Z, Yao T, et al. Action Recognition by Learning Deep Multi-Granular Spatio-Temporal Video Representation[C]// ACM on International Conference on Multimedia Retrieval. ACM, 2016:159-166.\nvision+Language\nUnsupervised Alignment of Actions in Video with Text Descriptions\nYou Q, Jin H, Wang Z, et al. Image Captioning with Semantic Attention[J]. 2016:4651-4659.\nLi Y, Song Y, Cao L, et al. TGIF: A New Dataset and Benchmark on Animated GIF Description[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2016:4641-4650.\n什么是Image Captioning？\n感觉是从图像中获取信息，caption应该是字幕的意思，那么应该是从图像中识别一些信息并把它标注出来，看起来应该是这样（Image Captioning - Kiran Vodrahalli）：\n视频和文本的无监督对准\nNaim I, Song Y C, Liu Q, et al. Unsupervised alignment of natural language instructions with video segments[C]// Twenty-Eighth AAAI Conference on Artificial Intelligence. AAAI Press, 2014:1558-1564.\nImage/Video Captioning with visual and semantic attention\nYou Q, Jin H, Wang Z, et al. Image Captioning with Semantic Attention[J]. 2016:4651-4659.\n情感计算\n情感计算—人工智能的重要发展方向\nYou Q, Luo J, Jin H, et al. Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark[J]. 2016.\nYou Q, Cao L, Jin H, et al. Robust Visual-Textual Sentiment Analysis: When Attention meets Tree-structured Recursive Neural Networks[C]// ACM, 2016:1008-1017.\n研究自拍的\nChen T, Chen Y, Luo J. A Selfie is Worth a Thousand Words: Mining Personal Patterns behind User Selfie-posting Behaviours[J]. 2017.\n用大数据来找到热销衣服的特征：\nChen K T, Luo J. When Fashion Meets Big Data: Discriminative Mining of Best Selling Clothing Features[J]. 2017.\n医疗方面的应用\n帮助新手医生练习手术"}
{"content2":"1 致谢\n感谢网友su_yuheng提供的帮助\n原文链接如下：\nhttps://blog.csdn.net/su_yuheng/article/details/78543726\n2 前言\n梯度在计算机视觉中是很重要的概念，其实，梯度在高等数学中是有自己的定义的，详细的可以参照百度百科，其定义为：\n但是这个定义是用导数定义的，而图像数据实际上是一种二维离散函数，所以其定义会发生相应的变化，下面我们就来讲述图像处理中梯度的定义。\n3 图像中的梯度\n图像中的梯度定义如下："}
{"content2":"文字识别是计算机视觉研究领域的分支之一，归属于模式识别和人工智能，是计算机科学的重要组成部分。计算机文字识别，俗称光学字符识别，英文全称是Optical Character Recognition(简称OCR)，它是利用光学技术和计算机技术把印在或写在纸上的文字读取出来，并转换成一种计算机能够接受、人可以理解的格式。OCR技术是实现文字高速录入的一项关键技术。\nOCR技术的兴起便是从印刷体识别开始的，印刷体识别的成功为后来手写体的发展奠定了坚实的基础。文字识别的主要流程大致分为以下几个部分：图像预处理、版面处理、图像切分、特征提取和模型训练、识别后处理。\n图像预处理\n输入文本经过扫描仪进入计算机后，由于纸张的厚薄、光洁度和印刷质量都会造成文字畸变，产生断笔、粘连和污点等干扰，所以在进行文字识别之前，要对带有噪声的文字图像进行处理。由于这种处理工作是在文字识别之前，所以被称为预处理。预处理一般包括灰度化、二值化，倾斜检测与校正，行、字切分，平滑，规范化等等。\n版面处理\n版面处理分为三个主要部分，版面分析、版面理解、版面重构。\n版面分析将文本图像分割为不同部分，并标定各部分属性，如：文本、图像、表格。目前在版面分析方面的工作核心思想都是基于连通域分析法，后衍生出的基于神经网络的版面分析法等也都是以连通域为基础进行的。连通域是指将图像经过二值化后转为的二值矩阵中任选一个像素点，若包围其的所有像素点中存在相同像素值的像素点则视为两点连通，以此类推，这样的像素点构成的一个集合在图像中所在的区域即一个连通域。根据连通域大小或像素点分布等特征可以将连通域的属性标记出来，用作进一步处理的依据。\n图像切分\n图像切分大致可以分为两个主要类别，行(列)切分和字切分。经过切分处理后，才能方便对单个文字进行识别处理。\n特征提取与模型训练\n特征提取与模型训练在深度学习广泛应用于图像识别领域之前，模板匹配是较为常见的一种识别方式，之后由于神经网络的复苏，基于反馈的神经网络给OCR领域带来了又一春。现在随着计算机硬件计算能力的提升，利用大批数据训练深度神经网络在图像识别方面取得了傲人的成绩。\n特征提取是从单个字符图像上提取统计特征或结构特征的过程。特征匹配是从已有的特征库中找到与待识别文字相似度最高的文字的过程。\n识别后处理\n识别校正是在识别结果基础上根据语种的语言模型进行，当然在单文种识别中相对容易一些，而在多语种则较为复杂。\n1.深度学习与计算机视觉的具体介绍\nhttp://www.duozhishidai.com/article-15924-1.html\n2.机器人视觉系统分为哪几种，主要包括哪些关键技术？\nhttp://www.duozhishidai.com/article-1753-1.html\n3.图像识别经历了哪几个阶段，主要应用在哪些领域？\nhttp://www.duozhishidai.com/article-6461-1.html"}
{"content2":"以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别\n（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；\n（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；\n（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习\n（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning\n（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\n（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判\n（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/\n（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别\n（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/\n（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model\n（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索\n（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配\n（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索\n（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别\n（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析\n（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别\n（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别\n（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；\n（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索\n（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索\n（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\n（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取\n（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率\n（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪\n（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪\n（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率\n（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊\n（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计\n（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；\n（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；\n（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊\n（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；\n（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习\n（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库\n（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割\n（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊\n（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合\n（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html\n（197）清华大学程明明博士：http://mmcheng.net/ 图像分割、检索\n（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建\n（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、\n（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:\nhttp://www.image-net.org/challenges/LSVRC/2013/\n（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示\n（202）人脸识别测试图片集：http://www.mlcv.net/\n（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；\n（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）\n（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；\n（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶\n（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、\n（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪\n（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库\n（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理\n（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、\n（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强\n（213）北京航空航天大学周付根教授：http://www.sa.buaa.edu.cn/html/yhxy/szll/jsfc/txcl/1521.html  医学图像处理\n（214）北京航空航天大学姜志国教授：http://www.sa.buaa.edu.cn/html/yhxy/xygk/zyld/1595.html 医学图像处理，遥感图像分割；\n转载自：http://blog.csdn.net/carson2005/article/details/6601109"}
{"content2":"斯坦福计算机视觉相关课程\nCS131 基础知识\nCS231a 通用的计算机视觉\nCS231A: Computer Vision, From 3D Reconstruction to Recognition\nCS231n 神经网络在计算机视觉里的应用\nhttp://cs231n.stanford.edu/\nCS331\nCS431\nHubel-Wiesel模型\nHubel, D. H., & Wiesel, T. N. (1959). Receptive fields of single neurones in the cat’s striate cortex. Journal of Physiology, 148(3),574.\n真实的鱼没刺激神经元 鱼的图片有\n基础视觉区的神经元是按一列一列的组织起来\n每一列的神经元只喜欢特定的形状\n视觉第一步并不是处理整体，而是对简单的边缘和形状结构进行处理\n边缘决定结构\n计算机视觉起源\n1963\nLarry Roberts 解析图片的边缘 计算机视觉的第一篇论文\n1966\nMIT人工智能实验室做这方面研究，计算机视觉从此成为计算机的一个分支\n1970s\nDavid Marr 视觉是分层的\n视觉是2D的\n1973\nFischler & Elschlager:\npictorial structures\nhttp://www.cs.princeton.edu/courses/archive/spr08/cos598B/Lectures/PictorialStructure.pdf\nhttp://www.cs.cornell.edu/~dph/papers/pict-struct-ijcv.pdf\n1979\nBrooks&Binford:\nGeneralized Cylinder\n1997\nShi & Malik\n彩色图像\n把图像切分为有意义的几部分（感知分组）\n1999\nDavid Lowe\n“SIFT”& Object Recognition\n特征检测\n2001\nViola&jones\nFace Detction\nPascal VOC & ImageNet\nPASCAL 20种物体\nhttp://host.robots.ox.ac.uk/pascal/VOC/\nImageNet 图像数据库\nhttp://www.image-net.org/\nCS231n overview\nobject detection\naction classification\nimage captioning\nCNN is an important tool for object detection."}
{"content2":"在多视角几何中，特别是在一些恢复相机运动轨迹的模型中，我们需要将相机的旋转和平移表示出来。通常情况下，我们都是在欧几里得空间中用R和t来进行相应的运算得到相机轨迹。然而，在很多论文中，作者们却喜欢用Lie algebra se(3)、so(3) 以及 Lie group SE(3)、SO(3) 之类的表示。紧接着，出现了很多术语，比如twist, tangent space，也出现了一些运算，比如exp(),log()之类的，看得我是云里雾里。\n比如下面这个公式， 是由三个角速度组成的向量，表示对应的旋转矩阵， 表示由三个角速度组成的反对称矩阵（skew-symmetric matrix）。\n自然要问为啥通过指数运算能够把角速度映射到旋转矩阵，它的背后又有什么样的物理意义，这中间是否有一些尽量直观的解释。\n本篇博客将从最基础的内容出发，用直观的容易理解的方式对旋转矩阵和李代数之间的关系进行推导，如有错误，请指出，希望共同进步。\n预备知识：\n在进入正题之前，我们先需要复习下向量叉积（cross product），以及反对称矩阵(skew symmetric matrix)，在计算机视觉中最初遇到这些概念应该是在求解本征矩阵时，然而，他们在沟通刚体变换矩阵和李代数之间扮演着十分重要的作用。\n对给定的两个三维实数向量，它们的叉积可以通过下面的公式计算出来：\n叉积有如下性质：\n这说明通过叉积运算得到的结果实际上是一个垂直于u,v 平面的向量。\n这种两个向量的运算可以通过一个矩阵来表示，即 其中 是一个3*3的实数矩阵：\n注意向量u上带一个帽子表示的是由它构成的反对称矩阵。同时由这个矩阵也很容易看出有 请记住这个性质。并且任意一个3*3的反对称矩阵，我们能够找到一个三维向量和它对应。\n至于什么是刚体变换，什么是旋转矩阵，旋转矩阵有哪些性质这些更基础的知识在这里不再一一补充。\n下面的内容中，都是基于3维空间，所以没有特别说明时，所说的旋转矩阵都是3*3的，平移向量也是3维的。并且所有向量上带一个帽子的表示的是它的反对称矩阵形式。\n旋转矩阵与 so(3)：\n我们知道对于旋转矩阵，旋转矩阵本身乘以它的转置等于单位矩阵：\n对上式求导可以得到：\n(1)\n这个1式推得的结果不正好满足反对称矩阵的性质吗？因此肯定存在一个三维向量，使得该向量组成的反对称矩阵满足：\n(2)\n对2式中两边各右乘一个旋转R(t)，得到：\n(3)\n这个等式是不是很熟悉？一个变量的导数等于它本身再乘以一个系数。Bingo,是他是他就是他，我们高斯微积分中熟悉的指数！\n在介绍我们期待已久的指数映射前，先挖掘挖掘(3)式的一些含义。如果初始时刻，那么有。那么在单位矩阵附近的一个旋转矩阵，我们能用一阶导来近似得到：\n这是不是有点类似于速度位移之类的。我们把所有的这些反对称矩阵集合起来就组成了一个所谓的李代数Lie algebra so(3) :\n而把所有的旋转矩阵集合起来呢，就有了一个所谓的李群Lie group SO(3) 注意是大写：\n上式中的约束表示R是旋转矩阵。由前面的推导公式三知道如果R是单位矩阵，那它的导数就是一个反对称矩阵，所以只有反对称矩阵组成的空间，即 so(3)，我们称之为在在单位矩阵处的正切空间tangent space.为什么称为正切呢？回忆二维曲线在某处的导数是一条切线。对于这个三维球面，那么它的导数应该是个切面。如下图所示，图片来源于tangent space 的 wiki：\n可是对于那些不是单位矩阵的旋转矩阵R该怎么找在他们位置处的正切空间呢？由公式3我们知道，在反对称矩阵的右边乘以R就能够得到R的导数，所以在非单位矩阵的R处的正切空间就是反对称矩阵乘以R就行了。\n指数映射：\n回到公式(3)，把旋转矩阵R用x替换掉，如下：\n求微分方程得到：\n其中是矩阵的指数映射:\n) (4)\n好了，此时我们假设R(0)=I，即单位矩阵，就可以得到：\n(5)\n当然可以验证验证这个指数是不是旋转矩阵:\n所以，我们能够说指数映射将so(3)映射到了SO(3)：\n也就是说找了一个通道，它将切平面上的一点可以映射到球面体上去。但是，指数还带一个矩阵这不好算吧，没关系，马上带来Rodrigues formula for rotation matrix。首先不妨看看简单的情况，假设w模长为1，||w||=1，这时候有如下性质：\n此时指数映射的泰勒展开式(4)式能够整理成如下形式：\n注意公式中两个括号里面的内容，他们就是sin(t)和(1-cost(t))，其实这种形式的推导在欧拉方程那里也见到过。最后得到：\n当然，对于任意的旋转矩阵，我们也能够找到一组对应的w,t：\n上面推导的是连续时间的，并且假设||w||=1，即旋转速度为单位速度，t是一个时间跨度，联合起来的物理意义就是在单位旋转速度w下，经过时间t后，旋转了多少。可是，我们常常见到的是另一种情况，单位时间t，旋转速度却不为单位模长了，||w||≠1。表达成公式就是如下情况：\n简单整理：\n这个时候||w||≠1，因此，有\n最后，综合起来就是：\n即：\n这公式就只和w这个三维向量有关了，||w||的计算轻松加随意，三维向量变换成反对称矩阵也是容易，所以整个将三维旋转速度映射到旋转矩阵编程实现是不是也很容易了。\n刚体变换和SE(3)：\n前面还只说了旋转，实际上刚体变换还有平移。所以，和只有旋转矩阵构成的李群SO(3) 一样，我们也可以有由刚体变换得到的李群SE(3) :\n和之前的推导一样，我们可以对如下的刚体变换矩阵求导：\n对这个矩阵的每部分进行分析可知，存在一个反对称矩阵和三维向量使得下面两式成立：\n即可以得到：\n所以，我们可以和反对称矩阵一样，定一个矩阵，注意这里这个帽子不代表反对称矩阵了：\n接着我们就能得到：\n这和之前的推导太像了。由于有了前面的铺垫，我们可以直接给出g(t)附近的近似，而不必像之前那样从R(t)=I开始:\n可以称之为在曲线g(t)处的”正切向量”,而在机器人领域，我们称它为twist。这个twist呢，就像我们开葡萄酒塞时螺旋的角速度和前进的线速度。于是把这些twist集合起来就有了刚体变换的李代数se(3):\n当然这个twist矩阵也可以表示成一个六维的向量：\n同理，由\n可以得到：\n如果假设g(0)=I，有\n又到了指数映射,这个矩阵的指数映射可以写成如下形式：\n于是我们知道了如何将se(3) 映射到SE(3)。\n这里也来一个去掉时间t的版本：\n矩阵的各个部分对应如下：\n到这里基本理清了SE,SO之类的与刚体变换之间的关系，看视觉SLAM类的论文以及相应代码中有关lie部分应该没啥压力了。\n各种论文里涉及到的求解位姿矩阵时的非线性最小二乘优化（牛顿法，LM法），其中增量都是在单位矩阵处的tangent space se(3)上计算，获得的增量(即相邻位姿变换关系)通过指数映射映射回多面体SE(3)上。\n通过这种方式，能够避免奇异点，保证很小的变换矩阵也能够表示出来。这一段引用自论文《Scale Drift-Aware Large Scale Monocular SLAM》。\n这篇博文可以说是我看慕尼黑工大(TUM)多视角几何教学视频的笔记，YouTube链接点击这里，这位牛的飞老师的英语吐字清晰，大家应该能够听懂。当然，老师也是参看的别人的文档，这里我也把讲lie 和计算机视觉的两个文档传到了csdn上，供大家下载。\n注意：原文链接http://blog.csdn.net/heyijia0327/article/details/50446140"}
{"content2":"iiMedia Research（艾媒咨询）数据显示，2017年中国计算机视觉行业市场规模为68亿元，预计2020年市场规模达到780亿元，年均复合增长率达125.5%。2018年，计算机视觉行业获得了大批融资，企业对行业的研发投入加强，技术获得了突破。\n(欢迎加v：iimediaLucy，参与更多行业交流。)\n以下为报告节选内容：\n2018Q1行业热点：手机厂商跟进AI摄影\n2018年4月，华为发布HUAWEIP20系列，提出“AI智慧摄影”。除了华为，小米、vivo、OPPO、一加等手机厂商也推出了带有AI摄影功能的手机。艾媒大数据舆情监控系统监测显示，“AI摄影”言值数据为61，整体舆论偏正向。艾媒咨询分析师认为，AI摄影能有效缩小普通用户与专业摄影人员的差别，提升手机用户的拍摄体验以及照片质量，网民对AI摄影的评价偏友好。\n2018Q1计算机视觉热点：3D结构光技术\n3D结构光的基本原理是结构光投射特定的光信息到物体表面后，由摄像头采集，根据物体造成的光信号的变化来计算物体的位置和深度等信息，进而高分辨率、高精确度地复原整个三维空间。3D结构光技术运用于人脸识别当中，能够提高识别的精确度和实用性。此前苹果手机已突破这一技术并运用到iPhoneX当中。目前，中国加快3D结构光技术的研发，未来落地市场广阔。2018年2月，云从科技正式在国内首发“3D结构光技术”，打破了苹果FaceID的垄断，成为中国最早一批将结构光技术应用在人脸识别系统的企业。\n计算视觉在广东的讨论度最高，网民舆论偏正向\n艾媒大数据舆情监控系统显示，在系统监测期间，“计算机视觉”在广东地区的讨论度最高，网民评论偏正向。\n预计到2020年中国计算机视觉市场规模达780亿元\niiMediaResearch（艾媒咨询）数据显示，2017年中国计算机视觉行业市场规模为68亿元，预计2020年市场规模达到780亿元，年均复合增长率达125.5%。艾媒咨询分析师认为，计算机视觉商业落地场景广阔，能够有效解决应用行业的痛点，助力应用行业提升运营效率，实现行业升级，市场发展空间巨大。另外，国家政策的扶持也为计算机视觉的发展提供有利的环境，未来计算机视觉市场规模将迎来高速发展。\n人脸识别技术落地应用受网民期待\niiMediaResearch（艾媒咨询）数据显示，均有超过七成的网民表示对照片美化、人脸解锁手机或APP了解过，其中超四成的网民体验过照片美化，近三成的网民体验过人脸解锁手机或APP。艾媒咨询分析师认为，照片美化在网民的渗透率高，能够优化用户拍照体验，受到网民的认可，大众的体验程度高。另外，随着3D结构光人脸识别技术的突破和应用，未来人脸解锁在手机的渗透率将会提升。\n银行业务是网民最为了解的人脸识别技术应用场景\niiMediaResearch（艾媒咨询）数据显示，60.6%的网民表示听过人脸识别技术在银行自主办理业务中的使用，其次是手机解锁、APP应用和安防领域。艾媒咨询分析师认为，金融业、手机应用和安防领域是目前人脸识别技术应用渗透率较高的领域，网民体验程度高、对其认知程度度高。\n交通关卡人脸识别应用的操作简单性最受网民认可\niiMediaResearch（艾媒咨询）数据显示，在体验过高铁、机场、火车等交通关卡人脸识别应用的网民当中，对应用的操作性体验最好，而对稳定性的体验最差。目前交通关卡的人证对比、刷脸进站等服务操作简单，不需复杂的步骤，能使用户快速通过关卡，用户对应用的操作简单性体验良好。\n人脸识别技术的精确性还需提高\niiMediaResearch（艾媒咨询）数据显示，有59.2%网民认为人脸识别技术存在识别不够准确的问题，仅有17.1%的网民认为成本昂贵。\n超过四成网民认为人脸识别技术存在风险\niiMediaResearch（艾媒咨询）数据显示，35.1%的网民认为人脸识别技术安全，44.3%的网民则认为存在风险。而定时更新面部信息资料库、提高辨别精确度则是网民认为提高人脸识别技术安全性的主要途径。\n过半网民看好人脸识别技术的发展\niiMediaResearch（艾媒咨询）数据显示，过半的网民认为人脸识别技术未来发展前景好，将会成为科技发展的主流，仅9.5%的网民认为不够实用，可能会被淘汰。艾媒咨询分析师认为，人脸识别技术日趋成熟，应用场景不断拓展，未来的发展前景广阔。\n网民普遍认为AI摄影能提升拍照体验\niiMediaResearch（艾媒咨询）数据显示，超半数受访网民认为AI摄影能够优化用户拍照体验，只有15.1%的认为AI摄影实际效果不好。\n网民对AI摄影的前景看好\niiMediaResearch（艾媒咨询）数据显示，48.9%的网民认为AI摄影或成为手机未来的发展趋势，34.4%的网民则表示观望的态度。\n商汤科技：多款新品发布\n2018年4月，商汤科技在人工智能峰会发布多个最新产品：增强现实黑科技SenseAR，智能图片视频审核平台SenseMedia，智慧城市、平安城市解决方案SenseFace3.0和SenseFoundry，以及智能汽车产品驾驶员监控系统SenseDriveDMS。较于以往，商汤从技术更多地将关注点转到落地行业、赋能行业上面。\n商汤科技：强化上下游产业链\n2018年4月，商汤科技宣布完成由阿里巴巴集团领投，新加坡主权基金淡马锡、苏宁等投资机构和战略伙伴跟投6亿美元C轮融资。一方面，商汤科技通过融资强化在安防、手机、自动驾驶及互动娱乐等行业应用，另外一面，商汤通过加强于上下游企业的合作，拓展更丰富的应用场景，加速AI技术落地。\n云从科技：专注计算机视觉技术研发和应用\n云从科技成立于2015年3月，是一家孵化于中国科学院重庆研究院、专注于计算机视觉与人工智能的高科技企业。云从科技搭建的“三层研发体系”，覆盖了从前沿科学到市场应用的全过程，为云从聚集技术人才。同时，云从科技利用平台资源、大数据资源，不断实现数据积累、算法迭代、技术更新。\n云从科技：专注计算机视觉技术研发和应用\n2018年以来，云从科技跨镜追踪技术和3D结构光人脸识别技术取得突破。艾媒咨询分析师认为，技术是计算机视觉企业发展基础，提高技术的准确性、加快关键技术的研发，有利于技术商业落地，迅速占领市场份额。\n云从科技：深耕重点行业，推动AI+商业落地\n2017年11月，云从科技宣布完成并由顺为资本、元禾原点、普华资本联合领投B轮5亿元人民币融资，加上此前广州市政府对云从科技的20亿政府资金支持，此次总计获得25亿元发展资金。云从科技凭借自身技术优势，深耕银行、机场、安防等重点行业，并拓展电信业等其它细分市场，为应用行业提供技术赋能。计算机视觉是人工智能发展较为成熟的技术，商业落地机会多，拥有广阔的应用前景和市场机遇。艾媒咨询分析师认为，云从科技聚焦计算机视觉，加速技术在重点行业商业落地，并拓展其他细分领域，不断完善AI布局，有利于加强自身综合竞争能力。\n云从科技：精耕用户需求，不断优化产品\n云从科技为用户提供软、硬件以及一体化的服务。云从科技深入银行、安防等行业，了解客户真正的需求，结合场景不断做出新的产品和解决方案，进而解决应用场景的痛点，提升应用场景的运营效率，实现理论创新到产业应用，从而赢得用户口碑，实现企业持续健康发展。\n中国计算机视觉行业未来发展趋势\n头部公司战略投资、收购相关领域初创公司或成常态\n头部公司面临业务快速扩张，但存在技术和商业落地场景欠缺等问题。在多轮融资后，头部公司手握大量资金，通过战略投资的方式能够解决技术短板和商用实现周期长的问题，并推出可快速复制的产品或者服务，进而扩大自身现有优势。而对产业上下游的初创公司来说，当前行业市场竞争加剧，要自建技术团队、自研出较高水平技术，拓展应用场景，难度高投入大，接受头部公司的投资能够增强自身技术水平、提高竞争力。头部玩家通过战略投资、收购相关领域初创公司等方式，加强与产业上下游企业的合作，将是未来计算机视觉行业发展的一个趋势。\n应用场景进一步渗透，形成AI+\n当前，计算机视觉技术在金融领域、安防领域、手机端、广告领域应用比较广泛，并向新零售、智能工厂、无人驾驶、智能医疗等领域拓展，计算视觉技术的延伸和应用逐渐体现出其可持续发展的重要价值。随着计算机视觉技术成熟，行业应用解决方案的建立和完善，应用行业智能化转型需求的增长，以及政府政策对计算机视觉行业的扶持和对人工智能教育的普及，计算机视觉行业的应用场景将进一步渗透，形成AI+，将助力各应用行业解决行业痛点，提高运营效率，实现行业转型和升级。\n5G时代加速到来，计算视觉产业迎来发展机遇\n当前，5G技术取得突破，有望在2020年实现大规模商用。5G网络速率量级提升和时延降低的特点，将给计算机视觉技术在垂直行业应用带来突破性增长，如在工业自动化、机器人、自动驾驶等领域，计算视觉产业将迎来新的发展机遇。比如，在交通出行上，5G的低时延+大带宽将确保远程操控安全，满足各种自动驾驶场景诉求，提升自动驾驶在通行中的安全性，实现自动驾驶的规模化商用。在智慧安防的打造上，5G技术将为安防提供数据基础，促进安防神经系统的构建，助力公共部门实现高效运营和维护公共安全。5G时代的到来，将给计算视觉行业带来更大的市场机遇，形成新的增长点。\n关于艾媒咨询\niiMedia Research(艾媒咨询)是全球知名的新经济产业第三方数据挖掘和分析机构，2007年诞生于广州，在广州、香港、北京、上海、硅谷设有运营和分析机构。艾媒咨询致力于输出有观点、有态度、有结论的研究报告，以权威第三方实力，通过艾媒大数据决策和智能分析系统，结合具有国际化视野的艾媒分析师观点，在产业数据监测、调查分析和趋势发展等方向的大数据咨询具有丰富经验。艾媒每年公开或定制发布新经济前沿报告超过2000份，覆盖了人工智能、新零售、电商、教育、视频、生物、医疗、音乐、出行、房产、营销、文娱、传媒、金融、环保与公共治理等领域，通过深入数据挖掘，通过数学建模，分析推理与科学算法结合，打造有数据、有理论支撑的大数据分析成果。艾媒咨询的数据报告、分析师观点平均每天被全球超过100家主流媒体，1500家(个)自媒体、行业KOL广泛引用，覆盖语言类型包括中、英、日、法、意、德、俄、阿等约二十种主流官方版本。\n基于公司独立自主研发的“中国移动互联网大数据挖掘与分析系统(CMDAS)”，艾媒咨询建立了互联网运营数据、企业舆情和商情、用户属性和行为偏好、零售数据挖掘、广告投放效果、商业模式等多维度的数据监测体系，可视化还原“数据真相”，实现市场趋势的捕捉和用户信息的洞察，提升品牌的行业竞争和影响力。"}
{"content2":"博主github：https://github.com/MichaelBeechan\n博主CSDN：https://blog.csdn.net/u011344545\n计算机视觉数据集：https://github.com/MichaelBeechan/CV-Datasets\nImage Motion and Tracking\n数据集\n功能\n解释\n链接\nALOV300++ Dataset\nVisual Tracking\nVideos for evaluating visual trackers robustness\nALOV300++\nVOT2015 Benchmark\nVisual Tracking\nThe dataset comprises 60 short sequences showing various objects in challenging backgrounds.\nVOT2015\nMOT Benchmark\nThe Multiple Object Tracking Benchmark\nincluding object detection, pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation\nMOTChallenge\nTHUMOS\nTemporal action localization(动作定位)\nUCF101、Pre-computed low-level Features (Improved DTF [1])、List of action classes and their numerical index\nTHUMOS Challenge 2014\nActivityNet\nLarge Scale Activity Recognition\nUntrimmed Classification Challenge、Detection Challenge\nActivityNet\nScene Flow Datasets\nOptical Flow\nA Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation\nScene Flow\nKITTI Vision Benchmark\nOptical Flow\nstereo 2012, flow 2012, odometry, object detection or tracking benchmarks\nKITTI\nMPI Sintel Dataset\nOptical Flow Training & Test Data\nNew training data is available! Please see the dedicated pages for Stereo and disparity, Depth and camera motion, and Segmentation.\nMPI\n3D Computer Vision\n数据集\n功能\n解释\n链接\nNYU Depth V2 dataset\n关于RGBD 图像场景理解的数据库\nThe NYU-Depth V2 data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect.\nNYU Depth Dataset V2\nSUN RGBD dataset\n3D bounding boxes和room layouts的标注\nDataset for IKEA 3D models and aligned images\nVOT2015\n(Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild)PASCAL3D+ :http://cvgl.stanford.edu/projects/pascal3d.html\nSUN RGB-D: A RGB-D Scene Understanding Benchmark Suite : http://rgbd.cs.princeton.edu/\nNew Tsukuba Dataset：http://www.cvlab.cs.tsukuba.ac.jp/dataset/tsukubastereo.php\nDTU Robot Image Data Sets：http://roboimagedata.compute.dtu.dk/\nOXFORD ROBOTCAR DATASET：https://robotcar-dataset.robots.ox.ac.uk/\nShapeNet ：https://www.shapenet.org/\nmiddlebury：http://vision.middlebury.edu/stereo/\nOther Datasets you can find in https://github.com/MichaelBeechan/CV-Datasets"}
{"content2":"人工智能受益于神经网络和深度学习在算法上的突破，技术水平得到飞跃提升。未来，计算机视觉、机器学习、自然语言处理、机器人技术、语音识别等人工智能技术或将给整个人类社会带来巨大改变。\n语音识别与自然语言处理应用\n智能客服\n整合全集团对外的客户服务通道，提供多模式融合（包括电话、网页在线、微信、短信及APP等）的在线智能客服；对内实现语音分析、客服助理等商业智能应用。为坐席提供一种辅助手段，帮助坐席快速解决客户问题。客服助理通过实时语音识别，实时语义理解，掌握客户需求，自动推送客户特征、知识库等内容。借助于微信公众号等平台，推出语音问答系统，打造个人金融助理形象。\n语音数据挖掘\n语音语义分析自动给出重点信息聚类，联想数据集合关联性，检索关键词，并汇总热词，发现最新的市场机遇和客户关注热点。同时，根据金融行业客服与客户的通话情况，可进行业务咨询热点问题梳理统计，由机器进行自动学习，梳理生成知识问答库，并作为后续机器自动回复客户问题的参考依据。\n计算机视觉与生物特征识别应用\n人像监控预警\n利用网点和ATM摄像头，增加人像识别功能，提前识别可疑人员、提示可疑行为动作，识别VIP客户。\n员工违规行为监控\n利用网点柜台内部摄像头，增加员工可疑行为识别监控功能，记录并标记疑似交易，并提醒后台监控人员进一步分析，同时起到警示作用。\n核心区域安全监控\n在银行内部核心区域增加人像识别摄像头，人员进出必须通过人脸识别及证件一致方可进入，同时对于所有进出人员进行人像登记，防止陌生人尾随进出相关区域。如集中运营中心、数据中心机房等。\n机器学习、神经网络应用与知识图谱\n金融预测、反欺诈\n大规模采用机器学习，导入海量金融交易数据，使用深度学习技术，从金融数据中自动发现模式，如分析信用卡数据，识别欺诈交易，并提前预测交易变化趋势，提前做出相应对策。基于机器学习技术构建金融知识图谱，基于大数据的风控需要把不同来源的数据(结构化，非结构)整合到一起，它可以检测数据当中的不一致性，分析企业的上下游、合作、竞争对手、子母公司、投资、对标等关系。\n融资授信决策\n通过数据筛选、建模和预测打分，并将不同的资产分类和做分别处理。比如：坏资产可直接标签为“司法诉讼”，并提醒相关人员进行诉讼流程。通过提取个人及企业在其主页、社交媒体等地方的数据，一来可以判断企业或其产品在社会中的影响力，比如观测App下载量，微博中提及产品的次数，对其产品的评价;此外将数据结构化后，也可推测投资的风险点。借助机器学习完成传统金融企业无法做到的放贷过程中对借款人还贷能力进行实时监控，从而及时对后续可能无法还贷的人进行事前的干预，以减少因坏账而带来的损失。\n智能投顾\n根据马科维茨的现代资产组合理论（MTP），结合个人客户的风险偏好和理财目标，利用人工智能算法和互联网技术为客户提供资产管理和在线投资建议服务，实现个人客户的批量投资顾问服务。\n服务机器人技术应用\n在机房、服务器等核心区域投放24小时巡检机器人，及时发现处理潜在风险，替代或辅助人工进行监控。在网点大堂尝试设置智慧机器人，赋予机器人拟人化，赋予其人类的形象和相应感情、动作。对网点客户进行业务咨询答疑、辅助分流，采集客户数据，开展大数据营销工作，完成查询、开卡、销卡等业务的辅助办理。\n当前，人工智能技术在辅助人工、提高劳动生产率上发挥了积极作用，金融行业作为科技发展的重要应用和践行者，紧跟人工智能发展趋势，积极尝试在各领域的运用与验证，促进社会发展。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n1.人工智能时代，AI人才都有哪些特征？\nhttp://www.duozhishidai.com/article-1792-1.html\n2.大数据携手人工智能，高校人才培养面临新挑战\nhttp://www.duozhishidai.com/article-7555-1.html\n3.人工智能，机器学习和深度学习之间，主要有什么差异\nhttp://www.duozhishidai.com/article-15858-1.html\n4.大数据人工智能领域，如何从菜鸟晋级为大神\nhttp://www.duozhishidai.com/article-1427-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"人工智能(AI)如何模拟人类智能?微软亚洲研究院由低到高，将人类智能分为计算与记忆力、感知、认知、创造力、智慧五大层级，而人工智能将逐步从最底层开始对人类智能进行模拟。报告认为，在最底层的记忆力和计算层面，计算机已成为人类不可或缺的助手。而下一层面，即感知层面，是当下人工智能发展聚焦的主要层面。\n报告称，基于视觉、听觉等感知能力的感知智能近年来取得了相当多的突破，在业界多项权威测试中，人工智能系统都已经达到甚至超过人类水平，感知智能正迎来它最好的时代。人脸识别、语音识别等感知智能技术如今已运用在图片处理、安防、教育、医疗等多个领域。\n比感知智能更高一层的认知层面，人工智能也已开展探索发展。报告指出，AI的认知智能包括自然语言理解、知识和推理等，其中，核心为自然语言理解，它的进步将推动AI认知能力的提升。如今，AI已经开始渗透和改变商业智能的传统做法，但关键的分析和决策领域，AI仍需和人类共同完成。\n在更高一层的创造力层面，人工智能也并非无所建树。按照报告，目前，在大数据基础上，利用深度学习等技术，AI也可以在一定程度上模拟人类的创作过程。\n大连渤海医院\nmobile.84239650.cn"}
{"content2":"注明：以下图片来源于人工智能头条公众号，公众号上的图片会更清楚一些哦！如果觉得图片太小，看不清楚，可以单击图片放大，效果会好很多。\n一、自动驾驶技能树\n二、对话系统技能树\n三、推荐系统技能树\n四、知识图谱技能树\n五、自然语言处理技能树\n六、计算机视觉技能树\n七、语音识别技能树\n八、异构并行计算工程师技能树\n九、数据科学家技能树\n十、机器学习算法工程师技能树"}
{"content2":"第二届人工智能竞赛——题目二、目标检测\n文章目录\n第二届人工智能竞赛——题目二、目标检测\n一、简介\n二、题目要求\n三、参考资料\n一、简介\n目标检测作为图像处理和计算机视觉领域中的经典课题,在交通监控、图像检索、人机交互等方面有着广泛的应用。它旨在一个静态图像(或动态视频)中检测出人们感兴趣的目标对象。\n二、题目要求\n利用现有的深度学习模型实现目标检测，正确识别出视频流中的人的物体。\n利用 QT 设计 UI 界面，界面要求:友好、功能完善、简介。\n评估模型的效果，求 recall、roc 等曲线\n三、参考资料\nhttps://github.com/mystic123/tensorflow-yolo-v3\nhttps://github.com/pjreddie/darknet\nhttps://github.com/endernewton/tf-faster-rcnn\nhttps://github.com/hoya012/deep_learning_object_detection\nhttps://github.com/tensorflow/models/tree/master/research/object_detection\nUI界面可用QT编写"}
{"content2":"HSV(Hue, Saturation, Value)是根据颜色的直观特性由A. R. Smith在1978年创建的一种颜色空间, 也称六角锥体模型(Hexcone Model)。\n这个模型中颜色的参数分别是：色调（H），饱和度（S），明度（V）。\n色调H\n用角度度量，取值范围为0°～360°，从红色开始按逆时针方向计算，红色为0°，绿色为120°,蓝色为240°。它们的补色是：黄色为60°，青色为180°,品红为300°；\n饱和度S\n饱和度S表示颜色接近光谱色的程度。一种颜色，可以看成是某种光谱色与白色混合的结果。其中光谱色所占的比例愈大，颜色接近光谱色的程度就愈高，颜色的饱和度也就愈高。饱和度高，颜色则深而艳。光谱色的白光成分为0，饱和度达到最高。通常取值范围为0%～100%，值越大，颜色越饱和。\n明度V\n明度表示颜色明亮的程度，对于光源色，明度值与发光体的光亮度有关；对于物体色，此值和物体的透射比或反射比有关。通常取值范围为0%（黑）到100%（白）。\nRGB和CMY颜色模型都是面向硬件的，而HSV（Hue Saturation Value）颜色模型是面向用户的。\nHSV模型的三维表示从RGB立方体演化而来。设想从RGB沿立方体对角线的白色顶点向黑色顶点观察，就可以看到立方体的六边形外形。六边形边界表示色彩，水平轴表示纯度，明度沿垂直轴测量。\n#coding:utf-8\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimg = cv2.imread(\"./imgs/7.jpg\")\nimg_cp = np.copy(img)\nimg_cp = cv2.cvtColor(img_cp,cv2.COLOR_BGR2HSV)\nplt.figure()\nf,(a1,a2,a3,a4) = plt.subplots(1,4,figsize=(200,200))\nh_img = img_cp[:,:,0]\ns_img = img_cp[:,:,1]\nv_img = img_cp[:,:,2]\na1.set_title(\"HSV channel\")\na1.imshow(img_cp,cmap='gray')\na2.set_title(\"H channel\")\na2.imshow(h_img,cmap='gray')\na3.set_title(\"S channel\")\na3.imshow(s_img,cmap='gray')\na4.set_title(\"V channel\")\na4.imshow(v_img,cmap='gray')\n#plt.imshow(img_cp)\n#plt.show()\nl_threhold = np.array([100,43,46])#blue range\nh_threhold = np.array([124,255,255])\nmask = cv2.inRange(img_cp,l_threhold,h_threhold)\nmasked_img = np.copy(img_cp)\nmasked_img[mask != 0] = [0,0,0]\nplt.figure()\nplt.imshow(cv2.cvtColor(masked_img,cv2.COLOR_HSV2RGB))\nplt.show()\nHSV 不同通道\n过滤蓝色效果"}
{"content2":"本文系网易新闻-智能工作室出品\n聚焦AI，读懂下一个大时代！\n近日，中国科学院深圳先进技术研究院副院长、香港中文大学教授汤晓鸥教授在杭州云栖大会发表题目为《人工智能的云中漫步》的演讲。\n他表示，总结起来做人工智能的跟阿里的理念其实相似，阿里讲“让天下没有难做的生意”，做人工智能是讲“让天下没有难吹的牛”。\n他还分享了商汤科技在计算机视觉方面的研究成果，比如如何分辨云和雪和地面的物体，用人工智能、图像识别技术，可以做的比人更精准，他举例到，十一的时候很多人去旅游、去登山，山上可以看到半山腰有些云，登到山上以后发现进到云里就变成雾了，拍照就不太清晰，我们有个算法可以帮你把雾去掉。\n汤晓鸥还介绍了目前有关城市大脑的应用实践。\n1\n人脸识别已经可以做到没有任何人工配合的情况下实时的识别人物、抓捕犯人，在广州、深圳、重庆等几十个城市都已经开始帮助公安解决了大量的案件，抓了很多犯人。\n2\n人群，现在可以在上海外滩这样的公共场合，实时判断每一个点的人群的密度、人数，进来多少人，出去多少人，还有人流有没有逆行等特殊情况，这样可以防止踩踏事件。\n3\n视频结构化，可以把视频里面所有的人、车、非机动车、自行车检测、追踪，识别出来属性，比如这个人穿什么衣服，男的女的，多大年龄，车什么牌子，哪年生产的，这些东西都从视频处理成文本文件，你可以进行对应物体的快速搜索。\n以下是汤晓鸥教授演讲实录：\n汤晓鸥：我先帮大会发一个通知，今天午饭取消了，改下午茶了，大家不着急， 慢慢听吧。\n非常感谢阿里的邀请，尤其是做压轴演讲，我跟阿里说太客气了，压轴这么重要的演讲应该马总做，我做个简单的开场演讲就差不多了，后来他们坚持我在午饭时间做压轴。\n另外他们还告诉我说今天有大概一千万人在网上看直播，所以我非常紧张，我就做了个一百页的PPT，我想十万人一页也对得起观众了，但是组委会就非常紧张，一直问我说40分钟讲一百页，会不会超时，我就跟他们保证说，放心吧，一定会的。昨天奥委会的客人讲到奥运要更快更高更强，那云栖大会的特点就是要更长。\n前些时候我跟马化腾还有一些学者在清华做了一次对话，我当时当着马化腾先生的面提了一些意见。今天我到了阿里这里，我想我也不会客气的，作为学者，我们就是要敢于提意见。所以当时我敢于当面给Pony（马化腾）提意见，今天（到了阿里这）我就准备在背后再给Pony提一些意见（此处玩笑，请勿当真）。\n现在言归正传，今天我讲的题目是《人工智能的云中漫步》。人工智能其实我也听了很多人讲，我自己也讲了很多，我觉得总结起来做人工智能的跟阿里的理念其实非常相似。阿里是讲“让天下没有难做的生意”，做人工智能的是讲“让天下没有难吹的牛”：）。\n这两天大会听大家讲了半天人工智能，讲了半天的云，一直到今天为止，一直到现在为止，我们其实一直没有看到真正的云，现在我给大家看一下。\n这是高分一号卫星拍的云图。其实我们发射卫星拍摄这些图像，是为了分析地面上的情况。高分一号拍出来的图，有云有雪，遮盖了地貌，如何分辨云和雪和地面的物体，我们用人工智能、图像识别技术，可以做的比人更精准。大家可以看到褐色的是雪，白色的云，绿色的是物体。我们识别的这些云以后，还能用算法把这些云去掉了，这样卫星就可以识别云下面的东西。\n十一的时候我想很多人去旅游、去登山，山上可以看到半山腰有些云，登到山上以后发现进到云里就变成雾了，拍照就不太清晰，我们有个算法可以帮你把雾去掉。还有你航拍的时候，有一些云、雾，我们也可以用算法实时的在视频里把它去掉。\n大家想我们杭州好像很少有雾霾，这个跟杭州有什么关系？确实也没什么关系，当年做的时候，是专门给北京做的，给北京量身定制的，奥运会时直接把雾去掉了，蓝天白云的，我们把这个叫商汤蓝。\n这个算法，这个应用我们已经把它做到微博相机上成为产品了，去年就已经上线了。\n如果这么一直讲下去，阿里的人可能急了，我们是阿里云，不是阿里气象局。当然，我们讲的是虚拟云，云计算，我们其实在不知不觉间已经生活在云中间了，我们生活在物理云下面，实际上我们也生活在虚拟云上面。今天就给大家讲你是如何在云上生活一天的，大概要讲8个小时。\n一开始，早上起来要化妆。就是拿着手机可以当镜子，可以做美颜，换衣服，用各种特效效果看一整天该穿什么。这个化妆下来大概的时间从8点开始的，最后结束了以后，就到9点了，一般女孩的话大概也确实需要一个小时化妆，最后通过美颜、增强现实（AR）这些特效，不知道为什么最后这张图成兔女郎了。\n然后接着这些AR技术还可以应用在其它的场景上，比如社交场景应用，你们现在看到的这些拍照APP，直播APP，有很多AR特效，其实绝大部分都是基于我们提供的人工智能技术，比如人脸的106点和最新的240点的追踪分析，是我们定义的行业标准。\n我们不但做人脸、手势识别，现在已经做到三维的SLAM特效了，大家可能看过这种特效，游戏里面可以把虚拟物体加到这个现实世界里面，但是以前看的都是在一台很强的计算机上算出来的，现在我们这个是在手机端，手机上实时算出来，这是非常难的事情。\n还有你刚才为什么花了一个小时换衣服呢？要一件一件换，不合适换另一件，很麻烦。我们实际上可以用计算机帮你换衣服，计算机生成衣服。这个用什么做的呢？用基于自然语言处理的图像图像生成技术，比如说我要一只小鸟，有白色的胸脯，灰色的头部，就生成这样的小鸟；再要一只红色的小鸟，黑色的翅膀，就再对应生成出来图像。这都是计算机自动的根据你的语言描述生成的，或者是花也一样，可以生成一些不同的花。\n更实用的应用是什么呢？是衣服。我可以说我想穿一件浅蓝色的连衣裙就换成浅蓝色的连衣裙，或者黑色无袖外套就给你换上了，这样换衣服的速度非常快，几分钟就完事了。\n十点钟要出门了。出门走路的时候，可能没有什么感觉，但实际上每个城市里，刚才讲都有几十万台甚至百万台相机，这些相机做的事情是把人、车，物体都检测、识别、分析出来。\n今天讲了很多关于城市大脑的问题。\n我们要解决这些问题还是需要核心技术，来一样一样完成这些任务。首先我们人脸识别，已经可以做到没有任何人工配合的情况下实时的识别人物、抓捕犯人，在广州、深圳、重庆等几十个城市都已经开始帮助公安解决了大量的案件，抓了很多犯人。\n人群，我们可以在上海外滩这样的公共场合，实时判断每一个点的人群的密度、人数，进来多少人，出去多少人，还有人流有没有逆行等特殊情况，这样可以防止踩踏事件。\n再就是视频结构化，可以把视频里面所有的人、车、非机动车、自行车检测、追踪，识别出来属性，比如这个人穿什么衣服，男的女的，多大年龄，车什么牌子，哪年生产的，这些东西都从视频处理成文本文件，你可以进行对应物体的快速搜索。\n所以其实你在走在路上的时候，所有的这些信息都是可以记录下来的。所以以后如果做坏事会越来越难。大家如果现在还有什么事没做赶紧做，以后再做相对会困难很多了。\n12点钟大家可能出去跟朋友玩了。拍一些自拍照，其实拍的时候，就是用了我们的一些视频处理的技术，比如把一个手机拍照拍成单反的效果，这也是我们做的技术，先拍照后聚焦，拍完点什么地方就聚焦到什么地方。另外在拍之前，我就想看看单反预览效果是什么样子的，所以这时候你在动的时候，效果就要显示出来，这就是要实时视频级的处理。视频上能够实时把深度信息算出来，预览做出来。这些技术已经在OPPO R9S和R11用了很长时间了，包括里面的人脸技术都是使用我们的技术支持。\n还有手机上可以做一些智能相册的特效、处理。计算机识别你的照片内容，然后根据内容打标签、分类管理。\n这些特效，大家现在手机上可能节日期间也会用到一些这些应用，比如把卡通图片里的脸换成自己小孩的脸。但是我给你演示这些是我们十年前做的，我们十年前已经做到这个效果了，当然那个时候是在计算机上做出来的，现在把这些技术可以做到手机上了。\n我们跟小米合作做了小米智能相册，跟华为合作做了华为智能相册，跟微博合作，把大V的照片管理做起来。\n两点多钟，你照完相了，吃完饭回来，对照片想处理，做一些新的艺术化的滤镜。\n感觉我们公司的人基本不干活，整天在玩手机。\n处理出来这些特效，这是在图像上做成的特效，其实这是我们两年前做的工作，现在满大街都是。我们现在又做了新的工作，是视频上实时也可以做出特效，而且可以做出各种特效。\n4点钟，大家可以出去玩一玩，可以做一些体育运动了，大家可以想像一下，我们公司4点钟就下班了，开始去玩了。\n这个就是我们在实时的把人体的整个结构都能跟踪出来，大家可能觉得这个不是什么新鲜事，因为几年前Kinect体感摄像头就能做的，但是原来是一个昂贵的特殊设备做的，设备有两个摄像头还有激光投影，我们是用一个几块钱的单个webcam，可以实时做这件事情，所以这个应用可以在各种的智能家居、自动驾驶，各种地方做到实用。\n再往下用这些技术还可以做体育运动的分析。昨天讲到奥委会跟阿里合作，我们也在跟国家体育总局做合作。这个大家可以看到我们用智能分析的方法跟踪运动员的动作。然后也可以帮助运动员做康复的训练。所以昨天奥委会朋友讲，奥运会要做到更高、更快、更强、更聪明，那其实我现在给你讲的，就是如何做到更聪明。\n同时我们可以用跟踪的算法，然后把整个画面分析清楚，用自然语言描述视频里运动员到底在干什么。\n然后大家下班的时候要坐车回家了。这时候可以乘坐由我们自动驾驶技术支持的汽车。自动驾驶里面我们做了六个大的方向，三十几项技术，目前跟全球前五大车企其中一个顶级的厂商进行合作。\n下面看一下刷脸支付场景，因为你下班了，总是要买东西的。可以用刷脸支付，阿里无人店可以用这些技术。还有一些门禁系统，酒店，机场等等应用，所有这些地方其实现都在用我们做的人脸识别技术，现在的准确率从当年第一次超过人眼睛极限的时候，从97.5%，到99.15%，到99.55%，一直做到万分之一，十万分之一，百万分之一，今天我们早就做到亿分之一，实际上已经达到了八位数密码的精度，可以做各种应用了。\n到了晚上，这个视频里，我们分析人的运动方向。这些对整个分析视频的结构也是非常重要的技术。\n我们综合前面这些技术，可以把整个这个视频场景分析全部做出来。可以看到左下角会讲你在什么地方，什么样的活动，每个人是哪一个人，哪一个演员，穿的什么衣服，后面有什么物体，骑的什么摩托车，所有这些结构化都可以做出来，大家网上看到很多公司用这两段视频结果演示做宣传，这个原创是我们做的，视频分析演示也是我们做的。这是《欢乐颂》，本来想做一个更新的，想用《我的前半生》，后来一想我的前半生也快过去了，还是做《欢乐颂》了。\n刚才很多是我们已经落地的产品，是由我们的400多家合作厂商真正落地来用了的。下面还有一些新的技术突破，明天就可以马上用出去，就是因为这些新的技术突破，才继续推动做出来新的应用。\n首先讲运动监测。还是回到奥运会这个应用，实际上我们可以在体育的视频里面把这些射门的镜头提取出来，两个小时的比赛可以很快缩到几分钟，可以完全自动做的。\n或者田径比赛，真的很漫长的，但是精彩的镜头，百米、跳高那几个镜头，就是那几块，我们可以自动的识别提取出来，同时你也可以进行描述，要求怎么样提取出来，你感兴趣的部分。\n然后还可以进行搜索。比如你要搜索音乐表演的视频，战争场面的视频，都可以自动搜索出来。\n或者你要想做电影自动理解。比如可以明白这个镜头到底是灾难的镜头还是浪漫的镜头，用我们前面说的技术来分析整个场景到底是什么样的，红线代表浪漫的，蓝色是灾难的，实时分析镜头。或者说他们在吵架还是浪漫的镜头。都可以实时分析出来。\n可以用自然语言来描述来搜索电影的场景。就是你可以说一段话，它就把那一段镜头的场景把它给搜出来，同时把所有人，物体和各种东西都检测出来。\n还有对体育场景进行分析，就是说可以直接对运动视频进行描述，自动用自然语言描述到底发生了什么事情，这个时候其实我们就不需要播音员了，机器自动分析运动场景做什么，直接给大家讲解，就像一个专业播音员一样。\n还有图像的分割，以前大家讲图像分割都是前景和背景分开，现在做的分割是不但把前景和背景分开，而且还可以像素级地把前景的每一个物体分开，前面有很多跳舞的，每一个人都标注出来，每一个物体，和背景都分割开来，就可以做很多很多各种各样的特效。\n还有就是判断两个人的关系，如果你在网上放了照片，我们根据你这两个人的姿势和两个人的表情，分析出来你们两个人的关系。这个有什么用呢？比如说你跟一个很有钱的人照相，分析的结果是很友好，说明你认识有钱人这样可能你的可信度就增高了，我就可以把钱借给你了，可以做征信的一个维度。\n还有我小孩的照片，他女朋友比较多，想知道哪个是他真正的女朋友，可以分析识别一下，后来发现每一个都是，他跟我一样对每一个都很专一。\n我给学生发了一些比较难处理的关系的照片，比如铭铭6个月的时候跟他第一个女朋友的照片，第一个关系分析的还可以，第二个也分析出来了。后面两张照片就难多了，最后基本上搞不清楚他在干什么了，当然最后这张的这种探索精神还是值得敬佩的。\n我们以前在微软的时候，出去玩的时候照了照片，我也拿过来让机器分析，这是我的两个同事照的我们在九寨沟的照片，机器分析出来的结果不明白真正的含义是什么？（这两个男同事的背影合影）实际的含义是我们在演绎《断背山》这个电影。下面这几张就更难的让机器分析了。一个人的背影还好，两个人也可以理解，出来三个人的背影，机器就糊涂了，到四个人的时候可更糊涂了，五个人就更接受不了了。所以这种对机器来说很难理解，对我们来说，我们是很开心的可以笑出来。我想在这里，提出一个新研究课题，提出一个挑战吧，就叫XO Challenge吧，就是我们怎么能让机器笑？就是你怎么能让机器识别一张图像是搞笑的，我们人可以分辨，机器能不能做到？我希望我们研究人员以后可以试试，看看我们是不是能够让机器看到这些图像，也会会心一笑。\n最后我用一个我们研究的例子来讲一下原创的难度，我刚才讲的每一个技术其实都不是那么简单的，都不是说一拍脑袋一下就做出来的，有非常多的事情要做的，这个例子是图像超分辨率增强，就是我们怎么把一张图放的很大，能够恢复的很清晰。这是美国的一个电影，FBI在抓人。最后他抓到一张很模糊的图像图像放大做成清晰的图像，当时觉得FBI很厉害，非常棒。我们用传统的技术也试图把这个图像恢复一下。当时希望把小图恢复成这样，用传统算法做了最大的努力，最后的结果是这样，所以我们很不满意。\n这个应用有什么用处呢？实际上是把可以进行图像、视频放大，可以把普通的电视信号变成4K的高清信号，8K的高清信号，这是我们最新做出来的结果，可以看到如果直接放大是很模糊的，现在用新的结果基本上达到高清的效果，已经达到实用的阶段。\n在日本有个工作叫WAIFU2X，他们用我们的技术做了演示，就是把太太（二次元妹子）放大两倍，然后用这个图，最后的效果非常清晰。\n超分辨率这个工作是很重要的，因为有很多场合有应用。所以谷歌、推特也对这个非常重视，他们在2016年连着发四篇文章做这个工作。按照以往，大家可能都是跟着谷歌后面做，而我们不是，我们发表了全球第一篇用深度学习超分辨率文章，那是2014年，早于谷歌两年，2015年又发了一篇，2016年两篇，2017发了三篇，我们不但是做的最早的，第一个做的，而且也是目前做的最好的。所以是谷歌在跟着我们做！\n做这一项工作要想做成功，牵扯的工作是非常多的，有各种各样的技术，涉及到十几篇几十篇的文章才能做到现在的效果。\n所以现在我们已经可以做到实用，在街头上拍的照片，模糊照片可以真正看到罪犯的样子。\n而且已经给深圳的公安用了，公安用手机可以拍人的照片，很模糊的图像可以在库里搜索，实时抓捕罪犯。\n经过我们的努力，所有这些加一起，从原来这个效果现在可以做到这个效果了。\n所以每一项工作后面都有大量的工作需要做的，都有大量的顶级文章。我们不是刚刚这几年人工智能热了才开始做的，而是十五六年的积累，04年到08年我们统计了一下在两个顶级的会议上，我们一个实验室发了57篇论文，而MIT全校是51篇，伯克利大学是33篇，牛津大学是45篇；我们十几年在顶级会议文章数量上一直是在全球领先的。在过去两年，三个顶级会议上我们统计了数据，微软最多是发了124篇，CMU是86篇，我们排第三是76篇，是亚洲唯一的进入前十名的。所以我们是有这种强大的人才和经验的积累，才做出刚才这些真正落地的产品。\n在2011年到2013年深度学习刚刚开始的时候，这两个顶级会议上，29篇文章我们占了14篇，全球的一半，这里面16项技术，都是我们第一个真正成功的把深度学习应用到这些技术领域。\n所以我们是深度学习的原创技术公司，是真正做平台的，和脸书的Torch、谷歌的TensorFlow一样，我们做了自己的原创平台Parrots，来在这上面开发我们深度学习相关的技术。\n7月份的时候，我很荣幸作为国际期刊IJCV主编，召集了夏威夷IJCV Night晚宴会议，计算机视觉领域很多顶级学者都参加了我们的这个晚宴。我们在马上10月份，在威尼斯的ICCV大会上会再开一次这样的国际顶级学者的·聚会，欢迎大家过来参加。\n最后，大家看一下这一页上的这些图像的一个共同的点是什么？米开朗基罗、贝多芬、梵高、乔布斯、兰博基尼的设计首席设计师，这些人有一个共同特点，其实就是两个字：原创。中国最缺的就是原创，我们现在做的就是原创，做原创是非常难的一件事情，但是不做原创一个国家是永远也发展不起来的。\n我们在做电影分析的时候，看到这些老的电影，《上甘岭》《英雄儿女》《小兵张嘎》，我们团队的120个博士很像当年《上甘岭》上最后一个加强连，一个博士的加强连。但是以我们这一个加强连的兵力看起来很强大，但是对手是谷歌、微软、IBM这样强大的对手，我们是需要援军的，需要炮火支援，用《英雄儿女》里面王成的一句话，就是向我开炮，我们这代人好好努力，我相信我们下一代人，小兵汤嘎们就会比上一代的小兵张嘎的生活过的更好。谢谢大家！\n点击阅读原文，观看更多精彩内容"}
{"content2":"计算机视觉入门（转载汇总）\n一，基本知识\n1.1图像和视频，你要知道的概念\n图像\n一张图片包含了：维数、高度、宽度、深度、通道数、颜色格式、数据首地址、结束地址、数据量等等。\n图像深度：存储每个像素所用的位数（bits）\n当一个像素占用的位数越多时，它所能表现的颜色就更多，更丰富。\n举例：一张400*400的8位图，这张图的原始数据量是多少？像素值如果是整型的话，取值范围是多少？\n1，原始数据量计算：400 * 400 * ( 8/8 )=160,000Bytes\n(约为160K)\n2，取值范围：2的8次方，0~255\n图片格式与压缩：常见的图片格式JPEG，PNG，BMP等本质上都是图片的一种压缩编码方式\n举例：JPEG压缩\n1，将原始图像分为8*8的小块，每个block里有64pixels。\n2，将图像中每个8*8的block进行DCT变换（越是复杂的图像，越不容易被压缩）\n3，不同的图像被分割后，每个小块的复杂度不一样，所以最终的压缩结果也不一样\n视频\n原始视频=图片序列。\n视频中的每张有序图片称为“帧（frame）”。压缩后的视频，会采取各种算法减少数据的容量，其中IPB就是最常见的。\nI帧：表示关键帧，可以理解为这一幅画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）\nP帧：表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧画面差别的数据）\nB帧：表示双向差别帧，记录的本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，要通过前后画面与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码比较麻烦。\n码率：码率越大，体积越大；码率越小，体积越小。\n码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。也就是取样率（并不等同于采样率，采样率用的单位是Hz，表示每秒采样的次数），单位时间内取样率越大，精度就越高，处理出来的文件就越接近原始文件，但是文件体积与取样率是成正比的，所以几乎所有的编码格式重视的都是如何用最低的码率达到最少的失真，围绕这个核心衍生出来cbr（固定码率）与vbr（可变码率），码率越高越清晰，反之则画面粗糙而且多马赛克。\n帧率：影响画面流畅度，与画面流畅度成正比：帧率越大，画面越流畅；帧率越小，画面越有跳动感。如果码率为变量，则帧率也会影响体积，帧率越高，每秒钟经过的画面就越多，需要的码率也越高，体积也越大。\n帧率就是在一秒钟时间里传输的图片的帧数，也可以理解为图形处理器每秒钟刷新的次数。\n分辨率：影响图像大小，与图像大小成正比；分辨率越高，图像越大；分辨率越低，图像越小。\n清晰度：在码率一定的情况下，分辨率与清晰度成反比关系：分辨率越高，图像越不清晰，分辨率越低，图像越清晰\n在分辨率一定的情况下，码率与清晰度成正比关系：码率越高，图像越清晰；码率越低，图像越不清晰\n带宽、帧率：例如在ADSL线路上传输图像，上行带宽只有512Kbps，但要传输4路CIF分辨率的图像。按照常规，CIF分辨率建议码率是512Kbps，那么照此计算就只能传一路，降低码率势必会影响图像质量。那么为了确保图像质量，就必须降低帧率，这样一来，即便降低码率也不会影响图像质量，但在图像的连贯性上会有影响。：\n1.2摄像机\n摄像机的分类：\n监控摄像机（网络摄像机和摸你摄像机）\n不同行业需求的摄像机（超宽动态摄像机、红外摄像机、热成像摄像机等）\n智能摄像机\n工业摄像机\n当前的摄像机硬件我们可以分为监控摄像机、专业行业应用的摄像机、智能摄像机和工业摄像机。而在监控摄像机里面，当前用的比较多的两个类型一个叫做网络摄像机，一个叫做模拟摄相机，他们主要是成像的原理不太一样。\n网络摄像机一般比传统模拟摄相机的清晰度要高一些，模拟摄像机当前应该说是慢慢处于一个淘汰的状态，它可以理解为是上一代的监控摄像机，而网络摄像机是当前的一个主流的摄相机，大概在 13 年的时候，可能市场上 70% 到 80% 多都是模拟摄像机，而现在可能 60% 到 70% 都是的网络摄像机。\n除此之外，不同的行业其时会有特定的相机，想超宽动态摄像机以及红外摄像机、热成像摄像机，都是在专用的特定的领域里面可能用到的，而且他获得的画面跟图像是完全不一样的。如果我们要做图像处理跟计算机视觉分析，什么样的相机对你更有利，我们要学会利用硬件的优势。\n如果是做研究的话一般是可以控制我们用什么样的摄相机，但如果是在实际的应用场景，这个把控的可能性会稍微小一点，但是在这里你要知道，有些问题可能你换一种硬件，它就能够很好的被解决，这是一个思路。\n还有些问题你可能用算法弄了很久也没能解决，甚至是你的效率非常差，成本非常高，但是稍稍换一换硬件，你会发现原来的问题都不存在了，都被很好的解决了，这个就是硬件对你的一个新的处境了。\n包括现在还有智能摄像机、工业摄像机，工业摄像机一般的价格也会比较贵，因为他专用于各种工业领域，或者是做一些精密仪器，高精度高清晰度要求的摄像机。\n1.3 CPU和GPU\n接下来给大家讲一下 CPU 跟 GPU，如果说你要做计算机视觉跟图像处理，那么肯定跳不过 GPU 运算，GPU 运算这一块可能也是接下来需要学习或者自学的一个知识点。\n因为可以看到，当前大部分关于计算机视觉的论文，很多实现起来都是用 GPU 去实现的，但是在应用领域，因为 GPU 的价格比较昂贵，所以 CPU 的应用场景相对来说还是占大部分。\n而 CPU 跟 GPU 的差别主要在哪里呢？ 它们的差别主要可以在两个方面去对比，第一个叫性能，第二个叫做吞吐量。\n性能，换言之，性能会换成另外一个单词叫做 Latency（低延时性）。低延时性就是当你的性能越好，你处理分析的效率越高，相当于你的延时性就越低，这个是性能。另外一个叫做吞吐量，吞吐量的意思就是你同时能够处理的数据量。\n而 CPU 跟 GPU 的差别在哪里呢？主要就在于这两个地方，CPU 它是一个高性能，就是超低延时性的，他能够快速的去做复杂运算，并且能达到一个很好的性能要求。而 GPU是以一个叫做运算单元为格式的，所以他的优点不在于低延时性，因为他确实不善于做复杂运算，他每一个处理器都非常的小，相对来说会很弱，但是它可以让它所有的弱处理器，同时去做处理，那相当于他就能够同时处理大量的数据，那这个就意味着它的吞吐量非常大，所以 CPU重视的是性能，GPU重视的是吞吐量。\n所以大部分时候，GPU 他会跟另外一个词语联系在一起，叫做并行计算，意思就是它可以同时做大量的线程运算，为什么图像会特别适合用 GPU 运算呢？这是因为 GPU 它最开始的设计就是叫做图形处理单元，它的意思就是我可以把每一个像素，分割为一个线程去运算，每一个像素只做一些简单的运算，这个就是最开始图形处理器出现的原理。\n它要做图形渲染的时候，要计算的是每一个像素的变换。所以每一个像素变换的计算量是很小很小的，可能就是一个公式的计算，计算量很少，它可以放在一个简单的计算单元里面去做计算，那这个就是 CPU 跟 GPU 的差别。\n基于这样的差别，我们才会去设计什么时候用 CPU，什么时候用 GPU。如果你当前设计的算法，它的并行能力不是很强，从头到尾从上到下都是一个复杂的计算，没有太多可并性的地方，那么即使你用了 GPU，也不能帮助你很好提升计算性能。\n所以，不要说别人都在用 GPU 那你就用 GPU，我们要了解的是为什么要用 GPU ，以及什么样的情况下用 GPU，它效果能够发挥出来最好。\n二，计算机视觉介绍\n计算机视觉包括两个主要研究维度：语义感知(Semantic)、几何属性(Geometry)\n2.1 计算机视觉的基础\n数字图像处理\n空域分析及变换（Sobel，拉普拉斯，高斯，中值等）\n频域分析及变换（Fourier & Wavelet Transform）\n模板匹配，金字塔，滤波器组\n特征数据操作：主成分分析/PCA，奇异值分解/SVD，聚类/Cluster\n图像特征及描述\n颜色特征（RBG，HSV，Lab等）\n几何特征（Edge，Corner，Blob等）\n纹理特征（HOG，LBP，Gabor等）\n局部特征（SIFT，SURF，FAST等）\n2.2 计算机视觉问题解决方案\n方式\n特征提取\n决策模型\n传统方式\nSIFT，HOG， Raw Pixel …\nSVM， Random Forest， Linear Regression …\n深度学习\nCNN …\nCNN …\n传统的计算机视觉对待问题的解决方案基本上都是遵循： 图像预处理 → 提取特征 → 建立模型（分类器/回归器） → 输出 的流程。 而在深度学习中，大多问题都会采用端到端（End to End）的解决思路，即从输入到输出一气呵成。\n2.2.1 深度监督学习在计算机视觉中的应用\n图像分类(Image Classification)\n卷积神经网络CNN\n对应有没有问题？，有的话，给出属于某类概率的多少？\n图像检测(Image Detection)\n区域卷积神经网络R-CNN,Fast R-CNN,Faster R-CNN,YOLO,SSD等知名框架\n对应目标在哪儿问题？，用矩形框框出目标\n图像分割(Image Segmentation)\n全卷积神经网络FCN\n对应每个像素的类别问题？，用不同颜色画出图像中所有类别的区域轮廓\n图像识别(Image Identification)\n人脸识别、车牌识别、字符识别、行为识别等\n对应内容是什么问题？\n注意它和Image Verification的区别？\n图像描述(Image Captioning)\n迭代神经网络(Vanilla-RNN，LSTM，GRU)\n图像问答(Image Question Answering)\n迭代神经网络RNN\n2.2.2 强化学习\n在监督学习任务中，我们都是给定样本一个固定标签，然后去训练模型，可是，在真实环境中，我们很难给出所有样本的标签，这时候，强化学习就派上了用场。简单来说，我们给定一些奖励或惩罚，强化学习就是让模型自己去试错，模型自己去优化怎么才能得到更多的分数。2016年大火的AlphaGo就是利用了强化学习去训练，它在不断的自我试错和博弈中掌握了最优的策略。\n2.2.3 深度无监督学习（Deep Unsupervised Learning）–预测学习\n图片生成\nGAN之后相继出现了条件生成对抗网络(Conditional Generative Adversarial Nets)和信息生成对抗网络(InfoGAN),深度卷积生成对抗网络(Deep Convolutional Generative Adversarial Network, DCGAN),\n当前生成对抗网络把触角伸到了视频预测领域，众所周知，人类主要是靠视频序列来理解自然界的，图片只占非常小的一部分，当人工智能学会理解视频后，它也真正开始显现出威力了。\n三，学习资料\n3.0 数学\n微积分：比如图像找边缘即求微分在数字图像里是做差分（离散化）啦，光流算法里用到泰勒级数啦，空间域转频域的傅立叶变换啦，还有牛顿法、梯度下降、最小二乘等等这些都用的特别普遍了。\n概率论与统计：这个比较高深，是应用在机器学习领域里最重要的数序分支。应用比如：条件概率、相关系数、最大似然、大数定律、马尔可夫链等等。 浙大的《概率论与数理统计》感觉还行，够用。\n线性代数与矩阵：数字图像本身就是以矩阵的形式呈现的，多个向量组成的样本也是矩阵这种形式非常常见，大多机器学习算法里每个样本都是以向量的形式存在的，多个矩阵叠加则是以张量(tensor)的形式存在google深度学习库tensorflow的字面意思之一。具体应用，比如：世界坐标系->相机坐标系->图像坐标系之间的转换，特征值、特征向量，范数等。 推荐国外的上课教材《线性代数》。因为浙大的那本教材感觉实在不太行，买过之后还是又买了这本。\n凸优化：这个需要单独拎出来说一下。因为太多问题（尤其机器学习领域）都是优化问题（求最优），凸优化是里面最简单的形式，所以大家都在想办法怎么把一般的优化问题转化为凸优化问题。至于单纯的凸优化理论，好像已经比较成熟了。在机器学习里，经常会看到什么求对偶问题、KKT条件等，潜下心花两天学一学。 建议备一份高校关于凸优化的教学课件，大家对这一块毕竟比较生，缺乏系统感。比如北大的《凸优化》课程。这些数学知识没必要系统学习，效率低又耗时。毕竟大家都有本科的基础，够了。一般用到的时候学，学完之后总结一下。\n3.1 计算机基础系列\n《深入理解计算机系统》 对计算机编程最底层的东西有了更深入的认识\n《鸟哥的Linux私房菜》基础篇 包括指令与Linux系统深入理解 机器学习与深度学习等相关项目大多都在Linux系统下进行\n3.2 编程语言系列\nC++：\n《C++Primer》作者：Stanley Lippman, Josée Lajoie, and Barbara E. Moo (更新到C++11) （不要和 C++ Primer Plus–Stephen Prata搞混了）近1千页，本书透彻的介绍了C++，以浅显和详细的方式讲到C++语言差不多所有内容。2012年8月发行的第五版包含C++11的内容\n电子书：第三版（中文）、第四版（中文）、第五版（英文版）\n《EffectiveC++》作者：Scott Meyers 本书以瞄准成为C++程序员必读的第二本书籍而写，Scott Meyers成功了。早期的版本面向从C语言转过来的程序员。第三版修改为面向从类似Jave等语言转来的程序员。内容覆盖了50多个很容易记住的条款，每个条款深入浅出（并且有趣）讲到了你可能没有考虑过的C++规则。\n电子书：第三版（英文）、第三版（中文)\n数据结构的话推荐B站上浙大陈姥姥将的数据结构，算法的话推荐B站上小象学院林沐讲的面试算法LeetCode刷题班的课程\n3.3 视觉知识\n计算机视觉实在很广了，目前比较热门的方向总体上分为两大块：一块是深度学习，一块做SLAM。它们的研究点区别在哪呢？深度学习这一群体侧重于解决识别感知（是什么）问题，SLAM侧重于解决几何测量（在哪里）问题ICCV研讨会：实时SLAM的未来以及深度学习与SLAM的比较。拿机器人来说，如果你想要它走到你的冰箱面前而不撞到墙壁，那就需要使用 SLAM；如果你想要它能识别并拿起冰箱中的物品，那就需要用到深度学习机器人抓取时怎么定位的？用什么传感器来检测？。当然这两方面在research上也有互相交叉融合的趋势。 不过在学习这些之前，一般都会先掌握下传统的计算机视觉知识，也就是图像处理这一部分了。我之前大致总结过一次：\n计算机视觉初级部分知识体系。这些基础知识的理解还是挺有必要的，有助于你理解更高层知识的本质，比如为什么会出现deeplearning等这些新的理论知识（感觉有点像读史了，给你智慧和自由）。这一部分学习资料的话还是挺推荐浅墨的《OpenCV3编程入门》 也可以看他的博客。当然他的书有一个问题就是涉及理论知识太少，所以推荐自己再另备一本偏理论一点的图像处理相关的书，我手边放的是《数字图像处理：原理与实践》，差强人意吧。\n参考书\n第一本叫《Computer Vision：Models, Learning and Inference》written by Simon J.D. prince，这个主要讲的更适合入门级别的，因为这本书里面配套了非常多的代码，Matlab 代码，C 的代码都有，配套了非常多的学习代码，以及参考资料、文献，都配得非常详细，所以它很适合入门级别的同学去看。\n第二本《Computer Vision：Algorithms and Applications》written by Richard Szeliski，这是一本非常经典，非常权威的参考资料，这本书不是用来看的，是用来查的，类似于一本工具书，它是涵盖面最广的一本参考书籍，所以一般会可以当成工具书去看，去查阅。\n第三本《OpenCV3编程入门》作者：毛星云，冷雪飞 ，如果想快速的上手去实现一些项目，可以看看这本书，它可以教你动手实现一些例子，并且学习到 OpenCV 最经典、最广泛的计算机视觉开源库。\n公开课：\nStanford CS223B\n比较适合基础，适合刚刚入门的同学，跟深度学习的结合相对来说会少一点，不会整门课讲深度学习，而是主要讲计算机视觉，方方面面都会讲到。\nStanford CS231N\n这个应该不用介绍了，一般很多人都知道，这个是计算机视觉和深度学习结合的一门课，我们上 YouTube 就能够看到，这门课的授课老师就是李飞飞老师，如果说不知道的话可以查一下，做计算机视觉的话，此人算是业界和学术界的“执牛耳”了。\n3.4 机器学习\n计算机视觉中使用的机器学习方法个人感觉不算多，早期的时候会用SVM做分类，现在基本都用深度学习选特征+分类。原因在于统计机器学习这一块虽然方法不少，但是基本都无法应对图像这么大的数据量。 不过大家在学习过程中很容易接触到各种机器学习方法的名字因为现在大数据分析、机器学习、语音识别、计算机视觉等这些其实分得不是很开，然后不自觉地就会去了解和学习。这样我感觉总体来说是好的。不过在学习一些暂时用不着的算法时，个人感觉没必要做的太深：重在理解其思想，抓住问题本质，了解其应用方向。\n下面分开介绍一下传统机器学习算法和深度神经网络。\n传统机器学习一般也就决策树、神经网络、支持向量机、boosting、贝叶斯网等等吧。方法挺多的，同一类方法不同的变形更多。除了这些监督式学习，还有非监督学习、半监督学习、强化学习。当然还有一些降维算法（如PCA）等。对这些个人整体把握的也不是特别好，太多了。\n学习资料，吴恩达的coursera课程《Machine Learning》，还有李航的《统计学习方法》和周志华的《机器学习》，两本在国内机器学习界成为经典的书。\ndeeplearning，漫天飞的各种资源。可以看一看李宏毅的一天搞懂深度学习课件 youtube上有一个一天搞懂深度學習–學習心得；李飞飞的CS231n课程，网易云课堂有大数据文摘翻译的中文字幕版课程，知乎专栏智能单元有CS231N课程翻译（非常好）；三巨头之一Yoshua Bengio的新作《DEEP LEARNING》，目前已有中译版本 （《Deep Learning》Written by lan Goodfellow and YoshuaBengio ）。\n3.5 文献\narxiv链接 ：每天去更新一下别人最新的工作\n计算视觉的顶会\nICCV：国际计算机视觉大会\nCVPR：国际计算机视觉与模式识别大会\nECCV：欧洲计算机视觉大会\n里程碑式的文献\n先熟悉所在方向的发展历程，历程中的里程碑式的文献必须要精读。\n例如，深度学习做目标检测，RCNN、Fast RCNN、Fater RCNN、SPPNET、SSD和YOLO等模型；又例如，深度学习做目标跟踪，DLT、SO-DLT等等；再例如，对抗网络GAN、CGAN、DCGAN、LAPGAN等等。\n文献汇总\nCV Papers"}
{"content2":"有没有这样的经历，也许你保存了某篇文章里的一张图片，忽然有一天你想要重读这篇文章，结果你又不能用手上的图片反向检索文章。事实上直到最近1、2年，很多搜索引擎采用的依然是文字驱动技术，而不是通过图像搜索。换言之，用户能不能检索出自己想要的东西，取决于这个东西在搜索引擎内部是否被分类标记，或者是否有准确的文字描述。\n\\\\\n最近一两年是人工智能的启动元年,人工智能已经推动计算机视觉技术达到了一个新的高度。在这个高度，业界的目标是在像素级理解图像，而不是之前的需要文字描述、分类等方式协助。这种进步帮助我们的系统认识图像里面是什么内容，图像里面是什么场景，例如是不是一个有名的旅游胜地。反过来看，这种技术可以更好地为视障人士提供帮助，帮助他们更好地使用搜索引擎搜索图像和视频。通过Joaquin Quiñonero Candela的文章《Building scalable systems to understand content》，我们可以大概了解一下。\n\\\\\n构建人工智能工厂\n\\为了让AI（人工智能）更容易地进入Facebook的软件大家庭，Facebook管理层认为需要引入一个通用的平台，这个平台需要可以支持工程师自己控制集群规模，最终引入的平台叫做FBLearner Flow。有了FBLearner Flow平台之后，Facebook的工程师在运行、调试机器学习程序的时候不用再担心集群负载海量实时任务的业务负担了。Facebook目前在FBLearner Flow平台上平均每个月需要运行120万个AI任务，这个数字是1年以前的6倍。\n\\\\\nFacebook还在不断地加强FBLearner Flow平台，包括提供自动化处理机器学习的工具、构建专用内容理解引擎等。此外，Facebook也允许工程师通过编写多机运行的训练代码方式构建整个生态环境，这样的运行环境和代码可以被全公司的工程师复用。\n\\\\\n\\\\\n计算机视觉平台\n\\FBLearner Flow平台最初起源于Facebook人工智能研究的一个小型研究项目，当FBLearner Flow平台达到了生产环境要求的时候，FBLearner Flow平台和整个小组成员已经转换为应用机器学习团队，并作为FaceBook计算机视觉团队的分析引擎。\n\\\\\nLumos构建于FBLearner Flow平台之上，它是专用于图像和视频的学习平台。Lumos很容易使用，如果一个Facebook工程师想要使用Lumos训练和部署一个新的模型，他不需要接受针对深度学习或者计算机视觉技术的培训。Lumos平台一直都在不断提升，无论是通过标定数据，还是通过Facebook各种应用程序提供的注释数据。\n\\\\\n深度学习的进步让Facebook在图像分类领域有了重大提升，比如“图像中有什么？”和“目标在哪里？”这样的问题可以通过系统回答，而且精度较之前提高了很多。Facebook通过在给定图像上检测和分割对象的设计方案加强搜索准确度。\n\\\\\n让我们想象一下整个流程。这些技术被应用在Facebook，图像可以穿过深度学习引擎，穿越过程中分割图像并且识别对象和场景，进而可以让图像本身附带更多的意义。这个过程为Facebook的产品或者服务提供大量可以使用的数据。Facebook的几十个小组已经在Lumos上通过训练和部署了超过200个视觉模型，针对例如不良内容检测、反垃圾邮件、自动图像字母等等目的。从Facebook的连通性实验室到无障碍团队都在使用这些应用程序。\n\\\\\n\\\\\n提升内容\n\\Facebook目前正在研究将图像识别技术应用到提升自动高亮图像文字描述（AAT），这项技术可以帮助视觉障碍人士理解图像内容。\n\\\\\n之前的一段时间，Facebook技术只能描述图像当中的对象。2017年2月2日，Facebook宣布增加了一组12个动作，图像描述对应可以增加类似于“走路的人”、“跳舞的人”、“骑马的人”，以及“弹奏乐器的人”等等。\n\\\\\nAAT的这次更新在两个部分执行，通过Lumos实现快速、可扩展的迭代更新。由于Facebook应用里面有相当多的图像包含了人类，所以Facebook着重提供针对人的自动描述。人工智能团队从Facebook应用上搜索了13万张包含人的图片（用户完全公开的图片）。标定人员需要假定自己在像视觉障碍人士解释图片的内容，通过一句话方式描述图片。然后Facebook利用这些注释建立一个机器学习模型，支持推断出图片中人的动作，进而用于AAT功能。\n\\\\\n\\\\\nLumos允许这个任务快速迭代，让另一个任务通过接口使用前一次训练模型的标记。例如，如果我们正在训练“骑马的人”分类，想要去增加包括马的图片（没人骑的马），开发人员可以使用模型的一部分标定示例去学习是否图片里面有一匹马。\n\\\\\n\\\\\nLumos允许通过检索和聚类相结合的方式生成训练数据。给定一组标签或搜索术语，Lumos平台可以检索互联网上的图像的一小部分，找到匹配标签的图像。这些图像通过语义集合方式更快地分类标签，Lumos用户选择按照用户用例对样本进行负面或者正面的注解分类，无论是在一个大规模数据集级别或者是针对数据集内部的每一张图片，都可以这样注释。这种做法可以帮助一个有初始数据集的分类任务进一步扩散，通过迭代方式训练并获得更高的精确度/召回分类。\n\\\\\nAAT应用对于Facebook来说至关重要，因为它解决了Facebook视觉障碍用户的使用问题，业界也有其他的应用程序尝试提供类似的方式解决问题，但是更多地把精力放在了搜索参数多样性上面，而不是人工智能领域。\n\\\\\n深入理解\n\\Facebook宣称他们的搜索系统可以通过大量的信息以及相关图像，利用图像的理解进行快速排序。换句话说，比如我们正在搜索“黑色衬衫照片”，Facebook系统可以自我确定是否图像中存在黑色衬衫，即便图像上没有标签信息。这些工作是通过图像分类器完成的。\n\\\\\n为了理解图像中的内容，Facebook团队使用了尖端的深度学习技术，使用该技术学习几十亿的图像数据，弄清楚这些图像的原始语义。\n\\\\\n团队为了更好地对图像进行分类，使用以下几点技术：\n\\\\\n对象识别：图片识别模型的底层是一个带有数以百万计学习参数的深度神经网络。它构建于最先进的深度残差网络之上，使用数千万带有标签的图像训练对象识别。它可以自动预测一系列丰富的语义，包括场景（例如花园）、对象（例如汽车）、动物（例如企鹅）、地点和景点（例如金门大桥），以及穿戴内容（例如围巾）。\\\\t\n图像嵌入：这个特性也会生成高层次的语义特征，它是深度神经网络最后几层的输出的量化版本。这些丰富的信息对于提炼图像搜索结果很有用。\\\n\\\\\n原始语义特征是高维浮点数向量，特别是当工程师不得不索引很多图片的时候，它使用了大量的存储索引。通过利用量化技术，这些特征被进一步压缩成几个Bit，这种情况下依然可以保留大部分的语义。压缩成Bit级别的图片可以被直接用来作为搜索引擎排名、检索。\n\\\\\n实现这种目标的一种方式是从图像中提取出预测的概念和类别，然后解析搜索查询并连接到实体和提取物，进而在两组概念之间使用相似性函数确定相关性。\n\\\\\n对于Facebook团队来说这是一个好的开端，但是Facebook并没有止步于预测图像分类，他们进一步使用了查询和图像联合嵌入方式，动态提升准确率和查全率。\n\\\\\nFacebook采用这种方式从多方面解决排名问题。此外，Facebook也在图像之间使用了一种相似性测量办法，确保图像搜索结果多样性。\n\\\\\n\\\\\n后续计划\n\\Facebook团队确实做了大量的工作，将FBLearner Flow平台纳入到了生产环境使用。Facebook团队表态还有很长的路要走，目前仅仅触及了自服务式计算机视觉平台的皮毛。未来将会利用FBLearner Flow平台建立训练更多的模型，用于Facebook各个产品领域。\n\\\\\n相关知识\n\\\\\n\\\\t\n机器学习与数据挖掘的区别\n\\\\t不管是数据挖掘，还是机器学习，都是以统计学为基础。统计学着重于数据的收集、组织、分析和解释，可以分为描述性统计学和推断统计学。描述性统计学注重的是对数据整理与分析，得出数据的分布状态、数字特征和随即变量之间的关系进行估计和描述，可以细分为数据的集中、离散和相关性分析。推断统计学侧重于通过样本数据推断总体。数据挖掘在通过算法得到的结果上，采用描述性统计学解释问题。而机器学习则是侧重于通过推断统计学来实现自学习。\n\\\\t\\\\t\n\\\\t\nDeep Neural NetWork（深度神经网络）\n\\\\t2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层（参考论文：Hinton G E,Salakhutdinov R R. Reducing the Dimensionality of Data with Neural NetWorks[J].Science,2006），神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义，在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了sigmoid，形成了如今DNN的基本形式。单从结构上来说，全连接的DNN和多层感知机是没有任何区别的。值得一提的是，高速公路网络（Hignway Network）和深度残差学习（Deep Residual Learning）进一步避免了梯度弥散问题，网络成熟达到了前所未有的一百多层。\n\\\\t\\\\t\n\\\\t\nDeep Residual Network（深度残差网络）\n\\\\t它的主要思想是在标准的前馈卷积网络上，加一个跳跃绕过一些层的连接。每绕过一层就产生一个残差块（Residual Block），卷基层预测加输入张量的残差。\n\\\\t\\\n感谢刘志勇对本文的审校。\n\\\\\n给InfoQ中文站投稿或者参与内容翻译工作，请邮件至editors@cn.infoq.com。也欢迎大家通过新浪微博（@InfoQ，@丁晓昀），微信（微信号：InfoQChina）关注我们。"}
{"content2":"第二届人工智能竞赛——题目七、花卉分类\n文章目录\n第二届人工智能竞赛——题目七、花卉分类\n一、简介\n二、题目要求\n三、参考资料\n一、简介\n生活中我们会看到很多花，但对于不熟知花卉品种的人来说，就想要有工具能够帮他识别花的种类，，你可以采用你所熟知或当前流行的方法去实现一个花卉分类器，通过输入一张花卉的图片，得出该品种花卉的基本信息。\n二、题目要求\n1.通过CNN数据集进行花卉分类，得到训练模型，用训练模型进行花卉类别预测。\n2.利用 QT 设计 UI 界面，界面要求:友好、功能完善、简介。\n3.评估模型效果，准确率\n三、参考资料\nhttps://blog.csdn.net/qq_25987491/article/details/80889446\nUI界面可用QT编写"}
{"content2":"attention—OCR\n首先同样学习一门技术的同时学习背后的历史，计算机是非常讲究实现性科学。所以我们必须研究历史来完善技术的演化便于我们理解和仍然存在的技术问题进行论证。\n最早成大脑成像机制winner-take-all的建模方法来实现这个过程，那么怎么可以使得计算机拥有自主学习注意力的机制那么我们就要研究神经网络的图像的机制，我个人非常讨厌现在很多书没有解释说明数字图像，虽然很多书都有对图像概念性描述，但是图像的表示，图像的知识表示我只在一本比较旧的书看到过，很多书没有对位图、矢量图、索引图相关技术细节做深入研究知识，如果工程基础好这些在项目中可以反推，但是我估计很多书的作者都不会这么干。\n软注意力问题，通过滤波器的通道关注区域，这个在感受视野和RPN及ROI都有很好体现，这种网络生产的关键区域书可以微分的，通过微分的注意力可以传个神经网络技术梯度并且反省传播利用范数的方案指导这种反馈来学习注意力权重。\n强注意力问题是关注于焦点或角点来衍射一个区域更加注重变换和动态性和随机的预测过程，强注意力过程不可微分的注意力，因此可以考虑差分办法来解决问题缺陷实现增强，也有人用reinforcement learning方法来完成这项工作。\nhttps://github.com/da03/Attention-OCR\n今天有点累了 ，改天再写"}
{"content2":"1.人工智能的基本概念\n智能的概念\n自然界的四大奥秘之一：智能的发生、物质的本质、宇宙的起源、生命的本质。\n智能研究的三大观点：\n思维理论：认为智能的核心是思维，人的一切智能都来自大脑的思维活动，人类的一切知识都是人类思维的产物。\n知识阈值理论：智能行为取决于知识的数量及其一般化的程度，一个系统之所以有智能是因为它具有可运用的知识。将智能定义为：智能就是在巨大的搜索空间迅速找到一个满意解的能力。\n进化理论：认为人的本质能力是在动态环境中的行走能力、对外界事务的感知能力、维持生命和繁衍生息的能力。而智能是某种复杂系统所表现的性质， 是没有明显的可操作的内部表达的情况下产生的，也可以在没有明显的推理系统出现的情况下产生。用控制取代表示，否定抽象对于智能及智能模拟的必要。\n总结：智能就是知识与智力的总和。其中，知识是一切智能行为的基础，而智力是获取知识并应用知识求解问题的能力。\n智能的特征\n具有感知能力：通过视觉、听觉、触觉、嗅觉等感觉器官感知外部世界的能力。人工智能的机器感知主要研究机器视觉和机器听觉方面。\n具有记忆思维能力：\n1.记忆：用于存储由感知器官感知到的外部信息以及由思维所产生的知识。 2.思维：用于对记忆的信息进行处理，即利用已有的知识对信息进行吻戏、计算、比较、判断、推理、联想及决策。\n思维的分类：\n（1）逻辑思维：根据逻辑规则对信息进行处理的理性思维方式。\n首先获得外部事物的感性认识，存储到大脑。然后通过匹配逻辑规则进行相应的逻辑推理。推理是否成功取决于，推理规则是否完备和已知信息是否完善、\n可靠。\n逻辑思维的特点：\n①依靠逻辑进行思维\n②思维过程是串行的，表现为一个线性过程。\n③容易形式化，其思维过程可以用符号串表达出来。\n④思维过程具有严密性、可靠性，能对事物未来的发展给出逻辑上合理的预测，可使人们对事物的认识不断深化。\n（2）形象思维：一种以客观现象为思维对象、以感性形象认识为思维材料、以意向为主要思维工具、以指导创造物化形象的实践为主要目的的思维活动。\n形象思维的思维过程的两次飞跃：\n·从感性形象认识到理性形象认识的飞跃：把对事物的感觉组合起来，形成反映事物多方面属性的整体性认识（即知觉），再在知觉的基础上形成具有一定\n概括性的感觉反映形式（即表象），然后经形象分析、形象比较、形象概括及组合形成对事物的理性形象认识。\n· 从理性形象认识到实践的飞跃：对理性形象认识进行联想、想象等加工，在大脑中形成新的意向，然后回到实践中，接受实践的检验。\n形象思维的特点：\n①主要是依据直觉，即感性形象进行思维。\n②思维过程是并行协同式的，表现为一个非线性过程。\n③形式化困难，没有统一的形象联系规则，对象不同、场合不同，形象的联系规则亦不相同，不能直接套用。\n④在信息变形或缺少的情况下仍有可能得到比较满意的结果。\n（3）顿悟思维：一种显意识与潜意识互相作用的思维方式。\n顿悟思维的特点：\n①具有不定期的突发性\n②具有非线性独创性及模糊性\n③穿插于形象思维与逻辑思维之中，起着突破、创新及升华的作用。\n在求解问题时，通常将逻辑思维和形象思维结合起来使用，首先使用形象思维给出假设，然后用逻辑思维进行论证。\n3.具有学习能力\n4.具有行为能力：感知能力看做是信息的输入，行为能力可以看做信息的输出，他们都受到神经系统控制\n人工智能\n图灵测试的提出---------衡量机器智能的测试。\n没有涉及思维过程----------“中文屋思想实验”。\n人工智能是一门研究如何构造智能机器或智能系统，使它能模拟、延伸、扩展人类智能的学科。\n2.人工智能发展史\n孕育\n①公元前384-322,演绎推理的基本依据------三段论。\n②培根，系统的提出归纳法。\n③莱布尼茨，万能符号和推理计算的思想，现代机器思维设计思想的萌芽。\n④布尔，思维规律形式化和实现机械化，创立布尔代数。首次用符号语言描述了思维活动的基本推理法则。\n⑤图灵，1936提出理想计算机的数学模型，即图灵机，为电子数字计算机的问世奠定了理论基础。\n⑥麦克洛奇与匹兹,1943建成第一个神经网络模型（M-P模型），开创了微观人工智能的研究领域，为人工神经网络的研究奠定基础。\n⑦阿塔纳索夫和贝瑞1937-1941开发世界上第一台电子计算机“阿塔纳索夫-贝瑞计算机（ABC）”为人工智能的研究奠定了物质基础。\n形成\n1956-1969\n①1956夏：美国达特茅斯大学召开为时两个月的学术研讨会。麦卡锡（人工智能之父）提出“人工智能”这一名词。标志人工智能作为一门新型学科正式诞生\n②机器学习方面：1957年Rosenblatt研制成功了感知机（将神经元用于识别的系统）。\n③定理证明方面：王浩，1958，IBM-704用时3-5min证明了《数学原理》中有关命题演算的全部定理；1965，鲁宾逊提出归结原理，为定理的机器证明作出突出贡献。\n④模式识别方面：1959赛尔夫里奇推出一个模式识别程序；1965年罗伯特编制出了可分辨积木构造的程序。\n⑤专家系统方面：1965-1968，弗根鲍姆领导的小组，根据质谱仪实验，通过分析决定化合物的分子结构的DENDRAL专家系统。对知识表示、存储、获取、推理及利用等技术是一次非常有益的探索。\n⑥人工智能语言：1960麦卡锡研制出人工智能语言LISP，建造专家系统的重要工具。\n⑦1969年成立的国际人工智能联合会议（IJCAI）,标志人工智能得到世界认可。\n⑧1970创刊的国际性人工智能杂志《Aitificial Intelligence》\n发展\n1970-今\n①1972法国马赛大学，科麦瑞尔提出并实现逻辑程序语言PROLOG。\n②1972斯坦福大学的肖特利夫等人研制用于诊断和治疗感染疾病的专家系统MYCIN。\n③1977弗根鲍姆提出“知识工程”,人工智能迎来以知识为中心的新时期。\n④地矿勘探专家系统PROSPECTOR、根据用户要求确定计算机配置的专家系统XCON、信用卡认证辅助决策专家系统American Express\n⑤人工智能在博弈方面：IBM深蓝、Google Alpha Go\n⑥我国1978年把“智能模拟”作为国家科学技术发展规划的主要研究课题；1981年成立了中国人工智能学会CAAI。\n3.人工智能研究的基本内容\n1.知识表示\n(1)符号表示法：用各种包含具体含义的符号，以各种不同的方式和顺序组合起来表示知识的一类方法。主要用来表示路机型知识。目前使用的知识表示法：一阶谓词逻辑表示法、产生式表示法、框架表示法、语义网络表示法、状态空间表示法、神经网络表示法、脚本表示法、过程表示法、Petri网络表示法及面向对象表示法等。\n(2)连接机制表示法：用神经网络表示知识的一种方法。它把各种物理对象以不同的方式及顺序连接起来，并在其间互相传递及加工各种包含具体意义的信息，以此来表示相关的概念及知识。适用于表示各种形象性的知识。\n2.机器感知\n使机器具有类似于人的感知能力，其中以机器视觉与机器听觉为主。对应人工智能两个专门的研究领域，模式识别和自然语言理解。\n3.机器思维\n指对通过感知得来的外部信息及机器内部的各种工作信息进行有目的的处理。是人工智能研究中最重要、最关键的部分。\n4.机器学习\n研究如何使计算机具有类似于人的学习能力，使它能通过学习自动地获取知识。难度较大，与脑科学、神经心理学、计算机视觉、计算机听觉等都有密切联系。\n5.机器行为\n与人的行为能力对应，机器行为主要是指计算机的表达能力。即“说”、“写”、“画”等能力。\n4.人工智能的主要研究领域\n1.自动定理证明\n海博伦与鲁宾逊先后进行卓有效的研究，提出相应的理论及方法，为自动定理证明奠定基础。我国吴文俊院士提出并实现的几何定理机器证明“吴氏方法”,是机器定理证明领域的一项标志性成果。\n2.博弈\n人工智能研究博弈的目的并不是为了让计算机与人进行下棋、打牌之类的游戏，而是通过对博弈的研究来检验某些人工智能技术是否能实现对人类智慧的模拟，促进人工智能的研究\n3.模式识别\n研究对象描述和分类方法的学科。分析和识别的模式可以使信号、图像或者普通数据。\n模式是对一个物体或者某些其他感兴趣实体定量的或者结构的描述，而模式类是指具有某些共同属性的模式集合。用机器进行模式识别的主要内容是研究一种自动技术，依靠这种技术，机器可以自动地或者尽可能少需要人工干预地把模式分配到它们各自的模式类中去。\n传统模式识别方法有统计模式识别和结构模式识别等类型。近年来迅速发展的模糊数学及人工神经网络技术已经应用到模式识别中，形成模糊模式识别、神经网络模式识别等方法，展示了巨大的发展潜力。\n4.机器视觉\n用机器代替人眼测量和判断，是模式识别研究的一个重要方面。机器视觉与模式识别存在很大程度的交叉性，两者的主要区别是机器视觉更注重三维视觉信息的处理，而模式识别仅仅关心模式的类别。此外模式识别还包括听觉等非视觉信息。\n5.自然语言理解\n研究如何让计算机理解人类自然语言，是人工智能中十分重要的一个研究领域。是研究能够实现人与计算机之间用自然语言进行通信的理论与方法。需要达到如下三个目标：\n(1)计算机能正确理解人们用自然语言输入的信息，并能正确回答输入信息中的有关问题。\n(2)对输入的自然原因信息，计算机能够产生相应的摘要，能用不同词语复述输入信息的内容。\n(3)计算机能把用某一种自然语言表示的信息自动翻译为另一种自然语言表示的相同信息。\n6.智能信息检索\n智能信息检索系统应具有下述功能：\n(1)能理解自然语言。允许用户使用自然语言提出检索要求和询问。\n(2)具有推理能力。能根据数据库存储的事实，推理产生用户要求和询问的答案。\n(3)系统具有一定的常识性知识。\n7.数据挖掘与知识发现\n数据挖掘的目的是从数据库中找出有意义的模式。这些模式可以是一组规则、聚类、决策树、依赖网络或以其他方式表示的知识。一个典型的数据挖掘过程可以分成4个阶段，即数据预处理、建模、模型评估及模型应用。\n知识发现系统通过各种学习方法，自动处理数据库中大量的原始数据，提炼出具有必然性的、有意义的知识，从而揭示出蕴含在这些数据后的内在联系和本质规律，实现知识的自动获取。\n8.专家系统\n是一个智能的计算机程序，运用知识和推理步骤来解决只有专家才能解决的疑难问题。\n9.自动化程序设计\n是将自然语言描述的程序自动转换成可执行程序的技术。是人工智能与软件工程相结合的课题。\n10.人工神经网络\n一个用大量简单处理单元经广泛连接而组成的人工网络，用来模拟大脑神经系统的结构和功能。神经网络已经在模式识别、图像处理、组合优化、自动控制、信息处理、机器人学等领域获得日益广泛的应用。\n此外还有机器人、组合优化问题、分布式人工智能与多智能体、智能控制、智能仿真、智能CAD、智能CAI、智能管理与智能决策、智能多媒体系统、智能操作系统、智能计算机系统、智能通信、智能网络系统、人工生命等。\n参考文献：高等教育出版社，人工智能导论（第3版）,王万良 编著。"}
{"content2":"2014-09-03\n我从学校的时候，就开始关注Google Glass这个东西了，那时候，觉得这样的东西实在是太高端了，碉堡了。科幻电影里面的东西成为现实了啊，太惊人了。我要在这个领域有所做为，最先应用的就属游戏了。\n我从13年冬开始研究计算机图形学、数字图像处理，14年初开始研究计算机视觉，有太多的东西看不懂了，发觉是自己的基础不够，又去复习了微积分、线性代数，学习了信号与系统。一路走来，不艰难，也不容易。我学习编译原理的时候，大部分材料是英文的，夹杂了一些中文的书籍、文档，而学习视觉和图像这方面，所用到的教材基本上都是英文版的了，当初在学校立下的“用英文学习”的目标在那时候才算真正意义上的达成了。也买了Raspberry Pi 这样的硬件来玩儿，尝试移动处理器对图形图像的处理能力。我的另外一个个人项目，使用Pi 动态的从纸质书上面提取英文单词，也在进行中。 现在，我在研究各种3D的技术，虽然达不到专业的技术，我的做游戏前端的好友，也多来和我讨论问题。 总算是有了一些成就。我在14年年初找工作的时候，其实在投前后端都做的职位的，说起来有点自大了，所幸也有两家公司给了我offer，我去了一家。\n这些东西都是在业余时间做的，同时，我们仍然在继续学习编译原理的知识，搞的我整个13年、14年前半年周末都没有休息，14年初搬到海淀西三旗，和同学合租了一个大房子之后才回复了定期的运动，想想也是蛮拼的。在我看来，高中不努力，大学就要努力；大学不努力，现在就要努力。非常可惜的是，我在高中是不够努力的，为了一些现在看起来非常愚蠢的问题而烦恼，浪费了时间，所以上了一个普通的学校。再次可惜的是，我大学前两年不够努力，学的平常，读书一般，所以，现在的忙碌是合理的，是应该的。\n等到我在这个方面的学习研究稍有成就之后，我会再写文章来回顾整个过程的。\n[ 主页 ]"}
{"content2":"资源目录汇总\n计算机主页汇总：https://blog.csdn.net/Leo_whj/article/details/78281793\n计算机视觉发展历程：http://homepages.inf.ed.ac.uk/rbf/CVonline/\n计算机视觉软件：http://peipa.essex.ac.uk/info/software.html\nCVPR论文：http://www.cvpapers.com/\n计算机视觉研究小组：http://peipa.essex.ac.uk/info/groups.html\n计算机视觉资源：http://www.visionbib.com/index.php\n计算机视觉书目：http://www.visionbib.com/bibliography/contents.html\n计算机视觉会议\nCVPR2016：http://cvpr2016.thecvf.com/\nCVPR2017：http://cvpr2017.thecvf.com/\nCVPR2018：http://cvpr2018.thecvf.com/\nCVPR2019：http://cvpr2019.thecvf.com/\n英国机器视觉会议：http://www.bmva.org/bmvc/?id=bmvc\nICCV2017：http://iccv2017.thecvf.com/\nECCV2016：http://www.eccv2016.org/\nECCV2018：https://eccv2018.org/\nIEEE：https://ieeexplore.ieee.org/Xplore/home.jsp\nICRA：http://www.icra2017.org/\n企业实验室\nGoogle实验室：https://experiments.withgoogle.com/\nGoogle博客：https://ai.google/research\nFacebook博客：https://research.fb.com/blog/\n旷世：https://www.megvii.com/technologies/\n商汤科技：https://www.sensetime.com/\n高校实验室\n斯坦福：http://vision.stanford.edu/publications.html\n普林斯顿：https://vision.in.tum.de/publications\n卡内基梅隆：http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n伯克利：https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/\n加州大学：https://sites.usc.edu/iris-cvlab/\n普度大学：https://engineering.purdue.edu/RVL/Publications.html\n科隆大学：https://arxiv.org/list/cs/recent\n曼切斯特：https://personalpages.manchester.ac.uk/staff/timothy.f.cootes/refs_by_subject.html\n香港中文大学：http://mmlab.ie.cuhk.edu.hk/\n中国科学院大学：https://ai.ucas.ac.cn/index.php/zh-cn/\n魏茨曼科学研究所：http://www.weizmann.ac.il/math/ronen/\n哥伦比亚大学：http://www1.cs.columbia.edu/CAVE/\n麻省理工：https://www.csail.mit.edu/research\n牛津大学：http://www.robots.ox.ac.uk/\n剑桥大学：http://svr-www.eng.cam.ac.uk/Main/CVR"}
{"content2":"1.图像数据处理\n空域分析及变换\nSobel,拉普拉斯,高斯,中值等\n频域分析及变换\n傅里叶( Fourier )变换\n小波( Wavelet )变换\n模板匹配,金字塔,滤波器组\n特征数据操作\n主成分分析/PCA\n奇异值分解/SVD\n聚类/Cluster\n2.图像特征及描述\n颜色特征\nRGB,HSV,Lab等\n直方图\n几何特征\nEdge,Corner,Blob等\n纹理特征\nHOG,LBP,Gabor等\n局部特征\nSIFT,SURF,FAST等\n1.图像数据处理（Image Data Processing）\n1.1.图片存储原理\n• RGB 颜色空间\n加法混色,彩色显示器\n• 3通道\nRed通道\nGreen通道\nBlue通道\n• 一个像素颜色值\n(b, g, r)\n• 取值范围\n[0, 255]\n[0.0, 1.0]\n1.2.空域分析及变换：滤波/卷积\n1.2.1.什么是卷积\n卷积是在每个图片位置(x, y)上进行基于邻域的函数计算。\n其中滤波函数又有以下叫法：卷积核、卷积模板；滤波器、滤波模板；扫描窗\n不同功能需要定义不同的滤波函数，卷积的作用有以下两个：\n1.图像增强：平滑/去燥；梯度/锐化\n2.信息提取、检测：边缘、显著点、纹理；模式\n卷积的边界补充有以下类型：\n1.2.2.按作用分类卷积的类型\n1.平滑/去噪\n1.中值滤波\n2.均值滤波\n3.高斯滤波\n2.梯度/锐化\n\n1.3.频域分析及变换\n\n1.4.金字塔\n\n1.4.模板匹配\n模板图片匹配 vs 卷积：\n1.作用:同尺度目标检测\n2.模板:真实图片 vs 卷积核\n3.操作:使用模板图片扫描整个图片 vs 卷积扫描\n4.匹配结果:相似度量 vs 权重相加\n返回相似度图\n相似距离计算：\n(标准化)欧式距离\n(标准化)相关\n(标准化)去均值相关\n1.5.代码实践\n•平滑滤波\nhttp://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html\n•梯度滤波\nhttp://docs.opencv.org/master/d5/d0f/tutorial_py_gradients.html\n•傅里叶变换\nhttp://docs.opencv.org/master/de/dbc/tutorial_py_fourier_transform.html\n•模板匹配\nhttp://docs.opencv.org/master/d4/dc6/tutorial_py_template_matching.html\n•金字塔\nhttp://docs.opencv.org/master/dc/dff/tutorial_py_pyramids.html\n2.图像特征及描述(Image Feature & Descriptor)\n2.1.特征提取方法(直方图,聚类)\n2.1.1.直方图\n定义：颜色直方图是在许多图像检索系统中被广泛采用的颜色特征。它所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置，即无法描述图像中的对象或物体。颜色直方图特别适于描述那些难以进行自动分割的图像。\n直方图：用于计算图片特征(Feature)、表达(representation)\n分类：对图片数据/特征分布的一种统计：\n灰度、颜色\n梯度/边缘、形状、纹理\n局部特征点、视觉词汇\n区间(bin)：\n具有一定的统计或物理意义\n一种数据或特征的代表\n需要预定义或基于数据进行学习\n数值是一种统计量:概率、频数、特定积累\n维度小于原始数据\n对数据空间(bin)进行量化方法：\n人工分割\n聚类算法进行无监督学习\n2.1.2.聚类(Clustering)\n目标:找出混合样本集中内在的组群关系，把一个对象集合分组或分割为子集或类,使得：\n• 类内对象之间的相关性高\n• 类间对象之间的相关性差\n常用方法：\nKmeans (只覆盖)\nEM算法\nMean shift\n谱聚类\n层次聚类\n2.2.颜色特征(RGB,HSV,Lab)\n2.3.几何特征(Edge,Corner,Blob)\n2.4.纹理特征(HOG,LBP,Gabor)\n2.5.局部特征(SIFT,SURF)\n2.6.代码实践"}
{"content2":"本文转载文章，原文http://www.sohu.com/a/206707295_465975，备份在此，为了将来方便查阅，如有不妥，请联系我删除，谢谢原创分享\n选自The M tank 参与：蒋思源、刘晓坤\nThe M Tank 编辑了一份报告《A Year in Computer Vision》，记录了 2016 至 2017 年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。该材料共包括四大部分，在本文中机器之心对第一部分做了编译介绍，后续会放出其他部分内容。\n内容目录\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率、风格迁移、着色\n动作识别\n第三部分\n3D 目标\n人体姿势估计\n3D 重建\n其他未分类 3D\n总结\n第四部分\n卷积架构\n数据集\n不可分类的其他材料与有趣趋势\n结论\n完整 PDF 地址：http://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf\n简介\n计算机视觉是关于研究机器视觉能力的学科，或者说是使机器能对环境和其中的刺激进行可视化分析的学科。机器视觉通常涉及对图像或视频的评估，英国机器视觉协会（BMVA）将机器视觉定义为「对单张图像或一系列图像的有用信息进行自动提取、分析和理解」。\n对我们环境的真正理解不是仅通过视觉表征就可以达成的。更准确地说，是视觉线索通过视觉神经传输到主视觉皮层，然后由大脑以高度特征化的形式进行分析的过程。从这种感觉信息中提取解释几乎包含了我们所有的自然演化和主体经验，即进化如何令我们生存下来，以及我们如何在一生中对世界进行学习和理解。\n从这方面来说，视觉过程仅仅是传输图像并进行解释的过程，然而从计算的角度看，图像其实更接近思想或认知，涉及大脑的大量功能。因此，由于跨领域特性很显著，很多人认为计算机视觉是对视觉环境和其中语境的真实理解，并将引领我们实现强人工智能。\n不过，我们目前仍然处于这个领域发展的胚胎期。这篇文章的目的在于阐明 2016 至 2017 年计算机视觉最主要的进步，以及这些进步对实际应用的促进。\n为简单起见，这篇文章将仅限于基本的定义，并会省略很多内容，特别是关于各种卷积神经网络的设计架构等方面。\n这里推荐一些学习资料，其中前两个适用与初学者快速打好基础，后两个可以作为进阶学习：\nAndrej Karpathy:「What a Deep Neural Network thinks about your #selfie」，这是理解 CNN 的应用和设计功能的最好文章 [4]。\nQuora:「what is a convolutional neural network?」，解释清晰明了，尤其适合初学者 [5]。\nCS231n: Convolutional Neural Networks for Visual Recognition，斯坦福大学课程，是进阶学习的绝佳资源 [6]。\nDeep Learning(Goodfellow,Bengio&Courville,2016)，这本书在第 9 章提供了对 CNN 的特征和架构设计等详尽解释，网上有免费资源 [7]。\n对于还想进一步了解神经网络和深度学习的，我们推荐：\nNeural Networks and Deep Learning(Nielsen,2017)，这是一本免费在线书籍，可为读者提供对神经网络和深度学习的复杂性的直观理解。即使只阅读了第 1 章也可以帮助初学者透彻地理解这篇文章。\n下面我们先简介本文的第一部分，这一部分主要叙述了目标分类与定位、目标检测与目标追踪等十分基础与流行的计算机视觉任务。而后机器之心将陆续分享 Benjamin F. Duffy 和 Daniel R. Flynn 后面 3 部分对计算机视觉论述，包括第二部分的语义分割、超分辨率、风格迁移和动作识别，第三部分三维目标识别与重建、和第四部分卷积网络的架构与数据集等内容。\n基础的计算机视觉任务\n分类/定位\n图像分类任务通常是指为整张图像分配特定的标签，如下左图整张图像的标签为 CAT。而定位是指找到识别目标在图像中出现的位置，通常这种位置信息将由对象周围的一些边界框表示出来。目前 ImageNet [9] 上的分类/定位的准确度已经超过了一组训练有素的人类 [10]。因此相对于前一部分的基础，我们会着重介绍后面如语义分割、3D 重建等内容。\n图 1：计算机视觉任务，来源 cs231n 课程资料。\n然而随着目标类别 [11] 的增加，引入大型数据集将为近期的研究进展提供新的度量标准。在这一方面，Keras [12] 创始人 Francois Chollet 将包括 Xception 等架构和新技术应用到谷歌内部的大型数据集中，该数据集包含 1.7 万个目标类别，共计 350M（Million）的多类别图像。\n图 2：ILSVRC 竞赛中，分类/定位的逐年错误率，来源 Jia Deng (2016)，ILSVRC2016。\nImageNet LSVRC（2016）亮点：\n场景分类是指用「温室」、「体育场」和「大教堂」等特定场景对图像进行分类。ImageNet 去年举办了基于 Places2[15] 子数据的场景分类挑战赛，该数据集有 365 个场景共计 8 百万 训练图像。海康威视 [16] 选择了深度类 Inception 的网络和并不太深的 ResNet，并利用它们的集成实现 9% 的 Top-5 误差率以赢得竞赛。\nTrimps-Soushen 以 2.99% 的 Top-5 分类误差率和 7.71% 的定位误差率赢得了 ImageNet 分类任务的胜利。该团队使用了分类模型的集成（即 Inception、Inception-ResNet、ResNet 和宽度残差网络模块 [17] 的平均结果）和基于标注的定位模型 Faster R-CNN [18] 来完成任务。训练数据集有 1000 个类别共计 120 万的图像数据，分割的测试集还包括训练未见过的 10 万张测试图像。\nFacebook 的 ResNeXt 通过使用从原始 ResNet [19] 扩展出来的新架构而实现了 3.03% 的 Top-5 分类误差率。\n目标检测\n目标检测（Object Detection）即如字面所说的检测图像中包含的物体或目标。ILSVRC 2016 [20] 对目标检测的定义为输出单个物体或对象的边界框与标签。这与分类/定位任务不同，目标检测将分类和定位技术应用到一张图像的多个目标而不是一个主要的目标。\n图 3：仅有人脸一个类别的目标检测。图为人脸检测的一个示例，作者表示目标识别的一个问题是小物体检测，检测图中较小的人脸有助于挖掘模型的尺度不变性、图像分辨率和情景推理的能力，来源 Hu and Ramanan (2016, p. 1)[21]。\n目标识别领域在 2016 年主要的趋势之一是转向更快、更高效的检测系统。这一特性在 YOLO、SSD 和 R-FCN 方法上非常显著，它们都倾向于在整张图像上共享计算。因此可以将它们与 Fast/Faster R-CNN 等成本较高的子网络技术区分开开来，这些更快和高效的检测系统通常可以指代「端到端的训练或学习」。\n这种共享计算的基本原理通常是避免将独立的算法聚焦在各自的子问题上，因为这样可以避免训练时长的增加和网络准确度的降低。也就是说这种端到端的适应性网络通常发生在子网络解决方案的初始之后，因此是一种可回溯的优化（retrospective optimisation）。然而，Fast/Faster R-CNN 技术仍然非常有效，仍然广泛用于目标检测任务。\nSSD：Single Shot MultiBox Detector[22] 利用封装了所有必要计算并消除了高成本通信的单一神经网络，以实现了 75.1% mAP 和超过 Faster R-CNN 模型的性能（Liu et al. 2016）。\n我们在 2016 年看到最引人注目的系统是「YOLO9000: Better, Faster, Stronger」[23]，它引入了 YOLOv2 和 YOLO9000 检测系统 [24]。YOLOv2 很大程度上提升了 2015 年提出的 YOLO 模型 [25] 性能，它能以非常高的 FPS（使用原版 GTX Titan X 在低分辨率图像上达到 90FPS）实现更好的结果。除了完成的速度外，系统在特定目标检测数据集上准确度要优于带有 ReNet 和 SSD 的 Faster RCNN。\nYOLO9000 实现了检测和分类的联合训练，并将其预测泛化能力扩展到未知的检测数据上，即它能检测从未见过的目标或物体。YOLO9000 模型提供了 9000 多个类别的实时目标检测，缩小了分类和检测数据集间的鸿沟。该模型其它详细的信息和预训练模型请查看：http://pjreddie.com/darknet/yolo/。\nFeature Pyramid Networks for Object Detection [27] 是 FAIR [28] 实验室提出的，它能利用「深度卷积网络的内部多尺度、金字塔型的层级结构构建具有边际额外成本的特征金字塔」，这意味着表征能更强大和快速。Lin et al. (2016) 在 COCO[29] 数据集上实现了顶尖的单模型结果。若与基础的 Faster R-CNN 相结合，将超过 2016 年最好的结果。\nR-FCN：Object Detection via Region-based Fully Convolutional Networks [30]，这是另一种在图像上避免应用数百次高成本的各区域子网络方法，它通过使基于区域的检测器在整张图像上进行全卷积和共享计算。「我们每张图像的测试时间只需要 170ms，要比 Faster R-CNN 快 2.5 到 20 倍」(Dai et al., 2016)。\n图 4：目标检测中的准确率权衡，来源 Huang et al. (2016, p. 9)[31]。\n注意：Y 轴表示的是平均准确率（mAP），X 轴表示不同元架构（meta-architecture）的各种特征提取器（VGG、MobileNet...Inception ResNet V2）。此外，mAP small、medium 和 large 分别表示对小型、中型和大型目标的检测平均准确率。即准确率是按「目标尺寸、元架构和特征提取器」进行分层的，并且图像的分辨率固定为 300。虽然 Faster R-CNN 在上述样本中表现得更好，但是这并没有什么价值，因为该元架构相比 R-FCN 来说慢得多。\nHuang et al. (2016)[32] 的论文提供了 R-FCN、SSD 和 Faster R-CNN 的深度性能对比。由于机器学习准确率对比中存在的问题，这里使用的是一种标准化的方法。这些架构被视为元架构，因为它们可以组合不同的特征提取器，比如 ResNet 或 Inception。\n论文的作者通过改变元架构、特征提取器和图像分辨率研究准确率和速度之间的权衡。例如，对不同特征提取器的选择可以造成元架构对比的非常大的变化。\n实时商业应用中需要低功耗和高效同时能保持准确率的目标检测方法，尤其是自动驾驶应用，SqueezeDet[33] 和 PVANet[34] 在论文中描述了这种发展趋势。\nCOCO[36] 是另一个常用的图像数据集。然而，它相对于 ImageNet 来说更小，更常被用作备选数据集。ImageNet 聚焦于目标识别，拥有情景理解的更广泛的语境。组织者主办了一场包括目标检测、分割和关键点标注的年度挑战赛。在 ILSVRC[37] 和 COCO[38] 上进行的目标检测挑战赛的结果如下：\nImageNet LSVRC 图像目标检测（DET）：CUImage 66% 平均准确率，在 200 个类别中有 109 个胜出。\nImageNet LSVRC 视频目标检测（VID）：NUIST 80.8% 平均准确率。\nImageNet LSVRC 视频追踪目标检测：CUvideo 55.8% 平均准确率。\nCOCO 2016 目标检测挑战赛（边界框）：G-RMI（谷歌）41.5% 平均准确率（比 2015 的胜者 MSRAVC 高出 4.2% 绝对百分点）。\n从以上结果可以看出，在 ImageNet 上的结果表明「MSRAVC 2015 的结果为『引入 ResNet』设置了很高的标准。在整个项目中对所有的类别的目标检测性能都有所提升。在两个挑战赛中，定位任务的性能都得到较大的提升。关于小型目标实例的大幅性能提升结果详见参考文献」（ImageNet,2016）。[39]\n图 5.ILSVRC 的图像目标检测结果（2013-2016），来源 ImageNet. 2016. [Online] Workshop\n目标追踪\n目标追踪即在给定的场景中追踪感兴趣的一个或多个特定目标的过程，在视频和现实世界的交互中（通常是从追踪初始的目标检测开始的）有很多应用，且对于自动驾驶而言非常重要。\nFully-Convolutional Siamese Networks for Object Tracking[40]，将一个连体网络（Siamese network）结合一个基础的追踪算法，使用端到端的训练方法，达到了当前最佳，图框显示率超过了实时应用的需求。这篇论文利用传统在线学习方法构建追踪模型。\nLearning to Track at 100 FPS with Deep Regression Networks[41]，该论文试图改善在线训练方法中存在的缺陷。他们构建了一个使用前馈网络学习目标运动、外观和方向中的普遍关系的追踪器，从而可以在没有在线训练的情况下有效地追踪到新的目标。该算法在一个标准的追踪基准测试中达到了当前最佳，同时可以 100FPS 的帧数追踪所有的目标（Held et al.,2016）。\nDeep Motion Features for Visual Tracking[43] 结合了手工设计的特征、深度外观特征（利用 CNN）和深度运动特征（在光流图像上训练），并取得了当前最佳的结果。虽然深度运动特征在动作识别和视频分类中很常见，但作者声称这是其首次被应用于视觉追踪上。该论文获得了 ICPR2016 的「计算机视觉和机器人视觉」的最佳论文。\n「本论文展示了深度运动特征（motion features）对检测和追踪框架的影响。我们还进一步说明了手工制作的特征、深度 RGB 和深度运用特征包含互补信息。据我们所知，这是第一个提出融合外表信息和深度运动特征，并用于视觉追踪的研究。我们全面的实验表明融合方法具有深度运动特征，并超过了单纯依赖外表信息的方法。」\nVirtual Worlds as Proxy for Multi-Object Tracking Analysis [44] 方法解决了现有虚拟世界中缺乏真实可变性视频追踪基准和数据集。该论文提出了一种新的真实世界复制方法，该方法从头开始生成丰富、虚拟、合成和照片逼真的环境。此外，该方法还能克服现有数据集中存在的一些内容匮乏问题。生成的图像能自动通过正确的真值进行标注，并允许应用于除目标检测/追踪外其它如光流等任务。\nGlobally Optimal Object Tracking with Fully Convolutional Networks [45] 专注处理目标变化和遮挡，并将它们作为目标追踪的两个根本限制。「我们提出的方法通过使用全卷积网络解决物体或目标外表的变化，还通过动态规划的方法解决遮挡情况」(Lee et al., 2016)。\n参考文献：\n[1] British Machine Vision Association (BMVA). 2016. What is computer vision? [Online] Available at: http://www.bmva.org/visionoverview [Accessed 21/12/2016]\n[2] Krizhevsky, A., Sutskever, I. and Hinton, G. E. 2012. ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada. Available: http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf\n[3] Kuhn, T. S. 1962. The Structure of Scientific Revolutions. 4th ed. United States: The University of Chicago Press.\n[4] Karpathy, A. 2015. What a Deep Neural Network thinks about your #selfie. [Blog] Andrej Karpathy Blog. Available: http://karpathy.github.io/2015/10/25/selfie/ [Accessed: 21/12/2016]\n[5] Quora. 2016. What is a convolutional neural network? [Online] Available: https://www.quora.com/What-is-a-convolutional-neural-network [Accessed: 21/12/2016]\n[6] Stanford University. 2016. Convolutional Neural Networks for Visual Recognition. [Online] CS231n. Available: http://cs231n.stanford.edu/ [Accessed 21/12/2016]\n[7] Goodfellow et al. 2016. Deep Learning. MIT Press. [Online] http://www.deeplearningbook.org/ [Accessed: 21/12/2016] Note: Chapter 9, Convolutional Networks [Available: http://www.deeplearningbook.org/contents/convnets.html]\n[8] Nielsen, M. 2017. Neural Networks and Deep Learning. [Online] EBook. Available: http://neuralnetworksanddeeplearning.com/index.html [Accessed: 06/03/2017].\n[9] ImageNet refers to a popular image dataset for Computer Vision. Each year entrants compete in a series of different tasks called the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Available: http://image-net.org/challenges/LSVRC/2016/index\n[10] See「What I learned from competing against a ConvNet on ImageNet」by Andrej Karpathy. The blog post details the author』s journey to provide a human benchmark against the ILSVRC 2014 dataset. The error rate was approximately 5.1% versus a then state-of-the-art GoogLeNet classification error of 6.8%. Available: http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\n[11] See new datasets later in this piece.\n[12] Keras is a popular neural network-based deep learning library: https://keras.io/\n[13] Chollet, F. 2016. Information-theoretical label embeddings for large-scale image classification. [Online] arXiv: 1607.05691. Available: arXiv:1607.05691v1\n[14] Chollet, F. 2016. Xception: Deep Learning with Depthwise Separable Convolutions. [Online] arXiv:1610.02357. Available: arXiv:1610.02357v2\n[15] Places2 dataset, details available: http://places2.csail.mit.edu/. See also new datasets section.\n[16] Hikvision. 2016. Hikvision ranked No.1 in Scene Classification at ImageNet 2016 challenge. [Online] Security News Desk. Available: http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/ [Accessed: 20/03/2017].\n[17] See Residual Networks in Part Four of this publication for more details.\n[18] Details available under team information Trimps-Soushen from: http://image-net.org/challenges/LSVRC/2016/results\n[19] Xie, S., Girshick, R., Dollar, P., Tu, Z. & He, K. 2016. Aggregated Residual Transformations for Deep Neural Networks. [Online] arXiv: 1611.05431. Available: arXiv:1611.05431v1\n[20] ImageNet Large Scale Visual Recognition Challenge (2016), Part II, Available: http://image-net.org/challenges/LSVRC/2016/ [Accessed: 22/11/2016]\n[21] Hu and Ramanan. 2016. Finding Tiny Faces. [Online] arXiv: 1612.04402. Available: arXiv:1612.04402v1\n[22] Liu et al. 2016. SSD: Single Shot MultiBox Detector. [Online] arXiv: 1512.02325v5. Available: arXiv:1512.02325v5\n[23] Redmon, J. Farhadi, A. 2016. YOLO9000: Better, Faster, Stronger. [Online] arXiv: 1612.08242v1. Available: arXiv:1612.08242v1\n[24] YOLO stands for「You Only Look Once」.\n[25] Redmon et al. 2016. You Only Look Once: Unified, Real-Time Object Detection. [Online] arXiv: 1506.02640. Available: arXiv:1506.02640v5\n[26]Redmon. 2017. YOLO: Real-Time Object Detection. [Website] pjreddie.com. Available: https://pjreddie.com/darknet/yolo/ [Accessed: 01/03/2017].\n[27] Lin et al. 2016. Feature Pyramid Networks for Object Detection. [Online] arXiv: 1612.03144. Available: arXiv:1612.03144v1\n[28] Facebook's Artificial Intelligence Research\n[29] Common Objects in Context (COCO) image dataset\n[30] Dai et al. 2016. R-FCN: Object Detection via Region-based Fully Convolutional Networks. [Online] arXiv: 1605.06409. Available: arXiv:1605.06409v2\n[31] Huang et al. 2016. Speed/accuracy trade-offs for modern convolutional object detectors. [Online] arXiv: 1611.10012. Available: arXiv:1611.10012v1\n[32] ibid\n[33] Wu et al. 2016. SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving. [Online] arXiv: 1612.01051. Available: arXiv:1612.01051v2\n[34] Hong et al. 2016. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection. [Online] arXiv: 1611.08588v2. Available: arXiv:1611.08588v2\n[35] DeepGlint Official. 2016. DeepGlint CVPR2016. [Online] Youtube.com. Available: https://www.youtube.com/watch?v=xhp47v5OBXQ [Accessed: 01/03/2017].\n[36] COCO - Common Objects in Common. 2016. [Website] Available: http://mscoco.org/ [Accessed: 04/01/2017].\n[37] ILSRVC results taken from: ImageNet. 2016. Large Scale Visual Recognition Challenge 2016.\n[Website] Object Detection. Available: http://image-net.org/challenges/LSVRC/2016/results [Accessed: 04/01/2017].\n[38] COCO Detection Challenge results taken from: COCO - Common Objects in Common. 2016. Detections Leaderboard [Website] mscoco.org. Available: http://mscoco.org/dataset/#detections-leaderboard [Accessed: 05/01/2017].\n[39] ImageNet. 2016. [Online] Workshop Presentation, Slide 31. Available: http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf [Accessed: 06/01/2017].\n[40] Bertinetto et al. 2016. Fully-Convolutional Siamese Networks for Object Tracking. [Online] arXiv: 1606.09549. Available: https://arxiv.org/abs/1606.09549v2\n[41] Held et al. 2016. Learning to Track at 100 FPS with Deep Regression Networks. [Online] arXiv: 1604.01802. Available: https://arxiv.org/abs/1604.01802v2\n[42] David Held. 2016. GOTURN - a neural network tracker. [Online] YouTube.com. Available: https://www.youtube.com/watch?v=kMhwXnLgT_I [Accessed: 03/03/2017].\n[43] Gladh et al. 2016. Deep Motion Features for Visual Tracking. [Online] arXiv: 1612.06615. Available: arXiv:1612.06615v1\n[44] Gaidon et al. 2016. Virtual Worlds as Proxy for Multi-Object Tracking Analysis. [Online] arXiv: 1605.06457. Available: arXiv:1605.06457v1\n[45] Lee et al. 2016. Globally Optimal Object Tracking with Fully Convolutional Networks. [Online] arXiv: 1612.08274. Available: arXiv:1612.08274v1\n原报告地址：http://www.themtank.org/a-year-in-computer-vision"}
{"content2":"3　机器学习\n随着2006年以Hadoop为代表的大数据技术的蓬勃兴起，解决了数据库时代的数据存储和处理能力的不足限制；云计算技术的大规模应用，比如Amazon和阿里云为代表的云计算厂商，将处理能力和计算能力的成本大大降低，从而让大规模的集群计算系统变得非常廉价；从而将针对数据的分析拓展至全量的数据分析，而非数据抽样。另外一个方面是将从前在数据挖掘时代无法应用的算法和思路变成了可能。这个时代ML(Machine Learning)逐渐取代数据挖掘，成为火热的关键词。\n那机器学习与数据挖掘的关系是什么呢？　机器学习是建立在数据挖掘技术之上发展而来，结合大数据技术(Hadoop, MapReduce, Spark/Storm等），逐步开发和应用了若干新的分析方法逐步演变而来形成的；这两个领域彼此之间交叉渗透，彼此都会利用对方发展起来的技术方法来实现业务目标，数据挖掘的概念更广一下，机器学习只是数据挖掘领域中的一个新兴分支与细分领域，只不过基于大数据技术让其逐渐成为了当下显学和主流。\n以下是摘在百度知道的定义：机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n上述定义的核心是尝试基于计算机系统来模拟人类的学习行为，从而获取新的知识与技能；换句话说，机器学习可以发展我们人类未曾发现的知识和规律，学习到人类从未掌握的技能；这是一个非常惊人的进步，超越人类的认知极限，从而引领人类进入了一个崭新的机器时代。\n机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。在我们当下的生活中，语音输入识别、手写输入识别等技术，识别率相比之前若干年的技术识别率提升非常巨大，达到了将近97%以上，大家可以在各自的手机上体验这些功能，这些技术来自于机器学习技术的应用。Google Translate技术据说已经达到了类比人工翻译的准确程度，兼具“信达雅”的特性，能做到这一点就来自于Google对其进行了大量语言学习的训练而成的。\n机器学习主要以监督学习(supervised learning)、无监督学习(unsupervised learning)、半监督学习和强化学习等形式。下面我们简要介绍以下这几种学习形式的基本内容：\n监督学习是 对具有概念标记（分类）的训练样本进行学习，以尽可能对训练样本集外的数据进行标记（分类）预测。这里，所有的标记（分类）是已知的。监督学习是训练神经网络和决策树的最常见技术。这两种技术（神经网络和决策树）高度依赖于事先确定的分类系统给出的信息。\n在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。\n在此学习方式下，介于监督学习和半监督学习之间。输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据 来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预 测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。\n在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈 到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。\n以上几种学习形式，目前使用最多的是监督学习和非监督学习模式，在自然语言处理(NLP)，图形图像识别等领域应用甚广。强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。\n此类的技术解决方案在Python, R都提供了相应的机器学习的算法实现，比如scikit-learn和R中的内置算法实现。\n4 深度学习（Deep Learning)\n深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。晦涩难懂的概念，略微有些难以理解，但是在其高冷的背后，却有深远的应用场景和未来。\n那深度学习和机器学习是什么关系呢？ 深度学习是实现机器学习的一种方式和一条路径。其核心是模拟和学习人类大脑的神经元工作方式，比如其按特定的物理距离连接；而深度学习使用独立的层、连接，还有数据传播方向，比如最近大火的卷积神经网络是第一个真正多层结构学习算法，它利用空间相对关系减少参数数目以提高训练性能，让机器认知过程逐层进行，逐步抽象，从而大幅度提升识别的准确性和效率。\n到了当下，经过深度学习技术训练的机器在识别图像时比人类更好，比如识别猫、识别血液中的癌细胞特征、识别MRI扫描图片中的肿瘤。谷歌AlphaGo学习围棋等等领域，已经超越了人类目前的认知和能力极限。\n主要的实现框架包括有：Google的Tensorflow, 源自加州伯克利分校的Caffe, 诞生于蒙特利尔理工学院的Python实现Theano， 来自facebook的Torch， Java版的深度学习框架DeepLearning4j等等不一而足。\n5　人工智能（Artifical Intelligence, AI)\n人工智能目前在业界是炙手可热，所有的互联网公司以及各路大迦们纷纷表态AI将是下一个时代的革命性技术，可与互联网、移动互联网时代的变更相媲美；AlphaGo在围棋领域战胜人类最顶尖的棋手让大众第一次直观认知到了AI的威力和强大，于是大家都不禁在思考到底什么是人工智能， 它将带给人类一个什么样的变化和未来?\n1956年，在达特茅斯会议（Dartmouth Conferences）上，计算机科学家首次提出了“AI”术语之时，就设想是否有一天机器可以像人一样拥有意识(consciousness)、自我（Self)和心智（Mind)，随着计算机工业和科学的飞速发展，曾经的幻想和遥不可及的设想已经可以看到变成现实的曙光。\n那人工智能到底是什么呢？ 笔者个人的理解是人工智能将学会人脑一样的思考、分析、推理和学习，具备人类相应的智商和独立思考能力；进而可能具备自我迭代和进化能力，帮助人类共同进行进化，极大提升目前人类社会的智能化程度。\n正如我们曾经看到过的诸多科幻典型中所描述的各类场景，笔者个人也比较认同其中的某些负面的可能性，不如当机器具备类似人类的智能之后，机器智能和人类将如何相处？ 如果发生冲突，人类何以控制机器智能，并引导人工智能按照人类的意志来发展？ 人类是否有能力控制机器智能的运行机制？等等之类的问题只能有待未来的科学家们来解决。\n深度学习、机器学习都是人工智能发展的重要领域，这些技术手段让人工智能从虚幻逐步变为现实，在带给人类诸多便利和大幅度的社会效率提升中，即将革命性地改变我们人类社会的进程发展。\n6 总结\n深度学习、机器学习的发展带了许多实际的商业应用，让虚幻的AI逐步落地，进而影响人类社会发展； 深度学习、机器学习以及未来的AI技术，将让无人驾驶汽车、更好的预防性治疗技术、更发达智能的疾病治疗诊断系统、更好的人类生活娱乐辅助推荐系统等，逐步融入人类社会的方方面面。\nAI既是现在，也是未来，不再是一种科幻影像和概念，业界变成了人类社会当下的一种存在，不管人类是否喜欢或者理解，他们都将革命性地改变创造AI的我们人类自身；至于未来，没有人会知道会如何，会不会真得如Matrix中的人类最终被机器所篡养，不得而知；但有一点是确定的，人类孜孜以求的研究和发展，AI时代终将到来。\n--------------------------------------- 罪恶的分割线--------------------------------\n本文系CDSN的博主《木小鱼的笔记》个人原创，如无允许，请勿转载。如要转载，请保留原始链接和原作者信息，支持原创，尊重原创，让知识的世界更美好。\n作者本人也维护了一个今日头条上的头条号：程序加油站，欢迎大家关注。"}
{"content2":"机器学习\n搞懂以下三个问题，你就算了解了机器学习的概念！\n问题1：机器学习的发展已经很多年了，它是人工智能的入门级技术，那么什么是机器学习呢？\n顾名思义：机器通过学习，不断调整和优化自己认知世界的方式；它是一帮计算机科学家希望计算机像人一样思考所研发出来的理论，既然要学习，肯定得需要大量学习资料（数据）。它是一门跨学科的技术，涉及到的知识有python编程（主流），大数据（hadoop即可），概率论，统计学等学科；\n问题2：机器学习有哪些实际运用呢？\n比如文本处理：广告计算，机器翻译等；机器视觉：无人车，百度图片搜索，人脸识别等；语音识别：地图导航，只能家居等；\n应用很广，接下来会有更多应用越来越多并且越大众化。\n问题3：机器学习有哪些分类？\n一般来说分为4-5类\n我们把数据和程序都取个别名，理解接下来的概念有助于你了解下面的知识。\n数据：1000万张猫，1000万张狗图片\n有标签的图片：一只猫的图片，对应告诉Alpha蚂蚁这是一只猫\n没有标签的图片：一只猫的图片，对应告诉Alpha蚂蚁这是一个图片，但是不知道是猫还是狗\n程序：Alpha蚂蚁\n监督学习：提供数据和对应的标签值\n比如让Alpha蚂蚁识别猫和狗的图片，先要给大量的猫和狗的图片（比如1000万张猫，1000万张狗）给Alpha蚂蚁识别，并且告诉它，哪一张图是猫，哪一张图是狗；让Alpha蚂蚁去学习这2000万张图片的70%左右，剩下的30%图片作为测试，测试这个训练出的结果正确率如何（类似于人类世界的考试，根据考试结果判定你学习得怎么样），如果测试结果达到90%多，那么算是不错的，目前人脸识别能达到99%多；接下来的事情就容易了，下次你再来一张猫或狗的图片，Alpha蚂蚁就自然能够认识你的图片是猫还是狗了。\n无监督学习：不提供相应的标签（观察各种数据背后的规律）\n大部分情况下，数据是有很多，但是呢，相应的标签是比较乱的，甚至很多都是错误的；类似于试卷很多，但是呢，答案很多都是错误的，这种情况下，机器学习就学到了错误的答案，这可是不允许的；所以，在这种情况下，就要使用无监督学习了，无监督学习就是只有数据，不再有对应的标签，还是举猫狗案例，1000万张猫，1000万张狗，Alpha蚂蚁这个时候去观察这些猫猫狗狗的信息，找出它们相应共有的特征，这样，将这些猫狗图片就能进行分类识别了。\n半监督学习：结合监督学习和无监督学习，如何利用少量有标签的样本和大量没有标签的样本进行分类\n突然一个时候，告诉你只有3张猫的图片，和三张狗的图片，剩下的猫狗图片有很多，但是没做分类，不知道是猫还是狗，Alpha蚂蚁没办法了，这么少的图片，肯定没法去训练，那该怎么办，先就用半监督学习，利用这6张图进行监督学习的分类训练，再利用剩下的没有标签的图片进行无监督训练，最终达到给一张图片就能识别是猫还是狗的分类。\n强化学习：让计算机完成一个从未接触过的任务，提供相应的规则，让计算机自己去总结和学习\n现在为止，Alpha蚂蚁已经十分强大了，能干很多事，但是，现在要把Alpha蚂蚁训练成为一个投篮高手，但是投篮是一个很多样化的操作，不太好提供数据训练，那么怎么办？就让Alpha蚂蚁自己去投篮，不管你用什么方式，投篮进去了奖励你吃一个苹果（这可是Alpha蚂蚁最喜欢的水果），进不去就扇Alpha蚂蚁一嘴巴子（被虐待的好惨）；Alpha蚂蚁就自己在那里练习投篮，通过不断的练习，Alpha蚂蚁最终掌握了最好的投篮方式，命中率也越来越高，最终成为了投篮高手！没错，AlphaGo就是这么玩的。\n遗传算法：模拟进化理论，淘汰弱智，保留强者\nAlpha蚂蚁要是一直练不会投篮怎么办呢？它就是一个不合格的投篮者，永远练不会怎么办。那就这样，淘汰这货，再生出1个更优的Alpha蚂蚁一代，依次进行淘汰，淘汰弱智，保留强者，这样就训练出了一个终极的Alpha蚂蚁N代，这是一个非常非常牛逼的新生代，是Alpha蚂蚁的重重重重重重重重重重重重重重重重孙子。\n至此，我们就把机器学习的基本内容讲完了，并未涉及任何算法，希望能让大家对机器学习感兴趣，研究相应的实现！"}
{"content2":"首先介绍计算机视觉领域的4个顶级代表性期刊吧。\n(1) IEEE Transactions on Pattern Analysis and Machine Intelligence，IEEE模式分析与机器智能汇刊，简称PAMI，是IEEE最重要的学术性汇刊之一。在各种统计中，PAMI被认为有着很强的影响因子（2011年影响因子为4.9）和很高的排名。显然，这个期刊的中稿难度那是相当的大，一般先投中CVPR之后再加点东西投该期刊会比较好中一点。\n(2) ACM Transactions on Graphics。美国计算机协会图形汇刊，简称TOG，该刊侧重于计算机图形的处理，影响因子在该领域也比较高，2011年为3.5。中稿的难度也极大，一般该刊对每年的SIGGRAPH(Special Interest Group for Computer GRAPHICS,计算机图形图像特别兴趣小组）会议论文全文收录。\n(3) International Journal of Computer vision，该刊也是该领域的顶级期刊之一，相比于PAMI来讲，该刊侧重于理论的推导。2011年影响因子为3.7，中稿难度也相当大。\n(4) IEEE Transactions on Image Processing，该刊也是图像处理领域的代表性期刊之一，相比于上面三个期刊来讲，该刊稍微低一点层次。2011年影响因子为3.042，中稿难度也比较大。审稿周期一年左右。\n除了上述让人望而生畏的顶级期刊之外，我们再看看一般的期刊吧。\n(1)Pattern recognition letters, 从投稿到发表，一年半时间;\n(2)Pattern recognition 不好中，时间长;\n(3)IEICE Transactions on Information and Systems， 作者中有一个必须是会员。收费高，审稿快。影响因子0.4;\n(4)International Journal of Pattern Recognition and Artificial Intelligence ， 审稿周期一般6–12周，影响因子偏低，容易中;\n(5)Computational Intelligence， 中等偏上，要求较高，杂志级别不错，关注人数偏少，比较冷门;\n(6)information processing letters, 影响因子低0.5左右，接搞量大，容易发表，审稿周期一般3–6个月;\n(7)Computer vision and image understanding, 9个月审稿期，平均投稿命中率20%，业内比较认可;\n(8)journal of visual communication and image representation， 投稿容易，审稿周期一年以上;\n(9)Signal processing letters, 影响因子0.99， 美国，审稿一个多月;\n(10)International Journal on Graphics, Vision and Image Processing (GVIP);\n(11)IET Image Processing, 影响因子0.758， EI Compendex ，审稿周期一年以上;\n(12)IET Computer Vision ，影响因子0.969;\n(13)SIAM Journal on Imaging Sciences;\n(14)International journal of imaging systems and technology，影响因子偏低，容易中，审稿周期一到两个月;\n(15)IEEE Signal Processing Letters， 审稿4—8周左右，影响因子不高，容易中，关注人不多;\n(16)Journal of Logic and Computation, 影响因子，0.789，SCI检索;\n(17)IEICE Transactions on Information and Systems 审稿时间2–4周，容易中，影响因子小，相对冷门，关注人数不多;\n(18)COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING，影响因子偏低，但仍然需要一定水平才可以投，审稿2–4周，SCI,EI检索;\n(19)Signal Processing: Image Communication，容易中，审稿周期半年到一年;\n(20)International Journal of Computer Vision， 较难，审稿周期半年到一年，EI,SCI检索;\n(21)Journal of Mathematical Imaging and Vision，审稿一个月左右，影响因子不高（1.3左右），Elsevier旗下，不容易中，稍微有些冷门，偏重数学推导;\n(22)Machine Vision and Applications， 影响因子偏低，但是接稿量不是很大，审稿周期一年以上，但容易发表，SCI,EI检索;\n(23)Pattern Analysis & Applications， 影响因子不高，影响力也比较小，审稿时间一年以上，但容易投中;\n(24)Signal Image and Video Processing， 容易中，审稿时间半年到一年，EI检索;\n(25)Pattern recognition and image analysis， EI检索;\n(26)Journal of digital imaging ，审稿周期半年到一年，影响因子偏低，容易中，很少有人关注;\n(27)Journal of VLSI signal processing systems for signal image and video ，影响因子偏低，容易中，审稿周期一年以上，关注人比较少;\n(28) Neural Processing Letters, 影响因子0.75左右，审稿三个月内给出审稿意见，比较快。（发现一年只发表20篇左右，一年投稿量估计200多篇（从编号可估出），可看出命中率绝对在10%以下，待考察）\n(29) COMPUTERS & GRAPHICS-UK (一般简称为COMPUTERS & GRAPHICS)，Elsevier旗下图像处理领域期刊之一，2011影响因子为1.0。审稿速度（同行例子：9月底投稿，10月中旬送审，12月初大修，2月中旬小修后录用。审稿速度和编辑处理速度都比较快！）。感觉要求不是很高！\n(30) EURASIP Journal on Image and Video Processing，影响因子2011年为0.5，有同学投过，速度比较快，2-3个月搞定。\n(31) Multimedia Tools and Applications，2012年影响因子为1.014，据说比较好中，速度也还可以。\n(32) Communications of the ACM，2012你那影响因子为2.511。中科院分区SCI大类分区2区，小类分区2区。\n(33) IEEE Transactions on Visualization and Computer Graphics，2012年影响因子为1.898，中科院分区SCI大类分区2区，小类分区2区。\n(34) IEEE Computer Graphics and Applications，2012年影响因子为1.228，中科院分区SCI大类分区3区，小类分区2区。\n(35) Graphical Models，2012年影响因子为0.697，中科院分区SCI大类分区4区，小类分区4区。\n(36) Computer Aided Geometric Design，2012年影响因子为0.810，中科院分区SCI大类分区4区，小类分区3区。\n(37) Computer Animation and Virtual Worlds，2012年影响因子为0.436，中科院分区SCI大类分区4区，小类分区4区。\n(38) Visual Computer，2012年影响因子为0.909，中科院分区SCI大类分区4区，小类分区4区。\n(39) Computer Graphics Forum，2012年影响因子为1.638，中科院分区SCI大类分区3区，小类分区2区。\n(40) International Journal of Computational Geometry and Applications，2012年影响因子为0.176，中科院（数学方向）分区SCI大类分区4区，小类分区4区。\n(41) Computational Geometry-Theory and Applications，2012年影响因子为0.545，中科院（数学方向）分区SCI大类分区2区，小类分区2区。\n(42) Journal of Visualization，2012年影响因子为0.506，中科院（工程技术）分区SCI大类分区4区，小类分区4区。\n(43) Computer Graphics World，2012年影响因子为0.000，中科院（工程技术）分区SCI大类分区4区，小类分区4区。\n(44) Virtual Reality，2012年影响因子为0.341，中科院（工程技术）分区SCI大类分区4区，小类分区4区。\n(43) Image and Vision Computing，2012年影响因子为1.959，中科院（工程技术）分区SCI大类分区3区，小类分区3区。\n(43) Computer Graphics World，2012年影响因子为0.000，中科院（工程技术）分区SCI大类分区4区，小类分区4区。\n影响因子(Impact Factor) : 某期刊前两年发表的论文在该报告年份（JCR year）中被引用总次数除以该期刊在这两年内发表的论文总数。这是一个国际上通行的期刊评价指标。\n转载： http://blog.sciencenet.cn/blog-370458-750306.html"}
{"content2":"作者：xiabodan 出处：http://blog.csdn.net/xiabodan/\n一般要去国外找资料，翻墙是第一道工序，也是一件一劳永逸的事情，会为以后的研究，资料查找节约很多的时间。后面有一些收集到的资源网站，不是每一个都标注了的，可以根据自己的喜好自行选择。\n想要在如茫茫大海的互联网中找到你想要的资料，没有一定技巧是不行的。搜索要做的第一件事就是极力的找一篇很好的paper的参考文献，无论google，即使像一篇国内的垃圾期刊也会有几篇比较好的参考文章，再通过参考文章的参考文章继续搜索。这样一层一层的总会找到本领域相关的大牛，只要找到一个大牛，那就很好办了，因为大牛们都是成群结队的，不论是文章的参考文祥要互相应用，连youtube等等这些他们也会互相评论、关注的，接下来你就只需要去关注他们，看看有什么成果是和自己相关的就OK了。\n一般来说了解一个领域需要从以下几个方面着手，专利，论文，博客，研究机构主页\n学习专利公开课，论文的话就不贴出来了，直接google或者在校内网搜索\n专利：\n如何查到一篇文献的DOI号或通过DOI找到原始文献? | 参考咨询知识库\nResolve a DOI\n中文DOI\ncrossref.org\n中国专利下载\nFPO IP Research & Communities\n佰腾网——中国专利和科技创新服务门户网站 - 网站首页\nSooPAT 专利搜索\n学习、公开课\n(电子书资源下载)Electronic library\ncoursera     免费在线教育\n慕课网：http://www.imooc.com/course/list  各种C/C++ JAVA PHP Linux ............完全免费，极力推荐推荐\nTED: Ideas worth spreading\n网易公开课：http://open.163.com/\n斯坦福大学公开课：机器人学    http://open.163.com/special/opencourse/robotics.html\n麻省理工大学公开课：算法导论 http://open.163.com/special/opencourse/algorithms.html\n斯坦福大学公开课：编程范式  http://open.163.com/special/opencourse/paradigms.html\n斯坦福大学公开课：编程方法学  http://open.163.com/special/sp/programming.html\nyoutube 视频，国外大牛的成果基本都会在youtube上公布，可以选择自己喜欢的大牛关注其动态。下图是我的网易课程与youtube关注\nhttps://www.youtube.com/?gl=HK&tab=w1\nzynq&&cortex-a9 FPGA研究\n新加坡国立大学real-time football cup     多核计算     hamsterworks  一个开源的FPGA project主页     VAST LAB(丛京生实验室)     ZYNQ研究shakith Fernando的博客主页，主要做图像处理FPGA，GPU并行加速，其实验室位于荷兰埃因霍芬理工大学并行计算研究室http://parse.ele.tue.nl/     荷兰埃因霍芬理工大学：University of Technology Eindhoven     FPGA 2015国际研讨会     linaro文件系统          博洛尼亚大学 FPGA立体视觉     Home :: OpenCores   一个开源的FPGA IP库\nXilinx wiki开源网站  包含了Xilinx公司各种demo  project\n目标检测跟踪研究\n1、USC Computer 研究组 （美国南加州大学）\n研究方向：图像分割，运动分析，大数据分析，模型分析，目标跟踪与识别。\nhttp://iris.usc.edu/USC-Computer-Vision.html\n2、The Center for vision ,Cognition ,Learning,and Art （美国加利福尼亚洛杉矶分校）\n研究方向：视觉，识别，学习，行为分析\n3、ETHZ Computer Vision Laboratory 研究组 （瑞士苏黎世联邦理工学院）\n研究方向：医学图像分析，目标识别，手势分析，跟踪，场景理解与建模\nhttp://www.vision.ee.ethz.ch/research/projects_icu.cgi?topic=3\n4、EPFLComputer Vision Laboratory 研究组 （瑞士洛桑联邦理工学院）\n研究方向：图像描述子，可变形的外表建模，目标跟踪，人体建模\nhttp://cvlab.epfl.ch/research/body/surv\n5、learning ，Recognition,and Surveillance 研究组（奥地利格拉茨技术大学）\n研究方向：机器学习，目标识别，目标检测与跟踪\nhttp://lrs.icg.tugraz.at/research/classifiergrid/classifiergrid.php\n6、The Australian Center for Visual Technologies 研究组 （澳大利亚阿德莱德大学）\n研究方向：增强现实，机器学习，3D建模，参数估计，监控，跟踪\nhttp://blogs.adelaide.edu.au/acvt/\n7、香港中文大学多媒体实验室\n研究方向：深度学习，面部分析，视频监控跟踪，图像视频搜索\nhttp://mmlab.ie.cuhk.edu.hk/projects.html\n8：KLT（光流法特征跟踪）\nhttp://web.yonsei.ac.kr/jksuhr/articles/Kanade-Lucas-Tomasi%20Tracker.pdf\nhttp://www.ces.clemson.edu/~stb/klt/\nhttp://vision.ucla.edu//MASKS/labs.html\n部分图像处理，机器视觉研究机构主页\n此博客收集大量的计算机视觉牛人的主页，以及很有实力的研究机构 http://blog.csdn.net/carson2005/article/details/6601109\n科学网博客，转载了许多机器视觉的资源网址收藏： http://blog.sciencenet.cn/blog-454498-456241.html\n机器学习资料大汇总 | 我爱机器学习\n(普林斯顿大学视觉组)Sliding Shapes for 3D Object Detection in Depth Images\n(德国慕尼黑大学增强现实与计算机视觉)Chair for Computer Aided Medical Procedures and Augmented Reality - Lehrstuhl für Informatikanwendungen in der Medizin und Augmented Reality\n(加州理工大学计算机视觉)Computational Vision: [Research]\n(计算机视觉大牛主页code)Hernan Badino's Homepage\n斯坦福人工智能实验室    斯坦福大学机器视觉主页    Cvlibs Andreas Geiger 主页    (国外知名的视觉数据库code)The KITTI Vision Benchmark Suite ：www.cvlibs.net/datasets/kitti/    Albert Huang - MIT    项志宇-浙江大学 人工智能实验室个人主页    卡内基梅隆大学机器人实验室paper    西雅图机器人协会Seattle Robotics Society Encoder    （单目视觉里程计，google code）Monocular Visual Odometry » Dr. Rainer Hessmer\nKinect Fusion 微软的kinect 应用\nProjects « CMP 机器感知研究\n部分机器人、视觉、图像处理库\nMarco Zuliani's web page（matlab图像处理库）    MRPT | Mobile Robot Programming Toolkit library 移动机器人开发库，包含图像处理，视觉，导航，定位，相机标定等    BoofCV(java图像处理机器视觉库)    Peter Kovesi（MATLAB视觉库）    微软研究院 ： http://research.microsoft.com/en-us/um/people/kahe/  不说了 太牛了啥都有\n(图像处理库)IPOL Journal · Image Processing On Line     math-neon(ARM下neon实现的数学运算库)    Image Stitching（图像拼接库code）    Stan Birchfield（付澄提供网站）    (多媒体处理参考)HIPR Table of Contents and Main Index    opencv图像处理http://docs.opencv.org/    OpenSLAM.org 开源SLAM库\nmatlab 单/双目相机标定 Camera Calibration Toolbox for Matlab\n工作笔试相关\n首先推荐 协议森林 - Vamei - 博客园  包含算法与数据结构  Linux网络协议森林\n排序算法详解 C语言版 Sorting Algorithm Animations\n在线算法题目测试Problems | LeetCode OJ\n笔试题-面试题 - lionel的专栏 - 博客频道 - CSDN.NET\nC++ - 标签 - Alexia(minmin) - 博客园    02：C语言 - 随笔分类 - cv_ml_张欣男 - 博客园    程序员面试笔试宝典学习记录（一）（常见面试笔试题目） - weixliu - 博客园\n笔试面试必会代码 以及必看书籍 http://blog.csdn.net/liuqiyao_01/article/details/16960695\nLinux相关\nPOSIX Threads Programming\nLinux下多线程和共享内存混合编程实例-liuyang_430068-ChinaUnix博客\nc - POSIX pthread programming - Stack Overflow\nWindows多线程 - MoreWindows Blog - 博客频道 - CSDN.NET\n相关书籍（嵌入式软开方向）网络编程 + Linux驱动/应用程序设计 + 数据结构与算法（重点）+ C/C++，边看边写程序\nAPUE（advance programming UNIX environment）\n编程珠玑\nLinux设备驱动程序\nLinux系统编程\nLinux网络编程\n并行程序设计导论\n数据结构与算法分析\n程序员面试宝典（第四版） 小心点看，有很多错误的地方\n关注的公司\nactel China  IEEE Xplore - IEEE Std 1164-1993   Linear Technology - 主页  Microchip Technology Inc  ModelSim - Advanced Simulation and Debugging  ModelSim ASIC and FPGA Design Simulator - Mentor Graphics  PLX Technology xiabodan@263.com xiabo891219  windriver Download free trial  亚德诺半导体 Analog Devices 半导体和信号处理IC  全面可编程和器件  来自Maxim的模拟、线性、混合信号器件  模拟, 半导体, 数字信号处理 - 德州仪器  细节决定成败，细节创造利润--周立功单片机  Kontron - Embedded Computers, Industrial PC  Lytro  PCI-A429 - ARINC PCI接口卡 - ARINC - 阿尔塔数据技术  英飞凌——半导体与系统解决方案 - Infineon Technologies  DDC - Data Device Corporation | Supplier of MIL-STD-1553, ARINC 429, and other data interface products.  Altera – FPGA、CPLD、ASIC和可编程逻辑  USB.org - Documents  Standards & Documents Search | JEDEC  International Rectifier - High Reliability  Vishay - 威世半导体 - 威世品牌  Discrete, Analog and Logic Semiconductor Components | Diodes, Inc.  ON Semiconductor  Allied Electronics – Electronic Parts and Components Distributor  Micron Technology, Inc. DRAM, NAND Flash, NOR Flash, MCP, Hybrid Memory Cube, SSD  Cadence OrCAD Downloads  arm inc.460557758@qq.com Xiabo891219  micrium.com summer wMUunio3iX3M  dililentchina  zedboard  digilent  logicbricks(xilinx-IP)  adapteva(f傅立叶公司 芯片)  omnitek（XILINX）  beecube  zrobot  freeRTOS\n国内论坛 （很少看）\namoBBS 阿莫电子论坛 首页   FPGA-CPLD - 电子工程世界-论坛  queuequeue - C++ Reference  【新提醒】一伙儿网 - Yihuor  个人中心 - 程序员联合开发网   中国电子顶级开发网论坛(EETOP) 国内最顶级的电子论坛，最活跃的电子工程师交流社区  看雪安全论坛 - www.pediy  第九单片机论坛 -帐号  Local configuration register配置  Xilinx 大学计划：开发板组合  创新网altium中文社区   OpenHW-中国首个开放源码硬件社区  论坛 - SoC Vista -- 中国芯动力 -- SoC/FPGA/ASIC设计家园 - Powered by Discuz!  MATLAB中文论坛|Simulink中文论坛 xiabodan Xiabo891219  love in C++, live on MFC - C++博客  Forum for Electronics(电子论坛)  Index - Arduino Forum  fpga4fun.com - I2C  最活跃FPGA论坛推荐社区  德州仪器在线技术支持社区  上海库源电气科技有限公司-PSpice技术支持中心（Cadence代理 Allegro代理 OrCAD代理 PSpice代理） - Powered by phpwind  库源电气-Cadence代理、OrCAD代理、Allegro代理、PSpice代理，为您提供最专业的Cadence PCB解决方案  Download - OpenCV China ：图像处理,计算机视觉库,Image Processing, Computer Vision  freeIP与参考设计  立即注册 - OpenCV论坛  iVeia Git • Index page  电子发烧友论坛-中国电子技术论坛 - 最好最受欢迎电子论坛!  综合电子论坛 good  eclipse  xilinx.eetrend.com  XILINX开源网站  ohwr(开源硬件)  中电网(在线培训)  电路网  elements  matlab  adept forums  chinaunix  u-boot"}
{"content2":"计算机视觉领域的一些牛人博客，超有实力的研究机构等的网站链接\nhttp://blog.csdn.net/baijingjing425/article/details/7591358\n李航：http://research.microsoft.com/en-us/people/hangli/，是MSRA Web Search and Mining Group高级研究员和主管，主要研究领域是信息检索，自然语言处理和统计学习。近年来，主要与人合作使用机器学习方法对信息检索中排序，相关性等问题的研究。曾在人大听过一场他的讲座，对实际应用的问题抽象，转化和解决能力值得学习。\n周志华：http://cs.nju.edu.cn/zhouzh/，是南京大学的杰青，机器学习和数据挖掘方面国内的领军人物，其好几个研究生都进入了美国一流高校如uiuc，cmu等学习和深造。周教授在半监督学习，multi-label学习和集成学习方面在国际上有一定的影响力。另外，他也是ACML的创始人。人也很nice，曾经发邮件咨询过一个naive的问题，周老师还在百忙之中回复了我，并对我如何发邮件给了些许建议。\n杨强：http://www.cse.ust.hk/~qyang/，香港科技大学教授，也是KDD 2012的会议主席，可见功力非同一般。杨教授是迁移学习的国际领军人物，曾经的中国第一位acm全球冠军上交的戴文渊硕士期间就是跟他合作发表了一系列高水平的文章。还有，杨教授曾有一个关于机器学习和数据挖掘有意思的比喻：比如你训练一只狗，若干年后，如果它忽然有一天能帮你擦鞋洗衣服，那么这就是数据挖掘；要是忽然有一天，你发现狗发装成一个老太婆消失了，那么这就是机器学习。\n李建中：http://db.hit.edu.cn/jianzhongli/，哈工大和黑大共有教授，是分布式数据库的领军人物。近年来，其团队在不确定性数据，sensor network方面也发表了一系列有名文章。李教授为人师表，教书育人都做得了最好，在圈内是让人称道的好老师和好学者。\n唐杰：http://keg.cs.tsinghua.edu.cn/jietang/，清华大学副教授，是图挖掘方面的专家。他主持设计和实现的Arnetminer是国内领先的图挖掘系统，该系统也是多个会议的支持商。\n张钹：http://www.csai.tsinghua.edu.cn/ ... zhang_bo/index.html 清华大学教授，中科院院士，。现任清华大学信息技术研究院指导委员会主任，微软亚洲研究院技术顾问等。主要从事人工智能、神经网络、遗传算法、智能机器人、模式识别以及智能控制等领域的研究工作。在过去二十多年中，张钹教授系统地提出了问题求解的商空间理论。近年来，他建立了神经与认知计算研究中心以及多媒体信息处理研究组。该研究组已在图像和视频的分析与检索方面取得一些重要研究成果。\n刘铁岩：http://research.microsoft.com/en-us/people/tyliu/ MSRA研究主管，是learning to rank的国际知名学者。近年逐步转向管理，研究兴趣则开始关注计算广告学方面。\n王海峰：http://ir.hit.edu.cn/~wanghaifeng/ 信息检索，自然语言处理，机器翻译方面的专家，ACL的副主席，百度高级科学家。近年，在百度主持研发了百度翻译产品。\n何晓飞：http://people.cs.uchicago.edu/~xiaofei/ 浙江大学教授，多媒体处理，图像检索以及流型学习的国际领先学者。\n朱军：http://www.ml-thu.net/~jun/ 清华大学副教授，机器学习绝对重量级新星。主要研究领域是latent variable models, large-margin learning, Bayesian nonparametrics, and sparse learning in high dimensions. 他也是今年龙星计划的机器学习领域的主讲人之一。\n吴军：http://www.cs.jhu.edu/~junwu/ 腾讯副总裁，前google研究员。著名《数学之美》和《浪潮之巅》系列的作者。\n张栋：http://weibo.com/machinelearning 前百度科学家和google研究员，机器学习工业界的代表人物之一。\n戴文渊：http://apex.sjtu.edu.cn/apex_wiki/Wenyuan_Dai 现百度凤巢ctr预估组leader。前ACM大赛冠军，硕士期间一系列transfer learning方面的高水平论文让人瞠目结舌。\n国外高校研究团队\n伯克利大学机器视觉小组\nUCLA 的VCLA研究小组\n玛丽女王大学机器视觉小组\n苏黎世联邦理工学院机器视觉实验室\n南加利福尼亚大学机器视觉小组\n德雷塞尔大学生物医学计算小组\n剑桥大学视觉与机器人研究小组\n伦敦大学学院视觉和图像研究小组\n多伦多大学机器视觉小组\n达姆施塔特工业大学机器视觉研究小组\n卡耐基梅隆大学机器人学院\n加州大学河滨分校视频计算小组\n阿姆斯特丹大学智能系统实验室\nMIT 计算机科学与人工智能实验室\nMIT 机器视觉实验室\nMIT 生物与机器学习实验室\nMIT 媒体实验室\nMIT 认知科学实验室\n林雪平大学机器视觉实验室\n格拉茨技术大学计算机图形与视觉实验室\n佐治亚理工学院机器人技术与智能机器实验室\n治亚理工学院计算感知实验室\n瑞典皇家理工学院机器视觉与行动感知实验室\n加州大学圣塔芭芭拉分校视觉研究实验室\n加利福尼亚大学圣迭戈分校视觉与机器人研究实验室\n加利福尼亚大学圣迭分校机器视觉实验室\n加利福尼亚大学尔湾分校机器视觉实验室\n利福尼亚大学圣克鲁兹分校认知科学实验室\n加州理工学院计算视觉实验室\n佛罗里达大学计算机视觉实验室\n科罗拉多州立大学机器视觉实验室\n哥伦毕业大学机器视觉实验室\n康奈尔大学计算机视觉小组\n康奈尔大学机器人实验室\n佐治亚大学视觉与并行计算实验室\n伊利诺伊大学芝加哥分校机器人实验室\n马里兰大学自动化研究中心\n麻省大学机器视觉实验室\n莱布尼茨大学信息处理所\n密歇根大学安娜堡分校人工智能实验室\n内华达大学雷诺分校机器视觉实验室\n俄勒冈州立大学医学工程医学图像研究组\n宾夕法尼亚大学机器人自动化感知实验室\n宾夕法尼亚大学医学图像处理小组\n宾州州立大学视觉感知，动作和认知实验室\n普渡大学机器人视觉实验室\n普渡大学视频和图像处理实验室\n斯坦福大学视觉与图像科学实验室\n斯坦福视觉实验室\n德克萨斯大学奥斯汀分校图像和视频工程实验室\n犹他大学计算科学与图像学院\n威斯康辛大学机器视觉小组\n柏林工业大学机器视觉与遥感小组\n波士顿大学图像与视频计算小组\n都柏林城市大学视觉系统小组\n邓迪大学机器视觉与图像处理小组\n约克大学视觉图像与机器人实验室\n约克大学视觉研究中心\n亚琛工业大学语言与模式识别实验室\n阿德莱德大学视觉技术实验室\n雷丁大学计算视觉小组\n马德里理工大学机器视觉小组\n普利茅斯大学机器人与智能系统小组\n帕维亚大学视觉与多媒体实验室\n圣保罗控大学视觉控制实验室\n乌内迪大学人工视觉与实时系统\n维罗纳大学视觉与图像处理实验室\n西英格兰大学机器视觉中心\n乌得勒支大学图像科学学院\n以色列魏茨曼科技大学机器视觉实验室\n英国哥伦毕业大学人工智能实验室\n曼切斯特大学图像科学实验室\n牛津大学视觉集合团队\n伦敦帝国学校视觉研究小组\n国内高校研究团队\n天津大学计算机视觉实验室\n上海交通大学计算机视觉实验室\n中国科学院自动化研究所模式识别重点实验室\n香港城市大学视频检索小组\n中科院跨媒体计算研究组\n北京交通大学信息科学研究所\n香港中文大学多媒体实验室\n企业研究团队\n微软亚洲研究院计算机视觉研究组\n微软剑桥研究院ML与CV研究组\n微软交互式视觉媒体研究组\n顶级机器人研究室\nIBM研究院\n谷歌研究院\n三菱电子研究实验室\nAdobe研究院\n迪士尼研究院"}
{"content2":"作者/赵屹华\n年初回到杭州工作，今年的人工智能大会也移到杭州举办，感到甚是幸运。感谢CSDN的朋友们给我提供了零距离接触大师、聆听分享的机会。\n几十场分享和讨论中，我印象最深刻的还是李德毅院士关于L3无人驾驶车的报告。自动挡、定速巡航的功能仿佛才普及不久，无人驾驶技术已经扑面而来，真是让我这个手动挡爱好者有点儿猝不及防。李院士从设计者和研发者的角度对无人车的挑战和量产做了深入剖析，我就从使用者的角度谈一点思考。\n李院士在报告中提到了无人车有360度的高精度雷达、100亿公里的道路测试，并且经过严格的专业测试才可能获得执照。作为乘客，我想再冗余的测试也不嫌多。有人曾经评价百度的无人车“三千多个场景，一万多个if，巨坑无比”。不知这位同学是高级黑还是真愚蠢。保守估计linux系统内核都有三万个if，眼观六路、耳听八方的无人车才区区一万多个if怎么嫌多。百度要是宣布再增加几万个if我都不拒绝。\n做过软件开发的同学都知道，一个完善的软件产品，主要的逻辑部分代码可能只占10%，其余90%的代码都是在处理边界条件和异常。天知道用户会输入什么千奇百怪的数据把控制系统搞挂了。宁可处理器响应时间慢几毫秒，也不见得比驾驶员的反应时间还慢。\n跑在路上的无人驾驶汽车，它不仅要面临诸如传感器失效、数据传输失败等bug，甚至真的会碰到地震、塌方、桥梁断裂等不测风云。我们都不希望自己是这个倒霉蛋。我巴不得让工程师在汽车出厂前给控制系统跑一亿个单元测试，训练数据能够穷举世界上所有可能发生的路况以及处置办法，深度学习模型的精度超过99.9999%。在ImageNet比赛中0.1%的精度提升可能只是排位的变化，现实中的0.1%可能就是几百起车祸。\n还有，技术的角色在有些问题面前却显得很尴尬，即使李院士这样的大牛可能也进退维谷，比如著名的“电车难题”。\n一个疯子把五个无辜的人绑在电车轨道上。一辆失控的电车朝他们驶来，我若拉下控制杆，电车朝另一条轨道方向驶去，但那条轨道上有一个同样无辜的人。把电车换成无人驾驶车该怎么办呢？\n从功利主义的角度来说，选择换轨道的损失较小。这里假设我们追求人生而平等的理想，生命和生命是等价的。无人驾驶算法为了最小化损失函数的值，会选择用一条生命换回五条生命。于是，在这种情况下更换轨道似乎是更合理的选择。\n如果我们换一下假设条件，五个无辜者是即将被枪决的罪犯，躺在另一条轨道上的无辜者是Geoffrey Hinton，并且无人车的传感器识别出了他们。再一次以功利的角度审判，Hinton的价值显然高于五个将被枪决的罪犯，控制算法为了找到最优解，似乎不应该更换轨道。\n那么问题来了，如果五个罪犯换成了五个博士生呢？尴尬，这个优化函数又该怎么解？深度学习模型的训练样本又该由谁来标记呢？想象一下，今后你我都是谷歌或者百度无人车深度学习模型里的一维特征，GPU集群哼哧哼哧地计算了三天三夜终于得到了收敛的权值参数。你的权重是0.63，我的权重是0.78，而你和玛莎拉蒂的交叉特征权重又高于我和长安之星的权重。是不是细思极恐！\n不过，我向来相信技术，认为无人驾驶的处理器比我对车辆的操控能力更强，高精度的雷达和传感器比我的眼睛和耳朵更敏锐，GPS和深度学习算法比我规划线路的能力更强。人工智能方兴未艾，无人驾驶车是领域的探路者，无人驾驶技术如今面临的问题在其它人工智能产品中迟早都会遇到。晚来不如早来，早来早受益。至于矛盾，不妨借鉴小平同志的智慧，“搁置争议，让更聪明的后人来解决它们”吧。"}
{"content2":"转自：http://blog.csdn.NET/jing_xin/article/details/18216647\n以下链接是关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/P ... CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/ci ... ision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/gro ... fault.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际会议VOC(微软剑桥研究院组织): http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1,http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type= ... son_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/ 人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/ 人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；最有影响力的研究成果：目标检测；2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别"}
{"content2":"AI职位：人工智能职位之计算机视觉算法工程师的简介、知识结构、发展方向之详细攻略\n计算机视觉算法工程师的简介\n截止到2019年7月份，关键指数显示\n1、各大互联网巨头的薪资介绍\n2、计算机视觉应用领域\n3、各大公司职位简介\n(1)、商汤科技   深度学习/计算机视觉研究员\n职位诱惑：人工智能独角兽,技术大牛多,工作氛围好\n工作职责:\n1. 负责计算机视觉和深度学习基本算法的开发与性能提升，涉及的问题包括但不限于：检测、跟踪、分类、语义分割、强化学习、3D视觉和图像处理等\n2. 推动计算机视觉算法和深度学习在众多实际应用领域的性能优化和落地\n3. 提出和实现最前沿的算法，保持算法在工业界和学术界的领先\n任职资格:\n1. 熟练掌握机器学习（特别是深度学习）和计算机视觉的基本方法\n2. 优秀的分析问题和解决问题的能力，对解决具有挑战性的问题充满激情\n3. 较强的算法实现能力，熟练掌握 C/C++ 编程，熟悉 Shell/Python/Matlab 编程\n4. 有较强的研究能力优先，如研究生发表过第一作者CCF A类会议或期刊等论文，或本科发表过第一作者CCF B类以上会议或期刊论文\n5. 有较强的代码能力优先，获得过ACM或其他商业代码竞赛的荣誉，如ACM区预赛金牌、NOI银牌以上、百度之星决赛等；或代码开源在github上并有较大影响\n6. 有较强的学术比赛经验或者在重要数据集的Leaderboard上排名靠前，比如ImageNet等学术数据集或者Kaggle等一些国内外商业比赛\n7. 有较丰富的相关经验优先，如有一年以上在BAT或人工智能知名创业公司进行视觉算法或工程相关的工作经验，或来自国内外计算机视觉/计算机图形学/机器学习/数据挖掘等领域内知名实验室。\n(2)、腾讯  上海-计算机视觉工程师\n职位诱惑：大平台\n岗位职责：\n1、负责视觉相关算法的研究和应用，包括但不限于如下方向：目标追踪，目标检测，图像分割，图像分类等计算机视觉技术。\n2、紧跟前沿算法和技术，推动计算机视觉算法和深度学习在实际应用领域的性能优化和落地。\n岗位要求：\n1、图像处理、计算机视觉、模式识别、机器学习等相关专业硕士及以上；\n2、2年以上图像识别/计算机视觉领域相关工作经验，熟悉图像识别/计算机视觉领域的主流模型和算法，关注领域内的最新进展；\n3、较强的算法实现能力，熟练掌握C/C++，Python，Java等至少一门语言；熟悉Linux环境开发；熟悉OpenCV等常用库，熟练使用一种或几种深度学习框架（caffe，tensorflow， Pytorch）；\n4、有目标检测、语义分割、人脸识别等实际应用经验者优先；\n5、有较强的研究能力者优先，如在领域顶级会议如CVPR，ICCV，ECCV，NIPS等发表过高质量论文； 6、有良好的团队合作和沟通能力。\n(3)、字节跳动  头条实验室工程师-计算机视觉\n职位诱惑：弹性工作，免费三餐，租房补贴，带薪休假\n职位职责：\n1、为产品应用提出人工智能解决方案和模型；\n2、人工智能技术的工程化；\n3、开发新技术的原型系统。\n职位要求：\n1、扎实的数学和算法基础：概率统计、数值优化算法；\n2、快速学习新技术的能力，能够在较短时间内理解前沿论文并评价；\n3、实践动手能力强， ACMICPC, NOI/IOI，top coder，Kaggle比赛获奖者优先；\n4、在机器学习、图像视频理解、图像分类、物体检测等算法方面有经验者优先；\n5、在顶级学术会议上发表论文者优先；\n6、有大规模分布式系统工程经验者优先；\n7、能与团队融洽合作相处；\n8、积极主动有热情。\n(4)、拼多多\n职位诱惑：平台好,大牛多,薪资待遇佳\n岗位职责：\n1、负责计算机视觉、深度学习相关方向的技术难点攻关与前瞻研究。\n2、负责计算机视觉、深度学习相关的技术实现与产品的研发工作。\n3、负责计算机视觉、深度学习相关的算法计算性能优化，并推动其上线应用。\n任职要求：\n1. 硕士及以上学历，模式识别、计算机视觉、数学等相关专业；\n2. 对计算机视觉和模式识别方向有扎实的理论和实践基础；\n3. 有图像检测、数字信号处理、统计机器学习、深度学习项目经验者优先；\n4. 熟悉物体（人体、人脸、通用目标）检测、跟踪与识别的基本算法；\n5. 具有较强编程能力，熟练使用至少一种脚本语言（python/shell等），有C++和java开发经验，熟悉linux开发环境，有caffe, pytorch, tensorflow经验更佳；\n6. 正直诚信、有团队精神，有良好的学习能力，沟通能力和独立解决技术问题的能力。\n计算机视觉算法工程师的知识结构\n1、深度学习原理\n2、深度学习框架\n3、编程语言\n4、专业知识(数字图像处理及OpenCV)\n计算机视觉算法工程师的发展方向\n后期更新……"}
{"content2":"关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/P ... CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/ci ... ision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/gro ... fault.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际会议VOC(微软剑桥研究院组织): http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\n(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1\n(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/\n(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/\n(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html\n(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/\n(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1,http://www.vision.ee.ethz.ch/~vangool/\n(115)行人检测主页：http://www.pedestrian-detection.com/\n(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576http://mldemos.epfl.ch/\n(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/\n(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html\n(119)计算机视觉分类信息导航：http://www.visionbib.com/\n(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/\n(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/\n(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/\n(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/\n(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/\n(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html\n(126)微软学术搜索：http://libra.msra.cn/\n(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪\n(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html\nhttp://www.ri.cmu.edu/person.html?type= ... son_id=741 AAM,三维重建\n（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理\n（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索\n（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理\n（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位\n（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/ 人脸识别、监控、3D、视频检索、\n（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建\n（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、\n（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类\n（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率\n（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习\n（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\n（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、\n（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛\n（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习\n（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习\n（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/ 人的姿态估计和跟踪\n（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别\n（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术\n（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建\n（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航\n（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成\n（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别\n（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成\n（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；最有影响力的研究成果：目标检测；2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购\n（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法\n（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；\n（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率\n（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实\n（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别"}
{"content2":"国内机构： 上海交通大学图像处理与模式识别研究所 微软亚洲研究院计算机视觉研究组 武汉大学数字摄影测量与计算机视觉研究中心 浙江大学图像技术研究与应用团队 西安交通大学人工智能与机器人研究所 天津大学精密测试技术及仪器国家重点实验室 清华大学精密测试技术及仪器国家重点实验室 清华大学电子工程系智能图文信息处理实验室 中科院生物识别与安全技术研究中心 中科院自动化所 中科院自动化所医学影像研究室 中科院模式识别国家重点实验室 中科院长春光机所 中科院沈阳自动化所 中科院西安光机所 北京大学智能科学系 中科院计算所视觉信息处理和学习实验室 中科院自动化所医学影像研究室 中科院计算所视觉信息处理和学习实验室 JDL实验室\n国外机构： Stanford vision实验室 加州大学伯克利分校CV小组 南加州大学CV实验室 卡内基梅隆大学CV主页 微软剑桥研究院ML与CV研究组 普渡大学机器人视觉实验室 宾利州立大学感知、运动与认识实验室 宾夕法尼亚大学GRASP实验室 内达华大学里诺校区CV实验室 密西根大学vision实验室 麻省大学视觉实验室 ETH-Zurich CV实验室 CMU大学ILIM实验室 微软剑桥研究院组织 启明大学计算机视觉与模式识别实验室 康奈尔大学视觉与图像分析组 密西根州立大学生物识别研究组 柏林科技大学计算机视觉与遥感实验室 布里斯托大学数字多媒体研究组 萨利大学视觉、语音与信号处理中心 格拉纳达大学超分辨率重建项目组 巴塞罗那大学计算机视觉中心 Deep Learning Elefant 格拉斯哥大学信息检索小组 威兹曼科技大学超分辨率\n国内牛人：深圳大学于仕祺 微软CV张正友 中科院自动化所李子青 中科院计算所山世光 清华大学章毓晋 上海交通大学刘允才 清华大学丁晓青 北京大学高文 清华大学艾海舟 中科院田捷 微软Redmond研究院Simon Baker 微软亚洲研究院孙剑 微软亚洲研究院马毅 香港中文大学贾佳亚 南京大学吴建鑫 GE研究院李关 百度技术副总监于凯 西安电子科技大学高新波 中科院樊彬 凤巢系统架构师张栋 中科院自动化所肖柏华 中科院自动化所孙哲南 南京信息工程大学刘青山 清华大学冯建江 中山大学郑伟诗 百度深度学习余轶南 中科院计算所常虹 南开大学程明明 江西财经大学袁飞牛 北京航空航天大学周付根 北京航空航天大学姜志国 西南财经大学段江 中科院沈阳自动化所华春生 华中科技大学张天序 南京大学周志华 香港理工大学张磊\n国外牛人： 香港中文大学汤晓鸥 MIT林达华 MIT Douglas Lanman Stanford崔靖宇 UCLA朱松纯 美国Rutgers刘青山 香港中文大学王晓刚 U.C. San Diego 德克萨斯州奥斯汀分校Kristen Grauman 巴塞尔大学Thomas Vetter 俄勒冈州立大学 Rob Hess 卡内基梅隆大学Robert T. Collins MIT Chris Stauffer 密歇根州立大学 Anil K. Jain 伊利诺伊州立大学Thomas S. Huang 巴塞尔大学Sami Romdhani CMU大学Yang Wang 曼彻斯特大学Tim Cootes 罗彻斯特大学Jiebo Luo 华盛顿大学Iva Kemelmacher 魏茨曼科技大学Ronen Basri 普林斯顿大学李凯 普林斯顿大学贾登 牛津大学Andrew Zisserman leeds大学Mark Everingham 爱丁堡大学Chris William 微软剑桥研究院John Winn 佐治亚理工学院Monson H.Hayes\n牛人博客：小魏的修行路邹宇华张睿卿邹晓艺CVPR|Opencv脚踏实地crazy_sparrowyang xinthefutureisour张驰原晃晃悠悠萝卜驿站C++的罗浮宫MIND HACKSeegfmricelerychenxceman1997zouxy09darkscope陈皓Matrix67徐宥赵劼白明懒人有禅图灵社区JerryLeadJulyMoreWindows曹胜欢yang_xian521Sophia_qing\n国际会议： CV会议日历 CV论文索引 CV会议搜索 IEEE 会议 wikicfp CCF推荐排名 ICCV ECCV CVPR AAAI ICIP ICPR ACCV BMVC IAPR MVA SIGGRAPH EUROGRAPHICS IJCAI ICASSP ICML\n国际期刊： IJCV PAMI CVIU TIP PR PR Letters CVGIP IJPRAI 国内期刊 自动化学报 计算机学报 软件学报 电子学报 中国图象图形学报 模式识别与人工智能 光电子激光 光学精密工程 展会信息 慕尼黑上海光博会 中国国际机器视觉展览会 国际光电博览会 光电子中国博览会 中国国际工业博览会 中国国际机器人展览会 美国消费电子展\n代码托管： Github sourceforge Gitlab BitBucket Google code GitCafe GitShell 开源中国 淘宝code 京东code 新浪 sae CSDN code Coding svnchina"}
{"content2":"1956年，几个计算机科学家相聚在达特茅斯会议（Dartmouth Conferences），提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言；或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。\n过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流（大数据）的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发\n人工智能、机器学习和深度学习之间的关系\n人工智能（Artificial Intelligence）——为机器赋予人的智能\n早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。\n人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。\n我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，Pinterest上的图像分类；或者Facebook的人脸识别。\n这些是弱人工智能在实践中的例子。这些技术实现的是人类智能的一些具体的局部。但它们是如何实现的？这种智能是从何而来？这就带我们来到同心圆的里面一层，机器学习。\n机器学习—— 一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、强化学习和贝叶斯网络等等。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。\n机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。\n这个结果还算不错，但并不是那种能让人为之一振的成功。特别是遇到云雾天，标志牌变得不是那么清晰可见，又或者被树遮挡一部分，算法就难以成功了。这就是为什么前一段时间，计算机视觉的性能一直无法接近到人的能力。它太僵化，太容易受环境条件的干扰。\n深度学习——一种实现机器学习的技术\n人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。\n例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。\n每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。\n我们仍以停止（Stop）标志牌为例。将一个停止标志牌图像的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、救火车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有权重，给出一个经过深思熟虑的猜测——“概率向量”。\n这个例子里，系统可能会给出这样的结果：86%可能是一个停止标志牌；7%的可能是一个限速标志牌；5%的可能是一个风筝挂在树上等等。然后网络结构告知神经网络，它的结论是否正确。\n即使是这个例子，也算是比较超前了。直到前不久，神经网络也还是为人工智能圈所淡忘。其实在人工智能出现的早期，神经网络就已经存在了，但神经网络对于“智能”的贡献微乎其微。主要问题是，即使是最基本的神经网络，也需要大量的运算。神经网络算法的运算需求难以得到满足。\n不过，还是有一些虔诚的研究团队，以多伦多大学的Geoffrey Hinton为代表，坚持研究，实现了以超算为目标的并行算法的运行与概念证明。但也直到GPU得到广泛应用，这些努力才见到成效。\n我们回过头来看这个停止标志识别的例子。神经网络是调制、训练出来的，时不时还是很容易出错的。它最需要的，就是训练。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结果。\n只有这个时候，我们才可以说神经网络成功地自学习到一个停止标志的样子；或者在Facebook的应用里，神经网络自学习了你妈妈的脸；又或者是2012年吴恩达（Andrew Ng）教授在Google实现了神经网络学习到猫的样子等等。\n吴教授的突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中的图像。吴教授为深度学习（deep learning）加入了“深度”（deep）。这里的“深度”就是说神经网络中众多的层。\n现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下围棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。\n深度学习，给人工智能以璀璨的未来\n深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n人工智能就在现在，就在明天。有了深度学习，人工智能甚至可以达到我们畅想的科幻小说一般。你的C-3PO我拿走了，你有你的终结者就好了。\n【注】机器视觉（Machine Vision, MV） & 计算机视觉(Computer Vision, CV)\n从学科分类上， 二者都被认为是 Artificial Intelligence 下属科目。\n计算机视觉是采用图像处理、模式识别、人工智能技术相结合的手段，着重于一幅或多幅图像的计算机分析。图像可以由单个或多个传感器获取，也可以在单个传感器在不同时刻获取的图像序列，分析是对目标物体的识别，确定目标物体的位置和姿态，对三维景物进行符号描述和解释。在计算机视觉中，经常使用几何模型、复杂的知识表达，采用基于模型的匹配和搜索技术，搜索的策略常使用自底向上、自顶向下、分层和启发式控制策略。\n机器视觉则偏重于计算机技术工程化，你问哪个够自动获取和分析特定的图像和场景，以控制相应的行为。具体的说，计算机视觉为机器视觉提供图像和精武分析的理论及算法基础，机器视觉为计算机视觉的实现提供了传感器模型、系统构造和实现手段。因此可以认为，一个机器视觉系统就是一个自动获取一幅或多幅目标物体图像，对所获取图像的各种特征量进行处理、分析和测量，并对测量结果做出定性的分析和定量解释，从而得到有关目标物体的某种认识并做出下相应决策的系统。机器视觉系统的功能包括：物体定位、特征检测、缺陷判断、目标识别、计数和运动跟踪。"}
{"content2":"继阿里、腾讯、金山、美团等相继宣布“AI云平台“战略后，华为也高调宣布：”将与在计算机视觉领域的大佬英伟达展开深度合作，在人工智能、机器学习、深度学习方面共同构建公有云AI平台。\n无独有偶，近日云计算厂商UCloud也被曝出，成为由创新工场、搜狗和今日头条联合宣布三方携手发起“AI Challenger全球AI挑战赛”的唯一AI GPU合作方，为大赛提供AI模型训练服务，再加上其UAI-Service和安全屋等产品，和即将于9月发布的UAI-Train，打造AI云平台，构建一站式AI全服务的野心昭然若揭。\n“人工智能+云计算“的竞争赛道开始变的越来越热闹。原因在于各国政府对人工智能未来的重视程度，互联网科技公司争先恐后的入局，云端领域对于GPU计算能力的需求在持续增长。\n阿里云的先发优势：前沿技术+数据布局\n阿里作为中国云市场份额排名第一的公有云公司，此前在云栖大会上高调发布ET城市大脑、ET工业大脑、ET医疗大脑，并同样与英伟达签署合作协议，建立联合实验室，进行高性能计算领域相关技术的联合攻关。\n如果说公有云的布局是对未来5-10年的技术发展方向所做的前瞻性布局，那么阿里投资的饿了么兼并百度外卖，则是奠定其未来云服务第一位置的一个基础。大餐饮、大物流和新零售对应的是口碑、饿了么、菜鸟物流、点我达、盒马生鲜，作为餐饮流量的入口，未来合并百度外卖占据市场一半份额的饿了么，必将会为靠流量起家，有众多数据支撑的阿里提供更多的数据优势，作为人工智能+云计算的养分，这也会为未来阿里在人工智能领域的布局取得先发优势。\n而这一切受益最大的除了社会，可能就是人工智能行业的程序员了。\n云栖大会：站在现在，看懂未来\n一个优秀的程序员最高明之处便在于积极主动努力提升自己上；在于不断学习新的技术与概念、读书、更新博客与社区的人员交流上；专注于对你影响力最大的东西。作为改变社会前沿的程序员，拥有不一样的技术视野是十分重要的，而2017云栖大会便是一个程序员提升自己，扩展视野的好机会。\n这届云栖大会的主题是“飞天·智能” 大会议题覆盖了云计算、大数据、人工智能、智能硬件、VR、AR、芯片技术、数据库、IoT、操作系统、生物识别、天文科研、金融科技、量子计算等前沿领域，是程序员了解人工智能、大数据、云计算发展趋势，规划未来发展路径的一个灯塔。\n除了技术展示，届时还会有各种各样的主论坛，分论坛、科技成果展，全面展示阿里未来的技术战略布局，各行业领袖对未来技术发展的理解、以及其他公司的最新科技成果。\n开源中国购票享独家福利\n开源中国现已开通独家优惠购票通道，现在登陆 https://www.oschina.net/osc-aliyun-yunqi/?s=csdn-wz 购票即可享100元返现优惠。想了解阿里“NASA“计划进展到哪一步了吗？想了解各家科技巨头的“黑科技”产品落地情况吗？想知道未来科技是怎么推动社会与经济发展的吗？那还不赶紧买票去？\n就算你不想知道这些，了解一下你所关注领域的发展情况，看看哪家企业研究的事情是你最感兴趣的，为你未来跳槽去干一份自己喜欢的工作挣大钱！去一次也值啊！！！\n据悉这次云栖大会将首次推出人工智能云栖助理，在现场与观众互动，除了能问它各种企业信息外，还能调戏它噢！\n一起来玩玩？"}
{"content2":"以下内容整理自 2017 年 6 月 29 日由“趣直播–知识直播平台”邀请的嘉宾实录。\n分享嘉宾: 罗韵\n目前，人工智能，机器学习，深度学习，计算机视觉等已经成为新时代的风向标。这篇文章主要介绍了下面几点：\n第一点，如果说你要入门计算机视觉，需要了解哪一些基础知识？\n第二点，既然你要往这方面学习，你要了解的参考书籍，可以学习的一些公开课有哪些？\n第三点，可能是大家都比较感兴趣的，就是计算机视觉作为人工智能的一个分支，它不可避免的要跟深度学习做结合，而深度学习也可以说是融合到了计算机视觉、图像处理，包括我们说的自然语言处理，所以本文也会简单介绍一下计算机视觉与深度学习的结合。\n第四点，身处计算机领域，我们不可避免的会去做开源的工作，所以本文会给大家介绍一些开源的软件。\n第五点，要学习或者研究计算机视觉，肯定是需要去阅读一些文献的，那么我们如何开始阅读文献，以及慢慢的找到自己在这个领域的方向，这些都会在本文理进行简单的介绍。\n1.基础知识\n接下来要介绍的，第一点是计算机视觉是什么意思，其次是图像、视频的一些基础知识。包括摄像机的硬件，以及 CPU 和 GPU 的运算。\n在计算机视觉里面，我们也不可避免的会涉及到考虑去使用 CPU 还是使用 GPU 去做运算。然后就是它跟其他学科的交叉，因为计算机视觉可以和很多的学科做交叉，而且在做学科交叉的时候，能够发挥的意义和使用价值也会更大。另外，对于以前并不是做人工智能的朋友，可能是做软件开发的，想去转型做计算机视觉，该如何转型？需要学习哪些编程语言以及数学基础？这些都会在第一小节给大家介绍。\n1.0 什么是计算机视觉\n计算机视觉是一门研究如何使机器“看”的科学。\n更进一步的说，就是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给一起检测的图像\n作为一个科学学科，计算机视觉研究相关的理论和技术，视图建立能够从图像或者多维数据中获取“信息”的人工智能系统。\n目前，非常火的VR、AR，3D处理等方向，都是计算机视觉的一部分。\n计算机视觉的应用\n无人驾驶\n无人安防\n人脸识别\n车辆车牌识别\n以图搜图\nVR/AR\n3D重构\n医学图像分析\n无人机\n其他\n了解了计算机视觉是什么之后，给大家列了一下当前计算机视觉领域的一些应用，几乎可以说是无处不在，而且当前最火的所有创业的方向都涵盖在里面了。其中包括我们经常提到的无人驾驶、无人安防、人脸识别。人脸识别相对来说已经是一个最成熟的应用领域了，然后还有文字识别、车辆车牌识别，还有以图搜图、 VR/AR，还包括 3D 重构，以及当下很有前景的领域–医学图像分析。\n医学图像分析他在很早就被提出来了，已经研究了很久，但是现在得到了一个重新的发展，更多的研究人员包括无论是做图像的研究人员，还是本身就在医疗领域的研究人员，都越来越关注计算机视觉、人工智能跟医学图像的分析。而且在当下，医学图像分析也孕育了不少的创业公司，这个方向的未来前景还是很值得期待的。然后除此之外还包括无人机，无人驾驶等，都应用到了计算机视觉的技术。\n1.1图像和视频，你要知道的概念\n图像\n一张图片包含了：维数、高度、宽度、深度、通道数、颜色格式、数据首地址、结束地址、数据量等等。\n图像深度：存储每个像素所用的位数（bits）\n当一个像素占用的位数越多时，它所能表现的颜色就更多，更丰富。\n举例：一张400*400的8位图，这张图的原始数据量是多少？像素值如果是整型的话，取值范围是多少？\n1，原始数据量计算：400 * 400 * ( 8/8 )=160,000Bytes\n(约为160K)\n2，取值范围：2的8次方，0~255\n图片格式与压缩：常见的图片格式JPEG，PNG，BMP等本质上都是图片的一种压缩编码方式\n举例：JPEG压缩\n1，将原始图像分为8*8的小块，每个block里有64pixels。\n2，将图像中每个8*8的block进行DCT变换（越是复杂的图像，越不容易被压缩）\n3，不同的图像被分割后，每个小块的复杂度不一样，所以最终的压缩结果也不一样\n视频\n原始视频=图片序列。\n视频中的每张有序图片称为“帧（frame）”。压缩后的视频，会采取各种算法减少数据的容量，其中IPB就是最常见的。\nI帧：表示关键帧，可以理解为这一幅画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）\nP帧：表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧画面差别的数据）\nB帧表示双向差别帧，记录的本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，要通过前后画面与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码比较麻烦。\n码率：码率越大，体积越大；码率越小，体积越小。\n码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。也就是取样率（并不等同于采样率，采样率用的单位是Hz，表示每秒采样的次数），单位时间内取样率越大，精度就越高，处理出来的文件就越接近原始文件，但是文件体积与取样率是成正比的，所以几乎所有的编码格式重视的都是如何用最低的码率达到最少的失真，围绕这个核心衍生出来cbr（固定码率）与vbr（可变码率），码率越高越清晰，反之则画面粗糙而且多马赛克。\n帧率\n影响画面流畅度，与画面流畅度成正比：帧率越大，画面越流畅；帧率越小，画面越有跳动感。如果码率为变量，则帧率也会影响体积，帧率越高，每秒钟经过的画面就越多，需要的码率也越高，体积也越大。\n帧率就是在一秒钟时间里传输的图片的帧数，也可以理解为图形处理器每秒钟刷新的次数。\n分辨率\n影响图像大小，与图像大小成正比；分辨率越高，图像越大；分辨率越低，图像越小。\n清晰度\n在码率一定的情况下，分辨率与清晰度成反比关系：分辨率越高，图像越不清晰，分辨率越低，图像越清晰\n在分辨率一定的情况下，码率与清晰度成正比关系：码率越高，图像越清晰；码率越低，图像越不清晰\n带宽、帧率\n例如在ADSL线路上传输图像，上行带宽只有512Kbps，但要传输4路CIF分辨率的图像。按照常规，CIF分辨率建议码率是512Kbps，那么照此计算就只能传一路，降低码率势必会影响图像质量。那么为了确保图像质量，就必须降低帧率，这样一来，即便降低码率也不会影响图像质量，但在图像的连贯性上会有影响。\n1.2摄像机\n摄像机的分类：\n监控摄像机（网络摄像机和摸你摄像机）\n不同行业需求的摄像机（超宽动态摄像机、红外摄像机、热成像摄像机等）\n智能摄像机\n工业摄像机\n当前的摄像机硬件我们可以分为监控摄像机、专业行业应用的摄像机、智能摄像机和工业摄像机。而在监控摄像机里面，当前用的比较多的两个类型一个叫做网络摄像机，一个叫做模拟摄相机，他们主要是成像的原理不太一样。\n网络摄像机一般比传统模拟摄相机的清晰度要高一些，模拟摄像机当前应该说是慢慢处于一个淘汰的状态，它可以理解为是上一代的监控摄像机，而网络摄像机是当前的一个主流的摄相机，大概在 13 年的时候，可能市场上 70% 到 80% 多都是模拟摄像机，而现在可能 60% 到 70% 都是的网络摄像机。\n除此之外，不同的行业其时会有特定的相机，想超宽动态摄像机以及红外摄像机、热成像摄像机，都是在专用的特定的领域里面可能用到的，而且他获得的画面跟图像是完全不一样的。如果我们要做图像处理跟计算机视觉分析，什么样的相机对你更有利，我们要学会利用硬件的优势。\n如果是做研究的话一般是可以控制我们用什么样的摄相机，但如果是在实际的应用场景，这个把控的可能性会稍微小一点，但是在这里你要知道，有些问题可能你换一种硬件，它就能够很好的被解决，这是一个思路。\n还有些问题你可能用算法弄了很久也没能解决，甚至是你的效率非常差，成本非常高，但是稍稍换一换硬件，你会发现原来的问题都不存在了，都被很好的解决了，这个就是硬件对你的一个新的处境了。\n包括现在还有智能摄像机、工业摄像机，工业摄像机一般的价格也会比较贵，因为他专用于各种工业领域，或者是做一些精密仪器，高精度高清晰度要求的摄像机。\n1.3 CPU和GPU\n接下来给大家讲一下 CPU 跟 GPU，如果说你要做计算机视觉跟图像处理，那么肯定跳不过 GPU 运算，GPU 运算这一块可能也是接下来需要学习或者自学的一个知识点。\n因为可以看到，当前大部分关于计算机视觉的论文，很多实现起来都是用 GPU 去实现的，但是在应用领域，因为 GPU 的价格比较昂贵，所以 CPU 的应用场景相对来说还是占大部分。\n而 CPU 跟 GPU 的差别主要在哪里呢？ 它们的差别主要可以在两个方面去对比，第一个叫性能，第二个叫做吞吐量。\n性能，换言之，性能会换成另外一个单词叫做 Latency（低延时性）。低延时性就是当你的性能越好，你处理分析的效率越高，相当于你的延时性就越低，这个是性能。另外一个叫做吞吐量，吞吐量的意思就是你同时能够处理的数据量。\n而 CPU 跟 GPU 的差别在哪里呢？主要就在于这两个地方，CPU 它是一个高性能，就是超低延时性的，他能够快速的去做复杂运算，并且能达到一个很好的性能要求。而 GPU是以一个叫做运算单元为格式的，所以他的优点不在于低延时性，因为他确实不善于做复杂运算，他每一个处理器都非常的小，相对来说会很弱，但是它可以让它所有的弱处理器，同时去做处理，那相当于他就能够同时处理大量的数据，那这个就意味着它的吞吐量非常大，所以 CPU重视的是性能，GPU重视的是吞吐量。\n所以大部分时候，GPU 他会跟另外一个词语联系在一起，叫做并行计算，意思就是它可以同时做大量的线程运算，为什么图像会特别适合用 GPU 运算呢？这是因为 GPU 它最开始的设计就是叫做图形处理单元，它的意思就是我可以把每一个像素，分割为一个线程去运算，每一个像素只做一些简单的运算，这个就是最开始图形处理器出现的原理。\n它要做图形渲染的时候，要计算的是每一个像素的变换。所以每一个像素变换的计算量是很小很小的，可能就是一个公式的计算，计算量很少，它可以放在一个简单的计算单元里面去做计算，那这个就是 CPU 跟 GPU 的差别。\n基于这样的差别，我们才会去设计什么时候用 CPU，什么时候用 GPU。如果你当前设计的算法，它的并行能力不是很强，从头到尾从上到下都是一个复杂的计算，没有太多可并性的地方，那么即使你用了 GPU，也不能帮助你很好提升计算性能。\n所以，不要说别人都在用 GPU 那你就用 GPU，我们要了解的是为什么要用 GPU ，以及什么样的情况下用 GPU，它效果能够发挥出来最好。\n1.4计算机视觉与其他学科的关系\n计算机视觉目前跟其他学科的关系非常的多，包括机器人，以及刚才提到的医疗、物理、图像、卫星图片的处理，这些都会经常使用到计算机视觉，那这里呢，最常问到的问题无非就是有三个概念，一个叫做计算机视觉，一个叫做机器视觉，一个叫做图像处理，那这三个东西有什么区别呢？\n这三个东西的区别还是挺因人而异的，每一个研究人员对它的理解都不一样。\n首先，Image Processing更多的是图形图像的一些处理，图像像素级别的一些处理，包括 3D 的处理，更多的会理解为是一个图像的处理；而机器视觉呢，更多的是它还结合到了硬件层面的处理，就是软硬件结合的图形计算的能力，跟图形智能化的能力，我们一般会理解为他就是所谓的机器视觉。\n而我们今天所说的计算机视觉，更多的是偏向于软件层面的计算机处理，而且不是说做图像的识别这么简单，更多的还包括了对图像的理解，甚至是对图像的一些变换处理，当前我们涉及到的一些图像的生成，也是可以归类到这个计算机视觉领域里面的。\n所以说计算机视觉它本身的也是一个很基础的学科，可以跟各个学科做交叉，同时，它自己内部也会分的比较细，包括机器视觉、图像处理。\n1.5 编程语言AND数学基础\n这一部分的内容可以参见《非计算机专业，如何学习计算机视觉》\n2.参考书籍和公开课\n参考书\n第一本叫《Computer Vision：Models, Learning and Inference》written by Simon J.D. prince，这个主要讲的更适合入门级别的，因为这本书里面配套了非常多的代码，Matlab 代码，C 的代码都有，配套了非常多的学习代码，以及参考资料、文献，都配得非常详细，所以它很适合入门级别的同学去看。\n第二本《Computer Vision：Algorithms and Applications》written by Richard Szeliski，这是一本非常经典，非常权威的参考资料，这本书不是用来看的，是用来查的，类似于一本工具书，它是涵盖面最广的一本参考书籍，所以一般会可以当成工具书去看，去查阅。\n第三本《OpenCV3编程入门》作者：毛星云，冷雪飞 ，如果想快速的上手去实现一些项目，可以看看这本书，它可以教你动手实现一些例子，并且学习到 OpenCV 最经典、最广泛的计算机视觉开源库。\n公开课：\nStanford CS223B\n比较适合基础，适合刚刚入门的同学，跟深度学习的结合相对来说会少一点，不会整门课讲深度学习，而是主要讲计算机视觉，方方面面都会讲到。\nStanford CS231N\n这个应该不用介绍了，一般很多人都知道，这个是计算机视觉和深度学习结合的一门课，我们上 YouTube 就能够看到，这门课的授课老师就是李飞飞老师，如果说不知道的话可以查一下，做计算机视觉的话，此人算是业界和学术界的“执牛耳”了。\n3.需要了解的深度学习知识\n深度学习没有太多的要讲的，不是说内容不多，是非常多，这里只推荐一本书给大家，这本书是去年年底才出的，是最新的一本深度学习的书，它讲得非常全面，从基础的数学，到刚才说的概率学、统计学、机器学习以及微积分、线性几何的知识点，非常的全面。\n4.需要了解和学习的开源软件\nOpenCV\n它是一个很经典的计算机视觉库，实现了很多计算机视觉的常用算法。可以帮助大家快速上手。\nCaffe\n如果是做计算机视觉的话，比较建议 Caffe。Caffe 更擅长做的是卷积神经网络，卷积神经网络在计算机视觉里面用的是最多的。\n所以无论你后面学什么样其它的开源软件， Caffe 是必不可免的，因为学完 Caffe 之后你会发现，如果你理解了 Caffe，会用 Caffe，甚至是有能力去改它的源代码，你就会发现你对深度学习有了一个质的飞跃的理解。\nTensorFlow\nTensorFlow 最近很火，但是它的入门门槛不低，你要学会使用它需要的时间远比其他所有的软件都要多，其次就是它当前还不是特别的成熟稳定，所以版本之间的更新迭代非常的多，兼容性并不好，运行效率还有非常大的提升空间。\n5.如何阅读相关的文献\n先熟悉所在方向的发展历程，然后精读历程中的里程碑式的文献。\n例如：深度学习做目标检测，RCNN，Fast RCNN，Faster RCNN，SPPNET，SSD和YOLO这些模型肯定是要知道的。又例如，深度学习做目标跟踪，DLT，SO-DLT等。\n计算机视觉的顶会：\nICCV：International Conference on Computer Vision，国际计算机视觉大会\nCVPR：International Conference on Computer Vision and Pattern Recognition，国际计算机视觉与模式识别大会\nECCV：European Conference on Computer Vision，欧洲计算机视觉大会\n除了顶会之外呢，还有顶刊。像 PAMI、IJCV，这些都是顶刊，它代表着这个领域里面最尖端最前沿以及当下的研究方向。\n---------------------\n原文：https://blog.csdn.net/ksws0292756/article/details/78881839"}
{"content2":"来源： 数据观\n概要：从目前来看，计算机视觉技术是人工智能的核心技术之一，广泛的商业化渠道和技术基础使其最为热门。\n目前，中国的人工智能研究处于爆发期，行业巨头公司正逐渐完善自身在人工智能的产业链布局，而不断涌现出的创业公司将持续在垂直领域深耕深挖。\n从目前来看，计算机视觉技术是人工智能的核心技术之一，广泛的商业化渠道和技术基础使其最为热门。\n未来，“人工智能+”有望成为新业态，人工智能产业将成为独角兽集中地，而人才储备则将成为制约中国人工智能发展的重要因素。\n\n未来智能实验室是世界第一个AI智商评测与趋势研究机构，基于“标准智能系统”、“AI智商测试量表”、“智能系统7个等级划分”、“互联网（城市）云脑架构”等研究成果，致力于评测智能系统智商发展水平，研究智能系统未来发展趋势。详细介绍请点击这里\n欢迎访问未来智能实验室的官方网站：FutureAILab.org； 未来智能实验室微信公众号：未来智能实验室，FutureAILab\n扫描二维码，关注未来智能实验室（FutureAILab）"}
{"content2":"AI URLs\n北京大学视觉与听觉信息处理实验室\n北京邮电大学模式识别与智能系统学科\n复旦大学智能信息处理开放实验室\nIEEE Computer Society北京映象站点\n计算机科学论坛\n机器人足球赛\n模式识别国家重点实验室\n南京航空航天大学模式识别与神经计算实验室 - PARNEC\n南京大学机器学习与数据挖掘研究所 - LAMDA\n南京大学人工智能实验室\n数据挖掘研究院\n微软亚洲研究院\n中国科技大学人工智能中心\n中科院计算所\n中科院计算所生物信息学实验室\n中科院软件所\n中科院自动化所\n中科院自动化所人工智能实验室\n更多信息请看：https://www.jianshu.com/p/949eaeb495d2"}
{"content2":"1.计算机视觉\n对于计算机视觉领域来说，贡献最大的当然是 CVPR 与 ICCV，其它如 IJCAI 等也有相关主题的获奖论文。这些获奖论文具体研究的方向主要有目标检测、图像标注、图像生成、语义分割、卷积神经网络架构等方面。今年唯一以研究卷积架构为主题的获奖论文是康奈尔与清华大学联合完成的 Densely Connected Convolutional Networks，他们发现如果卷积神经网络在接近输入层和输出层的层级中包含较短的连接，那么 CNN 就能在训练上显著地变得更深、更精确和拥有更高的效率。据此，他们提出了密集卷积网络（DenseNet），这种卷积神经网络以前馈的方式将每一层与其他层相连接起来。这篇论文的评价非常高，很多研究者认为 DenseNet 在 ResNet 基础上提出了更优秀的密集型连接方式，这种连接不仅能使得特征更加稳健，同时还能产生更快的收敛速度。虽然有学者指出 DenseNet 的内存占用太大，训练成本很高，但也有研究者测试表明在推断时它所需要的内存要比 ResNet 少。以下展示了 DenseNet 的基本架构：\n除了卷积架构外，语义分割或目标实例分割最有影响力之一的获奖论文就是何凯明等研究者提出来的 Mask R-CNN，它是一种简单、灵活和高效的通用目标分割框架。Mask R-CNN 是基于 Faster R-CNN 的扩展，它在用于边界框识别的分支上添加了一个并行的分支用于预测目标的掩码。因此这种方法不仅能够有效地检测图像中的目标，同时还能为每个实例生成一个高质量的分割掩码。值得注意的是，何凯明是该最佳论文的第一作者，同时是今年最佳学生论文的作者之一，若加上 CVPR 2009、CVPR 2016 两篇最佳论文，那么他已有四篇获计算机视觉顶会的最佳论文。\nMask R-CNN 框架\n在计算机视觉研究主题中，今年获奖论文讨论得比较多的可能就是目标检测。在 YOLO9000: Better, Faster, Stronger 论文中，作者提出了 YOLOv2 和 YOLO9000 检测系统。YOLOv2 能大大改善 YOLO 模型，并且以非常高的 FPS 获得更好的结果，而 YOLO9000 这一网络结构可以实时地检测超过 9000 种物体分类，这主要可以归因于 WordTree 混合了目标检测数据集与目标识别数据集，因此通过联合训练能实现非常好的效果。而在 Focal Loss for Dense Object Detection 论文中，研究者提出的全新 Focal Loss 方法，它集中于稀疏、困难样本中的训练，避免了训练过程中可能出现的大量负面因素。他们表明使用 Focal Loss 进行训练的 RetinaNet 可以在目标检测任务上达到一步检测器的速度，同时准确性高于业内最佳的两步检测器。\n图像生成其实也是今年获奖论文比较关注的主题，例如苹果公司的 Learning from Simulated and Unsupervised Images through Adversarial Training 提出了模拟加非监督学习方法在使用合成图像方面展现出了显著的提升效果。而另一篇 Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering 提出了一种条理化的标签解纠缠的生成对抗网络（TDGAN），该 TDGAN 通过指定多个场景属性（如视角、照明和表现等）从单张图片重新渲染出感兴趣目标的新图片。若给定一张输入图像，解纠缠网络会抽取解开的、可解释性的表征，然后这些表征再投入到生成网络以生成图片。"}
{"content2":"一 前言\n计算机视觉的两个重要维度：\n1.语义感知（semantic）\n2.几何属性（Geometry）\n细节：\n关于通道:\n1.RGB通道\n2.卷积后通道表示有多少个卷积核\n平滑 = 模糊\n目标检测和实体分割的区别：\n目标检测：画框，说出框中的类型是什么\n实体分割：说清每一个像素值属于哪个类别"}
{"content2":"作者简介：\n吴双，原百度研究院硅谷人工智能实验室高级研究员，百度美国研发中心高级架构师。美国南加州大学物理博士，加州大学洛杉矶分校博士后，研究方向包括计算机和生物视觉，互联网广告算法，互联网文本和视频的推荐系统，语音识别和自然语言处理，曾在NIPS等国际会议中发表文章。\n刘少山，PerceptIn联合创始人。加州大学欧文分校计算机博士，研究方向包括智能感知计算、系统软件、体系结构与异构计算。现在PerceptIn主要专注于SLAM技术及其在智能硬件上的实现与优化。曾在百度美国研发中心负责百度无人车系统架构与产品化。剧透：刘少山将在MDCC 2016移动开发者大会上分享无人驾驶通用技术干货。\n本文为《程序员》原创文章，未经允许不得转载，更多精彩文章请订阅2016年《程序员》\n本文是无人驾驶技术系列的第三篇，着重介绍基于计算机视觉的无人驾驶感知系统。在现有的无人驾驶系统中，LiDAR是当仁不让的感知主角。但是由于LiDAR的成本高等因素，业界有许多是否可以使用成本较低的摄像头去承担更多感知任务的讨论。本文探索了基于计算机视觉的无人驾驶感知方案。首先，验证一个方案是否可行需要一个标准的测试方法，我们介绍了广泛被使用的无人驾驶视觉感知数据集KITTI。然后，我们讨论了在无人驾驶场景中使用到的具体计算机视觉技术，包括Optical Flow和立体视觉、物体的识别和跟踪以及视觉里程计算法。\n无人驾驶的感知\n在无人驾驶技术中，感知是最基础的部分，没有对车辆周围三维环境的定量感知，就有如人没有了眼睛，无人驾驶的决策系统就无法正常工作。为了安全与准确的感知，无人驾驶系统使用了多种传感器，其中可视为广义“视觉”的有超声波雷达、毫米波雷达、激光雷达（LiDAR）和摄像头等。超声波雷达由于反应速度和分辨率的问题主要用于倒车雷达，毫米波雷达和激光雷达承担了主要的中长距测距和环境感知，而摄像头主要用于交通信号灯和其他物体的识别。\nLiDAR由于出色的精度和速度，一直是无人驾驶感知系统中的主角，是厘米级的高精度定位中不可或缺的部分。但是正如本系列第一篇文章《光学雷达在无人驾驶技术中的应用》分析，LiDAR存在成本昂贵、空气中的悬浮物影响精度等问题，毫米波雷达虽然相比LiDAR可以适应较恶劣的天气和灰尘，但也需要防止其他通讯设备和雷达间的电磁波干扰。\n传统计算机视觉领域的主要研究方向是基于可见光的摄像头的视觉问题，从摄像头采集的二维图像推断三维物理世界的信息。那么最常见、成本较低的摄像头能不能承担更多的感知任务呢?在本文中，我们将探索基于计算机视觉的无人驾驶感知方案。首先，要验证一个方案是否可行，我们需要一个标准的测试方法。本文将介绍由德国卡尔斯鲁厄技术研究院（KIT）和丰田芝加哥技术研究院（TTIC）共同开发的KITTI数据集。在有了标准的数据集之后，研究人员可以开发基于视觉的无人驾驶感知算法，并使用数据集对算法进行验证。我们将介绍计算机视觉在无人驾驶感知方面的前沿研究，包括光流（Optical Flow）和立体视觉、物体的检测和跟踪以及视觉里程计算法。\nKITTI数据集\nKITTI数据集是由KIT和TTIC在2012年开始的一个合作项目，网站在http://www.cvlibs.net/datasets/kitti/，这个项目的主要目的是建立一个具有挑战性的、来自真实世界的测试集。他们使用的数据采集车配备了：\n一对140万像素的彩色摄像头Point Grey Flea2（FL2-14S3C-C），采集频率10赫兹。\n一对140万像素的黑白摄像头Point Grey Flea2（FL2-14S3M-C），采集频率10赫兹。\n一个激光雷达Velodne HDL-64E。\n一个GPS/IMU定位系统OXTSRT 3003。\n\n这辆车在卡尔斯鲁厄的高速公路和城区的多种交通环境下收集了数据，用激光雷达提供的数据作为ground truth，建立了面向多个测试任务的数据集：\nStereo/Optical Flow数据集：如，数据集由图片对组成。一个Stereo图片对是两个摄像头在不同的位置同时拍摄的，Optical Flow图片对是同一个摄像头在相邻时间点拍摄的。训练数据集有194对，测试数据集有195对，大约50%的像素有确定的偏移量数据。\nStereo/Optical Flow数据集\n视觉里程测量数据集：如，数据集由22个Stereo图片对序列组成，一个4万多帧，覆盖39.2公里的里程。\n\n三维物体检测数据集：手工标注，包含轿车、厢车、卡车、行人、自行车者、电车等类别，用三维框标注物体的大小和朝向，有多种遮挡情况，并且一张图片通常有多个物体实例，如。\n\n物体追踪数据集：手工标注，包含21个训练序列和29个测试序列，主要追踪目标类型是行人和轿车，如。\n\n路面和车道检测数据集：手工标注，包含未标明车道、标明双向单车道和标明双向多车道三种情况，289张训练图片和290张测试图片，ground truth包括路面（所有车道）和自车道，如。\n\n和以往计算机视觉领域的数据集相比，KITTI数据集有以下特点：\n由无人驾驶车上常见的多种传感器收集，用LiDAR提供高精度的三维空间数据，有较好的ground truth。\n不是用计算机图形学技术生成的，更加接近实际情况。\n覆盖了计算机视觉在无人驾驶车上应用的多个方面。\n由于这些特点，越来越多的研究工作基于这个数据集开展，一个新的算法在这个数据集上的测试结果有较高的可信度。\n计算机视觉能帮助无人驾驶解决的问题\n计算机视觉在无人车上的使用有一些比较直观的例子，比如交通标志和信号灯的识别（Google）、高速公路车道的检测定位（特斯拉）。现在基于LiDAR信息实现的一些功能模块，其实也可以用摄像头基于计算机视觉来实现。下面我们介绍计算机视觉在无人驾驶车上的几个应用前景。当然，这只是计算机视觉在无人车上的部分应用，随着技术的发展，越来越多的基于摄像头的算法会让无人车的感知更准确、更快速、更全面。\n计算机视觉在无人车场景中解决的最主要的问题可以分为两大类：物体的识别与跟踪，以及车辆本身的定位。\n物体的识别与跟踪：通过深度学习的方法，我们可以识别在行驶途中遇到的物体，比如行人、空旷的行驶空间、地上的标志、红绿灯以及旁边的车辆等。由于行人以及旁边的车辆等物体都是在运动的，我们需要跟踪这些物体以达到防止碰撞的目的，这就涉及到Optical Flow等运动预测算法。\n车辆本身的定位：通过基于拓扑与地标算法，或者是基于几何的视觉里程计算法，无人车可以实时确定自身位置，满足自主导航的需求。\nOptical Flow和立体视觉\n物体的识别与跟踪，以及车辆本身的定位都离不开底层的Optical Flow与立体视觉技术。在计算机视觉领域，Optical Flow是图片序列或者视频中像素级的密集对应关系，例如在每个像素上估算一个2维的偏移矢量，得到的Optical Flow以2维矢量场表示。立体视觉则是从两个或更多的视角得到的图像中建立对应关系。这两个问题有高度相关性，一个是基于单个摄像头在连续时刻的图像，另一个是基于多个摄像头在同一时刻的图片。解决这类问题时有两个基本假设：\n不同图像中对应点都来自物理世界中同一点的成像，所以“外观”相似。\n不同图像中的对应点集合的空间变换基本满足刚体条件，或者说空间上分割为多个刚体的运动。从这个假设我们自然得到Optical Flow的二维矢量场片状平滑的结论。\n在今年6月于美国拉斯维加斯召开的CVRP大会上，多伦多大学的Raquel Urtasun教授和她的学生改进了深度学习中的Siamese网络，用一个内积层代替了拼接层，把处理一对图片的时间从一分钟左右降低到一秒以内。\nSiamese结构的深度神经网络\n如所示，这个Siamese结构的深度神经网络分左右两部分，各为一个多层的卷积神经网络（CNN），两个CNN共享网络权重。Optical Flow的偏移矢量估计问题转化为一个分类问题，输入是两个9x9的图片块，输出是128或者256个可能的偏移矢量y。通过从已知偏移矢量的图片对中抽取的图片块输入到左右两个CNN，然后最小化交叉熵（cross-entropy）：\n我们可以用监督学习的方法训练整个神经网络。\ni是像素的指标。\ny_i是像素i可能的偏移矢量。\np_gt是一个平滑过的目标分布，用来给一两个像素的预估误差反馈一个非0的概率，gt表示ground truth。\np_i (y_i,w)是神经网络输出的给定w时y_i的概率。\n在KITTI的Stereo2012数据集上，这样一个算法可以在0.34秒完成计算，并达到相当出色的精度，偏移估计误差在3-4像素左右，对大于3像素的偏移估计误差在8.61像素，都好于其他速度慢很多的算法。\n在得到每个像素y_i上的分布后，我们还需要加入空间上的平滑约束，这篇文章试验了三种方法：\n最简单直接的5x5窗口平均。\n加入了相邻像素一致性的半全局块匹配（Semi Global Block Matching，SGBM）。\n超像素+3维斜面。\n这些平滑方法一起，能把偏移估计的误差再降低大约50%，这样一个比较准确的2维偏移矢量场就得到了。基于它，我们就能够得到如所示场景3维深度/距离估计。这样的信息对无人驾驶非常重要。\n\n物体的识别与追踪\n从像素层面的颜色、偏移和距离信息到物体层面的空间位置和运动轨迹，是无人车视觉感知系统的重要功能。无人车的感知系统需要实时识别和追踪多个运动目标（Multi-ObjectTracking，MOT），例如车辆和行人。物体识别是计算机视觉的核心问题之一，最近几年由于深度学习的革命性发展，计算机视觉领域大量使用CNN，物体识别的准确率和速度得到了很大提升，但总的来说物体识别算法的输出一般是有噪音的：物体的识别有可能不稳定，物体可能被遮挡，可能有短暂误识别等。自然地，MOT问题中流行的Tracking-by-detection方法就要解决这样一个难点：如何基于有噪音的识别结果获得鲁棒的物体运动轨迹。在ICCV 2015会议上，斯坦福大学的研究者发表了基于马尔可夫决策过程（MDP）的MOT算法来解决这个问题，下面我们就详细介绍这个工作。\n运动目标的追踪用一个MDP来建模（）：\nDMM状态图\n运动目标的状态：s∈S=S_active∪S_tracked∪S_lost∪S_inactive,这几个子空间各自包含无穷多个目标状态。被识别到的目标首先进入active状态，如果是误识别，目标进入inactive状态，否则进入tracked状态。处于tracked状态的目标可能进入lost状态，处于lost状态的目标可能返回tracked状态，或者保持lost状态，或者在足够长时间之后进入inactive状态。\n作用a∈A，所有作用都是确定性的。\n状态变化函数T:S×A→S定义了在状态s和作用a下目标状态变为s'。\n奖励函数R:S×A→R定义了作用a之后到达状态s的即时奖励，这个函数是从训练数据中学习的。\n规则π:S→A决定了在状态s采用的作用a。\n如，这个MDP的状态空间变化如下：\n在active状态下，物体识别算法提出的物体候选通过一个线下训练的支持向量机（SVM），判断下一步的作用是a_1还是a_2，这个SVM的输入是候选物体的特征向量，空间位置大小等，它决定了在S_active中的MDP规则π_active。\n在tracked状态下，一个基于tracking-learning-detection追踪算法的物体线上外观模型被用来决定目标物体是否保持在tracker状态还是进入lost状态。这个外观模型（appearance model）使用当前帧中目标物体所在的矩形（bounding box）作为模板（template），所有在tracked状态下收集的物体外观模板在lost状态下被用来判断目标物体是否回到tracked状态。另外在tracked状态下，物体的追踪使用上述外观模型模板，矩形范围内的Optical Flow和物体识别算法提供的候选物体和目标物体的重合比例来决定是否保持在tracked状态，如果是，那么目标物体的外观模板自动更新。\n在lost状态下，如果一个物体保持lost状态超过一个阈值帧数，就进入inactive状态；物体是否返回tracked状态由一个基于目标物体和候选物体相似性特征向量的分类器决定，对应了S_lost中的π_lost。\n这个基于MDP的算法在KITTI数据集的物体追踪评估中达到了业界领先水平。\n视觉里程计算法\n基于视觉的定位算法有两大分类：一种是基于拓扑与地标的算法，另一种是基于几何的视觉里程计算法。基于拓扑与地标的算法把所有的地标组成一个拓扑图，然后当无人车监测到某个地标时，便可以大致推断出自己所在的位置。基于拓扑与地标的算法相对于基于几何的方法容易，但是要求预先建立精准的拓扑图，比如将每个路口的标志物做成地标。基于几何的视觉里程计算法计算比较复杂，但是不需要预先建立精准的拓扑图，这种算法可以在定位的同时扩展地图。以下着重介绍视觉里程计算法。\n视觉里程计算法主要分为单目以及双目两种，纯单目的算法的问题是无法推算出观察到的物体的大小，所以使用者必须假设或者推算出一个初步的大小，或者通过与其它传感器（如陀螺仪）的结合去进行准确的定位。双目的视觉里程计算法通过左右图三角剖分（Triangulation）计算出特征点的深度，然后从深度信息中推算出物体的大小。展示了双目视觉里程计算法的具体计算流程：\n双目摄像机抓取左右两图。\n双目图像经过Triangulation产生当前帧的视差图（Disparity Map）。\n提取当前帧与之前帧的特征点，如果之前帧的特征点已经提取好了，那么我们可以直接使用之前帧的特征点。特征点提取可以使用Harris Corner Detector。\n对比当前帧与之前帧的特征点，找出帧与帧之间的特征点对应关系。具体可以使用随机抽样一致（RANdom Sample Consensus，RANSAC）算法。\n根据帧与帧之间的特征点对应关系，推算出两帧之间车辆的运动。这个推算是最小化两帧之间的重投影误差（Reprojection Error）实现的。\n根据推算出的两帧之间车辆的运动，以及之前的车辆位置，计算出最新的车辆位置。\n通过以上的视觉里程计算法，无人车可以实时推算出自己的位置，进行自主导航。但是纯视觉定位计算的一个很大问题是算法本身对光线相当敏感。在不同的光线条件下，同样的场景不能被识别。特别在光线较弱时，图像会有很多噪点，极大地影响了特征点的质量。在反光的路面，这种算法也很容易失效。这也是影响视觉里程计算法在无人驾驶场景普及的一个主要原因。一个可能的解决方法，是在光线条件不好的情况下，更加依赖根据车轮以及雷达返回的信息进行定位，这将会在后续的文章中详细讨论。\n结论\n在本文中，我们探索了基于视觉的无人驾驶感知方案。首先，要验证一个方案是否可行，我们需要一个标准的测试方法。为此我们介绍了无人驾驶的标准KITTI数据集。在有了标准的数据集之后，研究人员可以开发基于视觉的无人驾驶感知算法，并使用数据集对算法进行验证。然后，我们详细介绍了计算机视觉的Optical Flow和立体视觉、物体的识别和跟踪与视觉里程计算法等技术，以及这些技术在无人驾驶场景的应用。视觉主导的无人车系统是目前研究的前沿，虽然目前各项基于视觉的技术还没完全成熟，我们相信在未来五年，如果LiDAR的成本不能降下来，基于摄像机的视觉感知会逐步取代LiDAR的功能，为无人车的普及打好基础。\n从计算机视觉的角度，无人驾驶可能是一次难得的机遇，无人车产业爆发带来的资源、无人车收集的大量真实世界数据和LiDAR提供的高精度三维信息可能意味着计算机视觉将要迎来“大数据”和“大计算”带来的红利，数据的极大丰富和算法的迭代提高相辅相成，会推动计算机视觉研究前进，并使之在无人驾驶中起到更加不可或缺的作用。\n无人驾驶技术系列：\n光学雷达（LiDAR）在无人驾驶技术中的应用\n基于ROS的无人驾驶系统\n基于计算机视觉的无人驾驶感知系统"}
{"content2":"之前一直使用opencv，觉得opencv还不错，关注的人多，相关资料丰富，但是opencv的一些Vision相关的高级算法实现基本都不是最好的。\n前几天，发现一个比较好用的计算机视觉库——VLFeat。VLFeat比opencv是一个比opencv更适合做产品开发的质量比较高的库，结构和逻辑要更清晰一些、更容易读懂。\n关于VLFeat的配置，可参考 http://blog.sina.com.cn/s/blog_7c7b71530100to24.html 和 http://www.vlfeat.org/vsexpress.html ，在MATLAB中的配置可参考http://www.cnblogs.com/woshitianma/p/3872939.html和http://www.vlfeat.org/install-matlab.html。"}
{"content2":"前言\n注意！注意！注意！本文是针对中国科学院计算所山世光教授于2017年1月7日于北京师范大学所做的《深度学习在计算机视觉中的应用与前景》讲座的内容总结梳理。\n1 视觉智能的内涵\n计算机视觉系统的任务就是像人一样描述摄像机拍摄到的内容。\n常见的视觉任务：\n距离估计\n目标检测与跟踪\n物体分割\n目标识别\n内容理解\n下图所示就是计算机视觉任务中的物体识别的一个例子，如单一物体（图中猫）的识别，或多物体的识别（如图中的狗、猫和鸭子等的识别）。\n从2012年到2016年，计算机视觉经历了跨越式发展。在ImageNet ILSRVRC 图像分类上 1000类Top5错误率：26%-> 3.6%。\n2 视觉跨越式发展源于深度卷积网络CNN\n2.1 计算机视觉的基本任务\n针对待检测图片，识别出图片中的目标物体对象；\n对图片内容进行理解，实现图片的语义分割。\n处理的流程图如下：\n2.2 深度学习的起源——生物神经网络\n深度学习的基本原理是“加权投票模型”，它来源与生物神经学系统中的神经元系统的启示。\n如图所示，生物学中，一个完成的神经元主要包括轴突和树突两大部分，神经信号的传递主要是轴突的神经末梢受体释放后经过突触间隙被树突的受体捕获，产生电位传递给胞体。胞体将获得的电信号进行汇总给出决策：产生激励信号或者抑制。\n最后整个大脑的决策就是所有神经元信号的加权投票决策。\n2.3 人工神经网络\n正是受到生物神经网络的启发，我们在计算机中构建人工神经网络模型。\n2.3.1 单一神经元模型\n加权求和（卷积）+ 非线性激活函数\n如图所示，我们的输入信号\nx1,x2,⋯,xn\nx_1,x_2, \\cdots,x_n 可以看作是神经元中来自其他神经元轴突的信号，对应的\nwk1,wk2,⋯,wkn\nw_{k1} , w_{k2}, \\cdots, w_{kn} 可以看作是对应的突触的信号的权值，中间的求和结点\n∑ni=0(wixi+b)\n\\sum_{i=0}^{n} (w_ix_i+b) 是对应细胞体的求和决策，再通过一个激活函数\nψ(⋅)\n\\psi(\\cdot) 输出对应的决策\nyk\ny_k 。\n激励函数\n我们常用的激活函数有Sigmoid 和tanh 等函数，因为这些函数有光滑、连续、可导、有界等优良的性质。\n2.3.2 人工神经网络\n一个神经元看的更远（视野更大、更宽）知识因为它站在了其他神经元的肩上。\n神经网络是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。\n如图所示，对于一幅图像的特征，low-level 是一些简单、底层的特征（如像素特征），到了mid-level 就是一些复杂的特征（如图像中的一些边之类的特征），到了high-level 就是更复杂的特征（如图像中边的组合特征）。处在high-level 的神经元此时它们的决策就更抽象重要。\n神经网络的训练\n网络结构认为确定\n训练集：“有标注” 的样本\n{Xk,Yk,k=1,2,⋯,N}\n\\{ {\\bf X}^k, {\\bf Y}^k,k=1,2,\\cdots,N\\}\n学习目标——权重值：每个神经元都有大量的\nwi\n{\\bf w}_i 需要学习\n学习方法——BP算法：基本原理：调整权重，使最后层输出错误率下降:\nerror=||Yo−Yg||\nerror = ||{\\bf Y}^o- {\\bf Y}_g||\n2.4 卷积神经网络（CNN）\n卷积神经网络主要层叠包括以下三个步骤+全连接层\n卷积层：局部连接（卷积、滤波器…)\n非线性激活\nPooling 层：下采样，降维\n下图是一个经典的CNN结构，称为LeNet-5网络，关于LeNet 的详解可以参考 LeNet-5 网络详解 。LeNet-5 网络共有7层（不包括输入层），每层都包含不同数量的训练参数，主要的有卷积层、下抽样层、全连接层3中连接方式。从中可以看出，CNN中主要有两种类型的网络层，分别是卷积层和池化/采样层（Pooling）。\n其中卷积层的作用是提取图像的各种特征；池化层的作用是对原始特征信号进行抽样，从而大幅减少训练参数，另外还可以减轻模型过拟合的程度。\n2.4.1 卷积层\n卷积层是卷积核在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，（通常还要再加上一个偏置参数），得到卷积层上的结果。如下图所示。\n下面的动图能够更好地解释卷积过程：\n从图中我们知道卷积的实质是局部连接（卷积、滤波器）、共享权重（若干种滤波器，大大减少学习的权重数），等价于信号中的滤波器。\n2.4.2 非线性激活\n在卷积神经网络中，我们采用的是Sigmoid 等非线性函数，原因是如果是线性激励，无论神经网络的层数有多少，最终都会坍塌为一个线性激励。\n2.4.3 池化/采样层（Pooling）\n通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化/采样(Pooling)处理。池化/采样的方式通常有以下两种：\nMax-Pooling: 选择Pooling窗口中的最大值作为采样值；\nMean-Pooling: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值；\n如下图所示：\n使用Pooling 技术的另一个目的也是为了获取不变性：\n小的平移不变性：有即可，不管在哪里\n旋转不变性：9个不同朝向的filters。\n2.4.4 CNN结构演化——新的功能模块\nCNN+RNN/LSTM\n其中CNN学习表示， RNN/LSTM 建模时序信号\n应用： Image Caption， Image QA， Viedo Representation\n上图所示：Show and tell: A neural image caption generator。\n上图所示： Long-term Recurrent Convolutional Networks 。\n深度神经网络已经由传统CNN 向RNN（CVPR14， TPAMI15）和 CNN Cascade 演进。\n3 计算机视觉应用——人脸识别系统\n全自动人脸识别系统流程：\n深度学习主要作用在特征提取器这一步上。\n特征提取器\n第一代：完全人工涉及特征\n也是非常底层的特征，包括几何特征，图像模板，Fourier 频谱，如形状，颜色，纹理，频谱等\n第一代特征是知识驱动\n第二代：（子空间）变换特征\n此时的特征是经过处理或变换后的特征，经过如PCA， LDA， LPP， SR 等特征变换处理后得到的特征。\n第二代特征是数据驱动（学习\nW\n\\bf W 矩阵，\ny=Wx\n\\bf y = Wx ）。\n第三代： 人工设计局部特征+ （子空间）变换特征\n如Gabor滤波器，LBP+PCA， LDA等\n第三代特征是知识+数据驱动，\ny=W(f(x))\n\\bf y = W(f(x)) 。\n第四代：特征学习\n局部特征参数可学习\n变换可学习\n非线性\n第四代特征是完全数据驱动的特征。\n卷积神经网络（CNN） 本质上是层次抽象的滤波型局部特征。\n其与之前局部特征的不同：\nGabor ：权值固定，人为设定（如加窗傅里叶型函数），没有目标函数\nCNN： 数据驱动的权值学习（最有利于目标函数达成的）。\n如图所示，理想的深度卷积神经网络是从数据中学习多层特征，如对图像中人脸的学习，可从底层的像素特征学习到第一层的边的特征，然后再到第2层基本脸部部位器官（如鼻子，嘴）等特征，再到高维的特征脸特征。\n4 总结\n自从深度学习与深度卷积神经网络取得了重大的突破，其给计算机视觉领域带来巨大的性能提升，深度模型极大推进了人脸识别能力，使得人脸的检测与识别不再具有特殊性，其甚至在一些场景上超过了人眼能力。\n深度学习本质上是特征的学习，它也是隐式学习，借鉴的是生物神经元的思维模式，虽然对于学习到的模型在理论上无法做出清晰明显的解释，但是其极高的性能提高能力还是令相关领域的专家学者报以很高的学习热情。\n深度学习也由一些不足之处：如学习模型的计算量大，缺少基础理论的支撑等。"}
{"content2":"本文为数盟原创译文，转载请注明出处为数盟社区\n在这个月经系列的第四个问题中，你会发现Qualcomm是如何把深度学习和AI引向智能设备，为什么戴姆勒发送自驾卡车到所有欧洲各地，如何模仿伦勃朗在深度学习的帮助下最好的工作，还有更多。\n下一个伦勃朗\n从史密森传来的消息，一个必须要看的迷人的视频，关于使用伦勃朗的作品中超过168,000个数据片段，并按照伦勃朗的标志性风格培养而创作的一幅画。在18个月的过程中，一组工程师，伦勃朗专家和数据科学家分析了伦勃朗的346幅作品，然后培养深度学习机器，在主人的招牌式风格下的“作画”。\n为了保持伦勃朗艺术的真实性，团队决定在肖像上弯曲发动机的肌肉。他们分析了伦勃朗一生所画的人民的人口特征，确定它应该绘制的是一个30岁到40岁之间的白人男性，全身黑色的衣服，白色的衣领和帽子，面部有毛发。\n机器使用它所了解的伦勃朗的风格和他从几何到油漆使用的一切，生产了一个2D的艺术作品，这可能是由荷兰画家本人创作的。但事情并没有就此结束，然后，该团队使用了高度的三维技术扫描了伦勃朗的画作来模仿他的笔触。使用3D打印机和高度图，它们印刷了13层颜料。最终结果是，总共148万像素的它，看上去非常像伦勃朗活着时候作的画，如果你在参观他的作品集时径直通过它，你会被原谅的。\n但是为什么做这一切？罗恩奥古斯都，微软在中小企业的市场总监，把它放在了视频中，“我们使用了大量的数据，以提高企业的生命，但我们还没有使用很多以触动人灵魂的方式存在的数据”。该项目成功地展示了深度学习是如何创造一些与一个人的工作惊人的相似的产品，而丝毫不提及艺术天才。也就是说，实验没有声称是伦勃朗的替代品，而这幅画背后的团队也小心的避免直接比较。\n资料来源：Smithsonian, Slate.\nQualcomm发布零度深度学习SDK\n本月初，Qualcomm为他们的“机器智能平台”零度发布了一款新的软件开发工具包（SDK）。该SDK将使企业更容易的直接在智能手机和无人驾驶飞机等设备上运行深度学习计划，当然这些设备是由Qualcomm公司中的一个芯片提供支持。\n“这意味着更好的私密性和更低的延迟性，因为没有上传到云中，”高通公司的产品管理主管，加里·布洛特曼告诉Verge。他可以提供医疗应用的例子，以帮助医生来分析皮肤状况。 “在设备上做不上云的图像分类是有意义的，”布洛特曼说。 “数据类型并不重要，它可能是视觉的，也可能是声音。”\n虽然在本地运行深度学习操作意味着限制其复杂性，但是您可以在手机或其他便携设备上运行得这种程序仍然会令人印象深刻。真正的限制将是Qualcomm的芯片。新的SDK将只适用于2016年下半年最新的Snapdragon820处理器，而且公司并没有透露它是否计划扩大可用性。\nQualcomm加入了其他公司，例如一直在与谷歌合作生产专门从事机器视觉处理器的芯片制造商Movidius。 Movidius的无数的芯片现在支持第一代谷歌的空间映射Tango平板电脑，最近，正在从DJI进军自主无人机。 Movidius的成功表明，甚至有专门的深度学习芯片的市场。时间会告诉我们这项技术将如何抗衡于Qualcomm更多的通才的做法。在任何情况下，智能手机和其他移动小工具在未来只会更聪明。\n来源: Qualcomm, The Verge.\n国家机器人周\n对于美国机器人来说四月是个大月，因为这个领域内的人主办了第七届全国机器人周，为任何对机器人技术感兴趣的人提供了众多机器人的活动、讲座及研讨会。机器人周的目的是在这个领域快速向前发展而又存在很多困难的时候，积极鼓舞人们参与。尤其是对于孩子，不同的活动为孩子们提供了一个关于什么是机器人的难得的见识，也为他们展示了这其中有多少的乐趣。\n活动在全国各地举办，一组在加利福尼亚州，机器人花园，站出来作为主办者开展了大多数活动，从一个乐高机器人俱乐部到机器人交换事件等。如果你错过了，不要担心，机器人花园举一整年都会举办活动，只要注意查看他们的网站。\n新20TB数据集自动驾驶研究\n移动机器人集团在英国牛津大学已经发布了这一年多的时间过程中，他们用RobotCar平台收集的自主汽车研究的一个巨大的数据集。该数据集包含价值20TB的立体声、单声道、激光雷达和GPS数据，这些数据来自于重复100次的通过英国牛津的持续10公里路线，根据天气、交通、行人以及建筑和道路施工的不同组合而得出。这导致大约一千公里的行车记录，与来自6台安装在车辆上的摄像机收集的超过20万张图片。\n该数据集加入了一些基于其他视角的自主驾驶的数据集，如KITTI和都市风景的。然而，并没有数据集解决长期自治的挑战，如显著不同条件下的定位与地图在城市环境和映射中随着时间推移的情况下结构的变化。该数据集应该在2016年中旬完成。\n来源: Mobile Robotics Group at Uni Oxford, UK.\n杂项\n研究人员在华盛顿2016年海洋可持续发展峰会会晤，讨论清洁我们的海域的解决方案。合并溶液与AI（Aquaai），Simeon的硅谷机器人的开始呈现了机器鱼的原型 ——一条看起来像尼莫的小丑鱼，它将点亮并发送水面下的鸣叫图像。该仿生车辆（BIV）可以用于钻井平台和船体的检查。由于它的专利（专利申请中）鳍推进系统，它使用非常小的外部电源。 （RoboHub）\n瑞士的Kickstart的加速器项目目前接受来自早期阶段的创业公司制作的“智能连接设备”的应用。它们特别（但不排他）有意在智能家庭、运输、移动和物流的领域的应用。这个3个月的计划运行在夏季从8月下旬到11月，以选定的团队提供指导、专业知识、高达25K瑞士法郎的种子资金、生活津贴和办公空间。该方案在演示日，在每次启动时介绍他们公司的瑞士风险投资家、企业领袖和记者。\n作为欧洲卡车Platooning挑战的一部分，戴姆勒让三名自主车行驶在德国和荷兰的道路上。卡车从斯图加特驱车前往鹿特丹使用该公司的关连公路试验系统，平安的于4月7日达到他们的目的地。（戴姆勒）\n赛车无人机正在成为一个“运动”。从今年八月开始，ESPN将通过网络播出第一个美国国家赛车无人机锦标赛。（ESPN）\n原文链接：\nhttp://www.askaswiss.com/2016/05/highlights-discoveries-computer-vision-machine-learning-ai-2016-04.html"}
{"content2":"一、什么是人工智能\n人工智能是人们心中美好的设想，目的是希望通过当时新兴的计算机，打造拥有相当于人类智能的复杂机器。这就是我们所说的“通用人工智能”（General AI）概念，拥有人类五感（视觉，听觉等）、推理能力以及人类思维方式的神奇机器。\n人工智能的应用场景：\n二、什么是机器学习\n机器学习就是通过算法，使得机器能从大量历史数据中学习规律，从而对新的样本做智能识别或对未来做预测。\n它利用大量现有的数据（大数据）对机器进行训练，使得机器在面对下一次实验测试的时候能够根据既有的经验对数据进行区分（比如是猫还是狗）。\n机器学习分为监督式学习，非监督式学习，强化学习\n常用的机器学习算法有：\n线性回归\n逻辑回归\n决策树\nSVM\n朴素贝叶斯\nK最近邻算法\nK均值算法\n随机森林算法\n降维算法\nGradient Boost 和 Adaboost 算法\n决策树：根据一些 feature 进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些问题是根据已有数据学习出来的，\n再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上\n三、什么是深度学习\n那机器是怎么学习到这个规则（什么样的是猫？）的呢？\n没错，是通过机器学习算法。而神经网络，恰好就是一种机器学习算法。\n其实神经网络最初得名，就是其在模拟人的大脑，把每一个节点当作一个神经元，这些“神经元”组成的网络就是神经网络。而由于计算机出色的计算能力和细节把握能力，在大数据的基础上，神经网络往往有比人有更出色的表现。深度学习算法并不是直接通过将输入映射到输出的方式，而是依赖于几层处理单元。 每个层将其输出传递到下一个层，进行处理，然后再传递到下一层。 在某些模型中，计算可能会在处理层之间来回流动多次。现已证明深度学习在各种任务中非常有效，包括图像字幕，语音识别和语言翻译。\n四、如何入门人工智能\n选择方向：1、算法应用。专注于了解业务以及算法的使用场景。　　2、开发算法。关注数学推导能力以及编程技巧。\nStep1：了解行业资讯，先来一波科普\nStep2：务实基础—\n高数：线性代数和微积分，统计学相关基础\nPython: 重点关注爬虫，数值计算，数据可视化方面的应用\nStep3：机器学习算法+实践\n机器学习常见的问题分为三种，分类，聚类，回归\n分类：KNN算法，决策树，朴素贝叶斯，支持向量机，逻辑斯蒂回归\n聚类：K-mean\n回归：决策树，朴素贝叶斯，向量机\n实践方法：\n这里需要选择一个应用方向，是图像(计算机视觉)，音频(语音识别)，还是文本(自然语言处理)。这里推荐选择图像领域，这里面的开源项目较多，入门也较简单，可以使用OpenCV做开发，里面已经实现好了神经网络，SVM等机器学习算法。项目做好后，可以开源到到 Github 上面，然后不断完善它。实战项目做完后，你可以继续进一步深入学习，这时候有两个选择，深度学习和继续机器学习\nExample:\n数据分析流程：\n1、问题定义\n2、数据获取\n3、数据预处理\n4、数据分析与建模\n5、数据可视化及数据报告的撰写\nStep4：深度学习\n十种深度学习算法要点及代码常见如下博客：\nhttps://blog.csdn.net/northhan/article/details/72724058"}
{"content2":"深度学习是大数据下最热门的趋势之一。上一个十年是互联网的时代，下一个十年将是人工智能的时代。国内已经产生大量的人工智能公司，分布在不同的领域。2016年是人工智能的元年，2017年将迎来“人工智能+”的春天。\n未来几年也是人工智能在金融、医疗、教育等大数据行业以及感知交互领域全面渗透的时期，我们正迎来人工智能产业应用百花齐放的时代。安防、金融、医疗、汽车、教育、信息安全、零售等行业电子化程度高、数据较集中且数据质量较高，因此这些行业将会略先涌现大量的人工智能场景应用。\n人脸识别\n商汤科技\nhttp://www.sensetime.com/\n格灵深瞳\nhttp://www.deepglint.com/\n项目介绍：格灵深瞳是一家计算机视觉与人工智能公司，致力于让计算机看懂世界，用广泛的视觉传感器网络，构建真实世界的搜索引擎。\n格灵深瞳创立于2013年初，联合创始人、CEO何搏飞毕业于斯坦福大学商学院，曾先后成功创建了两家美国公司中国区的团队和业务，29岁就担任了美国上市公司中国区总经理。联合创始人、CTO赵勇博士毕业于布朗大学计算机工程系，是前Google研究院高级研究员。\nLinkFace\nhttps://www.linkface.cn/about.html\nLinkface，全球领先的人脸检测技术服务。\n旷视科技\nhttps://www.megvii.com/\nface++\nhttps://www.faceplusplus.com.cn/\n云从科技\nhttp://www.cloudwalk.cn/\n格灵深瞳\nhttp://www.deepglint.com/\n2016年在CVPR上面的视频\nhttps://www.youtube.com/watch?v=xhp47v5OBXQ\n中科视拓\nhttp://www.seetatech.com/\n图普科技\nhttps://www.tuputech.com/\n图普科技是一家将图像识别技术应用于企业服务领域的人工智能科技公司，率先推出基于图像识别技术的第三方内容审核服务，在识别色情、暴恐、时政敏感信息、小广告等违规图片和视频方面居于领先水平，市场占有率处于领先地位。目前图像识别云平台还提供了人脸识别、OCR、场景识别、物体识别、艺术滤镜、图像搜索等技术服务。图普科技于今年9月份完成了新一轮融资，金额为千万美元。\n依图科技\nhttp://www.yitutech.com/\n极视角\nhttp://www.extremevision.mo/\n视频监控\n海康威视\nhttp://www1.hikvision.com/cn/index.html\n浙江大华技术股份有限公司\nhttp://www.dahuatech.com/\n宇视科技\nhttp://cn.uniview.com/\n自动驾驶\nminieye\n地点：深圳总部 南京研发中心\n做ADAS\nhttp://www.minieye.cc/home\n医学图像\n推想科技\nhttp://www.infervision.com/\n汇医慧影\nhttp://www.huiyihuiying.com/idoctor/html/newLoginVersion2/index.html\nDeepcare\nCEO:丁鹏，DeepCare创始人兼CTO。\nhttp://www.deepcare.com/\n健培科技\nhttp://www.jianpeicn.com/\n智影医疗\nhttp://www.zying.com.cn/#page5\n图玛深纬\nhttp://tumashenwei.langye.net/news/copnew/2016-11-28/61.html\n连心医疗\nhttp://www.linkingmed.com/\n拍医拍 机器视觉融合大数据医疗\nhttp://www.91paiyipai.com/\n无人驾驶\n图森\nhttp://www.tusimple.com/\n京东无人驾驶\n滴滴无人驾驶\n百度无人驾驶\n乐视无人驾驶\nhttps://cn.ff.com/zh-hans/ff-91/\n游侠汽车\nhttp://www.youxiamotors.com/\n游侠汽车又名上海修源网络科技有限公司，是国内第一家发布完整纯电动汽车产品的新兴科技企业，于2014年4月在中国最大的汽车科技产业中心——上海成立，是一家集整车研发、设计、生产、销售、服务为一体的新能源汽车综合制造企业。公司致力于用尖端科技打造“高性能、豪华、智能”的高端电动汽车，以领先于行业标准的高端技术，推动智能交通科技发展的进程。\n蔚来汽车\nhttp://www.nio.com/formulae\nNEXTEV蔚来汽车[1] 是一家从事高性能智能电动汽车研发的公司，由顶尖互联网企业和企业家投资数亿美金所创建，包括领先的互联网企业腾讯、易车创始人李斌、汽车之家创始人李想、京东创始人刘强东，及知名投资机构高瓴资本共同发起设立。蔚来汽车已在美国硅谷、德国慕尼黑、中国上海、北京、香港和英国伦敦设立了研发、设计及商务机构。\n生活\n衣+\nhttp://www.dress-plus.com/\n其它\n码隆科技\nhttp://www.malongtech.cn/\n诺亦腾科技\nhttp://www.noitom.com.cn/\n图漾科技\nhttp://www.percipio.xyz/\n速感科技\nhttp://www.qfeeltech.com/page/\n彩云天气\n人工智能分钟级天气预报\nhttp://www.caiyunapp.com/\n小萝卜机器人\nhttp://www.luobotec.com/\n一款人工智能系统的早教启蒙机器人\n图灵机器人\nhttp://www.tuling123.com/\n蓦然认知\nhttp://mor.ai/\n第四范式\nhttps://4paradigm.com/homepage.html#myCarousel\n项目介绍：2014年，一群80后的奶妈奶爸，怀着共同的创业梦想—为小孩打造一款超级伙伴机器人，成立了萝卜科技团队。一年半的时间里，团队合力研究幼儿领域的产品细节和市场需求点，经过反复的测试、实验，一款支持对话畅聊、DIY运动编程、早教学习的机器人“小萝卜”终于诞生。\nBAT\n百度深度学习实验室 IDL\nhttp://idl.baidu.com/index.html\n阿里巴巴 iDST\n海康威视研究院\n网易人工智能事业部\nhttp://ai.163.com/#/m/overview/\n腾讯优图\nhttp://youtu.qq.com/\n2016年深度学习行业分析报告\nhttp://wenku.baidu.com/link?url=VW0Yg4r_f0UFJ19AxIbKQBdURp-xx6LE9snAiCVBOhEjSBGVVylcsXiuvMo9sKWLBAw-g9PuLMNgXbNYFNNkgvjWUqGukR94R5shaSrPbR7\nhttp://www.iyiou.com/p/37045\n视觉识别与机器视觉公司2017趋势：迎来一波热钱、差异化竞争\nhttp://www.iyiou.com/p/36936"}
{"content2":"转自：[原文](http://blog.csdn.net/hanlin_tan/article/details/50447895)\n搞了CV一段时间，仍时不时因为概念问题而困惑，搞不清楚计算机视觉(Computer Vision)，计算机图形学(Computer Graphics)和图像处理(Image Processing)的区别和联系。在知乎上看到了一个帖子，觉得解释的很好，结合自己的理解，形成此文存档。\n1.基本概念\n从定义理解概念是最严谨的。所以首先搞清楚维基百科中这些概念的定义。\n计算机视觉(CV)：\nComputer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1]\n直译过来就是\n计算机视觉是一个学科/领域，它包括获取、处理、分析和理解图像或者更一般意义的真实世界的高维数据的方法；它的目的是产生决策形式的数字或者符号信息。\n计算机图像学(CG)：\nComputer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.[2]\n直译过来就是\n计算机图形学是计算机科学的一个子领域，它包括数字合成和操作可视内容（图像、视频）的方法。尽管这个术语通常指三维计算机图形学的研究，但它也包括二维图形学和图像处理。\n图像处理（IP）：\nIn imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image.[3]\n直译过来就是\n在图像科学中，图像处理是用任何信号处理等数学操作处理图像的过程，输入时图像（摄影图像或者视频帧），输出是图像或者与输入图像有关的特征、参数的集合。\n2.区别和联系[4]\n2.1 精简的概括\nComputer Graphics和Computer Vision是同一过程的两个方向。Computer Graphics将抽象的语义信息转化成图像，Computer Vision从图像中提取抽象的语义信息。Image Processing探索的是从一个图像或者一组图像之间的互相转化和关系，与语义信息无关。\n2.2 从输入输出角度看\n(1)区别\nComputer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb 颜色等。输出的是图像，即二维像素数组。\nComputer Vision，简称 CV。输入的是图像或图像序列，通常来自相机、摄像头或视频文件。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。\nDigital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop 中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。\n(2)联系\nCG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。\nCV 更是大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理。\n最后还要提到近年来的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。\n(3)图解\n这里还有一张图，简明地表达了CV、CG、DIP和AI的区别和联系。\n2.3 从问题本身看\n(1)区别\n从问题本身来说，这三者主要以两类问题区分：是根据状态模拟观测环境，还是根据观测的环境来推测状态。假设观测是Z，状态是X：Computer Graphics是一个Forwad Problem (Z|X)： 给你光源的位置，物体形状，物体表面信息，你如何根据已有的变量的状态模拟出一个环境出来。\nComputer Vision正好相反，是一个Inverse Problem (X|Z)：你所有能得到的都是观测信息(measurements), 根据得到的每一个Pixel的信息(颜色，深度)，我要来估计物体环境的特征和状态出来，比如物体运动(Tracking)，三维结构（SFM）,物体类别（Classification and Segmentation）等等。\n对于Image Processing来说，它恰好介于两者之间，两种问题都有。但对于State-of-art的研究来说，Image Processing更偏于Computer Vision, 或者看上去更像Computer Vision的子类。尽管这三类研究中，随着CV领域的不断进步，以及越来越高级相机传感器出现（Depth Camera, Event Camera），很多算法都被互相用到，但是从Motivation来看，并没有太大变化。\n(2)联系\n得益于这几个领域的共同进步，所以你能看到Graphics和Computer Vision现在出现越来越多的交集。如果根据观测量（图片），Computer Vision可以越来越准确的估计出越来越多的变量，那么这些变量套到Graphics算法中，就可以模拟出一个跟真实环境一样的场景出来。\n与此同时，Graphics需要构建更真实的场景，也希望能够将变量更加接机与实际，或者通过算法估计出来，这就引入了Vision的动机。这也是近年来三维重建算法，同时大量发表在Graphics和Vision的会议的原因。随着CV从2D向3D发展，以后两者的交集会越来越大，除了learning以外的其他很多问题融合并到一个领域我也不会奇怪。\n参考文献\n[1] https://en.wikipedia.org/wiki/Computer_vision\n[2]https://en.wikipedia.org/wiki/Computer_graphics_(computer_science)\n[3] https://en.wikipedia.org/wiki/Image_processing\n[4] 张静, 知乎,\nhttp://www.zhihu.com/question/20672053/answer/15854031\n(function () {(function () { ('pre.prettyprint code').each(function () { var lines =\n(this).text().split(′\\n′).length;var\n(this).text().split('\\n').length; var numbering = $('\n').addClass('pre-numbering').hide();\n(this).addClass(′has−numbering′).parent().append(\n(this).addClass('has-numbering').parent().append(numbering); for (i = 1; i"}
{"content2":"https://www.toutiao.com/a6646959085440729608/\n行业级最先进的 计算机视觉技术\n如今，人工智能在工业领域有着蓬勃发展趋势，因为自动化以及优化仍是数字革命的主要焦点。\n在本文中，我们将回顾近几年在AI社区中那些令人兴奋的最先进的计算机视觉技术，这些技术被认为是工业就绪的，而且对工业用例产生重大而又实际的影响。\n其中一些技术对性能的提升达到了令人难以置信的程度，超越了人类能达到的性能水平，从而超出了大多数行业所期望的精度和可靠性标准。\n在基本的计算机视觉任务（例如图像分类）中取得的惊人进步，使得可靠地结合多种技术来创建新的复合技术从而实现之前从未在工业环境中探索过的全新用例成为可能。\n话虽如此，这些新技术已经证明其结果可与那些只能通过非常密集的硬件专用系统才能获得的精度和可靠性结果相媲美。虽然在实现这些专用系统和安装与之相关的硬件方面存在实际的困难和限制，但相机是很容易买到的，从而极大地扩大了用例范围。\nAI赋能的计算机视觉系统使得有可能跨入到一个新的领域，加速了工业4.0，真正数字化和物理现实增强的进程。\n在我们深入了解计算机视觉领域的最新进展之前，让我们先介绍一些基本概念以及深度学习和计算机视觉这方面的历史事件。\n计算机视觉是什么？\n计算机视觉是一门科学，旨在使计算机能够理解并从图形和视频中洞悉信息。计算机视觉，即自动执行视觉任务的能力，例如从图形或视频中提取和分析有用的信息。\n机器学习和深度学习的关系！\n机器学习是算法和统计模型的科学研究，它依赖于数据驱动的方法来做决策而不是基于规则的方法。给定大量高质量数据并通过改进算法，机器学习系统能够逐步提高其在特定任务上的性能。\n深度学习是机器学习的子类，完全侧重于一组可描述为网络的数学算法。它们起初受到人脑中发现的生物神经网络的启发，同样，人工神经网络具有数百万个人工突触，数学上由数百万个简单的线性代数方程表示。\n深度学习驱动计算机视觉\n自2012年深度学习神经网络一直是计算机视觉的主要关注点是有理由的。由深度学习驱动的计算机视觉系统的优点是它们具有更高准确性，更灵活，且对大量的光线条件变化，视点，尺度，方向，与背景融合，类内差异，变形以及视觉遮挡等情况具有更高容忍度。但最重要的是，它们启发了新的用例。\n早期的计算机视觉模型依赖于原始像素数据作为机器学习模型的输入。然而，单独的原始像素数据不足以包含图像中对象的千变万化。\n深度学习驱动的计算机视觉基于深度神经网络可在训练阶段自动提取和创建特定任务的特征，然后将其用于执行计算机视觉任务。\n下图突出了深度学习和计算机视觉近6年历史中最重要的一些事件。\n2012年引入深度神经网络所带来的突破使得图像分类误差减少了约10%（从2011年的25.8%降至2012年的16.4%）。\n2015年最先进的算法在图像分类方面的表现超过了人类水平（5.1%，Russakovsky et al.）,准确率为3.57%。\n总体而言，深度神经网络的引入导致图像分类误差减少10倍（从2011年的25.8%将至2017年的2.3%）。\n值得注意的是，上述结果是在ImageNet数据集上实现的，其中20,000个类别具有典型类别，例如“气球”或“草莓”，由数百个低分辨率469x387像素图像组成。计算机视觉系统应用于具有较少类别，较少变化和较多数量的较高分辨率图像的特定任务时，其准确度可以高达99.9%。这使得完全独立自信地运行一个系统成为可能。\n详细了解计算机视觉技术\n现在我们已经介绍了基础知识，我们可以更详细地了解这些技术了。\n图像分类\n在本节中，我们将介绍图像分类，这是将一组固定类别中的一个标签分配给图像的任务。这是计算机视觉中的核心问题之一，尽管其简单，但其具有各种各样的实际应用。许多其它看似不同的计算机视觉任务（例如图像 字幕，目标检测，关键点检测和分割）可以简化为图像分类，其它任务利用全新的神经网络架构。以下视频片段说明了一个非常简单的分类事例。\n图像关键字和字幕\n该技术处于计算机视觉和自然语言处理（NLP）这两AI中最有趣领域的交点。关键字是用于描述照片或图像元素的单词。关键字是对照片添加描述性术语的过程。\n图像字幕是指基于图像中的对象和动作从图像或视频生成文本描述的过程。在下图中可以看到这方面的一个例子。\n目标检测\n目标检测是一种计算机视觉技术，用于识别和定位图像或视频中的对象。这通常通过带边框标记的框包围对象来完成。目标检测是自动驾驶汽车背后的关键技术，使它们能够识别其他汽车或区分行人与灯柱。它还可以用于各种应用，例如工业检测和机器人视觉。由于ImageNet竞赛，仅2010年至2014年间，定位误差（从42.5%降至25.3%）就减少了1.7倍。下面的视频片段显示了该技术的实时实施结果，用于检测城市中发现的与一辆自动驾驶视觉系统相关的车，人以及其他常见物体。\n关键点检测和姿态估计\n关键点被视为图像有趣或重要部分的特征。它们是图像中的空间位置或点，定义图像中有趣的内容或突出的内容。关键点之所以特殊，是因为它使得跟踪修改后的图像中的相同关键点成为可能，其中图像或图像中的对象会发生旋转、收缩/膨胀或变形。\n姿态估计是计算机视觉中的一个普遍问题，其目的是检测物体的位置和方向。这通常意味着检测对象的关键点位置。这种技术可以用来创建一个非常精确的二维/三维模型，描述对象关键点的位置，然后可以用来创建一个数字孪生兄弟。\n例如，在姿态估计问题中，可以检测到常见的方形家居对象的角点，从而可以深入了解对象在环境中的三维位置。\n同样的方法也可以用于检测人体姿势，人体上的关键点如肩膀、肘部、手、膝盖和脚都会被检测到。\n语义分割\n下一种技术称为语义分割（也称为对象掩蔽），它解决了计算机视觉领域的一个关键问题：直观地分离图像中的物体。从大的图像上看，语义分割为完全理解场景铺平了道路。这是非常有用的，因为它使计算机能够精确地识别不同物体的边界。场景理解作为一个计算机视觉的核心问题，其重要性在于从语义分割中所获得的知识使得越来越多的应用程序的健壮性得以提升。在下面所示的自动驾驶汽车示例中，它帮助汽车识别道路和其他物体的准确位置。\n图像到图像转化\n下面提到的技术属于图像到图像转化的范畴。对于下面的技术，网络通过提高质量而不是提取见解或得出结论来增强图像和视频。\n超分辨率：\n此任务的目标是在同时提高细节级别的同时提高图像的分辨率。一个非常深的神经网络最近在图像超分辨率方面取得了巨大的成功。放大倍数适用于2倍放大，如下图所示。\n超分辨率图像残留的密集网络（Zhang等人,2018日三月）\n夜视\n在弱光下成像是一项挑战。短曝光图像会产生噪声，长曝光时间会导致动态模糊。后者通常也不切实际，尤其是对于手持摄影。人们已经提出了各种去噪、去模糊和增强技术，但它们的效果在极端条件下是有限的，例如夜间高速摄影。为了提高目前的标准，研究人员引入了一种基于深度网络端到端训练的低光图像处理技术。该网络直接利用原始传感器数据，取代了许多传统的图像处理技术。这可以在下面的图像中清楚地看到，暗噪声图像得到了显著的增强。\n在黑暗中学会看东西（Chen等人，2018年五月）\nSuper SloMo\n视频插值旨在在两个连续帧之间生成中间帧。这些人工生成的画面与原始图像有着不可区分的视觉特征。这项技术是放大摄像系统性能的理想方法。对多个数据集的实验结果表明，深度学习方法比现有的方法具有更好的一致性。这项技术的结果可以在下面的视频剪辑中看到，在原始帧之间添加7个中间帧来创建平滑的慢动作视频。\nSuper SloMo:视频插值多中间帧的高质量估计（Jiang等人,2018年7月）\n在本文中，我们研究了许多计算机视觉技术，这些技术是由最近几个月开发的深入学习提供动力的，并且已经展示了令人难以置信的结果，并准备在行业中实施。这些技术处于技术的前沿，通过提高速度、准确性、可靠性和灵活性，表现出明显的优于以前的技术。\n创新的关键驱动因素是近年来人工智能研究论文的数量激增，特别是在计算机视觉领域，使充分利用技术进步来改善工业运营的最新趋势变得更加重要。\n谢谢你的阅读！希望，你学到了一些新的和有用的关于最先进的计算机视觉技术的东西，这些技术已经为工业上的实际应用做好了准备。\n如果你想了解更多，请一定要为这篇文章鼓掌，并跟随我。"}
{"content2":"1什么是计算机视觉\n计算机视觉（Computer Vision）是指用计算机实现人的视觉功能——对客观世界的三维场景的感知、识别和理解。\n这意味着计算机视觉技术的研究目标是使计算机具有通过二维图像认知三维环境信息的能力。因此不仅需要使机器能感知三维环境中物体的几何信息（形状、位置、姿态、运动等）而且能对它们进行描述、存储、识别与理解。可以认为，计算机视觉与研究人类或动物的视觉是不同的：它借助于几何、物理和学习技术来构筑模型，用统计的方法来处理数据。\n人工智能的完整闭环包括感知、认知、推理再反馈到感知的过程，其中视觉在我们的感知系统中占据大部分的感知过程。所以研究视觉是研究计算机的感知重要的一步。\n2发展的几个重要节点\n视觉研究的开端-Hubel和Wiesel关于大脑视皮层细脑感受野的论述\n感受野-（一个感觉神经元的感受野是指这个位置里适当的刺激能够引起该神经元反应的区域。感受野一词主要是指听觉系统、本体感觉系统和视觉系统中神经元的一些性质。）\n1959年，Hubel和Wiesel猫实验的故事，把微电极埋进猫的视皮质细胞，之后在屏幕上打出一些光影和图形。通过固定猫的头部来控制视网膜上的成像，并测试细胞对线条、直角、边缘线等图形的反应。Hubel和Wiesel告诉我们视觉识别应该从简单的形状开始。\n对于看到鱼和老鼠投像的猫来说，视觉处理的前期并不是对整体的鱼或者老鼠进行处理，视觉处理流程的第一步是对简单的形状的结构处理、边缘排列。只有当图片切换时的反应激烈。\n二维到三维- Roberts积木世界让计算机理解三维场景\n20世纪50年代主要分析二维图像，而Lary Roberts 1963年写的论文《block world》（积木世界），运用计算机程序，试图从图像中阐释出诸如立方体等多面体的这些边缘和形状。它根据线画图来理解由多面体构成的景物，并对物体形状物体的空间关系进行描述。\n学科的诞生\n计算机视觉真正的诞生时间是在1966年，MIT人工智能实验室成立了计算机视觉学科，标志着CV成为一门人工智能领域中的可研究的学科，同时历史的发展也证明了CV是人工智能领域中增长最快的一个学科。\n视觉理论：视觉是分层的\n20世纪80年代初，MIT人工智能实验室的David Marr出版了一本书《视觉》（全名《Vision： A Computational Investigation into the Human Representation and Processing of Visual Information》），他提出了一个观点：视觉是分层的。\n他认为视觉是个信息处理任务，应该从三个层次来研究和理解，即计算理论、算法、实现算法的机制或硬件。\n一、信息处理的计算理论，在这个层次研究的是对什么信息进行计算和为什么要进行这些计算。\n二、算法，在这个层次研究的是如何进行所要求的计算，即设计特定的算法\n三、实现算法的机制或硬件，在这个层次上研究完成某一特定算法的计算机构。\n例如根据 Fourier 分析理论，任意连续函数可用它的 Fourier 频谱来表示，因此 Fourier 变换是属于第一层的理论，而计算Fourier 变换的算法是属于第二个层次的，至于实现快速，Fourier算法的阵列处理机就属于第三层次。\n视觉理论使人们对视觉信息的研究有了明确的内容和较完整的基本体系，仍被看做是研究的主流；\n3计算机视觉是一门交叉学科\n计算机视觉技术是一种典型的交叉学科研究领域，包含了生物、心理，物理，工程，数学，计算机科学等领域，存在与其他许多学科或研究方向之间相互渗透、相互支撑的关系。在概念的理解中我们常常听到AI、图像处理、模式识别、机器视觉等词语，那么他们和计算机视觉之间是怎样的关系呢？\n（图片来自网络）\n计算机视觉与人工智能\n人工智能技术主要研究智能系统的设计和有关智能的计算理论与方法。 人工智能可被分为三个阶段感知 、认知和动作执行。计算机视觉常被视为A I的一分支 。\n计算机视觉与图像处理\n图像处理中，人是最终的解释者；计算机视觉中，计算机是图像的解释者。图像处理算法在机器视觉系统的早期阶段起着很大的作用，它们通常被用来增强特定信息并抑制噪声。计算机视觉系统必须有图像处理模块存在。\n（图片来自wikipedia）\n计算机视觉与模式识别\n模式识别是根据从图像中抽取的统计特性或结构信息，把图像分为设定的类别。图像模式的分类是计算机视觉中的一个重要问题。模式识别中的许多方法可以应用于计算机视觉中。\n计算机视觉与机器视觉\n计算机视觉技术的研究目标是使计算机具有通过一幅或多幅图像认知周围环境的能力（包括对客观世界三维环境的感知 、识别与理解）。 这意味着计算机不仅要模拟人眼的功能，而且更重要的是使计算机完成人眼所不能胜任的工作。而机器视觉则是建立在计算机视觉理论基础之上，偏重于计算机视觉技术的工程化，能够自动获取和分析特定的图像，以控制相应的行为。与计算机视觉所研究的视觉模式识别、视觉理解等内容不同，机器视觉技术重点在于感知环境中物体的形状、位置 、姿态 、运动等几何信息 。两者基本理论框架、底层理论、算法相似，只是研究的最终目的不同。所以实际中并不加以严格划分，对于工业应用常使用“机器视觉” ，而一般情况下则常用“计算机视觉“。（部分选自《基于 OpenCV 的计算机视觉技术实现》）\n4计算机视觉的关键任务\n1、物体识别，识别图像物体属于的类别。\n2、物体检测，用框去标出物体的位置，并给出物体的类别。\n3、分类+定位，分类问题就是给输入图像分配标签的任务，找到图像中某一目标物体在图像中的位置，即定位。\n4、图像分割，将数字图像细分为多个图像子区域（像素的集合，也被称作超像素）的过程。\n5值得关注的人工智能开放平台\nFace++人工智能开放平台包含了包括人脸识别、人体识别、证件识别、图像识别在内的CV能力的体现。承接了如手机、营销、教育、汽车等行业的落地解接方案。"}
{"content2":"开源人工智能使用卷积网格自动编码器生成3D面部摘要：人脸的学习3D表示对于计算机视觉问题是有用的，例如3D面部跟踪和从图像重建，以及诸如角色生成和动画的图形应用。传统模型使用线性子空间或高阶张量概括来学习面部的潜在表示。由于这种线性，它们无法捕获极端变形和非线性表达式。为了解决这个问题，我们引入了一个多功能模型，该模型使用网格表面上的光谱卷积来学习面部的非线性表示。我们引入了网格采样操作，这种操作能够实现分层网格表示，捕获模型中多个尺度的形状和表达的非线性变化。在变分设置中，我们的模型从多元高斯分布中采样不同的逼真3D面。我们的训练数据包括在12个不同科目中捕获的20,466个极端表情网格。尽管训练数据有限，但我们训练有素的模型优于最先进的面部模型，重建误差降低50％，而参数减少75％。我们还表明，用我们的自动编码器替换现有最先进的人脸模型的表达空间，可以实现较低的重建误差。我们的数据，模型和代码可在关键词coma上找到。\n开源人工智能使用卷积网格自动编码器生成3D面部介绍：人脸在形状上变化很大，因为它受到诸如年龄，性别，种族等许多因素的影响，并且随着表情而显着变形。现有技术的3D人脸表示主要使用线性变换[40,28,41]或更高阶张量概括[45,12,14]。这些3D人脸模型有多种应用，包括人脸识别[39]，生成和动画人脸[28]以及从图像中估计3D人脸[43]。然而，由于这些模型是线性的，它们不会捕获由于极端面部表情引起的非线性变形。这些表达对于捕捉3D脸部的真实感至关重要。\n与此同时，卷积神经网络（CNNs）已成为生成图像[22,34]，音频[33]等的丰富模式。其成功的原因之一归因于CNN的多尺度层次结构。允许他们学习平移不变的局部特征。最近的工作已经探索了用于3D表示的体积卷积[8]。但是，体积操作需要大量内存，并且仅限于低分辨率3D卷。对3D网格进行建模卷积可以实现内存效率，并允许处理高分辨率3D结构。然而，CNN大多数在欧几里得域中已经成功地使用基于网格的结构化数据，并且CNN到网格的泛化并非无足轻重。将CNN扩展到图形结构和网格最近才引起了人们的极大关注[11,17,10]。 CNN中的层级操作（例如最大池化和上采样）尚未适应网格。此外，由于当前3D数据集的大小有限，在3D面部数据上训练CNN是具有挑战性的。现有的大规模数据集[14,16,49,48,37]不包含高分辨率的极端面部表情。\n贡献：为了解决这些问题，我们引入了具有新颖网格采样操作的卷积网格自动编码器（CoMA），其保留了神经网络中不同尺度的网格特征的拓扑结构。我们遵循Defferrard等人的工作。 [17]关于使用快速切比雪夫滤波器在图上推广卷积，并使用它们的公式在我们的面部网格上进行卷积。我们执行网格的光谱分解并直接在频率空间中应用轮廓。这使得卷积存储器有效且可行地处理高分辨率网格。我们将卷积和采样操作结合起来，以卷积网格自动编码器的形式构建我们的模型。我们表明，CoMA在捕获具有较少模型参数的高度非线性极端面部表情时，比最先进的面部模型表现更好。在我们的模型中具有较少的参数使其更紧凑，更容易训练。参数的减少归因于可以在网格表面上共享的局部不变卷积滤波器。\n我们通过在多相机有源立体声系统中捕获具有极端面部表情的20,466个高分辨率网格来解决数据限制问题。我们的数据集涵盖12个主题，执行12种不同的表达。表达式选择复杂且不对称，面部组织发生明显变形。\n总之，我们的工作引入了一种表示，该表示使用分层多尺度方法对网格表面上的变化进行建模，并且可以推广到其他3D网格处理应用程序。我们的主要贡献是：\n1.我们介绍了一种卷积网格自动编码器，它由网格下采样和网格上采样层组成，在网格表面定义了快速局部卷积滤波器。\n我们表明我们的模型准确地表示了低维的3D面部潜在空间的性能比用于最先进面部模型的PCA模型好50％，如[40,28,1,7,46]。\n3.我们的自动编码器使用的参数比线性PCA模型少75％，而在重建误差方面更准确。\n4.我们表明，通过CoMA替换现有技术面部模型的表达空间FLAME [28]可以提高其重建精度。\n5.我们证明我们的模型可以用于变分设置来采样a来自已知高斯分布的面部网格的多样性。\n6.我们提供来自12个不同科目的20,466帧复杂3D头部网格，以及一系列极端的面部表情以及我们的代码和训练模型用于研究目的。\n开源人工智能使用卷积网格自动编码器生成3D面部相关工作：面部表征。 Blanz和Vetter [2]介绍了可变形模型;基于主成分分析（PCA）的3D面部的第一个通用表示，用于描述面部形状和纹理变化。我们还将读者推荐给Brunton等人。 [13]全面概述了3D人脸表现。迄今为止，巴塞尔人脸模型（BFM）[35]，即可变形模型的公开可用变体，是中性表达中最广泛使用的3D脸部形状的表示。 Booth等人。 [3]最近提出了另一种线性中性表情3D人脸模型，该模型是从近千种不同科目的面部扫描中学到的。\n用线性空间或其更高阶通用表示面部表情仍然是最先进的。线性表达基础矢量可以使用PCA [1,7,28,40,46]计算，或者使用线性混合形状手动定义（例如[41,27,6]）。杨等人。 [46]使用多个PCA模型，每个表达一个，Amberg等。 [1]将中性形状PCA模型与PCA模型结合起来，对中性形状的表达残差进行了研究。在Face2Face框架中使用了具有额外反照率模型的类似模型[42]。最近发布的FLAME模型[28]还模拟了头部旋转和偏航运动以及线性混合涂层，并实现了最先进的结果。 Vlasic等。 [45]引入多线性模型，即PCA的高阶概括以模拟表达3D面部。最近，Fern'andez等人。 [18]提出了一种自动编码器，其具有基于CNN的编码器和多线性模型作为解码器。与我们的网格自动编码器相反，它们的编码器在深度图像上运行，而不是直接在网格上运行。对于所有这些方法，模型参数全局影响形状;即每个参数影响面部网格的所有顶点。然而，我们的卷积网格自动编码器模拟了局部变化，这是由于卷积的分级多尺度特性与下采样和上采样相结合。\n为了捕捉局部的面部细节，Neumann等人。 [32]和法拉利等人。 [19]使用稀疏线性模型。布伦顿等人。 [12]通过在小波系数上计算局部多线性模型，使用分层多尺度方法。而Brunton等人。 [12]也使用了分层多尺度表示，他们的方法不使用整个域中的共享参数。请注意，由于面部特征的局部性，在局部低维空间[12]中采样是困难的;局部面部特征的组合不太可能形成合理的全局面部形状。我们工作的一个目标是通过对潜在空间进行采样来生成新的面网格，因此我们将自动编码器设计为使用单个低维潜在空间。\n杰克逊等人。 [25]在其基于CNN的框架中使用体积面部表示。与现有的面部表示方法相比，我们的网格自动化toencoder使用卷积层来表示具有明显更少参数的面。由于它完全在网格空间上定义，因此我们没有内存约束来影响用于表示3D模型的体积卷积方法。\n卷积网络。布朗斯坦等人。 [10]全面概述了非欧几里得域上CNN的推广，包括网格和图。 Masci等人。 [30]定义第一个网格卷积，通过使用测地极坐标对每个点周围的表面进行局部参数化，并在得到的角度仓上定义卷积。在后续工作中，Boscaini等人。\n[5]使用各向异性热核参数化每个点周围的局部固有斑块。 Monti等人。 [31]引入了d维伪坐标，它用权重函数定义了每个点周围的局部系统。对于权重函数的特定选择，该方法类似于[30]和[5]的内在网格卷积。相比之下，Monti等人[31]使用具有可训练均值向量和协方差矩阵的高斯核作为权函数。\nVerma等人。 [44]对图形进行动态过滤，其中滤波器权重取决于输入数据。但是，这项工作并不专注于减少图形或网格的维数。 Yi等人。 [47]也提供了用于标记节点的光谱CNN，但不涉及网格的任何维数减少。 Sinha等。 [38]和Maron等人。 [29]将网格表面嵌入平面图像中以应用传统的CNN。 Sinha等。使用强大的球面参数化将表面投影到八面体上，然后将其切割和展开以形成正方形图像。马龙等人。 [29]引入了从网格表面到平坦圆环的共形映射。\n尽管上述方法提出了网格上的卷积的概括，但是它们不使用结构来将网格减少到低维空间。所提出的网格自动编码器通过将网格卷积与有效网格缩减相结合来有效地处理这些问题pling and mesh-upampling operators.Bruna et al。 [11]通过利用拉普拉斯图和傅里叶图的连接，提出了图上CNN的第一次推广（更多细节参见第3节）。这导致光谱过滤器概括了图形的概念。 Boscaini等。 [4]使用窗口傅立叶变换对频率空间进行局部化。 Henaff等人。 [24]以Bruna等人的工作为基础。通过添加一个程序来估计图的结构。为了降低谱图卷积的计算复杂度，Defferrard等人。 [17]通过截断的Chebyshev多项式逼近光谱滤波器，避免明确计算拉普拉斯算子特征向量，并为图形引入有效的汇集算子。 Kipf和Welling [26]仅使用一阶Chebyshev多项式简化了这一点。但是，这些图形CNN不直接应用于3D网格。 CoMA使用截断的切比雪夫多项式[17]作为网格卷积。此外，我们定义网格下采样和上采样层，以获得完整的网格自动编码器结构，以表示高度复杂的3D面，获得3D人脸建模的最新结果。\n开源人工智能使用卷积网格自动编码器生成3D面部Mesh自动编码器：4.1网络架构，我们的自动编码器由编码器和解码器组成。编码器的结构如表1所示。编码器由4个切比雪夫卷积滤波器和K = 6切比雪夫多项式组成。每个卷积之后都有一个有偏见的ReLU [21]。下采样层在卷积层之间交错。每个下采样层将网格顶点的数量减少大约4倍。编码器使用末端的完全连接层将面网格从Rn×3变换为8维潜在向量。\n解码器的结构如表2所示。解码器类似地由完全连接的层组成，该层将从R8到R20×32的潜在矢量变换，可以进一步上采样以重建网格。在解码器的完全连接层之后，具有交错上采样层的4个卷积层生成R5023×3中的3D网格。每个卷积之后是类似于编码器网络的偏置ReLU。每个上采样层将顶点数量增加大约4倍。显示了网格自动编码器的完整结构。\n开源人工智能使用卷积网格自动编码器生成3D面部结论：我们引入了CoMA，这是一种不同形状和表达的3D面孔的新表示。我们将CoMA设计为分层的，多尺度的表示，以捕获多尺度的全局和局部形状和表达变化。为此，我们引入了新颖的采样操作，并将这些操作与自动编码器网络中的快速图形卷积相结合。在网格表面共享的局部不变滤波器显着减少了网络中滤波器参数的数量，非线性激活函数捕获了极端的面部表情。我们在极端3D面部表情的数据集上评估了CoMA，我们将公开提供用于研究目的的训练模型。我们发现CoMA在3D面部重建应用中的性能明显优于最先进的模型，同时使用的模型参数减少了75％。在插值实验中，CoMA优于线性PCA模型50％，并且在完全看不见的面部表情上更好地概括。我们进一步证明了变分环境中的CoMA允许我们通过对潜在空间进行采样来合成新的表现面。"}
{"content2":"计算机视觉入门笔记\n一、基本的图像操作和处理\n图像直方图和直方图均衡化\n图片的直方图：直方图是用来量化曝光量的，也叫柱状图，是一张二维的坐标图。 图中横轴（X轴）代表的是图像中的亮度，由左向右，从全黑逐渐过渡到全白。 纵轴（Y轴）代表的则是图像中处于这个亮度范围的像素的相对数量。 当直方图中的黑色色块偏向于左边时，说明这张照片的整体色调偏暗，也可以理解为照片欠曝。 当黑色色块集中在右边时，说明这张照片整体色调偏亮，除非是特殊构图需要，否则我们可以理解为照片过曝。 一幅好的照片应该明暗细节都有，在柱状图上就是从左到右都有分布，两边都到最低，即直方图的两侧不贴边。\n图像的直方图用来表征该图像像素值的分布情况。图像灰度变换中一个非常有用的例子就是直方图均衡化。直方图均衡化是指将一幅图像的灰度直方图变平，使变换后的图像中的每个灰度值的分布概率相同，并且可以增强图像的对比度。\n【数字图像处理】直方图均衡化详解及编程实现\n图像平均\n图像平均操作是减少图像噪声的一种简单方式。\n图像平均及其在降噪方面的应用\n图像的主成分分析\nhttps://my.oschina.net/gujianhan/blog/225241#OSC_h2_1\n图像模糊与图像导数\n图像的一阶与二阶导数计算在图像特征提取与边缘提取中十分重要。一阶与二阶导数的\n作用，通常情况下：\n一阶导数可以反应出图像灰度梯度的变化情况\n二阶导数可以提取出图像的细节同时双响应图像梯度变化情况\n高斯模糊的算法\n图像处理之高斯一阶及二阶导数计算\nsobel算子\n图像去噪\n图像去噪算法简介"}
{"content2":"计算机视觉、计算机图形学与图像处理概念辨析\n在一次联系导师的过程中傻傻分不清楚这几个概念，血泪教训！！！在这里做一下总结，也为其他学子敲响警钟。\n一、名词解释\n计算机视觉（Computer Vision）简称CV\n有时候老师会称之为vision，听到的时候应该明白是什么。\n维基百科对其定义是：\nComputer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.\n计算机图形学（Computer Graphics）简称CG\n维基百科对其定义是：\nComputer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.\n图像处理（Image Processing）简称IP\n维基百科对其定义是：\nIn imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image.\n二、区别与联系\n从输入输出的角度来看，这三者的区别就很明显了：\nComputer Vision\n输入的是图像或图像序列，通常来自相机、摄像头或视频文件。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。\nComputer Graphics\n输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb 颜色等。输出的是图像，即二维像素数组。\nDigital Image Processing\n输入的是图像，输出的也是图像。Photoshop 中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。"}
{"content2":"计算机视觉中的难题\n计算机视觉有七类分类问题：\n不同的视角，不同的大小，物体的形变问题，物体的遮挡问题，光照条件，背景复杂的问题，每一类中有多种形态的问题。\n而数据增广的思路也就是解决这个问题。数据增广如何增广就要从实际的问题出发，比如医学的图片基本上拍摄的时候视角是固定的，所以就不需要不同视角的增广。木纹检测中视角是不固定的，就需要不同的视角，不同的大小的增广，还需要应不同的光照条件对数据进行增广。\n不同视角的增广\n不同的视角可以用三维观察的知识来解决，也就是三维矩阵的问题。关于三维观察的知识我想再另一篇博客上写。"}
{"content2":"谷歌的AIY套件对那些想要探索计算机视觉等人工智能概念的创客群体，做了很多帮助。旧版本的套件，用户需要自己提供Raspberry Pi和其他必需零部件，所以对新手来说，比较的麻烦。\n从现在开始，AIY套件将变得更加容易，更适合初学者、学生等等群体：Google发布了更新的AIY Vision和AIY Voice套件，其中包括您开始使用所需的所有组件内容。两者都包括Raspberry Pi Zero WH板和预先配置的SD卡，而Vision Kit也引入了Raspberry Pi Camera v2。所以用户无需购买摄像头和树莓派板子了。\n与此同时，谷歌承诺在您准备DIY时提供更多帮助。一个配套的Android应用程序（AIY app）有助于设置您的AIY套件，并且AIY网站本身已经针对年轻创作者更清晰的指示进行了操作改进。现在，这些套件应该更适合STEM学生，而不仅仅是那些喜欢DIY的创客们。\nVision（89.99美元）和Voice（49.99美元）套件已经在美国的Target在线和零售商店销售。但是库存很少，所以在其他卖家手上（比如eBay、亚马逊等等），价格都在上涨。"}
{"content2":"机器学习与计算机视觉入门项目——视频投篮检测（一）\n随着机器学习、深度学习技术的迅猛发展，计算机视觉无疑是近年来发展最快的AI领域之一。正因如此，互联网上有关计算机视觉和机器学习、深度学习的社区、博文、论文、代码、算法框架等极大地丰富起来，给初入该领域的新人丰富的参考资料。同时计算机视觉比较易于理解，适合新手入门，培养最基本的工程素养和代码项目经验，从而了解处理具体计算机科学相关领域的基本流程。\n——浙江大学教授 王跃明\n该博客是由我2018年8月1日至22日在浙江大学玉泉校区参加计算机学院保研夏令营时所做的项目整理而来。在此，向培训期间授课和指导的王跃明老师、潘纲老师、祁玉老师表示感谢！向与我同组的西北工业大学祝同学表示感谢，她负责了项目的绝大部分实验和PPT，她的认真态度和拼搏精神永远值得我学习，祝她前程似锦！感谢一直陪伴我们的姜学长，整个过程中，姜学长给了我很多启发和指导，祝他在浙大求得真理！\n正式开始\nGetting Started:Practice Machine Learning Algorithms in Computer Vision\n计算机视觉和机器学习的关系\nMachine learning focuses more than a model itself, has more\nmathematics, emphasizes the understanding of models and algorithms. Computer vision can be seen as an application field, and seems more like problem(s). Obviously, the models in ML can be applied to solve CV problems. Indeed, people are doing this.\n显然，机器学习是解决计算机视觉问题的一个有力工具，但是也并不能解决CV的任何问题。同时计算机视觉也有着自己的特点：\nData are usually large\nProcessing skills of image and video data are varied\nMany problems have priors\nRequiring good programming skills\n总结一句话，将机器学习算法应用到CV问题上时，应充分考虑到问题的特性，充分应用问题的先验知识改造我们的机器学习“工具”。\n项目简介——篮球进球检测\n给定一个篮球赛视频，如何确定何时进球得分？更进一步说，如何判断进球时刻的frames？\n让我们来考虑几个现实的问题，看看是否能得出思路。\n首先，视频是由若干帧图片组成的，分析进球与否可以通过判断球框区域有无篮球来完成。即，构造一个图像分类器，送入一张图片，判断其中有无篮球。这是一个标准的机器学习的二分类问题。\n这样问题就从一个视频分析任务转化成了图像分析任务，只不过需要考虑一下单帧图像的处理速度，能否胜任视频的播放速度。不过在我们的项目中运算量还不是很大，基本不用考虑。\n其次，若以25帧/秒的速率录制视频，一次进球的过程可能长达十几帧。这么多帧的图片都是一次进球，我们的图像分类器将每张图片都判定成进球怎么办？\n然后，我们应该如何选取篮筐区域的大小？篮框的区域总不能太大，否则将包含太多无用信息，干扰模型的训练；也不能太小，因为进球情况多样，有相当数量的情况是倾斜进框的，篮框区域太小会使篮球捕捉不完整。总之这是一个需要反复琢磨和思考的问题，直接影响了后面的特征提取和分类器训练。\n最后，我们通过截帧和标注，获得了一个进球数据集，然后选择了一个合适的分类器模型进行训练好了，那我们如何评价训练的模型好坏呢？一个模型有很多参数，我们如何衡量参数改变带来的模型分类性能的变化？如何划定训练集和测试集的比例，这对模型有何影响？\n该博客将在随后的逐渐回答这些问题或者给出作者的思考，也希望各位前辈同行不吝赐教！\n最后是训练与检测的框图\n工欲善其事，必先利其器——环境配置\n关于Python的安装和配置请看我的博客\nWindows10 GPU版Tensorflow配置教程+Anaconda3+Jupyter Notebook\n项目常用到的package有如下：\nnumpy # 矩阵运算 scipy # 科学计算 matplotlib # 画图 random # 随机数相关 sklearn # 机器学习 cv2 # OpenCV计算机视觉 pickle # 保存变量 os # 文件操作 theano # 机器学习和深度学习框架 tensorflow # 同上 keras # 同上\n这些package利用Anaconda都可以很方便的安装，conda install yourpackage，或者是Python自带的。\n代码的最开头，我都会引入这些\n# -- coding: utf-8 -- import cv2 import math import matplotlib.pyplot as plt import numpy as np import scipy import copy import os import pickle import random from scipy import interp from sklearn import svm,datasets from sklearn.metrics import roc_curve, auc from sklearn.cross_validation import StratifiedKFold import tensorflow as tf import tensorflow.examples.tutorials.mnist.input_data as input_data import theano %matplotlib inline\nIDE我用的是Jupyter Notebook，解释性比较好，Anaconda自带。当然，Pycharm也不错。\n数据集制作\n曾听过这样两句话，“人工智能，有多少人工，就有多少智能”，“数据集的好坏决定了这个任务的performance上限，而后的模型选择、参数调整不过是在不断接近这个上限”。\n因此数据集的制作是很重要的，直接影响了我们的分类器性能的好坏。同时，这也体现了较多的主观性，毕竟判断是否是进球是一个很模糊的概念，比如\n那么我们现在开始处理视频。视频的链接在这里https://pan.baidu.com/s/1Vq9cr5HUSaRyPk1tinztFA\n处理视频并得到篮筐位置截图的流程如下：\n读入视频\n拾取鼠标位置坐标，获得篮框位置\n依次读取每帧图片，截取第二步获得的区域\n保存至指定目录\n我首先用面向过程的方法编写代码，调试BUG，直至无误后将其封装成Annotation类。\n下面是面向过程的代码.\n代码的第三部分，受姜学长启发，是我关于自动标注图片的一个尝试。运行代码可以实现给图片自动打标签（0 不进球 1 进球）的功能。\n其基本思想是这样的：灰度显示的篮筐区域里，255代表白色，0代表黑色，进球的图像中，由于球体的颜色偏深灰，因而篮球会使整个区域的像素和降低，即，区域灰度值之和关于时间的图像上会出现一个负尖峰。如下：\n当选取合适的阈值，就可以截获所有进球的帧。\n但是问题没有那么简单。很多具有迷惑性的情况也会出现明显的负尖峰，比如由于摄像机角度的问题，篮球出现在了球网的前面，读者可以自行想象。或者球网摆动造成的毛刺现象。因此我又对上图做了均值平滑滤波（或者高斯平滑滤波），削弱“毛刺”现象，并将平滑前后图像相减，得到下图\n选取合适阈值，截取负尖峰对应的帧就完成了进球帧的标注。但是理想总是美好的，实现起来效果感人。一是不明显的进球没有检测出来，二是错误的标注了一些很有迷惑性的负样本。当然这只是手工标注前的一个辅助工作，没有期望具有很高的精度。\n还有一个做法是手工标注小规模数据，训练一个分类器，用这个分类器帮助我们标注剩下的图片，这当然比上面只用灰度值好得多\ndef annotation(filename,dir_to_save,smoothing_method,waitMs = 40): # 该函数用于 # 1）标定视频中的篮球框，获得篮球框的左上角和右下角坐标 # 2）截取视频（每一帧）的篮球框图片，尺寸约38*37，三十分钟视频约45000帧 # 3）根据进球与否，标定篮框图片为正负样本，正样本有球，负样本无球，并将其命名 #---------------task1: get the position of hoop--------------- print('----------task1: get the position of hoop----------\\n') # 视频名称 filename_video = filename # 每帧间隔（毫秒） waitMs = waitMs # 播放视频，鼠标拾取篮框坐标 def getHoopPosition(filename,waitMs): \"\"\" function: get the position of hoop filename: the name of video, datatype string waitMs : the interval of 2 frame, generally 40 ms, controls the speed of video \"\"\" # 存储篮框位置的全局变量 global hoopPos hoopPos = np.zeros((2,2),np.int) # 鼠标响应函数 def hoopPosition(event,x,y,flags,param): # 鼠标左键点击，记录左上角点 if event == cv2.EVENT_LBUTTONDOWN: hoopPos[0,:] = x,y # 鼠标右键点击，记录右上角点 if event == cv2.EVENT_RBUTTONDOWN: hoopPos[1,:] = x,y # 打开可调大小的窗口 cv2.namedWindow('image',cv2.WINDOW_NORMAL) # 设置鼠标响应，名字应一致 cv2.setMouseCallback('image',hoopPosition) # 读取视频 cap = cv2.VideoCapture(filename) # 显示视频帧率 fps = cap.get(cv2.CAP_PROP_FPS) print(\"FPS of the video: \", fps) # 显示视频总帧数 counts = cap.get(cv2.CAP_PROP_FRAME_COUNT) print(\"Total number of frames: \", counts) # 读取视频尺寸 size=[(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))] print(\"The size of the video: \", size) # 播放视频，isOpened()检测视频是否正确读入 if cap.isOpened() == True: print(\"The video is loaded correctly!\") else: print(\"The video is not loaded correctly! Please check the path!\") while(cap.isOpened()): ret, frame=cap.read() if ret==True: cv2.imshow(\"image\",frame) # 1000ms播放25帧，每帧停顿40ms k=cv2.waitKey(waitMs) # 64位系统都要 &0xff if (k&0xff==ord('q')): break # 释放cap对象 cap.release() # 关闭视频窗口 cv2.destroyAllWindows() # 打印篮框位置 print('The position of hoop is:') print(hoopPos) return(hoopPos,fps,counts,size) # 篮球框 # (591,78) # (628,116) hoopPos1 = np.zeros((2,2),np.int) # 注意，fps和counts是浮点 hoopPos1,fps_video,counts_video,size_video = getHoopPosition(filename,waitMs) counts_video = int(int(counts_video)/100) # 显示矩形框 cap=cv2.VideoCapture(filename_video) if(cap.isOpened()): ret, frame=cap.read() if ret==True: # 画矩形 cv2.rectangle(frame,(hoopPos1[0,0],hoopPos1[0,1]),(hoopPos1[1,0],hoopPos1[1,1]),(0,255,0),1) cv2.imshow(\"video\",frame) k=cv2.waitKey(0) cap.release() cv2.destroyAllWindows() #---------------task2: crop the image of hoop--------------- print('\\n----------task2: crop the image of hoop----------\\n') # 先存所有图片，再打标签 cap = cv2.VideoCapture(filename_video) # 图片计数 count = 0 while(cap.isOpened()): ret, frame = cap.read() if ret == True: # 转灰度图 frame_GRAY = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) # 扣取篮筐 frame_GRAY = frame_GRAY[hoopPos1[0,1]:hoopPos1[1,1],hoopPos1[0,0]:hoopPos1[1,0]] # 生成文件名 filename_pic = dir_to_save +str(count)+\".png\" # 写文件 cv2.imwrite(filename_pic,frame_GRAY) # 计数+1 count += 1 if count%50 == 0: print(\"saving \",count,\" pictures\") if count == counts_video: print(\"all the pictures have been saved correctly!\") break cap.release() cv2.destroyAllWindows() #---------------task3: label the pic--------------- print('\\n----------task3: label the pic----------\\n') # 首先对每一帧计算篮筐区域所有像素值的和，然后根据该值判断有无篮球进筐 print(\"calculate the sum of pixel value of hoop region.\") cap=cv2.VideoCapture(filename_video) count = 0 # 篮框区域内所有像素值的和 PixelValue = [] while(cap.isOpened()): ret, frame=cap.read() if ret==True: frame_GRAY = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) # 扣取篮筐 frame_GRAY = frame_GRAY[hoopPos[0,1]:hoopPos[1,1],hoopPos[0,0]:hoopPos[1,0]] PixelValue.append(frame_GRAY.sum()) count += 1 if count%50 == 0: print(\"now we calculate \",count,\"pictures\") if count == counts_video: print(\"done!\") break cap.release() cv2.destroyAllWindows() # 画像素之和的曲线图 fig1=plt.figure('fig1') plt.plot(range(0,count),PixelValue,'b-',label='PixelValue') plt.title('Pixel Value of Each Frame') plt.xlabel('Frame') plt.ylabel('Pixel Value') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') # plt.legend(loc='lower right') # python内核经常崩坏，要把PixelValue保存起来 print(\"saving the PixelValue in a txt\") fileObject = open('PixelValue.txt', 'w') for item in PixelValue: fileObject.write(str(item)) fileObject.write('\\n') fileObject.close() if smoothing_method == 'linear': # 均值滤波操作 print(\"smooth the PixelValue using linear filter\") tmp1 = [] for i in range(0,counts_video-2): tmp1.append((PixelValue[i]+PixelValue[i+1]+PixelValue[i+2])/3) tmp1.append(PixelValue[counts_video-2]) tmp1.append(PixelValue[counts_video-1]) fig2=plt.figure('fig2') plt.plot(range(0,counts_video),tmp1,'b-',label='PixelValue after smoothing') plt.title('PixelValue after average filtering') plt.xlabel('Frame') plt.ylabel('Pixel Value') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') temp1 = [] for i in range(0,counts_video): temp1.append(PixelValue[i]-tmp1[i]) fig3=plt.figure('fig3') plt.plot(range(0,counts_video),temp1,'b-',label='difference of PixelValue') plt.title('difference between PixelValue and smooth result') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') plt.xlabel('Frame') plt.ylabel('D-Value') if smoothing_method == 'gaussian': # 高斯平滑操作 print(\"smooth the PixelValue using Gaussian filter\") tmp2 = [] for i in range(0,counts_video-4): tmp2.append((PixelValue[i]+3*PixelValue[i+1]+5*PixelValue[i+2]+3*PixelValue[i+3]+PixelValue[i+4])/13) tmp2.append(PixelValue[counts_video-4]) tmp2.append(PixelValue[counts_video-3]) tmp2.append(PixelValue[counts_video-2]) tmp2.append(PixelValue[counts_video-1]) fig2=plt.figure('fig2') plt.plot(range(0,int(counts_video)),tmp2,'b-',label='PixelValue after smoothing') plt.title('PixelValue after Gaussian filtering') plt.xlabel('Frame') plt.ylabel('Pixel Value') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') temp2 = [] for i in range(0,counts_video): temp2.append(PixelValue[i]-tmp2[i]) fig3=plt.figure('fig3') plt.plot(range(0,counts_video),temp2,'b-',label='difference of PixelValue') plt.title('difference between PixelValue and smooth result') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') plt.xlabel('Frame') plt.ylabel('D-value') # 找出所有的正样本，可能有假阳性样本 # 用户输入阈值 def rename(): threshold = input(\"input the threshold\") threshold = int(threshold) positive_list = [] for i in range(0,counts_video): if smoothing_method == 'linear': if temp1[i] <=threshold: positive_list.append(i) if smoothing_method == 'gaussian': if temp2[i] <=threshold: positive_list.append(i) # 打标签 filelist = os.listdir(dir_to_save) for item in filelist: src = os.path.join(os.path.abspath(dir_to_save), item) if (int(item.split('.')[0])) in positive_list: dst = os.path.join(os.path.abspath(dir_to_save), \"pos_\" + item) os.rename(src, dst) else: dst = os.path.join(os.path.abspath(dir_to_save), \"neg_\" + item) os.rename(src, dst) print(\"labeling work done!\") rename()\n这是封装成类的代码\nclass Annotation: ''' 该对象用于： 1）标定视频中的篮球框，获得篮球框的左上角和右下角坐标 2）截取视频（每一帧）的篮球框图片，尺寸约38*37，三十分钟视频约45000帧 3）根据进球与否，标定篮框图片为正负样本，正样本有球，负样本无球，并将其命名 参数： 1）filename,视频路径+名称 2）dir_to_save,截图存放路径 3）smoothing_method,数据平滑方法，'linear'或'gaussian' 4）waitMs,视频播放每帧间隔，以毫秒计 ''' def __init__(self,filename,dir_to_save,smoothing_method = 'linear',waitMs = 40): self.filename = filename self.dir_to_save = dir_to_save self.smoothing_method = smoothing_method self.waitMs = waitMs def getHoopPosition(self): \"\"\" 播放视频，鼠标拾取篮框坐标 function: get the position of hoop filename: the name of video, datatype string waitMs : the interval of 2 frame, generally 40 ms, controls the speed of video \"\"\" print('----------task: get the position of hoop----------\\n') # 存储篮框位置的全局变量 global hoopPos hoopPos = np.zeros((2,2),np.int) # 鼠标响应函数 def hoopPosition(event,x,y,flags,param): if event == cv2.EVENT_LBUTTONDOWN:# 鼠标左键点击，记录左上角点 hoopPos[0,:] = x,y if event == cv2.EVENT_RBUTTONDOWN:# 鼠标右键点击，记录右上角点 hoopPos[1,:] = x,y cv2.namedWindow('image',cv2.WINDOW_NORMAL)# 打开可调大小的窗口 cv2.setMouseCallback('image',hoopPosition)# 设置鼠标响应，名字应一致 cap = cv2.VideoCapture(self.filename)# 读取视频 fps = cap.get(cv2.CAP_PROP_FPS)# 显示视频帧率 print(\"FPS of the video: \", fps) counts = cap.get(cv2.CAP_PROP_FRAME_COUNT)# 显示视频总帧数 print(\"Total number of frames: \", counts) size=[(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))),# 读取视频尺寸 int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))] print(\"The size of the video: \", size) if cap.isOpened() == True:# 播放视频，isOpened()检测视频是否正确读入 print(\"The video is loaded correctly!\") else: print(\"The video is not loaded correctly! Please check the path!\") while(cap.isOpened()): ret, frame=cap.read() if ret==True: cv2.imshow(\"image\",frame) k=cv2.waitKey(self.waitMs) if (k&0xff==ord('q')):# 64位系统都要 &0xff break cap.release()# 释放cap对象 cv2.destroyAllWindows()# 关闭视频窗口 print('The position of hoop is:') print(hoopPos) return(hoopPos,fps,counts,size) def drawRectangle(self,hoopPos1):# 画篮框 cap=cv2.VideoCapture(self.filename) if(cap.isOpened()): ret, frame=cap.read() if ret==True: cv2.rectangle(frame,(hoopPos1[0,0],hoopPos1[0,1]),(hoopPos1[1,0],hoopPos1[1,1]),(0,255,0),1) cv2.imshow(\"video\",frame) k=cv2.waitKey(0) cap.release() cv2.destroyAllWindows() def saveHoopImage(self,hoopPos1): cap = cv2.VideoCapture(self.filename) count = 0 while(cap.isOpened()): ret, frame = cap.read() if ret == True: frame_GRAY = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) frame_GRAY = frame_GRAY[hoopPos1[0,1]:hoopPos1[1,1],hoopPos1[0,0]:hoopPos1[1,0]]# 扣取篮筐 filename_pic = dir_to_save +str(count)+\".png\" cv2.imwrite(filename_pic,frame_GRAY) count += 1 if count%50 == 0: print(\"saving \",count,\" pictures\") if count == counts_video: print(\"all the pictures have been saved correctly!\") break cap.release() cv2.destroyAllWindows() def evaluationPixelValue(self,hoopPos): # 首先对每一帧计算篮筐区域所有像素值的和，然后根据该值判断有无篮球进筐 print(\"calculate the sum of pixel value of hoop region.\") cap=cv2.VideoCapture(self.filename) count = 0 PixelValue = []# 篮框区域内所有像素值的和,存储45000个值的列表 while(cap.isOpened()): ret, frame=cap.read() if ret==True: frame_GRAY = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) frame_GRAY = frame_GRAY[hoopPos[0,1]:hoopPos[1,1],hoopPos[0,0]:hoopPos[1,0]] PixelValue.append(frame_GRAY.sum()) count += 1 if count%5000 == 0: print(\"now we calculate \",count,\"pictures\") if count == counts_video: print(\"done!\") break cap.release() cv2.destroyAllWindows() fig1=plt.figure('fig1')# 画像素之和的曲线图 plt.plot(range(0,count),PixelValue,'b-',label='PixelValue') plt.title('Pixel Value of Each Frame') plt.xlabel('Frame') plt.ylabel('Pixel Value') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') return PixelValue def save_PixelValue(self,PixelValue):# Jupyter内核经常崩坏，要把PixelValue保存起来 print(\"saving the PixelValue in a pkl file\") PixelValue_output = open('PixelValue.pkl', 'wb') pickle.dump(PixelValue, PixelValue_output) PixelValue_output.close() print(\"done!\") def smooth_PixelValue_draw(self,PixelValue,counts_video):# 平滑像素曲线图，并用原图与之相减，凸显进球时的像素脉冲 if self.smoothing_method == 'linear':# 均值滤波操作 print(\"smooth the PixelValue using linear filter\") tmp1 = [] for i in range(0,counts_video-2): tmp1.append((PixelValue[i]+PixelValue[i+1]+PixelValue[i+2])/3) tmp1.append(PixelValue[counts_video-2]) tmp1.append(PixelValue[counts_video-1]) fig1=plt.figure('fig1') plt.plot(range(0,counts_video),tmp1,'b-',label='PixelValue after smoothing') plt.title('PixelValue after average filtering') plt.xlabel('Frame') plt.ylabel('Pixel Value') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') temp1 = [] for i in range(0,counts_video): temp1.append(PixelValue[i]-tmp1[i]) fig2=plt.figure('fig2') plt.plot(range(0,counts_video),temp1,'b-',label='difference of PixelValue') plt.title('difference between PixelValue and smooth result') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') plt.xlabel('Frame') plt.ylabel('D-Value') if smoothing_method == 'gaussian':# 高斯平滑操作 print(\"smooth the PixelValue using Gaussian filter\") tmp2 = [] for i in range(0,counts_video-4): tmp2.append((PixelValue[i]+3*PixelValue[i+1]+5*PixelValue[i+2]+3*PixelValue[i+3]+PixelValue[i+4])/13) tmp2.append(PixelValue[counts_video-4]) tmp2.append(PixelValue[counts_video-3]) tmp2.append(PixelValue[counts_video-2]) tmp2.append(PixelValue[counts_video-1]) fig1=plt.figure('fig1') plt.plot(range(0,int(counts_video)),tmp2,'b-',label='PixelValue after smoothing') plt.title('PixelValue after Gaussian filtering') plt.xlabel('Frame') plt.ylabel('Pixel Value') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') temp2 = [] for i in range(0,counts_video): temp2.append(PixelValue[i]-tmp2[i]) fig2=plt.figure('fig2') plt.plot(range(0,counts_video),temp2,'b-',label='difference of PixelValue') plt.title('difference between PixelValue and smooth result') plt.grid(color='b' , linewidth='0.3' ,linestyle='--') plt.xlabel('Frame') plt.ylabel('D-value') # 找出所有的正样本，可能有假阳性样本 # 用户输入阈值 def rename(self): threshold = input(\"input the threshold\") threshold = int(threshold) positive_list = [] for i in range(0,counts_video): if self.smoothing_method == 'linear': if temp1[i] <=threshold: positive_list.append(i) if self.smoothing_method == 'gaussian': if temp2[i] <=threshold: positive_list.append(i) filelist = os.listdir(self.dir_to_save) for item in filelist: src = os.path.join(os.path.abspath(self.dir_to_save), item) if (int(item.split('.')[0])) in positive_list: dst = os.path.join(os.path.abspath(self.dir_to_save), \"pos_\" + item) os.rename(src, dst) else: dst = os.path.join(os.path.abspath(self.dir_to_save), \"neg_\" + item) os.rename(src, dst) print(\"labeling work done!\")\n标注好的数据集（1张图片是一个样本）大概这个样子：\n下面我们将对数据集提取特征了，特征的选取也是图像分类的关键一环，敬请期待！"}
{"content2":"计算机视觉从入门到放肆\n一、基础知识\n1.1 计算机视觉到底是什么？\n计算机视觉是一门研究如何让机器“看”的科学\n更进一步的说，就是使用摄像机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。\n作为一门科学学科，计算机视觉研究相关的理论和技术，视图建立能够从图像或者多维数据中获取’信息’的人工智能系统。\n1.2 图像\n当程序在读取一张图片时，需要考虑以下数据：\n高度、宽度\n假如一张照片的分辨率为：1920*1080(单位为dpi，全称为 dot per inch)，1920 就是照片的宽度，1080 就是图片的高度。\n深度\n存储每个像素所用的位数，比如正常RGB的深度就是 2^8 * 3 = 256 * 3 = 768 ， 那么此类图片中的深度为768，每个像素点都能够代表768中颜色。\n通道数\nRGB图片就是有三通道，RGBA类图片就是有四通道\n颜色格式\n是将某种颜色表现为数字形式的模型，或者说是一种记录图像颜色的方式。比较常见的有：RGB模式、RGBA模式、CMYK模式、位图模式、灰度模式、索引颜色模式、双色调模式和多通道模式。\nmore\n图像中的知识点太多，做基本图像处理，了解以上知识个人感觉可以了。等到以后如果做深入研究，或许有机会做更多的学习\n1.3 视频\n原始视频 = 图片序列，视频中的每张有序图片被称为“帧(frame)”。压缩后的视频，会采取各种算法减少数据的容量，其中IPB就是最常见的。\n码率\n数据传输时单位时间传送的数据位数，通俗一点的理解就是取样率，单位时间取样率越大，精度就越高，即分辨率越高\n帧率\n每秒传输的帧数，fps（有没有一种似曾相识的感觉~~~），全称为 frames per second\n分辨率\n每帧图片的分辨率\n清晰度\n平常看片中，有不同清晰度，实际上就对应着不同的分辨率\nIPB\n在网络视频流中，并不是把每一帧图片全部发送到客户端来展示，而是传输每一帧的差别数据（IPB），客户端然后对其进行解析，最终补充每一帧完整图片\n1.4 摄像机\n在实际应用当中，基本上都是通过不同种类的摄像机来获取数据，然后发送给服务端（AI Server）进行处理，分类有：\n监控摄像机（网络摄像机和模拟摄像机）\n行业摄像机（超快动态摄像机、红外摄像机、热成像摄像机等）\n智能摄像机\n工业摄像机\n1.5 CPU和GPU\n我想大家肯定是知道，目前很多人工智能计算都迁移到GPU上进行，tensorflow甚至还有cpu和gpu版本，所以其两者的差别和使用方法，这是绕不开的问题。\n废话少说，先来上图：\n架构上的对比\n绿色：计算单元\n橙红色：存储单元\n橙黄色：控制单元\n整体对比\nCache、Local Memory ： CPU > GPU\nThreads（线程数）：GPU > CPU\nRegisters（寄存器）：GPU > CPU\nSIMD Unit（单指令多数据流）：GPU > CPU\nCPU在设计上，低延迟，可是低吞吐量，CPU的ALU（算数运算单元）虽然少，可是很强大，可以在很少的时钟周期内完成算数计算，或许数量少，就可以任性的减少时钟周期，所以其频率非常高，能够达到1.532 ~ 3 （千兆，10的9次方）。\n大缓存容量、复杂的逻辑控制单元也可以减低延迟。\nGPU在设计上，高延迟，可是高吞吐量。GPU的特点是有很多的ALU和很少的cache. 缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为thread提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram（因为需要访问的数据保存在dram中而不是cache里面），获取数据后cache会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问dram，自然会带来延时的问题。\n参考链接\nCuda (Compute Unified Device Architecture)\n是显卡厂商NVIDIA推出的运算平台，采用并行计算架构，是GPU能够解决复杂的计算问题。包含了CUDA指令集架构以及GPU内部的并行计算引擎。\n安装教程自行搜索脑补就行。\n1.6 编程语言 + 数学基础\npython\n推荐作为入门语言，简单容易上手，需要了解一些库：numpy、pandas、matplotlib等。\nC++\n作为深入了解并尝试进行优化，C++必不可少，也是编写并修改的最佳语言。当然，如果你了解C、Matlab等语言那也是甚好的。\n线性代数\n可以把重点放在矩阵运算上。\n概率统计\n了解基本概率统计知识、高斯分布、中值、标准差和方差等概念。\nMachineLearning\n能够用公式表示代价函数、使用低度下降法来优化模型。当然机器学习内容实在是很多，建议能够完整走一遍，也可以看斯坦福的CS229课程\n1.7 计算机视觉的应用\n计算机视觉之于未来人工智能，就好比眼睛之于人的重要性一样。是未来很多领域自动化获取数据的主要渠道之一，也是处理数据的重要工具之一。目前可以预想到的应用主要有如下：\n- 无人驾驶\n- 无人安防\n- 人脸识别\n- 文字识别\n- 车辆车牌识别\n- 以图搜图\n- VR/AR\n- 3D重构\n- 医学图像分析\n- 无人机\n- more ……\n二、推荐参考书和公开课\n2.1 参考书籍\n《Computer Vision : Models,Learning and Inference》\n理论入门书籍\n《Learning OpenCV》\n计算机视觉必备工具\n《Computer Vision : Algorithms and Applications》\n计算机视觉算法和应用，属于进阶篇，这样的书一般都有中文译本。本人也是几经周折，找到了一些资料，供大家下载学习。\n2.2 公开课\n李飞飞计算机视觉系列课程\n这个课程作为入门非常合适，里面也会分享一些干货\nStanford CS231N\nB站资源链接\n这两门课我觉得经典的课程，如果认真学完的话，基本上是已经入门了，找一般的工作工作应该是没有问题。\n2.3 网站\nVisionbib\n这个网站是国外大佬从1994年开始专注于计算机视觉研究，上面收录了很多与此相关文献，大家可以看一些里程碑文献，让自己能够更好地理解视觉发展历程。\nvision.stanford\n没事上斯坦福大学计算机视觉研究团队官网看看，大佬们有没有发表一些研究成果文章，学习一番之后，将其翻译成blog也不失为一个好的学习方法（装逼方法）。\n这两个网站已经足够了，不要太多，学好才是最关键的。\n三、你还是需要学习一些深度学习知识\n关于深度学习，评价最高的莫过于：《Deep Learning》Written by lan Goodfellow and YoshuaBengio\n购买链接，这本AI圣经我就不多废话了，攒钱买回去好好修炼吧！\n四、开源框架必不可少\n关于开源框架，仁者见仁智者见智，我也免得引起战争，所以就罗列给一下个人不成熟的小建议。\nCaffe\n深度学习卷积神经网络开源框架。\nTensorflow\n开源机器学习深度学习框架。\n(Torch and Maxnet)\n其他深度学习开源框架\nffmpeg\n强大的视频处理工具\n流行框架的对比图:\n参考链接\n五、深入，则必须阅读相关文献\n当我们需要学习各种经典模型的时候，到哪里去找资料呢？一般大家都会直接wikipedia，可是我只想说，上面的也只是英文版汉译过来的，最好还是找一手资料，不然你吸收的知识，就不知道是被多少人消化过多少遍后得来的。当然也是有好的，不过那些大牛都是直接看原版才能得出更加深刻的结论，所以看原版文献是一件很重要的学习途径，不然就永远装不了*（学习不到最纯正的knowledge）。\n5.1 里程碑式的文献\n先熟悉所在方向的发展历程，历程中的里程碑式的文献必须要精读。 例如，深度学习做目标检测，RCNN、Fast RCNN、Fater RCNN、SPPNET、SSD和YOLO等模型；又例如，深度学习做目标跟踪，DLT、SO-DLT等等；再例如，对抗网络GAN、CGAN、DCGAN、LAPGAN等等。\n5.2 文献网站\n[arxiv](https://arxiv.org/list/cs.CV/recent) ：每天去更新一下别人最新的工作\n5.3 计算视觉的顶会\nICCV：国际计算机视觉大会\nCVPR：国际计算机视觉与模式识别大会\nECCV：欧洲计算机视觉大会\n5.4 计算机视觉的顶刊\nPAMI：IEEE 模式分析与机器智能杂志\nIJCV：国际计算机视觉杂志\n六：总结\n无论别人给出多好的资料，最终还是要靠自己踏实下来，对各种知识点细嚼慢咽。AI 不易，且行且珍惜\n~_~\n我个人对计算机视觉非常感兴趣，接下来会不断的分享各种关于CV的学习心得和干货。小白上路，大家如果能施舍一些star或者follow将会是给我最大的动力。\n- Blog\n- Github"}
{"content2":"聪明出于勤奋，天才在于积累。\n结合上文，本篇首先介绍数字图像处理的基础知识，由于篇幅有限，本人尽量结合日常工作中用到的或者熟悉的部分总结。后续介绍OpenCV库中图像处理函数的实现以及在应用平台中的使用。\n在计算机世界里图像信息就是矩阵，图像处理技术按照应用来看，一般是图像的数字化、几何变换、对比对增强、图像平滑、阈值分割、形态学处理、边缘检测、规则形状的检测和拟合、傅立叶变换、频域处理。\n图像的数字化是图像处理的第一步，从传感器采集的模拟图像信息要转换为计算机可以处理和计算数字图像信息，关键就是将图像转换为数字矩阵，便于后续处理。模拟信号数字化的步骤：抽样、量化、编码。\n图像的几何变换，一般指对图像的放大、缩小、旋转等操作。这类操作改变了原图中各区域的空间关系，完成一张图像的几何变换一般需要进行空间（坐标）变换，像素的插值（空间变换表示像素的一一对应关系，像素插值就是当原始像素值映射到一个变换后的空间，像素值不再整数位，需要近似表示的一种方法）。图像的三种常用几何变换算法：仿射变换、投影变换、极坐标变换。常用的几种插值算法：最邻近插值、双线性插值、三次样条插值。\n对比度增强：这是改变图像质量（灰度）的一种方式，目的是得到细节更加清晰、内容的质量更加优秀的图片。常用的增强方法有线性变换、分段线性变换、伽马变换、直方图正规化、直方图均衡化、局部自适应等等。简单的线性变换可以理解为对像素的值放大或者缩小，分段线性变换可以认为是对部分像素点的放大或者缩小计算，图像的伽马变换可以认为是对图像像素值的幂运算。\n图像平滑：平滑的目的是降低噪声对图像内容的干扰，平滑的实质是滤波。常用的平滑技术有：高斯平滑、均值平滑、基于统计学方法的中值平滑、双边滤波、导向滤波等。高斯平滑和均值平滑会使图像边缘信息变得模糊，双边滤波和导向滤波可以保持边缘，高斯平滑尤其对纹理信息的平滑效果更好。\n图像分割：要从图像中获取直观有效的信息，人们一般习惯于分割成不同的模块或者物体来处理，所以图像分割也就是为了把图像分成若干个信息量比较独特的区域来进行信息处理的手段，把不同的物体或者要识别的部分从原始图像中分离出来，是一种为后面图像特征分析、识别等处理不得不考虑的办法。常用的图像分割方法：阈值分割，区域分割、边缘分割等等。阈值分割最简单的一种形式就是灰度图像的二值化，如果图像中目标物体和背景的特征差异较大，使用阈值分割法不妨为一种提取目标前景物体的办法。\n形态学处理：图像分割的结果往往包含一些不可避免的干扰，生态学利用生长变胖变瘦的思想，对图像的分割区域进行调整，能带来比较好的效果。常用的生态学处理方法有：腐蚀、膨胀、开运算、闭运算、顶帽运算、底帽运算等等。腐蚀的核心算法是对原始图像用核函数求卷积最小值，膨胀恰好相反。开运算就是先腐蚀后膨胀处理的过程，这种方法一般能消除亮度较高的细小区域，在纤细出分离物体，对于较大物体，可以在不改变其面积的情况下平滑边界。闭运算恰好相反，她能填充白色物体内黑色空洞，链接临近物体、同一个结构元、多次迭代处理等作用。开运算和闭运算的差就是形态学梯度。原始图像减去开运算结果就是顶帽变换，原始图像减去闭运算结果就是底帽变换。\n边缘检测：图像的边缘这里指图像像素值走势急剧变化的位置，由于人类的感知，在对图像理解上像素值发生急剧变化的地方往往含有的信息量越大，更好的理解图像内容不可避免的要感知边缘信息。图像处理一般通过遍历每个像素以及临域像素变化度量，这种度量方式一般习惯用求导或者差分的思路来检测像素走势。常用的卷积求导算子有canny、roberts、prewitt、soble、scharr、kirsch、laplacian、高斯差分法等等。目前对于边缘的检测算法比较成熟，但是对于不同的应用场景，运算效率等方面还是有一定的提升优化空间。\n规则形状的检测拟合：常见的规则形状不外乎直线、圆、椭圆、三角形、矩形等，检测到规则图像的形状对于理解图像中物质的信息尤其关键。形状的检测也称为轮廓检测，常用的算法有霍夫曼变换法，当然，上文提到的各种算子都可以用来识别检测图中那形状。\n上文简单介绍了理解图像中信息的常用手法，接下来再介绍分析图像的另一种角度。横看成林侧成峰，从不同的角度看问题得到的结果也不一样。\n傅立叶变换：一种利用傅立叶正交函数，将信号转换到频率空间来提取特征的方法，矩阵也就转化成了幅度谱和相位谱的表示方式。这样的正交函数还有小波、Gabor等。\n频域处理：图像在变换到频率域，低频信息表示的是图像像素值变换平缓的部分，高频部分表示像素值急剧变化的部分，如边缘区域。要保留图像的平缓区域，可以找一个把高频区域过滤掉的滤波器，这种滤波器就是低通滤波器。这种滤波器有巴特沃斯、类高斯、朴实低通滤波器。相反，高通滤波器类似。带通滤波器是一种既能过滤掉高频部分，又能过滤掉底频部分的滤波器。\n了解了基本概念，我们就来使用OpenCV的函数库。官网手册：file:///Users/moscar/Downloads/opencv-python-tutroals-latest/index.html#py-table-of-content-core（本地版本）\n自己赶紧修练吧！骚年。"}
{"content2":"看了网上很多贴子，发现很多人都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？\n我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。\n可以从以下几点说明：\n（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。\n（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。\n（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n一、\n以下是不完整的列表，但基本覆盖。\n机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）\n计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）\n人工智能：IJCAI, AAAI; （期刊AI）\n另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。\n特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。\n二、\n另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/;  JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。\n三、\n说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。\n对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。"}
{"content2":"计算机视觉前沿技术探索\n摘要：计算机视觉与最前沿技术如何结合？\n计算机视觉软件正在改变行业，使用户的生活变得不仅更容易，而且更有趣。作为一个有潜力的领域，计算机视觉已经获得了大量的投资。北美计算机视觉软件市场的总投资额为1.2亿美元，而中国市场则飙升至39亿美元。让我们来看看一些最有前途和更有趣的技术，因为这些技术可以让计算机视觉软件开发市场增长的更快。\n一、深度学习的进步\n深度学习因其在提供准确结果方面而广受欢迎。\n传统的机器学习算法尽管很复杂，但其核心仍然非常简单。他们的训练需要大量的专业领域的知识和数据（这是昂贵的），在训练发生错误时需要进行人为干预，而且，他们只擅长于他们接受过训练的任务。\n另一方面，深度学习算法通过将任务映射为概念层次结构的神经元网络了解手头的任务。每个复杂的概念都由一系列更简单的概念组合定义，而所有这些算法都可以自己完成。在计算机视觉的背景下，图像分类需要首先识别亮区和暗区，然后在移向全画面识别之前对线进行分类，然后进行形状分类。\n当你为他们提供更多数据时，深度学习算法也会表现得更好，这是典型的机器学习算法做不到的。对于计算机视觉，深度学习是一个好的方向。它不仅允许在深度学习算法的训练中使用更多的图片和视频，而且还减轻了许多与注释和标记数据相关的工作。\n零售业一直是实施计算机视觉软件的先驱。2017年，ASOS在为他们的应用添加了一个按照照片搜索的选项，之后许多零售商都跟进了。有些人甚至更进一步，并使用计算机视觉软件将在线和离线体验更紧密地结合在一起。\n一家名为Lolli＆Pops的美食糖果零售商使用面部识别来识别经常走进商店的购物者。因此，商店的员工可以通过提供个性化的产品推荐和千人千面的折扣来个性化购物体验。\n特殊待遇可以提升品牌忠诚度，并将偶尔的购物者转变为经常性购物者。\n二、边缘计算的兴起\n连接到互联网和云的机器能够从整个网络收集的数据中学习并相应地进行调整，从而优化系统的性能。但是，并不能保证机器能够始终连接到互联网和云，这就是边缘计算的用武之地。\n边缘计算是指附接到物理机器的技术，例如燃气轮机，喷气发动机或MRI扫描仪。它允许在收集数据的地方处理和分析数据，而不是在云中或数据中心。\n边缘计算不能取代云。它只是允许机器在需要时单独处理新的数据。换句话说，边缘的机器可以根据自己的经验学习和调整，而不依赖于更大的网络。\n边缘计算解决了网络可访问性和延迟的问题。在边缘计算的发展下，设备可以放置在网络连接不良或不存在的区域，此外，边缘计算还可以抵消用于数据共享的云计算的使用和维护的一些成本。\n对于计算机视觉软件，这意味着可以实时更好地响应，并且只将相关数据发送到云中进行进一步分析，此功能对自动驾驶汽车特别有用。\n为了安全运行，车辆将需要收集和分析与其周围环境，方向和天气状况有关的大量数据，更不用说与路上的其他车辆通信，所有这些都没有延迟。如果通过云中心化的解决方案来分析数据可能很危险，因为延迟可能导致事故。\n三、点云（point cloud）对象识别\n最近在对象识别和对象跟踪中更频繁使用的技术是点云。简而言之，点云是在三维坐标系内定义的数据点的集合。\n该技术通常在空间（例如房间或容器）内使用，其中每个对象的位置和形状由坐标列表（X，Y和Z）表示，坐标列表称为“点云”。\n该技术准确地表示了物体在空间中的位置，并且可以精确地跟踪任何移动。点云的应用是无止境的。以下是一些行业的例子以及他们从这项技术中获得的好处：\n记录：资产监测，跟踪施工现场，故意破坏检测；\n分类：城市规划，审计工具，便于分析，绘制必要的公用事业工作\n变更检测：资产管理，货物跟踪，自然灾害管理。\n预测性维护：持续监控资产和基础设施，以预测何时需要维修。\n四、融合现实：VR和AR增强\n今天，任何VR或AR系统都会创建一个沉浸式3D环境，但它与用户所处的真实环境几乎没有关系。大多数AR设备可以执行简单的环境扫描（例如，Google ARCore可以检测平面和光线条件的变化），VR系统可以通过头部跟踪，控制器等检测用户的运动，但他们的功能也就这样了。\n计算机视觉软件正在推动VR和AR进入下一阶段的开发，有些人称之为Merged Reality（MR）。\n借助外部摄像头和传感器映射环境，以及眼动跟踪解决方案和陀螺仪来定位用户，VR和AR系统能够：\n感知环境并引导用户远离墙壁，物品或其他用户等障碍物。\n检测用户的眼睛和身体运动并相应地采用VR环境。\n提供室内环境，公共场所，地下等的指引。\nLowe's五金店已在他们的商店中使用它，每个购物者都可以借用AR设备来制作他们的购物清单，并获得商店中每件商品的指示。AR设备可以实时使用楼层平面图，库存信息和环境映射以给出准确的指示。\n我们也可以通过实时3D面部识别功能更新虚拟艺术家应用程序，让客户可以看到不同的化妆产品在他们的脸上和不同光线条件下的外观。\n五、语义实例分割\n为了理解语义实例分割是什么，让我们首先将这个概念分为两 部分：语义分割和实例分割。\n实例分割在像素级别识别对象轮廓，而语义分割仅将像素分组到特定对象组。让我们使用气球图像来说明与其他技术相比的两种技术：\n分类：此图像中有一个气球；\n语义分割：这些都是气球像素；\n物体检测：此图像中有7个气球，我们开始考虑重叠的对象；\n实例分割：这些位置有7个气球，这些是属于每个气球的像素；\n如果放在一起，语义实例分割方法将成为一个强大的工具。该工具不仅可以检测属于图片中对象的所有像素，还可以确定哪些像素属于哪个对象以及对象所在的图片中的位置。\n语义实例分割是土地覆盖分类的有用工具，具有各种应用。通过卫星图像进行的土地制图可以用于政府机构监测森林砍伐（特别是非法），城市化交通等。\n许多建筑师事务所也将这些数据用于城市规划和建筑开发，有些人甚至更进一步将其与AR设备相结合，以了解他们的设计在现实生活中的样子。\n作者：城市中迷途小书童\n链接：https://www.jianshu.com/p/8efe575c999c\n來源：简书\n简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。"}
{"content2":"图像分类\nImageNet 图像库（斯坦福计算机视觉实验室）\n1400w+images 2w+ labels\nILSVRC（ImageNet Large Scale Visual Recognition Challenge 大规模视觉识别挑战赛）\nAlexNet\nResNet（微软）\nInception-v3/v4（Google）\n目标检测\ntwo stage（物体识别、定位分为两个步骤；速度较慢，不能实时）\nRCNN 局部卷积神经网络\nRPN (Region Proposal Networks 候选区域生成网络)\nFast R-CNN\nFaster R-CNN\none stage\nSSD\nYOLO\nYOLO v2\n图像分割\nmask RCNN\n数据集\nCOCO\nPASCAL VOC\n经典网络\nLeNet-5\nLeCun et al., 1998, Gradient-based learning applied to document recognition\n针对灰度图像训练\n平均池化avg pooling，现在用最大池化max pooling更多一点\n当时不太使用padding和有效卷积valid convolutions\n随着网络越来越深，图像的高度和宽度都在缩小，信道数在增加\nconv pool conv pool fc fc output\n由于当时计算机计算的限制，采用了复杂的计算方法，filter的信道数和图像的信道数相同…\nAlexNet\nKrizhevsky et al., 2012, ImageNet classification with deep convolutional neural networks\n原文是224* 224* 3，但是227* 227* 3更好\nLeNet和AlexNet对比：\nLeNet 6w+参数；AlexNet6000w+参数\nAlexNet能够处理非常相似的包含大量隐藏单元或数据的基本构造模块\nLeNet使用的是sigmoid/tanh激活函数；AlexNet用的ReLU\n当时GPU速度较慢，模型在两个GPU上训练，将层拆分到两个GPU上，使用某种方法让两个GPU进行交流\nVGG-16\n一种只需要专注于构建卷积层的简单网络；16：包含16个卷积层和全连接层\n1.38亿+参数\nResNet\nResidual block (He et al., 2015, Deep residual networks for image recognition)\n普通网络 Plain network\nshortcut/ skip connections 跳远连接\nresidual block 残差块\n普通网络在构建更深的网络时误差会减少后增加，ResNet可以避免这个问题，构建更深的网络\nInception\nSzegedy et al., 2014, Going deeper with convolutions\n代替人工来确定卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层\n给网络添加这些参数的所有可能值，然后把输出连接起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合\n先压缩成一个小的网络，再扩大，减小了运算成本\nbottleneck layer瓶颈层\n计算成本从1.2亿降到12.4million\nInception模块 Inception网络\nOverfeat\nSermanet et al., 2014, OverFeat: Integrated recognition localization and detection using convolutional networks\n不需要把输入图片分割成四个子集分别执行前向传播，把它们作为一张图片输入给卷积网络进行计算即可，其中的公共区域可以共享很多计算\n缺点：边界框的位置不够准确\nYOLO\nRedmon et al., 2015, You Only Look Once: Unified real-time object detection\n将图片分为几个部分，分别计算y\n物体的中心点在哪个框，该框的y1=1\n非极大值抑制 Non-max suppression example\n算法可能会对同一对象做出多次检测，非极大值抑制做的就是清理这些检测结果，使一个对象只检测一次\n选中概率最大的矩形后，抑制去掉其他IoU值很高的矩形。\n概率大的矩形会被高亮，其他的会变暗\nAnchor Boxes\n一个格子检测出多个对象\n用anchor boxes表示不同类别的对象，用同一个y（3* 3* 16）表示（如图）\nR-CNN\nGirshik et. al, 2013, Rich feature hierarchies for accurate object detection and semantic segmentation\nRegion proposal 候选区域\n不再滑动区域，在每个窗口都卷积，是选择部分窗口进行卷积操作（跑分类器算法）\n运行图像分割算法 segmentation algorithm，图像分成色块\n先找出2000+色块，在色块上放置边界框并跑分类器（比所有位置跑一遍快，减少卷积花费时间）\n比较慢\nFast R-CNN\n用卷积实现了滑动窗法，但是得到候选区域的聚类步骤仍然非常缓慢\nFaster R-CNN\n用卷积神经网络得到色块，而不是图像分割的方法"}
{"content2":"无人驾驶中的人工智能技术（Drive.Ai)\n1 环境感知，这是计算机视觉领域的研究重点， 常说的slam就是指这个，基于激光雷达的slam系统目前已经能较好的进行地图定位，局部环境地图构建\n2  标识识别，包括车道识别  交通标志识别（比如红绿灯） 车辆行人识别和运动跟踪，在这里，CNN（Convolutional Neural Network,卷积神经网络）技术成了目前最好的技术，标识识别是无人驾驶行为决策的基础\ncnn技术也是对激光雷达的一个很好的补充，因为激光雷达是低像素，不能很好的识别障碍物\n3 行为决策系统技术\n行为决策系统或者叫驾驶决策系统，包括全局的路径规划导航 和 局部的避障避险，以及常规的基于交通规则的行驶策略（最简单的，让车保持在车道内），使用到的技术分成三类\n一，基于推理逻辑和规则的技术\n全局路径规划导航的A*,D＊算法，局部避障的dwa算法，常规的最优控制数学办法(比如多目标决策），以及基于交通规则的fsm规则引擎都属于这类技术\n二，快速优化的遗传算法\n当目前有多个策略选择时，如何选择最好的目标或策略，基于线性规划或动态规划的数学办法存在计算速度缓慢，很多情况下无法建模或计算量过大无法计算，这就是遗传算法发挥作用的地方\n三 ，神经网络技术\n利用神经网络来进行自动驾驶训练是最新的研究热点，也就是常说的，让神经网络学会像人类一样开车是很振奋人心的目标\n但是，神经网络的问题在于它是不透明的，是个黑盒系统，是不可解释的，基本你无法说出训练模型里一个节点的值为什么是0.1而不是0.5，这是由神经网络的特点决定的，另外，利用训练数据训练出来的很好的模型到了新的环境能不能发挥同样好的作用也是疑问\n有一种最简单的设计无人驾驶系统的办法，就是只用神经网路来做一切控制，我们只需要用大量的数据来训练它，这样就不用写复杂的控制策略算法代码了，我们只要训练好神经网络，然后用很少的代码让它运行就可以了，但是，在神经网络的可解释性不能解决前，完全基于神经网络的自动驾驶系统很显然是不能让人信服和舒服的\n所以无人驾驶系统中基于推理逻辑的控制策略仍然是很重要的，让基于推理逻辑的白盒控制系统和基于神经网络的黑盒控制系统协同工作，是最为可行的方向\n4 车辆控制系统技术\n车辆控制系统技术除了传统的pid控制外，在无人车系统也越来越多的引入了神经网络模糊控制\n无人驾驶（自动驾驶）系统常见的驾驶决策控制策略\n1 全局导航路径规划a*，图像识别神经网络和即时驾驶和避障dwa相结合的控制策略\n这是传统的无人驾驶控制策略，全局路径导航使用常规的a*算法（也可以使用其他的），车辆摄像头的图像经过神经网络处理，提取出车道 交通标识和车辆行人信息，然后以这些信息作为输入，利用vfh/dwa算法进行即时驾驶和避障控制，比如变道 减速 刹车等等\n谷歌无人车自动驾驶系统使用的就是这种控制策略\n2 全局路径规划和即时驾驶和避障神经网络相结合的高度智能的控制策略\n全局规划还是使用和策略一相同的算法，但是在即时驾驶和避障中，完全使用了神经网络技术，把从摄像头捕捉到的原始像素图特征作为神经网络的输入端，输出端为为汽车的操控命令。我们不再从原始像素图提取车道信息，交通标识，车辆行人标识，这一切都交给神经网络自动识别，最后输出的是转向 循迹 减速 刹车等汽车控制命令，神经网络使用了数以亿计的节点\n这个是英伟达的主流自动驾驶系统技术\n顶"}
{"content2":"作为人类，你本能地知道豹子比摩托车更接近猫，但我们训练大多数人工智能的方式让它们忽略了这些关系。在《科学机器人》杂志上发表的一篇新论文中，作者写道，在我们的算法中构建相似性的概念，可能会让算法变得更加强大。\n卷积神经网络已经彻底改变了计算机视觉领域，在一些最具挑战性的视觉任务上，机器的表现已经超过了人类。但我们训练它们分析图像的方式与人类学习的方式非常不同，KTH皇家理工学院副教授Atsuto Maki说。\n他写道：“想象一下，你两岁时，有人问你在一张豹纹照片中看到了什么。”“你可能会回答‘一只猫’，而你的父母可能会说，‘是的，不完全一样，但很相似。’”\n相比之下，我们训练神经网络的方式很少给予这种部分信任。他们通常被训练对正确的标签有很高的信心，认为所有错误的标签，无论是“猫”还是“摩托车”，都是错误的。Maki说，这是一个错误，因为忽略了一些事情可以“更少出错”的事实意味着你没有利用培训数据中的所有信息。\n即使当模型以这种方式训练时，分配给不正确标签的概率也会有很小的差异，这可以告诉你模型如何很好地将它学到的东西推广到看不见的数据。\n如果你给一个模型看一张豹子的照片，它给“猫”的概率是5%，给“摩托车”的概率是1%，这表明它意识到猫比摩托车更接近豹子这一事实。相反，如果数字是相反的，这意味着模型还没有学会猫和豹相似的广泛特征，这在分析新数据时可能是有帮助的。\nMaki说，如果我们能够提高识别类之间相似性的能力，我们就应该能够创建更灵活的模型，从而更好地进行泛化。最近的研究表明，一种称为正则化的方法的变化可能有助于我们实现这一目标。\n神经网络容易问题称为“过度拟合”,指倾向于过多注意小细节和噪声具体训练集，当这种情况发生时，模型将执行优化他们的训练数据但不佳时应用于看不见的测试数据没有这些特定的怪癖。\n正则化用于规避这个问题，通常通过降低网络学习所有这些不必要信息的能力，从而提高其泛化到新数据的能力。技术是多种多样的，但通常涉及修改网络的结构或人工神经元之间权重的强度。\n然而，最近，研究人员提出了新的正则化方法，通过在所有类别中鼓励更广泛的概率分布来发挥作用。Maki说，这本质上帮助他们捕捉更多的类相似性，从而提高他们的概括能力。\n谷歌大脑研究人员在深度学习先驱杰弗里·辛顿的带领下，于2017年设计了一种这样的方法。他们在训练过程中引入了一种惩罚，直接惩罚模型输出中过于自信的预测，以及一种称为标签平滑的技术，这种技术可以防止最大概率变得比其他所有技术都大得多。这意味着正确标签的概率更低，而错误标签的概率更高，这被发现可以提高模型在从图像分类到语音识别等各种任务中的性能。\n另一个来自Maki本人，他在2017年也实现了同样的目标，但是通过抑制模型特征向量的高值——描述一个对象所有重要特征的数学构造。这对输出概率的分布具有连锁效应，也有助于提高各种图像分类任务的性能。\n虽然这种方法还处于早期阶段，但人类能够利用这些相似性来更有效地学习的事实表明，将这些相似性结合起来的模型是有希望的。Maki指出，它在机器人抓取等应用中尤其有用，在这些应用中，区分各种相似的物体非常重要。\n大连人流医院哪家好 mobile.84239650.cn"}
{"content2":"Computer Vision—计算机视觉（一）\n（本文为总体概述，后续具体深入。原因的话，记录分享和本人水平成正比嘛~~~算是刚入门~~~）\n总的来说，CV（计算机视觉）领域因为深度学习的引入，方兴未艾、蓬勃发展……\n先贴一个某深圳互联网科技公司该方向实习生招聘职责|要求：\n（其实各大公司此方向要求大多如此）\n传统CV\n传统CV说白了就是特征+分类器。\n单张图片—>大量图片—>视频。\n单张图片\n依赖于图片的像素矩阵表示，有了矩阵，很多方法如二值化，阈值化，色彩均化，滤波（模糊/光滑），形态学开集和闭集，以及联通区域划分，图像金字塔等都可以做。这些处理方法在实际当中应用广泛，例如许多图片应用的滤镜/增强/变形效果,以及图片压缩。\n大量图片\n图片分类以及基于内容的检索等实际需求。传统机器视觉的方法或者说套路是，先针对问题和对特征的具体要求（例如希望特征具有旋转不变性等）设计一些特征抽取方法，有了特征之后，就能拿去喂给一些机器学习算法做分类等其他后续工作。\n那么特征提取就是重点，如颜色直方图，只是一个简单的统计描述，而其他常用的特征，如Harris角点，FAST角点，图像梯度以及HOG, LBP(local binary pattern), SIFT特征以及其变体SUFT和ORB, haar 等。经过精心设计，并且涉及一些诸如利用积分图像来优化计算的技巧，是传统计算机视觉的重要成果。\n有了特征之后，就能解决：边缘检测/轮廓提取，图像分割，图片分类，人脸识别，图片拼接(image stitching)，视觉测距（包括单目视觉测距(Monocular Visual Odometry)和立体视觉测距(Stereo Visual Odometry)）等经典问题。\n视频\n视频无非就是连续（帧）的图片，图片处理技术就自然而然地扩展到了视频上来（一般的话，图片是一帧数据，视频处理就是在图片处理的外面加一个大循环，不断的处理一帧又一帧的图片就行。当然图片或视频的获取方式有不少，USB、HDMI或者是直接从内存读取等等，不同方式下图片和视频读取的方式也有差别，但大同小异），但是视频具有动态特征也有其特殊的对待，如均值漂移、GMM（混合高斯模型）等背景建模的方法，以及利用光流法等实现物体的跟踪。\n（附：传统方法使用滑动窗口的框架，把一张图分解成几百万个不同位置不同尺度的子窗口，针对每一个窗口使用分类器判断是否包含目标物体。传统方法针对不同的类别的物体，一般会设计不同的特征和分类算法，比如人脸检测的经典算法是Harr特征+Adaboosting分类器；行人检测的经典算法是HOG(histogram of gradients) + Support Vector Machine；一般性物体的检测的话是HOG的特征加上DPM(deformable part model)的算法)\n之后，随着人工智能技术的崛起，深度学习大潮席卷了包括计算机视觉和自然语言处理在内的诸多领域，许多在之前无法有效解决的问题，如图像语义分析、图片/视频内容描述、图片/视频问答等开始得到解决。许多新的问题，如图片生成（GAN）、图片风格迁移、图像预测、图像|视频搜索、OCR等开始出现，并在深度学习的火炬下显示出无限可能，而传统的方法，那些人工设计特征的时代，似乎正在渐行渐远。\n基于机器学习|深度学习的CV\n没错，这是CV领域以后的发展方向，包括但不限于：图像检测、图像分类、图像分割、图像跟踪、视频语义分析、人脸识别与分析、车辆与人员的检测识别与跟踪、图像/视频搜索、页面分析与自动合成、OCR等算法与系统研发领域。\n最主要的原因是，精度，深度学习可以做到传统方法无法企及的精度。另外，深度学习其他原因|优点：通用性强，特征迁移能力强，工程框架统一等。\n深度学习技术框架是一颗树形结构：\n训练平台是树根，如tensorflow、theano等；\n模型是树干，是深度学习的重点。典型成果有AlexNet、VGGNet、GoogleNet、ResNet等。学术界—怎么提高模型精度，工业界—还要考虑怎么把模型做得更快，更小。\n（核心）任务|方向是树枝，检测、识别、分割、特征点定位、序列学习等5大任务。任何计算机视觉的具体应用都可以由这五个任务组合而成，如人脸识别，涉及到人脸检测、特征点定位，特征提取&验证。这就包含了检测、识别、特征点定位三个部分。\n下图给出：方向—网络对应（常用）\n以上为借鉴各路大神观点的总结，以下为个人不成熟见解：\nCV+机器|深度学习还是十分有趣的，应用也广，像你从摄像头前经过，瞬间就可以知道你的性别、年龄、情绪等等；路上行人、车辆|车牌检测，哪里有老人跌倒，哪里发生车祸，犯人逃到哪里，不需要人为通知，第一时间就可以知道并处理；目标跟踪可以一直监控你；风格迁移-滤镜，拍照神器；图像生成（GAN），比如可以用来直播换脸，想象一下你正在看的20多岁的少女直播镜头前其实是一个东北大汉……像自动驾驶（视频分割、图像预测等等）、机器人视觉啥的就更不用说了（相信未来自动驾驶可以用视觉完全替代掉激光雷达，不然，车怕是卖不出去，雷达贵~~~）\nCV+深度学习的内容确实太多（废话，钱给的多），学习、入门感觉难度不小，但是还是有方法的。比如：实践+理论，具体就是大致了解整个框架情况后，直接找个具体实例，像人脸识别、行人|车辆检测都不错，仅用传统方法和传统+深度学习的都需要学习。这些例子里就是在用那些看上去很牛X的理论，各种图像预处理、特征提取|降维、训练|分类器、后处理，各种像检测|识别|分割|特征点定位等这些方法、数据集、模型、框架平台等等，通过1-2个这样具体的例子，大致弄懂功能——复现——读源码|理解原理，里面程序有搞不懂的就去查，直到搞懂。然后基本就入门有感觉了，卯足劲儿再去深入学习，后面内容还多着呢。（建议还是不要一开始就抓着一堆看着就头疼的理论读来读去，效率比较低。其实现在这一行，大多都是这样学比较好吧。还有，想到一定要做到！）\n本人也是刚入门吧，对这一块儿很感兴趣，还有很长路要走，滚去学习了……\n（推荐一些大神更详细的文章|资源，很不错：）\nhttps://www.leiphone.com/news/201712/97BMlbEQ6DXEv8ke.html\nhttps://zhuanlan.zhihu.com/p/21533690\nhttps://blog.csdn.net/x1kz18nkbqg/article/details/78700447\nGitHub:https://github.com/666DZY666/Computer-vision\n公众号：https://mp.weixin.qq.com/s/vKnWYhcn-h_kaJQ9myfgYw"}
{"content2":"【 声明：版权所有，欢迎转载，请勿用于商业用途。 联系信箱：feixiaoxing @163.com】\n要说很多现在最火的AI是什么，那就是深度学习。那么深度学习和机器学习有什么关系呢？我们可以通过一问一答的形式来解决。\n1、什么是机器学习？\n机器学习一般是指传统的人工智能方法，它包括bayes、决策树、svm、线性回归、逻辑回归、神经网络、knn、kmeans等。目前使用较多的机器学习库就是sklearn。\n2、深度学习是什么？\n深度学习来自于传统的机器学习方法－神经网络，即nerual network。只不过深度学习比传统的神经网络层数更多、计算量更大，深度学习中大量使用的反向传播算法起始就来自于神经网络。深度学习少则几层，多则上百层，随之而来的训练时间也很长。即使使用分布式、gpu运算，也要很长的时间。\n3、深度学习需要哪些基础？\n深度学习来源于机器学习，而机器学习的本身和矩阵论、概率论、凸函数和优化方法息息相关。所以，如果需要深入了解深度学习，也需要多看看数学相关的内容。深度学习没有脱离模型、损失函数、优化方法的基本框架。\n4、目前使用比较广泛的深度学习模型有哪些？\n目前使用比较多的深度模型有cnn、rnn、lstm。其中cnn多用于图像，rnn多用于语音、而lstm多用于行为识别。当然，2018年除了深度学习模型以外，gan也非常火，即生成对抗网络。姑且不论gan的具体含义，但是它所体现的思想非常有意义。当然，如果是图像本身，那么一般是cnn+opencv一起使用，如果是语音，一般是kaldi+rnn一起使用。因为我们实在没有必要从头开始做一些基础工作。\n5、目前开源的深度学习框架有哪些？\n现在使用较多的深度学习框架有tensorflow、keras、caffe、mxnet和darknet等。这几种深度学习框架结构差不多，一般都包含数据表示、layer、network、solver这几个模块。可以重点学习一种，其他几种用到的时候看一下就可以了。我个人比较推荐的是darknet。darknet结构简单，可以移植到多个cpu体系，不依赖于第三方库，支持GPU、cuDNN，使用方便。\n6、为什么大多数深度学习多使用python来操作？\n大多数深度学习多支持python操作，python不用编译，和matlab类似，也不涉及版权，第三方库多，所以使用广泛。当然，如果是caffe的使用者，可能连编译代码都不需要。当然如果需要将深度学习框架port到嵌入式设备上，那么c&c++是少不了的，大家可以试试darknet移植到arm开发板上面。\n7、很多论文中提到的yolo、r-cnn、fast r-cnn、faster r-cnn、ssd又是指什么？\n这是针对物体检测提出的优化深度学习网络，比如道路上的行人检测、汽车检测、符号检测等等。我们可以看成是针对cnn和应用场景的进一步优化。\n8、一般深度学习怎么训练？\n深度学习可以用cpu训练，也可以用gpu训练。但是gpu一般比cpu快十几到几十倍。首先，我们需要自定义网络模型；然后定义网络训练参数，最后输入数据开始训练。等到训练结束后，我们就会得到一个训练模型。使用这个训练模型，我们就可以进行检测、分类和识别了。\n9、个人如何使用别人训练好的模型？\n可以利用别人的模型加上自己的数据继续训练，或者直接修改别人的softmax层，添加自己的分类选项。个人如果训练数据，可以选择数据量比较少的模型进行训练，如果是项目需要，尽量复用别人的模型，特别是imagenet上面公用的权重模型数据。"}
{"content2":"[计算机视觉] 四. Stereopsis立体视觉\nhttps://mbd.baidu.com/newspage/data/landingshare?context=%7B%22nid%22%3A%22news_8983098410710448491%22%2C%22sourceFrom%22%3A%22bjh%22%2C%22url_data%22%3A%22bjhauthor%22%7D"}
{"content2":"来自 https://blog.csdn.net/xiaoqu001/article/details/79350364\n斯坦福大学的CS231n课程的主要内容是计算机视觉（computer vision），或者说是图像识别（ visual recognition），算法主要关注CNN（convolutional neural network）或者说泛指的深度学习。计算机视觉是一门很强的交叉学科，生物学、心理学、物理学、工程和数学等等。\n第一节课的主要内容有两个，一是研究计算机视觉的重要性，二是计算机视觉的发展简史。\n计算机视觉的重要性\n研究计算机视觉的重要性源自现在无处不在的摄像头以及大众对于视觉信息的青睐，视频和图像信息占据了互联网上流量的大部分，而且视频和图像信息增长迅速。讲者将互联网上的视频信息比喻成黑洞，意指体量大但是蕴涵其中的内容难以观测和分析。另外，计算机视觉也支撑着机器生产力的提高以及人工智能的普及。\n计算机视觉简史\n生物视觉(biological vision)的发展\n大约543百万年前，地球上的古老生物还没有眼睛，在大海中张着嘴漫游，那时地球上可能只有几种生物。澳大利亚一位生物学家Anderw Parker在化石中发现了第一个拥有眼睛的生物，这个生物的时间大约在540百万年前，而后的一千万年间地球上的生物发生了指数式的增长，生物学家称之为生物进化的大爆炸（Big Bang of Revolution），此前关于此次生物大爆炸的猜测理论有很多，在发现了这只带眼睛的生物后，这位生物学家提出了一个理论：因为生物进化出眼睛，生物中的扑食者和猎物之间的活动开始变得更加积极（preactive）,由此导致了生物进化速度的加快，生物数量从几个增长到千百万。到今天，在人类身上，视觉已经发展成为最大的感官系统(sensor system)，视觉皮层（visual cortex）大约占据了大脑皮层的一半。可见视觉系统对于生物发展的重要性。\n机器视觉(mechanical vision)的发展\nCamera Obscura：六十年代文艺复兴时期开始有一些利用pinhole camera theory的相机诞生\n人类在机器视觉上最早的尝试应该是暗箱成像，成像的原理是针孔成像，与生物眼睛的成像原理有相同之处。后来这种暗箱进一步发展，增加了双凸透镜、光圈和感光材料等等，经过几代技术革新就发展成今天的相机。\n计算机视觉（computer vision）的发展\nHuble和Wiesel于1959年\n人类对于视觉形成原理的探索应该在很早就开始了，最早的一个具有里程碑意义的实验发生在1959年[1][2]，生物学家Huble和Wiesel将电极植入猫的视觉皮层，并在猫的眼前投影各种线条和形状，他们发现猫的视觉平层中一些细胞会对特定的线条、形状或者角度敏感，他们将这些细胞称之为“small cell”，而还有一些“complex cell”可以检测特定的edge（与位置无关）或者特定的移动方向。后来这个实验的结论被不断总结为：视觉神经系统对事物复杂的表示来源于简单的特征，也就是一种层层抽象的表示方法。下图为原文中两个结论的截图。\nLarry Roberts于1963年\n他是第一篇计算机视觉博士论文的作者。在他的论文中，世界被简化为简单的几何图形，研究的目标是识别这些几何图形并且重组出这些图形(the world is simplified into simple geometric shapes and the goal is to able to recognize them and reconstruct what these shapes are)。\nMIT于1966年\nMIT(Massachusa Institute of Technology)的summer project，成立人工智能实验室，旨在希望在几个月内研究出视觉感知的机理，当然了这个目标很激进，至今我们都没将这一问题参透。\nDavid Marr于1970\nDavid Marr提出了视觉神经系统认知的过程，第一阶段是primal sketch，包含边缘、线条、条纹、界限和线条等等，第二阶段是2+1/2D，第二阶段可以分清整块的表面、深度和层次等等(piece together the surfaces, the depth information, the layers, or the discontinuities of the visual scene) ，第三阶段是3D抽象，层次化的表面和体积元信息(hierarchically organized in terms of surface and volumetric primitives)\n写了本书，同时他提出了分层的概念：edge->sketch->3D model\nBrook和Binford于1979年以及Fischler and Elschlager于1973年\n分别提出对复杂物体的抽象描述的理论，分别为：generalized cylinder和pictorial structure。他们指出每一个物体都是有简单的几何形状构成的。\nDavid Lowe于1980s\nDavid Lowe尝试运用线条和线条的组合来重组所见的图形 ，80年的David Lowe识别了一组剃刀的图片，指出物体可以由线和线的组合来表示\n以上从60s-80s年代，计算机视觉的发展主要体现在理论猜想上，还是基于人的所见来分析神经系统的处理过程， 不是对相机拍摄得到图像数据的分析（当然了，那个时间也没有数字的图片信息），后来还有人提出图像分割的理论，试图通过色彩信息先进行图像分割再进行识别，这也基本停留在猜想阶段。\n机器学习的发展\n1997年Shi & Malik提出Normalized Cut，告诉我们如果直接识别物体太困难就先做图片的分割，分割成有意义的部分。\n1990s到2010s之间机器学习算法的发展尤其是统计机器学习(statistical machine learning)，比如支持向量机SVM、boosting和graphical models等，机器学习在计算机视觉领域一个成功的案例就是人脸识别，Paul Viola和Michael Jones于2001年提出运用Adaboost算法进行实时的人脸识别，并且FujiFilm公司运用这一算法在2006年产出了可以实时识别人脸的相机，实现了快速的成果转化。另外指的一提的是当时卷积神经网络已经被提出了，是在1998年由Yan Lecun等人提出，用来对邮票上的字符进行识别，当时神经网络在功能上根本无法与传统的机器学习算法抗衡，几乎奄奄一息。经过Yan Lecun等人的不懈坚持，神经网络算法终于迎来了丰富快速的计算资源以及大量的数据集，在2012年通过AlexNet（另外有个名字是supervision）打了一场翻身仗，自此开始迎来了蓬勃发展的时代。\nDavid Lowe于1999年\n提出了基于特征的图像识别，也就是SIFT(scale-invariant feature transform)，尺度不变特征转换。基于特征的图像识别的内涵是一种事物的图像无论在角度、光线、变形和遮挡等变量影响下还会有一些典型特征(critical feature)是不变的(some features that tend to remain diagnostic and invariant to changes)，通过将图像的这些典型特征提取出来进行特征对比要优于对整个图像进行模式对比(pattern comparison)\n2006年Lazebnik et al.\n提出了Spatial Pyramid Matching来识别整个图片的场景。他们发现实际图片中，无论是什么场景，都可以通过features，然后用SVM来进行分类。\n人体识别：Histogram of Gradients/Deformable Part Model\n数据集的发展\n为了评估计算机视觉的发展，以及得益于互联网的发展，开始出现作为计算机视觉算法测试激励的数据集。从2007-2012比较有影响的是PASCAL visual object challenge，后来就是李飞飞教授带领团队完成的ImageNet数据集。课程中李飞飞教授讲了两点制作这个数据集的目的，一是测试机器学习算法能否识别世界万物；二是将机器学习算法拉回来面对过拟合的问题。\nPASCAL Visual Object Challenge：20种物体图片\nIMAGENET：1400万张图片，2.2万种类\nImageNet图片识别大赛：\n从2010年到2015年图识别的错误率逐渐降低，但是在2010和2011年错误率还在25%上下徘徊。\n在2012年显著的下降了接近10%。获得冠军的就是Hinton提出的7层CNN，这也是这门课的重点。\nhttps://www.kaggle.com/c/cifar-10/leaderboard   Imaginet的排名，可看下。\n计算机视觉的发展任重道远\n在CNN的推动下，计算机视觉在某些情况下的识别精度和速率已经可以和人类不相上下，但是计算机视觉还处在初级阶段，计算机视觉在对图像信息的理解上还与人类有较大差距，人类看见一幅图可以产生很多符合情境的猜测，但是计算机还不能，相比于人类，计算机视觉要显得呆板很多，另外计算机视觉在识别中的功耗以及必须的计算资源也是一个问题。\nCS231n OVERVIEW\n课程重点\ncs231n focuses on one of the most important problems of visual recognition – image classification\nThere is a number of visual recognition problemsthat are related to image classification：\nobject detection：利用画框标记物体在图片中的位置，并识别画框内的物体\nimage captioning：用自然语言来描述图片\nAction classification\nConvolutional Neural Networks (CNN) have become an important tool for object recognition\nCNN的历史\n刚才提到的ImageNetd大赛，从2012年之后，获胜的算法都是深度神经网络，在此之前，都是识别特征然后用SVM进行分类的。在2015年微软的团队MSRA甚至用到了152层的nn。\n但是CNN并非2012才被发明的，早在1998年LeCun在贝尔实验室就使用了这种神经网络来识别手写钞票中的数字。\n为什么直到现在CNN和深度神经网络才流行起来呢？主要是有两方面的原因：\n一是CPU和GPU的计算能力不断加强，尤其是GPU的并行能力对于神经网络中的计算很有帮助。\n二是大数据，无论是什么样的机器学习算法，没有足够的数据量都容易overfitting，而现在有了足够大的有标记的数据库\n其他的问题\n3D重建\nActivity Recognition\n机器人视觉等\n[1]: Receptive fields of single neurons in the cat’s striate cortex\n[2]: Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex"}
{"content2":"(1)pattern recognition letters, 从投稿到发表，一年半时间审稿期7个月（Elsevier）。\n(2)Pattern recognition 不好中，时间长\n(3)IEICE Transactions on Information and Systems， 作者中有一个必须是会员。收费高，审稿快。影响因子0.4（费用非常高7000元以上）\n(4)International Journal of Pattern Recognition and Artificial Intelligence ， 审稿周期一般6--12周，影响因子偏低，容易中。（费用非常高10000元以上）\n(5)Computational Intelligence， 中等偏上，要求较高，杂志级别不错，关注人数偏少，比较冷门（影响因子7，季刊）\n(6)information processing letters, 影响因子低0.5左右，接搞量大，容易发表，审稿周期7个月（Elsevier）。\n(7)Computer vision and image understanding, 9个月审稿期，平均投稿命中率20%，业内比较认可\n(8)journal of visual communication and image representation， 投稿容易，审稿周期一年以上。（Elsevier）影响因子1.2左右\n(10)International Journal on Graphics, Vision and Image Processing (GVIP),\n(11)IET Image Processing, 影响因子0.758， EI Compendex ，审稿周期一年以上\n(12)IET Computer Vision ，影响因子0.969，\n(13)SIAM Journal on Imaging Sciences，\n(14)International Journal of Pattern Recognition and Artificial Intelligence (IJPRAI)，影响因子0.5， EI compendex, sci, 审稿时间超长，一两年\n(15)IEEE Signal Processing Letters， 审稿4---8周左右，影响因子不高，容易中，关注人不多\n(16)Journal of Logic and Computation, 影响因子，0.789，SCI检索\n(18)COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING，影响因子偏低，但仍然需要一定水平才可以投，审稿2--4周，SCI,EI检索\n(19)Signal Processing: Image Communication，容易中，审稿周期半年到一年\n(20)International Journal of Computer Vision， 较难，审稿周期半年到一年，EI,SCI检索\n(21)Journal of Mathematical Imaging and Vision，审稿半年到一年，影响因子不高，不容易中，稍微有些冷门。\n(22)Machine Vision and Applications， 影响因子偏低，但是接稿量不是很大，审稿周期一年以上，但容易发表，SCI,EI检索\n(23)Pattern Analysis & Applications， 影响因子不高，影响力也比较小，审稿时间一年以上，但容易投中。\n(24)Signal Image and Video Processing， 容易中，审稿时间半年到一年，EI检索（已经不是EI）\n(25)Pattern recognition and image analysis， EI检索，\n(26)Journal of digital imaging ，审稿周期半年到一年，影响因子偏低，容易中，很少有人关注  （医学图像处理）\n(27)International journal of pattern recognition and artificial intelligence，影响因子偏低，容易中，关注人比较少。审稿周期半年到一年。\n(28)International journal of imaging systems and technology，影响因子偏低，容易中，审稿周期半年到一年。\n(29)journal of vlsi signal processing systems for signal image and video ，影响因子偏低，容易中，审稿周期一年以上，关注人比较少。"}
{"content2":"首先，我们说计算机视觉是什么样的学科，要做什么的事情？\n很多人不了解这件事为什么那么难？如果看一下我们需要处理的对象，就会发现它确实是非常难的任务。\n我们所谓的图像是用摄像头来捕捉物体表面反射的不同颜色的光，进行采样，每个点即像素都用红绿蓝三个不同的分量数值表示不同的颜色。所以，到了计算机里面，每幅图像就是很多很多0-255之间的整数值。大家看这些数。相信没有一个人在非常短的时间内，能够通过观察这些数告诉我图像里的内容是什么。计算机视觉要完成的就是这样的任务，通过对这些数的分析完成对图像内容的理解。\n这次人工智能的浪潮，首先在语音识别和图像识别领域取得了显著的进步，并进一步引发了AI在更多领域的应用。\n从图像识别或计算机视觉角度讲，在2012年，深度学习首次在Imagnet评测数据集上应用，一下子将分类错误率降低了10个百分点。从图像分类的角度来讲，在2011年，图像分类错误率是26%，到了2012年，利用深度学习之后，下降到16%。到了2016年，随着深度学习模型深度不断加深，错误率进一步下降到了2.3%。也就是说，大概在5年时间里，图像识别率的错误率降低了10倍。\n下面是其他五个深度学习带来重要进步的典型例子。\n在物体检测领域。所谓物体检测就是提供给一张照片，把照片里不同的物体，如车、人等物体框出来。2013年，在Imagnet测试集上检测正确率只有23%，到了2017年，正确率达到了73%，在视频里寻找30类物体也达到80%的精度。\n在视频监控领域，我们希望能够对人、车、物进行检测识别，利用深度学习，现在很多系统包括中科视拓的技术都可以实现对人、车、骑行的准确检测、跟踪以及对性别、车型等属性的大致分类。\n在图像分割领域，例如为了实现自动驾驶，给一幅图像之后，我们希望算法能够知道哪块是道路、哪块是树木、哪块是建筑，这是一个分割问题。从2013年到2017年，分割的准确率也从50%提高到了86.9%。\n还有一个任务从2015年左右才开始逐渐得到重视，即所谓的“看图作文”，就是在提供一幅图像之后，希望计算机能够生成一句或一段文本描述图像里的内容。在过去两三年里，这一技术得到了非常大的进步，有些系统产生的文本描述已经可以和人对这个图像的描述媲美，甚至有些普通人已经不能够判断到底是机器自动生成的一段话，还是真人写出来的一段话。实现这一任务采用的方法也是以深度学习为基础的。\n还有一些类似艺术创作的技术进展，比如我们可以通过计算给一幅图像转化风格，把一个人的头发颜色改掉，加上一个刘海，或者加上眼镜，所产生的图像可以以假乱真；我们也可以把一副普通的图像变成莫奈风格的油画，把马变成斑马，把冬天的照片变成夏天的照片。\n上个月，iPhone X的发布使我们进一步对人脸识别应用有了更加深刻的认识。其实在人脸识别领域，过去两到三年，也出现了2-4个数量级的错误率下降。苹果声称iPhone X所采用的FaceID错误率大概在百万分之一，意味着如果有一百万个人捡到你的手机，只有一个人可以冒充你解锁成功。因为它采用的传感器是RGBD相机，里面不仅有彩色照片，还有深度信息、近红外信息，同时在注册阶段也会捕捉你的多幅照片，以及在识别阶段也是近距离进场的识别等等，这些方式都使得iPhone X的FaceID识别任务成为人脸识别领域一个相对比较容易的任务。其实三星Note3几年前就已经可以用人脸识别解锁，华为也在去年与我们合作将人脸识别应用到了其荣耀Magic手机上去实现对手机的半解锁。\n其实人脸识别有非常多不同的应用场景，手机的应用只是其中之一，即使是一比一验证你是不是你的任务，也有不同的应用场景。比如，在机场、车站等应用场景，用身份证中的卡内人脸照片和持卡人人脸比对，在过去3-4年里错误率大概下降了2-4个数量级，达到了万分之一甚至更低的错误率，即有一万个人试图冒充你，只有一个人可能成功，在这种情况下，本人持自己身份证可以有95%以上的正确识别率。企业员工刷卡后进行人脸验证的正确率则可以高达99%。\n对于这些技术背后的AI，如果我们用一个简单的公式来表达，那就是“A+B+C”。A是Algorithm即算法，B是Bigdata大数据，C是算力Computing。我想这样的公式或这样的说法，最近一段时间大家都越来越熟悉了。这三者中，A即算法，最主要的就是指深度学习算法了。\n所谓深度学习其实并不是新的技术，在上世纪八十年代中后期的时候，理论、方法就基本成熟，但因为当时没有大量数据，没有足够强的计算能力，这就使在当时我们不可能发挥它的作用。\n2012年之后，因为互联网和物联网的发展，使我们有更多机会收集大量数据，再加上有GPU等平民化高性能计算设备的出现，使我们有机会完成大规模的深度学习算法的训练。\n深度学习在计算机视觉领域，解决了或者至少推动了一大类非线性的映射函数学习的问题。换句话说，给我们一张照片，这些照片就是一些数值，形成输入x，我们通过深度模型学习一个F函数，用F作用于x，即F(x)得到我们想要得到的Y，这个Y可能是一个标签（比如猫，狗），也可能是我们想要分割的结果。\n这样的方式，使我们做AI的方法论产生了极大变化。从过去，我们大量依赖人类专家知识来设计算法，到现在，变成有大监督大数据驱动的方法为主。\n以一个具体的应用需求为例（从客户那挖掘出来的案例）。\n我们一个客户做了小区巡逻机器人，物业希望这个机器人可以帮助解决小区管理中的一个痛点问题。小区里经常有小狗乱拉屎，所以物业需要一个狗屎检测系统。这样的话，巡逻机器人可以及时发现这样的垃圾，然后“报警”，由保洁及时来清除掉。\n在没有深度学习的时候，我们需要做的是：\n第一步，收集一定量的包含狗屎的图像数据。\n第二步，人工设计或选择一些特征。\n第三步，选择某种分类器在收集的数据集合上测试它，看它效果好不好。如果不够好就回到第二步，不断进行反馈和调整。\n这是一个人工设计特征的过程，这样的方式非常耗时，非常不高效。我们做人脸检测花了20年，做行人车辆检测大概花了10年，即使狗屎检测相对容易，可能也需要至少一年。深度学习来了之后，整个过程变得很不一样。如果我们采用众包等方式，可能在一个月时间里就可以收集上万张标注了狗屎的照片，然后算法工程师可以根据经验选择一个深度学习算法，并设定一些超参数，然后只需要让机器在收集的数据集上进行训练和学习就可以了，这个过程可以非常快速的完成，大概只需要三个月。从过去的数年到现在的数月，显然大大提高了我们研发一项AI技术的效率。\n这样的方法论极大的提高了视觉技术的水平和落地效率。\n我认为很多场景下能看的AI才有真的智能。所以，视觉智能会有大量场景化需求，如果我们去细看每一个领域，从公共安全、机器人、自动驾驶、无人机到医疗，每个领域我们都可以非常轻易的发现视觉的用武之地。如果AI有一双眼睛（也就是有摄像头），我们背后有合适的算法，机器就可以更多的替换或者辅助人更好、更高效的做我们想要它做的事情。\n但从落地角度来讲，也存在非常多问题。\n问题一：个性化需求非常多，可批量复制的“标品”比较少。\n以“狗屎”识别机器人为例，可能明天还需要一个塑料瓶子检测，后天是塑料袋识别，再后天是白菜识别，这么多不同的物体，如果我们都采用前面说的那种开发方式，每种东西需要至少三个月，那么我们就会面临非常重的开发任务，关键是现在并没有这么多人才可以去做这么多事。\n从落地角度来看，谁来做、谁去买单、谁去开发算法，采用什么样的商业模式和合作模式都是问题。\n问题二：从计算力角度讲，深度学习的计算成本相对比较高。最近很多的AI专用芯片市场就是在解决这类问题。\nAI技术的生产效率现在是比较低的，我们要加快生产效率，就需要人力，需要高水平的AI算法人才。可是AI的人才奇缺。现在深度学习专业硕士毕业生可以拿到30-50万年薪，博士则可以高达50-80万年薪。在座的女孩们，如果没有男朋友的话，到我们这个领域看一看。\n相比可用的人才数量，这么多的视觉处理任务，如果每个任务都要2个硕士博士做3-5个月才能完成，这将是灾难性的事情。\n所以，未来我们需要新的方法论，从现在有监督大数据驱动的方法论，变成知识和数据联合驱动的方法论，为了完成这些事情，我们需要更强大的机器学习方法。使得我们在不同数据条件下也可以获得稳定、可靠的识别能力，这就体现在我们可能需要利用小数据、脏数据进行机器学习。\n此外，用来学习的数据还可能是半监督的数据、弱监督的数据，比如给你一张照片告诉你其中有狗屎，但并没有明确告诉你这个狗屎在什么位置，如果我们能有可以充分利用这些数据的更好的机器学习方法，我们才可能更加快速的开发AI技术。\n这还不够，我们还希望有更快捷的AI开发方法。比如我们希望开发一个安全帽检测的引擎，这是实际需求。我们一旦把这个任务交给机器之后，希望AI生产平台可以全自动完成全部的开发过程。机器完成这个任务的可能流程是：首先，它会理解这是一个检测任务，检测目标是安全帽，然后机器自动在百度上去搜索大量安全帽的图像，然后在百度上搜索一些关于安全帽的知识描述，例如安全帽多数是圆的，颜色各异，经常戴在人头上等等。然后，算法就通过对这样一些数据的自动处理和学习，以及知识的利用完成一个“安全帽检测”AI引擎的开发。\n遗憾的是，从算法的角度来讲，以我对该领域学术前沿的了解，要达到这样的目标我们可能还需要5-10年，还不一定100%完全做到那种程度。\n在此之前，工业界最靠谱的做法恐怕还是采用“数据暴力”来完成多数类似AI任务的研发。但我们如何解决缺少大量AI算法工程师的问题呢？我认为我们需要一个更强大、更便捷的AI算法生产基础设施。这样的基础设施，就像当年从专业相机到傻瓜相机的历史演变一样。\n为了让我们有更多的人才可以开发AI，以满足大量的视觉智能开发任务，我们的AI生产工具要从Caffe，Mxnet和Tensorflow等只能昂贵的高端人才可以使用的开发工具发展到“傻瓜式”的AI开发平台。这样的平台应该使更多的中低端人才，即使不懂AI、不懂深度学习，也可以经过简单的培训，就可以利用自己的私有数据，在这样的软硬建基础设施平台上，方便快捷的开发出自己所需要的AI技术引擎，并便捷的嵌入自己的业务系统中。\n我作为主要创始人成立的中科视拓，自去年8月成立以来，不仅做了大量人脸识别、无人机视觉等计算机视觉技术服务项目，开发了多款人脸识别产品和解决方案。与此同时，我们已经研发了一个称为SeeTaaS的深度学习算法开发平台，这个平台不但在我们公司内部逐步得到了应用，也已经开始提供给我们的B端客户，使他们也具备了用自己的私有数据训练自己所需的深度学习算法和引擎的能力。相信这个SeeTaaS平台会越来越好用，最终实现我们“让天下没有难开发的AI”这一梦想！"}
{"content2":"摘要：随着计算机等技术的不断发展，计算机视觉技术被广泛运用到各个领域中。与此同时，随着人口数量的增长、城市化进程导致耕地面积的减少，农业向着高质量、高产量方向的发展成为关键。将计算机视觉技术应用在农业领域能够在一定程度上降低虫害等对农业的影响，推进农业向着高质量、高产量的方向不断发展。本文简要回顾计算机视觉领域的几个重要任务和方法，介绍当前计算机视觉技术在农业领域中的应用。\n关键词：计算机视觉；农业；深度学习\n一、引言\n从2016年AlphaGO战胜人类围棋冠军李世石，到2017年升级版的AlphaGO战胜世界排名第一的棋手柯洁，AlphaGO也从Lee版本发展到Master版本再到Zero版本[1, 2]，最终来到基于强化学习的Alpha Zero，人工智能得到越来越多的关注。计算机视觉，利用计算机强大的计算能力去感知世界，并作为人工智能的一个重要分支，在整个人工智能发展史上有着重要作用。随着深度学习时代的来临，计算机得以在视觉相关的任务中取得优于人类的表现，2015年Microsoft Research提出的ResNet夺得ImageNet挑战赛中分类任务冠军并首次超越人类在该任务中的表现[3]。\n计算机视觉技术不断发展，在各项视觉任务中取得优异表现，也使其被应用于多个领域，如医疗、交通等。2018年，加州大学圣地亚哥分校张康教授团队使用基于图像的深度学习工具对疾病等进行诊断，在多个任务中超越人类专家，该项研究成果被刊登在《Celll》杂志封面[4]；随着无人驾驶、智慧城市等的发展，计算机视觉也被广泛应用于交通领域用于缓解城市道路拥挤、减少交通事故等。\n此外，据不完全估计地球总人口将在2050年突破100亿，人口的爆炸式增长必将带来食物需求的增加，环境的破坏会使得全球气候恶劣、温室效应等，同时城市化进程使得耕地面积减少。人类对食物的需求、对饮食质量的要求，与耕地面积、气候环境构成了矛盾，人类需要用更少的土地种植出更多更好的农产品以满足人类发展的需求。人工智能技术尤其是计算机视觉技术，结合基因组学、植物表型、农业工程等使得这一矛盾得以解决，大力推进农业向着高质量、高产量的方向不断发展。\n二、计算机视觉\n计算机视觉，（Computer Vision, CV）是一门研究如何使机器“看”的科学，其以图像（视频）为输入，以对环境的表达（representation）和理解为目标，研究图像信息组织、物体和场景识别等，包括但不限于物体分类、物体检测、物体分割、物体追踪、三维重建等。\n2.1 视觉任务\n\n物体分类（Object Classification）任务要求回答图像中是否包含某种物体，如一张图像中的动物是猫还是狗。物体检测（Object Detection）任务不仅需要识别出图像中所含物体的种类，而且需要指出该物体所处的位置并且使用边框（Bounding Box）标出。物体分割（Object Segmentation）任务在尺度上更细，往往在像素尺度进行分类，将不同的物体分割。\n\n计算机视觉中，一个场景的二维表示往往会丢失物体的某些信息，而三维表示能够更全面地描述一个场景或者物体，所以有一些研究者将目光聚焦在同时定位与地图创建（Simultaneous Localization and Mapping, SLAM）、三维重建（3D Reconstruction）等。\n2.2 研究方法\n在前深度学习时代，各类视觉任务主要依赖人工选取的特征和分类器，如传统的机器学习算法——决策树（Decision Tree）、随机森林（Random Forest）、支持向量机（Support Vector Machine, SVM）等。但是这类方法通常需要人工设计或者选择特征，如何让计算机具有表示学习的能力成为关键。\n前深度学习时代的机器学习算法主要依赖于人工设计的特征，学习算法在该特征基础上对权重等进行数值优化。而深度学习具有表示学习的能力，无需人工设计的特征进行学习。2012年，Hinton等使用AlexNet在ImageNet大规模视觉识别挑战赛分类任务中获得冠军，并大幅降低top-5错误率[5]。2015年Microsoft Research提出的ResNet夺得ImageNet大规模视觉识别挑战赛分类任务冠军并首次超越人类在该任务中的表现[3]。\nResNet[3]\n在物体检测任务中，基本思路是使用不同大小的窗口在图像上滑动，在每个区域，对窗口内的区域进行目标定位。RCNN从输入图像中选择候选区域，对于每个候选区域使用卷积神经网络计算特征，基于该特征使用支持向量机进行训练并将输出结果进行回归得到最终的检测结果[6]。Fast R-CNN中不同候选区域的卷积特征提取部分共享，基于候选区域生成算法的结果在卷积特征上进行采样，最终对每个候选区域进行目标定位[7]。Faster R-CNN利用候选区域网络从conv5特征中产生候选区域，并且将候选区域网络集成到整个网络中端到端训练，将检测速度进一步提升[8]。在物体分割任务中主要使用的网络结构为Mask R-CNN，其在Fast R-CNN输出的基础上增加掩膜的输出，实现对物体的分割[9]。\nR-CNN[6]\n2.3 主要数据集\n2.3.1 ImageNet\n该数据集来自斯坦福计算机视觉实验室，训练集包含128万张图像，测试集包含10万张图像，涉及的物体种类超过1000类[10]。\nImageNet\n2.3.2 PASCAL VOC\nPASCAL Visual Object Classes数据集包含20个类别，主要用于物体检测任务，通常使用VOC07和VOC12的训练集并集作为训练，用VOC07的测试集作为测试[11]。\n2.3.3 COCO\n微软的Common Objects in Context数据集用于物体检测、物体分割等任务，提供图像和对图像的语义文本描述信息[12]。\n三、计算机视觉在农业领域中的应用\n在前深度学习时代，人工设计的特征和浅层分类器被广泛应用于各个领域。\nBakhshipour A等人将彩色RGB图像转换为二值图像，分别通过检测边缘抽取傅里叶描述子、计算时不变特征和外形因素等，以均方误差为评价指标，使用支持向量机和人工神经网络进行训练[13]。\nGuo W等人将物体分割物体问题变为分类问题[14]，通过一系列假设将图像中的不同像素进行标注——将像素分为植物和背景，并基于该标注数据提取不同颜色空间的特征训练决策树模型。为了保证结果的鲁棒性，作者使用5张不同情况下的图像，包括晴天、阴影、光反射、雨天等，从图像中选取135000个像素进行训练，经过消除噪声等步骤后生成的掩膜能够较好地对图像进行分割，结果如所示。\n\n在深度学习时代，卷积神经网络被广泛用于物体分类等领域，经典的网络结构包含卷积层、池化层、随机失活层、非线性激活函数等。Khan Z等人提出了一种使用RGB彩色图像进行基于模型的植被指数估计的新方法[15]，在保留卷积层、池化层等卷积神经网络常规结构的基础上通过更换新的损失函数，实现对植被指数的估计。整个实验以RGB图像作为输入，输出结果为对应的植被指数，进而获取RGB图像所对应区域的作物健康状况、水分含量、营养含量等信息，具体结构如所示。\n\n植物表型指受基因和环境因素决定或影响并反映植物结构等的特征，通常能够很好地反应基因、环境对植物的影响。通过分析表型等，能够很好地指导农学专家将数据与已知遗传数据对比，将基因型和表型进行关联分析，达到高级遗传育种的目的。\n传统的检测设备体积过大或者过于复杂，农学专家操作复杂的检测设备较为困难，考虑到移动便携设备如智能手机在计算、摄像等方面具有很高的性能，Aquino A等人开发了一个基于安卓智能手机的应用程序vitisBerry用于估计葡萄粒数[16]。该应用程序通过手机自带摄像头获取葡萄图像，并对该图像进行处理和分析以估计葡萄粒数。\nvitisBerry[16]\nBaweja H S等人提出StalkNet，该方法利用深度学习方法获得植物秸秆数及茎宽[17]。StalkNet使用Faster RCNN进行物体检测并对检测结果进行计数，该计数结果即为该植物的秸秆数。StalkNet以物体检测结果作为全连接网络的输入，输出为植物秸秆的像素宽度，通过立体视觉匹配将像素宽度转换成植物秸秆的实际茎宽。\nStalkNet[17]\n上述的研究主要集中在二维图像，而三维物体的二维表示通常会丢失掉某些重要信息，三维表示能够更好反映物体在真实空间中的位置以及表型信息，下面介绍三维重建在农业领域中的应用。\nNguyen T T等人构建了基于结构光的植物三维重建系统[18]，如所示。该系统包括若干双目相机、结构光装置、可旋转平台等。该系统使用相机阵列获取图像，使用GPU完成匹配，生成点云并对植物进行三维重建。\nHuY等人使用Kinect相机获取叶类植物的图像，将获取的图像转换为点云信息，利用该信息生成被测量植物的三维模型，通过该三维模型能够获取该植物的相对高度、绝对高度、体积、叶片面积等信息，实现对植物生长参数的无损测量[19]。\n四、未来与挑战\n计算机视觉技术仍在快速发展，也越来越多地被运用在农业领域，但是还存在着一些挑战。\n4.1 农业领域大规模数据集\n目前，农业领域没有大规模的公共数据集，一些研究成果通常依赖研究人员自身收集的数据，在评价指标相同的情况下，研究结果不具有可比性。基于大规模公共数据集，不同的研究人员能够很好地比较其提出的方法和前人方法的优劣。同时，大规模数据集也使得基于深度学习的方法能取得更好的性能。\n4.2 最新方法的应用\n在前深度学习时代，计算机视觉技术往往是指基于图像处理的传统方法，在深度学习时代，越来越多的研究工作使用深度学习方法。与此同时，随着计算机科学的发展，越来越多的结构和方法层出不穷，注意力机制（Attention Mechanism）[20]和生成式对抗网络（Generative Adversarial Networks，GAN）[21]的出现使得深度学习领域进一步发展。\n将state-of-the-art的方法运用到农业领域也成为未来的一个可能，注意力机制模仿人看图像时，目光的焦点在不同的物体上移动，当神经网络对图像进行识别时，每次集中于部分特征上，识别结果更加准确；GAN通过对抗过程估计生成模型的新框架，判别模型和生成模型相互对抗，被用于图像生成或者数据增强，使得农业大规模数据集成为可能。\n4.3 复杂情况下的鲁棒表现\n不同于理想的实验室环境，实际的农业场景通常很复杂，不同的光照条件带来的阴影、亮度过亮等问题，混乱的背景等带来图像中的遮挡，不同的天气条件以及研究对象在不同生长时期具有不同的形态等，上述问题都给相关研究带来了巨大挑战。\n此外，农业是我国的第一产业，其发展与大众的衣食密不可分，与此同时农业领域的较多问题仍有待解决，希望计算机视觉技术被更广泛更好地运用到农业领域以解决实际问题。\n参考文献:\n[1] Silver D, Huang A, Maddison C J, etal. Mastering the game of Go with deep neural networks and tree search[J].Nature, 2016, 529(7587):484-489.\n[2] Silver D, Schrittwieser J, Simonyan K,et al. Mastering the game of Go without human knowledge[J]. Nature, 2017,550(7676):354-359.\n[3] He K, Zhang X, Ren S, et al. DeepResidual Learning for Image Recognition[J]. 2015:770-778.\n[4] Kermany D S, Goldbaum M, Cai W, et al.Identifying Medical Diagnoses and Treatable Diseases by Image-Based DeepLearning[J]. Cell, 2018, 172(5):1122–1131.e9.\n[5] Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]//International Conference on Neural Information Processing Systems. CurranAssociates Inc. 2012:1097-1105.\n[6] Girshick R, Donahue J, Darrell T, etal. Rich Feature Hierarchies for Accurate Object Detection and SemanticSegmentation[C]// IEEE Conference on Computer Vision and Pattern Recognition.IEEE Computer Society, 2014:580-587.\n[7] Girshick R. Fast R-CNN[J]. ComputerScience, 2015.\n[8] Ren S, Girshick R, Girshick R, et al.Faster R-CNN: Towards Real-Time Object Detection with Region ProposalNetworks[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence,2017, 39(6):1137-1149.\n[9] He K, Gkioxari G, Dollar P, et al.Mask R-CNN[C]// IEEE International Conference on Computer Vision. IEEE,2017:2980-2988.\n[10] Deng J, Dong W, Socher R, et al.ImageNet: A large-scale hierarchical image database[C]// Computer Vision andPattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009:248-255.\n[11] Everingham M, Gool L, Williams C K,et al. The Pascal Visual Object Classes (VOC) Challenge[J]. InternationalJournal of Computer Vision, 2010, 88(2):303-338.\n[12] Lin T Y, Maire M, Belongie S, et al.Microsoft COCO: Common Objects in Context[J]. 2014, 8693:740-755.\n[13] Bakhshipour A, Jafari A. Evaluationof support vector machine and artificial neural networks in weed detectionusing shape features[J]. Computers & Electronics in Agriculture, 2018,145:153-160.\n[14] Guo W, Rage U K, Ninomiya S.Illumination invariant segmentation of vegetation for time series wheat imagesbased on decision tree model[J]. Computers & Electronics in Agriculture,2013, 96(6):58-66.\n[15] Khan Z, Rahimieichi V, Haefele S, etal. Estimation of vegetation indices for high-throughput phenotyping of wheatusing aerial imaging[J]. Plant Methods, 2018, 14(1):20.\n[16] Aquino A, Barrio I, Diago M P, et al.vitisBerry: An Android-smartphone application to early evaluate the number ofgrapevine berries by means of image analysis[J]. Computers & Electronics inAgriculture, 2018, 148:19-28.\n[17] Baweja H S, Parhar T, Mirbod O, etal. StalkNet: A Deep Learning Pipeline for High-Throughput Measurement of PlantStalk Count and Stalk Width[M]// Field and Service Robotics. 2018.\n[18] Nguyen T T, Slaughter D C, Max N, etal. Structured Light-Based 3D Reconstruction System for Plants.[J]. Sensors,2015, 15(8):18587-612.\n[19] Hu Y, Wang L, Xiang L, et al.Automatic Non-Destructive Growth Measurement of Leafy Vegetables Based onKinect[J]. Sensors, 2018, 18(3):806.\n[20] Xiao T, Xu Y, Yang K, et al. Theapplication of two-level attention models in deep convolutional neural networkfor fine-grained image classification[C]//Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition. 2015: 842-850.\n[21] Goodfellow I J, Pouget-Abadie J,Mirza M, et al. Generative Adversarial Networks[J]. Advances in NeuralInformation Processing Systems, 2014, 3:2672-2680."}
{"content2":"机器学习、数据挖掘、计算机视觉等领域经典书籍推荐\n人工智能、机器学习、模式识别、计算机视觉、数据挖掘、信息检索、自然语言处理等作为计算机科学重要的研究分支，不论是学术界还是工业界，有关这方面的研究都在如火如荼地进行着，学习这些方面的内容有一些经典书籍，现总结如下，方便自己和大家以后学习研究：\n人工智能：\n《Artificial Intelligence: A Modern Approach》，第三版，Russell著，权威、经典的人工智能教材，阐述了人工智能的核心内容，反映了人工智能最近10年来的新进展。\n《ProgrammingCollective Intelligence》，Toby Segaran著，本书将带你进入机器学习和统计学的世界，对算法的描述简明清晰，很对代码都可以直接拿去实际应用。\n数据挖掘：\n《DataMining, Concepts and Techniques》，第三版，Han著，数据挖掘领域最具里程碑意义的经典著作。\n《DataMining: Practical Machine Learning Tools and Techniques》,第二版，Witten著，介绍了机器学习的基本理论和实践方法，并提供了一个公开的数据挖掘工作平台Weka，算法部分介绍得很详细。\n信息检索：\n《An Introductionto Information Retrieval》,Manning著，这是一本介绍信息检索的入门书籍，书中对信息检索的基本概念和基本算法做了介绍，适合初学者。\n《Search Engines Information Retrieval in Practice》,Croft著，这本书讲述了搜索引擎的构造方法，通过实际代码展示了搜索引擎的工作原理，对于学生和从事相关领域的工程师，本书都值得一看。\n《Managing Gigabytes》，《Mining the Web -Discovering Knowledge from Hypertext Data》\n《Information Theory：Inference and Learning Algorithms》。\n模式识别和机器学习：\n《Pattern Classification 》，第二版，Duda著，模式识别的奠基之作，但对SVM、Boosting几乎没提，有挂一漏万之嫌。\n《Pattern Recognition and Machine Learning》,Bishop著，侧重概率模型，详细介绍了Bayesian方法、有向图、无向图理论等，体系完备。\n《Kernel Methods for Pattern Analysis》,John Shawe-Taylor著，SVM等统计学的诸多工具里都用到了核方法，可以将将低维非线性空间映射到高维的线性空间中，但同时会引入高维数据的难题。\n计算机视觉：\n《Computer Vision: A Modern Approach》，第二版，Forsyth著，一本不错的计算机视觉教材，全书理论联系实际，并加入了计算机视觉领域的最新研究成果。\n《Computer Vision: Algorithms and Applications》,Richard Szeliski的大作，《数字图像处理》课程老师推荐的一本书籍，这本书我还没有看完，书中对计算机视觉领域最新的一些算法进行了汇编，包括图像分割，特征检测和匹配，运动检测，图像缝合，3D重建，对象识别等图像处理的诸多方面，借助本书我们可以对最新主流图像处理算法有个全局把握。\n线性代数：\n《Linear Algebra and Its Applications》Fourth Edition, Gilbert Strang的著作，本书详细介绍了向量空间、线性变换、本征值和本征向量等线性代数的重要基本概念，把抽象的线性空间形象地表达出来，适合初学者。\n《Introduction to Probability Models》第10版，Ross著，一本书能够发行到第十版，你说是不是很经典呢？\n离散数学：\n《Discrete Mathematics and Its Applications》，第六版，Rosen著，本书囊括了离散数学推导、组合分析、算法及其应用、计算理论等多方面的内容，适合初学者。\n矩阵数学：\n《Matrix Analysis》,Horn著,本书无疑是矩阵论领域的经典著作了，风行几十年了。\n概率论与数理统计：\n《All Of Statistics》,Wasserman著，一本数理统计的简介读本。\n《Introductionto Mathematical Statistics》，第六版，Hogg著，本书介绍了概率统计的基本概念以及各种分布，以及ML，Bayesian方法等内容。\n《Statistical Learning Theory》Vapnik的大作，统计学界的权威，本书将理论上升到了哲学层面，他的另一本书《The Nature ofStatistical Learning Theory》也是统计学习研究不可多得的好书，但是这两本书都比较深入，适合有一定基础的读者。\n《统计学习方法》，李航著，国内很多大学都在用这本书，本书从具体问题入手，由浅入深，简明地介绍了统计学习的主要方法，适合初学者而又想对统计学习理论有一个全局理解的学生。\n《The Elements of Statistical Learning-Data Mining, Inference, and Prediction》,第二版，Trevor Hastie著，机器学习方面非常优秀的一本书，较PC和PRML,此书更加深入，对工程人员的价值也许更大一点。\n《AnIntroduction to Probabilistic Graphical Models》,Jordan著，本书介绍了条件独立、分解、混合、条件混合等图模型中的基本概念，对隐变量（潜在变量）也做了详细介绍，相信大家在隐马尔科夫链和用Gaussian混合模型来实现EM算法时遇到过这个概念。\n《Probabilistic Graphical Models-Principles and Techniques》，Koller著，一本很厚很全面的书，理论性很强，可以作为参考书使用。\n最优化方法：\n《Convex Optimization》，Boyd的经典书籍，被引用次数超过14000次，面向实际应用，并且有配套代码，是一本不可多得的好书，网址http://www.stanford.edu/~boyd/cvxbook/。\n《Numerical Optimization》，第二版，Nocedal著，非常适合非数值专业的学生和工程师参考，算法流程清晰详细，原理清楚。\n另外推荐几个博客和网站：\nhttps://www.coursera.org/，这是一个由世界顶级大学联合创办的网上在线视频公开课网站，里面有stanford, MIT,CMU等计算机科学一流大学提供的免费教学视频，内容全面，计算机科学方面的资源较网易视频公开课网站（http://open.163.com/）内容要新、要全。\nhttp://blog.csdn.net/pongba/article/details/2915005，本文的部分内容就是借鉴刘未鹏大神的博客而来的，也正是看过他的那个书单后，我才决定写一个总结归纳性的文章，这样可以方便大家学习，更可以勉励自己多看些有益的经典书籍。\nhttp://blog.pluskid.org/，这是浙大学生张驰原的博客网站，现在他去了MIT，博客里面的很多资源都值得一看，博文的很大一部分都是关于机器学习的，加入了作者自己的理解，深入浅出。\nhttp://blog.csdn.net/ffeng271/article/details/7164498，林达华推荐的基本数学书，转自MIT大牛博客。"}
{"content2":"计算机视觉在交通领域主要有如下几个方面的应用：第一个是感知，既车辆的检测，第二个是车辆身份的识别，第三是车辆身份的比对，第四个是车辆的行为分析，第五个是驾控，也就是现在非常火的汽车辅助驾驶与无人驾驶。\n车辆检测与感知\n检测就是计算机通过图片或者视频，把其中的车辆或其它关注目标准确的“框”出来，检测是很多系统的基础。在2012年以前，很多智能交通系统中用的检测是一种基于运动的检测，这种检测会受天气、光线等方面的影响，在不同天气下会存在很多问题。而基于深度学习的检测，是基于车辆的轮廓和形态的检测，是完全模拟人看车的方式，只要人眼可以辨识那是一辆车，就可以“框”出来，这个就可以解决很多过去车辆检测中存在的问题，排除了天气光线等来带的干扰。\n路口的感知\n目前的国内很多城市交通拥堵情况很严重，很多十字路口的红绿灯配时其实并不是最优的，通过基于深度学习的车辆精确感知检测，可以精准的感知交通路口各个方向的车辆数量、流量和密度，从而可以给交通路口的最优配时提供准确依据。如果各个路口都用上这种车辆检测技术，那对交通拥堵将是极大的缓解。\n路段的感知\n经过过去几年的建设，我国的大中型城市都安装了很多监控摄像头，通过路段的感知，可以基于原有监控系统获取到道路的总体交通路况，通过这种车辆检测技术就可以为道路路况分析、交通大数据、交通规划等提供可靠的数据依据。\n路侧停车的感知\n有两个方面的应用，一个是路侧违法停车的感知和抓拍，不再需要摄像机去轮询检测，大大提高了摄像机的使用寿命。另外一个就是路侧停车位的管理，之前的方案在外场要感知车位是否被占用，一般通过地磁感知，成本非常高，系统可靠性也是问题；基于图像的识别则可以很好的解决这个问题，一台摄像机即可监控和感知一大片区域的停车位是否被占用，成本低还所见即所得。\n停车场的感知\n现在室内停车场应用图像识别实现车位检测的已经比较多了，但是现在很多车的检测都是基于车牌，有车牌就可以检测出来，没车牌检测不出来，甚至有的车牌效果不太好也无法检测。而基于深度学习的车辆检测，只看车辆的轮廓，不看车牌，只要看起来像个车的，就可以检测出来，而且精度很高。现在通过计算机视觉技术，可以做到模拟人的视觉感知，哪个地方有车停，哪个地方是空位，直接检测出来把数据发送给平台，发布到停车场诱导系统上。\n车辆身份特征识别\n计算机视觉用于智能交通的第二个大的应用领域就是车辆的身份识别。目前，常用的ETC和电子标签技术识别车辆确实非常可靠，而且精度还是比图像识别要高一些。但是现实中还存在很多现实问题，比如说现在很多大货车无法用ETC，还有ETC系统遭到破坏，怎么办？而电子标签真正落地还需要时间。在这个时间窗口，如何实现车辆身份特征的唯一性识别？通过深度学习提升的车辆识别不仅仅是车辆的车牌识别准确率，还能实现更多维度的识别，现在的“车脸识别”技术不仅能精确识别车牌，还有车辆的颜色、类型、品牌年款、车辆里人物、车辆挡风玻璃上的特殊标志以及车辆尾部的特征标志等。\n车辆的比对\n计算机视觉用于智能交通的第三个大的应用领域就是车辆的比对，最典型的应用就是以图搜图，如何在海量图片里精准的找到一辆车，所谓世界上没有两片相同的树叶，也没有两辆完全一样的车。如下图，看着像是一样的，但是仔细区分，还是会发现不同。\n基于视图大数据的以图搜图功能，可以在海量图片里找到一辆特定的车，不管有没有号牌，这里还包括一些其它的功能，如套牌车分析等等，套牌车在以前，唯一的方法就是举报，但现在计算机可以通过两个车牌是完全一样的车，通过车型比对和车辆特征比对来鉴定是否套牌车。\n车辆比对的另外一个应用场景就是收费结算，目前车牌识别用在停车场的支付里，还有一些遗留问题，就是还存在无牌车、污牌车和套牌车，因而依然必须依赖人工参与。有没有一种办法可以减少或者是不让人工参与呢？车脸识别就可以解决这一问题，可以构建车辆多层多维度的特征，相当于得到一个车辆的肖像，然后通过特征比对去判断是否同一辆车。\n交通视频的分析应用\n计算机视觉应用在智能交通的第四个大的应用领域就是车辆的行为分析。\n第一个是交通事故及事件检测，基于连续视频可以分析车辆的行为，检测如车辆停车、逆行等行为，发现交通事故和交通拥堵进行报警。借助深度学习技术，能实现真正准确的交通事件检测系统，真正的帮交通运营部门提供准确及时的报警信息。\n第二个就是车辆违章抓拍，这些近几年在我国应用非常广泛，而且利用视频检测实现的非现场执法的种类越来越多，现在甚至连开车接打电话都可以识别抓拍，这些都得益于计算机视觉技术的快速进步。\n无人驾驶和汽车辅助驾驶\n最后要说的一个应用领域就是汽车驾控，就是当前非常热的无人驾驶和汽车辅助驾驶。其中非常重要的一个技术点就是图像识别，通过图像识别前方车辆、行人、障碍物、道路以及交通信号灯和交通标识，这项技术的落地应用将给人类带来前所未有的出行体验，重塑交通体系，并构建真正的智能交通时代。\n总结一下，计算机视觉技术过去5年内取得的成绩甚至是远远超过了之前的20年，得益于深度学习技术带来的巨大进步，计算机视觉的广泛应用，能够大大提升智能交通系统的感知精度与维度，让智能交通系统更加智慧。\n通过深度学习技术，未来能够让移动支付在智能交通系统中更加快速的落地，让无人驾驶的美好梦想变成现实，从而给全人类带来更加安全、便捷、舒适的出行体验。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n人工智能与计算机视觉\nhttp://www.duozhishidai.com/article-15129-1.html\n计算机视觉如何入门\nhttp://www.duozhishidai.com/article-8235-1.html\n计算机视觉领域内，必知的十个深度学习架构\nhttp://www.duozhishidai.com/article-861-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"1. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)\n* Location / Date: Salt Lake City, June 18-21, 2018\n* Paper Submission Deadline: November 15, 2017\n* Acceptance rate: 20% ~ 30%\n* http://cvpr2018.thecvf.com/\n2.  The 15th European Conference on Computer Vision (ECCV 2018) (两年一次)\n* Location / Date: Munich, Germany; September 8-14, 2018\n* Paper Submission Deadline: March 14, 2018 (23:59 CET)\n* Acceptance rate: 20% ~ 30%\n* https://eccv2018.org/\n3. IEEE International Conference on Computer Vision (ICCV 2018)\n* Location / Date: Istanbul, Turkey; January 30-31, 2018\n* Paper Submission Deadline: November 30, 2017\n* Acceptance rate: 20% ~ 30%\n* http://www.guide2research.com/conference/iccv-2018\n4. Conference on Neural Information Processing Systems (NIPS 2018)\n* Location / Date: Montreal, Canada; December 03-08, 2018\n* Paper Submission Deadline: May 18, 2018\n* Acceptance rate: 20%~ 30%\n* https://nips.cc/\n5. The 14th Asian Conference on Computer Vision (ACCV 2018)\n* Location / Date:Perth, Australia; December 02-06, 2018\n* Paper Submission Deadline: July 05, 2018\n* Acceptance rate: ~ 30%\n* http://accv2018.net/\n6. The 29th British Machine Vision Conference (BMVC 2018)\n* Location / Date:Northumbria University; September 03-06, 2018\n* Paper Submission Deadline: May 07, 2018\n* Acceptance rate: ~ 40%\n* http://bmvc2018.org/index.html\n7. IEEE International Conference on Image Processing (ICIP 2018)\n* Location / Date: Athens, Greece; October 7-10, 2018\n* Paper Submission Deadline: February 07, 2018\n* Acceptance rate: ~ 45%\n* https://2018.ieeeicip.org/\n8. The 24th International Conference on Pattern Recognition (ICPR 2018)\n* Location / Date: Beijing, China; August 20-24, 2018\n* Paper Submission Deadline: January 22, 2018\n* Acceptance rate: ~ 55%\n* http://www.icpr2018.org/\n9. The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)\n* Location / Date: Louisiana, USA; February 2-7, 2018\n* Paper Submission Deadline: October 13, 2017\n* Acceptance rate:\n* https://aaai.org/Conferences/AAAI-18/"}
{"content2":"在看论文的时候经常会碰到register这个词，比如：\nGiven an image (or possibly several images of the scene), retrieve a set of intrinsic images, all registered with the original image, each describing one intrinsic characteristic\n在wiki上查到有关image registration的描述：\nImage registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints.\n阅读之后发现没什么复杂的，说图片A is registered with B 的意思就是A，B这两张图片共用一个坐标系，比如A照片是相机a拍的，B照片是相机b拍的，A is registered with B 的意思就是将A进行变换，使其拍摄相机的坐标与b的坐标相同，还有就是要共用一个尺度，比如图片A的数据是小数，范围从0到1，图片b的数据为整数，范围从0到255，我猜，将A的数据乘以255就是A is registered with B 的意思。我理解的registered with 有这两层意思，还望指点。"}
{"content2":"CV：人工智能之计算机视觉方向的简介、相关算法、计算机视觉八大应用(知识导图+经典案例)之详细攻略\n计算机视觉方向的简介\n计算机视觉是一门研究如何使机器“看“的科学。\n用摄影机和计算机代替人眼对目标进行获取、处理、分析、识别等，使计算机从数字图像或视频 中获得高层次的理解。\n从工程的角度来看，它寻求自动化人类视觉系统可以完成的任务。\n深度学习＋计算机视觉：目前人工智能最活跃的领域\n计算机视觉的应用\n无人驾驶，\n无人安防，\n人脸识别，\n车辆车牌识别，\n以图搜图，\nVR/AR,\n3D重构，\n医学图 像分析，\n机器人，\n无人机\n计算机视觉方向的相关算法\n后期更新……\n计算机视觉方向的经典案例\n1、图像分类(Image Classification)\n2、目标检测(Object Detection )\n3、图像分割(Image Segmentation)\n语义分割\n实例分割\n全景分割\n4、图像生成(Image Generation)\n5、图像检索(Image Retrieval)\n查询图像→特征提取→特征向量→相似度计算→\n图像数据库→特征提取→特征数据库→相似度计算→\n6、图像描述(Image Captioning)\n7、人脸识别(Face Recognition)\n人脸验证\n人脸识别\n8、超分辨率Super resolution\nSRGAN\n9、图像风格迁移(Neural Style 图像风格变换) CycleGAN"}
{"content2":"现在刚接触人工智能，接触到了以下一些名词，需要好好了解一下。\n人工智能前期调研：\n神经网络\n深度学习\n机器学习\n自然语言识别\n计算机视觉\n神经网络\n参考博客\n深度学习框架\n主流深度学习框架\n机器学习之深度学习\n深度学习之与机器学习\n总结\n人工智能是一个很大的集合它诞生于计算机技术刚刚给人们带来一些欣喜的时候即从上世纪五十年代一直到上世纪八十年代，上世纪八十年代人们在从长期的计算机应用基础上总结出来了机器学习的一些算法并将其发展成了一门重要的计算机应用技术。直到2010年，随着高性能的移动互联网设备的全面普及，在机器学习里面的一项需要大量计算的算法开始流行起来–神经网络算法。而当前最流行的深度学习就是基于机器学习里面的神经网络算法。\n深度学习的应用领域\n在图像识别领域，应用深度学习的卷积神经网络（CNN）算法，在语音识别领域，则应用了深度学习的递归神经网络（RNN）算法。"}
{"content2":"与所有其它学术领域都不同，计算机科学使用会议而不是期刊作为发表研究成果的主要方式。目前国外计算机界评价学术水平主要看在顶级学术会议上发表的论文。特别是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。（但中国目前的国情不同于国外，我国主要看在学术期刊上发表的SCI论文。这种“一切以SCI期刊为评价标准”的做法已有不少批评。）\n会议论文比期刊论文更重要的原因是：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n作为刚入门的CV新人，有必要记住计算机视觉方面的三大顶级国际会议：ICCV，CVPR和ECCV，统称为ICE。\nICCV的全称是International Comference on Computer Vision（上一篇文章介绍我自己的id的时候介绍过，呵呵），正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。\nCVPR的全称是International Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster挑自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。\nICCV/CVPR/ECCV三个顶级会议, 都在一流会议行列, 没有必要给个高下. 有些us的人认为ICCV/CVPR略好于ECCV,而欧洲人大都认为ICCV/ECCV略好于CVPR。\n笔者就个人经验浅谈三会异同, 以供大家参考和讨论. 三者乃cv领域的旗舰和风向标,其oral paper (包括best paper) 代表当年度cv的最高水准, 在此引用Harry Shum的一句话, 想知道某个领域在做些什么, 找最近几年此领域的proceeding看看就知道了. ICCV/CVPR由IEEE Computer Society牵头组织, ECCV好像没有专门负责的组织. CVPR每年(除2002年)都在美国开, ECCV每两年开一次,仅限欧洲, ICCV也是每两年一次, 各洲轮值. 基本可以保证每年有两个会议开, 这样研究者就有两次跻身牛会的机会.\n就录取率而言, 三会都有波动. 如ICCV2001录取率>30%, 且出现两个人(华人)各有三篇第一作者的paper的情况, 这在顶级牛会是不常见的 (灌水嫌疑). 但是, ICCV2003, 2005\n两次录取率都很低, 大约20%左右. ECCV也是类似规律, 在2004年以前都是>30%, 2006年降低到20%左右. CVPR的录取率近年来一直偏高, 从2004年开始一直都在[25%,30%].最近一次CVPR2006是28.1%, CVPR2007还不知道统计数据. 笔者猜测为了维持录取paper的绝对数量, 当submission少的时候录取率偏高, 反之偏低, 近几年三大会议的投稿数量全部超过1000, 相对2000年前, 三会录取率均大幅度降低, 最大幅度50%->20%. 对录取率走势感兴趣的朋友, 可参考http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的),http://www.adaptivebox.net/research/bookmark/CICON_stat.html .\n显然, 投入cv的人越来越多,这个领域也是越来越大, 这点颇不似machine learning一直奉行愚蠢的小圈子主义. 另外一点值得注意, ICCV/ECCV只收vision相关的topic, 而cvpr会收少量的pattern recognition paper, 如fingerprint等, 但是不收和image/video完全不占边的pr paper,如speech recognition等. 我一个朋友曾经review过一篇投往CVPR的speech的paper, 三个reviewer一致拒绝, 其中一个reviewer搞笑的指出, 你这篇paper应该是投ICASSP被据而转投CVPR的. 就topic而言, CVPR涵盖最广. 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会, 故CVPR会优先接收很多来自us的paper (让大家都happy).\n以上对三会的分析对我们投paper是很有指导作用的. 目前的research我想绝大部分还是纸上谈兵, 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程. 故了解投paper的一些基本技巧, 掌握领域的走向和热点, 是非常必要的. 避免做无用功,选择切合的topic, 改善presentation, 注意格式 (遵守规定的模板), 我想这是很多新手需要注意的问题. 如ICCV2007明文规定不写summary page直接reject, 但是仍然有人忽视, 这是相当不值得的."}
{"content2":"1----ICCV 的全称是 IEEE International Conference on Computer Vision，国际计算机视觉大会\n2--ECCV的全称是Europeon Conference on Computer Vision\n3--CVRP 国际计算机视觉与模式识别学术会议即International Conference on Computer VisionPattern Recognition"}
{"content2":"人工智能时代，机器视觉技术的未来趋势\nhttps://www.toutiao.com/a6639554626255323651/\n2018-12-27 14:22:57\n机器视觉在工厂自动化中起着至关重要的作用，而且其正向更多领域快速发展。虽然机器视觉曾经被认为仅仅是人类视觉的替代品，但今天，它早已成为提升质量和生产效率的驱动力，具有能够将多维的、甚至肉眼不可见的微米级信息捕获到的强大能力。\n行业专家预测未来5年，机器视觉的复合年增长率将突破8%，到2022年，全球机器视觉市场规模预计或将超过140亿美元。那么，关于机器视觉技术，有哪些新的发展趋势值得我们关注的呢？\n●工业物联网（IIoT）\n随着IIoT的发展，机器视觉应该会大幅提升。 IIoT将信息技术与运营技术连接起来，因此它需要通过广泛的数据采集和分析，以不断优化工厂的运行。机器视觉是为IIoT提供信息的最重要的基础技术之一。全球制造业在IIoT方面的快速发展，导致了机器人技术的复兴和对机器视觉产品的新需求。\nIIoT加速了人类和机器人如何协同合作的进程，但是如果没有机器视觉技术将大量的设备、机器人和人连接在一起，这些都不会发生。\n●更易使用\n机器视觉系统的操作员更偏好用户友好的界面和更直观反映工厂车间环境的产品。但与此同时，视觉感知输入却比以往任何时候都更加复杂，这对于设备和软件设计人员简化他们的界面来说是一个巨大的挑战。\n产品的标准化将帮助用户集成和运行视觉系统，同时降低设备的可替换性成本。 此外，在设置应用程序、定位和检查零件，以及通过与HMI、PLC和机器人设备的通信来配置结果等方面，软件设计人员将进一步简化流程，通过更简洁的界面提升效率。\n●CoaXPress标准\nCoaXPress是一种非对称的高速点对点串行通信数字接口标准，可通过相机到相机之间的同轴电缆实现传输和接收数据。新一代CXP单链路相机不但价格更低，还具有更小的体积、更低的功耗和产生更少的热量，进一步降低了集成商设计基于CXP接口系统的难度。\nCoaXPress可以高达6.25G比特/秒的速度传输数据，这几乎是USB 3 Vision标准在现实环境中能达到的数据速率的2倍，甚至比最新的GigE Vision的数据传输速率还快。CoaXPress标准可以为今后十年间不断增长的带宽需求提供支持，同时还可以满足许多额外要求，如：更高的可靠性、灵活性、实时性以及成本效益等。\n●扩展到更多领域\n更低的成本和持续改进的视觉组件，如3D彩色摄像机和机器学习技术，将进一步推动机器视觉扩展到非工业领域的应用中，如“无人驾驶”汽车系统、IP视频监控、智能交通系统、智慧农业和医疗等。\n曾经被认为是脆弱的、不能在任何地方使用，只能在环境控制很好的室内区域使用的机器视觉产品，现在已经足够坚固，以至于可以适用于户外的苛刻环境。"}
{"content2":"如果想要机器能够进行思考，我们需要先教会它们去看。\n李飞飞——Director of Stanford AI Lab and Stanford Vision Lab\n计算机视觉（Computer vision）是一门研究如何使机器“看”的科学，更进一步的说，就是指用摄影机和计算机代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图像处理，用计算机处理成更适合人眼观察或进行仪器检测的图像。\n学习和运算能让机器能够更好的理解图片环境，并且建立具有真正智能的视觉系统。当下环境中存在着大量的图片和视频内容，这些内容亟需学者们理解并在其中找出模式，来揭示那些我们以前不曾注意过的细节。 计算机视觉的实现基本过程为：\n计算机从图片中生成数学模型\n计算机图形在模型中对图像进行绘制，然后在图像处理过程中将其作为输入，另外给出处理图像作为输出\n计算机视觉的理念在某些方面其实与很多概念有部分重叠，包括：人工智能、数字图像处理、机器学习、深度学习、模式识别、概率图模型、科学计算以及一系列的数学计算等。因此，你可以将本文看成是深入这个领域研究的第一步。本文将尽量包涵到尽可能多的内容，但是可能仍然会存在一些较为复杂的主题，也有可能存在某些遗漏之处，敬请见谅。\n丨第一步——背景\n通常来说，你应该具有一点相关的学术背景，比如上过有关概率学、统计学、线性代数、微积分（微分与积分）等相关课程，对矩阵计算有一定了解更好。另外，从我的经验来看如果你对数字信号处理有了解的话，在以后对于概念的理解来说会更加容易。\n在实现层面来说，你最好能够会用MATLAB或者Python中的一种，一定要记住的是计算机视觉几乎全部与计算机编程有关。\n你也可以在Coursera上选修《概率绘图模型》一课，这门课程相对较难（讲得比较深入），你也可以在学习一段时间之后再对其进行了解。\n丨第二步——数字图像处理\n观看来自杜克大学的Guillermo Sapiro所教授的课程——《图像和视频处理：从火星到好莱坞Image and Video Processing: From Mars to Hollywood with a Stop at the Hospital》，该课程所提供的教学大纲每章都是独立的且包涵大量的练习，你可以在coursera和YouTube上找到相关的课程视频信息。另外你可以看下Gonzalez与Woods编写的《数字图像处理（Digital Image Processing）》一书，使用MATLAB来运行其中所提到的范例，相信一定会有所获。\n丨第三步——计算机视觉\n一旦学习完有关数字图像处理有关内容，接下来应该了解相关的数学模型在各种图像和视频内容中的应用方法。来自佛罗里达大学的Mubarak Shah教授在计算机视觉方面的课程可以作为一门很好的入门课程，其涵盖了几乎所有的基础概念。\n观看这些影片的同时，可以学习Gatech的James Hays教授的计算机视觉项目课程所使用的概念和算法，这些练习也都是基于MATLAB的。千万不要跳过这些练习，只有在真正的练习过程中才会对这些算法和公式有更深入的了解。\n丨第四步——高级计算机视觉\n如果你认真学习了前三步中的内容，现在可以进入到高级计算机视觉相关学习了。\n来自巴黎中央理工学院的Nikos Paragios和Pawan Kumar讲授了一门人工视觉中的离散推理（Discrete Inference in Artificial Vision）课程，它能提供相关的概率图形模型和计算机视觉相关的大量数学知识。\n到现在这一步来看就比较有趣了，这门课程一定能让你感受到用简单模型构筑机器视觉系统有多么复杂。学完这门课程的话，在接触学术论文之前又迈进一大步。\n丨第五步——引入Python和开源框架\n这一步我们要接触到Python编程语言。\n就Python而言有许多像 OpenCV、PIL、vlfeat这样的相关扩展包，现在就是将这些扩展包运用到你的项目中的最好时机。因为如果有其他的开源框架存在的话，没有必要从头开始来编写一切内容。\n如果需要参考资料的话可以考虑《使用Python对计算机视觉进行编程 Programming Computer Vision with Python》，使用这本书就够了。你可以动手去尝试下，看看MATLAB和Python结合的话如何来实现你的算法。\n丨第六步——机器学习与CovNets（卷积神经网络）\n有关如何从头开始机器学习的资料实在太多，你可以从在网上查找到大量相关教程。\n从现在开始最好一直使用Python进行编程，可以看下《使用Python建立机器学习系统——Building Machine Learning Systems with Python》和《Python机器学习——Python Machine Learning》这两本书。\n目前深度学习正大行其道，可以试着学习卷积神经网络在计算机视觉中的应用（ Computer Vision: the use of CovNets），在此推荐斯坦福的CS231n课程：针对视觉识别的卷积神经网络。\n丨第七步——如何才能更进一步\n行文至此，你可能会觉得已经讲了太多的内容，需要学的已经太多。但是，你还可以进一步进行探索研究。\n其中一个方法是看看由多伦多大学的Sanja Fidler和James Hays所举行的一系列研讨会课程，能帮助你对当下计算机视觉研究方向的最新概念有所理解。\n另一种即跟着 CVPR、ICCV、 ECCV、 BMVC这些顶级学术会议的相关学术论文（也可关注雷锋网(搜索“雷锋网”公众号关注)的相关报道），通过会上的研讨会、主旨演讲以及tutorial等日程一定能学到不少知识。\n总结：如果你按照步骤一步步完成所有的学习任务，届时你将大概了解计算机视觉中有关滤波器、特征检测、描述、相机模型、追踪器的历史，另外还学习到分割和识别、神经网络和深度学习的最新进展。希望本文能帮助你在计算机视觉领域走得更远，学习得更加深入。\nPS : 本文由雷锋网独家编译，未经许可拒绝转载！\nvia kdnuggets\n题图来自aitists.com"}
{"content2":"offer参考\n作者：Amber20181008173077\n链接：https://www.nowcoder.com/discuss/135497\n来源：牛客网\n【公司简介】 精微视达医疗科技（武汉）有限公司（简称精微视达）2014年成立于武汉光谷，专注开发光学活检技术， 聚集了光学、精密机械、电子、通信、医学图像处理等前沿行业的顶尖工程技术人员，毕业院校包括清华大学、华中科技大学、电子科技大学、北京科技大学、南京理工大学、苏州大学等，曾经就职机构包括强生、美敦力、华为、中冶南方、大族激光等世界级跨国企业。 精微视达联合权威医疗机构，在国内率先自主研发了首套针对于早期消化道癌症的光学活检系统，目前已进入注册申报环节。 精微视达已经完成A轮融资，累计融资金额数千万元。同时，精微视达也是国家十三五重点研发计划数字诊疗装备专项牵头单位，项目获得数千万中央财政拨款支持。\n【招聘岗位】\n（一）图像算法工程师\n>>岗位职责<<\n1.负责公司产品和在研项目图像处理算法设计、实现、验证及优化； 2.跟踪公司产品和在研项目图像处理算法国内外最新发展方向和相应技术； 3.负责项目中算法相关技术文档撰写。 >>任职要求<< 1.图像处理、模式识别、计算机视觉、人工智能等相关专业，硕士以上学历； 2.熟悉图像滤波、轮廓提取、边缘检测、形态学处理、图像分割、图像拼接、图像融合、去噪等各类图像处理理论； 3.掌握模式识别相关基础理论，掌握各种常用模式识别算法的设计和使用方法，并对其中至少一种有一定的使用和深入研究；掌握特征提取相关方法； 4.掌握C/C++，有良好的编程规范，能熟练运用 C/C++进行代码开发； 5.掌握OpenCV库，能熟练应用OpenCV开发各种图像处理算法； 6.熟练应用visual studio、Matlab开发工具； 7.有两年或以上图像处理算法开发经验； 8.具有良好的沟通表达能力和团队合作意识。\n（二） C++开发工程师\n>>岗位职责<<\n1. 从事医疗器械类软件基础与核心开发，完成相应功能和模块开发； 2. Windows/Linux平台应用软件开发、调试、缺陷跟踪和持续改进； 3. 参与产品/项目需求分析、架构设计及详细设计，按照规范格式撰写开发文档。 >>任职要求<< 1. 本科及以上学历，计算机、软件、自动化、生物医学工程等相关专业，热爱医疗行业； 2. 精通C/C++程序开发，具有2年以上Windows/Linux Native程序开发经验，熟练使用VS/eclipse+CDT 等集成开发环境； 3. 熟练掌握至少一种GUI开发库，如Qt/MFC/GTK等，熟悉基本的设计模式； 4. 有良好的交流沟通能力，团队协作能力，良好的文档撰写能力； 5. 有医疗行业从业经验者优先。\n薪酬：为一流的学生提供一流的薪酬；只要足够优秀，薪酬不是问题 福利：五险一金+股票期权+年终奖金+绩效奖金+定期体检+交通补贴+餐饮补贴\n简历请投递至邮箱hr@biopsee.cn 邮件标题请注明岗位名称，如：XX简历-图像算法工程师"}
{"content2":"交比不变\n几何不变量是计算机视觉中的重要概念。在匹配，识别方面有非常关键的用途。\n一般情况下，计算机视觉可认为是有实际空间到相机空间的摄影变换。对于摄影变换而言，交比是非常重要的不变量。\n所谓交比，指的是简比之比。记作CrossRatio(CR)\n图中，X1,X2,X3,X4，是空间中共面但不一定共线的四点。O表示投影中心，红线代表被投影的直线，黑线代表投影方向线。第一组投影点为\nx1\nx_1,\nx2\nx_2,\nx3\nx_3,\nx4\nx_4。第二组投影点为\nx′1\nx_1',\nx′2\nx_2'，\nx′3\nx_3',\nx′4\nx_4'。其中交比可表示为\nCR=|x1x2||x3x4||x1x3||x2x4|=|x′1x′2||x′3x′4||x′1x′3||x′2x′4|\nCR=\\frac {|x_1x_2| |x_3x_4|} {|x_1x_3| |x_2x_4|}=\\frac {|x_1'x_2'| |x_3'x_4'|} {|x_1'x_3'| |x_2'x_4'|}\n简而言之，交比是一种不变量，这个不变是针对被投影线而言的。只要投影中心不变，空间点相同。无论被投影线是哪条，交比都不变。"}
{"content2":"计算机视觉与深度学习公司\n计算机视觉公司整理\n国内从事计算机视觉领域的公司"}
{"content2":"一、竞赛目的\n比赛中问题明确，是技术的磨刀石\n二、重要比赛及开源（对现在有参考和学习价值）\n1. 智能盘点—钢筋数量AI识别(2019/1/10 - 3/3，正在进行)(有参考代码)\n比赛地址：智能盘点—钢筋数量AI识别\n推荐理由：\n紧贴生活、实际运用性强\n检测算法用到实际场景\n公开代码：\n基于RetinaNet (pytorch0.4.1)\n基于Cascade R-CNN (pytorch)\n基于YOLOv3(?)\n2. Kaggle 人类蛋白质图谱图像分类解决方案（已结束）（有代码）\n比赛地址：Human Protein Atlas Image Classification\n推荐理由：\n分类问题的比赛，有较强参考价值\n公开代码：\n第 3 名: code link(ubuntu16.04LTS + Taitan X + pytorch1.0.0)\n第11名: code link、知乎链接\n3. PRCV2018 美图短视频实时分类挑战赛冠军技术方案（已结束）（无代码）\n方案总结：http://bbs.cvmart.net/topics/121\n推荐理由：\n短视频的分类，在处理问题的思路上有较强参考价值\n三、一些经典的比赛\n1. 手写数字识别(入门级)\n2. 猫狗识别（练手级）\n3. CIFAR-10（入门级）\n四、其他总结\n极市总结：各大视觉技术竞赛冠军及 TOP 方案集锦"}
{"content2":"I need to see today：\n谷歌大脑深度学习：https://www.tinymind.cn/collections/11\n微软系列培训：https://www.msaischool.com/courses\nHinton神经网络：http://study.163.com/course/courseMain.htm?courseId=1003842018\npytorch-book：https://github.com/QueenJuliaZxx/pytorch-book\nDocker入门和进阶：https://cloud.tencent.com/developer/article/1004996\n一个神奇的网站:http://www.zhuanzhi.ai/timeline\n我的github：https://github.com/QueenJuliaZxx\n论文速递：\n1、https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html\n2、https://github.com/amusi/awesome-object-detection\n3、知乎：目标检测\n4、项目集结号：https://www.zhihu.com/people/zhao95/posts\n书籍资料：\n1、数字图像处理\n2、计算机图形学\n3、斯坦福大学吴恩达机器学习\nhttp://cs229.stanford.edu/syllabus.html\nhttp://open.163.com/special/opencourse/machinelearning.html\n4、斯坦福大学吴恩达深度学习\n5、opencv图像处理（c++）\n6、周志华-西瓜书\n7、张志华-深度学习译本\n9、计算机算法与应用\n10、李宏毅 深度学习\n11、tensorflow 、keras、pytorch（caffe）\n12、python图像处理\n13、算法设计与分析\n专栏：\n深度学习与计算机视觉\n计算机视觉编程与cuda\n课程：\n1、cs231n\n2、cs231a\n3、cs131c\n其他\ncs229 Machine Learning，\ncs229T Statistical Learning Theory，\ncs231N Convolutional Neural Networks for Visual Recognition，\ncs231A Computer Vision:From 3D Recontruct to Recognition，\ncs231B The Cutting Edge of Computer Vision，\ncs221 Artificial Intelligence: Principles & Techniques，\ncs131 Computer Vision: Foundations and Applications，\ncs369L A Theoretical Perspective on Machine Learning，\ncs205A Mathematical Methods for Robotics, Vision & Graph，\ncs231MMobile Computer Vision，\n项目练习：\nTo Myself ：我的AI之路\n【深度学习】写诗机器人tensorflow实现\n【深度学习】谷歌deepdream原理及tensorflow实现\n《使用OpenCV开发机器视觉项目》\nGitHub 最受欢迎深度学习应用项目 Top 16\n深度学习的一些演示项目\ngithub收藏\n项目管理：\n学做项目管理：https://blog.csdn.net/column/details/14834.html\n炫酷简历：\n简历的开源地址：https://github.com/sitexa/anires\n实际效果地址：https://www.sitexa.org/anires/public\n技术和启发来自：http://strml.net/\n专栏：https://www.zhihu.com/question/26836846\n\nReady！！！\n数据结构与算法\n计算机网络\n操作系统\n计算机组成原理\nMysql数据库\n计算机视觉算法；深度学习基础；数字图像处理；OpenCV基础；\n概率与数理统计；\n计算机视觉重要的知识点：\n重要1：https://blog.csdn.net/Murray_/article/details/79952076\n重要2：https://www.nowcoder.com/discuss/66114"}
{"content2":"从数据量角度看待计算机视觉\n现有数据集太小—–对象检测—-图像识别—–语音识别—–数据集很大\n现有小的数据集意味着需要跟多的手工工程，而有了大数据集就可以使用逻辑简单、体积很大的网络，较少的手工工程。\n两种知识来源：\nLabeled data(x,y)标签\nHand engineered features/network architecture/other components精心找到的特征、设计的网络结构等\n计算机视觉领域中，一直没有足够大的数据，所以很多时候需要手工工程，复杂的网络结构和超参数。 当数据集小的时候，考虑迁移学习。\nTips for doing well on benchmark/winning competitions\n基准测试：若在基准上做得好就容易发论文，但是一般不会用于生产环境。\n基准测试中表现良好的tips：\n集成ensembing：如果你已经知道了如何构建网络，那么生成不同的几个网络，同事输出结果，并取几个结果的平均值作为最终预测结果，可能可以提高1%-2%，有助于赢得比赛\nMulti-crop at test time:比如10-crop，两张原图像，生成两个镜像，分别取中心crop，左上crop，左下crop等，通过分类器输入这10种图像，结果取平均"}
{"content2":"结合本人学习学习《人工智能原理及应用》，现做如下总结！！\n概述：本文涵盖人工智能在现阶段主要的研究领域，包括：机器思维、机器学习、机器感知、机器学习的详细介绍等。\n1、机器思维\n机器思维主要模拟人类的思维功能。在人工智能中，与机器思维有关的研究主要包括推理、搜索、规划等。\n这里对该概念就不再赘述了，感兴趣的同僚可以参考《人工智能原理及应用》\n2、机器学习\n机器学习是机器获取知识的根本途径，同时也是机器具有智能的重要标志，有人认为，一个计算机系统如果不具备学习功能，就不能称其为智能系统。机器学习有多重不同的分类方法，如果按照对人类学习的模拟方式，机器学习可分为符号学习和联结学习。\n1）符号学习\n符号学习是指从功能上模拟人类学习能力的机器学习方法，它使一种基于符号主义学派的机器学习挂点。按照这种观点，知识可以用符号来表示，机器学习过程实际是一种符号运算过程。对符号学习，可根据学习策略，即学习中所使用的推理方法，将其分为记忆学习、归纳学习、研一学习等。\n记忆学习也叫死记硬背学习，它是一种最基本的学习方法，原因是任何学习系统都必须记住它们所获取的知识，以便将来使用。归纳学习是指以归纳推理为基础的学习，它是机器学习中研究较多的一种学习类型，其任务是要从关于某个概念的一系列一直的具体例子出发，归纳出一般的结论，像示例学习、决策树学习和统计学习等都是归纳学习方法。演绎学习是指以演绎推理为基础的学习，解释学习是一种典型的演绎学习方法，它是在领域知识的知道下，通过对单个问题求解例子的分析，构造出求解过程的因果解释结构，并对该解释结构进行概括化处理，得到用来求解类似问题的一般性知识。\n2）联结学习\n联结学习也称为神经学习，它是一种基于人工神经网络的学习方法。  现有研究表明，人脑的学习和记忆过程都是通过神经系统来完成的。在神经系统中，神经元及时学习的基本单元，也是记忆的基本单位。连接学习可以有多种不同的分类方法。比较典型的学习算法有感知器学习、BP网络学习和Hopfield网络学习等。\n感知器学习实际上是一种基于纠错学习规则，采用迭代思想对联结权重和阈值进行不断调整，直到满足结束条件为止的学习算法。BP网络学习是一种误差反向传播网络学习算法。这种学习算法的学习过程由输出模式的正向传播过程和误差的反向传播过程组成。其中误差的反向传播过程用于修改各层神经元的连接权值，以肘部减少误差信号，直至得到所期望的输出模式为止。Hopfield网络学习实际上是要寻求系统的稳定状态，即从网络的初试状态开始逐渐向其稳定状态过度，直至达到稳定状态为止。至于网络的稳定性，则是通过一个能量函数来描述的。\n3）知识发现和数据挖掘\n知识发现（knowledge discover）和数据发觉（data mining）是在数据库的基础上实现的一种知识发现系统。他通过综合运用统计学、粗糙集、模糊数学、机器学习和专家系统等多种学习手段和方法，从数据库中提炼和抽取知识，从而可以揭示出蕴涵在这些数据背后的客观世界的内在联系和本质原理，实现知识的自动获取。\n传统的数据库技术仅限于对数据库的查询和检索，不能够从数据库中提取知识，使得数据库中所蕴涵的丰富知识被白白浪费。知识发现和数据挖掘以数据库作为知识源去抽取知识，不仅可以提高数据库中数据的利用价值，同时也为各种智能系统的知识获取开辟了一条新的途径。目前，随着大规模数据库和互联网的迅速发展，知识发现和数据挖掘已从面向数据库的结构化信息的数据挖掘，发展到面向数据仓库和互联网的海量、半结构化或非结构化信息的数据挖掘。\n3、机器感知\n机器感知作为机器获取外界信息的主要途径，是机器智能的重要组成部分。下面介绍机器视觉、模式识别、自然语言理解。\n1）机器视觉\n机器视觉是一门用计算机模拟或实现人类视觉功能的新兴学科。其主要研究目标是使计算机具有通过二维图像认知三维环境信息的能力。这种能力不仅包括 对三维环境中物体形状、位置、姿态、运动等几何信息的感知，还包括对这些信息的描述、存储、识别与理解。\n视觉是人类各种感知能力中最重要的一部分，在人类感知到的外界信息中，约80%以上是通过视觉得到的。人类对视觉信息获取、处理与理解的大概过程是：人们视野中的物体在可见光的照射下，先再眼睛的视网膜上形成图像，再有感光细胞转换成神经脉冲信号，经神经纤维传入大脑皮层，最后由大脑皮层对其进行处理与理解。可见视觉不仅指对光信号的感受，它还包括了对视觉信号的获取、传输、处理、存储于理解的全过程。\n目前，计算机视觉已在人类社会的许多领域得到了成功的应用。例如，在图像、图形识别方面有指纹识别、染色体识别、字符识别等；在航天与军事方面有卫星图像处理、飞行器跟踪、成像精确制导、景物识别、景物识别、目标检测等；在医学方面有CT图像的脏器重建、医学图像分析等；在工业方面有各种检测系统和生产过程监控系统等。\n2）模式识别\n模式识别是人工智能最早的研究领域之一。“模式”一词的原意是指提供模仿用的完美无缺的一些标本。在日常生活中，可以把那些客观存在的事物心事称为模式。例如，一幅画、一个景物、一段音乐、一幢建筑等。在模式识别理论中，通常把对某一事物所做的定量或结构性描述的集合称为模式。\n所谓模式识别就是让计算机能够对给定的事务进行鉴别，并把它归入与其相同或相似的模式中。其中被鉴别的事务可以使物理的、化学的、生理的，也可以是文字、图像、声音等。为了能使计算机进行模式识别，通常需要给它配上各种感知器官，使其能够直接感知外界信息。模式识别的一般过程是先采集待识别事务的模式信息，然后对其进行各种变换和预处理，从中抽出有意义的特征或基元，得到待识别事务的模式，然后在于机器中原有的各种标准模式进行比较，完成对待识别事物的分类识别，最后输出识别结果。\n根据给出的标准模式的不同，模式识别技术可有多种不同的识别方法。其中经常采用的方法有末班匹配法、统计模式法、模糊模式法、神经网络法等。\n3）自然语言理解\n自然语言理解一直是人工智能的一个重要领域，它主要研究如何使计算机能够理解和生成自然语言。自然语言是人类进行信息交流的主要媒介，但由于它的多义性和不确定性，是得人类与计算机系统之间的交流还主要依靠那种收到严格限制的非自然语言。要真正实现人机之间的直接自然语言交流，还有待遇自然语言理解研究的突破性进展。\n自然语言理解可分为声音语言理解和书面语言理解两大类。其中声音语言的理解过程包括语音分析、词法分析、句法分析、语义分析和御用分析五个阶段；书面语言的理解过程除不需要语音分析外，其他四个阶段与声音语言理解相同。自然语言理解的主要困难在御用分析阶段，原因是它涉及上下文知识，需要考虑语境对语言的影响。\n与自然语言理解密切相关的另一个领域是机器翻译，即用计算机把一种语言翻译成另外一种语言。尽管自然语言理解和机器翻译都已取得了很多进展，但离计算机完全理解人类自然语言的目标还相距甚远。自然语言理解的研究不仅对智能人机接口有着重要的实际意义，而且对不确定的人工智能的洋酒也具有重大的理论价值。\n4、机器行为\n机器行为作为计算机作用于外界环境的主要途径，也是机器智能的主要组成部分。其主要内容包括：智能控制、智能制造。此处不再赘述，详情可以参考文末资料。\n参考：《人工智能原理及其应用》_王万森\n整理不易！！ 转载请注明出处！\nhttps://my.oschina.net/u/3702502/blog/1823431"}
{"content2":"机器视觉和计算机视觉是两个既有区别又有联系的专业术语。\n机器视觉（machine vision）偏重于计算机视觉技术工程化，能够自动获取和分析特定的图像，以控制相应的行为。计算机视觉（computer vision）是采用图像处理、模式识别、人工智能技术相结合的手段，着重于一幅或多幅图像的计算机分析。具体来说，计算机视觉为机器视觉提供图像和景物分析的理论及算法基础，机器视觉为计算机视觉的实现提供传感器模型、系统构造和实现手段。\n在某种意义上可以说，一个机器视觉系统就是一个能自动获取一幅或多幅目标物体图像，对所获取图像的各种特征量进行处理、分析和测量，并对测量就诶过做出定性分析和定量解释，从而得到有关目标物体的某种认识并作出相应决策的系统。\n机器视觉和计算机视觉属于不同的学科。研究机器视觉是更好的为制造业提供更多有利于提高产品质量和生产效率的技术支持；研究计算机视觉的目的就是根据人类的视觉特性来给计算机带来“光明”，让它更好的来代替人来工作或者完成人类不能完成的工作，更大的提高生产效率，同时也不断提高人们的生活质量。\n机器视觉中把计算机作为载体或者说是工具，主要是利用计算机高效率的CPU。因为视觉里看到的都是图像，而对图像的处理往往比较耗时，所以能更快的完成图像处理，为以后的控制赢得时间。\n机器视觉是自动化领域一项新型技术，简单来说，就是给机器增加一个智能的眼睛，让机器具备视觉的功能，能看能检测能判断，可以替代传统的人工或者简单的机械治具。其原理是将需检测的产品或区域进行成像，然后根据其图像信息用专用的图像处理软件进行处理，根据处理结果软件能自动判断产品的位置、尺寸、外观信息，并根据人为预先设定的标准进行合格与否的判断，输出其判断信息给执行机构。\n机器视觉的功能包括：物体定位、特征检测、缺陷判断、目标识别、计数和运动跟踪。正是由于机器视觉系统可以快速获取大量信息，而且易于自动处理，也易于同设计信息以及加工控制信息集成，因此，在现代自动化生产过程中，人们将机器视觉系统广泛地用于工况监视、成品检验和质量控制等领域。\n计算机视觉是指用摄像机和计算机代替人眼对目标进行识别、跟踪和测量等，通过计算机实现人的视觉功能，对客观世界的三维场景的感知、识别和理解。通常有两类方法：一类是仿生学的方法，参照人类视觉系统的结构原理，建立相应的处理模块完成类似的功能和工作；另一类是工程的方法，从分析人类视觉过程的功能着手，并不去刻意模拟人类视觉系统内部结构，而仅考虑系统的输入和输出，并采用任何现有的可行的手段实现系统功能。\n在计算机视觉中图像可以由单个或者多个传感器获取，也可以是单个传感器在不同时刻获取的图像序列，然后对目标物体进行分析识别，确定目标物体的位置和姿态，对三维景物进行符号描述和解释。在计算机视觉研究中，经常使用几何模型、复杂的知识表达，采用基于模型的匹配和搜索技术，搜索的策略常使用自底向上、自顶向下、分层和启发式控制策略。\n本文转自：http://www.xms-sz.com/html_news/jiqishijuehejisuanqishijuequbieyulianxi-80.html"}
{"content2":"转自：http://www.sohu.com/a/206338786_100007727\n2018年【计算机视觉&机器学习&人工智能】国际重要会议汇总\n每年全世界都会举办很多计算机视觉（Computer Vision，CV）、 机器学习（Machine Learning，ML）、人工智能（Artificial Intelligence ，AI）领域的学术会议。笔者选取了其中影响力较大，有代表性的重要会议进行了汇总。如有遗漏，还请留言补充。\nAAAI 2018（美国新奥尔良）\n全称：the Association for the Advancement of Artificial Intelligence\n时间：2018.02.02-07\n地点：Hilton New Orleans Riverside, New Orleans, Lousiana, USA\n介绍：人工智能领域顶级会议\n官网：https://aaai.org/Conferences/AAAI-18/\nMMM2018 (泰国曼谷)\n全称：the 24rd International Conference on MultiMedia Modeling\n时间：2018.02.05-08\n地点：Dusit Thani Hotel, Bangkok, Thailand\n介绍：多媒体建模及应用领域国际权威会议\n官网：http://mmm2018.chula.ac.th/\nICIGP 2018（中国香港）\n全称：INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING\n时间：2018.02.24-26\n地点：Regal Oriental Hotel，Hong Kong\n介绍：促进学术界和工业界交流的图形图像处理会议\n官网：http://icigp.org/\nACM - ICIAI 2018（中国上海）\n全称：2018 2nd International Conference on Innovation in Artificial Intelligence\n时间：2018.03.09-12\n地点：Shanghai, China\n网址：http://www.iciai.org/\nWACV 2018 (美国内华达州)\n全称：IEEE Winter Conference on Applications of Computer Vision\n时间：2018.03.12-15\n地点：Harvey’s Casino in Lake Tahoe, Nevada，U.S.\n介绍：侧重计算机视觉应用的国际知名会议\n网址：http://wacv18.uccs.us/\nICASSP 2018（加拿大卡尔加里）\n全称：International Conference on Acoustics, Speech and Signal Processing\n时间：2018.04.15-20\n地点：ELUS Convention Center, Calgary, Alberta, Canada\n介绍：声学、语音与信号处理及其应用国际顶级会议\n官网：https://2018.ieeeicassp.org/\nCVM2018（中国上海）\n全称：Computational Visual Media conference\n时间：2018.4.18-20\n地点：上海，上海交通大学\n介绍：计算机视觉相关的基础研究和应用会议\n官网：http://iccvm.org/2018/\nICLR 2018（加拿大温哥华）\n全称：6th International Conference on Learning Representations\n时间：2018.04.30-05.03\n地点：Vancouver Convention Center, Vancouver, BC, Canada\n主题：deep learning，compositional modeling, structured prediction, reinforcement learning, large-scale learning，non-convex optimization\n官网：http://www.iclr.cc/doku.php?id=ICLR2018:main&redirect=1\nICRA 2018（澳大利亚昆士兰）\n全称：IEEE International Conference on Robotics and Automation\n时间：2018.05.21-25\n地点：The Brisbane Convention & Exhibition Venue,, Brisbane, Queensland Australia\n介绍：机器人及自动化领域国际顶级会议\n官网：http://icra2018.org/\nCVPR 2018（美国盐湖城）\n全称：IEEE Conference on Computer Vision and Pattern Recognition\n时间：2018.06.18-22\n地点：SALT LAKE CITY, UTAH，USA\n介绍：计算机视觉及模式识别领域国际三大顶级会议之一\n官网： http://cvpr2018.thecvf.com/\nICML 2018（瑞典斯德哥尔摩）\n全称：Thirty-fifth International Conference on Machine Learning\n时间：2018.07.10-15\n地点：Stockholm Sweden\n介绍：机器学习国际著名会议\n官网：https://2017.icml.cc/\nIJCAI-ECAI 2018（瑞典斯德哥尔摩）\n全称： 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence\n时间：2018.07.13-19\n地点：Stockholm, Sweden\n介绍：国际顶级人工智能联合大会\n官网：https://www.ijcai-18.org/\nSIGGRAPH 2018（加拿大温哥华）\n时间：2018.08.12-16\n地点：Vancouver, Canada\n介绍：计算机图形学领域最权威、影响力最大的国际会议\n官网：http://s2018.siggraph.org/conference/conference-overview/\nICPR 2018(中国北京)\n全称：24th International Conference on Pattern Recognition\n时间：2018.08.20-24\n地点：Beijing, China\n介绍：模式识别国际著名会议\n官网：http://www.icpr2018.org/\nBMVC 2018（英国纽卡斯尔）\n全称： 29TH BRITISH MACHINE VISION CONFERENCE\n时间：2018.09.03-06\n地点：Northumbria University， North East of England\n介绍：计算机视觉领域著名国际会议\n官网：http://bmvc2018.org/\n3D VISON 2018(意大利维罗那)\n全称：International Conference on 3D Vision\n时间：2018.09.05-08\n地点： Verona, Italy\n主题：3D视觉研究及应用的著名会议\n官网：http://3dv18.uniud.it/\nECCV 2018（德国慕尼黑）\n全称：European Conference on Computer Vision\n时间：2018.09.08-14\n地点：Munich, Germany\n介绍：计算机视觉国际三大顶级会议之一\n官网：https://eccv2018.org/\nICCVG 2018（波兰华沙）\n全称：International Conference on Computer Vision and Graphics\n时间：2018.09.17-19\n地点：Warsaw, Poland\n官网：http://iccvg.wzim.sggw.pl/\nIROS 2018（西班牙马德里）\n全称：IEEE/RSJ International Conference on Intelligent Robots and Systems\n时间：2018.10.01-05\n地点：Madrid, Spain\n介绍：智能机器人系统领域国际顶级会议\n官网：http://www.iros2018.org/\nICIP 2018 （希腊雅典）\n全称：2017 IEEE International Conference on Image Processing\n时间：2018.10.07-10\n地点：Athens, Greece\n介绍：图像处理领域国际著名会议\n官网：https://2018.ieeeicip.org/\nNIPS 2018（加拿大蒙特利尔）\n全称：The Conference and Workshop on Neural Information Processing Systems\n时间：2018.12.03-08\n地点：Palais des Congrès de Montréal, Montréal CANADA\n介绍：Machine learning , computational neuroscience领域国际顶级会议\n官网：https://nips.cc/Conferences/2018\nJCRAI 2018 （埃及开罗）\n全称：International Joint Conference on Robotics and Artificial Intelligence\n时间：2018.12.21-23\n地点：Cairo Egypt\n介绍：图像处理领域国际会议\n官网：http://www.jcrai.org/\n转自：http://www.sohu.com/a/206338786_100007727"}
{"content2":"基于深度学习的计算机视觉：原理与实践（上部）\n基于深度学习的计算机视觉：原理与实践（下部）\n本课程适合具有一定深度学习基础，希望发展为深度学习之计算机视觉方向的算法工程师和研发人员的同学们。\n基于深度学习的计算机视觉是目前人工智能最活跃的领域，应用非常广泛，如人脸识别和无人驾驶中的机器视觉等。该领域的发展日新月异，网络模型和算法层出不穷。如何快速入门并达到可以从事研发的高度对新手和中级水平的学生而言面临不少的挑战。精心准备的本课程希望帮助大家尽快掌握基于深度学习的计算机视觉的基本原理、核心算法和当前的领先技术，从而有望成为深度学习之计算机视觉方向的算法工程师和研发人员。\n本课程系统全面地讲述基于深度学习的计算机视觉技术的原理并进行项目实践。课程涵盖计算机视觉的七大任务，包括图像分类、目标检测、图像分割（语义分割、实例分割、全景分割）、人脸识别、图像描述、图像检索、图像生成（利用生成对抗网络）。本课程注重原理和实践相结合，逐篇深入解读经典和前沿论文，图文并茂破译算法难点, 使用思维导图梳理技术要点。项目实践使用Keras框架(后端为Tensorflow)，学员可快速上手。\n通过本课程的学习，学员可把握基于深度学习的计算机视觉的技术发展脉络，掌握相关技术原理和算法，有助于开展该领域的研究与开发实战工作。另外，深度学习之计算机视觉方向的知识结构及学习建议请参见本人CSDN博客。\n本课程提供课程资料的课件PPT（pdf格式）和项目实践代码，方便学员学习和复习。\n本课程分为上下两部分，其中上部包含课程的前五章（课程介绍、深度学习基础、图像分类、目标检测、图像分割），下部包含课程的后四章（人脸识别、图像描述、图像检索、图像生成）。"}
{"content2":"开始机器学习知识的学习，差不多已经有一年的时间。这期间看了很多教材和书籍，有些深入进行了学习，有些书（比如深度学习领域著名的西瓜书）则看不下去。机器学习其实也有许多方向，比如强化学习、计算机视觉、自然语言处理等等，如果每个方向都学习的话，普通人也没有那么多精力。\n在经过一年的泛泛的学习之后，决定将计算机视觉作为我的主攻方向，主要也是因为我对计算机图像这个领域比较感兴趣。在网上搜索了一些资料，以及推荐书单后，决定选择《Deep Learning for Computer Vision with Python》作为认真研读的一本书，目前已经差不多看完了第一部: Starter Bundle，觉得非常不错，推荐给有志于从事计算机视觉方向的朋友。\n首先需要说明的是，这本书目前还没有中文版，好在作者没有使用生僻的词汇，用词遣句也比较简练，配合着Google翻译，读起来还算顺利。在公众号后台回复“计算机视觉”可以下载本书的电子版。\n由计算机视觉专家Adrian Rosebrock编写的《Deep Learning for Computer Vision with Python》被评为当前最好的深度学习和计算机视觉资源之一。Google 的 AI 研究员和 Keras 库的作者Francois Chollet对于本书做出过这样的评价：\n这是一部关于计算机视觉的卓越的、深入且实用的深度学习实践作品。我认为它非常易读易懂：书中的解释清晰而又详细。在书中你能够找到许多在其他书籍或大学课程中难以见到的实用的建议。对于从业者和初学者，我强烈推荐这本书。\n这本书分为三个bundle:\nStarter Bundle - 这部分的内容比较基础，包括从零开始实现回归算法、深度神经网络和卷积神经网络。对于完全没有机器学习基础的人而言，可以从实例中学习到深度学习的基础知识。如果有一定的深度学习知识背景，也可以学习到在实际中如果应用深度学习（主要是图像分类），加深对深度学习的理解。\nPractition Bundle - 这部分的内容在Starter Bundle基础上更进一步，探讨的是在实际中可能会碰到的问题及解决之道，比如提高识别的精度、模型选择、超大数据集，最后引入了几个大型的、复杂的网络模型。\nImageNet Bundle - 这部分更多的关注于实战，前半部分是在ImageNet数据集上训练各种复杂的网络，后半部分则是解决实际生活中的问题，包括表情检测、车辆识别、年龄预测等等。完成这部分的学习之后，想必你的实战能力会提高一大截。\n如果你对在计算机视觉（图像分类、对象检测、图像理解等）中应用深度学习有兴趣，那这本书再好不过了。\n在这本书中，你将能够：\n理论和实践并重地学习机器学习和深度学习的基础内容\n学习先进的深度学习技术，包括对象检测、多GPU训练、迁移学习以及生成对抗网络等\n复现最前沿的论文成果，包括ResNet、 SqueezeNet、 VGGNet以及其他存在于 ImageNet 数据集中的成果\n这本书最吸引我的地方在于，书籍兼顾了理论和实践两者之间的平衡，对每一个深度学习理论，都会有一个关联的Python实现来帮助你巩固对其的理解和学到的知识。书中有详细的代码，且代码都有比较详细的说明，对Engineer非常友好。\n我在学习的过程中，都会尝试着输入代码，加深对代码的理解。你可以访问：https://github.com/mogoweb/aiexamples获得我的代码。\n在公众号后台回复“计算机视觉”可以下载本书的电子版。"}
{"content2":"先来看一下，什么是人类视觉。\n视觉，作为人类的感官之一，可以说是人类获取信息最直接有效的方式。毕竟，相对来说，光是传播最快的。视觉是一个物理学和生物学的结合体，首先是光照射在物体上，在物体上发生反射，当反射光映射在人的视网膜上，人们便接受到了信息（通常是接收漫反射）。这一过程是物理过程，在此之后，便是生理过程。映射到视网膜上的光，变成图像信息，通过视觉神经传输到大脑，这是一个转变过程。最终，大脑对图像信息进行处理，并作出反应，这是处理过程，也是人类视觉的重点。至于大脑是怎么处理，对于我一个小白来说太过复杂，暂且不提。整个流程下来，便是人类视觉。\n再来看计算机视觉\n在第一问中说道，计算机视觉是让计算机获得类似人的所见，所识，所思的能力。那么，光不变，光的反射不变，也就是物理过程不变。变的是把人的生理过程变成机械化，数字化。计算机的摄像头，相当于人类的眼睛，但没有人的眼睛那么灵活。电线，相当于人类的神经细胞（虽然这个比喻不够恰当），摄像头获取图像数据后，转变为数字信号传输。CPU，就相当于人的大脑，但是比较死板，而且并没有达到所思的能力。计算机视觉只是在模仿人类视觉，但并不能做到人类视觉。有一句话来形容比较恰当，一直被模仿，从未被超越，不仅现在是，以后也是。\n总得来说，计算机视觉和人类视觉的区别有以下几点：\n一个是生物，一个是机器（这个谁都知道）；\n人类的眼睛比摄像机更加灵活；\n人类的神经更加复杂；\nCPU只是按照人类的指示做事，这是和人类大脑根本上的区别；\n计算机视觉可以获取人类视觉获取不到的信息，例如：红外摄像机；\n计算机视觉可以到人类到不了的地方，例如：真空作业。"}
{"content2":"python计算机视觉+opencv3编程入门+opencv计算机视觉编程攻略下载地址分享：\nhttps://download.csdn.net/download/xiaopc3357/10558180\n三本高清书已经打包好！！！\n1. python计算机视觉\n2. opencv3编程入门\n3. opencv计算机视觉编程攻略"}
{"content2":"首先来一张图：\n人工智能：\n人工智能（ArtificialIntelligence），英文缩写为AI。是计算机科学的一个分支。人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。数学常被认为是多种学科的基础科学，数学也进入语言、思维领域，人工智能学科也必须借用数学工具。\n人工智能实际应用：机器视觉，指纹识别，人脸识别，视网膜识别，虹膜识别，掌纹识别，专家系统，自动规划，智能搜索，定理证明，博弈，自动程序设计，智能控制，机器人学，语言和图像理解，遗传编程等。人工智能目前也分为：强人工智能(BOTTOM-UPAI)和弱人工智能(TOP-DOWNAI)，有兴趣大家可以自行查看下区别。\n机器学习：\n机器学习(MachineLearning,ML)，是人工智能的核心，属于人工智能的一个分支。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。所以机器学习的核心就是数据，算法（模型），算力（计算机运算能力）。机器学习应用领域十分广泛，例如：数据挖掘、数据分类、计算机视觉、自然语言处理(NLP)、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用等。\n机器学习就是设计一个算法模型来处理数据，输出我们想要的结果，我们可以针对算法模型进行不断的调优，形成更准确的数据处理能力。但这种学习不会让机器产生意识。\n机器学习的工作方式\n选择数据：将你的数据分成三组：训练数据、验证数据和测试数据。\n模型数据：使用训练数据来构建使用相关特征的模型。\n验证模型：使用你的验证数据接入你的模型。\n测试模型：使用你的测试数据检查被验证的模型的表现。\n使用模型：使用完全训练好的模型在新数据上做预测。\n调优模型：使用更多数据、不同的特征或调整过的参数来提升算法的性能表现。\n机器学习的分类\n基于学习策略的分类\n1、机械学习(Rotelearning)\n2、示教学习(Learningfrominstruction或Learningbybeingtold)\n3、演绎学习(Learningbydeduction)\n4、类比学习(Learningbyanalogy)\n5、基于解释的学习(Explanation-basedlearning,EBL)\n6、归纳学习(Learningfrominduction)\n基于所获取知识的表示形式分类\n1、代数表达式参数\n2、决策树\n3、形式文法\n4、产生式规则\n5、形式逻辑表达式\n6、图和网络\n7、框架和模式（schema）\n8、计算机程序和其它的过程编码\n9、神经网络\n10、多种表示形式的组合\n综合分类\n1、经验性归纳学习(empiricalinductivelearning)\n2、分析学习（analyticlearning）\n3、类比学习\n4、遗传算法（geneticalgorithm）\n5、联接学习\n6、增强学习（reinforcementlearning）\n学习形式分类\n1、监督学习(supervisedlearning)\n2、非监督学习(unsupervisedlearning)\n注：细分的话还有半监督学习和强化学习。当然，后面的深度学习也有监督学习、半监督学习和非监督学习的区分。\n机器学习之监督学习\n监督学习（SupervisedLearning）是指利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。也就是我们输入的数据是有标签的样本数据（有一个明确的标识或结果、分类）。例如我们输入了50000套房子的数据，这些数据都具有房价这个属性标签。\n监督学习就是人们常说的分类，通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的）。再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。就像我输入了一个人的信息，他是有性别属性的。我们输入我们的模型后，我们就明确的知道了输出的结果，也可以验证模型的对错。\n举个例子，我们从小并不知道什么是手机、电视、鸟、猪，那么这些东西就是输入数据，而家长会根据他的经验指点告诉我们哪些是手机、电视、鸟、猪。这就是通过模型判断分类。当我们掌握了这些数据分类模型，我们就可以对这些数据进行自己的判断和分类了。\n在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。\n监督式学习的常见应用场景如分类问题和回归问题。常见监督式学习算法有决策树（ID3，C4.5算法等），朴素贝叶斯分类器，最小二乘法，逻辑回归（LogisticRegression），支持向量机（SVM），K最近邻算法（KNN，K-NearestNeighbor），线性回归（LR，LinearRegreesion），人工神经网络（ANN，ArtificialNeuralNetwork），集成学习以及反向传递神经网络（BackPropagationNeuralNetwork）等等。\n机器学习之非监督学习\n非监督学习（UnsupervisedLearing）是另一种研究的比较多的学习方法，它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。是否有监督（Supervised），就看输入数据是否有标签（Label）。输入数据有标签（即数据有标识分类），则为有监督学习，没标签则为无监督学习（非监督学习）。在很多实际应用中，并没有大量的标识数据进行使用，并且标识数据需要大量的人工工作量，非常困难。我们就需要非监督学习根据数据的相似度，特征及相关联系进行模糊判断分类。\n机器学习之半监督学习\n半监督学习（Semi-supervisedLearning）是有标签数据的标签不是确定的，类似于：肯定不是某某某，很可能是某某某。是监督学习与无监督学习相结合的一种学习方法。半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。当使用半监督学习时，将会要求尽量少的人员来从事工作，同时，又能够带来比较高的准确性。\n【注】\n单独使用有标记样本,我们能够生成有监督分类算法\n单独使用无标记样本,我们能够生成无监督聚类算法\n两者都使用,我们希望在1中加入无标记样本,增强有监督分类的效果;同样的,我们希望在2中加入有标记样本,增强无监督聚类的效果\n一般而言,半监督学习侧重于在有监督的分类算法中加入无标记样本来实现半监督分类，也就是在1中加入无标记样本，增强分类效果。\n应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如自训练算法(self-training)、多视角算法(Multi-View)、生成模型（EnerativeModels）、图论推理算法（GraphInference）或者拉普拉斯支持向量机（LaplacianSVM）等。"}
{"content2":"昨晚（3月12号）终于收到了商汤的实习offer，磕磕绊绊为期一个月的实习面试之旅终于告一段落。记录一下，希望能有所收获。\n实习岗位：计算机视觉\n个人相关经历：一篇SCI 1区文章在投（和视觉无关）；一篇卷积神经网络特征可视化文章（中文核心，本科毕设整理）；一段跨度半年的机器人比赛（我负责机器人定位和目标定位）；一段比较水的人脸检测实习；一段从交通流视频中提取交通流信息（车辆数量、速度等）的实习，这段实习经历让我较为完整的掌握了传统的目标检测跟踪等算法；还有其他零零散散的比赛和课程等；编程主要是C++，剑指刷了一半。\n面试公司：Momenta、今日头条、商汤（去年面了微软等，暂不在讨论范围；另外还投了face++、百度、地平线等，简历未通过）\nMomenta\n1、面试前期\n2月10号左右投的简历，第二天就确定了面试时间，效率真高；\n另外我2月8号将做了好大半年的系统控制方向且和视觉不沾边的论文才投出去，所以这大半年几乎未碰视觉，简历上写的那些经历也差不多忘光了；当时想去头条，所以这个面试没打算过，权当练手\n2、面试过程\n主要问项目方面的东西，着重问了我物流机器人比赛双目目标定位，并让我推出双目测距的公式--忘得差不多了，胡乱回答；\n编程方面，主要是C++，问容器vector、list、map等的区别--没看过，不知道怎么回答\n3、面试结果\n20min后，卒\n今日头条\n1、面试前期\n2月8号投的简历，HR想年底面试，因为我这边完全未准备，协调到年后2月27号面试\n2、面试过程\n头条面试类似于打怪升级，一面未过即挂。\n简历方面，首先了解我刚投的文章，和计算机视觉无关，没有继续问；\n后全程问我那篇卷积神经网络特征可视化论文，面试官人很nice，给了我很多建议；\n编程方面，手写代码求根号n，其中n是double类型--因为太急躁，出现一些语法错误，并且饶了很久才写完\n3、面试结果\n面试官反馈代码能力太弱（头条一般面试没有反馈，后来问HR时透露的），一面卒。\n商汤\n1、面试前期\n2月中旬投的简历，3月初才确定3月5号面试；\n将前两次面试出现的问题进行了总结，进行查缺补漏\n2、面试过程\n一共三面，综合评价（感觉这种更科学）。\n一面：香港打过来的电话面试。\n因为面试官是做人脸表情迁移的，所以对我京东物流机器人比赛方面定位部分的东西比较感兴趣，主要问了我机器人定位是怎么做的，然后问了我里面AprilTag定位是如何具体实现的--这一块刚好没仔细复习，所以坦诚自己不太会，但介绍了大致的流程，以及AprilTag出现的关键技术及Quad边缘检测。最后面试官说还OK，然后介绍了自己所在项目组的工作，问我有啥问题没有，还给了我很多做研究的建议，真的超感谢！整个面试大概花了30min。\n二面：去商汤现场面。\n简历方面凡是和视觉相关的全都问了，比如可视化论文、两个比赛、两段实习、上过的数字图像处理课程等--基本都清楚的答出来了，比如DPM如何做的、YOLO算法是怎么实现的等等；\n编程方面，手写代码找到两段链表公共部分起始位置--面试官提醒了一下，然后写出来了；\n最后面试官介绍了自己项目组在做的工作。整个面试大概1h。\n三面：二面过了20min左右，三面开始。\n首先是沿着简历问，问得很细，从项目背景、如何做的、横向比较相关的知识基本都问了；\n另外我那篇在投的文章和视觉无关，但涉及到level set（水平集）方法，这个在图像分割里面会用到，面试官因为是做医学图像处理的，好像比较感兴趣，也详细问了；\n面试官很看重深度学习项目经历，因为我有一段用tensorflow进行人脸检测的实习经历，所以被问人脸检测现状、当前比较好的人脸检测的深度学习模型及其比较、如何获取数据集、检测效果。。。又问了fast r-cnn具体实现，以及r-cnn发展历程--我简单介绍了fast r-cnn流程，很可惜这段实习当时比较水，所以坦诚不太会。\n三面面试官对我深度学习方面缺乏项目经历有些意见，不过面完后发了微信表示要是过了可以去他那里实习，再次表示感谢，虽然当时被虐的够惨，但也学到了很多。整个面试大概花了1个半小时。\n3、面试结果\n第二天hr给了我电话，问我可实习的时间等；一周后，正式发放实习offer。\n面试体会\n保证好心态，学习总结提高。"}
{"content2":"计算机视觉CV的入门理解\n最近开始，工作中需要使用计算机视觉CV，而博主之前没有系统学习过，因此把学习的知识记录下来，作为入门理解。\n如果想要机器能够进行思考，我们需要先教会它们去看。 ——斯坦福人工智能实验室和视觉实验室主任-李飞飞\n在机器学习大热的前景之下，计算机视觉与自然语言处理（Natural Language Process， NLP）及语音识别（Speech Recognition）并列为机器学习方向的三大热点方向。\n用于自动驾驶的图像分割：\n1. 计算机视觉cv\n1.1 基本概念\n计算机视觉（Computer vision）：是一门研究如何使机器“看”的科学，更进一步的说，就是指用摄影机和计算机代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图像处理，用计算机处理成更适合人眼观察或进行仪器检测的图像。\n1.2 主要技术点\n计算机视觉五大技术：图像分类、对象检测、目标跟踪、语义分割和实例分割。\n1.3 应用场景\n以下给出关于CV技术的使用场景：\n（来自商汤）：\n（来自百度）：\n1.4 实际应用：\n近几年随着人脸识别在生活中的应用场景不断增加，计算机视觉开始渐渐走入大众视野，近年来的一些实际的使用中：\n人脸识别： Snapchat 和 Facebook 使用人脸检测算法来识别人脸。\n图像检索：Google Images 使用基于内容的查询来搜索相关图片，算法分析查询图像中的内容并根据最佳匹配内容返回结果。\n游戏和控制：使用立体视觉较为成功的游戏应用产品是：微软 Kinect。\n监测：用于监测可疑行为的监视摄像头遍布于各大公共场所中。\n生物识别技术：指纹、虹膜和人脸匹配仍然是生物识别领域的一些常用方法。\n智能汽车：计算机视觉仍然是检测交通标志、灯光和其他视觉特征的主要信息来源。\n除了上面这些看起来高大上的技术，其实在现实生活中，我们都有过接触类似的。比如美图秀秀里的美妆，就用到了人脸检测、人脸关键点定位的技术。百度识花，用到了通用物品识别的技术等等。\n1.5 专业工具\n工欲善其事，必先利其器。\nOpenCV（开源计算机视觉库）是一个非常强大的学习资料库，包括了计算机视觉，模式识别，图像处理等许多基本算法。它免费提供给学术和商业用途，有C++，C，Python和java接口，支持Windows、Linux、Mac OS、iOS和Android。\n在编程语言方面，我主要使用Python，计算机视觉离开计算机编程是完全行不通的。\n基础技能，需要学会一些数字图像处理，它是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。\n参考文献：\n图片：https://www.jianshu.com/p/91de7a37d8e1\n慕课：https://www.imooc.com/article/27970\n李飞飞计算机视觉系列视频：https://study.163.com/course/introduction.htm?courseId=1003223001#/courseDetail?tab=1\n好了，我要开始我的计算机视觉学习之旅了。"}
{"content2":"计算机视觉（computer vision）和机器视觉（machine vision）两个术语既有区别又有联系。计算机视觉是采用图像处理、模式识别、人工智能技术相结合的手段，着重于一幅或多幅图像的计算机分析。图像可以由多个或者多个传感器获取，也可以是单个传感器在不同时刻获取的图像序列。分析师对目标物体的识别，确定目标物体的位置和姿态，对三维景物进行符号描述和解释。在计算机视觉研究中， 经常使用几何模型、复杂的知识表达，采用基于模型的匹配和搜索技术，搜索的策略常使用自底向上、自顶向下、分层和启发式控制策略。\n机器视觉则偏重于计算机视觉技术工程化，能够自动获取和分析特定的图像，以控制相应的行为。具体的说，计算机视觉为机器视觉提供图像和景物分析的理论及算法基础，机器视觉为计算机视觉的实现提供传感器模型、系统构造和实现手段。因此可以认为，一个机器视觉系统就是一个能自动获取一幅或多幅目标物体图像，对所获取图像的各种特征量进行处理、分析和测量，并对测量就诶过做出定性分析和定量解释，从而得到有关目标物体的某种认识并作出相应决策的系统。功能包括：物体定位、特征检测、缺陷判断、目标识别、计数和运动跟踪。"}
{"content2":"1. 计算机视觉在机器人上的应用：\n该部分内容源自一篇中文文献，由于是在大约一年前读的，现在只是把当时的笔记复制过来，具体是哪篇文章会随后去找，如果有知情的也可以告诉我，谢谢大家。\n1.1 传统工业机器人（机械臂）的工作原理是“示教-再现”的模式，这种模式缺乏对非设计情况的适应性。这篇文章是将计算机视觉技术利用在原有机械臂控制系统上，从而提高机械臂对不同环境条件的适用性。\n1.2 具体构成图如下：\n1.3 视觉系统算法构成：\n1.4 软件实现基本流程：\n1.5 这个项目中用的是定标算法，在实际拍摄场景中标定几个定位点，然后根据定位点确定摄像机和物体的3D位置。\n1.6 本项目实现流程图：\n2. 人机交互中计算机视觉应用：\n2.1 人机交互的组成：人机交互可以大概分为两个组成部分，语音和视觉，语音对应于语音自动识别，其试图构造能够感知人们交流的文字方面的机器；视觉对应于计算机视觉技术。其致力于构造能够“观察人”并自动感知相关视觉信息的机器。\n2.2 计算机视觉在的发展定义：计算机视觉是一门试图通过图像处理或视频处理而使计算机具备“看”的能力的学科。通过理解图像形成的几何和辐射线测定，接受器（相机）的属性和物理世界的属性，就有可能（至少在某些情况下）从图像中推断出关于事物的有用信息。\n2.3 计算机视觉发展的关联性：传统意义上，计算机视觉由诸如生物视觉建模、机器人导航和操作、监控安防、医疗图像及各种检查、检测和识别推动的。近年来，计算机视觉呈现多模态感知交互的态势。\n2.4 计算机视觉中的人机交互：着重于建模、识别和解释人的行为。\n1）人脸检测和定位：场景中有多少人，他们在哪里？\n2）人脸识别：他是谁？\n3）头和脸部的跟踪：用户的头部在哪里，脸部的位置和方向是什么？\n4）脸部表情分析：用户在微笑，大笑，皱眉，说话还是困乏？\n5）视听语音识别：使用语音识别以及伴随视话（lip-reading）和face-reading，判断用户说什么？\n6）眼睛注视跟踪：用户的眼睛朝哪里看？\n7）身体跟踪：用户的身体在何处？关节处（articulation）是什么？\n8）手跟踪：用户的手在哪里？是2维的还是3维的？特别地，手的结构是怎样的？\n9）步态识别：这是谁的走路/跑步风格？\n10）姿势、手势和活动识别：这个人在做什么？\n※ 人机交互的难点：\n这些任务都非常困难，从一个摄像机拍得图像（有时或者是多相机从不同的视角）开始，这项 工作典型情况下至少包括每秒30次的240*320个像素（每像素24比特）。我们试图很快地使这些数据变得有意义。与语音识别问题相比较，语音识别是从一个一维的，时间序列信号开始，然后尝试将其分段并分类成相对少数目的已知类别（音素或词）。计算机视觉事实上是一堆子问题的集合，这些子问题彼此间很少有共同点，且都非常复杂。\n2.5 基于计算机视觉的前沿成果：\n虽然计算机视觉在局部取得了进展，但是依然没有被真正的商业应用，不过有一些征兆显示商业应用即将到来。\n1）摩尔定律（英特尔的创始人Gordon Moore：当价格不变时，集成电路上可容纳的元器件的数目，约每隔18-24个月便会增加一倍，性能也将提升一倍。）；相机技术的进步；相机的进步；数码视频的普及；软件的推广（Inter的OpenCV库）。\n2）美国政府资助的人脸识别项目：FERET项目（1993-1997）和FRVT项目（2000-2002）。\n3）DARPA资助的远距离识别人和视频监防的大型工程。\n4）Geometrix,A4Vision和3Dbiometrics。\n5）MIT媒体实验室的幼儿室工程。\n2.6 技术挑战：\n虽然有很多这一类研究项目，但是为了从实验室走向商业化，几个问题需要说明：\n1）鲁棒性：大多数视觉技术是脆弱，缺乏鲁棒性的，照明和相机位置的微小变化可能会导致系统出错。系统需要在各种条件下工作，且能适度地、快速地从错误中恢复。\n2）速度：对于大多数计算机视觉技术，在全面和快速交互两者间都采取了实际折衷。视频数据太多了，以至于无法实时地做复杂处理。我们需要更好的算法、更快的硬件设备和更灵巧的方法来决策需要计算什么，可以忽略什么。（提供了已处理的图像流的数码相机能够有很大的帮助）\n3）初始化：许多技术在得到了初始模型后，跟踪效果都很好，但是初始化步骤往往很慢且需要用户参与。系统必须能快速和透明地进行初始化。\n前三个问题已经在日常的研究实验室和全球的产品研发组织得到了关注，使用性和上下文集成很少被考虑，但是随着更多的应用开发，这两个问题将会提到研究日程的前面。\n4）使用性：对于开发系统的人来说（花费了许多时间研究复杂难点）视觉技术的示范使用能工作地很好，但对于那些没有经过“系统训练”的新手却很困难。这些系统需 要适应用户，处理无法预期的用户行为。此外，它们需要提供简单的纠错和处理错误解释机制以及能提供反馈给用户，以避免预料之外的灾难性后果。\n5）上下文集成：一个基于视觉的交互技术本身不是最终的结果，而是一个更庞大的系统中的组成部分。手势和活动需要放在合适的应用过程中加以理解，而不是孤立的行为。从长期来看，这需要在各种应用的上下文关系中深刻地理解人类行为。\n3. 基于计算机视觉的智能机器人设计：\n3.1 这个项目的设计内容包括传感器模块，图像处理模块和执行模块，目的是用于煤矿井下发生突发事件时井下环境的探测，完成安全检查和监控等功能。\n3.2 原始输入图像是连续的数字视频图像，系统工作时，需调用图像处理模块对原始输入图像进行缩小、边缘检测、二值化、哈夫变换等处理，从而获得有用的路径信息，运动控制模块根据此信息作出决策，通过无线串口将控制命令发给机器人。机器人接收指令在电机控制模块下作出相应的移动。\n3.3 具体来说，这个项目是在视频中加有一个引导线，引导线在图像中的方位将会控制机器人的走向。此项目的局限性在于，此项目的机器人需要时刻保证引导线在视野内，这对于环境的适用性下降了。\n3.4 本项目的设计模块统称为“上位机”。下位机是具体的执行平台。下位机的控制芯片采用NXP公司的ARM7中的PC2132微处理器，用来完成命令接收、电机控制和机器人状态信息的上传等功能。\n3.5 电机控制电路利用L298芯片构成差动方式驱动电机运行，通过处理器驱动PWM信号控制电机转速，并且采用了PID闭环控制。"}
{"content2":"图像识别与人工智能的联系\n电子科技大学格拉斯哥学院大二通信三班黄聿洲2017200603039\n对于图像识别，自然应当与当今的时代潮流人工智能相结合起来。正如今年下半年在天津的夏季达沃斯峰会和在上海的中国国际进口博览会中所展示出来的，人工智能就是人类的第四次工业革命，而各个国家、企业都在展示当今时代人工智能的成果。而图像识别，正是人工智能的一个重要的研究方向。如何教会机器像人类一样会看会识别，是当今时代重点研究的一个课题。\n图像识别，是指利用计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对像的技术。一般工业使用中，采用工业相机拍摄图片，然后再利用软件根据图片灰度差，做进一步识别处理，图像识别软件国外代表的有康耐视等，国内代表的有图智能等。另外在地理学中指将遥感图像进行分类的技术。\n而联系到机器学习，主要涉及到的算法包含SVM，一种支持向量机，在机器学习领域，是一个有监督的学习模型，通常用来进行模式识别、分类以及回归分析的算法。以及CNN卷积神经网络算法，基于卷积神经网络CNN的人工智能技术的发展，目前基于主流的深度学习框架的直接开发出端到端不分割的识别方式，而且在没有经过太多trick的情况下，已经可以达到95%以上的识别率。\n机器是如何学习的呢？这是一个非常值得思考的问题。人类正是有着聪明的大脑才能成为世界上最高级的生物，而教会机器像人的大脑一样去看这个世界并不容易，一个小孩从出生到这个世界到三岁，就已经看到了数以亿万计的、海量的图片，而一个机器，简单将已有的而较为寻常普遍的图片给其学习，远远难以达到人类大脑所能识别的信息。因此，早在2007年，TED便在网上发起了图片收集，在ImageNet上收集了来自全世界各个国家各个地区所传来的各式各样的图片，数量达到接近十亿，以此庞大数量的图像数据，来使机器所学习到的图像信息尽可能的贴近一个人类，或说是一个三岁孩子所学习到的信息。这便是机器学习的出发点，教会机器像人类一样去看这个世界。\n而本次我将重点介绍CNN卷积神经网络，并用此种方法浅显的展现出机器学习与图像识别的联系。为了像人一样，人的大脑便是由一个庞大而缜密的神经网络组成，而每一个神经元就是最小最基本的单位。而卷积神经网络便是模仿了人脑的这一特点，由一个个神经元组成了一个神经网络，再经由机器学习以后，将训练的结果用于实际测试。\n利用课余时间，我在网上收集资料并尝试搭建框架经行图像识别验证码。我运行的环境是基于Windows10，Python3.6,编译器为Pycharm，同时，主要利用Tensorflow进行机器训练学习这一过程。TensorFlow是一个基于数据流编程的符号数学系统，被广泛应用于各类机器学习算法的编程实现，拥有多层级结构，可部署于各类服务器、PC终端和网页并支持GPU和TPU高性能数值计算，被广泛应用于谷歌内部的产品开发和各领域的科学研究。\n在此条件下，在机器学习过程中首先构建卷积神经网络并训练，经由卷积层（这里一共包含三层卷积层）、全连接层、池化层（每次池化后图片的高度和宽度都缩小为原来的一半）、输出层，这里在Python中调用Captcha库函数进行验证码的下载，训练集包含10000张图片（从0000-9999），同时生成测试集，即随机挑选50张验证码图片在机器经过学习后测试，这里的机器学习时长根据电脑配置不同而不同，CPU所需时间大约为20小时，而GPU所需时间大约为5-6小时。最后，这里我将生成的测试集数据以及测试情况截图：\n最后所得到结果，正确率为92%，由于测试集包含数量较少，以及验证码的灰度、散点都有所不同，此次数据结果仅为参考值。\n图像识别作为人工智能的一个大类，如今已经变得越来越热门，在前几年江苏卫视《最强大脑》节目中也已经展现出百度机器人小度对于图像识别以及信息综合处理的能力，在技术发展越来越快、效果越来越完善的现在，我们应该要明白，最难的不是使用一个算法，而是真正明白我们要解决的问题。\n参考文献：\n1. 机器学习初步与实践\n2. 使用Tensorflow构建和训练自己的CNN来做简单的验证码识别\n3. 基于Opencv和Tensorflow实现实时手势识别\n4. 李飞飞：如何教计算机理解图片"}
{"content2":"关于举办“OpenCV 计算机视觉及图像处理核心技术与应用”高级实操班的培训通知\n各有关学校：\n随着人工智能（AI）特别是深度学习（Deep Learning）近年来的飞速发展，在多个领域的成功应用，已经成为当前学术界和各行业最炙手可热的研究应用方向。OpenCV 计算机视觉及图像处理成为最为火热的领域之一。不仅广泛应用于搜索引擎、电子商务、社交网络等互联网服务，自然语言处理、金融、生物医药等行业AI的研究与应用也呈现爆发式增长。同时由于深度学习（Deep Learning）需要处理的海量数据非常庞大，为加强AI技术的创新发展和应用，培养社会急缺的深度学习专业人才，北京中科云畅应用技术研究院特别邀请深度学习领域的专家，举办“人工智能AI以及深度学习核心理论与实战 ”专题培训班，具体培训通知如下。\n培训时间、地点： 2018年12月14日 —— 2018年12月17日     西安\n2018年12月20日 ----2018年12月23日        北京\n（第1天报到，培训3天）\n课程为多期多地点，主要城市都有涵盖，详情加微信咨询：\n培训费用：每人3900元（含报名费、培训费、资料费），食宿可统一安排，费用自理。\n培训对象：各院校计算机专业、网络通信专业、电子工程专业、信息计算科学专业、统计学专业等对AI /深度学习技术及研发感兴趣的老师、研究生等。相关从事大数据、数据挖掘、机器学习、计算视觉、自然语言处理、人机交互等领域研发的单位的技术部门、IT企业的工程师、研发负责人、算法工程师等。\n培训方式:  1、名师讲座；     2、高校计算环境下的上机实操；   3、专题小组研讨与案例讲解分析结合；\n报名办法：\n请各有关部门统一组织本地区行政、企事业单位报名参加会议，各单位也可直接报名参加。报名回执表请传真至会务处。\n北京中科云畅应用技术研究院\n2018年10月25日\n附件\n一、主讲专家：\n主讲专家来自中科院及高级专家，拥有丰富的科研及工程技术经验，长期从事相关领域国家重大项目研究，具有资深的技术底蕴和专业背景。\n二、培训内容：\n一、OpenCV  源码使用与介绍\n1.OpenCV 入门介绍   2.什么是 OpenCV    3.OpenCV 的结构和内容 4.安装 OpenCV  库\n5.OpenCV  源码架构讲解\nOpenCV  源码使用与介绍\n图像的基础知识，图像的输入输出，视频的基础知识，视频的输入输出与参数控制方法\n6.OpenCV  中常用数据结构和函数\n(Point 类，Size 类，Rect 类，Scalar 类和 cvtColor 函数)，core 组件，imgproc  组件\n二、OpenCV  图像数字化\n1.Numpy 中的 ndarray 1.1 构造 ndarray 对象  1.2 访问 ndarray  中的值\n2.OpenCV 中的 Mat 类 2.1  构造单通道 Mat 对象 2.2  获得单通道 Mat  的基本信息\n2.4  访问单通道 Mat 对象中的值 2.5  向量类 Vec 2.6 构造多通道 Mat  对象\n2.7  访问多通道 Mat 对象中的值 2.8  获得 Mat  中某一区域的值\n3.矩阵运算  3.1 加法运算 3.2 减法运算  3.3 点乘运算 3.4 点除运算  3.5 乘法运算 3.6  其他运算\n4.灰度图像数字化   4.1 将灰度图像转换为 Mat   4.2 将灰度图转换为  ndarray\n5.彩色图像数字化\n5.1 将 RGB 彩色图像转换为多通道 Mat   5.2 将 RGB 彩色图转换为三维的  ndarray\n三、OpenCV 图像平滑与边缘检测\n一、OpenCV  图像平滑技术\n1.图像采样  2.傅里叶变换 3.图像噪声  4.空间平滑 5.案例分析讲解\n二、OpenCV  边缘检测技术\n1.边缘检测基础 2 基本边缘检测算子-Sobel 3 基本边缘检测算子-Laplace\n4.基本边缘检测算子—Roberts 5.基本边缘检测算子—Prewitt   6.改进边缘检测算子—Canny\n7.改进边缘检测算子—Marr-Hildreth 8.几何检测  9.形状检测 10.  角点检测\n四，OpenCV 图像变换原理和函数使用\n1.图像处理：使用 OpenCV 实现线性滤波器、非线性滤波器和5种高级形态学滤波操作，图形缩放，图\n字塔和阈值化\n2.图像变换：讲解各种类型的图形变换方法，包括使用 OpenCV 做边缘检测用到的 canny 算子、sobel  算函数使用Laplace  算子，进行图像特征提取的霍夫线变换、霍夫圆变换，重映射，仿射变换和直方图均衡化\n3.图像分割：使用 OpenCV  实现常用前景检测方法，寻找物体的凸包，使用多边形包围轮廓，角点检\n五、OpenCV  形态学\n腐蚀膨胀操作\n开闭运算操作\n形态学梯度\n形态学  Top-Hat\n案例 1.形态学滤波角点提取   案例 2.去干扰与噪声小块\n六，OpenCV  视频跟踪\nOpenCV  中开展模块编译与处理\n2.光流法实现移动对象跟踪\n3.基于直方图反向投影的 CAS  跟踪\n4.单对象的视频跟踪方法\n5.多对象的视频跟踪方法\n6.从检测到跟踪\n七、OpenCV  深度学习\n1. OpenCV  中支持的深度学习框架与前馈网络\n2.  支持的网络模型\n3.  深度视觉支持 图像分类、对象检测、实时视频对象检测、行人检测、人脸检测\n4.  深度学习模型导入与使用，代码演示\n八、OpenCV  案例解析\n案例 1.使用 OpenCV，基于 BLOB  颜色对象跟踪\n案例 2.使用 OpenCV 实时人脸检测与对象跟踪\n案例 3..使用 OpenCV  实现监控视频的行人识别与跟踪\n案例 4..使用 OpenCV  实现车牌目标提取\n三、颁发证书：\n学员经培训考试合格后可以获得：由 北京中科云畅应用技术研究院 颁发的结业证书。\n备注：请学员自带身份证复印件一张（办理证书使用）"}
{"content2":"#计算机视觉任务：\n##图像分类（image classification）\n图像分类：根据图像的主要内容进行分类。\n数据集：MNIST, CIFAR, ImageNet\n##目标检测（object detection）\n给定一幅图像，只需要找到一类目标所在的矩形框\n人脸检测：人脸为目标，框出一幅图片中所有人脸所在的位置，背景为非目标\n汽车检测：汽车为目标、框出一幅图片中所有汽车所在的位置，背景为非目标\n数据集：PASCAL, COCO\n##目标识别（object recognition）\n将需要识别的目标，和数据库中的某个样例对应起来，完成识别功能\n人脸识别：人脸检测，得到的人脸，再和数据库中的某个样例对应起来，进行识别，得到人脸的具体信息\n数据集：PASCAL, COCO\n##语义分割（semantic segmentation）\n对图像中的每个像素都划分出对应的类别，即对一幅图像实现像素级别的分类\n数据集：PASCAL, COCO\n##实例分割（instance segmentation）\n对图像中的每个像素都划分出对应的类别，即实现像素级别的分类，类的具体对象，即为实例，那么实例分割不但要进行像素级别的分类，还需在具体的类别基础上区别开不同的实例。\n比如说图像有多个人甲、乙、丙，那边他们的语义分割结果都是人，而实例分割结果却是不同的对象，具体如下图（依次为：原图 ，语义分割 ，实例分割）：\n数据集：PASCAL, COCO\n###不同数据集的介绍参考博客：计算机视觉相关数据集和比赛\n#通俗的讲解如此下：\n图像识别中，目标分割、目标识别、目标检测和目标跟踪这几个方面区别是什么？\n作者：许铁-巡洋舰科技\n来源：知乎，著作权归作者所有"}
{"content2":"深度学习与计算机视觉入门系列（中）\n数据嗨客最近发布了一个深度学习系列，觉得还不错，主要对深度学习与计算机视觉相关内容做了系统的介绍，看了一遍，在这里做一下笔记。\n目录\n深度学习与计算机视觉入门系列（中）\n目录\n深度学习第6期：循环神经网络RNN\n深度学习第7期：生成对抗网络GAN\n判别模型与生成模型\n自回归类的方法，例如Pixel CNN等。\n自编码类的方法，例如VAE等；\n对抗生成模型（GAN）\nGAN的变体\n总结\n深度学习第6期：循环神经网络RNN\nCNN适用于图片类型的数据，RNN适用于时间序列型的数据。\nLSTM相关内容。\n深度学习第7期：生成对抗网络GAN\n判别模型与生成模型\n机器学习的模型大体分为两类，判别模型（Discriminative Model）与生成模型（Generative Model）。判别模型是用来预测或判断输入的类别的。例如给我们一张动物的图片，让我们的分类器判断这个图片里的动物是不是牛，或者判断图片里的动物是什么，这都属于判别模型；而生成模型则是用来随机产生观测数据（或者根据某些随机生成的噪声来生成观测数据）。例如当我们给定一系列牛的图片，然后让我们的生成器根据这类图片再生成一张牛的图片。如果从数学的角度看，判别模型相当于学习一个条件分布P(y|x)，而生成模型则相当于在学习一个概率分布P(x)。\n对于图片判断类别（Classification）、检测物体的位置与分类（Object Detection）或是分割（Segmentation），都是针对输入的图片进行某些信息的判断的，属于判别模型。\n生成模型的损失函数相对难以定义，定位输出的是图片，而不是一张图片的label或者一些像素的label。\n目前，能够利用深度学习相关技术实现较好的生成模型的主要方法有三种：\n自回归类的方法，例如Pixel CNN等。\n自回归类的算法基于一个基本的假设——我们要生成的数据在因果关系上是顺序的。在这个顺序的结构中，前面的元素决定了后面元素的概率分布。也就是说，对于所有的i，我们由模型得到P(xi|x1,x2,…,xi−1)，并从中生成出xi。然后，我们再根据x1,x2,…,xi去生成xi+1，并以此类推生成整个序列。\n在目前的自然语言处理领域，例如“生成讲话”、“生成同人文”等一类应用中，最常见的生成模型都相当于在使用RNN（例如LSTM或GRU等）进行自回归。其首先学习给定数据集，学习语言的条件概率分布。然后再在随机初始化之后，根据学到的语言概率分布去生成序列。\n在图片生成的领域，我们可以假定各个像素之间也是根据一定顺序排列而成的，并且像素之间也有因果的决定关系。例如，我们可以将图片中最左上角的像素命名为1号像素，将它紧邻右边的一个像素命名为2号像素，剩下的根据先从左到右，再从上到下的顺序排列好。然后，我们可以先随机初始化1号像素，接着根据1号像素生成2号像素，根据1、2号像素生成3号，根据1、2、3号像素生成4号。以此类推，生成整个图片。\nPixel-CNN大体上就是根据这种原理设计而成的。\n除了图像与自然语言的领域外，还有类似的自回归结构运用在别的领域也取得了成功。例如WaveNet在音频生成上取得了重大成功等。不过要注意的是，与语言、语音这种天然具有序列型的结构相比，将图像中的像素强行组织成这种顺序结构，从逻辑上是有一定不足的。这也导致虽然Pixel-CNN与WaveNet结构几乎差不多，但是目前Pixel-CNN在图片生成的领域远远不具有WaveNet在音频生成领域那样的地位。\n自编码类的方法，例如VAE等；\n变分自编码（Variational AutoEncoder，简称VAE）的方法，也是利用自编码技术做生成模型的主流。它与一般的自编码稍有不同。我们在使用VAE建立编码器与解码器的时候，不但像一般的自编码器一样考虑让解码后的重建损失尽量小，我们同时还要考虑让编码尽量接近正态分布。在VAE中，我们一般用交叉熵（KL散度）来衡量z的分布与正态分布的差异，KL散度越大则意味着z与正态分布的差异越大，反之则z越接近正态分布。故而在VAE中，我们要优化的损失是一个重建损失与KL散度的和。当VAE优化完成后，我们就得到了一个编码器，可以将我们手头的图片都编码成某个正态分布。而当我们根据该正态分布生成一个新的z时，将其放入我们的解码器进行解码，即可得到生成的图片。如下图所示：\n总结：\n我们已经介绍了自回归与VAE这两种常见的生成模型的思想。我们想象图片是一个高维数据(x1,x2,…,xn)，我们要求的是其联合分布P(x1,x2,…,xn)。自回归的方法相当于将这个联合分布分解为一系列条件分布的连乘，即:\n而VAE的方法则相当于将这个联合分布分解为隐变量的分布与关于隐变量的条件分布的乘积，即：\n而GAN与上述二者不同。它相当于是“直接”解出联合分布P(x1,x2,…,xn)。与上述二者相比，它在数学上的含义是更加模糊的。但是目前，它的效果是最好的。\n对抗生成模型（GAN）\n我们有许多牛的图片，而我们希望利用深度学习技术来造这样一个生成模型——当我们输入一个随机产生的噪声图片，它通过我们的网络后，就能够变成一个“像模像样”的牛。 然而，深度学习技术本身是一种优化的技术。如果我们有一个“标准图片”，我们可以衡量输出与标准图片的距离，作为需要优化的损失。这样，经过多次训练之后，我们网络就可以把噪声输入给变得接近于标准图片一样输出。但是，在前面我们也已经说过了，上述的“标准图片”是不存在的。我们不知道何种损失才能衡量“是否像一头牛”。 GAN的想法是，既然我们不明白如何定义一个损失去衡量“是否像一头牛”，我们不妨将其想象成一个极其复杂的函数。既然是一个复杂的函数，则我们就可以用另外一个神经网络去拟合它。如果我们有一个完美的神经网络，判断一个图片“是否像一头牛”，则我们就可以根据这个网络提供的损失优化我们的生成网络。这事实上就是GAN的主要思想。 GAN的设计如下：它同时训练一个生成网络G与一个判别网络D。G的作用是接受随机的高维噪音作为输入，输出一个尽量像牛的图片。而我们将G网络输出的、企图伪装为牛的图片，增加一个“假”的标签，而将训练集里的牛的图片增加一个“真”的标签。将这一组混合了“真牛”与“假牛”的图片交给D网络，令其学习如何分别一个图片是否是真的牛。训练的过程就好像是一个对抗的过程，D是“火眼金睛”的警察，努力想辨别出G“造假”的图片；而G则是“以假乱真”的骗子，努力打磨自己的技术，企图有一天能够骗过D的眼睛，造出和训练集一模一样的图片。其示意图如下：\n对于D而言，其训练是相对简单的。它的训练集是许多有标记的数据集，包括被标记为“真”的现实中牛的图片与被标记为“假”的G造出来的图片。我们可以建立一个CNN，其输入为图片，输出为一个经过压缩函数（Sigmoid或tanh）压缩到0与1之间的数，表示其为真的概率；也可以让其输出两个数，经过softmax之后，得到其为真的概率与为假的概率。总之，这可以做为一个二分类问题，或是一个回归问题，比较简单。\n而对于G而言，其训练相对复杂。它的输入是高维噪声z，输出是造假的图片G(z)。在监督学习问题中，我们有一个标签L，我们的目的是让损失(L−G(z))2尽量接近0。而这里，我们没有标签L，我们只有判断输出是否为真的函数D。所以此处我们的目的是让D(G(z))尽量接近1。通过回传损失log(1-D(G(z)))可以达到目的。\nGoodfellow创立GAN的第一篇论文《Generative Adversarial Nets》中明确了其严格的数学定义:\n理论上而言，GAN的训练是一个min-max的过程，即训练最终要优化的目标是：\n这意味着，我们要在D做到最好的情况下，来优化G。\n这意味着，我们要在D做到最好的情况下，来优化G——这是合乎情理的——因为我们对G的训练是为了让它能够迎合D的判断。如果D的判断能力完全和人眼一模一样，而你让G去努力迎合D的判断，就是让G往正确的方向学习；而如果D完全是瞎判断，则你让G去迎合D的判断，起不到任何效果。一般来说，D越好的时候你让G去学习迎合它的判断就越有意义。这就仿佛是面对强大的对手时候，才能有效地提升自己。而面对太弱小的对手时，即使熟练地学会了击败他的方法，也毫无意义。\n但是另一方面，如果D太强大而G太弱，也是无法训练的。因为D会将G(z)及其邻域都判断为0，导致G找不到更新的方向。这就好像遇到了强大的对手，你如何微调自己的技巧，都没有使得自己获胜的希望增加一点。这也是对于提升自己不利的。\n所以，在GAN的训练过程中，为了求出上述的min-max点，我们一般同时、交替地训练两个网络。对D网络进行梯度上升，对G网络进行梯度下降。这样一来，两个网络就可以同时成长、同时提高。每个阶段他们都可以从与自己同等水平的对手进行对抗。训练初期，不太强大的G在努力学习造假，以骗过不太强大的D的眼睛；而不太强大的D则在努力学习辨认，企图找出不太强大的G造的赝品；等到了训练末期，强大的G已经可以造出以假乱真的图片，而强大的D也火眼金睛地能够察觉赝品中不起眼的细节。由于训练的过程充满了对抗性，所以被称为Generative Adversarial Nets，这也是GAN名字的由来。\nGAN的变体\nCGAN（Conditional Generative Adversarial Nets）：\n同样对于MINIST数据集的数字字体生成，CGAN尝试比普通的GAN更进一步。它希望能够接受一个人为指定的数字作为输入，然后输出对应的字体。\n在CGAN中，人们实现了人为调整输入，以获得所需输出这样的功能。但是，这毕竟还是一个有监督学习的过程，我们需要人为地对训练数据加上标签，才能实现这一点。\nInfo-GAN：\n一个无监督学习过程，让GAN自己学习特征。\n在Goodfellow发明了基础版的GAN之后，许多人对其进行了改进。其中比较有代表性的改进除了CGAN、Info-GAN外，还有LAPGAN、DCGAN、GRAN、VAEGAN等等。\n这里我们再展示几个用GAN及其变体可以达到的神奇效果。例如生成的卧室图片：\n例如用包含戴眼镜的男人、不带眼镜的男人，以及不戴眼镜的女人，生成戴眼镜的女人的图片：\n例如根据客户给的涂鸦，生成一张风景图：\n总结\n本文中，我们主要介绍了生成模型。生成模型是为了学习一个分布，而判别模型是为了学习一个条件分布；生成模型一般接受低维输入，输出高维的结果，而判别模型一般接受高维输入，输出低维的结果。从各个角度来看，生成模型都比判别模型要更难。这也是目前针对生成模型的研究少于判别模型的主要原因。但是随着技术的发展，越来越多研究者开始将目光聚焦于生成模型的领域。\n目前主要有三种利用深度学习的生成模型。第一种是自回归的模型，例如生成文字的LSTM、生成图片的Pixel-CNN，以及生成音频的WaveNet等等。它试图将一个联合分布拆开成为一系列条件分布的乘积，加以学习；第二种是VAE，它企图将联合分布拆开为隐含变量的分布以及数据关于隐含变量的分布，利用自编码的技术加以学习；第三种即是我们重点介绍的GAN，它企图直接学习整个数据的联合分布，是一种最接近于深度学习思维的算法。具体而言，它用一个D网络来学习数据联合分布的密度，用一个G网络来学习将低维噪声往数据联合分布的映射方式。它让二者同时训练、相互对抗、共同进步，使得最后G网络能够很好地生成以假乱真的数据。\nGAN有许多变体，而我们主要介绍了CGAN与Info-GAN两种。它们都试图让输入的数据具有一定的意义，以实现通过人为调节输入来控制输出。具体而言，CGAN采取的是有监督学习的策略，需要将数据人为标注上人们能够理解的的数据特性；而Info-GAN则采取的是无监督学习的策略，希望网络可以自己学习到数据的一些特性。当Info-GAN学习完毕后，我们也常常能够看得出来其自主学习到的数据特性是什么，这正是其神奇之处。如果同学们想了解更多有关GAN的内容，可以自行查阅有关资料。"}
{"content2":"作者：Dean0Winchester\n原文：https://blog.csdn.net/qq_38906523/article/details/77688294\n随着深度学习的不断发展，人工智能在未来几年将会出现井喷式的发展，而计算机视觉则是其重要的一个分支，计算机视觉是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。\n预计中国2018年计算机视觉的市场将会达到几百亿美元的市场，下面是最火的中国计算机视觉Top16公司，而且也是目前中国最受资本欢迎的公司。\nNO.1 图普科技\n广州图普网络科技有限公司是一家站在人工智能前沿的创业科技公司，专注于图像识别整体解决方案。致力于打造新一代的计算机视觉理解和人工智能引擎，让计算机可以识人、识物，教会计算机看懂这个世界。\n公司主要为企业提供智能审核、图片增值、图像搜索、深度定制服务。产品在2015年4月上线后，仅半年已经和迅雷、酷狗、唱吧、58同城、秒拍、小咖秀、秀色互娱、21CN等国内知名公司正式合作，此外，通过入驻七牛云，融云，UCloud，我们的产品间接服务了数百个客户，业务规模在快速发展中。\nNO.2诺亦腾科技\n北京诺亦腾科技有限公司（Noitom Technology Ltd.）是一家在动作捕捉领域具有国际竞争力的公司，成立于2012年。公司核心团队由一群具有世界顶尖水准的工程师组成，公司主要创始人均具有海外留学与工作背景。研究领域涉及传感器、模态识别、运动科学、有限元分析、生物力学以及虚拟现实等。诺亦腾开发了具有国际领先水平的“基于MEMS惯性传感器的动作捕捉技术”，并在此基础上形成了一系列具有完全自主知识产权的低成本高精度动作捕捉产品。已经成功应用于动画与游戏制作、体育训练、医疗诊断、虚拟现实以及机器人等领域，并得到全球业内的高度认可。“Noitom”是英文“运动”（Motion）单词的倒序拼写，代表了公司目标：颠覆运动捕捉行业格局。\n目前Perception拥有两款产品：面向高端客户的Perception Legacy与面向个人开发者的Perception Neuron。在未来，公司将继续完善动作捕捉技术平台；诺亦腾还开发了mySwing，一套基于惯性原理动作捕捉的高精度高尔夫训练系统，采用当今世界领先的无线高速动作捕捉技术研发。这套系统将会为高尔夫训练带来革命性的变化。\nNO.3速感科技\n速感科技是一家以机器视觉为核心的人工智能创业公司。公司成立于2014年7月，并已先后完成两轮融资。团队主创人员为清华大学计算机系、信息设计系，北京航空航天大学计算机学院及美国宾夕法尼亚大学研究生与博士背景，并参与微软亚洲研究院、国家重点基础项目的技术合作。目前公司的主要产品为以视觉跟随为核心的智能跟随机器人，面向智能无人机、机器人的系统化操作解决方案，并以机器视觉为主要核心模块面向不同应用用户进行定制化的系统设计。\nNO.4 商汤科技\n商汤集团是一家科技创新公司，致力于引领人工智能核心“深度学习”技术突破，构建人工智能、大数据分析行业解决方案。在人工智能产业兴起的大背景下，商汤集团凭借在技术、人才、专利上超过十年的积累，迅速成为了人工智能行业领军企业。\n应用性技术上，基于深度学习的人脸识别、文字识别、人体识别、车辆识别、物体识别、图像处理等技术在业界遥遥领先；业务上，商汤集团深耕金融、移动互联网、安防监控三大行业，与银联、京东、拉卡拉、华为、小米、新浪微博、科大讯飞、东方网力、英伟达等各行业巨头深度合作，推动行业产品智能化升级。\nNO.5 码隆科技\n码隆科技（Malong Technologies）是一家专注于引领深度学习与计算机视觉技术突破的人工智能公司，致力于打造全球领先的视觉决策引擎，并为企业提供国际领先、定制化的计算机视觉解决方案。\nNO.6 格林深瞳\n格灵深瞳是一家将计算机视觉和深度学习技术应用于商业领域的科技公司，自主研发的深瞳技术在人和车的检测、跟踪与识别方面居于世界领先水平。公司借助海量数据，让计算机像人一样看懂这个世界，实时获取自然世界正在发生的一切，打造自然世界的搜索引擎。\nNO.7 旷视科技\n北京旷视科技有限公司是一家专注机器视觉和人工智能的极客技术公司，打造领先的人脸识别，图像识别，和深度学习技术服务云平台。旷视科技将致力于“先让机器看懂世界，再让机器真正思考”。\nNO.8 依图科技\n上海依图网络科技有限公司创立于2012年，是目前全球唯一能对人脸以及车辆都进行精确识别的公司。公司的愿景在于“变革人机视觉交互”，公司目前主要致力于计算机视觉、图像视频智能理解和分布式系统及大数据应用的研究。\n我们认为计算机视觉的突破需要三个要素：理论框架、真实场景数据、大规模计算平台。前者是学术界的擅长，但后两者的学术界局限大大限制了看问题的视角并阻碍了开拓的速度。\nNO.9云从科技\n广州云从信息科技有限公司（简称云从科技）是一家专注于计算机视觉与人工智能的高科技企业，核心技术源于四院院士、计算机视觉之父——Thomas S. Huang黄煦涛教授。核心团队曾于2007年到2011年6次斩获智能识别世界冠军，得到上市公司佳都科技与香港杰翱资本的战略投资。公司主要技术团队来自中国科学院重庆分院，是中科院研发实力最雄厚的人脸识别团队，并作为中科院战略性先导科技专项的唯一人脸识别团队，代表参与了新疆喀什等地安防布控。\nNO.10 拍医拍\n拍医拍是一家专注于大数据的移动医疗初创企业，通过机器视觉交互获取用户的诊疗数据，勾画用户真实有效的“医疗图谱”，通过大数据挖掘，构建以患者为中心的医疗健康生态圈。\n拍医拍的开发愿景是基于图像内容分析的医学影像管理；基于移动视觉的移动外伤治疗；基于机器学习的皮肤疾病智能诊断；基于谱分析的咽喉炎症分析；以及基于大数据分析的个体和群体健康感知与预测分析......\nNO.11 北京陌上花科技有限公司\n“衣+”是计算机视觉搜索引擎，团队来自斯坦福、新加坡国立大学、南洋理工大学、北大、上海交大、微软、IBM、阿里巴巴、百度等学校与企业。衣+目前已经和多家顶级企业合作，提供边看边买API、图像视频内容分析API、人脸属性识别API等服务海量用户。\nNO.12 图漾科技\n图漾科技是一家专业从事计算视觉的技术公司，专注于 三维深度信息测量的基础性技术研究。我们不断追求更 完善的传感器感知技术和方案，旨在为我们的客户提供 各种高效可靠的三维测量解决方案，增强产品对三维世 界的感知能力，帮助客户开发更加智能的产品。\nNO.13 极视角科技\n深圳极视角科技有限公司（Extreme Vision）是专业的计算机视觉与云端服务提供商，目前已为零售、通讯、地产、工业、餐饮、交通、公安等众多领域提供智能监控与视频分析服务，备受客户认可。公司总部设于中国深圳，并在澳门、北京、首尔等国内外城市设有服务支持团队，为客户及合作伙伴提供及时服务\n极视角拥有国际领先的视频分析技术，汇集了来自腾讯、华为、香港科技大学、北京大学等顶尖企业及实验室的博士研发团队，更邀请到全球首位跻身AAAI（国际人工智能协会）Fellow的华人科学家杨强教授担任首席技术顾问，让极视角的技术研发与世界接轨。\n目前极视角已为中国电信、恒大集团等客户提供服务，并获得众多客户的认可。未来极视角将致力于推出更丰富的视觉识别算法，为各领域、多样化的客户提供高质量的解决方案，与客户、合作商共赢发展。\nNO.14 Linkface\nLinkface以在线API和离线SDK的产品形式装载最新的人脸技术，提供给开发者和企业级用户。\n我们的人脸云平台搭载稳定版的人脸测试和识别模型，目前提供免费测试，并提供高并发无调用限额的企业级人脸\n云服务。企业客户可选择调用由我们维护的云服务，我们也帮助有需要的客户搭建其自有的私有人脸云服务。\nNO.15 骏聿科技\n骏聿科技的主要技术人员在物体跟踪识别领域均有超过 10 年的研究开发经验，有成功的商品应用；拥有 3 项在美国、日本的发明专利，8 项国内专利授权；获得 2008 年国务院颁发 的国家科技进步二等奖；在国际最顶尖的机器学习，人工智能，计算机视觉等学术会议杂志 等发表多篇文章。 骏聿科技管理团队成员分别拥有清华大学博士学位、 英国兰卡斯特大学硕士学位和复旦大学 MBA 学位；有先进的管理经验并有严格的质量控制流程和标准。\n在图像识别领域最前沿的人脸识别、目标对象识别和行为识别位于全球前列，其中人脸识别技术在全球一些评测中位列前茅，行人流量统计（拥挤度分析）居世界领先行列。\n公司的人脸识别产品在金融、银行、社保、公交、航空、ATM 等行业进行了成功的推广应用。公司是国内在人脸识别领域的技术创新及产业化领先者。\nNO.16 灏泷智能\n公司核心团队以原中国人民解放军某部，军用图形图像技术转业专家发起组建，公司所研发的视觉识别技术，通过日本、欧洲多家跨国电子企业及国内车厂的综合测试要求。\n灏泷智能科技，在车联网“主动安全驾驶“领域，作为国内外开发高级驾驶辅助系统的先行者，中国领先的“车辆视觉识别预警系统“技术供应商。\n灏泷智能科技的客户包括，前装市场：整车制造商、一级集成供应商；以及后装市场用户：客运、货运、危险运输、出租车、校车、高铁机车、巴士公交等交通部强制监管的“两客一危“运输企业等。"}
{"content2":"相同点：\n理论基础都是针孔成像（像点，镜头中心，物点共线）\n区别：\n1、出发点不同导致基本参数物理意义的差异：摄影测量中的外部定向是确定影像在空间相对于物体的位置与方位，而计算机视觉则是物体相对于影像的位置与方位来描述问题。\n2、由于1中不同的出发点导致基本公式的差异：摄影测量中最为基本的是共线条件方程而视觉测量中最为基本的公式是用齐次坐标表示的投影方程。\n3、数学处理算法的不同：摄影测量源于测绘科学，基于非迭代的最小二乘平差求解贯穿于数字近景摄影测量的全过程，而计算机视觉强调矩阵分解，总是设法将非线性问题转化成为线性问题，尽可能避免求解非线性方程组。\n数字近景摄影测量是基于数字摄影测量的基本原理，融合计算机视觉的相关理论，应用计算机技术、数字图像处理技术，模式识别等学科的理论与方法，利用数字相机获取被测目标的数字图像来得到目标的空间三维坐标从而完成对目标物体的测量。"}
{"content2":"计算机视觉分很多方向，个人比较熟悉的是图像方面的，包括：图像分类、目标检测、图像分割、视频处理等，这些小的方面每一个也学要很长的一段时间去学习.\n入门这个东西很难定义，如果只是对一个方向看一下，了解一点东西，有一点谈资的话不算入门，我认为入门至少代表你对这个领域的某些东西进行过处理实践才可以，所以我比较喜欢的一种学习新知识的方式就是：先找一个综述的书随便翻一翻，对目录有个印象就行，然后针对自己喜欢的部分，去查资料，做实例，看论文，一段时间后，自然就会有自己的理解了.\n需要哪些数学编程工具等知识？\n数学知识的话，大概基础的就是概率论、数理统计、矩阵论\n编程基础的是python、matlab工具上，现在一般是用框架，我常用caffe，还有一些其他常用的框架tensorflow、pytorch、mxnet等入门的话，本科的数学知识基本就够了，然后开始看论文吧。\n基本现在搞计算机视觉（ComputerVision,CV）都是基于深度学习的。第一篇推荐alexnet，算是深度学习搞CV的开山之作，接着基本就是需要先把分类的看一看，现在的CV大多数任务都是基于分类任务的深度网络进行扩展的。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，有兴趣的朋友，可以查阅多智时代，在此为你推荐几篇优质好文：\n1.模式识别应用于哪些领域，模式识别技术的发展趋势\nhttp://www.duozhishidai.com/article-15389-1.html\n2.计算机视觉与图像处理、模式识别和机器学习，之间有什么关系？\nhttp://www.duozhishidai.com/article-4119-1.html\n3.语音的识别过程主要分哪几步，常用的识别方法是什么？\nhttp://www.duozhishidai.com/article-1739-1.html\n自然语言理解过程主要有哪些层次，各层次的功能是怎么样？\nhttp://www.duozhishidai.com/article-1726-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"近期主要学习了灰度直方图、Haar-like、HOG、LBP、PCA、SIFT和SURF，以及距离度量方法。\n一、灰度直方图\n单个像素灰度分布的一阶统计量。纹理是灰度在空间以一定的形式变换产生的图案，直方图是描述图像中像素灰度级分布的工具，可以用直方图或其统计特征作为图像的纹理特征。灰度具有一定的稳定性，对大小、方向都不敏感，能表现出相当强的鲁棒性。在医学图像中，大多是灰度图像，基于灰色直方图的特征提取是医学图像颜色特征提取中的一个典型算法。\n基本概念：\n1.灰度级L 包括纯黑、纯白和其间的无数级灰度，这里取L=256;\n2.灰度值Zi 灰度图像在计算机中的表示是一个M*N的二维矩阵，一个像素点i就对应着矩阵中相应位置的灰度值Zi。由于灰度级为256，所以灰度值范围在0-255间，0代表黑，255代表白。\n3.h(i) 直方图中统计的灰度为Zi的像素个数\n4.p(i) 归一化直方图灰度级分布中，灰度为Zi的概率。与h(i)的关系为：p(i)=h(i)/sum(h(i))\n一般在图像处理过程中，将图像转为灰度模式，可以加快后续图像分析处理的进程。\n二、Haar-like\nHaar-like的优势在于能更好地描述明暗变化，如：眼睛比脸颊的颜色要深，鼻梁两侧比鼻梁颜色要深等，因此用于检测正面的人脸。\n特征值：特征模板内有白色和黑色两种矩形（如下图），并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和，反映了图像的灰度变化，但是矩形特征只对简单的图形结构，如边缘、线段较敏感，因此只能描述特定方向（水平、垂直、对角）的结构。\nHaar-like在opencv下检测人脸可以直接调用相关模块。\n以下贴一块代码，参考了http://blog.csdn.net/xingchenbingbuyu/article/details/51386949\n#include<opencv2\\opencv.hpp> #include <iostream> #include <stdio.h> using namespace std; using namespace cv; // Function Headers void detectAndDisplay(Mat frame); // Global variables String face_cascade_name = \"haarcascade_frontalface_default.xml\"; String eyes_cascade_name = \"haarcascade_eye_tree_eyeglasses.xml\"; CascadeClassifier face_cascade; //定义人脸分类器 CascadeClassifier eyes_cascade; //定义人眼分类器 String window_name = \"Capture - Face detection\"; // @function main int main(void) { Mat frame = imread(\"4.jpg\"); //VideoCapture capture; //Mat frame; //-- 1. Load the cascades if (!face_cascade.load(face_cascade_name)) { printf(\"--(!)Error loading face cascade\\n\"); return -1; }; if (!eyes_cascade.load(eyes_cascade_name)) { printf(\"--(!)Error loading eyes cascade\\n\"); return -1; }; //-- 2. Read the video stream //capture.open(0); //if (!capture.isOpened()) { printf(\"--(!)Error opening video capture\\n\"); return -1; } //while (capture.read(frame)) //{ // if (frame.empty()) // { // printf(\" --(!) No captured frame -- Break!\"); // break; // } //-- 3. Apply the classifier to the frame detectAndDisplay(frame); int c = waitKey(0); if ((char)c == 27) { return 0; } // escape //} return 0; } // @function detectAndDisplay void detectAndDisplay(Mat frame) { face_cascade.load(face_cascade_name); eyes_cascade.load(eyes_cascade_name); std::vector<Rect> faces; Mat frame_gray; cvtColor(frame, frame_gray, COLOR_BGR2GRAY); equalizeHist(frame_gray, frame_gray); //-- Detect faces face_cascade.detectMultiScale(frame_gray, faces, 1.1, 3, CV_HAAR_DO_ROUGH_SEARCH, Size(50, 50)); for (size_t j = 0; j < faces.size(); j++) { Mat faceROI = frame(faces[j]); Mat MyFace; if (faceROI.cols > 100) { resize(faceROI, MyFace, Size(92, 112)); string str = format(\"D:\\\\研究生阶段2016-2019\\\\计算机视觉\\\\实验室CV\\\\第二节\\\\小组展示\\\\Face Recognition\\\\Face Recognition\\\\Face Recognition\\\\MyFcae\\\\%d.jpg\", 4); imwrite(str, MyFace); imshow(\"ii\", MyFace); } waitKey(10); } /*以下可以同时检测出人眼和眼睛。 face_cascade.detectMultiScale(frame_gray, faces, 1.1, 3, CV_HAAR_DO_ROUGH_SEARCH, Size(10, 10), Size(400, 400)); namedWindow(window_name, 2); for (size_t i = 0; i < faces.size(); i++) { //Point center(faces[i].x + faces[i].width / 2, faces[i].y + faces[i].height / 2); //ellipse(frame, center, Size(faces[i].width / 2, faces[i].height / 2), 0, 0, 360, Scalar(255, 0, 255), 4, 8, 0); rectangle(frame, faces[i], Scalar(255, 0, 0), 2, 8, 0); Mat faceROI = frame_gray(faces[i]); std::vector<Rect> eyes; //-- In each face, detect eyes eyes_cascade.detectMultiScale(faceROI, eyes, 1.1, 1, CV_HAAR_DO_ROUGH_SEARCH, Size(3, 3)); for (size_t j = 0; j < eyes.size(); j++) { Rect rect(faces[i].x + eyes[j].x, faces[i].y + eyes[j].y, eyes[j].width, eyes[j].height); //Point eye_center(faces[i].x + eyes[j].x + eyes[j].width / 2, faces[i].y + eyes[j].y + eyes[j].height / 2); //int radius = cvRound((eyes[j].width + eyes[j].height)*0.25); //circle(frame, eye_center, radius, Scalar(255, 0, 0), 4, 8, 0); rectangle(frame, rect, Scalar(0, 255, 0), 2, 8, 0); } }*/ //-- Show what you got namedWindow(window_name, 2); imshow(window_name, frame); }\n以上贴出的代码，就是传入一张带有人脸的图片，程序会识别出其中的人脸，并将其用矩形框出来。\n三、HOG特征提取\nHOG特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。通过计算和统计图像局部区域的梯度方向直方图来构成特征。其本质为梯度的统计信息，而梯度主要存在于边缘的地方。可以很好的运用于行人目标的检测，可以和svm或者softmax结合在一起用于对目标的分类。\n四、LBP\nLBP （Local Binary Pattern）局部二值模式：是一种用来描述图像局部特征的算子，LBP特征具有灰度不变性和旋转不变性等显著优点。LBP特征比较出名的应用是用在人脸识别和目标检测中。\nLBP可以和Haar-like搭配起来，先用Haar-like检测出图像中的人脸，挑选出一部分训练集，用于opencv中LBP的训练，之后可以在电脑上开启摄像头，实现在线的人脸检测。\nPtr<FaceRecognizer> model2 = createLBPHFaceRecognizer(); model2->train(images, labels); model2->save(\"MyFaceLBPHModel.xml\");\n五、PCA\nCA（Principal Component Analysis）主成因分析:是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。PCA是人脸识别的主流方法之一。其核心思想是：重整高维数据，提取其中的重要部分，忽略其中无关紧要的部分。\n其实，PCA与LBP类似，均可以用来训练图像中的人脸，训练处所需识别人脸的模型用以检测。\n六、SIFT\n尺度不变特征变换匹配算法(Scale Invariant Feature Transform , SIFT)，SIFT算法是用来提取图像局部特征的经典算法，SIFT算法的实质是在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。\n优点\n1. SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性。\n2. 独特性好，信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配；\n3. 多量性，即使少数的几个物体也可以产生大量的SIFT特征向量。\n4. 高速性，经优化的SIFT匹配算法甚至可以达到实时的要求。\n5. 可扩展性，可以很方便的与其他形式的特征向量进行联合。\n缺点\n1. 实时性不够高。\n2. 对边缘光滑的目标无法准确提取特征点，对圆更是无能为力。\n3. 对模糊的图像，检测出的特征点过少。\n主要应用\n1. 目标的旋转、缩放、平移\n2. 图像仿射/投影变换\n3. 光照影响\n4. 目标遮挡\n5. 杂物场景\n6. 噪声\n典型应用于物体识别、机器人定位与导航、图像拼接、三维建模、手势识别、视频跟踪、笔记鉴定、指纹与人脸识别、犯罪现场特征提取......\n七、SURF\nSift算法的优点是特征稳定，对旋转、尺度变换、亮度保持不变性，对视角变换、噪声也有一定程度的稳定性；缺点是实时性不高，并且对于边缘光滑目标的特征点提取能力较弱。\nSurf（Speeded Up Robust Features）改进了特征的提取和描述方式，用一种更为高效的方式完成特征的提取和描述。\n具体实现流程如下：\n1. 构建Hessian（黑塞矩阵），生成所有的兴趣点，用于特征的提取；\n2. 构建尺度空间\n3. 特征点定位\n4. 特征点主方向分配\n5. 生成特征点描述子\n6. 特征点匹配\n八、距离度量方法\n（1）、欧式距离\n欧氏距离是最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中。\n（2）、曼哈顿距离\n可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。\n（3）、切比雪夫距离\n各对应坐标数值差的最大值。\n（4）、闵可夫斯基距离\n闵氏距离不是一种距离，而是上述三组距离的总定义。\n（5）、标准化欧氏距离\n标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。"}
{"content2":"目录\n0  如果你觉得这个页面广告太多，请点击下面教程（我写的）去广告\n1  网络拆分，一个网络里面有什么，作用是什么\n1.0   基础知识，从零开始\n30分钟明白深度学习怎么学习的，权重是什么，什么是梯度下降，损失怎么计算的。油管播放量60多万的视频，很社会了。那么或许你已经看过一些书和论文，甚至能够搭建自己的网络，但我还是看了，觉得有所收获。\n1.1  激活函数\n入门简单易懂：但是我没看明白什么叫神经元死了（ReLU）\n在第一篇的基础上往广度拓展，但是没有拓展深度，同样没有解释什么叫神经元没反应了，具体到feature map与conv层上，不过我们可以明确的是小心设置比较小的learning rate会让节点不轻易死掉\n很有趣的补充，涉及到人脑神经元的特性，稀疏性，单侧抑制，宽阔的阈值边界\n1.2 BN batch normalization 归一化\n普通的机器学习归一化作为基础\n把归一化应用与网络\n2.流行网络举例\n2.1   ResNet\n1.第一个链接我给youtube,由于我在看Andrew Wang的CNN视频，里面提到了ResNet网络可以在加深网络的时候不回弹识别率，因为一般网络深度超过一个程度，错误率会上升。\n2.给妹纸的深度学习教学（4）\n解决梯度消失\n油管一个中文的视频\n2.2 reinforcement learning\n2.3 YOLO vs RCNN\n3.tensorflow 教程\n3.1 keras\n0  如果你觉得这个页面广告太多，请点击下面教程（我写的）去广告\nhttps://blog.csdn.net/u013249853/article/details/79814508\n这个广告是这个网站自己放的，很影响专注程度，所以如果你比较常用线上学习，推荐去掉广告一劳永逸，不然很分散注意力的，降低学习效率。\n1  网络拆分，一个网络里面有什么，作用是什么\n那啥，你可能不想看我的读后感，或者评价或者总结，或者是废话，那你可以等我出完完全版整个单纯的链接。教程是这样的，你作为基础的，最先看得必须是最权威的东西，这个给你打基础，然后你看不明白，再去看看别人是怎么理解的。所以知识的一手资源是论文，其次是论文作者，或者权威教授写的书，然后是一些大神的产物比如视频，博客，这个知识被咀嚼这么多遍，变得容易理解，教学技巧性更强，更有说服力，走入了大众视野，就成了各种普通人，比如我，的博客。越往下走，越可能产生错误，因为博客主人会加入自己的理解，除非他是转载，所以有可能出错，毕竟没有经过论文审核，出版审计。比如我，通常不会通读自己写的东西第3遍。\n怎么找这些东西，一般情况下，你看的教程都会推荐，比方说下面的图片提到的note\n所以推荐的推荐，越高越难，越学术，越理论。\n所以打基础的看官务必不要忽视权威的重要性，不要在csdn上闲逛，看到什么都觉得说得对，事实上，我看到过不少错误，小错，虽然我不会，但我能推啊。所以大家学习的时候一定要先建立一个基础概念，比方说上个课，网易云课堂，coursera，英文好可以上油管，我下面推的那个油管andrew wang的视频就是cousera的。很基础，很短，很直观。\n之后，你再去随意浏览一些博客，看看别人怎么想的。不论你看什么一定要质疑，即使是大牛论文，虽然最后结果一定人家对，自己没理解好，那你这不就加深理解，查漏补缺，揪出自己的bug了吗。你在找别人问题的时候一定是思考轮子转的最快的时候，真的。\n1.0   基础知识，从零开始\n30分钟明白深度学习怎么学习的，权重是什么，什么是梯度下降，损失怎么计算的。油管播放量60多万的视频，很社会了。那么或许你已经看过一些书和论文，甚至能够搭建自己的网络，但我还是看了，觉得有所收获。\nhttps://www.youtube.com/watch?v=ILsA4nyG7I0\n图像的基本单位是像素，显示为一个一个小方格，数值是0-255，0是灰色的。如果是彩色，那么有三个通道。每个位置确定好颜色，就是一张图片了。每个位置的数值，或者是单值，比如23（灰度图像只有黑白），或者是一个三维的，(23,34,255)（彩色图像）。\n看不明白的话就暂停，想一想，或者多看几遍。记点笔记。\n推荐在油管看，用自动生成字幕，作者母语英语，自动生成大部分都很准。下面是B站地址，不用科学上网也能上：\nhttps://www.bilibili.com/video/av14095273?from=search&seid=10011126792909487890\n点击设置键打开字幕，视频右下角，第二个小齿轮，上面有鲜红色的HD，如下图：\n推荐在油管看，用英文字幕，油管自动翻译我没试过，不过谷歌的自动翻译很不错的。\n26分钟搞清楚卷积神经网络是什么，卷积怎么卷，能干啥。30多万播放量\nhttps://www.youtube.com/watch?v=FmpDIaiMIeA&feature=youtu.be\nhttp://brohrer.github.io/how_convolutional_neural_networks_work.html\n下面是B站地址：\nhttps://www.bilibili.com/video/av19231561?from=search&seid=13469907213316774113\n1.1  激活函数\n默认每层之间都有relu\n当你计算完卷积了，你要形成新的一层了，这个时候，桥豆麻袋！！你还没用激活函数呢，你的新网络每个像素的值有正有负，大于零的原样输出，小于零的统统归零。简单直观。你就是拿个筛子，筛掉小于零的部分。嗯这就是Relu这种激活函数。还有其他种的，可以在下面的教程里面找到。不过现在都用relu。\n入门简单易懂：但是我没看明白什么叫神经元死了（ReLU）\nhttps://www.jianshu.com/p/22d9720dbf1a\n在第一篇的基础上往广度拓展，但是没有拓展深度，同样没有解释什么叫神经元没反应了，具体到feature map与conv层上，不过我们可以明确的是小心设置比较小的learning rate会让节点不轻易死掉\nhttps://blog.csdn.net/cyh_24/article/details/50593400\n很有趣的补充，涉及到人脑神经元的特性，稀疏性，单侧抑制，宽阔的阈值边界\nhttp://www.cnblogs.com/neopenx/p/4453161.html\n我觉得稀疏性可能对应我们人类这么一个现象，我们不会记住耳机的具体形状，我们只有注意力集中的时候才能学习，我们只有思考的时候才有收获，或者及水到渠成的灵光一闪。于是我们的意识可以控制我们的神经元。比如你扫一眼桌子，别说话，扫。\n你闭上眼睛，你不会记住你看到了什么，你眼睛的焦点不会自动对准每一个物体。但是如果你有目的扫过，你会发现，你桌子上的东西存在感比之前要强。呃，不过可能你的桌子比较整齐，没有我这么深的感触。\n至于稀疏激活，我认为是数值小于阈值的就让他等于0，这样我如果设置一个比较高的阈值，那么大部分都会被一致，也就是等于零。\n这里再说下他所说的信息解离，信息之间是有相关性的，特征向量就是信息，你一次采集了很多特征向量，任何一条信息都不能被其他信息线性组合表达，那么就是线性无关，如果我们做的是非线性处理，让我的数据之间不仅线性无关，完全就是无关。这个时候，信息的表达是最简介，少，且有价值，简直黄金信息。如果把这些相互牵连的冗余噪声比作赘肉的话，大概就会很重视它了。相信这个谁都不陌生，多少都听过。非常感兴趣可以看下多媒体技术，信息熵。\n1.2 BN batch normalization 归一化\n普通的机器学习归一化作为基础\nhttps://blog.csdn.net/qq_28618765/article/details/78221571\n把归一化应用与网络\nhttps://blog.csdn.net/hjimce/article/details/50866313\n2.流行网络举例\n2.1   ResNet\n1.第一个链接我给youtube,由于我在看Andrew Wang的CNN视频，里面提到了ResNet网络可以在加深网络的时候不回弹识别率，因为一般网络深度超过一个程度，错误率会上升。\nhttps://www.youtube.com/watch?v=GSsKdtoatm8\n2.给妹纸的深度学习教学（4）\nhttps://zhuanlan.zhihu.com/p/28413039\n这个右边的是针对网络变深，这时通道变多，那么我们可以先拧出水分，拿64个通道来稀疏一下，这个感觉，类似主成分分析，然后再用3*3大小的卷积模板开始提取特征。之后我们再给他还原回去\n解决梯度消失\nhttps://zh.gluon.ai/chapter_convolutional-neural-networks/resnet-gluon.html\n前面两个都没有说为什么残差网络好，或者说没有解释道这个点，梯度消失。应该和之前的死掉的神经元不是一个概念。\n油管一个中文的视频\n地址在这里，比之前推荐的要好，感觉说的更加明白，说出了关键的部分就是梯度的算法的不同，以及零一取让该层网络被忽略会比较有效，也比较有条理。我自己用文字总结了一下，在这里\n2.2 reinforcement learning\n这里说的是一个不同于有标签的监督性训练的，reward机制的训练。可以让机器像人一样玩游戏，像素游戏，通过奖惩机制让他得到更高的分数，比如alpha go，还有很多其他游戏等，这个也是个比较新的概念。教程有个油管小哥总结得很快，能够让你快速了解。然后他又推荐了一篇博客。有趣的是还有一个视频说的是热reinfocement在股票市场的应用，但是这里并不是股票预测，而是将machine培训成一个玩家，那么虽然课程里面的内容很新颖，但是由于是个互动式录屏，所以感觉一个小时的东西由于不断地交互被硬生生拉长到了两个小时，至于reinforcement learning也是在1.30出现。而且感觉讲师没有做到把复杂的东西讲简单，导致重点不明。记住预测股票是很难的，你需要很多信息包括各国整层，尤其是美国，财务报表，机构动向等等信息都要输入给机器。小股票噪声大难以预测，大股票比如微软，稳定，容易预测，但是没啥用，因为他稳。所以目前更好的方法是用reinforcement learning 去做一个对抗博弈模型，将你的机器拍养成巴菲特。那么感觉这也很难对吧。所以应用点在哪里，就在于短期预测，比如这5分钟究竟是买入还是卖出，这种人力大脑没法算出来的东西，可以参考下机器。\n2.3 YOLO vs RCNN\n在目标检测这件事上，yolo和RCNN是两种主流框架，yolo非常快，该算法作者在TED有一篇演讲，可以看一下。这歌油管小哥总结了下两者，他更加偏爱YOLO，毕竟快。有趣的是比YOLO原作者视频播放量还高。\n3.tensorflow 教程\n视频的话，这里有个小哥在线码代码。\n同时这里有个比较好的，比较有延展性的教学文字帖，他们还提供了一个jupyter notebook版本，这里。\n3.1 keras\n这里有一份keras的中文文档。这里有一份官方的中文文档。那么document其实不等同于教程，因为他更类似于一个字典。所以最好是从例子开始。这里推荐看看官方的例子，以及opencv提供的一份基础的，更加面向初学者的教程,该教程用到的示例代码可以在github下载到。另外官方还额外的推荐了一个专门修改微调现有的流行网络（resnet inception等）的博客。"}
{"content2":"题主对计算机视觉非常感兴趣，听到、看到与视觉相关的东西时就很激动。但是，一开始学习，自己就没有怎么激动了，主要是感觉视觉相关的知识不是太好理解，学着学着就学不下去了。想请问各位大佬，对于我这种情况该如何解救？希望能够得到各位大牛的指点。"}
{"content2":"v  个人背景\nü  上海交通大学计算机系计算机应用专业博士毕业，在校期间主研方向为：数据挖掘、机器学习、深度学习、大数据、等人工智能领域。\nü  在国内外期刊、会议中发表论文数十篇篇（包含专著一篇），其中被SCI/EI收录4篇，论文被引用数量达242次。\nü  开发实战经验丰富，曾单独完成大型智能软件，负责多项人工智能项目。\nü  现任某大型上市IT企业的架构师、资深技术专家、行业总监；高级工程师，获科技创新一等奖。\nv  授课经历和特色\nü  有12年一线技术、行业经验，5年以上的培训经验，曾特邀为各大高校培训人工智能师资。\nü  理论功底非常扎实，制作的课件条理清晰，结构严谨、深入浅出，环环相扣。\nü  擅于将复杂的人工智能理论知识和数学推导以通俗易懂的形式表达出来。\nü  擅于将理论知识与实际案例相结合，敏锐捕捉到知识体系中的“难点”、“重点”和“遗忘点”，并通过合适的案例加以强化。\nü  声音洪亮、口齿清晰，普通话非常标准。曾开设线上直播，录播，面授等多种形式的人工智能系列课程，均获得良好的反馈。\nü  为不同行业和不同需求定制课程和案例，贴近企业的需求。\nv  主要资质\nü  工学博士，上海交通大学计算机专业人工智能研究领域\nü  高级工程师\nü  高级程序员\nv  课程目录\nü  《数据分析与数据挖掘导论》\nü  《人工智能系列课程》\nü  《数据挖掘与机器学习》\nü  《健康医疗大数据》\nv  培训客户：\nü   南京晓庄学院师资培训\nü   ……\nv  项目经验：\n……加助理QQ详细沟通"}
{"content2":"北京大学设立人工智能学院不是梦！\n根据党中央、国务院的部署，国务院近期发布“新一代人工智能发展规划的通知”(国发〔2017〕35号)，其中相关条款规定：“建设人工智能学科。完善人工智能领域学科布局，设立人工智能专业，推动人工智能领域一级学科建设，尽快在试点院校建立人工智能学院，增加人工智能相关学科方向的博士、硕士招生名额。鼓励高校在原有基础上拓宽人工智能专业教育内容，形成“人工智能+X”复合专业培养新模式，重视人工智能与数学、计算机科学、物理学、生物学、心理学、社会学、法学等学科专业教育的交叉融合。加强产学研合作，鼓励高校、科研院所与企业等机构合作开展人工智能学科建设。”\n北京大学学科齐全，巡视整改成功，争当“试点”单位不成问题。顺势而为，必定成功！\n袁萌   9月2日"}
{"content2":"信号与图像处理、模式识别与计算机视觉\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n国际计算机视觉与模式识别会议，计算机视觉领域三大顶会之一，每年在美国召开\nJun. 16-21, 2019, Long Beach, CA, USA\n主页：cvpr2019.thecvf.com\nH5指数：188\nInternational Conference on Computer Vision (ICCV)\n国际计算机视觉会议，计算机视觉领域三大顶会之一，奇数年召开\nOct. 27 - Nov. 03, 2019, Seoul, Korea\n主页：iccv2019.thecvf.com\nH5指数：124\nEuropean Conference on Computer Vision (ECCV)\n欧洲计算机视觉会议，计算机视觉领域三大顶会之一，偶数年在欧洲召开\nAug. 23-28, 2020, Glasgow, Scotland\n主页：eccv2020.eu\nH5指数：104\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n国际声学、语音与信号处理会议，规模最大、最全面的信号处理及其应用方面的会议，每年召开\nMay 04-09, 2020, Barcelona, Spain\n主页：2020.ieeeicassp.org\nH5指数：79\nBritish Machine Vision Conference (BMVC)\n英国机器视觉会议，计算机视觉领域著名国际会议，每年在英国召开\nSept. 09-12, 2019, Cardiff University, UK\n主页：bmvc2019.org\nH5指数：42\nIEEE International Conference on Image Processing (ICIP)\n国际图像处理会议，每年召开\nSept. 25-28, 2020, Abu Dhabi, UAE\n主页：Coming soon\nH5指数：41\nAsian Conference on Computer Vision (ACCV)\n亚洲计算机视觉会议，在亚太地区召开，2010年之前奇数年召开，之后偶数年\nNov. 30 - Dec. 04, 2020, Kyoto, Japan\n主页：accv2020.kyoto\nH5指数：30\nIEEE Global Conference on Signal and Information Processing (GlobalSIP)\n全球信号与信息处理会议，每年召开\nNov. 11–14, 2019, Ottawa, Canada\n主页：2019.ieeeglobalsip.org\nH5指数：28\nInternational Conference on Pattern Recognition (ICPR)\n国际模式识别会议，偶数年召开\nSep. 13-18, 2020, Milan, Italy\n主页：icpr2020.com\nH5指数：28\nEuropean Signal Processing Conference (EUSIPCO)\n欧洲信号处理会议，每年在欧洲召开\nAug. 24-28, 2020, Amsterdam, Netherlands\n主页：eusipco2020.org\nH5指数：23\nIEEE International Conference on Visual Communications and Image Processing (VCIP)\n国际视觉通信与图像处理会议，每年召开\nDec. 01-04, 2019, Sydney, Australia\n主页：www.multimediauts.org/VCIP2019\nH5指数：14\n雷达遥感\nIEEE International Geoscience and Remote Sensing Symposium (IGARSS)\n国际地球科学与遥感大会，每年召开\nJul. 19-24, 2020, Hawaii, USA\n主页：www.igarss2020.org\nH5指数：21\nSPIE Remote Sensing\nSPIE(国际光学工程学会)国际遥感会议，每年在欧洲召开\nSept. 09-12, 2019, Strasbourg, France\n主页：spie.org/conferences-and-exhibitions/remote-sensing\nIEEE Radar Conference (RadarConf)\nIEEE雷达会议，每年召开\nSep. 21-25, 2020, Florence, Italy\n主页：www.radarconf20.org\nH5指数：25\nIEEE International Radar Conference (RADAR)\n国际雷达会议，美中英澳法五国每年轮流召开\nApr. 27 - May. 01, 2020, Washington, DC, USA\n主页：Coming soon\nH5指数：9\nIET International Radar Conference (IRC)\nIET国际雷达会议，每五年召开两次\nOct. 17-19, 2018, Nanjing, China\n主页：www.ietradar.org/\nH5指数：13\nInternational Radar Symposium (IRS)\n国际雷达研讨会，每年在欧洲召开\nJun. 26-28, 2019, Ulm, Germany\n主页：www.dgon-irs.org\nH5指数：14\nEuropean Radar Conference (EuRAD)\n欧洲雷达会议，欧洲微波年会(Europe Microwave Week)之一，每年在欧洲召开\nOct. 02-04, 2019, Paris, France\n主页：www.eumweek.com\nH5指数：12\nEuropean Conference on Synthetic Aperture Radar (EuSAR)\n欧洲合成孔径雷达会议，偶数年在欧洲召开\nJun. 15-18, 2020, Leipzig, Germany\n主页：www.eusar.de\nAsia-Pacific Conference on Synthetic Aperture Radar (APSAR)\n亚太合成孔径雷达会议，奇数年在亚太地区召开\nNov. 26-29, 2019, Xiamen, China\n主页：apsar2019.org.cn\nH5指数：9\nSPIE Asia-Pacific Remote Sensing\nSPIE（国际光学工程学会）亚太遥感会议，偶数年在亚太地区召开\nSept. 24-26, 2018, Honolulu, Hawaii, United States\n主页：spie.org/conferences-and-exhibitions/asia-pacific-remote-sensing\n机器学习与人工智能\nNeural Information Processing Systems (NeurIPS)\n神经信息处理系统大会，机器学习领域的顶级会议，每年召开\nDec. 08-14, 2019, Vancouver, Canada\n主页：nips.cc/Conferences/2019\nH5指数：134\nInternational Conference on Machine Learning (ICML)\n国际机器学习会议，如今已发展为由国际机器学习学会（IMLS）主办的年度机器学习国际顶级会议，每年召开\nJul. 13-18, 2020, Vienna, Austria\n主页：icml.cc/Conferences/2019\nH5指数：113\nAAAI Conference on Artificial Intelligence (AAAI)\n美国人工智能协会主办的人工智能大会，人工智能领域的顶级会议之一，每年在美国召开（个别年份在加拿大）\nJan. 27 - Feb. 01, 2019, Honolulu, Hawaii, USA\n主页：aaai.org/Conferences/AAAI-19\nH5指数：69\nInternational Joint Conference on Artificial Intelligence (IJCAI)\n国际人工智能联合会议，人工智能领域顶级国际会议，原为奇数年召开，自2015年起改为每年召开\nAug. 10-16, 2019, Macao, China\n主页：ijcai19.org\nH5指数：61\nInternational Conference on Learning Representations (ICLR)\n国际学习表征会议，深度学习顶级会议，由位列深度学习三大巨头之二的 Yoshua Bengio 和 Yann LeCun 牵头创办，每年召开\nMay 06-09, 2019, New Orleans, USA\n主页：iclr.cc/Conferences/2019"}
{"content2":"计算机视觉领域会议和期刊排名（根据CCF排名）：\n中国计算机协会推荐国际学术会议：\nA类：\n会议简称\n会议全称\n出版社\n网址\n会议时间\nCVPR\nIEEE Conference on Computer Vision and Pattern Recognition\nIEEE\nhttp://dblp.uni-trier.de/db/conf/cvpr/\n一年一次\nICCV\nInternational Conference on Computer Vision\nIEEE\nhttp://dblp.uni-trier.de/db/conf/iccv/\n两年一次  奇数年\nB类：\nECCV\nEuropean Conference on Computer Vision\nSpringer\nhttp://dblp.uni-trier.de/db/conf/eccv/\n两年一次偶数年\nC类：\nACCV\nAsian Conference on Computer Vision\nSpringer\nhttp://dblp.uni-trier.de/db/conf/accv/\n两年一次\n偶数年\nICPR\nInternational Conference on Pattern Recognition\nIEEE\nhttp://dblp.uni-trier.de/db/conf/icpr/\n两年一次\n偶数年\nICB\nInternational Joint\nConference on Biometrics\nIEEE\nhttp://dblp.uni-trier.de/db/conf/icb/\nBMVC\nBritish Machine Vision Conference\nBritish Machine Vision Association\nhttp://dblp.uni-trier.de/db/conf/bmvc/\n一年一次\nICIP\nInternational Conference on Image Processing\nhttp://dblp.uni-trier.de/db/conf/icip/\n一年一次\n中国计算机协会推荐国际学术期刊：\nA类：\n刊物简称\n刊物全称\n出版社\n网址\nTPAMI\nIEEE Trans on Pattern Analysis and Machine Intelligence\nIEEE\nhttp://dblp.uni-trier.de/db/journals/pami/\nIJCV\nInternational Journal of Computer Vision\nSpringer\nhttp://dblp.uni-trier.de/db/journals/ijcv/\nB类：\nCVIU\nComputer Vision and Image Understanding\nElsevier\nhttp://dblp.uni-trier.de/db/journals/cviu/\nJAIR\nJournal of AI\nResearch\nAAAI\nhttp://dblp.uni-trier.de/db/journals/jair/index.html\nPR\nPattern Recognition\nElsevier\nhttp://dblp.uni-trier.de/db/conf/par/\nhttps://www.sciencedirect.com/journal/pattern-recognition/issues\nC类：\nIET-CVI\nIET Computer Vision\nIET\nhttp://digital-library.theiet.org/content/journals/iet-cvi\nIVC\nImage and Vision Computing\nElsevier\nhttp://dblp.uni-trier.de/db/journals/ivc/\nIJPRAI\nInternational Journal of Pattern Recognition and Artificial Intelligence\nWorld Scientific\nhttp://dblp.uni-trier.de/db/journals/ijprai/\nPAA\nPattern Analysis and Applications\nSpringer\nhttp://dblp.uni-trier.de/db/journals/paa/\nPRL\nPattern Recognition Letters\nElsevier\nhttp://dblp.uni-trier.de/db/journals/prl/\n参考链接：http://history.ccf.org.cn/sites/ccf/biaodan.jsp?contentId=2903940690839"}
{"content2":"以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/P ... CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页； http://www-2.cs.cmu.edu/afs/cs/project/ ... ision.html\n（17）微软CV研究员Richard Szeliski； http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/gro ... fault.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision; http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups; http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews"}
{"content2":"在计算机视觉领域，每年都会有很多顶级会议召开，如比较著名的CVPR，ICCV等，在会议上会有CV各个领域的新思想、新方法被提出来，推动着这个领域的发展，以下为2019年各个会议的时间地点，还有会议相关链接。\n1. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019)\nLocation ：Long Beach Convention & Entertainment Center, Los Angeles CA, United States\nDate：Jun 15 - Jun 21, 2019\nPaper Submission Deadline：Nov 16, 2018 (92)\nhttp://cvpr2019.thecvf.com\n２. IEEE International Conference on Computer Vision (ICCV 2019)\nLocation ：Seoul, South Korea\nDate: Oct 27 - Nov 3, 2019\nPaper Submission Deadline: Mar, 2019\nhttp://iccv2019.thecvf.com\nICML 2019\n3. International Conference on Machine Learning（ICML 2019）\nLocation：Long Beach Convention Center, Long Beach, United States\nDate：Jun 10 - Jun 15, 2019\nPaper Submission Deadline: TBD\nhttps://icml.cc/Conferences/2019\nAAAI 2019\n4.Association for the Advancement of Artificial Intelligence（AAAI 2019）\nLocation：Hilton Hawaiian Village, Waikiki Beach, Honolulu, Hawaii, United States\nDate：Jan 27 - Feb 1, 2019\nPaper Submission Deadline:Sep 5, 2018 (20)\nhttp://www.aaai.org/aaai19\n5.International Joint Conference on Artificial Intelligence（IJCAI 2019）\nLocation：Macao, China\nDate：Aug 10 - Aug 16, 2019\nPaper Submission Deadline：Feb 25, 2019\nhttp://www.ijcai19.org\n6. Computer Graphics and Interactive Techniques（SIGGRAPH 2019）\nLocation：Los Angeles, California, United States\nDate：Jul 29 – Aug 1, 2019\nPaper Submission Deadline：TBD\nhttp://s2019.siggraph.org\n7. ACM SIGKDD International Conference on Knowledge discovery and data mining（SIGKDD 2019）\nLocation：Anchorage, Alaska, United States\nDate：Aug 3 - Aug 7, 2019\nPaper Submission Deadline：TBD\nhttp://www.kdd.org/kdd2019\n8. Annual Meeting of the Association for Computational Linguistics（ACL 2019）\nLocation：Florence, Italy\nDate：Jul 28 - Aug 2, 2019\nPaper Submission Deadline：TBD\nhttp://acl2019.org\nICLR 2019\n9. International Conference on Learning Representations（ICLR 2019)\nLocation：New Orleans, United States\nDate：May 6 - May 9. 2019\nPaper Submission Deadline：Sep 27, 2018 (42)\nhttp://www.iclr.cc\nAAAI 2019\n会议名称：Association for the Advancement of Artificial Intelligence\n会议地点：美国夏威夷\n会议时间：2019.01.27 - 02.01\n网址：http://www.aaai.org/aaai19\n介绍：人工智能领域顶级会议\nICLR 2019\n会议名称：International Conference on Learning Representations\n会议地点：美国新奥尔良\n会议时间：2019.05.06 - 09\n网址： http://www.iclr.cc\n介绍：神经网络顶会\nCVPR 2019\n会议名称： IEEE Conference on Computer Vision and Pattern Recognition\n会议地点：美国洛杉矶\n会议时间：2019.06.15 - 21\n网址：http://cvpr2019.thecvf.com\n介绍：计算机视觉及模式识别领域国际三大顶级会议之一\nICCV 2019\n会议名称：International Conference on Computer Vision\n会议地点：韩国首尔\n会议时间：2019.10.27 - 11.03\n网址： http://iccv2019.thecvf.com\n介绍：计算机视觉及模式识别领域国际三大顶级会议之一\n---------------------\n参照：\nhttps://blog.csdn.net/hitzijiyingcai/article/details/81709755\nhttps://baijiahao.baidu.com/s?id=1589082262597219216&wfr=spider&for=pc\nhttps://blog.csdn.net/electech6/article/details/84384852"}
{"content2":"http://blog.csdn.net/zouxy09\n2012年8月21号开始了我的第一篇博文，也开始了我的研究生生涯。怀着对机器学习和计算机视觉等等领域的懵懂，从一个电子材料的领域跨入这个高速发展的人工智能领域。从开始的因无知而惊慌，因陌生而乏力，到一步步的成长。这过程的知识积累也都大部分反映在这个博客上面了。感谢这个平台促使自己去总结去坚持去进步。也感谢这个平台给我带来了和大家交流的机会。借此博文总结自己过去与未来可能散乱的博文。在此也谢谢大家一直的支持和鼓励，谢谢。\n一、基于计算机视觉的目标跟踪\n计算机视觉、机器学习相关领域论文和源代码大集合\n计算机视觉目标检测的框架与过程\n最简单的目标跟踪（模版匹配）\n压缩感知（Compressive Sensing）学习之（一）\n压缩感知（Compressive Sensing）学习之（二）\n压缩跟踪Compressive Tracking\n压缩跟踪Compressive Tracking源码理解\nTLD（Tracking-Learning-Detection）学习与源码理解之（一）\nTLD（Tracking-Learning-Detection）学习与源码理解之（二）\nTLD（Tracking-Learning-Detection）学习与源码理解之（三）\nTLD（Tracking-Learning-Detection）学习与源码理解之（四）\nTLD（Tracking-Learning-Detection）学习与源码理解之（五）\nTLD（Tracking-Learning-Detection）学习与源码理解之（六）\nTLD（Tracking-Learning-Detection）学习与源码理解之（七）\nCVPR2013一些论文集合供下载（visual tracking相关）\n时空上下文视觉跟踪（STC）算法的解读与代码复现\n基于感知哈希算法的视觉目标跟踪\n基于meanshift的手势跟踪与电脑鼠标控制（手势交互系统）\n关于计算机视觉（随谈）\n二、Deep Learning 深度学习\nDeep Learning（深度学习）学习笔记整理系列之（一）\nDeep Learning（深度学习）学习笔记整理系列之（二）\nDeep Learning（深度学习）学习笔记整理系列之（三）\nDeep Learning（深度学习）学习笔记整理系列之（四）\nDeep Learning（深度学习）学习笔记整理系列之（五）\nDeep Learning（深度学习）学习笔记整理系列之（六）\nDeep Learning（深度学习）学习笔记整理系列之（七）\nDeepLearning（深度学习）学习笔记整理系列之（八）\nDeepLearning源代码收集\nDeep Learning论文笔记之（一）K-means特征学习\nDeep Learning论文笔记之（二）Sparse Filtering稀疏滤波\nDeep Learning论文笔记之（三）单层非监督学习网络分析\nDeep Learning论文笔记之（四）CNN卷积神经网络推导和实现\nDeep Learning论文笔记之（五）CNN卷积神经网络代码理解\nDeep Learning论文笔记之（六）Multi-Stage多级架构分析\nDeep Learning论文笔记之（七）深度网络高层特征可视化\nDeep Learning论文笔记之（八）Deep Learning最新综述\n基于3D卷积神经网络的行为理解（论文笔记）\n三、机器学习相关\n机器学习算法中文视频教程\n机器学习知识点学习\n从最大似然到EM算法浅解\n浅说机器学习中“迭代法”\n径向基网络（RBF network）之BP监督训练\n模板匹配中差值的平方和（SSD）与互相关准则的关系\n生成模型和判别模型\n机器学习中的范数规则化之（一）L0、L1与L2范数\n机器学习中的范数规则化之（二）核范数与规则项参数选择\nLibLinear（SVM包）使用说明之（一）README\nLibLinear（SVM包）使用说明之（二）MATLAB接口\nLibLinear（SVM包）使用说明之（三）实践\n计算机视觉、机器学习相关领域论文和源代码大集合\n机器学习算法与Python实践之（一）k近邻（KNN）\n机器学习算法与Python实践之（二）支持向量机（SVM）初级\n机器学习算法与Python实践之（三）支持向量机（SVM）进阶\n机器学习算法与Python实践之（四）支持向量机（SVM）实现\n机器学习算法与Python实践之（五）k均值聚类（k-means）\n机器学习算法与Python实践之（六）二分k均值聚类\n机器学习算法与Python实践之（七）逻辑回归（Logistic Regression）\n基于稀疏矩阵的k近邻（KNN）实现\n神经网络训练中的Tricks之高效BP（反向传播算法）\n人脸识别之特征脸方法（Eigenface）\n四、Kinect相关学习与实践\nKinectSDK v1.7 新特性、交互框架与新概念\nKinect开发学习笔记之（一）Kinect介绍和应用\nKinect开发学习笔记之（二）Kinect开发学习资源\nKinect开发学习笔记之（三）Kinect开发环境配置\nKinect开发学习笔记之（四）提取颜色数据并用OpenCV显示\nKinect开发学习笔记之（五）不带游戏者ID的深度数据的提取\nKinect开发学习笔记之（六）带游戏者ID的深度数据的提取\nKinect开发学习笔记之（七）骨骼数据的提取\nKinect开发学习笔记之（八）彩色、深度、骨骼和用户抠图结合\n五、语音信号处理与语音识别\n语音信号处理之（一）动态时间规整（DTW）\n语音信号处理之（二）基音周期估计（Pitch Detection）\n语音信号处理之（三）矢量量化（Vector Quantization）\n语音信号处理之（四）梅尔频率倒谱系数（MFCC）\n语音的基本概念--译自CMU sphinx\n语音识别的基础知识与CMUsphinx介绍\nPocketSphinx语音识别系统的编译、安装和使用\nPocketSphinx语音识别系统语言模型的训练和声学模型的改进\nPocketSphinx语音识别系统声学模型的训练与使用\nPocketSphinx语音识别系统的编程\n六、运动检测\n运动检测（前景检测）之（一）ViBe\n运动检测（前景检测）之（二）混合高斯模型GMM\n七、图像特征分析\n目标检测的图像特征提取之（一）HOG特征\n目标检测的图像特征提取之（二）LBP特征\n目标检测的图像特征提取之（三）Haar特征\n八、图像处理相关\n简单粗糙的指尖检测方法（FingerTipsDetection）\n光流Optical Flow介绍与OpenCV实现\n用单张2D图像重构3D场景\n九、图像分割\n图像分割之（一）概述\n图像分割之（二）Graph Cut（图割）\n图像分割之（三）从Graph Cut到Grab Cut\n图像分割之（四）OpenCV的GrabCut函数使用和源码解读\n图像分割之（五）活动轮廓模型之Snake模型简介\n图像分割之（六）交叉视觉皮质模型（ICM）\n十、系统工程\n基于Qt的P2P局域网聊天及文件传送软件设计\n基于FPGA的红外遥控解码与PC串口通信\n交互系统的构建之（一）重写Makefile编译TLD系统\n交互系统的构建之（二）Linux下鼠标和键盘的模拟控制\n交互系统的构建之（三）TTS语音合成的加盟\n交互系统的构建之（四）手掌与拳头检测加盟TLD\n基于meanshift的手势跟踪与电脑鼠标控制（手势交互系统）\n十一、嵌入式系统\nubuntu12.04安装与配置\nOpenAL跨平台音效API的安装与移植\nYaffs2根文件系统制作\nTTS技术简单介绍和Ekho（余音）TTS的安装与编程\nAndroid学习笔记之（一）开发环境搭建\n十二、编程相关\nPython基础学习笔记之（一）\nPython基础学习笔记之（二）\nMatlab与C++混合编程（依赖OpenCV）\n十三、一些行业调研\nOmniVision的CMOS 图像传感器技术发展路线\nSONY的CMOS 图像传感器技术发展路线\nSAMSUNG的CMOS 图像传感器技术发展路线\nCMOS图像传感器应用实例及其发展趋势分析\n十四、杂乱\nzigzag模式提取矩阵元素"}
{"content2":"一、什么是计算机视觉\n计算机视觉(Computer Vision)是一门如何使计算机“看”的学问，让摄像头和电脑代替人眼对目标进行识别、跟踪、测量等机器视觉。计算机视觉能够\n模拟人类视觉的优越的能力\n，如识别物体、估计立体空间与距离、躲避障碍、理解图像、想象图像的能力，一定程度上，计算机视觉模拟的是人眼和人脑，不仅让计算机“看”，还要让计算机“思”。此外，计算机视觉还能够\n弥补人类视觉的缺陷\n，如不擅长长时间观察同一事物、容易忽略细节。\n二、计算机视觉的主要目标\n解决“像素值”与“语义”之间的差距（Gap）。\n计算机所接收到的图像是一个个像素值矩阵，如何让计算机通过这些数值矩阵认识图片并完成特定的任务是计算机视觉的主要的目标。\n三、计算机视觉的主要任务\n传统的计算机视觉的经典任务主要是三大类：\n分类\n（Classification）、\n检测\n（Detection）、\n分割\n（Segmentation），分类解决的是“是什么”的问题，检测和分割解决的是“在哪里的问题”。\n现在，更多的问题涌现出来，像\n图像描述\n（Image Captioning）、\n图像问答\n（Image Q&A）、\n图像生成\n（Image Generation）、\n图像检索\n（Content-based Image Retrieval）等。上述无论是传统经典任务还是这些新出现的任务都属于语义层面的问题，还有一类问题也属于计算机视觉研究的范畴，属于三维几何领域内的问题，如\n三维建模\n、\n增强现实\n、\n双目视觉\n等。\n四、深度学习在计算机视觉中的应用\n深度学习是引领计算机视觉的技术，各式各样的神经网络方法解决着计算机视觉领域中各式各样的问题。\n图像分类——卷积神经网络（CNN），目标检测——区域卷积神经网络（R-CNN），图像分割——全卷积神经网络（FCN），图像生成——生成对抗网络（GAN），图像问答——循环神经网络（RNN），我们将在后边的学习中深入了解这些深度学习方法。"}
{"content2":"Artificial Intelligence 人工智能学科的基本思想和基本内容。即人工智能是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。人工智能就其本质而言，是对人的思维的信息过程的模拟。\nMachine Learning 机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。是对人的学习行为的模拟。\nComputer Vision计算机视觉：用计算机来模拟人的视觉机理获取和处理信息的能力。是对人视觉的模拟。与下内容相近：\n图像处理： 图像处理技术把输入图像转换成具有所希望特性的另一幅图像。例如，可通过处理使输出图象有较高的信-噪比，或通过增强处理突出图象的细节，以便于操作员的检验。在计算机视觉研究中经常利用图象处理技术进行预处理和特征抽取。\n模式识别：模式识别技术根据从图象抽取的统计特性或结构信息，把图像分成予定的类别。例如，文字识别或指纹识别。在计算机视觉中模式识别技术经常用于对图象中的某些部分，例如分割区域的识别和分类。\n图像理解：给定一幅图像，图象理解程序不仅描述图象本身，而且描述和解释图象所代表的景物，以便对图像代表的内容作出决定。在人工智能视觉研究的初期经常使用景物分析这个术语，以强调二维图象与三维景物之间的区别。图象理解除了需要复杂的图象处理以外还需要具有关于景物成象的物理规律的知识以及与景物内容有关的知识。\n计算机视觉的研究对象主要是映射到单幅或多幅图像上的三维场景，例如三维场景的重建。计算机视觉的研究很大程度上针对图像的内容。\n图象处理与图像分析的研究对象主要是二维图像，实现图像的转化，尤其针对像素级的操作，例如提高图像对比度，边缘提取，去噪声和几何变换如图像旋转。这一特征表明无论是图像处理还是图像分析其研究内容都和图像的具体内容无关。\n机器视觉主要是指工业领域的视觉研究，例如自主机器人的视觉，用于检测和测量的视觉。这表明在这一领域通过软件硬件，图像感知与控制理论往往与图像处理得到紧密结合来实现高效的机器人控制或各种实时操作。\n模式识别使用各种方法从信号中提取信息，主要运用统计学的理论。此领域的一个主要方向便是从图像数据中提取信息。\n还有一个领域被称为成像技术。这一领域最初的研究内容主要是制作图像，但有时也涉及到图像分析和处理。例如，医学成像就包含大量的医学领域的图像分析。\n对于所有这些领域，一个可能的过程是你在计算机视觉的实验室工作，工作中从事着图象处理，最终解决了机器视觉领域的问题，然后把自己的成果发表在了模式识别的会议上。"}
{"content2":"人工智能的浪潮从2012年开始至今，席卷全球，几乎是家喻户晓的词汇。同时，人工智能也是多学科交叉的领域，涵盖了专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等诸多领域。\n目前我们所研究和应用的，是专用人工智能（或称为弱人工智能、感知人工智能）；与专用人工智能相对的，是通用人工智能。专用人工智能阶段的智能体仅在特定领域有效果，比如AlphaGo在围棋领域战胜人类，但在象棋等其它棋类游戏中，用相同的程序就无法实现比较好的效果。而人类这一通用人工智能的智能体，不仅可以识人辨物，还会下象棋、打扑克或者麻将，不限于特定领域。\n机器学习：一种实现人工智能的方法\n当下专用人工智能的突破，归功于机器学习。但是，机器学习仅仅是实现人工智能的一种或者一类方法，并不是全部，这一点希望大家有清晰的认识。\n机器学习从上个世纪50年代发展至今，衍生出了很多热门的研究子领域，以21世纪为例，先后经历了流形学习、稀疏学习、深度学习、深度强化学习、迁移学习等数个热点。甚至直到现在，学术界对于机器学习都没有完全统一的定义。\n机器学习数学基础\n总体来看，机器学习涵盖了微积分、概率论与数理统计、线性代数、矩阵论、信息论、优化理论等数学基础。近期，我们联合南京大学计算机科学与技术系博士生与中科院自动化所博士，共同推出『机器学习数学基础』在线课程，课程大纲分享给大家，包含了机器学习涉及到的主要数学知识点。\nChaper1：引言\n数学之于机器学习的必要性和重要性\nChaper2：函数求导\n1. 背景介绍：以误差逆传播（BP）算法为例\n2. 函数的极限\n3. 偏导数，方向导数，梯度\n4. 复合函数求导的链式法则\n5. 案例分析：BP算法及其应用（以手写数字识别为例）\nChaper3：矩阵论\n1. 背景介绍：以线性回归为例\n2. 矩阵概念与运算\n3. 矩阵范数\n3.1 范数定义\n3.2 Lp范数\n3.3 最小二乘回归误差度量\n4. 矩阵的行列式、逆、秩和迹\n4.1 求解线性方程组：高斯消去法\n4.2 矩阵的逆\n4.3 线性空间及其基\n4.4 方程组求解的行列式表达\n4.5 矩阵伪逆\n5. 矩阵的特征值和特征向量\n5.1矩阵特征值定义\n5.2 矩阵的迹和行列式与特征值的关系\n6. 奇异值分解\n6.1 矩阵奇异值的定义\n6.2 矩阵的奇异值分解\n7. 矩阵求导\n7.1矩阵导数的定义\n7.2 矩阵导数对最小二乘的应用\n8. 矩阵二次型与半正定\n8.1 矩阵二次型\n8.2 矩阵半正定的定义\n8.3 最小二乘的半正定视角\n9. 案例分析：线性回归及其应用（以前列腺癌发病率预测为例）\nChaper4：凸优化\n1. 背景介绍：以SVM为例\n1.1 优化背景介绍\n1.2 SVM应用案例\n2. 优化问题与极值\n2.1 优化问题及其标准型\n2.2 优化问题的极值点\n2.3 拉格朗日函数及KKT条件\n2.4 SVM的最大间隔的数学表达\n3. 凸优化基础\n3.1 凸集与凸问题\n3.2 凸函数与琴生不等式\n3.3 保持凸性的运算\n3.4 SVM的凸性分析\n4. 对偶理论\n4.1 对偶问题\n4.2 强对偶\n4.3 SVM的对偶求解\n5. 案例分析：SVM及其应用（以Iris数据集分类为例）\nChaper5：概率论与数理统计\n1. 背景介绍：以朴素贝叶斯为例\n2. 随机变量，概率分布（离散随机变量，连续随机变量）\n3. 联合概率，边缘概率，条件概率，贝叶斯定理\n4. 期望、方差/标准差、协方差\n5. 不等式（切比雪夫不等式等）\n6. 独立性，条件独立性，相关性\n7. 常用分布：二项分布/Bernoulli分布分布（特例），多项式分布/Multinoulli分布（特例），均匀分布（离散/连续），高斯分布，指数分布\n8. KL散度\n9. 极大似然估计\n10. 案例分析：朴素贝叶斯及其应用（以乳腺癌诊断和信用风险评级为例）\nChaper6：信息论基础\n1. 背景介绍：以决策树为例\n2. 信息论中的基本概念I：离散随机变量（熵、联合熵、条件熵、互信息、相对熵，以及相互之间的关系）\n3. 信息论中的基本概念II：连续随机变量（微分熵、交叉熵、多元高斯分布的熵）\n4. 案例分析：决策树及其应用（以乳腺癌诊断和信用风险评级为例）\n转行AI的建议\n（1）人工智能人才的缺口，更多地在于高端人才，而不是调参工。\n这就要求我们在学习机器学习时，不仅要知其然，更要知其所以然，通俗点讲，对于每个机器学习算法，仅仅会调用现成的函数库是不行的，要了解算法背后的原理，亲自推导一遍，亲自写代码实现这个算法，效果最佳。\n（2）等学习完数学知识后，再学习机器学习的做法，未必可取。\n机器学习涉及的数学知识点很多，在实际学习过程中，如果我们一味地学习数学，很容易枯燥厌烦，进而坚持不下来。\n最好的做法是，将数学与机器学习内容高度融合，学习完几个知识点后，接着学习这些知识点对应的机器学习相关算法，会让我们信心倍增。\n随时欢迎想转行人工智能的伙伴，联系我们。我们会抽出专门的时间一一为大家答疑解惑，根据每个人的情况，给予合理的建议或者劝告。"}
{"content2":"图像预处理\n内容提要:\n图像显示与存储原理\n图像增强的目标\n点运算：基于直方图的对比度增强\n形态学处理\n空间域处理：卷积\n卷积的应用（平滑、边缘检测、锐化等）\n频率域处理：傅里叶变换、小波变换\n应用案例：平滑、边缘检测、CLAHE等\n1.图像的显示与储存原理\n（1）颜色空间\nRGB：越叠加越亮\nCMYK:Cyan(青),Magenta(品红),Yellow,Key\nHSV:Hue(色调，颜色种类)，Saturation(饱和度，色彩的纯度),Value(明度，色彩的明亮度)\n（2）颜色存储原理\nGray = R*0.3 + G*0.59 + B*0.11（典型的比例分配）\n常见的存储的格式有：bmp, jpg, png, tiff, gif, pcx, tga, exif, fpx, svg, psd, cdr, pcd, dxf, ufo, eps, ai, raw, WMF, webp等\nBMP：采用位映射存储格式，不采用其他任何 压缩，所占用的空间很大。\nJPG：最常见的有损压缩格式，能够将图像压缩 到很小的空间，压缩比可达10:1到40:1之间。\nGIF：基于LZW算法的连续色调的无损压缩格式 ，其压缩率一般在50%左右。\nPNG：是比较新的图像文件格式，能够提供长 度比GIF小30%的无损压缩图像文件\n2.图像增强的目标\n改善图像的视觉效果；\n转换为更适合于人或机器分析处理的形式；\n突出对人或机器分析有意义的信息；\n抑制无用信息，提高图像的使用价值\n常见操作有：平滑、锐化、去噪、对比度增强（灰度调整）\n图像处理方法：\n3.点运算：直方图均衡化（HE）\n直方图均衡化： 实质上是对图像 进行非线性拉伸\n重新分配各个灰 度单位中的像素 点数量，使一定 灰度范围像素点 数量的值大致相 等。\n直方图均衡的经典算法对整幅图像的像素使用相同的变换，如果图像中包括明显亮的或 者暗的区域，则经典算法作用有限。\n自适应直方图均衡化 （AHE）\n移动模板在原始图片上按特定步长滑动；\n每次移动后，模板区域内做直方图均衡，映射 后的结果赋值给模板区域内所有点\n每个点会有多次赋值，最终的取值为这些赋值 的均值。\n限制对比度自适应直方图均衡(CLAHE)\nAHE会过度放大图像中相对均匀区域的噪音,与普通的自适应直方图均衡相比，CLAHE的不 同地方在于直方图修剪过程，用修剪后的直方图 均衡图像时，图像对比度会更自然。\n小黑点的灰度直接由映射函数计算得到；\n粉色区域内点的灰度由映射函数计算而得；\n绿色区域内点的灰度由相邻2块灰度映射值线性 插值而得；\n其他区域所有点的灰度由相邻4块的灰度映射值双 线性插值而得\nCLAHE算法步骤\n1.图像分块，以块为单位；\n2.先计算直方图，然后修剪直方图，最后均衡；\n3.遍历操作各个图像块，进行块间双线性插值；\n4.与原图做图层滤色混合操作。（可选）\n4.形态学处理\n膨胀是图像中的高亮部分进行膨胀，类似于 领域扩张。\n腐蚀是原图的高亮部分被腐蚀，类似于领域 被蚕食。\n开运算：先腐蚀再膨胀，可以去掉目标外的孤立点\n闭运算：先膨胀再腐蚀，可以去掉目标内的孔\n通常，当有噪声的图像用阈值二值化后，所 得到的边界是很不平滑的，物体区域具有一 些错判的孔洞，背景区域散布着一些小的噪 声物体，连续的开和闭运算可以显著的改善 这种情况。\n5.空间域处理及其变换\n首先明确几个相同表达意思的黑话\n卷积=滤波\n卷积核 = 卷积模板 = 扫描窗 = 滤波核 = 滤波模板\n• 参数解释\n• x, y是像素在图片中的位置/坐标\n• k, l是卷积核中的位置/坐标 ,中心点的坐标是（0,0）\n• f[k, l]是卷积核中在（k, l）上的权重参数\n• I[x+k, y+l]是与f[k, l]相对应的图片像素值\n• h[x, y]是图片中(x, y)像素的滤波/卷积结果\n不同功能需要定义不同函数：\n平滑/去噪\n梯度/锐化\n边缘、显著点、纹理\n模式检测\n边界填充策略\n补零（zero-padding）\n边界复制（replication）\n镜像（reflection）\n块复制（wraparound）\n平滑均值滤波/卷积\n特征：3*3，扫描步长：1，边框补0\n均值滤波本身存在缺陷，既没有 很好地去除噪声点，也破坏了图 像的细节反而使图像变得模糊\n特点： 奇数尺寸 • 3x3，5x5，7x7，2n-1 x 2n-1\n• 参数和为：1\nimport cv2 as cv import numpy as np # 均值模糊 def blur_demo(img): dst = cv.blur(img, (5, 5)) cv.imshow(\"blur image\", dst)\n平滑中值滤波/卷积\n• 奇数尺寸\n• 3x3，5x5，7x7，2n-1 x 2n-1\n• 操作原理\n• 卷积域内的像素值从小到大排序\n• 取中间值作为卷积输出\n• 有效去除椒盐噪声\n将领域矩阵中的N个像素 进行排序，并将这个矩阵 的中心点赋值为这N个像 素的中值。\nimport cv2 as cv import numpy as np # 中值模糊 def median_blur_demo(img): dst = cv.medianBlur(img, 5) cv.imshow(\"median_blur_demo\", dst)\n平滑高斯滤波/卷积\n• 奇数尺寸\n• 3x3，5x5，7x7，2n-1 x 2n-1\n• 模拟人眼，关注中心区域\n• 有效去除高斯噪声\n• 参数\n• x, y是卷积参数坐标\n• 标准差\n越小，关注区域越集中\n分解特性（级联高斯）\n2D卷积拆分成两个相同的1D卷积\n• 列卷积\n• 行卷积\n降计算\n• 2D卷积：KxK次计算\n• 2x1D卷积：2K次计算"}
{"content2":"Artificial Intelligence 人工智能学科的基本思想和基本内容。即人工智能是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。人工智能就其本质而言，是对人的思维的信息过程的模拟。\nMachine Learning 机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。是对人的学习行为的模拟。\nComputer Vision计算机视觉：用计算机来模拟人的视觉机理获取和处理信息的能力。是对人视觉的模拟。与下内容相近：\n图像处理： 图像处理技术把输入图像转换成具有所希望特性的另一幅图像。例如，可通过处理使输出图象有较高的信-噪比，或通过增强处理突出图象的细节，以便于操作员的检验。在计算机视觉研究中经常利用图象处理技术进行预处理和特征抽取。\n模式识别：模式识别技术根据从图象抽取的统计特性或结构信息，把图像分成予定的类别。例如，文字识别或指纹识别。在计算机视觉中模式识别技术经常用于对图象中的某些部分，例如分割区域的识别和分类。\n图像理解：给定一幅图像，图象理解程序不仅描述图象本身，而且描述和解释图象所代表的景物，以便对图像代表的内容作出决定。在人工智能视觉研究的初期经常使用景物分析这个术语，以强调二维图象与三维景物之间的区别。图象理解除了需要复杂的图象处理以外还需要具有关于景物成象的物理规律的知识以及与景物内容有关的知识。\n计算机视觉的研究对象主要是映射到单幅或多幅图像上的三维场景，例如三维场景的重建。计算机视觉的研究很大程度上针对图像的内容。\n图象处理与图像分析的研究对象主要是二维图像，实现图像的转化，尤其针对像素级的操作，例如提高图像对比度，边缘提取，去噪声和几何变换如图像旋转。这一特征表明无论是图像处理还是图像分析其研究内容都和图像的具体内容无关。\n机器视觉主要是指工业领域的视觉研究，例如自主机器人的视觉，用于检测和测量的视觉。这表明在这一领域通过软件硬件，图像感知与控制理论往往与图像处理得到紧密结合来实现高效的机器人控制或各种实时操作。\n模式识别使用各种方法从信号中提取信息，主要运用统计学的理论。此领域的一个主要方向便是从图像数据中提取信息。\n还有一个领域被称为成像技术。这一领域最初的研究内容主要是制作图像，但有时也涉及到图像分析和处理。例如，医学成像就包含大量的医学领域的图像分析。\n对于所有这些领域，一个可能的过程是你在计算机视觉的实验室工作，工作中从事着图象处理，最终解决了机器视觉领域的问题，然后把自己的成果发表在了模式识别的会议上。"}
{"content2":"AI\n菌\n最近AI菌决定把自己的机器学习之路向计算机视觉方面发展。所以今天就来给大家分享一下AI菌收集到的资料以及心得\nThe M Tank 编辑了一份报告《A Year in Computer Vision》，记录了 2016 至 2017 年计算机视觉领域的研究成果，对开发者和研究人员来说是不可多得的一份详细材料。该材料共包括四大部分\n简介\n第一部分\n分类/定位\n目标检测\n目标追踪\n第二部分\n分割\n超分辨率、风格迁移、着色\n动作识别\n第三部分\n3D 目标\n人体姿势估计\n3D 重建\n其他未分类 3D\n总结\n第四部分\n卷积架构\n数据集\n不可分类的其他材料与有趣趋势\n结论\n有兴趣的同学可以读一读，完整 PDF 地址：http://www.themtank.org/pdfs/AYearofComputerVisionPDF.pdf\n下面是收集到的学习资料与心得的汇总：\n（文中没有发的资源将在之后陆续放出）\n01 掌握好相应的基础能力\n计算机视觉的理念其实与很多概念有部分重叠，包括：人工智能、数字图像处理、机器学习、深度学习、模式识别、概率图模型、科学计算以及一系列的数学计算等。\n所以在入门CV之前，同学们最好对基础的学术课程都有对应的了解，比如数学方面的微积分，概率学，统计学，线性代数这几门基础课程。\n在编程语言方面，Matlab，Python，C++，最好熟悉其中2种，因为计算机视觉离开计算机编程是完全行不通的\n\n02 需要的专业工具\n工欲善其事，必先利其器。对于想要学好计算机视觉的同学来说，一个专业的工具，绝对是助攻的不二神器。\nOpenCV（开源计算机视觉库）是一个非常强大的学习资料库，包括了计算机视觉，模式识别，图像处理等许多基本算法。\n它免费提供给学术和商业用途，有C++，C，Python和java接口，支持Windows、Linux、Mac OS、iOS和Android。\n而关于OpenCV的学习，AI菌推荐（其中第三本目前无中文版）：\n学习OpenCV(Learning.OpenCV)\n链接：\nhttps://pan.baidu.com/s/1c2GrPEK 密码：7012\n毛星云老师编著的OpenCV3编程入门\n链接：\nhttps://pan.baidu.com/s/1c2xuVFq 密码：2s4a\n学习OpenCV3（\nLearning OpenCV 3\n）\n链接：\nhttps://pan.baidu.com/s/1geQeT0J 密码：cuco\n而深度学习方面，有TensorFlow，PyTorch，Caffe等深度学习框架，它们也内置了OpenCV的API接口。而哪种框架好，就要看你自己的需要了\n推荐资料：\n莫凡教程系列之PyTorch :https://morvanzhou.github.io/tutorials/machine-learning/torch/\nTensorFlow中文社区：\nhttp://www.tensorfly.cn/\n深度学习 21天实战Caffe\n\n03 绕不开的数字图像处理与模式识别\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。\n入门的同学推荐\n冈萨雷斯的《数字图像处理》《数字图像处理(第3版)(英文版)》和对应的Matlab版本\n一本讲基础的理论，一本讲怎么用Matlab实现。\n除此之外同学们还可以去YouTube上找到相关的课程信息，相信大家会有所收获的。\n模式识别（Pattern Recognition），就是通过计算机用数学技术方法来研究模式的自动处理和判读。我们把环境与客体统称为“模式”。\n计算机视觉很多东西都是基于图像识别的，图像识别就是模式识别的一种。\n模式识别通常是训练一个模型来拟合当前的数据，当我们拿到一堆数据或图片，需要从当中找到它们的关系，最便捷的便是用模式识别算法来训练一个模型。\nAI菌推荐一本模式识别入门级的教材《模式分类》，相对于《模式识别》这本书来说可能比较难，但书中介绍了很多模式识别经典的分类器，还是很值得一读。\n其中的一些思想在神经网络中也可以应用的\n\n04 系统的学习下计算机视觉课程\n对于CV新手来说，想要从小白到大神，最快的方法就是先系统的学习一下计算机视觉的课程，全面了解一下计算机视觉这个领域的背景及其发展、这个领域有哪些基本的问题、哪些问题的研究已经比较成熟了，哪些问题的研究还处于基础阶段。\n在这里AI菌推荐3本经典教材：\n《计算机视觉：一种现代方法》（Computer Vision: A Modern Approach）\n《计算机视觉_算法与应用》\n（Computer Vision: Algorithms and Applications）\n《计算机视觉：模型 学习和推理》\n（Computer Vision: Models, Learning, and Inference）\n这三本教材AI菌认为是计算机视觉最好的入门教材了，内容丰富，难度适中，其中第二本书涉及大量的文献，很适合对计算机视觉没什么概念的同学。\n虽然其中的一些方法在现在看来已经过时了，但还是值得一读\n05 深度学习与CNN\n关于深度学习这几年讲的已经太多了，资料也非常多，AI菌在这里就不在赘述啦\n计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。\n什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。\n同学们可以试着学习下CNN在计算机视觉当中的应用\n推荐的资料：\n斯坦福CS231n—深度学习与计算机视觉网易云课堂课程：http://study.163.com/course/introduction.htm?courseId=1003223001\n斯坦福CS231n—深度学习与计算机视觉官方课程：http://cs231n.stanford.edu/\nCS231n官方笔记授权翻译总集篇：https://www.52ml.net/17723.html\n吴恩达 deeplearning.ai与网易云课堂的微专业深度学习工程师卷积神经网络\nhttp://mooc.study.163.com/course/2001281004?tid=2001392030#/info\n神经网络方面的经典教材\n《深度学习》\n（Deep Learning）\n《神经⽹络与深度学习》\n（Neural Networks and Deep Learning(Nielsen,2017)）\n\n06 了解最新领域动态\n很多同学做研究的时候，容易陷入自我封闭的“怪圈”，过于执着于埋头学习相关知识，有时候会忘记及时了解相关领域的最新动态，这是非常不科学的。\n同学们在学习计算机视觉相关知识的时候，可以通过最新的paper来了解这个领域最新提出的一些概念以及发展的情况。\n计算机视觉的期刊有两个PAMI（模式分析与机器智能汇刊）和IJCV（计算机视觉国际期刊）\n顶级的学术会议有 CVPR、ICCV、 ECCV、 BMVC这四个，同学们可以跟着浏览这些期刊论文以及会议文章，相信一定可以学到不少有用的知识。\n\nAI\n菌\n听做视觉的师兄师姐硕：做好计算机视觉研究并不是一件容易的事情，在大多数情况下它甚至是一件很枯燥的事情。\n研究成果毫无进展，研究方向不在明朗等等，这一切都会给你前所未有的压力\n所以希望同学们在决定入这一行的时候，是出于自己的热爱，而不是出于当前的趋势。\n因为热爱不会变，但趋势每一年都在变。\n不失初心，不忘初衷\nAI玩转智能"}
{"content2":"|懒人阅读：计算机视觉的应用无处不在，就像视觉是我们感知世界的最主要方式之一，所以其应用场景和公司也数不胜数。机器学习、深度学习等技术使用到CV之中后，为很多复杂视觉信号的处理带来了可能，从而可以进行更加精准的目标识别、目标跟踪、场景重建等应用。\n\n|如果想要机器能够进行思考，我们需要先教会它们去看。\n李飞飞——Director of Stanford AI Lab and Stanford Vision Lab\n\n|CV定义（参考）和机器学习技术的使用\n机器学习、深度学习等技术使用到CV之中后，为很多复杂视觉信号的处理带来了可能，例如传统的采集、预处理、特征提取、目标识别等过程可以通过一个CNN或GAN网络结构实现，从而可以进行更加精准的目标识别、目标跟踪、场景重建等应用，也有很多功能、产品和公司出现。\n计算机视觉（CV，computer vision）狭义上说是使用计算机及相关设备对生物视觉的一种模拟，可简单理解为替代“眼睛”。它的主要任务就是通过对采集的图片或视频进行处理以获得相应场景的三维信息，就像人类和许多其他类生物每天所做的那样。\n广义上说是通过二维三维图像、视频等图像信息的感知，进而开展决策的一种科学。其中包括计算机科学和工程、信号处理、物理学、应用数学和统计学，神经生理学和认知科学等。\n可能的理解误区\n不意味着计算机必须按人类视觉的方法完成视觉信息的处理。\n个人认为不必严格界定机器视觉、计算机视觉和模式识别等方向的区别，重点在于对实际问题的解决，下图是维基百科上的一种划分方式，供参考。\n\n|主要技术架构\n感知：采集视觉信号，感知器可以是各种光敏摄像机，包括遥感设备，X射线断层摄影仪，雷达，超声波接收器等。\n处理：现在的机器学习很多算法已经可以将整个处理过程在一个网络结构中完成。主要涉及环节有去噪、取样等，减少目标干扰。提取目标特征。检测分割：分割一或多幅图片中含有特定目标的部分。\n应用：\n识别评估：人脸识别、姿态识别、字符识别等\n目标跟踪：运动信号监测、图像跟踪等；\n场景重建：给定一个场景的二或多幅图像或者一段录像，场景重建寻求为该场景建立一个计算机模型/三维模型等。\n|公司、产品及应用场景（部分）\n谷歌，微软，Facebook、亚马逊、苹果、英特尔、华为、BAT等无一没有建立自己的AI实验室，AI里面，计算机视觉或图像处理是非常重要的一块。\n世界各大汽车公司，如特斯拉、通用、宝马等，在推动无人驾驶技术之中，视觉导航是核心关键技术之一。\n同样道理，无人机公司（大疆）、机器人公司（地平线）、摄像头公司（海康威视、旷视科技、商汤科技）、图像处理软件公司（Adobe、美图），甚至是迪士尼等电影动画制作公司都在CV方面大量投入资源开展应用。\n可以看出计算机视觉的应用无处不在，很好理解，图像无处不在是我们感知世界的最主要方式之一，所以其应用场景和公司数不胜数，挂一漏万，盘点如下：\n无人驾驶的视觉导航：还没有条件实现象人那样能识别和理解任何环境，完成自主导航的系统，如避障、路径规划等。相关公司及产品有：\n工业机器人，也被称为机器视觉，指的是自主机器人的视觉，用于检测和测量的视觉。相关公司及产品有：速感科技（让机器人认识世界，用机器人改变世界），是一家以机器视觉为核心的人工智能创业公司，目标是成为机器人行业领先的视觉解决方案提供商，产品线包括：三维视觉传感器、机器人移动开发底盘、AGV导航定位模块、智能跟随机器人。\n视频监控（安防、金融安全）：小区门禁、身份识别（金融、安防）、社会场所安全监视及目标识别跟踪，包括摄像头跟踪（运动匹配）、监视、人脸识别等。相关公司及产品有：海康威视；旷视科技，Face++专注于人脸识别技术和相关产品应用研究，面向开发者提供服务。拥有一套非常强大的人脸检测系统；商汤科技，早期专注于安防领域，现在扩展到互联网＋。格灵深瞳深耕安防和商业数据分析领域，自主研发的深瞳技术在人和车的检测、跟踪与识别方面居于世界领先水平，产品线包括人眼摄像机、行为分析仪、车辆特征识别系统、视图大数据分析平台。\n\n医疗计算机视觉和医学图像处理：从显微镜图像，X射线图像，血管造影图像，超声图像和断层图像等之中检测肿瘤，动脉粥样硬化或其他恶性变化。还可以作为医学测量的新手段，如脑结构，评估医学治疗质量、超声图像、X射线图像，降低噪声的影响的图像等。\n工业制造中的质量控制与测量：食物的光学分拣、缺陷自动检测、机器人臂的位置和细节取向测量。\n军事应用：有关作战的场景的丰富感知，如探测敌方士兵或车辆和导弹制导、雷达图像分析等。\n这里推荐一篇文章，计算机视觉与深度学习公司，对于公司盘点的比较全面，传送门：http://blog.csdn.net/hduxiejun/article/details/53725836\n|技术沿革、当前发展、热点方向\n技术沿革：计算机视觉领域的突出特点是其多样性与不完善性，直到20世纪70年代后期，当计算机的性能提高到足以处理诸如图像这样的大规模数据时，计算机视觉才得到了正式的关注和发展。涉及的主要技术领域：物理（电磁波：主要是可见光与红外线部分）、生物视觉系统（视觉的生物机制）、信号处理（尤其是时变信号处理）和数学（统计学，最优化理论以及几何学）。\n当前发展：计算机视觉的经典问题进入了传统方法的瓶颈期，在不使用神经网络等算法时可能存在很多困难，如果把Deep Learning进入CV的2012年作为新时代的开始，很多神经网络对于各种视觉场景识别问题的解决都实现了很好的效果。\n热点方向：当前发展毋庸置疑是AI技术，尤其是机器学习、深度学习在CV中的应用，赋予了这个方向新的生命。引用知乎答主周博磊的话：计算机视觉在人工智能和深度学习的大背景下方兴未艾。现在的CV和AI研究其实是变得越来越扁平快，论文数量和研究方向也是繁多. 已经很难follow。目前在技术上有一些可能的热点：\n机器人视觉\n基于GAN的生成视觉模型方向\n多媒体计算机视觉，也叫多模态视觉\n会议及期刊\n顶级会议\nICCV：International Conference on Computer Vision，国际计算机视觉大会\nCVPR：International Conference on Computer Vision and Pattern Recognition，国际计算机视觉与模式识别大会\nECCV：European Conference on Computer Vision，欧洲计算机视觉大会\n较好会议\nICIP：International Conference on Image Processing，国际图像处理大会\nBMVC：British Machine Vision Conference，英国机器视觉大会\nICPR：International Conference on Pattern Recognition，国际模式识别大会\nACCV：Asian Conference on Computer Vision，亚洲计算机视觉大会\n顶级期刊\nPAMI：IEEE Transactions on Pattern Analysis and Machine Intelligence，IEEE 模式分析与机器智能杂志\nIJCV：International Journal on Computer Vision，国际计算机视觉杂志\n较好期刊\nTIP:IEEE Transactions on Image Processing，IEEE图像处理杂志\nCVIU：Computer Vision and Image Understanding，计算机视觉与图像理解\nPR：Pattern Recognition，模式识别\nPRL：Pattern Recognition Letters，模式识别快报\n|授人以渔，资料分享\n卷积神经网络在计算机视觉中的应用（ Computer Vision: the use of CovNets），在此推荐斯坦福的CS231n课程：针对视觉识别的卷积神经网络。\nMIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测\n初探计算机视觉的三个源头、兼谈人工智能｜正本清源：http://mp.weixin.qq.com/s?__biz=MzI3MTM5ODA0Nw==&mid=100000002&idx=2&sn=32face7f1acb17e07f3c38dde41d880e；计算机视觉领军人物之一加州大学洛杉矶分校UCLA统计学和计算机科学教授Song-Chun Zhu的访谈录\n杜克大学的Guillermo Sapiro所教授的课程——《图像和视频处理：从火星到好莱坞Image and Video Processing: From Mars to Hollywood with a Stop at the Hospital》，可以在coursera和YouTube上找到相关的课程视频信息。\nGonzalez与Woods编写的《数字图像处理（Digital Image Processing）》一书，使用MATLAB来运行其中所提到的范例。\n佛罗里达大学的Mubarak Shah教授在计算机视觉方面的课程可以作为一门很好的入门课程\n黎中央理工学院的Nikos Paragios和Pawan Kumar讲授了一门人工视觉中的离散推理（Discrete Inference in Artificial Vision）课程，它能提供相关的概率图形模型和计算机视觉相关的大量数学知识。\n《使用Python对计算机视觉进行编程/Programming Computer Vision with Python》\n|参考资料\n百度百科，计算机视觉：https://baike.baidu.com/item/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2803351?fr=aladdin\n雷锋网，七步带你认识计算机视觉：https://www.leiphone.com/news/201608/UaRVIbntJCdv4G9K.html\n计算机视觉领域的一些牛人博客，超有实力的研究机构等的网站链接：http://blog.csdn.net/carson2005/article/details/6601109\n知乎神帖，中国计算机视觉的前途在哪？机器视觉工程师又何去何从？：https://www.zhihu.com/question/20451261\n知乎神帖：机器视觉与计算机视觉的区别？：https://www.zhihu.com/question/23183532\n善用智能之道：行业动态、技术前沿、产业服务，欢迎关注联系：九三智能控\nPS：可申请进入微信群交流，不定期分享资料，拓展行业人脉。添加微信：yan_kylin，注明名字+研究领域/专业/学校/公司，或在公众号留言。"}
{"content2":"基于AI专用的APiM架构，无需外部缓存的模块化深度神经网络学习加速器，用于高性能边缘计算领域，可作为基于视觉的深度学习运算和AI算法加速。外形小巧，极低功耗，拥有着强劲算力，配套完整易用的模型训练工具、网络训练模型实例，搭配专业硬件平台，可快速应用于人工智能行业中。\n5.6Tops强劲算力\nNCC S1基于AI嵌入式神经网络处理器（NPU），拥有28000个并行神经计算核，支持芯片上并行与原位计算，峰值运算能力高达5.6Tops，是市面上其他方案的数十倍。其强劲的算力，能进行复杂的高密度计算，适用于高性能边缘计算领域。\nAI处理架构APiM\n采用AI专用的MPE矩阵引擎和APiM（AI processing in Memory，存储中的AI处理）架构，以革命性的方式处理AI，一次升级网络预加载，无需指令、总线，无需外部DDR缓存，大量数据可直接输入/输出硅片，从而大大提高了AI的处理速度，降低处理能耗。\n9.3 Tops/W超高效能\nNCC S1神经网络计算卡的核心采用28nm工艺制程，在2.8 Tops算力时功率仅300mW，效率能耗比高达为9.3 Tops/W，在拥有超强的算力同时保持了极低的能耗，让其应用在终端设备的边缘计算领域中极具优势。\n高性能硬件平台\nNCC S1神经网络计算卡可搭配ROC-RK3399-PC开源主板，配置高性能RK3399六核处理器，拥有丰富的硬件接口，可快速集成边缘计算的硬件平台，搭建产品原型，加速AI产品的项目进程。\n配套模型训练工具\n提供基于PyTorch完整易用的模型训练工具PLAI（People Learn AI）， 可在Windows 10与Ubuntu 16.04系统上开发，更简单快捷地添加自定义网络模型，大大降低了使用AI的技术门槛，让更多人能更容易打开AI的大门。\n提供网络训练模型\n支持GNet1，GNet18和GNetfc三种网络训练模型实例，后续会持续增加网络实例，轻松在设备上测试大量深度学习应用。\n进入Firefly商城可以了解更多内容\n———————————————\n更多信息请关注Firefly公众微信号fireflytee:\n———————————————\nFirefly官网：http://www.t-firefly.com\nFirefly开源社区：\nhttp://developer.t-firefly.com"}
{"content2":"#CNN与计算机视觉\nSENet\nDenseNet\nResNet\nopencv 深度学习图像处理库 http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/tutorials.html\nPIL python图像处理库"}
{"content2":"人工智能被认为是第四次工业革命，google，facebook等全球顶尖、最有影响力的技术公司都将目光转向AI，虽然免不了存在泡沫，被部分媒体夸大宣传，神经网络对图像识别，语音识别，自然语言处理，无人车等方面的贡献是毋庸置疑的，随着算法的不断完善，部分垂直领域的研究已经落地应用。\n在计算机视觉领域，目前神经网络的应用主要有图像识别，目标定位与检测，语义分割。图像识别告诉你图像是什么，目标定位与检测告诉你图像中目标在哪里，语义分割则是从像素级别回答上面两个问题。因为项目需要对遥感影像中的小麦和玉米进行语义分割，这几天在做相关方向的研究，下面给大家简单介绍下语义分割的相关知识。\n语义分割是什么\n图像语义分割（semantic segmentation），从字面意思上理解就是让计算机根据图像的语义来进行分割，例如计算机在输入图１(原图)的情况下，能够输出图２（标签）。语义在语音识别中指的是语音的意思，在图像领域，语义指的是图像的内容，对图片意思的理解，比如的语义就是三个人骑着三辆自行车；分割的意思是从像素的角度分割出图片中的不同对象，对原图中的每个像素都进行标注，比如中粉红色代表人，绿色代表自行车。\n语义分割当前应用\n目前，语义分割的应用领域主要有地理信息系统，无人车驾驶，医疗影像分析，机器人等领域。\n在地理信息系统中，(查查有没有常用的数据集)，通过训练神经网络让机器自动识别道路，河流，庄稼，建筑物等。下图左边为卫星遥感图像，中间为真实的标签，右边为神经网络预测的标签结果。使用ResNet FCN网络进行训练，随着训练加深，准确率不断提升.(参考:https://www.azavea.com/blog/2017/05/30/deep-learning-on-aerial-imagery/)\n语义分割也是无人车驾驶的核心算法技术，车载摄像头，或者激光雷达探查到图像后输入到神经网络中，后台计算机可以自动将图像分割归类，以避让行人和车辆等障碍。(下图为自动驾驶数据集cityscapes图片)\n随着人工智能的崛起，将神经网络与医疗诊断结合结合也成为研究热点，智能医疗研究逐渐成熟。在智能医疗领域，语义分割主要应用有肿瘤图像分割，龋齿诊断等。腾讯旗下首款人工智能医疗产品“腾讯觅影”目前已经落地河北省。(图片来自qure.ai)\n常用数据集\n在“数据，算法，计算力”这AI发展的三大驱动力中，眼下最重要的就是数据，数据集在人工智能中有着举足轻重的地位，在实际项目中,遇到的困难往往是没有合适的数据集来训练神经网络。具体根据不同的应用领域，目前的数据集主要有：\nPascal VOC系列:　http://host.robots.ox.ac.uk/pascal/VOC/voc2012/　通常采用PASCAL VOC 2012，最开始有1464 张具有标注信息的训练图片，2014 年增加到10582张训练图片。主要涉及了日常生活中常见的物体，包括汽车，狗，船等20个分类。\nMicrosoft COCO:　http://link.zhihu.com/?target=http%3A//mscoco.org/explore/　一共有80个类别。这个数据集主要用于实例级别的分割（Instance-level Segmentation）以及图片描述Image Caption）。\nCityscapes: https://www.cityscapes-dataset.com/ ,奔驰主推, 适用于汽车自动驾驶的训练数据集，包括19种都市街道场景：road、side-walk、building、wal、fence、pole、traficlight、trafic　sign、vegetation、terain、sky、person、rider、car、truck、bus、train、motorcycle 和 bicycle。该数据库中用于训练和校验的精细标注的图片数量为3475，同时也包含了 2 万张粗糙的标记图片。\n语义分割中的技术\n(参考:https://zhuanlan.zhihu.com/p/22308032)\n全卷积神经网络 FCN（2015）\n论文：Fully Convolutional Networks for Semantic Segmentation\nFCN 提出所追求的是，输入是一张图片是，输出也是一张图片，学习像素到像素的映射，端到端的映射，网络结构如下图所示：\n全卷积神经网络主要使用了三种技术：\n卷积化（Convolutional）\n上采样（Upsample）\n跳跃结构（Skip Layer）\n卷积化（Convolutional）\n卷积化即是将普通的分类网络，比如VGG16，ResNet50/101等网络丢弃全连接层，换上对应的卷积层即可。\n上采样（Upsample）\n(这个地方还要继续再研究下, 下面这个图可能不对)\n有的说叫conv_transpose更为合适。因为普通的池化会缩小图片的尺寸，比如VGG16 五次池化后图片被缩小了32倍。为了得到和原图等大的分割图，我们需要上采样/反卷积。反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。图解如下：\n跳跃结构（Skip Layer）\n这个结构的作用就在于优化结果，因为如果将全卷积之后的结果直接上采样得到的结果是很粗糙的，所以作者将不同池化层的结果进行上采样之后来优化输出。具体结构如下：\n而不同上采样结构得到的结果对比如下：\n这是第一种结构，也是深度学习应用于图像语义分割的开山之作，获得了CVPR2015的最佳论文。但还是无法避免有很多问题，比如，精度问题，对细节不敏感，以及像素与像素之间的关系，忽略空间的一致性等，后面的研究极大的改善了这些问题。\nSegNet（2015）\n论文：A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\n主要贡献：将最大池化指数转移至解码器中，改善了分割分辨率。\n空洞卷积（2015）\n论文：Multi-Scale Context Aggregation by Dilated Convolutions\n主要贡献：使用了空洞卷积，这是一种可用于密集预测的卷积层；提出在多尺度聚集条件下使用空洞卷积的“背景模块”。\nDeepLab（2016）\n论文：DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\n主要贡献：使用了空洞卷积；提出了在空间维度上实现金字塔型的空洞池化atrous spatial pyramid pooling(ASPP)；使用了全连接条件随机场。\n参考：https://www.azavea.com/blog/2017/05/30/deep-learning-on-aerial-imagery/\n参考：https://zhuanlan.zhihu.com/p/22308032"}
{"content2":"计算机视觉会议\nA类\nICCV: International Conference on Computer Vision\nCVPR: International Conference on Computer Vision and Pattern Recognition\nB类\nECCV: European Conference on Computer Vision\nC类\nACCV: Asian Conference on Computer Vision\nICPR: International Conference on Pattern Recognition\nBMVC: British Machine Vision Conference\n计算机视觉刊物\nA类\nTPAMI: IEEE Trans on Pattern Analysis and Machine Intelligence\nIJCV: International Journal of Computer Vision\nB类\nCVIU: Computer Vision and Image Understanding\nPattern Recognition\nC类\nIET-CVI: IET Computer Vision\nIVC: Image and Vision Computing\nIJPRAI: International Journal of Pattern Recognition and Artificial Intelligence\nMachine Vision and Applications\nPRL: Pattern Recognition Letters"}
{"content2":"什么是计算机视觉？\n我个人的观点：\n用一句话概括，就是让计算机拥有人能所见、人能所识、人能所思的能力，就可以称计算机拥有视觉，即计算机视觉。\n人能所见，是指能看得见。对于计算机而言，是指能够获取图像。最常见的是通过摄像头来获取图像，所以摄像头这样的获取图像的设备，被称作计算机的眼睛。人能所识，是指人能够对看到的景象进行辨识，即回答，看到的是什么。对于计算机，这就是物体检测。人能所思，是指能够理解看到的景象有什么关联。举个例子，你看到一群人，你可以知道这群人正在干什么，或者将要干什么，又或者是刚干完什么，哪怕你看到的只是一张静止的图片。对计算机来说，就是让计算机理解图像之间的联系，或者是图像里不同物体间的联系。\n所见，所识，所思，缺一不可，少一样都不能称之为完整的计算机视觉。必须三个能力同时达到，才能称为真正的计算机视觉。\n百度百科的介绍为：\n计算机视觉是使用计算机及相关设备对生物视觉的一种模拟。它的主要任务就是通过对采集的图片或视频进行处理以获得相应场景的三维信息，就像人类和许多其他类生物每天所做的那样。\n计算机视觉是一门关于如何运用照相机和计算机来获取我们所需的，被拍摄对象的数据与信息的学问。形象地说，就是给计算机安装上眼睛（照相机）和大脑（算法），让计算机能够感知环境。我们中国人的成语\"眼见为实\"和西方人常说的\"One picture is worth ten thousand words\"表达了视觉对人类的重要性。不难想象，具有视觉的机器的应用前景能有多么地宽广。\n计算机视觉既是工程领域，也是科学领域中的一个富有挑战性重要研究领域。计算机视觉是一门综合性的学科，它已经吸引了来自各个学科的研究者参加到对它的研究之中。其中包括计算机科学和工程、信号处理、物理学、应用数学和统计学，神经生理学和认知科学等。"}
{"content2":"收藏：全网最大机器学习数据集，视觉、NLP、音频都在这了\n280万分割掩码，谷歌Open Images数据集再更新\n从图像中检测和识别表格，北航&微软提出新型数据集TableBank\n人类穿着数据集3DPeople发布，微软建立人工智能商学院 | AI一周学术\n数据集查找神器！100个大型机器学习数据集都汇总在这了 | 资源\n【收藏】8款大型机器学习数据集顶级资源\n资源 | 这是一份非常全面的开源数据集，你，真的不想要吗\n计算机视觉\n自然语言处理\n语音\nGoogle Audioset：扩展了 632 个音频分类样本，并从 YouTube 视频中提取了 2，084，320 个人类标记的 10 秒声音片段。\n无人驾驶\nGoogle-Landmarks-v2：谷歌开源的最大地标数据集包含500万张图片和200000个地标。\nUber 2B trip data：首次展示 2 百万公里的出行数据。\n本田公布104小时驾驶行为数据集：本田最近与波士顿大学合作，公布了在旧金山湾区采集的104小时**驾驶行为数据集，总体积大约150GB。收集了包括GPS、图像、激光雷达、汽车导航、司机驾驶行为等方面的信息。\nDBNet数据集：厦门大学 SCSC 实验室李军教授团队与上海交大 MVIG 实验室卢策吾教授团队联合发布大规模驾驶行为数据集。DBNet 是专为研究驾驶行为的策略学习而设置的。DBNet 数据集记录了视频、激光雷达点云，以及对应的资深驾驶员（驾龄超过 10 年）的真实驾驶行为。\nKITTI：由德国卡尔斯鲁厄理工学院和丰田美国技术研究院联合创办，是目前国际上最大的自动驾驶场景下的计算机视觉算法评测数据集。该数据集用于评测立体图像(stereo)，光流(optical flow)，视觉测距(visual odometry)，3D物体检测(object detection)和3D跟踪(tracking)等计算机视觉技术在车载环境下的性能。KITTI包含市区、乡村和高速公路等场景采集的真实图像数据，每张图像中最多达15辆车和30个行人，还有各种程度的遮挡与截断。整个数据集由389对立体图像和光流图，39.2 km视觉测距序列以及超过200k 3D标注物体的图像组成，以10Hz的频率采样及同步。\ncomma2k19：comma.ai 发布了 comma2k19, 这是加利福尼亚280高速公路上超过33小时通勤的数据集。 这意味着在加利福尼亚州圣何塞和旧金山之间20公里的高速公路上行驶了2019段，每段1分钟。 comma2k19是一个完全可重现且可扩展的数据集。 数据采用comma EONs收集，其传感器类似于任何现代智能手机，包括道路相机，手机GPS，温度计和9轴IMU。 此外，EON还使用comma grey panda捕获原始GNSS测量值和汽车发送的所有CAN数据。\nBerkeley DeepDrive BDD100k：这是目前最大的自动驾驶 数据集。里面有超过 1,100 多个小时驾驶体验的视频，包含10 万个在一天中不同时段以及在不同天气条件下的数据。\n百度 Apolloscapes：大型数据集，定义了26种不同的语义项，如汽车，自行车，行人，建筑物，路灯等。\nComma.ai：超过7个小时的高速公路驾驶视频。里面的数据包括汽车的速度、加速度、转向角和GPS坐标。\n城市景观数据集：记录50个不同城市的城市街道场景的大型数据集。\nCSSAD数据集：包含自动车辆的感知和导航等数据，但着重于发达国家的道路。\n麻省理工学院AGE实验室（MIT AGE Lab:）：在AgeLab收集的1,000多小时多传感器驾驶数据集的样本。\nLISA：智能和安全汽车实验室，加州大学圣地亚哥分校数据集：该数据集包括交通标志，车辆检测，交通信号灯和轨迹模式。\n博世小型交通灯数据集（Bosch Small Traffic Light Dataset）：用于深入学习的小交通灯数据集。\nLaRa交通灯识别（LaRa Traffic Light Recognition）：巴黎交通灯的数据集。\nWPI 数据集：交通灯、行人和车道检测的数据集。\n牛津的机器人汽车：这个数据集来自牛津的机器人汽车，它于一年时间内在英国牛津的同一条路上，反反复复跑了超过100次，捕捉了天气、交通和行人的不同组合，以及建筑和道路工程等长期变化。\nKUL比利时交通标志数据集：来自比利时法兰德斯地区数以千计的实体交通标志的超过10000条注释。\nMIT AGE Lab：在AgeLab收集的1,000多小时多传感器驾驶数据集的样本。\n医疗\nHealth Data：可搜索的主题包括医疗设备、环境卫生、药物滥用、精神健康等等。\n头部 CT 扫描数据集：491 次扫描的 CQ500 数据集。\nCheXpert：斯坦福发布，内含224316X光胸部图片，共涉及65,240名患者。数据量级和标注精准度都非常高。标注了 14 种常见的胸部放射影像观察结果。\n吴恩达医学影像数据集：含有4万张人体上肢端的X光片的数据集MURA，并用这个数据集训练CNN寻找并定位X光片的异常部分。数据集要等到2月才会公布，可以持续关注Stanford ML\nMIMIC-CXR：斯坦福与麻省理工学院的联合发布，内含371,920张带标签的胸部X射线图片，数据量级和标注精准度都非常高\n慢性病数据（Chronic disease data）：美国各地慢性病指标的数据。\nMIMIC-III：MIT计算生理学实验室的公开数据集，标记了约40000名重症监护患者的健康数据，包括人口统计学、生命体征、实验室测试、药物等维度。\n金融和经济\nQuandl： 里面有很多经济和金融数据，你可以使用这些数据建立预测经济指标或股价的模型。\n世界银行开放数据（World Bank Open Data）：涵盖世界各地人口统计、大量经济和发展指标的数据集。\n国际货币基金组织的数据（IMF Data）：国际货币基金组织公布关于国际金融、债务率、外汇储备、商品价格和投资的数据。\n英国金融时报金融时报市场数据（Financial Times Market Data：）：里面有来自世界各地的最新金融市场信息，包括股票价格指数、商品和外汇。\n谷歌趋势（Google Trends）：观察和分析有关互联网搜索活动和世界各地新闻故事趋势的数据。\n美国经济协会(AEA)：这这里你可以找到美国宏观经济的相关数据。\n公共政府数据集\n免费图像：免费图像来源列表以及列表中的所有数据\nGitHub 上的 BuzzFeed News：提供了来自 Buzzfeed 的数据。如果你想了解 2016 年至 2018 年期间的假新闻，那么这个就是你的最佳选择。\nGroup Lens：很多关于书籍和电影的信息。\nFive Thirty Eight：有关于政治、体育、科学、健康、经济和文化方面的数据。\nBureau of Labor Statistics：有关美国劳动力市场活跃度、工作条件和价格变化的数据。\nCenters for Disease Control and Prevention：包括各种健康主题，可让你访问大量可浏览和可搜索的数据。\nPew Internet：社会学数据。\nNASA 的 Earth Data：地球观测系统数据和信息系统包含了美国宇航局的地球观测数据，其中包含如 NC 地表温度和碳通量等信息。\nReddit：可以搜索数据集并查找提供信息和请求信息的人。总的来说，Reddit 也是一个寻找信息并了解行业趋势的好地方。\nNational Center for Environmental Information：涵盖地球物理学、大气和海洋数据。他们目前是世界上最大的气候和天气信息提供商。\nOpen Corporates：全球最大的公司开放数据集，可让你访问超过 1 亿家公司的信息。你可以按公司或高级职员进行搜索，并在需要的时候限制你的搜索范围。\nAltmetric：提供每年发布的最具热度的前 100 篇文章。\nThe World Factbook：该数据集包含 267 个国家和地区的信息，这是一个数据宝库，每周更新一次有关全球的信息。\n欧盟开放数据门户：\n美国政府数据：\n新西兰政府数据集：\n印度政府数据集：\n首个官方气象数据集公开，已训练出20多个“青出于蓝”的AI\nData.gov：在这里可以下载到多个美国政府机构的数据。从政府预算到学校成绩。但要注意的是，很多数据还有待进一步研究。\n食品环境地图集（Food Environment Atlas）：当地的食物选择如何影响美国饮食的数据。\n学校系统财务状况（School system finances）：这里有美国学校系统财务状况的调查。\n美国国家教育统计中心（The US National Center for Education Statistics）：来自美国和世界各地的教育机构和教育人口统计数据。\n英国数据服务：英国最大的社会、经济和人口数据收集机构。\n数据美国（Data USA）：全面的、可视化的美国公共数据。\n国家统计局\n微软恶意软件数据集：每一行数据都对应着一个MachineIdentifier，相当于设备ID，也都包含一个代表着真实值的标签HasDetections，显示这台设备有没有感染恶意软件。\n来源\n[1] https://mp.weixin.qq.com/s/NjJRSim8DLvKoI01PMkNfw"}
{"content2":"1.计算机视觉简介\n计算机视觉是一门利用计算机使机器模拟人的视觉系统的科学，通俗的说是用计算机处理图像数据的科学。可以分为低级视觉处理和高级视觉处理。更进一步的说，是通过摄相机和计算机代替人眼对目标识别、分析和跟踪、重建等机器视觉处理。计算机视觉研究相关的理论和技术，通过计算机搭建软硬的图像处理和分析的人工智能系统。其应用包括AI、VR、3D等人工智能领域，具体的工业应用领域为自动驾驶、安防、人脸识别、物体识别、医学图像分析，无人机、VR/AR，3D等。其中很多方向和前景工业产业都应用到了计算机视觉技术。\n2.编程能力\n1.1 主流编程语言简介（C/C++,Python）\n计算机视觉是计算机要使用计算机的语言来处理计算机任务的技术性学科，目前主流的编程语言主要是C/C++和Python。\nC/C++:C/C++语言是当今最流行的程序设计语言之一，学术界和工业界都比较流行，尤其是工业应用广泛，其功能丰富、表达能力强、灵活方便应用广、可植入性好。执行效率高，既有高级语言的特点也有低级语言的特点，是编程入门的经典语言之一。\nPython:Python是一种动态的、面向对象的解释性脚本语言。很多丰富的库功能堪比MATLAB的部分功能。最近几年由于深度学习的火热，可用于图像处理、支持opencv、；科学计算功能类似MATLAB，机器学习和深度学习使用Python语言更加广泛，各种开源的丰富库可以调用，缺点是执行效率不高。下图为最近两年年计算机语言排名\n1.2 C/C++和Python快速入门和进阶\nC/C++:如果你有或者没有C语言（或者先快速的过一下C语言程序设计）的基础，都可以跳过C语言直接学习C++。推荐以下书籍，C++ Primer（第五版）作者：Stanley Lippman, Josée Lajoie, and Barbara E. Moo (更新到C++11)，其他C/C++资源参考C++必读书籍推荐。\npython:python语言相对比较好入门一些，参考廖雪峰的python教程，接着可以看《笨方法学Python》,《Python数据手册》等。\n1.2图像处理编程(可以一边练习编程一边图像处理编程，提高成就感)\nViusal studio + opencv:参考《opencv3编程入门》，一开始可以与C/C++同步练习，熟悉用opencv进行图像处理的技术和方法。一边练习代码一边练习图像处理，既要理解图像处理的理论也要读懂图像处理的代码，图像处理的理论书籍可以阅读刚萨雷斯的《数字图像处理》书中的理论尽量和代码对应的来理解，如果对C/C++和opencv的使用熟悉了，Python+OpenCV也相对容易。\n提示：入门计算机视觉可以多阅读CSDN网络博客：\n推荐博客：\n如何学习计算机视觉\n计算机视觉入门学习指南"}
{"content2":"光流的概念是Gibson在1950年首先提出来的。它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。一般而言，光流是由于场景中前景目标本身的移动、相机的运动，或者两者的共同运动所产生的。其计算方法可以分为三类：\n（1）基于区域或者基于特征的匹配方法；\n（2）基于频域的方法；\n（3）基于梯度的方法；\n简单来说，光流是空间运动物体在观测成像平面上的像素运动的“瞬时速度”。光流的研究是利用图像序列中的像素强度数据的时域变化和相关性来确定各自像素位置的“运动”。研究光流场的目的就是为了从图片序列中近似得到不能直接得到的运动场。\n光流法的前提假设：\n（1）相邻帧之间的亮度恒定；\n（2）相邻视频帧的取帧时间连续，或者，相邻帧之间物体的运动比较“微小”；\n（3）保持空间一致性；即，同一子图像的像素点具有相同的运动\n这里有两个概念需要解释：\n运动场，其实就是物体在三维真实世界中的运动；\n光流场，是运动场在二维图像平面上的投影。\n如上图所示，H中的像素点(x,y)在I中的移动到了(x+u,y+v)的位置，偏移量为(u,v)。\n光流法用于目标检测的原理：给图像中的每个像素点赋予一个速度矢量，这样就形成了一个运动矢量场。在某一特定时刻，图像上的点与三维物体上的点一一对应，这种对应关系可以通过投影来计算得到。根据各个像素点的速度矢量特征，可以对图像进行动态分析。如果图像中没有运动目标，则光流矢量在整个图像区域是连续变化的。当图像中有运动物体时，目标和背景存在着相对运动。运动物体所形成的速度矢量必然和背景的速度矢量有所不同，如此便可以计算出运动物体的位置。需要提醒的是，利用光流法进行运动物体检测时，计算量较大，无法保证实时性和实用性。\n光流法用于目标跟踪的原理：\n（1）对一个连续的视频帧序列进行处理；\n（2）针对每一个视频序列，利用一定的目标检测方法，检测可能出现的前景目标；\n（3）如果某一帧出现了前景目标，找到其具有代表性的关键特征点（可以随机产生，也可以利用角点来做特征点）；\n（4）对之后的任意两个相邻视频帧而言，寻找上一帧中出现的关键特征点在当前帧中的最佳位置，从而得到前景目标在当前帧中的位置坐标；\n（5）如此迭代进行，便可实现目标的跟踪；\n原文地址：https://blog.csdn.net/carson2005/article/details/7581642"}
{"content2":"计算机视觉方面的三大国际会议是ICCV, CVPR和ECCV\nICCV的全称是International Comference on Computer Vision，正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。\nCVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。\nECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。\n总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster调自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。\n计算机方向的一些顶级会议和期刊\nhttp://kapoc.blogdriver.com/kapoc/1134028.html\nComputer Vision\nConf.:\nBest: ICCV, Inter. Conf. on Computer Vision\nCVPR, Inter. Conf. on Computer Vision and Pattern Recognition\nGood: ECCV, Euro. Conf. on Comp. Vision\nICIP, Inter. Conf. on Image Processing\nICPR, Inter. Conf. on Pattern Recognition\nACCV, Asia Conf. on Comp. Vision\nJour.:\nBest: PAMI, IEEE Trans. on Patt. Analysis and Machine Intelligence\nIJCV, Inter. Jour. on Comp. Vision\nGood: CVIU, Computer Vision and Image Understanding\nPR,   Pattern Reco.\nNetwork\nConf.:\nACM/SigCOMM ACM Special Interest Group of Communication..\nACM/SigMetric   这个系统方面也有不少的\nInfo Com  几百人的大会，不如ACM/SIG的精。\nGlobe Com 这个就很一般了，不过有时候会有一些新的想法提出来。\nJour.:\nToN (ACM/IEEE Transaction on Network)\nA.I.\nConf.:\nAAAI: American Association for Artificial Intelligence\nACM/SigIR: 这个是IR方面的，可能DB/AI的人都有\nIJCAI: International Joint Conference on Artificial Intelligence\nNIPS: Neural Information Processing Systems\nICML: International Conference on Machine Learning\nJour.:\nMachine Learning\nNEURAL COMPUTATION: 这个的影响因子在AI里最高，2000年为1.921\nARTIFICIAL INTELLIGENCE: 1.683(2000年的数据，下同)\nPAMI: 1.668\nIEEE TRANSACTIONS ON FUZZY SYSTEMS: 1.597\nIEEE TRANSACTIONS ON NEURAL NETWORKS: 1.395\nAI MAGAZINE: 1.044\nNEURAL NETWORKS: 1.019\nPATTERN RECOGNITION: 0.781\nIMAGE AND VISION COMPUTING: 0.616\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING: 0.465\nAPPLIED INTELLIGENCE: 0.268\nGraphics\nConf.:\nBest:\nSiggraph: ACM SigGraph\nGood:\nEuro Graph\nJour.:\nIEEE(ACM) Trans. on Graphics\nIEEE Trans. on Visualization and Computer Graphics\nCAD\nJour.:\nCAD\nCAGD"}
{"content2":"为什么要谈这个：再过几个月就是“春招”，虽然三月份还没到，但是已经火药味十足，各大企业已经开始招收实习生，并且给出了不少转正名额！当下在硕士生圈流传着一句话“大厂实习经历抵得上一篇论文！”，让不少因“种种原因”不能去实习的小伙伴，面红耳赤，两字“上火！！！”。但是不无道理，人家去实习了，相当于去“展示自己的才艺”，展示的好自然可以留下来，不好也算是个经历！\n开门见山-说一说目前国内有哪些可以去的计算机视觉岗位：\n不归类们：\n·百度-计算机视觉岗位-年薪35万左右 http://idl.baidu.com/index.html\n·阿里巴巴·～不说了\n·腾讯-计算机视觉岗位（优图）-年薪：有人知道的话评论下 http://youtu.qq.com/\n·海康威视 -国内最大的监控设备供应商？ http://campus.hikvision.com/zpdetail/150161660?r=-1&p=&c=&d=&k=%E8%A7%86%E8%A7%89\n·平安科技-比如平安医疗等 http://tech.pingan.com/agility.html\n·网易 http://ai.163.com/#/m/overview/\n·美图\n·美团（无人驾驶那些，做送外卖机器人？）\n·深圳大疆\n·华为\n·地平线 https://www.horizon.ai/\n感谢：https://blog.csdn.net/hduxiejun/article/details/53725836 （计算机视觉与深度学习公司），部分内容来自这个地方\n方向比较多们：\n·商汤科技-工资高也是众所周知 http://www.sensetime.com/\n·旷世科技Face++ https://www.megvii.com/campus/graduate/\n·格灵深瞳  http://www.deepglint.com/\n·极视角 http://www.extremevision.mo/\n·云从科技 http://www.cloudwalk.cn/\n·LinkFace https://www.linkface.cn/about.html\n·中科视拓 http://www.seetatech.com/\n·浙江大华 http://www.dahuatech.com/\n·图谱科技 https://www.tuputech.com/\n·依图科技 http://www.yitutech.com/joinus/campus/185.html\n·宇视科技 http://cn.uniview.com/\n无人驾驶：\n·minieye 无人驾驶 （还有京东，滴滴，百度）http://www.minieye.cc/home\n·蔚来（车） http://www.nio.com/formulae\n·游侠 (车）http://www.youxiamotors.com/\n·比亚迪\n·Montana（无人驾驶）\n·中汽（国企）\n·起亚 我记得有吧\n医疗类：\n·图森 http://www.tusimple.com/\n·推想科技 http://www.infervision.com/\n·汇医慧影 ·http://www.huiyihuiying.com/idoctor/html/newLoginVersion2/index.html\n·DeepCare http://www.deepcare.com/\n·智影医疗 http://www.zying.com.cn/\n·健培科技 http://www.jianpeicn.com/\n·图玛深纬 （岗位上没发现有需要深度学习的工程师之类，可能不缺了吧）http://tumashenwei.langye.net/career/\n·连心医疗 （可能也没有位置了）http://www.linkingmed.com/\n·拍医拍 机器视觉融合大数据医疗 https://xiaoyuan.shixiseng.com/company/xri_ssa0lzd4ilea?act=zw#1\n其他的话，可以参考下我一开始给出的那个地址连接\n括弧：https://blog.csdn.net/Maximize1/article/details/82562090 这篇博客有几个月前发出来的“计算机视觉岗位”招聘，有四个公司“百度”、“华为”、“海康威视”、“商汤科技”，2018年9月份发出来的，“详细的说了这四个公司招的一些岗位，工作内容等！”\n谈一谈“寒冬”：\n都在说“互联网”的寒冬到来了～～～，其实我们看一个图～～～这是来自google的，其实百度也是一样的。裁员高峰时隔多年再次到来，在2018年底到达了高潮。网上消息众说风云“知乎大裁员”、“美团”、“富士康万人大裁员”～～谣言？不少辟谣的！～等等，知乎上不少人说，突然接到电话“明天不用来上班了！”。这是 经济问题，贸易战也有一定因素！对于AI的一些企业来说，AI火起来的这些年，技术也越来越精了！企业也需要更加精的人才，炒概念圈钱越来越来，企业开始要做实事了！需要招到“能做实时，能把概念落地的人才”！所以，大家感觉AI相关的岗位越来越难了！因为通常会遇到一些“实用性”的问题！\n不管怎么说，数据实打实在那里，确实日子没有那么好过了！互联网公司，也有不少撑不下去了！很多人以ofo为例子，其实ofo严格来说是本身经营不善！概念超太凶，忘记了赚钱->_->。对于计算机的算法刚来说，经历过秋招的人，都知道“激烈”已经不足以形容了，计算机视觉岗位的人多到爆炸！甚至有传言说“一篇顶会”是简历不被扔进垃圾桶的最低标准！\n顶会！！！计算机视觉顶会这几年有多难！CVPR、ECCV、ICCV等等！这几年简直是疯狂啊，而且那些顶着名头的“学术活动家”们太多了！如果有人带着发一篇自然是荣幸，或者实力强悍的自己来个idea发个！但是对于大多数人来说～～～几乎～～～！\n那么多计算机视觉岗位的应聘人员是哪里来的：\n1、硕士生去了趟培训机构（人工智能速成班）、正如广告“不是计算机专业，如何速成python AI？”\n2、初级调包侠 （不会自己写从0开始写模型，dataload也不会写！只能是git clone，make/setup/train/test等），理论一点不懂。（Pytorch的出现，对于很多研究人员是福音，其实对于调包侠们也是福音……^_^，终于能看懂了，内心OS）\n3、高级调包侠 - 参加过很多比赛，是个比赛就去水一下，找个baseline 跑一跑，会调一些“没有灵魂”的参数！混一点小成绩，通常具有一定的理论基础，水/发 过一两篇SCI / IEEE ，或者做过相关项目！\n4、非计算机专业的计算机视觉、深度学习方向人员（众所周知，通信、自动化、机械电子等一些和计算机方向沾边的专业的一些老师，都会开设一个方向：机器智能与模式识别，那么在这几年就是“CNN”专业，“我这么说有失偏颇，sorry！”，但是确实，找工作比本专业的一些其他技术，相对来说容易一些。）\n5、真正的大牛 - 大侠，通常手握顶会、项目一二、比赛也通常是国际顶级比赛的前几或冠军，这类人其实一般不会是“对手”，因为他们的工作岗位不和我们相冲突！\n这么多人呢！都挤到一块了！人数多了，自然工作难找！这也导致了开发岗位的紧缺！其实很多被直接刷掉的都是“1，2”，那部分可能也是人数最多的一部分！稍微有一点技术的，18年的秋招，都是拿到了一些offer的！\n如何武装自己：\n语言：Python 、C++ （虽然很多算法岗位要求python就行了，但是～～～从落地而言，眼光长远些，C++有必要吧）\n框架：Pytorch   Caffe/tensorflow 最好熟悉两个\n比赛：最好名次靠前！\n实习经历：好多国内高校～～～，其实实习真的蛮好的！但是种种原因，去不了也没办法！\n论文：SCI  IEEE EI能发就发些！但是面试官也看得出来，很多水文～～～现在顶会好多文章都很水通常名字是拼音->_->，真不是我夸外国人！\n项目：我感觉应该还蛮看中的！毕竟企业是要赚钱的！项目很关键吧？\nBB了这些！！！主要是想说，怎么办，大家都在这个大坑里，一起玩吧*^_^*！！！还没经历过面试，不知道这些说的对不对，简历也没啥能拿的出来的，好慌！如果有什么不对的地方，希望多多指正！！\n有人要问了，武装自己需要这么多东西！没有怎么办？没关系，T_T太难了，我也不知道怎么办，面对白纸一样的简历，一起加油吧！！！\n现在开放的比赛：\n目标检测：\n数钢管 https://www.datafountain.cn/\n数人头 https://biendata.com\n识别：在kaggle\n鲸鱼识别\n可爱程度预测\n天池还有几个："}
