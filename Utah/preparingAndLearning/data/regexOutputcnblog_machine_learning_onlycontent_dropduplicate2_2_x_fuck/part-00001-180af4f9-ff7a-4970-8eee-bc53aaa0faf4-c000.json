{"content2":"前言：昨天看到微博上分享的深度学习视频，为上海复旦大学吴立德教授深度学习课程，一共六课时，视频地址点击打开链接。在此我结合其他资料，写点个人的学习笔记进行记录。\n第一课：概述\n本课吴老师对深度学习做了一个简单的介绍。内容分为三块：1.深度学习简介  2.深度学习应用   3.领域概况\n1.深度学习简介\n深度学习（deep learning）是机器学习（machine learning）的一个方法。本节首先简单介绍机器学习，然后引入深度学习的概念。\n1.1 机器学习简介\n机器学习四要素：\n[星号**内容为我自己补充内容，部分内容来自《统计机器学习》]\na， 数据\n** 机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。（监督）学习中训练数据由输入（或特征向量）与输出对组成。\nb， 模型\n线性模型：\n广义线性模型：\n非线性模型：       ANN （人工神经网络 维基百科点击打开链接） 常见的一种是前馈神经网络，深度学习主要针对就是这种前馈神经网络。\n** 在监督学习过程中，模型就是所要学习的条件概率分布或据侧函数。模型的假设空间包含所有的可能的条件概率分布或决策函数。即假设空间中的模型一般无穷多个。\nc， 准则\n损失函数：\n经验风险：\n正则项：\n学习目标：\n** 设假设空间中的模型一般无穷多个，我们具体选择哪一个使用，需要一个模型评价指标。这里的准则就是评价指标。选择模型的好坏就是按照这个标准来说。\nd， 算法\n深度学习（Deep Learning）就是针对深度前馈神经网络的学习算法。\n**算法是指学习模型的具体计算方法，统计学习基于训练数据集，根据学习策略，从假设空间中选择最优的模型，最后需要考虑用什么样的计算方法求解最优模型。说白了就是在假设中间中怎么去找那个在评价准则中最优的模型。\n1.2 深度学习简介\n深度前馈神经网络的学习问题就是深度学习。\n1.2.1  广义线性模型 vs 线性模型  vs 非线性模型\n课程中讲授线性模型和广义线性模型的区别，我们从上述公式中也可以看到。线性模型因变量y是由自变量x线性得到，而广义线性模型中我们找到一个变换，然后先对自变量x做一个变换，成为然后进行线性加和。在模式识别和机器学习中，我们称那个需要找到的变换为特征。找到一个好的变换对我们的学习很有帮助。在具体的应用任务中，寻找特征需要人工进行（特征工程）。现在在深度学习中，特征本身需要从数据中自己学习。\n关于线性模型和广义线性模型可以参考点击打开链接\n“深度”就是前馈神经网络层数较多（层数>5）。\n**浅层模型有一个重要特点，就是假设靠人工经验来抽取样本的特征，而强调模型主要是负责分类或预测。在模型的运用不出差错的前提下（如假设互联网公司聘请的是机器学习的专家），特征的好坏就成为整个系统性能的瓶颈。因此，通常一个开发团队中更多的人力是投入到发掘更好的特征上去的。要发现一个好的特征，就要求开发人员对待解决的问题要有很深入的理解。而达到这个程度，往往需要反复地摸索，甚至是数年磨一剑。因此，人工设计样本特征，不是一个可扩展的途径。深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。所以“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：1. 强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2. 明确突出了特征学习的重要性，也就是说，同过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，使分类或预测更加容易。（本段内容来余凯文章）\n1.2.2  深度学习难处：\n可训练参数太多\n反应在这个问题上，就是\n1. 计算资源要大  需要海量的CPU来玩这个算法。\n2.数据量充足     数据量小，参数多，过拟合的问题就严重了。\n3.算法效率要高  茫茫大海中如何去寻找那组最优的参数需要高效的进行。\n4.解释困难   虽然模型取得效果，但是很难直观解释为什么模型会work。\n非凸优化\n调参困难\n2，深度学习应用和领域概况\n应用：语音识别   目标识别  自然语言处理 参考余凯老师的文章：点击打开链接\n主要大牛：\nUniversity of Toronto     Hinton   个人主页http://www.cs.toronto.edu/~hinton/[兼任Google特聘研究员 ]\nNew York University      LeCun   个人主页http://yann.lecun.com/ [前段时间Facebook宣布聘请纽约大学教授扬•乐康（Yann LeCun）掌管其新建的人工智能实验室，就是这哥们]\nUniversity of Montreal    Bengio   个人主页http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html\nStanford University         Andrew Ng 个人主页http://cs.stanford.edu/people/ang/  [Google Brain项目联合创建人]"}
{"content2":"起源\n人工智能的起源普遍认为是 1956 年的达特茅斯会议。 因为这次会议本身就是为了人工智能而召开的，而且参会的人后来也成了人工智能各个方向上的大牛。 参会的主要 6 个人：\n麦卡锡，会议的召集人，也是 LISP 的发明者\n克门尼，BASIC 发明人，做过爱因斯坦的数学助理，和麦卡锡一起研究出了 分时系统\n明斯基，提出神经网络\n香农，信息论创始人\n纽厄儿，图灵奖得主\n司马贺，图灵奖得主，诺贝尔奖得主，纽厄儿的学生，他们俩都是人工智能中的符号学派\n人工智能的概念虽然是达特茅斯会议提出的，但是在此之前已经有关于人工智能的研究，只不过那时不叫人工智能， 而是称为\"机器智能\"或是\"智能机器\"，对此，最广为人知的就是图灵的论文和他的图灵测试。\n发展\n人工智能提出的很早，但发展的并不顺，不像其他学科都逐渐走向统一，从诞生之初，人工智能的研究方向就不断分裂。 目前主流的人工智能方向有：\n符号学派\n这是传统的人工智能方向，专注于如何制造智能机器特别是智能的计算机程序和工程，但不局限于模拟生物的智能行为。 这一学派和图灵的研究一脉相承。\n符号学派利用知识和搜索来代替真实人脑的神经网络结构，擅长解决利用现有知识做比较复杂的推理，规划，逻辑运算和判断等问题。\n连接学派\n之所以成为连接学派，是因为这一学派认为高级的智能行为是从大量神经网络的连接中自发出现的。 神经网络就是这个学派提出的，但是神经网路发展也不顺利，后来借助统计学习理论，稍有起色，在某些方面形成了优势。\n连接学派可以很好的解决机器学习的问题，并自动获取知识。擅长解决模式识别，聚类，联想等非结构化的问题。\n行为学派\n行为学派和前 2 个学派不一样，没有把重点关注在高级智能上，而是关注比人类低级的多的昆虫上。 这个学派产生了极其紧密的极其昆虫，也有能够适应各种环境的机器狗，当然，从动物身上不仅仅学到这些， 还有遗传算法，这是对进化方面的研究。\n行为学派关注模拟身体的运作机制，而不是脑。擅长解决适应性，学习，快速行为反应等问题，也可以解决一定的识别，聚类，联想等问题。\n其他\n这 3 个学派大致是从软件，硬件，身体 3 个角度来模拟和理解智能的。 随着人工智能的发展，单独依靠某一个学派，发展都会遇到瓶颈，于是，很多人工智能研究者逐渐不再关注人工智能的理论， 而是从实际应用出发，能够解决实际问题的就是好方法，就这样，逐渐产生了很多新兴的学科：\n自动定理证明\n模式识别\n机器学习\n自然语言理解\n计算机视觉\n自动程序设计\n总结\n人工智能进入 21 世纪第二个十年，最引人注目的成就就是 深度学习 深度学习也是一种对大脑的模拟，它模仿了人类大脑的深层体系结构。 到了今天，充足的食物（大数据）和强劲的消化系统（GPU，云计算）成为了深度学习崛起的契机。\n除此之外，还有一种 被称为 人类计算 的方式，也叫人工人工智能就是利用互联网上大量的人群来完成需要机器完成的工作。 这种方式比较有意思，实际的案例有：\n卡内基梅隆大学的 路易斯-冯-安 reCAPTCHE: 利用验证码来完成英文古文献的数字化\n同样是冯-安 设计的 Verbosity 游戏，目的是构建知识库\n此外还有 Matchin，Duolingo\n现在，最火的人工智能方式是机器学习，但是人工智能远远不只是机器学习，我们在学习，讨论机器学习的同时， 也要站在更高的角度上看待人工智能，因为未来人工智能的统一在何方还是未知。"}
{"content2":"《自然语言处理理论与实战》（试读版）\n内容介绍\n本书分四个部分，第一部分主要介绍基础知识，包括认识机器学习和自然语言处理、快速上手Python、线性代数、概率论和统计学；第二部分主要介绍自然语言处理技术，包括自然语言处理介绍、语料库技术、中文分词、数据预处理、马尔科夫模型、条件随机场、模型评估、剖析自然处理工具背后的原理；第三部分主要介绍机器学习技术，包括认识机器学习、常见机器学习算法、机器学习算法案例源码实现。第四部分主要介绍工程项目实践，包括Python项目实战、自然语言处理项目实战、机器学习结合自然语言处理综合项目实战。\n作者介绍\n唐聃，男，教授，中科院工学博士。现工作于成都信息工程大学软件工程学院。研究方向包括自然语言处理、信息安全、数据分析。曾参与多项国家863项目和中科院知识创新工程项目、省科技厅和教育厅项目；2016年入选中国科学院西部之光人才计划（中国科学院西部青年学者）。\n白宁超，男，硕士学位。现工作于四川省计算机研究院，自然语言处理工程师。研究方向主要包括自然语言处理和机器学习，自然语言处理和机器学习相关的系列博文曾被CSDN、阿里云栖等多个平台转载。曾参与多项四川省科技厅项目。\n冯暄，男，高级工程师，硕士学位。现任四川省计算机研究院信息化工程研究所所长。研究方向包括物联网、多源信息融合、软件工程。主持或参与国家级、省级科研项目16项。获得四川省科技进步奖二等奖2项、四川省科技进步奖三等奖1项。\n文俊，男，硕士学位，现工作于成都广播电视台橙视传媒大数据中心，大数据算法工程师。研究方向主要包括数据挖掘、机器学习、自然语言处理、深度学习以及云计算。\n卿鸿宾，男，四川大学中文系在校生。研究方向包括应用语言学、计算语言学、韵律句法学等。常年从事文学创作与文字工作，2017年作品《黄昏速写》发表于《子曰书院》微信公众号，取得不错的影响\n专家推荐\n众所周知，自然语言处理是多学科交叉的一门学科，本书涵盖知识面较广且详，较适合该技术领域读者阅读。\n——— 王道顺，清华大学教授、博士生导师\n九层之台，起于垒土。基础知识的掌握，原理的深度理解尤其重要。有益于快速准确解决问题，这也是这本书的特点。\n——— 周世杰，电子科技大学教授、博士生导师\n市场同类书籍中对交叉学科基础内容阐述较少，尤其是语言学部分；这是本书的亮点之一，对于NLP兴趣者来说，非常适合阅读。\n——— 崔喆，中国科学院研究员、博士生导师\n万事开头难，做好一件事最重要的是清楚自己需要做好哪些准备。该书从基础出发由点到面，深入浅出，循序渐进的讲解如何学习人工智能自然语言处理的那些内容，是非常适合AI初学者的一本好书。\n——— 潘耀峰，百度企业智能平台，大数据高级工程师\n本书源码下载：自然语言处理理论与实战源码 https://github.com/BaiNingchao/NLP-ML\n自然语言处理、数据挖掘与深度学习的应用目前已成为互联行业三大热点，而自然语言处理结合数学、计算机科学、语言学一体，门槛较高。本书是作者总结多年在自然语言处理方面学习和研究的成果，理论结合实战，由浅入深，是本适合初学者的领路书籍。"}
{"content2":"摘要：本文简单叙述了如何用聚类来通过投票记录分析美国参议员的实际政治倾向\n声明：（本文的内容非原创，但经过本人翻译和总结而来，转载请注明出处）\n本文内容来源：https://www.dataquest.io/mission/60/clustering-basics\n在前面的两篇文章中使用的线性回归和分类都属于有监督的机器学习（根据已有的数据训练模型，然后预测未知的数据），而无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类。聚类算法是把具有相同特征的数据聚集在一组。\n原始数据展现\n在美国的参议院要通过一项法律时，需要由参议员来投票，而这些议员主要来自于两个政党，共和党（Democrats）和民主党（Republicans），现在使用的数据就是这些议员的投票记录，每一行代表了一个议员的情况（party – 所属政党，D代表共和党，R代表民主党，I代表无党派， 第三列之后都代表了某一个法案的投票情况，1代表赞成，0代表反对，0.5代表弃权）\nimport pandas votes = pandas.read_csv('114_congress.csv')\n统计一下每个政党的人数\nprint(votes[\"party\"].value_counts())\n聚类算法\n计算距离\n为了把具有相同特征的议员聚集在一组，就需要衡量两个议员的特征究竟有多么的接近，在这里使用的是欧几里德距离计算公式\n譬如取前两个议员的记录来进行计算，这是他们的投票结果：\n计算结果：d = 1.73\n在python中计算欧几里德距离，可以通过scikit-learn库的euclidean_distances()方法，现在仍然计算前两个议员的距离\nfrom sklearn.metrics.pairwise import euclidean_distances print(euclidean_distances(votes.iloc[0,3:], votes.iloc[1,3:])) # 因为前三列不是数字类型，所以要排除前三列的数据\n聚类\n接下来会使用k-means聚类算法来根据欧几里德距离来将数据进行分组聚集，每一组都会有一个中心点，然后计算每个议员到这个中心点的距离，再将该议员分配到距离最小的那个中心点所属的组中。下面使用scikit-learn库来训练一个k-means模型（因为主要有两个政党，所以分为两组即可）\nimport pandas as pd from sklearn.cluster import KMeans # n_clusters参数指定分组数量，random_state = 1用来重现同样的结果 kmeans_model = KMeans(n_clusters=2, random_state=1) # 通过fit_transform()方法来训练模型 senator_distances = kmeans_model.fit_transform(votes.iloc[:, 3:])\n生成的是一个ndarray，每一行代表了一个议员，第一列代表了该议员与第一组中心点的距离，第二列代表了该议员与第二组中心点的距离\n统计\n经过上面的计算，现在要统计在每一组中究竟分布着多少个来自于不同政党的议员（类似于透视图），使用Pandas中的crosstab()方法可以进行统计，该方法需要两个向量或者Series作为参数来进行统计\nlabels = kmeans_model.labels_ print(pd.crosstab(labels, votes[\"party\"]))\n上面语句中的labels变量的结果如下：（在ndarray中的每个元素代表了一个议员所属的组编号，其编号取其距离最小的那组）\n上面的结果显示，第一组包含了41个民主党议员和两个无党派议员，第二组包含了3个民主党议员和54个共和党议员，看起来有3个民主党议员的政治倾向更偏于共和党，而这三位仁兄就是\ndemocratic_outliers = votes[(labels == 1) & (votes[\"party\"] == \"D\")]\n数据可视化\n在上面的计算中，已经把每位议员到两个组的举例计算出来了，现在将这两个距离数据分别作为x和y坐标，然后做一个散点图，并且根据它们的组编号进行不同着色\nplt.scatter(x=senator_distances[:,0], y=senator_distances[:,1], c=labels) plt.show()\n寻找激进分子\n可以根据上面计算的每个议员与组的距离来判断一个议员是否属于激进分子，最激进的议员就是那些远离一个组中心点最远的数据点，而处于两个中心点的数据点则表明这是比较温和的议员。要衡量这个激进程度，可以通过对两个距离计算结果进行指数运算来放大差异性。例如对于某个激进议员extremist = [3.4, .24]和温和议员moderate = [2.6, 2]，如果只是简单相加其结果，就会得到3.4 + .24 = 3.64, 和2.6 + 2 = 4.6，看起来他们之间的差距并不大。然而，将它们分别进行立方运算再相加3.4 ** 3 + .24 ** 3 = 39.3, 和2.6 ** 3 + 2 ** 3 = 25.5，就能体现他们之间的差异性了\nextremism = (senator_distances ** 3).sum(axis=1) votes[\"extremism\"] = extremism votes.sort(\"extremism\", inplace=True, ascending=False) # 根据激进性进行降序排序\n总结\n聚类是一个用来寻找数据特征的强有力的方法，在使用有监督的机器学习方法并没有取得进展时，可以尝试使用无监督的学习方法，通常来说，在使用有监督学习方法之前先使用无监督学习方法是一个不错的开始。"}
{"content2":"机器学习：从入门到沉迷\n最简单的机器学习介绍\n你是不是也经常听人说起机器学习但是完全不能明白到底什么才是机器学习，是不是完全厌倦了与同事点头一样的交谈，让我们通过这篇文章改变这一现状吧\n这是一篇针对想知道什么是机器学习但是不知道如何入门的人的入门。我猜有很多人厌倦了通过图阅读维基百科的文章来了解机器学习，想有一篇通俗的文章来介绍什么才是机器学习，那么这篇文章就是这样的\n什么是机器学习\n当你要通过很多数据去发现某个问题的时候，你不用去针对特定的问题编写代码而是通过使用很多通用算法来获得获得你所需要的东西，这就是机器学习。相对于编写很多代码，你只需要给予它足够的数据让他基于这些数据建立一个逻辑算法。\n例如，一种算法是分类算法，它可以把数据分成不同的组。本来是用来识别是否是手写体数据的算法可以在不改变任何一行代码的情况下用来实现垃圾邮件的自动分类。完全相同的算法只是使用了不同的数据便产生了不同的逻辑过程。\n机器学习就是很多种类似于这种处理的合称。\n两种机器学习算法\n主要的机器学习算法有两个主要分类－－监督学习和无监督学习。他们之间的区别很简单但是很重要。\n监督学习\n假设你是一个房地产经纪人，你的业务正在高速增长所以你聘请了很多实习生了帮助你。但是有一个问题，你是一名出色的经纪人所以你能很快知道哪种房子比较受欢迎，而那些实习生并没有你这个能力去正确评估房子。\n为了帮助你的实习生正确判断每一个房子的价格你决定写一个小的app，它可以根据房子的大小，邻居等信息去类比相似的房子来决定这个房子的价格\n因此你写下近三个月被卖出的房子的价格。对于每一个房子你都写了细节，房间数目，大小，邻居等，但是最终要的是你写下了最重的价格。\n使用这些数据我们希望创建一个程序来决定你这个区域其他房子的价格。\n通过这些数据我们希望创建一个程序来预测该区域其他房子的价格\n这就是一种监督学习模型，你知道每套房子的价格，即你知道这个问题的答案，并可以从这个答案想问题出发推理出一个逻辑。\n开发这个APP，你把你的房子数据输入到机器学习的算法中。然后这个算法自己试图计算出需要解决一个什么样的数学问题然后给出正确的答案。\n这种就是有点类似于指导答案但是要去推算出数学公式的问题\n通过这些数据，你能找出这是在测试哪种数学运算吗？你知道你应该通过左边的数据进行某些运算来得到右边的答案。\n在监督学习模型下，你会让电脑帮你计算出他们之间的关系。一旦你知道了需要什么数学模型来解决这个特定问题的时候，你就能回答同一类型的其它问题了。\n无监督学习\n让我们回到原来的房地产经纪人的例子。如果你不知道每间房间的价格该怎么办？事实上当你知道房子的大小，位置等信息的时候你仍然可以做一些非常有趣的事情。这就是无监督学习。\n这种就类似于有人给你一串数字然后告诉你他完全不知道这些数字的含义，但是你或许可以找出其中的规律。\n那么你可以用这些数据干些什么呢？首先，你可以通过一个算法在你的这些数据中来识别不同的细分市场，或许你会发现附近大学附近的购房者喜欢有很多房间的房子，但是郊区的购房者喜欢有三间卧室但是很大的房子。了解这些不同的客户可以帮助你实现定点营销。\n你可以做的另外一件很有意思的事情是可以识别出一些比较独特的房子。而这些独特的房子这有可能是一些豪宅，你可以让你最好的销售人员来处理这些房子因为他们可以赚取更多的佣金。\n监督学习是通过过去的一些事实来完成剩下的事，但这并不说无监督学习就没有用或者没有学习的价值。事实上，正因为无监督学习的没有标准答案的优点让它随着算法的逐渐改善在现实社会中变得越来越重要。\n上面的介绍确实很有意思，但是能正确评估房子价格真的能算“学习”？\n下篇正式开始写算法了，有点小激动。\n（注，本文翻译自一篇外文，我在原文中添加了一些自己的理解）"}
{"content2":"转自：http://blog.csdn.net/woaidapaopao/article/details/77806273\n第一部分：深度学习\n1、神经网络基础问题\n（1）Backpropagation（要能推倒）\n后向传播是在求解损失函数L对参数w求导时候用到的方法，目的是通过链式法则对参数进行一层一层的求导。这里重点强调：要将参数进行随机初始化而不是全部置0，否则所有隐层的数值都会与输入相关，这称为对称失效。\n大致过程是:\n首先前向传导计算出所有节点的激活值和输出值，\n计算整体损失函数：\n然后针对第L层的每个节点计算出残差（这里是因为UFLDL中说的是残差，本质就是整体损失函数对每一层激活值Z的导数），所以要对W求导只要再乘上激活函数对W的导数即可\n（2）梯度消失、梯度爆炸\n梯度消失：这本质上是由于激活函数的选择导致的， 最简单的sigmoid函数为例，在函数的两端梯度求导结果非常小（饱和区），导致后向传播过程中由于多次用到激活函数的导数值使得整体的乘积梯度结果变得越来越小，也就出现了梯度消失的现象。\n梯度爆炸：同理，出现在激活函数处在激活区，而且权重W过大的情况下。但是梯度爆炸不如梯度消失出现的机会多。\n（3）常用的激活函数\n激活函数\n公式\n缺点\n优点\nSigmoid\nσ(x)=1/(1+e−x)σ(x)=1/(1+e−x)\n1、会有梯度弥散\n2、不是关于原点对称\n3、计算exp比较耗时\n-\nTanh\ntanh(x)=2σ(2x)−1tanh⁡(x)=2σ(2x)−1\n梯度弥散没解决\n1、解决了原点对称问题\n2、比sigmoid更快\nReLU\nf(x)=max(0,x)f(x)=max(0,x)\n梯度弥散没完全解决，在（-）部分相当于神经元死亡而且不会复活\n1、解决了部分梯度弥散问题\n2、收敛速度更快\nLeaky ReLU\nf(x)=1(x<0)(αx)+1(x>=0)(x)f(x)=1(x<0)(αx)+1(x>=0)(x)\n-\n解决了神经死亡问题\nMaxout\nmax(wT1x+b1,wT2x+b2)max(w1Tx+b1,w2Tx+b2)\n参数比较多,本质上是在输出结果上又增加了一层\n克服了ReLU的缺点，比较提倡使用\n（4）参数更新方法\n方法名称\n公式\nVanilla update\nx += - learning_rate * dx\nMomentum update动量更新\nv = mu * v - learning_rate * dx # integrate velocity\nx += v # integrate position\nNesterov Momentum\nx_ahead = x + mu * v\nv = mu * v - learning_rate * dx_ahead\nx += v\nAdagrad\n(自适应的方法，梯度大的方向学习率越来越小,由快到慢)\ncache += dx**2\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\nAdam\nm = beta1*m + (1-beta1)dx\nv = beta2*v + (1-beta2)(dx**2)\nx += - learning_rate * m / (np.sqrt(v) + eps)\n（5）解决overfitting的方法\ndropout， regularization， batch normalizatin，但是要注意dropout只在训练的时候用，让一部分神经元随机失活。\nBatch normalization是为了让输出都是单位高斯激活，方法是在连接和激活函数之间加入BatchNorm层，计算每个特征的均值和方差进行规则化。\n2、CNN问题\n（1） 思想\n改变全连接为局部连接，这是由于图片的特殊性造成的（图像的一部分的统计特性与其他部分是一样的），通过局部连接和参数共享大范围的减少参数值。可以通过使用多个filter来提取图片的不同特征（多卷积核）。\n（2）filter尺寸的选择\n通常尺寸多为奇数（1，3，5，7）\n（3）输出尺寸计算公式\n输出尺寸=(N - F +padding*2)/stride + 1\n步长可以自由选择通过补零的方式来实现连接。\n（4）pooling池化的作用\n虽然通过.卷积的方式可以大范围的减少输出尺寸（特征数），但是依然很难计算而且很容易过拟合，所以依然利用图片的静态特性通过池化的方式进一步减少尺寸。\n（5）常用的几个模型，这个最好能记住模型大致的尺寸参数。\n名称\n特点\nLeNet5\n–没啥特点-不过是第一个CNN应该要知道\nAlexNet\n引入了ReLU和dropout，引入数据增强、池化相互之间有覆盖，三个卷积一个最大池化+三个全连接层\nVGGNet\n采用1*1和3*3的卷积核以及2*2的最大池化使得层数变得更深。常用VGGNet-16和VGGNet19\nGoogle Inception Net\n我称为盗梦空间网络\n这个在控制了计算量和参数量的同时，获得了比较好的分类性能，和上面相比有几个大的改进：\n1、去除了最后的全连接层，而是用一个全局的平均池化来取代它；\n2、引入Inception Module，这是一个4个分支结合的结构。所有的分支都用到了1*1的卷积，这是因为1*1性价比很高，可以用很少的参数达到非线性和特征变换。\n3、Inception V2第二版将所有的5*5变成2个3*3，而且提出来著名的Batch Normalization；\n4、Inception V3第三版就更变态了，把较大的二维卷积拆成了两个较小的一维卷积，加速运算、减少过拟合，同时还更改了Inception Module的结构。\n微软ResNet残差神经网络(Residual Neural Network)\n1、引入高速公路结构，可以让神经网络变得非常深\n2、ResNet第二个版本将ReLU激活函数变成y=x的线性函数\n2、RNN\n1、RNN原理：\n在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(Feed-forward+Neural+Networks)。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出。所以叫循环神经网络\n2、RNN、LSTM、GRU区别\nRNN引入了循环的概念，但是在实际过程中却出现了初始信息随时间消失的问题，即长期依赖（Long-Term Dependencies）问题，所以引入了LSTM。\nLSTM：因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。推导forget gate，input gate，cell state， hidden information等因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸的变化是关键，下图非常明确适合记忆：\nGRU是LSTM的变体，将忘记门和输入们合成了一个单一的更新门。\n3、LSTM防止梯度弥散和爆炸\nLSTM用加和的方式取代了乘积，使得很难出现梯度弥散。但是相应的更大的几率会出现梯度爆炸，但是可以通过给梯度加门限解决这一问题。\n4、引出word2vec\n这个也就是Word Embedding，是一种高效的从原始语料中学习字词空间向量的预测模型。分为CBOW(Continous Bag of Words)和Skip-Gram两种形式。其中CBOW是从原始语句推测目标词汇，而Skip-Gram相反。CBOW可以用于小语料库，Skip-Gram用于大语料库。具体的就不是很会了。\n3、GAN\n1、GAN的思想\nGAN结合了生成模型和判别模型，相当于矛与盾的撞击。生成模型负责生成最好的数据骗过判别模型，而判别模型负责识别出哪些是真的哪些是生成模型生成的。但是这些只是在了解了GAN之后才体会到的，但是为什么这样会有效呢？\n假设我们有分布Pdata(x)，我们希望能建立一个生成模型来模拟真实的数据分布，假设生成模型为Pg(x;θθ)，我们的目的是求解θθ的值，通常我们都是用最大似然估计。但是现在的问题是由于我们相用NN来模拟Pdata(x)，但是我们很难求解似然函数，因为我们没办法写出生成模型的具体表达形式，于是才有了GAN，也就是用判别模型来代替求解最大似然的过程。\n在最理想的状态下，G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此D(G(z)) = 0.5。这样我们的目的就达成了：我们得到了一个生成式的模型G，它可以用来生成图片。\n2、GAN的表达式\n通过分析GAN的表达可以看出本质上就是一个minmax问题。其中V(D, G)可以看成是生成模型和判别模型的差异，而minmaxD说的是最大的差异越小越好。这种度量差异的方式实际上叫做Jensen-Shannon divergence。\n3、GAN的实际计算方法\n因为我们不可能有Pdata(x)的分布，所以我们实际中都是用采样的方式来计算差异（也就是积分变求和）。具体实现过程如下：\n有几个关键点：判别方程训练K次，而生成模型只需要每次迭代训练一次，先最大化（梯度上升）再最小化（梯度下降）。\n但是实际计算时V的后面一项在D(x)很小的情况下由于log函数的原因会导致更新很慢，所以实际中通常将后一项的log(1-D(x))变为-logD(x)。\n实际计算的时候还发现不论生成器设计的多好，判别器总是能判断出真假，也就是loss几乎都是0，这可能是因为抽样造成的，生成数据与真实数据的交集过小，无论生成模型多好，判别模型也能分辨出来。解决方法有两个：1、用WGAN 2、引入随时间减少的噪声\n4、对GAN有一些改进有引入f-divergence，取代Jensen-Shannon divergence，还有很多，这里主要介绍WGAN\n5、WGAN\n上面说过了用f-divergence来衡量两个分布的差异，而WGAN的思路是使用Earth Mover distance (挖掘机距离 Wasserstein distance)。\n第二部分、机器学习准备\n1、决策树树相关问题\n（1）各种熵的计算\n熵、联合熵、条件熵、交叉熵、KL散度（相对熵）\n熵用于衡量不确定性，所以均分的时候熵最大\nKL散度用于度量两个分布的不相似性，KL(p||q)等于交叉熵H(p,q)-熵H(p)。交叉熵可以看成是用q编码P所需的bit数，减去p本身需要的bit数，KL散度相当于用q编码p需要的额外bits。\n交互信息Mutual information ：I(x,y) = H(x)-H(x|y) = H(y)-H(y|x) 表示观察到x后，y的熵会减少多少。\n（2）常用的树搭建方法：ID3、C4.5、CART\n上述几种树分别利用信息增益、信息增益率、Gini指数作为数据分割标准。\n其中信息增益衡量按照某个特征分割前后熵的减少程度，其实就是上面说的交互信息。\n用上述信息增益会出现优先选择具有较多属性的特征，毕竟分的越细的属性确定性越高。所以提出了信息增益率的概念，让含有较多属性的特征的作用降低。\nCART树在分类过程中使用的基尼指数Gini，只能用于切分二叉树，而且和ID3、C4.5树不同，Cart树不会在每一个步骤删除所用特征。\n（3）防止过拟合：剪枝\n剪枝分为前剪枝和后剪枝，前剪枝本质就是早停止，后剪枝通常是通过衡量剪枝后损失函数变化来决定是否剪枝。后剪枝有：错误率降低剪枝、悲观剪枝、代价复杂度剪枝\n（4）前剪枝的几种停止条件\n节点中样本为同一类\n特征不足返回多类\n如果某个分支没有值则返回父节点中的多类\n样本个数小于阈值返回多类\n2、逻辑回归相关问题\n（1）公式推导一定要会\n（2）逻辑回归的基本概念\n这个最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。\n（3）L1-norm和L2-norm\n其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。\n但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。\n（4）LR和SVM对比\n首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss。\n其次，两者都是线性模型。\n最后，SVM只考虑支持向量（也就是和分类相关的少数点）\n（5）LR和随机森林区别\n随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。\n（6）常用的优化方法\n逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。\n一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。\n二阶方法：牛顿法、拟牛顿法：\n这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。\n拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。\n3、SVM相关问题\n（1）带核的SVM为什么能分类非线性问题？\n核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积\n（2）RBF核一定是线性可分的吗\n不一定，RBF核比较难调参而且容易出现维度灾难，要知道无穷维的概念是从泰勒展开得出的。\n（3）常用核函数及核函数的条件：\n核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。\nRBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。\n线性核：主要用于线性可分的情况\n多项式核\n（4）SVM的基本思想：\n间隔最大化来得到最优分离超平面。方法是将这个问题形式化为一个凸二次规划问题，还可以等价位一个正则化的合页损失最小化问题。SVM又有硬间隔最大化和软间隔SVM两种。这时首先要考虑的是如何定义间隔，这就引出了函数间隔和几何间隔的概念（这里只说思路），我们选择了几何间隔作为距离评定标准（为什么要这样，怎么求出来的要知道），我们希望能够最大化与超平面之间的几何间隔x，同时要求所有点都大于这个值，通过一些变化就得到了我们常见的SVM表达式。接着我们发现定义出的x只是由个别几个支持向量决定的。对于原始问题（primal problem）而言，可以利用凸函数的函数包来进行求解，但是发现如果用对偶问题（dual ）求解会变得更简单，而且可以引入核函数。而原始问题转为对偶问题需要满足KKT条件（这个条件应该细细思考一下）到这里还都是比较好求解的。因为我们前面说过可以变成软间隔问题，引入了惩罚系数，这样还可以引出hinge损失的等价形式（这样可以用梯度下降的思想求解SVM了）。我个人认为难的地方在于求解参数的SMO算法。\n（5）是否所有的优化问题都可以转化为对偶问题：\n这个问题我感觉非常好，有了强对偶和弱对偶的概念。用知乎大神的解释吧\n（6）处理数据偏斜：\n可以对数量多的类使得惩罚系数C越小表示越不重视，相反另数量少的类惩罚系数变大。\n4、Boosting和Bagging\n（1）随机森林\n随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：1、Boostrap从袋内有放回的抽取样本值2、每次随机抽取一定数量的特征（通常为sqr(n)）。\n分类问题：采用Bagging投票的方式选择类别频次最高的\n回归问题：直接取每颗树结果的平均值。\n常见参数\n误差分析\n优点\n缺点\n1、树最大深度\n2、树的个数\n3、节点上的最小样本数\n4、特征数(sqr(n))\noob(out-of-bag)\n将各个树的未采样样本作为预测样本统计误差作为误分率\n可以并行计算\n不需要特征选择\n可以总结出特征重要性\n可以处理缺失数据\n不需要额外设计测试集\n在回归上不能输出连续结果\n（2）Boosting之AdaBoost\nBoosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。\n（3）Boosting之GBDT\n将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。\n注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。\n（4）GBDT和Random Forest区别\n这个就和上面说的差不多。\n（5）Xgboost\n这个工具主要有以下几个特点：\n支持线性分类器\n可以自定义损失函数，并且可以用二阶偏导\n加入了正则化项：叶节点数、每个叶节点输出score的L2-norm\n支持特征抽样\n在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。\n5、KNN和Kmean\n（1）KNN 和Kmean缺点\n都属于惰性学习机制，需要大量的计算距离过程，速度慢的可以（但是都有相应的优化方法）。\n（2）KNN\nKNN不需要进行训练，只要对于一个陌生的点利用离其最近的K个点的标签判断其结果。KNN相当于多数表决，也就等价于经验最小化。而KNN的优化方式就是用Kd树来实现。\n（3）Kmean\n要求自定义K个聚类中心，然后人为的初始化聚类中心，通过不断增加新点变换中心位置得到最终结果。Kmean的缺点可以用Kmean++方法进行一些解决（思想是使得初始聚类中心之间的距离最大化）\n6、EM算法、HMM、CRF\n这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。\n（1）EM算法\nEM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。\n注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。\n（2）HMM算法\n隐马尔可夫模型是用于标注问题的生成模型。有几个参数（ππ，A，B）：初始状态概率向量ππ，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。\n马尔科夫三个基本问题：\n概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法\n学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。\n预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）\n（3）条件随机场CRF\n给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。\n之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。\n（4）HMM和CRF对比\n其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。\n7、常见基础问题\n（1）数据归一化（或者标准化，注意归一化和标准化不同）的原因\n要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。\n有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。\n有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。\n补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。\n（2）衡量分类器的好坏：\n这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。\n几种常用的指标：\n精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）\n召回率 recall = TP/(TP+FN) = TP/ P\nF1值： 2/F1 = 1/recall + 1/precision\nROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N\n（3）SVD和PCA\nPCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。\n（4）防止过拟合的方法\n过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。\n处理方法：\n早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练\n数据集扩增：原有数据增加、原有数据加随机噪声、重采样\n正则化\n交叉验证\n特征选择/特征降维\n（5）数据不平衡问题\n这主要是由于数据分布不平衡造成的。解决方法如下：\n采样，对小样本加噪声采样，对大样本进行下采样\n进行特殊的加权，如在Adaboost中或者SVM中\n采用对不平衡数据集不敏感的算法\n改变评价标准：用AUC/ROC来进行评价\n采用Bagging/Boosting/ensemble等方法\n考虑数据的先验分布"}
{"content2":"今天看到一篇文章  Google’s Image Classification Model is now Free to Learn\n说是狗狗的机器学习速成课程（Machine Learning Crash Course）现在可以免费学习啦，因为一开始年初的时候是内部使用的，后来开放给大众了。大家有谁对不作恶家的机器学习感兴趣的话，可以点击连接去看看。\n但是以上不是我说的重点。\n说狗狗的原因，是为了引出我大微软的机器学习。\n在2018年3月7日，在Windows开发者日活动中，微软宣布推出Windows人工智能平台Windows ML。\nML means machine learning, not make love. Understand???\n在Windows ML平台下，开发人员能够将不同的AI平台导入现有的学习模型，并在安装了Windows10系统的PC设备上使用预先培训的ML模型，并利用CPU和GPU（AMD，Intel，NVIDIA、Qualcomm）硬件进行加速，而非云端。从而加快对本地图像及视频数据的实时分析，甚至是后台任务的改进。\n此外该技术支持ONNX格式的ML模型行业标准，开发者能够添加ONNX文件至UWP应用中，在并项目中生成模型界面。\n目前微软已将自家的AI技术融入进了Office 365、Windows 10 照片中，甚至还使用了Windows Hello面部识别技术，来替换传统的开机密码。\n看看你看，这么牛B的技术，我们怎么不来尝鲜呢。不过也不鲜了，已经过去仨月了。但是哪一家的技术不是先画一个饼，过很久你才能看到样品。哈哈。\n现在学习ML还来得及。\n在操作之前，先来说一下需要什么配置吧。\n1. Windows 10 1803 或者更高\n2. Visual Studio 15.7.1或更高\n3. Microsoft Visual Studio Tools for AI，在工具——扩展和更新 里面搜索AI即可找到。\nOK，大体说一下流程。\n1. 创建和训练机器学习的模型\n要实现对某一张图像的辨别，首先我们需要用一些数据来训练机器，告诉它这个是啥。也就是加标签tag.\n比如，之前微软的小冰识狗，那你得首先找很多狗的照片吧，你要是拿猫的照片来训练机器，告诉它这是狗，也不是不可以。因为历史上也有指鹿为马的故事呢。当然在一个很大数据下，比如你拿了10万张狗的图片，里面有那么几张是猫的，鸡的图片，这样训练出来也没事。因为机器会在训练之后给你一个数据让你参考。在数据很大的前提下，允许小错的。\n2. 代码实战\n用代码来实现一下，并且随机挑一张照片，叫机器辨别它是个啥。因为机器刚才学习了啊，如果他认识，那么就会给出相应的可能性大小。\n1. 创建和训练机器学习的模型\n用你的Microsoft账号登陆 https://www.customvision.ai/projects, ，创建项目，类型就选择图像分类，Domains领域选择了General（Compact），带Compact是可以到处到Android和ios上用模型\n接下来你会看到下图，你可以先加标签tag，在给标签添加相应的图像。也可以先加图像，然后新加标签的。\n我先训练一个川普出来试试，\n你可以多加几个标签。我一共做了两个。一个是川普，一个是一种花，一年蓬。\n等把标签和对应的图像都上传完毕后，点击上面的【训练】\n然后训练结果马上就出来了。\n第一个Precision，表示模型包含的标签预测的精度，越大越好。\n第一个Recall，模型标签外的预测精度，也是越大越好。\n当然，你也可以现在试验一下。点击右上方的Quick Test，即可测试。。\n然后，点击正上方的Export，导出模型。支持4种格式，Android，Ios，ONNX，DockFile。我们选择WIndows标准的ONNX。好了。第一步基本结束。很简单，都是点几下就搞定。\n如果你好奇ONNX里面是啥样子，那么恭喜你，你很好学。去 https://github.com/lutzroeder/Netron 下载一个软件，看看吧。\n2. 代码实战\n模型做好了，就该写代码了。代码也不多，很简单滴。\n新建一个UWP 程序，在Assets资产文件夹里面，添加刚才下载的ONNX文件（该文件可以随意重命名，也最好Rename一下，不然文件名字太长了），设置它的生成操作为【Content 内容】。\n这是你会发现，多了一个.cs类。\n打开Vincent.cs看看啊，没错，又是有点乱。改一下咯\nusing System; using System.Collections.Generic; using System.Threading.Tasks; using Windows.Media; using Windows.Storage; using Windows.AI.MachineLearning.Preview; // e6c82f6e-c60f-422a-97b6-e0406cba82da_6ed0259c-001e-4895-be7a-4a930321a307 namespace VincentML { public sealed class ModelInput { public VideoFrame data { get; set; } } public sealed class ModelOutput { public IList<string> classLabel { get; set; } public IDictionary<string, float> loss { get; set; } public ModelOutput() { this.classLabel = new List<string>(); this.loss = new Dictionary<string, float>() { { \"Donald Trump\", float.NaN }, { \"Yinianpeng\", float.NaN }, }; } } public sealed class Model { private LearningModelPreview learningModel; public static async Task<Model> CreateModel(StorageFile file) { LearningModelPreview learningModel = await LearningModelPreview.LoadModelFromStorageFileAsync(file); Model model = new Model(); model.learningModel = learningModel; return model; } public async Task<ModelOutput> EvaluateAsync(ModelInput input) { ModelOutput output = new ModelOutput(); LearningModelBindingPreview binding = new LearningModelBindingPreview(learningModel); binding.Bind(\"data\", input.data); binding.Bind(\"classLabel\", output.classLabel); binding.Bind(\"loss\", output.loss); LearningModelEvaluationResultPreview evalResult = await learningModel.EvaluateAsync(binding, string.Empty); return output; } } }\n好，接下来写一个简单的界面，一个图像Image和一个按钮Button，一个文本TextBlock\n<Grid> <Grid> <Grid.RowDefinitions> <RowDefinition/> <RowDefinition Height=\"Auto\"/> <RowDefinition Height=\"Auto\"/> </Grid.RowDefinitions> <Image x:Name=\"image\"/> <TextBlock Grid.Row=\"1\" x:Name=\"tbResult\" HorizontalAlignment=\"Center\"/> <Button Grid.Row=\"2\" Content=\"Choose a picture\" HorizontalAlignment=\"Center\" Click=\"ChooseImage\"/> </Grid> </Grid>\n主要看后台代码ChooseImage。\n龙宫分四步：\n1. 加载模型\n2. 选择一个图片\n3. 设置模型的输入数据\n4. 输出结果\n//1. 加载模型\nStorageFile modelDile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($\"ms-appx:///Assets/Vincent.onnx\")); Model model = await Model.CreateModel(modelDile);\n//2. 选择一个图片 FileOpenPicker picker = new FileOpenPicker(); picker.FileTypeFilter.Add(\".jpg\"); picker.FileTypeFilter.Add(\".jpeg\"); picker.FileTypeFilter.Add(\".png\"); picker.FileTypeFilter.Add(\".bmp\"); picker.SuggestedStartLocation = PickerLocationId.PicturesLibrary; var file = await picker.PickSingleFileAsync(); if (file != null) {\nBitmapImage src = new BitmapImage();\nusing (IRandomAccessStream stream = await file.OpenAsync(FileAccessMode.Read))\n{\nawait src.SetSourceAsync(stream);\nstream.Dispose();\n};\nimage.Source = src;\n//3. 设置模型的输入数据 ModelInput modelInput = new ModelInput(); modelInput.data = await GetVideoFrame(file);\n//4. 输出结果 ModelOutput modelOutput = await model.EvaluateAsync(modelInput); var topCategory = modelOutput.loss.OrderByDescending(kvp => kvp.Value).FirstOrDefault().Key; }\n注意一下，ModelInput的输如数据类型是VideoFrame，所以需要将图片转换一下。\nprivate async Task<VideoFrame> GetVideoFrame(StorageFile file) { SoftwareBitmap softwareBitmap; using (IRandomAccessStream stream = await file.OpenAsync(FileAccessMode.Read)) { // Create the decoder from the stream BitmapDecoder decoder = await BitmapDecoder.CreateAsync(stream); // Get the SoftwareBitmap representation of the file in BGRA8 format softwareBitmap = await decoder.GetSoftwareBitmapAsync(); softwareBitmap = SoftwareBitmap.Convert(softwareBitmap, BitmapPixelFormat.Bgra8, BitmapAlphaMode.Premultiplied); return VideoFrame.CreateWithSoftwareBitmap(softwareBitmap); } }\n好了，看一下咋样，运行一下。\n我还特地找了一张川总很酷的发型图\n如果你选择了一个别的照片，比如狗，会得到这样的。\n但是你非要说这条狗就叫Donald Trump，那我无F*ck可说了。\n最后，欢迎大家去全球最大的同性恋交友平台Fork/Star我的项目：https://github.com/hupo376787/MachineLearningOnUWP"}
{"content2":"------------------------------------本博客所有内容以学习、研究和分享为主，如需转载，请联系本人，标明作者和出处，并且是非商业用途，谢谢！--------------------------------\n想写这个系列很久了，最近刚好项目结束了闲下来有点时间，于是决定把之前学过的东西做个总结。之前看过一些机器学习方面的书，每本书都各有侧重点，机器学习实战和集体智慧编程更偏向与实战，侧重于对每个算法的实际操作过程，但是没有对整个数据挖掘项目做介绍，李航老师的统计学习方法和周志华老师的机器学习这两本书侧重对原理的讲解和公式的推导，但是实战方面可能会少一点。我结合之前看过的书，以及自己的一些项目经验做了一些总结，一是回顾自己还有哪些遗漏，二是希望给新入门的同学一个参考。至于编程语言，主要用python，也会有少部分R，java和scala之类，毕竟实际项目中也不可能使用一种语言。此外，本系列所用到的所有数据我会传到Github上，需要的同学可以自行下载。为保证文章质量，每周二周四更新，下面是主要的目录（可能会根据实际情况调整）：\n第一部分 模型的评估与数据处理\n机器学习基础与实践（一）----数据清洗\n机器学习基础与实践（二）----数据转换\n机器学习基础与实践（三）----数据降维\n第二部分 特征工程\n机器学习基础与实践（四）----特征选择\n机器学习基础与实践（五）----特征提取\n机器学习基础与实践（六）----模型选择与评估\n第三部分 算法基础之有监督算法\n机器学习基础与实践（七）----广义线性模型\n机器学习基础与实践（八）----最小二乘法\n机器学习基础与实践（九）----LDA\n机器学习基础与实践（十）----SGD\n机器学习基础与实践（十一）----K近邻\n机器学习基础与实践（十二）----高斯过程\n机器学习基础与实践（十三）----决策树（ID3，C4.5，C5.0，CART）\n机器学习基础与实践（十四）----朴素贝叶斯\n机器学习基础与实践（十五）----支持向量机\n机器学习基础与实践（十六）----集成学习（Bagging，RF，AdaBoost，Gradient Tree Boosting，Voting Classifier）\n机器学习基础与实践（十七）----感知机模型\n机器学习基础与实践（十八）----多分类算法\n第四部分 算法基础之无监督算法\n机器学习基础与实践（十九）----K-means\n机器学习基础与实践（二十）----Affinity propagation\n机器学习基础与实践（二十一）----Mean-shift\n机器学习基础与实践（二十二）----Spectral clustering\n机器学习基础与实践（二十三）----Ward hierachical\n机器学习基础与实践（二十四）----Agglomerative clustering\n机器学习基础与实践（二十五）----DBSCAN\n机器学习基础与实践（二十六）----Gaussian mixtures\n机器学习基础与实践（二十七）----Birch\n第五部分 算法基础之推荐算法\n机器学习基础与实践（二十八）----相似度计算\n机器学习基础与实践（二十九）----Arules关联规则\n机器学习基础与实践（三十）----Fp-Growth\n机器学习基础与实践（三十一）----User-based or Item-based\n第六部分 算法基础之半监督模型\n机器学习基础与实践（三十二）----Label Propagation\n第七部分 算法基础之其他模型\n机器学习基础与实践（三十三）----概率图模型\n机器学习基础与实践（三十四）----最大熵模型\n机器学习基础与实践（三十五）----规则学习\n机器学习基础与实践（三十六）----强化学习\n机器学习基础与实践（三十七）----条件随机场\n机器学习基础与实践（三十八）----保序回归（Isotonic regression）\n机器学习基础与实践（三十九）----Probability calibration\n正文：\n按照我做项目的经验，来了项目，首先是分析项目的目的和需求，了解这个项目属于什么问题，要达到什么效果。然后提取数据，做基本的数据清洗。第三步是特征工程，这个属于脏活累活，需要耗费很大的精力，如果特征工程做的好，那么，后面选择什么算法其实差异不大，反之，不管选择什么算法，效果都不会有突破性的提高。第四步，是跑算法，通常情况下，我会把所有能跑的算法先跑一遍，看看效果，分析一下precesion/recall和f1-score，看看有没有什么异常（譬如有好几个算法precision特别好，但是recall特别低，这就要从数据中找原因，或者从算法中看是不是因为算法不适合这个数据），如果没有异常，那么就进行下一步，选择一两个跑的结果最好的算法进行调优。调优的方法很多，调整参数的话可以用网格搜索、随机搜索等，调整性能的话，可以根据具体的数据和场景进行具体分析。调优后再跑一边算法，看结果有没有提高，如果没有，找原因，数据 or 算法？是数据质量不好，还是特征问题还是算法问题。一个一个排查，找解决方法。特征问题就回到第三步再进行特征工程，数据质量问题就回到第一步看数据清洗有没有遗漏，异常值是否影响了算法的结果，算法问题就回到第四步，看算法流程中哪一步出了问题。如果实在不行，可以搜一下相关的论文，看看论文中有没有解决方法。这样反复来几遍，就可以出结果了，写技术文档和分析报告，再向业务人员或产品讲解我们做的东西，然后他们再提建议/该需求，不断循环，最后代码上线，改bug，直到结项。\n直观来看，可以用一个流程图来表示：\n今天讲数据清洗，为什么要进行数据清洗呢？我们在书上看到的数据，譬如常见的iris数据集，房价数据，电影评分数据集等等，数据质量都很高，没有缺失值，没有异常点，也没有噪音，而在真实数据中，我们拿到的数据可能包含了大量的缺失值，可能包含大量的噪音，也可能因为人工录入错误导致有异常点存在，对我们挖据出有效信息造成了一定的困扰，所以我们需要通过一些方法，尽量提高数据的质量。数据清洗一般包括以下几个步骤：\n一.分析数据\n二.缺失值处理\n三.异常值处理\n四.去重处理\n五.噪音数据处理\n六.一些实用的数据处理小工具\n一.分析数据\n在实际项目中，当我们确定需求后就会去找相应的数据，拿到数据后，首先要对数据进行描述性统计分析，查看哪些数据是不合理的，也可以知道数据的基本情况。如果是销售额数据可以通过分析不同商品的销售总额、人均消费额、人均消费次数等，同一商品的不同时间的消费额、消费频次等等，了解数据的基本情况。此外可以通过作图的方式了解数据的质量，有无异常点，有无噪音等。举个例子（这里数据较少，就直接用R作图了）：\n1 #一组年薪超过10万元的经理收入 2 pay=c(11,19,14,22,14,28,13,81,12,43,11,16,31,16,23.42,22,26,17,22,13,27,180,16,43,82,14,11,51,76,28,66,29,14,14,65,37,16,37,35,39,27,14,17,13,38,28,40,85,32,25,26,16,12,54,40,18,27,16,14,33,29,77,50,19,34) 3 par(mfrow=c(2,2))#将绘图窗口改成2*2，可同时显示四幅图 4 hist(pay)#绘制直方图 5 dotchart(pay)#绘制点图 6 barplot(pay,horizontal=T)#绘制箱型图 7 qqnorm(pay);qqline(pay)#绘制Q-Q图\n从上面四幅图可以很清楚的看出，180是异常值，即第23个数据需要清理。\npython中也包含了大量的统计命令，其中主要的统计特征函数如下图所示：\n二.缺失值处理\n缺失值在实际数据中是不可避免的问题，有的人看到有缺失的数据就直接删除了，有的人直接赋予0值或者某一个特殊的值，那么到底该怎么处理呢？对于不同的数据场景应该采取不同的策略，首先应该判断缺失值的分布情况：\n1 import scipy as sp 2 data = sp.genfromtxt(\"web_traffic.tsv\",delimiter = \"\\t\")\n数据情况如下：\n1 >>>data 2 array([[ 1.00000000e+00, 2.27200000e+03], 3 [ 2.00000000e+00, nan], 4 [ 3.00000000e+00, 1.38600000e+03], 5 ..., 6 [ 7.41000000e+02, 5.39200000e+03], 7 [ 7.42000000e+02, 5.90600000e+03], 8 [ 7.43000000e+02, 4.88100000e+03]]) 9 10 >>> print data[:10] 11 [[ 1.00000000e+00 2.27200000e+03] 12 [ 2.00000000e+00 nan] 13 [ 3.00000000e+00 1.38600000e+03] 14 [ 4.00000000e+00 1.36500000e+03] 15 [ 5.00000000e+00 1.48800000e+03] 16 [ 6.00000000e+00 1.33700000e+03] 17 [ 7.00000000e+00 1.88300000e+03] 18 [ 8.00000000e+00 2.28300000e+03] 19 [ 9.00000000e+00 1.33500000e+03] 20 [ 1.00000000e+01 1.02500000e+03]] 21 22 >>> data.shape 23 (743, 2)\n可以看到，第2列已经出现了缺失值，现在我们来看一下缺失值的数量：\n1 >>> x = data[:,0] 2 >>> y = data[:,1] 3 >>> sp.sum(sp.isnan(y)) 4 8\n在743个数据里只有8个数据缺失，所以删除它们对于整体数据情况影响不大。当然，这是缺失值少的情况下，在缺失值值比较多，而这个维度的信息还很重要的时候（因为缺失值如果占了95%以上，可以直接去掉这个维度的数据了），直接删除会对后面的算法跑的结果造成不好的影响。我们常用的方法有以下几种：\n1.直接删除----适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况\n2.使用一个全局常量填充---譬如将缺失值用“Unknown”等填充，但是效果不一定好，因为算法可能会把它识别为一个新的类别，一般很少用\n3.使用均值或中位数代替----优点：不会减少样本信息，处理简单。缺点：当缺失数据不是随机数据时会产生偏差.对于正常分布的数据可以使用均值代替，如果数据是倾斜的，使用中位数可能更好。\n4.插补法\n1）随机插补法----从总体中随机抽取某个样本代替缺失样本\n2）多重插补法----通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理\n3）热平台插补----指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。\n优点：简单易行，准去率较高\n缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补\n4)拉格朗日差值法和牛顿插值法（简单高效，数值分析里的内容，数学公式以后再补 = =）\n5.建模法\n可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。\n以上方法各有优缺点，具体情况要根据实际数据分分布情况、倾斜程度、缺失值所占比例等等来选择方法。一般而言，建模法是比较常用的方法，它根据已有的值来预测缺失值，准确率更高。\n三.异常值处理\n异常值我们通常也称为“离群点”。在讲分析数据时，我们举了个例子说明如何发现离群点，除了画图（画图其实并不常用，因为数据量多时不好画图，而且慢），还有很多其他方法:\n1.简单的统计分析\n拿到数据后可以对数据进行一个简单的描述性统计分析，譬如最大最小值可以用来判断这个变量的取值是否超过了合理的范围，如客户的年龄为-20岁或200岁，显然是不合常理的，为异常值。\n在python中可以直接用pandas的describe()：\n1 >>> import pandas as pd 2 >>> data = pd.read_table(\"web_traffic.tsv\",header = None) 3 >>> data.describe() 4 0 1 5 count 743.000000 735.000000 6 mean 372.000000 1962.165986 7 std 214.629914 860.720997 8 min 1.000000 472.000000 9 25% 186.500000 1391.000000 10 50% 372.000000 1764.000000 11 75% 557.500000 2217.500000 12 max 743.000000 5906.000000\n2.3∂原则\n如果数据服从正态分布，在3∂原则下，异常值为一组测定值中与平均值的偏差超过3倍标准差的值。如果数据服从正态分布，距离平均值3∂之外的值出现的概率为P(|x-u| > 3∂) <= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。\n3.箱型图分析\n箱型图提供了识别异常值的一个标准：如果一个值小于QL01.5IQR或大于OU-1.5IQR的值，则被称为异常值。QL为下四分位数，表示全部观察值中有四分之一的数据取值比它小；QU为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR为四分位数间距，是上四分位数QU与下四分位数QL的差值，包含了全部观察值的一半。箱型图判断异常值的方法以四分位数和四分位距为基础，四分位数具有鲁棒性：25%的数据可以变得任意远并且不会干扰四分位数，所以异常值不能对这个标准施加影响。因此箱型图识别异常值比较客观，在识别异常值时有一定的优越性。\n4.基于模型检测\n首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象\n优缺点：1.有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；2.对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。\n5.基于距离\n通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象\n优缺点：1.简单；2.缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；3.该方法对参数的选择也是敏感的；4.不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。\n6.基于密度\n当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。\n优缺点：1.给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；2.与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；3.参数选择困难。虽然算法通过观察不同的k值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。\n7.基于聚类：\n基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。\n优缺点：1.基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；2.簇的定义通常是离群点的补，因此可能同时发现簇和离群点；3.产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；4.聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。\n处理方法：\n1.删除异常值----明显看出是异常且数量较少可以直接删除\n2.不处理---如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的。\n3.平均值替代----损失信息小，简单高效。\n4.视为缺失值----可以按照处理缺失值的方法来处理\n四.去重处理\n以DataFrame数据格式为例：\n1 #创建数据，data里包含重复数据 2 >>> data = pd.DataFrame({'v1':['a']*5+['b']* 4,'v2':[1,2,2,2,3,4,4,5,3]}) 3 >>> data 4 v1 v2 5 0 a 1 6 1 a 2 7 2 a 2 8 3 a 2 9 4 a 3 10 5 b 4 11 6 b 4 12 7 b 5 13 8 b 3 14 15 #DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行 16 >>> data.duplicated() 17 0 False 18 1 False 19 2 True 20 3 True 21 4 False 22 5 False 23 6 True 24 7 False 25 8 False 26 dtype: bool 27 28 #drop_duplicates方法用于返回一个移除了重复行的DataFrame 29 >>> data.drop_duplicates() 30 v1 v2 31 0 a 1 32 1 a 2 33 4 a 3 34 5 b 4 35 7 b 5 36 8 b 3 37 38 #这两个方法默认会判断全部列，你也可以指定部分列进行重复项判断。假设你还有一列值，且只希望根据v1列过滤重复项： 39 >>> data['v3']=range(9) 40 >>> data 41 v1 v2 v3 42 0 a 1 0 43 1 a 2 1 44 2 a 2 2 45 3 a 2 3 46 4 a 3 4 47 5 b 4 5 48 6 b 4 6 49 7 b 5 7 50 8 b 3 8 51 >>> data.drop_duplicates(['v1']) 52 v1 v2 v3 53 0 a 1 0 54 5 b 4 5 55 56 #duplicated和drop_duplicates默认保留的是第一个出现的值组合。传入take_last=True则保留最后一个： 57 >>> data.drop_duplicates(['v1','v2'],take_last = True) 58 v1 v2 v3 59 0 a 1 0 60 3 a 2 3 61 4 a 3 4 62 6 b 4 6 63 7 b 5 7 64 8 b 3 8\n如果数据是列表格式的，有以下几种方法可以删除：\n1 list0=['b','c', 'd','b','c','a','a'] 2 3 方法1：使用set() 4 5 list1=sorted(set(list0),key=list0.index) # sorted output 6 print( list1) 7 8 方法2：使用 {}.fromkeys().keys() 9 10 list2={}.fromkeys(list0).keys() 11 print(list2) 12 13 方法3：set()+sort() 14 15 list3=list(set(list0)) 16 list3.sort(key=list0.index) 17 print(list3) 18 19 方法4：迭代 20 21 list4=[] 22 for i in list0: 23 if not i in list4: 24 list4.append(i) 25 print(list4) 26 27 方法5：排序后比较相邻2个元素的数据，重复的删除 28 29 def sortlist(list0): 30 list0.sort() 31 last=list0[-1] 32 for i in range(len(list0)-2,-1,-1): 33 if list0[i]==last: 34 list0.remove(list0[i]) 35 else: 36 last=list0[i] 37 return list0 38 39 print(sortlist(list0))\n五.噪音处理\n噪音，是被测量变量的随机误差或方差。我们在上文中提到过异常点（离群点），那么离群点和噪音是不是一回事呢？我们知道，观测量(Measurement) = 真实数据(True Data) + 噪声 (Noise)。离群点(Outlier)属于观测量，既有可能是真实数据产生的，也有可能是噪声带来的，但是总的来说是和大部分观测量之间有明显不同的观测值。。噪音包括错误值或偏离期望的孤立点值，但也不能说噪声点包含离群点，虽然大部分数据挖掘方法都将离群点视为噪声或异常而丢弃。然而，在一些应用（例如：欺诈检测），会针对离群点做离群点分析或异常挖掘。而且有些点在局部是属于离群点，但从全局看是正常的。\n我在quora上看到过一个解释噪音与离群点的有趣的例子：\nOutlier: you are enumerating meticulously everything you have. You found 3 dimes, 1 quarter and wow a 100 USD bill you had put there last time you bought some booze and had totally forgot there. The 100 USD bill is an outlier, as it's not commonly expected in a pocket.\nNoise: you have just come back from that club and are pretty much wasted. You try to find some money to buy something to sober up, but you have trouble reading the figures correctly on the coins. You found 3 dimes, 1 quarter and wow a 100 USD bill. But in fact, you have mistaken the quarter for a dime: this mistake introduces noise in the data you have access to.\nTo put it otherwise, data = true signal + noise. Outliers are part of the data.\n翻译过来就是：\n离群点： 你正在从口袋的零钱包里面穷举里面的钱，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。这个100元就是个离群点，因为并不应该常出现在口袋里..\n噪声： 你晚上去三里屯喝的酩酊大醉，很需要买点东西清醒清醒，这时候你开始翻口袋的零钱包，嘛，你发现了3个一角，1个五毛，和一张100元的毛爷爷向你微笑。但是你突然眼晕，把那三个一角看成了三个1元...这样错误的判断使得数据集中出现了噪声\n那么对于噪音我们应该如何处理呢？有以下几种方法：\n1.分箱法\n分箱方法通过考察数据的“近邻”（即，周围的值）来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。\n用箱均值光滑：箱中每一个值被箱中的平均值替换。\n用箱中位数平滑：箱中的每一个值被箱中的中位数替换。\n用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。\n一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用.\n2.  回归法\n可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。\n六.一些实用的数据处理小工具\n1.去掉文件中多余的空行\n空行主要指的是（\\n,\\r,\\r\\n,\\n\\r等），在python中有个strip()的方法，该方法可以去掉字符串两端多余的“空白”，此处的空白主要包括空格，制表符(\\t)，换行符。不过亲测以后发现，strip()可以匹配掉\\n,\\r\\n,\\n\\r等，但是过滤不掉单独的\\r。为了万无一失，我还是喜欢用麻烦的办法，如下：\n1 #-*- coding :utf-8 -*- 2 #文本格式化处理，过滤掉空行 3 4 file = open('123.txt') 5 6 i = 0 7 while 1: 8 line = file.readline().strip() 9 if not line: 10 break 11 i = i + 1 12 line1 = line.replace('\\r','') 13 f1 = open('filename.txt','a') 14 f1.write(line1 + '\\n') 15 f1.close() 16 print str(i)\n2.如何判断文件的编码格式\n1 #-*- coding:utf8 -*- 2 #批量处理编码格式转换（优化） 3 import os 4 import chardet 5 6 path1 = 'E://2016txtutf/' 7 def dirlist(path): 8 filelist = os.listdir(path) 9 for filename in filelist: 10 filepath = os.path.join(path, filename) 11 if os.path.isdir(filepath): 12 dirlist(filepath) 13 else: 14 if filepath.endswith('.txt'): 15 f = open(filepath) 16 data = f.read() 17 if chardet.detect(data)['encoding'] != 'utf-8': 18 print filepath + \"----\"+ chardet.detect(data)['encoding'] 19 20 dirlist(path1)\n3.文件编码格式转换，gbk与utf-8之间的转换\n这个主要是在一些对文件编码格式有特殊需求的时候，需要批量将gbk的转utf-8的或者将utf-8编码的文件转成gbk编码格式的。\n1 #-*- coding:gbk -*- 2 #批量处理编码格式转换 3 import codecs 4 import os 5 path1 = 'E://dir/' 6 def ReadFile(filePath,encoding=\"utf-8\"): 7 with codecs.open(filePath,\"r\",encoding) as f: 8 return f.read() 9 10 def WriteFile(filePath,u,encoding=\"gbk\"): 11 with codecs.open(filePath,\"w\",encoding) as f: 12 f.write(u) 13 14 def UTF8_2_GBK(src,dst): 15 content = ReadFile(src,encoding=\"utf-8\") 16 WriteFile(dst,content,encoding=\"gbk\") 17 18 def GBK_2_UTF8(src,dst): 19 content = ReadFile(src,encoding=\"gbk\") 20 WriteFile(dst,content,encoding=\"utf-8\") 21 22 def dirlist(path): 23 filelist = os.listdir(path) 24 for filename in filelist: 25 filepath = os.path.join(path, filename) 26 if os.path.isdir(filepath): 27 dirlist(filepath) 28 else: 29 if filepath.endswith('.txt'): 30 print filepath 31 #os.rename(filepath, filepath.replace('.txt','.doc')) 32 try: 33 UTF8_2_GBK(filepath,filepath) 34 except Exception,ex: 35 f = open('error.txt','a') 36 f.write(filepath + '\\n') 37 f.close() 38 39 dirlist(path1)\n刚写完比较粗糙，以后会不断修改。下篇写数据转换方面的内容，包括标准化，归一化正则化等。如果有错误，欢迎指正！\nps：声明一下，上周发现知乎大V某某大师抄袭我的【原】数据分析/数据挖掘/机器学习---- 必读书目，虽然只是自己平常读书的一些记录，但是也是我的文字，不想被抄袭，如需转载，请联系本人，署名并标明出处，且是非商业用途，谢谢~！\n参考文献：\n1.http://blog.csdn.net/u012950655/article/details/16946021\n2.http://my.oschina.net/dfsj66011/blog/601546\n3.http://www.cnblogs.com/nzyjlr/p/4174145.html\n4.https://www.quora.com/What-is-the-basic-difference-between-noise-and-outliers-in-Data-mining\n6.《数据挖掘：概念与技术》，Jiawei Han Micheline Kamber Jian Pei"}
{"content2":"第一章 机器学习的基础\n1.1编程语言与开发环境\n1.1.1 Python 安装（略）\n1.2.2 Python安装包的安装：可以选选择安装集成包anaconda（略）\n1.1.3 IDE配置及安装测试\nIDE选择UltraEdit高级文本编辑器，配置步骤如下：\n（1）选择“高级”-->“用户工具”命令，如.4所示。\n.5 配置UltraEdit步骤1\n（2）在如.5所示输入各项参数，然后单击“应用按钮”\n.5 配置UltraEdit步骤2\n（3）按照如.6所示进行设置，然后单击“确定”按钮\n.6 配置UltraEdit步骤3\n通过测试代码，检验安装效果：\n#coding:utf-8 #Filename:mytest1.py import numpy as np #导入NumPy库 from numpy import * #导入NumPy库 import matplotlib.pyplot as plt #测试数据集————二维list dataSet = [[1,2],[3,4],[5,6],[7,8],[9,10]] dataMat = mat(dataSet).T #将数据集转换为NumPy矩阵，并转秩 plt.scatter(dataMat[0],dataMat[1],c = 'red',marker = 'o') #绘制数据散点图 #绘制直线图形 X = np.linspace(-2,2,100) #产生直线数据 #建立线性方程 Y = 2.8*X+9 plt.plot(X,Y) #绘制直线图 plt.show() #显示绘制效果\n.7 显示执行效果\n1.2 对象、矩阵与矢量化编程\n1.2.1对象与维度（略）\n1.2.2初识矩阵（略）\n1.2.3矢量化编程与GPU运算（略）\n1.2.4理解数学公式与NumPy矩阵运算\n1.矩阵的初始化\n#coding:utf-8 import numpy as np #导入NumPy包 #创建3*5的全0矩阵和全1的矩阵 myZero = np.zeros([3,5])#3*5的全0矩阵 print myZero myZero = np.ones([3,5])##3*5的全1矩阵 print myZero\n输出结果：\nConnected to pydev debugger (build 141.1580) [[ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.]] [[ 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1.]]\n#生成随机矩阵 myRand = np.random.rand(3,4)#3行4列的0~1之间的随机数矩阵 print myRand 输出结果如下： [[ 0.14689327 0.15077077 0.88018968 0.75112348] [ 0.30944489 0.77563281 0.82905038 0.25430367] [ 0.53958541 0.89695376 0.90681161 0.25453046]]\n#单位阵 myEye = np.eye(3)#3*3的矩阵 print myEye 输出结果如下： [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.]]\n2.矩阵的元素运算\n矩阵的元素运算是指矩阵在元素级别的加减乘除运算。\n#元素的加和减：条件是矩阵的行数和列数必须相同 from numpy import *#导入NumPy包 myOnes = ones([3,3])#3*3的全1矩阵 myEye = eye（3） print myOnes+myEye print myOnes-myEye 输出结果如下： [[ 2. 1. 1.] [ 1. 2. 1.] [ 1. 1. 2.]] [[ 0. 1. 1.] [ 1. 0. 1.] [ 1. 1. 0.]]\n#矩阵乘法 mymatrix = mat([[1,2,3],[4,5,6],[7,8,9]]) a = 10 print a*mymatrix 输出结果： [[10 20 30] [40 50 60] [70 80 90]]\n#矩阵所有元素求和 mymatrix = mat([[1,2,3],[4,5,6],[7,8,9]]) print mymatrix.sum() 输出结果： 45\n''' 矩阵各元素的积：矩阵的点乘同维对应元素的相乘。 当矩阵的维度不同时，会根据一定的广播将维数扩 充到一致的形式 ''' mymatrix1 = mat([[1,2,3],[4,5,6],[7,8,9]]) mymatrix2 = 1.5*ones([3,3]) print multiply(mymatrix1,mymatrix2) 输出结果： [[ 1.5 3. 4.5] [ 6. 7.5 9. ] [ 10.5 12. 13.5]]\n#矩阵各元素的n次幂:n=2 mymatrix1 = mat([[1,2,3],[4,5,6],[7,8,9]]) print power(mymatrix1,2) 输出结果： [[ 1 4 9] [16 25 36] [49 64 81]]\n#矩阵乘以矩阵 mymatrix1 = mat([[1,2,3],[4,5,6],[7,8,9]]) mymatrix2 = mat([[1],[2],[3]]) print mymatrix1*mymatrix2 输出结果： [[14] [32] [50]]\n#矩阵的转置 mymatrix1 = mat([[1,2,3],[4,5,6],[7,8,9]]) print mymatrix1.T #矩阵的转置 mymatrix1.transpose() #矩阵的转置 print mymatrix1 输出结果如下： [[1 4 7] [2 5 8] [3 6 9]] [[1 2 3] [4 5 6] [7 8 9]]\nmymatrix = mymatrix1[0]#按行切片 print u\"按行切片：\",mymatrix mymatrix = mymatrix1.T[0]#按列切片 print u\"按列切片：\",mymatrix mymatrix = mymatrix1.copy()#矩阵的复制 print u\"复制矩阵：\",mymatrix #比较 print u\"矩阵元素的比较:\\n\",mymatrix<mymatrix1.T 输出结果： 矩阵的行数和列数： 3 3 按行切片： [[1 2 3]] 按列切片： [[1 4 7]] 复制矩阵： [[1 2 3] [4 5 6] [7 8 9]] 矩阵元素的比较: [[False True True] [False False True] [False False False]]\n1.2.5 Linalg线性代数库\n在矩阵的基本运算基础之上，NumPy的Linalg库可以满足大多数的线性代数运算。\n.矩阵的行列式\n.矩阵的逆\n.矩阵的对称\n.矩阵的秩\n.可逆矩阵求解线性方程\n1.矩阵的行列式\nIn [4]: from numpy import * In [5]: #n阶矩阵的行列式运算 In [6]: A = mat([[1,2,3],[4,5,6],[7,8,9]]) In [7]: print \"det(A):\",linalg.det(A) det(A): 6.66133814775e-16\n2.矩阵的逆\nIn [8]: from numpy import * In [9]: A = mat([[1,2,3],[4,5,6],[7,8,9]]) In [10]: invA = linalg.inv(A)#矩阵的逆 In [11]: print \"inv(A):\",invA inv(A): [[ -4.50359963e+15 9.00719925e+15 -4.50359963e+15] [ 9.00719925e+15 -1.80143985e+16 9.00719925e+15] [ -4.50359963e+15 9.00719925e+15 -4.50359963e+15]]\n3.矩阵的对称\nIn [12]: from numpy import * In [13]: A = mat([[1,2,3],[4,5,6],[7,8,9]]) In [14]: AT= A.T In [15]: print A*AT [[ 14 32 50] [ 32 77 122] [ 50 122 194]]\n4.矩阵的秩\nIn [16]: from numpy import * In [17]: A = mat([[1,2,3],[4,5,6],[7,8,9]]) In [18]: print linalg.matrix_rank(A)#矩阵的秩 2\n5.可逆矩阵求解\n1.3 机器学习的数学基础\n1.3.1 相似度的度量\n范数（来自百度百科）：向量的范数可以简单、形象的理解为长度，或者向量到坐标系原点的距离，或者相应空间内两点之间的距离。\n向量的范数定义：向量的范数是一个函数||x||,满足非负性||x||≥0，齐次性||cx|| = |c| ||x||,三角不等式||x+y||≤||x||+||y||。\nL1范数:||x||为x向量各个元素绝对值之和\nL2范数：||x||为x向量各个元素的平方和的开方。L2范数又称Euclidean范数或者Frobenius范数\nLp范数：||x||为x向量各个元素绝对值p次方和的1/p次方。\nL∞范数：||x||为x向量各个元素绝对值最大的元素，如下：\n向量范数的运算如下：\nIn [27]: A = [8,1,6] In [28]: #手工计算 In [29]: modA = sqrt(sum(power(A,2))) In [30]: #库函数 In [31]: normA = linalg.norm(A) In [32]: print \"modA:\",modA,\"norm(A):\",normA modA: 10.0498756211 norm(A): 10.0498756211\n1.3.2 各类距离的意义与Python代码的实现 　　本小节所列的距离公式列表和代码如下：\n•闵可夫斯基距离（Minkowski Distance）\n•欧式距离(Euclidean Distance)\n•曼哈顿距离(Manhattan Distance)\n•切比雪夫距离(Chebyshev Distance)\n•夹角余弦(Cosine)\n•汉明距离(Hamming Distance)\n•杰卡德相似系数(Jaccard Similiarity Coeffcient)\n1. 闵可夫斯基距离（Minkowski Distance）\n严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义。\n两个n维变量A（x11，x12,...,x1n）与B（x21，x22,...,x2n）间的闵可夫斯基距离定义为：\n其中p是一个变参数\n•当p=1时，就是曼哈顿距离\n•当p=2时，就是欧式距离\n•当p=∞时，就是切比雪夫距离\n根据变参数的不同，闵可供夫斯基可以表示一类的距离\n2.欧式距离（Euclidean Distance）\n欧氏距离（L2范数）是最易于理解的一种距离计算方法，源自欧式空间中两点间的距离公式，如图所示\n（1）二维平面上两点a(x1,y1)与b（x2,y2）间的欧式距离\n（2）三维空间两点A（x1,y1,z1）与B（x2,y2,z2）\n（3）　两个n维向量A(x11,x12,...,x1n)与B(x21,x22,...,x2n)间的欧氏距离：\n（4）Python实现欧式距离\nIn [33]: from numpy import * In [34]: vector1 = mat([1,2,3]) In [35]: vector2 = mat([4,5,6]) In [36]: print sqrt((vector1-vector2)*((vector1-vector2).T)) [[ 5.19615242]]\n3.曼哈顿距离（Manhattan Distance）\n（1）二维平面两点A(x1,y1)与B(x2,y2)间的曼哈顿距离：\n（2）两个n维向量A(x11,x12,...,x1n)与B（x21,x22,...,x2n）间的曼哈顿距离\n（3）Python实现曼哈顿\nIn [1]: from numpy import * In [2]: vector1 = mat([1,2,3]) In [3]: vector2 = mat([4,5,6]) In [4]: print sum(abs(vector1-vector2)) 9\n4、切比雪夫距离（Chebyshev Distance）\n（1）二维平面两点A(x1,y1)与B（x2,y2）间的切比雪夫距离：\n（2）两个n维平面两点A(x11,y12，..,x1n)与B（x21,y22，..,x2n）间的切比雪夫距离：\n这个公式的另外一种等价形式是：\n（3）Python实现切比雪夫距离。\nIn [1]: from numpy import * In [2]: vector1 = mat([1,2,3]) In [3]: vector2 = mat([4,7,5]) In [4]: print abs(vector1-vector2).max() 5\n5.夹角余弦（Consine）\n几何中的夹角余弦可用来衡量两个向量方向的差异\n（1）二维平面两点A(x1,y1)与B（x2,y2）间的夹角余弦公式：\n(2)两个n维样本点A（x11,x12,x13,...,x1n）与B（x21,x22,...,x2n）的夹角余弦：\n即：\n夹角余弦取值范围为[-1,1]。夹角余弦越大，表示向量的夹角越小；夹角余弦越小，表示两个向量的夹角越大。当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取值最小值-1.\n（3）Python实现夹角余弦\nIn [7]: from numpy import * In [8]: vector1 = mat([1,2,3]) In [9]: vector2 = mat([4,7,5]) In [10]: cosV12 = dot(vector1,vector2.T)/(linalg.norm(vector1)*linalg.norm(vector2)) In [11]: print cosV12 [[ 0.92966968]]\n6.汉明距离（Hamming Distance）\n（1）汉明距离的定义：两个等长的字符串s1和s2之间的汉明距离定义为将其中一个变为另外一个所需要的最小替换次数。例如字符串“111“与“1001”之间的汉明距离为2.\n应用：信息编码（为了增强容错性，应使编码间的最小汉明距离尽可能大）。\n（2）使用Python实现汉明距离。\nIn [20]: from numpy import * In [21]: matV = mat([[1,1,0,1,0,1,0,0,1],[0,1,1,0,0,0,1,1,1]]) In [22]: smstr = nonzero(matV[0]-matV[1]) In [23]: smstr Out[23]: (array([0, 0, 0, 0, 0, 0], dtype=int64), array([0, 2, 3, 5, 6, 7], dtype=int64)) In [24]: print shape(smstr[0]) (6L,)\n7.杰卡德相似系数（Jaccard Similarity Coefficient）\n（1）杰卡德相似系数：两个集合A和B的交集元素在A、B的并集中所占的比例，成为两个集合的杰卡德相似系数，用符号J(A,B)表示。\n杰卡德相似系数是衡量两个集合的相似度的一种指标。\n（2）杰卡德距离：与杰卡德相似系数相反的概念是杰卡德距离（Jaccard Distance），杰卡德距离可用如下的公式表示：\n杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度\n（3）杰卡德相似系数与杰卡德距离的应用。\n可将杰卡德相似系数用在衡量样本的相似度上。\n样本A与样本B是两个n维向量，而且所有维度上的取值都是0或者1.例如，A（0111）和B（1011）。我们将样本看成一个集合，1表示该集合包含该元素，0表示集合不包含该元素。\nP：样本A与B都是1的维度的个数\nq：样本A是1、样本B是0的维度的个数\nr: 样本A是0、样本B是1的维度的个数\ns：样本A与B都是0的维度的个数\n那么样本A与B的杰卡德相似系数可以表示为：\n这里p+q+r可以理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。\n（4）Python实现杰卡德距离。\nIn [25]: from numpy import * In [26]: import scipy.spatial.distance as dist #导入Scipy距离公式 In [27]: matV = mat([[1,1,0,1,0,1,0,0,1],[0,1,1,0,0,0,1,1,1]]) In [28]: print \"dist.jaccard:\",dist.pdist(matV,'jaccard') dist.jaccard: [ 0.75]\n1.3.3理解随机性（略）\n1.3.4回顾概率论（略）\n1.3.5多元统计基础（略）\n1.3.6特征相关性\n1.相关系数\n（1）相关系数定义：\n（2）相关距离定义：\n（3）Python实现相关系数\nIn [13]: from numpy import * In [14]: matV = mat([[1,1,0,1,0,1,0,0,1],[0,1,1,0,0,0,1,1,1]]) ...: In [15]: mv1 = mean(matV[0])#第一列的均值 In [16]: mv2 = mean(matV[1])#第二列的均值 In [17]: #计算两列的标准差 In [18]: dv1 = std(matV[0]) In [19]: dv2 = std(matV[1]) In [20]: corref = mean(multiply(matV[0]-mv1,matV[1]-mv2))/(dv1*dv2) In [21]: print corref\n2马氏距离\n（1）马氏距离的定义：有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到μ的距离为：\n而其中xi与Xj之间的马氏距离为：\n若协方差矩阵是单位矩阵（各个样本向量之间独立同分布），则公式变成欧式距离公式：\n若协方差是对角矩阵，则公式变成可标准化的欧式距离公式\n（2）马氏距离的优点：量纲无关，排除变量之间的相关性的干扰。\n（3）马氏距离的Python计算：\nIn [49]: from numpy import * In [50]: matV = mat([[1,1,0,1,0,1,0,0,1],[0,1,1,0,0,0,1,1,1]]) In [51]: covinv = linalg.inv(cov(matV)) In [52]: tp =matV.T[0]-matV.T[1] In [53]: distma = sqrt(dot(dot(tp,coninv),tp.T)) In [54]: distma = sqrt(dot(dot(tp,covinv),tp.T)) In [55]: distma Out[55]: matrix([[ 2.02547873]])\n1.3.7 再谈矩阵-空间的变换（略）\n5.特征值和特征向量\npython求取矩阵的特征值和特征向量。\nIn [56]: A = [[8,1,6],[3,5,7],[4,9,2]] In [57]: evals,evecs = linalg.eig(A) In [58]: print '特征值:',evals,'\\n特征向量:',evecs 特征值: [ 15. 4.89897949 -4.89897949] 特征向量: [[-0.57735027 -0.81305253 -0.34164801] [-0.57735027 0.47140452 -0.47140452] [-0.57735027 0.34164801 0.81305253]]\n1.3.8  数据的归一化\n2.欧式距离标准化：\nX* = （X-M）/S\n标准化的后的值 = （标准化前的值-分量的均值）/分量的标准差\n两个n维向量的之间的标准化的欧式距离公式：\n标准化欧式距离Python的实现\nIn [2]: from numpy import * In [3]: vectormat = mat([[1,2,3],[4,5,6]]) In [4]: v12 = vectormat[0]-vectormat[1] In [5]: print sqrt(v12*v12.T) [[ 5.19615242]] In [6]: #norm In [7]: varmat = std(vectormat.T,axis=0) In [16]: normvmat =(vectormat-mean(vectormat))/varmat.T In [17]: normv12 =normvmat[0]-normvmat[1] In [18]: sqrt( normv12* normv12.T) Out[18]: matrix([[ 8.5732141]])\n1.4 数据处理和可视化\n1.4.1 数据的导入和内存管理\n1.数据表文件的读取\nPython读取数据表例程：\n#coding:utf-8 import sys import os from numpy import * #配置UTF-8的输出环境 reload(sys) sys.setdefaultencoding('utf-8') #数据文件转矩阵 #path：数据文件路径 #delimiter：行内字段分隔符 def file2matrix(path,delimiter): recordlist = [] fp = open(path,\"rb\")#读取文件内容 content = fp.read() fp.close() rowlist = content.splitlines()#按行转化为一维表 #逐行遍历，结果按分割符分割为行向量 recordlist = [map(eval,row.split(delimiter)) for row in rowlist if row.strip()] return mat(recordlist)#返回转换后的矩阵形式 root = \"testdata\" #数据文件所在路径 pathlist = os.listdir(root) for path in pathlist: recordmat = file2matrix(root + \"/\"+path,\" \")#文件到矩阵的转换 print shape(recordmat)\n2.对象的持久化\n有时候，我们希望数据以对象的方式保存。Pytho提供了cPickle模块支持对象的读写\n#继续上面的代码 import cPickle as pickle #导入序列化库 file_obj = open(root +\"/recordmat.dat\",\"wb\") pickle.dump(recordmat[0],file_obj)#强生成的矩阵对象保存到指定的位置 file_obj.close() #c此段代码可将刚才转换为矩阵的数据持久化为对象的文件 #读取序列化后的文件 read_obj = open(root+\"/recordmat.dat\",\"rb\") readmat = pickle.load(read_obj) #从指定的位置读取对象 print shape(readmat)\n3.高效的读取大文本的文件\n#按行读取文件，读取指定的行数：nmax = 0 按行读取全部 def readfilelines(path,nmax = 0): fp = open(path,\"rb\") ncount = 0 #读取行 while True: content = fp.readline() if content == \"\" or (ncount>=nmax and nmax !=0):#判断文件尾或读完指定行数 break yield content#返回读取的行 if nmax != 0: ncount += 1 fp.close() root = \"testdata/01.txt\" #数据文件所在路径 for line in readfilelines(path,nmax=10):#读取10行 print line.strip()\n1.4.2 表与线性结构的可视化\n示例代码：\nIn [1]: import numpy as np In [2]: import matplotlib.pyplot as plt In [3]: #曲线数据加入噪声 In [4]: x = np.linspace(-5,5,200) In [5]: y = np.sin(x) #给出y与x的基本关系 In [6]: yn = y+np.random.rand(1,len(y))*1.5#加入噪声的点集 In [7]: #绘图 In [8]: fig = plt.figure() In [9]: ax = fig.add_subplot(1,1,1) In [10]: ax.scatter(x,yn,c='blue',marker='o') Out[10]: <matplotlib.collections.PathCollection at 0x6b7f780> In [11]: ax.plot(x,y+0.75,'r') Out[11]: [<matplotlib.lines.Line2D at 0x6d510f0>] In [12]: plt.show()\n1.4.3 树与分类结构的可视化（略）\n1.4.4 图与网格结构的可视化（略）\n资料来源及版权所有：《机器学习算法原理与编程实践》郑捷"}
{"content2":"感觉狼厂有些把机器学习和数据挖掘神话了，机器学习、数据挖掘的能力其实是有边界的。机器学习、数据挖掘永远是给大公司的业务锦上添花的东西，它可以帮助公司赚更多的钱，却不能帮助公司在与其他公司的竞争中取得领先优势，所以小公司招聘数据挖掘/机器学习不是为了装逼就是在自寻死路。可是相比Java和C++语言开发来说，机器学习/数据挖掘确实是新一些老人占的坑少一些，而且可以经常接触一些新的东西。还是赶紧再次抓住机会集中的再总结一下吧，不能再拖拖拉拉了。\n其实数据挖掘的主要任务是分类、聚类、关联分析、预测、时序模式和偏差分析。本文先系统的介绍一下机器学习中的分类算法，主要目录如下：\n常用分类算法\nBayes\n朴素贝叶斯的优缺点\n朴素贝叶斯的公式\nDecision Tree\n决策树的优缺点\n决策树公式\nSVM\n支持向量机的优缺点\n支持向量机的公式\nKNN\nK近邻的优缺点\nK近邻的公式\nLogistic Regression\n逻辑回归的优缺点\n逻辑回归的公式\n逻辑回归的问题\n神经网络\n神经网络的优缺点\n神经网络公式\n深度学习\nEnsemble learning\nGBDT\nAdaboost\nRandom Forest\n参考文献\n常用分类算法\nBayes\n贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。\n朴素贝叶斯的优缺点\n优点\n所需估计的参数少，对于缺失数据不敏感。\n缺点\n假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。\n需要知道先验概率。\n分类决策错误率。\n朴素贝叶斯的公式\n朴素贝叶斯求解：\nP(C|F1,...,Fn)=p(C)p(F1,...,Fn|C)p(F1,...,Fn)=p(C)∏i=1np(Fi|C)\nDecision Tree\n决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。决策树的每个内部节点表示在一个属性上的测试，每个分枝代表该测试的一个输出，而每个树叶结点存放着一个类标号。\n在决策树算法中，ID3基于信息增益作为属性选择的度量，C4.5基于信息增益比作为属性选择的度量，CART基于基尼指数作为属性选择的度量。\n决策树代码\n1\n1\n决策树的优缺点\n优点\n不需要任何领域知识或参数假设。\n适合高维数据。\n简单易于理解。\n短时间内处理大量数据，得到可行且效果较好的结果。\n缺点\n对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。\n易于过拟合。\n忽略属性之间的相关性。\n不支持在线学习\n决策树公式\n熵：\nEntropy(S)=−∑pilogpi\n信息增益：\nEntropy(S,A)=Entropy(S)−∑v∈V(A)|Sv||S|Entropy(Sv)\n分裂信息：\nSplitInfoR=−∑j=1k|Dj||D|log2|Dj||D|\n增益比率：\nGainRatio(R)=Gain(R)SplitInfoR(D)\n基尼指数：\nGini(S)=1−∑imp2i\nSVM\n支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。\n支持向量机的优缺点\n优点\n可以解决小样本下机器学习的问题。\n提高泛化性能。\n可以解决高维、非线性问题。超高维文本分类仍受欢迎。\n避免神经网络结构选择和局部极小的问题。\n缺点\n缺失数据敏感。\n内存消耗大，难以解释。\n运行和调差略烦人。\n支持向量机的公式\n转自研究者July: SVM的求解，先导出12||w||2，继而引入拉格朗日函数，转化为单一因子对偶变量a的求解。如此求w.b与a的等价，而求a的解法即为SMO。把求分类函数f(x)=ω∗x+b的问题转化为求w,b的最优化问题，即凸二次规划问题，妙。\n从上图我们可以看出，这条红色的线（超平面）把红色的点和蓝色的点分开了。超平面一边的点对应的y全部是-1，而另外一边全部是1。\n接着我们可以令分类函数：f(x)=ωTx+b。显然x是超平面上的点时，f(x)=0。那么我们不妨要求所有满足f(x)<0的点，其对应的y等于-1，而f(x)>0则对应的y=1的数据点。（我盗用了很多图。。。）\n回忆之前的目标函数：\nmax1||ω||s.t.,yi(ωT+b)≥1,i=1,...,n\n这个问题等价于\nmax1||ω||2s.t.,yi(ωT+b)≥1,i=1,...,n\n很显然这是一个凸优化的问题，更具体的，它是一个二次优化问题—目标函数是二次的，约束条件是线性的。这个问题可以用任何现成的QP（Quadratic Programming）优化包解决。但是因为这个问题的特殊性，我们还可以通过Lagrange Duality变换到对偶变量的优化问题，找到一种更加行之有效的方法求解。首先我们给每一个约束条件加上一个Lagrange mutiplier,我们可以将它们融合到目标函数中去。 L(ω,b,a)=12||ω||2−∑ni=1α(yi(wTxi+b)−1)，然后我们令θ(ω)=maxai≥0L(ω,b,a)容易验证，当某个约束条件不满足时，例如yi(wTxi+b)<1，那么我们显然有θ(w)=∞。而当所有约束条件都满足时，则有θ(ω)=12||ω||2，亦即我们最初要最小化的量。那么我们现在的目标函数就变成了：minw,bθ(ω)=minω,bmaxai≥0L(ω,b,α)=p∗，并且我们有d∗≤p∗，因为最大值中最小的一个一定要大于最小值中最大的一个。总之p∗提供了一个第一个问题的最优值p∗的一个下界。在满足KKT条件时，二者相等，我们可以通过求解第二个问题来求解第一个问题。\n先让L关于ω和b最小化，我们分别把L对w和b求偏导：\n∂L∂ω=0⟹ω=∑i=1nαiyixi\n∂L∂b=0⟹∑i=1nαiyi=0\n再带回L得到：\nL(ω,b,a)=12∑i,j=1nαiαjyiyjxTixj−∑i,j=1nαiαjyiyjxTixj−b∑i=1nαiyi+∑i=1nαi=∑i=1nαi−12∑i,j=1nαiαjyiyjxTixj\n此时我们得到关于dual variable α的优化问题：\nmaxα∑ni=1αi−12∑ni,j=1αiαjyiyjxTixjs.t.,αi≥0,i=1,...,n∑ni=1αiyi=0\n这个问题存在高效的算法，不过求解过程就不在这里介绍了。对于一个数据点进行分类时，我们是把x带入到f(x)=wTx+b中，然后根据其正负号来进行类别划分的。把ω=∑ni=1αiyixi代入到f(x)=wTx+b，我们就可以得到f(x)=∑ni=1αiyi<xi,x>+b，这里的形式的有趣之处在于，对于新点x的检测，只需要计算它与训练数据点的内积即可。\n为什么非支持向量的α等于零呢？因为对于非支持向量来说，L(ω,b,a)=12||ω||2−∑ni=1α(yi(wTxi+b)−1)中的，(yi(wTxi+b)−1)是大于0的，而且αi又是非负的，为了满足最大化，αi必须等于0。悲剧的非支持向量就被无声的秒杀了。。。\nKNN\nK近邻的优缺点\n优点\n暂无\n缺点\n计算量太大\n对于样本分类不均衡的问题，会产生误判。\nK近邻的公式\nLogistic Regression\n逻辑回归的优缺点\n优点\n速度快。\n简单易于理解，直接看到各个特征的权重。\n能容易地更新模型吸收新的数据。\n如果想要一个概率框架，动态调整分类阀值。\n缺点\n特征处理复杂。需要归一化和较多的特征工程。\n逻辑回归的公式\n如果是连续的，那么就是多重线性回归；如果是二项分布，就是Logistic回归；如果是Poission分布，就是Poisson回归；如果是负二项分布，那么就是负二项分布。 回归问题常见步骤是：寻找h函数；构造J函数；想办法使得J函数最小并求得回归参数。逻辑回归的h函数为：\nhθ(x)=g(θTx)=11+e−θTx\n其中hθ(x)的值表示结果取1的概率。\n那么对于输入x的分类结果对于类别1和类别0的概率分别为：\nP(y=1|x;θ)=hθ(x)\nP(y=0|x;θ)=1−hθ(x)\n那么对于构造损失函数J，它们基于最大似然估计推到得到的：\n∑ni=1Cost(hθ(xi),yi)=−1m[∑ni=1yiloghθ(xi)+(1−yi)log(1−hθ(xi))]\n最小化上式，与最大化下式子类似：\nP(y|x;θ)=(hθ(x))y(1−hθ(x))1−y\n取似然函数：\nl(θ)=logL(θ)=∑ni=1yi(loghθ(xi)+(1−yi)log(1−hθ(xi)))\n使用梯度上升的方法，求解θ，同样如果把J(θ)取为\n−1ml(θ)，这样通过梯度下降求解梯度最小值。\n梯度下降法求最小值：\nθj:=θj−ασσθiJ(θ)\n代入后得到：\nθj:=θj−α1m∑mi=1(hθ(xi)−yi)xji\n然后θ的更新过程如下：\nθj:=θj−α1mxTE\n其中E=g(A)-y。\n正则化Regularization：\n正则化是在经验风线上增加一个正则化项或者惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化就越大。\nJ(θ)=12m∑i=1n(hθ(xi)−yi)2+λ∑j=1nθ2j\nλ是正则项系数。多分类时可以去样本被判定为分类概率最大的那个类。\n逻辑回归的问题\n过拟合问题\n减少feature个数\n规格化\n神经网络\n神经网络的优缺点\n优点\n分类准确率高。\n并行处理能力强。\n分布式存储和学习能力强。\n鲁棒性较强，不易受噪声影响。\n缺点\n需要大量参数（网络拓扑、阀值、阈值）。\n结果难以解释。\n训练时间过长。\n神经网络公式\n深度学习？？？\nEnsemble learning\n集成学习的思路是在对新的实例进行分类的时候，把多个单分类器的结果进行某种组合，来对最终的结果进行分类。\n更好的数据往往打败更好的算法，设计好的特征大有脾益。并且如果你有一个庞大的数据集，使用某种特定的算法的性能可能并不要紧。大可以挨个分类器尝试，并且选取最好的一个。（可以多从易用性和性能考虑）\n而且从Netfliex Prize的经验教训来看，尝试各类分类器、交叉验证、集成方法往往能取得更好的结果，一般的boosting>bagging>single classifier。集成学习的方法主要有一下三种：\n1. 在样本上做文章，基分类器为同一个分类算法，主要有bagging和boosting。\n2. 在分类算法上做文章，即用于训练基分类器的样本相同。基分类器的算法不同。\n3. 在样本属性集上做文章，即在不同的属性上构建分类器，比较出名的是randomforest Tree的算法，这个有weka也有实现。\n1998年Jerome Friedman & Trevor Hastie & Robert Tibshirani发表文章Additive Logistic Regression: a Statistical View of Boosting，中提到Bagging是一个纯粹的降低相关度的方法。如果树的节点具有很高的相关性，bagging就会有很好的效果。\nGBDT\n回归树类似决策树，使用叶子节点的平均值作为判定的结果。如果不是叶子节点，那么就继续向下寻找。GBDT几乎可用于所有的回归问题，亦可以适用于二分类问题。 GBDT使用新生成的树来拟合之前的树拟合的残差。\nAdaboost\nAdaboost目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。 Adaboost的算法流程如下，首先初始化训练数据的权值分布。每个训练样本最开始都被赋予相同的权重：1/N。计算Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度。\nαm=12log1−emem，在em<=1/2时，am>=0，且am随着em的减小而增大。\n更新训练数据集的权值分布（目的：得到样本的新权值分布），用于下一轮迭代：\nDm+1=(wm+1,1,wm+1,2,...wm+1,i...,wm+1,N),\nwm+1,i=wmiZmexp(−αmyiGm(xi)),i=1,2,...,N\n使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过Zm=∑Ni=1wmiexp(−αmyiGm(xi))，使得Dm+1成为一个概率分布。然后组合各个弱分类器f(x)=∑Mm=1αmGm(x)，而得到的最终分类器G(x)=sign(f(x))=sign[∑Mm=1αmGm(x)]。\nRandom Forest\n随机森林指通过多颗决策树联合组成的预测模型，可以对样本或者特征取bagging。\n参考文献"}
{"content2":"反作弊主要业务流程：\n常见作弊方式：\n机器作弊：机器刷量、任务分发、流量劫持\n人为作弊：QQ群/水军、直接人工、诱导\n常见作弊手段：\n电商：刷单，刷信誉，刷好评，职业差评师\n支付平台：洗钱，诈骗\n广告：数据造假、刷流量 （引流—广告展示—广告点击—转化）\n自媒体、社交软件：刷粉丝、刷点击、阅读量\n搜索：seo使用作弊手段刷排名（案例：2015蜻蜓FM “普罗米修斯”、“宙斯”函数，修改转化量、流量在前端展现欺骗投资人，被对手反编译识破）\n广告作弊涉及的点击类型分类：\n1、  按照是否找商品找服务为目的\n2、  是否按照是否恶意，有无真实转化为依据\n（CPC基于点击计费的模式、CPA基于成交的点击进行收费）\n点击四大分类：\n无效点击（没有形成转化的意愿，仅仅浏览）\n恶意点击（必须识别出来）\n转化点击（真实意愿点击）\n误点（不是以找商品为目的，例如内部人员点击，需要识别出来）\n人群划分：\n误点：员工、广告主自己、竞品销售中介、爬虫\n恶意点击：同行、同行朋友、联盟站点、机器\n反作弊策略应对框架：\n数据层：鼠标轨迹行为、指纹数据、案例库、行为数据\n特征层：离散指标、连续指标\n行为识别层：点击识别模型、异常监测模型、流量识别模型、关系图模型、人群识别模型\n策略应对层：规则\n三层监控指标体系，提前预警：\n运营指标监控：投诉率、转化率、撞线速率/频率、消耗速率、通过率\n规则监控指标：拦截率、准确率、覆盖率\n异常监控指标：IP维度、Cookie维度、计费名维度、广告维度、设备维度、鼠标轨迹维度\n分类监控、分级响应：\n1、  针对监控情况、采用四级响应机制；\n2、  红色：非常严重，需要自动化采取短期策略应对，例如临时黑名单机制\n3、  橙色：较为严重，短信举报，要求4h内完成分析和短平快策略压制，后续进一步处理\n机器学习在反作弊应用几个案例：\n如关联规则、决策树模型：策略挖掘—规则自动提取\n确定建模问题：自动发现规则、辅助策略设计；\n应用：挖出的规则，上线到离线反作弊系统\n评估指标：支持度、置信度、覆盖率、拦截率\n流量聚类分级模型，kmeans算法：异常行为识别—流量识别\n作用：支持流量分级打折策略\n例如分为以下类别，特征描述：\n1、  主要为品牌浏览器入口，转化效果较好，用户粘性较高；\n2、  电话转化很好，包括搜索行为、点击行为、转化行为都较好；\n3、  电话转化良好，没有明显的特征异常；\n4、  电话转化率略偏低，详情页其它点击行为略偏少；\n5、  详情页停留时间短，转化效果特别差；\n6、  电话转化很好，无其它任何转化行为，专门点击商业广告，行为非常异常；\n7、  点击率高，无转化，行为非常异常.\n如半监督或图算法：异常行为识别—基于关系发现：\n作弊用户标签比较少，如何召回更多的数量？\n借助半监督或图算法发现更多的异常用户·\nSVM算法：异常行为识别—销售识别\n作用：识别用户是否销售\n数据来源：行为日志，聊天记录\n惩罚系数C，选择RBF函数作为kernel的参数gamma的选择.\n粗粒度搜索：\n对大数据集，先选择一个较小的子集做粗粒度搜索；\n选择较大的步长，找到一个最优的（c,g）局部区域.\n细粒度搜索：\n在局部区域，以较小的步长，找到全局最优的（c,g）\n如图论与余弦距离：搜索引擎防作弊\n图论：作弊的网站一般需要相互链接，以提高自己的排名，这样在互联网大图中形成一些Clique.图论中有专门发现Clique的方法.\n余弦距离：那些卖链接的网站，都有大量的出链（这些出链的特点和不作弊的网站出链特点大不相同）.每一个网站到其他网站的出链数目可以作为一个向量，计算余弦距离.发现，有些网站的出链向量之间的余弦距离几乎为1.一般来讲这些网站通常是一个人建的，目的只有一个:卖链接."}
{"content2":"人工智能影响力报告\nAlphaGo一战成名后，越来越多国民的目光聚焦向了人工智能。人工智能（AI）是一门综合了计算机科学、生理学、哲学的交叉学科。凡是使用机器代替人类实现认知、识别、分析、决策等功能，均可认为使用了人工智能技术。当你用苹果的Siri助手、用今日头条看资讯时，你都用到了人工智能技术。\n这个已经存在了60年度的技术领域，经过两起两落后，再次受到追捧。作为一家正在用人工智能重新定义人类社会连接人与信息的方式的科技公司，今日头条推出《人工智能影响力报告》，旨在记录本轮人工智能浪潮下，最有影响力的公司、技术和科学家，以及国民心中对AI的期盼与担忧。\n核心发现\n综合性AI公司的影响力远大于单一应用领域的AI公司\nAI巨头不断开放的开源平台将带来下游应用的蓬勃发展\n由通用芯片向定制芯片过渡最终走向类脑计算芯片是大势所趋\n随着开源平台的进一步开放，算法和芯片的进一步提升，人工智能的核心竞争将从技术竞争转向人才和应用场景竞争\n中国国民AI信心指数为83，乐观且理性\n一图看懂人工智能大家庭\n什么是人工智能？它和神经网络、机器学习、深度学习、数据挖掘这类热门词汇有什么关系？撇开复杂的概念和高冷的定义，一图看懂人工智能相关领域的错综复杂的关系。由图可见，人工智能、机器学习、深度学习并非是层层包含的关系，而最近火热的神经网络也只是与人工智能有交叉而非人工智能的实现方式或者子集。\n人工智能相关领域热度\n人工智能的三轮高潮\n1956年的达特矛斯会议确立了AI的名称和基本任务，因此这一事件被广泛承认为AI诞生的标志。\n1975年Paul Werbos 提出了BP算法，使得多层人工神经元网络的的学习变成可能。1986年两层神经元网络的提出，是整个人工智能浪潮的奠基性工作。\n2012年Hinton的学生在图片分类竞赛ImageNet上大大降低了错误率，打败了工业界的巨头Google，这不仅学术意义重大，更是吸引了工业界大规模的对深度学习的投入。\n本轮人工智能的兴起\n在谷歌趋势上搜索人工智能、大数据、机器学习、深度学习的中英文可以比对不同的关注走向。人工智能和大数据此消彼长，早在2004年人工智能就受到中英文领域的双重关注。在2006-2007年人工智能中文搜索开始下降，大数据的概念开始火过于人工智能。在英文搜索领域，大数据的浪潮直到2012年后才开始超过中文领域。深度学习在中文领域的搜索热度，一度在2009年和人工智能齐平。相反在英文领域，深度学习的搜索关注度直到2013年后才开始逐步提升。\n数据量的上涨、运算力的提升和深度学习算法的出现极大的促进了这一轮人工智能行业的发展\n人工智能产业结构图\n人工智能公司影响力榜单\n机器学习开源平台/框架影响力榜单\n人工智能十大热门芯片榜单\n人工智能十大热门算法榜单\n国内，微软研究院堪称AI界的黄埔军校。即便从微软研究院出来的科学家们，走向BAT、今日头条等公司，但微软依旧占据着中国AI人才的首把交椅，拥有25位AI科学家。由图可见，AI科学家们基本就集中百度、阿里巴巴等已经在人工智能领域耕耘多年的大公司里。西门子、IBM、Facebook也占据了不少中国的AI人才。可见中国AI的国际地位与影响力。当然除了上述大公司，像清华大学、中科院、香港中文大学、北京大学等高校也是AI科学家的聚集地。\n人工智能科学家毕业院校和学历\nAI科学家主要毕业于自清华大学、中国科学院、中国科技大学、麻省理工大学、北京大学等等。学历上看，68%的AI科学家都拥有博士学历。可见AI领域是高学历人才的聚集地。\nAI“十问”\n在AI相关的话题领域里，大家最为关注的还是和切身利益相关的生计问题——自己的工作是否会被取代，以及AI发展带来的危害，以及AI发展带来的法律、道德问题，可以说三个都是负面倾向的问题，可见虽然民众对于AI发展是支持的，但他们也更希望了解更多关于AI带来的直接风险。至于AlphaGo是否会胜利，事实上吃瓜群众们也就是看看热闹，并没有多大的热情。"}
{"content2":"【注】该系列文章以及使用到安装包/测试数据 可以在《倾情大奉送--Spark入门实战系列》获取\n1、机器学习概念\n1.1 机器学习的定义\n在维基百科上对机器学习提出以下几种定义：\nl“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。\nl“机器学习是对能通过经验自动改进的计算机算法的研究”。\nl“机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E。\n可以看出机器学习强调三个关键词：算法、经验、性能，其处理过程如下图所示。\n上图表明机器学习是数据通过算法构建出模型并对模型进行评估，评估的性能如果达到要求就拿这个模型来测试其他的数据，如果达不到要求就要调整算法来重新建立模型，再次进行评估，如此循环往复，最终获得满意的经验来处理其他的数据。\n1.2 机器学习的分类\n1.2.1 监督学习\n监督是从给定的训练数据集中学习一个函数（模型），当新的数据到来时，可以根据这个函数（模型）预测结果。监督学习的训练集要求包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注（标量）的。在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”、“非垃圾邮件”，对手写数字识别中的“1”、“2”、“3”等。在建立预测模型时，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断调整预测模型，直到模型的预测结果达到一个预期的准确率。常见的监督学习算法包括回归分析和统计分类：\nl  二元分类是机器学习要解决的基本问题，将测试数据分成两个类，如垃圾邮件的判别、房贷是否允许等问题的判断。\nl  多元分类是二元分类的逻辑延伸。例如，在因特网的流分类的情况下，根据问题的分类，网页可以被归类为体育、新闻、技术等，依此类推。\n监督学习常常用于分类，因为目标往往是让计算机去学习我们已经创建好的分类系统。数字识别再一次成为分类学习的常见样本。一般来说，对于那些有用的分类系统和容易判断的分类系统，分类学习都适用。\n监督学习是训练神经网络和决策树的最常见技术。神经网络和决策树技术高度依赖于事先确定的分类系统给出的信息。对于神经网络来说，分类系统用于判断网络的错误，然后调整网络去适应它；对于决策树，分类系统用来判断哪些属性提供了最多的信息，如此一来可以用它解决分类系统的问题。\n1.2.2 无监督学习\n与监督学习相比，无监督学习的训练集没有人为标注的结果。在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法和k-Means算法。这类学习类型的目标不是让效用函数最大化，而是找到训练数据中的近似点。聚类常常能发现那些与假设匹配的相当好的直观分类，例如基于人口统计的聚合个体可能会在一个群体中形成一个富有的聚合，以及其他的贫穷的聚合。\n非监督学习看起来非常困难：目标是我们不告诉计算机怎么做，而是让它（计算机）自己去学习怎样做一些事情。非监督学习一般有两种思路：第一种思路是在指导Agent时不为其指定明确的分类，而是在成功时采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是产生一个分类系统，而是做出最大回报的决定。这种思路很好地概括了现实世界，Agent可以对那些正确的行为做出激励，并对其他的行为进行处罚。\n因为无监督学习假定没有事先分类的样本，这在一些情况下会非常强大，例如，我们的分类方法可能并非最佳选择。在这方面一个突出的例子是Backgammon（西洋双陆棋）游戏，有一系列计算机程序（例如neuro-gammon和TD-gammon）通过非监督学习自己一遍又一遍地玩这个游戏，变得比最强的人类棋手还要出色。这些程序发现的一些原则甚至令双陆棋专家都感到惊讶，并且它们比那些使用预分类样本训练的双陆棋程序工作得更出色。\n1.2.3 半监督学习\n半监督学习（Semi-supervised Learning）是介于监督学习与无监督学习之间一种机器学习方式，是模式识别和机器学习领域研究的重点问题。它主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。半监督学习对于减少标注代价，提高学习机器性能具有非常重大的实际意义。主要算法有五类：基于概率的算法；在现有监督算法基础上进行修改的方法；直接依赖于聚类假设的方法等，在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理地组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测，如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。\n半监督学习分类算法提出的时间比较短，还有许多方面没有更深入的研究。半监督学习从诞生以来，主要用于处理人工合成数据，无噪声干扰的样本数据是当前大部分半监督学习方法使用的数据，而在实际生活中用到的数据却大部分不是无干扰的，通常都比较难以得到纯样本数据。\n1.2.4 强化学习\n强化学习通过观察来学习动作的完成，每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻做出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning 以及时间差学习（Temporal difference learning）。\n在企业数据应用的场景下，人们最常用的可能就是监督式学习和非监督式学习的模型。在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据，目前半监督式学习是一个很热的话题。而强化学习更多地应用在机器人控制及其他需要进行系统控制的领域。\n1.3 机器学习的常见算法\n常见的机器学习算法有：\nl  构造条件概率：回归分析和统计分类；\nl  人工神经网络；\nl  决策树；\nl  高斯过程回归；\nl  线性判别分析；\nl  最近邻居法；\nl  感知器；\nl  径向基函数核；\nl  支持向量机；\nl  通过再生模型构造概率密度函数；\nl  最大期望算法；\nl  graphical model：包括贝叶斯网和Markov随机场；\nl  Generative Topographic Mapping；\nl  近似推断技术；\nl  马尔可夫链蒙特卡罗方法；\nl  变分法；\nl  最优化：大多数以上方法，直接或者间接使用最优化算法。\n根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题，下面用一些相对比较容易理解的方式来解析一些主要的机器学习算法：\n1.3.1 回归算法\n回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。\n1.3.2 基于实例的算法\n基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor (KNN)，、学习矢量量化（Learning Vector Quantization， LVQ）以及自组织映射算法（Self-Organizing Map，SOM）\n1.3.3 正则化方法\n正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression、Least Absolute Shrinkage and Selection Operator（LASSO）以及弹性网络（Elastic Net）。\n1.3.4 决策树学习\n决策树算法根据数据的属性采用树状结构建立决策模型，决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）、 ID3 (Iterative Dichotomiser 3)、C4.5、Chi-squared Automatic Interaction Detection (CHAID)、Decision Stump、机森林（Random Forest）、多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine，GBM）。\n1.3.5 贝叶斯学习\n贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法、平均单依赖估计（Averaged One-Dependence Estimators， AODE）以及 Bayesian Belief Network（BBN）。\n1.3.6 基于核的算法\n基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易解决。常见的基于核的算法包括：支持向量机（Support Vector Machine，SVM）、径向基函数（Radial Basis Function，RBF)以及线性判别分析（Linear Discriminate Analysis，LDA)等。\n1.3.7 聚类算法\n聚类就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所有的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means 算法以及期望最大化算法（Expectation Maximization，EM）。\n1.3.8 关联规则学习\n关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori 算法和 Eclat 算法等。\n1.3.9 人工神经网络算法\n人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法（其中深度学习就是其中的一类算法，我们会单独讨论）。重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）、反向传递（Back Propagation）、Hopfield 网络、自组织映射（Self-Organizing Map, SOM）、学习矢量量化（Learning Vector Quantization，LVQ）。\n1.3.10 深度学习算法\n深度学习算法是对人工神经网络的发展，在近期赢得了很多关注，特别是百度也开始发力深度学习后，更是在国内引起了很多关注。在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）、 Deep Belief Networks（DBN）、卷积网络（Convolutional Network）、堆栈式自动编码器（Stacked Auto-encoders）。\n1.3.11 降低维度算法\n像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式，试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis， PCA）、偏最小二乘回归（Partial Least Square Regression，PLS）、 Sammon 映射、多维尺度（Multi-Dimensional Scaling, MDS）、投影追踪（Projection Pursuit）等。\n1.3.12 集成算法\n集成算法用一些相对较弱的学习模型独立地对同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting、Bootstrapped Aggregation（Bagging）、AdaBoost、堆叠泛化（Stacked Generalization， Blending）、梯度推进机（Gradient Boosting Machine, GBM）、随机森林（Random Forest）。\n2、Spark MLlib介绍\nSpark之所以在机器学习方面具有得天独厚的优势，有以下几点原因：\n（1）机器学习算法一般都有很多个步骤迭代计算的过程，机器学习的计算需要在多次迭代后获得足够小的误差或者足够收敛才会停止，迭代时如果使用Hadoop的MapReduce计算框架，每次计算都要读/写磁盘以及任务的启动等工作，这回导致非常大的I/O和CPU消耗。而Spark基于内存的计算模型天生就擅长迭代计算，多个步骤计算直接在内存中完成，只有在必要时才会操作磁盘和网络，所以说Spark正是机器学习的理想的平台。\n（2）从通信的角度讲，如果使用Hadoop的MapReduce计算框架，JobTracker和TaskTracker之间由于是通过heartbeat的方式来进行的通信和传递数据，会导致非常慢的执行速度，而Spark具有出色而高效的Akka和Netty通信系统，通信效率极高。\nMLlib(Machine Learnig lib) 是Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。Spark的设计初衷就是为了支持一些迭代的Job, 这正好符合很多机器学习算法的特点。在Spark官方首页中展示了Logistic Regression算法在Spark和Hadoop中运行的性能比较，如图下图所示。\n可以看出在Logistic Regression的运算场景下，Spark比Hadoop快了100倍以上！\nMLlib目前支持4种常见的机器学习问题: 分类、回归、聚类和协同过滤，MLlib在Spark整个生态系统中的位置如图下图所示。\nMLlib基于RDD，天生就可以与Spark SQL、GraphX、Spark Streaming无缝集成，以RDD为基石，4个子框架可联手构建大数据计算中心！\nMLlib是MLBase一部分，其中MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。\nl  ML Optimizer会选择它认为最适合的已经在内部实现好了的机器学习算法和相关参数，来处理用户输入的数据，并返回模型或别的帮助分析的结果；\nl  MLI 是一个进行特征抽取和高级ML编程抽象的算法实现的API或平台；\nl  MLlib是Spark实现一些常见的机器学习算法和实用程序，包括分类、回归、聚类、协同过滤、降维以及底层优化，该算法可以进行可扩充； MLRuntime 基于Spark计算框架，将Spark的分布式计算应用到机器学习领域。\n3、Spark MLlib架构解析\n从架构图可以看出MLlib主要包含三个部分：\nl  底层基础：包括Spark的运行库、矩阵库和向量库；\nl  算法库：包含广义线性模型、推荐系统、聚类、决策树和评估的算法；\nl  实用程序：包括测试数据的生成、外部数据的读入等功能。\n3.1 MLlib的底层基础解析\n底层基础部分主要包括向量接口和矩阵接口，这两种接口都会使用Scala语言基于Netlib和BLAS/LAPACK开发的线性代数库Breeze。\nMLlib支持本地的密集向量和稀疏向量，并且支持标量向量。\nMLlib同时支持本地矩阵和分布式矩阵，支持的分布式矩阵分为RowMatrix、IndexedRowMatrix、CoordinateMatrix等。\n关于密集型和稀疏型的向量Vector的示例如下所示。\n疏矩阵在含有大量非零元素的向量Vector计算中会节省大量的空间并大幅度提高计算速度，如下图所示。\n标量LabledPoint在实际中也被大量使用，例如判断邮件是否为垃圾邮件时就可以使用类似于以下的代码：\n可以把表示为1.0的判断为正常邮件，而表示为0.0则作为垃圾邮件来看待。\n对于矩阵Matrix而言，本地模式的矩阵如下所示。\n分布式矩阵如下所示。\nRowMatrix直接通过RDD[Vector]来定义并可以用来统计平均数、方差、协同方差等：\n而IndexedRowMatrix是带有索引的Matrix，但其可以通过toRowMatrix方法来转换为RowMatrix，从而利用其统计功能，代码示例如下所示。\nCoordinateMatrix常用于稀疏性比较高的计算中，是由RDD[MatrixEntry]来构建的，MatrixEntry是一个Tuple类型的元素，其中包含行、列和元素值，代码示例如下所示：\n3.2 MLlib的算法库分析\n下图是MLlib算法库的核心内容。\n在这里我们分析一些Spark中常用的算法：\n3.2.1 分类算法\n分类算法属于监督式学习，使用类标签已知的样本建立一个分类函数或分类模型，应用分类模型，能把数据库中的类标签未知的数据进行归类。分类在数据挖掘中是一项重要的任务，目前在商业上应用最多，常见的典型应用场景有流失预测、精确营销、客户获取、个性偏好等。MLlib 目前支持分类算法有：逻辑回归、支持向量机、朴素贝叶斯和决策树。\n案例：导入训练数据集，然后在训练集上执行训练算法，最后在所得模型上进行预测并计算训练误差。\nimport org.apache.spark.SparkContext\nimport org.apache.spark.mllib.classification.SVMWithSGD\nimport org.apache.spark.mllib.regression.LabeledPoint\n// 加载和解析数据文件\nval data = sc.textFile(\"mllib/data/sample_svm_data.txt\")\nval parsedData = data.map { line =>\nval parts = line.split(' ')\nLabeledPoint(parts(0).toDouble, parts.tail.map(x => x.toDouble).toArray)\n}\n// 设置迭代次数并进行进行训练\nval numIterations = 20\nval model = SVMWithSGD.train(parsedData, numIterations)\n// 统计分类错误的样本比例\nval labelAndPreds = parsedData.map { point =>\nval prediction = model.predict(point.features)\n(point.label, prediction)\n}\nval trainErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / parsedData.count\nprintln(\"Training Error = \" + trainErr)\n3.2.2 回归算法\n回归算法属于监督式学习，每个个体都有一个与之相关联的实数标签，并且我们希望在给出用于表示这些实体的数值特征后，所预测出的标签值可以尽可能接近实际值。MLlib 目前支持回归算法有：线性回归、岭回归、Lasso和决策树。\n案例：导入训练数据集，将其解析为带标签点的RDD，使用 LinearRegressionWithSGD 算法建立一个简单的线性模型来预测标签的值，最后计算均方差来评估预测值与实际值的吻合度。\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD\nimport org.apache.spark.mllib.regression.LabeledPoint\n// 加载和解析数据文件\nval data = sc.textFile(\"mllib/data/ridge-data/lpsa.data\")\nval parsedData = data.map { line =>\nval parts = line.split(',')\nLabeledPoint(parts(0).toDouble, parts(1).split(' ').map(x => x.toDouble).toArray)\n}\n//设置迭代次数并进行训练\nval numIterations = 20\nval model = LinearRegressionWithSGD.train(parsedData, numIterations)\n// 统计回归错误的样本比例\nval valuesAndPreds = parsedData.map { point =>\nval prediction = model.predict(point.features)\n(point.label, prediction)\n}\nval MSE = valuesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/valuesAndPreds.count\nprintln(\"training Mean Squared Error = \" + MSE)\n3.2.3 聚类算法\n聚类算法属于非监督式学习，通常被用于探索性的分析，是根据“物以类聚”的原理，将本身没有类别的样本聚集成不同的组，这样的一组数据对象的集合叫做簇，并且对每一个这样的簇进行描述的过程。它的目的是使得属于同一簇的样本之间应该彼此相似，而不同簇的样本应该足够不相似，常见的典型应用场景有客户细分、客户研究、市场细分、价值评估。MLlib 目前支持广泛使用的KMmeans聚类算法。\n案例：导入训练数据集，使用 KMeans 对象来将数据聚类到两个类簇当中，所需的类簇个数会被传递到算法中，然后计算集内均方差总和 (WSSSE)，可以通过增加类簇的个数 k 来减小误差。 实际上，最优的类簇数通常是 1，因为这一点通常是WSSSE图中的 “低谷点”。\nimport org.apache.spark.mllib.clustering.KMeans\n// 加载和解析数据文件\nval data = sc.textFile(\"kmeans_data.txt\")\nval parsedData = data.map( _.split(' ').map(_.toDouble))\n// 设置迭代次数、类簇的个数\nval numIterations = 20\nval numClusters = 2\n// 进行训练\nval clusters = KMeans.train(parsedData, numClusters, numIterations)\n// 统计聚类错误的样本比例\nval WSSSE = clusters.computeCost(parsedData)\nprintln(\"Within Set Sum of Squared Errors = \" + WSSSE)\n3.2.4 协同过滤\n协同过滤常被应用于推荐系统，这些技术旨在补充用户-商品关联矩阵中所缺失的部分。MLlib当前支持基于模型的协同过滤，其中用户和商品通过一小组隐语义因子进行表达，并且这些因子也用于预测缺失的元素。\n案例：导入训练数据集，数据每一行由一个用户、一个商品和相应的评分组成。假设评分是显性的，使用默认的ALS.train()方法，通过计算预测出的评分的均方差来评估这个推荐模型。\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.Rating\n// 加载和解析数据文件\nval data = sc.textFile(\"mllib/data/als/test.data\")\nval ratings = data.map(_.split(',') match {\ncase Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble)\n})\n// 设置迭代次数\nval numIterations = 20\nval model = ALS.train(ratings, 1, 20, 0.01)\n// 对推荐模型进行评分\nval usersProducts = ratings.map{ case Rating(user, product, rate) => (user, product)}\nval predictions = model.predict(usersProducts).map{\ncase Rating(user, product, rate) => ((user, product), rate)\n}\nval ratesAndPreds = ratings.map{\ncase Rating(user, product, rate) => ((user, product), rate)\n}.join(predictions)\nval MSE = ratesAndPreds.map{\ncase ((user, product), (r1, r2)) => math.pow((r1- r2), 2)\n}.reduce(_ + _)/ratesAndPreds.count\nprintln(\"Mean Squared Error = \" + MSE)\n3.3 MLlib的实用程序分析\n实用程序部分包括数据的验证器、Label的二元和多元的分析器、多种数据生成器、数据加载器。\n4、参考资料\n（1）Spark官网 mlllib说明  http://spark.apache.org/docs/1.1.0/mllib-guide.html\n（2）《机器学习常见算法分类汇总》 http://www.ctocio.com/hotnews/15919.html"}
{"content2":"一、概述\nk-近邻算法采用测量不同特征值之间的距离方法进行分类。\n工作原理：首先有一个样本数据集合（训练样本集），并且样本数据集合中每条数据都存在标签（分类），即我们知道样本数据中每一条数据与所属分类的对应关系，输入没有标签的数据之后，将新数据的每个特征与样本集的数据对应的特征进行比较（欧式距离运算），然后算出新数据与样本集中特征最相似（最近邻）的数据的分类标签，一般我们选择样本数据集中前k个最相似的数据，然后再从k个数据集中选出出现分类最多的分类作为新数据的分类。\n二、优缺点\n优点：精度高、对异常值不敏感、无数据输入假定。\n缺点：计算度复杂、空间度复杂。\n适用范围：数值型和标称型\n三、数学公式\n欧式距离：欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。\n(1)二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：\n(2)三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：\n(3)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：\n三、算法实现\nk-近邻算法的伪代码\n对未知类型属性的数据集中的每个点依次执行以下操作：\n(1) 计算已知类别数据集中的点与当前点之间的距离；\n(2) 按照距离增序排序；\n(3) 选取与当前点距离最近的k个点；\n(4) 决定这k个点所属类别的出现频率；\n(5) 返回前k个点出现频率最高的类别作为当前点的预测分类。\n1、构造数据\n1 def createDataSet(): 2 group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) 3 labels = ['A','A','B','B'] 4 return group, labels\n这里有4组数据，每组数据的列代表不同属性的特征值，向量labels包含了每个数据点的标签信息，也可以叫分类。这里有两类数据，A和B。\n2、实施算法\ntile:重复某个数组。比如tile(A,n)，功能是将数组A重复n次，构成一个新的数组.\n1 >>> tile([1,2],(4)) 2 array([1, 2, 1, 2, 1, 2, 1, 2]) 3 >>> tile([1,2],(4,1)) 4 array([[1, 2], 5 [1, 2], 6 [1, 2], 7 [1, 2]]) 8 >>> tile([1,2],(4,2)) 9 array([[1, 2, 1, 2], 10 [1, 2, 1, 2], 11 [1, 2, 1, 2], 12 [1, 2, 1, 2]])\n欧式距离算法实现：\n1 def classify0(inX, dataSet, labels, k): 2 dataSetSize = dataSet.shape[0] 3 diffMat = tile(inX, (dataSetSize,1)) - dataSet #新数据与样本数据每一行的值相减 [[x-x1,y-y1],[x-x2,y-y2],[x-x3,y-y3],.....] 4 sqDiffMat = diffMat**2 #数组每一项进行平方[[(x-x1)^2,(y-y1)^2],........] 5 sqDistances = sqDiffMat.sum(axis=1)#数组每个特证求和[[(x-xi)^2+(y-yi)^2],......] 6 distances = sqDistances**0.5 #数组每个值 开根号 ，，欧式距离公式 完成。。。。 7 sortedDistIndicies = distances.argsort() #argsort函数返回的是数组值从小到大的索引值 8 classCount={} #以下是选取 距离最小的前k个值的索引，从k个中选取分类最多的一个作为新数据的分类 9 for i in range(k):# 统计前k个点所属的类别 10 voteIlabel = labels[sortedDistIndicies[i]] 11 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 12 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) 13 return sortedClassCount[0][0]# 返回前k个点中频率最高的类别\n其中 inX:需要分类的新数据，dataSet：样本数据特征，labels：样本数据分类，k：选取前k个最近的距离\n测试算法：\n1 >>> group,labels = kNN.createDataSet() 2 >>> group,labels 3 (array([[ 1. , 1.1], 4 [ 1. , 1. ], 5 [ 0. , 0. ], 6 [ 0. , 0.1]]), ['A', 'A', 'B', 'B']) 7 >>> kNN.classify0([0,0],group,labels,3) 8 'B' 9 >>>\n测试结果：[0,0]属于分类B.\n3、如何测试分类器\n四、 示例：使用k-近邻算法改进约会网站的配对效果\n我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象。尽管约会网站会推荐不同的人选，但她并不是喜欢每一个人。经过一番总结，她发现曾交往过三种类型的人：\n不喜欢的人\n魅力一般的人\n极具魅力的人\n海伦希望我们的分类软件可以更好地帮助她将匹配对象划分到确切的分类中。此外海伦还收集了一些约会网站未曾记录的数据信息，她认为这些数据更有助于匹配对象的归类。\n1、准备数据：从文本文件中解析数据\n数据存放在文本文件datingTestSet.txt中，每个样本数据占据一行，总共有1000行。\n海伦的样本主要包含以下3种特征：\n每年获得的飞行常客里程数\n玩视频游戏所耗时间百分比\n每周消费的冰淇淋公升数\n2、分析数据：使用Matplotlib创建散点图\n散点图使用datingDataMat矩阵的第一、第二列数据，分别表示特征值“每年获得的飞行常客里程数”和“玩视频游戏所耗时间百分比”。\n每年赢得的飞行常客里程数与玩视频游戏所占百分比的约会数据散点图\n3、准备数据：归一化数值\n不同特征值有不同的均值和取值范围，如果直接使用特征值计算距离，取值范围较大的特征将对距离计算的结果产生绝对得影响，而使较小的特征值几乎没有作用，近乎没有用到该属性。如两组特征：{0, 20000, 1.1}和{67, 32000, 0.1}，计算距离的算式为：\n显然第二个特征将对结果产生绝对得影响，第一个特征和第三个特征几乎不起作用。\n然而，对于识别的过程，我们认为这不同特征是同等重要的，因此作为三个等权重的特征之一，飞行常客里程数并不应该如此严重地影响到计算结果。\n在处理这种不同取值范围的特征值时，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或者1到1之间。下面的公式可以将任意取值范围的特征值转化为0到1区间内的值：\nnewValue = (oldValue – min) / (max – min)\n其中min和max分别是数据集中的最小特征值和最大特征值。\n添加autoNorm()函数，用于将数字特征值归一化：\n1 def autoNorm(dataSet): 2 minVals = dataSet.min(0)# 分别求各个特征的最小值 3 maxVals = dataSet.max(0)# 分别求各个特征的最大值 4 ranges = maxVals - minVals# 各个特征的取值范围 5 normDataSet = zeros(shape(dataSet)) 6 m = dataSet.shape[0] 7 normDataSet = dataSet - tile(minVals, (m,1)) # oldValue - min 8 normDataSet = normDataSet/tile(ranges, (m,1)) #element wise divide (oldValue-min)/(max-min) 数据归一化处理 9 return normDataSet, ranges, minVals\n对这个函数，要注意返回结果除了归一化好的数据，还包括用来归一化的范围值ranges和最小值minVals，这将用于对测试数据的归一化。\n注意，对测试数据集的归一化过程必须使用和训练数据集相同的参数（ranges和minVals），不能针对测试数据单独计算ranges和minVals，否则将造成同一组数据在训练数据集和测试数据集中的不一致。\n4、测试算法：作为完整程序验证分类器\n机器学习算法一个很重要的工作就是评估算法的正确率，通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率。需要注意的是，10%的测试数据应该是随机选择的。由于海伦提供的数据并没有按照特定目的来排序，所以我们可以随意选择10%数据而不影响其随机性。\n创建分类器针对约会网站的测试代码：利用样本集数据进行测试算法\n1 def datingClassTest(): 2 hoRatio = 0.50 #hold out 10% 3 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt') #load data setfrom file 4 normMat, ranges, minVals = autoNorm(datingDataMat) 5 m = normMat.shape[0] 6 numTestVecs = int(m*hoRatio) 7 errorCount = 0.0 8 for i in range(numTestVecs): 9 classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) 10 print \"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, datingLabels[i]) 11 if (classifierResult != datingLabels[i]): errorCount += 1.0 12 print \"the total error rate is: %f\" % (errorCount/float(numTestVecs)) 13 print errorCount\n执行分类器测试程序：\n1 >>> kNN.datingClassTest() 2 3 the classifier came back with: 2, the real answer is: 1 4 5 the classifier came back with: 2, the real answer is: 2 6 7 the classifier came back with: 1, the real answer is: 1 8 9 the classifier came back with: 1, the real answer is: 1 10 11 the classifier came back with: 2, the real answer is: 2 12 13 ................................................. 14 15 the total error rate is: 0.064000 16 17 32.0\n分类器处理约会数据集的错误率是6.4%，这是一个相当不错的结果。我们可以改变函数datingClassTest内变量hoRatio和变量k的值，检测错误率是否随着变量值的变化而增加。\n这个例子表明我们可以正确地预测分类，错误率仅仅是2.4%。海伦完全可以输入未知对象的属性信息，由分类软件来帮助她判定某一对象的可交往程度：讨厌、一般喜欢、非常喜欢。\n5、使用算法：构建完整可用系统\n综合上述代码，我们可以构建完整的约会网站预测函数：对输入的数据需要 归一化处理\n1 def classifyPerson(): 2 resultList = ['not at all', 'in small doses', 'in large doses'] 3 percentTats = float(raw_input(\"Percentage of time spent playing video game?\")) 4 ffMiles = float(raw_input(\"Frequent flier miles earned per year?\")) 5 iceCream = float(raw_input(\"Liters of ice cream consumed per year?\")) 6 datingDataMat, datingLabels = file2matrix('datingTestSet.txt') 7 normMat, ranges, minVals = autoNorm(datingDataMat) 8 inArr = array([ffMiles, percentTats, iceCream]) #新数据 需要归一化处理 9 classifierResult = classify((inArr - minVals) / ranges, normMat, datingLabels, 3) 10 print \"You will probably like this person: \", resultList[classifierResult - 1]\n目前为止，我们已经看到如何在数据上构建分类器。\n完整代码：\n1 ''' 2 Created on Sep 16, 2010 3 kNN: k Nearest Neighbors 4 5 Input: inX: vector to compare to existing dataset (1xN) 6 dataSet: size m data set of known vectors (NxM) 7 labels: data set labels (1xM vector) 8 k: number of neighbors to use for comparison (should be an odd number) 9 10 Output: the most popular class label 11 12 @author: pbharrin 13 ''' 14 from numpy import * 15 import operator 16 from os import listdir 17 import matplotlib 18 import matplotlib.pyplot as plt 19 def show(d,l): 20 #d,l=kNN.file2matrix('datingTestSet2.txt') 21 fig=plt.figure() 22 ax=fig.add_subplot(111) 23 ax.scatter(d[:,0],d[:,1],15*array(l),15*array(l)) 24 plt.show() 25 def show2(): 26 datingDataMat,datingLabels=file2matrix('datingTestSet2.txt') 27 fig = plt.figure() 28 ax = fig.add_subplot(111) 29 l=datingDataMat.shape[0] 30 X1=[] 31 Y1=[] 32 X2=[] 33 Y2=[] 34 X3=[] 35 Y3=[] 36 for i in range(l): 37 if datingLabels[i]==1: 38 X1.append(datingDataMat[i,0]);Y1.append(datingDataMat[i,1]) 39 elif datingLabels[i]==2: 40 X2.append(datingDataMat[i,0]);Y2.append(datingDataMat[i,1]) 41 else: 42 X3.append(datingDataMat[i,0]);Y3.append(datingDataMat[i,1]) 43 type1=ax.scatter(X1,Y1,c='red') 44 type2=ax.scatter(X2,Y2,c='green') 45 type3=ax.scatter(X3,Y3,c='blue') 46 #ax.axis([-2,25,-0.2,2.0]) 47 ax.legend([type1, type2, type3], [\"Did Not Like\", \"Liked in Small Doses\", \"Liked in Large Doses\"], loc=2) 48 plt.xlabel('Percentage of Time Spent Playing Video Games') 49 plt.ylabel('Liters of Ice Cream Consumed Per Week') 50 plt.show() 51 52 def classify0(inX, dataSet, labels, k): 53 dataSetSize = dataSet.shape[0] 54 diffMat = tile(inX, (dataSetSize,1)) - dataSet 55 sqDiffMat = diffMat**2 56 sqDistances = sqDiffMat.sum(axis=1) 57 distances = sqDistances**0.5 58 sortedDistIndicies = distances.argsort() 59 classCount={} 60 for i in range(k): 61 voteIlabel = labels[sortedDistIndicies[i]] 62 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 63 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) 64 return sortedClassCount[0][0] 65 66 def createDataSet(): 67 group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) 68 labels = ['A','A','B','B'] 69 return group, labels 70 71 def file2matrix(filename): 72 fr = open(filename) 73 numberOfLines = len(fr.readlines()) #get the number of lines in the file 74 returnMat = zeros((numberOfLines,3)) #prepare matrix to return 75 classLabelVector = [] #prepare labels return 76 fr = open(filename) 77 index = 0 78 for line in fr.readlines(): 79 line = line.strip() 80 listFromLine = line.split('\\t') 81 returnMat[index,:] = listFromLine[0:3] 82 classLabelVector.append(int(listFromLine[-1])) 83 index += 1 84 return returnMat,classLabelVector 85 86 def autoNorm(dataSet): 87 minVals = dataSet.min(0) 88 maxVals = dataSet.max(0) 89 ranges = maxVals - minVals 90 normDataSet = zeros(shape(dataSet)) 91 m = dataSet.shape[0] 92 normDataSet = dataSet - tile(minVals, (m,1)) 93 normDataSet = normDataSet/tile(ranges, (m,1)) #element wise divide 94 return normDataSet, ranges, minVals 95 96 def datingClassTest(): 97 hoRatio = 0.50 #hold out 10% 98 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt') #load data setfrom file 99 normMat, ranges, minVals = autoNorm(datingDataMat) 100 m = normMat.shape[0] 101 numTestVecs = int(m*hoRatio) 102 errorCount = 0.0 103 for i in range(numTestVecs): 104 classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) 105 print \"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, datingLabels[i]) 106 if (classifierResult != datingLabels[i]): errorCount += 1.0 107 print \"the total error rate is: %f\" % (errorCount/float(numTestVecs)) 108 print errorCount 109 110 def img2vector(filename): 111 returnVect = zeros((1,1024)) 112 fr = open(filename) 113 for i in range(32): 114 lineStr = fr.readline() 115 for j in range(32): 116 returnVect[0,32*i+j] = int(lineStr[j]) 117 return returnVect 118 119 def handwritingClassTest(): 120 hwLabels = [] 121 trainingFileList = listdir('trainingDigits') #load the training set 122 m = len(trainingFileList) 123 trainingMat = zeros((m,1024)) 124 for i in range(m): 125 fileNameStr = trainingFileList[i] 126 fileStr = fileNameStr.split('.')[0] #take off .txt 127 classNumStr = int(fileStr.split('_')[0]) 128 hwLabels.append(classNumStr) 129 trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr) 130 testFileList = listdir('testDigits') #iterate through the test set 131 errorCount = 0.0 132 mTest = len(testFileList) 133 for i in range(mTest): 134 fileNameStr = testFileList[i] 135 fileStr = fileNameStr.split('.')[0] #take off .txt 136 classNumStr = int(fileStr.split('_')[0]) 137 vectorUnderTest = img2vector('testDigits/%s' % fileNameStr) 138 classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) 139 print \"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, classNumStr) 140 if (classifierResult != classNumStr): errorCount += 1.0 141 print \"\\nthe total number of errors is: %d\" % errorCount 142 print \"\\nthe total error rate is: %f\" % (errorCount/float(mTest))\nView Code"}
{"content2":"转：http://isilic.iteye.com/blog/1851048\n决策树的重要性和入门可以参考前面两篇文章：\n在清华水木上有个Machine Learning的书单: http://www.newsmth.net/nForum/#!article/AI/34859\n其中作为入门的几本书也不简单，都是经典的作品PRML或者是最新的著作(ML-APP)，这些书在网上都能找到，不过找到不过不看放在硬盘里的话，其实这些书对你的用处并不大。\n这些书都能在网上找到，我就不贴下载了，大家可以自行查找。\n入门：\nPattern Recognition And Machine Learning\nAuthor：hristopher M. Bishop\nMachine Learning : A Probabilistic Perspective\nKevin P. Murphy\nThe Elements of Statistical Learning : Data Mining, Inference, and Prediction\nTrevor Hastie, Robert Tibshirani, Jerome Friedman\nInformation Theory, Inference and Learning Algorithms\nDavid J. C. MacKay\nAll of Statistics : A Concise Course in Statistical Inference\nLarry Wasserman\n优化：\nConvex Optimization\nStephen Boyd, Lieven Vandenberghe\nNumerical Optimization\nJorge Nocedal, Stephen Wright\nOptimization for Machine Learning\nSuvrit Sra, Sebastian Nowozin, Stephen J. Wright\n核方法：\nKernel Methods for Pattern Analysis\nJohn Shawe-Taylor, Nello Cristianini\nLearning with Kernels : Support Vector Machines, Regularization, Optimization, and Beyond\nBernhard Schlkopf, Alexander J. Smola\n半监督：\nSemi-Supervised Learning\nOlivier Chapelle\n高斯过程：\nGaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)\nCarl Edward Rasmussen, Christopher K. I. Williams\n概率图模型：\nGraphical Models, Exponential Families, and Variational Inference\nMartin J Wainwright, Michael I Jordan\nBoosting:\nBoosting : Foundations and Algorithms\nSchapire, Robert E.; Freund, Yoav\n贝叶斯:\nStatistical Decision Theory and Bayesian Analysis\nJames O. Berger\nThe Bayesian Choice : From Decision-Theoretic Foundations to Computational Implementation\nChristian P. Robert\nBayesian Nonparametrics\nNils Lid Hjort, Chris Holmes, Peter Müller, Stephen G. Walker\nPrinciples of Uncertainty\nJoseph B. Kadane\nDecision Theory : Principles and Approaches\nGiovanni Parmigiani, Lurdes Inoue\n蒙特卡洛：\nMonte Carlo Strategies in Scientific Computing\nJun S. Liu\nMonte Carlo Statistical Methods\nChristian P.Robert, George Casella\n信息几何：\nMethods of Information Geometry\nShun-Ichi Amari, Hiroshi Nagaoka\nAlgebraic Geometry and Statistical Learning Theory\nWatanabe, Sumio\nDifferential Geometry and Statistics\nM.K. Murray, J.W. Rice\n渐进收敛：\nAsymptotic Statistics\nA. W. van der Vaart\nEmpirical Processes in M-estimation\nGeer, Sara A. van de\n不推荐：\nStatistical Learning Theory\nVladimir N. Vapnik\nBayesian Data Analysis, Second Edition\nAndrew Gelman, John B. Carlin, Hal S. Stern, Donald B. Rubin\nProbabilistic Graphical Models : Principles and Techniques\nDaphne Koller, Nir Friedman\n另外在微博上也有北美比较常用的机器学习/自然语言处理/语音处理经典书籍的推荐，其中的推荐面比较广，可以看下，和水木上的推荐有重叠。"}
{"content2":"1. 从一个栗子开始 - Slot Filling\n比如在一个订票系统上，我们的输入 “Arrive Taipei on November 2nd” 这样一个序列，我们设置几个槽位（Slot），希望算法能够将关键词'Taipei'放入目的地（Destination）槽位， 将November和2nd放入到达时间（Time of Arrival）槽位，将Arrive和on放入其他（Other）槽位，实现对输入序列的一个归类，以便后续提取相应信息。\n用前馈神经网络（Feedforward Neural Network）来解决这个问题的话，我们首先要对输入序列向量化，将每一个输入的单词用向量表示，可以使用 One-of-N Encoding 或者是 Word hashing 等编码方法，输出预测槽位的概率分布。\n但是这样做的话，有个问题就出现了。如果现在又有一个输入是 “Leave Taipei on November 2nd”，这里Taipei是作为一个出发地（Place of Departure），所以我们应当是把Taipei放入Departure槽位而不是Destination 槽位，可是对于前馈网络来说，对于同一个输入，输出的概率分布应该也是一样的，不可能出现既是Destination的概率最高又是Departure的概率最高。\n所以我们就希望能够让神经网络拥有“记忆”的能力，能够根据之前的信息（在这个例子中是Arrive或Leave）从而得到不同的输出。将两段序列中的Taipei分别归入Destionation槽位和Departure槽位。\n2. RNN\n基本概念\n在RNN中，隐层神经元的输出值都被保存到记忆单元中，下一次再计算输出时，隐层神经元会将记忆单元中的值认为是输入的一部分来考虑\nRNN中考虑了输入序列顺序，序列顺序的改变会影响输出的结果。\n常见变体\nElman Network\n将隐层的输出（即记忆单元中的值）作为下一次的输入\n\\(h_t = \\sigma_h(W_hx_t + U_h\\color{green}{h_{t-1}} + b_h)\\)\n\\(y_t = \\sigma_h(W_yh_t + b_y)\\)\nJordan Network\n将上一时间点的输出值作为输入\n\\(h_t = \\sigma_h(W_hx_t + U_h\\color{green}{y_{t-1}} + b_h)\\)\n\\(y_t = \\sigma_h(W_yh_t + b_y)\\)\nBidirectional RNN\n3. Long Short-term Memory （LSTM）\n基本结构\n由Memory Cell， Input Gate， Output Gate， Forget Gate 组成\n特殊的神经元结构，包含4个input（三个Gate的控制信号以及输入的数据），1个output\n激活函数通常选用sigmoid function， sigmoid的输出介于0到1之间，表征了Gate的打开程度。\nTraditional LSTM\n\\[ \\begin{align} f_t & = \\sigma_g(W_fx_t + \\color{green}{U_fh_{t-1}} + b_f) \\\\ i_t & = \\sigma_g(W_i x_t + \\color{green}{U_ih_{t-1}} + b_i) \\\\ o_t & = \\sigma_g(W_o x_t + \\color{green}{U_oh_{t-1}} + b_o) \\\\ c_t & = f_t\\,{\\circ}\\,c_{t-1} + i_t\\,{\\circ}\\,\\sigma_c(W_cx_t\\color{green}{+ U_ch_{t-1}} +b_c) \\\\ h_t & = o_t \\,{\\circ}\\, \\sigma_h(c_t) \\end{align} \\]\nPeephole LSTM， 在大部分的情况下，用\\(\\color{blue}{c_{t-1}}\\)取代\\(\\color{green}{h_{t-1}}\\)\n\\[ \\begin{align} f_t & = \\sigma_g(W_fx_t + \\color{green}{U_f\\color{blue}{c_{t-1}}} + b_f) \\\\ i_t & = \\sigma_g(W_i x_t + \\color{green}{U_i\\color{blue}{c_{t-1}}} + b_i) \\\\ o_t & = \\sigma_g(W_o x_t + \\color{green}{U_o\\color{blue}{c_{t-1}}} + b_o) \\\\ c_t & = f_t\\,{\\circ}\\,c_{t-1} + i_t\\,{\\circ}\\,\\sigma_c(W_cx_t +b_c) \\\\ h_t & = o_t \\,{\\circ}\\, \\sigma_h(c_t) \\end{align} \\]\n\\(x_t\\)表示输入向量，\\(h_t\\)表示输出向量，\\(c_t\\)表示记忆单元的状态向量，\\(\\circ\\)代表Hadamard product（A.k.a. Schur product）\n\\(W\\)表示输入权重，\\(U\\)表示循环权重，\\(b\\)表示偏置\n\\(\\delta_g\\)代表sigmoid function，\\(\\delta_c\\)代表hyperbolic tangent， \\(\\delta_h\\)表示 hyperbolic tangent（peephole LSTM论文中建议选用\\(\\delta_h(x)=x\\)）\n\\(f_t\\)，\\(i_t\\)和\\(o_t\\)表示门控向量值\n\\(f_t\\)表示遗忘门向量，表征记忆旧信息的能力\n\\(i_t\\)表示输入门向量，表征获取新信息的能力\n\\(o_t\\)表示输出门向量，表征输出信息的能力\n补充知识点\nShort-term，表示保留对前一时间点输出的短期记忆，相比于最原始的RNN结构中的记忆单元（每次有新的输入时记忆体的状态就会被更新，因此是短期的记忆），而LSTM的记忆体则拥有相对较长的记忆时间（由Forget Gate决定），所以是Long Short-term\nLSTM一般采用多层结构组合，Multiple-layer LSTM\nKeras中实现了LSTM，GRU（[Cho,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,EMNLP'14] 只有两个Gate，容易训练），SimpleRNN层，可以方便的调用。\n4. RNN如何学习？\n损失函数的定义：\n每一个时间点的RNN的输出和标签值的交叉熵（cross-entropy）之和\n训练过程：\n使用被称作Backpropagation through time（BPTT）的梯度下降法\n训练其实是比较困难的，因为Total Loss可能会出现剧烈的抖动\n根据论文[Razvan Pascanu,On the difficulty of training Recurrent Neural Networks,ICML'13]上对RNN的分析，损失函数的表面要么非常平坦，要么非常陡峭（The error surface is either very flat or very steep），当你的参数值在较为平坦的区域做更新时，因此该区域梯度值比较小，此时的学习率一般会变得的较大，如果突然到达了陡峭的区域，梯度值陡增，再与此时较大的学习率相乘，参数就有很大幅度更新（实线表示的轨迹），因此学习过程非常不稳定。Razvan Pascanu使用了叫做“Clipping”的训练技巧：为梯度设置阈值，超过该阈值的梯度值都会被cut，这样参数更新的幅度就不会过大（虚线表示的轨迹），因此容易收敛。\n为什么在RNN中会有这种问题？\n是因为激活函数选用了sigmoid而不是ReLU么？然而并不是。事实上，在RNN中使用ReLU反而效果会不如Sigmoid，不过也是看你的参数初始化值的选取，所以也不一定，比如后面提到的Quoc V.Le的那篇文章，使用特别初始化技巧硬训ReLU的RNN得到了可比拟LSTM的效果。因此激活函数并不是这里的关键点。\n那究竟是什么原因呢？我们来分析梯度更新公式中的\\(w-\\eta\\frac{\\partial{L}}{\\partial{w}}\\)来探寻一番。但是这样一个偏微分的关系我们应该如何来分析呢？这里我们用一个技巧：给w值一个微小的变化，观察对应的Loss的变化情况。假设当前模型是1000个只含有一个线性隐层的RNN级联结构。并假设我们当前的输入是100000……（只有第一个值是1，剩下全是0），因此最后的输出值是\\(w^{999}\\)。现在假设我们\\(w\\)的值是1，那么RNN在最后时间点的输出是1，给\\(w\\)一个微小的变化+0.01，此时的输出变成了大约20000！这段区域呈现出一个陡峭的趋势。如果给\\(w\\)一个微小的变化-0.01变为0.99，测试的输出基本变成0，哪怕是\\(w\\)变到0.01时，输出依旧是0，这段区域呈现出一个平坦的趋势。因此我们可以看出由于RNN采用时间序列的结构，权重值在不同时间点被反复使用，这种累积性的变化可能对结果造成极大的影响，也可能会很长一段时间保持平稳。\n常用的技巧\n使用LSTM单元。LSTM单元可以处理梯度消失问题，但无法处理梯度爆炸问题。为什么呢？这是因为RNN和LSTM对待记忆单元的做法是不同的，RNN中每一个时间点的记忆单元中的内容（状态）都会更新，而LSTM则是将记忆单元中的值与输入值相加（按某种权值）再更新状态，记忆单元中的值会始终对输出产生影响（除非Forget Gate完全的关闭），因此不用担心梯度值会弥散，相反的，这倒极易引起梯度爆炸。\n采用一些更新颖的结构或训练方法，比如：\nClockwise RNN [Jan Koutnik,A Clockwork RNN,JMLR'14]\nStructurally Constrained Recurrent Network（SCRN）[Tomas Mikolov,Learning Longer Memory in Recurrent Neural Networks,ICLR'15]\nVanilla RNN Initialized with Identity matrix + ReLU activation function [Quoc V.Le,A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,arXiv'15]该位仁兄用了特别的初始化技巧，硬训RNN，效果可比拟甚至超越LSTM\n5. RNN的更多应用场景\nSentiment Analysis 情感分析\nKey Term Extraction 关键字提取\nSpeech Recognition 语音辨识\nConnectionist Temporal Classification（CTC）:语音辨识中的一个关键技术，通过增加一个额外的Symbol代表NULL来解决叠字问题（参考论文[Graves, Alex, and Navdeep Jaitly. \"Towards end-to-end speech recognition with recurrent neural networks.\" Proceedings of the 31st International Conference on Machine Learning (ICML-14). 2014.]）。\nSequence to sequence learning（输入和输出都是不同长度的序列）\nMachine Translation 机器翻译\nSyntactic parsing 句法分析\nSeq-to-seq Auto-encoder\n将文档转换为向量表示（BoW模型会忽略掉语序，在某些情况下相反意思的语句会产生相同的词袋模型，而RNN的方法考虑语序，因此更为鲁棒）\n将语音转换为向量表示\nAttention-based Model 注意力模型\nNeural Turing Machine 神经图灵机\nReading Comprehension\n[End-To-End Memory Networks. S. Sukhbaatar, A. Szlam, J. Weston, R. Fergus. NIPS, 2015.]\n基于Keras实现的一个example\n问答系统\n6. 其他的学习资料\nThe Unreasonable Effectiveness of Recurrent Neural Networks\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/\nUnderstanding LSTM Networks\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/\n7. 本文参考资料\nMachine Learning (2016,Fall), Hung-yi Lee, NTU\nhttp://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/RNN%20(v2).pdf\nDeep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville\nhttp://www.deeplearningbook.org/\nDeep Learning in a Nutshell: Sequence Learning\nhttps://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning/\nLong short-term memory\nhttps://en.wikipedia.org/wiki/Long_short-term_memory\nDeep & Structured 未完待续"}
{"content2":"几千年来，人们就已经有了思考如何构建智能机器的想法。从那时开始，人工智能 (AI) 经历了起起落落，这证明了它的成功以及还未实现的潜能。如今，随时都能听到应用机器学习算法来解决新问题的新闻。从癌症检测和预测到图像理解和总结以及自然语言处理，AI 正在增强人们的能力和改变我们的世界。\n现代 AI 的历史包含一部伟大的戏剧应具有的所有要素。上世纪 50 年代，随着对思维机器及阿兰·图灵和约翰·冯·诺依曼等著名人物的关注，AI 开始崭露头角。尽管随后经历了数十年的繁荣与萧条，并被寄予了难以实现的厚望，但 AI 和它的先驱们仍然一直在努力前行。如今，AI 展现出了它的真正潜力，专注于应用并提供深度学习和认知计算等技术。\n本文将探索 AI 的一些重要方面和它的子领域。我们首先会分析 AI 的时间线，然后深入介绍每种要素。\n现代 AI 的时间线\n从上世纪 50 年代开始，现代 AI 开始专注于所谓的强 AI，强 AI 指的是能普遍执行人类所能执行的任何智能任务的 AI。强 AI 的进展乏力，最终导致了所谓的弱 AI，或者将 AI 技术应用于更小范围的问题。直到上世纪 80 年代，AI 研究被拆分为这两种范式。但在 1980 年左右，机器学习成为了一个突出的研究领域，它的目标是让计算机能学习并构建模型，以便能够执行一些活动，比如特定领域中的预测。\n点击查看大图深度学习于 2000 年左右出现，建立在 AI 和机器学习的研究成果之上。计算机科学家通过新的拓扑结构和学习方法，在许多层中使用神经网络。神经网络的这次演变成功解决了各种不同领域的复杂问题。\n在过去 10 年中，认知计算兴起，它的目标是构建能学习并自然地与人交流的系统。IBM Watson 通过在 Jeopardy 比赛上成功击败世界级对手，证明了认知计算的能力。\n在本教程中，我将探索每个领域，解释一些促使认知计算取得成功的重要算法。\n基础 AI\n1950 年前的研究中提出了大脑由电脉冲网络组成的理念，这些脉冲触发并以某种方式精心组织形成思想和意识。阿兰·图灵表明任何计算都能以数字方式实现，那时，距离实现构建一台能模仿人脑的机器的想法也就不远了。\n许多早期研究都重点关注过这个强 AI 方面，但这一时期也引入了一些基础概念，如今的所有机器学习和深度学习都是在这些概念基础上建立起来的。\n图 2. 1980 年前的人工智能方法的时间线\nAI 即搜索\nAI 中的许多问题都可以通过暴力搜索(比如深度或广度优先搜索)来解决。但是，考虑到普通问题的搜索空间，基本搜索很快就会招架不住。AI 即搜索的最早示例之一是一个下棋程序的开发。Arthur Samuel 在 IBM 701 Electronic Data Processing Machine 上构建了第一个这样的程序，对搜索树执行一种名为 α-β 剪枝技术(alpha-beta pruning)的优化。他的程序还会记录特定某步棋的回报，允许应用程序学习每一场比赛(使它成为了第一个自主学习的程序)。为了提高程序的学习速度，Samuel 将它设计为能够自己跟自己下棋，提高了它的下棋和学习能力。\n尽管可以成功地应用对许多简单问题的搜索，但随着选择数量的增加，该方法很快就会行不通。以简单的井字棋游戏为例。在游戏开始时，有 9 种可能的棋着。每步棋着会导致 8 种可能的对抗棋着，以此类推。井字棋完整的棋着树(未进行旋转优化来删除重复棋着)有 362,880 个节点。如果您将同样的思维试验推广到象棋或围棋，很快就会看到搜索的缺点。\n感知器\n感知器是一种用于单层神经网络的早期的监督式学习算法。给定一个输入特征矢量，感知器算法就能学习将输入划分到特定类别。通过使用训练集，可以更新线性分类的网络的权值和阀值。感知器最初是针对 IBM 704 实现的，随后被用在定制硬件上，用于图像识别。\n图 3. 感知器和线性分类\n作为线性分类器，感知器能线性地分离问题。感知器的局限性的重要示例是，它无法学习一个异或 (XOR) 函数。多层感知器解决了这一问题，为更复杂的算法、网络拓扑结构和深度学习铺平了道路。\n集群算法\n对于感知器，学习方法是监督式的。用户提供数据来训练网络，然后针对新数据来测试网络。集群算法采用了一种不同的学习方法，叫做无监督学习。在此模型中，算法基于数据的一个或多个属性，将一组特征矢量组织到集群中。\n可通过少量代码实现的最简单的算法之一称为 k 均值。在此算法中，k 表示您可向其中分配样本的集群数量。您可以使用一个随机特征矢量初始化一个集群，然后将其他所有样本添加到离它们最近的集群(前提是每个样本表示一个特征矢量，而且使用了一种欧几里德距离来标识 “距离”)。随着您将样本添加到集群中，它的质心 — 即集群的中心 — 会被重新计算。然后该算法会再次检查样本，确保它们存在于最近的集群中，并在没有样本改变集群成员关系时停止运行。\n尽管 k 均值的效率相对较高，但您必须提前指定 k。根据所用的数据，其他方法可能更高效，比如分层或基于分布的集群方法 。\n决策树\n与集群紧密相关的是决策树。决策树是一种预测模型，对可得出某个结论的观察值进行预测。树上的树叶代表结论，而节点是观察值分叉时所在的决策点。决策树是利用决策树学习算法来构建的，它们根据属性值测试将数据集拆分为子集(通过一个称为递归分区的流程)。\n考虑下图中的示例。在这个数据集中，我可以根据 3 个因素来观察某个人何时的生产力较高。使用决策树学习算法时，我可以使用一个指标来识别属性(比如信息增益)。在这个示例中，情绪是生产力的主要因素，所以我依据 “good mood” 是 Yes 还是 No 来拆分数据集。No 分支很简单：它始终导致生产力低下。但是，Yes 分支需要根据其他两个属性来再次拆分数据集。我给数据集涂上颜色，以演示何处的观察值通向我的叶节点。\n点击查看大图决策树的一个有用方面是它们的内在组织，您能轻松且图形化地解释您是如何分类一个数据项的。流行的决策树学习算法包括 C4.5 和分类回归树。\n基于规则的系统\n第一个根据规则和推断来构建的系统称为 Dendral，是 1965 年开发出来的，但直到上世纪 70 年代，这些所谓的 “专家系统” 才得到大力发展。基于规则的系统可以存储知识和规则，并使用一个推理系统来得出结论。\n基于规则的系统通常包含一个规则集、一个知识库、一个推理引擎(使用前向或后向规则链)，以及一个用户界面。在下图中，我使用一段信息(“苏格拉底是一个凡人”)、一条规则(“凡人终有一死”)和一次关于谁会死的交互。\n基于规则的系统已应用于语音识别，规划和控制，以及疾病识别。上世纪 90 年代开发的一个监视和诊断坝体稳定性的系统 Kaleidos 至今仍在运营。\n机器学习\n机器学习是 AI 和计算机科学的一个子领域，起源于统计学和数学优化。机器学习涵盖应用于预测、分析和数据挖掘的监督式和非监督式学习技术。它并不仅限于深度学习，在本节中，我将探讨一些实现这种效率奇高的方法的算法。板面的做法和配料\n反向传播算法(Backpropagation)\n神经网络的真正威力在于它们的多层变形。训练单层感知器很简单，但得到的网络不是很强大。那么问题就变成了如何训练有多个层的网络?这时就会用到反向传播算法。\n反向传播是一种训练有许多层的神经网络的算法。它分两个阶段执行。第一阶段是通过一个神经网络将输入传播到最后一层(称为前馈)。在第二阶段，算法计算一个错误，然后将此错误从最后一层反向传播(调节权值)到第一层。\n在训练期间，网络的中间层自行进行组织，以便将输入空间的各部分映射到输出空间。通过监督式学习，反向传播识别输入-输出映射中的错误，然后相应地(以一定的学习速率)调整权值来更正此错误。反向传播一直是神经网络学习的一个重要方面。随着计算资源消耗得更快和变得更廉价，反向传播会继续被应用于更大更密集的网络。\n卷积神经网络(Convolutional neural networks)\n卷积神经网络 (CNN) 是受动物视觉皮质启发的多层神经网络。该架构在各种应用中都很有用，包括图像处理。第一个 CNN 是 Yann LeCun 创建的，当时，该架构专注于手写字符识别任务，比如读取邮政编码。\nLeNet CNN 架构包含多层，这些层实现了特征提取，然后实现了分类。图像被分成多个接受区，注入可从输入图像中提取特征的卷积层。下一步是池化，它可以(通过下采样)降低提取特征的维度，同时(通常通过最大池化)保留最重要的信息。然后该算法执行另一个卷积和池化步骤，注入一个完全连通的多层感知器。此网络的最终输出层是一组节点，这些节点标识了图像的特征(在本例中，每个节点对应一个识别出的数字)。用户可以通过反向传播训练该网络。\n深层处理、卷积、池化和一个完全连通的分类层的使用，为神经网络的各种新应用开启了一扇门。除了图像处理之外，CNN 还被成功应用到许多视频识别和自然语言处理的任务中。CNN 也已在 GPU 中获得高效实现，显著提高了它们的性能。\n长短期记忆\n回想一下，在反向传播的讨论中曾提到过，该网络是用前馈方式进行训练的。在这个架构中，用户将输入注入网络中，通过隐藏层将它们前向传播到输出层。但是，还有许多其他神经网络拓扑结构。此处分析的拓扑结构允许在节点之间建立连接，以便形成一个定向循环。这些网络被称为递归神经网络，它们能反向馈送到前几层或它们的层中的后续节点。该属性使这些网络成为处理时序数据的理想选择。\n1997 年，人们创建了一种名为长短期记忆 (LSTM) 的特殊回归网络。LSTM 由记忆细胞组成，网络中的这些细胞会短期或长期记住一些值。\n记忆细胞包含控制信息如何流进或流出细胞的闸门。输入门控制新信息何时能流入记忆中。遗忘门控制一段现有信息保留的时长。最后，输出门控制细胞中包含的信息何时用在来自该细胞的输出中。记忆细胞还包含控制每个门的权值。训练算法通常沿时间反向传播(反向传播的一种变体)，可以根据得到的错误来优化这些权值。\nLSTM 已被应用于语音识别、手写体识别、文本到语音合成、图像字幕和其他各种任务。我很快会再介绍 LSTM。\n深度学习\n深度学习是一组相对较新的方法，它们正从根本上改变机器学习。深度学习本身不是一种算法，而是一系列通过无监督学习来实现深度网络的算法。这些网络非常深，以至于(除了计算节点集群外)需要采用新计算方法(比如 GPU)来构建它们。\n本文目前为止探讨了两种深度学习算法：CNNs 和 LSTMs。这些算法的组合已用于实现多种非常智能的任务。如下图所示，CNN 和 LSTM 已用于识别，以及使用自然语言描述照片或视频。\n点击查看大图深度学习算法也应用于面部识别，能以 96% 的准确度识别肺结核，自动驾驶汽车，以及其他许多复杂的问题。\n但是，尽管应用深度学习算法取得了这些成果，但是仍有一些亟待我们解决的问题。最近，深度学习在皮肤癌检测上的应用发现，该算法比获得职业认证的皮肤科医生更准确。但是，皮肤科医生能列举促使他们得出诊断结果的因素，而深度学习程序无法识别其在分类时使用了哪些因素。这就是所谓的深度学习黑盒问题。\n另一种应用称为 Deep Patient，能根据患者的医疗记录成功地预测疾病。事实证明，该应用预测疾病的能力比医生好得多 — 甚至是众所周知难以预测的精神分裂症。所以，尽管这些模型很有效，但没有人能真正弄清楚庞大的神经网络行之有效的原因。\n认知计算\nAI 和机器学习都有许多生物灵感方面的示例。早期 AI 专注于构建模仿人脑的机器的宏伟目标，而认知计算也致力于实现此目标。\n认知计算是根据神经网络和深度学习来构建的，正在应用来自认知科学的知识来构建模拟人类思维过程的系统。但是，认知计算没有专注于单组技术，而是涵盖多个学科，包括机器学习、自然语言处理、视觉和人机交互。\nIBM Watson 就是认知计算的一个示例，在 Jeopardy 上，IBM Watson 证实了它最先进的问答交互能力，但自那时起，IBM 已通过一系列 Web 服务扩展了该能力。这些服务公开的应用编程接口可用于视觉识别、语音到文本和文本到语音转换功能，语言理解和翻译，以及对话引擎，以构建强大的虚拟代理。"}
{"content2":"Python将是人工智能时代的最佳编程语言\n移动互联网取代PC互联网领跑在互联网时代的最前沿，Android和iOS一度成为移动互联网应用平台的两大霸主，成为移动开发者首选的两门技术，HTML5以其跨平台的优势在移动互联网应用平台占据重要位置，可以说是后来者居上。  由于技术的限制难以催生出更多的新应用，互联网+的产品日渐饱和，移动互联网从巅峰时代逐渐趋于平缓发展，下一个时代谁是主场？下一门应用技术谁来掌门？\n在第三届互联网大会中百度CEO李彦宏曾表述：靠移动互联网的风口已经没有可能再出现独角兽了，因为市场已经进入了一个相对平稳的发展阶段，互联网人口渗透率已经超过了50%。而未来的机会在人工智能。的确互联网巨头公司在人工智能领域投入明显增大，都力争做人工智能时代的“带头大哥”。\nPython作为一门编程语言，其魅力远超C#，Java,C,C++，它被昵称为“胶水语言”，更被热爱它的程序员誉为“最美丽的”编程语言。从云端、客户端，到物联网终端，python应用无处不在，同时也是人工智能首先的编程语言。\n在人工智能上使用Python编程语言的优势\n1.优质的文档\n2.平台无关，可以在现在每一个*nix版本上使用\n3.和其他面向对象编程语言比学习更加简单快速\n4.Python有许多图像加强库像Python Imaging Libary,VTK和Maya 3D可视化工具包，Numeric Python, Scientific Python和其他很多可用工具可以于数值和科学应用。\n5.Python的设计非常好，快速，坚固，可移植，可扩展。很明显这些对于人工智能应用来说都是非常重要的因素。\n6.对于科学用途的广泛编程任务都很有用，无论从小的shell脚本还是整个网站应用。\n7.最后，它是开源的。可以得到相同的社区支持。\nAI的Python库\n总体的AI库\nAIMA：Python实现了从Russell到Norvigs的“人工智能：一种现代的方法”的算法\npyDatalog：Python中的逻辑编程引擎baqist.cn\nSimpleAI：Python实现在“人工智能：一种现代的方法”这本书中描述过的人工智能的算法。它专注于提供一个易于使用，有良好文档和测试的库。\nEasyAI：一个双人AI游戏的python引擎（负极大值，置换表、游戏解决）\n机器学习库\nPyBrain 一个灵活，简单而有效的针对机器学习任务的算法，它是模块化的Python机器学习库。它也提供了多种预定义好的环境来测试和比较你的算法。\nPyML 一个用Python写的双边框架，重点研究SVM和其他内核方法。它支持Linux和Mac OS X。\nscikit-learn 旨在提供简单而强大的解决方案，可以在不同的上下文中重用：机器学习作为科学和工程的一个多功能工具。它是python的一个模块，集成了经典的机器学习的算法，这些算法是和python科学包（numpy,scipy.matplotlib）紧密联系在一起的。gbpcci.cn\nMDP-Toolkit 这是一个Python数据处理的框架，可以很容易的进行扩展。它海收集了有监管和没有监管的学习算法和其他数据处理单元，可以组合成数据处理序列或者更复杂的前馈网络结构。新算法的实现是简单和直观的。可用的算法是在不断的稳定增加的，包括信号处理方法（主成分分析、独立成分分析、慢特征分析），流型学习方法（局部线性嵌入），集中分类，概率方法（因子分析，RBM),数据预处理方法等等。\n自然语言和文本处理库\nNLTK 开源的Python模块，语言学数据和文档，用来研究和开发自然语言处理和文本分析。有windows,Mac OSX和Linux版本。\nPython势必成为人工智能时代的新宠儿，Python这门学科也将引入大量的学习者，任何行业的成功人士当属那些先行者，人工智能的浪潮还未席卷，选择Python这门学科就是有先见之明。在适合的时期选择适合的培训机构是至关重要的。027yeshenghuowang.com\n保质量，求真实，能学会，可就业，拿高薪的培训机构才是最佳选项。在培训机构中常见低价聘请新手Python开发者做讲师、常见其他学科讲师现学Python充当讲师，耽误无数学生！千锋Python教学部特聘请尹老师担任教学总监，毕业于清华大学，微软全球最具价值专家，资深软件架构师，CSDN著名技术专家，微软-清华大学联合实验室技术顾问，清华大学Oracle-java创始人，清华大学Google技术俱乐部创始人，清华大学Linux 技术俱乐部创始人。精通Python，C/C++，对于移动3G、语音技术、javaEE、信息安全、大数据高并发都有丰富的开发经验，拥有多年世界顶尖IT企业工作经验。不用坐在教室看1个讲师全国同步视频授课，不用在毫无学习氛围的教室看在线直播授课。千锋采用100%全程面授，名师一点胜庸师百万。"}
{"content2":"写这个系列是因为最近公司在搞技术分享，学习Spark，我的任务是讲PySpark的应用，因为我主要用Python，结合Spark，就讲PySpark了。然而我在学习的过程中发现，PySpark很鸡肋（至少现在我觉得我不会拿PySpark做开发）。为什么呢？原因如下：\n1.PySpark支持的算法太少了。我们看一下PySpark支持的算法:(参考官方文档)\n前面两个pyspark.sql和pyspark.streaming是对sql和streaming的支持。主要是读取数据，和streaming处理这种方式（当然这是spark的优势，要是这也不支持真是见鬼了）。pyspark.ml和pyspark.mllib分别是ml的api和mllib的api，ml的算法真心少啊，而且支持的功能很有限，譬如Lr（逻辑回归）和GBT目前只支持二分类，不支持多分类。mllib相对好点，支持的算法也多点，虽然昨天发的博文讲mlllib的时候说过有的算法不支持分布式，所以才会有限，但是我在想，如果我需要用到A算法，而Ml和Mllib的包里面都没有，这样是不是意味着要自己开发分布式算法呢？代价有点大诶，感觉写这个的时间不如多找找有用的特征，然后上LR，这样效果说不定更好。因为目前还没有在实际中用过，所以以上只是我的想法。下面把ml和mllib的所有api列出来，这样看的更清楚。\n图一 pyspark.ml的api\n图二 pyspark.mllib的api\n从上面两张图可以看到，mllib的功能比ml强大的不是一点半点啊，那ml这个包的存在还有什么意义呢？不懂（如果有了解的欢迎留言）。虽然有这么多疑问，但是我还是跟大家讲了，用的数据依然是iris（其实我真心想换个数据集啊 == ，下次换）。上代码：\n1 from pyspark.sql import SQLContext 2 sqlContext = SQLContext(sc) 3 df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('iris.csv') 4 # Displays the content of the DataFrame to stdout 5 df.show() 6 7 8 from pyspark.ml.feature import StringIndexer 9 indexer = StringIndexer(inputCol=\"Species\", outputCol=\"labelindex\") 10 indexed = indexer.fit(df).transform(df) 11 indexed.show() 12 13 from pyspark.sql import Row 14 from pyspark.mllib.linalg import Vectors 15 from pyspark.ml.classification import NaiveBayes 16 17 # Load and parse the data 18 def parseRow(row): 19 return Row(label=row[\"labelindex\"], 20 features=Vectors.dense([row[\"Sepal.Length\"], 21 row[\"Sepal.Width\"], 22 row[\"Petal.Length\"], 23 row[\"Petal.Width\"]])) 24 25 ## Must convert to dataframe after mapping 26 parsedData = indexed.map(parseRow).toDF() 27 labeled = StringIndexer(inputCol=\"label\", outputCol=\"labelpoint\") 28 data = labeled.fit(parsedData).transform(parsedData) 29 data.show() 30 31 ## 训练模型 32 #Naive Bayes 33 nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\") 34 model_NB = nb.fit(data) 35 predict_data= model_NB.transform(data) 36 traing_err = predict_data.filter(predict_data['label'] != predict_data['prediction']).count() 37 total = predict_data.count() 38 nb_scores = float(traing_err)/total 39 print traing_err, total, nb_scores 40 #7 150 0.0466666666667 41 42 43 #Logistic Regression########################################################### 44 # Logistic regression. Currently, this class only supports binary classification. 45 from pyspark.ml.classification import LogisticRegression 46 lr = LogisticRegression(maxIter=5, regParam=0.01) 47 model_lr = lr.fit(data) 48 predict_data= model_lr.transform(data) 49 traing_err = predict_data.filter(predict_data['label'] != predict_data['prediction']).count() 50 total = predict_data.count() 51 lr_scores = float(traing_err)/total 52 print traing_err, total, float(traing_err)/total 53 54 55 #Decision Tree 56 from pyspark.ml.classification import DecisionTreeClassifier 57 dt = DecisionTreeClassifier(maxDepth=2,labelCol = 'labelpoint') 58 model_DT= dt.fit(data) 59 predict_data= model_DT.transform(data) 60 traing_err = predict_data.filter(predict_data['label'] != predict_data['prediction']).count() 61 total = predict_data.count() 62 dt_scores = float(traing_err)/total 63 print traing_err, total, float(traing_err)/total 64 65 66 #GBT########################################################### 67 ## GBT. Currently, this class only supports binary classification. 68 from pyspark.ml.classification import GBTClassifier 69 gbt = GBTClassifier(maxIter=5, maxDepth=2,labelCol=\"labelpoint\") 70 model_gbt = gbt.fit(data) 71 predict_data= model_gbt.transform(data) 72 traing_err = predict_data.filter(predict_data['label'] != predict_data['prediction']).count() 73 total = predict_data.count() 74 dt_scores = float(traing_err)/total 75 print traing_err, total, float(traing_err)/total 76 77 78 #Random Forest 79 from pyspark.ml.classification import RandomForestClassifier 80 rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"labelpoint\", seed=42) 81 model_rf= rf.fit(data) 82 predict_data= model_rf.transform(data) 83 traing_err = predict_data.filter(predict_data['label'] != predict_data['prediction']).count() 84 total = predict_data.count() 85 dt_scores = float(traing_err)/total 86 print traing_err, total, float(traing_err)/total 87 88 #MultilayerPerceptronClassifier########################################################### 89 # Classifier trainer based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax. 90 # Number of inputs has to be equal to the size of feature vectors. Number of outputs has to be equal to the total number of labels. 91 from pyspark.ml.classification import MultilayerPerceptronClassifier 92 mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[150, 5, 150], blockSize=1, seed=11) 93 model_mlp= mlp.fit(parsedData) 94 predict_data= model_mlp.transform(parsedData) 95 traing_err = predict_data.filter(predict_data['label'] != predict_data['prediction']).count() 96 total = predict_data.count() 97 dt_scores = float(traing_err)/total 98 print traing_err, total, float(traing_err)/total\n因为数据集和上次讲pyspark聚类应用的数据是一样的，就不一步步的展示了，但是我这个程序里只有NaiveBayes的效果还行，0.94的正确率，其他的像DecisionTree等，效果真心差，可能参数还需要调。先掌握怎么用再来调参，官方文档里关于参数的解释也非常详细，可以看看。下一次讲回归，我决定不只写pyspark.ml的应用了，因为实在是图样图naive，想弄清楚pyspark的机器学习算法是怎么运行的，跟普通的算法运行有什么区别，优势等，再写个pyspark.mllib，看相同的算法在ml和mllib的包里运行效果有什么差异，如果有，是为什么，去看源码怎么写的。此外，我真的想弄清楚这货在实际生产中到底有用吗，毕竟还是要落实生产的，我之前想，如果python的sklearn能够在spark上应用就好了，后来在databricks里面找到了一个包好像是准备把sklearn弄到spark上来，当然算法肯定要重新写，不过还没有发布，期待发布的时候。此外，我在知乎上也看到过有人提问说“spark上能用skearn吗？”（大概是这意思，应该很好搜），里面有个回答好像说可以，不过不是直接用（等我找到了把链接放出来）。其实换一种想法，不用spark也行，直接用mapreduce编程序，但是mapreduce慢啊（此处不严谨，因为并没有测试过两者的性能差异，待补充），在我使用spark的短暂时间内，我个人认为spark的优势在于数据处理快，它不需要像mapreduce一样把数据切分成这么多块计算然后再reduce合并，而是直接将数据导入的时候就指定分区，运行机制不同，尤其是spark streaming的功能，还是很快的，所以这是spark的优势（鄙人拙见，如有错误欢迎指出）。而spark的劣势也比较明显，因为它对设备的要求太高了（吃内存啊能不高吗!）,这也是它快的原因，你把数据都放在内存里，取的时间比放在磁盘里当然要快，不过实际上在存储数据或者输出结果的时候还是会选择（memory+disk）的方式，保险嘛。前段时间看的alluxio也是占了内存的优势。恩，说了很多废话。下周争取研究的深一点，不然在公司里讲都没人听 = =。"}
{"content2":"欢迎大家前往腾讯云技术社区，获取更多腾讯海量技术实践干货哦~\n作者：汪毅雄\n导语 本文用容易理解的语言和例子来解释了决策树三种常见的算法及其优劣、随机森林的含义，相信能帮助初学者真正地理解相关知识。\n决策树\n引言\n决策树，是机器学习中一种非常常见的分类方法，也可以说是所有算法中最直观也最好理解的算法。先举个最简单的例子：\nA：你去不去吃饭？\nB：你去我就去。\n“你去我就去”，这是典型的决策树思想。\n再举个例子：\n有人找我借钱（当然不太可能。。。），借还是不借？我会结合根据我自己有没有钱、我自己用不用钱、对方信用好不好这三个特征来决定我的答案。\n我们把转到更普遍一点的视角，对于一些有特征的数据，如果我们能够有这么一颗决策树，我们也就能非常容易地预测样本的结论。所以问题就转换成怎么求一颗合适的决策树，也就是怎么对这些特征进行排序。\n在对特征排序前先设想一下，对某一个特征进行决策时，我们肯定希望分类后样本的纯度越高越好，也就是说分支结点的样本尽可能属于同一类别。\n所以在选择根节点的时候，我们应该选择能够使得“分支结点纯度最高”的那个特征。在处理完根节点后，对于其分支节点，继续套用根节点的思想不断递归，这样就能形成一颗树。这其实也是贪心算法的基本思想。那怎么量化“纯度最高”呢？熵就当仁不让了，它是我们最常用的度量纯度的指标。其数学表达式如下：\n其中N表示结论有多少种可能取值，p表示在取第k个值的时候发生的概率，对于样本而言就是发生的频率/总个数。\n熵越小，说明样本越纯。\n以一个两点分布样本X（x=0或1）的熵的函数图像来说明吧，横坐标表示样本值为1的概率，纵坐标表示熵。\n可以看到到当p（x=1）=0时，也就是说所有的样本都为0，此时熵为0.\n当p（x=1）=1时，也就是说所有的样本都为1，熵也为0.\n当p（x=1）=0.5时，也就是样本中0，1各占一半，此时熵能取得最大值。\n扩展一下，样本X可能取值为n种（x1。。。。xn）。可以证明，当p（xi）都等于1/n 时，也就是样本绝对均匀，熵能达到最大。当p（xi）有一个为1，其他都为0时，也就是样本取值都是xi，熵最小。\n决策树算法\nID3\n假设在样本集X中，对于一个特征a，它可能有（a1，a2。。。an）这些取值，如果用特征a对样本集X进行划分（把它当根节点），肯定会有n个分支结点。刚才提了，我们希望划分后，分支结点的样本越纯越好，也就是分支结点的“总熵”越小越好。\n因为每个分支结点的个数不一样，因此我们计算“总熵”时应该做一个加权，假设第i个结点样本个数为W(ai)，其在所有样本中的权值为W(ai) / W(X)。所以我们可以得到一个总熵：\n这个公式代表含义一句话：加权后各个结点的熵的总和。这个值应该越小，纯度越高。\n这时候，我们引入一个名词叫信息增益G（X，a），意思就是a这个特征给样本带来的信息的提升。公式就是：，由于H(X)对一个样本而言，是一个固定值，因此信息增益G应该越大越好。寻找使得信息增益最大的特征作为目标结点，并逐步递归构建树，这就是ID3算法的思想，\n好了以一个简单的例子来说明信息增益的计算：\n上面的例子，我计算一下特征1的信息增益\n首先计算样本的熵H（X）\n再计算总熵，可以看到特征1有3个结点A、B、C，其分别为6个、6个、5个\n所以A的权值为6/(6+6+5), B的权值为6/(6+6+5), C的为5/(6+6+5)\n因为我们希望划分后结点的纯度越高越好，因此还需要再分别计算结点A、B、C的熵\n特征1=A：3个是、3个否，其熵为\n特征1=B：2个是、4个否，其熵为\n特征1=C：4个是、1个否，其熵为\n这样分支结点的总熵就等于：\n特征1的信息增益就等于0.998-0.889=0.109\n类似地，我们也能算出其他的特征的信息增益，最终取信息增益最大的特征作为根节点。\n以上计算也可以有经验条件熵来推导：G（X,A）=H(X) - H(X|A)，这部分有兴趣的同学可以了解一下。\nC4.5\n在ID3算法中其实有个很明显的问题。\n如果有一个样本集，它有一个叫id或者姓名之类的（唯一的）的特征，那就完蛋了。设想一下，如果有n个样本，id这个特征肯定会把这个样本也分成n份，也就是有n个结点，每个结点只有一个值，那每个结点的熵就为0。就是说所有分支结点的总熵为0，那么这个特征的信息增益一定会达到最大值。因此如果此时用ID3作为决策树算法，根节点必然是id这个特征。但是显然这是不合理的。。。\n当然上面说的是极限情况，一般情况下，如果一个特征对样本划分的过于稀疏，这个也是不合理的（换句话就是，偏向更多取值的特征）。为了解决这个问题，C4.5算法采用了信息增益率来作为特征选取标准。\n所谓信息增益率，是在信息增益基础上，除了一项split information,来惩罚值更多的属性。\n而这个split information其实就是特征个数的熵H（A）。\n为什么这样可以减少呢，以上面id的例子来理解一下。如果id把n个样本分成了n份，那id这个特征的取值的概率都是1/n，文章引言已经说了，样本绝对均匀的时候，熵最大。\n因此这种情况，以id为特征，虽然信息增益最大，但是惩罚因子split information也最大，以此来拉低其增益率，这就是C4.5的思想。\nCART\n决策树的目的最终还是寻找到区分样本的纯度的量化标准。在CART决策树中，采用的是基尼指数来作为其衡量标准。基尼系数直观的理解是，从集合中随机抽取两个样本，如果样本集合越纯，取到不同样本的概率越小。这个概率反应的就是基尼系数。\n因此如果一个样本有K个分类。假设样本的某一个特征a有n个取值的话，其某一个结点取到不同样本的概率为：\n因此k个分类的概率总和，我们称之为基尼系数：\n而基尼指数，则是对所有结点的基尼系数进行加权处理\n计算出来后，我们会选择基尼系数最小的那个特征作为最优划分特征。\n剪枝\n剪枝的目的其实就是防止过拟合，它是决策树防止过拟合的最主要手段。决策树中，为了尽可能争取的分类训练样本，所以我们的决策树也会一直生长。但是呢，有时候训练样本可能会学的太好，以至于把某些样本的特有属性当成一般属性。这时候就我们就需要主动去除一些分支，来降低过拟合的风险。\n剪枝一般有两种方式：预剪枝和后剪枝。\n预剪枝\n一般情况下，只要结点样本已经100%纯了，树才会停止生长。但这个可能会产生过拟合，因此我们没有必要让它100%生长，所以在这之前，设定一些终止条件来提前终止它。这就叫预剪枝，这个过程发生在决策树生成之前。\n一般我们预剪枝的手段有：\n1、限定树的深度\n2、节点的子节点数目小于阈值\n3、设定结点熵的阈值\n等等。\n后剪枝\n顾名思义，这个剪枝是在决策树建立过程后。后剪枝算法的算法很多，有些也挺深奥，这里提一个简单的算法的思想，就不深究啦。\nReduced-Error Pruning (REP)\n该剪枝方法考虑将树上的每个节点都作为修剪的候选对象，但是有一些条件决定是否修剪，通常有这几步：\n1、删除其所有的子树，使其成为叶节点。\n2、赋予该节点最关联的分类\n3、用验证数据验证其准确度与处理前比较\n如果不比原来差，则真正删除其子树。然后反复从下往上对结点处理。这个处理方式其实是处理掉那些“有害”的节点。\n随机森林\n随机森林的理论其实和决策树本身不应该牵扯在一起，决策树只能作为其思想的一种算法。\n为什么要引入随机森林呢。我们知道，同一批数据，我们只能产生一颗决策树，这个变化就比较单一了。还有要用多个算法的结合呢？\n这就有了集成学习的概念。\n图中可以看到，每个个体学习器（弱学习器）都可包含一种算法，算法可以相同也可以不同。如果相同，我们把它叫做同质集成，反之则为异质。\n随机森林则是集成学习采用基于bagging策略的一个特例。\n从上图可以看出，bagging的个体学习器的训练集是通过随机采样得到的。通过n次的随机采样，我们就可以得到n个样本集。对于这n个样本集，我们可以分别独立的训练出n个个体学习器，再对这n个个体学习器通过集合策略来得到最终的输出，这n个个体学习器之间是相互独立的，可以并行。\n注：集成学习还有另一种方式叫boosting，这种方式学习器之间存在强关联，有兴趣的可以了解下。\n随机森林采用的采样方法一般是是Bootstap sampling，对于原始样本集，我们每次先随机采集一个样本放入采样集，然后放回，也就是说下次采样时该样本仍有可能被采集到，经过一定数量的采样后得到一个样本集。由于是随机采样，这样每次的采样集是和原始样本集不同的，和其他采样集也是不同的，这样得到的个体学习器也是不同的。\n随机森林最主要的问题是有了n个结果，怎么设定结合策略，主要方式也有这么几种：\n加权平均法：\n平均法常用于回归。做法就是，先对每个学习器都有一个事先设定的权值wi，\n然后最终的输出就是：\n当学习器的权值都为1/n时，这个平均法叫简单平均法。\n投票法：\n投票法类似我们生活中的投票，如果每个学习器的权值都是一样的。\n那么有绝对投票法，也就是票数过半。相对投票法，少数服从多数。\n如果有加权，依然是少数服从多数，只不过这里面的数是加权后的。\n例子\n以一个简单的二次函数的代码来看看决策树怎么用吧。\n训练数据是100个随机的真实的平方数据，不同的深度将会得到不同的曲线\n测试数据也是随机数据，但是不同深度的树的模型，产生的预测值也不太一样。如图\n这幅图的代码如下：\n我的是python 3.6环境，需要安装numpy、matplotlib、sklearn这三个库，需要的话直接pip install，大家可以跑跑看看，虽然简单但挺有趣。\n#!/usr/bin/python # -*- coding:utf-8 -*- import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor if __name__ == \"__main__\": # 准备训练数据 N = 100 x = np.random.rand(N) * 6 - 3 x.sort() y = x*x x = x.reshape(-1, 1) mpl.rcParams['font.sans-serif'] = ['SimHei'] mpl.rcParams['axes.unicode_minus'] = False # 决策树深度及其曲线颜色 depth = [2, 4, 6, 8, 10] clr = 'rgbmy' # 实际值 plt.figure(facecolor='w') plt.plot(x, y, 'ro', ms=5, mec='k', label='实际值') # 准备测试数据 x_test = np.linspace(-3, 3, 50).reshape(-1, 1) # 构建决策树 dtr = DecisionTreeRegressor() # 循环不同深度情况下决策树的模型，并用之测试数据的输出 for d, c in zip(depth, clr): # 设置最大深度（预剪枝） dtr.set_params(max_depth=d) # 训练决策树 dtr.fit(x, y) # 用训练数据得到的模型来验证测试数据 y_hat = dtr.predict(x_test) # 画出模型得到的曲线 plt.plot(x_test, y_hat, '-', color=c, linewidth=2, markeredgecolor='k', label='Depth=%d' % d) # 一些画图的基本参数 plt.legend(loc='upper center', fontsize=12) plt.xlabel('X') plt.ylabel('Y') plt.grid(b=True, ls=':', color='#606060') plt.title('二次函数决策树', fontsize=15) plt.tight_layout(2) plt.show()\n参考资料\n机器学习 周志华\n机器学习课程 邹博\n相关阅读\n机器学习：从入门到第一个模型\nGBDT 算法：原理篇\n道器相融，由 Angel 谈一个优秀机器学习平台的自我修养（下）\n此文已由作者授权腾讯云技术社区发布，转载请注明文章出处\n原文链接：https://www.qcloud.com/community/article/160232"}
{"content2":"绘制了一张导图,有不对的地方欢迎指正:\n下载地址\n机器学习中,特征是很关键的.其中包括,特征的提取和特征的选择.他们是降维的两种方法,但又有所不同:\n特征抽取（Feature Extraction）:Creatting a subset of new features by combinations of the exsiting features.也就是说，特征抽取后的新特征是原来特征的一个映射。\n特征选择（Feature Selection）:choosing a subset of all the features(the ones more informative)。也就是说，特征选择后的特征是原来特征的一个子集。\n特征提取\n最好的情况下,当然是有专家知道该提取什么样的特征,但是在不知道的前提下,一般的降维方法可以派上用场:(from wiki)\nPrincipal component analysis\nSemidefinite embedding\nMultifactor dimensionality reduction\nMultilinear subspace learning\nNonlinear dimensionality reduction\nIsomap\nKernel PCA\nMultilinear PCA\nLatent semantic analysis\nPartial least squares\nIndependent component analysis\nAutoencoder\n（1）Signal representation(信号表示): The goal of the feature extraction mapping is to represent the samples  accurately in a  low-dimensional space. 也就是说，特征抽取后的特征要能够精确地表示样本信息，使得信息丢失很小。对应的方法是PCA.\n（2）Signal classification（信号分类): The goal of the feature extraction mapping is toenhance the class-discriminatory  information in a low-dimensional space.  也就是说，特征抽取后的特征，要使得分类后的准确率很高，不能比原来特征进行分类的准确率低。对与线性来说，对应的方法是LDA  .\n在图像处理方面,有广泛的应用.\n特征选择\n主要过程:\n(1)产生过程\n2.2.1完全搜索\n完全搜索分为穷举搜索(Exhaustive)与非穷举搜索(Non-Exhaustive)两类。\n(1) 广度优先搜索( Breadth First Search )\n算法描述：广度优先遍历特征子空间。\n算法评价：枚举了所有的特征组合，属于穷举搜索，时间复杂度是O(2n)，实用性不高。\n(2)分支限界搜索( Branch and Bound )\n算法描述：在穷举搜索的基础上加入分支限界。例如：若断定某些分支不可能搜索出比当前找到的最优解更优的解，则可以剪掉这些分支。\n(3) 定向搜索 (Beam Search )\n算法描述：首先选择N个得分最高的特征作为特征子集，将其加入一个限制最大长度的优先队列，每次从队列中取出得分最高的子集，然后穷举向该子集加入1个特征后产生的所有特征集，将这些特征集加入队列。\n(4) 最优优先搜索 ( Best First Search )\n算法描述：与定向搜索类似，唯一的不同点是不限制优先队列的长度。\n2.2.2 启发式搜索\n(1)序列前向选择( SFS , Sequential Forward Selection )\n算法描述：特征子集X从空集开始，每次选择一个特征x加入特征子集X，使得特征函数J( X)最优。简单说就是，每次都选择一个使得评价函数的取值达到最优的特征加入，其实就是一种简单的贪心算法。\n算法评价：缺点是只能加入特征而不能去除特征。例如：特征A完全依赖于特征B与C，可以认为如果加入了特征B与C则A就是多余的。假设序列前向选择算法首先将A加入特征集，然后又将B与C加入，那么特征子集中就包含了多余的特征A。\n(2)序列后向选择( SBS , Sequential Backward Selection )\n算法描述：从特征全集O开始，每次从特征集O中剔除一个特征x，使得剔除特征x后评价函数值达到最优。\n算法评价：序列后向选择与序列前向选择正好相反，它的缺点是特征只能去除不能加入。\n另外，SFS与SBS都属于贪心算法，容易陷入局部最优值。\n(3) 双向搜索( BDS , Bidirectional Search )\n算法描述：使用序列前向选择(SFS)从空集开始，同时使用序列后向选择(SBS)从全集开始搜索，当两者搜索到一个相同的特征子集C时停止搜索。\n双向搜索的出发点是 。如下图所示，O点代表搜索起点，A点代表搜索目标。灰色的圆代表单向搜索可能的搜索范围，绿色的2个圆表示某次双向搜索的搜索范围，容易证明绿色的面积必定要比灰色的要小。\n(4) 增L去R选择算法 ( LRS , Plus-L Minus-R Selection )\n该算法有两种形式:\n<1> 算法从空集开始，每轮先加入L个特征，然后从中去除R个特征，使得评价函数值最优。( L > R )\n<2> 算法从全集开始，每轮先去除R个特征，然后加入L个特征，使得评价函数值最优。( L < R )\n算法评价：增L去R选择算法结合了序列前向选择与序列后向选择思想， L与R的选择是算法的关键。\n(5) 序列浮动选择( Sequential Floating Selection )\n算法描述：序列浮动选择由增L去R选择算法发展而来，该算法与增L去R选择算法的不同之处在于：序列浮动选择的L与R不是固定的，而是“浮动”的，也就是会变化的。\n序列浮动选择根据搜索方向的不同，有以下两种变种。\n<1>序列浮动前向选择( SFFS , Sequential Floating Forward Selection )\n算法描述：从空集开始，每轮在未选择的特征中选择一个子集x，使加入子集x后评价函数达到最优，然后在已选择的特征中选择子集z，使剔除子集z后评价函数达到最优。\n<2>序列浮动后向选择( SFBS , Sequential Floating Backward Selection )\n算法描述：与SFFS类似，不同之处在于SFBS是从全集开始，每轮先剔除特征，然后加入特征。\n算法评价：序列浮动选择结合了序列前向选择、序列后向选择、增L去R选择的特点，并弥补了它们的缺点。\n(6) 决策树( Decision Tree Method , DTM)\n算法描述：在训练样本集上运行C4.5或其他决策树生成算法，待决策树充分生长后，再在树上运行剪枝算法。则最终决策树各分支处的特征就是选出来的特征子集了。决策树方法一般使用信息增益作为评价函数。\n2.2.3 随机算法\n(1) 随机产生序列选择算法(RGSS, Random Generation plus Sequential Selection)\n算法描述：随机产生一个特征子集，然后在该子集上执行SFS与SBS算法。\n算法评价：可作为SFS与SBS的补充，用于跳出局部最优值。\n(2) 模拟退火算法( SA, Simulated Annealing )\n模拟退火算法可参考 大白话解析模拟退火算法 。\n算法评价：模拟退火一定程度克服了序列搜索算法容易陷入局部最优值的缺点，但是若最优解的区域太小（如所谓的“高尔夫球洞”地形），则模拟退火难以求解。\n(3) 遗传算法( GA,  Genetic Algorithms )\n遗传算法可参考 遗传算法入门 。\n算法描述：首先随机产生一批特征子集，并用评价函数给这些特征子集评分，然后通过交叉、突变等操作繁殖出下一代的特征子集，并且评分越高的特征子集被选中参加繁殖的概率越高。这样经过N代的繁殖和优胜劣汰后，种群中就可能产生了评价函数值最高的特征子集。\n随机算法的共同缺点：依赖于随机因素，有实验结果难以重现。\n(2)评价函数\n(1) 相关性( Correlation)------------filter\n运用相关性来度量特征子集的好坏是基于这样一个假设：好的特征子集所包含的特征应该是与分类的相关度较高（相关度高），而特征之间相关度较低的（亢余度低）。\n可以使用线性相关系数(correlation coefficient) 来衡量向量之间线性相关度。\n( 2) 距离 (Distance Metrics )------------filter\n运用距离度量进行特征选择是基于这样的假设：好的特征子集应该使得属于同一类的样本距离尽可能小，属于不同类的样本之间的距离尽可能远。\n常用的距离度量（相似性度量）包括欧氏距离、标准化欧氏距离、马氏距离等。\n(3) 信息增益( Information Gain )------------filter\n假设存在离散变量Y，Y中的取值包括{y1，y2，....，ym} ，yi出现的概率为Pi。则Y的信息熵定义为：\n信息熵有如下特性：若集合Y的元素分布越“纯”，则其信息熵越小；若Y分布越“紊乱”，则其信息熵越大。在极端的情况下：若Y只能取一个值，即P1=1，则H(Y)取最小值0；反之若各种取值出现的概率都相等，即都是1/m，则H(Y)取最大值log2m。\n在附加条件另一个变量X，而且知道X=xi后，Y的条件信息熵(Conditional Entropy)表示为：\n在加入条件X前后的Y的信息增益定义为\n类似的，分类标记C的信息熵H( C )可表示为：\n将特征Fj用于分类后的分类C的条件信息熵H( C | Fj )表示为：\n选用特征Fj前后的C的信息熵的变化成为C的信息增益(Information Gain)，用表示，公式为：\n假设存在特征子集A和特征子集B，分类变量为C，若IG( C|A ) > IG( C|B ) ，则认为选用特征子集A的分类结果比B好，因此倾向于选用特征子集A。\n(4)一致性( Consistency )-------------filter\n若样本1与样本2属于不同的分类，但在特征A、 B上的取值完全一样，那么特征子集{A，B}不应该选作最终的特征集。\n(5)分类器错误率 (Classifier error rate )---------------wrapper\n使用特定的分类器，用给定的特征子集对样本集进行分类，用分类的精度来衡量特征子集的好坏。\n以上5种度量方法中，相关性、距离、信息增益、一致性属于筛选器，而分类器错误率属于封装器。\n(3)停止准则\n(4)验证过程\n主要分3类:(from wiki)\nFilter Method\n思想:与模型无关.基于一些变特征的衡量标准(即给每一个特征打分,表示这个特征的重要程度),排序后除去那些得分较低的特征..\n主要方法:\n1.Chi-squared test(卡方检验)\n2.information gain(信息增益)或信息增益率\n3.correlation coefficient scores(相关系数)\n优点:计算时间上较高效,对于过拟合问题具有较高的鲁棒性\n缺点:倾向于选择冗余的特征,因为他们不考虑特征之间的相关性,有可能某一个特征的分类能力很差，但是它和某些其它特征组合起来会得到不错的效果\nWrapper Method\n思想:根据不同的特征集合所获得的预测效果建立一个黑盒学习,不断优化.\n通过目标学习算法来评估特征集合\n假如有p个特征，那么就会有2p种特征组合，每种组合对应了一个模型。Wrapper类方法的思想是枚举出所有可能的情况，从中选取最好的特征组合。\n这种方式的问题是：由于每种特征组合都需要训练一次模型，而训练模型的代价实际上是很大的，如果p非常大，那么上述方式显然不具有可操作性。下面介绍两种优化的方法：forward search（前向搜索）和backward search（后向搜索）。\nforward search初始时假设已选特征的集合为空集，算法采取贪心的方式逐步扩充该集合，直到该集合的特征数达到一个阈值，该阈值可以预先设定，也可以通过交叉验证获得。算法的伪码如下：\n对于算法的外重循环，当F中包含所有特征时或者F中的特征数达到了阈值，则循环结束，算法最后选出在整个搜索过程中最优的特征集合。\nbackward search初始时假设已选特征集合F为特征的全集，算法每次删除一个特征，直到F的特征数达到指定的阈值或者F被删空。该算法在选择删除哪一个特征时和forward search在选择一个特征加入F时是一样的做法。\n将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个优化问题，\n主要方法:recursive feature elimination algorithm(递归特征消除算法).这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA，PSO，DE，ABC等，详见“优化算法——人工蜂群算法(ABC)”，“优化算法——粒子群算法(PSO)”。\n优点:考虑到特征与特征之间的关联性\n缺点:1.当观测数据较少时容易过拟合;2.当特征数量较多时,计算时间增长;\nEmbedded Method(折中)\n思想:旨在集合filter和wrapper方法的优点(时间复杂度较低，并且也考虑特征之间的组合关系),在事先了解什么是好的特征的的前提下才可以使用该方法\n主要方法:正则化，可以见“简单易学的机器学习算法——岭回归(Ridge Regression)”，岭回归就是在基本线性回归的过程中加入了正则项。我们知道L1正则化自带特征选择的功能，它倾向于留下相关特征而删除无关特征。比如在文本分类中，我们不再需要进行显示的特征选择这一步，而是直接将所有特征扔进带有L1正则化的模型里，由模型的训练过程来进行特征的选择。\n优点:集合了前面两种方法的优点\n缺点:必须事先知道什么是好的选择\n一般来说，特征选择算法的选用需要考虑下因素：\n1) 分类器的性能。要显著提高学习算法的性能，可以采用 Wrapper 模型。例如可以选用采用启发式搜索策略的 SBS-SLASH 算法，或基于遗传算法的Wrapper 方法（GA）。\n2) 能否去除冗余特征。如果只是要去除不相关的特征，可以采用 Relief 系列算法、互信息法（MI）或 Symmetric Uncertainty，这些算法可以有效的去除和类别不相关的特征，但是无法去除冗余特征。若要同时除去不相关的和冗余特征，可采用 CFS 算法或 FCBF。另外还可以考虑多种算法的结合，例如先用 Relief 算法快速去除不相关的特征，然后采用一种 Wrapper 方法去除冗余特征。\n3) 数据集的规模。对于小规模数据，可以采用使用完全搜索策略的 Filter 模型或 Wrapper 模型，例如 BB、BFF、Bobro。对于大规模数据，一般采用运行速度快的 Filter 模型，例如 Relief 系列算法及 FCBF。\n4) 类别信息。目前非监督的特征选择算法还比较少，在样本类别未知的情况下，需要选用无监督的特征选择算法，例如 Dash 等提出的一种基于熵的 Filter模型。\n5) 数据集的数据类型。Relief 系列算法可以处理数值的（numeric）或名词性的（nominal）属性。互信息（MI）、FCBF 在处理连续的数值属性时，需要预\n先对特征离散化。BB、BFF 及 Bobro 等则不能处理名词性的属性\nRef:\nGuyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. The Journal of Machine Learning Research, 3, 1157-1182.\nHall, M. A. (1999). Correlation-based feature selection for machine learning (Doctoral dissertation, The University of Waikato).(第3\\4章)\nKohavi, R., & John, G. H. (1997). Wrappers for feature subset selection.Artificial intelligence, 97(1), 273-324.\nM. Dash, H. Liu, Feature Selection for Classification. In:Intelligent Data Analysis 1 (1997) 131–156.\nLei Yu,Huan Liu, Feature Selection for High-Dimensional Data:A Fast Correlation-Based Filter Solution\nRicardo Gutierrez-Osuna, Introduction to Pattern Analysis ( LECTURE 11: Sequential Feature Selection )\nhttp://courses.cs.tamu.edu/rgutier/cpsc689_f08/l11.pdf\nhttp://blog.csdn.net/henryczj/article/details/41043883\nhttp://www.cnblogs.com/heaad/archive/2011/01/02/1924088.html"}
{"content2":"我们对于“人工智能”这个术语都很熟悉。毕竟，它是《终结者》,《黑客帝国》和《机械姬》等美国大片电影中非常流行的关键词。但你最近或许也听说过其他术语，像“机器学习”和“深度学习”，有时这两个术语会和“人工智能”互相替换使用，前年早些时候，Google DeepMind的AlphaGo打败了韩国的围棋大师李世乭九段。在媒体描述DeepMind胜利的时候，将人工智能（AI）、机器学习（machine learning）和深度学习（deep learning）都用上了。这三者在AlphaGo击败李世乭的过程中都起了作用，但它们说的并不是一回事。那么这三个名词之间有什么区别？\n我会先解释一下人工智能（AI）、机器学习（ML）和深度学习（DL），以及它们有怎样的区别。\n1 三者的概念\n人工智能（英语：Artificial Intelligence, AI）：是指由人工制造出来的系统所表现出来的智能。通常人工智能是指通过普通电脑实现的智能。人工智能的研究可以分为几个技术问题。其分支领域主要集中在解决具体问题，其中之一是，如何使用各种不同的工具完成特定的应用程序。AI的核心问题包括推理、知识、规划、学习、交流、感知、移动和操作物体的能力等。\n目前有大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于仿生学、认知心理学，以及基于概率论和经济学的算法等等也在逐步探索当中。\n机器学习（英语：Machine Learning）：是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。\n机器学习有下面几种定义：\n机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是 如何在经验学习中改善具体算法的性能。 机器学习是对能通过经验自动改进的计算机算法的研究。 机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。\n机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。\n机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。\n深度学习（英语：Deep Learning）：是机器学习拉出的分支，它试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。\n深度学习是机器学习中一种基于对数据进行表征学习的方法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。\n统计学习：关于计算机基于数据构建概率统计模型，并运用模型对数据进行预测与分析的一门学科。 机器学习：致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。 深度学习：机器学习中的神经网络算法的延伸，可以理解为包含很多个隐层的神经网络模型。\n2  三者的区别\n人工智能：人工智能是人类社会发展主要目标 机器学习：机器学习是实现人工智能的核心技术 深度学习：是机器学习中最热门的算法\n1956年，约翰·麦卡锡成为了第一位创造了人工智能机器的人。他制造的机器具备足够高的能力，得以执行类似人类智力水平的任务，包括：做出规划、理解语言、识别对象和声音、学习并解决问题等。\n对于人工智能，我们可以从广义和狭义两个层面来理解。广义层面来讲，AI应该具备人类智力的所有特征，包括上述的能力。狭义层面的人工智能则只具备部分人类智力某些方面的能力，并且能在这些领域内做的非常出众，但可能缺乏其他领域的能力。比如说，一个人工智能机器可能拥有强大的图像识别功能，但除此之外并无他用，这就是狭义层面AI的例子。\n从核心上来说，机器学习是实现人工智能的一种途径。\n1959年，Arthur Samuel在AI之后创造了“机器学习”这个短语，并将其定义为“在没有被明确编程的情况下就能学习的能力。”当然，你可以不使用机器学习的方式来实现人工智能，不过这需要你运用复杂的规则和决策树，再敲下几百万行的代码才行。\n实际上，机器学习是一种“训练”算法的方式，目的是使机器能够向算法传送大量的数据，并允许算法进行自我调整和改进，而不是利用具有特定指令的编码软件例程来完成指定的任务。\n举个例子，机器学习已经被用于计算机视觉（机器具备识别图像或视频中的对象的能力）方面，并已经有了显著的进步。你可以收集数十万甚至数百万张图片，然后让人标记它们。例如，让人标记出其中含有猫的图片。对于算法，它也能够尝试建立一个模型，可以像人一样准确地标记出含有猫的图片。一旦精度水平足够高，机器就相当于“掌握”了猫的样子。\n深度学习是机器学习的众多方法之一。其他方法包括决策树学习、归纳逻辑编程、聚类、强化学习和贝叶斯网络等。\n深度学习的灵感来自大脑的结构和功能，即许多神经元的互连。人工神经网络（ANN）是模拟大脑生物结构的算法。\n在ANN中，存在具有离散层和与其他“神经元”连接的“神经元”。每个图层挑选出一个要学习的特征，如图像识别中的曲线/边缘。 正是这种分层赋予了“深度学习”这样的名字，深度就是通过使用多层创建的，而不是单层。\n3  深度学习，给人工智能以璀璨的未来\n深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n所以我们就安心学习好机器学习就好，那么如何学习好机器学习呢，下面用几张图片展示！\n4   如何学好机器学习？\n机器学习可以分为下面几种类别\n监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练数据中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。\n无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类。\n半监督学习介于监督学习与无监督学习之间。它主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。。\n增强学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。\n在传统的机器学习领域，监督学习最大的问题是训练数据标注成本比较高，而无监督学习应用范围有限。利用少量的训练样本和大量无标注数据的半监督学习一直是机器学习的研究重点。\n当前非常流行的深度学习GAN模型和半监督学习的思路有相通之处，GAN是“生成对抗网络”（Generative Adversarial Networks）的简称，包括了一个生成模型G和一个判别模型D，GAN的目标函数是关于D与G的一个零和游戏，也是一个最小-最大化问题。\nGAN实际上就是生成模型和判别模型之间的一个模仿游戏。生成模型的目的，就是要尽量去模仿、建模和学习真实数据的分布规律；而判别模型则是要判别自己所得到的一个输入数据，究竟是来自于真实的数据分布还是来自于一个生成模型。通过这两个内部模型之间不断的竞争，从而提高两个模型的生成能力和判别能力。\n图片总结的如此清晰，我就不重复说了（此图片来此天善智能某课堂的PPT）\n5  机器学习知识结构\n小编从学习机器学习需要的各个方面在此阐述了要想学习机器学习，首先需要学习或者说准备什么东西，从以下四个方面说起。\n5.1 数学基础\n大学专业不是数学的同志们需要恶补的知识科目如下：\n微积分 线性代数 矩阵论 凸优化 离散数学 概率论 统计学 随机过程\n5.2   机器学习理论\n机器学习的理论知识如下，其中推荐的包括算法和学习模型，还有训练的网址，全是干货哦，当然还是不全，以后小编了解到会逐渐加上的。\n有监督机器学习模型和算法：分类和回归 线性回归 感知机器学习 决策树 朴素贝叶斯 人工神经网络，逻辑回归，随机森林，GBDT lightgbm xgboost....\n5.3  编程与开发\n编程开发使用的主要是python语言和Linux服务器，加上TensorFlow\npython：numpy pandas matplotlib seaborn sklearn Linux : java c Spark Hadoop SQL excel..\n5.4 英文能力\n熟练地英语阅读能力\n6  以下文章将会有助于你更加深入了解人工智能、机器学习、深度学习：\n小编文中许多知识点都是参考下面的文章，大家有兴趣的可以继续了解三者的区别。\n1、Artificial Intelligence, Machine Learning, and Deep Learning\n2、Why Deep Learning is Radically Different from Machine Learning\n3、一篇文章讲清楚人工智能、机器学习和深度学习的区别\n4、人工智能，机器学习和深度学习有什么区别？\n5、如何区分人工智能、机器学习和深度学习？\n6、WHY DEEP LEARNING IS SUDDENLY CHANGING YOUR LIFE\n7、The Current State of Machine Intelligence 3.0\n8、Here are 50 Companies Leading the AI Revolution"}
{"content2":"机器学习与人工智能学习资源导引\nhttp://blog.csdn.net/pongba/article/details/2915005\nTopLanguage(https://groups.google.com/group/pongba/)\n我经常在 TopLanguage 讨论组上推荐一些书籍，也经常问里面的牛人们搜罗一些有关的资料，人工智能、机器学习、自然语言处理、知识发现（特别地，数据挖掘）、信息检索 这些无疑是 CS 领域最好玩的分支了（也是互相紧密联系的），这里将最近有关机器学习和人工智能相关的一些学习资源归一个类：\n首先是两个非常棒的 Wikipedia 条目，我也算是 wikipedia 的重度用户了，学习一门东西的时候常常发现是始于 wikipedia 中间经过若干次 google ，然后止于某一本或几本著作。\n第一个是“人工智能的历史”（History of Artificial Intelligence），我在讨论组上写道：\n而今天看到的这篇文章是我在 wikipedia 浏览至今觉得最好的。文章名为《人工智能的历史》，顺着 AI 发展时间线娓娓道来，中间穿插无数牛人故事，且一波三折大气磅礴，可谓\"事实比想象更令人惊讶\"。人工智能始于哲学思辨，中间经历了一个没有心理学（尤其是认知神经科学的）的帮助的阶段，仅通过牛人对人类思维的外在表现的归纳、内省，以及数学工具进行探索，其间最令人激动的是 Herbert Simon （决策理论之父，诺奖，跨领域牛人）写的一个自动证明机，证明了罗素的数学原理中的二十几个定理，其中有一个定理比原书中的还要优雅，Simon 的程序用的是启发式搜索，因为公理系统中的证明可以简化为从条件到结论的树状搜索（但由于组合爆炸，所以必须使用启发式剪枝）。后来 Simon 又写了 GPS （General Problem Solver），据说能解决一些能良好形式化的问题，如汉诺塔。但说到底 Simon 的研究毕竟只触及了人类思维的一个很小很小的方面 —— Formal Logic，甚至更狭义一点 Deductive Reasoning （即不包含 Inductive Reasoning , Transductive Reasoning (俗称 analogic thinking）。还有诸多比如 Common Sense、Vision、尤其是最为复杂的 Language 、Consciousness 都还谜团未解。还有一个比较有趣的就是有人认为 AI 问题必须要以一个物理的 Body 为支撑，一个能够感受这个世界的物理规则的身体本身就是一个强大的信息来源，基于这个信息来源，人类能够自身与时俱进地总结所谓的 Common-Sense Knowledge （这个就是所谓的 Emboddied  Mind 理论。 ），否则像一些老兄直接手动构建 Common-Sense Knowledge Base ，就很傻很天真了，须知人根据感知系统从自然界获取知识是一个动态的自动更新的系统，而手动构建常识库则无异于古老的 Expert System 的做法。当然，以上只总结了很小一部分我个人觉得比较有趣或新颖的，每个人看到的有趣的地方不一样，比如里面相当详细地介绍了神经网络理论的兴衰。所以我强烈建议你看自己一遍，别忘了里面链接到其他地方的链接。\n顺便一说，徐宥同学打算找时间把这个条目翻译出来，这是一个相当长的条目，看不动 E 文的等着看翻译吧:)\n第二个则是“人工智能”（Artificial Intelligence）。当然，还有机器学习等等。从这些条目出发能够找到许多非常有用和靠谱的深入参考资料。\n然后是一些书籍\n书籍：\n1. 《Programming Collective Intelligence》，近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的:P\n2. Peter Norvig 的《AI, Modern Approach 2nd》（无争议的领域经典）。\n3. 《The Elements of Statistical Learning》，数学性比较强，可以做参考了。\n4. 《Foundations of Statistical Natural Language Processing》，自然语言处理领域公认经典。\n5. 《Data Mining, Concepts and Techniques》，华裔科学家写的书，相当深入浅出。\n6. 《Managing Gigabytes》，信息检索好书。\n7. 《Information Theory：Inference and Learning Algorithms》，参考书吧，比较深。\n相关数学基础（参考书，不适合拿来通读）：\n1. 线性代数：这个参考书就不列了，很多。\n2. 矩阵数学：《矩阵分析》，Roger Horn。矩阵分析领域无争议的经典。\n3. 概率论与统计：《概率论及其应用》，威廉·费勒。也是极牛的书，可数学味道太重，不适合做机器学习的。于是讨论组里的 Du Lei 同学推荐了《All Of Statistics》并说到\n机器学习这个方向，统计学也一样非常重要。推荐All of statistics，这是CMU的一本很简洁的教科书，注重概念，简化计算，简化与Machine Learning无关的概念和统计内容，可以说是很好的快速入门材料。\n4. 最优化方法：《Nonlinear Programming, 2nd》非线性规划的参考书。《Convex Optimization》凸优化的参考书。此外还有一些书可以参考 wikipedia 上的最优化方法条目。要深入理解机器学习方法的技术细节很多时候（如SVM）需要最优化方法作为铺垫。\n王宁同学推荐了好几本书：\n《Machine Learning, Tom Michell》, 1997.\n老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能\"新\"到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n《Modern Information Retrieval, Ricardo Baeza-Yates et al》. 1999\n老书，牛人。貌似第一本完整讲述IR的书。可惜IR这些年进展迅猛，这本书略有些过时了。翻翻做参考还是不错的。另外，Ricardo同学现在是Yahoo Research for Europe and Latin Ameria的头头。\n《Pattern Classification (2ed)》, Richard O. Duda, Peter E. Hart, David G. Stork\n大约也是01年左右的大块头，有影印版，彩色。没读完，但如果想深入学习ML和IR，前三章（介绍，贝叶斯学习，线性分类器）必修。\n还有些经典与我只有一面之缘，没有资格评价。另外还有两本小册子，论文集性质的，倒是讲到了了不少前沿和细节，诸如索引如何压缩之类。可惜忘了名字，又被我压在箱底，下次搬家前怕是难见天日了。\n（呵呵，想起来一本：《Mining the Web - Discovering Knowledge from Hypertext Data》 ）\n说一本名气很大的书：《Data Mining: Practical Machine Learning Tools and Techniques》。Weka 的作者写的。可惜内容一般。理论部分太单薄，而实践部分也很脱离实际。DM的入门书已经不少，这一本应该可以不看了。如果要学习了解 Weka ，看文档就好。第二版已经出了，没读过，不清楚。\n信息检索方面，Du Lei 同学再次推荐：\n信息检索方面的书现在建议看Stanford的那本《Introduction to Information Retrieval》，这书刚刚正式出版，内容当然up to date。另外信息检索第一大牛Croft老爷也正在写教科书，应该很快就要面世了。据说是非常pratical的一本书。\n对信息检索有兴趣的同学，强烈推荐翟成祥博士在北大的暑期学校课程，这里有全slides和阅读材料：http://net.pku.edu.cn/~course/cs410/schedule.html\nmaximzhao 同学推荐了一本机器学习：\n加一本书：Bishop, 《Pattern Recognition and Machine Learning》. 没有影印的，但是网上能下到。经典中的经典。Pattern Classification 和这本书是两本必读之书。《Pattern Recognition and Machine Learning》是很新（07年），深入浅出，手不释卷。\n最后，关于人工智能方面（特别地，决策与判断），再推荐两本有意思的书，\n一本是《Simple Heuristics that Makes Us Smart》\n另一本是《Bounded Rationality: The Adaptive Toolbox》\n不同于计算机学界所采用的统计机器学习方法，这两本书更多地着眼于人类实际上所采用的认知方式，以下是我在讨论组上写的简介：\n这两本都是德国ABC研究小组（一个由计算机科学家、认知科学家、神经科学家、经济学家、数学家、统计学家等组成的跨学科研究团体）集体写的，都是引起领域内广泛关注的书，尤其是前一本，後一本则是对 Herbert Simon （决策科学之父，诺奖获得者）提出的人类理性模型的扩充研究），可以说是把什么是真正的人类智能这个问题提上了台面。核心思想是，我们的大脑根本不能做大量的统计计算，使用fancy的数学手法去解释和预测这个世界，而是通过简单而鲁棒的启发法来面对不确定的世界（比如第一本书中提到的两个后来非常著名的启发法：再认启发法（cognition heuristics）和选择最佳（Take the Best）。当然，这两本书并没有排斥统计方法就是了，数据量大的时候统计优势就出来了，而数据量小的时候统计方法就变得非常糟糕；人类简单的启发法则充分利用生态环境中的规律性（regularities），都做到计算复杂性小且鲁棒。\n关于第二本书的简介：\n1. 谁是 Herbert Simon\n2. 什么是 Bounded Rationality\n3. 这本书讲啥的：\n我一直觉得人类的决策与判断是一个非常迷人的问题。这本书简单地说可以看作是《决策与判断》的更全面更理论的版本。系统且理论化地介绍人类决策与判断过程中的各种启发式方法（heuristics）及其利弊 （为什么他们是最优化方法在信息不足情况下的快捷且鲁棒的逼近，以及为什么在一些情况下会带来糟糕的后果等，比如学过机器学习的都知道朴素贝叶斯方法在许多情况下往往并不比贝叶斯网络效果差，而且还速度快；比如多项式插值的维数越高越容易overfit，而基于低阶多项式的分段样条插值却被证明是一个非常鲁棒的方案）。\n在此提一个书中提到的例子，非常有意思：两个团队被派去设计一个能够在场上接住抛过来的棒球的机器人。第一组做了详细的数学分析，建立了一个相当复杂的抛物线近似模型（因为还要考虑空气阻力之类的原因，所以并非严格抛物线），用于计算球的落点，以便正确地接到球。显然这个方案耗资巨大，而且实际运算也需要时间，大家都知道生物的神经网络中生物电流传输只有百米每秒之内，所以 computational complexity 对于生物来说是个宝贵资源，所以这个方案虽然可行，但不够好。第二组则采访了真正的运动员，听取他们总结自己到底是如何接球的感受，然后他们做了这样一个机器人：这个机器人在球抛出的一开始一半路程啥也不做，等到比较近了才开始跑动，并在跑动中一直保持眼睛于球之间的视角不变，后者就保证了机器人的跑动路线一定会和球的轨迹有交点；整个过程中这个机器人只做非常粗糙的轨迹估算。体会一下你接球的时候是不是眼睛一直都盯着球，然后根据视线角度来调整跑动方向？实际上人类就是这么干的，这就是 heuristics 的力量。\n相对于偏向于心理学以及科普的《决策与判断》来说，这本书的理论性更强，引用文献也很多而经典，而且与人工智能和机器学习都有交叉，里面也有不少数学内容，全书由十几个章节构成，每个章节都是由不同的作者写的，类似于 paper 一样的，很严谨，也没啥废话，跟 《Psychology of Problem Solving》类似。比较适合 geeks 阅读哈。\n另外，对理论的技术细节看不下去的也建议看看《决策与判断》这类书（以及像《别做正常的傻瓜》这样的傻瓜科普读本），对自己在生活中做决策有莫大的好处。人类决策与判断中使用了很多的 heuristics ，很不幸的是，其中许多都是在适应几十万年前的社会环境中建立起来的，并不适合于现代社会，所以了解这些思维中的缺点、盲点，对自己成为一个良好的决策者有很大的好处，而且这本身也是一个非常有趣的领域。\n（完）"}
{"content2":"引言：我想说明的是，这只是我开始初涉人工智能知识的思考日志。\n一款有趣的游戏，必定需要游戏中各个环节的出色的发挥，比如人工智能模块的提现。还记得早期的电视游戏中的那些通关秘技吗？其实，当你在享受通关的快乐的时候，这款游戏可玩性也大大是缩短了。那么，如果Boss“学会”了思考呢？于是更智能的Boss出现了。\n需要说明的是，其实游戏中人工智能只能算是人工智能，这个深奥的领域的一部分，它具有人工智能的一部分特性，但却又不完全的等同与人工智能。所以，也许Artificial Intelligence，这个名字更好些。\n既然是人造智能，那它必然需要一个载体，比如人的智慧在人体的大脑中，而这里所说的AI的载体，当然指的是计算机了。\n先来看下以往游戏的AI应用。它们大部分是以两种方式来体现的。第一种，严格意义来说并不能算是AI，因为它使用的是作弊机制，即，在判断行为之前，机器人是获取得到对方的准确数据指标的，从而根据开发者的定义来决定对拿到的这份数据做什么，这种情况下，机器人只有不想赢，没有赢不了。第二种则在Half-life中体现的非常不错，它有很好的AI机制，有些遗憾的是，它基于了大量的脚本化操作。\n那如果我要撇开这两种方式，让AI更加名副其实呢？神经网络系统？或许有些宏大，但我想我可以学习其中一些概念。\n首先，在我的思考里，有这样一个初步的模型。\n创造机器人，将是第一步。人对事物的容载量是在脑细胞可以承载的范围内，同样，在AI机器人创建和使用的时候，就有必要对当前载体——计算机的情况做一个评估，以保证合理应用硬件资源。就好像人类先天的天赋，这将决定后面机器思考的范围和深度。\n其次，赋予情感。人类对事物的判断，大致上基于两方面的信息：情感和思维判断。在这里，感觉也属于情感的系列。我们希望即便是机器人，也能有些性格，这样，或许看起来更丰富些，此外就像人一样，当自身的思维判断不足以作出动向的时候，就需要“感觉”去做，这，符合人类的习惯。\n再次，是计算机最擅长的方面：思维逻辑。因为在上一条，我们已经撇开了情感这种感觉类因子，所以这里的逻辑判断，也就类似人类的纯常识性判断，即，根据存储数据，以学习到的思维模式去求解，这应该是整个AI模型中很重要的一部分内容。\n情感与一些能力是人类的先天属性，AI机器人同样也需要它们，我想，或许承载它们的是一种合理的，高效的数据结构。\n有了上面的思考，我的心中有了这样一个模型，或许它很初步，或许它很幼稚，或许它很不合理，但却是我了解这部分的开始，我希望它可以产生很多错误，这样我就可以不断的纠正自己。\n这个模型的定义应该是分为两个方面的：AI机器的先天属性和后天处理。\n先天部分，应该充分利用Script System，它有利于定义出一些诞生以后就不太需要改变的东西，就好像人一样，在这个部分应当包含一些FSM,FuSM,和Message，这其中包含了一定的自我修复能力，当然了，也可能被损坏，这应当是被允许的。同时它的极限能力，则是AI系统的运转能力。\n这个过程，可能以GA的方式为统筹处理以便达到生产，自我修复，接受修复，等能力的实现。因为我们的定义是要应用于游戏中，所以在设计的时候，应该注意到GA的缺点，并做出反应。当然，它不会是这部分的核心而是一个很好的统筹辅助，比如全局数量性的问题，一些非线性，非连续的处理，GA都能够很好的胜任。而它的不足之处也是它不能作为这部分核心的主要原因：时间成本，算法的随机性（在接收到的提示出现问题的时候），此外还有因为heuristic technique，而始得得到的答案未必是最优的，此外重要的是，GA调试起来非常麻烦。还有些其他问题。所以，我们要做的应该是让GA的优点极限发挥。同它一起协作辅助的还有ACO，Coevolution，AGA等。\n后天的思维模式，将会是基于NN的。它与GA看起来有些地方类似。当然它也有调试困难和学习周期等问题的存在，所以，初期只打算让它有个简单的雏形存在。除了NN，后天部分还打算具有一些Artificial Life，Planning Algorithm，Production Systems，Decision Tree，Fuzzy Logic等等。\n两个部分想要以Layered Architecture的分布方式去架构，当然了，我很清楚，学习是一个过程，我的想法是在实践中掌握。\n初期，就从一个机器人智能对话的程序开始吧。\nGo，Go，Go。"}
{"content2":"牛人主页（主页有很多论文代码）\nSerge Belongie at UC San Diego\nAntonio Torralba at MIT\nAlexei Ffros at CMU\nCe Liu at Microsoft Research New England\nVittorio Ferrari at Univ.of Edinburgh\nKristen Grauman at UT Austin\nDevi Parikh at  TTI-Chicago (Marr Prize at ICCV2011)\nJohn Wright at Columbia Univ.\nPiotr Dollar at CalTech\nBoris Babenko at UC San Diego\nDavid Ross at Google/Youtube\nDavid Donoho at Stanford Univ.\n大神们：\nWilliam T. Freeman at MIT\nRoberto Cipolla at Cambridge\nDavid Lowe at Univ. of British Columbia\nMubarak Shah at Univ. of Central Florida\nYi Ma at MSRA\nTinne Tuytelaars at K.U. Leuven\nTrevor Darrell at U.C. Berkeley\nMichael J. Black at Brown Univ.\n\n重要研究组：\nComputer Vision Group at UC Berkeley\nRobotics Research Group at Univ. of Oxford\nLEAR at INRIA\nComputer Vision Lab at Stanford\nComputer Vision Lab at EPFL\nComputer Vision Lab at ETH Zurich\nComputer Vision Lab at Seoul National Univ.\nComputer Vision Lab at UC San Diego\nComputer Vision Lab at UC Santa Cruz\nComputer Vision Lab at Univ. of Southern California\nComputer Vision Lab at Univ. of Central Florida\nComputer Vision Lab at Columbia Univ.\nUCLA Vision Lab\nMotion and Shape Computing Group at George Mason Univ.\nRobust Image Understanding Lab at Rutgers Univ.\nIntelligent Vision Systems Group at Univ. of Bonn\nInstitute for Computer Graphics and Vision at Graz Univ. of Tech.\nComputer Vision Lab. at Vienna Univ. of Tech.\nComputational Image Analysis and Radiology at Medical Univ. of Vienna\nPersonal Robotics Lab at CMU\nVisual Perception Lab at Purdue Univ.\n潜力牛人：\nJuergen Gall at ETH Zurich\nMatt Flagg at Georgia Tech.\nMathieu Salzmann at TTI-Chicago\nGerg Shakhnarovich at TTI-Chicago\nTaeg Sang Cho at MIT\nJianchao Yang at UIUC\nStefan Roth at TU Darmstadt\nPeter Kontschieder at Graz Univ. of Tech.\nDominik Alexander Klein at Univ. of Bonn\nYinan Yu at CASIA (PASCAL VOC 2010 Detection Challenge Winner)\nZdenek Kalal at FPFL\nJulien Pilet at FPFL\nKenji Okuma\n（1）googleResearch； http://research.google.com/index.html\n（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html\n（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/\n（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\n（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html\n（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/\n（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/\n（8）中国人工智能网； http://www.chinaai.org/\n（9）中国视觉网； http://www.china-vision.net/\n（10）中科院自动化所； http://www.ia.cas.cn/\n（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/\n（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/\n（13）人脸识别主页； http://www.face-rec.org/\n（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\n（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html\n（16）卡内基梅隆大学CV主页；\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\n（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/\n（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/\n（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx\n（20）研学论坛； http://bbs.matwav.com/\n（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/\n（22）计算机视觉最新资讯网； http://www.cvchina.info/\n（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287\n（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/\n(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/\n(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home\n(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/\n(28)computer vision software; http://peipa.essex.ac.uk/info/software.html\n(29)Computer Vision Resource; http://www.cvpapers.com/\n(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html\n(31)computer vision center; http://computervisioncentral.com/cvcnews\n(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/\n(33)自动识别网：http://www.autoid-china.com.cn/\n(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\n(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/\n(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/\n(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/\n(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/\n(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\n(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/\n(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz\n(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp\n(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html\n(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/\n(45)深圳大学 于仕祺副教授：http://yushiqi.cn/\n(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/\n(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background\n(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php\n(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/\n(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1\n(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp\n(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/\n(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html\n(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\n(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/\n(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html\n(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml\n(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/\n(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php\n(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html\n(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html\n(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi\n(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html\n(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\n(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/\n(66)中科院自动化所医学影像研究室：http://www.3dmed.net/\n(67)中科院田捷研究员：http://www.3dmed.net/tian/\n(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/\n(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/\n(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/\n(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/\n(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/\n(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/\n(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/\n(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html\n(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/\n(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/\n(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/\n(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/\n(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/\n(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/\n(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/\n(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/\n(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123\n(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/\n(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/\n(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/\n(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/\n(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/\n(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/\n(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/\n(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/\n(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/\n(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/\n(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/\n(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/\n(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/\n(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/\n(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm\n(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html\n(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\n(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html\n(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\n(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/\n(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/\n(106)图片检索国际会议VOC(微软剑桥研究院组织): http://pascallin.ecs.soton.ac.uk/challenges/VOC/\n(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/\n(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html\nabout multi-camera: http://server.cs.ucf.edu/~vision/projects.html\nabout 3D Voxel Coloring   Rob Hess: http://blogs.oregonstate.edu/hess/code/voxels/\nAbout  the particle filters--condensation filter:http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/ISARD1/condensation.html\nMachine Learning Open Source Software：http://jmlr.csail.mit.edu/mloss/\n1、动作识别数据库：Recognition of human actions：http://www.nada.kth.se/cvap/actions/\n2、Datasets for Computer Vision Research：http://www-cvr.ai.uiuc.edu/ponce_grp/data/\n3、Computer Vision Datasets:http://clickdamage.com/sourcecode/cv_datasets.php\n4、里面有好多基本算法 matlab：  http://www.mathworks.cn/index.html\n5、CVPR 2011中关于grassmann 流形文章的源码： http://itee.uq.edu.au/~uqmhara1/code.html\nMatlab Codefor Graph Embedding Discriminant Analysis on Grassmannian Manifolds for Improved Image Set Matching (CVPR), 2011.\nMatlab Codefor Optimal Local Basis: A Reinforcement Learning Approach for Face Recognition(IJCV), vol. 81, no. 2, pp. 191-204, 2009.\n牛人bolg：\n1、Hong Kong Polytechnic University ：http://www4.comp.polyu.edu.hk/~cslzhang/\n2、Computer Vision Resources：资源非常丰富，包含有基本算法。https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\n3、源代码非常丰富~~  http://homepage.tudelft.nl/19j49/Publications.html\nCVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htm\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm\n李子青的大作：\nMarkov Random Field Modeling in Computer Vision\nhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.html\nHandbook of Face Recognition (PDF)\nhttp://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf\n张正友的有关参数鲁棒估计著作：\nParameter Estimation Techniques:A Tutorial with Application to Conic Fitting\nhttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.html\nAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Vision\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007\n有关马尔可夫蒙特卡罗方法的资料：\nAn introduction to Markov chain Monte Carlo\nhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.html\nMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05\nhttp://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm\n有关独立成分分析（Independent Component Analysis , ICA）的资料：\nAn ICA-Page\nhttp://www.cnl.salk.edu/~tony/ica.html\nFast ICA\nhttp://www.cis.hut.fi/projects/ica/fastica/\nThe Kalman Filter (介绍卡尔曼滤波器的终极网页)\nhttp://www.cs.unc.edu/~welch/kalman/index.html\nCached k-d tree search for ICP algorithms\nhttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html\n几个计算机视觉研究工具\nMachine Vision Toolbox for Matlab\nhttp://www.petercorke.com/Machine%20Vision%20Toolbox.html\nMatlab and Octave Function for Computer Vision and Image Processing\nhttp://www.csse.uwa.edu.au/~pk/research/matlabfns/\nBayes Net Toolbox for Matlab\nhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.html\nOpenCV (Chinese)\nhttp://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\nGandalf (A Computer Vision and Numerical Algorithm Labrary)\nhttp://gandalf-library.sourceforge.net/\nCMU Computer Vision Home Page\nhttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\nMachine Learning Resource Links\nhttp://www.cse.ust.hk/~ivor/resource.htm\nThe Bayesian Filtering Library\nhttp://www.orocos.org/bfl\nOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)\nhttp://of-eval.sourceforge.net/\nMATLAB code for ICP algorithm\nhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html\n牛人主页：\n朱松纯 （Song-Chun Zhu）\nhttp://www.stat.ucla.edu/~sczhu/\nDavid Lowe (SIFT) (很帅的一个老头哦 ^ ^)\nhttp://www.cs.ubc.ca/~lowe/\nAndrea Vedaldi (SIFT)\nhttp://vision.ucla.edu/~vedaldi/index.html\nPedro F. Felzenszwalb\nhttp://people.cs.uchicago.edu/~pff/\nDougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)\nhttp://mesh.brown.edu/dlanman/courses.html\nJianbo Shi (Ncuts 的始作俑者)\nhttp://www.cis.upenn.edu/~jshi/\nActive Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)\nhttp://www.robots.ox.ac.uk/ActiveVision/index.html\nJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）\nhttp://www.cse.msu.edu/~weng/\n测试图片或视频：\nMiddlebury College‘s Stereo Vision Data Set\nhttp://cat.middlebury.edu/stereo/data.html\nIntelligent Vehicle:\nIVSource\nwww.ivsoruce.net\nRobot Car\nhttp://www.plyojump.com/robot_cars.html\nHow to Build a Robot: The Computer Vision Part\nhttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml\n收集的一般牛人主页（带代码）:\nXiaofei He(machine learning code)\nhttp://people.cs.uchicago.edu/~xiaofei/\nYingNian Wu(active base model code)\nhttp://www.stat.ucla.edu/~ywu/research.html\n布朗大学计算机主页（可找到该校CS牛人博客）\nhttp://www.cs.brown.edu/research/areas.html\nNavneet Dalal(Histograms of Oriented Gradients for Human Detection )\nhttp://www.navneetdalal.com/software\nPaul Viola(Robust Real-time Object Detection)\nhttp://research.microsoft.com/en-us/um/people/viola/\nActive LearningRMw平坦软件园\nhttp://active-learning.net/，这里包括了关于Active Learning理论以及应用的一些文章，特别是那篇Survey。\nTransfer LearningRMw平坦软件园\nhttp://www.cse.ust.hk/TL/，包括经典的论文以及附带有源码，很方便。\nGaussian ProcessesRMw平坦软件园\nRMw平坦软件园\nhttp://www.gaussianprocess.org 包括相关的书籍（有 Carl Edward Rasmussen 的书），相关的程序以及分类的 paper 列表。这也是由 Carl 自己维护的，他应该是将 GP 引入 machine learning 最早的人之一了吧，Hinton 的学生。\nNonparametric Bayesian MethodsRMw平坦软件园\nhttp://www.cs.berkeley.edu/~jordan/npb.html 这个一看就知道是 Jordan 维护的，主要包括 Dirichlet process 以及相关的其他随机过程在 machine learning 里面如何进行建模，如何进行 approximate inference。主要是文章列表。\nProbabilistic Graphical ModelRMw平坦软件园\nhttp://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html 是 Kevin Murphy 所维护的关于 Bayesian belief networks 的介绍，含有最基本的概念、相关的文献和软件的链接。罕见的 UCB 出来的不是 Jordan 的学生（老板是 Stuart Russel）。\nhttp://www.cs.berkeley.edu/~jordan/graphical.html 是 Jordan 系关于这个方面的论文汇编。\nhttp://www.inference.phy.cam.ac.uk/hmw26/crf/ 是关于 Conditional Random Fields 方面论文和软件的收集，由 Hanna Wallach 维护。\nCompressed SensingRMw平坦软件园\nhttp://www-dsp.rice.edu/cs 这是 Rice 大学维护的论文分类列表、软件链接等。推荐 Emmanuel Candès 所写的tutorial，这人是 David Donoho 的学生。\nTensorRMw平坦软件园\nhttp://csmr.ca.sandia.gov/~tgkolda/pubs/index.html 关于 tensor 的一些偏数学的文章。\nDeep Belief NetworkRMw平坦软件园\nhttp://www.cs.toronto.edu/~hinton/csc2515/deeprefs.html 是 Geoffrey Hinton 为研究生开设的 machine learning 课程的 DBN 的 reading list。\nKernel MethodsRMw平坦软件园\nhttp://www.cs.berkeley.edu/~jordan/kernels.html 是 Jordan 维护的关于 kernel methods 的文章列表。\nMarkov LogicRMw平坦软件园\nhttp://ai.cs.washington.edu/pubs 是 UW AI 组的文章，里面关于 Markov logic 的比较多，因为 Pedro Domingos 就是这个组的。\nMachine learning theory\nhttp://hunch.net/这个网站主要是一些learning theory的东西比较多，想在machine learning 理论上有所建树的同志们可以去看看\n牛人：Iasonas Kokkinos （搞统计模型视觉）\nhttp://vision.mas.ecp.fr/Personnel/iasonas/index.html\n人工智能与模式识别国际顶级期刊会议目录（包含该领域最权威的期刊和会议）"}
{"content2":"在微博上看到七月算法寒老师总结的完整机器的学习项目的工作流程，结合天池比赛的经历写的。现在机器学习应用非常流行，了解机器学习项目的流程，能帮助我们更好的使用机器学习工具来处理实际问题。\n1. 理解实际问题，抽象为机器学习能处理的数学问题\n理解实际业务场景问题是机器学习的第一步，机器学习中特征工程和模型训练都是非常费时的，深入理解要处理的问题，能避免走很多弯路。理解问题，包括明确可以获得的数据，机器学习的目标是分类、回归还是聚类。如果都不是的话，考虑将它们转变为机器学习问题。参考机器学习分类能帮助从问题提炼出一个合适的机器学习方法。\n2. 获取数据\n获取数据包括获取原始数据以及从原始数据中经过特征工程从原始数据中提取训练、测试数据。机器学习比赛中原始数据都是直接提供的，但是实际问题需要自己获得原始数据。“ 数据决定机器学习结果的上限，而算法只是尽可能的逼近这个上限”，可见数据在机器学习中的作用。总的来说数据要有具有“代表性”，对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。不仅如此还要对评估数据的量级，样本数量、特征数量，估算训练模型对内存的消耗。如果数据量太大可以考虑减少训练样本、降维或者使用分布式机器学习系统。\n3. 特征工程\n特征工程是非常能体现一个机器学习者的功底的。特征工程包括从原始数据中特征构建、特征提取、特征选择，非常有讲究。深入理解实际业务场景下的问题，丰富的机器学习经验能帮助我们更好的处理特征工程。特征工程做的好能发挥原始数据的最大效力，往往能够使得算法的效果和性能得到显著的提升，有时能使简单的模型的效果比复杂的模型效果好。数据挖掘的大部分时间就花在特征工程上面，是机器学习非常基础而又必备的步骤。数据预处理、数据清洗、筛选显著特征、摒弃非显著特征等等都非常重要，建议深入学习。\n4. 模型训练、诊断、调优\n现在有很多的机器学习算法的工具包，例如sklearn，使用非常方便，真正考验水平的根据对算法的理解调节参数，使模型达到最优。当然，能自己实现算法的是最牛的。模型诊断中至关重要的是判断过拟合、欠拟合，常见的方法是绘制学习曲线，交叉验证。通过增加训练的数据量、降低模型复杂度来降低过拟合的风险，提高特征的数量和质量、增加模型复杂来防止欠拟合。诊断后的模型需要进行进一步调优，调优后的新模型需要重新诊断，这是一个反复迭代不断逼近的过程，需要不断的尝试，进而达到最优的状态。\n5. 模型验证、误差分析\n模型验证和误差分析也是机器学习中非常重要的一步，通过测试数据，验证模型的有效性，观察误差样本，分析误差产生的原因，往往能使得我们找到提升算法性能的突破点。误差分析主要是分析出误差来源与数据、特征、算法。\n6 . 模型融合\n一般来说实际中，成熟的机器算法也就那么些，提升算法的准确度主要方法是模型的前端（特征工程、清洗、预处理、采样）和后端的模型融合。在机器学习比赛中模型融合非常常见，基本都能使得效果有一定的提升。这篇博客中提到了模型融合的方法，主要包括一人一票的统一融合，线性融合和堆融合。\n参考：http://ask.julyedu.com/question/7013"}
{"content2":"人工智能是一门研究模拟人类智能，实现机器智能的一门科学，那么，在找工作过程中，这些面试题常常会被问到。了解一二，有备无患。\n关于Python\n1、Python的函数参数传递方法？\n2、Python中的元类(metaclass)有哪些？\n3、@staticmethod和@classmethod的区别？\n4、类变量和实例变量区别？\n5、Python中单下划线和双下划线？\n6、字符串格式化:%和.format？\n7、迭代器和生成器的区别？\n8、说一说面向切面编程AOP和装饰器？\n9、怎么理解Python中重载？\n10、新式类和旧式类\n11、__new__和__init__的区别\n12、单例模式\n13、Python中的作用域\n14、GIL线程全局锁\n15、 协程\n16、Python函数式编程\n17、Python里的拷贝\n18、Python垃圾回收机制\n19、read,readline和readlines\n20、Python2和3的区别\n21、调度算法的步骤\n22、静态链接和动态链接的区别是什么\n23、虚拟内存技术\n24、分页和分段\n25、页面置换算法\n26、边沿触发和水平触发\n27、Redis原理\n28、乐观锁和悲观锁\n29、 MyISAM和InnoDB\n30、urllib和urllib2的区别\n31、apache和nginx的区别\n32、幂等 Idempotence\n33、RESTful架构(SOAP,RPC)\n34、CGI和WSGI\n35、unix进程间通信方式(IPC)\n36、广度遍历和深度遍历二叉树编程问题\n关于人工智能\n1.什么是机器学习\n2.机器学习与数据挖掘的区别\n3.什么是机器学习的过度拟合现象\n4.过度拟合产生的原因\n5.如何避免过度拟合\n6.什么是感应式的机器学习？\n7.什么是机器学习的五个流行的算法？\n8.机器学习有哪些不同的算法技术？\n9.在机器学习中，建立假设或者模型的三个阶段指的是什么？\n10.什么是监督学习的标准方法？\n11.什么是训练数据集和测试数据集？\n12.下面列出机器学习的各种方法？\n13.非机器学习有哪些类型？\n14.什么是非监督学习的功能？\n15.什么是监督学习的功能？\n16.什么是算法独立的机器学习？\n17.人工智能与机器学习的区别？\n18.在机器学习中分类器指的是什么？\n19.朴素贝叶斯方法的优势是什么？\n20.在哪些领域使用模式识别技术？\n21.什么是遗传编程？\n22.在机器学习中归纳逻辑程序设计是指什么？\n23.在机器学习中，模型的选择是指？\n24.用于监督学习校准两种方法是什么？\n25. 什么方法通常用于防止过拟合？\n26.规则学习的启发式方法和决策树的启发式方法之间的区别是什么？\n27.什么是感知机器学习？\n28.贝叶斯逻辑程序的两个组成部分是什么？\n29.什么是贝叶斯网络？\n30.为什么基于实例的学习算法有时也被称为懒惰学习算法？\n31.支持向量机能处理哪两种分类方法？\n32.什么是集成学习？\n33.为什么集成学习被应用？\n34.什么使用集成学习？\n35.什么是集成方法的两种范式？\n36.什么是集成方法的一般原则，在集成方法中套袋（bagging）和爆发（boosting）指的是什么？\n37.什么是集成方法分类错误的偏置方差分解？\n38.在集成方法中什么是增量合成方法？\n39.PCA，KPCA和ICE如何使用？\n40.在机器学习中降维是什么意思？\n41.什么是支持向量机？\n42.关系评价技术的组成部分是什么？\n43.连续监督学习有什么不同方法？\n44.在机器人技术和信息处理技术的哪些方面会相继出现预测问题？\n45.什么是批量统计学习？\n46什么是PAC学习？\n47有哪些不同的类别可以分为序列学习过程？\n48什么是序列学习？\n49.机器学习的两种技术是什么？\n50.你在日常工作中看到的机器学习的一个流行应用是什么？"}
{"content2":"这周我来跟大家分享的是在Microsoft Build 2016上发布的微软聊天机器人的框架。\n现如今，各种人工智能充斥在我们的生活里。最典型的人工智能产品就是聊天机器人，它既可以陪我们聊天，也可以替代客服人员回答客户的问题，甚至还可以充当秘书帮助我们订电影票、飞机票等等。最成功的产品就是苹果公司的Siri和微软公司的Cortana。\n那么如何搭建一个自己的聊天机器人呢？今天我会用微软公司出品的 Bot Framework 来搭建一个聊天机器人。\n官网上介绍 Bot Framework 可以用C#, Node.js来编写，由于我个人技术限制问题，我只介绍C#的部分。大家可以使用Node.js来编写。你也可以从Github获取官方事例代码。\n前提准备\nVisual Studio 2015 或者 Visual Studio 2017\n在Visual Studio 中，将所有插件进行更新\n下载 Bot Framework 模板，将下载的zip文件夹直接复制到%USERPROFILE%\\Documents\\Visual Studio 2017\\Templates\\ProjectTemplates\\Visual C#\\\n创建你的机器人\n打开Visual Studio 并且创建一个C#项目，选择Bot Application作为项目的模板。\n使用 Bot Application作为项目模板创建出来的项目已经包含了一个简单的机器人。请检查一下项目是否包含了最新的Bot Application模板：\n右键点击项目，找到Manage NuGet Packages\n在Browse标签栏，输入“Microsoft.Bot.Builder”\n在结果中找到 Microsoft.Bot.Builder 并点击后面的更新按钮\n根据提示来更改并更新引用的程序包\n输入代码\n首先，编写 Controllers\\MessagesController.cs文件中的Post方法来获得用户的消息并且调用根对话框\nC# [BotAuthentication] public class MessagesController : ApiController { /// <summary> /// POST: api/Messages /// Receive a message from a user and reply to it /// </summary> public async Task<HttpResponseMessage> Post([FromBody]Activity activity) { if (activity.Type == ActivityTypes.Message) { await Conversation.SendAsync(activity, () => new Dialogs.RootDialog()); } else { HandleSystemMessage(activity); } var response = Request.CreateResponse(HttpStatusCode.OK); return response; } ... }\n这个根对话框会处理消息并且生成一个回复，Dialogs\\RootDialog.cs文件中的MessageRevicedAsync方法会回复用户的消息。\nC# [Serializable] public class RootDialog : IDialog<object> { public Task StartAsync(IDialogContext context) { context.Wait(MessageReceivedAsync); return Task.CompletedTask; } private async Task MessageReceivedAsync(IDialogContext context, IAwaitable<object> result) { var activity = await result as Activity; // calculate something for us to return int length = (activity.Text ?? string.Empty).Length; // return our reply to the user await context.PostAsync($\"You sent {activity.Text} which was {length} characters\"); context.Wait(MessageReceivedAsync); } }\n测试你的机器人\n下一步，使用 Bot Framework Emulator来测试你的机器人，首先你需要下载并安装它。\n运行你的机器人\n在安装完Bot Framework Emulator之后，你可以点击 \"Microsoft Edge\"按钮来运行它。\n运行成功界面如下图所示\n打开你的emulator并且连接到机器人\n首先，需要确保你的机器人正在本机上运行，之后打开emulator然后连接到你的机器人：\n在地址栏输入http://localhost:port-number/api/messages，port-number是你浏览器中显示的，机器人占用的端口号码\n点击 Connect,你不需要输入Microsoft App ID 和Microsoft App Password。现在他们是空白的也没有问题，在之后，如果你注册了bot， 你会得到这些信息。\n测试你的机器人\n现在你的机器人已经开始运行了，并且emulator已经开始工作了。你可以输入你想输入的任何东西。在输入后，你会看到*'You sent' and ending with the text 'which was ## characters', where ## is the total number of characters in the message that you sent.* 这样的回复。\n结语\n好啦，搭建自己的机器人是不是非常简单呢。但是，如果想搭建一个如Siri或者Cortana这样的机器人，那是需要海量数据来支持的，甚至还需要自学习算法以及人工智能的知识。因为当一个用户输入一段话的时候，机器人必须了解这句话的含义，甚至要去猜测这句话的意思，并给出正确或者接近的答案。这也是为什么能做聊天机器人的公司大都是搜索公司或者用户量极大的互联网公司。\n这个项目只是我们触及机器人的一小步，还有很多的知识来不及介绍，比如如何将Bot发布到服务器上，并且用微信或者Skype进行访问，如何将Bot和一些Cortana这些智能平台进行交互，这些大家可以自行到官网上去了解并学习。\nHow the Bot Framework works\nPrinciples of bot design\nBot Builder SDK for .NET\nDeploy a bot to the cloud\nBot Framework FAQ"}
{"content2":"优达学院-纳米学位-机器学习工程师专业视频教程\n关键字：优达学院，纳米学位，机器学习，人工智能，视频，教程\n获取视频教程\n目录\n1-欢迎学习机器学习纳米学位工程师课程\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习\n1-欢迎学习机器学习纳米学位工程师课程/3-探索性项目-泰坦尼克号幸存者分析\n2-模型评估和验证\n2-模型评估和验证/139-code\n2-模型评估和验证/98-code\n3-监督学习-构建学生干预系统\n3-监督学习-构建学生干预系统/1-项目描述\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料\n3-监督学习-构建学生干预系统/10-内核方法和SVM\n3-监督学习-构建学生干预系统/11-SVM\n3-监督学习-构建学生干预系统/12-基于实例的学习\n3-监督学习-构建学生干预系统/13-朴素贝叶斯\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-code\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/13-code\n3-监督学习-构建学生干预系统/14-贝叶斯学习\n3-监督学习-构建学生干预系统/15-贝叶斯推理\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目\n3-监督学习-构建学生干预系统/17-集成B&B\n3-监督学习-构建学生干预系统/18-构建学生干预系统\n3-监督学习-构建学生干预系统/2-监督学习简介\n3-监督学习-构建学生干预系统/3-决策树\n3-监督学习-构建学生干预系统/4-更多决策树\n3-监督学习-构建学生干预系统/4-更多决策树/7-决策树编码-code\n3-监督学习-构建学生干预系统/5-回归和分类\n3-监督学习-构建学生干预系统/6-回归\n3-监督学习-构建学生干预系统/6-回归/18-code\n3-监督学习-构建学生干预系统/6-回归/21-练习提取信息\n3-监督学习-构建学生干预系统/7-更多回归\n3-监督学习-构建学生干预系统/8-神经网络\n3-监督学习-构建学生干预系统/9-神经网络迷你项目\n4-非监督学习-创建客户细分\n4-非监督学习-创建客户细分/1-非监督学习简介\n4-非监督学习-创建客户细分/10-结尾\n4-非监督学习-创建客户细分/11-项目\n4-非监督学习-创建客户细分/2-聚类\n4-非监督学习-创建客户细分/3-更多聚类\n4-非监督学习-创建客户细分/4-聚类迷你项目\n4-非监督学习-创建客户细分/5-特征缩放\n4-非监督学习-创建客户细分/6-特征选择\n4-非监督学习-创建客户细分/7-PCA\n4-非监督学习-创建客户细分/8-PCA迷你项目\n4-非监督学习-创建客户细分/9-特征转换\n5-强化学习-训练智能出租车学会驾驶\n5-强化学习-训练智能出租车学会驾驶/1-强化学习简介\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程\n5-强化学习-训练智能出租车学会驾驶/3-强化学习\n5-强化学习-训练智能出租车学会驾驶/4-博弈论\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论\n5-强化学习-训练智能出租车学会驾驶/6-项目\n6-kaggle\n6-kaggle/Kaggle -采访Ben Hamner\n6-kaggle/laggle挑战\n7-机器学习工程师模拟面试\n所有文件\n作业数据---machine-learning-master.zip\n机器学习模块作业以及练习数据链接.txt\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师/1-欢迎来到机器学习课程.mp4\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师/2-机器学习与传统编程的异同.mp4\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师/3-机器学习无处不在\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师/3-机器学习无处不在 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师/3-机器学习无处不在.mp4\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师/4-学习项目介绍.mp4\n1-欢迎学习机器学习纳米学位工程师课程/1-欢迎学习机器学习工程师/说明.docx\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/1-开始机器学习.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/10-什么事基于知识的人工智能\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/10-什么是基于知识的人工智能？ 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/10-什么是基于知识的人工智能？2.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/11-人工智能的四个学派\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/11-人工智能的四个学派 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/11-人工智能的四个学派.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/12-贝叶斯公式入门.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/12-贝叶斯规则.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/13-贝叶斯网络\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/13-贝叶斯网络 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/13-贝叶斯网络.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/14-机器学习与数据科学.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/15-什么是数据科学家？.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/16-什么是数据科学家？II.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/17-数据科学家都做些什么？.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/18-Pi Chuan - 什么是数据科学？.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/19-数据科学家的基本技能.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/2-人工智能简介.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/20-数据科学解决的问题.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/21-从人工智能到机器学习.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/22-简介 - 第 2 部分.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/22-简介.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/23-佐治亚理工学院机器学习课程.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/24-机器学习的定义.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/25-强化学习.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/25-监督学习.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/26-归纳法与演绎法.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/27-归纳法，演绎法与溯因法.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/28-非监督学习.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/29-机器学习的实际应用.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/3-人工智能难题.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/30-Stanley Darpa 超级挑战赛.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/31-医疗保健现在的问题.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/32-认知计算：现代应用.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/33-简介.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/34-基本要素.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/35-分类法.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/36-监督学习\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/36-监督学习 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/36-监督学习.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/37-垃圾邮件检测\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/37-垃圾邮件检测 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/37-垃圾邮件检测.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/38-分类和回归\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/38-分类和回归 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/38-分类和回归.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/39-线性回归\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/39-线性回归 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/39-线性回归.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/4-人工智能问题的特点.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/40-更多线性回归.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/41-基础知识总结.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/5-人工智能和不确定性.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/6-有哪些人工智能的问题.png\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/6-有哪些人工智能问题？ 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/6-有哪些人工智能问题？.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/7-人工智能的实际运用\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/7-人工智能的实际运用：Watson 答案.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/7-人工智能的实际运用：Watson.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/8-什么是基于知识的人工智能？.mp4\n1-欢迎学习机器学习纳米学位工程师课程/2-从人工智能到机器学习/9-基础知识：人工智能的四个学派.mp4\n1-欢迎学习机器学习纳米学位工程师课程/3-探索性项目-泰坦尼克号幸存者分析/项目.docx\n2-模型评估和验证/1-模型评估和验证简介.mp4\n2-模型评估和验证/10-数据集的众数答案.mp4\n2-模型评估和验证/10-选择哪个数字？.mp4\n2-模型评估和验证/100-选择合适的指标.txt\n2-模型评估和验证/101-分类和回归.txt\n2-模型评估和验证/102-分类指标与回归指标.txt\n2-模型评估和验证/103-分类指标.txt\n2-模型评估和验证/104-准确率.txt\n2-模型评估和验证/105\n2-模型评估和验证/105-准确率的缺陷 答案.mp4\n2-模型评估和验证/105-准确率的缺陷.mp4\n2-模型评估和验证/106-选择最合适的指标.mp4\n2-模型评估和验证/107\n2-模型评估和验证/107-混淆矩阵 打啊 .mp4\n2-模型评估和验证/107-混淆矩阵.mp4\n2-模型评估和验证/107-混淆矩阵练习 1 答案.mp4\n2-模型评估和验证/108\n2-模型评估和验证/108-混淆矩阵练习 1.mp4\n2-模型评估和验证/108-混淆矩阵练习 2 答案.mp4\n2-模型评估和验证/108-混淆矩阵练习 2.mp4\n2-模型评估和验证/109\n2-模型评估和验证/109-填充混淆矩阵 答案.mp4\n2-模型评估和验证/109-填充混淆矩阵.mp4\n2-模型评估和验证/11-分布的众数.mp4\n2-模型评估和验证/11-数据集的众数.mp4\n2-模型评估和验证/110\n2-模型评估和验证/110-混淆矩阵：误报 答案.mp4\n2-模型评估和验证/110-混淆矩阵：误报.mp4\n2-模型评估和验证/111\n2-模型评估和验证/111-决策树混淆矩阵 答案.mp4\n2-模型评估和验证/111-决策树混淆矩阵.mp4\n2-模型评估和验证/112-精确率和召回率.mp4\n2-模型评估和验证/113\n2-模型评估和验证/113-鲍威尔精确率和召回率.mp4\n2-模型评估和验证/113-鲍威尔精确率和召回率答案.mp4\n2-模型评估和验证/114\n2-模型评估和验证/114-布什精确率和召回率 答案.mp4\n2-模型评估和验证/114-布什精确率和召回率.mp4\n2-模型评估和验证/115\n2-模型评估和验证/115-特征脸方法中的 True Positives 答案.mp4\n2-模型评估和验证/115-特征脸方法中的 True Positives.mp4\n2-模型评估和验证/116-\n2-模型评估和验证/116-特征脸方法中的 False Positives 答案.mp4\n2-模型评估和验证/116-特征脸方法中的 False Positives.mp4\n2-模型评估和验证/117\n2-模型评估和验证/117-特征脸方法中的 False Negatives 答案.mp4\n2-模型评估和验证/117-特征脸方法中的 False Negatives-c.mp4\n2-模型评估和验证/117-特征脸方法中的 False Negatives.mp4\n2-模型评估和验证/118\n2-模型评估和验证/118-对拉姆斯菲尔德练习 TP、FP、FN.mp4\n2-模型评估和验证/118-答案.mp4\n2-模型评估和验证/119\n2-模型评估和验证/119-精确率公式 答案.mp4\n2-模型评估和验证/119-精确率公式.mp4\n2-模型评估和验证/12-众数 - 负偏斜分布 答案.mp4\n2-模型评估和验证/12-众数 - 负偏斜分布.mp4\n2-模型评估和验证/120\n2-模型评估和验证/120-召回率公式 答案.mp4\n2-模型评估和验证/120-召回率公式.mp4\n2-模型评估和验证/121-F1分数.txt\n2-模型评估和验证/122-回归指标.txt\n2-模型评估和验证/123-平均绝对误差.txt\n2-模型评估和验证/124-均方误差.txt\n2-模型评估和验证/125-回归分数函数.txt\n2-模型评估和验证/126-误差原因.txt\n2-模型评估和验证/127-偏差造成的误差.txt\n2-模型评估和验证/128-Linear Learner, Quadratic Data.txt\n2-模型评估和验证/129-方差造成的误差.txt\n2-模型评估和验证/13-众数 - 均匀分布.mp4\n2-模型评估和验证/13-众数 - 均匀分布答案.mp4\n2-模型评估和验证/13-联系\n2-模型评估和验证/130-Noisy Data, Complex Model.txt\n2-模型评估和验证/131-改进模型的有效性.txt\n2-模型评估和验证/132-偏差、方差和特征数量\n2-模型评估和验证/132-偏差、方差和特征数量 答案.mp4\n2-模型评估和验证/132-偏差、方差和特征数量.mp4\n2-模型评估和验证/133\n2-模型评估和验证/133-偏差、方差和特征数量 2 答案.mp4\n2-模型评估和验证/133-偏差、方差和特征数量 2.mp4\n2-模型评估和验证/134-肉眼过拟合.mp4\n2-模型评估和验证/135-数据类型 1 - 数值数据.mp4\n2-模型评估和验证/136-数据类型 2 - 分类数据.mp4\n2-模型评估和验证/137-数据类型 3 - 时间序列数据.mp4\n2-模型评估和验证/138\n2-模型评估和验证/138-数据类型 3 - 时间序列数据 答案.mp4\n2-模型评估和验证/138-数据类型 3 - 时间序列数据.mp4\n2-模型评估和验证/139-在 Sklearn 中训练测试分离.mp4\n2-模型评估和验证/14-不止一个众数？ 练习.jpg\n2-模型评估和验证/14-不止一个众数？.mp4\n2-模型评估和验证/14-不止一个众数？答案.mp4\n2-模型评估和验证/140\n2-模型评估和验证/140-K 折交叉验证 答案.mp4\n2-模型评估和验证/140-K 折交叉验证.mp4\n2-模型评估和验证/141-Sklearn 中的 K 折 CV.mp4\n2-模型评估和验证/142-针对 Sklearn 中的 K 折的实用建议.mp4\n2-模型评估和验证/143-为调整参数而进行的交叉验证.mp4\n2-模型评估和验证/144\n2-模型评估和验证/144-Sklearn 中的 GridSearchCV - 练习.txt\n2-模型评估和验证/144-Sklearn 中的 GridSearchCV.txt\n2-模型评估和验证/145-总结.txt\n2-模型评估和验证/146-维度灾难.mp4\n2-模型评估和验证/147-维度灾难 2.mp4\n2-模型评估和验证/148-学习曲线.txt\n2-模型评估和验证/148-学习曲线2.txt\n2-模型评估和验证/149-理想的学习曲线.txt\n2-模型评估和验证/15-分类数据的众数 答案.mp4\n2-模型评估和验证/15-分类数据的众数 练习.jpg\n2-模型评估和验证/15-分类数据的众数.mp4\n2-模型评估和验证/150-模型复杂度.txt\n2-模型评估和验证/151-学习曲线与模型复杂度.txt\n2-模型评估和验证/152-模型复杂度的实际使用.txt\n2-模型评估和验证/153-摘要.txt\n2-模型评估和验证/154-问题和报告结构.txt\n2-模型评估和验证/154-项目.txt\n2-模型评估和验证/16-众数的更多信息！.mp4\n2-模型评估和验证/16-众数的更多信息！答案.mp4\n2-模型评估和验证/16-众数的更多信息！练习.jpg\n2-模型评估和验证/17-找出均值 答案.mp4\n2-模型评估和验证/17-找出均值 练习.jpg\n2-模型评估和验证/17-找出均值.mp4\n2-模型评估和验证/18-找出均值的步骤 答案.mp4\n2-模型评估和验证/18-找出均值的步骤 练习.jpg\n2-模型评估和验证/18-找出均值的步骤.mp4\n2-模型评估和验证/19-迭代过程 答案.mp4\n2-模型评估和验证/19-迭代过程 练习.jpg\n2-模型评估和验证/19-迭代过程.mp4\n2-模型评估和验证/2-模型评估 - 你将看到什么.mp4\n2-模型评估和验证/20-有用的符号.mp4\n2-模型评估和验证/21-均值的特性.jpg\n2-模型评估和验证/21-均值的特性.mp4\n2-模型评估和验证/21-均值的特性答案.mp4\n2-模型评估和验证/22-含异常值的均值 答案.mp4\n2-模型评估和验证/22-含异常值的均值 练习.jpg\n2-模型评估和验证/22-含异常值的均值.mp4\n2-模型评估和验证/23-可以期望多高的薪资？ 答案.mp4\n2-模型评估和验证/23-可以期望多高的薪资？.jpg\n2-模型评估和验证/23-可以期望多高的薪资？.mp4\n2-模型评估和验证/24-北卡莱罗纳大学.mp4\n2-模型评估和验证/25-中位数的要求 答案.mp4\n2-模型评估和验证/25-中位数的要求.jpg\n2-模型评估和验证/25-中位数的要求.mp4\n2-模型评估和验证/26-找出中位数.mp4\n2-模型评估和验证/26-找出中位数答案.mp4\n2-模型评估和验证/27-含异常值的中位数.mp4\n2-模型评估和验证/27-含异常值的中位数答案.mp4\n2-模型评估和验证/28-找出含异常值的中位数 答案.mp4\n2-模型评估和验证/28-找出含异常值的中位数.mp4\n2-模型评估和验证/29-中心测量值.mp4\n2-模型评估和验证/3-模型评估 - 你将学到什么.mp4\n2-模型评估和验证/30-对中心测量值排序 1 答案.mp4\n2-模型评估和验证/30-对中心测量值排序 1.jpg\n2-模型评估和验证/30-对中心测量值排序 1.mp4\n2-模型评估和验证/31-对中心测量值排序 2答案.mp4\n2-模型评估和验证/31对中心测量值排序 2.mp4\n2-模型评估和验证/32-使用中心测量值来比较.mp4\n2-模型评估和验证/33-优达学城员工的 Facebook 好友数 - 均值 答案.mp4.mp4\n2-模型评估和验证/33-优达学城员工的 Facebook 好友数 - 均值.jpg\n2-模型评估和验证/33-优达学城员工的 Facebook 好友数 - 均值.mp4\n2-模型评估和验证/34-优达学城员工的 Facebook 好友数 - 中位数.mp4\n2-模型评估和验证/35-中位数位置公式\n2-模型评估和验证/35-中位数位置公式.mp4\n2-模型评估和验证/36\n2-模型评估和验证/36-小结 - 中心测量值 答案.mp4\n2-模型评估和验证/36-小结 - 中心测量值.mp4\n2-模型评估和验证/37-真棒！.mp4\n2-模型评估和验证/38\n2-模型评估和验证/38-社交网络工作人员的薪酬 答案.mp4\n2-模型评估和验证/38-社交网络工作人员的薪酬.mp4\n2-模型评估和验证/39-你应该注册帐号吗？\n2-模型评估和验证/39-你应该注册帐号吗？.mp4\n2-模型评估和验证/4-构建完整的模型.png\n2-模型评估和验证/40-有什么不同 答案.mp4\n2-模型评估和验证/40-有什么不同？\n2-模型评估和验证/40-有什么不同？.mp4\n2-模型评估和验证/41\n2-模型评估和验证/41-量化数据的分布形态 答案.mp4\n2-模型评估和验证/41-量化数据的分布形态.mp4\n2-模型评估和验证/42\n2-模型评估和验证/42-值域是否改变？ 答案.mp4\n2-模型评估和验证/42-值域是否改变？.mp4\n2-模型评估和验证/43-扎克伯格的薪酬：一个异常值\n2-模型评估和验证/43-扎克伯格的薪酬：一个异常值 答案.mp4\n2-模型评估和验证/43-扎克伯格的薪酬：一个异常值.mp4\n2-模型评估和验证/44-砍掉尾巴.mp4\n2-模型评估和验证/45\n2-模型评估和验证/45-Q1 在哪里？.mp4\n2-模型评估和验证/45-Q1 在哪里？答案.mp4\n2-模型评估和验证/46\n2-模型评估和验证/46-Q3 - Q1.mp4\n2-模型评估和验证/47-IQR\n2-模型评估和验证/47-IQR.mp4\n2-模型评估和验证/48-IQR 答案.mp4\n2-模型评估和验证/49\n2-模型评估和验证/49-什么是异常值？ 答案.mp4\n2-模型评估和验证/49-什么是异常值？.mp4\n2-模型评估和验证/5-模型评估 - 你将做什么.mp4\n2-模型评估和验证/50-匹配对应的箱线图\n2-模型评估和验证/50-定义异常值.mp4\n2-模型评估和验证/50匹配对应的箱线图 答案.mp4\n2-模型评估和验证/50匹配对应的箱线图 答案.mp4.mp4\n2-模型评估和验证/51-均值在 IQR 中吗？\n2-模型评估和验证/51-均值在 IQR 中吗？.mp4\n2-模型评估和验证/51-均值在 IQR 中吗？答案.mp4\n2-模型评估和验证/52-IQR 的不足.mp4\n2-模型评估和验证/53\n2-模型评估和验证/53-衡量差异性的方法 答案.mp4\n2-模型评估和验证/53-衡量差异性的方法.mp4\n2-模型评估和验证/54-计算均值\n2-模型评估和验证/54-计算均值 答案.mp4\n2-模型评估和验证/54-计算均值.mp4\n2-模型评估和验证/55\n2-模型评估和验证/55-离均差 答案.mp4\n2-模型评估和验证/55-离均差.mp4\n2-模型评估和验证/56\n2-模型评估和验证/56-平均偏差 答案.mp4\n2-模型评估和验证/56-平均偏差.mp4\n2-模型评估和验证/57\n2-模型评估和验证/57-平均偏差的公式 答案.mp4\n2-模型评估和验证/57-平均偏差的公式.mp4\n2-模型评估和验证/58\n2-模型评估和验证/58- 摆脱负值，开心起来.mp4\n2-模型评估和验证/58-摆脱负值，开心起来 答案.mp4\n2-模型评估和验证/59\n2-模型评估和验证/59-绝对偏差 答案.mp4\n2-模型评估和验证/59-绝对偏差.mp4\n2-模型评估和验证/6-统计学回顾与支持库.png\n2-模型评估和验证/60\n2-模型评估和验证/60-平均绝对偏差 答案.mp4\n2-模型评估和验证/60-平均绝对偏差.mp4\n2-模型评估和验证/61\n2-模型评估和验证/61-平均绝对偏差的公式 答案.mp4\n2-模型评估和验证/61-平均绝对偏差的公式.mp4\n2-模型评估和验证/62\n2-模型评估和验证/62-平方偏差 答案.mp4\n2-模型评估和验证/62-平方偏差.mp4\n2-模型评估和验证/63\n2-模型评估和验证/63-平方和.mp4\n2-模型评估和验证/64-平方和.mp4\n2-模型评估和验证/65-平均平方偏差\n2-模型评估和验证/65-平均平方偏差 .mp4\n2-模型评估和验证/65-平均平方偏差 答案.mp4\n2-模型评估和验证/66\n2-模型评估和验证/66-用语言解释平均平方偏差.mp4\n2-模型评估和验证/67\n2-模型评估和验证/67-一维的数据 答案.mp4\n2-模型评估和验证/67-一维的数据.mp4\n2-模型评估和验证/68-标准偏差 SD.mp4\n2-模型评估和验证/69-计算标准偏差 SD\n2-模型评估和验证/69-计算标准偏差 SD 答案.mp4\n2-模型评估和验证/69-计算标准偏差 SD.mp4\n2-模型评估和验证/7-先修要求.mp4\n2-模型评估和验证/70\n2-模型评估和验证/70-社交网络工作人员薪酬的 SD 值 答案.mp4\n2-模型评估和验证/70-社交网络工作人员薪酬的 SD 值.mp4\n2-模型评估和验证/71\n2-模型评估和验证/71-用语言解释标准偏差 答案.mp4\n2-模型评估和验证/71-用语言解释标准偏差.mp4\n2-模型评估和验证/72\n2-模型评估和验证/72-Sample_Social_Networkers_Salary_n=100_Lesson_4.xlsx\n2-模型评估和验证/72-用电子表格计算 SD 值.mp4\n2-模型评估和验证/73-用电子表格计算 SD 值 答案.mp4\n2-模型评估和验证/74-SD 值的重要性.mp4\n2-模型评估和验证/75\n2-模型评估和验证/75-找到偏差对应的值 答案.mp4\n2-模型评估和验证/75-找到偏差对应的值.mp4\n2-模型评估和验证/76\n2-模型评估和验证/76-所选样本的 SD 值 答案.mp4\n2-模型评估和验证/76-所选样本的 SD 值.mp4\n2-模型评估和验证/77\n2-模型评估和验证/77-贝塞耳校正 答案.mp4\n2-模型评估和验证/77-贝塞耳校正.mp4\n2-模型评估和验证/78-澄清样本 SD 值的真正含义.mp4\n2-模型评估和验证/79-举例：果冻豆.mp4\n2-模型评估和验证/8-哪个专业？.mp4\n2-模型评估和验证/8-哪个专业？答案.mp4\n2-模型评估和验证/80-Numpy 和 Pandas 教程.png\n2-模型评估和验证/81-Numpy.mp4\n2-模型评估和验证/82-Numpy Playground.py.txt\n2-模型评估和验证/83-Pandas.mp4\n2-模型评估和验证/84-Pandas Playground - 系列.txt\n2-模型评估和验证/85-Pandas Playground - 数据框.txt\n2-模型评估和验证/86-创建新 DataFrame 答案.mp4\n2-模型评估和验证/86-创建新 DataFrame.mp4\n2-模型评估和验证/86-创建新 DataFrame.txt\n2-模型评估和验证/87-数据框列.mp4\n2-模型评估和验证/88-Pandas Playground - 索引数据框.txt\n2-模型评估和验证/89-Pandas 向量化方法.mp4\n2-模型评估和验证/9-用一个数字描述数据.mp4\n2-模型评估和验证/90-平均铜牌数-答案.mp4\n2-模型评估和验证/90-平均铜牌数.mp4\n2-模型评估和验证/90-平均铜牌数.txt\n2-模型评估和验证/91-平均金、银和铜牌数 答案.mp4\n2-模型评估和验证/91-平均金、银和铜牌数.mp4\n2-模型评估和验证/91-平均金、银和铜牌数.txt\n2-模型评估和验证/92-矩阵乘法与 Numpy Dot.mp4\n2-模型评估和验证/93-奥林匹克奖牌分数 答案.mp4\n2-模型评估和验证/93-奥林匹克奖牌分数.mp4\n2-模型评估和验证/93-奥林匹克奖牌数.txt\n2-模型评估和验证/94-安装 scikit-learn.png\n2-模型评估和验证/94-安装 scikit-learn.txt\n2-模型评估和验证/95.png\n2-模型评估和验证/96-sklearn 使用入门.mp4\n2-模型评估和验证/97-高斯朴素贝叶斯示例.mp4\n2-模型评估和验证/98-有关地形数据的高斯 NB 部署 答案.mp4\n2-模型评估和验证/98-有关地形数据的高斯 NB 部署.mp4\n2-模型评估和验证/99-评估指标.mp4\n2-模型评估和验证/139-code/studentCode.py\n2-模型评估和验证/139-code/subFunction.py\n2-模型评估和验证/98-code/ClassifyNB.py\n2-模型评估和验证/98-code/class_vis.py\n2-模型评估和验证/98-code/prep_terrain_data.py\n2-模型评估和验证/98-code/studentMain.py\n3-监督学习-构建学生干预系统/1-项目描述/1-项目描述.png\n3-监督学习-构建学生干预系统/1-项目描述/2-语言、库和成果 链接.txt\n3-监督学习-构建学生干预系统/1-项目描述/2-语言、库和成果.png\n3-监督学习-构建学生干预系统/1-项目描述/3-问题和报告结构.txt\n3-监督学习-构建学生干预系统/1-项目描述/4-构建学生干预系统.txt\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Bayesian Inference.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Bayesian Learning.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Gradient Descent.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/ID3 Algorithm for Decision Trees.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Instance Based Learning.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Intro to Boosting.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Kernel_Methods_and_SVMs.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Linear Regression Review.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/Neural Networks.pdf\n3-监督学习-构建学生干预系统/1-项目描述/4-辅助材料/P2- Building an Intervention System Videos.zip\n3-监督学习-构建学生干预系统/10-内核方法和SVM/1-最佳直线\n3-监督学习-构建学生干预系统/10-内核方法和SVM/1-最佳直线 答案.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/1-最佳直线.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/10-小节.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/11-真的小节.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/2-支持向量机.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/3-平面之间的距离\n3-监督学习-构建学生干预系统/10-内核方法和SVM/3-平面之间的距离 答案.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/3-平面之间的距离.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/4-还是支持向量机.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/5-更多支持向量机.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/6-最佳分离器\n3-监督学习-构建学生干预系统/10-内核方法和SVM/6-最佳分离器 答案.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/6-最佳分离器.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/7-线性结合.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/8-输出的内容是什么？\n3-监督学习-构建学生干预系统/10-内核方法和SVM/8-输出的内容是什么？ 答案.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/8-输出的内容是什么？.mp4\n3-监督学习-构建学生干预系统/10-内核方法和SVM/9-内核.mp4\n3-监督学习-构建学生干预系统/11-SVM/1-欢迎学习 SVM.mp4\n3-监督学习-构建学生干预系统/11-SVM/10-SKlearn 中的 SVM.mp4\n3-监督学习-构建学生干预系统/11-SVM/11-SVM 决策边界.mp4\n3-监督学习-构建学生干预系统/11-SVM/12-SVM 编码 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/12-SVM 编码.mp4\n3-监督学习-构建学生干预系统/11-SVM/12-SVM编码.py\n3-监督学习-构建学生干预系统/11-SVM/13-非线性 SVM.mp4\n3-监督学习-构建学生干预系统/11-SVM/14-非线性数据\n3-监督学习-构建学生干预系统/11-SVM/14-非线性数据 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/14-非线性数据.mp4\n3-监督学习-构建学生干预系统/11-SVM/15-新特征\n3-监督学习-构建学生干预系统/11-SVM/15-新特征.mp4\n3-监督学习-构建学生干预系统/11-SVM/16- 与新特征分隔 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/16-与新特征分隔\n3-监督学习-构建学生干预系统/11-SVM/16-与新特征分隔.mp4\n3-监督学习-构建学生干预系统/11-SVM/16-可视化新特征.mp4\n3-监督学习-构建学生干预系统/11-SVM/17-练习创建新特征\n3-监督学习-构建学生干预系统/11-SVM/17-练习创建新特征 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/17-练习创建新特征.mp4\n3-监督学习-构建学生干预系统/11-SVM/18-核技巧.mp4\n3-监督学习-构建学生干预系统/11-SVM/19-尝试选择各种核\n3-监督学习-构建学生干预系统/11-SVM/19-尝试选择各种核 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/19-尝试选择各种核.mp4\n3-监督学习-构建学生干预系统/11-SVM/2-分隔线\n3-监督学习-构建学生干预系统/11-SVM/2-分隔线 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/2-分隔线.mp4\n3-监督学习-构建学生干预系统/11-SVM/20-核和伽玛\n3-监督学习-构建学生干预系统/11-SVM/20-核和伽玛 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/20-核和伽玛.mp4\n3-监督学习-构建学生干预系统/11-SVM/21-SVM C 参数\n3-监督学习-构建学生干预系统/11-SVM/21-SVM C 参数 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/21-SVM C 参数.mp4\n3-监督学习-构建学生干预系统/11-SVM/22-过拟合\n3-监督学习-构建学生干预系统/11-SVM/22-过拟合 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/22-过拟合.mp4\n3-监督学习-构建学生干预系统/11-SVM/23-SVM 的优缺点.mp4\n3-监督学习-构建学生干预系统/11-SVM/3-选择分隔线\n3-监督学习-构建学生干预系统/11-SVM/3-选择分隔线.mp4\n3-监督学习-构建学生干预系统/11-SVM/4-好的分隔线有何特点\n3-监督学习-构建学生干预系统/11-SVM/4-好的分隔线有何特点 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/4-好的分隔线有何特点.mp4\n3-监督学习-构建学生干预系统/11-SVM/5-间隔练习\n3-监督学习-构建学生干预系统/11-SVM/5-间隔练习.mp4\n3-监督学习-构建学生干预系统/11-SVM/6-SVMs 和棘手的数据分布\n3-监督学习-构建学生干预系统/11-SVM/6-SVMs 和棘手的数据分布 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/6-SVMs 和棘手的数据分布.mp4\n3-监督学习-构建学生干预系统/11-SVM/7-SVM 对异常值的响应\n3-监督学习-构建学生干预系统/11-SVM/7-SVM 对异常值的响应.mp4\n3-监督学习-构建学生干预系统/11-SVM/8-SVM 异常值练习\n3-监督学习-构建学生干预系统/11-SVM/8-SVM 异常值练习 答案.mp4\n3-监督学习-构建学生干预系统/11-SVM/8-SVM 异常值练习.mp4\n3-监督学习-构建学生干预系统/11-SVM/9-移交给 Katie.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/1-基于实例的学习（以前）.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/10-我们学到了什么.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/2-基于实例的学习（现在）.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/3-房价 2.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/3-房价.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/4-K NN.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/5-您是否不会计算我的近邻\n3-监督学习-构建学生干预系统/12-基于实例的学习/5-您是否不会计算我的近邻 答案.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/5-您是否不会计算我的近邻.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/6-域 K NNowledge\n3-监督学习-构建学生干预系统/12-基于实例的学习/6-域 K NNowledge 答案.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/6-域 K NNowledge.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/7-K NN 偏差.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/8-维度灾难 2.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/8-维度灾难.mp4\n3-监督学习-构建学生干预系统/12-基于实例的学习/9-另一些东西.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/1-速度散点图：坡度和颠簸度\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/1-速度散点图：坡度和颠簸度.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/10-sklearn 使用入门.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/11-高斯朴素贝叶斯示例.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-有关地形数据的高斯 NB 部署 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-有关地形数据的高斯 NB 部署.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/13-计算 NB 准确性.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/14-训练数据和测试数据.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/15-使用贝叶斯规则将 NB 拆包.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/16-贝叶斯规则.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/17-癌症测试\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/17-癌症测试 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/17-癌症测试.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/17-资料.txt\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/18-先验和后验\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/18-先验和后验 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/18-先验和后验.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 1\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 1 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 1.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 2\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 2 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 2.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 3\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 3 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/19-规范化 3.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/2-速度散点图 2\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/2-速度散点图 2.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/20-全概率\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/20-全概率 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/20-全概率.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/21-贝叶斯规则图.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/22-用于分类的贝叶斯规则.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/23-Chris 或 Sara.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/24-后验概率\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/24-后验概率 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/24-后验概率.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/25-你独自得出的贝叶斯概率\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/25-你独自得出的贝叶斯概率 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/25-你独自得出的贝叶斯概率.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/26-为何朴素贝叶斯很朴素\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/26-为何朴素贝叶斯很朴素 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/26-为何朴素贝叶斯很朴素.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/3-速度散点图 2\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/3-速度散点图 2 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/3-速度散点图 2.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/4-从散点图到预测\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/4-从散点图到预测 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/4-从散点图到预测.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/5-从散点图到预测 2\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/5-从散点图到预测 2 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/5-从散点图到预测 2.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/6-从散点图到决策面\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/6-从散点图到决策面 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/6-从散点图到决策面.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/7-良好的线性决策面\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/7-良好的线性决策面 答案.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/7-良好的线性决策面.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/8-转为使用朴素贝叶斯.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/9-Python 中的 NB 决策边界.mp4\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-code/ClassifyNB.py\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-code/class_vis.py\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-code/prep_terrain_data.py\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-code/studentMain.py\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/12-code/练习.txt\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/13-code/classify.py\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/13-code/studentCode.py\n3-监督学习-构建学生干预系统/13-朴素贝叶斯/13-code/说明.txt\n3-监督学习-构建学生干预系统/14-贝叶斯学习/1-简介.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/10-最短描述长度.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/11-哪颗树\n3-监督学习-构建学生干预系统/14-贝叶斯学习/11-哪颗树 答案.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/11-哪颗树.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/12-贝叶斯分类\n3-监督学习-构建学生干预系统/14-贝叶斯学习/12-贝叶斯分类 .mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/12-贝叶斯分类 答案.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/13-小结.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/2-贝叶斯规则.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/3-贝叶斯规则 2.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/4-贝叶斯规则 测验\n3-监督学习-构建学生干预系统/14-贝叶斯学习/4-贝叶斯规则 测验 答案.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/4-贝叶斯规则 测验.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/5-贝叶斯学习.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/6-实践中的贝叶斯学习.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/7-噪声数据\n3-监督学习-构建学生干预系统/14-贝叶斯学习/7-噪声数据 daan .mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/7-噪声数据.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/8-返回到贝叶斯学习.mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/9-最佳假设\n3-监督学习-构建学生干预系统/14-贝叶斯学习/9-最佳假设 daan .mp4\n3-监督学习-构建学生干预系统/14-贝叶斯学习/9-最佳假设.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/1-简介.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/10-取样.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/11-推理规则.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/12-推理规则 测验\n3-监督学习-构建学生干预系统/15-贝叶斯推理/12-推理规则 测验 答案.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/12-推理规则 测验.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/13-手工推理\n3-监督学习-构建学生干预系统/15-贝叶斯推理/13-手工推理 答案.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/13-手工推理.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/14-朴素贝叶斯.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/15-为何朴素贝叶斯很酷.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/16-小结.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/2-联合分布.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/3-联合分布 测验\n3-监督学习-构建学生干预系统/15-贝叶斯推理/3-联合分布 测验 答案.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/3-联合分布 测验.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/3-错误.txt\n3-监督学习-构建学生干预系统/15-贝叶斯推理/4-添加属性.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/5-条件独立性.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/6-条件 测验\n3-监督学习-构建学生干预系统/15-贝叶斯推理/6-条件 测验 答案.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/6-条件 测验.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/7-信念网络\n3-监督学习-构建学生干预系统/15-贝叶斯推理/7-信念网络 答案.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/7-信念网络.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/8-从联合分布中取样\n3-监督学习-构建学生干预系统/15-贝叶斯推理/8-从联合分布中取样 答案.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/8-从联合分布中取样.mp4\n3-监督学习-构建学生干预系统/15-贝叶斯推理/9-恢复联合分布.mp4\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/1-项目说明.docx\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/2-计算.docx\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/3-最大可能性.py\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/3-最大可能性.txt\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/4-NLP声明.txt\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/5-最佳分类器 示例.docx\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/5-最佳分类器练习.py\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/6-词语调解.docx\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/7-联合分布分析\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/8-区间知识测验\n3-监督学习-构建学生干预系统/16-贝叶斯NLP迷你项目/9-区间知识填入\n3-监督学习-构建学生干预系统/17-集成B&B/10-最重要的部分.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/11-在 D 同意时\n3-监督学习-构建学生干预系统/17-集成B&B/11-在 D 同意时 答案.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/11-在 D 同意时.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/12-最终假设.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/13-三个小箱子.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/14-哪个假设.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/15-好的答案.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/16-返回到 Boosting.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/17-Boosting 容易过拟合 答案.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/17-Boosting 容易过拟合.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/17-Bootsting容易过拟合.docx\n3-监督学习-构建学生干预系统/17-集成B&B/18-小结.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/1集成学习 Boosting.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/2-集成学习的简单规则.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/3-集成学习算法.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/4-集成学习的输出\n3-监督学习-构建学生干预系统/17-集成B&B/4-集成学习的输出 答案.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/4-集成学习的输出.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/5-集成学习 示例.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/6-集成 Boosting.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/7-集成 Boosting 测验\n3-监督学习-构建学生干预系统/17-集成B&B/7-集成 Boosting 测验 2\n3-监督学习-构建学生干预系统/17-集成B&B/7-集成 Boosting 测验 2 答案.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/7-集成 Boosting 测验 2.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/7-集成 Boosting 测验 答案.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/7-集成 Boosting 测验.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/8-弱学习\n3-监督学习-构建学生干预系统/17-集成B&B/8-弱学习 答案.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/8-弱学习-练习.txt\n3-监督学习-构建学生干预系统/17-集成B&B/8-弱学习.mp4\n3-监督学习-构建学生干预系统/17-集成B&B/9-代码中的 Boosting.mp4\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Bayesian Inference.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Bayesian Learning.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Gradient Descent.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/ID3 Algorithm for Decision Trees.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Instance Based Learning.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Intro to Boosting.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Kernel_Methods_and_SVMs.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Linear Regression Review.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/Neural Networks.pdf\n3-监督学习-构建学生干预系统/18-构建学生干预系统/P2- Building an Intervention System Videos.zip\n3-监督学习-构建学生干预系统/18-构建学生干预系统/构建学生干预系统.docx\n3-监督学习-构建学生干预系统/2-监督学习简介/1-监督学习.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/2-你将看到和学到什么.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/3-Google 无人驾驶汽车中的机器学习.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/4-监督式学习 - 你将做什么.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/5-acerous 还是 non-acerous？\n3-监督学习-构建学生干预系统/2-监督学习简介/5-acerous 还是 non-acerous？ 答案.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/5-acerous 还是 non-acerous？.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/6-监督分类示例\n3-监督学习-构建学生干预系统/2-监督学习简介/6-监督分类示例 答案.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/6-监督分类示例.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/7-特征和标签音乐示例.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/8-特征可视化\n3-监督学习-构建学生干预系统/2-监督学习简介/8-特征可视化 答案.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/8-特征可视化.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/9-肉眼分类\n3-监督学习-构建学生干预系统/2-监督学习简介/9-肉眼分类 答案.mp4\n3-监督学习-构建学生干预系统/2-监督学习简介/9-肉眼分类.mp4\n3-监督学习-构建学生干预系统/3-决策树/10-分类与回归之间的区别.mp4\n3-监督学习-构建学生干预系统/3-决策树/11-小测验\n3-监督学习-构建学生干预系统/3-决策树/11-小测验 答案.mp4\n3-监督学习-构建学生干预系统/3-决策树/11-小测验.mp4\n3-监督学习-构建学生干预系统/3-决策树/12-分类学习 1.mp4\n3-监督学习-构建学生干预系统/3-决策树/13-分类学习 2.mp4\n3-监督学习-构建学生干预系统/3-决策树/14-分类学习 3.mp4\n3-监督学习-构建学生干预系统/3-决策树/15-示例 1：约会.mp4\n3-监督学习-构建学生干预系统/3-决策树/16-表示法.mp4\n3-监督学习-构建学生干预系统/3-决策树/17-表示法 测验\n3-监督学习-构建学生干预系统/3-决策树/17-表示法 测验 答案.mp4\n3-监督学习-构建学生干预系统/3-决策树/17-表示法 测验.mp4\n3-监督学习-构建学生干预系统/3-决策树/18-示例 2：20 个问题.mp4\n3-监督学习-构建学生干预系统/3-决策树/19-决策树学习.mp4\n3-监督学习-构建学生干预系统/3-决策树/20-最佳属性 测验\n3-监督学习-构建学生干预系统/3-决策树/20-最佳属性 测验 答案.mp4\n3-监督学习-构建学生干预系统/3-决策树/20-最佳属性 测验.mp4\n3-监督学习-构建学生干预系统/3-决策树/21-决策树可表达性 AND.mp4\n3-监督学习-构建学生干预系统/3-决策树/22-决策树可表达性 OR.mp4\n3-监督学习-构建学生干预系统/3-决策树/23-决策树可表达性 XOR.mp4\n3-监督学习-构建学生干预系统/3-决策树/24-决策树可表达性.mp4\n3-监督学习-构建学生干预系统/3-决策树/25-决策树可表达性 测验\n3-监督学习-构建学生干预系统/3-决策树/25-决策树可表达性 测验 2\n3-监督学习-构建学生干预系统/3-决策树/25-决策树可表达性 测验 2 答案.mp4\n3-监督学习-构建学生干预系统/3-决策树/25-决策树可表达性 测验 2.mp4\n3-监督学习-构建学生干预系统/3-决策树/25-决策树可表达性 测验 答案.mp4\n3-监督学习-构建学生干预系统/3-决策树/25-决策树可表达性 测验.mp4\n3-监督学习-构建学生干预系统/3-决策树/26-ID3 偏差.mp4\n3-监督学习-构建学生干预系统/3-决策树/26-ID3.mp4\n3-监督学习-构建学生干预系统/3-决策树/27-决策树其他注意事项.mp4\n3-监督学习-构建学生干预系统/3-决策树/27-决策树连续属性.mp4\n3-监督学习-构建学生干预系统/3-决策树/28-决策树其他注意事项 练习\n3-监督学习-构建学生干预系统/3-决策树/28-决策树其他注意事项 练习 答案网.mp4\n3-监督学习-构建学生干预系统/3-决策树/28-决策树其他注意事项 练习.mp4\n3-监督学习-构建学生干预系统/3-决策树/29-决策树其他注意事项 回归.mp4\n3-监督学习-构建学生干预系统/3-决策树/30-决策树小结.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/1-可线性分离的数据\n3-监督学习-构建学生干预系统/4-更多决策树/1-可线性分离的数据 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/1-可线性分离的数据.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/10-决策树参数\n3-监督学习-构建学生干预系统/4-更多决策树/10-决策树参数 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/10-决策树参数.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/11-最小样本分割\n3-监督学习-构建学生干预系统/4-更多决策树/11-最小样本分割 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/11-最小样本分割.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/12-决策树准确性 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/12-决策树准确性 练习.py\n3-监督学习-构建学生干预系统/4-更多决策树/12-决策树准确性.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/13-数据杂质与熵.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/14-在分割中尽可能减少杂质\n3-监督学习-构建学生干预系统/4-更多决策树/14-在分割中尽可能减少杂质 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/14-在分割中尽可能减少杂质.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/15-熵公式.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/15-熵公式.txt\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 1 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 1 部分） 打哪.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 1 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 2 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 2 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 2 部分）答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 3 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 3 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 4 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 4 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 4 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 5 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 5 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-熵计算（第 5 部分）答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/16-练习 熵计算（第 1 部分）.docx\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算 第五部分练习.txt\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 1 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 1 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 1 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 10 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 10 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 2 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 2 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 2 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 3 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 3 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 3 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 4 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 4 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 4 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 5 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 5 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 5 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 6 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 6 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 6 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 7 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 7 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 7 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 8 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 8 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 8 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 9 部分）\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 9 部分） 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/17-信息增益计算（第 9 部分）.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/18-调整标准参数.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/19-偏差-方差困境.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/2-多元线性问题\n3-监督学习-构建学生干预系统/4-更多决策树/2-多元线性问题 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/2-多元线性问题.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/20-DT 的优缺点.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/3-构建决策树-第一次分割\n3-监督学习-构建学生干预系统/4-更多决策树/3-构建决策树-第一次分割.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/4-构建决策树&第二次分割\n3-监督学习-构建学生干预系统/4-更多决策树/4-构建决策树&第二次分割.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/4-构建决策树-第二次分割 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/4-构建决策树-第二次分割.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/5-第二次分割后的类标签\n3-监督学习-构建学生干预系统/4-更多决策树/5-第二次分割后的类标签.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/6-构建决策树&第三次分割\n3-监督学习-构建学生干预系统/4-更多决策树/6-构建决策树&第三次分割.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/7-决策树编码.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/7-决策树编码图.jpg\n3-监督学习-构建学生干预系统/4-更多决策树/8-决策树编码 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/9-决策树准确性 答案.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/9-决策树准确性.mp4\n3-监督学习-构建学生干预系统/4-更多决策树/9-决策树准确性.py\n3-监督学习-构建学生干预系统/4-更多决策树/7-决策树编码-code/classifyDT.py\n3-监督学习-构建学生干预系统/4-更多决策树/7-决策树编码-code/studentMain.py\n3-监督学习-构建学生干预系统/5-回归和分类/1-什么是回归？\n3-监督学习-构建学生干预系统/5-回归和分类/1-什么是回归？ 答案.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/1-什么是回归？.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/10-波士顿房价示例回顾.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/11-其他输入空间.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/12-结论.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/2-回归与函数逼近.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/3-线性回归.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/4-找到最佳拟合\n3-监督学习-构建学生干预系统/5-回归和分类/4-找到最佳拟合.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/5-多项式的阶.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/6-选择成都-练习.txt\n3-监督学习-构建学生干预系统/5-回归和分类/6-选择程度\n3-监督学习-构建学生干预系统/5-回归和分类/6-选择程度 答案.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/6-选择程度.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/7-多项式回归.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/8-错误\n3-监督学习-构建学生干预系统/5-回归和分类/8-错误 答案.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/8-错误-练习.txt\n3-监督学习-构建学生干预系统/5-回归和分类/8-错误.mp4\n3-监督学习-构建学生干预系统/5-回归和分类/9-交叉验证.mp4\n3-监督学习-构建学生干预系统/6-回归/1-连续输出\n3-监督学习-构建学生干预系统/6-回归/1-连续输出 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/1-连续输出.mp4\n3-监督学习-构建学生干预系统/6-回归/10-回归线性方程\n3-监督学习-构建学生干预系统/6-回归/10-回归线性方程.mp4\n3-监督学习-构建学生干预系统/6-回归/11-斜率和截距.mp4\n3-监督学习-构建学生干预系统/6-回归/12-斜率\n3-监督学习-构建学生干预系统/6-回归/12-斜率 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/12-斜率.mp4\n3-监督学习-构建学生干预系统/6-回归/13-截距\n3-监督学习-构建学生干预系统/6-回归/13-截距 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/13-截距.mp4\n3-监督学习-构建学生干预系统/6-回归/14-使用回归的预测\n3-监督学习-构建学生干预系统/6-回归/14-使用回归的预测 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/14-使用回归的预测.mp4\n3-监督学习-构建学生干预系统/6-回归/15-添加截距\n3-监督学习-构建学生干预系统/6-回归/15-添加截距 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/15-添加截距.mp4\n3-监督学习-构建学生干预系统/6-回归/16-移交给 Katie.mp4\n3-监督学习-构建学生干预系统/6-回归/17-编码.mp4\n3-监督学习-构建学生干预系统/6-回归/18-sklearn 中的年龄净值回归.mp4\n3-监督学习-构建学生干预系统/6-回归/18-sklern中的年两&净值回归 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/19-通过 sklearn 提取信息.mp4\n3-监督学习-构建学生干预系统/6-回归/2-连续\n3-监督学习-构建学生干预系统/6-回归/2-连续 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/2-连续.mp4\n3-监督学习-构建学生干预系统/6-回归/20-通过 sklearn 提取分数数据.mp4\n3-监督学习-构建学生干预系统/6-回归/22-线性回归误差.mp4\n3-监督学习-构建学生干预系统/6-回归/23-误差\n3-监督学习-构建学生干预系统/6-回归/23-误差 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/23-误差.mp4\n3-监督学习-构建学生干预系统/6-回归/24-24-误差和拟合质量 .mp4.mp4\n3-监督学习-构建学生干预系统/6-回归/24-误差和拟合质量\n3-监督学习-构建学生干预系统/6-回归/24-误差和拟合质量 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/25-最小化误差平方和.mp4\n3-监督学习-构建学生干预系统/6-回归/26-最最小化误差平方和的算法.mp4\n3-监督学习-构建学生干预系统/6-回归/27-为何最小化 SSE\n3-监督学习-构建学生干预系统/6-回归/27-为何最小化 SSE 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/27-为何最小化 SSE.mp4\n3-监督学习-构建学生干预系统/6-回归/28-最小化绝对误差的问题.mp4\n3-监督学习-构建学生干预系统/6-回归/29-肉眼评估回归\n3-监督学习-构建学生干预系统/6-回归/29-肉眼评估回归 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/29-肉眼评估回归.mp4\n3-监督学习-构建学生干预系统/6-回归/3-年龄：连续还是离散？\n3-监督学习-构建学生干预系统/6-回归/3-年龄：连续还是离散？ 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/3-年龄：连续还是离散？.mp4\n3-监督学习-构建学生干预系统/6-回归/30-SSE 的问题\n3-监督学习-构建学生干预系统/6-回归/30-SSE 的问题 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/30-SSE 的问题.mp4\n3-监督学习-构建学生干预系统/6-回归/31-回归的 R 平方指标.mp4\n3-监督学习-构建学生干预系统/6-回归/32-SKlearn 中的 R 平方.mp4\n3-监督学习-构建学生干预系统/6-回归/33-可视化回归.mp4\n3-监督学习-构建学生干预系统/6-回归/34-34-什么数据适用于线性回归.mp4\n3-监督学习-构建学生干预系统/6-回归/34-什么数据适用于线性回归\n3-监督学习-构建学生干预系统/6-回归/34-什么数据适用于线性回归 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/35-比较分类与回归.mp4\n3-监督学习-构建学生干预系统/6-回归/36-多元回归\n3-监督学习-构建学生干预系统/6-回归/36-多元回归 2\n3-监督学习-构建学生干预系统/6-回归/36-多元回归 2 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/36-多元回归 2.mp4\n3-监督学习-构建学生干预系统/6-回归/36-多元回归 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/36-多元回归.mp4\n3-监督学习-构建学生干预系统/6-回归/4-天气：连续还是离散？\n3-监督学习-构建学生干预系统/6-回归/4-天气：连续还是离散？ 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/4-天气：连续还是离散？.mp4\n3-监督学习-构建学生干预系统/6-回归/5-电子邮件作者：连续还是离散？\n3-监督学习-构建学生干预系统/6-回归/5-电子邮件作者：连续还是离散？ 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/5-电子邮件作者：连续还是离散？.mp4\n3-监督学习-构建学生干预系统/6-回归/6-电话号码：连续还是离散？\n3-监督学习-构建学生干预系统/6-回归/6-电话号码：连续还是离散？ 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/6-电话号码：连续还是离散？.mp4\n3-监督学习-构建学生干预系统/6-回归/7-收入：连续还是离散？\n3-监督学习-构建学生干预系统/6-回归/7-收入：连续还是离散？ 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/7-收入：连续还是离散？.mp4\n3-监督学习-构建学生干预系统/6-回归/8-连续特征\n3-监督学习-构建学生干预系统/6-回归/8-连续特征 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/8-连续特征.mp4\n3-监督学习-构建学生干预系统/6-回归/9-具有连续输出的监督学习\n3-监督学习-构建学生干预系统/6-回归/9-具有连续输出的监督学习 答案.mp4\n3-监督学习-构建学生干预系统/6-回归/9-具有连续输出的监督学习.mp4\n3-监督学习-构建学生干预系统/6-回归/18-code/studentMain.py\n3-监督学习-构建学生干预系统/6-回归/18-code/studentRegression.py\n3-监督学习-构建学生干预系统/6-回归/21-练习提取信息/ages_net_worths.py\n3-监督学习-构建学生干预系统/6-回归/21-练习提取信息/regressionQuiz.py\n3-监督学习-构建学生干预系统/7-更多回归/1-简介.mp4\n3-监督学习-构建学生干预系统/7-更多回归/2-参数回归.mp4\n3-监督学习-构建学生干预系统/7-更多回归/3-K 最近邻.mp4\n3-监督学习-构建学生干预系统/7-更多回归/4-如何预测\n3-监督学习-构建学生干预系统/7-更多回归/4-如何预测 答案.mp4\n3-监督学习-构建学生干预系统/7-更多回归/4-如何预测.mp4\n3-监督学习-构建学生干预系统/7-更多回归/5-内核回归.mp4\n3-监督学习-构建学生干预系统/7-更多回归/6-参数与非参数\n3-监督学习-构建学生干预系统/7-更多回归/6-参数与非参数 答案.mp4\n3-监督学习-构建学生干预系统/7-更多回归/6-参数与非参数.mp4\n3-监督学习-构建学生干预系统/8-神经网络/1-神经网络.mp4\n3-监督学习-构建学生干预系统/8-神经网络/10-Sigmoid.mp4\n3-监督学习-构建学生干预系统/8-神经网络/11-神经网络草图.mp4\n3-监督学习-构建学生干预系统/8-神经网络/12-优化权重.mp4\n3-监督学习-构建学生干预系统/8-神经网络/13-限制偏差.mp4\n3-监督学习-构建学生干预系统/8-神经网络/14-喜好偏差.mp4\n3-监督学习-构建学生干预系统/8-神经网络/15-小结.mp4\n3-监督学习-构建学生干预系统/8-神经网络/2-人工神经网络\n3-监督学习-构建学生干预系统/8-神经网络/2-人工神经网络 答案.mp4\n3-监督学习-构建学生干预系统/8-神经网络/2-人工神经网络.mp4\n3-监督学习-构建学生干预系统/8-神经网络/3-感知器单元有多强大.mp4\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 NOT 测验\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 NOT 测验 答案.mp4\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 NOT 测验.mp4\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 OR 测验\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 OR 测验 答案.mp4\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 OR 测验.mp4\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 测验\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 测验 答案.mp4\n3-监督学习-构建学生干预系统/8-神经网络/4-感知器单元有多强大 测验.mp4\n3-监督学习-构建学生干预系统/8-神经网络/5-作为感知器网络的 XOR\n3-监督学习-构建学生干预系统/8-神经网络/5-作为感知器网络的 XOR 答案.mp4\n3-监督学习-构建学生干预系统/8-神经网络/5-作为感知器网络的 XOR.mp4\n3-监督学习-构建学生干预系统/8-神经网络/6-感知器训练.mp4\n3-监督学习-构建学生干预系统/8-神经网络/7-梯度下降.mp4\n3-监督学习-构建学生干预系统/8-神经网络/8-学习规则比较.mp4\n3-监督学习-构建学生干预系统/8-神经网络/9-学习规则比较 测验\n3-监督学习-构建学生干预系统/8-神经网络/9-学习规则比较 测验 答案.mp4\n3-监督学习-构建学生干预系统/8-神经网络/9-学习规则比较 测验.mp4\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/1-创建感知.py\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/10-创建 XOR 网络.py\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/11-离散测验\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/11-练习.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/12-连续性.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/13-激活函数沙盒.py\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/14-激活函数 测验\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/14-激活函数沙盒.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/15-感知 v.s. Sigmoid\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/15-感知 v.s.Sigmoid.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/16-Sigmoid 学习\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/16-Sigmoid.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/17-梯度下降问题\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/17-梯度下降问题.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/18-Sigmoid练习.py\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/2-练习.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/2-阀值调解\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/3-在哪儿训练感知\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/3-在哪儿训练感知.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/4-感知 v.s. 回归\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/4-感知vs. 回归.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/5-感知输入\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/5-感知输入.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/6-神经网络输出\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/6-神经网络输出.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/7-感知更新规则.py\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/8-多层网络示例\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/8-多层网络示例.txt\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/9-线性表征能力\n3-监督学习-构建学生干预系统/9-神经网络迷你项目/9-线性表征能力.txt\n4-非监督学习-创建客户细分/1-非监督学习简介/1-非监督学习.mp4\n4-非监督学习-创建客户细分/1-非监督学习简介/2-你将看到和学到什么.mp4\n4-非监督学习-创建客户细分/10-结尾/1-结尾 - 第 1 部分.mp4\n4-非监督学习-创建客户细分/11-项目/项目-.docx\n4-非监督学习-创建客户细分/2-聚类/1-非监督学习.mp4\n4-非监督学习-创建客户细分/2-聚类/10-K-均值聚类可视化 2.mp4\n4-非监督学习-创建客户细分/2-聚类/11-K-均值聚类可视化 3.mp4\n4-非监督学习-创建客户细分/2-聚类/12-Sklearn.mp4\n4-非监督学习-创建客户细分/2-聚类/13-K-均值的局限.mp4\n4-非监督学习-创建客户细分/2-聚类/13-K-均值的挑战.mp4\n4-非监督学习-创建客户细分/2-聚类/14-反直觉的聚类\n4-非监督学习-创建客户细分/2-聚类/14-反直觉的聚类 答案.mp4\n4-非监督学习-创建客户细分/2-聚类/14-反直觉的聚类.mp4\n4-非监督学习-创建客户细分/2-聚类/15-反直觉的聚类 2\n4-非监督学习-创建客户细分/2-聚类/15-反直觉的聚类 2 答案.mp4\n4-非监督学习-创建客户细分/2-聚类/15-反直觉的聚类 2.mp4\n4-非监督学习-创建客户细分/2-聚类/2-聚类电影.mp4\n4-非监督学习-创建客户细分/2-聚类/3-多少个聚类？\n4-非监督学习-创建客户细分/2-聚类/3-多少个聚类？.mp4\n4-非监督学习-创建客户细分/2-聚类/3-多少个聚类？答案.mp4\n4-非监督学习-创建客户细分/2-聚类/4-将点与聚类匹配\n4-非监督学习-创建客户细分/2-聚类/4-将点与聚类匹配 答案.mp4\n4-非监督学习-创建客户细分/2-聚类/4-将点与聚类匹配.mp4\n4-非监督学习-创建客户细分/2-聚类/5-优化中心（橡皮筋）\n4-非监督学习-创建客户细分/2-聚类/5-优化中心（橡皮筋） 答案.mp4\n4-非监督学习-创建客户细分/2-聚类/5-优化中心（橡皮筋）.mp4\n4-非监督学习-创建客户细分/2-聚类/6-移动中心 2\n4-非监督学习-创建客户细分/2-聚类/6-移动中心 2 答案.mp4\n4-非监督学习-创建客户细分/2-聚类/6-移动中心 2.mp4\n4-非监督学习-创建客户细分/2-聚类/7-匹配点（再次）\n4-非监督学习-创建客户细分/2-聚类/7-匹配点（再次） 答案.mp4\n4-非监督学习-创建客户细分/2-聚类/7-匹配点（再次）.mp4\n4-非监督学习-创建客户细分/2-聚类/8-移交给 Katie.mp4\n4-非监督学习-创建客户细分/2-聚类/9-K-均值聚类可视化.mp4\n4-非监督学习-创建客户细分/2-聚类/9-k均值可视化 答案.mp4\n4-非监督学习-创建客户细分/2-聚类/9-练习 K-均值聚类可视化\n4-非监督学习-创建客户细分/2-聚类/9-练习 K-均值聚类可视化 2.txt\n4-非监督学习-创建客户细分/2-聚类/9-练习 K-均值聚类可视化.txt\n4-非监督学习-创建客户细分/3-更多聚类/1-单连锁聚类\n4-非监督学习-创建客户细分/3-更多聚类/1-单连锁聚类 答案.mp4\n4-非监督学习-创建客户细分/3-更多聚类/1-单连锁聚类.mp4\n4-非监督学习-创建客户细分/3-更多聚类/10-EM 的属性.mp4\n4-非监督学习-创建客户细分/3-更多聚类/11-聚类属性.mp4\n4-非监督学习-创建客户细分/3-更多聚类/12-聚类属性 测验\n4-非监督学习-创建客户细分/3-更多聚类/12-聚类属性 测验 答案.mp4\n4-非监督学习-创建客户细分/3-更多聚类/12-聚类属性 测验.mp4\n4-非监督学习-创建客户细分/3-更多聚类/13-不可能定理.mp4\n4-非监督学习-创建客户细分/3-更多聚类/14-我们学到了什么？.mp4\n4-非监督学习-创建客户细分/3-更多聚类/2-单连锁聚类 2.mp4\n4-非监督学习-创建客户细分/3-更多聚类/3-SLC 的运行时间\n4-非监督学习-创建客户细分/3-更多聚类/3-SLC 的运行时间 答案.mp4\n4-非监督学习-创建客户细分/3-更多聚类/3-SLC 的运行时间.mp4\n4-非监督学习-创建客户细分/3-更多聚类/4-SLC 的问题\n4-非监督学习-创建客户细分/3-更多聚类/4-SLC 的问题 答案.mp4\n4-非监督学习-创建客户细分/3-更多聚类/4-SLC 的问题.mp4\n4-非监督学习-创建客户细分/3-更多聚类/5-练习.txt\n4-非监督学习-创建客户细分/3-更多聚类/5-软聚类 测验\n4-非监督学习-创建客户细分/3-更多聚类/5-软聚类 测验 答案.mp4\n4-非监督学习-创建客户细分/3-更多聚类/5-软聚类 测验.mp4\n4-非监督学习-创建客户细分/3-更多聚类/6-软聚类.mp4\n4-非监督学习-创建客户细分/3-更多聚类/7-高斯最大似然.mp4\n4-非监督学习-创建客户细分/3-更多聚类/8-期望最大化.mp4\n4-非监督学习-创建客户细分/3-更多聚类/8-错误.txt\n4-非监督学习-创建客户细分/3-更多聚类/9-EM 示例.mp4\n4-非监督学习-创建客户细分/4-聚类迷你项目/1-聚类迷你项目视频.mp4\n4-非监督学习-创建客户细分/4-聚类迷你项目/2-K-均值聚类迷你项目.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/3-聚类特征\n4-非监督学习-创建客户细分/4-聚类迷你项目/3-聚类特征.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/4-部署聚类\n4-非监督学习-创建客户细分/4-聚类迷你项目/4-部署聚类 - 练习.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/4-部署聚类.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/5-使用 3 个特征聚类\n4-非监督学习-创建客户细分/4-聚类迷你项目/5-使用3个特征聚类.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/6-股票期权范围.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/7-薪酬范围.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/8-聚类更改\n4-非监督学习-创建客户细分/4-聚类迷你项目/8-聚类更改.txt\n4-非监督学习-创建客户细分/4-聚类迷你项目/位置.txt\n4-非监督学习-创建客户细分/5-特征缩放/1-Chris 的 T 恤尺寸（直觉）\n4-非监督学习-创建客户细分/5-特征缩放/1-Chris 的 T 恤尺寸（直觉） 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/1-Chris 的 T 恤尺寸（直觉）.mp4\n4-非监督学习-创建客户细分/5-特征缩放/10-需要重缩放的算法练习\n4-非监督学习-创建客户细分/5-特征缩放/10-需要重缩放的算法练习 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/10-需要重缩放的算法练习.mp4\n4-非监督学习-创建客户细分/5-特征缩放/2-针对 Chris 的度量\n4-非监督学习-创建客户细分/5-特征缩放/2-针对 Chris 的度量.mp4\n4-非监督学习-创建客户细分/5-特征缩放/3-针对 Chris 的度量 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/4-Sarah 的身高 + 体重\n4-非监督学习-创建客户细分/5-特征缩放/4-Sarah 的身高 + 体重 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/4-Sarah 的身高 + 体重.mp4\n4-非监督学习-创建客户细分/5-特征缩放/5-由我们的度量确定的 Chris T恤尺寸\n4-非监督学习-创建客户细分/5-特征缩放/5-由我们的度量确定的 Chris T恤尺寸 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/5-由我们的度量确定的 Chris T恤尺寸.mp4\n4-非监督学习-创建客户细分/5-特征缩放/6-利用不同的尺度来比较特征.mp4\n4-非监督学习-创建客户细分/5-特征缩放/7-3错误.txt\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 1\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 1 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 1.mp4\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 2\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 2 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 2.mp4\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 3\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 3 答案.mp4\n4-非监督学习-创建客户细分/5-特征缩放/7-特征缩放公式练习 3.mp4\n4-非监督学习-创建客户细分/5-特征缩放/8-最小值&最大值重缩放器编码练习.mp4\n4-非监督学习-创建客户细分/5-特征缩放/8-练习 最小值最大值重缩放器编码练习.txt\n4-非监督学习-创建客户细分/5-特征缩放/8-练习.py\n4-非监督学习-创建客户细分/5-特征缩放/9-sklearn 中的最小值、最大值缩放器.mp4\n4-非监督学习-创建客户细分/6-特征选择/1-简介.mp4\n4-非监督学习-创建客户细分/6-特征选择/10-我们学到了什么.mp4\n4-非监督学习-创建客户细分/6-特征选择/2-特征选择.mp4\n4-非监督学习-创建客户细分/6-特征选择/3-算法\n4-非监督学习-创建客户细分/6-特征选择/3-算法 答案.mp4\n4-非监督学习-创建客户细分/6-特征选择/3-算法.mp4\n4-非监督学习-创建客户细分/6-特征选择/4-过滤和封装.mp4\n4-非监督学习-创建客户细分/6-特征选择/5-速度.mp4\n4-非监督学习-创建客户细分/6-特征选择/6-速度.mp4\n4-非监督学习-创建客户细分/6-特征选择/7-搜索.mp4\n4-非监督学习-创建客户细分/6-特征选择/8-所需的最小特征\n4-非监督学习-创建客户细分/6-特征选择/8-所需的最小特征.mp4\n4-非监督学习-创建客户细分/6-特征选择/9-相关性.mp4\n4-非监督学习-创建客户细分/6-特征选择/9-相关性与有用性.mp4\n4-非监督学习-创建客户细分/7-PCA/1-数据维度.mp4\n4-非监督学习-创建客户细分/7-PCA/10-练习查找中心\n4-非监督学习-创建客户细分/7-PCA/10-练习查找中心 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/10-练习查找中心.mp4\n4-非监督学习-创建客户细分/7-PCA/11-练习查找新轴\n4-非监督学习-创建客户细分/7-PCA/11-练习查找新轴 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/11-练习查找新轴.mp4\n4-非监督学习-创建客户细分/7-PCA/12-哪些数据可用于 PCA\n4-非监督学习-创建客户细分/7-PCA/12-哪些数据可用于 PCA 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/12-哪些数据可用于 PCA.mp4\n4-非监督学习-创建客户细分/7-PCA/13-轴何时占主导地位\n4-非监督学习-创建客户细分/7-PCA/13-轴何时占主导地位 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/13-轴何时占主导地位.mp4\n4-非监督学习-创建客户细分/7-PCA/14-可测量的特征与潜在的特征练习\n4-非监督学习-创建客户细分/7-PCA/14-可测量的特征与潜在的特征练习 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/14-可测量的特征与潜在的特征练习.mp4\n4-非监督学习-创建客户细分/7-PCA/15-从四个特征到两个\n4-非监督学习-创建客户细分/7-PCA/15-从四个特征到两个 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/15-从四个特征到两个.mp4\n4-非监督学习-创建客户细分/7-PCA/16-在保留信息的同时压缩\n4-非监督学习-创建客户细分/7-PCA/16-在保留信息的同时压缩 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/16-在保留信息的同时压缩.mp4\n4-非监督学习-创建客户细分/7-PCA/17-复合特征\n4-非监督学习-创建客户细分/7-PCA/17-复合特征 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/17-复合特征.mp4\n4-非监督学习-创建客户细分/7-PCA/18-最大方差\n4-非监督学习-创建客户细分/7-PCA/18-最大方差 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/18-最大方差.mp4\n4-非监督学习-创建客户细分/7-PCA/19-最大方差的优点\n4-非监督学习-创建客户细分/7-PCA/19-最大方差的优点 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/19-最大方差的优点.mp4\n4-非监督学习-创建客户细分/7-PCA/2-较棘手的数据维度\n4-非监督学习-创建客户细分/7-PCA/2-较棘手的数据维度 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/2-较棘手的数据维度.mp4\n4-非监督学习-创建客户细分/7-PCA/20-最大方差与信息损失\n4-非监督学习-创建客户细分/7-PCA/20-最大方差与信息损失 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/20-最大方差与信息损失.mp4\n4-非监督学习-创建客户细分/7-PCA/21-信息损失和主成分.mp4\n4-非监督学习-创建客户细分/7-PCA/22-相邻复合特征\n4-非监督学习-创建客户细分/7-PCA/22-相邻复合特征 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/22-相邻复合特征.mp4\n4-非监督学习-创建客户细分/7-PCA/23-用于特征转换的 PCA.mp4\n4-非监督学习-创建客户细分/7-PCA/24-最大主成分数量\n4-非监督学习-创建客户细分/7-PCA/24-最大主成分数量 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/24-最大主成分数量.mp4\n4-非监督学习-创建客户细分/7-PCA/25-PCA 的回顾、定义.mp4\n4-非监督学习-创建客户细分/7-PCA/26-安然财务数据的 PCA\n4-非监督学习-创建客户细分/7-PCA/26-安然财务数据的 PCA 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/26-安然财务数据的 PCA.mp4\n4-非监督学习-创建客户细分/7-PCA/26-将PCA应用到实际数据.txt\n4-非监督学习-创建客户细分/7-PCA/27-sklearn 中的 PCA.mp4\n4-非监督学习-创建客户细分/7-PCA/27-sklearn中的PCA.docx\n4-非监督学习-创建客户细分/7-PCA/28-何时使用 PCA.mp4\n4-非监督学习-创建客户细分/7-PCA/29-用于人脸识别的 PCA\n4-非监督学习-创建客户细分/7-PCA/29-用于人脸识别的 PCA 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/29-用于人脸识别的 PCA.mp4\n4-非监督学习-创建客户细分/7-PCA/29-用于人脸识别的PCA.txt\n4-非监督学习-创建客户细分/7-PCA/3-一维或二维？\n4-非监督学习-创建客户细分/7-PCA/3-一维或二维？ 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/3-一维或二维？.mp4\n4-非监督学习-创建客户细分/7-PCA/30-特征脸方法代码.mp4\n4-非监督学习-创建客户细分/7-PCA/4-略微不完美的数据\n4-非监督学习-创建客户细分/7-PCA/4-略微不完美的数据 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/4-略微不完美的数据.mp4\n4-非监督学习-创建客户细分/7-PCA/5-最棘手的数据维度\n4-非监督学习-创建客户细分/7-PCA/5-最棘手的数据维度 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/5-最棘手的数据维度.mp4\n4-非监督学习-创建客户细分/7-PCA/6-用于数据转换的 PCA.mp4\n4-非监督学习-创建客户细分/7-PCA/7-新坐标系的中心\n4-非监督学习-创建客户细分/7-PCA/7-新坐标系的中心 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/7-新坐标系的中心.mp4\n4-非监督学习-创建客户细分/7-PCA/8-新坐标系的主轴\n4-非监督学习-创建客户细分/7-PCA/8-新坐标系的主轴 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/8-新坐标系的主轴.mp4\n4-非监督学习-创建客户细分/7-PCA/9-新系统的第二主成分\n4-非监督学习-创建客户细分/7-PCA/9-新系统的第二主成分 答案.mp4\n4-非监督学习-创建客户细分/7-PCA/9-新系统的第二主成分.mp4\n4-非监督学习-创建客户细分/8-PCA迷你项目/1-PCA 迷你项目简介.mp4\n4-非监督学习-创建客户细分/8-PCA迷你项目/2-PCA 迷你项目.txt\n4-非监督学习-创建客户细分/8-PCA迷你项目/3-每个主成分的可释方差\n4-非监督学习-创建客户细分/8-PCA迷你项目/3-每个主成分的可释方差.txt\n4-非监督学习-创建客户细分/8-PCA迷你项目/4-要使用多少个主成分\n4-非监督学习-创建客户细分/8-PCA迷你项目/4-要使用多少个主成分？.txt\n4-非监督学习-创建客户细分/8-PCA迷你项目/5-F1 分数与使用的主成分数\n4-非监督学习-创建客户细分/8-PCA迷你项目/5-F1 分数与使用的主成分数.txt\n4-非监督学习-创建客户细分/8-PCA迷你项目/6-维度降低与过拟合\n4-非监督学习-创建客户细分/8-PCA迷你项目/6-维度降低与过拟合.txt\n4-非监督学习-创建客户细分/8-PCA迷你项目/7-选择主成分.mp4\n4-非监督学习-创建客户细分/9-特征转换/1-特征转换简介.mp4\n4-非监督学习-创建客户细分/9-特征转换/10-PCA 与 ICA（续）.mp4\n4-非监督学习-创建客户细分/9-特征转换/11-替代选择.mp4\n4-非监督学习-创建客户细分/9-特征转换/12-替代选择 2.mp4\n4-非监督学习-创建客户细分/9-特征转换/12-替代选择 测验\n4-非监督学习-创建客户细分/9-特征转换/12-替代选择 测验 答案.mp4\n4-非监督学习-创建客户细分/9-特征转换/12-替代选择 测验.mp4\n4-非监督学习-创建客户细分/9-特征转换/13-小结.mp4\n4-非监督学习-创建客户细分/9-特征转换/2-特征转换.mp4\n4-非监督学习-创建客户细分/9-特征转换/3-我们的特征是什么.mp4\n4-非监督学习-创建客户细分/9-特征转换/4-Tesla 等单词.mp4\n4-非监督学习-创建客户细分/9-特征转换/5-独立成分分析.mp4\n4-非监督学习-创建客户细分/9-特征转换/6-独立成分分析 2.mp4\n4-非监督学习-创建客户细分/9-特征转换/7-鸡尾酒会问题.mp4\n4-非监督学习-创建客户细分/9-特征转换/8-矩阵.mp4\n4-非监督学习-创建客户细分/9-特征转换/9-PCA 与 ICA\n4-非监督学习-创建客户细分/9-特征转换/9-PCA 与 ICA 答案.mp4\n4-非监督学习-创建客户细分/9-特征转换/9-PCA 与 ICA.mp4\n5-强化学习-训练智能出租车学会驾驶/1-强化学习简介/1-强化学习.mp4\n5-强化学习-训练智能出租车学会驾驶/1-强化学习简介/2-您将看到和学到什么.mp4\n5-强化学习-训练智能出租车学会驾驶/1-强化学习简介/3-强化学习 - 您将做什么.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/1-简介.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/10-小结.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/2-决策与强化学习.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/3-世界 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/3-世界 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/4-Markov 决策过程 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/4-Markov 决策过程 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/4-Markov 决策过程 - 3.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/4-Markov 决策过程 - 4.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/5-奖励详情 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/5-奖励详情 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/5-奖励详情 - 3 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/5-奖励详情 - 3.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/6-奖励的序列 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/6-奖励的序列 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/6-奖励的序列 - 3\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/6-奖励的序列 - 3 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/6-奖励的序列 - 3.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/6-奖励的序列 - 4.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/7-假设.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/8-策略 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/8-策略 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/9-查找策略 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/9-查找策略 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/9-查找策略 - 3\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/9-查找策略 - 3 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/9-查找策略 - 3.mp4\n5-强化学习-训练智能出租车学会驾驶/2-MARKOV决策过程/9-查找策略 - 4.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/1-强化学习.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/10-在转换后估算 Q（二） 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/11-Q 学习收敛性.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/11-中央博弈.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/11-告密者 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/12-选择动作 (二).mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/12-选择动作.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/13-贪婪的探索.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/14-我们学到了什么.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/2-Rat Dinosaur.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/3-API.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/4-API 测试题\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/4-API 测试题 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/4-API测试题.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/5-强化学习的三种方法.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/6-一种新的价值函数.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/6-价值函数测试题\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/6-价值函数测试题 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/6-价值函数测试题.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/7-Q 学习\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/7-Q 学习 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/7-Q 学习.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/8-在转换后估算 Q.mp4\n5-强化学习-训练智能出租车学会驾驶/3-强化学习/9-递增地学习.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/1-博弈论.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/10-混合策略\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/10-混合策略 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/10-混合策略.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/11-行\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/11-行 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/11-行.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/12-告密者 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/12-告密者 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/12-告密者 - 3.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/13-美妙的均衡 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/13-美妙的均衡 - 2\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/13-美妙的均衡 - 2 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/13-美妙的均衡 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/13-美妙的均衡 - 3.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/14-两步.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/15-2Step2Furious.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/16-我们学到了什么？.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/2-博弈论是什么.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/3-简单博弈 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/3-简单博弈 - 2\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/3-简单博弈 - 2 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/3-简单博弈 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/3-简单博弈 - 3\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/3-简单博弈 - 3 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/3-简单博弈 - 3.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/4-极小极大.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/5-基本结果.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/6-博弈树 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/6-博弈树 - 2\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/6-博弈树 - 2 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/7-Von Neumann.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/8-迷你扑克.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/9-迷你扑克树\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/9-迷你扑克树 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/4-博弈论/9-迷你扑克树.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/1-随机博弈与多主体 RL.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/2-随机博弈.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/3-模型与随机博弈\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/3-模型与随机博弈 答案.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/3-模型与随机博弈.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/4-零和随机博弈 - 1.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/4-零和随机博弈 - 2.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/5-一般和博弈.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/6-各种想法.mp4\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/7-项目准备.docx\n5-强化学习-训练智能出租车学会驾驶/5-更多信息 博弈论/材料- Train a Smartcab to Drive Videos.zip\n6-kaggle/laggle挑战/1-展示你积极性的示例.mp4\n6-kaggle/laggle挑战/1-简介.mp4\n6-kaggle/laggle挑战/10-职业发展.mp4\n6-kaggle/laggle挑战/11-教训.mp4\n6-kaggle/laggle挑战/12-教训.mp4\n6-kaggle/laggle挑战/2-在工作场所，你以什么作为自己的动力？ Search 课程资源.mp4\n6-kaggle/laggle挑战/2-挑战.mp4\n6-kaggle/laggle挑战/3-与业界的关系.mp4\n6-kaggle/laggle挑战/4-成为 Kaggler.mp4\n6-kaggle/laggle挑战/5-社区的力量.mp4\n6-kaggle/laggle挑战/6-开始起步.mp4\n6-kaggle/laggle挑战/7-技术和语言.mp4\n6-kaggle/laggle挑战/8-贴士.mp4\n6-kaggle/laggle挑战/9-技巧.mp4\n6-kaggle/laggle挑战/kaggle.docx\n7-机器学习工程师模拟面试/你了解公司的哪些信息？.mp4\n7-机器学习工程师模拟面试/在工作场所，你以什么作为自己的动力.mp4\n7-机器学习工程师模拟面试/如何应对失败？.mp4\n7-机器学习工程师模拟面试/工作中的问题以及你是如何解决的.mp4\n7-机器学习工程师模拟面试/机器学习面试排练.docx\n7-机器学习工程师模拟面试/获得面试.docx\n7-机器学习工程师模拟面试/面试中的期望.docx\n获取视频教程"}
{"content2":"在做分类时常常需要估算不同样本之间的相似性度量(Similarity Measurement)，这时通常采用的方法就是计算样本间的“距离”(Distance)。采用什么样的方法计算距离是很讲究，甚至关系到分类的正确与否。\n本文的目的就是对常用的相似性度量作一个总结。\n本文目录：\n1. 欧氏距离\n2. 曼哈顿距离\n3. 切比雪夫距离\n4. 闵可夫斯基距离\n5. 标准化欧氏距离\n6. 马氏距离\n7. 夹角余弦\n8. 汉明距离\n9. 杰卡德距离 & 杰卡德相似系数\n10. 相关系数 & 相关距离\n11. 信息熵\n1. 欧氏距离(Euclidean Distance)\n欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。\n(1)二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：\n(2)三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：\n(3)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：\n也可以用表示成向量运算的形式：\n(4)Matlab计算欧氏距离\nMatlab计算距离主要使用pdist函数。若X是一个M×N的矩阵，则pdist(X)将X矩阵M行的每一行作为一个N维向量，然后计算这M个向量两两间的距离。\n例子：计算向量(0,0)、(1,0)、(0,2)两两间的欧式距离\nX = [0 0 ; 1 0 ; 0 2]\nD = pdist(X,'euclidean')\n结果：\nD =\n1.0000    2.0000    2.2361\n2. 曼哈顿距离(Manhattan Distance)\n从名字就可以猜出这种距离的计算方法了。想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源， 曼哈顿距离也称为城市街区距离(City Block distance)。\n(1)二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离\n(2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离\n(3) Matlab计算曼哈顿距离\n例子：计算向量(0,0)、(1,0)、(0,2)两两间的曼哈顿距离\nX = [0 0 ; 1 0 ; 0 2]\nD = pdist(X, 'cityblock')\n结果：\nD =\n1     2     3\n3. 切比雪夫距离 ( Chebyshev Distance )\n国际象棋玩过么？国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？自己走走试试。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。\n(1)二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离\n(2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离\n这个公式的另一种等价形式是\n看不出两个公式是等价的？提示一下：试试用放缩法和夹逼法则来证明。\n(3)Matlab计算切比雪夫距离\n例子：计算向量(0,0)、(1,0)、(0,2)两两间的切比雪夫距离\nX = [0 0 ; 1 0 ; 0 2]\nD = pdist(X, 'chebychev')\n结果：\nD =\n1     2     2\n4. 闵可夫斯基距离(Minkowski Distance)\n闵氏距离不是一种距离，而是一组距离的定义。\n(1) 闵氏距离的定义\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n其中p是一个变参数。\n当p=1时，就是曼哈顿距离\n当p=2时，就是欧氏距离\n当p→∞时，就是切比雪夫距离\n根据变参数的不同，闵氏距离可以表示一类的距离。\n(2)闵氏距离的缺点\n闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。\n举个例子：二维样本(身高,体重)，其中身高范围是150~190，体重范围是50~60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。\n简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。\n(3)Matlab计算闵氏距离\n例子：计算向量(0,0)、(1,0)、(0,2)两两间的闵氏距离（以变参数为2的欧氏距离为例）\nX = [0 0 ; 1 0 ; 0 2]\nD = pdist(X,'minkowski',2)\n结果：\nD =\n1.0000    2.0000    2.2361\n5. 标准化欧氏距离 (Standardized Euclidean distance )\n(1)标准欧氏距离的定义\n标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：\n而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：\n标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差\n经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：\n如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。\n(2)Matlab计算标准化欧氏距离\n例子：计算向量(0,0)、(1,0)、(0,2)两两间的标准化欧氏距离 (假设两个分量的标准差分别为0.5和1)\nX = [0 0 ; 1 0 ; 0 2]\nD = pdist(X, 'seuclidean',[0.5,1])\n结果：\nD =\n2.0000    2.0000    2.8284\n6. 马氏距离(Mahalanobis Distance)\n（1）马氏距离定义\n有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：\n而其中向量Xi与Xj之间的马氏距离定义为：\n若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：\n也就是欧氏距离了。\n若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。\n(2)马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。\n(3) Matlab计算(1 2)，( 1 3)，( 2 2)，( 3 1)两两之间的马氏距离\nX = [1 2; 1 3; 2 2; 3 1]\nY = pdist(X,'mahalanobis')\n结果：\nY =\n2.3452    2.0000    2.3452    1.2247    2.4495    1.2247\n7. 夹角余弦(Cosine)\n有没有搞错，又不是学几何，怎么扯到夹角余弦了？各位看官稍安勿躁。几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。\n(1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：\n(2) 两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦\n类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。\n即：\n夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。\n夹角余弦的具体应用可以参阅参考文献[1]。\n(3)Matlab计算夹角余弦\n例子：计算(1,0)、( 1,1.732)、( -1,0)两两间的夹角余弦\nX = [1 0 ; 1 1.732 ; -1 0]\nD = 1- pdist(X, 'cosine')  % Matlab中的pdist(X, 'cosine')得到的是1减夹角余弦的值\n结果：\nD =\n0.5000   -1.0000   -0.5000\n8. 汉明距离(Hamming distance)\n(1)汉明距离的定义\n两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。\n应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。\n(2)Matlab计算汉明距离\nMatlab中2个向量之间的汉明距离的定义为2个向量不同的分量所占的百分比。\n例子：计算向量(0,0)、(1,0)、(0,2)两两间的汉明距离\nX = [0 0 ; 1 0 ; 0 2];\nD = PDIST(X, 'hamming')\n结果：\nD =\n0.5000    0.5000    1.0000\n9. 杰卡德相似系数(Jaccard similarity coefficient)\n(1) 杰卡德相似系数\n两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。\n杰卡德相似系数是衡量两个集合的相似度一种指标。\n(2) 杰卡德距离\n与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：\n杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。\n(3) 杰卡德相似系数与杰卡德距离的应用\n可将杰卡德相似系数用在衡量样本的相似度上。\n样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。\np ：样本A与B都是1的维度的个数\nq ：样本A是1，样本B是0的维度的个数\nr ：样本A是0，样本B是1的维度的个数\ns ：样本A与B都是0的维度的个数\n那么样本A与B的杰卡德相似系数可以表示为：\n这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。\n而样本A与B的杰卡德距离表示为：\n(4)Matlab 计算杰卡德距离\nMatlab的pdist函数定义的杰卡德距离跟我这里的定义有一些差别，Matlab中将其定义为不同的维度的个数占“非全零维度”的比例。\n例子：计算(1,1,0)、(1,-1,0)、(-1,1,0)两两之间的杰卡德距离\nX = [1 1 0; 1 -1 0; -1 1 0]\nD = pdist( X , 'jaccard')\n结果\nD =\n0.5000    0.5000    1.0000\n10. 相关系数 ( Correlation coefficient )与相关距离(Correlation distance)\n(1) 相关系数的定义\n相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。\n(2)相关距离的定义\n(3)Matlab计算(1, 2 ,3 ,4 )与( 3 ,8 ,7 ,6 )之间的相关系数与相关距离\nX = [1 2 3 4 ; 3 8 7 6]\nC = corrcoef( X' )   %将返回相关系数矩阵\nD = pdist( X , 'correlation')\n结果：\nC =\n1.0000    0.4781\n0.4781    1.0000\nD =\n0.5219\n其中0.4781就是相关系数，0.5219是相关距离。\n11. 信息熵(Information Entropy)\n信息熵并不属于一种相似性度量。那为什么放在这篇文章中啊？这个。。。我也不知道。 (╯▽╰)\n信息熵是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。\n计算给定的样本集X的信息熵的公式：\n参数的含义：\nn：样本集X的分类数\npi：X中第i类元素出现的概率\n信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0\n参考资料：\n[1]吴军. 数学之美 系列 12 - 余弦定理和新闻的分类.\nhttp://www.google.com.hk/ggblog/googlechinablog/2006/07/12_4010.html\n[2] Wikipedia. Jaccard index.\nhttp://en.wikipedia.org/wiki/Jaccard_index\n[3] Wikipedia. Hamming distance\nhttp://en.wikipedia.org/wiki/Hamming_distance\n[4] 求马氏距离（Mahalanobis distance ）matlab版\nhttp://junjun0595.blog.163.com/blog/static/969561420100633351210/\n[5] Pearson product-moment correlation coefficient\nhttp://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient"}
{"content2":"知道某个算法，和运用一个算法是两码事儿。\n当你训练出数据后，发觉模型有太大误差，怎么办？\n1）获取更多的数据。也许有用吧。\n2）减少特征维度。你可以自己手动选择，也可以利用诸如PCA等数学方法。\n3）获取更多的特征。当然这个方法很耗时，而且不一定有用。\n4）添加多项式特征。你在抓救命稻草么？\n5）构建属于你自己的，新的，更好的特征。有点儿冒险。\n6）调整正则化参数lambuda。\n以上方法的尝试有些碰运气，搞不好就是浪费大把时间。\nmachine learning diagonostic. 机器学习诊断。检查正确性，提升效率，节省调试时间。\n一，评估假设\nloss越小，不代表模型越好，有可能出现过拟合的现象。\n正确的方法是：将数据分割为训练集和测试集。利用训练集训练数据，测试集进行测试求出测试集误差（test set error）\n二，模型选择与训练验证测试集\n如何选择正则化参数和多项式次数（模型选择）\n尝试不同的正则化参数和多项式次数，选择在测试集上损失最小的model。这似乎可行，但都是针对测试集计算，无法验证泛化能力。\n解决的方法就是划分出三个集合：训练集，验证集，和测试集。\n利用验证集选择最佳的参数模型，之后再在测试集上计算泛化损失。\n三，模型诊断：bias vs variance\n过拟合和欠拟合的判断方法\n绘制曲线\n当d过小，有可能是欠拟合\n当d过大，有可能是过拟合\n对于欠拟合而言，验证集和训练集的loss均非常大\n对于过拟合而言，训练集的loss很小，而验证集的loss很大。\n四，正则化参数对欠拟合过拟合的平衡\nlambuda很大的话，容易欠拟合，过小则容易过拟合。\n如何选择？\n设置一个正则化参数的选择范围，在验证集上计算每一个值所对应的loss的大小，选择最小的那个。\n五，学习曲线\nhigh bias:\nJcv和Jtrain在m很大的情况下，都很高。\n此时，增加样本数将没有效果。因为模型本身出了问题。可能的问题是模型过于简单。\nhigh variance:\nJcv和Jtrain之间间隔很大。\n此时，增加训练样本数有可能会有很好的效果。\n六、总结\n1）获取更多样本：解决过拟合。欠拟合则不行。\n2）更小的特征集：同上。\n3）添加其他特征：解决欠拟合\n4）添加多项式：解决欠拟合\n5）减小lambuda: 解决欠拟合\n6）增大Lambuda：解决过拟合"}
{"content2":"数据分析， 就是对数据进行分析， 得出一些结论性的内容， 用于决策。 分析什么哪？ 根据分析现状、 分析原因、 预测未来。 分析现状和分析原因， 需要结合业务才能解释清楚。 用到的技术比较简单， 最简单的数据分析工具就是 Excel。 预测未来指的是分析未来一段时间的销售额之类的。 在预测未来方面， 一般用到数据挖掘的技术了。\n数据挖掘， 从字面意思上看， 是从数据中挖掘出有价值的信息。 比如， 超市通过对一段时间的客户消费记录能发现， 哪些物品经常被顾客一起购买。 那么， 就可以把这些物品摆放的位置近一些， 或者一起促销。 在这里， 客户的消费记录是“数据” ， “挖掘” 出的信息是哪些商品经常被一起购买。 “价值” 指的是超市可以据此搞促销， 提高超市的销售额。 挖掘出这些有价值信息的方法就是课程中需要学习的。 数据挖掘关注的是一些方法如何在商业中应用， 并不是纯粹的理论和学术。\n机器学习， 是研究如何让计算机去学习。 学习什么哪？ 根据一些过去的事实， 学习如何适应新的环境。太小白了， 严肃点！ 机器学习， 是研究算法的学科， 研究的是如何让计算机根据以往的经验去适应新的环境。 这里“以往的经验” 指的是历史数据， “适应” 指的是通过历史数据创造一个很牛逼的函数， “新的环境” 指的是把新的数据输入到这个函数中， 产生一个新的输出。 机器学习本质上是研究自学习算法的科学， 这些算法是帮助软件和机器进行自我学习解决问题的算法。\n神经网络， 是机器学习中的一个算法模型， 指的是模拟人的神经系统。 大家知道， 人的神经非常复杂，所以神经网络算法需要的计算量很大。 神经网络在以前一直不温不火， 原因是计算机硬件不足以支撑神经网络的计算量。 现在大数据技术的发展， 让神经网络迎来了春天。 比如人脸识别、 交通领域的车牌识别技\n术都是神经网络的应用。\n深度学习， 属于神经网络的一个发展分支， 指的是层数很多的神经网络， 可以简单理解为更加高级的神经网络。 把神经网络比作数学学科， 深度学习类似于高等数学。 无人驾驶汽车属于深度学习的典型应用。\n人工智能， 缩写是 AI， 就是像人一样的智能、 会思考。 人工智能更适合理解为一个产业， 泛指产生更加智能的软件和硬件。 人工智能实现的方法就是机器学习， 所以谈人工智能技术， 实际上就是机器学习的各种算法的应用。 各种智能家居、 智能机器人都是人工智能产业的方向。\n综上， 人工智能就是一个产业， 人工智能的实现手段主要靠机器学习的各种算法。 在机器学习的算法中， 深度学习是一个智能化程度非常高的算法。 现在云计算和大数据技术的发展， 让神经网络和深度学习得以在实际中应用。\n大数据时代， 数据是企业的最值钱的财富， 但海量的数据并非都是有价值的， 如何挖掘出有用的数据变成商业价值， 就需要机器学习算法。 大数据和机器学习势必颠覆传统行业的运营方式， 必将驱动公司业务的发展。 目前， 越来越多的机器学习/数据挖掘/深度学习算法被应用在电商、 搜索、 金融、 游戏， 医疗等\n领域中的分析、 挖掘、 推荐上。\n但懂机器学习算法的人才却少之又少， 物以稀为贵， 致使这个行业的工资奇高。"}
{"content2":"提高逼格，给自己的网站加入智能聊天功能\n引言\n现在突然发现有很多 QQ 群都开启了群机器人的功能，其中有两个角色，他们分别是：Baby Q 和 QQ 小冰。在 Q 群中，你可以对他们进行任意程度的调戏，不过，遗憾的是鱼和熊掌不可得兼，一个群只能进行二选一。据说 Baby Q 来自图灵公司，而小冰却是出生自微软公司。\n无论是Baby Q，还是小冰，向我们展现的都是人工智能技术。这些产品都是利用深度神经网络算法模仿人脑的运算方式，让机器可以持续的自我学习。\n目录\n简介\n机器人看板\n简单调用 API\n备注\n简介\n终于要进入正题了，这里不是教大家如何开发一个智能聊天的机器人，而是通过调用第三方的 API，给自己的网站加入这样一个类似的功能。\n这里，我介绍的是图灵的智能聊天机器人，据说它是中文语境下智能度最高的机器人大脑。官方网站：http://www.tuling123.com/。\n免费版本的就可以实现调侃、笑话、翻译和查天气等功能啦，非常强大，一起来体验一下吧。\n机器人看板\n进入官方页面需要进行注册，登录后会进入看板页面，在这里你可以添加多种机器人，默认是免费 5000次/天，也分配给用户一个调用 API 的 key。\n创建机器人：\n还有一些数据监控等功能需要自己去进行学习和了解。\n简单调用 API\n根据官方文档的要求，建议采用 POST，还有两个必须的参数名为 info 和 key。\n自己搞个 API 调用：\n你可以进入 http://www.fanguzai.net/#/robot 进行测试体验，大致效果图：\n备注\n所有的聊天回复内容都是图灵机器人 API 自身的知识库，我们可以自行登录官网，通过知识库页面批量添加或导入自己的问题和答案，就可以打造一个属于自己的 perfect 的个人定制化 Robot。\n你也可以和下面的网友一样，自己学习如何给微信公众号增加智能自动回复的功能。\n【博主】反骨仔\n【原文】http://www.cnblogs.com/liqingwen/p/6884748.html"}
{"content2":"TensorFlow简介：\n在使用TensorFlow之前我们要了解TensorFlow是什么，如果喜欢看视频的同学，可以看2017年8月6号谷歌大脑资深研究员刘小兵在极客公园Rebuild大会上的演讲。\n接下来让我们用文字谈谈TensorFlow。//所有事物进行数字化//机器迁就与人 或者反而\n首先谈谈人工智能，首先看看以前的机器能做什么？我给机器一个指令，机器按我的想法去做，不会超出我的期望，也不会违背。而我们现在的人工智能也还是在做这一件事，我给它算法，它按照算法一步一步的执行，判断。但现在的与之前的有什么区别呢？重点是它现在可以对某件事给出它的意见，而这件事我们之前可能未曾发觉，比如某些病症的初期症状检测，物品推荐算法等等。\n人们常说，学习一样事情最好的方法是把它教给别人，在我们在教机器学习的时候，其实我们也在思索我们本身所使用的方式。即当我们在说话时，我们在说什么，我们在画画时，我们在画什么？当我在写这篇文章的时候，我在写什么？如果我不清楚一件事情，我是很难鞭辟入里地解释清楚的。\n在说完人工智能后，我们再谈谈机器学习。机器学习是人工智能的子集。换而言之，机器学习是实现人工智能的一种方式，搞清了关系，首先我们来给机器学习一个定义，引用Herbert A. Simon的一段话，“如果一个系统能够通过执行某个过程改进它的性能，这就是学习“。实质上就是一个学习总结，进行校正，长经验的过程。所有只要用机器可以实现这样想法的都可以称之为机器学习，就是让机器具有从数据中抽取信息，并归纳信息讲了什么，可以想想我们在上学时是怎么学习的？语文中的阅读理解部分是否和这个场景很相似呢？而要实现机器学习，那么第一件事便是让计算机认识数据，我们平常说的话，画的图像，计算机是不认识的，那么要让计算机去学习，这些语音，图画作为信息载体的东西，就需要让计算机进行辩解，这也是现在我们所在做的计算机视觉(computer vision)简称CV，自然语言处理(Natural Language Processing)简称NLP ，声纹识别(Voice Print Recognition )简称VPR，让计算机能看，能听，并且能判别是谁说的。\n在现在的时代，如果我们谈到机器学习，那深度学习是必然绕不过去，深度学习又是机器学习的一个子集，是实现机器学习的一种方法。引用wikipedia的一段话“一类通过多层非线性变换堆对高复杂性数据建模算法的合集”。深度学习的深度基本上就是指其中的多层，不断矫正，不断逼近正确答案。具体的以后文章详细介绍，这里先进行概念的梳理。\n而TensorFlow就是这样一个平台，我们可以用用其提供的方法对所设计的模型进行训练，实现自己的想法。也支持分布式计算，并且已经开源，我们可以对其进行源码的学习。\nTensorFlow的安装\nTensorFlow支持C++,Python语言，因其对Python语言支持的最为全面，因此我在以后的学习中选择Python语言，（Python语言也是这样在实践中进行学习）\n下载python环境安装后，确保自己安装了python的包管理工具--pip .方法是打开命令行，敲入pip，如果提示找不到该命令，则未安装或者在安装Python的时候，没有将python的命令添加进系统变量。在确保pip安装后，\n# python2 用 pip   python3  用 pip3\n在命令行键入  pip/pip3  install --upgrade tensorflow\n等待命令执行完后，安装就完成了。\nTensorFlow的简单使用\n在安装完成之后，首先介绍TensorFlow中的基本概念。\n在TensorFlow中，所有的运算是以图的形式进行组织的，节点是运算，边是依赖关系。\n以下是实例程序：\nimport tensorflow as tf\na = tf.constant([1.0, 2.0], name= \"a\")\nb = tf.constant([2.0, 3.0], name= \"b\")\nresult = a + b\nsess = tf.Session()\nsess.run(result)\n#结果为array([3., 5.], dtype= float32)\n后边文章通过例子来实践，来推理机器算法的得出。提高自己的算法能力。"}
{"content2":"编者按：人工智能的浪潮正如火如荼地袭来，未来人工智能将大有所为，人们的生活轨迹也正在技术不断向前推进的过程中逐渐改变。人工智能不是科研人员或开发人员的专属，微软希望能够将人工智能带给每个人，从开发者到数据科学家，从技术爱好者到学生，从而激发出更前沿更独到的技术，催生出更富有生命力的产品。\n为了帮助大家更好地理解人工智能相关的服务、技术和产品，微软最新推出“人工智能大礼包”——在线学习课程：微软人工智能公开课、云技术学校：Azure School、人工智能前沿技术分享：AI讲堂，一网打尽人工智能领域的前沿技术和大咖观点，每天学习一点点，助你秒变AI达人。\nAI MOOC：微软人工智能公开课\n《微软人工智能公开课》是一套人工智能领域的“全民课程”，面向所有人群免费开放。公开课拥有全明星讲师阵容，由微软全球执行副总裁、微软亚太研发集团主席、微软亚洲研究院院长洪小文博士领衔，并涵盖微软亚洲研究院、微软中国云计算与企业事业部、微软中国开发体验及平台合作事业部以及微软合作伙伴的多位技术专家，希望为大家提供最好的学习体验。\n公开课共包括微软人工智能四个篇章：概览、服务和API、数据分析平台和深度学习框架。这套课程适用于开发者、数据科学家、技术爱好者和学生系统化地了解如何使用微软的人工智能平台开发出自己的智能服务。\n精彩课程亮点集结：\n第一篇章：微软人工智能概览\n微软研究院最新AI研究成果；微软 AI 服务如何赋能开发者；微软云 Azure 对 AI 的强大支持；微软人工智能公开课介绍\n第二篇章：服务和API\n微软人工智能服务和API概览；微软认知服务 – 视觉API；微软认知服务 - 语义及语音API；微软对话即平台及会话机器人框架Bot Framework\n第三篇章：数据分析平台\n数据分析平台概览；微软Azure机器学习服务；微软R服务\n第四篇章：深度学习\n深度学习概览；Azure GPU助力基于TensorFlow框架的深度学习；Cognitive Toolkit框架以及Azure上DSVM虚拟机模板；最佳实践：基于Azure和Caffe框架的深度学习\n《微软人工智能公开课》课程持续更新，扫描二维码发现你的新课程！\nAzure School：云技术在线学习网站\n人工智能正在席卷各大领域，层出不穷的新奇功能和应用令众多技术爱好者怦然心动。然而，很多人能够理解相关概念和原理，但在实际构建技术方面却四处碰壁、无从下手，想要真正掌握新技术并不容易。\n近日，微软正式推出Azure School，这是一个围绕各类Azure服务和技术构建的在线学习网站，能够提供课程、学习资源、学习路径、最佳实践、常见问题解答、视频、演示文档等学习过程中需要的各类内容。网站所有内容均由微软专家团队精心准备，并按照大家的常见需求和使用场景进行了场景化的整理。\n无论你想要做什么或者学什么，只要与Azure有关，都可以来这里按图索骥，通过妥善归纳整理的课程，循序渐进地掌握相关知识。各种纯干货，言简意赅的内容，只需几分钟就可以学得知识点，坚持积累学习所有相关课程后，即可顺利点亮技能点，轻松变身云技术达人。\n整个Azure School的内容分为五大板块：\nAzure上云须知：为云计算新手提供的基础内容，帮助大家快速了解并上手Azure。\n5 分钟Azure快速入门：通过简明扼要的内容帮助大家了解不同Azure服务的作用、使用方法。\n项目搭建：针对具体需求带领大家搭建解决方案，包括背景介绍、项目架构、准备和实施步骤等。\n技术专栏：展示专家们针对具体服务或应用场景撰写的深度技术文章。\n推荐专家：来自微软与合作伙伴公司的技术专家，大家可以在这里了解他们的专长，撰写的专栏文章和讲授的课程，并能直接向专家提问。\n丰富的内容，顶尖专家大咖，定期更新的课程、技术、知识、观点……所有内容从细微之处着手，按照具体的场景和需求进行整理汇总，这就是Azure School。\n扫描二维码秒变云技术小达人\nAI讲堂：大咖前沿分享“大本营”\nAI讲堂是微软亚洲研究院推出的系列活动，希望借助讲座、分享、讨论等多样化的形式，向更多的人们传递知识、普及科学，为大家揭开人工智能的神秘面纱。这里汇聚了国内外人工智能领域的大咖，为大家分享最新鲜的观点和最前沿的技术，希望能够给大家带来一些新的思考和启迪。\nAI讲堂不仅是大师云集，而且几乎可以说涵盖人工智能领域的方方面面，从专业技术到行业未来观察，360度无死角地展现人工智能的独特魅力。目前，AI讲堂已邀请到众多人工智能大神，如卡内基梅隆大学（CMU）计算机学院院长Andrew W. Moore和副院长Philip L. Lehman、微软亚洲研究院副院长周明博士、清华大学计算机科学与技术系孙茂松教授等等。\n精彩分享亮点集结：\n周明：自然语言对话引擎\n如何通过自然语言聊天、问答和对话实现更高层次的人机对话？\nCMU计算机学院院长Andrew Moore带你回顾与展望2017年的AI发展\n两年内，人工智能领域产生哪些技术突破？从业界和学界两个角度看，人工智能接下来将走向何方？\n孙茂松：运用之妙，存乎一心——从机器翻译到古诗生成\n孙茂松教授通过介绍机器翻译的发展,讲解了多语的重要性及机器翻译从统计到神经模型的过渡，并由此改进以创作古诗。\nAI讲堂持续更新，扫描二维码开启人工智能之旅\n立即访问http://market.azure.cn"}
{"content2":"最近太忙已经好久没有写博客了，今天整理分享一篇关于损失函数的文章吧，以前对损失函数的理解不够深入，没有真正理解每个损失函数的特点以及应用范围，如果文中有任何错误，请各位朋友指教，谢谢~\n损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：\n其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是找到使目标函数最小时的θ值。下面主要列出几种常见的损失函数。\n理解：损失函数旨在表示出logit和label的差异程度，不同的损失函数有不同的表示意义，也就是在最小化损失函数过程中，logit逼近label的方式不同，得到的结果可能也不同。\n一般情况下，softmax和sigmoid使用交叉熵损失（logloss），hingeloss是SVM推导出的，hingeloss的输入使用原始logit即可。\n一、LogLoss对数损失函数（逻辑回归，交叉熵损失）\n有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —> min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。\nlog损失函数的标准形式：\n刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE（最大似然估计）中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。\n逻辑回归的P(Y=y|x)表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：\n将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：\n逻辑回归最后得到的目标式子如下：\n上面是针对二分类而言的。这里需要解释一下：之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉。\n这里有个PDF可以参考一下：Lecture 6: logistic regression.pdf.\n注意：softmax使用的即为交叉熵损失函数，binary_cossentropy为二分类交叉熵损失，categorical_crossentropy为多分类交叉熵损失，当使用多分类交叉熵损失函数时，标签应该为多分类模式，即使用one-hot编码的向量。\n二、平方损失函数（最小二乘法, Ordinary Least Squares ）\n最小二乘法是线性回归的一种，最小二乘法（OLS）将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：\n简单，计算方便；\n欧氏距离是一种很好的相似性度量标准；\n在不同的表示域变换后特征性质不变。\n平方损失（Square loss）的标准形式如下：\n当样本个数为n时，此时的损失函数变为：\nY-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。\n而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：\n上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数α的线性函数。在机器学习中，通常指的都是后一种情况。\n三、指数损失函数（Adaboost）\n学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到fm(x):\nAdaboost每次迭代时的目的是为了找到最小化下列式子时的参数α 和G：\n而指数损失函数（exp-loss）的标准形式如下\n可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：\n关于Adaboost的推导，可以参考Wikipedia：AdaBoost或者《统计学习方法》P145.\n四、Hinge损失函数（SVM）\n在机器学习算法中，hinge损失函数和SVM是息息相关的。在线性支持向量机中，最优化问题可以等价于下列式子：\n下面来对式子做个变形，令：\n于是，原式就变成了：\n如若取λ=1/(2C)，式子就可以表示成：\n可以看出，该式子与下式非常相似：\n前半部分中的 l 就是hinge损失函数，而后面相当于L2正则项。\nHinge 损失函数的标准形式\n可以看出，当|y|>=1时，L(y)=0。\n更多内容，参考Hinge-loss。\n补充一下：在libsvm中一共有4中核函数可以选择，对应的是-t参数分别是：\n0-线性核；\n1-多项式核；\n2-RBF核；\n3-sigmoid核。\n五、其它损失函数\n除了以上这几种损失函数，常用的还有：\n0-1损失函数\n绝对值损失函数\n下面来看看几种损失函数的可视化图像，对着图看看横坐标，看看纵坐标，再看看每条线都表示什么损失函数，多看几次好好消化消化。\n六、Keras / TensorFlow 中常用 Cost Function 总结\nmean_squared_error或mse\nmean_absolute_error或mae\nmean_absolute_percentage_error或mape\nmean_squared_logarithmic_error或msle\nsquared_hinge\nhinge\ncategorical_hinge\nbinary_crossentropy（亦称作对数损失，logloss）\nlogcosh\ncategorical_crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如(nb_samples, nb_classes)的二值序列\nsparse_categorical_crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：np.expand_dims(y,-1)\nkullback_leibler_divergence:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.\npoisson：即(predictions - targets * log(predictions))的均值\ncosine_proximity：即预测值与真实标签的余弦距离平均值的相反数\n需要记住的是：参数越多，模型越复杂，而越复杂的模型越容易过拟合。过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，从而使模型具有更好的泛化能力。"}
{"content2":"背景\n提及机器学习（Machine Learning），大多数人的脑海中首先浮现出来的就是各种机器学习的模型策略。当一个问题的数据集（data set）确定后，我们便开始观察数据，处理特征，确定模型。然而，为什么机器学习这个工具可以大概正确地预测数据的目标结果？在某个数据集上的学习是否具有可行性（feasibility of learning）？机器学习的学习理论对这些问题作出了解释。本文以理论推导为主，结合具体的学习模型来介绍学习理论的内容。\n学习问题\n存在未知的目标函数\n\\[f:\\mathcal{X}\\rightarrow\\mathcal{Y}\\] 和样本集\\[\\mathcal{D}=\\{(\\mathbf{x_1},y_1),(\\mathbf{x_2},y_2),\\cdots,(\\mathbf{x_N},y_N)\\}\\]其中，\\(\\mathbf{x_i} \\in \\mathcal{X}，y_i \\in \\mathcal{Y}，i = 1,2,\\cdots,N\\)。\n那么，我们能否通过学习算法，在假设空间 \\(\\mathcal{H}\\) 中找到一个假设 \\(h\\)，使得该假设近似地等于目标函数 \\(f\\)？\n注：这里提到 \\(f\\) 是不可知的unknown，即无解析解analytic solution，我们的任务就是使用数据来构造一个经验解empirical solution。如果有解析解，就不需要机器学习方法。\n我们仅对二分类（dichotomy）问题进行讨论\n该问题中的假设 \\[h:\\{x_1 ,x_2,\\cdots,x_n\\}\\rightarrow\\{-1,+1\\}\\] 其中，\\(\\{x_1 ,x_2,\\cdots,x_n\\}\\) 是输入空间或样本空间。\\(\\{-1,+1\\}\\) 是输出空间。样本集中的输入向量集合 \\(\\{\\mathbf{x_1},\\mathbf{x_2},\\cdots,\\mathbf{x_N}\\}\\) 是输入空间的一个子集，\\(\\mathbf{x_i}\\)是一个 \\(n\\) 维向量。\n我们知道样本内的实例的标记，需要预测除样本外的实例的标记。若能通过学习找到一个假设，使得对不同的实例，预测的标记与通过 \\(f\\) 得到的标记相近，那么这个学习过程是可行的，否则就是不可行的。\n若学习不可行，无论选择什么学习算法，什么样的假设空间，哪怕一个假设对样本集拟合地再好，对未知数据的预测都是不可信的。\n第一部分\n首先，学习的可行性的讨论基于一个这样的假设：样本集 \\(\\mathcal{D}\\) 是在样本空间内依据概率分布 \\(P\\) 随机抽取的，属于独立同分布。我们不知道具体的概率分布 \\(P\\)。如果样本集是被人精心设计构造的（非随机抽取），那么通过学习得到的假设和真实的结果一定是大相径庭的。\n在这个假设下，我们可以以概率的方式，使用 \\(\\mathcal{D}\\) 来推断 \\(\\mathcal{D}\\) 以外的实例。\n一、引入Hoeffding不等式\nHoeffding不等式是关于一组随机变量均值的概率不等式。如果 \\(X_1,X_2,\\cdots,X_n\\) 为一组独立同分布的参数为 \\(p\\) 的伯努利分布随机变量，\\(n\\) 为随机变量的个数。定义这组随机变量的均值为：\\[\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n X_i\\]对于任意 \\(\\epsilon > 0\\)，Hoeffding不等式可以表示为：\\[P\\{|\\overline{X} - E[\\overline{X}]|>\\epsilon\\} \\leq 2e^{-2\\epsilon^2 n}\\]\n二、联系学习问题\n介绍：假设空间（hypothesis set）\n假设空间是所有假设的集合。在 \\(h:\\{x_1 ,x_2,\\cdots,x_n\\}\\rightarrow\\{-1,+1\\}\\) 中，我们设 \\(x_i\\) 的可能取值有 \\(N_i\\) 的，那么对应的假设空间的大小为 \\[2 \\times \\prod_{i=1}^{n} N_i \\]我们先讨论假设空间中只有一个假设的情况，然后扩展到有限个假设，最后扩展到无限个假设\n只有一个假设的情况（ \\(\\mathcal{H}=\\{ h \\}\\) ）\n样本内误差（in-sample error）：\\[E_{in}(h)=\\frac{1}{N}\\sum_{i=1}^N I\\{h(x_i) \\neq f(x_i)\\}\\]\n样本外误差（out-sample error）：\\[E_{out}(h)=P\\{h(x) \\neq f(x)\\}\\]\n在学习问题（learning problem）中，样本内误差和样本外误差分别相当于Hoeffding不等式中的 \\(\\overline{X}\\) 和 \\(E[\\overline{X}]\\)。此时，Hoeffding不等式转变为：\\[P\\{|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\leq 2e^{-2\\epsilon^2 N}\\]\n从上面的公式可知，拟合训练数据的假设与该假设针对整个样本空间的预测，这两者的错误率差别很大的那种情况发生的概率是很小的。\n然而，这不是一个学习过程，只是验证一个假设的预测能力，它不能证明这个假设对应的样本外误差是最小的，不能证明这个假设在假设空间内是最佳假设。\n有限假设空间情况（ \\(\\mathcal{H}=\\{h_1,h_2,\\cdots,h_M\\}\\) ）\n在假设空间 \\(\\mathcal{H}\\) 中，每个假设 \\(h_i\\) 都是固定不变的。给定一个的样本集，学习算法的任务就是在给定不变的M个假设中找到一个最终的假设 \\(g\\)，使得对于任意 \\(i\\in\\{1,2,\\cdots,M\\}\\)，都有\n\\[P\\{|E_{in}(g) - E_{out}(g)|>\\epsilon\\} \\leqslant P\\{|E_{in}(h_i) - E_{out}(h_i)|>\\epsilon\\}\\]\n其上界为：\n\\[P\\{|E_{in}(g) - E_{out}(g)|>\\epsilon\\} \\leqslant \\, \\sum_{i=1}^M P\\{|E_{in}(h_i) - E_{out}(h_i)|>\\epsilon\\} \\leqslant 2Me^{-2\\epsilon^2 N}\\]\n令 \\(\\delta = P\\{|E_{in}(g) - E_{out}(g)|>\\epsilon\\}\\)，则\\[N \\geqslant\\frac{1}{2\\epsilon^2}[lnM+ln{\\frac{1}{\\delta}}]\\]\n也就是说，在有限个假设且样本集足够大的情况下，样本内误差和样本外误差的差的绝对值超过 \\(\\epsilon\\) 的概率为 \\(\\delta\\)。\n三、总结\n再次回到学习问题\n回到问题：我们能否通过学习算法，在假设空间 \\(\\mathcal{H}\\) 中找到一个假设 \\(h\\)，使得该假设近似地等于目标函数 \\(f\\)？\n这里的“近似地等于”，意味着 \\(E_{out}(g) \\approx 0\\)\n确定性答案[deterministic answer]：不能。我们不能确定任何一个在 \\(\\mathcal{D}\\) 之外的实例的标记。\n概率性答案[probabilistic answer]：可以。我们可以得到一个在 \\(\\mathcal{D}\\) 之外的实例的最可能的标记结果。\n根据上文的讨论可知，学习问题可以分为两个子问题：\n我们是否可以确定 \\(E_{out}(g) \\approx E_{in}(g)\\)？\n为了令 \\(E_{}(g) \\approx 0\\)，我们是否可以使 \\(E_{in}(g)\\) 尽可能小？\n复杂性权衡\n假设空间的复杂性\n当假设空间的大小 \\(M\\) 变大，\\(E_{in}(g)\\) 偏离 \\(E_{out}(g)\\) 的可能性就越大，但是，由于 \\(g\\) 来源于 \\(\\mathcal{H}\\)，假设空间的复杂性增加了我们找到一个较小的 \\(E_{in}(g)\\) 的概率。在学习理论中，对 \\(\\mathcal{H}\\) 的复杂性的权衡是一个很重要的话题。\n目标函数的复杂性\n\\(f\\) 复杂而 \\(\\mathcal{H}\\) 简单时，很难找到一个 \\(E_{in}(g)\\) 的假设；而\\(f\\) 复杂同时 \\(\\mathcal{H}\\) 复杂时，又会遇到了 \\(E_{in}(g)\\) 偏离 \\(E_{out}(g)\\) 的问题；当 \\(f\\) 过于复杂时，学习是不可行的。\n实际问题中，大多数的目标函数都不会过于复杂。只要保证\\(\\mathcal{H}\\)的复杂性可以给我们一个较小的Hoeffing边界，那么，样本集的匹配程度就决定了我们学习 \\(f\\) 的成败。\n学习模型 probably approximately correct, PAC\n我们训练学习器的目标是，能够从合理数量的训练数据中通过合理的计算量可靠地学习到知识（这里的知识指目标函数 \\(f\\)）。\n机器学习的现实情况：除非对每个可能的数据进行训练，否则总会存在多个假设使得真实错误率不为零，即学习器无法保证和目标函数完全一致。此外，训练样本是随机选取的，训练样本总有一定的误导性。为此，我们要弱化对学习器的要求：\n我们不要求学习器输出零错误率的假设，只要求错误率被限制在某常数 \\(\\epsilon\\) 范围内，\\(\\epsilon\\) 可为任意小。\n不要求学习器对所有任意抽取的数据都能成功预测，只要求其失败的概率被限定在某个常数 \\(\\delta\\) 的范围内， \\(\\delta\\) 可取任意小。\n简而言之，我们只要求学习器可能学习到一个近似正确的假设，故得到了“可能近似正确学习”（PAC学习）。\n一个可PAC学习的学习器要满足两个条件：学习器必须以任意高的概率输出一个错误率任意低的假设。学习过程的时间最多以多项式方式增长。\n对于PAC学习来说，训练样本的数量和学习所需的计算资源是密切相关的。如果学习器对每个训练样本需要某最小处理时间，那么为了使目标函数 \\(f\\) 是可PAC学习的，学习器必须在多项式数量的训练样本中进行学习。\n第二部分\n在第一部分中我们讨论了有限假设空间的情况，不过，现实中遇到的大多数学习问题的假设集的大小都是无限的。此时，\\(P\\{|E_{in}(g) - E_{out}(g)|>\\epsilon\\}\\) 的上界 \\(2Me^{-2\\epsilon^2 N}\\) 中的 \\(M\\) 趋于无穷，使得该上界没有任何意义了。因此，在第二部分中，我们将推导出一个更加精确的上界。\n一、泛化理论 Theory of Generalization\n泛化误差（generalization error）\n在第一部分的最后，我们得到\n\\[P\\{|E_{in}(g) - E_{out}(g)|>\\epsilon\\} \\leqslant 2Me^{-2\\epsilon^2 N}\\]我们选择一个容忍度（tolerance level） \\(\\delta=P\\{|E_{in}(g) - E_{out}(g)|>\\epsilon\\}\\)，那么，至少有 \\(1-\\delta\\) 概率，使得\n\\[E_{out}(g) \\leqslant E_{in}(g) + \\sqrt{\\frac{1}{2N} ln{\\frac{2M}{\\delta}}}\\]\\[E_{out}(g) \\geqslant E_{in}(g) - \\sqrt{\\frac{1}{2N} ln{\\frac{2M}{\\delta}}}\\]即泛化边界。\n在有限假设集中，当 \\(M\\) 一定时，随着样本数 \\(N\\) 的增加，\\(E_{out}(g) \\approx E_{in}(g)\\)。而对于有一个无限大小的假设集模型来说，我们可以找一个更精确的值来取代假设空间的实际大小 \\(M\\)。当 \\(M\\) 趋于无穷时，该值仍是一个有穷值。为什么 \\(M\\) 可以优化呢？在第一部分，我们利用联合界（union bound），令\\[P\\{\\bigcup_{i=1}^{M}A_i\\} \\leqslant \\sum_{i=1}^M P\\{A_i\\}\\]然而在一个典型的学习模型里，许多假设都很类似。下面我们引入成长函数和VC维等概念。\n引入概念\n二分类（dichotomies）\n对于某个 \\(h \\in \\mathcal{H}\\)，\\((h(\\mathbf{x_1}),\\cdots,h(\\mathbf{x_N}))\\) 是该假设所对应的二分类。可见\\[(h(\\mathbf{x_1}),\\cdots,h(\\mathbf{x_N})) \\in \\{+1, -1\\}^N\\]\n成长函数（growth function）\n令 \\(\\mathcal{H}(\\mathbf{x_1},\\cdots,\\mathbf{x_N}) = \\{(h(\\mathbf{x_1}),\\cdots,h(\\mathbf{x_N}))|h \\in \\mathcal{H}\\}\\)，则一个假设空间的成长函数为：\\[m_\\mathcal{H}(N) = \\max_{\\mathbf{x_1},\\cdots,\\mathbf{x_N} \\in \\mathcal{X}}|\\mathcal{H}(\\mathbf{x_1},\\cdots,\\mathbf{x_N})|\\]对于不同的 \\(\\mathcal{D}\\)，由于内部 \\(N\\) 个点的分布方式不同，其 \\(|\\mathcal{H}(\\mathbf{x_1},\\cdots,\\mathbf{x_N})|\\)也可能不同。\n成长函数是假设空间在有限的样本集上的可以产生的不同假设的数量，而不是在整个输入空间里。因此，成长函数的值取决于N个样本的可行的分类结果（任意一个样本的分类结果是+1或-1）的数目。\n打散（shatter）\n若一个假设空间 \\(\\mathcal{H}\\) 能产生样本集上的所有假设，即 \\(\\mathcal{H}(\\mathbf{x_1},\\cdots,\\mathbf{x_N})=\\{-1,+1\\}^N\\)，此时，我们说 \\(\\mathcal{H}\\) 可以打散 \\(\\mathbf{x_1},\\cdots,\\mathbf{x_N}\\)，\\(m_\\mathcal{H}(N)=2^N\\)。可以看出，对于任意正整数 \\(N\\)，都满足\\[m_\\mathcal{H}(N)\\leqslant2^N\\]\n断点（break point）\n如果没有一个大小为 \\(k\\) 的数据集可以被 \\(\\mathcal{H}\\) 打散，则 \\(k\\) 是 \\(\\mathcal{H}\\) 的一个断点，此时\\[m_\\mathcal{H}(k) < 2^k\\]\nVC维（Vapnik-Chervonenkis dimension）\n一个假设空间 \\(\\mathcal{H}\\) 的VC维 \\(d_{vc}(\\mathcal{H})\\)，是满足 \\(m_\\mathcal{H}(N)=2^N\\) 的最大整数 \\(N\\)。如果对于任意 \\(N\\) 都有 \\(m_\\mathcal{H}(N)=2^N\\) （即没有断点），则\\(d_{vc}(\\mathcal{H}) = \\infty\\)。\n假设空间的有效大小\n给定一个 \\(\\mathcal{D}\\)，假设空间中便存在多个具有相同的二分法的假设。对于我们的学习问题来说，这些具有相同二分法的假设就是冗余。因此，尽管假设空间的实际大小是无穷的，但我们有可能可以找到一个有穷的有效大小。\n只要 \\(\\mathcal{H}\\) 存在断点（即 \\(d_{vc}(\\mathcal{H}) \\neq \\infty\\)），\\(m_\\mathcal{H}(N)\\) 就是关于 \\(N\\) 的多项式。若 \\(m_\\mathcal{H}(N)\\) 可以取代 \\(M\\)，则随着 \\(N\\) 的增大，泛化误差\\(\\sqrt{\\frac{1}{2N} ln{\\frac{2M}{\\delta}}}\\)将逐渐减小为 \\(0\\)；相反地，若不存在断点，泛化误差永远不会趋向于 \\(0\\)。\n注：在有些问题里只有 \\(N\\) 个点的分布方式极其特殊时，才能被 \\(\\mathcal{H}\\) 打散。这时，虽然这时 \\(m_\\mathcal{H}(N)=2^N\\)，但考虑平均情况，\\(m_\\mathcal{H}(N)\\) 仍是关于 \\(N\\) 的多项式。\n下面证明只要 \\(\\mathcal{H}\\) 存在断点，\\(m_\\mathcal{H}(N)\\) 就是关于 \\(N\\) 的多项式。\n定义 \\(B(N, k)\\) 为 \\(\\mathcal{H}(\\mathbf{x_1},\\cdots,\\mathbf{x_N})\\) 的最大数目。其中，使 \\(\\mathcal{H}(\\mathbf{x_1},\\cdots,\\mathbf{x_N})\\) 数目最大的样本集 \\(\\mathcal{D}\\) 中不存在可以被 \\(\\mathcal{H}\\) 打散的数目为 \\(k\\) 的子集。\\(B(N, k)\\)是比 \\(2^N\\) 更准确的 \\(m_\\mathcal{H}(N)\\) 的上界。\\[m_\\mathcal{H}(N)\\leqslant B(N, k)\\leqslant2^N, \\space \\space \\text{if k is a break point for }\\mathcal{H}\\]可以根据下式利用动态规划求出具体的 \\(B(N,k)\\) 的值\\[B(N, k) \\leqslant B(N-1, k)+B(N-1, k-1)\\]也可以直接利用归纳法证明 \\[B(N, k) \\leqslant \\sum_{i=0}^{k-1} \\binom{N}{i}\\]最后，由于断点 \\(k = d_{vc}(\\mathcal{H})+1\\)，而且给定一个 \\(\\mathcal{H}\\) 后，断点和VC维就是固定不动的。所以\\[m_\\mathcal{H}(N)\\leqslant B(N, k) \\leqslant \\sum_{i=0}^{d_{vc}(\\mathcal{H})} \\binom{N}{i} \\leqslant N^{d_{vc}(\\mathcal{H})} + 1\\]是一个关于 \\(N\\) 的多项式。\n1) 动态规划\n初始状态： \\(B(N, 1) = 1\\)；\\(k>1\\) 时，\\(B(1, k) = 2\\)\n建立递归： 将 \\((h(\\mathbf{x_1}),\\cdots,h(\\mathbf{x_N}))\\) 划分为 \\((h(\\mathbf{x_1}),\\cdots,h(\\mathbf{x_{N-1}}))\\) 和 \\(h(\\mathbf{x_N})\\) 两部分。在前一部分仅出现过一次的序列 \\(S_1\\) 的数目记作 \\(\\alpha\\)。在剩下的 \\(B(N,k)- \\alpha\\) 个序列中，将 \\(h(\\mathbf{x_{N-1}}) = +1\\) 分为一类 \\(S_2^+\\)， \\(h(\\mathbf{x_{N-1}}) = -1\\) 分为一类 \\(S_2^-\\)，容易得知，这两类里的序列数目相等，记作 \\(\\beta\\)。此时，\\[B(N, k)=\\alpha+2\\beta\\]在集合 \\(S_1+S_2^+\\) 中，在第一部分中不存在可以被 \\(\\mathcal{H}\\) 打散的数目为 \\(k\\) 的子集，即\\[\\alpha+\\beta \\leqslant B(N-1,k)\\]在集合 \\(S_2^-\\) 不存在可以被 \\(\\mathcal{H}\\) 打散的数目为 \\(k-1\\) 的子集（若存在，则在集合 \\(S_2^- + S_2^+\\) 中也存在可以被 \\(\\mathcal{H}\\) 打散的数目为 \\(k\\) 的子集，与初始定义矛盾），即\\[\\beta \\leqslant B(N-1,k-1)\\]综上，\\[B(N, k) \\leqslant B(N-1, k)+B(N-1, k-1)\\]利用动态规划求出：\\[B(N, k) \\leqslant \\sum_{i=0}^{k-1} \\binom{N}{i}\\]时间复杂度为 \\(O(N*k)\\)，空间复杂度为 \\(O( min(N,k) )\\)。\n2) 归纳法\n这里直接用归纳法证明上式的正确性。\n当 \\(k=1\\) 或 \\(N=1\\)时，上式成立。\n假设对于所有的 \\(N \\leqslant N_0\\)，上式正确。\n当 \\(N=N_0+1\\) 时，\n\\[B(N_0+1,k) \\leqslant \\sum_{i=0}^{k-1} \\binom{N_0}{i} + \\sum_{i=0}^{k-2} \\binom{N_0}{i} = 1 + \\sum_{i=1}^{k-1} \\binom{N_0}{i} + \\sum_{i=1}^{k-1} \\binom{N_0}{i-1} = 1 + \\sum_{i=1}^{k-1} \\binom{N_0+1}{i} = \\sum_{i=0}^{k-1} \\binom{N_0+1}{i}\\]\n用 \\(m_\\mathcal{H}(N)\\) 取代 \\(M\\)\n若用 \\(m_\\mathcal{H}(N)\\) 取代 \\(M\\)，则对应的Hoeffing不等式转变为VC不等式：\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\leqslant 4m_\\mathcal{H}(2N)e^{-\\frac{1}{8} \\epsilon^2 N}\\]给定任意的容忍度\\(\\delta > 0\\)，那么至少有 \\(1-\\delta\\) 概率，使得： \\[E_{out}(g) \\leqslant E_{in}(g) + \\sqrt{\\frac{8}{N} ln{\\frac{4m_\\mathcal{H}(2N)}{\\delta}}}\\]即VC泛化边界。\n尽管比之前的泛化边界的限定更弱，但只要VC维不是无穷大的，那么泛化误差最终会收敛于 \\(0\\)。成长函数作为假设空间的有效大小，取代了假设空间的实际大小 \\(M\\)。因此，VC泛化边界确定了无限大小的假设空间上学习的可行性。\n二、VC边界的证明\n\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\leqslant 4m_\\mathcal{H}(2N)e^{-\\frac{1}{8} \\epsilon^2 N}\\]对于任意的目标函数（\\(f\\) 或者 \\(P(y|\\mathbf{x})\\)）都成立。每个数据集都是独立同分布的（iid），数据集中的每个数据依 \\(P(\\mathbf{x},y)\\) 独立生成。\n难题：\\(E_{out}(h)\\) 取决于整个输入空间，不容易处理\n解决：避免计算 \\(E_{out}(h)\\)。找到 \\(E_{in}\\) 和 \\(E_{out}\\) 的差值和 \\(E_{in}\\) 和另一个独立的数据集的样本内误差的差值的关系。\n泛化误差和样本内偏差的联系\n第二个大小为 \\(N\\) 的数据集 \\(\\mathcal{D}'\\) 从 \\(P(\\mathbf{x},y)\\) 抽样而得且独立于 \\(\\mathcal{D}\\)。\n对于只有一个假设的情况，随着 \\(N\\) 的数量的增大， \\(E_{in}(h)\\) 和 \\(E'_{in}(h)\\) 可以粗略地看作以 \\(E_{out}(h)\\) 为均值的高斯分布。因此 可以直观地看出，\\[P\\{|E_{in}(h) - E_{out}(h)|\\text{ is large}\\} \\leqslant 2P\\{|E_{in}(h) - E'_{in}(h)| \\text{ is large}\\}\\]\n对于多个假设的情况，\n\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2}\\} \\geqslant P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space and \\space\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} = P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\times P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\}\\]\n设 \\(h^{*}\\) 为任意一个满足 \\(|E_{in}(h^{*}) - E_{out}(h^{*})|>\\epsilon\\) 的假设，则\n\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\geqslant P\\{|E_{in}(h^{*}) - E'_{in}(h^{*})|>\\frac{\\epsilon}{2} \\space | \\space\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\geqslant P\\{|E'_{in}(h^{*}) - E_{out}(h^{*})| \\leqslant \\frac{\\epsilon}{2} \\space | \\space\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\geqslant 1-2e^{-\\frac{1}{2} \\epsilon^2 N}\\]\n我们假设 \\(e^{-\\frac{1}{2} \\epsilon^2 N} < \\frac{1}{4}\\)（否则，\\(4m_\\mathcal{H}(2N)e^{-\\frac{1}{8} \\epsilon^2 N}\\) 恒大于 \\(1\\)，没有证明的必要里），此时可以得到\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E_{out}(h)|>\\epsilon\\} \\leqslant 2P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2}\\}\\]\n用成长函数限定最坏情况偏差\n一种选取 \\(\\mathcal{D}\\) 和 \\(\\mathcal{D}'\\) 的方法是：首先从输入空间中随机地抽取 \\(2N\\) 个样本，然后随机地划分成 \\(\\mathcal{D}\\) 和 \\(\\mathcal{D}'\\)，即此时\n\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2}\\} = \\sum_{S} P\\{S\\} \\times P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space S\\}\\leqslant \\sup_{S} P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space S\\}\\]\n假设 \\(M \\leqslant m_{\\mathcal{H}}(2N)\\)，则\n\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space S\\} = P\\{\\sup_{h \\in \\{h_1,\\cdots,h_M\\}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space S\\} \\leqslant \\sum_{i=1}^{M} P\\{|E_{in}(h_i) - E'_{in}(h_i)|>\\frac{\\epsilon}{2} \\space | \\space S\\} \\leqslant M \\times \\sup_{h \\in \\mathcal{H}} P\\{|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space S\\}\\]\n综上，\n\\[P\\{\\sup_{h \\in \\mathcal{H}}|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2}\\} \\leqslant m_{\\mathcal{H}}(2N) \\times \\sup_{S} \\sup_{h \\in \\mathcal{H}} P\\{|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space S\\}\\]\n限定样本内偏差\n首先引入不放回取样的Hoeffing不等式\n令 \\(\\mathcal{A} = \\{a_1,\\cdots,a_{2N}\\}\\)，其中 \\(a_i \\in [0,1]\\)，令 \\(\\mu = \\frac{1}{2N} \\sum_{i=1}^{2N}a_i\\)，令 \\(\\mathcal{D} = {z_1,\\cdots,z_N}\\) 为一个从 \\(\\mathcal{A}\\) 中不放回随机取样而得到的大小为 \\(N\\) 的样本，则\\[P\\{|\\frac{1}{N}\\sum_{i=1}^{N}z_i - \\mu| > \\epsilon\\} \\leqslant 2e^{-2\\epsilon^2N}\\]\n当 \\(h(\\mathbf{x}_i) \\ne y_i\\) 时，\\(a_i = 1\\)，否则为 \\(0\\)。此时\\[E_{in}(h) = \\frac{1}{N} \\sum_{a_i \\in \\mathcal{D}}a_i \\text{, and } E'_{in}(h) = \\frac{1}{N} \\sum_{a'_i \\in \\mathcal{D}'}a'_i\\] \\[\\mu = \\frac{1}{2N} \\sum_{i=1}^{2N}a_i = \\frac{E_{in}(h) + E'_{in}(h)}{2}\\]\n带入Hoeffing不等式，得\\[P\\{|E_{in}(h) - E'_{in}(h)|>2t \\space | \\space S\\} \\leqslant 2e^{-2t^2N}\\]最后，令 \\(4t = \\epsilon\\)，得\n\\[P\\{|E_{in}(h) - E'_{in}(h)|>\\frac{\\epsilon}{2} \\space | \\space S\\} \\leqslant 2e^{-\\frac{1}{8}\\epsilon^2N}\\]\n第三部分\n一、泛化边界的解释\nVC泛化边界适用于所有的假设集、学习算法、输入空间、概率分布和二分目标函数。但它是一个非常宽松的边界。\n推导过程中适用的Hoeffing不等式本身就是松弛的。\n为了使我们的VC分析结果独立于输入空间 \\(\\mathcal{X}\\) 的概率分布 \\(P\\)， \\(m_{\\mathcal{H}}(N)\\) 给出了在最坏样本集的情况下的估计。\n限定 \\(m_{\\mathcal{H}}(N)\\) 时我们简单地选择了 \\(d_{vc}(\\mathcal{H})\\) 的多项式。\n虽然VC分析得到的泛化边界很松弛，但它是我们评估无限假设集下学习的可行性的一个依据。对不同的学习模型来说，VC分析有着同样松弛的边界，因此我们可以比较不同的模型在同一样本集下泛化能力的好坏。\n样本复杂度\n样本复杂度是指达到我们期望的泛化能力所需要的样本数目。\n给定 \\(\\delta > 0\\)，假设我们期望泛化误差不超过 \\(\\epsilon\\)，则\\[N \\geqslant \\frac{8}{\\epsilon^2}ln(\\frac{4m_{\\mathcal{H}}(2N)}{\\delta}) \\geqslant \\frac{8}{\\epsilon^2}ln\\frac{4((2N)^{d_{vc}}+1)}{\\delta}\\]\n\\(N\\) 和 \\(d_{vc}\\) 的比例大概是 \\(10000:1\\)，这是一个严格的最大界限。在实践中这一比例大约是 \\(10:1\\)。\n对模型复杂度的惩罚项\n对模型复杂度的惩罚项，就是给定一个样本集 \\(\\mathcal{D}\\)（即给定 \\(N\\)）时我们可以期望的泛化能力。\\[E_{out}(g) \\leqslant E_{in}(g) + \\Omega(N, \\mathcal{H}, \\delta)\\]这里的 \\(\\Omega(N, \\mathcal{H}, \\delta)\\) 就是模型的复杂度的惩罚项。\\[\\Omega(N,\\mathcal{H},\\delta)=\\sqrt{\\frac{8}{N} ln{\\frac{4m_\\mathcal{H}(2N)}{\\delta}}} \\leqslant \\sqrt{\\frac{8}{N} ln{\\frac{4((2N)^{d_{vc}}+1)}{\\delta}}}\\]因此，我们需要一个权衡：模型复杂度的增长会降低 \\(E_{in}\\) 但增加 \\(\\Omega(N,\\mathcal{H},\\delta)\\)。一个理想的模型应该是使最小化这两项的组合。\n测试集\n尽管我们的VC分析是基于二分目标函数的，但我们可以把它扩大到其他类型的目标函数，甚至是回归目标函数。\n二、近似-泛化权衡 Approximation-Generalization Tradeoff\n在 \\(\\mathcal{H}\\) 上选择的假设，既要在样本集上近似地接近目标函数 \\(f\\) ，又要在新的数据集上具有较好的泛化能力。\nVC泛化边界提供了考虑权衡的一种方法。如果 \\(\\mathcal{H}\\) 太简单，我们可能无法在样本集上近似地接近目标函数，即无法得到一个较小的 \\(E_{in}\\)。如果 \\(\\mathcal{H}\\) 太复杂，我们可能得到一个泛化能力较差的假设。\n而偏差-方差分析提供给我们另一种方法来看待近似-泛化权衡问题。\nVC分析基于0-1误差计算，用 \\(E_{in}\\) 加上一个惩罚项 \\(\\Omega\\) 来限定 \\(E_{out}\\)。而偏差方差分析中基于平方误差计算，将 \\(E_{out}\\) 分解为两个不同的部分。此时，样本外误差为\\[E_{out}(g^{(\\mathcal{D})}) = E_{\\mathbf{x}}[(g^{(\\mathcal{D})}(\\mathbf{x}) -f(\\mathbf{x}))^2]\\]可见，我们的最终假设依赖于样本集 \\(\\mathcal{D}\\)。要想不依赖于某个样本集，我们可以计算所有数据集的期望\n\\[E_{\\mathcal{D}}[{E_{out}(g^{(\\mathcal{D})})}] = E_{\\mathcal{D}}[E_{\\mathbf{x}}[(g^{(\\mathcal{D})}(\\mathbf{x}) -f(\\mathbf{x}))^2]] = E_{\\mathbf{x}}[E_{\\mathcal{D}}[(g^{(\\mathcal{D})}(\\mathbf{x}) -f(\\mathbf{x}))^2]] = E_{\\mathbf{x}}[E_{\\mathcal{D}}[(g^{(\\mathcal{D})}(\\mathbf{x})^2] - 2E_{\\mathcal{D}}[(g^{(\\mathcal{D})}(\\mathbf{x})]f(\\mathbf{x})+f(\\mathbf{x})^2]\\]\n用 \\(\\overline{g}(\\mathbf{x})\\) 来表示 \\(E_{\\mathcal{D}}[(g^{(\\mathcal{D})}(\\mathbf{x})]\\)，则\n\\[E_{\\mathcal{D}}[{E_{out}(g^{(\\mathcal{D})})}] = E_{\\mathbf{x}}[E_{\\mathcal{D}}[(g^{(\\mathcal{D})}(\\mathbf{x})^2] - 2\\overline{g}(\\mathbf{x})f(\\mathbf{x})+f(\\mathbf{x})^2] = E_{\\mathbf{x}}[E_{\\mathcal{D}}[(g^{(\\mathcal{D})}(\\mathbf{x})-\\overline{g}(\\mathbf{x}))^2] + (\\overline{g}(\\mathbf{x})-f(\\mathbf{x}))^2]\\]\n\\(bias(\\mathbf{x}) = (\\overline{g}(\\mathbf{x})-f(\\mathbf{x}))^2\\) 表示我们使用不同的样本集学习而得到的平均假设与目标函数的偏差。由于 \\(\\overline{g}(\\mathbf{x})\\) 不受样本集的限制，因此偏差值仅受学习模型本身的限制。\n\\(var(\\mathbf{x}) = E_{\\mathcal{D}}[(g^{(\\mathcal{D})}(\\mathbf{x})-\\overline{g}(\\mathbf{x}))^2]\\) 表示基于不同样本集得到的假设与平均假设之间的差异程度。\n综上，我们将样本外误差分解为：\\[E_{\\mathcal{D}}[{E_{out}(g^{(\\mathcal{D})})}] = E_{\\mathbf{x}}[bias(\\mathbf{x})+var(\\mathbf{x})] = bias+var\\]我们的推导过程忽略了数据的噪声，虽然噪声是不可避免的，但我们关心的主要是偏差和方差。\n使用偏差方差分解方法来看待近似-泛化权衡\n偏差方差分解只是一个理论观点，在实际问题中，我们没有产生 \\(\\overline{g}(\\mathbf{x})\\) 的多个样本集。我们只有一个样本集，因此较简单的模型（\\(\\mathcal{H}\\) 较简单）可能会产生一个较小的样本外误差。但是，随着 \\(N\\) 的增加， \\(var\\) 会逐渐减小，此时 \\(bias\\) 是 \\(E_{out}\\) 的主要组成，较复杂的模型（\\(\\mathcal{H}\\) 较复杂）的表现会更好。\n与VC分析的一些差异\n在VC分析中， \\(E_{out}\\) 被认为是 \\(E_{in}\\) 和 以 \\(\\Omega(N,\\mathcal{H},\\delta)\\) 为上界的泛化误差的和。在偏差方差分析里中，\\(E_{out}\\) 被看作是偏差和方差的和。不过，随着样本数的增多，不管是泛化误差还是方差，都在降低。\nVC分析独立于学习算法 \\(\\mathcal{A}\\)，而偏差方差分析中，假设空间 \\(\\mathcal{H}\\) 和 \\(\\mathcal{A}\\) 都对结果产生影响。相同的 \\(\\mathcal{H}\\) 下，不同的 \\(\\mathcal{A}\\) 将产生不同的 \\(g^{(\\mathcal{D})}\\)，最终产生不同的偏差和方差。\nVC分析要求最小化样本集的0-1误差（即 \\(E_{in}\\)）。而尽管偏差方差分析是基于平方误差计算，但是学习算法并不一定是采取最小化平方误差的策略来产生 \\(g^{(\\mathcal{D})}\\)。不过，一旦产生 \\(g^{(\\mathcal{D})}\\)，我们就用平方误差法来计算 \\(g^{(\\mathcal{D})}\\) 的偏差和方差。\n三、补充：误差和噪声\n误差测量\n总体误差：\\(E(h,f)\\)，每一个点的误差：\\(e(h(\\mathbf{x}),f(\\mathbf{x}))\\)。\\(E(h,f)\\)可以是每一个点的误差和的平均值，也可以是用户自定义的函数。在之前的讨论中，\\[E_{in}(h) = \\frac{1}{N} \\sum_{i=1}^{N}e(h(\\mathbf{x}_i),f(\\mathbf{x}_i))\\]\\[E_{out}(h) = E_{\\mathbf{x}}[e(h(\\mathbf{x}),f(\\mathbf{x}))]\\]有两种类型的误差：错误接受（false accept）和错误拒绝（false reject）。误差测量的方式应该取决于我们设计的系统的用途。我们根据FA和FR两者不同错误成本（也可以称作权重），来考虑不同的算法设计策略。\n噪声目标\n目标函数不一定是一个函数，数据总是在存在噪声的情况下生成。因此，我们不使用函数 \\(y =f(\\mathbf{x})\\)，而是选择概率分布 \\(P\\{y|\\mathbf{x}\\}\\)。即 \\((\\mathbf{x},y)\\) 由联合概率分布 \\(P\\{y|\\mathbf{x}\\}P\\{\\mathbf{x}\\}\\) 独立生成。\n此时，噪声目标就等于确定性目标 \\(f(\\mathbf{x})\\) 加上噪声部分 \\(y-f(\\mathbf{x})\\)。通常来说，我们对学习可行性的分析，适用于噪声目标。\n后记\n本文是笔者在学习加州理工学院公开课-机器学习与数据挖掘时的一些笔记。\n后来阅读了配套教材《Learning From Data》，将该书所介绍的学习可行性的推导证明进行了整理。"}
{"content2":"1. Classification\n这篇文章我们来讨论分类问题（classification problems），也就是说你想预测的变量 y 是一个离散的值。我们会使用逻辑回归算法来解决分类问题。\n之前的文章中，我们讨论的垃圾邮件分类实际上就是一个分类问题。类似的例子还有很多，例如一个在线交易网站判断一次交易是否带有欺诈性（有些人可以使用偷来的信用卡，你懂的）。再如，之前判断一个肿瘤是良性的还是恶性的，也是一个分类问题。\n在以上的这些例子中，我们想预测的是一个二值的变量，或者为0，或者为1；或者是一封垃圾邮件，或者不是；或者是带有欺诈性的交易，或者不是；或者是一个恶性肿瘤，或者不是。\n我们可以将因变量（dependant variable）可能属于的两个类分别称为负向类（negative class）和正向类（positive class）。可以使用0来代表负向类，1来代表正向类。\n现在，我们的分类问题仅仅局限在两类上：0或者1。之后我们会讨论多分类问题，也就是说，变量 y 可以取多个值，例如0，1，2，3。\n那么，我们如何来解决一个分类问题呢？来看以下例子：\n现在有这样一个分类任务，需要根据肿瘤大小来判断肿瘤的良性与否。训练集如上图所示，横轴代表肿瘤大小，纵轴表示肿瘤的良性与否，注意，纵轴只有两个取值，1（代表恶性肿瘤）和0（代表良性肿瘤）。\n通过之前的博文，我们已经知道对于以上数据集使用线性回归来处理，实际上就是用一条直线去拟合这些数据。因此，你得到的 Hypothesis 可能如下：\n那么，如果你想做出预测，一种可行的方式是如下：\n从以上这个例子来看，似乎线性回归也能很好的解决分类问题。现在，我们对以上问题稍作一些改动。\n将横轴向右扩展，并且增加一个训练样本，如下：\n此时，我们使用线性回归，会得到一条新的直线：\n此时，我们再用0.5作为阈值来预测肿瘤的良性与否，就不合适了。\n2. Hypothesis Representation\n3. Decision boundary\n强调一下，决策边界不是训练集的属性，而是假设本身及其参数的属性。只要我们给定了参数向量θ，决策边界就确定了。我们不是用训练集来定义的决策边界，我们用训练集来拟合参数θ，以后我们将谈论如何做到这一点。但是，一旦你有参数θ它就确定了决策边界。\n4. Cost function\n1\n现在我们来讨论如何拟合逻辑回归中模型的参数θ。\n具体来说，\n我们需要定义optimization objective 或者 cost function 来拟合参数θ\n，这便是监督学习问题中的逻辑回归模型的拟合问题。\n如上图所示，我们有一个训练集，里面有m个训练样本，同之前一样，我们的每个样本使用n+1维的特征向量表示（x0 = 1）。并且由于是分类问题，我们训练集中的所有y，取值不是0就是1。假设函数的参数即为θ。那么，对于这个给定的训练集，我们如何拟合参数θ（或者说是选择参数θ）？\n之前，我们使用线性回归模型来拟合假说参数θ时，使用了如下的代价函数，我们稍作改变，将原先的1/2m中的原先的1/2放到了求和符号里面去了。\n现在我们使用另一种方式，来书写代价函数：\n现在，我们能更清楚的看到代价函数是这个Cost函数（代价项）在训练集范围上的求和，再求均值（乘以1/m）。\n我们稍微简化一下这个式子，去掉这些上标会显得方便一些，所以Cost函数直接定义为：\n对这个代价项（Cost函数）的理解是这样的：y我所期望的值，通过学习算法如果想要达到这个值，那么假设h(x)所需要付出的代价即为这个代价项。这个希望的预测值是h(x)，而实际值则是y，干脆，全部去掉那些上标好了。\n显然，在线性回归中，代价项（Cost函数）会被定义为：1/2乘以预测值h和实际值观测的结果y的差的平方。这个代价值可以很好地用在线性回归里\n，但是对于逻辑回归却是不合适的。\n2\n如果我们可以最小化代价函数J(θ)中的代价项（Cost函数），那么我们的确可以使用该代价项。但实际上，如果我们使用该代价项，那么代价函数J(θ)会变成关于参数θ的非凸函数。Why？\n对于逻辑回归来说，这里的h函数是非线性的：\n可以说：\n是一个很复杂的非线性函数，因此如果用h函数来构造我们在线性回归中所使用的代价项（Cost函数），接着再用该代价项来构造代价函数J(θ)。\n那么J(θ)可能是一个这样的函数，有很多局部最优值：\n实际上，这就是一个非凸函数。\n不难发现，如果你把梯度下降法用在一个这样的函数上的话，我们并不能保证它会收敛到全局最小值。\n显然，我们希望我们的代价函数J(θ)是一个凸函数，也就是一个单弓形函数，如下图所示：\n如果对它使用梯度下降法，那么我们可以保证梯度下降法会收敛到该函数的全局最小值。\n因此我们在逻辑回归中使用这个代价项（Cost函数）的问题在于非线性的sigmoid函数的出现导致J(θ)成为一个非凸函数。\n3\n我们需要做的是，另外找一个本身是凸函数的代价项（Cost函数），可以让我们使用类似于梯度下降的算法来找到一个全局最小值。以下就是一个我们将要在逻辑回归中使用的代价项（Cost函数）：\n5. Simplified cost function and gradient descent\n注意，此时θ是变量。我们的目标就是找出使J(θ)最小的θ值。"}
{"content2":"引言：随着AlphaGo战胜李世石。人工智能和深度学习这些概念已经成为一个很火的话题。\n人工智能、机器学习与深度学习这几个关键词时常出如今媒体新闻中，并错误地被觉得是等同的概念。本文将介绍人工智能、机器学习以及深度学习的概念。并着重解析它们之间的关系。本文将从不同领域须要解决的问题入手。依次介绍这些领域的基本概念以及解决领域内问题的主要思路。\n本文选自《Tensorflow：实战Google深度学习框架》。\n从计算机发明之初，人们就希望它能够帮助甚至取代人类完毕反复性劳作。利用巨大的存储空间和超高的运算速度，计算机已经能够很轻易地完毕一些对于人类很困难，但对计算机相对简单的问题。比方，统计一本书中不同单词出现的次数。存储一个图书馆中全部的藏书。或是计算很复杂的数学公式，都能够轻松通过计算机解决。然而。一些人类通过直觉能够很快解决的问题，眼下却很难通过计算机解决。\n这些问题包含自然语言理解、图像识别、语音识别。等等。而它们就是人工智能须要解决的问题。\n计算机要像人类一样完毕很多其它智能的工作，须要掌握关于这个世界海量的知识。比方要实现汽车自己主动驾驶。计算机至少须要可以推断哪里是路，哪里是障碍物。这个对人类很直观的东西，但对计算机却是相当困难的。\n路有水泥的、沥青的，也有石子的甚至土路。这些不同材质铺成的路在计算机看来差距很大。怎样让计算机掌握这些人类看起来很直观的常识。对于人工智能的发展是一个巨大的挑战。\n很多早期的人工智能系统仅仅能成功应用于相对特定的环境（specific domain），在这些特定环境下，计算机须要了解的知识很easy被严格而且完整地定义。比如。IBM的深蓝（Deep Blue）在1997年打败了国际象棋冠军卡斯帕罗夫。设计出下象棋软件是人工智能史上的重大成就。但其主要挑战不在于让计算机掌握国际象棋中的规则。\n国际象棋是一个特定的环境，在这个环境中，计算机仅仅须要了解每个棋子规定的行动范围和行动方法就可以。\n尽管计算机早在1997年就能够击败国际象棋的世界冠军，可是直到20年后的今天，让计算机实现大部分成年人都能够完毕的汽车驾驶却仍然依然十分困难。\n为了使计算机很多其它地掌握开放环境（open domain）下的知识，研究人员进行了非常多尝试。当中一个影响力非常大的领域是知识图库（Ontology）。WordNet是在开放环境中建立的一个较大且有影响力的知识图库。WordNet是由普林斯顿大学（Princeton University）的George Armitage Miller教授和Christiane Fellbaum教授带领开发的。它将155287个单词整理为了117659个近义词集（synsets）。基于这些近义词集，WordNet进一步定义了近义词集之间的关系。\n比方同义词集“狗”属于同义词集“犬科动物”。他们之间存在种属关系（hypernyms/hyponyms）。除了WordNet，也有不少研究人员尝试将Wikipedia中的知识整理成知识图库。谷歌的知识图库就是基于Wikipedia创建的。\n尽管使用知识图库能够让计算机非常好地掌握人工定义的知识。但建立知识图库一方面须要花费大量的人力物力，还有一方面能够通过知识图库方式明白定义的知识有限。不是全部的知识都能够明白地定义成计算机能够理解的固定格式。非常大一部分无法明白定义的知识，就是人类的经验。\n比方我们须要推断一封邮件是否为垃圾邮件。会综合考虑邮件发出的地址、邮件的标题、邮件的内容以及邮件收件人的长度。等等。这是收到无数垃圾邮件骚扰之后总结出来的经验。这个经验非常难以固定的方式表达出来。并且不同人对垃圾邮件的推断也会不一样。怎样让计算机能够跟人类一样从历史的经验中获取新的知识呢？这就是机器学习须要解决的问题。\n卡内基梅隆大学（Carnegie Mellon University）的Tom Michael Mitchell教授在1997年出版的书籍Machine Learning中对机器学习进行过很专业的定义，这个定义在学术界内被多次引用。在这本书中对机器学习的定义为“假设一个程序能够在任务T上。随着经验E的添加。效果P也能够随之添加。则称这个程序能够从经验中学习”。通过垃圾邮件分类的问题来解释机器学习的定义。在垃圾邮件分类问题中。“一个程序”指的是须要用到的机器学习算法。比方逻辑回归算法。“任务T”是指区分垃圾邮件的任务；“经验E”为已经区分过是否为垃圾邮件的历史邮件，在监督式机器学习问题中。这也被称之为训练数据；“效果P”为机器学习算法在区分是否为垃圾邮件任务上的正确率。\n在使用逻辑回归算法解决垃圾邮件分类问题时。会先从每一封邮件中抽取对分类结果可能有影响的因素，比方说上文提到的发邮件的地址、邮件的标题及收件人的长度。等等。每个因素被称之为一个特征（feature）。逻辑回归算法能够从训练数据中计算出每个特征和预測结果的相关度。比方在垃圾邮件分类问题中，可能会发现假设一个邮件的收件人越多。那么邮件为垃圾邮件的概率也就越高。在对一封未知的邮件做推断时，逻辑回归算法会依据从这封邮件中抽取得到的每个特征以及这些特征和垃圾邮件的相关度来推断这封邮件是否为垃圾邮件。\n在大部分情况下，在训练数据达到一定数量之前。越多的训练数据能够使逻辑回归算法对未知邮件做出的推断越精准。也就是说逻辑回归算法能够依据训练数据（经验E）提高在垃圾邮件分类问题（任务T）上的正确率（效果P）。之所以说在大部分情况下，是由于逻辑回归算法的效果除了依赖于训练数据。也依赖于从数据中提取的特征。如果从邮件中抽取的特征仅仅有邮件发送的时间。那么即使有再多的训练数据，逻辑回归算法也无法非常好地利用。\n这是由于邮件发送的时间和邮件是否为垃圾邮件之间的关联不大。而逻辑回归算法无法从数据中习得更好的特征表达。这也是非常多传统机器学习算法的一个共同的问题。\n类似从邮件中提取特征。怎样数字化地表达现实世界中的实体，一直是计算机科学中一个很重要问题。假设将图书馆中的图书名称储存为结构化的数据，比方储存在Excel表格中，那么能够很easy地通过书名查询一本书是否在图书馆中。假设图书的书名都是存在非结构化的图片中，那么要完毕书名查找任务的难度将大大添加。\n类似的道理。怎样从实体中提取特征，对于许多传统机器学习算法的性能有巨大影响。我们看一个简单的样例。\n不同的数据表达对使用直线划分不同颜色结点的难度影响\n假设通过笛卡尔坐标系（cartesian coordinates）来表示数据。那么不同颜色的结点无法被一条直线划分。假设将这些点映射到极角坐标系（polar coordinates），那么使用直线划分就非常easy了。\n相同的数据使用不同的表达方式会极大地影响解决这个问题的难度。一旦攻克了数据表达和特征提取，非常多人工智能任务也就攻克了90%。\n然而，对很多机器学习问题来说。特征提取不是一件简单的事情。在一些复杂问题上，要通过人工的方式设计有效的特征集合，须要非常多的时间和精力。有时甚至须要整个领域数十年的研究投入。\n比如，如果想从非常多照片中识别汽车。如今已知的是汽车有轮子。所以希望在图片中抽取“图片中是否出现了轮子”这个特征。但实际上。要从图片的像素中描写叙述一个轮子的模式是非常难的。\n尽管车轮的形状非常easy，但在实际图片中，车轮上可能会有来自车身的阴影、金属车轴的反光，周围物品也可能会部分遮挡车轮。\n实际图片中各种不确定的因素让我们非常难直接抽取这种特征。\n既然人工的方式无法非常好地抽取实体中的特征。那么是否有自己主动的方式呢？答案是肯定的。深度学习解决的核心问题之中的一个就是自己主动地将简单的特征组合成更加复杂的特征。并使用这些组合特征解决这个问题。深度学习是机器学习的一个分支，它除了能够学习特征和任务之间的关联以外。还能自己主动从简单特征中提取更加复杂的特征。下图展示了深度学习和传统机器学习在流程上的差异。\n传统机器学习和深度学习流程对照\n如图所看到的，深度学习算法能够从数据中学习更加复杂的特征表达，使得最后一步权重学习变得更加简单且有效。\n深度学习在图像分类问题上的算法流程例子\n在上图中，展示了通过深度学习解决图像分类问题的详细例子。深度学习能够一层一层地将简单特征逐步转化成更加复杂的特征，从而使得不同类别的图像更加可分。\n比方图中展示了深度学习算法能够从图像的像素特征中逐渐组合出线条、边、角、简单形状、复杂形状等更加有效的复杂特征。\n早期的深度学习受到了神经科学的启示。它们之间有非常密切的联系。科学家们在神经科学上的发现使得我们相信深度学习能够胜任非常多人工智能的任务。神经科学家发现。假设将小白鼠的视觉神经连接到听觉中枢，一段时间之后小鼠能够习得使用听觉中枢“看”世界。这说明尽管哺乳动物大脑分为了非常多区域。但这些区域的学习机制却是相似的。在这一假想得到验证之前，机器学习的研究者们一般会为不同的任务设计不同的算法。并且直到今天。学术机构的机器学习领域也被分为了自然语言处理、计算机视觉和语音识别等不同的实验室。由于深度学习的通用性，深度学习的研究者往往能够跨越多个研究方向甚至同一时候活跃于全部的研究方向。\n尽管深度学习领域的研究人员相比其它机器学习领域很多其它地受到了大脑工作原理的启示，并且媒体界也常常强调深度学习算法和大脑工作原理的相似性。但现代深度学习的发展并不拘泥于模拟人脑神经元和人脑的工作机理。模拟人类大脑也不再是深度学习研究的主导方向。我们不应该觉得深度学习是在试图模仿人类大脑。眼下科学家对人类大脑学习机制的理解还不足以为当下的深度学习模型提供指导。\n现代的深度学习已经超越了神经科学观点，它能够更广泛地适用于各种并非由神经网络启示而来的机器学习框架。\n值得注意的是，有一个领域的研究者试图从算法层理解大脑的工作机制。它不同于深度学习的领域。被称为“计算神经学”（computational neuroscience）。深度学习领域主要关注怎样搭建智能的计算机系统，解决人工智能中遇到的问题。计算神经学则主要关注怎样建立更准确的模型来模拟人类大脑的工作。\n总的来说。人工智能、机器学习和深度学习是很相关的几个领域。\n下图总结了它们之间的关系。\n人工智能、机器学习以及深度学习之间的关系图\n人工智能是一类非常广泛的问题，机器学习是解决这类问题的一个重要手段。\n深度学习则是机器学习的一个分支。\n在非常多人工智能问题上。深度学习的方法突破了传统机器学习方法的瓶颈，推动了人工智能领域的发展。\n本节部分内容參见：Goodfellow I, Bengio Y, Courville A. Deep learning [M]. The MIT Press,2016.\nMitchell T M, Carbonell J G, Michalski R S. Machine Learning [M]. McGraw-Hill, 2003.\n本文选自《Tensorflow：实战Google深度学习框架》，点此链接可在博文视点官网查看此书。\n想及时获得很多其它精彩文章，可在微信中搜索“博文视点”或者扫描下方二维码并关注。"}
{"content2":"《机器学习》这本书算是很好的一本了解机器学习知识的一本入门书籍吧，是南京大学周志华老师所著的鸿篇大作，很早就听闻周老师大名了，算是国内机器学习领域少数的大牛了吧，刚好研究生做这个方向相关的内容，所以今天买了一本所谓的西瓜书，准备研读，光读书记性不好，边读边做笔记练习印象深刻，接下来我就把自己的学习过程按每章节的内容整理如下：\nDay1 第一章 绪论部分\n本书作者周志华老师通过聊天的口吻开篇，以日常生活的小案例和场景，向读者介绍什么是机器学习，以及什么是学习算法。在这一章介绍了很多机器学习相关的术语概念。\n首先，要做学习，先得有数据，我们要学习的对象记录收集起来组成的集合叫做一个“数据集（data set）”，把里面记录、对象的描述，称为一个“示例（instance）”或者“样本（sample）”，反应集合内事件或对象在某方面的表现或性质的事项，我们把它称为“属性（attribute）”或“特征（feature）”，属性张成的空间称为“属性空间（attribute space）”、 “样本空间（sample space）” 或者“输入空间”，由于空间中的每一个点对应一个坐标向量，因此我们把一个示例称为一个“特征向量（feature vector）”，这里的属性数量就是我们说的样本的“维数（dimensionality）”\n上面得到了数据了，我们要从数据中学得模型的过程叫“学习（learning）”或者“训练（training）”，在这个过程执行某个学习算法来完成，训练的过程使用的数据称为“训练数据（集）（training data）”，这其中的每一个小样本叫一个“训练样本（training sample）”，训练样本构成的集合组成的集合叫做“训练集training set”，训练得到的模型对应数据的某种潜在的规律，把这种结果称为“假设（hypothesis）”，我们要用学习的结果来“预测（prediction）”，用学得模型进行预测的过程称为“测试（testing）”，被预测的样本叫“测试样本（testing sample）”。\n我们要预测的是离散值，这类学习任务称为“分类（classification）”，要预测的是连续值，把这类学习任务称为“回归（regression）”，当然我们也可以对数据做“聚类（clustering）”，即把训练集中的对象分成若干组，每个组称为一个“簇（cluster）”。\n我们根据训练数据是否拥有标记信息，学习任务可大致分为两大类：“监督学习（supervised learning）”和“无监督学习（unsupervised learning）”，分类和回归是前者的代表，聚类是后者的代表。\n学得的模型适用于新样本的能力，称为“泛化（generation）”。\n通常，我们假设样本空间中全体样本服从一个未知“分布（distribution）”，我们获得的每一个样本都是独立从这分布上采样得到的，即“独立同分布（independent and identically distributed简称i.i.d.）”。\n归纳（induction）和演绎（deduction）是科学推理的两大基本手段，前者是从特殊到一般的“泛化（generation）”过程，后者是一般到特殊的“特化（specialization）”过程，如数学上由数学公理推出与之相洽的定理，这是演绎过程，而“从样本中学习”是一个归纳过程，叫做“归纳学习（inductive learning）”。\n归纳学习中有归纳偏好，这里遵循奥卡姆剃刀原则。\n发展历程：机器学习是人工智能（artificial intelligence）研究发展到一定阶段的必然产物。二十世纪五十年代到七十年代，人工智能研究处于“推理期”，那时的人们认为只要能赋予机器逻辑推理能力，机器就能拥有智能。二十世纪七十年代中期开始，人工智能研究进入“知识期”，这一时期大量的专家系统问世。二十世纪八十年代是机器学习成为一个独立的学科领域，各种机器学习技术百花初绽的时期。二十世纪九十年代中期，“统计学习（statistical learning）”闪亮登场并迅速占据主流舞台，代表技术是支持向量机（Support Vector Machine，简称SVM）以及更一般的“核方法（kernel methods）”。二十一世纪初，连接主义学习又卷土从来（五十年代中后期基于神经网络的“连接主义”），掀起了以“深度学习”为名的热潮，所谓深度学习，狭义地说就是“很多层”的神经网络\n现在，机器学习已经发展成为一个相当大的学科领域，当今算力的提升和大数据的加持，逐步把机器学习推向高潮。\n（第一章笔记到此，继续学习后续章节）"}
{"content2":"版权声明：\n本文由LeftNotEasy所有，发布于http://leftnoteasy.cnblogs.com。如果转载，请注明出处，在未经作者同意下将本文用于商业用途，将追究其法律责任。如果有问题，请联系作者 wheeleast@gmail.com\n前言：\n距离上次发文章，也快有半个月的时间了，这半个月的时间里又在学习机器学习的道路上摸索着前进，积累了一点心得，以后会慢慢的写写这些心得。写文章是促进自己对知识认识的一个好方法，看书的时候往往不是非常细，所以有些公式、知识点什么的就一带而过，里面的一些具体意义就不容易理解了。而写文章，特别是写科普性的文章，需要对里面的具体意义弄明白，甚至还要能举出更生动的例子，这是一个挑战。为了写文章，往往需要把之前自己认为看明白的内容重新理解一下。\n机器学习可不是一个完全的技术性的东西，之前和部门老大在outing的时候一直在聊这个问题，机器学习绝对不是一个一个孤立的算法堆砌起来的，想要像看《算法导论》这样看机器学习是个不可取的方法，机器学习里面有几个东西一直贯穿全书，比如说数据的分布、最大似然（以及求极值的几个方法，不过这个比较数学了），偏差、方差的权衡，还有特征选择，模型选择，混合模型等等知识，这些知识像砖头、水泥一样构成了机器学习里面的一个个的算法。想要真正学好这些算法，一定要静下心来将这些基础知识弄清楚，才能够真正理解、实现好各种机器学习算法。\n今天的主题是线性回归，也会提一下偏差、方差的均衡这个主题。\n线性回归定义：\n在上一个主题中，也是一个与回归相关的，不过上一节更侧重于梯度这个概念，这一节更侧重于回归本身与偏差和方差的概念。\n回归最简单的定义是，给出一个点集D，用一个函数去拟合这个点集，并且使得点集与拟合函数间的误差最小。\n上图所示，给出一个点集(x,y), 需要用一个函数去拟合这个点集，蓝色的点是点集中的点，而红色的曲线是函数的曲线，第一张图是一个最简单的模型，对应的函数为y = f(x) = ax + b，这个就是一个线性函数，\n第二张图是二次曲线，对应的函数是y = f(x) = ax^2 + b。\n第三张图我也不知道是什么函数，瞎画的。\n第四张图可以认为是一个N次曲线，N = M - 1，M是点集中点的个数，有一个定理是，对于给定的M个点，我们可以用一个M - 1次的函数去完美的经过这个点集。\n真正的线性回归，不仅会考虑使得曲线与给定点集的拟合程度最好，还会考虑模型最简单，这个话题我们将在本章后面的偏差、方差的权衡中深入的说，另外这个话题还可以参考我之前的一篇文章：贝叶斯、概率分布与机器学习，里面对模型复杂度的问题也进行了一些讨论。\n线性回归(linear regression)，并非是指的线性函数，也就是\n（为了方便起见，以后向量我就不在上面加箭头了）\nx0,x1…表示一个点不同的维度，比如说上一节中提到的，房子的价钱是由包括面积、房间的个数、房屋的朝向等等因素去决定的。而是用广义的线性函数：\nwj是系数，w就是这个系数组成的向量，它影响着不同维度的Φj(x)在回归函数中的影响度，比如说对于房屋的售价来说，房间朝向的w一定比房间面积的w更小。Φ(x)是可以换成不同的函数，不一定要求Φ(x)=x，这样的模型我们认为是广义线性模型。\n最小二乘法与最大似然：\n这个话题在此处有一个很详细的讨论，我这里主要谈谈这个问题的理解。最小二乘法是线性回归中一个最简单的方法，它的推导有一个假设，就是回归函数的估计值与真实值间的误差假设是一个高斯分布。这个用公式来表示是下面的样子： ，y(x,w)就是给定了w系数向量下的回归函数的估计值，而t就是真实值了，ε表示误差。我们可以接下来推出下面的式子：\n这是一个简单的条件概率表达式，表示在给定了x，w，β的情况下，得到真实值t的概率，由于ε服从高斯分布，则从估计值到真实值间的概率也是高斯分布的，看起来像下面的样子：\n贝叶斯、概率分布与机器学习这篇文章中对分布影响结果这个话题讨论比较多，可以回过头去看看，由于最小二乘法有这样一个假设，则会导致，如果我们给出的估计函数y(x,w)与真实值t不是高斯分布的，甚至是一个差距很大的分布，那么算出来的模型一定是不正确的，当给定一个新的点x’想要求出一个估计值y’，与真实值t’可能就非常的远了。\n概率分布是一个可爱又可恨的东西，当我们能够准确的预知某些数据的分布时，那我们可以做出一个非常精确的模型去预测它，但是在大多数真实的应用场景中，数据的分布是不可知的，我们也很难去用一个分布、甚至多个分布的混合去表示数据的真实分布，比如说给定了1亿篇网页，希望用一个现有的分布（比如说混合高斯分布）去匹配里面词频的分布，是不可能的。在这种情况下，我们只能得到词的出现概率，比如p(的)的概率是0.5，也就是一个网页有1/2的概率出现“的”。如果一个算法，是对里面的分布进行了某些假设，那么可能这个算法在真实的应用中就会表现欠佳。最小二乘法对于类似的一个复杂问题，就很无力了\n偏差、方差的权衡(trade-off)：\n偏差(bias)和方差(variance)是统计学的概念，刚进公司的时候，看到每个人的嘴里随时蹦出这两个词，觉得很可怕。首先得明确的，方差是多个模型间的比较，而非对一个模型而言的，对于单独的一个模型，比如说:\n这样的一个给定了具体系数的估计函数，是不能说f(x)的方差是多少。而偏差可以是单个数据集中的，也可以是多个数据集中的，这个得看具体的定义。\n方差和偏差一般来说，是从同一个数据集中，用科学的采样方法得到几个不同的子数据集，用这些子数据集得到的模型，就可以谈他们的方差和偏差的情况了。方差和偏差的变化一般是和模型的复杂程度成正比的，就像本文一开始那四张小图片一样，当我们一味的追求模型精确匹配，则可能会导致同一组数据训练出不同的模型，它们之间的差异非常大。这就叫做方差，不过他们的偏差就很小了，如下图所示：\n上图的蓝色和绿色的点是表示一个数据集中采样得到的不同的子数据集，我们有两个N次的曲线去拟合这些点集，则可以得到两条曲线（蓝色和深绿色），它们的差异就很大，但是他们本是由同一个数据集生成的，这个就是模型复杂造成的方差大。模型越复杂，偏差就越小，而模型越简单，偏差就越大，方差和偏差是按下面的方式进行变化的:\n当方差和偏差加起来最优的点，就是我们最佳的模型复杂度。\n用一个很通俗的例子来说，现在咱们国家一味的追求GDP，GDP就像是模型的偏差，国家希望现有的GDP和目标的GDP差异尽量的小，但是其中使用了很多复杂的手段，比如说倒卖土地、强拆等等，这个增加了模型的复杂度，也会使得偏差（居民的收入分配）变大，穷的人越穷(被赶出城市的人与进入城市买不起房的人），富的人越富（倒卖土地的人与卖房子的人）。其实本来模型不需要这么复杂，能够让居民的收入分配与国家的发展取得一个平衡的模型是最好的模型。\n最后还是用数学的语言来描述一下偏差和方差：\nE(L)是损失函数，h(x)表示真实值的平均，第一部分是与y（模型的估计函数）有关的，这个部分是由于我们选择不同的估计函数（模型）带来的差异，而第二部分是与y无关的，这个部分可以认为是模型的固有噪声。\n对于上面公式的第一部分，我们可以化成下面的形式：\n这个部分在PRML的1.5.5推导，前一半是表示偏差，而后一半表示方差，我们可以得出：损失函数=偏差^2+方差+固有噪音。\n下图也来自PRML：\n这是一个曲线拟合的问题，对同分布的不同的数据集进行了多次的曲线拟合，左边表示方差，右边表示偏差，绿色是真实值函数。ln lambda表示模型的复杂程度，这个值越小，表示模型的复杂程度越高，在第一行，大家的复杂度都很低（每个人都很穷）的时候，方差是很小的，但是偏差同样很小（国家也很穷），但是到了最后一幅图，我们可以得到，每个人的复杂程度都很高的情况下，不同的函数就有着天壤之别了（贫富差异大），但是偏差就很小了（国家很富有）。\n预告：\n接下来准备谈谈线性分类的一些问题，敬请关注：）"}
{"content2":"转载请注明出处：http://www.cnblogs.com/ymingjingr/p/4271742.html\n目录\n机器学习基石笔记1——在何时可以使用机器学习(1)\n机器学习基石笔记2——在何时可以使用机器学习(2)\n机器学习基石笔记3——在何时可以使用机器学习(3)(修改版)\n机器学习基石笔记4——在何时可以使用机器学习（4）\n机器学习基石笔记5——为什么机器可以学习（1）\n机器学习基石笔记6——为什么机器可以学习（2）\n机器学习基石笔记7——为什么机器可以学习（3）\n机器学习基石笔记8——为什么机器可以学习（4）\n机器学习基石笔记9——机器可以怎样学习（1）\n机器学习基石笔记10——机器可以怎样学习（2）\n机器学习基石笔记11——机器可以怎样学习（3）\n机器学习基石笔记12——机器可以怎样学习（4）\n机器学习基石笔记13——机器可以怎样学得更好（1）\n机器学习基石笔记14——机器可以怎样学得更好（2）\n机器学习基石笔记15——机器可以怎样学得更好（3）\n机器学习基石笔记16——机器可以怎样学得更好（4）\n十三、Hazard of Overfitting\n过拟合的危害\n13.1 What is Overfitting?\n什么是过拟合？\n假设一个输入空间X是一维的，样本点数量为5的回归问题，其目标函数为二次函数，但是标记 包含噪音，因此标记表示为 。使用四次多项式转换结合线性回归求解该问题的权值向量w，得出 的唯一解，如-1所示。\n-1 二次的目标函数与四次多项式的最优假设函数\n从图中不难得出该假设函数的 很大，因此四次的多项式函数有很差的泛化能力。\n依据-2中分析此问题，在VC维 变大时，变小，但变大，此种情况称作过拟合（overfitting），意思为在训练样本上拟合做的很好，非常小，但是过度了，使得泛化能力变差，导致很大；当然还有一种情况是VC维 变小时，出现了越来越小，同时也越来越小，此种情况称为欠拟合（underfitting）。其中如何解决欠拟合的问题已经在12.4节作了介绍， 不断地提高多项式次数，使得VC维提高，达到拟合的效果，但过拟合的问题更为复杂，以后的章节会更深入的探讨。过拟合和坏的泛化有所不同，过拟合指的是和变化的过程，在变小，变大时称为过拟合；而坏的泛化是在某一点，很小，很大。\n-2 VC维与错误率之间的关系\n或许理解上还是有些困难，用一个类比的方式，便于人理解。将机器学习比作开车，如表13-1所示。\n表13-1\n机器学习\n开车\n过拟合\n出车祸\n使用过度的VC维\n开得太快\n噪音\n颠簸的路面\n数据量的大小\n对路面状况的观察\n其中第3~5行表示构成第2行的原因，即除了VC维度之外噪音和训练数据的大小对过拟合都有影响。\n13.2 The Role of Noise and Data Size\n噪音与数据量所扮演的角色。\n为了更直观的解释产生过拟合的因素，设计两个实验，分别设计两个目标函数，一个10次多项式，另一个50次多项式，前者加上噪音，后者无噪音，生成如-3所示的数据点。\n-3 a) 10次多项式加上噪音生成的训练数据 b) 10次多项式没有噪音生成的训练数据\n使用两种不同的学习模型（二次式假设空间 与10次多项式假设空间 ）分别根据以上两个生成的训练数据进行学习。\n首先，使用两种模型学习13-3a)生成的数据，两种模型产生的最优假设如-4所示，其中绿线表示二次模型学习得到的假设函数g， ，红色表示10次模型学习得到的假设函数g，。\n-4 两种模型学习10次目标函数生成的数据\n这两个最优函数的错误率如表13-2所示，在上，显然二次函数不如10次函数的错误率低，但是在上，10次函数远远的高于2次函数（能够知道是因为已知目标函数，当然在现实中是不可能的），说明在做10次函数生成的训练数据上，使用2次函数模型会得到效果更优的。\n表13-2两种模型从10次目标函数生成数据产生的错误率\n0.050\n0.034\n0.129\n9.00\n继续使用这两种模型对13-3 b)生成的数据进行学习，得到两种假设函数如-5所示。\n-5两种模型学习50次目标函数生成的数据\n这两个最优函数的错误率如表13-3所示，在上，显然二次函数不如10次函数的错误率低，但是在上，10次函数远远的高于2次函数，说明在做50次函数生成的训练数据上，使用2次函数模型会得到效果更优的。\n表13-3两种模型从50次目标函数生成数据产生的错误率\n0.029\n0.00001\n0.120\n7680\n难道在两种情况下，从2次函数到10次函数都是过拟合？答案是对的。为什么二次式模型与目标函数的次数有很大差距反而比10次多项式模型学习能力还好？\n要从学习鸿沟说起（learning curves），二次函数和10次函数的学习鸿沟如-6所示。\n-6 a) 二次函数学习鸿沟 b) 10次函数学习鸿沟\n从图中可以看出，数据量少时，尽管2次假设的比10次函数的大很多，但是2次假设中和的差距比10次假设小的多，因此在样本点不多时，低次假设的学习泛化能力更强，即在灰色区域（样本不多的情况下）中，高次假设函数发生了过拟合。\n上述阐述了在含有噪音的情况下，低次多项式假设比和目标函数同次的多项式假设表现更好，那如何解释在50次多项式函数中也是二次式表现好的现象呢？因为50次目标函数对于不论是2次假设还是10次假设都相当于一种含有噪音的情况（两者都无法做到50次的目标函数，因此相当于含有噪音）。\n13.3 Deterministic Noise\n确定性噪音。\n数据样本由两个部分组成，一是目标函数产生，和在此之上夹杂的噪音。其中假设噪音服从高斯分布，称作高斯噪音，其强度为 ；目标函数使用复杂度 表示，即次多项式函数。\n不难看出过拟合和噪音强度、目标函数复杂度（上节最后说明高次也是一种噪音形式）和训练数据量N都有着密切的关系，以下通过固定某一参数对比其他两个参数的方式，观察每个参数对过拟合的影响，分为 和 。\n为了体现各个参数的影响，通过编写的程序完成一些规定的实验，绘制成具体的图像，便于理解。和上一节相同使用两种学习模型测试，二次式模型和10次多项式模型，其最优假设函数分别表示为 ，，错误率满足，并使用 作为过拟合的衡量。分别固定复杂度和噪音强度得到如-7所示的两幅图。\n-7 a) 固定算法强度时 与N对拟合度的影响 b) 固定噪音强度时与N对拟合度的影响\n-7 a)表示在固定算法强度时 ，噪音强度与样本数据量N对拟合度的影响，图中的颜色表示过拟合程度，深红色的部分表示过拟合，蓝色表示表现好，从图中得知在噪音强度越大与样本数据量N越小时，过拟合越严重，有关高斯噪音产生的噪音又被称作随机噪音（stochastic noise）；-7 b)表示在固定噪音强度时 ，算法强度与样本数据量N对拟合度的影响，从图中得知在算法强度越大且样本数据量N越小时，过拟合越严重，在图的左下角的表现，与-7 a)的表现略有不同，即在算法强度且数据量很小成三角形的区域，造成该现象的原因是此处选用的两个模型是二次式与10次多项式，而在低于10次的目标函数中使用10次多项式模型学习，即产生了13.1节中提过的，过度的VC维使用，有关算法强度产生也相当于产生了噪音，称作确定性噪音（deterministic noise）。\n总结，造成严重过拟合现象的原因有四个：数据量N少，随机噪音高，确定性噪音高，过量的VC维。\n可能确定性噪音比较难于理解，通过-8再做一次简单的解释。\n-8 确定性噪音示意图\n其中蓝色曲线表示目标函数，红色取消表示二次式函数模型学习到的曲线，其中红色曲线的弯曲的形状使用2次函数是不可能模仿的，因此就相当于一种噪音。为什么使用低次函数学习效果却好呢？；这类似于教小孩学习学习简单的问题反而有助于成长。\n13.4 Dealing with Overfitting\n处理过拟合。\n回忆13.1节中的表13-1,，提到了产生过拟合的三种原因，本节提出防止出现过拟合的几种情况，该情况与防止出车祸的应对措施作对比如表13-4所示。\n表13-4 防止过拟合的措施与防止出车祸的措施的对比\n从简单的模型出发\n开慢点\n数据清理/裁剪（data cleaning/pruning）\n更准确的路况信息\n数据提示（data hinting）\n获取更多的路况信息\n正则化（regularization）\n踩刹车\n确认（validation）\n安装仪表盘\n从简单模型出发的措施在前几节中都有体现，不再赘述，本节主要介绍数据清理以及数据提示，而正则化和确认则在后面的章节介绍。\n以手写数字数据为例，介绍数据清理和数据裁剪，观察-9，手写数字为1的使用 表示；手写数字为5的使用 表示。其中在数字1中存在一个数字5的样本点，即图中左上角中的，查看该样本的原图很难看出是数字5，类似这种离不同类别很近，离相同类别很远的样本，可以认为是噪音或者是离群点（outlier）。应对该种情况，有两种措施可用：纠正标识号，即数据清理（data cleaning）的方式处理该情况；删除错误样本，即数据裁剪（data pruning）的方式处理。处理措施很简单，但是发现样本是噪音或离群点却比较困难。\n-9 手写数字的分布情况\n继续介绍数据提示，还是以手写数字集为例，将如-10所示的手写数字集略作修改产生更多的手写数字样本，达到增加数据量N的目的。如将如下手写数字略作旋转（rotating）和平移（shifting），但要注意旋转和平移的幅度都不能太大，如6转180°就成了9。还需注意这种方式产生的虚拟样本（virtual examples），不在符合独立同分布，因此产生的虚拟样本与实际样本差距一定不宜太大。\n-10 手写数字集"}
{"content2":"一、集成学习法\n在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。\n集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。\n集成学习在各个规模的数据集上都有很好的策略。\n数据集大：划分成多个小数据集，学习多个模型进行组合\n数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合\n集合方法可分为两类：\n序列集成方法，其中参与训练的基础学习器按照顺序生成（例如 AdaBoost）。序列方法的原理是利用基础学习器之间的依赖关系。通过对之前训练中错误标记的样本赋值较高的权重，可以提高整体的预测效果。\n并行集成方法，其中参与训练的基础学习器并行生成（例如 Random Forest）。并行方法的原理是利用基础学习器之间的独立性，通过平均可以显著降低错误。\n总结一下，集成学习法的特点：\n①  将多个分类方法聚集在一起，以提高分类的准确率。\n（这些算法可以是不同的算法，也可以是相同的算法。）\n②  集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类\n③  严格来说，集成学习并不算是一种分类器，而是一种分类器结合的方法。\n④  通常一个集成分类器的分类性能会好于单个分类器\n⑤  如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。\n自然地，就产生两个问题：\n1）怎么训练每个算法？\n2）怎么融合每个算法？\n这篇博客介绍一下集成学习的几个方法：Bagging，Boosting以及Stacking。\n1、Bagging（bootstrap aggregating，装袋）\nBagging即套袋法，先说一下bootstrap，bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间，其算法过程如下：\nA）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\nB）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\nC）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）\n为了让更好地理解bagging方法，这里提供一个例子。\nX 表示一维属性，Y 表示类标号（1或-1）测试条件：当x<=k时，y=？；当x>k时，y=？；k为最佳分裂点\n下表为属性x对应的唯一正确的y类别\n现在进行5轮随机抽样，结果如下\n每一轮随机抽样后，都生成一个分类器\n然后再将五轮分类融合\n对比符号和实际类，我们可以发现：在该例子中，Bagging使得准确率可达90%\n由此，总结一下bagging方法：\n①  Bagging通过降低基分类器的方差，改善了泛化误差\n②  其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起\n③  由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例\n常用的集成算法类是随机森林。\n在随机森林中，集成中的每棵树都是由从训练集中抽取的样本（即 bootstrap 样本）构建的。另外，与使用所有特征不同，这里随机选择特征子集，从而进一步达到对树的随机化目的。\n因此，随机森林产生的偏差略有增加，但是由于对相关性较小的树计算平均值，估计方差减小了，导致模型的整体效果更好。\n2、Boosting\n其主要思想是将弱分类器组装成一个强分类器。在PAC（probably approximately correct，概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。\n关于Boosting的两个核心问题：\n1）在每一轮如何改变训练数据的权值或概率分布？\n通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。\n2）通过什么方式来组合弱分类器？\n通过加法模型将弱分类器进行线性组合，比如：\nAdaBoost（Adaptive boosting）算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。\nGBDT（Gradient Boost Decision Tree)，每一次的计算是为了减少上一次的残差，GBDT在残差减少（负梯度）的方向上建立一个新的模型。\n3、Stacking\nStacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。\n如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，然后将输出用于训练第二层分类器。\n二、Bagging，Boosting二者之间的区别\n1、Bagging和Boosting的区别：\n1）样本选择上：\nBagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\nBoosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n2）样例权重：\nBagging：使用均匀取样，每个样例的权重相等\nBoosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n3）预测函数：\nBagging：所有预测函数的权重相等。\nBoosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n4）并行计算：\nBagging：各个预测函数可以并行生成\nBoosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。\n2、决策树与这些算法框架进行结合所得到的新的算法：\n1）Bagging + 决策树 = 随机森林\n2）AdaBoost + 决策树 = 提升树\n3）Gradient Boosting + 决策树 = GBDT\n参考博文：\n【1】集成学习总结 & Stacking方法详解  https://blog.csdn.net/willduan1/article/details/73618677\n【2】Bagging和Boosting 概念及区别  https://www.cnblogs.com/liuwu265/p/4690486.html\n【3】集成学习法之bagging方法和boosting方法 https://blog.csdn.net/qq_30189255/article/details/51532442\n【4】机器学习中的集成学习（Ensemble Learning)  http://baijiahao.baidu.com/s?id=1590266955499942419&wfr=spider&for=pc\n【5】简单易学的机器学习算法——集成方法(Ensemble Method) https://blog.csdn.net/google19890102/article/details/46507387"}
{"content2":"1. Alternating Least Square\nALS(Alternating Least Square)，交替最小二乘法。在机器学习中，特指使用最小二乘法的一种协同推荐算法。如下图所示，u表示用户，v表示商品，用户给商品打分，但是并不是每一个用户都会给每一种商品打分。比如用户u6就没有给商品v3打分，需要我们推断出来，这就是机器学习的任务。\n由于并不是每个用户给每种商品都打了分，可以假设ALS矩阵是低秩的，即一个m*n的矩阵，是由m*k和k*n两个矩阵相乘得到的，其中k<<m,n。\nAm×n=Um×k×Vk×n\n这种假设是合理的，因为用户和商品都包含了一些低维度的隐藏特征，比如我们只要知道某个人喜欢碳酸饮料，就可以推断出他喜欢百世可乐、可口可乐、芬达，而不需要明确指出他喜欢这三种饮料。这里的碳酸饮料就相当于一个隐藏特征。上面的公式中，Um×k表示用户对隐藏特征的偏好，Vk×n表示产品包含隐藏特征的程度。机器学习的任务就是求出Um×k和Vk×n。可知uiTvj是用户i对商品j的偏好，使用Frobenius范数来量化重构U和V产生的误差。由于矩阵中很多地方都是空白的，即用户没有对商品打分，对于这种情况我们就不用计算未知元了，只计算观察到的(用户,商品)集合R。\n这样就将协同推荐问题转换成了一个优化问题。目标函数中U和V相互耦合，这就需要使用交替二乘算法。即先假设U的初始值U(0)，这样就将问题转化成了一个最小二乘问题，可以根据U(0)可以计算出V(0)，再根据V(0)计算出U(1)，这样迭代下去，直到迭代了一定的次数，或者收敛为止。虽然不能保证收敛的全局最优解，但是影响不大。\n2. MLlib的ALS实现\nMLlib的ALS采用了数据分区结构，即将U分解成u1,u2,u3,...um，V分解成v1,v2,v3,...vn，相关的u和v存放在同一个分区，从而减少分区间数据交换的成本。比如通过U计算V时，存储u的分区是P1,P2...，存储v的分区是Q1,Q2...，需要将不同的u发送给不同的Q，存放这个关系的块称作OutBlock；在P中，计算v时需要哪些u，存放这个关系的块称作InBlock。\n比如R中有a12,a13,a15，u1存放在P1，v2,v3存放在Q2，v5存放在Q3，则需要将P1中的u1发送给Q2和Q3，这个信息存储在OutBlock；R中有a12,a32，因此计算v2需要u1和u3，这个信息存储在InBlock。\n直接上代码：\nimport org.apache.log4j.{ Level, Logger } import org.apache.spark.{ SparkConf, SparkContext } import org.apache.spark.mllib.recommendation.ALS import org.apache.spark.mllib.recommendation.Rating /** * Created by Administrator on 2017/7/19. */ object ALSTest01 { def main(args:Array[String]) ={ // 设置运行环境 val conf = new SparkConf().setAppName(\"ALS 01\") .setMaster(\"spark://master:7077\").setJars(Seq(\"E:\\\\Intellij\\\\Projects\\\\MachineLearning\\\\MachineLearning.jar\")) val sc = new SparkContext(conf) Logger.getRootLogger.setLevel(Level.WARN) // 读取样本数据并解析 val dataRDD = sc.textFile(\"hdfs://master:9000/ml/data/test.data\") val ratingRDD = dataRDD.map(_.split(',') match { case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble) }) // 拆分成训练集和测试集 val dataParts = ratingRDD.randomSplit(Array(0.8, 0.2)) val trainingRDD = dataParts(0) val testRDD = dataParts(1) // 建立ALS交替最小二乘算法模型并训练 val rank = 10 val numIterations = 10 val alsModel = ALS.train(trainingRDD, rank, numIterations, 0.01) // 预测 val user_product = trainingRDD.map { case Rating(user, product, rate) => (user, product) } val predictions = alsModel.predict(user_product).map { case Rating(user, product, rate) => ((user, product), rate) } val ratesAndPredictions = trainingRDD.map { case Rating(user, product, rate) => ((user, product), rate) }.join(predictions) val MSE = ratesAndPredictions.map { case ((user, product), (r1, r2)) => val err = (r1 - r2) err * err }.mean() println(\"Mean Squared Error = \" + MSE) println(\"User\" + \"\\t\" + \"Products\" + \"\\t\" + \"Rate\" + \"\\t\" + \"Prediction\") ratesAndPredictions.collect.foreach( rating => { println(rating._1._1 + \"\\t\" + rating._1._2 + \"\\t\" + rating._2._1 + \"\\t\" + rating._2._2) } ) } }\n其中ALS.train()函数的4个参数分别是训练用的数据集，特征数量，迭代次数，和正则因子。\n运行结果：\n可见，预测结果还是非常准确的。"}
{"content2":"主成分分析（principal component analysis）是一种常见的数据降维方法，其目的是在“信息”损失较小的前提下，将高维的数据转换到低维，从而减小计算量。\nPCA的本质就是找一些投影方向，使得数据在这些投影方向上的方差最大，而且这些投影方向是相互正交的。这其实就是找新的正交基的过程，计算原始数据在这些正交基上投影的方差，方差越大，就说明在对应正交基上包含了更多的信息量。后面会证明，原始数据协方差矩阵的特征值越大，对应的方差越大，在对应的特征向量上投影的信息量就越大。反之，如果特征值较小，则说明数据在这些特征向量上投影的信息量很小，可以将小特征值对应方向的数据删除，从而达到了降维的目的。\nPCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。\n因此，关键点就在于：如何找到新的投影方向使得原始数据的“信息量”损失最少？\n1.样本“信息量”的衡量\n样本的“信息量”指的是样本在特征方向上投影的方差。方差越大，则样本在该特征上的差异就越大，因此该特征就越重要。以《机器学习实战》上的图说明，在分类问题里，样本的方差越大，越容易将不同类别的样本区分开。\n图中共有3个类别的数据，很显然，方差越大，越容易分开不同类别的点。样本在X轴上的投影方差较大，在Y轴的投影方差较小。方差最大的方向应该是中间斜向上的方向（图中红线方向）。如果将样本按照中间斜向上的方向进行映射，则只要一维的数据就可以对其进行分类，相比二维的原数据，就相当降了一维。\n在原始数据更多维的情况下，先得到一个数据变换后方差最大的方向，然后选择与第一个方向正交的方向，该方向是方差次大的方向，如此下去，直到变换出与原特征个数相同的新特征或者变换出前N个特征（在这前N个特征包含了数据的绝大部分信息），简而言之，PCA是一个降维的过程，将数据映射到新的特征，新特征是原始特征的线性组合。\n2.计算过程（因为插入公式比较麻烦，就直接采用截图的方式）\n3.python实现\n#coding=utf-8 from numpy import * '''通过方差的百分比来计算将数据降到多少维是比较合适的， 函数传入的参数是特征值和百分比percentage，返回需要降到的维度数num''' def eigValPct(eigVals,percentage): sortArray=sort(eigVals) #使用numpy中的sort()对特征值按照从小到大排序 sortArray=sortArray[-1::-1] #特征值从大到小排序 arraySum=sum(sortArray) #数据全部的方差arraySum tempSum=0 num=0 for i in sortArray: tempSum+=i num+=1 if tempsum>=arraySum*percentage: return num '''pca函数有两个参数，其中dataMat是已经转换成矩阵matrix形式的数据集，列表示特征； 其中的percentage表示取前多少个特征需要达到的方差占比，默认为0.9''' def pca(dataMat,percentage=0.9): meanVals=mean(dataMat,axis=0) #对每一列求平均值，因为协方差的计算中需要减去均值 meanRemoved=dataMat-meanVals covMat=cov(meanRemoved,rowvar=0) #cov()计算方差 eigVals,eigVects=linalg.eig(mat(covMat)) #利用numpy中寻找特征值和特征向量的模块linalg中的eig()方法 k=eigValPct(eigVals,percentage) #要达到方差的百分比percentage，需要前k个向量 eigValInd=argsort(eigVals) #对特征值eigVals从小到大排序 eigValInd=eigValInd[:-(k+1):-1] #从排好序的特征值，从后往前取k个，这样就实现了特征值的从大到小排列 redEigVects=eigVects[:,eigValInd] #返回排序后特征值对应的特征向量redEigVects（主成分） lowDDataMat=meanRemoved*redEigVects #将原始数据投影到主成分上得到新的低维数据lowDDataMat reconMat=(lowDDataMat*redEigVects.T)+meanVals #得到重构数据reconMat return lowDDataMat,reconMat\nReference：\n1. Peter Harrington，《机器学习实战》，人民邮电出版社，2013\n2. http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html (其中有PCA的计算实例)\n3. 张学工，《模式识别》（第三版），清华大学出版社，2010"}
{"content2":"中国人工智能行业正处于一个创新发展时期，对人才的需求也在同步急剧增长。商情数据旗下国内领先的产业研究咨询服务机构中商产业研究院权威发布《2017年中国人工智能行业市场前景研究报告》。报告显示，目前，我国人工智能领域发展迅速。据中商产业研究院大数据显示，2015年中国的人工智能市场规模达12亿美元，预测将在2020年达到91亿美元的规模，这意味着在未来几年内，每年的增长速度都达到50%。未来将只有两种公司，一种是有人工智能的公司，一种是不赚钱的公司，何去何从，应早有打算。\n如今程序员转人工智能的优势就在于具备行业基础，既然不敢直接了当转去别的行业，为何不奋勇向前，继续IT之路？对于还没有毕业或者刚刚毕业的大学生，恰好也是在最好的时机，新青年可以很快接受、理解新事物，学习能力也更强，既年轻又有兴趣那是最好不过了。\n以下数据显示，管理岗在各岗位中薪酬最高，平均达到 23k，数据开发和人工智能紧随其后，都在 20k以上。人工智能在互联网岗位薪酬排名中位列第三。\n当你确定好转人工智能时，问题就来了，你不知道该如何入手，你去网站收集各大网站的免费教学视频，书籍推荐买了许多本，真正看完的三分之一不到，既学不会又浪费时间，想自学的人比比皆是，但是真正靠自学成AI高技术人材的寥寥无几。\n所以伍老师给大家梳理了一条学习路径，希望对你的自学之路能够有起到一定的引导作用：\n首先，你是零基础的话，就先将高等数学基础知识学透，从基础的数据分析、线性代数及矩阵等等入门，只有基础有了，才会层层积累，不能没有逻辑性的看一块学一块。具体学习内容请看图。\n其次就是Python，Python具有丰富和强大的库。它常被昵称为胶水语言，能够把用其他语言制作的各种模块（尤其是C/C++）很轻松地联结在一起。比如3D游戏中的图形渲染模块，性能要求特别高，就可以用C/C++重写，而后封装为Python可以调用的扩展类库。这也是人工智能必备知识。\n接下来就是人工智能的重点学习内容，如果是已经从业多年的程序员可以就此开始学习：\n机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。\n它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n推荐算法是计算机专业中的一种算法，通过一些数学算法，推测出用户可能喜欢的东西，推荐算法就是利用用户的一些行为，通过一些数学算法，推测出用户可能喜欢的东西。在人工智能里起到一定的判断作用。\n人工智能深度学习以及数据挖掘都是对机器学习的进一步探究，学习过程中不能缺少实际项目应用的操作，如果没有实操性的学习在以后的工作中很难适应新项目。\n分布式搜索引擎是根据地域、主题、IP地址及其它的划分标准，将全网分成若干个自治区域，在每个自治区域内设立一个检索服务器的装置。这些就是人工智能主要应该学习的内容。\n老师认为自学是一种低效且不划算的学习方式，既浪费了时间还不能把知识点学透。要想得到就先付出，所以伍老师建议在有条件的情况下报班学习，学习过后既提升了自己的工作技能又可以在工作中挣回学费，一举两得。不管你是小白还是有经验的程序员，转人工智能只要你找到方法坚持学习，从业时间越长，价值也就会逐渐体现出来了，想转人工智能并非难事。"}
{"content2":"了解更多技术文章请点击原文链接\n随着科学技术的发展以及硬件计算能力的大幅提升，人工智能已经从几十年的幕后工作一下子跃入人们眼帘。人工智能的背后源自于大数据、高性能的硬件与优秀的算法的支持。2016年，深度学习已成为Google搜索的热词，随着最近一两年的围棋人机大战中，阿法狗完胜世界冠军后，人们感觉到再也无法抵挡住AI的车轮的快速驶来。在2017年这一年中，AI已经突破天际，相关产品也出现在人们的生活中，比如智能机器人、无人驾驶以及语音搜索等。最近，世界智能大会在天津举办成功，大会上许多业内行家及企业家发表自己对未来的看法，可以了解到，大多数的科技公司及研究机构都非常看好人工智能的前景，比如百度公司将自己的全部身家压在人工智能上，不管破釜沉舟后是一举成名还是一败涂地，只要不是一无所获就行。为什么突然之间深度学习会有这么大的效应与热潮呢？这是因为科技改变生活，很多的职业可能在今后的时间里慢慢被人工智能所取代。全民都在热议人工智能与深度学习，就连Yann LeCun大牛都感受到了人工智能在中国的火热!\n言归正传，人工智能的背后是大数据、优秀的算法以及强大运算能力的硬件支持。比如，英伟达公司凭借自己的强大的硬件研发能力以及对深度学习框架的支持夺得世全球最聪明的五十家公司榜首。另外优秀的深度学习算法有很多，时不时就会出现一个新的算法，真是令人眼花缭乱。但大多都是基于经典的算法改进而来，比如卷积神经网络（CNN）、深度信念网络（DBN）、循环神经网络（RNN）等等。\n本文将介绍经典的网络之循环神经网络（RNN），这一网络也是时序数据的首选网络。当涉及某些顺序机器学习任务时，RNN可以达到很高的精度，没有其他算法可以与之一较高下。这是由于传统的神经网络只是具有一种短期记忆，而RNN具有有限的短期记忆的优势。然而，第一代RNNs网络并没有引起人们着重的注意，这是由于研究人员在利用反向传播和梯度下降算法过程中遭受到了严重的梯度消失问题，阻碍了RNN几十年的发展。最后，于90年代后期出现了重大突破，导致更加准确的新一代RNN的问世。基于这一突破的近二十年，直到Google Voice Search和Apple Siri等应用程序开始抢夺其关键流程，开发人员完善和优化了新一代的RNN。现在，RNN网络遍布各个研究领域，并且正在帮助点燃人工智能的复兴之火。\n与过去有关的神经网络（RNN）\n大多数人造神经网络，如前馈神经网络，都没有记忆它们刚刚收到的输入。例如，如果提供前馈神经网络的字符“WISDOM”，当它到达字符“D”时，它已经忘记了它刚刚读过字符“S”，这是一个大问题。无论训练该网络是多么的辛苦，总是很难猜出下一个最有可能的字符“O”。这使得它成为某些任务的一个相当无用的候选人，例如在语音识别中，识别的好坏在很大程度上受益于预测下一个字符的能力。另一方面，RNN网络确实记住了之前的输入，但是处于一个非常复杂的水平。\n我们再次输入“WISDOM”，并将其应用到一个复发性网络中。RNN网络中的单元或人造神经元在接收到“D”时也将其之前接收到的字符“S”作为其输入。换句话说，就是把刚刚过去的事情联合现在的事情作为输入，来预测接下来会发生的事情，这给了它有限的短期记忆的优势。当训练时，提供足够的背景下，可以猜测下一个字符最有可能是“O”。\n调整和重新调整\n像所有人工神经网络一样，RNN的单元为其多个输入分配一个权重矩阵，这些权重代表各个输入在网络层中所占的比重；然后对这些权重应用一个函数来确定单个输出，这个函数一般被称为损失函数（代价函数），限定实际输出与目标输出之间的误差。然而，循环神经网络不仅对当前输入分配权重，而且还从对过去时刻输入分配权重。然后，通过使得损失函数最下来动态的调整分配给当前输入和过去输入的权重，这个过程涉及到两个关键概念：梯度下降和反向传播（BPTT）。\n梯度下降\n机器学习中最著名的算法之一就是梯度下降算法。它的主要优点在于它显着的回避了“维数灾难”。什么是“维数灾难”呢，就是说在涉及到向量的计算问题中，随着维数的增加，计算量会呈指数倍增长。这个问题困扰着诸多神经网络系统，因为太多的变量需要计算来达到最小的损失函数。然而，梯度下降算法通过放大多维误差或代价函数的局部最小值来打破维数灾难。这有助于系统调整分配给各个单元的权重值，以使网络变得更加精确。\n通过时间的反向传播\nRNN通过反向推理微调其权重来训练其单元。简单的说，就是根据单元计算出的总输出与目标输出之间的误差，从网络的最终输出端反向逐层回归，利用损失函数的偏导调整每个单元的权重。这就是著名的BP算法，关于BP算法可以看本博主之前的相关博客。而RNN网络使用的是类似的一个版本，称为通过时间的反向传播（BPTT）。该版本扩展了调整过程，包括负责前一时刻（T-1）输入值对应的每个单元的记忆的权重。\nYikes：梯度消失问题\n尽管在梯度下降算法和BPTT的帮助下享有一些初步的成功，但是许多人造神经网络（包括第一代RNNs网络），最终都遭受了严重的挫折——梯度消失问题。什么是梯度消失问题呢，其基本思想其实很简单。首先，来看一个梯度的概念，将梯度视为斜率。在训练深层神经网络的背景中，梯度值越大代表坡度越陡峭，系统能够越快地下滑到终点线并完成训练。但这也是研究者陷入困境的地方——当斜坡太平坦时，无法进行快速的训练。这对于深层网络中的第一层而言特别关键，因为若第一层的梯度值为零，说明没有了调整方向，无法调整相关的权重值来最下化损失函数，这一现象就是“消梯度失”。随着梯度越来越小，训练时间也会越来越长，类似于物理学中的沿直线运动，光滑表面，小球会一直运动下去。\n大的突破：长短期记忆（LSTM）\n在九十年代后期，一个重大的突破解决了上述梯度消失问题，给RNN网络发展带来了第二次研究热潮。这种大突破的中心思想是引入了单元长短期记忆（LSTM）。\nLSTM的引入给AI领域创造了一个不同的世界。这是由于这些新单元或人造神经元（如RNN的标准短期记忆单元）从一开始就记住了它们的输入。然而，与标准的RNN单元不同，LSTM可以挂载在它们的存储器上，这些存储器具有类似于常规计算机中的存储器寄存器的读/写属性。另外LSTM是模拟的，而不是数字，使得它们的特征可以区分。换句话说，它们的曲线是连续的，可以找到它们的斜坡的陡度。因此，LSTM特别适合于反向传播和梯度下降中所涉及的偏微积分。\n总而言之，LSTM不仅可以调整其权重，还可以根据训练的梯度来保留、删除、转换和控制其存储数据的流入和流出。最重要的是，LSTM可以长时间保存重要的错误信息，以使梯度相对陡峭，从而网络的训练时间相对较短。这解决了梯度消失的问题，并大大提高了当今基于LSTM的RNN网络的准确性。由于RNN架构的显著改进，谷歌、苹果及许多其他先进的公司现在正在使用RNN为其业务中心的应用提供推动力。\n总结\n循环神经网络（RNN）可以记住其以前的输入，当涉及到连续的、与上下文相关的任务（如语音识别）时，它比其他人造神经网络具有更大的优势。\n关于RNN网络的发展历程：第一代RNNs通过反向传播和梯度下降算法达到了纠正错误的能力。但梯度消失问题阻止了RNN的发展；直到1997年，引入了一个基于LSTM的架构后，取得了大的突破。\n新的方法有效地将RNN网络中的每个单元转变成一个模拟计算机，大大提高了网络精度。\n作者信息\nJason Roell：软件工程师，热爱深度学习及其可改变技术的应用。\nLinkedin：http://www.linkedin.com/in/jason-roell-47830817/\n本文由北邮@爱可可-爱生活老师推荐，阿里云云栖社区组织翻译。\n文章原标题《Understanding Recurrent Neural Networks: The Preferred Neural Network for Time-Series Data》，作者：Jason Roel，译者：海棠，审阅：袁虎\n附件为原文的pdf\n文章为简译，更为详细的内容，请查看原文\n了解更多技术文章请点击原文链接"}
{"content2":"不多说，直接上干货！\n问题详解\n启动ambari-server出现\nCaused by: java.lang.RuntimeException:java.lang.ClassNotFoundEception:com.mysql.jdbc.Driver\n解决办法\n说白了，就是你放置的mysql驱动包位置不完整。\nmysql如果报错 java.lang.ClassNotFoundException: com.mysql.jdbc.Driver, 需要查看如下几个目录, 确保 JDBC 在\n1、/usr/share/java\n2、/var/lib/ambari-server/resources  和  /var/lib/ambari-server\n3、/usr/lib/ambari-server\n1、　/usr/share/java下\n或者，如下，需要赋予权限\n2、/var/lib/ambari-server/resources  和 /var/lib/ambari-server\n3、/usr/lib/ambari-server\n最后是\n再次启动\nsudo service ambari-server status sudo service ambari-server start\n成功！\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"目录：\n1.关联分析\n2. Apriori 原理\n3. 使用 Apriori 算法来发现频繁集\n4.从频繁集中挖掘关联规则\n5. 总结\n1.关联分析  返回目录\n关联分析是一种在大规模数据集中寻找有趣关系的任务。这种关系表现为两种形式：\n1.频繁项集(frequency item sets)：经常同时出现的一些元素的集合；\n2.关联规则(association rules): 意味着两种元素之间存在很强的关系。\n下面举例来说明上面的两个概念：\n表1 一个来自Hole Foods天食品店的简单交易清单\n交易号码\n商品\n0\n豆奶， 莴苣\n1\n莴苣，尿布，葡萄酒，甜菜\n2\n莴苣，尿布，葡萄酒，橙汁\n3\n莴苣，豆奶，尿布，葡萄酒\n4\n莴苣，豆奶，尿布，橙汁\n频繁项集是指经常出现在一起的元素的集合，上表中的集合 {葡萄酒，尿布，豆奶} 就是频繁项集的一个例子。同样可以找到如 “尿布 --> 葡萄酒”的关联规则，意味着如果有人买了尿布，就很可能也会买葡萄酒。使用频繁项集和关联规则，商家可以更好地理解顾客的消费行为，所以大部分关联规则分析示例来自零售业。\n理解关联分析首先需要搞清楚下面三个问题：\n1.如何定义这些有用的关系？\n2.这些关系的强弱程度又是如何定义？\n3.频繁的定义是什么？\n要回答上面的问题，最重要的是理解两个概念：支持度和可信度。\n支持度：一个项集的支持度(support)被定义为数据集中包含该项集的记录占总记录的比例。从表1 可以看出 项集 {豆奶} 的支持度为 $4/5$; 而在 5 条交易记录中 3 条包含 {豆奶，尿布}，因此 {豆奶，尿布} 的支持度为 $3/5$.\n可信度或置信度(confidence)：是针对一条诸如${尿布}-->{葡萄酒}$的关联规则来定义的，这条规则的可信度被定义为“ 支持度({尿布,葡萄酒})  /  支持度({尿布})”。在表1 中可以发现  {尿布,葡萄酒} 的支持度是 $3/5$, {尿布} 的支持度为 $4/5$, 所以关联规则 “尿布 --> 葡萄酒”的可信度为 $3/4 = 0.75$, 意思是对于所有包含 \"尿布\"的记录中，该关联规则对其中的 75% 记录都适用。\n2. Apriori 原理 返回目录\n假设经营了一家杂货店，于是我们对那些经常在一起购买的商品非常感兴趣。假设我们只有 4 种商品：商品0，商品1，商品 2，商品3. 那么如何得可能被一起购买的商品的组合？\n上图显示了物品之间所有可能的组合，从上往下一个集合是 $\\textrm{Ø}$，表示不包含任何物品的空集，物品集合之间的连线表明两个或者更多集合可以组合形成一个更大的集合。\n我们的目标是找到经常在一起购买的物品集合。这里使用集合的支持度来度量其出现的频率。一个集合出现的支持度是指有多少比例的交易记录包含该集合。例如，对于上图，要计算 ${0,3}$ 的支持度，直接的想法是遍历每条记录，统计包含有 $0$ 和 $3$ 的记录的数量，使用该数量除以总记录数，就可以得到支持度。而这只是针对单个集合 ${0,3}$. 要获得每种可能集合的支持度就需要多次重复上述过程。对于上图，虽然仅有4中物品，也需要遍历数据15次。随着物品数目的增加，遍历次数会急剧增加，对于包含 $N$ 种物品的数据集共有 $2^{N}-1$ 种项集组合。所以即使只出售 $100$  种商品的商店也会有 $1.26\\times10^{30}$ 中可能的组合。计算量太大。\n为了降低计算时间，研究人员发现了 $Apriori$ 原理，可以帮我们减少感兴趣的频繁项集的数目。\n$Apriori$ 的原理：如果某个项集是频繁项集，那么它所有的子集也是频繁的。\n即如果 {0,1} 是频繁的，那么 {0}, {1} 也一定是频繁的。\n这个原理直观上没有什么用，但是反过来看就有用了，也就是说如果一个项集是非频繁的，那么它的所有超集也是非频繁的。如下图所示：\n3. 使用 Apriori 算法来发现频繁集 返回目录\n上面提到，关联分析的两个目标：发现频繁项集和发现关联规则。首先需要找到频繁项集，然后根据频繁项集获得关联规则。首先来讨论发现频繁项集。Apriori 是发现频繁项集的一种方法。\n首先会生成所有单个物品的项集列表；\n扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度的集合会被去掉；\n对剩下的集合进行组合以生成包含两个元素的项集；\n接下来重新扫描交易记录，去掉不满足最小支持度的项集，重复进行直到所有项集都被去掉。\n数据集扫描的伪代码：\n对数据集中的每条交易记录tran:\n对每个候选项集can:\n检查一下can是否是tran的子集：\n如果是，则增加can的计数值\n对每个候选项集：\n如果其支持度不低于最低值，则保留\n返回所有频繁项集列表\n有上面的伪代码写出代码如下：\n# -*- coding: utf-8 -*- \"\"\" Apriori exercise. Created on Fri Nov 27 11:09:03 2015 @author: 90Zeng \"\"\" def loadDataSet(): '''创建一个用于测试的简单的数据集''' return [ [ 1, 3, 4 ], [ 2, 3, 5 ], [ 1, 2, 3, 5 ], [ 2, 5 ] ] def createC1( dataSet ): ''' 构建初始候选项集的列表，即所有候选项集只包含一个元素， C1是大小为1的所有候选项集的集合 ''' C1 = [] for transaction in dataSet: for item in transaction: if [ item ] not in C1: C1.append( [ item ] ) C1.sort() return map( frozenset, C1 ) def scanD( D, Ck, minSupport ): ''' 计算Ck中的项集在数据集合D(记录或者transactions)中的支持度, 返回满足最小支持度的项集的集合，和所有项集支持度信息的字典。 ''' ssCnt = {} for tid in D: # 对于每一条transaction for can in Ck: # 对于每一个候选项集can，检查是否是transaction的一部分 # 即该候选can是否得到transaction的支持 if can.issubset( tid ): ssCnt[ can ] = ssCnt.get( can, 0) + 1 numItems = float( len( D ) ) retList = [] supportData = {} for key in ssCnt: # 每个项集的支持度 support = ssCnt[ key ] / numItems # 将满足最小支持度的项集，加入retList if support >= minSupport: retList.insert( 0, key ) # 汇总支持度数据 supportData[ key ] = support return retList, supportData\n注：关于上面代码中 \"frozenset\"，是为了冻结集合，使集合由“可变”变为 \"不可变\"，这样，这些集合就可以作为字典的键值。\n首先来测试一下上面代码，看看运行效果：\nif __name__ == '__main__': # 导入数据集 myDat = loadDataSet() # 构建第一个候选项集列表C1 C1 = createC1( myDat ) # 构建集合表示的数据集 D D = map( set, myDat ) # 选择出支持度不小于0.5 的项集作为频繁项集 L, suppData = scanD( D, C1, 0.5 ) print u\"频繁项集L：\", L print u\"所有候选项集的支持度信息：\", suppData\n运行结果：\n>>> runfile('E:/Python/PythonScripts/Apriori.py', wdir=r'E:/Python/PythonScripts')\n频繁项集L： [frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]\n所有候选项集的支持度信息： {frozenset([4]): 0.25, frozenset([5]): 0.75, frozenset([2]): 0.75, frozenset([3]): 0.75, frozenset([1]): 0.5}\n可以看出，只有支持度不小于 0.5 的项集被选中到 L 中作为频繁项集，根据不同的需求，我们可以设定最小支持度的值，从而得到我们想要的频繁项集。\n上面的示例只是选择出来了项集中只包含一个元素的频繁项集，下面需要整合上面的代码，选择出包含 2个，3个直至个数据等于所有候选元素个数的频繁项集，\n从而形成完整的 $Apriori$ 的算法，首先给出伪代码：\n当集合中的元素个数大于 $0$ 时：\n构建一个 $k$ 个项组成的候选项集列表\n检查数据，确认每个项集都是频繁项集\n保留频繁项集，并构建 $k+1$ 项组成的候选项集的列表\n程序清单：\n# Aprior算法 def aprioriGen( Lk, k ): ''' 由初始候选项集的集合Lk生成新的生成候选项集， k表示生成的新项集中所含有的元素个数 ''' retList = [] lenLk = len( Lk ) for i in range( lenLk ): for j in range( i + 1, lenLk ): L1 = list( Lk[ i ] )[ : k - 2 ]; L2 = list( Lk[ j ] )[ : k - 2 ]; L1.sort();L2.sort() if L1 == L2: retList.append( Lk[ i ] | Lk[ j ] ) return retList def apriori( dataSet, minSupport = 0.5 ): # 构建初始候选项集C1 C1 = createC1( dataSet ) # 将dataSet集合化，以满足scanD的格式要求 D = map( set, dataSet ) # 构建初始的频繁项集，即所有项集只有一个元素 L1, suppData = scanD( D, C1, minSupport ) L = [ L1 ] # 最初的L1中的每个项集含有一个元素，新生成的 # 项集应该含有2个元素，所以 k=2 k = 2 while ( len( L[ k - 2 ] ) > 0 ): Ck = aprioriGen( L[ k - 2 ], k ) Lk, supK = scanD( D, Ck, minSupport ) # 将新的项集的支持度数据加入原来的总支持度字典中 suppData.update( supK ) # 将符合最小支持度要求的项集加入L L.append( Lk ) # 新生成的项集中的元素个数应不断增加 k += 1 # 返回所有满足条件的频繁项集的列表，和所有候选项集的支持度信息 return L, suppData\n关于上面程序 函数 aprioriGen 中的 $k-2$的说明：当利用 {0}, {1}, {2} 这些只含有一个元素的候选项集构建含有 2 个元素的候选项集时，就是两两合并得到 {0,1}, {0,2}, {1,2}; 如果进一步用包含连个元素的候选项集来构建包含 3 个元素的候选项集，同样两两合并，就会得到 {0,1,2},{0,1,2},{0,1,2}. 就是说会出现重复的项集，接下来就需要扫描三元素项集得到非重复结果，显然增加了计算时间。现在，如果比较 {0,1}, {0,2}, {1,2} 的第 0 个元素并只对第 0 个元素相同的集合求并，就会得到 {0,1,2}, 而且只有一次操作，这样就不需要遍历列表来寻找非重复值。\n测试上面代码：\nif __name__ == '__main__': # 导入数据集 myDat = loadDataSet() # 选择频繁项集 L, suppData = apriori( myDat, 0.5 ) print u\"频繁项集L：\", L print u\"所有候选项集的支持度信息：\", suppData\n运行结果（最小支持度 0.5） ：\n>>> runfile('E:/Python/PythonScripts/Apriori.py', wdir=r'E:/Python/PythonScripts') 频繁项集L： [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])], []] 所有候选项集的支持度信息： {frozenset([5]): 0.75, frozenset([3]): 0.75, frozenset([2, 3, 5]): 0.5, frozenset([1, 2]): 0.25, frozenset([1, 5]): 0.25, frozenset([3, 5]): 0.5, frozenset([4]): 0.25, frozenset([2, 3]): 0.5, frozenset([2, 5]): 0.75, frozenset([1]): 0.5, frozenset([1, 3]): 0.5, frozenset([2]): 0.75}\n在测试一下最小支持度为 0.7 时的情况：\n>>> runfile('E:/Python/PythonScripts/Apriori.py', wdir=r'E:/Python/PythonScripts') 频繁项集L： [[frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([2, 5])], []] 所有候选项集的支持度信息： {frozenset([5]): 0.75, frozenset([3]): 0.75, frozenset([3, 5]): 0.5, frozenset([4]): 0.25, frozenset([2, 3]): 0.5, frozenset([2, 5]): 0.75, frozenset([1]): 0.5, frozenset([2]): 0.75}\n频繁项集相比最小支持度 0.5 时要少，符合预期。\n4.从频繁集中挖掘关联规则 返回目录\n要找到关联规则，先从一个频繁集开始，我们想知道对于频繁项集中的元素能否获取其它内容，即某个元素或者某个集合可能会推导出另一个元素。从表1 可以得到，如果有一个频繁项集 {豆奶，莴苣}，那么就可能有一条关联规则 \"豆奶 --> 莴苣\"，意味着如果有人购买了豆奶，那么在统计上他会购买莴苣的概率较大。但是这一条反过来并不一定成立。\n从一个频繁项集可以产生多少条关联规则呢？可以基于该频繁项集生成一个可能的规则列表，然后测试每条规则的可信度，如果可信度不满足最小值要求，则去掉该规则。类似于前面讨论的频繁项集生成，一个频繁项集可以产生许多可能的关联规则，如果能在计算规则可信度之前就减少规则的数目，就会很好的提高计算效率。\n这里有一条规律就是：如果某条规则并不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求，例如下图的解释：\n所以，可以利用上图所示的性质来减少测试的规则数目。\n关联规则生成函数清单：\n# 规则生成与评价 def calcConf( freqSet, H, supportData, brl, minConf=0.7 ): ''' 计算规则的可信度，返回满足最小可信度的规则。 freqSet(frozenset):频繁项集 H(frozenset):频繁项集中所有的元素 supportData(dic):频繁项集中所有元素的支持度 brl(tuple):满足可信度条件的关联规则 minConf(float):最小可信度 ''' prunedH = [] for conseq in H: conf = supportData[ freqSet ] / supportData[ freqSet - conseq ] if conf >= minConf: print freqSet - conseq, '-->', conseq, 'conf:', conf brl.append( ( freqSet - conseq, conseq, conf ) ) prunedH.append( conseq ) return prunedH def rulesFromConseq( freqSet, H, supportData, brl, minConf=0.7 ): ''' 对频繁项集中元素超过2的项集进行合并。 freqSet(frozenset):频繁项集 H(frozenset):频繁项集中的所有元素，即可以出现在规则右部的元素 supportData(dict):所有项集的支持度信息 brl(tuple):生成的规则 ''' m = len( H[ 0 ] ) # 查看频繁项集是否大到移除大小为 m　的子集 if len( freqSet ) > m + 1: Hmp1 = aprioriGen( H, m + 1 ) Hmp1 = calcConf( freqSet, Hmp1, supportData, brl, minConf ) # 如果不止一条规则满足要求，进一步递归合并 if len( Hmp1 ) > 1: rulesFromConseq( freqSet, Hmp1, supportData, brl, minConf ) def generateRules( L, supportData, minConf=0.7 ): ''' 根据频繁项集和最小可信度生成规则。 L(list):存储频繁项集 supportData(dict):存储着所有项集（不仅仅是频繁项集）的支持度 minConf(float):最小可信度 ''' bigRuleList = [] for i in range( 1, len( L ) ): for freqSet in L[ i ]: # 对于每一个频繁项集的集合freqSet H1 = [ frozenset( [ item ] ) for item in freqSet ] # 如果频繁项集中的元素个数大于2，需要进一步合并 if i > 1: rulesFromConseq( freqSet, H1, supportData, bigRuleList, minConf ) else: calcConf( freqSet, H1, supportData, bigRuleList, minConf ) return bigRuleList\n测试：\nif __name__ == '__main__': # 导入数据集 myDat = loadDataSet() # 选择频繁项集 L, suppData = apriori( myDat, 0.5 ) rules = generateRules( L, suppData, minConf=0.7 ) print 'rules:\\n', rules\n运行结果：\n>>> runfile('E:/Python/PythonScripts/Apriori.py', wdir=r'E:/Python/PythonScripts') frozenset([1]) --> frozenset([3]) conf: 1.0 frozenset([5]) --> frozenset([2]) conf: 1.0 frozenset([2]) --> frozenset([5]) conf: 1.0 rules: [(frozenset([1]), frozenset([3]), 1.0), (frozenset([5]), frozenset([2]), 1.0), (frozenset([2]), frozenset([5]), 1.0)]\n将可信度降为 0.5 之后：\n>>> runfile('E:/Python/PythonScripts/Apriori.py', wdir=r'E:/Python/PythonScripts') frozenset([3]) --> frozenset([1]) conf: 0.666666666667 frozenset([1]) --> frozenset([3]) conf: 1.0 frozenset([5]) --> frozenset([2]) conf: 1.0 frozenset([2]) --> frozenset([5]) conf: 1.0 frozenset([3]) --> frozenset([2]) conf: 0.666666666667 frozenset([2]) --> frozenset([3]) conf: 0.666666666667 frozenset([5]) --> frozenset([3]) conf: 0.666666666667 frozenset([3]) --> frozenset([5]) conf: 0.666666666667 frozenset([5]) --> frozenset([2, 3]) conf: 0.666666666667 frozenset([3]) --> frozenset([2, 5]) conf: 0.666666666667 frozenset([2]) --> frozenset([3, 5]) conf: 0.666666666667 rules: [(frozenset([3]), frozenset([1]), 0.6666666666666666), (frozenset([1]), frozenset([3]), 1.0), (frozenset([5]), frozenset([2]), 1.0), (frozenset([2]), frozenset([5]), 1.0), (frozenset([3]), frozenset([2]), 0.6666666666666666), (frozenset([2]), frozenset([3]), 0.6666666666666666), (frozenset([5]), frozenset([3]), 0.6666666666666666), (frozenset([3]), frozenset([5]), 0.6666666666666666), (frozenset([5]), frozenset([2, 3]), 0.6666666666666666), (frozenset([3]), frozenset([2, 5]), 0.6666666666666666), (frozenset([2]), frozenset([3, 5]), 0.6666666666666666)]\n一旦降低可信度阈值，就可以获得更多的规则。\n5. 总结 返回目录\n有上面分析可以看出 Apriori 算法易编码，缺点是在大数据集上可能较慢。"}
{"content2":"作者：Burak Kanber\n翻译：王维强\n原文：http://burakkanber.com/blog/machine-learning-in-other-languages-introduction/\n遗传算法应该是我接触到的机器学习算法中的最后一个，但是我喜欢把它作为这个系列文章的开始，因为这个算法非常适合介绍“价值函数”或称“误差函数”，还有就是局部和全局最优概念，二者都是机器学习中的重要概念。\n遗传算法的发明受自然界和进化论的启发，这对我来说非常酷。这并不奇怪，即使是人工神经网络（NN）也是从生物学发展起来的，进化是我们体会到的最好的通用学习算法，我们都知道人类的大脑是解决通用问题的最好利器。在人工智能和机器学习研究中两个成长最快的领域，也是我们生物存在的及其重要的两个部分，这就是我所感兴趣的遗传算法和神经网络，现在我把二者浓缩在一起。\n我在前面使用的术语“通用”极其重要，对大多数的特别计算问题，你可能会找到比遗传算法更高效的方案，但是关键点不在于具体的实施，也不在于遗传算法。 使用遗传算法并不是在你遇到复杂的问题时，而是问题的复杂度已经成为问题，又或者你有一些完全不相干的参数需要面对。\n一个典型的应用就是两足机器人行走问题。能让机器人靠两足行走是非常困难的，硬编码行走程序几乎不可能成功，即使你真能令机器人走起来，下一个机器人的平衡重心可能会轻微不同，也会使你的程序无法运行。你可以选择使用遗传算法来教会机器人如何学习行走，而不是直接教机器人行走。\n我们这就来用Javascript搭建一个遗传算法。\n问题\n用Javascript写出一个算法繁殖出一段文本“Hello, World!\"。\n对程序员来说“Hello, World!”几乎是万物之始，我们使用遗传算法繁殖出这段文字也算是师出有名。注意这个问题有很高的人工参与性，当然我们可以直接在源码中打印出“Hello, World!”。不过这看起来很傻，既然已经知道了结果，还要这算法做什么呢？答案很简单，这只是个学习的训练，下一个遗传算法（使用PHP）将减少人工痕迹，但是我们总要先开始。\n遗传算法基础\n算法的基本目的就是生成一串“备选答案”并使用一系列的反馈知道这些备选离最优方案还有多远。离最优方案最远的的被淘汰掉，离最优方案近的留下来和其他备选方案结合并做轻微的突变，一次次修改备选方案并时刻检查离最优解的距离。\n这些“备选答案”称作染色体。\n染色体间结合，产生后代并且突变，优胜劣汰，适者生存，它们产生的后代或许具有更多适应自然选择的特性。\n对于解决“Hello, World!”这样的问题，如此是不是很诡异？放心吧，遗传算法绝不是只善于解决这类问题。\n染色体\n染色体就是一个备选方案的表达，在我们的例子中，染色体本身就是一段字符，我们设定所有的染色体都是长度为13的字符串（Hello, World! 的长度就是13）。下面列出了一些符合备选方案的染色体：\nGekmo+ xosmd!\nGekln, worle\"\nFello, wosld!\nGello, wprld!\nHello, world!\n很明显，最后一个是“正确”（或全局最优的）的染色体，但是我们如何测量染色体是否优秀呢？\n价值函数\n价值函数(或误差函数)是一个测量染色体优秀程度的方法，如果我们把他们称为“适应度函数”，那么所得分数越高越好，如果我们使用的是“价值函数”，当然分数越低越好。\n在本例中，我们需要按以下规则定义价值函数：\n针对字符串的每个字节，指出备选方案和目标方案之间在ASCII码上的差值，然后取差值的平方，以确保值为正数。\n举例：如果我们有个字符“Ａ” (ASCII 65) ，但是期望的字符应该是“Ｃ”(ASCII 67)，那么价值计算的结果就为４(67 - 65 = 2, 2^2 = 4).\n之所以采用平方，就是要确保值为正数，你当然也可以取绝对值。为了加深学习，请在实际操作中灵活应用。\n采用这样的规则，我们能计算出以下５例染色体的价值：\nGekmo+ xosmd! (7)\nGekln, worle\" (5)\nFello, wosld! (5)\nGello, wprld! (2)\nHello, world! (0)\n在本例中，该方法简单而且人工痕迹明显，很显然我们的目标是使代价（cost）为零，一旦为零，程序就可以停下来了。有时情况并不如此，比如当你在寻找最低代价时，需要用不同的方法结束计算，反之，如果寻找的是适应性最高分值时，可能需要用到其他的条件来停止计算。\n价值函数是遗传算法中非常重要的内容，因为如果你足够聪明就能使用它来调和完全不相干的参数。 在本例中，我们只关注字符。但是如果你是在建立一套驾驶导航应用，需要权衡过路费，距离，速度，交通灯，糟糕的邻车还有桥梁等等情况，把这些完全不相干的参数封装进统一，优美，整洁的价值函数中来处理，最终依据参数不同的权重获得路径信息。\n交配和死亡\n交配是生活中的一个常态，我们会在遗传算法中大量使用它。交配绝对是一个魔幻时刻，两段染色体为分享彼此的信息坠入爱河。从技术层面描述交配就是“交叉”，但是我还是愿意称呼其为“交配”，因为能使所描绘的图景更加具有直觉性。\n到目前为止我们还没有谈到遗传算法中的“种群”概念，但是我敢说只要你运行一个遗传算法，你某个时刻看到的可不仅仅是一个染色体这么简单。你可能会同时拥有20，100或5000条染色体，就像进化一样，你可能会倾向于让那些最强壮的染色体彼此交配，希望得到的后代比其父母更优秀。\n实际上让字符交配是非常简单的，比如我们的例子“Hello，World!”，选取两段备选字符串（染色体），各自从中间截断成两个片段，这里你可以使用任何方法，如果你愿意甚至可以选取随机的点位进行截取。我们就选取中间位置吧，然后用第一段字符串的前半部分和第二段字符串的后半部分合成一个新的染色体（字符串）。继续用同样的方法把第二段字符串的前半部分和第一段字符串的后半部分合并成另一个新的染色体（字符串）。\n以下面两个字符串为例:\nHello, wprld! (1)\nIello, world! (1)\n从中间断开通过合并获得两个新的字符串，也就是两个新的孩子:\nIello, wprld! (2)\nHello, world! (0)\n如上所见，两个后代中，有一个包含了父母的最佳特质，简直完美，另一个则非常糟糕。\n交配就是把基因从父代传递到子代的过程。\n突变\n独自交配会产生一个问题：近亲繁殖。如果你只是让候选者们一代一代地交配下去，你会到达一个“局部最优”的境地并卡在那里出不来，这个答案虽然看起来还不错，但并不是你想要的“全局最优”。\n把基因生活的世界想象成一个物理设定，这里具有起伏的山峰和沟谷，有那么一个山谷是这个世界中的最低处，同时也有很多其他小一些的谷地，恰恰基因被这些较小的谷地围绕，整体而言还在海平面之上。需要寻找一个解决方案，就像从山顶不同的随机位置滚落一些球，很显然这些球会卡在某个低处，他们中的很多会被山上的微小凹陷(局部最优)卡住。你的工作就是确保至少有一个球抵达整个世界的最低处：全局最优。既然球是从随机位置开始滚落的，就很难从开始处掌控过程，几乎不可能预测哪个球会被卡在哪里。但是你能做的是随机挑选一些球并给他们一脚，可能就是这一脚会帮助他们滚向更低处，想法就是稍微晃动一下系统使得这些球不要在局部最优处停留太久。\n这就是突变，这是一个完全随机的由你选定一个神秘的未知基因产生一定比例个数的字符随机变化。\n如下例所示，你停在了这两个染色体上面。\nHfllp, worlb!\nHfllp, worlb!\n没错这是一个人为的案例，但真的会发生。你的两条染色体一模一样，意味着他们的子代与父代也一模一样，什么进展都没产生。但是如果100条染色体中有一个在某个字节上发生了突变，如上所示，第二条染色体仅仅发生一个突变，从 \"Hfllp, worlb!\" 变成了 \"Ifllp, worlb!\"。那么进化就会继续，因为子代和父代间再次产生了差异，是突变推动进化前行。\n什么时候怎么突变完全取决于你自己。\n再次，我们开始实验，后面我所提供的代码会有高达50%的突变几率，但是这也只是为了示范目的。你可以让它的突变几率低一些，比如1% 。我的代码中是让字符在ASCII码上移动1，你可以有自己更激进的设定。实验，测试，学习，这就是唯一的途径。\n染色体：总结\n染色体代表你要解决问题的备选方案，他们由表达本身组成（在我们的例子中，是一个长度为13的字符串），一个价值或适应性分数以及其函数，交配及突变的能力。\n我喜欢把这些东西用OOP的观念考虑进去，染色体的类可以像下面这样定义：\n属性：\nGenetic code\nCost/fitness score\n方法:\nMate\nMutate\nCalculate Fitness Score\n我们现在考虑怎么让基因在遗传算法的最后一个谜团——种群中交互.\n种群\n种群就是一组染色体，种群通常会保持相同的尺寸，但是会随着时间的推移，发展到一个成本更均匀的状态。\n你需要选择种群大小，我选择20。你可做任意选择，10，100或1000，如你所愿。当然有优势也有劣势，正如我几次提到的，实验并自己探索！\n种群离不开“代”，一个典型的代可能会包含：\n为每个染色体计算代价/适应性的分值\n以代价/适应性分值排序染色体\n淘汰一定数目的弱染色体\n让一定数目的最强的染色体交配\n随机突变某些成员\n某种完整性测试, 如：你怎么知道该问题得到了解决？\n开始和结束\n创建一个种群非常简单，只是让随机产生的染色体充满整个种群即可。在我们的例子中，完全随机字符串的成本分数将会很恐怖，所以在我的代码中以平均分30000的价值分数开始。数目庞大不是问题，这就是进化的目的，也是我们在这里的原因。\n知道如何停止种群繁衍需要一点小技巧，当前的例子很简单，当价值分数为0时就停止。但这不总是那么管用，有时你甚至不知道最小值是什么，如果用适应性代替的话，你不知道可能的最大值是什么。\n在这些情况下，你应该指定一个完整的标准，可以是任何你想要的，但是这里建议用下面的逻辑跳离算法\n如果经过一千代的繁衍，最佳值也没什么变化，可以说该值就是答案了，该停止计算了。\n这个判断标准可能意味着你永远得不到全局最优解，但是很多情况下你根本不需要得到全局最优解，足够接近就行了。\n代码\n我还是喜欢OOP方法，当然也喜欢粗旷简单的代码。 我会尽可能采用简单直接的策略，即使在某些地方还比较粗糙。\n（注意：即使我在上文中把基因改成了染色体，这里代码中还是使用基因作为术语，只是语义上有些区别罢了。）\nvar Gene = function(code) { if (code) this.code = code; this.cost = 9999; }; Gene.prototype.code = ''; Gene.prototype.random = function(length) { while (length--) { this.code += String.fromCharCode(Math.floor(Math.random()*255)); } };\n很简单，该类的构造函数接受一个字符串作为参数，设定一个“价值”（cost），一个辅助函数用来生成新的随机的染色体。\nGene.prototype.calcCost = function(compareTo) { var total = 0; for(i = 0; i < this.code.length; i++) { total += (this.code.charCodeAt(i) - compareTo.charCodeAt(i)) * (this.code.charCodeAt(i) - compareTo.charCodeAt(i)); } this.cost = total; };\n价值函数把“模型”——字符串作为一个参数，和自身的字符串在ASCII编码方面做差运算，然后取其平方值。\nGene.prototype.mate = function(gene) { var pivot = Math.round(this.code.length / 2) - 1; var child1 = this.code.substr(0, pivot) + gene.code.substr(pivot); var child2 = gene.code.substr(0, pivot) + this.code.substr(pivot); return [new Gene(child1), new Gene(child2)]; };\n交配函数以一个染色体为参数，找到中间点，以数组的方式返回两个新的片段。\nGene.prototype.mutate = function(chance) { if (Math.random() > chance) return; var index = Math.floor(Math.random()*this.code.length); var upOrDown = Math.random()\n突变函数把一个浮点值作为参数，代表染色体的突变几率。\nvar Population = function(goal, size) { 　　this.members = []; 　　this.goal = goal;\nthis.generationNumber = 0; while (size--) { 　　var gene = new Gene(); gene.random(this.goal.length); this.members.push(gene); } };\n种群类中的构造器以目标字符串和种群大小作为参数，然后用随机生成的染色体建立种群。\nPopulation.prototype.sort = function() { 　　this.members.sort(function(a, b) { 　　　　return a.cost - b.cost; }); }\n定义一个 Population.prototype.sort 方法作为一个辅助函数对种群依据他们的价值（cost）分数排序。\nPopulation.prototype.generation = function() { for (var i = 0; i < this.members.length; i++) { this.members[i].calcCost(this.goal); } this.sort(); this.display(); var children = this.members[0].mate(this.members[1]); this.members.splice(this.members.length - 2, 2, children[0], children[1]); for (var i = 0; i < this.members.length; i++) { this.members[i].mutate(0.5); this.members[i].calcCost(this.goal); if (this.members[i].code == this.goal) { this.sort(); this.display(); return true; } } this.generationNumber++; var scope = this; setTimeout(function() { scope.generation(); } , 20); };\n种群的生产方法是最重的部分，其实也没有什么魔法。display（）方法只是把结果渲染到页面上，我设置了代际间隔时长，不至于让事情爆炸般增长。\n注意，在本例中我仅仅让排在最顶端的两个染色体交配，至于在你自己的实践中怎么处理，可多做各种不同的尝试。\nwindow.onload = function() { var population = new Population(\"Hello, world!\", 20); population.generation(); };\n还是看实例吧:\nhttp://jsfiddle.net/bkanber/BBxc6/?utm_source=website&utm_medium=embed&utm_campaign=BBxc6"}
{"content2":"原创 2017-07-27 马文辉 MATLAB\n作 者 简 介\n马文辉，MathWorks中国应用工程师， 南开大学工学博士，在大数据处理与分析领域有多年研究与开发经验；曾就职于Nokia中国研究院，Adobe中国研发中心以及IBM中国。\n近年来，全国赛的题目中，多多少少都有些数据，而且数据量总体来说呈不断增加的趋势， 这是由于在科研界和工业界已积累了比较丰富的数据，伴随大数据概念的兴起及机器学习技术的发展， 这些数据需要转化成更有意义的知识或模型。 所以在建模比赛中， 只要数据量还比较大， 就有机器学习的用武之地。\n1. MATLAB机器学习概况\n机器学习 ( Machine Learning ) 是一门多领域交叉学科，它涉及到概率论、统计学、计算机科学以及软件工程。机器学习是指一套工具或方法，凭借这套工具和方法，利用历史数据对机器进行“训练”进而“学习”到某种模式或规律，并建立预测未来结果的模型。\n机器学习涉及两类学习方法（如）：有监督学习，主要用于决策支持，它利用有标识的历史数据进行训练，以实现对新数据的标识的预测。有监督学习方法主要包括分类和回归；无监督学习，主要用于知识发现，它在历史数据中发现隐藏的模式或内在结构。无监督学习方法主要包括聚类。\n\nMATLAB 统计与机器学习工具箱（Statistics and Machine Learning Toolbox）支持大量的分类模型、回归模型和聚类的模型，并提供专门应用程序（APP），以图形化的方式实现模型的训练、验证，以及模型之间的比较。\n分类\n分类技术预测的数据对象是离散值。例如，电子邮件是否为垃圾邮件，肿瘤是癌性还是良性等等。 分类模型将输入数据分类。 典型应用包括医学成像，信用评分等。MATLAB 提供的分类算法包括：\n\n回归\n回归技术预测的数据对象是连续值。 例如，温度变化或功率需求波动。 典型应用包括电力负荷预测和算法交易等。回归模型包括一元回归和多元回归，线性回归和非线性回归，MATLAB 提供的回归算法有：\n\n聚类\n聚类算法用于在数据中寻找隐藏的模式或分组。聚类算法构成分组或类，类中的数据具有更高的相似度。聚类建模的相似度衡量可以通过欧几里得距离、概率距离或其他指标进行定义。MATLAB 支持的聚类算法有：\n\n以下将通过一些示例演示如何使用 MATLAB 提供的机器学习相关算法进行数据的分类、回归和聚类。\n2. 分类技术\n支持向量机（SVM）\nSVM 在小样本、非线性及高维数据分类中具有很强的优势。在 MATLAB 中，可以利用 SVM 解决二分类问题。同时也可以使用 SVM 进行数据的多分类划分。\n1) 二分类\n以下示例显示了利用 MATLAB 提供的支持向量机模型进行二分类，并在图中画出了支持向量的分布情况（中圆圈内的点表示支持向量）。MATLAB 支持 SVM 的核函数（KernelFunction 参数）有：线性核函数（Linear）,多项式核函数（Polynomial）、高斯核函数（Gaussian）。\n%% 支持向量机模型\nload fisheriris;\n% 数据只取两个分类：‘versicolor' 和 'virginica'\ninds = ~strcmp(species, 'setosa');\n% 使用两个维度\nX = meas(inds,3:4);\ny = species(inds);\ntabulate(y)\nValue      Count      Percent\nversicolor            50          50.00%\nverginica            50          50.00%\n%% SVM模型训练，使用线性核函数\nSVMModel = fitcsvm(X, y, 'KernelFunction', 'linear' );\n%% 查看进行数据划分的支持向量\nsv = SVMModel.SupportVectors;\nfigure\ngscatter( X( : , 1) , X( : , 2) ,y)\nhold on\nplot(sv( : , 1) , sv( : , 2) , 'ko' , 'MarkerSize' , 10)\nlegend( 'versicolor' , 'virginica' , 'Support Vector' )\nhold off\n\n2) 多分类\nMATLAB 多分类问题的处理是基于二分类模型.下面的示例演示如何利用 SVM 的二分类模型并结合 fitcecoc 函数解决多分类问题。\n% 导入Fisher' s iris数据集\nload fisheriris\nX = meas;\nY = species;\ntabulate(Y)\nValue   Count   Percent\nsetosa         50       33.33%\nversicolor         50        33.33%\nvirginica         50        33.33%\n% 创建SVM模板（二分类模型），并对分类变量进行标准化处理\n% predictors\nt = templateSVM( 'Standardize' , 1);\n% 基于SVM二分类模型进行训练并生成多分类模型\nMdl = fitcecoc( X, Y, 'Learners' , t , . . . 'ClassNames' , {'setosa' , 'versicolor' , 'virginica'})\nMdl =\nClassificationECOC\nResponseName:    'Y'\nCategoricalPredictors:   [ ]\nClassNames:   {'setosa' 'versicolor' 'virginica'}\nScoreTransform:   'none'\nBinaryLearners:   {3*1 cell}\nCodingName:   'onevsone'\nMATLAB 的 fitcecoc 函数支持多种二分类模型，例如, templateKNN, templateTree, templateLinear, templateNaiveBayes, 等等。\n3. 回归\n回归模型描述了响应（输出）变量与一个或多个预测变量（输入）变量之间的关系。 MATLAB 支持线性，广义线性和非线性回归模型。以下示例演示如何训练逻辑回归模型。\n逻辑回归\n在 MATLAB 中，逻辑回归属于广义线性回归的范畴，可以通过使用 fitglm 函数实现逻辑回归模型的训练。\n% 判定不同体重、年龄和性别的人的吸烟概率\nload hospital\ndsa = hospital;\n% 指定模型使用的计算公式\n% 公式的书写方式符合 Wilkinson Notation, 详情请查看：\n% http://cn.mathworks.com/help/stats/wilkinson-notation.html\nmodelspec = 'Smoker ~ 1+ Age + Weight + Sex + Age:Weight + Age:Sex + Weight:Sex';\n% 通过参数 ’Disribution' 指定 ‘binomial' 构建逻辑回归模型\nmdl = fitglm(dsa, modelspec, 'Distribution', 'binomial')\n4. 聚类\n聚类是将数据集分成组或类。 形成类，使得同一类中的数据非常相似，而不同类中的数据差异非常明显。\n层次聚类\n下面以层次聚类方法为例，演示如何利用 MATLAB 进行聚类分析。\n% 数据导入\nload fisheriris\n% MATLAB中层次聚类是通过linkage函数实现\n% 通过参数可以配置距离计算方法\n% 类内距离的计算方法：'euclidean' ，欧几里得距离\neucD = pdist(mean , 'euclidean' );\n% 类间距离的计算方法：'ward' ，最小化两个类内点之间聚类平方和\nZ = linkage(eucD, 'ward');\n% 使用 cophenetic 相关系数评价聚类计算过程（类内距离最小，类间距离最大）\n% 值越大表明距离计算结果越好\ncophenet(Z, eucD)\nans = 0.872828315330562\n%生成4个类别的聚类结果\nc = cluster(Z, 'maxclust' , 4);\n可以显示层次聚类生成的聚类树，使用 dendrogram 函数：\n% 查看层次聚类树\ndendrogram(Z)\n\n以上只是简单的介绍了一下 MATLAB 支持的机器学习算法的使用方式，更多的信息可以查看 MathWorks 官网和 MATLAB 帮助文档。\n文中例程中的所有例程均可在 MATLAB 直接运行，程序使用的数据为 MATLAB 2017a 中的 demo 数据，大家快去动手实践吧！\n点击阅读原文，申请 MATLAB 正版软件授权（数学建模竞赛支持）。\n* 友情提醒：9月8日晚上7点截止软件下载申请。\n往期 | 数模专栏\n开篇：如何备战数学建模竞赛之 MATLAB 编程\n第二篇：MATLAB数学建模快速入门\n第三篇：MATLAB数据建模方法(上) —常用方法\n阅读原文\n微信扫一扫\n关注该公众号"}
{"content2":"不多说，直接上干货！\n目前啊，都知道，大数据集群管理方式分为手工方式（Apache hadoop）和工具方式（Ambari + hdp 和Cloudera Manger + CDH）。\n手工部署呢，需配置太多参数，但是，好理解其原理，建议初学这样做，能学到很多。该方式啊，均得由用户执行，细节太多，切当设计多个组件时，用户须自己解决组件间版本兼容问题。\n工具部署呢，比如Ambari或Cloudera Manger。（当前两大最主流的集群管理工具，前者是Hortonworks公司，后者是Cloudera公司）使用工具来，可以说是一键操作，难点都在工具Ambari或Cloudera Manger本身部署上。\n手工方式　　　　　　　　　　　　　　　　　　工具方式\n难易度　　　　　　　　难，几乎不可能成功　　　　　　　　　　　　　　简单，易行\n兼容性　　　　　　　　自己解决组件兼容性问题　　　　　　　　　　　　自动安装兼容组件\n组件支持数　　　　　　支持全部组件　　　　　　　　　　　　　　　　　支持常用组件\n优点　　　　　　　　　对组件和集群管理深刻　　　　　　　　　　　　  简单、容易、可行\n缺点　　　　　　　　　太复杂，不可能成功　　　　　　　　　　　　　　屏蔽太多细节，妨碍对组件理解\n工具名　　　　　　　　所属机构　　　　　　开源性　　　　　　　　社区支持性　　　　　　易用性、稳定性　　　　　　市场占有率\nCloudera Manger 　  Cloudera　　　　　   商用　　　　　　　　　　不支持　　　　　　　　易用、稳定　　　　　　　　     高\nAmbari　　　　　　Hortonwork　　　　　　开源　　　　　　　　　　支持　　　　　　　　 较易用、较稳定　　　　　　   较高\n常见的情况是，Cloudera Manger 去部署CDH\nAmbari去部署HDP，\n当然，两者也可以互相，也可以去部署Apache Hadoop\nCloudera Manager安装之利用parcels方式安装3节点集群（包含最新稳定版本或指定版本的安装）（添加服务）\nAmbari安装之部署3个节点的HA分布式集群\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"1. 前言\n熟悉机器学习的童鞋都知道，优化方法是其中一个非常重要的话题，最常见的情形就是利用目标函数的导数通过多次迭代来求解无约束最优化问题。实现简单，coding 方便，是训练模型的必备利器之一。这篇博客主要总结一下使用导数的最优化方法的几个基本方法，梳理梳理相关的数学知识，本人也是一边写一边学，如有问题，欢迎指正，共同学习，一起进步。\n2. 几个数学概念\n1) 梯度（一阶导数）\n考虑一座在 (x1, x2) 点高度是 f(x1, x2) 的山。那么，某一点的梯度方向是在该点坡度最陡的方向，而梯度的大小告诉我们坡度到底有多陡。注意，梯度也可以告诉我们不在最快变化方向的其他方向的变化速度（二维情况下，按照梯度方向倾斜的圆在平面上投影成一个椭圆）。对于一个含有 n 个变量的标量函数，即函数输入一个 n 维 的向量，输出一个数值，梯度可以定义为：\n2) Hesse 矩阵（二阶导数）\nHesse 矩阵常被应用于牛顿法解决的大规模优化问题(后面会介绍)，主要形式如下：\n当 f(x) 为二次函数时，梯度以及 Hesse 矩阵很容易求得。二次函数可以写成下列形式：\n其中 A 是 n 阶对称矩阵，b 是 n 维列向量， c 是常数。f(x) 梯度是 Ax+b, Hesse 矩阵等于 A。\n3) Jacobi 矩阵\nJacobi 矩阵实际上是向量值函数的梯度矩阵，假设F:Rn→Rm 是一个从n维欧氏空间转换到m维欧氏空间的函数。这个函数由m个实函数组成: 。这些函数的偏导数(如果存在)可以组成一个m行n列的矩阵(m by n)，这就是所谓的雅可比矩阵：\n总结一下,\na) 如果 f(x) 是一个标量函数，那么雅克比矩阵是一个向量，等于 f(x) 的梯度， Hesse 矩阵是一个二维矩阵。如果 f(x) 是一个向量值函数，那么Jacobi 矩阵是一个二维矩阵，Hesse 矩阵是一个三维矩阵。\nb) 梯度是 Jacobian 矩阵的特例，梯度的 jacobian 矩阵就是 Hesse 矩阵（一阶偏导与二阶偏导的关系）。\n3. 优化方法\n1) Gradient Descent\nGradient descent 又叫 steepest descent，是利用一阶的梯度信息找到函数局部最优解的一种方法，也是机器学习里面最简单最常用的一种优化方法。Gradient descent 是 line search 方法中的一种，主要迭代公式如下：\n其中，是第 k 次迭代我们选择移动的方向，在 steepest descent 中，移动的方向设定为梯度的负方向， 是第 k 次迭代用 line search 方法选择移动的距离，每次移动的距离系数可以相同，也可以不同，有时候我们也叫学习率（learning rate）。在数学上，移动的距离可以通过 line search 令导数为零找到该方向上的最小值，但是在实际编程的过程中，这样计算的代价太大，我们一般可以将它设定位一个常量。考虑一个包含三个变量的函数 ，计算梯度得到 。设定 learning rate = 1，算法代码如下：\n# Code from Chapter 11 of Machine Learning: An Algorithmic Perspective # by Stephen Marsland (http://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html) # Gradient Descent using steepest descent from numpy import * def Jacobian(x): return array([x[0], 0.4*x[1], 1.2*x[2]]) def steepest(x0): i = 0 iMax = 10 x = x0 Delta = 1 alpha = 1 while i<iMax and Delta>10**(-5): p = -Jacobian(x) xOld = x x = x + alpha*p Delta = sum((x-xOld)**2) print 'epoch', i, ':' print x, '\\n' i += 1 x0 = array([-2,2,-2]) steepest(x0)\nView Code\nSteepest gradient 方法得到的是局部最优解，如果目标函数是一个凸优化问题，那么局部最优解就是全局最优解，理想的优化效果如下图，值得注意一点的是，每一次迭代的移动方向都与出发点的等高线垂直：\n需要指出的是，在某些情况下，最速下降法存在锯齿现象（ zig-zagging）将会导致收敛速度变慢:\n粗略来讲，在二次函数中，椭球面的形状受 hesse 矩阵的条件数影响，长轴与短轴对应矩阵的最小特征值和最大特征值的方向，其大小与特征值的平方根成反比，最大特征值与最小特征值相差越大，椭球面越扁，那么优化路径需要走很大的弯路，计算效率很低。\n2) Newton's method\n在最速下降法中，我们看到，该方法主要利用的是目标函数的局部性质，具有一定的“盲目性”。牛顿法则是利用局部的一阶和二阶偏导信息，推测整个目标函数的形状，进而可以求得出近似函数的全局最小值，然后将当前的最小值设定近似函数的最小值。相比最速下降法，牛顿法带有一定对全局的预测性，收敛性质也更优良。牛顿法的主要推导过程如下：\n第一步，利用 Taylor 级数求得原目标函数的二阶近似：\n第二步，把 x 看做自变量， 所有带有 x^k 的项看做常量，令一阶导数为 0 ，即可求近似函数的最小值：\n即：\n第三步，将当前的最小值设定近似函数的最小值（或者乘以步长)。\n与 1) 中优化问题相同，牛顿法的代码如下：\nNewton.py\n# Code from Chapter 11 of Machine Learning: An Algorithmic Perspective # by Stephen Marsland (http://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html) # Gradient Descent using Newton's method from numpy import * def Jacobian(x): return array([x[0], 0.4*x[1], 1.2*x[2]]) def Hessian(x): return array([[1,0,0],[0,0.4,0],[0,0,1.2]]) def Newton(x0): i = 0 iMax = 10 x = x0 Delta = 1 alpha = 1 while i<iMax and Delta>10**(-5): p = -dot(linalg.inv(Hessian(x)),Jacobian(x)) xOld = x x = x + alpha*p Delta = sum((x-xOld)**2) i += 1 print x x0 = array([-2,2,-2]) Newton(x0)\nView Code\n上面例子中由于目标函数是二次凸函数，Taylor 展开等于原函数，所以能一次就求出最优解。\n牛顿法主要存在的问题是：\nHesse 矩阵不可逆时无法计算\n矩阵的逆计算复杂为 n 的立方，当问题规模比较大时，计算量很大，解决的办法是采用拟牛顿法如 BFGS, L-BFGS, DFP, Broyden's Algorithm 进行近似。\n如果初始值离局部极小值太远，Taylor 展开并不能对原函数进行良好的近似\n3) Levenberg–Marquardt Algorithm\nLevenberg–Marquardt algorithm 能结合以上两种优化方法的优点，并对两者的不足做出改进。与 line search 的方法不同，LMA 属于一种“信赖域法”(trust region)，牛顿法实际上也可以看做一种信赖域法，即利用局部信息对函数进行建模近似，求取局部最小值。所谓的信赖域法，就是从初始点开始，先假设一个可以信赖的最大位移 s（牛顿法里面 s 为无穷大），然后在以当前点为中心，以 s 为半径的区域内，通过寻找目标函数的一个近似函数（二次的）的最优点，来求解得到真正的位移。在得到了位移之后，再计算目标函数值，如果其使目标函数值的下降满足了一定条件，那么就说明这个位移是可靠的，则继续按此规则迭代计算下去；如果其不能使目标函数值的下降满足一定的条件，则应减小信赖域的范围，再重新求解。\nLMA 最早提出是用来解决最小二乘法曲线拟合的优化问题的，对于随机初始化的已知参数 beta， 求得的目标值为：\n对拟合曲线函数进行一阶 Jacobi 矩阵的近似：\n进而推测出 S 函数的周边信息：\n位移是多少时得到 S 函数的最小值呢？通过几何的概念，当残差 垂直于 J 矩阵的 span 空间时， S 取得最小（至于为什么？请参考之前博客的最后一部分)\n我们将这个公式略加修改，加入阻尼系数得到：\n就是莱文贝格－马夸特方法。这种方法只计算了一阶偏导，而且不是目标函数的 Jacobia 矩阵，而是拟合函数的 Jacobia 矩阵。当 大的时候可信域小，这种算法会接近最速下降法， 小的时候可信域大，会接近高斯-牛顿方法。\n算法过程如下：\n给定一个初识值 x0\n当 并且没有到达最大迭代次数时\n重复执行:\n算出移动向量\n计算更新值：\n计算目标函数真实减少量与预测减少量的比率\nif ，接受更新值\nelse if ，说明近似效果很好，接受更新值，扩大可信域（即减小阻尼系数）\nelse: 目标函数在变大，拒绝更新值，减小可信域（即增加阻尼系数）\n直到达到最大迭代次数\n维基百科在介绍 Gradient descent 时用包含了细长峡谷的 Rosenbrock function\n展示了 zig-zagging 锯齿现象：\n用 LMA 优化效率如何。套用到我们之前 LMA 公式中，有：\n代码如下：\nLevenbergMarquardt.py\n# Code from Chapter 11 of Machine Learning: An Algorithmic Perspective # by Stephen Marsland (http://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html) # The Levenberg Marquardt algorithm from numpy import * def function(p): r = array([10*(p[1]-p[0]**2),(1-p[0])]) fp = dot(transpose(r),r) #= 100*(p[1]-p[0]**2)**2 + (1-p[0])**2 J = (array([[-20*p[0],10],[-1,0]])) grad = dot(transpose(J),transpose(r)) return fp,r,grad,J def lm(p0,tol=10**(-5),maxits=100): nvars=shape(p0)[0] nu=0.01 p = p0 fp,r,grad,J = function(p) e = sum(dot(transpose(r),r)) nits = 0 while nits<maxits and linalg.norm(grad)>tol: nits += 1 fp,r,grad,J = function(p) H=dot(transpose(J),J) + nu*eye(nvars) pnew = zeros(shape(p)) nits2 = 0 while (p!=pnew).all() and nits2<maxits: nits2 += 1 dp,resid,rank,s = linalg.lstsq(H,grad) pnew = p - dp fpnew,rnew,gradnew,Jnew = function(pnew) enew = sum(dot(transpose(rnew),rnew)) rho = linalg.norm(dot(transpose(r),r)-dot(transpose(rnew),rnew)) rho /= linalg.norm(dot(transpose(grad),pnew-p)) if rho>0: update = 1 p = pnew e = enew if rho>0.25: nu=nu/10 else: nu=nu*10 update = 0 print fp, p, e, linalg.norm(grad), nu p0 = array([-1.92,2]) lm(p0)\nView Code\n大概 5 次迭代就可以得到最优解 (1, 1).\nLevenberg–Marquardt algorithm 对局部极小值很敏感，维基百科举了一个二乘法曲线拟合的例子，当使用不同的初始值时，得到的结果差距很大，我这里也有 python 代码，就不细说了。\n4) Conjugate Gradients\n共轭梯度法也是优化模型经常经常要用到的一个方法，背后的数学公式和原理稍微复杂一些，光这一个优化方法就可以写一篇很长的博文了，所以这里并不打算详细讲解每一步的推导过程，只简单写一下算法的实现过程。与最速梯度下降的不同，共轭梯度的优点主要体现在选择搜索方向上。在了解共轭梯度法之前，我们首先简单了解一下共轭方向：\n共轭方向和马氏距离的定义有类似之处，他们都考虑了全局的数据分布。如上图，d(1) 方向与二次函数的等值线相切，d(1) 的共轭方向 d(2) 则指向椭圆的中心。所以对于二维的二次函数，如果在两个共轭方向上进行一维搜索，经过两次迭代必然达到最小点。前面我们说过，等值线椭圆的形状由 Hesse 矩阵决定，那么，上图的两个方向关于 Hessen 矩阵正交，共轭方向的定义如下：\n如果椭圆是一个正圆， Hessen 矩阵是一个单位矩阵，上面等价于欧几里得空间中的正交。\n在优化过程中，如果我们确定了移动方向（GD：垂直于等值线，CG：共轭方向），然后在该方向上搜索极小值点（恰好与该处的等值线相切），然后移动到最小值点，重复以上过程，那么 Gradient Descent 和 Conjugate gradient descent 的优化过程可以用下图的绿线与红线表示：\n讲了这么多，共轭梯度算法究竟是如何算的呢？\n给定一个出发点 x0 和一个停止参数 e, 第一次移动方向为最速下降方向:\nwhile :\n用 Newton-Raphson 迭代计算移动距离，以便在该搜索方向移动到极小，公式就不写了，具体思路就是利用一阶梯度的信息向极小值点跳跃搜索\n移动当前的优化解 x：\n用 Gram-Schmidt 方法构造下一个共轭方向，即 , 按照 的确定公式又可以分为 FR 方法和 PR 和 HS 等。\n在很多的资料中，介绍共轭梯度法都举了一个求线性方程组 Ax = b 近似解的例子，实际上就相当于这里所说的\n还是用最开始的目标函数     来编写共轭梯度法的优化代码：\n# Code from Chapter 11 of Machine Learning: An Algorithmic Perspective # by Stephen Marsland (http://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html) # The conjugate gradients algorithm from numpy import * def Jacobian(x): #return array([.4*x[0],2*x[1]]) return array([x[0], 0.4*x[1], 1.2*x[2]]) def Hessian(x): #return array([[.2,0],[0,1]]) return array([[1,0,0],[0,0.4,0],[0,0,1.2]]) def CG(x0): i=0 k=0 r = -Jacobian(x0) p=r betaTop = dot(r.transpose(),r) beta0 = betaTop iMax = 3 epsilon = 10**(-2) jMax = 5 # Restart every nDim iterations nRestart = shape(x0)[0] x = x0 while i < iMax and betaTop > epsilon**2*beta0: j=0 dp = dot(p.transpose(),p) alpha = (epsilon+1)**2 # Newton-Raphson iteration while j < jMax and alpha**2 * dp > epsilon**2: # Line search alpha = -dot(Jacobian(x).transpose(),p) / (dot(p.transpose(),dot(Hessian(x),p))) print \"N-R\",x, alpha, p x = x + alpha * p j += 1 print x # Now construct beta r = -Jacobian(x) print \"r: \", r betaBottom = betaTop betaTop = dot(r.transpose(),r) beta = betaTop/betaBottom print \"Beta: \",beta # Update the estimate p = r + beta*p print \"p: \",p print \"----\" k += 1 if k==nRestart or dot(r.transpose(),p) <= 0: p = r k = 0 print \"Restarting\" i +=1 print x x0 = array([-2,2,-2]) CG(x0)\nView Code\n参考资料：\n[1] Machine Learning: An Algorithmic Perspective, chapter 11\n[2] 最优化理论与算法（第2版），陈宝林\n[3] wikipedia"}
{"content2":"转载请注明出处：http://www.cnblogs.com/ymingjingr/p/4271742.html\n目录\n机器学习基石笔记1——在何时可以使用机器学习(1)\n机器学习基石笔记2——在何时可以使用机器学习(2)\n机器学习基石笔记3——在何时可以使用机器学习(3)(修改版)\n机器学习基石笔记4——在何时可以使用机器学习（4）\n机器学习基石笔记5——为什么机器可以学习（1）\n机器学习基石笔记6——为什么机器可以学习（2）\n机器学习基石笔记7——为什么机器可以学习（3）\n机器学习基石笔记8——为什么机器可以学习（4）\n机器学习基石笔记9——机器可以怎样学习（1）\n机器学习基石笔记10——机器可以怎样学习（2）\n机器学习基石笔记11——机器可以怎样学习（3）\n机器学习基石笔记12——机器可以怎样学习（4）\n机器学习基石笔记13——机器可以怎样学得更好（1）\n机器学习基石笔记14——机器可以怎样学得更好（2）\n机器学习基石笔记15——机器可以怎样学得更好（3）\n机器学习基石笔记16——机器可以怎样学得更好（4）\n二、Learning to Answer Yes/No\n二元分类。\n解决上一章提出的银行发行信用卡的问题。\n2.1 Perceptron Hypothesis Set\n感知器的假设空间。\n还是银行发信用卡的例子，银行可能掌握了用户的各种属性，如年龄，年薪，工作年限，负债情况等等，这些属性可以作为上面提到的样本输入的向量属性值。但是这样还是无法进行机器学习，因为我们还需要另一个输入，即假设空间H。假设空间该如何表示呢？本节提出了一种表示方式，这种学习的模型称之为感知器（Perceptron）。\n其实感知器的产生很早，算是第一代的单层神经网络，这里就不多做介绍，在其他的读书笔记中进行说明。\n这种假设空间的思想就类似考试给的成绩，对每一题给一个特定的分数，即权重，说白了就是给输入向量的每个属性乘以一个加权值，在设计一个及格线，即所谓的阈值或者叫门槛值（threshold），如果加权求和的分数大于这个及格线就叫及格了，即对应的输出值为1，小于这个及格线成为不及格，对应的输出值为-1。\n其中h(x)∈H，如公式2-1所示。\n（公式2-1）\n其中sign括号中所包含的内容大于0时，取+1；小于0时，取-1。\n此时可以对h(x)做一些数学上的简化，注意这仅仅是一种数学表示方式的简化，如公式2-2所示。\n(公式2-2）\n如上所示，将阈值的负数表示为权值向量中的一项，用 表示，而对应权值分量的输入分量则被默认为1，用 最终将公式简化为两个向量内积的形式，其中T表示转置。\n这里必须说明一个问题，就是不同h(x) 对应着不同的向量，即可以说假设空间H就是向量 的取值范围。\n这么描述还是很抽象，因此引入一种方式就是使用图像（或者可以说是几何）来更形象更具体的来说明以上函数。（这里说点题外话，由于二元函数和三元函数可以使用几何图像来一一对应，用几何的方式更直观的表示函数的意义，方便大家理解，这在以后的章节中会不断使用）\n为了理解的方便将输入向量的维度限制为两个，即h函数可以表示成公式2-3。\n(公式2-3）\n将输入向量对应于一个二维平面上的点（如果向量的维度更高，对应于一个高维空间中的点）。\n输出y（在分类问题中又称作标签，label）使用○表示+1，×表示-1。\n假设h对应一条条的直线（如果在输入向量是高维空间的话，则对应于一个超平面）这里不止一条，不同的权值向量对应不同的直线，因为sign是以0为分界线的函数，所以可以设 ，该式恰是一条直线的表示。\n因此每条边的一边为正的，而另一边为表示为负的。\n最终得到的图像如-1所示。\n-1 感知器在维度为2时的几何表示\n因此这里将感知器作为一条二元线性分类器（linear ( binary) classifiers）。\n2.2 Perceptron Learning Algorithm (PLA)\n感知器学习算法。\n在第一章中，我们介绍过一个机器学习模型由两部分组成，而上一节仅仅介绍了它其中的一部分即假设空间H如何表示。\n本节我们将更详细的介绍感知器的算法A，即如何从假设空间中找到一个近似未知目标函数f的最好假设g(x)。\n问题是，我们如何找到这个g(x)呢？\n首先考虑，g(x)和目标函数f越接近越好，但问题是我们不知道f（如果知道了就不需要学习了）\n但是我们知道些什么呢？知道的是样本输入x在f(x)作用下得到的标记y。\n所以如果我们能使得g(x)在所有的样本输入中都能够得到跟f函数作用过输入得到的输出一样的话，我们认为这时的g是不错的。（在后面的章节还会在这种思想的基础上更深入的讨论这一问题）\n但是问题又来了，假设空间H的函数h(x)有无数种表示，即向量w有无数种取值。（如在二元输入时，假设空间对于在二维平面上的直线，在那个空间中可以画出无数条直线）\n面对这无数多种情况，我们又该如何求解？\n我们想到一个简单的方式，就是一步一步的修正错误的分类，在二维平面中可以想象成一条初始的直线，在经过不断的纠正它的错误（就是旋转平移之类的）使得最终的结果可以达到希望的效果。\n还要在重复上一节中已经得到的一个结论，在感知器模型中，每一个假设函数h都对应一个权值向量。因此我们要做的就是不断修正这个权值向量使得最接近目标函数f。\n下面来详细介绍一下PLA。\n首先我们在设置初始 （注意此处是向量不是向量的分量！），比如设置为0向量，然后使用训练样本来将权值向量修正的更接近目标函数f。其修正步骤如下：\n将权值向量的修正次数表示为t，t=0,1,2,…\n在何种情况下需要修正向量 呢？如公式2-4所示。\n(公式2-4）\n其中训练样本 ，为在t次时使用的输入向量，而为在t次时的标记量。\n该公式2-4的意思就是在t次时，选择的权值向量，有一个训练样本使得在经过 （即）假设计算的得到的标签与f(x)得到的标签不一致。\n在这种情况下就需要对权值向量进行修改，使它符合条件。修改的公式如公式2-5所示。\n（公式2-5）\n从直觉上理解这个公式相对困难，我们还是将它化成一个几何图形，更准确的说法变成向量加减的形式去理解它，如-2所示。\n-2 公式2-5的几何解 a) b)\n-2a中是在本身标记为+1时，权值向量和输入向量的内积为负数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。\n-2b中是在本身标记为-1时，权值向量和输入向量的内积为正数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。\n而非常巧合的是，只需要乘以一个该训练样本的标记就可以将这两种情况合为一种情况如公式2-5所示。\n如此这般的重复查找错误样本和修改加权向量，直到再也找不到可以使公式2-4成立的样本为止，此时得到的加权向量，即为我们想要的最终g。\n描述了上面内容之后，你很可能有一个疑问就如何查找错误样本点，或者如何确定没有错误的点了。\n一个简单的方式就是将训练样本编号，从1到n，整个训练样本就有n个点。以按从1到n的顺序不断查找错误点，如果没有错就自动的用下一个样本点继续查找，当从1到n这n个样本点都没有产生错误时，算法即结束得到g。将这种方式的算法叫做Cyclic PLA。\n这时候就又出来几个新的问题，第一，这个算法一定会找到一个能使所有的样本都不符合（即都被分对了类）的情况吗？就是这个算法会不会停止？第二个问题这个算法找到的真的是最好的g吗？看起来好像只是在训练样本中才符合这一性质，如果出现新的样本的话又会如何呢？\n第一个问题下一小节将进行介绍，而其他问题会在后面的章节中讨论。\n2.3 Guarantee of PLA\nPLA算法可行的保障。\nPLA算法只有在满足训练样本是线性可分（linear separable）的情况下才可以停止。\n什么是线性可分呢？简单的说就是存在一条直线能将两类样本点完全分开。\n如-3所示。\n-3 线性可分与线性不可分\n其中最左边的为线性可分的训练样本，而右边两个图形为线性不可分的两种情况，这两种情况会在后面的章节一一解释。\n我们需要证明在线性可分的情况下，权值向量在经过一段时间的修正会停止，即t次修正会有一个上界。\n首先我们考虑是否每次修正都可以使得权值向量 变得更好，就是是否会更接近未知的目标函数所表示的向量。有了这个思路，我们先假设目标函数的权值向量为，可以求解出两个向量相似度的度量方式有很多，其中比较常用的一种方式就是求两个向量的内积，于是我们对和做内积。其中T表示为停止时的次数。\n直接使用这两个向量做内积，其内积越大并不能代表这两个向量越接近，因为向量本身的变长也可以导致这一现象。因此我们需要求解的是这两个向量做归一化（就是各自除以自身的L1范式得到单位向量）之后的内积，这时它俩的内积有了上界即为1，如公式2-6所示。\n(公式2-6）\n乍一看公式2-6完全无从下手，是未知目标向量，是终止时的向量，也是一个未知向量，因此思路就是将其中一个未知量消除，消除的可能性不大，因此选择消除在公式中的不确定性，如公式2-7所示是解决归一化之前两个向量内积的问题。\n取所有的样本中的最小乘积（因为是在线性可分情况下的目标函数，所以所有的必定大于等于0。）\n进行迭代\n又因为初始值设置为0向量，因此\n(公式2-7）\n除了不容易确定之外，的L1范式也不容易得出，如公式2-8是求解L1范式的不等式，其思想如公式2-7。\n因为只有在犯错的情况下才会进行改变，那什么时候是犯错，就是在公式2-4成立的情况，即，该公式等价于 ，因此如下\n（公式2-8）\n通过公式2-7和公式2-8可以将公式2-6写成如公式2-9，如下式所示。\n（公式2-9）\n将公式2-9中的常数设置为C，该公式如公式2-10所示。\n(公式2-10）\n可以看出权值向量和目标函数内积会以的速度不断的增长，但是这种增长不是没有限制的，它最多只能等于1。\n因此有以下结论，如公式2-11所示。\n（公式2-11）\n求解得到公式2-12的结论。\n（公式2-12）\n将公式2-12中的值分别使用简单的数字符号代替，如公式2-13和公式2-14所示。\n（公式2-13）\n（公式2-14）\n从公式2-12中就可以看出T是有上界，即在线性可分的情况下PLA算法最终会停止，找到一个最接近目标函数的假设函数g。\n2.4 Non-Separable Data\n线性不可分的数据。\n上一节的阐述PLA这个算法一定会停下来这一结论，是建立在存在一个目标函数，可以将所有的数据点都线性分开这个假设的基础之上。对于一堆复杂的数据，如何能确定它一定是线性可分的？比如一个PLA算法运行了很长时间仍然没有停止，此时存在两种可能性，一是该数据集是线性可分的，但是还没有运行结束；另一种，压根就不存在一条直线可以将数据集分开，就是压根这个算法就不会终止。假如是后者又该如何处理？\n首先还是要解释下为什么会出现后者，此种情况出现的概率大吗？\n出现不可分的一种可能是从未知目标函数中产生的训练样本存在噪音（noise），如录入样本时有人工的错误等情况导致数据本身不正确，使得最终本可以线性可分的样本集变得线性不可分了，如-4所示。\n-4 加入噪音的机器学习流程图\n而噪音占整个数据集的比例一般不会太大，如-5所示。这种情况下我们又该如何计算出最佳的假设g呢？\n-5 存在噪音时线性不可分的情况\n其实-5已经在前面的小节中出现过。\n一种新的思路是找出犯错最少的权值向量 ，如公式2-15所示。\n（公式2-15）\n其中表示当满足条件时输出1，否则为0。但是这个公式在数学上是NP难问题，我们无法直接求解，于是我们需要找出一种近似的算法来求解这个问题。\n这里介绍一个叫pocket的算法，它的本质是一种贪心算法，做一简单的介绍：\n也是随机的初始化一个权值向量\n随机的使用n个点中的一个点去发现是否有错误（此处与cyclic PLA使用的循环方式有所不同，不是按顺序一个一个的查看是否符合条件，而是在n个点中随机的抽取，这种方式可以增加其寻找最优解的速度）\n和PLA一样使用公式2-5进行修正.\n如果有了修正，则计算出刚刚修正过的权值向量和上一个权值向量到底谁犯的错误比较少，将少的保留重复第2步到第4步的动作。\n假如很长时间都没有新的权值向量比当前的权值向量犯错更少，则返回该向量作为函数g。"}
{"content2":"0. 前言\n1. 损失函数\n2. Margin\n3. Cross-Entropy vs. Squared Error\n总结\n参考资料\n0. 前言\n“尽管新技术新算法层出不穷，但是掌握好基础算法就能解决手头 90% 的机器学习问题。”\n本系列参考书 \"Hands-on machine learning with scikit-learn and tensorflow\"以及kaggle相关资料，但是这篇文章没有参考，🤣\n“观察到的一个有意思的细节：一些喜好机器学习或者数据科学的初学工程师和有机器学习或者数据科学背景的科学家，在工作上的主要区别在于如何对待负面的实验（包括线下和线上）结果。初学者往往就开始琢磨如何改模型，加Feature，调参数；思考如何从简单模型转换到复杂模型。有经验的人往往更加去了解实验的设置有没有问题；实验的Metrics的Comparison是到底怎么计算的；到真需要去思考模型的问题的时候，有经验的人往往会先反思训练数据的收集情况，测试数据和测试评测的真实度问题。初学者有点类似程咬金的三板斧，有那么几个技能，用完了，要是还没有效果，也就完了。而有经验的数据科学家，往往是从问题出发，去看是不是对问题本质的把握（比如优化的目标是不是对；有没有Counterfactual的情况）出现了偏差，最后再讨论模型。”\n—— by @洪亮劼\n1. 损失函数\n前面一篇讨论了PRC、ROC、AUC等评测模型的不同侧重点，另外一方面，模型损失函数（目标函数）是机器学习里另外一个较为本质的问题，由于机器学习的损失函数根据任务、模型的不同，演化出各种形式，下面只谈谈分类模型里常见的情形。\n假设一个二分类问题，样本空间是 y={-1,1}，一个分类模型对其进行预测，输出值是f(x)，f > 0 判定为1，f < 0判定为 -1。计算模型预测准确率的时候，样本真实分类 y 如果和 f 符号相同，则表示分类正确，符号相反，则模型分类错误。表示成分类误差，可记为 if y * f(x) > 0 then error = 0 else error = 1。\n实际情况中，极少看到直接用分类误差作为模型的目标函数，原因主要有几点，其一，loss = 0 or 1 是平行于 x 轴的射线，求导为 0（原点除外），没法用梯度下降法来优化模型，而实际上这是个NP-Hard离散的非凸优化问题。其二，要训练一个好的模型还要让模型感知到，某个样本尽管分类正确了，但是到底有多“正确”，如果确信度低了还需要继续优化。举个例子：有三种水果，两种模型都进行了训练，在测试集上概率分布表现如下：\n模型A：\n预测概率\n真实概率\n分类误差\n0.3 0.3 0.4\n0 0 1 (苹果)\n0\n0.3 0.4 0.3\n0 1 0 (梨子)\n0\n0.1 0.2 0.7\n1 0 0 (桃子)\n1\n模型B：\n预测概率\n真实概率\n分类误差\n0.1 0.2 0.7\n0 0 1 (苹果)\n0\n0.1 0.8 0.1\n0 1 0 (梨子)\n0\n0.4 0.5 0.1\n1 0 0 (桃子)\n1\n目测可以看出，模型A和模型B分类误差都是0.333，但是模型B更“靠谱”一些。问题来了，这个“靠谱”如何衡量？\n2. Margin\nmargin 是衡量某次预测到底有多“准确”的一个指标，定义为 y*f(x)。简单说，希望正样本预测值为正尽量大，负样本预测值为负尽量小，就需要模型 max margin。\n以margin作为横轴，黑色表示分类loss，红色表示log loss，蓝色表示 hinge loss，绿色表示 square error，可以看到随着 margin 变大，loss 总体是单调递减的，但是 squared error 超过1后会递增：\n从上图可以看到，hinge loss 在margin达到一定阈值后（很确信分类正确的样本），loss降为0，对整个模型训练其实已经没有影响了。log loss 不管margin多大，loss永远不会降为0，会一直对模型有影响。squared error 里，如果f(x) 输出绝对值可以大于1，那么margin太大的点，对模型反而有不好的影响：\n中场休息时间。。。喝口茶~ 欢迎关注公众号：kaggle实战，或博客：http://www.cnblogs.com/daniel-D/\n3. Cross-Entropy vs. Squared Error\ncross-entropy 可以简单理解为上面的 log loss，在深度学习里面，最后一层往往是通过 softmax 计算出概率分布margin区间为[0,1]。squared error 并不存在上述 margin 太大的点对模型反而有负面影响的情况，这时候该采用 Cross-Entropy or Squared Error?\n结论是如果你使用的是神经网络的分类模型，建议使用 Cross-entropy。\n做分类任务的深度神经网络，最后一层一般为softmax，softmax 计算公式如下：\n一般采用反向传播的梯度下降方法优化，下面先把 softmax 的计算过程逐步剖析开来：（纠正图中一个错误：P=A * V）\n上图中O1，O2，O3 表示softmax的输入节点，每个节点先经过指数化得到 A，然后求和得到 S 即分母，倒数得到 V，再各自和指数相乘，得到该节点输出的概率 P。为了图片结构稍微美观点，这里 P2 就没画出来了。对于上图的 O1 来说，导数来源于虚线的链路，箭头上的文字表示局部导数，它的导数实际来自两部分，一是直接相连的 P1，以及无直接链接的P2、P3……对应输入层的某个节点 O1 应用链式法则，P1 节点对其偏导如下\n如果 i = j，即图中 O1 到 P1，导数由两条链路组成：\n如果 i ≠ j，即图中 O1 到 P3，导数由一条链路组成：\n如果模型训练得很好，Pj 和Pi都接近0或者其中一个接近于1，可以看到两种情况梯度都是接近于0，符合预期；如果模型很差，把某个错误的类别的概率也计算成1，那么 Pj 和 Pi 都接近0者其中一个接近于1，梯度也很小，不太符合预期，看看 Cross-entropy 是如何解决这个问题的。\n对于Cross-entropy（Logloss）误差，计算公式为：\n可以看到 Cross-entropy 只关注正确label上的概率大小，上图中 ，只有连到到 P1 的链路才是对梯度计算有效的，对 i = j 也就是上图中的 O1 来说：\n对 i ≠ j 也就是 O2 到 L1 链路来说：\n--备注：x 应该是 - 1/p_i，上面梯度应该多个负号，截图太累\n如果模型很差，正确 label 上的概率 Pi 接近0，实际上不会影响梯度大小。\n但是如果是 Squared Error，上图 P 到 L 连接上的 x 并不包含 logLoss 的倒数形式。由于正确类别和错误类别的损失函数都会影响 Oi 的偏导，整体公式比较复杂，这里就不做详细的推导了，但是整体上是先相乘，然后求和，梯度会很小，给优化带来阻碍。\n总结\nsoftmax部分在完全分类正确或者分类完全错误的情况下，该部分偏导都接近于0\nlogLoss 偏导中有倒数，可以“中和”softmax这个缺点，Squared Error 如果初始化不好很难克服这个问题，使用softmax分类模型的 Loss 推荐使用 cross entropy 而不是 classification error 或 squared error\nlogLoss 对完全错误分类的惩罚极大，但是其实偏导不会超过1\n对于上述 logLoss 这种“虚张声势”的做法，直接看logLoss可能无法这种体现模型的准确性，比如把1个样本分得很错的模型 vs. 把多个样本分得不那么错的模型，可能后者的 logLoss 更小，建议直接用分类误差评估。\n参考资料\nWhy You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training\nCross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison\nThe Softmax function and its derivative\nWhat are the impacts of choosing different loss functions in classification to approximate 0-1 loss\n附：公众号"}
{"content2":"原文：http://blog.csdn.net/heyongluoyao8/article/details/47840255\n常见的机器学习&数据挖掘知识点\n转载请说明出处\nBasis(基础)：\nSSE(Sum of Squared Error, 平方误差和)\nSAE(Sum of Absolute Error, 绝对误差和)\nSRE(Sum of Relative Error, 相对误差和)\nMSE(Mean Squared Error, 均方误差)\nRMSE(Root Mean Squared Error, 均方根误差)\nRRSE(Root Relative Squared Error, 相对平方根误差)\nMAE(Mean Absolute Error, 平均绝对误差)\nRAE(Root Absolute Error, 平均绝对误差平方根)\nMRSE(Mean Relative Square Error, 相对平均误差)\nRRSE(Root Relative Squared Error, 相对平方根误差)\nExpectation(期望)&Variance(方差)\nStandard Deviation(标准差，也称Root Mean Squared Error, 均方根误差)\nCP(Conditional Probability, 条件概率)\nJP(Joint Probability, 联合概率)\nMP(Marginal Probability, 边缘概率)\nBayesian Formula(贝叶斯公式)\nCC(Correlation Coefficient, 相关系数)\nQuantile (分位数)\nCovariance(协方差矩阵)\nGD(Gradient Descent, 梯度下降)\nSGD(Stochastic Gradient Descent, 随机梯度下降)\nLMS(Least Mean Squared, 最小均方)\nLSM(Least Square Methods, 最小二乘法)\nNE(Normal Equation, 正规方程)\nMLE(Maximum Likelihood Estimation, 极大似然估计)\nQP(Quadratic Programming, 二次规划)\nL1 /L2 Regularization(L1/L2正则, 以及更多的, 现在比较火的L2.5正则等)\nEigenvalue(特征值)\nEigenvector(特征向量)\nCommon Distribution(常见分布)：\nDiscrete Distribution(离散型分布)：\nBernoulli Distribution/Binomial Distribution(贝努利分布/二项分布)\nNegative Binomial Distribution(负二项分布)\nMultinomial Distribution(多项分布)\nGeometric Distribution(几何分布)\nHypergeometric Distribution(超几何分布)\nPoisson Distribution (泊松分布)\nContinuous Distribution (连续型分布)：\nUniform Distribution(均匀分布)\nNormal Distribution/Gaussian Distribution(正态分布/高斯分布)\nExponential Distribution(指数分布)\nLognormal Distribution(对数正态分布)\nGamma Distribution(Gamma分布)\nBeta Distribution(Beta分布)\nDirichlet Distribution(狄利克雷分布)\nRayleigh Distribution(瑞利分布)\nCauchy Distribution(柯西分布)\nWeibull Distribution (韦伯分布)\nThree Sampling Distribution(三大抽样分布)：\nChi-square Distribution(卡方分布)\nt-distribution(t-分布)\nF-distribution(F-分布)\nData Pre-processing(数据预处理)：\nMissing Value Imputation(缺失值填充)\nDiscretization(离散化)\nMapping(映射)\nNormalization(归一化/标准化)\nSampling(采样)：\nSimple Random Sampling(简单随机采样)\nOffline Sampling(离线等可能K采样)\nOnline Sampling(在线等可能K采样)\nRatio-based Sampling(等比例随机采样)\nAcceptance-rejection Sampling(接受-拒绝采样)\nImportance Sampling(重要性采样)\nMCMC(Markov Chain MonteCarlo 马尔科夫蒙特卡罗采样算法：Metropolis-Hasting& Gibbs)\nClustering(聚类)：\nK-MeansK-Mediods\n二分K-Means\nFK-Means\nCanopy\nSpectral-KMeans(谱聚类)\nGMM-EM(混合高斯模型-期望最大化算法解决)\nK-Pototypes\nCLARANS(基于划分)\nBIRCH(基于层次)\nCURE(基于层次)\nSTING(基于网格)\nCLIQUE(基于密度和基于网格)\n2014年Science上的密度聚类算法等\nClustering Effectiveness Evaluation(聚类效果评估)：\nPurity(纯度)\nRI(Rand Index, 芮氏指标)\nARI(Adjusted Rand Index, 调整的芮氏指标)\nNMI(Normalized Mutual Information, 规范化互信息)\nF-meaure(F测量)\nClassification&Regression(分类&回归)：\nLR(Linear Regression, 线性回归)\nLR(Logistic Regression, 逻辑回归)\nSR(Softmax Regression, 多分类逻辑回归)\nGLM(Generalized Linear Model, 广义线性模型)\nRR(Ridge Regression, 岭回归/L2正则最小二乘回归)，LASSO(Least Absolute Shrinkage and Selectionator Operator , L1正则最小二乘回归)\nDT(Decision Tree决策树)\nRF(Random Forest, 随机森林)\nGBDT(Gradient Boosting Decision Tree, 梯度下降决策树)\nCART(Classification And Regression Tree 分类回归树)\nKNN(K-Nearest Neighbor, K近邻)\nSVM(Support Vector Machine, 支持向量机, 包括SVC(分类)&SVR(回归))\nCBA(Classification based on Association Rule, 基于关联规则的分类)\nKF(Kernel Function, 核函数)\nPolynomial Kernel Function(多项式核函数)\nGuassian Kernel Function(高斯核函数)\nRadial Basis Function(RBF径向基函数)\nString Kernel Function 字符串核函数\nNB(Naive Bayesian,朴素贝叶斯)\nBN(Bayesian Network/Bayesian Belief Network/Belief Network 贝叶斯网络/贝叶斯信度网络/信念网络)\nLDA(Linear Discriminant Analysis/Fisher Linear Discriminant 线性判别分析/Fisher线性判别)\nEL(Ensemble Learning, 集成学习)\nBoosting\nBagging\nStacking\nAdaBoost(Adaptive Boosting 自适应增强)\nMEM(Maximum Entropy Model, 最大熵模型)\nClassification EffectivenessEvaluation(分类效果评估)：\nConfusion Matrix(混淆矩阵)\nPrecision(精确度)\nRecall(召回率)\nAccuracy(准确率)\nF-score(F得分)\nROC Curve(ROC曲线)\nAUC(AUC面积)\nLift Curve(Lift曲线)\nKS Curve(KS曲线)\nPGM(Probabilistic Graphical Models, 概率图模型)：\nBN(BayesianNetwork/Bayesian Belief Network/ Belief Network , 贝叶斯网络/贝叶斯信度网络/信念网络)\nMC(Markov Chain, 马尔科夫链)\nMEM(Maximum Entropy Model, 最大熵模型)\nHMM(Hidden Markov Model, 马尔科夫模型)\nMEMM(Maximum Entropy Markov Model, 最大熵马尔科夫模型)\nCRF(Conditional Random Field,条件随机场)\nMRF(Markov Random Field, 马尔科夫随机场)\nViterbi(维特比算法)\nNN(Neural Network, 神经网络)\nANN(Artificial Neural Network, 人工神经网络)\nSNN(Static Neural Network, 静态神经网络)\nBP(Error Back Propagation, 误差反向传播)\nHN(Hopfield Network)\nDNN(Dynamic Neural Network, 动态神经网络)\nRNN(Recurrent Neural Network, 循环神经网络)\nSRN(Simple Recurrent Network, 简单的循环神经网络)\nESN(Echo State Network, 回声状态网络)\nLSTM(Long Short Term Memory, 长短记忆神经网络)\nCW-RNN(Clockwork-Recurrent Neural Network, 时钟驱动循环神经网络, 2014ICML）等.\nDeep Learning(深度学习)：\nAuto-encoder(自动编码器)\nSAE(Stacked Auto-encoders堆叠自动编码器)\nSparse Auto-encoders(稀疏自动编码器)\nDenoising Auto-encoders(去噪自动编码器)\nContractive Auto-encoders(收缩自动编码器)\nRBM(Restricted Boltzmann Machine, 受限玻尔兹曼机)\nDBN(Deep Belief Network, 深度信念网络)\nCNN(Convolutional Neural Network, 卷积神经网络)\nWord2Vec(词向量学习模型)\nDimensionality Reduction(降维)：\nLDA(Linear Discriminant Analysis/Fisher Linear Discriminant, 线性判别分析/Fish线性判别)\nPCA(Principal Component Analysis, 主成分分析)\nICA(Independent Component Analysis, 独立成分分析)\nSVD(Singular Value Decomposition 奇异值分解)\nFA(Factor Analysis 因子分析法)\nText Mining(文本挖掘)：\nVSM(Vector Space Model, 向量空间模型)\nWord2Vec(词向量学习模型)\nTF(Term Frequency, 词频)\nTF-IDF(TermFrequency-Inverse Document Frequency, 词频-逆向文档频率)\nMI(Mutual Information, 互信息)\nECE(Expected Cross Entropy, 期望交叉熵)\nQEMI(二次信息熵)\nIG(Information Gain, 信息增益)\nIGR(Information Gain Ratio, 信息增益率)\nGini(基尼系数)\nx2 Statistic(x2统计量)\nTEW(Text Evidence Weight, 文本证据权)\nOR(Odds Ratio, 优势率)\nN-Gram Model\nLSA(Latent Semantic Analysis, 潜在语义分析)\nPLSA(Probabilistic Latent Semantic Analysis, 基于概率的潜在语义分析)\nLDA(Latent Dirichlet Allocation, 潜在狄利克雷模型)\nSLM(Statistical Language Model, 统计语言模型)\nNPLM(Neural Probabilistic Language Model, 神经概率语言模型)\nCBOW(Continuous Bag of Words Model, 连续词袋模型)\nSkip-gram(Skip-gram Model)\nAssociation Mining(关联挖掘)：\nApriori算法\nFP-growth(Frequency Pattern Tree Growth, 频繁模式树生长算法)\nMSApriori(Multi Support-based Apriori, 基于多支持度的Apriori算法)\nGSpan(Graph-based Substructure Pattern Mining, 频繁子图挖掘)\nSequential Patterns Analysis(序列模式分析)\nAprioriAll\nSpade\nGSP(Generalized Sequential Patterns, 广义序列模式)\nPrefixSpan\nForecast(预测)\nLR(Linear Regression, 线性回归)\nSVR(Support Vector Regression, 支持向量机回归)\nARIMA(Autoregressive Integrated Moving Average Model, 自回归积分滑动平均模型)\nGM(Gray Model, 灰色模型)\nBPNN(BP Neural Network, 反向传播神经网络)\nSRN(Simple Recurrent Network, 简单循环神经网络)\nLSTM(Long Short Term Memory, 长短记忆神经网络)\nCW-RNN(Clockwork Recurrent Neural Network, 时钟驱动循环神经网络)\n……\nLinked Analysis(链接分析)\nHITS(Hyperlink-Induced Topic Search, 基于超链接的主题检索算法)\nPageRank(网页排名)\nRecommendation Engine(推荐引擎)：\nSVD\nSlope One\nDBR(Demographic-based Recommendation, 基于人口统计学的推荐)\nCBR(Context-based Recommendation, 基于内容的推荐)\nCF(Collaborative Filtering, 协同过滤)\nUCF(User-based Collaborative Filtering Recommendation, 基于用户的协同过滤推荐)\nICF(Item-based Collaborative Filtering Recommendation, 基于项目的协同过滤推荐)\nSimilarity Measure&Distance Measure(相似性与距离度量)：\nEuclideanDistance(欧式距离)\nChebyshev Distance(切比雪夫距离)\nMinkowski Distance(闵可夫斯基距离)\nStandardized EuclideanDistance(标准化欧氏距离)\nMahalanobis Distance(马氏距离)\nCos(Cosine, 余弦)\nHamming Distance/Edit Distance(汉明距离/编辑距离)\nJaccard Distance(杰卡德距离)\nCorrelation Coefficient Distance(相关系数距离)\nInformation Entropy(信息熵)\nKL(Kullback-Leibler Divergence, KL散度/Relative Entropy, 相对熵)\nOptimization(最优化)：\nNon-constrained Optimization(无约束优化)：\nCyclic Variable Methods(变量轮换法)\nVariable Simplex Methods(可变单纯形法)\nNewton Methods(牛顿法)\nQuasi-Newton Methods(拟牛顿法)\nConjugate Gradient Methods(共轭梯度法)。\nConstrained Optimization(有约束优化)：\nApproximation Programming Methods(近似规划法)\nPenalty Function Methods(罚函数法)\nMultiplier Methods(乘子法)。\nHeuristic Algorithm(启发式算法)\nSA(Simulated Annealing, 模拟退火算法)\nGA(Genetic Algorithm, 遗传算法)\nACO(Ant Colony Optimization, 蚁群算法)\nFeature Selection(特征选择)：\nMutual Information(互信息)\nDocument Frequence(文档频率)\nInformation Gain(信息增益)\nChi-squared Test(卡方检验)\nGini(基尼系数)\nOutlier Detection(异常点检测)：\nStatistic-based(基于统计)\nDensity-based(基于密度)\nClustering-based(基于聚类)。\nLearning to Rank(基于学习的排序)：\nPointwise\nMcRank\nPairwise\nRankingSVM\nRankNet\nFrank\nRankBoost；\nListwise\nAdaRank\nSoftRank\nLamdaMART\nTool(工具)：\nMPI\nHadoop生态圈\nSpark\nIGraph\nBSP\nWeka\nMahout\nScikit-learn\nPyBrain\nTheano\n…\n以及一些具体的业务场景与case…"}
{"content2":"http://blog.sina.com.cn/s/blog_627a4f560100xmj1.html\n在机器学习(Machine learning)领域，监督学习(Supervised learning)、非监督学习(Unsupervised learning)以及半监督学习(Semi-supervised learning)是三类研究比较多，应用比较广的学习技术，wiki上对这三种学习的简单描述如下：\n监督学习：通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如分类。\n非监督学习：直接对输入数据集进行建模，例如聚类。\n半监督学习：综合利用有类标的数据和没有类标的数据，来生成合适的分类函数。\n以上表述是我直接翻译过来的，因为都是一句话，所以说得不是很清楚，下面我用一个例子来具体解释一下。\n其实很多机器学习都是在解决类别归属的问题，即给定一些数据，判断每条数据属于哪些类，或者和其他哪些数据属于同一类等等。这样，如果我们上来就对这一堆数据进行某种划分(聚类)，通过数据内在的一些属性和联系，将数据自动整理为某几类，这就属于非监督学习。如果我们一开始就知道了这些数据包含的类别，并且有一部分数据(训练数据)已经标上了类标，我们通过对这些已经标好类标的数据进行归纳总结，得出一个 “数据-->类别” 的映射函数，来对剩余的数据进行分类，这就属于监督学习。而半监督学习指的是在训练数据十分稀少的情况下，通过利用一些没有类标的数据，提高学习准确率的方法。\n铺垫了那么多，其实我想说的是，在wiki上对于半监督学习的解释是有一点点歧义的，这跟下面要介绍的主动学习有关。\n主动学习(active learning)，指的是这样一种学习方法：\n有的时候，有类标的数据比较稀少而没有类标的数据是相当丰富的，但是对数据进行人工标注又非常昂贵，这时候，学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。\n这个筛选过程也就是主动学习主要研究的地方了，怎么样筛选数据才能使得请求标注的次数尽量少而最终的结果又尽量好。\n主动学习的过程大致是这样的，有一个已经标好类标的数据集K(初始时可能为空)，和还没有标记的数据集U，通过K集合的信息，找出一个U的子集C，提出标注请求，待专家将数据集C标注完成后加入到K集合中，进行下一次迭代。\n按wiki上所描述的看，主动学习也属于半监督学习的范畴了，但实际上是不一样的，半监督学习和直推学习(transductive learning)以及主动学习，都属于利用未标记数据的学习技术，但基本思想还是有区别的。\n如上所述，主动学习的“主动”，指的是主动提出标注请求，也就是说，还是需要一个外在的能够对其请求进行标注的实体(通常就是相关领域人员)，即主动学习是交互进行的。\n而半监督学习，特指的是学习算法不需要人工的干预，基于自身对未标记数据加以利用。\n至于直推学习，它与半监督学习一样不需要人工干预，不同的是，直推学习假设未标记的数据就是最终要用来测试的数据，学习的目的就是在这些数据上取得最佳泛化能力。相对应的，半监督学习在学习时并不知道最终的测试用例是什么。\n也就是说，直推学习其实类似于半监督学习的一个子问题，或者说是一个特殊化的半监督学习，所以也有人将其归为半监督学习。\n而主动学习和半监督学习，其基本思想上就不一样了，所以还是要加以区分的，如果wiki上对半监督学习的解释能特别强调一下“是在不需要人工干预的条件下由算法自行完成对无标记数据的利用”，问题就会更清楚一些了。"}
{"content2":"来自http://www.iro.umontreal.ca/~pift6266/H10/notes/mlintro.html （估计有点老了，不过文字不多，看一眼也无妨）\nVery Brief Introduction to Machine Learning for AI\n对应的ppt地址：http://www.iro.umontreal.ca/~pift6266/H10/intro_diapos.pdf\n1、智能\n智能的概念可以被以很多种方式定义。这里根据一些标准（例如对于大多数动物来说的生存和繁衍）我们将它定义成能够做出正确决策的能力，为了得到更好的决策，我们需要知识，对于其中一种知识形式来说，就是操作，即可以处理感知数据，然后用这些信息来做决策。\n2、人工智能\n计算机已经能够具有一些智能了，这都归功于人类已经涉及到的所有项目和那些我们认为很有用的事情（基本上是那些能够让计算机做出正确决策的部分）。但是还是有许多任务动物和人类能够轻松解决而仍然超出计算机可做到的范围之外事情，在21世纪初期，许多任务都标榜自己是人工智能，包含了许多感知和控制的任务，可是为什么我们还是没法写出这些任务的项目程序呢？我认为主要是因为我们还不知道如何显式（正式的，标准的）的表达这些任务，即使我们的大脑（耦合着我们的身体）可以容易做到。这些任务涉及到的知识当前都是隐晦的，但是我们可以通过数据和例子来得到这些任务的许多信息（例如，观察一个人在给定特定的问题或者输入的时候会得到什么）。我们如何让机器也有这样的智能？使用数据和例子来建立可操作的知识就是我们所谓的机器的学习了。\n3、机器学习\n机器学习有着很长一段历史而且超多的文献都是有关机器学习的，在这些文献中，推荐看：\nChris Bishop, “Pattern Recognition and Machine Learning”, 2007\nSimon Haykin, “Neural Networks: a Comprehensive Foundation”, 2009 (3rd edition)\nRichard O. Duda, Peter E. Hart and David G. Stork, “Pattern Classification”, 2001 (2nd edition)\n这里只说很小的一些概念，一些与这个主题密切相关的部分。\n4、学习的规范化\n首先，给出最通用的学习的数学式框架。给定一些训练样本：\n这里zi 表示的是从未知过程P（Z）中得到的采样样本。同样还需要给定一个损失函数 L ，这个函数的会将决策函数 f  和样本 z ，作为它的参数，而且会返回一个实值标量。我们想要在这个未知的生成过程P（Z）背景下，最小化这个 L（f，z）的期望值。\n5、有监督学习\n在有监督学习中，每个样本都是一个（输入，目标）对：Z = （X,Y），f 将X 作为参数。最通用的讲解例子就是：\na）回归：Y是一个实值标量或者向量，f 的输出是和Y的值一样的集合，并通常使用平方误差来作为损失函数：\nb）分类：Y是一个有限整数（例如，一个符号）对应一个类别索引，我们通常将负条件似然log作为损失函数，用来估计：\n这里我们有约束条件：\n6、无监督学习\n在无监督学习中，我们需要学习一个函数 f 来帮助我们描述这个未知的分布P（Z）。有时候 f 直接就是一个有关P（Z）自身的估计（被称为密度估计）。然而，在许多其他情况下 f  是一个试图描述密度聚集的位置。聚类算法可以将输入空间划分成不同的区域（通常以一个原型样本或者中心点作为区域的中心）。许多聚类算法生成一个硬分区（例如，K-means算法）然而其他的生成的是软分区（例如：高斯混合模型GMM），也就是对每个Z指派一个概率值去标识它属于每个类的概率。另一种无监督算法就是为Z 构建一个新的表征，许多DL算法就属于这一种，同样的PCA也是。\n7、局部泛化\n学习算法的大量工作主要是利用一个单一原则来得到泛化：局部泛化。它假设如果输入样本 xi 很靠近输入样本 xj ，那么相对应的输出 f（xi）和 f（xj）应该也很近。这是实现局部插值的基本原则。这个原则很有用，但是他也有很多限制：要是我们需要推断呢？或者说，要是目标未知函数有比训练样本个数更多的变量呢？在这种情况下局部泛化就不能work了，因为我们最少需要和目标函数涉及的变量一样多的样本，从而能够覆盖所有的变量而且能够通过这个原则来生成。这个问题与被称为维数灾难的问题密切相关，而且有以下几个原因。当输入空间是高维的，输出的变化很可能按照输入维度的指数变化。例如，设想我们希望在每个输入变量（输入向量的每个元素）的10个不同的值之间做出区分，而且我们关心这些n个变量的所有的10^n个组合。只用局部泛化,我们需要至少观察这些10^n个 组合的至少各一个采样，这样才能将结论推广到所有变量。\n8、分布式对比：局部表征和非局部泛化\n当翻译到这一段的时候，发现居然有人翻译过了。\n下面这最后一段非原创：\nhttp://www.xuebuyuan.com/848002.html（来自这里，好吧其实是我懒了，这段不翻了。）\n一个简单的整数的N二进制本地表示是连续的B比特序列，并且，除了第个N比特都是0。一种简单的整数的二进制分布式表示是一系列使用通常的二进制编码的log_2(B)个比特。在这个例子中我们看出，分布式表示的效率较本地表示有指数的提高。一般地，对于学习算法，分布式表示具有在相同自由参数个数的情况下捕获更多（指数级）变化的潜力。因此分布式表示具有更好的generalization的潜力，因为学习理论指出需要的样本数量是O(B)的，自由度的有效维数是O(B)的。\n另一个对于分布式表示和本地表示的区别的说明 （相应的，本地和非本地的generalization）是关于聚簇clustering和PCA或者RBM的。前者是本地的，而后者是分布式的。使用k-means聚簇算法我们为每个prototype维护一个参数向量，也就是说，每个由学习者划分的区域一个。使用PCA算法，我们通过跟踪主要变化方向来标志其分布。现在设想一个简化的PCA的解释，在这里我们最关心的是在每个变化的方向上，数据在该方向上的投影是否超过或低于某一门限。在d个方向上，我们能够区分2^d个不同的区域。RBM算法与此类似，它定义d个超平面，并用一个比特来标志在平面一侧或另一侧。一个RBM将一个输入区域同一个标志位的组合联系起来（在神经网络的说法中，这些比特位被称为隐藏单元）。RBM的参数个数大约等于这些比特位相对输入维度的倍数（times，应该是倍数吧。。。）。再一次我们发现RBM或者PCA（分布式表示）可以表示的区域个数可以按照参数的指数规律增长，而传统的聚簇算法（例如，k-means或者高斯混合，都是本地表示）可以表示的区域个数进按照参数个数线性增长。另一种看待角度是，意识到RBM可以按照隐藏单元的组合归纳相应的新区域，即使这里还没有样本被观测到。这对于聚簇算法是不可能的（除了在那些周围区域已经有样本被观察到的区域）。"}
{"content2":"如何理解人工智能、机器学习和深度学习三者的关系\n今天我们就用最简单的方法——同心圆，可视化地展现出它们三者的关系和应用\n如上图，人工智能是最早出现的，也是最大、最外侧的同心圆;其次是机器学习，稍晚一点;最内侧，是深度学习，当今人工智能大爆炸的核心驱动。\n人工智能的一些较小的子集发展了起来。先是机器学习，然后是深度学习。深度学习又是机器学习的子集。而如今，深度学习造成了前所未有的巨大的影响。\n| 从概念的提出到走向繁荣\n1956年，几个计算机科学家相聚在达特茅斯会议(Dartmouth Conferences)，提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言;或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。\n过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流(大数据)的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发。\n让我们慢慢梳理一下计算机科学家们是如何将人工智能从最早的一点点苗头，发展到能够支撑那些每天被数亿用户使用的应用的。\n| 人工智能(Artificial Intelligence)——为机器赋予人的智能\n早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“ 强人工智能 ”(General AI)。这个无所不能的机器，它有着我们所有的感知(甚至比人更多)，我们所有的理性，可以像我们一样思考。\n人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO;邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。\n我们目前能实现的，一般被称为“ 弱人工智能 ”(Narrow AI)。 弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。 例如，Pinterest上的图像分类;或者Facebook的人脸识别。\n这些是弱人工智能在实践中的例子。这些技术实现的是人类智能的一些具体的局部。但它们是如何实现的?这种智能是从何而来?这就带我们来到同心圆的里面一层，机器学习。\n| 机器学习—— 一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、强化学习和贝叶斯网络等等。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。\n机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束;写形状检测程序来判断检测对象是不是有八条边;写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。\n这个结果还算不错，但并不是那种能让人为之一振的成功。特别是遇到云雾天，标志牌变得不是那么清晰可见，又或者被树遮挡一部分，算法就难以成功了。这就是为什么前一段时间，计算机视觉的性能一直无法接近到人的能力。它太僵化，太容易受环境条件的干扰。\n随着时间的推进，学习算法的发展改变了一切。\n| 深度学习——一种实现机器学习的技术\n人工神经网络(Artificial Neural Networks)是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同， 人工神经网络具有离散的层、连接和数据传播的方向 。\n例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。\n每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。\n我们仍以停止(Stop)标志牌为例。将一个停止标志牌图像的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、救火车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有权重，给出一个经过深思熟虑的猜测——“概率向量”。\n这个例子里，系统可能会给出这样的结果：86%可能是一个停止标志牌;7%的可能是一个限速标志牌;5%的可能是一个风筝挂在树上等等。然后网络结构告知神经网络，它的结论是否正确。\n即使是这个例子，也算是比较超前了。直到前不久，神经网络也还是为人工智能圈所淡忘。其实在人工智能出现的早期，神经网络就已经存在了，但神经网络对于“智能”的贡献微乎其微。主要问题是，即使是最基本的神经网络，也需要大量的运算。神经网络算法的运算需求难以得到满足。\n不过，还是有一些虔诚的研究团队，以多伦多大学的Geoffrey Hinton为代表，坚持研究，实现了以超算为目标的并行算法的运行与概念证明。但也直到GPU得到广泛应用，这些努力才见到成效。\n我们回过头来看这个停止标志识别的例子。 神经网络 是调制、训练出来的，时不时还是很容易出错的。它 最需要的，就是训练 。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结果。\n只有这个时候，我们才可以说神经网络成功地自学习到一个停止标志的样子;或者在Facebook的应用里，神经网络自学习了你妈妈的脸;又或者是2012年吴恩达(Andrew Ng)教授在Google实现了神经网络学习到猫的样子等等。\n吴教授的突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中的图像。吴教授为深度学习(deep learning)加入了“深度”(deep)。这里的 “深度”就是说神经网络中众多的层 。\n现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下围棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。\n| 深度学习，给人工智能以璀璨的未来\n深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。 深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n人工智能就在现在，就在明天。有了深度学习，人工智能甚至可以达到我们畅想的科幻小说一般。你的C-3PO我拿走了，你有你的终结者就好了。"}
{"content2":"课程设置和内容\n视频课程分为20集，每集72-85分钟。实体课程大概一周2次，中间还穿插助教上的习题课，大概一个学期的课程。\n内容涉及四大部分，分别是：监督学习（2-8集）、学习理论（9集-11集）、无监督学习（12-15集）、强化学习（16-20集）。监督学习和无监督学习，基本上是机器学习的二分法；强化学习位于两者之间；而学习理论则从总体上介绍了如何选择、使用机器学习来解决实际问题，以及调试（比如：误差分析、销蚀分析）、调优（比如：模型选择、特征选择）的各种方法和要注意的事项（比如，避免过早优化）。\n监督学习，介绍了回归、朴素贝叶斯、神经网络、SVM（支持向量机）、SMO（顺序最小优化）算法等；无监督学习讲了聚类、K-means、GMM（混合高斯模型）、EM算法 、PCA（主成分分析）、LSI（潜在语义索引）、SVD（奇异值分解）、ICA（独立成分分析）等；强化学习主要讲了这类连续决策学习（马尔科夫决策过程，MDP）中的值迭代（VI）和策略迭代（PI），以及如何定义回报函数，如何找到最佳策略等问题。\n授课方式\n网上有老师的讲义，可以在网易这门课的主页面上打包下载（网址http://v.163.com/special/opencourse/machinelearning.html）。老师基本上是写板书的，PPT是辅助。在黑板上用粉笔边讲解边书写，有助于带动学生的思考，使师生之间有交流有互动。个人以为，比直接显示PPT效果好。数学公式的推导很费时间，课堂上也不可能大多数的时间用来推导公式，所以大量的推导老师要求学生在课下看讲义，或者通过习题课听助教讲解。\n授课语言\n因为是美国的课堂，当然的教学语言是英语。网易做的不错，除了把老师说的话全部转写下来，还做了中文翻译，前14集翻译得不错，除了偶有错别字之外，专业术语翻译的很好，语句也很流畅。第15集以后一直到最后一堂课，翻译的不是太准确，一些专业术语都翻译错了，很让观者感到不适。但是，无论如何，还是感谢网易这些转写和翻译的无名网友无私的付出。这些小的瑕疵不会让真正热爱这门课程的学习者放弃学习，反而想加入翻译者的队伍，为传播科学知识而贡献力量呢。\n观后感\n总体感觉，老师讲的不错，是个真正懂机器学习的人。老师在课上也说过，很容易区分那些真正懂机器学习的人，和那些只会纸上谈兵的人。我希望成为第一类，并为此努力着。\n老师是华裔，中文名字叫吴恩达，生于伦敦，看上去很亲切。课堂很活跃，老师注重和学生交流，每讲完一个主题，会问学生有问题吗，然后一一作答。\n视频大概录制于2007年（个人推测，未经考证），内容上，与现在的机器学习技术比，稍微显得不够多。近年来，机器学习领域有了长足的发展，学术界和工业界齐发力，二者相互促进，达到了前所未有的高度。即便是曾经沉寂的神经网络，近年来也改头换面成了深度学习。不过，从专家的角度看，这不是一种新的机器学习技术，它只是涉及到其中的一个环节——特征选择，并不构成一个独立的学习方法。\n老师没有涉及实战。受限于课堂讲授的方式和时间上的限制，课上只能做必要知识点的讲解。\n数学公式比较多，似懂非懂的。如果不满足于“知其然”，还要“知其所以然”，以后的方向是搞模型、算法研究的话，那还要补习一下数学知识，必须的。如果仅是为了解决实际问题，对算法要求不高的话，那知道如何运用就够了。剩下的，随着应用系统的不断进展，对整个系统各方面要求的提高，那时会倒逼你进阶的。\n遗憾的是，因为没有完全掌握，所以再回看已经看过的视频，还是似懂非懂，但是比第一次要好很多。建议大家多看几遍，加强练习，跟自己的项目相结合，动手实现会加深理解。“精通的目的全在于应用”（毛语），机器学习只是工具，应用到解决实际问题上才能真正体现它的价值。\n跟这个课程最接近的，是加州理工学院的《机器学习与数据挖掘》（18集）（网址http://v.163.com/special/opencourse/learningfromdata.html），主讲老师有口音，很重，如果没有中文字幕的帮助，很难快速掌握。目前网易的进展是，翻译完了前4集。\n顺便说一句，以后想练专业口语的话，可以多看Andrew Ng这个，跟着说，以后在国际会议上就能充分表达了。听加州理工的这个，也能听懂那些非英语母语国家讲的英语了。不同的地方有不同的英语口音，我们还不算难听的，应该算是好听的，呵呵。\n又及，自己心里暗想，土鳖也能“准”“海归”一回。网络带来了革命，网络也给我们这些爱学习的人带来了真正“免费的午餐”。其实，话说回来，就像免费的搜索引擎一样，他们收获的是更大的名声上的胜利，扩大了影响，传播了美誉。像耶鲁大学的一个教授的一句玩笑话，其目的是争取“世界学术霸权”。\nAndrew Ng教授的《机器学习》公开课视频（30集）\nhttp://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning\nAndrew Ng教授的Deep Learning维基，有中文翻译\nhttp://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial\n其他教学资源\n韩家炜教授在北大的《数据挖掘》暑期班视频，英文PPT，中文讲解（22集）\nhttp://v.youku.com/v_show/id_XMzA3NDI5MzI=.html（视频：01数据挖掘概念，课程简介，数据库技术发展史，数据挖掘应用）\n韩家炜教授（UIUC大学）的《数据挖掘》在线课程\nhttps://wiki.engr.illinois.edu/display/cs412/Home;jsessionid=6BF0A2C36A95A31D2DA754A017756F4B\n卡内基•梅隆大学（CMU）的《机器学习》在线课程\nhttp://www.cs.cmu.edu/~epxing/Class/10701/lecture.html\n麻省理工学院（MIT）的《机器学习》在线课程\nhttp://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/index.htm\n加州理工学院（Caltech）的《机器学习与数据挖掘》在线课程\nhttp://work.caltech.edu/telecourse.html（同上述网易公开课http://v.163.com/special/opencourse/learningfromdata.html）\nUC Irvine的《机器学习与数据挖掘》在线课程\nhttp://sli.ics.uci.edu/Classes/2011W-178\n斯坦福大学的《数据挖掘》在线课程\nhttp://www.stanford.edu/class/stats202/\n其他资源\n北京机器学习读书会\nhttp://q.weibo.com/1644133\n机器学习相关电子书\nhttp://t.cn/zjtPuCS（打开artificial intelligence找子目录machine learning）\n附：\n主讲教师介绍：（新浪公开课：机器学习http://open.sina.com.cn/course/id_280/）\n讲师：Andrew Ng\n学校：斯坦福\n斯坦福大学计算机系副教授，人工智能实验室主任，致力于人工智能、机器学习，神经信息科学以及机器人学等研究方向。他和他的学生成功开发出新的机器视觉算法，大大简化了机器人的传感器系统。\n【原文链接：http://liliphd.iteye.com/blog/1929358】"}
{"content2":"Azure Machine Learning（简称“AML”）是微软在其公有云Azure上推出的基于Web使用的一项机器学习服务，机器学习属人工智能的一个分支，它技术借助算法让电脑对大量流动数据集进行识别。这种方式能够通过历史数据来预测未来事件和行为，其实现方式明显优于传统的商业智能形式。微软的目标是简化使用机器学习的过程，以便于开发人员、业务分析师和数据科学家进行广泛、便捷地应用。这款服务的目的在于“将机器学习动力与云计算的简单性相结合”。AML目前在微软的Global Azure云服务平台提供服务，用户可以通过站点：https://studio.azureml.net/ 申请免费试用。\n登录到试用账号之后可以看到如下的界面：\n对于初次使用者，可以通过选择左侧菜单的“Experiments”，然后选择左下角的New，弹出菜单后选择新增一个“Experiments Tutorial”，就能够启动一个内置的示例。这是一个根据已有数据包括年龄、教育层度、婚姻状态、职业、现收入等分析预测任何一类人群收入是否能超过50k的模型。通过点击下一步，用户就能轻而易举的了解如何导入数据、如何做数据的预处理、如何将数据分离用于训练模型和验证模型、如何选择算法训练模型以及如何评估模型的效果。整个过程无需编程，完全是通过拖拽和配置完成，非常的简单。能让用户快速上手AML用法，从而把更多的精力放在理解数据和算法上，工具本身并不会给你带来任何额外的学习成本。\n同时，用户还可以通过 https://azure.microsoft.com/en-us/documentation/articles/machine-learning-import-data/ （英文）了解所有关于微软Azure机器学习相关的知识。我这里针对几个大家普遍关心的问题做一些解释，帮助大家快速了解AML并快速上手。\n数据如何导入及数据的类型。\n对于所要用于训练及验证模型的数据，需要导入到AML的Studio中。目前支持的数据导入方式如下：\n•本地文件上传\n•Azure BLOB storage, table\n•Azure SQL database\n•Hadoop using HiveQL\n•A web URL using HTTP\n•A data feed provider（OData）\n支持的数据类型如下：\n• txt文本文件；\n• CSV 文件，包括.csv和.nh.csv；\n• TSV文件，包括.tsv和.nh.tsv；\n• Hadoop Hive table\n• SQL database table\n• OData values\n• SVMLight data (.svmlight) (具体描述见链接：http://svmlight.joachims.org/ )\n• Attribute Relation File Format (ARFF) data (.arff) (具体描述见链接：http://weka.wikispaces.com/ARFF )\n• Zip file (.zip)\n• R object or workspace file (.RData)\n2. 内置的算法\n概括的说，微软Azure机器学习内置了基于监督学习和非监督学习的分类、回归、聚类等的20多种算法，详细的算法描述详见链接：https://msdn.microsoft.com/en-us/library/azure/dn905812.aspx 。我也会在后面的博文中陆续向大家介绍。除了算法之外， AML还集成了400+多个R语言的程序包。\n对于选择什么样的算法，无论是对于初学者还是有经验的数据科学家，其实都是很让人费脑筋的事情。微软也提供了很多资料帮助大家判断应该选择哪些算法。如下是几个非常有用的链接（英文）：\n•Microsoft Azure Machine Learning Algorithm Cheat Sheet - https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-cheat-sheet/\n•Choosing a Learning Algorithm in Azure Machine Learning - http://blogs.technet.com/b/machinelearning/archive/2015/05/20/choosing-a-learning-algorithm-in-azure-ml.aspx\n•Choosing a Machine Learning Classifier - http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/\n•Choosing the right estimator - http://scikit-learn.org/stable/tutorial/machine_learning_map/\n3. 内置的应用模块\n为了方便微软Azure机器学习让更多的人很容易的上手和使用，AML原生内置了很多业务场景的原始数据和机器学习模块及API。用户可直接使用它们，或者做少量的修改为自己所用。主要的业务场景包括但不限于如下（还在持续增加中）。初学者可以先从这些已有的模块理解和掌握机器学习的使用。可以从登录首页上方菜单的“Gallery”中找到这些已经构建好的模型。\n• 文本分析；\n• 客户流失预测；\n• 推荐系统；\n• 预测性维护；\n• 欺诈监测；\n4. 如何计费\nAML作为一个云服务，通过Web访问的方式提供。目前提供免费和标准两种服务提供方式。标准级别按照使用时长计费，具体可参考：http://azure.microsoft.com/en-us/pricing/details/machine-learning/\n本文仅对微软的Azure机器学习服务做概要引导式的介绍，帮助大家对该服务有一个初步的了解。另外还有很多内容包括构建好机器学习模型之后如何发布等会在后面的博客中再向大家详细介绍。"}
{"content2":"简言\n机器学习的项目,不可避免的需要补充一些优化算法,对于优化算法,爬山算法还是比较重要的.鉴于此,花了些时间仔细阅读了些爬山算法的paper.基于这些,做一些总结.\n目录\n1. 爬山算法简单描述\n2. 爬山算法的主要算法\n2.1 首选爬山算法\n2.2 最陡爬山算法\n2.3 随机重新开始爬山算法\n2.4 模拟退火算法(也是爬山算法)\n3. 实例求解\n正文\n爬山算法,是一种局部贪心的最优算法. 该算法的主要思想是:每次拿相邻点与当前点进行比对,取两者中较优者,作为爬坡的下一步.\n举一个例子,求解下面表达式\n的最大值. 且假设 x,y均按为0.1间隔递增.\n为了更好的描述,我们先使用pyhton画出该函数的图像:\n图像的python代码:\n1 # encoding:utf8 2 from matplotlib import pyplot as plt 3 import numpy as np 4 from mpl_toolkits.mplot3d import Axes3D 5 6 7 def func(X, Y, x_move=0, y_move=0): 8 def mul(X, Y, alis=1): 9 return alis * np.exp(-(X * X + Y * Y)) 10 11 return mul(X, Y) + mul(X - x_move, Y - y_move, 2) 12 13 14 def show(X, Y): 15 fig = plt.figure() 16 ax = Axes3D(fig) 17 X, Y = np.meshgrid(X, Y) 18 Z = func(X, Y, 1.7, 1.7) 19 plt.title(\"demo_hill_climbing\") 20 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow', ) 21 ax.set_xlabel('x label', color='r') 22 ax.set_ylabel('y label', color='g') 23 ax.set_zlabel('z label', color='b') 24 # 具体函数方法可用 help(function) 查看，如：help(ax.plot_surface) 25 # ax.scatter(X,Y,Z,c='r') #绘点 26 plt.show() 27 28 if __name__ == '__main__': 29 X = np.arange(-2, 4, 0.1) 30 Y = np.arange(-2, 4, 0.1) 31 32 show(X,Y)\nView Code\n对于上面这个问题,我们使用爬山算法该如何求解呢? 下面我们从爬山算法中的几种方式分别求解一下这个小题.\n1. 首选爬山算法\n依次寻找该点X的邻近点中首次出现的比点X价值高的点,并将该点作为爬山的点(此处说的价值高,在该题中是指Z或f(x,y)值较大). 依次循环,直至该点的邻近点中不再有比其大的点. 我们成为该点就是山的顶点,又称为最优点.\n那么解题思路就有:\n1.  随机选择一个登山的起点S(x0,y0,z0),并以此为起点开始登山.直至\"登顶\".\n下面是我们实现的代码:\n1 # encoding:utf8 2 from random import random, randint 3 4 from matplotlib import pyplot as plt 5 import numpy as np 6 from mpl_toolkits.mplot3d import Axes3D 7 8 9 def func(X, Y, x_move=1.7, y_move=1.7): 10 def mul(X, Y, alis=1): 11 return alis * np.exp(-(X * X + Y * Y)) 12 13 return mul(X, Y) + mul(X - x_move, Y - y_move, 2) 14 15 16 def show(X, Y, Z): 17 fig = plt.figure() 18 ax = Axes3D(fig) 19 plt.title(\"demo_hill_climbing\") 20 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow', ) 21 ax.set_xlabel('x label', color='r') 22 ax.set_ylabel('y label', color='g') 23 ax.set_zlabel('z label', color='b') 24 # ax.scatter(X,Y,Z,c='r') #绘点 25 plt.show() 26 27 28 def drawPaht(X, Y, Z,px,py,pz): 29 fig = plt.figure() 30 ax = Axes3D(fig) 31 plt.title(\"demo_hill_climbing\") 32 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow', ) 33 ax.set_xlabel('x label', color='r') 34 ax.set_ylabel('y label', color='g') 35 ax.set_zlabel('z label', color='b') 36 ax.plot(px,py,pz,'r.') #绘点 37 plt.show() 38 39 40 def hill_climb(X, Y): 41 global_X = [] 42 global_Y = [] 43 44 len_x = len(X) 45 len_y = len(Y) 46 # 随机登山点 47 st_x = randint(0, len_x-1) 48 st_y = randint(0, len_y-1) 49 50 def argmax(stx, sty, alisx=0, alisy=0): 51 cur = func(X[0][st_x], Y[st_y][0]) 52 next = func(X[0][st_x + alisx], Y[st_y + alisy][0]) 53 54 return cur < next and True or False 55 56 while (len_x > st_x >= 0) or (len_y > st_y >= 0): 57 if st_x + 1 < len_x and argmax(st_x, st_y, 1): 58 st_x += 1 59 elif st_y + 1 < len_x and argmax(st_x, st_y, 0, 1): 60 st_y += 1 61 elif st_x >= 1 and argmax(st_x, st_y, -1): 62 st_x -= 1 63 elif st_y >= 1 and argmax(st_x, st_y, 0, -1): 64 st_y -= 1 65 else: 66 break 67 global_X.append(X[0][st_x]) 68 global_Y.append(Y[st_y][0]) 69 return global_X, global_Y, func(X[0][st_x], Y[st_y][0]) 70 71 72 if __name__ == '__main__': 73 X = np.arange(-2, 4, 0.1) 74 Y = np.arange(-2, 4, 0.1) 75 X, Y = np.meshgrid(X, Y) 76 Z = func(X, Y, 1.7, 1.7) 77 px, py, maxhill = hill_climb(X, Y) 78 print px,py,maxhill 79 drawPaht(X, Y, Z,px,py,func(np.array(px), np.array(py), 1.7, 1.7))\nView Code\n对比几次运行的结果:\n从上图中,我们可以比较清楚的观察到,首选爬山算法的缺陷.\n2.那么最陡爬山算法呢?\n简单描述:\n最陡爬山算法是在首选爬山算法上的一种改良,它规定每次选取邻近点价值最大的那个点作为爬上的点.\n下面我们来实现一下它:\n1 # encoding:utf8 2 from random import random, randint 3 4 from matplotlib import pyplot as plt 5 import numpy as np 6 from mpl_toolkits.mplot3d import Axes3D 7 8 9 def func(X, Y, x_move=1.7, y_move=1.7): 10 def mul(X, Y, alis=1): 11 return alis * np.exp(-(X * X + Y * Y)) 12 13 return mul(X, Y) + mul(X - x_move, Y - y_move, 2) 14 15 16 def show(X, Y, Z): 17 fig = plt.figure() 18 ax = Axes3D(fig) 19 plt.title(\"demo_hill_climbing\") 20 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow', ) 21 ax.set_xlabel('x label', color='r') 22 ax.set_ylabel('y label', color='g') 23 ax.set_zlabel('z label', color='b') 24 # ax.scatter(X,Y,Z,c='r') #绘点 25 plt.show() 26 27 28 def drawPaht(X, Y, Z, px, py, pz): 29 fig = plt.figure() 30 ax = Axes3D(fig) 31 plt.title(\"demo_hill_climbing\") 32 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow', ) 33 ax.set_xlabel('x label', color='r') 34 ax.set_ylabel('y label', color='g') 35 ax.set_zlabel('z label', color='b') 36 ax.plot(px, py, pz, 'r.') # 绘点 37 plt.show() 38 39 40 def hill_climb(X, Y): 41 global_X = [] 42 global_Y = [] 43 44 len_x = len(X) 45 len_y = len(Y) 46 # 随机登山点 47 st_x = randint(0, len_x - 1) 48 st_y = randint(0, len_y - 1) 49 50 def argmax(stx, sty, alisx, alisy): 51 cur = func(X[0][stx], Y[sty][0]) 52 next = func(X[0][alisx], Y[alisy][0]) 53 if cur < next: 54 return alisx, alisy 55 return stx, sty 56 #return cur < next and alisx, alisy or stx, sty 57 58 tmp_x = st_x 59 tmp_y = st_y 60 while (len_x > st_x >= 0) or (len_y > st_y >= 0): 61 if st_x + 1 < len_x: 62 tmp_x, tmp_y = argmax(tmp_x, tmp_y, (st_x + 1), st_y) 63 64 if st_x >= 1: 65 tmp_x, tmp_y = argmax(tmp_x, tmp_y, st_x - 1, st_y) 66 67 if st_y + 1 < len_x: 68 tmp_x, tmp_y = argmax(tmp_x, tmp_y, st_x, st_y + 1) 69 70 if st_y >= 1: 71 tmp_x, tmp_y = argmax(tmp_x, tmp_y, st_x, st_y - 1) 72 73 if tmp_x != st_x or tmp_y != st_y: 74 st_x = tmp_x 75 st_y = tmp_y 76 else: 77 break 78 global_X.append(X[0][st_x]) 79 global_Y.append(Y[st_y][0]) 80 return global_X, global_Y, func(X[0][st_x], Y[st_y][0]) 81 82 83 if __name__ == '__main__': 84 X = np.arange(-2, 4, 0.1) 85 Y = np.arange(-2, 4, 0.1) 86 X, Y = np.meshgrid(X, Y) 87 Z = func(X, Y, 1.7, 1.7) 88 px, py, maxhill = hill_climb(X, Y) 89 print px, py, maxhill 90 drawPaht(X, Y, Z, px, py, func(np.array(px), np.array(py), 1.7, 1.7))\nView Code\n从这个结果来看,因为范围扩大了一点,所以效果会好一点点,当依旧是一个局部最优算法.\n3.随机重新开始爬山算法呢?\n简单的描述:\n随机重新开始爬山算法是基于最陡爬山算法,其实就是加一个达到全局最优解的条件,如果满足该条件,就结束运算,反之则无限次重复运算最陡爬山算法.\n由于此题,并没有结束的特征条件,我们这里就不给予实现.\n4.模拟退火算法\n简单描述:\n(1)随机挑选一个单元k，并给它一个随机的位移，求出系统因此而产生的能量变化ΔEk。\n(2)若ΔEk⩽0，该位移可采纳，而变化后的系统状态可作为下次变化的起点；\n若ΔEk>0，位移后的状态可采纳的概率为\n式中T为温度，然后从(0,1)区间均匀分布的随机数中挑选一个数R，若R<Pk，则将变化后的状态作为下次的起点；否则，将变化前的状态作为下次的起点。\n(3)转第(1)步继续执行，知道达到平衡状态为止。\n代码实现为:\n1 # encoding:utf8 2 from random import random, randint 3 4 from matplotlib import pyplot as plt 5 import numpy as np 6 from mpl_toolkits.mplot3d import Axes3D 7 8 9 def func(X, Y, x_move=1.7, y_move=1.7): 10 def mul(X, Y, alis=1): 11 return alis * np.exp(-(X * X + Y * Y)) 12 13 return mul(X, Y) + mul(X - x_move, Y - y_move, 2) 14 15 16 def show(X, Y, Z): 17 fig = plt.figure() 18 ax = Axes3D(fig) 19 plt.title(\"demo_hill_climbing\") 20 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow', ) 21 ax.set_xlabel('x label', color='r') 22 ax.set_ylabel('y label', color='g') 23 ax.set_zlabel('z label', color='b') 24 # ax.scatter(X,Y,Z,c='r') #绘点 25 plt.show() 26 27 28 def drawPaht(X, Y, Z, px, py, pz): 29 fig = plt.figure() 30 ax = Axes3D(fig) 31 plt.title(\"demo_hill_climbing\") 32 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, color='b' ) 33 ax.set_xlabel('x label', color='r') 34 ax.set_ylabel('y label', color='g') 35 ax.set_zlabel('z label', color='b') 36 ax.plot(px, py, pz, 'r.') # 绘点 37 plt.show() 38 39 40 def hill_climb(X, Y): 41 global_X = [] 42 global_Y = [] 43 # 初始温度 44 temperature = 105.5 45 # 温度下降的比率 46 delta = 0.98 47 # 温度精确度 48 tmin = 1e-10 49 50 len_x = len(X) 51 len_y = len(Y) 52 53 # 随机登山点 54 st_x = X[0][randint(0, len_x - 1)] 55 st_y = Y[randint(0, len_y - 1)][0] 56 st_z = func(st_x, st_y) 57 58 def argmax(stx, sty, alisx, alisy): 59 cur = func(st_x, st_y) 60 next = func(alisx, alisy) 61 62 return cur < next and True or False 63 64 while (temperature > tmin): 65 # 随机产生一个新的邻近点 66 # 说明: 温度越高幅度邻近点跳跃的幅度越大 67 tmp_x = st_x + (random() * 2 - 1) * temperature 68 tmp_y = st_y + + (random() * 2 - 1) * temperature 69 if 4 > tmp_x >= -2 and 4 > tmp_y >= -2: 70 if argmax(st_x, st_y, tmp_x, tmp_y): 71 st_x = tmp_x 72 st_y = tmp_y 73 else: # 有机会跳出局域最优解 74 pp = 1.0 / (1.0 + np.exp(-(func(tmp_x, tmp_y) - func(st_x, st_y)) / temperature)) 75 if random() < pp: 76 st_x = tmp_x 77 st_y = tmp_y 78 temperature *= delta # 以一定的速率下降 79 global_X.append(st_x) 80 global_Y.append(st_y) 81 return global_X, global_Y, func(st_x, st_y) 82 83 84 if __name__ == '__main__': 85 X = np.arange(-2, 4, 0.1) 86 Y = np.arange(-2, 4, 0.1) 87 X, Y = np.meshgrid(X, Y) 88 Z = func(X, Y, 1.7, 1.7) 89 px, py, maxhill = hill_climb(X, Y) 90 print px, py, maxhill 91 drawPaht(X, Y, Z, px, py, func(np.array(px), np.array(py), 1.7, 1.7))\nView Code\n效果:"}
{"content2":"简单来说机器学习的核心步骤在于“获取学习数据；选择机器算法；定型模型；评估模型，预测模型结果”，下面本人就以判断日报内容是否合格为例为大家简单的阐述一下C#的机器学习。\n第一步：问题分析\n根据需求可以得出我们的模型是以日报的内容做为学习的特征确定的，然后通过模型判断将该目标对象预测为是否符合标准（合格与不合格），简单来说就是一种分类场景（此场景结果属于二元分类，不是A就是B），那么也就确定了核心算法为分类算法当然还有其它的分类算法有兴趣的可以自己去了解一下在这里就不多做说明了。\n第二步：环境准备\n其他的代码编译运行的环境并没有太多要求，你只需要引用C#机器学习的NuGet 包，名为Microsoft.ML 具体的安装步骤在此就不做详细介绍了。\n第三步：准备数据\n这里会准备两个数据集 一个定型模型的数据集（可以称之为学习资料）wikipedia-detox-250-line-data.tsv数据实例部分展示如下（你的数据按照这种排列格式即可该该格式的定义取决于你的输入数据集类的结构在下面会讲到）：\nSentiment SentimentText 1 第一天上班 无事 1 完成了领导的安排任务 1 编写了一些代码然后写了一些杂七杂八的文档 1 和一般的码农做了一样的事情 1 和产品经理一起做了一些项目上的事情 1 早上来的时候就开始讨论需求，然后开始写代码，快下班的时候完成了整个过程的文档分享 0 ***项目的整体编排会议，设计图的首页以及我的个人中心制作 0 **项目需求的对接，需求的梳理，实体结构的定义，数据库的迁移，脑图的完善 0 1、**项目的模板消息代码编写，2、**项目管理后台的模板发送完善，\n定型模型数据集准备好之后还有一个评估模型的测试数据集（可以称之为标准答案）wikipedia-detox-250-line-test.tsv格式与上面展示的评估数据集一样\n定型数据的数据越丰富算法的回归曲线方程就会越接近理想的模型方程，你的模型预测结果就会越符合你的要求。\n第四步：定义特征类\n根据分享的模型确定其分析的特征项并定义为相关的类并且需要引用机器学习的包using Microsoft.ML.Data;，由此模型定义的数据集类如下（结果可看注释）：\n/// <summary> /// 输入数据集类 /// </summary> public class SentimentData { /// <summary> /// 日志是否合格的值（0：为合格，1：不合格） /// </summary> [Column(ordinal: \"0\", name: \"Label\")] public float Sentiment; /// <summary> /// 日报内容 /// </summary> [Column(ordinal: \"1\")] public string SentimentText; } /// <summary> /// 预测结果集类 /// </summary> public class SentimentPrediction { /// <summary> /// 预测值（是否合格） /// </summary> [ColumnName(\"PredictedLabel\")] public bool Prediction { get; set; } /// <summary> /// 或然率（结果分布概率） /// </summary> [ColumnName(\"Probability\")] public float Probability { get; set; } }\n第一个SentimentData类为输入数据集类，指的就是根据定型的数据集的特征项定义的集类，第二个SentimentPrediction类为预测结果集类，也就是你所需要的结果的类定义 该类的结构一般受你所使用的学习算法影响，根据你的学习管道输出的结果以及个人需求的综合考虑来定义。输入集类带的Column属性标注其在数据集的格式位置的编排以及何为Label值。预测集的PredictedLabel在预测和评估过程中使用。\n第五步：代码实现\n首先定义以指定这些路径和 _textLoader 变量，用来读取数据或者是保存实验数据，具体如下所示：\n_trainDataPath 具有用于定型模型的数据集路径。\n_testDataPath 具有用于评估模型的数据集路径。\n_modelPath 具有在其中保存定型模型的路径。\n_textLoader 是用于加载和转换数据集的 TextLoader。\n然后定义程序的入口（main函数）以及相应的处理方法：\n定义SaveModelAsFile方法将模型保存为 .zip 文件代码如下所示：\nprivate static void SaveModelAsFile(MLContext mlContext, ITransformer model) { using (var fs = new FileStream(_modelPath, FileMode.Create, FileAccess.Write, FileShare.Write)) mlContext.Model.Save(model, fs); Console.WriteLine(\"模型保存路径为{0}\", _modelPath); Console.ReadLine(); }\n定义Train方法选择学习方法并且创建相应的学习管道，输出定型后的模型model代码如下所示：\npublic static ITransformer Train(MLContext mlContext, string dataPath) { IDataView dataView = _textLoader.Read(dataPath); //数据特征化（按照管道所需的格式转换数据） var pipeline = mlContext.Transforms.Text.FeaturizeText(inputColumnName: \"SentimentText\", outputColumnName: \"Features\") //根据学习算法添加学习管道 .Append(mlContext.BinaryClassification.Trainers.FastTree(numLeaves: 50, numTrees: 50, minDatapointsInLeaves: 20)); //得到模型 var model = pipeline.Fit(dataView); Console.WriteLine(); //返回定型模型 return model; }\n模型定型之后，我们需要创建一个方法（Evaluate）来评测该模型的质量，根据你自己的标准测试数据集与该模型的符合程度来判断，并且输出相应的指标，该指标参数根据你所调用的评估方法返回具体的根据你的算法方程返回相应的方程的参数 。代码如下所示：\npublic static void Evaluate(MLContext mlContext, ITransformer model) { var dataView = _textLoader.Read(_testDataPath); Console.WriteLine(\"===============用测试数据评估模型的准确性===============\"); var predictions = model.Transform(dataView); //评测定型模型的质量 var metrics = mlContext.BinaryClassification.Evaluate(predictions, \"Label\"); Console.WriteLine(); Console.WriteLine(\"模型质量量度评估\"); Console.WriteLine(\"--------------------------------\"); Console.WriteLine($\"精度: {metrics.Accuracy:P2}\"); Console.WriteLine($\"Auc: {metrics.Auc:P2}\"); Console.WriteLine(\"=============== 模型结束评价 ===============\"); Console.ReadLine(); //评测完成之后开始保存定型的模型 SaveModelAsFile(mlContext, model); }\n定义单个数据的预测方法（Predict）与批处理预测的方法（PredictWithModelLoadedFromFile）：\n单个数据集的预测代码如下所示：\nprivate static void Predict(MLContext mlContext, ITransformer model) { //创建包装器 var predictionFunction = model.CreatePredictionEngine<SentimentData, SentimentPrediction>(mlContext); SentimentData sampleStatement = new SentimentData { SentimentText = \"爱车新需求开发；麦扣日志监控部分页面数据绑定；\" }; //预测结果 var resultprediction = predictionFunction.Predict(sampleStatement); Console.WriteLine(); Console.WriteLine(\"===============单个测试数据预测 ===============\"); Console.WriteLine(); Console.WriteLine($\"日报内容: {sampleStatement.SentimentText} | 是否合格: {(Convert.ToBoolean(resultprediction.Prediction) ? \"合格\" : \"不合格\")} | 符合率: {resultprediction.Probability} \"); Console.WriteLine(\"=============== 预测结束 ===============\"); Console.WriteLine(); Console.ReadLine(); }\n批处理数据集预测方法代码如下所示：\npublic static void PredictWithModelLoadedFromFile(MLContext mlContext) { IEnumerable<SentimentData> sentiments = new[] { new SentimentData { SentimentText = \"1、完成爱车年卡代码编写 2、与客户完成需求对接\" }, new SentimentData { SentimentText = \"没有工作内容\" } }; ITransformer loadedModel; using (var stream = new FileStream(_modelPath, FileMode.Open, FileAccess.Read, FileShare.Read)) { loadedModel = mlContext.Model.Load(stream); } // 创建预测（也称之为创建预测房屋） var sentimentStreamingDataView = mlContext.Data.ReadFromEnumerable(sentiments); var predictions = loadedModel.Transform(sentimentStreamingDataView); // 使用模型预测结果值为1（不合格）还是0 （合格） var predictedResults = mlContext.CreateEnumerable<SentimentPrediction>(predictions, reuseRowObject: false); Console.WriteLine(); Console.WriteLine(\"=============== 多样本加载模型的预测试验 ===============\"); var sentimentsAndPredictions = sentiments.Zip(predictedResults, (sentiment, prediction) => (sentiment, prediction)); foreach (var item in sentimentsAndPredictions) { Console.WriteLine($\"日报内容: {item.sentiment.SentimentText} | 是否合格: {(Convert.ToBoolean(item.prediction.Prediction) ? \"合格\" : \"不合格\")} | 符合率: {item.prediction.Probability} \"); } Console.WriteLine(\"=============== 预测结束 ===============\"); Console.ReadLine(); }\n在以上的方法定义完成之后开始进行方法的调用：\npublic static void Main(string[] args) { //创建一个MLContext，为ML作业提供一个上下文 MLContext mlContext = new MLContext(seed: 0); //初始化_textLoader以将其重复应用于所需要的数据集 _textLoader = mlContext.Data.CreateTextLoader( columns: new TextLoader.Column[] { new TextLoader.Column(\"Label\", DataKind.Bool,0), new TextLoader.Column(\"SentimentText\", DataKind.Text,1) }, separatorChar: '\\t', hasHeader: true ); //定型模型 var model = Train(mlContext, _trainDataPath); //评测模型 Evaluate(mlContext, model); //单个数据预测 Predict(mlContext, model); //批处理预测数据 PredictWithModelLoadedFromFile(mlContext); }\n准备代码之后，你的小小的机器人就要开始学习啦，好吧开始编译运行吧。。。。。。\n运行产生结果为：\n由于训练的数据集特征化参数的准确性以及数据的涵盖广度不够导致定义的模型质量非常的不理想因此我们可以看到 我们的预测结果也是不够符合我们的理想状态，可见我们小机器的学习之路是非常漫长的过程啊。\n由此次的机器学习的小小实践本人也深有体会，机器就像一个小孩一样首先你得根据他的性格（特征化参数）确定应该给予他什么样的学习环境（学习算法创建的学习管道）并提供学习资料（定型机器学习模型数据集），然后为其确定一个发展目标（评估模型数据集），并且不断的进行考试（单个数据的预测与批量数据的预测），考试需要特定的考试场地（预测所需要调用的方法）。通过该种方式让机器不断的学习不断的精进。"}
{"content2":"自从计算机出现后，大量的数据都存储在计算机中，尤其是今几年提出的\"大数据\"，更是意味着存储在计算机中存储\n单位达到PB级。那如何对这些数据进行处理，对于简单的需求可能只需要对数据进行统计即可。但当数据变得复杂\n或有更高的需求时，就需要使用到机器学习。\n这篇文章就是简单介绍对机器学习的认识。\n一、什么是机器学习？\n机器学习，英文名称（Machine Learning），简称ML。顾名思义，它是说机器也就是计算机像人一样进行\"学习\"，但计算机怎么会学习呢？那首先让我们思考一下人类是怎么进行学习的，我们人类是通过对外界的知识的不断积累，包括对传授者的行为信息或书本的信息通过五官进行获取，然后大脑对信息进行处理，最后形成经验。那计算机如何进行学习呢，虽然计算机看上去计算数据很快，但其实很笨，那如何使笨笨的计算机进行学习呢？与人类学习一样，计算机使用大量已有的数据，对计算机训练出模型，并用该模型得出有用的信息。我的个人理解，机器学习是以潜在的模型将无序的数据转换成有用的信息。\n二、机器学习的分类\n按照常见的学习方式分类，能够将机器学习分为：监督式学习、无监督学习、半监督学习、强化学习。下面是常见的几种算法：\n监督学习\nK-近邻算法、线性回归、朴素贝叶斯、SVM、决策树、Logistic回归...\n无监督学习\nK-均值、最大期望算法...\n强化学习\nTD算法、Q算法...\n三、机器学习的应用领域\n机器学习已经应用于多个领域，举一个常见的例子，当你打开浏览器搜索一件商品，浏览器显示10条链接，你点击了第3条链接，搜索引擎后台就会记录你的这次点击，并从中学习以优化下次的搜索结果，当你进行下一次搜索时，可能会有上次搜索商品对应的广告或搜索结果。这其中就是典型的机器学习应用，当用户使用智能手机进行自拍时，有些手机就能进行人脸识别，锁定人的头像。机器学习应用多个领域，如改善商业决策、检测疾病、预测天气、信息安全、生物信息等。\n三、机器学习与数据挖掘及大数据之间的关系\n数据挖掘是指从大量数据中挖掘有趣模式和知识的过程。从数据本身来考虑，数据挖掘通常需要有信息收集、数据集成、数据规约、数据清理、数据变换、数据挖掘实施过程、模式评估和知识表示8个步骤。而机器学习是数据挖掘的一种方法。数据挖掘还包含其他的诸如统计学、模式识别、数据仓库等技术。\n大数据（BigData）是指无法在可承受的时间范围内用常规软件工具进行捕捉、管理和处理的数据集合。大数据的5V特征：Volume（大量）、Velocity（高速）、Variety（多样）、Value（价值）、Veracity（真实性）。大数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。大数据与机器学习两者是相辅相成的关系。"}
{"content2":"引言\n本系列文章是本人对Andrew NG的机器学习课程的一些笔记，如有错误，请读者以课程为准。\n在现实生活中，我们每天都可能在不知不觉中使用了各种各样的机器学习算法。\n例如，当你每一次使用 Google 时，它之所以可以运行良好，其中一个重要原因便是由 Google 实现的一种学习算法可以“学会”如何对网页进行排名。每当你使用 Facebook 或者 Apple 的照片处理应用时，它们都能自动识别出你朋友的照片，这也是机器学习的一种。每当你阅读电子邮件时，你的垃圾邮件过滤器将帮助你免受大量垃圾邮件的困扰，这也是通过一种学习算法实现的。\n我们有这样一个梦想，就是有朝一日，可以创造出像人类一样聪明的机器。很多人工智能专家认为实现这一目标最好的途径便是通过学习算法来模拟人类大脑的学习方式。\n机器学习发源于人工智能领域，我们希望能够创造出具有智慧的机器。我们可以通过编程来让机器完成一些基础的工作，例如如何找到从 A 到 B 的最短路径。但在大多数情况下，我们并不知道如何显式地编写人工智能程序来做一些更有趣的任务，例如网页搜索、标记照片和拦截垃圾邮件等。人们意识到唯一能够达成这些目标的方法就是让机器自己学会如何去做。\n现今，Machine Learning 已经发展成为计算机领域的一项新能力，并且与工业界和基础科学界有着紧密的联系。在硅谷，机器学习引导着大量的课，如自主机器人、计算生物学等。机器学习的实例还有很多，例如数据挖掘。\n机器学习之所以变得如此流行，原因之一便是网络和自动化算法的爆炸性增长。这意味着我们掌握了比以往多得多的数据集。举例来说，当今有数不胜数的硅谷企业，在收集有关网络点击的数据 (Clickstream Data)，并试图在这些数据上运用机器学习的算法来更好的理解和服务用户，这在硅谷已经成为了一项巨大的产业。\n随着电子自动化的发展，我们拥有了电子医疗记录，如果我们能够将这些记录转变为医学知识，那么，我们就能对各种疾病了解的更深入。同时，计算生物学也在电子自动化的辅助下快速发展，生物学家收集了大量有关基因序列以及DNA序列的数据，通过对其应用机器学习的算法可以帮助我们更深入地理解人类基因组及其人类基因组对我们人类的意义。\n几乎工程界的所有领域都在使用机器学习算法来分析日益增长的海量数据集。有些机器应用我们并不能够通过手工编程来实现。比如说，想要写出一个能让直升机自主飞行的程序几乎是不可能的任务。唯一可行的解决方案就是让一台计算机能够自主地学会如何让直升机飞行。\n再比如手写识别，如今将大量的邮件按地址分类寄送到全球各地的代价大大降低，其中重要的理由之一便是每当你写下这样一封信时，一个机器学习的算法已经学会如何读懂你的笔迹并自动地将你的信件发往它的目的地。\n你也许曾经接触过自然语言处理和计算机视觉。事实上，这些领域都是试图通过 AI 来理解人类的语言和图像，如今大多数的自然语言处理和计算机视觉都是对机器学习的一种应用。\n机器学习算法也在 self-customizing program 中有着广泛的应用。每当你使用亚马逊 Netflix 或 iTunes Genius 的服务时，都会收到它们为你量身推荐的电影或产品，这就是通过学习算法来实现的。很显然，这些应用都有着上千万的用户，而针对这些海量的用户，编写千万个不同的程序显然是不可能的，唯一有效的解决方案就是开发出能够进行自我学习，定制出符合你喜好的并据此进行推荐的软件。\n最后，机器学习算法已经被应用于探究人类的学习方式，并试图理解人类的大脑。\nWhat is machine learning\nWhat is machine learning? 不同人对机器学习有不同的定义。下面是 亚瑟·塞穆尔 (Arthur Samuel) 给出的机器学习的定义：\nArthur Samuel (1959).\nMachine Learning: Field of study that gives computers the ability to learn without being explicitly programmed.\n亚瑟·塞穆尔将机器学习定义为：在没有明确为计算机编写（完成某项具体任务的）程序的情况下，让计算机拥有 “学习” 能力的一个研究领域。\nSamuel 出名是因为在50年代 ，他编程实现了一个玩西洋跳棋的程序。这个跳棋程序的神奇之处在于，他让程序跟程序自身下了成千上万盘棋，跳棋程序通过观察分析什么样的棋局更容易致胜，什么样的棋局更容易输，逐渐学会了什么是好的棋局，什么是坏的棋局。最终，跳棋程序的下棋水平超过了 Samuel 。\n这是一个相当了不起的成果，虽然 Samuel 自己并不是一个很好的棋手，但因为计算机（跳棋程序）可以跟自身对弈成千上万次，通过这样的训练，计算机得到了很多的下棋经验，最终使得计算机最终成为了比 Samuel 更好的棋手。\n以上是一个不太正式并且有点老的定义，下面是一个更新的定义，来自 Carnegie Mellon University 的 Tom Mitchell 提出：\nTom Mitchell (1998).\nWell-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n如果一个计算机程序在任务 T 上的性能度量 P ，通过经验 E 而提高，那么我们称这个计算机程序通过经验 E 来学习。\n具体到下跳棋的例子里面，训练经验 E 指的是让计算机程序与 Samuel 对弈成千上万次的经验；任务 T 指的的是下跳棋这个任务，性能标准 P\n指的是跳棋程序在下一场面对新对手的比赛中获胜的概率。\n学习算法分好几个类型，主要分成两大类，分别是监督学习 (Supervised Learning) 和无监督学习 (Unsupervised Learning)，在后面的博文中我将介绍这些术语的具体含义。不过归根到底，Supervised Learning 就是我们要明确告诉计算机如何做某件事情，而 Unsupervised Learning 则意味着我们要让程序自己进行学习。\n在以后的博文中，我们也会讨论一些其他术语，比如强化学习 (Reinforcement Learning) 和推荐系统 (Recommender Systems)，这些其他类型的机器学习算法，我们在以后都会讨论，但两个最常用的学习算法实际上就是就是 Supervised Learning 和 Unsupervised Learning 。\n接下来，我们来讨论什么是 Supervised Learning ，什么是 Unsupervised Learning ，并且会讨论在什么情况下使用这两种算法。\nSupervised Learning\n我们用一个例子开头，介绍什么是监督学习，正式的定义会在后面介绍。\n假设你现在想要预测房价，并且拥有一些关于房价的数据，如下：\n其中横轴表示房子的面积（单位是平方英尺），纵轴表示房价（单位是千美元），假如你有一套750平方英尺大小的房子想要卖掉，那么基于以上数据，你如何推测房子大概值多少钱。\n对于这个问题，我们可以应用机器学习算法，在这组数据中画一条直线或者说是拟合一条直线，根据这条线我们可以推测出这套房子可能卖$150, 000。当然这不是唯一的算法，比如一个二次函数可能更适合已有的数据，我们使用这个二次函数的曲线来进行预测可能效果会更好。\n以上就是一个 Supervised Learning 的例子，可以看出 Supervised Learning 指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价，然后运用学习算法，计算出更多的正确答案，比如你的那个新房子的价格，用术语来讲，这叫做回归问题。\n我们试着推测出一个连续值的结果，即房子的价格。一般房子的价格会记到美分，所以房价实际上是一系列离散的值，但是我们通常又把房价看成实数，看成是标量，因此又把它看成一个连续的数值，回归这个词的意思指的就是，我们在试着推测出这一系列连续值属性。\n回归问题：我们所预测的结果是连续的值。\n我们再来讨论另外一个监督学习的例子，假使你希望通过查看病例来预测一个乳腺癌是否是良性的，这个数据集中，横轴表示肿瘤的大小，纵轴上，我标出 1 和 0 来分别表示是恶性肿瘤或者不是恶性肿瘤。我们之前见过的肿瘤，如果是恶性记为1，不是恶性（或者说是良性）则记为0。\n假设现在我们有一个朋友很不幸检查出乳腺肿瘤，假设说她的肿瘤大概这么大，那么机器学习的问题就在于，你能否估算出肿瘤是恶性的或是良性的概率。用术语来讲，这是一个分类问题。\n分类指的是我们试着推测出离散的输出值: 0 或 1 、良性或恶性。而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出0、1、2、3。其中 0 代表良性，1 表示第一类乳腺癌，2 表示第二类癌症，3 表示第三类。但是，实际上这也是分类问题，因为这几个离散的输出分别对应良性、第一类、第二类或者第三类癌症。\n在分类问题中我们可以用另一种方式来绘制这些数据点。我们可以用不同的符号来表示这些数据，既然我们把肿瘤的尺寸看做区分恶性或良性的特征，那么我们可以这么画，用不同的符号来表示良性和恶性肿瘤，或者说是负样本和正样本。现在我们不全部画X，而是改成良性的肿瘤用O表示，恶性的继续用X表示。我们所做的只是把上面的数据一一映射下来，映射到一根直线上，并且用不同的符号 O 和 X 来表示良性和恶性样本。\n注意，在这个例子中，我们只用了肿瘤的尺寸这一种特征来预测肿瘤的恶性与否，在其它一些机器学习问题中可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄，那现在数据集看起来可能是如下这个样子：\n也就是说，你现有的数据集是不同病人的年龄和她们身上肿瘤的尺寸大小以及这些肿瘤的良性与否。我们以横坐标为肿瘤的尺寸，以纵坐标为病人的年龄，以 O 代表良性肿瘤，以 X 代表恶性肿瘤。我们的学习算法要做的就是确定出这样一条直线，把恶性肿瘤和良性肿瘤分开。如果根据你学习算法得出的结论是你朋友的肿瘤落在良性这一边，那么现实中就更可能是良性的而不是恶性。\n在这个例子中，我们有两种特征，患者年龄和肿瘤大小，而在在其他机器学习问题中，我们通常有更多的特征。就以之前的乳腺癌为例来说，还可以采用这些特征：肿块密度、肿瘤细胞尺寸的一致性和形状的一致性等等。\n我们之后的博文会介绍一种学习算法，这种学习算法不仅能处理2种、3种或者5种特征，甚至即使有无限多种特征，它也都可以处理。如果你想用无限多种特征，以便让你的算法可以利用大量的特征或者说是线索来做推测，那么你如何来处理这无限多个特征，甚至怎么来存储这些特征都存在着很大的问题，比如说你电脑的内存肯定就不够用。之后我们会介绍这个算法，叫作SVN（支持向量机），里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。\n小结\n本章我们介绍了 Supervised Learning ，它的基本思想是我们数据集中的每个样本都有相应的“正确答案”，再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。\n我们还介绍了回归问题，即通过回归来推测出一个连续的输出。之后我们介绍了分类问题，其目标是推测出一组离散的结果。\n现在来个小测验，假设你经营着一家公司，你想开发学习算法来处理以下两个问题。\n第一个问题是，你有一大批相同的货物，你想预测接下来的三个月能卖出多少件。第二个问题是，你有许多客户，这时你想写一个软件来检验每一个用户的账户，而对于每一个账户，你要判断它们是否曾经被盗过。这两个问题，它们是属于分类问题，还是回归问题?\n显然，问题一是一个回归问题，因为如果有数千件货物，我们会把它看成一个实数，看成一个连续的值，因此卖出的物品数同样也是一个连续的值。问题二是一个分类问题，我们可以把预测的值用 0 来表示账户未被盗，用 1 表示账户曾经被盗过，就像乳腺癌的例子 0 代表良性，1 代表恶性，所以我们根据账号是否被盗过而把它们定为 0 或 1 ，然后用算法推测一个账号是 0 还是 1 ，因为只有少数的离散值，所以我们把它归为分类问题。\n以上就是 Supervised Learning 的内容，下面我们来看 Unsupervised Learning 。\nUnsupervised Learning\n我们现在来讨论 Unsupervised Learning ，之前我们已经讲过了 Supervised Learning 。回想一下之前的数据集，每个样本都已经被标明为\n正样本或者负样本，即良性或恶性肿瘤。因此，对于 Supervised Learning 中的每一个样本，我们已经被清楚地告知了，什么是所谓的正确答案，即它们是良性还是恶性。\n在 Unsupervised Learning 中，我们所用的数据会和 Supervised Learning 里的看起来有些不一样。在 Unsupervised Learning 中，没有属性或标签这一概念，也就是说所有的数据都是一样的，没有区别。\n所以在 Unsupervised Learning 中，我们只有一个数据集，没人告诉我们该怎么做，我们也不知道每个数据点究竟是什么意思。相反，它只告诉我们，现在有一个数据集，你能在其中找到某种结构吗？\n对于给定的数据集，Unsupervised Learning Algorithm可能判定该数据集包含两个不同的聚类。无监督学习算法会把这些数据分成两个不同的聚类，这就是所谓的聚类算法。\n聚类算法实例\n实际上 Unsupervised Learning 被用在许多地方。我们来举一个聚类算法的例子，是关于Google 新闻的例子。\n谷歌新闻每天都在干什么呢？他们每天会去收集成千上万的网络上的新闻，然后将他们分组，组成一个个新闻专题。谷歌新闻所做的就是去搜索成千上万条新闻，然后自动的将他们聚合在一起，有关同一主题的新闻被显示在一起。\n其实，聚类算法和无监督学习算法也可以被用于许多其他的问题。这里，我们举个它在基因组学中的应用，下面是一个关于基因芯片的例子：\n基本的思想是，给定一组不同的个体，对于每个个体，检测它们是否拥有某个特定的基因。也就是说，你要去分析有多少基因显现出来了。因此，这些颜色：红、绿、灰等等，它们展示了这些不同的个体是否拥有一个特定基因的不同程度。\n然后你所能做的就是运行一个聚类算法，把不同的个体归入不同的类或者说归为不同类型的人，这就是无监督学习。我们没有提前告知这个算法哪些是第一类的人、哪些是第二类的人、哪些是第三类的人等等。相反我们只是告诉算法，这儿有一堆数据，我不知道这个数据是什么东西，我不知道里面都有些什么类型，叫什么名字，我甚至不知道都有哪些类型。但是，请问你可以自动的找到这些数据中的类型吗？然后自动的按得到的类型把这些个体分类，虽然事先我并不知道哪些类型，因为对于这些数据样本来说，我们没有给算法一个正确答案，所以，这就是无监督学习。\n无监督学习或聚类算法在其他领域也有着大量的应用，它被用来组织大型的计算机集群。一些朋友在管理大型数据中心（大型计算机集群），并试图找出哪些机器趋向于协同工作，如果你把这些机器放在一起，你就可以让你的数据中心更高效地工作。\n还有应用可以用于社交网络的分析。所以，如果可以得知你用 email 联系最多的是哪些朋友，或者知道你的 Facebook 好友，或者你 Google+ 里的朋友，知道了这些之后信息后，我们可以自动识别哪些是很要好的朋友组，哪些仅仅是互相认识的朋友组。\n还有在市场分割中的应用，许多公司拥有庞大的客户信息数据库，那么给你一个客户数据集，你能否自动找出不同的市场分割，并自动将你的客户分到不同的细分市场中，从而有助于你在不同的细分市场中进行更有效的销售，这也是无监督学习。我们现在有这些客户数据，但我们预先并不知道有哪些细分市场，而且对于我们数据集的某个客户，我们也不能预先知道谁属于细分市场一，谁又属于细分市场二等等。但我们必须让这个算法自己去从数据中发现这一切。\n实际上无监督学习也被用于天文数据分析，通过这些聚类算法，我们发现了许多惊人的、有趣的、以及实用的关于星系是如何诞生的理论，所有这些都是聚类算法的例子。\n鸡尾酒宴问题\n再一个 Unsupervised Learning Algorithm 的例子是鸡尾酒宴问题。想象一下，一个宴会有一屋子的人，大家都坐在一起，并且在同时说话，因此会有许多声音混杂在一起，因为许多人会在同一时间说话，在这种情况下你很难听清楚你面前的人说的话。\n因此，比如有这样一个场景，宴会上只有两个人，两个人同时说话（恩，没错…这是个很小的鸡尾酒宴会），我们准备好了两个麦克风，把它们放在房间里，然后因为这两个麦克风距离这两个人的距离是不同的，每个麦克风都记录下了来自两个人的声音的不同组合。\n也许A的声音在第一个麦克风里的声音会响一点，也许B的声音在第二个麦克风里会比较响一些，因为2个麦克风的位置相对于2个说话者的位置是不同的，但每个麦克风都会录到来自两个说话者的重叠部分的声音。\n所以，我们能做的就是把这两个录音输入一种无监督学习算法中，称为“鸡尾酒会算法”。让这个算法帮你找出其中蕴含的分类，然后这个算法就可以去识别这些录音，分离出这两个被叠加到一起的音频源。以上所说的正是“鸡尾酒会问题”的简化版本。\n鸡尾酒会问题（Cocktail Party Problem），在一个满是人的房间中，人们都在互相对话，我们使用一些麦克风录下房间中的声音，利用非监督学习算法来识别房间中某一个人所说的话。\n总结：根据录音，算法找出蕴含分类，之后算法就可以识别其他合成的录音中，哪些是属于这个分类，哪些是属于那个分类。\n总结\n监督学习（分类，回归）\n无监督学习（聚类）"}
{"content2":"一,引言：\n上一章我们讲的kNN算法，虽然可以完成很多分类任务，但它最大的缺点是无法给出数据的内在含义，而决策树的主要优势就在于数据形式非常容易理解。决策树算法能够读取数据集合，决策树的一个重要任务是为了数据所蕴含的知识信息，因此，决策树可以使用不熟悉的数据集合，并从中提取一系列规则，在这些机器根据数据集创建规则是，就是机器学习的过程。\n二，相关知识\n1 决策树算法\n在构造决策树时，第一个需要解决的问题就是，如何确定出哪个特征在划分数据分类是起决定性作用，或者说使用哪个特征分类能实现最好的分类效果。这样，为了找到决定性的特征，划分川最好的结果，我们就需要评估每个特征。当找到最优特征后，依此特征，数据集就被划分为几个数据子集，这些数据自己会分布在该决策点的所有分支中。此时，如果某个分支下的数据属于同一类型，则该分支下的数据分类已经完成，无需进行下一步的数据集分类；如果分支下的数据子集内数据不属于同一类型，那么就要重复划分该数据集的过程，按照划分原始数据集相同的原则，确定出该数据子集中的最优特征，继续对数据子集进行分类，直到所有的特征已经遍历完成，或者所有叶结点分支下的数据具有相同的分类。\n创建分支的伪代码函数createBranch（）如下：\n检测数据集中的每一个子项是否属于同一分类：\nif so return 类标签; else 寻找划分数据集的最好特征 划分数据集 创建分支结点 for 每个分支结点 调用函数createBranch并增加返回结点到分支结点中//递归调用createBranch（） return 分支结点\n了解了如何划分数据集后，我们可以总结出决策树的一般流程：\n（1）收集数据\n（2）准备数据：构造树算法只适用于标称型数据，因此数值型数据必须离散化\n（3）分析数据\n（4）训练数据：上述的构造树过程构造决策树的数据结构\n（5）测试算法：使用经验树计算错误率\n（6）使用算法：在实际中更好地理解数据内在含义\n2 最好特征选取的规则：信息增益\n划分数据集的大原则是：将无序的数据变得更加有序。在划分数据集前后信息发生的变化称为信息增益，如果我们知道如何计算信息增益，就可以计算每个特征值划分数据集获得的信息增益，而获取信息增益最高的特征就是最好的特征。\n接下来，我们讲学习如何计算信息增益，而提到信息增益我们又不得不提到一个概念\"香农熵\"，或者简称熵。熵定义为信息的期望值。\n如果待分类的事物可能会出现多个结果x，则第i个结果xi发生的概率为p(xi),那么我们可以由此计算出xi的信息熵为l(xi)=p(xi)log(1/p(xi))=-p(xi)log(p(xi))\n那么，对于所有可能出现的结果，事物所包含的信息希望值（信息熵）就为：H=-Σp(xi)log(p(xi))，i属于所有可能的结果\n这样，假设利用数据集中某一特征A对数据集D（D的分类类别有n种）进行分类，而特征A取值有k种，那么此时，利用特征A对数据集进行分类的信息增益为：\n信息增益H(D,A)=原始数据集的信息熵H(D)-特征A对数据集进行划分后信息熵H(D/A)\n其中H(D/A)=∑|Aj|/|D|*H(Aj)，j属于A的k种取值，|Aj|和|D|分别表示，特征A第j种取值的样本数占所有取值样本总数的比例，以及数据集的样本总数\n三，构造决策树\n在知道了如何选取划分数据的最优特征后，我们就可以依据此来构建决策树了。\n1 由于要多次使用香农熵的公式，所以我们写出计算给定数据集的熵的公式：\n#计算给定数据集的熵 #导入log运算符 from math import log def calEnt(dataSet): #获取数据集的行数 numEntries=len(dataSet) #设置字典的数据结构 labelCounts={} #提取数据集的每一行的特征向量 for featVec in dataSet: #获取特征向量的最后一列的标签 currentLabel=featVec[-1] #检测字典的关键字key中是否存在该标签 #如果不存在keys()关键字 if currentLabel not in labelCounts.keys(): #将当前标签/0键值对存入字典中 labelCounts[currentLabel]=0 #否则将当前标签对应的键值加1 labelCounts[currentLabel]+=1 #初始化熵为0 Ent=0.0 #对于数据集中所有的分类类别 for key in labelCounts: #计算各个类别出现的频率 prob=float(labelCounts[key])/numEntries #计算各个类别信息期望值 Ent-=prob*log(prob,2) #返回熵 return Ent\n2 我们当然需要构建决策树的数据集：\n#创建一个简单的数据集 #数据集中包含两个特征'no surfacing','flippers'; #数据的类标签有两个'yes','no' def creatDataSet(): dataSet=[[1,1,'yes'], [1,1,'yes'], [1,0,'no'], [0,1,'no'], [0,1,'no']] labels=['no surfacing','flippers'] #返回数据集和类标签 return dataSet,labels\n需要说明的是，熵越高，那么混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大\n3 接下来我们就要通过上面讲到的信息增益公式得到划分数据集的最有特征，从而划分数据集\n首先划分数据集的代码：\n#划分数据集：按照最优特征划分数据集 #@dataSet:待划分的数据集 #@axis:划分数据集的特征 #@value:特征的取值 def splitDataSet(dataSet,axis,value): #需要说明的是,python语言传递参数列表时，传递的是列表的引用 #如果在函数内部对列表对象进行修改，将会导致列表发生变化，为了 #不修改原始数据集，创建一个新的列表对象进行操作 retDataSet=[] #提取数据集的每一行的特征向量 for featVec in dataSet: #针对axis特征不同的取值，将数据集划分为不同的分支 #如果该特征的取值为value if featVec[axis]==value: #将特征向量的0~axis-1列存入列表reducedFeatVec reducedFeatVec=featVec[:axis] #将特征向量的axis+1~最后一列存入列表reducedFeatVec #extend()是将另外一个列表中的元素（以列表中元素为对象）一一添加到当前列表中，构成一个列表 #比如a=[1,2,3],b=[4,5,6],则a.extend(b)=[1,2,3,4,5,6] reducedFeatVec.extend(featVec[axis+1:]) #简言之，就是将原始数据集去掉当前划分数据的特征列 #append()是将另外一个列表（以列表为对象）添加到当前列表中 ##比如a=[1,2,3],b=[4,5,6],则a.extend(b)=[1,2,3,[4,5,6]] retDataSet.append(reducedFeatVec) return retDataSet\n需要说明的是：\n（1）在划分数据集函数中，传递的参数dataSet列表的引用，在函数内部对该列表对象进行修改，会导致列表内容发生改变，于是，为了消除该影响，我们应该在函数中创建一个新的列表对象，将对列表对象操作后的数据集存入新的列表对象中\n（2）需要区分一下append()函数和extend()函数\n这两种方法的功能类似，都是在列表末尾添加新元素，但是在处理多个列表时，处理结果有所不同：\n比如：a=[1,2,3],b=[4,5,6]\n那么a.append(b)的结果为：[1,2,3,[4,5,6]],即使用append()函数会在列表末尾添加人新的列表对象b\n而a.extend(b)的结果为：[1,2,3,4,5,6]，即使用extend()函数\n接下来，我们再看选取最优特征的代码：\n#如何选择最好的划分数据集的特征 #使用某一特征划分数据集，信息增益最大，则选择该特征作为最优特征 def chooseBestFeatureToSplit(dataSet): #获取数据集特征的数目(不包含最后一列的类标签) numFeatures=len(dataSet[0])-1 #计算未进行划分的信息熵 baseEntropy=calEnt(dataSet) #最优信息增益 最优特征 bestInfoGain=0.0;bestFeature=-1 #利用每一个特征分别对数据集进行划分，计算信息增益 for i in range(numFeatures): #得到特征i的特征值列表 featList=[example[i] for example in dataSet] #利用set集合的性质--元素的唯一性，得到特征i的取值 uniqueVals=set(featList) #信息增益0.0 newEntropy=0.0 #对特征的每一个取值，分别构建相应的分支 for value in uniqueVals: #根据特征i的取值将数据集进行划分为不同的子集 #利用splitDataSet()获取特征取值Value分支包含的数据集 subDataSet=splitDataSet(dataSet,i,value) #计算特征取值value对应子集占数据集的比例 prob=len(subDataSet)/float(len(dataSet)) #计算占比*当前子集的信息熵,并进行累加得到总的信息熵 newEntropy+=prob*calEnt(subDataSet) #计算按此特征划分数据集的信息增益 #公式特征A,数据集D #则H(D,A)=H(D)-H(D/A) infoGain=baseEntropy-newEntropy #比较此增益与当前保存的最大的信息增益 if (infoGain>bestInfoGain): #保存信息增益的最大值 bestInfoGain=infoGain #相应地保存得到此最大增益的特征i bestFeature=i #返回最优特征 return bestFeature\n在函数调用中，数据必须满足一定的要求，首先，数据必须是由列表元素组成的列表，而且所有的列表元素具有相同的数据长度；其次，数据的最后一列或者每个实例的最后一个元素是当前实例的类别标签。这样，我们才能通过程序统一完成数据集的划分\n4，在通过以上的各个模块学习之后，我们接下来就要真正构建决策树，构建决策树的工作原理为：首先得到原始数据集，然后基于最好的属性划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将向下传递到树分支的下一个结点，在该结点上，我们可以再次划分数据。因此，我们可以采用递归的方法处理数据集，完成决策树构造。\n递归的条件是：程序遍历完所有划分数据集的属性，或者每个分之下的所有实例都具有相同的分类。如果所有的实例具有相同的分类，则得到一个叶子结点或者终止块。\n当然，我们可能会遇到，当遍历完所有的特征属性，但是某个或多个分支下实例类标签仍然不唯一，此时，我们需要确定出如何定义该叶子结点，在这种情况下，通过会采取多数表决的原则选取分支下实例中类标签种类最多的分类作为该叶子结点的分类\n这样，我们就需要先定义一个多数表决函数majorityCnt()\n#当遍历完所有的特征属性后，类标签仍然不唯一(分支下仍有不同分类的实例) #采用多数表决的方法完成分类 def majorityCnt(classList): #创建一个类标签的字典 classCount={} #遍历类标签列表中每一个元素 for vote in classList: #如果元素不在字典中 if vote not in classCount.keys(): #在字典中添加新的键值对 classCount[vote]=0 #否则，当前键对于的值加1 classCount[vote]+=1 #对字典中的键对应的值所在的列，按照又大到小进行排序 #@classCount.items 列表对象 #@key=operator.itemgetter(1) 获取列表对象的第一个域的值 #@reverse=true 降序排序，默认是升序排序 sortedClassCount=sorted(classCount.items,\\ key=operator.itemgetter(1),reverse=true) #返回出现次数最多的类标签 return sortedClassCount[0][0]\n好了，考虑了这种情况后，我们就可以通过递归的方式写出决策树的构建代码了\n#创建树 def createTree(dataSet,labels): #获取数据集中的最后一列的类标签，存入classList列表 classList=[example[-1] for example in dataSet] #通过count()函数获取类标签列表中第一个类标签的数目 #判断数目是否等于列表长度，相同表面所有类标签相同，属于同一类 if classList.count(classList[0])==len(classList): return classList[0] #遍历完所有的特征属性，此时数据集的列为1，即只有类标签列 if len(dataSet[0])==1: #多数表决原则，确定类标签 return majorityCnt(classList) #确定出当前最优的分类特征 bestFeat=chooseBestFeatureToSplit(dataSet) #在特征标签列表中获取该特征对应的值 bestFeatLabel=labels[bestFeat] #采用字典嵌套字典的方式，存储分类树信息 myTree={bestFeatLabel:{}} ##此位置书上写的有误，书上为del(labels[bestFeat]) ##相当于操作原始列表内容，导致原始列表内容发生改变 ##按此运行程序，报错'no surfacing'is not in list ##以下代码已改正 #复制当前特征标签列表，防止改变原始列表的内容 subLabels=labels[:] #删除属性列表中当前分类数据集特征 del(subLabels[bestFeat]) #获取数据集中最优特征所在列 featValues=[example[bestFeat] for example in dataSet] #采用set集合性质，获取特征的所有的唯一取值 uniqueVals=set(featValues) #遍历每一个特征取值 for value in uniqueVals: #采用递归的方法利用该特征对数据集进行分类 #@bestFeatLabel 分类特征的特征标签值 #@dataSet 要分类的数据集 #@bestFeat 分类特征的标称值 #@value 标称型特征的取值 #@subLabels 去除分类特征后的子特征标签列表 myTree[bestFeatLabel][value]=createTree(splitDataSet\\ (dataSet,bestFeat,value),subLabels) return myTree\n需要说明的是，此时参数dataSet为列表的引用，我们不能在函数中直接对列表进行修改，但是在书中代码中有del(labels[bestFeat])的删除列表某一列的操作，显然不可取，应该创建新的列表对象subLabels=labels[:]，再调用函数 del(subLabels[bestFeat])\n好了，接下来运行代码：\n5 接下来，我们可以通过决策树进行实际的分类了，利用构建好的决策树，输入符合要求的测试数据，比较测试数据与决策树上的数值，递归执行该过程直到叶子结点，最后将测试数据定义为叶子结点所有的分类，输出分类结果\n决策树分类函数代码为：\n#------------------------测试算法------------------------------ #完成决策树的构造后，采用决策树实现具体应用 #@intputTree 构建好的决策树 #@featLabels 特征标签列表 #@testVec 测试实例 def classify(inputTree,featLabels,testVec): #找到树的第一个分类特征，或者说根节点'no surfacing' #注意python2.x和3.x区别，2.x可写成firstStr=inputTree.keys()[0] #而不支持3.x firstStr=list(inputTree.keys())[0] #从树中得到该分类特征的分支，有0和1 secondDict=inputTree[firstStr] #根据分类特征的索引找到对应的标称型数据值 #'no surfacing'对应的索引为0 featIndex=featLabels.index(firstStr) #遍历分类特征所有的取值 for key in secondDict.keys(): #测试实例的第0个特征取值等于第key个子节点 if testVec[featIndex]==key: #type()函数判断该子节点是否为字典类型 if type(secondDict[key]).__name__=='dict': #子节点为字典类型，则从该分支树开始继续遍历分类 classLabel=classify(secondDict[key],featLabels,testVec) #如果是叶子节点，则返回节点取值 else: classLabel=secondDict[key] return classLabel\n输入实例，通过分类函数得到预测结果，可以与实际结果比对，计算错误率\n6  我们说一个好的分类算法要能够完成实际应用的需要，决策树算法也不例外，一个算法好不好，还是需要实际应用的检验才行，接下来我们会通过一个实例来使用决策树预测隐形眼镜的类型\n首先，我们知道构建决策树是非常耗时的任务，即使很小的数据集，也要花费几秒的时间来构建决策树，这样显然耗费计算时间。所以，我们可以将构建好的决策树保存在磁盘中，这样当我们需要的时候，再从磁盘中读取出来使用即可。\n如何进行对象的序列化操作，python的pickle模块足以胜任该任务，任何对象都可以通过pickle模块执行序列化操作，字典也不例外，使用pickle模块存储和读取决策树文件的代码如下：\n#决策树的存储：python的pickle模块序列化决策树对象，使决策树保存在磁盘中 #在需要时读取即可，数据集很大时，可以节省构造树的时间 #pickle模块存储决策树 def storeTree(inputTree,filename): #导入pickle模块 import pickle #创建一个可以'写'的文本文件 #这里，如果按树中写的'w',将会报错write() argument must be str,not bytes #所以这里改为二进制写入'wb' fw=open(filename,'wb') #pickle的dump函数将决策树写入文件中 pickle.dump(inputTree,fw) #写完成后关闭文件 fw.close() #取决策树操作 def grabTree(filename): import pickle #对应于二进制方式写入数据，'rb'采用二进制形式读出数据 fr=open(filename,'rb') return pickle.load(fr)\n这里，文件的写入操作为'wb'或'wb+',表示以byte的形式写入数据，相应'rb'以byte形式读入数据\n接下来，我们将通过隐形眼镜数据集构建决策树，从而预测患者需要佩戴的隐形眼镜的类型，步骤如下：\n（1）收集数据：文本数据集'lenses.txt'\n（2）准备数据：解析tab键分隔开的数据行\n（3）分析数据：快速检查数据，确保正确地解析数据内容\n（4）训练算法：构建决策树\n（5）测试算法：通过构建的决策树比较准确预测出分类结果\n（6）算法的分类准确类满足要求，将决策树存储下来，下次需要时读取使用\n其中，解析文本数据集和构建隐形眼镜类型决策树的函数代码如下：\n#------------------------示例：使用决策树预测隐形眼镜类型---------------- def predictLensesType(filename): #打开文本数据 fr=open(filename) #将文本数据的每一个数据行按照tab键分割，并依次存入lenses lenses=[inst.strip().split('\\t') for inst in fr.readlines()] #创建并存入特征标签列表 lensesLabels=['age','prescript','astigmatic','tearRate'] #根据继续文件得到的数据集和特征标签列表创建决策树 lensesTree=createTree(lenses,lensesLabels) return lensesTree\n当然，我们还可以通过python的matplotlib工具绘制出决策树的树形图，由于内容太多，就不一一讲解啦\n接下来，补充一下决策树算法可能或出现的过度匹配（过拟合）的问题，当决策树的复杂度较大时，很可能会造成过拟合问题。此时，我们可以通过裁剪决策树的办法，降低决策树的复杂度，提高决策树的泛化能力。比如，如果决策树的某一叶子结点只能增加很少的信息，那么我们就可将该节点删掉，将其并入到相邻的结点中去，这样，降低了决策树的复杂度，消除过拟合问题。"}
{"content2":"常见的机器学习模型：感知机，线性回归，逻辑回归，支持向量机，决策树，随机森林，GBDT，XGBoost，贝叶斯，KNN，K-means等；\n常见的机器学习理论：过拟合问题，交叉验证问题，模型选择问题，模型融合问题等；\nK近邻：算法采用测量不同特征值之间的距离的方法进行分类。\n优点：\n1.简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；\n2.可用于数值型数据和离散型数据；\n3.训练时间复杂度为O(n)；无数据输入假定；\n4.对异常值不敏感\n缺点：\n1.计算复杂性高；空间复杂性高；\n2.样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；\n3.一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少 否则容易发生误分。\n4.最大的缺点是无法给出数据的内在含义。\n朴素贝叶斯\n优点：\n1.生成式模型，通过计算概率来进行分类，可以用来处理多分类问题，\n2.对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。\n缺点：\n1.对输入数据的表达形式很敏感，\n2.由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。需要一个比较容易解释，而且不同维度之间相关性较小的模型的时候。\n3.需要计算先验概率，分类决策存在错误率。\n决策树\n优点：\n1.概念简单，计算复杂度不高，可解释性强，输出结果易于理解；\n2.数据的准备工作简单， 能够同时处理数据型和常规型属性，其他的技术往往要求数据属性的单一。\n3.对中间值得确实不敏感，比较适合处理有缺失属性值的样本，能够处理不相关的特征；\n4.应用范围广，可以对很多属性的数据集构造决策树，可扩展性强。决策树可以用于不熟悉的数据集合，并从中提取出一些列规则 这一点强于KNN。\n缺点：\n1.容易出现过拟合；\n2.对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征。\n3. 信息缺失时处理起来比较困难。 忽略数据集中属性之间的相关性。\n4.同时它也是相对容易被攻击的分类器。这里的攻击是指人为的改变一些特征，使得分类器判断错误\n随机森林\n严格来说，随机森林其实算是一种集成算法。它首先随机选取不同的特征(feature)和训练样本(training sample)，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。\n随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。\n适用情景：\n数据维度相对低（几十维），同时对准确性有较高要求时。\n因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。\nSvm\n优点：\n1.可用于线性/非线性分类，也可以用于回归，泛化错误率低，计算开销不大，结果容易解释；\n2.可以解决小样本情况下的机器学习问题，可以解决高维问题 可以避免神经网络结构选择和局部极小点问题。\n3.SVM是最好的现成的分类器，现成是指不加修改可直接使用。并且能够得到较低的错误率，SVM可以对训练集之外的数据点做很好的分类决策。\n4.SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。\n缺点：对参数调节和和函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题。\nLogistic回归：根据现有数据对分类边界线建立回归公式，依次进行分类。\n优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低；\n缺点：容易欠拟合，分类精度可能不高\nEM 期望最大化算法-上帝算法\n只要有一些训练数据，再定义一个最大化函数，采用EM算法，利用计算机经过若干次迭代，就可以得到所需的模型。EM算法是自收敛的分类算法，既不需要事先设定类别也不需要数据见的两两比较合并等操作。缺点是当所要优化的函数不是凸函数时，EM算法容易给出局部最佳解，而不是最优解。\n判别分析 (Discriminant analysis)\nLDA的核心思想是把高维的样本投射(project)到低维上，如果要分成两类，就投射到一维。要分三类就投射到二维平面上。这样的投射当然有很多种不同的方式，LDA投射的标准就是让同类的样本尽量靠近，而不同类的尽量分开。对于未来要预测的样本，用同样的方式投射之后就可以轻易地分辨类别了。\n使用情景：\n判别分析适用于高维数据需要降维的情况，自带降维功能使得我们能方便地观察样本分布。它的正确性有数学公式可以证明，所以同样是很经得住推敲的方式。\n但是它的分类准确率往往不是很高，所以不是统计系的人就把它作为降维工具用吧。\n同时注意它是假定样本成正态分布的，所以那种同心圆形的数据就不要尝试了。\n更多分析见 https://www.zhihu.com/question/26726794\n下边这个转自http://www.ppvke.com/Blog/archives/44028\n机器 学习常见算法\n机器学习领域涉及到很多的算法和模型，这里遴选一些常见的算法：\n正则化算法（Regularization Algorithms）\n集成算法（Ensemble Algorithms）\n决策树算法（Decision Tree Algorithm）\n回归（Regression）\n人工神经网络（Artificial Neural Network）\n深度学习（Deep Learning）\n支持向量机（Support Vector Machine）\n降维算法（Dimensionality Reduction Algorithms）\n聚类算法（Clustering Algorithms）\n基于实例的算法（Instance-based Algorithms）\n贝叶斯算法（Bayesian Algorithms）\n关联规则学习算法（Association Rule Learning Algorithms）\n图模型（Graphical Models） ### 正则化算法（Regularization Algorithms） 正则化算法是另一种方法（通常是回归方法）的拓展，这种方法会基于模型复杂性对其进行惩罚，它喜欢相对简单能够更好的泛化的模型。 正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。 算法实例：\n岭回归（Ridge Regression）\n最小绝对收缩与选择算子（LASSO）\nGLASSO\n弹性网络（Elastic Net）\n最小角回归（Least-Angle Regression） 详解链接：机器学习之正则化算法\n集成算法（Ensemble algorithms）\n集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。这类算法又称元算法(meta-algorithm)。最常见的集成思想有两种bagging和boosting。\nboosting\n基于错误提升分类器性能，通过集中关注被已有分类器分类错误的样本，构建新分类器并集成。\nbagging\n基于数据随机重抽样的分类器构建方法。\n算法实例：\nBoosting\nBootstrapped Aggregation（Bagging）\nAdaBoost\n层叠泛化（Stacked Generalization）（blending）\n梯度推进机（Gradient Boosting Machines，GBM）\n梯度提升回归树（Gradient Boosted Regression Trees，GBRT）\n随机森林（Random Forest） 总结：当先最先进的预测几乎都使用了算法集成。它比使用单个模型预测出来的结果要精确的多。但是该算法需要大量的维护工作。 详细讲解：机器学习算法之集成算法 ### 决策树算法（Decision Tree Algorithm） 决策树学习使用一个决策树作为一个预测模型，它将对一个 item（表征在分支上）观察所得映射成关于该 item 的目标值的结论（表征在叶子中）。 决策树通过把实例从艮节点排列到某个叶子结点来分类实例，叶子结点即为实例所属的分类。树上的每一个结点指定了对实例的某个属性的测试，并且该结点的每一个后继分支对应于该属性的一个可能值。分类实例的方法是从这棵树的根节点开始，测试这个结点的属性，然后按照给定实例的属性值对应的树枝向下移动。然后这个过程在以新结点的根的子树上重复。 算法实例：\n分类和回归树（Classification and Regression Tree，CART）\nIterative Dichotomiser 3（ID3）\nC4.5 和 C5.0（一种强大方法的两个不同版本） 详解：机器学习算法之决策树算法\n回归（Regression）算法\n回归是用于估计两种变量之间关系的统计过程。当用于分析因变量和一个 多个自变量之间的关系时，该算法能提供很多建模和分析多个变量的技巧。具体一点说，回归分析可以帮助我们理解当任意一个自变量变化，另一个自变量不变时，因变量变化的典型值。最常见的是，回归分析能在给定自变量的条件下估计出因变量的条件期望。\n算法实例：\n普通最小二乘回归（Ordinary Least Squares Regression，OLSR）\n线性回归（Linear Regression）\n逻辑回归（Logistic Regression）\n逐步回归（Stepwise Regression）\n多元自适应回归样条（Multivariate Adaptive Regression Splines，MARS）\n本地散点平滑估计（Locally Estimated Scatterplot Smoothing，LOESS）\n回归算法详解：机器学习算法之回归算法\n人工神经网络\n人工神经网络是受生物神经网络启发而构建的算法模型。它是一种模式匹配，常被用于回归和分类问题，但拥有庞大的子域，由数百种算法和各类问题的变体组成。\n人工神经网络（ANN）提供了一种普遍而且实际的方法从样例中学习值为实数、离散值或向量函数。人工神经网络由一系列简单的单元相互连接构成，其中每个单元有一定数量的实值输入，并产生单一的实值输出。\n算法实例：\n感知器\n反向传播\nHopfield 网络\n径向基函数网络（Radial Basis Function Network，RBFN）\n详细链接：机器学习算法之人工神经网络\n深度学习（Deep Learning）\n深度学习是人工神经网络的最新分支，它受益于当代硬件的快速发展。\n众多研究者目前的方向主要集中于构建更大、更复杂的神经网络，目前有许多方法正在聚焦半监督学习问题，其中用于训练的大数据集只包含很少的标记。\n算法实例：\n深玻耳兹曼机（Deep Boltzmann Machine，DBM）\nDeep Belief Networks（DBN）\n卷积神经网络（CNN）\nStacked Auto-Encoders\n深度学习详解：机器学习算法之深度学习\n支持向量机（Support Vector Machines）\n支持向量机是一种监督式学习 (Supervised Learning)的方法，主要用在统计分类 (Classification)问题和回归分析 (Regression)问题上。支持向量机属于一般化线性分类器，也可以被认为是提克洛夫规范化（Tikhonov Regularization）方法的一个特例。这族分类器的特点是他们能够同时最小化经验误差与最大化几何边缘区，因此支持向量机也被称为最大边缘区分类器。现在多简称为SVM。\n给定一组训练事例，其中每个事例都属于两个类别中的一个，支持向量机（SVM）训练算法可以在被输入新的事例后将其分类到两个类别中的一个，使自身成为非概率二进制线性分类器。\nSVM 模型将训练事例表示为空间中的点，它们被映射到一幅图中，由一条明确的、尽可能宽的间隔分开以区分两个类别。\n算法详解：机器学习算法之支持向量机\n降维算法（Dimensionality Reduction Algorithms）\n所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x->y，其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。\n这一算法可用于可视化高维数据或简化接下来可用于监督学习中的数据。许多这样的方法可针对分类和回归的使用进行调整。\n算法实例：\n主成分分析（Principal Component Analysis (PCA)）\n主成分回归（Principal Component Regression (PCR)）\n偏最小二乘回归（Partial Least Squares Regression (PLSR)）\nSammon 映射（Sammon Mapping）\n多维尺度变换（Multidimensional Scaling (MDS)）\n投影寻踪（Projection Pursuit）\n线性判别分析（Linear Discriminant Analysis (LDA)）\n混合判别分析（Mixture Discriminant Analysis (MDA)）\n二次判别分析（Quadratic Discriminant Analysis (QDA)）\n灵活判别分析（Flexible Discriminant Analysis (FDA)）\n详解链接：降维算法\n聚类算法（Clustering Algorithms）\n聚类算法是指对一组目标进行分类，属于同一组（亦即一个类，cluster）的目标被划分在一组中，与其他组目标相比，同一组目标更加彼此相似。\n优点是让数据变得有意义，缺点是结果难以解读，针对不同的数据组，结果可能无用。\n算法实例：\nK-均值（k-Means）\nk-Medians 算法\nExpectation Maximi 封层 ation (EM)\n最大期望算法（EM）\n分层集群（Hierarchical Clstering）\n聚类算法详解：机器学习算法之聚类算法\n贝叶斯算法（Bayesian Algorithms）\n贝叶斯定理（英语：Bayes’ theorem）是概率论中的一个定理，它跟随机变量的条件概率以及边缘概率分布有关。在有些关于概率的解说中，贝叶斯定理（贝叶斯更新）能够告知我们如何利用新证据修改已有的看法。贝叶斯方法是指明确应用了贝叶斯定理来解决如分类和回归等问题的方法。\n算法实例：\n朴素贝叶斯（Naive Bayes）\n高斯朴素贝叶斯（Gaussian Naive Bayes）\n多项式朴素贝叶斯（Multinomial Naive Bayes）\n平均一致依赖估计器（Averaged One-Dependence Estimators (AODE)）\n贝叶斯信念网络（Bayesian Belief Network (BBN)）\n贝叶斯网络（Bayesian Network (BN)）\n贝叶斯算法链接：贝叶斯算法详解\n关联规则学习算法（Association Rule Learning Algorithms）\n关联规则学习方法能够提取出对数据中的变量之间的关系的最佳解释。比如说一家超市的销售数据中存在规则 {洋葱，土豆}=> {汉堡}，那说明当一位客户同时购买了洋葱和土豆的时候，他很有可能还会购买汉堡肉。有点类似于联想算法。\n算法实例：\nApriori 算法（Apriori algorithm）\nEclat 算法（Eclat algorithm）\nFP-growth\n关联规则学习算法：关联规则学习算法\n图模型（Graphical Models）\n图模型(GraphicalModels)在概率论与图论之间建立起了联姻关系。它提供了一种自然工具来处理应用数学与工程中的两类问题——不确定性(Uncertainty)和复杂性(Complexity)问 题，特别是在机器学习算法的分析与设计中扮演着重要角色。图模型的基本理念是模块化的思想，复杂系统是通过组合简单系统建构的。概率论提供了一种粘合剂使 系统的各个部分组合在一起，确保系统作为整体的持续一致性，提供了多种数据接口模型方法。\n图模型或概率图模型（PGM/probabilistic graphical model）是一种概率模型，一个图（graph）可以通过其表示随机变量之间的条件依赖结构（conditional dependence structure）。\n算法实例：\n贝叶斯网络（Bayesian network）\n马尔可夫随机域（Markov random field）\n链图（Chain Graphs）\n祖先图（Ancestral graph）\n图模型详解：机器学习算法之图模型"}
{"content2":"最新更新：【深度学习系列】PaddlePaddle之手写数字识别（10.26更新）\n【深度学习系列】卷积神经网络CNN原理详解(一)——基本原理（11.1更新）\n【深度学习系列】PaddlePaddle之数据预处理（11.8更新）\n【深度学习系列】卷积神经网络详解(二)——自己手写一个卷积神经网络（11.22更新）\n【深度学习系列】用PaddlePaddle和Tensorflow进行图像分类（11.29更新）\n【深度学习系列】用PaddlePaddle和Tensorflow实现经典CNN网络AlexNet（12.06更新）\n【深度学习系列】用PaddlePaddle和Tensorflow实现经典CNN网络Vgg(12.13更新)\n【深度学习系列】用PaddlePaddle和Tensorflow实现经典CNN网络GoogLeNet（12.20更新）\n【深度学习系列】用PaddlePaddle和Tensorflow实现GoogLeNet InceptionV2/V3/V4（12.27更新）\n【深度学习系列】一起来参加百度 PaddlePaddle AI 大赛吧！(2018.1.4更新)\n【深度学习系列】关于PaddlePaddle的一些避“坑”技巧（2018.1.13更新）\n原文：\n不用理会标题，纯粹标题党，但是有干货！昨天看到某位\"大牛\"写了篇文章，上了首页推荐，叫做\"跟着弦哥学人工智能\"，看到标题还挺惊喜，毕竟在博客园这个以.net文章为主的技术论坛居然还有大佬愿意写AI方面的文章，于是点击去仔细看了看，发现文风浮夸，恩，没关系，有干货就行，结果翻到最后也没发现啥干货，看到了参考书目，挺有意思的。放个图在这：\n当时看到这个参考书目挺迷的，数学类从高中数学推荐到数学专业学生看的数学分析，计算机算法类一上来就推荐大块头的《算法导论》和理论性偏强的《数据挖掘：概念与技术》，认为这样入门的人来说并不合适。看书应当是有阶梯型的，不能一口吃成个大胖子，基于不想\"大牛\"误人子弟，于是我给出了如下建议：\n我的回复很平和，也给出了一些对新手比较友好的建议，并且有6个人支持我，想想算了，然而，今天，在首页中又看到了这位\"大牛\"在博文骂我是喷子：\n这我就不赞同而且不能忍了。对于任何人，不管你是大牛还是小白，我的原则都是，你可以反驳我的建议，有理有据就行，如果我错了，那就改，没有错，那就互相讨论，交流一下，气场合说不定还能成为个朋友呢。但是对于别人真诚的建议您回以\"喷子\"是一个有教养的人的表现吗？仗着自己是\"大牛\",这样没有素质的怼不觉得脸红吗？并且，我之所以给出这个建议，有以下三点：\n1.作为一个数学系的学生，学了四年数学，对于你胡乱给的参考书目非常的不赞同。一没有阶梯式，对新手不友好，您的标题和写这个系列的目的大概都是准备给小白看的，那么请问，一个小白需要看数学分析原理？？学习人工智能有必要需要看普林斯顿微积分原理？？以鄙人浅薄的认识来说，数学分析与高等数学最大的区别是同一个定理，高数只要求会用即可，数学分析本着严谨性，一定会给出证明。然而对于大多数人工智能里所需要的数学，您在工作中需要证明这个定理的正确性和完备性？？等你证出来恐怕项目早就结了。您回复我说这只是参考书目不是推荐书目，但是下面的评论大多数是看到书单就决定放弃。您把这些书放出来，就对看您文章的人有一定的引导性，我认为您这是在误人子弟。二是我通过您参考的数目粗略的推导您自己可能都没有对整个人工智能的数学有个框架式的梳理，不然不会有如此不负责任的推荐。但是出于对您的尊重，我并没有质疑您的能力，仅仅在评论里对于新手适合的数学书目给了一个简短的推荐。\n2.作为一个从事机器学习这一行两年多的程序媛，看到您推荐的计算机系列也认为非常的不靠谱。您推荐的这些书，我大部分都看过。尤其不推荐新手看的就是《算法导论》和《数据挖掘：概念与技术》，这两本书又厚又重，虽然内容全面讲的也不错，但是等你看完都不知道猴年马月了。新手需要的是什么？是上手！其次，《Python核心编程》您真的看完了吗？这本书并不是给Python新人读的，非常厚，而且有一定的难度，对于新手非常不友好。而且如果只是想做AI，那这本书的很多地方都不需要用到，web开发，Django框架对于我们AI工程师来说真的是必须的吗？不是。小白对于一本书没有重点和非重点的区分，花了大量的时间学了不需要的知识，真是得不偿失。给出引导性，针对性的推荐才是负责任的推荐。\n3.对于深度学习方面书籍的推荐我就不吐槽了。槽点太多，无力吐槽。省点力气后面推荐真正适合不同阶段的新手阅读的书籍好了。\n总结：这位大牛，我认为您可能在.net方面积累非常深，做的很好，吸引了一大波粉丝，这点我很佩服您。然而对于深度学习这一块您可能并不是很了解。对于一个您并不是很了解的领域，在这里毫不谦虚，对于别人的建议充耳不闻，还很得意的骂人“喷子”，恐怕您还是要多谦虚一点，多学习学习。从从业年龄来看，我是您的后辈，但从从事深度学习这个领域来看，您可能还是个新人，您说呢?并且到目前为止，您发了两篇文章在首页，都没有任何干货，希望您赶紧拿干货来打我的脸^_^\n下面，开始输出干货。\nAI处于目前的风口，于是很多人想要浑水摸鱼，都来分一杯羹，然而可能很多人连AI是什么都不知道。AI，深度学习，机器学习，数据挖掘，数据分析这几点的联系和区别也搞不清楚。于是滋生了很多培训班，收着不菲的费用，教教demo，调调参，教你一个月速成深度学习工程师，赚的盆满钵满。这种行业风气我们应该摒弃！我认为，目前市面上的任何AI培训都不值得参加！别撒钱给别人了，难道不会心痛吗？ - -然而，当大家自学的时候，又不知道从何学起。下了一堆资料，跑了一堆demo，报了一堆cousera，调调参，看看模型结果不错，就以为入了门，抱歉，不好意思，我说话比较直接，可能你连门都没入。我认为，深度学习这块，有几个层次：（自己胡乱起的名字，忽略吧  - -）\ndemo侠--->调参侠--->懂原理侠--->懂原理+能改模型细节侠--->超大数据操控侠--->模型/框架架构师\ndemo侠：下载了目前所有流行的框架，对不同框里的例子都跑一跑，看看结果，觉得不错就行了，进而觉得，嘛，深度学习也不过如此嘛，没有多难啊。这种人，我在面试的时候遇到了不少，很多学生或者刚转行的上来就是讲一个demo，手写数字识别，cifar10数据的图像分类等等，然而你问他这个手写数字识别的具体过程如何实现的？现在效果是不是目前做好的，可以再优化一下吗？为什么激活函数要选这个，可以选别的吗？CNN的原理能简单讲讲吗？懵逼了。\n调参侠：此类人可能不局限于跑了几个demo，对于模型里的参数也做了一些调整，不管调的好不好，先试了再说，每个都试一下，学习率调大了准确率下降了，那就调小一点，那个参数不知道啥意思，随便改一下值测一下准确率吧。这是大多数初级深度学习工程师的现状。当然，并不是这样不好，对于demo侠来说，已经进步了不少了，起码有思考。然而如果你问，你调整的这个参数为什么会对模型的准确率带来这些影响，这个参数调大调小对结果又会有哪些影响，就又是一问三不知了。\n懂原理侠：抱歉我起了个这么蠢的名字。但是，进阶到这一步，已经可以算是入门了，可以找一份能养活自己的工作了。CNN，RNN，LSTM信手拈来，原理讲的溜的飞起，对于不同的参数对模型的影响也是说的有理有据，然而，如果你要问，你可以手动写一个CNN吗？不用调包，实现一个最基础的网络结构即可，又gg了。\n懂原理+能改模型细节侠：如果你到了这一步，恭喜你，入门了。对于任何一个做机器学习/深度学习的人来说，只懂原理是远远不够的，因为公司不是招你来做研究员的，来了就要干活，干活就要落地。既然要落地，那就对于每一个你熟悉的，常见的模型能够自己手动写代码运行出来，这样对于公司的一些业务，可以对模型进行适当的调整和改动，来适应不同的业务场景。这也是大多数一二线公司的工程师们的现状。然而，对于模型的整体架构能力，超大数据的分布式运行能力，方案设计可能还有所欠缺，本人也一直在这个阶段不停努力，希望能够更进一步。\n超大数据操控侠：到这一阶段，基本上开始考虑超大数据的分布式运行方案，对整体架构有一个宏观的了解，对不同的框架也能指点一二。海量数据的分布式运行如何避免网络通信的延迟，如何更高效更迅速的训练都有一定经验。这类人，一般就是我这种虾米的领导了。\n模型/框架架构师：前面说了一堆都是对现有的框架/模型处理的经验，这个阶段的大侠，哦，不对，是大师可以独立设计开发一套新框架/算法来应对现有的业务场景，或者解决一直未解决的历史遗留问题。没啥好说了，膜拜！\n说了这么多，希望大家对自己找个清洗准确的定位，这样才能针对性的学习。下面基于我个人的经验对不同阶段的学习者做一些推荐：\ndemo侠+调参侠：这两个放在一起说，毕竟五十步笑百步，谁也没有比谁强多少。当然也不要妄自菲薄，大家都是从这个阶段过来的。这个阶段编程不好的就好好练编程，原理不懂的就好好看书理解原理。动手做是第一位，然后不断改一些模型的参数，看效果变化，再看背后的数学推导，理解原因，这样比先看一大堆数学公式的推导，把自己绕的晕晕乎乎在开始写代码要好得多。\n推荐书目\n数学类：\n高等数学（同济第七版）：没错我说的就是考研的那本参考书，真心不错，难易适中，配合相应的视频或者国外的一些基础课程的视频看，高数理解极限，导数，微分，积分就差不多了\n高等数学（北大第三版）：线性代数的书我看的不多，原来上学的时候学的是高等数学，不过不要紧，看前五章就行了。配合相应的视频，掌握矩阵，行列式相关知识即可。\n概率论：这个没有特别推荐的，因为学的并是不很好，所以不做推荐误人子弟。大家不管看什么书，只要掌握关键知识就行了。不能到时候问个贝叶斯你都不知道咋推吧 = =！\n信息论：忘记是哪个出版社了的，很薄的一本，讲的非常不错。里面关于信息的度量，熵的理解，马尔科夫过程都讲的不错（现在公司里没有，我回去找找再补上来）。掌握这个知识，那么对于你理解交叉熵，相对熵这一大堆名字看起来差不多但是又容易弄混的东东还是不错的。起码你知道了为啥很多机器学习算法喜欢用交叉熵来做cost function~\n编程类：\n笨方法学Python（Learn Python the Hard Way）这本书对于完全没有接触过Python，或者说完全没有接触过编程的人来说非常适合。虽然很多人说Python这么简单，一天/周/月就学会了，但是每个人的基础是不一样的，所以不要认为自己一天没学会就很认为自己很蠢，你应该想这样说的人很坏！不管怎么样，这是一本真正的从零开始学Python的书\n利用Python进行数据分析：这本书是Python的pandas这个包的详细说明版。学习这个可以掌握一些pandas的基本命令。然而这不是重点，因为pandas出来大量数据实在太慢了，还可能会崩溃（不知道现在有没有改善 - -！）重点是，通过学习这本书，对数据的操作有点感觉，熟悉基本的数据操作流程，里面所有的操作都可以用原生python来替代，不需要用到pandas这个包。找感觉，非常重要。\nPython参考手册：这本书只是作为一个工具书，当你遇到不会的时候翻翻书，巩固一下（当然，事实可能是直接去google了），此类书不用全部从头到尾刷完，查漏补缺即可（电子书就行）\n算法类：\nDeep Learning with Python：别看这又是一本英文书，但其实非常简单易读。我之前在工作一年的时候写了篇总结（【原】数据分析/数据挖掘/机器学习必读书目），也推荐了这本书。这本书其实主要是一个demo例子的集合，用keras写的，没有什么深度，主要是消除你对深度学习的畏难情绪，可以开始上手做，对整体能够做的事做一些宏观的展现。可以说，这本书是demo侠的最爱啦！\nDeep Learning：中文有翻译版的出来了，不过我其实不太想放在这里，因为这本书其实很偏理论。有些章节讲的是真不错，有些地方你完了又会觉得，这是啥？这玩意有啥用？会把新手绕来绕去的。大家就先买一本镇场子，有不懂的翻翻看，看不懂的就google，直接看论文，看别人总结的不错的博客，等等。总之只要你能把不懂的弄懂就行了。\n懂原理侠：很不错，你的经验值已经提升了不少了。然而还不能开始打怪，毕竟没有那个怪物可以直接被喷死的。你缺少工具。那么这个阶段，就需要多多加强编程能力。先找一个框架下下来，阅读源码，什么？你说你不会阅读源码，没关系，网上一大堆阅读源码的经验。当然，这些经验的基础无一例外都是：多读多写。在此基础上再找trick。下好框架的源码后，改动一些代码在运行，debug一下，再不断的找原因，看看每个api是怎么写的，自己试着写一写。多写多练，死磕coding三十年，你一定会有收获的。\n懂原理+能改模型细节侠：看论文看论文看论文！读源码读源码读源码！这里的读源码不仅仅局限于读一个框架的源码了，可以多看看其他优秀的框架，对于同一个层，同一个功能的实现机制，多比较多思考多总结多写。时间长了，肯定会有收获的。看论文是为了直接获得原作者的思想，避免了从博客解读论文里获得二手思想，毕竟每个人的理解都不一样，而且也不一定对，自己先看一遍，再看看别的理解，多和大牛讨论，思路就开阔了。\n超大数据操控侠：这个阶段我也还在摸索，给不了太多建议，只能给出目前总结的一点点经验：尽量扩大数据，看如何更快更好的处理。更快--采用分布式机制应该如何训练？模型并行还是数据并行？多机多卡之间如何减少机器之间的网络延迟和IO时间等等都是要思考的问题。更好--如何保证在提升速度的同时尽量减少精度的损失？如何改动可以提高模型的准确率、mAP等，也都是值得思考的问题。\n模型/框架架构师：抱歉，我不懂，不写了。\n总结：\n其实大家从我上面的推荐来看，打好基础是非常重要的，后续都是不断的多读优秀的论文/框架，多比较/实践和debug，就能一点点进步。打基础的阶段一定不能浮躁。扎扎实实把基础打好，后面会少走很多弯路。不要跟风盲目崇拜，经典永远不会过时，自己多看书/视频/优秀的博客，比无脑跟风要强得多。最后，我之所以今天这么生气，是因为这个行业目前太浮躁了，很多人太浮夸，误人子弟，有人说真话还被人骂喷子，真是气死我了！大家一定要擦亮双眼，多靠自己多努力。\n不好意思强行鸡汤了一波。本来去年打算写一个机器学习系列，但是因为工作和身体的原因写了三篇就没有更了。今年上半年做了一个大项目又累得要死，下半年才刚刚缓口气，所以之前欠的后续一定会继续更。为了不让大家盲目崇拜，我决定写一个深度学习系列，每周固定一篇，大概三个月完结。教小白如何入门。并且完！全！免！费！！不是简单的写写网上都有的demo和调参。拒绝demo侠从我做起！有不懂的欢迎大家在我的文章下留言，我看到了会尽量回复的。这个系列主要会采取PaddlaPaddle这个深度学习框架，同时会对比keras，tensorflow和mxnet这三个框架的优劣（因为我只用过这四个，写tensorflow的人太多了，paddlepaddle我目前用的还不错，就决定从这个入手），所有代码会放在github上（链接：https://github.com/huxiaoman7/PaddlePaddle_code），欢迎大家提issue和star。目前只写了第一篇（【深度学习系列】PaddlePaddle之手写数字识别），后面会有更深入的讲解和代码。目前做了个简单的大纲，大家如果有感兴趣的方向可以给我留言，我会参考加进去的~\n最后一句，低调做人，好好学习，大家下期再会^_^!\n----2018.12.3更新\n发现一个不错的评论，加到原文来！\n现在越来越发现作为一个算法工程师，工程能力也非常重要，如果只懂理论，不会实现，也没有什么竞争力。\nps:为了方便大家及时看到我的更新，我搞了一个公众号，以后文章会同步发布与公众号和博客园，这样大家就能及时收到通知啦，有不懂的问题也可以在公众号留言，这样我能够及时看到并回复。（公众号刚开始做，做的比较粗糙，里面还没有东西 = =，后期会慢慢完善~~）\n可以通过扫下面的二维码或者直接搜公众号：CharlotteDataMining 就可以了，谢谢关注^_^\n更新：我开课啦~ 点击链接：三个月教你从零入门人工智能！！| 深度学习精华实践课程 （免费）  就可以看到我的课程啦，用Tensorflow讲解的，每天更新一课时~"}
{"content2":"我们集中在伯克利校区里的一个会议室里，局域网的设备散落在我们周围。桌面上堆满了电脑和披萨，屏幕上投影的是一场《星际争霸》的比赛。Oriol Vinyals，一位计算机科学的博士生，正指挥着他的人族部队和虫族部队进行着殊死的战斗。\nOriol非常强——他曾经参加过WCG，是西班牙赛区的第一名，欧洲赛区的16强。但现在他的处境岌岌可危：他的机器人能够击退虫族的飞龙，但是却没法面面俱到。虫族玩家很狡猾，遇到凶猛的火力就撤退，但并不跑远，而是专捡软柿子捏，保持着对对手的压力。\nOriol犯了一个错误。他的机器人的走位出了一点问题，虽然只有几秒钟。但这就足够了。飞龙立刻做出了反应，由人族防守的间隙鱼贯而入直奔对方脆弱的农民。当Oriol指挥着他的机器人把飞龙赶走时，他的农民已经全部阵亡，经济遭受沉重打击。\nOriol对虫族基地发动了绝望的最后一击，试图在飞龙得到补充之前突破对方的防线，但这已经太迟了。他的大脚机器人一个接一个的被虫族消灭掉。当新一波飞龙从虫卵中孵化出来的时候，他只能认输——向刚刚击败了他的计算机人工智能认输。\n伯克利“主宰”的飞龙蜂拥而至\n房间里洋溢着庆祝的喜悦，Oriol也不例外。打败他的是一个计算机人工智能程序，名叫伯克利“主宰”，由我们小组花了几个月的时间开发而成。“主宰”是我们参加“2010星际争霸人工智能挑战赛”的作品。在数十场比赛后，它终于第一次击败了我们的人类星际高手。\n我们觉得很光荣，自尊心得到极大满足（不是John Henry那种），但我们并没有时间来细细品味这一切。距离提交代码的最后期限只剩三天了，我们还有很多清理和调试的工作。Dan Klein教授，我们的顾问、将军、教练和动力，微笑着转向了白板。他划掉了我们还需要测试的二十个场景之一。\n“好了，”他说。“我们可以打败大脚机器人的战术了。下面来测哪一个？”\n下面就是我们如何创造了伯克利“主宰”程序以及它背后的技术的故事。\n迫不及待的想要改变未来\n《星际争霸》是史上最流行的电子游戏，来自一家伟大的公司一款伟大的作品。它对玩家的技巧有很高的要求，是职业游戏联赛的主要项目。在韩国，《星际争霸》非常流行，其职业选手的合同收入达六位数，游戏比赛会在国家电视台上进行直播。\n这对于人工智能来说也恰好是一个非常具有挑战性的舞台，一个成功的《星际争霸》人工智能必须解决很多困难的问题。在伯克利教授人工智能入门课程的Dan说：“我能一一列举这门课向你展示的所有概念以及它们分别在《星际争霸》以及我们的程序中的应用。”\n《星际争霸》发布于1998年，在视频游戏领域看来这差不多是八百年以前了。多年以来，游戏的开发商暴雪一直在更新它，使得《星际争霸》成为史上制作最精良也是最平衡的即时战略游戏之一。游戏有三个种族：拥有我们熟悉的坦克和星舰人族（Terran），生物群落形态的虫族（Zerg）和科技发达、兵种强大而昂贵的神族（Protoss）。每个种族都有不同的单位和游戏理念，但没有任何一个种族或是兵种组合是无敌的。玩家的技巧、创造力和随机应变的能力都能决定胜负。\n精细复杂的设计使得《星际争霸》成为人工智能研究的理想环境。在即时战略游戏中，所有事件都是实时的，玩家的指令会被立即执行。玩家需要收集资源来生产单位进行战斗。地图被“战争迷雾”所笼罩，敌方的单位和建筑都只有在己方的单位或建筑物的视线之内才可见。玩家需要采集并分配资源，生产单位，移动单位进行战斗，侦查、分析并对敌方的行动作出响应，这些都是实时的。对于计算机来说这些都是难题。\nDan有时会把《星际争霸》和那些曾经驱动了人工智能研究前进的游戏作比较。“国际象棋难在你必须能看的远，围棋难在每一步棋的可能性太多，而扑克则难在不确定性。”他说。“但在《星际争霸》中你每时每刻都面临着所有这些问题，而你能够用来计算的时间却非常少。”\n人类高手通过培训和练习来掌握丰富的技巧和知识以解决这些问题。我们不可能简单的把这些人类掌握的知识移植到人工智能中去，因为它需要能够主动的判断游戏的形势并决定未来的行动。创建一个能够与人类选手匹敌的《星际争霸》人工智能需要把现在计算机的能力所及向前推进一大步，并且可能在游戏以外的领域得到应用。\n“星际争霸人工智能挑战赛”的目的是以《星际争霸》为环境推进人工智能的研究。人工智能研究人员在过去也曾使用过即时战略游戏，但都受限于可用的技术。开源的游戏有很多缺陷并且缺乏测试，而像《星际争霸》这样的商业游戏则是封闭的。\n在2009年初发布的母巢之战API（BWAPI）改变了这一切。BWAPI是由一群狂热的游戏爱好者开发的开源工具，使程序能够直接访问游戏。加州圣克鲁斯智。于是他着手举办了这项星际争霸人工智能比赛，以期能够激起大家的兴趣并开启研究的进程。\n比赛的消息正式宣布于2009年11月，并迅速在游戏类的网站和博客中传播开来：第一届“星际争霸人工智能挑战赛”将于2010年10月在斯坦福大学的“2010人工智能与互动数字娱乐大会”上举行。\n课程：星际争霸 101\n当Dan和他实验室的学生听说这项赛事时，他们立即兴奋了。“当我听说‘有API能够访问《星际争霸》’时，”他说，“我就知道有好玩的东西可以研究了，科学研究和课堂项目都有了很多新的空间。”\n第一个挑战是定义问题域：一个星际玩家需要完成哪些操作，如何把这些操作转化成人工智能的行为？\n此时小组的成员包括Dan和一些他的博士生，以及伯克利人工智能与机器人研究实验室的几个研究生（包括本文的作者）。这群人熟悉人工智能但却不会玩《星际争霸》，而伯克利的计算机科学的本科生中则有许多喜爱《星际争霸》的玩家。\n为了填补这一鸿沟，Dan开设了一门介绍人工智能概念的课程，并在课上为比赛设计和创建我们的程序。这门课给了我们分享知识并探索如何将游戏中的问题转化为算法可以解决的具体问题的机会。这也正是展现和解释人工智能概念的非常好的方法。\n这门课一经宣布便受到了热烈的回应，并且取得了巨大的成功。我们花了一学期的学习《星际争霸》和人工智能，探索算法和框架，并尽可能多的尝试了很多不同的东西。我们从中获得的宝贵的经验对我们的人工智能的进化非常有帮助。“可以肯定地说，没有这门课我们不可能做的这么好。”丹赛后说。\n在这门课的最后，我们已经把游戏对人工智能的要求归结为三个方面的问题。首先，程序需要采集并管理资源并在何时建造什么建筑或单位。用游戏术语来说，这叫做“宏观管理”，或者叫“大局观”。它本质上是一个规划与优化的问题。\n接下来的任务是“微观管理”。一旦我们有了一支部队，程序需要选择目标并且移动自己的单位。这是一个复杂的多对象控制问题。最后，程序需要管理信息，对敌人进行侦查并且有针对性的调整自己的策略。这需要单位控制和高层次规划的多方面知识。\n“你需要采集更多的vespene气体”\n我们的人工智能要做的的第一件事就是收集资源并生产建筑和单位。这既非常容易又十分困难。开局策略，也就是开局时建筑物和单位的建造顺序，在星际社区中已经是被详细研究过的话题了。和象棋一样，不同的开局意味着玩家将会在游戏中期采取不同的策略。早期低级兵种的rush、空中骚扰或是中后期暴兵都会有不同的开局。\n这些开局都是经过高度优化过的，用若干脚本或是有限自动机的方式简单的把它们直接硬编码到程序的“宏观规划”中去是十分诱人的想法。这正是《星际争霸》的内置AI以及很多其他游戏AI的工作方式：直接将人类专家的知识编码为一连串的动作或预定义的转型。生产坦克，并在对方生产空军单位的时候生产防空单位。如果敌人建造隐形单位，则生产探测器，等等。我们的人工智能的早期版本的确是采取了这种策略，用人族暴坦克。或许我们的第一个教训就是这种方法不靠谱。\n我们这一课是在尴尬中度过的。我们的终极目标和深蓝挑战卡斯帕罗夫是类似的，而《星际争霸》正是一块人类选手表现的很优秀而人工智能则很糟糕的领域。因此，检验和改进人工智能的唯一办法就是让它和人类玩家对战。带着已经能够打败内置AI的自豪，我们在课堂上让我们的人工智能和John Blitzer玩一局。他是Dan的一名博士后，经常在iCCup上玩天阶。\n这是一场灾难。\nBlitzer使用的战术叫“偷气矿”，就是让一个农民到我们的人工智能的家里把气矿修了。这下程序只能在将对手的气矿建筑打掉并建造了自己的气矿建筑之后才能开始采集气体。我们完全没有预料到这种战术，而我们的AI，由于战术中的建筑物非常依赖于气体资源，完全陷入了一片混乱。当人工智能还在等待气体来建造坦克时，Blitzer的大军就已经兵临城下了。我们曾认为Blitzer也许会用更好的战术击败人工智能的坦克大军，但我们从未想过比赛也许根本进行不到那个份上。\n“很抱歉，比赛和原计划略有不同，”Blitzer在赛后的一封邮件中分析了比赛。实际情况远不是这样。它给我们上了宝贵的一课：人工智能必须足够健壮，能够容忍干扰，否则它就会被卡住。\n想让AI变得强壮，一种办法是让人类程序员预见到所有可能性并将应对措施教给它。完全不这么做很难，但是在《星际争霸》这么复杂的系统中，事先预料到所有的场景是不可能的。这也是为什么内置的AI很容易被打败的原因：只要找到了它的缺陷就可以反复利用。\n下面的视频就是一个这样的例子。视频的主角是“神奇农民”Sparky。它是游戏早期我们的人工智能派出的一个探路农民，实际上也是我们这支队伍的吉祥物。“偷气矿”战术让我们对非常规早期战术重视起来，并且吸收了一部分到我们的人工智能中去。\n在录像中，Sparky攻击了内置AI的农民。内置AI的程序知道应该反击，但却不知道该何时停止反击。Sparky最后偷了对方的气矿，但它也可以很轻易的将对方的农民拖入无限的追击之中，这样内置AI就完全不会去采集资源和建造单位了。Sparky的这种行为对于内置AI的打击是毁灭性的，以至于在测试中我们必须禁止他这么干，否则游戏就无法进行到后续的环节。\nSparky在兜圈子\n应对Sparky的小把戏只需要一些人工干预，但管理资源和规划建造顺序则需要更灵活的策略。最终我们的建造规划器和资源分配器的结构和操作系统的调度器很相似。我们把动作和进程相对应。不同的动作需要向“主宰”的中央控制器请求不同的资源，比如水晶、气体或是单位。中央控制器负责根据优先级来满足不同动作的需求。这种体系结构给了我们很大的灵活性。\nDavid Hall是Dan小组的博士生，也是小组长之一。建造规划器——“我们基本上考虑到了所有当前能做的事情，然后选那个最好的”，他说。我们不会使用固定的建筑和单位建造顺序，而是给出最需要的建筑以及单位的组合，让规划器自己去尽可能的达到它。\n这样产生的建造顺序比固定顺序要强壮的多。人工智能不会再被无法预料到的敌方行动卡住。当对手成功的偷到了气矿的时候，我们的AI会去开另一片有气的新矿或是在恢复采气之前建造那些不需要气体的单位。\n最终，规划器产生的建造顺序和我们在网上能够看到的高手开局非常接近，而且它具有变通的能力。要赢得《星际争霸》比赛，这种灵活性和强壮性是必不可少的，无论是对于人类选手还是对于人工智能而言。开发过程的主题就是一次又一次在人工智能中将游戏各方面的人工行为的替换为能够推理和决策的工具。\n“我成为主宰之时，就是虫群统御天下之日”\n到2010年夏天，建筑部分的基础设施终于完工，到了研究如何使用这些建筑的时候了。人工智能要能在战斗中控制己方的部队，选择进攻的目标并按照战术移动己方的单位。但现在我们腹背受敌：挑战赛的最后提交期限是九月，我们快没时间了。\n我们必须有些节制。Dan的另一位博士生也是小组长David Burkett说：“我们发现控制混合部队有困难，因此生产多兵种的部队得不偿失。所以我们开始自问，哪个兵种的性价比最高？”\n我们把注意力集中在了虫族的飞龙：既能对空也能对地的高速类龙生物。他们的机动性无与伦比，我们认为用计算机控制他们特别合适。飞龙价廉物美，但在比赛中很少看到人类选手大量使用他们，因为只有叠起来的飞龙才比较好操作，但是这样又容易被面攻击（区域伤害，而非单体伤害）大量杀伤。而计算机则不会有这样的局限。\n另外，我们一直想设计一个虫族AI，并且把它命名为“主宰”，即《星际争霸》同名小说中控制虫群的大脑。Dan提出了唯一可能的反对意见：“你知道，从申请研究经费的角度来说，如果我们的AI控制的是人类力量而不是一群不断流口水的外星异形是不是更好一些……”\n理论上来说，电脑没有人类的局限性，应该能够同时控制多个单位。事实上，一个普遍的误解认为，《星际争霸》的实时性意味着在游戏中反应速度最重要的。虽然速度的确很有用而且很重要，但是最重要的是别干蠢事。正确的决策才是带兵之道。即使是让飞龙在战场上机动也并不是一件简单的事情。飞龙应该去攻击他们的目标但同时避开其他的敌人；它们应该集中火力，但在遇到有面攻击能力的敌方单位时又可以迅速的散开。最后，所有的飞龙的行为都应该同步。\n，飞龙最终会飞向合力的方向，这样的的策略简单而强健。像飞龙这样的空中单位即不会受到地形的限制也不会相互碰撞让我们的工作轻松了很多。\n势场控制非常强大。例如，只要在开火之后忽略引力就能实现“打了就跑”的战术，斥力会让飞龙自动在攻击间隙离开对方的武器射程。引力可以让编队中的单位集火，而一旦面杀伤的威胁出现时，斥力就使它们四散躲避。\n下面的视频展示了飞龙在遭遇白球（强力防空单位）时的这种行为。势场使得“我们期望的行为自然的产生了”，David Burkett说。\n。强斥力场可以保证飞龙的安全但是会妨碍它们集火，不同的敌人需要不同的应对方式，也就意味着不同的力场参数。但是人工遍历并调整所有参数花费的时间太多了。\n因此，与之相反，我们希望“主宰”能在战斗中学习。\n在北欧神话中，武士们的灵魂在一个叫“瓦尔哈拉”的天堂中进行着永恒的战斗。我们使用《星际争霸》的地图编辑器为“主宰”创造了一个瓦尔哈拉，它能够在其中自动的重复不同的战斗场景。通过瓦尔哈拉中的反复试炼和调整，我们的人工智能找到了对付各种敌人的最佳参数组合。\n下面的视频演示的是我们的飞龙和电兵的战斗。高阶圣堂武士（电兵）在人类比赛中是一种对抗飞龙的标准兵种，它的闪电能够覆盖一大片区域，屠杀聚集在一起的飞龙。在适当的斥力场作用下，飞龙在躲避心灵风暴时能够轻松的立即散开并重新集结发动攻击。\n飞龙的控制逻辑的最后一部分是智能的选择目标。在攻击指定目标时势场工作的很好，但如何选择正确的目标让我们很头疼。我们的飞龙也不能幸免于硬编码策略所带来的危害：早期的目标策略是基于简单的威胁等级指定的，在一般情况下表现尚可，但会周期性的犯傻。飞龙有时会在快要摧毁掉一个人族基地的时候突然转而飞跃半张地图去攻击一个枪兵，或是勇敢的在成排的防空塔阵地前自杀。\n最后，解决办法是赋予AI预测自己行为后果的能力。在游戏中，单位之间互相攻击的伤害是已知的，因此我们可以大概算出一群飞龙在消灭一个目标时会承受多少伤害以及花费多少时间。按照建造单位所消耗的资源为目标和飞龙都打分之后，人工智能就能知道目标单位的价值，能够判定最有价值攻击的单位以及攻击的优先级顺序。这样就实现了目标选择的智能化。\n下面就是“主宰”战胜Oriol的比赛的一段视频。其中的飞龙群已经集成了最后的改进，在进攻人族基地的时候能够在攻击最有价值的目标的同时最大限度的躲开敌方的防御。\n最后的结果是一群飞龙聪明的和对方交火，消灭高价值的目标并避免无意义的损失，直到它具有了压倒性的优势。对手的基地和部队被一点一点的吃掉了。\n拨开战争迷雾\n夏天过了一半的时候，灵活的建造规划器和经过瓦尔哈拉试炼的智能飞龙群已经使伯克利的“主宰”变成了一个可怕的对手。但是我们在测试中又发现了AI的一个弱点，这使我们懂得了获取和分析敌方情报的极端重要性。\n飞龙是二级兵种，这意味着需要一些建筑以及升级才能制造这个兵种。在这之前，我们的人工智能是非常脆弱的，这一点在测试中一览无余。一旦飞龙群达到一定规模，它们几乎是不可阻挡的，但对手能够通过前期的进攻来阻止飞龙成型并取得胜利。\n我们的第一反应是让建造规划器在游戏初期建造更多的防御建筑和廉价的地面部队。这的确可以防范前期对手的进攻，但也将生产飞龙的时间推后了。这反映出了一个普遍存在的经济和军事的平衡问题。农民补的太多会使前期很脆弱，但农民不足则不足以支撑一只庞大的部队。把资源分配给防御建筑就意味着进攻兵力的减少。\n这个平衡问题答案在于对手的部队数量和意图：对手前期进攻，我们就需要更多的防御；对手发展经济，我们就全力出飞龙。\n如果我们的人工智能可以看到对手正在生产什么，那么建造规划器就能相应的调整建造顺序。由于战争迷雾限制了选手的视野，收集对手信息就需要在敌方火力的威胁下不断的进行侦查。在游戏的早期，这意味着用一个农民或是地面单位溜到对方家里，并存活尽量多的时间以看到对方都造了些什么。\n对于虫族，游戏中期和后期的侦查大部分来自于良好运用的领主。领主飞行缓慢，但视野开阔，还能够提供人口。把领主分散到地图各处可以帮助AI更好的选择目标并且进行宏观规划，但是失去领主也意味着视野缩小，而且有可能卡人口。侦查的好处应该和失去领主的风险相平衡，人工智能也需要保护领主不被消灭。\n我们一开始控制领主的行动和侦查的策略很笨。《星际争霸》的地面单位寻路算法很糟糕，这已经困扰了所有玩家十多年了。随着开发的推进，Dan不想再被撞了南墙也不回头或是不断绕圈的部队郁闷了，于是我们决定开发我们自己的寻路算法。\n能成功的从A点走到B点已经很好了，但是真正的挑战在于在寻路中感知敌方的威胁。“主宰”会将所有它见过的敌方单位的最后位置保存下来并不断更新。由于《星际争霸》中所有单位的攻速和射程都是已知的，AI可以利用这些知识和敌人的位置创建一张威胁地图。它能够计算每一种单位在一片区域中的威胁等级。将威胁地图和寻路算法结合起来，把某个区域的威胁值作为穿越该区域的成本加入计算。高危的捷径就会被安全的远路所替代。我们需要花一点心思让改动过的算法足够快，但是搞定之后它的价值就体现出来了。尽管最初它只是被在探路农民上，我们发现它完全可以用在所有的地方。\n带有威胁感知的寻路算法带来了一系列的改进。飞龙可以在对手防守的缝隙中发动进攻。在游戏早期，地面单位能够溜进对方基地、避开对方的战斗单位长期存活。新的寻路算法也使得领主能够安全的散布到地图的各个角落。我们的人工智能的宏观视野因此得到很大的改善。对敌方力量了解的更多，建造规划器就能更好的做出应对、建造防御并在经济和军事间做出平衡。\n下面的视频就是一个这样的例子。其中我们的领主飞进了对方的基地并发现神族的飞机场是亮的。这使“主宰”在对手的空军来到之前就造好了防空建筑。\n这种寻路算法还可以应用到寻路以外的地方。除了扩大视野之外，领主的另一项重要任务是反隐。但是领主的速度跟不上飞龙，所以两者无法协同前进。于是AI会根据各个领主距离飞龙群的“安全距离”动态的使用不同的领主为飞龙提供反隐保护。“安全距离”最近的领主会被分派反隐的任务。随着飞龙在地图上移动，不同的领主会被分派这样的任务，这样就能够在保证一定的地图视野的同时为飞龙提供最大限度的反隐。\n在这段视频中你将看到一个领主被召唤来帮助飞龙反隐。\n威胁感知、寻路和侦查形成了一个良性循环。威胁地图使侦察单位活的更久，而更好的侦查又能更好的更新威胁地图。AI的视野越好，它越能更好的侦查，反之亦然。最后这个特性使得伯克利“主宰”更加强大，他能生产单位，指挥它们作战并智能的应对敌人的行动。\n让Oriol担任“教练”给了我们巨大的帮助。Oriol在进入学术界之前是《星际争霸》的职业级玩家，他在队伍中的作用既是教练，也是陪练，还是百科全书。\n在完成了所有算法并和人类高玩对抗了之后，我们的人工智能在最后的几个星期进步飞快，最终在最后期限前几天第一次击败了Oriol。我们在最后时刻提交了这个人工智能，时间已是最后一天的午夜。现在能做的只剩下等待了。\n“你的部队接敌了”\n在2010年9月15日比赛提交结束之后，实际比赛是九月底在线下进行的，最后结果会在10月13日的人工智能和互动数字娱乐（AIIDE）大会上公布。\n全部比赛分为四个类型。第一类和第二类的主题是小股部队的局部对抗。第三类是简单化了的游戏，没有战争迷雾，能够生产的单位也有限制。第四类是正常游戏。我们只参加了第四类比赛，规则是BO5双败赛。最终的结果是，我们的伯克利“主宰”赢得了所有的比赛，小分22战21胜，在17只参赛队中排名第一。\n比赛的录像既让人激动又让人担心，有些比赛非常非常接近。所有有上佳表现的人工智能都非常复杂。它们都能生产部队、骚扰、在遇到强大敌人时撤退并采用克制敌人的策略。Sparky能够骚扰到一些对手，但更多的对手防备得当。最强的对手使Sparky的骚扰完全失效，并且对偷气矿的战术免疫。有些对手也有它们的Sparky，也会使用偷气矿战术。\n比赛中也有许多惊喜。亚军程序Krasi0也非常强大，它的一种行为出乎了我们的意料，因为我们在人类比赛中没有见过。（译者注：不是吧，没见过scv的各种修？）人族农民有修理机械单位和建筑的能力，但是这中技能很少在战斗中使用，因为其需要的操作和带来的收益相比不值当。\nKrasi0把几个农民编为一个修理小分队专门照顾它的坦克和大脚机器人，有时候修理的速度会超过飞龙攻击的强度。我们的人工智能没有考虑到这一点，因此经常会做出错误的判断。我们和它的比赛都变成了残酷的消耗战，“主宰”依靠限制对方的扩张最后通过资源的优势取得了胜利。\n最后，AI的胜利让我们欢欣鼓舞，不止是因为它赢了，更因为它取得胜利的方式。“主宰”在比赛中的行为和优秀的人类选手很相似，不是因为我们把这些行为硬编码到了它的身体里，而是因为它有能力去思考和判断，而这些行动就是它判断的结果。建造规划器产生的建筑和单位顺序和已知的建筑顺序非常相像，当领主察觉到对手的进攻时，防御建筑也能够自然平滑的插入到建造序列当中。\n最有趣的是，这种击败了Oriol和Krasi0的“围堵-骚扰-扩张”战术完全是AI自创的。在预测代码的帮助下，我们的人工智能知道不应该进攻重兵防守的敌人基地，于是它就在其附近徘徊寻找战机。这一策略成功的阻止了对手的扩张，因为飞龙的高机动性使得对手无法同时防守多个基地。与此同时，“主宰”则飞快的占领了地图上所有其他的资源点，然后暴出一只无法阻挡的大军。所有这一切自然的产生于AI的各层决策，并非有人教给了它。\n“好了，下一步是什么？”\n最近的目标很明确：人工智能挑战赛会在下一届AIIDE大会上继续举行，所有的队伍都会仔细观看比赛录像并用各种方法改进他们的AI。\n我们也将这样做。想要了解更多的信息，请登录我们的网站overmind.cs.berkeley.edu，那里有最新的消息和许多视频。准备充分了之后，我们想让这个人工智能在网上和人类选手对战。我们也会将“主宰”背后的各种算法写成论文。\n尽管还有许多困难，这次比赛让我们感觉到未来的《星际争霸》人工智能还可以变得更强大。更高级的规划策略，多兵种的配合以及更好的信息管理都是有待解决的问题。\n伯克利“主宰”在对抗Oriol时取得进步让我们欣慰。头几场比赛Oriol轻松的取得了胜利。无论是前期rush还是后期大兵团作战，无论是用地面单位还是用空军，甚至是用农民，我们允许他使出任何战术、任意兵种组合，只要能够击败我们的AI。当提交截止日期临近之时，“主宰”的水平已经让他不得不认真应对了。\n“有一些我们想跑却没法实现的测试”，Dan说，“因为我们没法进行到游戏的某个阶段，要么他在前期就能取胜，要么飞龙会赢。”Dan认为在几年之内就会出现能够与最高水平的人类选手相抗衡的人工智能。虽然还有许多工作要做，但这一切看起来只是时间问题。\n来自：http://article.yeeyan.org/view/48380/167670"}
{"content2":"今天下午，知名创业导师李开复先生来访公司！！！作为初入职场的童鞋有幸见到了科技大佬的真容（激动地这辈子都不想洗眼睛了）！\n开复先生与大家进行了2个小时的交流，我对开复老师最深的印象是大气不失随和，言语睿智深刻。下面我对其中一些印象深刻的问题进行归纳总结一下。\n关于人工智能\n问题：\n人工智能很强大，永生技术大大延长人的寿命的情况下，未来人类社会会是什么形态？\n我有一个女儿，现在90多天大，为了她能更好的应对未来的时代，从现在到接下来的20年里，我能够或者应该做些什么？\n总结：\n你的女儿还很小，还有很长的路要走。现在大家问我更多的其实是：我的孩子大学该选什么专业？\n这些问题都很重要，随着AI 的发展，有些专业会慢慢消失。现在要做的就是教孩子什么要做深度的学习、思考。还有就是要跨领域的事情，AI是做不到跨领域的，只有人类可以。另一个就是学习文科，艺术、音乐、美好这些恰恰也是机器做不到的。\n我们应该帮每一个孩子找到自己的兴趣，随着他的兴趣，帮他找到方向，用他们自己的新一代的认知去发展。\nps: 人工智能的强大毋庸置疑，但是开复也指出人工智能的缺点：单领域、没有感情色彩。所以谈到将来职业发展，会有越来越多的职业逐渐被人工智能取代，开复明确表示driver这个职业10年之内将会被机器取代。但基于人工智能的缺点，跨领域的人才肯定会站在塔尖。同时文科类的有些职业也是人工智能所代替不了的，比如哲学、艺术类。\n关于出国\n问题：\n对于没什么机会出国的我们该怎么学习国外的知识？\n总结：\n\"首先当然是学会FQ\"，开复老师幽默地说，讲到FQ的话，我也推荐两个方式吧：修改host (免费,当时是window用过,不知道现在改了文件没);现在用的 (现在大概7元/月，急速稳定版) 或者这个 (两个可以配合用)，有不懂的可以问下我，个人还是推荐付费版 -- 这是对自己最好的投资。\n开复老师还是非常建议能出国学习下的~，他强调出国并不是为了拿个学位证书，还是培养一个creating thinking。这点上开复老师就非常赞赏知乎(一个问题能看到好多不同方向的见解)。\n当问到国内与国外公司工作氛围的时候，开复老师说国外往往是多个leader共同进行决策，国内往往顶层只有一个leader说了算，所以国外的公司创新能力更强，国内的公司执行能力更强。当然国内公司的加班强度基本是让国外畏惧的~。\n关于创业者的品质\n问题：\n对于刚入职场的新人，假如将来要创业，该培养什么品质？\n总结：\n开复老师说各个投资人的看着点不同，拿创新工场来说，看重创业者的执行力，人格魅力，管理能力等，更看重能不能在该领域上能让‘我’学到很多东西，比如开复老师对点我达独有的无分区，压力平衡体系及数据的积累与分析能力表现出了极大的兴趣。\n最后作为点我星人，祝福公司，用开复老师的话结尾吧。\n点我达积累了一个有价值的数据库。\n点我达所拥有的是大量的、源源不断的、别人没有的大数据，这是一个有价值的数据库，点我达或许正沿着成为下一个BAT级别公司的方向前行。这让我感到振奋！\n我相信点我达未来一定能成为一家超级独角兽。\n创新工场投资了300多家公司，这是我第一次用这种论述描述一家公司——点我达是一家我认为可以成为超级独角兽的公司。 一家坚韧的公司是值得尊重和期待的！"}
{"content2":"0 写在前面\n本文记录了两个月以来8场学科前沿技术讲座的课程总结与感悟。\n学院请到了很多厉害的教授以及企业的专家和学者，讲座的方向多以大数据和人工智能为主，作为计算机科学专业的学生，时刻保持对行业发展前沿领域的关注，我认为是十分必要的。\n1 课程感悟\n经过近两个月的讲座课程的学习，我对计算机科学的学术前沿内容有了更多、更深入的理解和感悟。\n讲座的内容很充实，形式也十分丰富，讲座的主题也涵盖了包括但不限于数据库原理、大数据、人工智能等等。我认为，在本科三年级的这个阶段，在核心专业课基础知识-包括数据结构与算法、计算机组成原理、编译原理、操作系统、面向对象等-已经熟练掌握得十分牢固的前提下，应该把目光放得长远。\n在邹欣老师的《构建之法》一书的前言中有所提到：\n学校想培养什么样的学生，是世界一流，中国一流，还是本省二流？有什么样的期望，就要有什么样的课程设计。\n作为北航的一名本科生，应该将成为国际一流人才作为自己的培养目标，而要成为这样的人才，就需要用国际一流的标准去要求自己。\n所以，能够在这个本科生涯即将告一段落、即将步入社会的重要关键节点，学院为我们开设这样一门课程，并请到了李波老师、马殿富老师、邹欣老师、马帅老师等等为我们深入地剖析当前计算机科学与技术的前沿知识，是我在这一学期的一大幸运。\n在众多精彩的讲座中，最吸引我的主题，非人工智能相关的话题莫属。一方面，是今年来，人工智能浪潮来袭，使人工智能技术再一次到达顶峰，与人工智能有关的内容成为炙手可热的话题。另一方面，也是我本人，对于人工智能领域的前沿技术的热爱，让我对老师们精彩的演讲产生了浓厚的兴趣。因此，若要在这短短五千字的报告中，用简洁凝练的语言，来表达我的感悟的话，那么我最想表达的内容，必定是我对于人工智能前沿技术的体会与心得。\n人工智能在历史上曾经历三起三落，现在正是人工智能技术走上坡路的时期，这一点是不难解释的，那就是数据量的不断增长、数据硬件存储能力的扩增以及数据计算能力的提升与计算成本的降低，为机器学习的算法实现提供了无限可能。\n另外，随着理论研究的不断深入，机器学习在传统领域的基础上，又扩展出了多个分支—强化学习、深度学习、多任务学习，等等。应用领域也得以扩展，从数据挖掘、图像检测、模式识别到自然语言处理等等，可以说机器学习已经遍及到人们生活的方方面面。\n讲座中所学到的内容与知识都是静态的，而只有将这些知识，实际运用起来才能让其变得生动灵活。正如马尔科姆•格拉德威尔在他的《逆转：弱者如何找到优势，反败为胜》一书中所提到的，如果一直停留在理论层面上去分析问题的话，那么有利局势的天平将很难朝你的一侧倾斜。因此，只有在真正的实践中，才能体会到讲座中，老师们所向我们介绍的人工智能的神奇力量。为此，我亲自尝试了人工智能的两个具体的应用，并用机器学习的方法，解决了实际生活中的问题，这一过程让我感触颇深，也是我在这门课中收获最大的地方。\n第一个实践是运用多任务学习的方法，尝试解决了一类商业选址问题。\n商业选址是一类重要的投资决策问题。其重要性主要体现在投资的长期性、固定性以及对经济效益的决定性上。在传统的商业选址问题中，通常的考量因素往往涵盖了地域、交通、竞争压力以及人流量等方面。在这种情况下，投资者的经验以及数据信息来源的有效性将起到决定性的作用。随着移动互联网时代的到来，越来越多的商业应用，如美团、大众点评等渐渐走入人们的生活。这其中蕴含着巨大潜在的商业价值有待挖掘，尤其对于商业选址这类重要的问题而言，数据所提供的参考信息已然成为大数据时代的选址利器。\n近年来，社会经济持续发展，企业不断扩张，连锁店的经营模式得到了更为广泛的应用。如餐饮业的海底捞火锅店、麦当劳、星巴克，服饰业的H&M、Nike、Zara等品牌的迅猛发展，连锁店这种商业模式开始逐渐在市场中占据主导地位。由此为这些连锁品牌带来一个关乎企业发展的核心问题，即连锁店的选址问题。为此，在我的实践中，综合考量投资所在地商场的内部和外部特征，为连锁品牌的投资者进行商业选址的推荐。对大众点评上的海量数据进行分析，并为投资者给出最优化的选址推荐。\n第二个实践则是综合运用了计算机视觉相关技术，实现了一种视频的风格迁移方法。\n随着手机等智能终端的兴起，许多软件如春笋般发芽成长。从用户的触媒习惯来看，大家投入在短视频上的时间越来越多。艾瑞数据显示，用户单机单日有效使用时长已经从2017年度第一季度的21.1分钟增长到2018第二季度的33.1分钟。短视频行业中，我们也可以看到，抖音、快手等短视频软件异军突起，发展迅猛。短视频行业的火热，与短视频相关的技术自然也是如鱼得水。我所实现的视频风格迁移方法，即拥有针对短视频进行视频风格迁移的能力。用户可以根据喜好，选择某种名画的风格，即可对自己拍摄的短视频进行加工，生成美轮美奂的带有名画风格的短视频。\n我在这一实践中，综合运用了多种计算机视觉的相关技术，包括但不限于采用convLSTM来捕捉视频的时序特征、WarpError来计算视频流中相邻帧之间的差值、引入了一种风格迁移模型RecoNet同时结合了Instance Normalization的方法，来代替传统的Batch Normalization，从而实现对多风格的控制，等等。最终使得这一视频风格迁移方法相较与目前最佳解决方案的效果，在视频稳定性等呈现效果上，有着更优的表现。\n通过这两个实践，我都成功地将讲座中老师们所介绍到的人工智能的理论应用到实际，从而让我真切体会到了人工智能对人们生活方式的改变。\n2 课程收获\n2-1 总述\n与课程感悟部分不同的是，在课程感悟部分，我重点论述了我在本学期两个月来，从头至尾完整、认真地听过8次讲座后在宏观层面的整体感受。而在这一章节，课程收获中，我将更偏重于将我本学期在讲座中所学习到的领域知识或是让我对整个科技前沿体系的理解有帮助的内容记录下来，形成一个相对完整的脉络。\n另外，这一部分对于课程收获的总结，也对我日后时常回顾这8场讲座的精彩内容，保留一个比较细致的记录。\n我在此课程中的收获，正如我在课程感悟中所提及的，正是我所感兴趣的人工智能相关的话题，因此，接下来我将从人工智能的发展历程，人工智能的发展现状，以及人工智能的发展前景展开论述，在最后的一个小节中，作为补充，我也来谈谈我自己对人工智能的认识和态度。\n2-2 人工智能发展历程\n随着众多核心技术的迅猛发展，已经诞生了半个多世纪的人工智能终于从研究与发现发展到如今的巅峰期。回顾起来，在过去半个多世纪中人工智能经历过黄金时代也曾有过低谷，不过科技的魅力在于历经起起伏伏之后，现在的人工智能已开始深深影响人类社会。\n都说人工智能在历史上经历了“三起三落”，那么这三“起”与三“落”到底指代着什么，发生的时间节点与背景优势什么，以下我将按照时间线顺序来记录一下。\n人工智能的第一次高峰：在1956年的这次会议之后，人工智能迎来了属于它的第一段高峰。在这段长达十余年的时间里，计算机被广泛应用于数学和自然语言领域，用来解决代数、几何和英语问题。这让很多研究学者看到了机器向人工智能发展的信心。甚至在当时，有很多学者认为：“二十年内，机器将能完成人能做到的一切。”\n人工智能第一次低谷： 70年代，人工智能进入了一段痛苦而艰难岁月。由于科研人员在人工智能的研究中对项目难度预估不足，不仅导致与美国国防高级研究计划署的合作计划失败，还让大家对人工智能的前景蒙上了一层阴影。与此同时，社会舆论的压力也开始慢慢压向人工智能这边,导致很多研究经费被转移到了其他项目上。\n在当时，人工智能面临的技术瓶颈主要是三个方面，第一,计算机性能不足，导致早期很多程序无法在人工智能领域得到应用；第二，问题的复杂性，早期人工智能程序主要是解决特定的问题，因为特定的问题对象少，复杂性低，可一旦问题上升维度，程序立马就不堪重负了；第三，数据量严重缺失，在当时不可能找到足够大的数据库来支撑程序进行深度学习，这很容易导致机器无法读取足够量的数据进行智能化。\n人工智能的崛起：1980年，卡内基梅隆大学为数字设备公司设计了一套名为XCON的“专家系统”。这是一种，采用人工智能程序的系统，可以简单的理解为“知识库+推理机”的组合，XCON是一套具有完整专业知识和经验的计算机智能系统。在这个时期，仅专家系统产业的价值就高达5亿美元。\n人工智能第二次低谷：可怜的是，命运的车轮再一次碾过人工智能，让其回到原点。仅仅在维持了7年之后，这个曾经轰动一时的人工智能系统就宣告结束历史进程。到1987年时，苹果和IBM公司生产的台式机性能都超过了Symbolics等厂商生产的通用计算机。从此，专家系统风光不再。\n人工智能再次崛起：上世纪九十年代中期开始，随着AI技术尤其是神经网络技术的逐步发展，以及人们对AI开始抱有客观理性的认知，人工智能技术开始进入平稳发展时期。1997年5月11日，IBM的计算机系统“深蓝”战胜了国际象棋世界冠军卡斯帕罗夫，又一次在公众领域引发了现象级的AI话题讨论。这是人工智能发展的一个重要里程。 2016年，alphago在围棋上击败了李世石，再一次向世人揭示了人工智能非凡力量。\n2-3 人工智能发展现状\n经历了技术驱动和数据驱动阶段，人工智能现在已经进入场景驱动阶段，深入落地到各个行业之中去解决不同场景的问题。此类行业实践应用也反过来持续优化人工智能的核心算法，形成正向发展的态势。\n老师们在讲座中所提及的人工智能的主要应用领域主要涵盖了以下几个方面，包括制造、家居、金融、零售、交通、安防、医疗、物流、教育等等。不难看出，人工智能在当前的应用已经十分广泛，可以说基本覆盖到了人们日常生活的方方面面。\n家居\n智能家居是老师在讲座中提到的一个常见的人工智能应用之一。\n智能家居主要是基于物联网技术，通过智能硬件、软件系统、云计算平台构成一套完整的家居生态圈。用户可以进行远程控制设备，设备间可以互联互通，并进行自我学习等，来整体优化家居环境的安全性、节能性、便捷性等。值得一提的是，近两年随着智能语音技术的发展，智能音箱成为一个爆发点。小米、天猫、Rokid 等企业纷纷推出自身的智能音箱，不仅成功打开家居市场，也为未来更多的智能家居用品培养了用户习惯。但目前家居市场智能产品种类繁杂，如何打通这些产品之间的沟通壁垒，以及建立安全可靠的智能家居服务环境，是该行业下一步的发力点。\n金融\n正如我在自我实践中所接触的项目一样，人工智能可以在商业、金融领域为人们提供可靠的数据保障。\n人工智能在金融领域的应用主要包括：身份识别、大数据风控、智能投顾、智能客服、金融云等，该行业也是人工智能渗透最早、最全面的行业。未来人工智能也将持续带动金融行业的智能应用升级和效率提升。例如第四范式开发的一套AI系统，不仅可以精确判断一个客户的资产配置，做清晰的风险评估，以及智能推荐产品给客户，将转化率提升65%。很多金融行业的应用，都可以作为人工智能在其他行业落地的典型案例。\n交通\n最近一段时间，我效力于王静远老师的实验室，主要负责配合实验室的学长，完成一些基本的开发工作，王老师的实验室主要负责与数据挖掘有关领域，解决交通与医疗相关问题。在交通方面，主要是智慧交通，对车辆做行车轨迹恢复以及行程时长估计。而在医疗方面则是与医院展开合作，对孕妇可能存在的潜在风险进行评估，从而保证孕妇妊娠过程的安全。\n智能交通系统是通信、信息和控制技术在交通系统中集成应用的产物。智能交通应用最广泛的地区是日本，其次是美国、欧洲等地区。目前，我国在智能交通方面的应用主要是通过对交通中的车辆流量、行车速度进行采集和分析，可以对交通进行实施监控和调度，有效提高通行能力、简化交通管理、降低环境污染等。\n医疗\n目前，在垂直领域的图像算法和自然语言处理技术已可基本满足医疗行业的需求，市场上出现了众多技术服务商，例如提供智能医学影像技术的德尚韵兴，研发人工智能细胞识别医学诊断系统的智微信科，提供智能辅助诊断服务平台的若水医疗，统计及处理医疗数据的易通天下等。尽管智能医疗在辅助诊疗、疾病预测、医疗影像辅助诊断、药物开发等方面发挥重要作用，但由于各医院之间医学影像数据、电子病历等不流通，导致企业与医院之间合作不透明等问题，使得技术发展与数据供给之间存在矛盾。\n2-4 人工智能发展前景\n对待人工智能当下火爆的市场形势，要判断和分析其在未来的发展，还需要冷静、客观。\n在影响就业之前，人工智能将会对雇主产生影响。\n长期来看,人工智能不会摧毁就业市场——至少在2018年是不可能的。但是企业面临着一个重大挑战：只有汇集了来自不同种类的数据以及不同学科的团队成员时，人工智能才能发挥出最大的效果。同时，它还需要借助相应的结构和技能来实现人机协作。但是大多数企业都把数据存放在联合企业和团队的数据库里。很少有企业开始为员工提供他们所需要的基本人工智能技能。普通的企业还没有准备好满足人工智能的需求。\n人工智能将融入现实，开始发挥其效用。\n它可能不会成为媒体的头条新闻,但人工智能现在已经准备好了，能够自动完成日益复杂的流程，识别出能够创造商业价值的趋势，并提供具有前瞻性的情报。\n这带来的结果是,人们的工作量减少,做出的战略决策也变得更好了：员工的工作也比以前更好了。但是,由于传统的投资回报率（ROI）策略可能无法准确地识别出这一价值，企业将需要考虑采取新的指标，以便更好地理解工智能可以为它们做什么。\n人工智能将帮助回答有关数据的重大问题。\n许多针对数据技术和数据集成的投资都未能回答这样的一个重大问题：投资回报率在哪？现在，人工智能正在为这些数据项目提供商业案例，新的工具将会使这些项目的价值凸显出来。\n企业不再需要决定“清理数据”——也不应该这样做。他们应该首先从一个业务问题开始来量化人工智能的好处。一旦数据被用来解决一个特定的问题，进一步开发数据驱动的人工智能解决方案就会变得更容易，从而就会形成一个良性循环。问题出在了哪里？一些企业仍然在犹豫要不要建立，或者是没有建立好数据基础。\n在未来，人工智能领域的投资将以“AI+行业”的方式展开，预计人工智能应用场景较为成熟且需求强烈的领域，如安防、语音识别、医疗、智慧城市、金融等领域，带来升级转换，提高行业智能化水平，改善企业的盈利能力，预计随着诸如无人驾驶汽车等认知智能技术的加速突破与应用，人工智能市场将加速爆发。\n3 后记\n老师们精彩的讲座，为我打开了人工智能世界的大门；通过两个具体案例的实践，让我真切的亲身探索了人工智能这个丰富多彩的世界。我被人工智能给人类社会和当今人们生活方式所带来的改变，深深折服，在惊叹于技术发展的同时，也让我对踏入人工智能行业充满了向往。\n但与此同时尽管人工智能作为行业内的新兴热点，随着时间的推移其热度一直有增无减，对其未来的发展，仍应保持理性与客观的态度。因此，对领域内学科技术前沿时刻保持高度的敏锐嗅觉，透过问题的表面现象看到本质，才是这门课带给我最大的思考。\n人工智能的未来何去何从，我拭目以待。"}
{"content2":"机器学习错题集\n1.    Some of the problems below are best addressed using a supervised learning algorithm, and the others with an unsupervised learning algorithm. Which of the following would you apply supervised learning to? (Select all that apply.) In each case,\nassume some appropriate dataset is available for your algorithm to learn from. 【A,C】\nA. Given historical data of childrens' ages and heights, predict children's height as a function of their age.\n【解析】This is a supervised learning, regression problem, where we can learn from a training set to predict height.\nB.  Examine a large collection of emails that are known to be spam email, to discover if there are sub-types of spam mail.\n【解析】This can addressed using a clustering (unsupervised learning) algorithm, to cluster spam mail into sub-types.\nC.  Examine the statistics of two football teams, and predicting which team will win tomorrow's match (given historical data of teams' wins/losses to learn from).\n【解析】This can be addressed using supervised learning, in which we learn from historical records to make win/loss predictions.\nD.  Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we                might tailor separate treatements.\n【解析】This can be addressed using an unsupervised learning, clustering, algorithm, in which we group patients into different clusters.\n2.   Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some θ0, θ1 such that J(θ0,θ1)=0. Which of the statements below must then\nbe true?  【A】\nA. For these values of θ0 and θ1 that satisfy J(θ0,θ1)=0, we have that hθ(x(i))=y(i) for every training example (x(i),y(i))\n【解析】J(θ0,θ1)=0, that means the line defined by the equation \"y=θ0+θ1x\" perfectly fits all of our data.\nB. For this to be true, we must have y(i)=0 for every value of i=1,2,…,m.\n【解析】So long as all of our training examples lie on a straight line, we will be able to find θ0 and θ1 so that J(θ0,θ1)=0. It is not necessary that y(i)=0 for all of our examples.\nC. Gradient descent is likely to get stuck at a local minimum and fail to find the global minimum.\n【解析】The cost function J(θ0,θ1) for linear regression has no local optima (other than the global minimum), so gradient descent will not get stuck at a bad local minimum.\nD.We can perfectly predict the value of y even for new examples that we have not yet seen. (e.g., we can perfectly predict prices of even new houses that we have not yet seen.)\n【解析】Even though we can fit our training set perfectly, this does not mean that we'll always make perfect predictions on houses in the future/on houses that we have not yet seen.\n3.  Which of the following are reasons for using feature scaling?\nIt speeds up gradient descent by making it require fewer iterations to get to a good solution.\n【解析】Feature scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features take on much larger values than the rest.\nThe cost function J(θ) for linear regression has no local optima.\nThe magnitude of the feature values are insignificant in terms of computational cost.\n4.You run gradient descent for 15 iterations with α=0.3 and compute J(θ) aftereach iteration. You find that the value of J(θ) decreases quickly\nthen levels off. Based on this, which of the following conclusions seems most plausible?\nA smaller learning rate will only decrease the rate of convergence to the cost function's minimum, thus increasing the number of iterations needed.\n5. You are training a classification model with logistic regression. Which of the following statements are true? Check all that apply.【D】\nA. Introducing regularization to the model always results in equal or better performance on the training set.\nIntroducing regularization to the model always results in equal or better performance on the training set.\n【解析】If we introduce too much regularization, we can underfit the training set and have worse performance on the training set.\nB.Adding many new features to the model helps prevent overfitting on the training set.\n【解析】Adding many new features gives us more expressive models which are able to better fit our training set. If too many new features are added, this can lead to overfitting of the training set.\nC. Adding a new feature to the model always results in equal or better performance on examples not in\nthe training set.\n【解析】Adding  more features might result in a model that overfits the training set, and thus can lead to worse performs for examples which are not in the training set.\nD.Adding a new feature to the model always results in equal or better performance on the training set.\n【解析】By adding a new feature, our model must be more (or just as) expressive, thus allowing it learn more complex hypotheses to fit the training set.\n6. Which\nof the following statements about regularization are true? Check all that apply.【D】\nA.Because regularization causes J(θ) to no longer be convex, gradient descent may not always converge to the global minimum (when λ>0, and when using an appropriate learning rate α).\n【解析】Regularized logistic regression and regularized linear regression are both convex, and thus gradient descent will still converge to the global minimum.\nB.Using too large a value of λ can cause your hypothesis to overfit the data; this can be avoided by reducing λ.\n【解析】Using a very large value of λ can lead to underfitting of the training set.\nC.Because logistic regression outputs values 0≤hθ(x)≤1, it's range of output values can only be \"shrunk\" slightly by regularization anyway, so regularization is generally not helpful for it.\n【解析】Regularization affects the parameters θ and is also helpful for logistic regression.\nD.Consider a classification problem. Adding regularization may cause your classifier to incorrectly classify some training examples (which it had correctly classified when not using regularization, i.e. when λ=0).\n【解析】Regularization penalizes complex models (with large values of θ).They can lead to a simpler models, which misclassifies more training examples.\n7. Which of the following statements about regularization are true? Check all that apply.【A,B,C,D】\nA.For computational efficiency, after we have performed gradient checking to verify that our backpropagation code is correct, we usually disable gradient checking before using backpropagation to train the network.\n【解析】Checking the gradient numerically is a debugging tool: it helps ensure a corre ct implementation,\nbut it is too slow to use as a method for actually computing gradients.\nB.If our neural network overfits the training set, one reasonable step to take is to increase the regularization\nparameter λ.\n【解析】Just as with logistic regression, a large value of λ will penalize large parameter values, thereby reducing the changes of overfitting the training set.\nC.Suppose you are training a neural network using gradient descent. Depending on your random initialization, your algorithm may converge to different local optima (i.e., if you run the algorithm twice with different random initializations, gradient descent may converge\nto two different solutions).\n【解析】The\ncost function for a neural network is non-convex, so it may have multiple minima. Which minimum you find with gradient descent depends on the initialization.\nD.Suppose we have a correct implementation of backpropagation, and are training a neural network using gradient descent. Suppose we plot J(Θ) as a function of the number of iterations, and find that it isincreasing rather than decreasing. One possible cause of this is that the learning rate α is too large.\n【解析】If the learning rate is too large, the cost function can diverge during gradient descent. Thus, you should select a smaller value of α.\nE.Suppose that the parameter Θ(1) is a square matrix (meaning the number of rows equals the number of columns). If we replace Θ(1) with its transpose (Θ(1))T, then we have not changed the function that the network is computing.\n【解析】Θ(1) can be an arbitrary matrix, so when you compute a(2)=g(Θ(1)a(1)), replacing Θ(1) with its transpose will compute a different value.\nF.Suppose we are using gradient descent with learning rate α. For logistic regression and linear regression, J(θ) was a convex optimization problem and thus we did not want to choose a learning rate α that is too large. For a neural network however, J(Θ) may not be convex, and thus choosing a very large value of α can only speed up convergence.\n【解析】Even when J(Θ) is not convex, a learning rate that is too large can prevent gradient descent from converging.\nG.Using a large value of λ cannot hurt the performance of your neural network; the only reason we do not set λ to be too large is to avoid numerical problems.\n【解析】A large value of λ can be quite detrimental. If you set it too high, then the network will be underfit to the training data and give poor predictions on both training data and new, unseen test data.\nH.Gradient checking is useful if we are using gradient descent as our optimization algorithm. However, it serves little purpose if we are using one of the advanced optimization methods (such as in fminunc).\n【解析】Gradient checking will still be useful with advanced optimization methods, as they depend on computing the gradient at given parameter settings. The difference is they use the gradient values in more sophisticated ways than gradient descent."}
{"content2":"摘要：\n数据挖掘、机器学习和推荐系统中的评测指标—准确率(Precision)、召回率(Recall)、F值(F-Measure)简介。\n引言：\n在机器学习、数据挖掘、推荐系统完成建模之后，需要对模型的效果做评价。\n业内目前常常采用的评价指标有准确率(Precision)、召回率(Recall)、F值(F-Measure)等，下图是不同机器学习算法的评价指标。下文讲对其中某些指标做简要介绍。\n本文针对二元分类器！\n本文针对二元分类器！！\n本文针对二元分类器！！！\n对分类的分类器的评价指标将在以后文章中介绍。\n在介绍指标前必须先了解“混淆矩阵”：\n混淆矩阵\nTrue Positive(真正，TP)：将正类预测为正类数\nTrue Negative(真负，TN)：将负类预测为负类数\nFalse Positive(假正，FP)：将负类预测为正类数误报 (Type I error)\nFalse Negative(假负，FN)：将正类预测为负类数→漏报 (Type II error)\n1、准确率（Accuracy）\n准确率(accuracy)计算公式为：\n注：准确率是我们最常见的评价指标，而且很容易理解，就是被分对的样本数除以所有的样本数，通常来说，正确率越高，分类器越好。\n准确率确实是一个很好很直观的评价指标，但是有时候准确率高并不能代表一个算法就好。比如某个地区某天地震的预测，假设我们有一堆的特征作为地震分类的属性，类别只有两个：0：不发生地震、1：发生地震。一个不加思考的分类器，对每一个测试用例都将类别划分为0，那那么它就可能达到99%的准确率，但真的地震来临时，这个分类器毫无察觉，这个分类带来的损失是巨大的。为什么99%的准确率的分类器却不是我们想要的，因为这里数据分布不均衡，类别1的数据太少，完全错分类别1依然可以达到很高的准确率却忽视了我们关注的东西。再举个例子说明下。在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc也有 99% 以上，没有意义。因此，单纯靠准确率来评价一个算法模型是远远不够科学全面的。\n2、错误率（Error rate）\n错误率则与准确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(TP+TN+FP+FN)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 - error rate。\n3、灵敏度（sensitive）\nsensitive = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。\n4、特效度（sensitive）\nspecificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。\n5、精确率、精度（Precision）\n精确率(precision)定义为：\n表示被分为正例的示例中实际为正例的比例。\n6、召回率（recall）\n召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitive，可以看到召回率与灵敏度是一样的。\n7、综合评价指标（F-Measure）\nP和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。\nF-Measure是Precision和Recall加权调和平均：\n当参数α=1时，就是最常见的F1，也即\n可知F1综合了P和R的结果，当F1较高时则能说明试验方法比较有效。\n8、其他评价指标\n计算速度：分类器训练和预测需要的时间；\n鲁棒性：处理缺失值和异常值的能力；\n可扩展性：处理大数据集的能力；\n可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。\n下面来看一下ROC和PR曲线（以下内容为自己总结）：\n1、ROC曲线：\nROC（Receiver Operating Characteristic）曲线是以假正率（FP_rate）和假负率（TP_rate）为轴的曲线，ROC曲线下面的面积我们叫做AUC，如下图所示：\n图片根据Paper：Learning from eImbalanced Data画出\n其中：\n（1）曲线与FP_rate轴围成的面积（记作AUC）越大，说明性能越好，即图上L2曲线对应的性能优于曲线L1对应的性能。即：曲线越靠近A点（左上方）性能越好，曲线越靠近B点（右下方）曲线性能越差。\n（2）A点是最完美的performance点，B处是性能最差点。\n（3）位于C-D线上的点说明算法性能和random猜测是一样的–如C、D、E点。位于C-D之上（即曲线位于白色的三角形内）说明算法性能优于随机猜测–如G点，位于C-D之下（即曲线位于灰色的三角形内）说明算法性能差于随机猜测–如F点。\n（4）虽然ROC曲线相比较于Precision和Recall等衡量指标更加合理，但是其在高不平衡数据条件下的的表现仍然过于理想，不能够很好的展示实际情况。\n2、PR曲线：\n即，PR（Precision-Recall）曲线。\n举个例子（例子来自Paper：Learning from eImbalanced Data）：\n假设N_c>>P_c（即Negative的数量远远大于Positive的数量），若FP很大，即有很多N的sample被预测为P，因为，因此FP_rate的值仍然很小（如果利用ROC曲线则会判断其性能很好，但是实际上其性能并不好），但是如果利用PR，因为Precision综合考虑了TP和FP的值，因此在极度不平衡的数据下（Positive的样本较少），PR曲线可能比ROC曲线更实用。\n转载自：机器学习算法中的准确率(Precision)、召回率(Recall)、F值(F-Measure)是怎么一回事"}
{"content2":"本人看过的关联规则博文，很少有清晰的把关联规则的算法说很明白的，希望读者读完本文可以有新的收获。本文是在默认读者有相关机器学习算法基础的，总结和提升对关联规则代码实现的理解，并介绍相关案例。语言：python\n一 引言\n关联规则起初是在购物篮分析中发现的，沃尔玛超市在美国某地区啤酒和尿布放在一起卖，这种关联规则有利于市场营销决策的制定。\n关联规则是非监督学习的一种。\n二 两个重要的概念\n我们认定满足支持度和置信度的规则是有趣的，\n支持度：P(A)，及项集A出现的概率（频数）；\n置信度：P（B / A）, 条件概率;　P（B / A）= P（AB）/P（A），所以支持度可以用来计算置信度，代码是学习算法最好的途径。\n三 Apriori算法\n网上很多文章介绍Apriori算法都是云里雾里，下面梳理一下脉络。\n核心是apriori原理：如果某个项集是频繁的，那么它是所有子集也是频繁的。\napriori原理的精妙在于他的逆否命题，若子集不是频繁的，则所有包含它的项集都是不频繁的。这样剪掉不频繁项集时，就可以同时剪掉很多包含这个项集的不频繁的项集了。若蛮力查找大频项集，时间复杂度是指数型，例如4个项集，它的所有组合的复杂度是15。同理，可以剪掉后件不满足置信度规则时，同时剪掉后件包含这个规则后件的规则。形象的说，看下图：\neg\n项集{0,1,2,3}，计算所有的组合情况如下图，好像一个格，时间复杂度是指数型的，按照项集元素数由小到大计算每个组合的支持度，{2，3}黑色圈不满足最小支持度，由apriori定理，则所有以它为子集的项集均不满足最小支持度，需要剪掉。\n下面继续考虑置信度，规则{012}->{3}不满足最小置信度，由apriori定理，所有后件包含这条规则后件的规则需要剪掉，即二后件规则：{01}->{23},{02}->{1,3},{12}->{03}和三后件规则{0}->{123},{1}->{023},{2}->{013}。\n读到这里有关联规则基础的人应该会有感悟，通过下面代码介绍可以细致的明白apriori算法的机制。\n四 apriori实现\n1 加载demo数据集(可以改成真实数据集，读文件):\nfrom numpy import * def loadDataSet(): return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n2 生成一频繁项集作为起始点，即长度为1的频繁集，图中的第一层：使用frozenset结构是因为set不可以作为dict的关键字\ndef createC1(dataSet):#create one item C1 = [] for transaction in dataSet: for item in transaction: if not [item] in C1: C1.append([item]) C1.sort() # print C1 return map(frozenset, C1)#use frozen set so we #can use it as a key in a dict\n3 计算Ck支持度，并剪掉不满足最小指出的项集。\ndef scanD(D, Ck, minSupport):#create one or more big frequence item ssCnt = {} for tid in D: for can in Ck: if can.issubset(tid): if not ssCnt.has_key(can): ssCnt[can]=1 else: ssCnt[can] += 1 numItems = float(len(D)) retList = [] supportData = {} for key in ssCnt: support = ssCnt[key]/numItems if support >= minSupport: retList.insert(0,key) supportData[key] = support return retList, supportData\n4 由m频繁项集生成m+1频繁项集：举例，Ck为1频繁项集{01}{12}{02},生成{012}，有个技巧，只合并前m-2项一样的项集，这样不会重复操作，如这个例子，{01}和{12}不合并，{12}{02}也不合并，只有{01}{02}合并，生成最后的{012}；否则前几个合并都是重复的。值得注意的是0-3项集要排好序，否则前k-2个没有比较的必要。\ndef aprioriGen(Lk, k): #creates Ck retList = [] lenLk = len(Lk) # print Lk for i in range(lenLk): for j in range(i+1, lenLk): L1 = list(Lk[i])[:k-2]; L2 = list(Lk[j])[:k-2] L1.sort(); L2.sort() # print L1,'--',L2 if L1==L2: #if first k-2 elements are equal retList.append(Lk[i] | Lk[j]) #set union # print retList return retList\n5 生成打频项集的算法：\n思路，生成1频项集，根据最小支持度过滤；依次由m频项集生成m+1频项集,直到m+1频项集为空停止迭代。\ndef apriori(dataSet, minSupport = 0.5): C1 = createC1(dataSet) D = map(set, dataSet) L1, supportData = scanD(D, C1, minSupport)#create one big frequence item L1 # print L1 L = [L1] k = 2 # print L while (len(L[k-2]) > 0): # print 'L[k-2]',L[k-2] # print L Ck = aprioriGen(L[k-2], k) Lk, supK = scanD(D, Ck, minSupport)#scan DB to get Lk supportData.update(supK) L.append(Lk) k += 1 return L, supportData\n以上都是大频项集的生成算法，下面继续有趣的关联规则的发现算法：\n6 计算后件为H的规则的置信度，代码可以看出只是一个条件概率公式而已；根据最小置信度，筛选出有趣的规则；\ndef calcConf(freqSet, H, supportData, brl, minConf=0.7): prunedH = [] #create new list to return for conseq in H: conf = supportData[freqSet]/supportData[freqSet-conseq] #calc confidence if conf >= minConf: brl.append((freqSet-conseq, conseq, conf)) prunedH.append(conseq) return prunedH\n7 由后件数为m的规则集生成后件为后件数为m+1的规则集，并计算置信度；递归到没有可以合并的规则集停止；\n直观的过程可以查看上图的格，eq {23}->{01}和{12}->{03} 合并为{2}->{013};因为标红处前k-2相同，为了避免重复的合并操作，同上面打大频项集合并。\ndef rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):if (len(freqSet) > (m + 1)): #try further merging Hmp1 = aprioriGen(H, m+1)#create Hm+1 new candidates Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)if (len(Hmp1) > 1): #need at least two sets to merge rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n8 产生关联规则的最后算法：\n思路，由于规则的前后件均不能为空，所以只有二频繁项集才能产生关联规则；\n1）首先由二频繁项集生成规则集，遍历所有的二频繁项集（每个元素轮流作为后件），根据最小置信度过滤规则集；\neg 二频繁项集{12},则计算规则{1}->{2}和{2}->{1}的置信度；\n2）依次迭代，在三大频项集生成规则集（每个元素轮流作为后件），需要考虑规则的合并，\neg 三大频项集{123},则{12}->{3},{13}->{2},{23}->{1}，此外考虑合并，{1}->{23},{2}->{13},{3}->{12},还要继续合并：根据后件，前k-2个同的合并，本例前k-2个同的个数为0，所以停止，复杂的情况看步骤7；\ndef generateRules(L, supportData, minConf=0.7): #supportData is a dict coming from scanD bigRuleList = [] for i in range(1, len(L)):#only get the sets with two or more items for freqSet in L[i]: H1 = [frozenset([item]) for item in freqSet] if (i > 1): rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) else: calcConf(freqSet, H1, supportData, bigRuleList, minConf) return bigRuleList\n五 应用\n关联规则可以应用到哪些问题呢？\n购物篮分析，搜索引擎的查询词，国会投票，毒蘑菇的相似特征提取等；\n六 毒蘑菇的相似特征提取\n毒蘑菇部分数据集如下:\n1 3 9 13 23 25 34 36 38 40 52 54 59 63 67 76 85 86 90 93 98 107 113 2 3 9 14 23 26 34 36 39 40 52 55 59 63 67 76 85 86 90 93 99 108 114 2 4 9 15 23 27 34 36 39 41 52 55 59 63 67 76 85 86 90 93 99 108 115 1 3 10 15 23 25 34 36 38 41 52 54 59 63 67 76 85 86 90 93 98 107 113 2 3 9 16 24 28 34 37 39 40 53 54 59 63 67 76 85 86 90 94 99 109 114 2 3 10 14 23 26 34 36 39 41 52 55 59 63 67 76 85 86 90 93 98 108 114 2 4 9 15 23 26 34 36 39 42 52 55 59 63 67 76 85 86 90 93 98 108 115 2 4 10 15 23 27 34 36 39 41 52 55 59 63 67 76 85 86 90 93 99 107 115 1 3 10 15 23 25 34 36 38 43 52 54 59 63 67 76 85 86 90 93 98 110 114 2 4 9 14 23 26 34 36 39 42 52 55 59 63 67 76 85 86 90 93 98 107 115 2 3 10 14 23 27 34 36 39 42 52 55 59 63 67 76 85 86 90 93 99 108 114 2 3 10 14 23 26 34 36 39 41 52 55 59 63 67 76 85 86 90 93 98 107 115\n每一行代表一个蘑菇的特征，第一列是决策类，1代表有毒，2代表五毒；\n加载数据集：\ndataset=[line.split() for line in open('mashroom.dat'),readline()]\n查找大频率项集：\nL，supp=apriori(dataset,0.3)\napriori函数在算法部分已经实现，直接调用即可。\n有时候我们只需要查找大频项集，并不需要关联规则，具体问题具体分析即可。\n七 总结\n整体算法就有两个核心，1计算满足最小支持度的大频率项集，2挖掘满足最小置信度的有趣规则；暴力遍历每种组 合的方式指数级，利用了apriori定理，剪掉了不满足要求的小项集和小后件规则的同时，剪掉包含他们的大频度项集和大后件规则；还有一点主意的是。按 照图中格的形式，一层一层有小到大迭代。"}
{"content2":"机器学习——异常检测\n在生产生活中，由于设备的误差或者人为操作失当，产品难免会出现错误。然后检查错误对人来说又是一个十分琐碎的事情。利用机器学习进行异常值检测可以让人类摆脱检错的烦恼。\n检测算法\n1.选定容易出错的\\(n\\)个特征\\(\\{x_1^{(i)},x_2^{(i)},\\ldots,x_n^{(i)}\\}\\)作为变量。\n2.计算m个样本的平均值和方差。\n\\[{\\mu_j} = {1 \\over m}\\sum\\limits_{i = 1}^m {x_j^{(i)}}\\]\n\\[ {\\sigma ^2} = {1 \\over m}\\sum\\limits_{i = 1}^m {(x_j^{(i)}} - {\\mu _j}{)^2} \\]\n3.给定监测点\\(x\\).计算\\(p(x)\\)\n\\[p(x) = \\prod \\limits_{j = 1}^n {p({x_j};{\\mu_j},\\sigma_j^2)}\\]\n4.如果\\(p(x)< \\epsilon\\),则为异常值；反之，不是。\n开发和评价一个异常检测系统\n异常检测算法是一个非监督学习算法，意味着我们无法通过结果变量判断我们的数据是否异常。所以我们需要另一种方法检测算法是否有效。当我们开发一个系统时，我们从有标签（知道是否异常）的数据入手，从中找出一部分正常数据作为训练集，剩余的正常数据和异常数据作为交叉检验集和测试集。\n具体评价方法如下：\n根据测试集数据，估计出特征的平均值和方差，构建\\(p(x)\\)函数\n对于交叉检验集，尝试使用不同的\\(\\epsilon\\)最为阈值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择\\(\\epsilon\\)\n选出\\(\\epsilon\\)后，针对测试集进行预测，计算异常检验系统的F1值或者查准率与查全率之比\n异常检测与监督学习对比\n异常检测\n监督学习\n大量的正常值（y=0）和少量的异常值(y=1)\n大量的正向类（y=0）和少量的负向类(y=1)\n异常数据太少，只能根据少量数据进行训练\n有足够多的正向和负向数据以供训练\n举例：1.欺诈行为检测；2.生产废品检测；3.检测机器运行状态\n举例：1.邮箱过滤器；2.天气预报；3.肿瘤分类\n分布的处理\n对于高斯分布的数据，直接运用以上算法就好。\n但是对于非高斯分布的数据，虽然也可是使用上面的算法，但是效果不是很好，所以我们尽量将非高斯分布转化成（近似）高斯分布，然后再进行处理。\n数据整体偏小，可以求\\(ln(x)\\)或者\\(x^a,0<a<1\\)\n数据整体偏大，可以求\\(e^x\\)或者\\(x^a,a>1\\)\n误差分析\n在误差分析中，如果我们可以发现我的选定的变量是否合适，进而进行相应的改正。如左图所示，异常点\\(x\\)对应的概率很高，显然这种分布方式不能很好地识别出异常值。所以我们尝试增加变量或者改变变量的类型来识别异常值。如右图所示，通过增加一个变量，我们能够更好地识别异常点。所以，误差分析对于一个问题来说还是很重要的。"}
{"content2":"开篇：\n公司正在全面推广RPA，正好借此机会学习一下，发现国内对RPA的了解较少，萌生了在博客园开博，同时锻炼一下自己的输出能力，纯笔记，如有不足之处，请指正，共勉。\n阅读目录：\n1. 什么是RPA\n2. 为什么要用RPA\n3. RPA的优势\n4. RPA的组成\n5. 各类RPA平台\n1. 什么是RPA\nRPA是Robotic Process Automation的缩写，也就是机器人流程自动化(笔者译)。Wikipedia给出的定义是基于软件机器人或人工智能（AI）工作人员概念的新兴业务过程自动化技术形式。我对RPA的理解是处理机器和计算机在商品和服务生产中的应用的技术，有助于在很少或没有人力帮助的情况下完成工作。我们生活中常见的自动化有：自动售货机，全自动洗衣机，无人驾驶等。WendyZheng\n2. 为什么要用RPA\n当我们遇到重复步骤，费时，高风险，低效益的时候，此时可以考虑一下自动化来帮助我们解放双手，提高效益。 关于程序中用到的自动化有：浏览器自动化，桌面自动化，Wrapper和API/Web服务/数据库集成。\n3. RPA的优势\nRPA采用的技术有：机器学习，自然语言处理，自然语言生成和计算机视觉。RPA允许Robots以人类相同的方式与任何应用程序进行交互。RPA与传统自动化的区别时，它使用的是说明性步骤，剥离代码层，因此，具有少量编程经验的人员也能将复杂的过程自动化。具体特点如下：\n1) 更高质量的服务，更高的准确性：WendyZheng\nRPA可减少人为错误及人类难以追踪的问题，因为自动化过程中的每一步都被记录下来，可以更快速地查明错误，减少或消除错误还意味着更高的数据准确性，从而实现更好的质量分析和决策。\n2) 改进的分析：\n由于这些软件机器人可以记录采用适当标签和元数据，因此获取业务数据非常容易。对收集到的数据使用分析功能，例如交易收到时间，交易完成时间和预测可用于传入交易量和按时完成任务的能力。\n3) 降低成本：\n一个员工每天工作八小时，而一个机器人可以不中断地工作24小时，可用性和生产力意味着运营成本大幅降低。\n4) 提高速度：\n机器人速度非常快，有时必须降低执行速度，以便与应用程序的速度和延迟相匹配。提高的速度可以更好的响应并增加正在执行的任务的数量。\n5) 更高的合规性：\n完整的审计跟踪是RPA的亮点之一，可以提高合规性，因为这些机器人不会偏离轨道去执行其他任务。\n6) 敏捷性：\n减少和增加机器人资源的数只需一个点击即可，可以同时部署多个机器人执行相同的任务。重新部署资源不需要任何形式的编码或重新配置。\n7) 全面的分析能力：\n除了审计追踪和时间戳之外，机器人还可以标记事务以便稍后用于报告中以供业务分析。通过这些分析可以做出更好的决策，此外这些数据也可以用于预测。\n8) 多功能性：\nRPA适用于大多数行业，从小到大的企业，简单到复杂的流程。\n9) 简单：\nRPA不需要更多的编程知识，大多数平台都以流程图的形式提供设计。这种简单性使业务流程的自动化变得轻松自如，让IT专业人士相对自由地开展更高价值的工作。此外，由于自动化是由部门或工作区域内的人员执行的，因此业务部门和开发团队之间的沟通不会有任何问题，这在传统自动化中可能会发生理解偏差。\n10) 可扩展性：WendyZheng\nRPA具有高度可扩展性，同时也可以降低人工成本，机器人都可以最低成本或零成本快速部署，同时保持工作质量。\n11) 节省时间：WendyZheng\n虚拟工作人员不仅精确地在更短的时间内完成大量工作。如果有任何变化，比如技术升级，虚拟员工就可以更容易更快地适应变化。这可以通过在编程中引入修改或引入新的过程来完成。对于人类来说，他们很难学习和接受新的事物的训练，这违背了执行重复性任务的旧习惯。\n12) 非侵入性：\n正如我们所知，RPA就像人类一样在用户界面上工作。这确保了可以在不改变现有计算机系统的情况下实施。这有助于降低传统IT部署中出现的风险和复杂性。\n13) 更好的管理：\nRPA允许通过集中式平台管理，部署和监控机器人，这也减少了对人工的管理需求。\n14) 更好的客户服务：\n由于机器人可以全天候工作，这使人类更专注于客户服务和满意度。此外，可以更快的速度向客户提供更高质量的服务，大大提高了客户满意度。\n15) 提高员工满意度：\n由于机器人可接受重复，沉重的任务，人类不仅可以减轻工作量，还可以参与更高质量的工作，如情绪智力，推理，或者趋向于客户服务。因此，RPA不会剥夺人类的工作，它只是让人类从繁琐的，令人头脑麻木的工作中解放出来。\n4. RPA的组成\n任何一个RPA平台都提供一些基本组件，如：\n1) Recorder\n2) Development Studio\n3) Plugin/Extension\n4) Bot Runner\n5) Control Center\n5. 各类RPA平台\nRPA平台\n范围\n总部\n关键客户\nAutomation Anywhere\n专注于RPA，认知数据（机器学习和自然语言处理）和业务分析。他们的机器人能够处理结构化和非结构化数据。\n美国加利福尼亚州\n德勤，埃森哲，AT＆T，通用汽车，J P Morgan Chase\nUiPath\nRPA技术供应商，负责设计和提供有助于实现业务自动化的软件\n罗马尼亚布加勒斯特\nAtos，安盛，BBC，凯捷，CenturyLink，Cognizant，Middlesea，OpusCapita和SAP\nBlue Prism\n提供企业根据自身需求使用的自动化功能，通过提供可扩展，可配置和集中管理的自动化来实现\n英国\nBNY Mellon，RWE npower和Telefonica O2\nWorkFusion\n提供软件作为自动化大量数据的解决方案，使人员和机器能够协同工作\n美国纽约\nThomson Reuters，资讯集团，花旗银行和Standard Bank\nThoughtonomy\n提供软件作可以帮助实现业务和IT流程的自动化，可定制\n英国伦敦\nAtos，富士通，CGI，联合BT和业务系统\nKOFAX\n能够自动化并交付重复性和基于规则的流程，该软件还可以组合机器人在高负载时应首先完成的高优先级任务。但是Kofax的软件没有机器学习功能\n加州尔湾\n艾睿电子，科罗拉多州三角洲齿科，俄亥俄州匹兹堡，奥迪"}
{"content2":"Machine Learning(机器学习)是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演译。\n在过去的十年中，机器学习帮助我们自动驾驶汽车，有效的语音识别，有效的网络搜索，并极大地提高了人类基因组的认识。\n机器学习是当今非常普遍，你可能会使用这一天几十倍而不自知。很多研究者也认为这是最好的人工智能的取得方式。\n在本课中，您将学习最有效的机器学习技术，并获得实践，让它们为自己的工作。更重要的是，你会不仅得到理论基础的学习，而且获得那些需要快速和强大的应用技术解决问题的实用技术。最后，你会学到一些硅谷利用机器学习和人工智能的最佳实践创新。\n本课程提供了一个广泛的介绍机器学习、数据挖掘、统计模式识别的课程。主题包括：\n（一）监督学习（参数/非参数算法，支持向量机，核函数，神经网络）\n（二）无监督学习（聚类，降维，推荐系统，深入学习推荐）\n（三）在机器学习的最佳实践（偏差/方差理 论；在机器学习和人工智能创新过程）\n本课程还将使用大量的案例研究，您还将学习如何 运用学习算法构建智能机器人（感知，控制），文本的理解（Web搜索，反垃圾邮件），\n计算机视觉，医疗信息，音频，数据挖掘，和其他领域。\n本课程需要 10 周共 18 节课，相对以前的机器学习视频，这个视频更加清晰，而且每课 都有 ppt 课件，推荐学习。\n第一课、 引言(Introduction)\n1.1  欢迎\n1.2  机器学习是什么？\n1.3  监督学习\n1.4  无监督学习\n1.1  欢迎\n第一个视频主要讲了什么是机器学习，机器学习能做些什么事情。 机器学习是目前信息技术中最激动人心的方向之一。在这门课中，你将学习到这门技术的前沿，并可以自己实现学习机器学习的算法。 你或许每天都在不知不觉中使用了机器学习的算法每次，你打开谷歌、必应搜索到你需要的内容，正是因为他们有良好的学习算法。谷歌和微软实现了学习算法来排行网页每次， 你用 Facebook 或苹果的图片分类程序他能认出你朋友的照片，这也是机器学习。每次您阅读您的电子邮件垃圾邮件筛选器，可以帮你过滤大量的垃圾邮，这也是一种学习算法。对我来说，我感到激动的原因之一是有一天做出一个和人类一样聪明的机器。实现这个想法任重而道远，许多 AI 研究者认为，实现这个目标最好的方法是通过让机器试着模仿人的大脑学 习我会在这门课中介绍一点这方面的内容。\n在这门课中，你还讲学习到关于机器学习的前沿状况。但事实上只了解算法、数学并不能解决你关心的实际的问题。所以，我们将花大量的时间做练习，从而你自己能实现每个这些算法，从而了解内部机理。那么，为什么机器学习如此受欢迎呢？原因是，机器学习不只是用于人工智能领域。 我们创造智能的机器，有很多基础的知识。比如，我们可以让机器找到 A 与 B 之间的最短 路径，但我们仍然不知道怎么让机器做更有趣的事情，如 web搜索、照片标记、反垃圾邮件。我们发现，唯一方法是让机器自己学习怎么来解决问题。所以，机器学习已经成为计算 机的一个能力。现在它涉及到各个行业和基础科学中。我从事于机器学习，但我每个星期都跟直升机飞行员、生物学家、很多计算机系统程序员交流（我在斯坦福大学的同事同时也是这样）和 平均每个星期会从硅谷收到两、三个电子邮件，这些联系我的人都对将学习算法应用于他们自己的问题感兴趣。这表明机器学习涉及的问题非常广泛。有机器人、计算生物学、硅谷中 大量的问题都收到机器学习的影响。\n这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公 司正在收集 web 上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据， 更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识， 我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都 知道这对人类意味着什么。再比如，工程方面，在工程的所有领域， 我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人 不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序 让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。\n手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你 写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我 们只需要花几个美分把这封信寄到数千英里外。事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于 AI 领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。学习算法还广泛 用于自定制程序。每次你去亚马逊或 Netflix 或 iTunes Genius，它都会给出其他电影或产品或音乐的建议，这是一种学习算法。仔细想一想，他们有百万的用户；但他们没有办法为百万 用户，编写百万个不同程序。软件能给这些自定制的建议的唯一方法是通过学习你的行为， 来为你定制服务。\n最后学习算法被用来理解人类的学习和了解大脑。\n我们将谈论如何用这些推进我们的 AI 梦想。几个月前，一名学生给我一篇文章关于最 顶尖的 12 个 IT 技能。拥有了这些技能 HR 绝对不会拒绝你。这是稍显陈旧的文章，但在这个列表最顶部就是机器学习的技能。在斯坦福大学，招聘人员联系我，让我推荐机器学习学生毕业的人远远多于机器学习的毕业生。所以我认为需求远远没有被满足现在学习“机器学习”非常好，在这门课中，我希 望能告诉你们很多机器学习的知识。\n在接下来的视频中，我们将开始给更正式的定义，什么是机器学习。然后我们会开始学习机器学习的主要问题和算法你会了解一些主要的机器学习的术语，并开始了解不同的算 法，用哪种算法更合适。\n1.2  机器学习是什么？\n机器学习是什么？在本视频中，我们会尝试着进行定义，同时让你懂得何时会使用机 器学习。实际上，即使是在机器学习的专业人士中，也不存在一个被广泛认可的定义来准确 定义机器学习是什么或不是什么，现在我将告诉你一些人们尝试定义的示例。第一个机器学习的定义来自于 Arthur Samuel。他定义机器学习为，在进行特定编程的情况下，给予计算机学习能力的领域。Samuel 的定义可以回溯到 50 年代，他编写了一个西洋棋程序。这程序神奇之处在于，编程者自己并不是个下棋高手。但因为他太菜了，于是就通过编程，让西洋棋 程序自己跟自己下了上万盘棋。通过观察哪种布局（棋盘位置）会赢，哪种布局会输，久而久之，这西洋棋程序明白了什么是好的布局，什么样是坏的布局。然后就牛逼大发了，程序通过学习后，玩西洋棋的水平超过了 Samuel。这绝对是令人注目的成果。 尽管编写者自己是个菜鸟，但因为计算机有着足够的耐心，去下上万盘的棋，没有人有这耐心去下这么多盘棋。通过这些练习，计算机获得无比丰富的经验，于是渐渐成为了比 Samuel 更厉害的西洋棋手。上述是个有点不正式的定义，也比较古老。另一个年代近一点 的定义，由 Tom Mitchell 提出，来自卡内基梅隆大学，Tom 定义的机器学习是，一个好的学习问题定义如下，他说，一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。我认为经验 e 就是程序上万次的自我练习的经验而任务 t 就是下棋。性能度量值 p 呢，就是它在与一 些新的对手比赛时，赢得比赛的概率。\n在这些视频中，除了我教你的内容以外，我偶尔会问你一个问题，确保你对内容有所理 解。说曹操，曹操到，顶部是 Tom Mitchell 的机器学习的定义，我们假设您的电子邮件程序会观察收到的邮件是否被你标记为垃圾邮件。在这种 Email 客户端中，你点击“垃圾邮件”按钮，报告某些 email 为垃圾邮件，不会影响别的邮件。基于被标记为垃圾的邮件，您的电 子邮件程序能更好地学习如何过滤垃圾邮件。请问，在这个设定中，任务 T 是什么？几秒钟 后，该视频将暂停。当它暂停时，您可以使用鼠标，选择这四个单选按钮中的一个，让我知道这四个，你所认为正确的选项。它可能是性能度量值 P。所以，以性能度量值 P 为标准， 这个任务的性能，也就是这个任务 T 的系统性能，将在学习经验 E 后得到提高。\n本课中，我希望教你有关各种不同类型的学习算法。目前存在几种不同类型的学习算法。 主要的两种类型被我们称之为监督学习和无监督学习。在接下来的几个视频中，我会给出这些术语的定义。这里简单说两句，监督学习这个想法是指，我们将教计算机如何去完成任务， 而在无监督学习中，我们打算让它自己进行学习。如果对这两个术语仍一头雾水，请不要担心，在后面的两个视频中，我会具体介绍这两种学习算法。此外你将听到诸如，强化学习和 推荐系统等各种术语。这些都是机器学习算法的一员，以后我们都将介绍到，但学习算法最常用两个类型就是监督学习、无监督学习。我会在接下来的两个视频中给出它们的定义。本课中，我们将花费最多的精力来讨论这两种学习算法。而另一个会花费大量时间的任务是了解应用学习算法的实用建议。\n我非常注重这部分内容，实际上，就这些内容而言我不知道还有哪所大学会介绍到。给你讲授学习算法就好像给你一套工具，相比于提供工具，可能更重要的，是教你如何使用这些工具。我喜欢把这比喻成学习当木匠。想象一下，某人教你如何成为一名木匠，说这是锤子，这是螺丝刀，锯子，祝你好运，再见。这种教法不好，不是吗？你拥有这些工具，但更重要的是，你要学会如何恰当地使用这些工具。会用与不会用的人之间，存在着鸿沟。尤其是知道如何使用这些机器学习算法的，与那些不知道如何使用的人。在硅谷我住的地方，当我走访不同的公司，即使是最顶尖的公司，很多时候我都看到人们试图将机器学习算法应用 于某些问题。有时他们甚至已经为此花了六个月之久。但当我看着他们所忙碌的事情时，我想说，哎呀，我本来可以在六个月前就告诉他们，他们应该采取一种学习算法，稍加修改进行使用，然后成功的机会绝对会高得多所以在本课中，我们要花很多时间来探讨，如果你真 的试图开发机器学习系统，探讨如何做出最好的实践类型决策，才能决定你的方式来构建你 的系统，这样做的话，当你运用学习算法时，就不太容易变成那些为寻找一个解决方案花费6个月之久的人们的中一员。他们可能已经有了大体的框架，只是没法正确的工作于是这就浪费了六个月的时间。所以我会花很多时间来教你这些机器学习、人工智能的最佳实践以及 如何让它们工作，我们该如何去做，硅谷和世界各地最优秀的人是怎样做的。我希望能帮你 成为最优秀的人才，通过了解如何设计和构建机器学习和人工智能系统。这就是机器学习，这些都是我希望讲授的主题。在下一个视频里，我会定义什么是监督学习，什么是无监督学习。此外，探讨何时使用二者。\n1.3  监督学习\n在这段视频中，我要定义可能是最常见一种机器学习问题：那就是监督学习。我将在后面正式定义监督学习。\n我们用一个例子介绍什么是监督学习把正式的定义放在后面介绍。假如说你想预测房价。 前阵子，一个学生从波特兰俄勒冈州的研究所收集了一些房价的数据。你把这些数据画\n出来，看起来是这个样子：横轴表示房子的面积，单位是平方英尺，纵轴表示房价，单位是千美元。那基于这组数据，假如你有一个朋友，他有一套 750 平方英尺房子，现在他希望把房子卖掉，他想知道这房子能卖多少钱。那么关于这个问题，机器学习算法将会怎么帮助你呢？\n我们应用学习算法，可以在这组数据中画一条直线，或者换句话说，拟合一条直线，根据这条线我们可以推测出，这套房子可能卖$150, 000，当然这不是唯一的算法。可能还有更好的，比如我们不用直线拟合这些数据，用二次方程去拟合可能效果会更好。根据二次方程的曲线，我们可以从这个点推测出，这套房子能卖接近$200, 000。稍后我们将讨论如何选择学习算法，如何决定用直线还是二次方程来拟合。两个方案中有一个能让你朋友的房子出售得更合理。这些都是学习算法里面很好的例子。以上就是监督学习的例子。\n可以看出，监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案” 组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价 格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的 价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格。 一般房子的价格会记到美分，所以房价实际上是一系列离散的值 但是我们通常又把房价看成实数，看成是标量，所以又把它看成一个连续的数值。\n回归这个词的意思是，我们在试着推测出这一系列连续值属性。 我再举另外一个监督学习的例子。我和一些朋友之前研究过这个。假设说你想通过查看病历来推测乳腺癌良性与否，假如有人检测出乳腺肿瘤，恶性肿瘤有害并且十分危险，而良性的肿瘤危害就没那么大，所以人们显然会很在意这个问题。\n让我们来看一组数据：这个数据集中，横轴表示肿瘤的大小，纵轴上，我标出1和0表示是或者不是恶性肿瘤。我们之前见过的肿瘤，如果是恶性则记为 1，不是恶性，或者说良 性记为 0。我有 5 个良性肿瘤样本，在 1 的位置有 5 个恶性肿瘤样本。现在我们有一个朋友很不幸 检查出乳腺肿瘤。假设说她的肿瘤大概这么大，那么机器学习的问题就在于，你能否估算出 肿瘤是恶性的或是良性的概率。用术语来讲，这是一个分类问题。\n分类指的是，我们试着推测出离散的输出值：0 或 1 良性或恶性，而事实上在分类问题 中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出 0 1 2 3  0 代表良性，1 表示第一类乳腺癌，2 表示第二类癌症，3 表示第三类，但这也是分类问 题。因为这几个离散的输出分别对应良性，第一类第二类或者第三类癌症，在分类问题中我们可以用另一种方式绘制这些数据点。 现在我用不同的符号来表示这些数据。既然我们把肿瘤的尺寸看做区分恶性或良性的特征，那么我可以这么画，我用不同的符号来表示良性和恶性肿瘤。或者说是负样本和正样本 现在我们不全部画 X，良性的肿瘤改成用 O 表示，恶性的继续用 X 表示。来预测肿瘤的恶性与否。\n在其它一些机器学习问题中，可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，我们通常有更多的特征，我朋友 研究这个问题时，通常采用这些特征，比如肿块密度，肿瘤细胞尺寸的一致性和形状的一致性等等，还有一些其他的特征。这就是我们即将学到最有趣的学习算法之一。那种算法不仅能处理 2 种 3 种或 5 种特征，即使有无限多种特征都可以处理。\n上图中，我列举了总共 5 种不同的特征，坐标轴上的两种和右边的 3 种，但是在一些学习问题中，你希望不只用 3 种或 5 种特征。相反，你想用无限多种特征，好让你的算法可以 利用大量的特征，或者说线索来做推测。那你怎么处理无限多个特征，甚至怎么存储这些特 征都存在问题，你电脑的内存肯定不够用。我们以后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。想象一下，我没有写下这两种和右边的三种特征，而是在一个无限长的列表里面，一直写一直写不停的写，写下无限多个特征， 事实上，我们能用算法来处理它们。\n现在来回顾一下，这节课我们介绍了监督学习。其基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。 我们还介绍了回归问题，即通过回归来推出一个连续的输出，之后我们介绍了分类问题，其 目标是推出一组离散的结果。\n现在来个小测验：假设你经营着一家公司，你想开发学习算法来处理这两个问题：\n1. 你有一大批同样的货物，想象一下，你有上千件一模一样的货物等待出售，这时你想预测接下来的三个月能卖多少件？\n2. 你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，你要判断它们是否曾经被盗过？\n那这两个问题，它们属于分类问题、还是回归问题? 问题一是一个回归问题，因为你知道，如果我有数千件货物，我会把它看成一个实数，一个连续的值。因此卖出的物品数，也是一个连续的值。 问题二是一个分类问题，因为我会把预测的值，用 0 来表示账户未被盗，用 1 表示账户曾经被盗过。所以我们根据账号是否被盗过，把它们定为 0 或 1，然后用算法推测一个账号是 0 还是 1，因为只有少数的离散值，所以我把它归为分类问题。 以上就是监督学习的内容。\n1.4  无监督学习\n本次视频中，我们将介绍第二种主要的机器学习问题。叫做无监督学习。\n上个视频中，已经介绍了监督学习。回想当时的数据集，如图表所示，这个数据集中每 条数据都已经标明是阴性或阳性，即是良性或恶性肿瘤。所以，对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案，是良性或恶性了。\n在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子， 即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能 从数据中找到某种结构吗？针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。 这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。事实证明，它能被用在很多地方。\n聚类应用的一个例子就是在谷歌新闻中。如果你以前从来没见过它，你可以到这个 URL 网址 news.google.com 去看看。谷歌新闻每天都在，收集非常多，非常多的网络的新闻内容。 它再将这些新闻分组，组成有关联的新闻。所以谷歌新闻做的就是搜索非常多的新闻事件，自动地把它们聚类到一起。所以，这些新闻事件全是同一主题的，所以显示到一起。 事实证明，聚类算法和无监督学习算法同样还用在很多其它的问题上。\n其中就有基因学的理解应用。一个 DNA 微观数据的例子。基本思想是输入一组不同个 体，对其中的每个个体，你要分析出它们是否有一个特定的基因。技术上，你要分析多少特定基因已经表达。所以这些颜色，红，绿，灰等等颜色，这些颜色展示了相应的程度，即不 同的个体是否有着一个特定的基因。你能做的就是运行一个聚类算法，把个体聚类到不同的类或不同类型的组（人）…… 所以这个就是无监督学习，因为我们没有提前告知算法一些信息，比如，这是第一类的人，那些是第二类的人，还有第三类，等等。我们只是说，是的，这是有一堆数据。我不知 道数据里面有什么。我不知道谁是什么类型。我甚至不知道人们有哪些不同的类型，这些类 型又是什么。但你能自动地找到数据中的结构吗？就是说你要自动地聚类那些个体到各个 类，我没法提前知道哪些是哪些。因为我们没有给算法正确答案来回应数据集中的数据，所以这就是无监督学习。\n无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发 email 的，或是你 Facebook 的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所 有人？还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。 我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、 有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一 种。\n我现在告诉你们另一种。我先来介绍鸡尾酒宴问题。嗯，你参加过鸡尾酒宴吧？你可以想像下，有个宴会房间里满是人，全部坐着，都在聊天，这么多人同时在聊天，声音彼此重 叠，因为每个人都在说话，同一时间都在说话，你几乎听不到你面前那人的声音。所以，可 能在一个这样的鸡尾酒宴中的两个人，他俩同时都在说话，假设现在是在个有些小的鸡尾酒 宴中。我们放两个麦克风在房间中，因为这些麦克风在两个地方，离说话人的距离不同每个麦克风记录下不同的声音，虽然是同样的两个说话人。听起来像是两份录音被叠加到一起， 或是被归结到一起，产生了我们现在的这些录音。另外，这个算法还会区分出两个音频资源， 这两个可以合成或合并成之前的录音，实际上，鸡尾酒算法的第一个输出结果是： 1，2，3，4，5，6，7，8，9，10,所以，已经把英语的声音从录音中分离出来了。第二个输出是这样：1，2，3，4，5，6，7，8，9，10。\n看看这个无监督学习算法，实现这个得要多么的复杂，是吧？它似乎是这样，为了构建这个应用，完成这个音频处理似乎需要你去写大量的代码或链接到一堆的合成器 JAVA 库，处理音频的库，看上去绝对是个复杂的程序，去完成这个从音频中分离出音频。事实上，这 个算法对应你刚才知道的那个问题的算法可以就用一行代码来完成。\n就是这里展示的代码：[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');\n研究人员花费了大量时间才最终实现这行代码。我不是说这个是简单的问题，但它证明 了，当你使用正确的编程环境，许多学习算法是相当短的程序。所以，这也是为什么在本课 中，我们打算使用 Octave 编程环境。Octave,是免费的开源软件，使用一个像 Octave 或 Matlab 的工具，许多学习算法变得只有几行代码就可实现。\n后面，我会教你们一点关于如何使用 Octave 的知识，你就可以用 Octave 来实现一些算 法了。或者，如果你有 Matlab（盗版？），你也可以用 Matlab。事实上，在硅谷里，对大量 机器学习算法，我们第一步就是建原型，在 Octave 建软件原型，因为软件在 Octave 中可以 令人难以置信地、快速地实现这些学习算法。这里的这些函数比如 SVM（支持向量机）函数， 奇异值分解，Octave 里已经建好了。如果你试图完成这个工作，但借助 C++或 JAVA 的话， 你会需要很多很多行的代码，并链接复杂的 C++或 Java 库。所以，你可以实现这些算法， 借助 C++或 Java 或 Python，它只是用这些语言来实现会更加复杂。\n我已经见到，在我教机器学习将近十年后的现在，发现学习可以更加高速，如果使用 Octave 作为编程环境，如果使用 Octave 作为学习工具，以及作为原型工具，它会让你对学习算法的学习和建原型快上许多。\n事实上，许多人在大硅谷的公司里做的其实就是，使用一种工具像 Octave 来做第一步的学习算法的原型搭建，只有在你已经让它工作后，你才移植它到 C++或 Java 或别的语言。 事实证明，这样做通常可以让你的算法运行得比直接用 C++实现更快，所以，我知道，作为 一名指导者，我必须说“相信我”，但对你们中从未使用过 Octave 这种编程环境的人，我还是要告诉你们这一点一定要相信我，我想，对你们而言，我认为你们的时间，你们的开发时间是最有价值的资源。我已经见过很多人这样做了，我把你看作是机器学习研究员，或机器学习开发人员，想更加高产的话，你要学会使用这个原型工具，开始使用 Octave。\n最后，总结下本视频内容，我有个简短的复习题给你们。\n我们介绍了无监督学习，它是学习策略，交给算法大量的数据，并让算法为我们从数据中找出某种结构。\n好的，希望你们还记得垃圾邮件问题。如果你有标记好的数据，区别好是垃圾还是非垃圾邮件，我们把这个当作监督学习问题。\n新闻事件分类的例子，就是那个谷歌新闻的例子，我们在本视频中有见到了，我们看到，可以用一个聚类算法来聚类这些文章到一起，所以是无监督学习。 细分市场的例子，我在更早一点的时间讲过，你可以当作无监督学习问题，因为我只是拿到算法数据，再让算法去自动地发现细分市场。 最后一个例子，糖尿病，这个其实就像是我们的乳腺癌，上个视频里的。只是替换了好、坏肿瘤，良性、恶性肿瘤，我们改用糖尿病或没病。所以我们把这个当作监督学习，我们能够解决它，作为一个监督学习问题，就像我们在乳腺癌数据中做的一样。 好了，以上就是无监督学习的视频内容，在下一个视频中，我们将深入探究特定的学习算法，开始介绍这些算法是如何工作的，和我们还有你如何来实现它们\nG\nM\nT\n檢測語言\n阿尔巴尼亚语\n阿拉伯语\n阿塞拜疆语\n爱尔兰语\n爱沙尼亚语\n巴斯克语\n白俄罗斯语\n保加利亚语\n冰岛语\n波兰语\n波斯尼亚语\n波斯语\n布尔语(南非荷兰语)\n丹麦语\n德语\n俄语\n法语\n菲律宾语\n芬兰语\n高棉语\n格鲁吉亚语\n古吉拉特语\n哈萨克语\n海地克里奥尔语\n韩语\n豪萨语\n荷兰语\n加利西亚语\n加泰罗尼亚语\n捷克语\n卡纳达语\n克罗地亚语\n拉丁语\n拉脱维亚语\n老挝语\n立陶宛语\n罗马尼亚语\n马尔加什语\n马耳他语\n马拉地语\n马拉雅拉姆语\n马来语\n马其顿语\n毛利语\n蒙古语\n孟加拉语\n缅甸语\n苗语\n南非祖鲁语\n尼泊尔语\n挪威语\n旁遮普语\n葡萄牙语\n齐切瓦语\n日语\n瑞典语\n塞尔维亚语\n塞索托语\n僧伽罗语\n世界语\n斯洛伐克语\n斯洛文尼亚语\n斯瓦希里语\n宿务语\n索马里语\n塔吉克语\n泰卢固语\n泰米尔语\n泰语\n土耳其语\n威尔士语\n乌尔都语\n乌克兰语\n乌兹别克语\n希伯来语\n希腊语\n西班牙语\n匈牙利语\n亚美尼亚语\n伊博语\n意大利语\n意第绪语\n印地语\n印尼巽他语\n印尼语\n印尼爪哇语\n英语\n约鲁巴语\n越南语\n中文简体\n中文繁体\n阿尔巴尼亚语\n阿拉伯语\n阿塞拜疆语\n爱尔兰语\n爱沙尼亚语\n巴斯克语\n白俄罗斯语\n保加利亚语\n冰岛语\n波兰语\n波斯尼亚语\n波斯语\n布尔语(南非荷兰语)\n丹麦语\n德语\n俄语\n法语\n菲律宾语\n芬兰语\n高棉语\n格鲁吉亚语\n古吉拉特语\n哈萨克语\n海地克里奥尔语\n韩语\n豪萨语\n荷兰语\n加利西亚语\n加泰罗尼亚语\n捷克语\n卡纳达语\n克罗地亚语\n拉丁语\n拉脱维亚语\n老挝语\n立陶宛语\n罗马尼亚语\n马尔加什语\n马耳他语\n马拉地语\n马拉雅拉姆语\n马来语\n马其顿语\n毛利语\n蒙古语\n孟加拉语\n缅甸语\n苗语\n南非祖鲁语\n尼泊尔语\n挪威语\n旁遮普语\n葡萄牙语\n齐切瓦语\n日语\n瑞典语\n塞尔维亚语\n塞索托语\n僧伽罗语\n世界语\n斯洛伐克语\n斯洛文尼亚语\n斯瓦希里语\n宿务语\n索马里语\n塔吉克语\n泰卢固语\n泰米尔语\n泰语\n土耳其语\n威尔士语\n乌尔都语\n乌克兰语\n乌兹别克语\n希伯来语\n希腊语\n西班牙语\n匈牙利语\n亚美尼亚语\n伊博语\n意大利语\n意第绪语\n印地语\n印尼巽他语\n印尼语\n印尼爪哇语\n英语\n约鲁巴语\n越南语\n中文简体\n中文繁体\n語言功能限100個字符\n選項 : 歷史 : 幫助 : 反饋關閉"}
{"content2":"在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。\n本系列主要参考文献为维基百科的Matrix Caculas和张贤达的《矩阵分析与应用》。\n1. 矩阵向量求导引入\n在高等数学里面，我们已经学过了标量对标量的求导，比如标量$y$对标量$x$的求导，可以表示为$\\frac{\\partial y}{\\partial x}$。\n有些时候，我们会有一组标量$y_i,i=1,2,...,m$来对一个标量$x$的求导,那么我们会得到一组标量求导的结果：$$\\frac{\\partial y_i}{\\partial x}, i=1,2.,,,m$$\n如果我们把这组标量写成向量的形式，即得到维度为m的一个向量$\\mathbf{y}$对一个标量$x$的求导，那么结果也是一个m维的向量：$\\frac{\\partial \\mathbf{y}}{\\partial x}$\n可见，所谓向量对标量的求导，其实就是向量里的每个分量分别对标量求导，最后把求导的结果排列在一起，按一个向量表示而已。类似的结论也存在于标量对向量的求导，向量对向量的求导，向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导等。\n总而言之，所谓的向量矩阵求导本质上就是多元函数求导，仅仅是把把函数的自变量，因变量以及标量求导的结果排列成了向量矩阵的形式，方便表达与计算，更加简洁而已。\n为了便于描述，后面如果没有指明，则求导的自变量用$x$表示标量，$\\mathbf{x}$表示n维向量，$\\mathbf{X}$表示$m \\times n$维度的矩阵，求导的因变量用$y$表示标量，$\\mathbf{y}$表示m维向量，$\\mathbf{Y}$表示$p \\times q$维度的矩阵。\n2. 矩阵向量求导定义\n根据求导的自变量和因变量是标量，向量还是矩阵，我们有9种可能的矩阵求导定义，如下：\n自变量\\因变量\n标量$y$\n向量$\\mathbf{y}$\n矩阵$\\mathbf{Y}$\n标量$x$\n$\\frac{\\partial y}{\\partial x}$\n$\\frac{\\partial  \\mathbf{y}}{\\partial x}$\n$\\frac{\\partial  \\mathbf{Y}}{\\partial x}$\n向量$\\mathbf{x}$\n$\\frac{\\partial y}{\\partial \\mathbf{x}}$\n$\\frac{\\partial  \\mathbf{y}}{\\partial \\mathbf{x}}$\n$\\frac{\\partial  \\mathbf{Y}}{\\partial \\mathbf{x}}$\n矩阵$\\mathbf{X}$\n$\\frac{\\partial y}{\\partial \\mathbf{X}}$\n$\\frac{\\partial  \\mathbf{y}}{\\partial \\mathbf{X}}$\n$\\frac{\\partial  \\mathbf{Y}}{\\partial \\mathbf{X}}$\n这9种里面，标量对标量的求导高数里面就有，不需要我们单独讨论，在剩下的8种情况里面，我们先讨论上图中标量对向量或矩阵求导，向量或矩阵对标量求导，以及向量对向量求导这5种情况。另外三种向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导我们在后面再讲。\n现在我们回看第一节讲到的例子，维度为m的一个向量$\\mathbf{y}$对一个标量$x$的求导，那么结果也是一个m维的向量：$\\frac{\\partial \\mathbf{y}}{\\partial x}$。这是我们表格里面向量对标量求导的情况。这里有一个问题没有讲到，就是这个m维的求导结果排列成的m维向量到底应该是列向量还是行向量？\n这个问题的答案是：行向量或者列向量皆可！毕竟我们求导的本质只是把标量求导的结果排列起来，至于是按行排列还是按列排列都是可以的。但是这样也有问题，在我们机器学习算法法优化过程中，如果行向量或者列向量随便写，那么结果就不唯一，乱套了。\n为了解决这个问题，我们引入求导布局的概念。\n3. 矩阵向量求导布局\n为了解决矩阵向量求导的结果不唯一，我们引入求导布局。最基本的求导布局有两个：分子布局(numerator layout)和分母布局(denominator layout )。\n对于分子布局来说，我们求导结果的维度以分子为主，比如对于我们上面对标量求导的例子，结果的维度和分子的维度是一致的。也就是说，如果向量$\\mathbf{y}$是一个m维的列向量，那么求导结果$\\frac{\\partial \\mathbf{y}}{\\partial x}$也是一个m维列向量。如果如果向量$\\mathbf{y}$是一个m维的行向量，那么求导结果$\\frac{\\partial \\mathbf{y}}{\\partial x}$也是一个m维行向量。\n对于分母布局来说，我们求导结果的维度以分母为主，比如对于我们上面对标量求导的例子，如果向量$\\mathbf{y}$是一个m维的列向量，那么求导结果$\\frac{\\partial \\mathbf{y}}{\\partial x}$是一个m维行向量。如果如果向量$\\mathbf{y}$是一个m维的行向量，那么求导结果$\\frac{\\partial \\mathbf{y}}{\\partial x}$是一个m维的列向量向量。\n可见，对于分子布局和分母布局的结果来说，两者相差一个转置。\n再举一个例子，标量$y$对矩阵$ \\mathbf{X}$求导，那么如果按分母布局，则求导结果的维度和矩阵$X$的维度$m \\times n$是一致的。如果是分子布局，则求导结果的维度为$n \\times m$。\n这样，对于标量对向量或者矩阵求导，向量或者矩阵对标量求导这4种情况，对应的分子布局和分母布局的排列方式已经确定了。\n稍微麻烦点的是向量对向量的求导，本文只讨论列向量对列向量的求导，其他的行向量求导只是差一个转置而已。比如m维列向量$\\mathbf{y}$对n维列向量$\\mathbf{x}$求导。它的求导结果在分子布局和分母布局各是什么呢？对于这2个向量求导，那么一共有$mn$个标量对标量的求导。求导的结果一般是排列为一个矩阵。如果是分子布局，则矩阵的第一个维度以分子为准，即结果是一个$m \\times n$的矩阵，如下：$$ \\frac{\\partial  \\mathbf{y}}{\\partial \\mathbf{x}} = \\left( \\begin{array}{ccc} \\frac{\\partial y_1}{\\partial x_1}& \\frac{\\partial y_1}{\\partial x_2}& \\ldots & \\frac{\\partial y_1}{\\partial x_n}\\\\  \\frac{\\partial y_2}{\\partial x_1}& \\frac{\\partial y_2}{\\partial x_2} & \\ldots & \\frac{\\partial y_2}{\\partial x_n}\\\\   \\vdots&  \\vdots &  \\ddots & \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1}& \\frac{\\partial y_m}{\\partial x_2} & \\ldots & \\frac{\\partial y_m}{\\partial x_n}  \\end{array} \\right)$$\n上边这个按分子布局的向量对向量求导的结果矩阵，我们一般叫做雅克比 (Jacobian)矩阵。有的资料上会使用$ \\frac{\\partial  \\mathbf{y}}{\\partial \\mathbf{x^T}}$来定义雅克比矩阵，意义是一样的。\n如果是按分母布局，则求导的结果矩阵的第一维度会以分母为准，即结果是一个$n \\times m$的矩阵，如下：$$ \\frac{\\partial  \\mathbf{y}}{\\partial \\mathbf{x}} = \\left( \\begin{array}{ccc} \\frac{\\partial y_1}{\\partial x_1}& \\frac{\\partial y_2}{\\partial x_1}& \\ldots & \\frac{\\partial y_m}{\\partial x_1}\\\\  \\frac{\\partial y_1}{\\partial x_2}& \\frac{\\partial y_2}{\\partial x_2} & \\ldots & \\frac{\\partial y_m}{\\partial x_2}\\\\   \\vdots&  \\vdots &  \\ddots & \\vdots \\\\ \\frac{\\partial y_1}{\\partial x_n}& \\frac{\\partial y_2}{\\partial x_n} & \\ldots & \\frac{\\partial y_m}{\\partial x_n}  \\end{array} \\right)$$\n上边这个按分母布局的向量对向量求导的结果矩阵，我们一般叫做梯度矩阵。有的资料上会使用$ \\frac{\\partial  \\mathbf{y^T}}{\\partial \\mathbf{x}}$来定义梯度矩阵，意义是一样的。\n有了布局的概念，我们对于上面5种求导类型，可以各选择一种布局来求导。但是对于某一种求导类型，不能同时使用分子布局和分母布局求导。\n但是在机器学习算法原理的资料推导里，我们并没有看到说正在使用什么布局，也就是说布局被隐含了，这就需要自己去推演，比较麻烦。但是一般来说我们会使用一种叫混合布局的思路，即如果是向量或者矩阵对标量求导，则使用分子布局为准，如果是标量对向量或者矩阵求导，则以分母布局为准。对于向量对对向量求导，有些分歧，我的所有文章中会以分子布局的雅克比矩阵为主。\n具体总结如下：\n自变量\\因变量\n标量$y$\n列向量$\\mathbf{y}$\n矩阵$\\mathbf{Y}$\n标量$x$\n/\n$\\frac{\\partial  \\mathbf{y}}{\\partial x}$\n分子布局：m维列向量（默认布局）\n分母布局：m维行向量\n$\\frac{\\partial  \\mathbf{Y}}{\\partial x}$\n分子布局：$p \\times q$矩阵（默认布局）\n分母布局：$q \\times p$矩阵\n列向量$\\mathbf{x}$\n$\\frac{\\partial y}{\\partial \\mathbf{x}}$\n分子布局：n维行向量\n分母布局：n维列向量（默认布局）\n$\\frac{\\partial  \\mathbf{y}}{\\partial \\mathbf{x}}$\n分子布局：$m \\times n$雅克比矩阵（默认布局）\n分母布局：$n \\times m$梯度矩阵\n/\n矩阵$\\mathbf{X}$\n$\\frac{\\partial y}{\\partial \\mathbf{X}}$\n分子布局：$n \\times m$矩阵\n分母布局：$m \\times n$矩阵（默认布局）\n/\n/\n4. 矩阵向量求导基础总结\n有了矩阵向量求导的定义和默认布局，我们后续就可以对上表中的5种矩阵向量求导过程进行一些常见的求导推导总结求导方法，并讨论向量求导的链式法则。\n（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）"}
{"content2":"在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。\n本文的标量对向量的求导，以及标量对矩阵的求导使用分母布局。如果遇到其他资料求导结果不同，请先确认布局是否一样。\n1. 矩阵微分\n在高数里面我们学习过标量的导数和微分，他们之间有这样的关系：$df =f'(x)dx$。如果是多变量的情况，则微分可以写成：$$df=\\sum\\limits_{i=1}^n\\frac{\\partial f}{\\partial x_i}dx_i = (\\frac{\\partial f}{\\partial \\mathbf{x}})^Td\\mathbf{x}$$\n从上次我们可以发现标量对向量的求导和它的向量微分有一个转置的关系。\n现在我们再推广到矩阵。对于矩阵微分，我们的定义为：$$df=\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n\\frac{\\partial f}{\\partial X_{ij}}dX_{ij} = tr((\\frac{\\partial f}{\\partial \\mathbf{X}})^Td\\mathbf{X})$$\n其中第二步使用了矩阵迹的性质，即迹函数等于主对角线的和。即$$tr(A^TB) = \\sum\\limits_{i,j}A_{ij}B_{ij}$$\n从上面矩阵微分的式子，我们可以看到矩阵微分和它的导数也有一个转置的关系，不过在外面套了一个迹函数而已。由于标量的迹函数就是它本身，那么矩阵微分和向量微分可以统一表示，即：$$df= tr((\\frac{\\partial f}{\\partial \\mathbf{X}})^Td\\mathbf{X})\\;\\; \\;df= tr((\\frac{\\partial f}{\\partial \\mathbf{x}})^Td\\mathbf{x})$$\n2. 矩阵微分的性质\n我们在讨论如何使用矩阵微分来求导前，先看看矩阵微分的性质：\n1）微分加减法：$d(X+Y) =dX+dY, d(X-Y) =dX-dY$\n2)  微分乘法：$d(XY) =(dX)Y + X(dY)$\n3)  微分转置：$d(X^T) =(dX)^T$\n4)  微分的迹：$dtr(X) =tr(dX)$\n5)  微分哈达马乘积： $d(X \\odot Y) = X\\odot dY + dX \\odot Y$\n6) 逐元素求导：$d \\sigma(X) =\\sigma'(X) \\odot dX$\n7) 逆矩阵微分：$d X^{-1}= -X^{-1}dXX^{-1}$\n8) 行列式微分：$d |X|= |X|tr(X^{-1}dX)$\n有了这些性质，我们再来看看如何由矩阵微分来求导数。\n3. 使用微分法求解矩阵向量求导\n由于第一节我们已经得到了矩阵微分和导数关系，现在我们就来使用微分法求解矩阵向量求导。\n若标量函数$f$是矩阵$X$经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对$f$求微分，再使用迹函数技巧给$df$套上迹并将其它项交换至$dX$左侧,那么对于迹函数里面在$dX$左边的部分，我们只需要加一个转置就可以得到导数了。\n这里需要用到的迹函数的技巧主要有这么几个：\n1)  标量的迹等于自己：$tr(x) =x$\n2)  转置不变：$tr(A^T) =tr(A)$\n3)  交换率：$tr(AB) =tr(BA)$,需要满足$A,B^T$同维度。\n4)  加减法：$tr(X+Y) =tr(X)+tr(Y), tr(X-Y) =tr(X)-tr(Y)$\n5) 矩阵乘法和迹交换：$tr((A\\odot B)^TC)= tr(A^T(B \\odot C))$,需要满足$A,B,C$同维度。\n我们先看第一个例子，我们使用上一篇定义法中的一个求导问题：$$y=\\mathbf{a}^T\\mathbf{X}\\mathbf{b}, \\frac{\\partial y}{\\partial \\mathbf{X}}$$\n首先，我们使用微分乘法的性质对$f$求微分，得到：$$dy = d\\mathbf{a}^T\\mathbf{X}\\mathbf{b} + \\mathbf{a}^Td\\mathbf{X}\\mathbf{b} + \\mathbf{a}^T\\mathbf{X}d\\mathbf{b} = \\mathbf{a}^Td\\mathbf{X}\\mathbf{b}$$\n第二步，就是两边套上迹函数，即：$$dy =tr(dy) = tr(\\mathbf{a}^Td\\mathbf{X}\\mathbf{b}) = tr(\\mathbf{b}\\mathbf{a}^Td\\mathbf{X})$$\n其中第一到第二步使用了上面迹函数性质1，第三步到第四步用到了上面迹函数的性质3.\n根据我们矩阵导数和微分的定义，迹函数里面在$dX$左边的部分$\\mathbf{b}\\mathbf{a}^T$,加上一个转置即为我们要求的导数，即：$$\\frac{\\partial f}{\\partial \\mathbf{X}} = (\\mathbf{b}\\mathbf{a}^T)^T =ab^T$$\n以上就是微分法的基本流程，先求微分再做迹函数变换，最后得到求导结果。比起定义法，我们现在不需要去对矩阵中的单个标量进行求导了。\n再来看看$$y=\\mathbf{a}^Texp(\\mathbf{X}\\mathbf{b}), \\frac{\\partial y}{\\partial \\mathbf{X}}$$\n$$dy =tr(dy) = tr(\\mathbf{a}^Tdexp(\\mathbf{X}\\mathbf{b})) = tr(\\mathbf{a}^T (exp(\\mathbf{X}\\mathbf{b}) \\odot d(\\mathbf{X}\\mathbf{b}))) = tr((\\mathbf{a}  \\odot exp(\\mathbf{X}\\mathbf{b}) )^T d\\mathbf{X}\\mathbf{b}) =  tr(\\mathbf{b}(\\mathbf{a}  \\odot exp(\\mathbf{X}\\mathbf{b}) )^T d\\mathbf{X})  $$\n其中第三步到第4步使用了上面迹函数的性质5. 这样我们的求导结果为：$$\\frac{\\partial y}{\\partial \\mathbf{X}} =(\\mathbf{a}  \\odot exp(\\mathbf{X}\\mathbf{b}) )b^T $$\n以上就是微分法的基本思路。\n4. 迹函数对向量矩阵求导\n由于微分法使用了迹函数的技巧，那么迹函数对对向量矩阵求导这一大类问题，使用微分法是最简单直接的。下面给出一些常见的迹函数的求导过程，也顺便给大家熟练掌握微分法的技巧。\n首先是$\\frac{\\partial tr(AB)}{\\partial A} = B^T, \\frac{\\partial tr(AB)}{\\partial B} =A^T$,这个直接根据矩阵微分的定义即可得到。\n再来看看$\\frac{\\partial tr(W^TAW)}{\\partial W}$:$$d(tr(W^TAW)) = tr(dW^TAW +W^TAdW) = tr(dW^TAW)+tr(W^TAdW) = tr((dW)^TAW) + tr(W^TAdW) = tr(W^TA^TdW) +  tr(W^TAdW) = tr(W^T(A+A^T)dW) $$\n因此可以得到：$$\\frac{\\partial tr(W^TAW)}{\\partial W} = (A+A^T)W$$\n最后来个更加复杂的迹函数求导：$\\frac{\\partial tr(B^TX^TCXB)}{\\partial X} $: $$d(tr(B^TX^TCXB)) = tr(B^TdX^TCXB) + tr(B^TX^TCdXB) = tr((dX)^TCXBB^T) + tr(BB^TX^TCdX) = tr(BB^TX^TC^TdX) + tr(BB^TX^TCdX) = tr((BB^TX^TC^T + BB^TX^TC)dX)$$\n因此可以得到：$$\\frac{\\partial tr(B^TX^TCXB)}{\\partial X}= (C+C^T)XBB^T$$\n5. 微分法求导小结\n使用矩阵微分，可以在不对向量或矩阵中的某一元素单独求导再拼接，因此会比较方便，当然熟练使用的前提是对上面矩阵微分的性质，以及迹函数的性质熟练运用。\n还有一些场景，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。如果我们可以利用一些常用的简单求导结果，再使用链式求导法则，则会非常的方便。因此下一篇我们讨论向量矩阵求导的链式法则。\n（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）"}
{"content2":"2018年小结和2019年展望\n年总结\n2018年还有两个小时就过去了，在这跨年之夜，有很多人选择看晚会，出去吃喝玩乐。我却选择了宅在家里，认真的回想了一下2018自己的所作所为，算是给自己一个交代。先说一下2018值得纪念的事情吧。\n1. 开始跑步，从三月份的开始器材锻炼，并坚持到现在。\n2. 五月份买房子。\n3. 八月份开始认真学习英语（单词，口语，听力），以前也学，只是打酱油的学。但坚持的不是很好。\n4. 九月搭建了个人博客，并坚持写博客。http://futuretechx.com/\n5. 十月开始学习python全栈开发，写了一些博客。Python开发系列\n6. 每周末看了一些电影，几乎每周末一部，当然还看了很多烂片，以下是自认为还不错的电影，推荐：\n《唐人街侦探2》，《红海行动》，《初恋这件小事》，《我不是药神》，《肖申克的救赎》，《百鸟朝凤》，《你的名字》，《放牛班的春天》，《钱学森》，《建国大业》，《机器总动员》，《天才少女》，《西虹市首富》，《霸王别姬》，《V字仇杀队》，《守护者联盟》，《让子弹飞》，《战狼2》，《追梦赤子心》等\n7. 有时间读了一些书：\n比较推荐的是：《原则》，《明朝那些事》，《三体》，《python爬虫开发与项目实践》，《价值投资》，《白鹿原》，《时间简史》，《未来简史》等\n微信读书\n8. 12月一次长跑\n9 戒掉游戏\n2018年有欢乐，有收获，也有低落，也有不如意，但终将过去。\n2019年即将到来，在这里立下Flag，希望来年能实现所有的计划。\n1 早起（每天不晚于6：30起床，一年不低于300天）\n2 运动（每天五十分，一年不低于300天）\n3 英语 (每天半小时，一年不低于300天)\n4 看50部经典电影 （一周1-2部）\n5 看十本书，写读后感 （一月一本）\n6 继续完成python全栈开发，深入学习pythohn（爬虫，数据分析，人工智能）\n7 转行到python开发\n8 在github上开启属于自己的project.\n9 多陪家人，少玩游戏，多出去走走。\n10 考研\n2019Flag"}
{"content2":"想从事数据科学家的自我修炼（浪叫兽的书单）一年之内从零基础入门（以不抱大腿的姿势）拿下数据竞赛 一等奖 ，二等奖，三等奖\n携程赛的初练书单\n在科赛网站上，我第一次报名参加了比赛是，酒店未来30天产量预测，当时参加比赛是16年的7月中旬了。距今日的17年5月中旬，不到一年的时间，在科赛网，同样是在携程的出题下拿到了一个冠军一等奖\n进入正题把，我来写一下我的历史读书记录  和  自发学习过程：\n其实刚刚入门是因为想玩爬虫爬新闻做预测：\n入门读的书有：\nPYTHON自然语言处理中文翻译 NLTK 中文版\nquantmod-R中的金融分析包\nR数据导入和导出（包括RODBC）\nRODBC中文介绍\n上面可能也是一些介绍把\n了解了一下R语言，因为当初搞不定字符问题 就转战了Python爬虫，进入到了Python语言的学习\nPython数据分析基础教程：NumPy学习指南（第2版）\n然后找寻金融相关的资料\n11.金融时间序列分析  第3版\n依旧对R 不放心的时代\nR语言核心技术手册（第2版）\n自己动手写网络爬虫\n用Python写网络爬虫\n此时应该是在QQ群里找大鱼学的爬虫 scrapy  和 urllib ,后面发现requests + bs4  入门爬虫比较适合我，写了爬虫直接写个for 就把新闻爬到数据库中了。\n在学numpy的过程中加入到了pandas的学习群  和牛叫兽学习pd 此时牛叫兽推荐我看的书是\n利用Python进行数据分析\nPython for Data Analysis\n因为要作图展示 找了一些资料\nmatplotlib函数汇总\n用Python做科学计算\n还陆陆续续的找了 R 和 Python资料\n一种面向金融时间序列的趋势特征挖掘算法研究 （这个好像是论文）\n数据分析   R语言实战\n数据可视化实战：使用D3设计交互式图表\n其实这里穿插了学习flask  + echart 把股票趋势 还有talib的指标通过网页展示\n此时已经很牛叫兽创建了自己的机器学习群然后在开始补充数学 和机器学习知识和官网文档\nscikit-learn.user_guide_0.16.1\nMatplotlib\nscipy-ref-0.17.1\nnumpy-ref-1.10.1\npandas\nPattern_Classification_Duba中文版（模式分类）\n统计思维：程序员数学之概率统计\nPRML中文版全文\n2012.李航.统计学习方法\n机器学习实战(单页视图)\n接下来漫长的资料狂过程 找寻了很多资料\n社交网站的数据挖掘与分析(中文版)\nPython灰帽子 黑客与逆向工程师的Python编程之道\n集体智慧编程-python算法应用\n笨办法学 Python(第四版）\n[黑客攻防技术宝典Web实战篇].Dafydd.Stuttard.第2版\n机器学习实战(单页视图)\nPYTHON数据可视化编程实战\n开始玩携程赛了，就开始迷恋了机器学习了\n利用python进行线性回归\nMachine Learning for Hackers\nMachine Learning in Python\nscikit-learn-docs\n机器学习系统设计.python\nThink Bayes\n机器学习算法原理与编程实践\nThink Complexity(复杂性思考)\nPRML笔记-Notes on Pattern Recognition and Machine Learning\nNeural Network and Deep Learning\n数学模型 姜启源第四版电子书\n数学之美  第2版\n凸优化——影印版\n[概率图模型  原理与技术][（美）科勒，（以）弗里德曼著][北京：清华大学出版社]\n数学建模算法与程序（这本很重要，我看了挺多次的）\n由于我并没正式的读过大学 在对一群人的羡慕下 我找了一些简单的资料\n线性代数基础讲义（完整版）\n矩阵分析与应用 第2版\n《线性代数学习辅导与习题全解》同济第五版\n支持向量机导论\n《线性代数》同济第五版\n概率论与数理统计习题全解指南（浙大四版）\n概率论与数理统计辅导及习题精解浙大版_13350053\n数学分析(第二册)(周民强)\n数学分析(第一册)(周民强)\n数学分析3周民强\n泛函分析\n周志华—机器学习（这本是一起找到的）\n托马斯微积分（第10版）\n然后还有其它的资料\n缺失数据的统计处理\n缺失数据统计分析(第二版)\nEnsemble_Methods_Foundations_and_Algorithms\nLarge Scale Machine Learning with Python-Packt Publishing(2016)\nCcf用电赛的时候\n探索性数据分析+[美]David C.Hoaglin等.现代外国统计学优秀着作译丛\nR语言与统计分析\n统计学与R读书笔记(第六版)\nxgboost导读和实战\n经典算法大全\n数据之魅：基于开源工具的数据分析\nMATLAB数据分析与挖掘实战（其实当时我有一个队友mat搞matlab的）\n爱上统计学\n下面的书是想对特征分析有更深的了解，主要是波形里面的东西\n音频信号分类算法研究\n倾向值分析：统计方法与应用（美）郭申阳\n非平稳信号特征提取方法及其应用\n小波分析及其应用__孙延奎\nMATLAB6.5辅助小波分析与应用\n矩阵分析与应用(张贤达)\n奥本海默 信号与系统 中文版 (第二版)\nCCF 拿下了二等奖，发现自己想知道数据挖掘是干嘛的   在找了一些科普资料来学习\n数据挖掘：概念与技术（中文第三版）\n推荐系统实践\n随机过程（Sheldon M.Ross 著）\n随机过程 内容、方法与技巧\n复旦大学所用随机过程\n通信系统中随机过程的模型研究\n数据挖掘原理与算法  第3版\n不确定性多目标优化的数据挖掘理论及应用\n对了  不得不说  我也开始了对哲学的思考\n爱比克泰德论说集.王文华译.商务印书馆(2009)   （这本我现在还在读）\n在插播一下 我还对英语感兴趣了，因为苍老师  吴彦祖 动不动的就是英语资料（我内心是崩溃的）\n新东方•英语语法新思维\n在与yin书和老王争论规则的时候 还有加入到莫言的群里\n粗糙集理论与方法_10465281\nkeras1.0中文文档_带书签和红字链接版\n数据科学实战_高清非扫描版\n[O'Reilly：社交网站的数据挖掘与分析].(Mattbew.A.Russell).师蓉\n数据挖掘技术与工程实践\n程序员的数学\n管理海量数据-压缩、索引和查询 第2版\n设计模式之禅(第二版)\n基于机器学习的智能运维--裴丹\n书差不多了 不过还有一些接下来看的书\n信息论与编码 曹雪虹\n《数据挖掘：概念与技术》【韩家炜】原书第三版 中文版\n麦肯锡：机器的崛起  中国高管眼中的人工智能\n信息论、推理与学习算法  翻译版\nPython机器学习——预测分析核心算法\n思维的乐趣matrix67数学笔记\n线性代数的几何意义\n数据仓库\n自适应滤波器原理\n《线性代数及其应用》(美 第三版)（中文版）\n我敢说 如果数据科学家如果招数是降龙十八掌，我就会一掌而已\n因为这些书名都存在我的电脑里面\n还有更多的书名我就不列了\n在发一下我的博客搜索历程\n\n\n\n\n\n\n\n\n\n\n\n完了吗  ？？\n嗯 完了  我在说一个这圈子里面我心里的一个疙瘩把。\n其实我现在不想在玩算法比赛了，真的没劲玩了\n助学金比赛回来心就有点累了，加上国网信通产业集团给了我一份工作，从之前的月收入5000 翻了3倍，我其实还是挺知足的，电网待我很不错。\n但是在不想玩比赛的情况下，我连续被两个群踢了，在京东赛的群因为发现了leak  没和管理员说，然后直接在群里说这个题目有leak,希望有用leak提分的人出来说一下，然后被管理员踢了，还说我影响了比赛。（对我来说这个过程非常的恶心）\n第二次可能是我在群里说  机器学习的成果普遍没被企业接受\n还写了一个可能在天池群看来是一个误导新手的导图。把我踢了，被踢时是在和京东赛的管理员争执机器学习在股票预测上的真伪。\n附上我写的指导图\n就写到这里把\n对了 博客里面不好传图片\n我写的word 不知道怎么放\n博客园好像说我这不是原创"}
{"content2":"转载请注明出处：http://www.cnblogs.com/ymingjingr/p/4271742.html\n目录\n机器学习基石笔记1——在何时可以使用机器学习(1)\n机器学习基石笔记2——在何时可以使用机器学习(2)\n机器学习基石笔记3——在何时可以使用机器学习(3)(修改版)\n机器学习基石笔记4——在何时可以使用机器学习（4）\n机器学习基石笔记5——为什么机器可以学习（1）\n机器学习基石笔记6——为什么机器可以学习（2）\n机器学习基石笔记7——为什么机器可以学习（3）\n机器学习基石笔记8——为什么机器可以学习（4）\n机器学习基石笔记9——机器可以怎样学习（1）\n机器学习基石笔记10——机器可以怎样学习（2）\n机器学习基石笔记11——机器可以怎样学习（3）\n机器学习基石笔记12——机器可以怎样学习（4）\n机器学习基石笔记13——机器可以怎样学得更好（1）\n机器学习基石笔记14——机器可以怎样学得更好（2）\n机器学习基石笔记15——机器可以怎样学得更好（3）\n机器学习基石笔记16——机器可以怎样学得更好（4）\n十、Logistic Regression\n罗杰斯特回归（最常见到的翻译：Logistic回归）。\n10.1 Logistic Regression Problem\nLogistic回归问题。\n使用二元分类分析心脏病复发问题，其输出空间只含有两项{+1，-1}，分别表示复发和不发复发。在含有噪音的情况下，目标函数f可以使用目标分布P来表示，如公式10-1所示，此情形的机器学习流程图如-1所示。\n（公式10-1）\n-1 心脏病复发二元分类流程图\n但是通常情况下，不会确定的告知患者，心脏病一定会复发或者一定不会，而是以概率的方式告知患者复发的可能性，如-2所示，一位患者心脏病复发的可能性为80%。\n-2 以概率的形式表示复发可能性\n此种情况被称为软二元分类（soft binary classification），目标函数f的表达如公式10-2所示，其输出以概率的形式，因此在0~1之间。\n（公式10-2）\n面对如公式10-2的目标函数，理想的数据集D（输入加输出空间）应如-3所示。\n-3 理想的数据集D\n所有的输出都以概率的形式存在，如 ，用心脏病复发的例子来说明，一般病人只有心脏病发与没复发两种情况，而不可能在病历中记录他曾经的病发概率，现实中的训练数据应如-4所示。\n-4 实际训练数据\n可以将实际训练数据看做含有噪音的理想训练数据。\n问题是如何使用这些实际的训练数据以解决软二元分类的问题，即假设函数如何设计。\n首先回忆在之前的几章内容中提到的两种假设函数（二元分类和线性回归）中都具有的是哪部分？\n答案是求输入 各属性的加权总分数（score），（还记得第二章中用成绩分数来说明加权求和的意义吗？）可以使用公式10-3表示。\n（公式10-3）\n如何把该得分从在整个实数范围内转换成为一个0~1之间的值呢？此处就引出了本章的主题，logistic函数（logistic function）用表示。分数s越大风险越高，分数s越小风险越低。假设函数h如公式10-4所示，函数曲线的示意图如-5所示。\n（公式10-4）\n-5 logistic函数的示意图\n具体的logistic函数的数学表达式如公式10-5所示。\n（公式10-5）\n代入几个特殊的数值检验是否能将整个实数集上的得分映射到0~1之间，代入负无穷，得；代入0，得；代入正无穷，得。logistic函数完美的将整个实数集上的值映射到了0~1区间上。\n观察函数的图形，该函数是一个平滑（处处可微分），单调（monotonic）的S形（sigmoid）函数，因此又被称为sigmoid函数。\n通过logistic函数的数学表达式，重写软二元分类的假设函数表达，如公式10-6所示。\n（公式10-6）\n10.2 Logistic Regression Error\nLogistic回归错误。\n将logisitic回归与之前学习的二元分类和线性回归做一对比，如-7所示。\n-7 二元分类、线性回归与logistic回归的对比\n其中分数s是在每个假设函数中都会出现的，前两个学习模型的错误衡量分别对应着0/1错误和平方错误，而logistic回归所使用的err函数应如何表示则是本节要介绍的内容。\n从logistic回归的目标函数可以推导出公式10-7成立。\n（公式10-7）\n其中花括号上半部分不难理解，是将目标函数等式左右对调的结果，而下半部分的推导也很简单，因为+1与-1的几率相加需要等于1。\n假设存在一个数据集 ,则通过目标函数产生此种数据集样本的概率可以用公式10-8表示。\n(公式 10-8)\n就是各输入样本产生对应输出标记概率的连乘。而从公式10-7可知公式10-8可以写成公式10-9的形式。\n（公式10-9）\n但是函数f是未知的，已知的只有假设函数h，可不可以将假设函数h取代公式10-9中的f呢？如果这样做意味着什么？意味着假设函数h产生同样数据集样本D的可能性多大，在数学上又翻译成似然（likelihood），替代之后的公式如公式10-10所示。\n（公式10-10）\n假设假设函数h和未知函数f很接近（即err很小），那么h产生数据样本D的可能性或叫似然（likelihood）和f产生同样数据D的可能性（probability）也很接近。函数f既然产生了数据样本D，那么可以认为函数f产生该数据样本D的可能性很大。因此可以推断出最好的假设函数g，应该是似然最大的假设函数h，用公式10-11表示。\n（公式10-11）\n在当假设函数h使用公式10-6的logistic函数，可以得到如公式10-12的特殊性质。\n（公式10-12）\n因此公式10-10可以写成公式10-13。\n此处注意，计算最大的时，所有的对大小没有影响，因为所有的假设函数都会乘以同样的，即h的似然只与函数h对每个样本的连乘有关，如公式10-14。\n(公式10-14)\n其中表示标记，将标记代替正负号放进假设函数中使得整个式子更加简洁。寻找的是似然最大的假设函数h，因此可以将公式10-14代入寻找最大似然的公式中，并通过一连串的转换得到公式10-15。\n（假设函数h与加权向量w一一对应）\n（连乘公式不容易求解最大问题，因此求其对数，此处以自然对数e为底）\n（之前都是在求最小问题，因此将最大问题加上一个负号转成了最小问题，为了与以前的错误衡量类似，多成了一个。）\n（将代入表达式得出上述结果）\n（公式10-15）\n公式10-15中，这个错误函数称作交叉熵错误（cross-entropy error）。\n10.3 Gradient of Logistic Regression Error\nLogistic回归错误的梯度。\n推导出logistic回归的 ，下一步的工作是寻找使得最小的权值向量w。\n的表达如公式10-16所示。\n（公式10-16）\n仔细的观察该公式，可以得出该函数为连续（continuous）可微（differentiable）的凸函数，因此其最小值在梯度为零时取得，即 。那如何求解呢？即为对权值向量w的各个分量求偏微分，对这种复杂公式求解偏微分可以使用微分中的连锁律。将公式10-16中复杂的表示方式用临时符号表示，为了强调符号的临时性，不使用字母表示，而是使用 和 ，具体如公式10-17。\n（公式10-17）\n对权值向量w的单个分量求偏微分过程如公式10-18所示。\n（公式10-18）\n其中 函数为10.1节中介绍的logistic函数。而求梯度的公式可以写成公式10-19所示。\n（公式10-19）\n求出的梯度后，由于为凸函数，令为零求出的权值向量w，即使函数取得最小的w。\n观察，发现该函数是一个 函数作为权值，关于的加权求和函数。\n假设一种特殊情况，函数的所有权值为零，即所有都为零，可以得出趋于负无穷，即，也意味着所有的都与对应的同号，即线性可分。\n排除这种特殊情况，当加权求和为零时，求该问题的解不能使用类似求解线性回归时使用的闭式解的求解方式，此最小值又该如何计算？\n还记得最早使用的PLA的求解方式吗？迭代求解，可以将PLA的求解步骤合并成如公式10-20的形式。\n（公式10-20）\n时，向量不变；时，加上。将使用一些符号将该公式更一般化的表示，如公式10-21所示。\n（公式10-21）\n其中多乘以一个1，用 表示，表示更新的步长，PLA中更新的部分用v来代表，表示更新的方向。而这类算法被称为迭代优化方法（iterative optimization approach）。\n10.4 Gradient Descent\n梯度下降。\nLogistic回归求解最小也使用上节中提到的迭代优化方法，通过一步一步改变权值向量，寻找使得最小的变权值向量，迭代优化方法的更新公式如公式10-22所示。\n（公式10-22）\n针对logistic回归个问题，如何设计该公式中的参数和是本节主要解决的问题。\n回忆PLA，其中参数 来自于修正错误，观察logistic回归的，针对其特性，设计一种能够快速寻找最佳权值向量的方法。\n如-8为logistic回归的关于权值向量w的示意图为一个平滑可微的凸函数，其中图像谷底的点对应着最佳w，使得最小。如何选择参数和可以使得更新公式快速到达该点？\n-8 logistic回归的示意图\n为了分工明确，设作为单位向量仅代表方向，代表步长表示每次更新改变的大小。在固定的情况下，如何选择的方向保证更新速度最快？是按照最陡峭的方向更改。即在固定，的情况下，最快的速度（有指导方向）找出使得最小的w，如公式10-23所示。\n（公式10-23）\n以上是非线性带约束的公式，寻找最小w仍然非常困难，考虑将其转换成一个近似的公式，通过寻找近似公式中最小w，达到寻找原公式最小w的目的，此处使用到泰勒展开（Taylor expansion），回忆一维空间下的泰勒公式，如公式10-24所示。\n(公式10-24)\n同理，在很小时，将公式10-23写成多维泰勒展开的形式，如公式10-25所示。\n（公式10-25）\n其中相当于公式10-24中的，相当于。通俗点解释，将原的曲线的形式看做一小段一小段的线段的形式，即的曲线可以看做周围一段很小的线段。\n因此求解公式10-26最小情况下的w，可以认为是近似的求解公式10-23最小状况下的w。\n（公式10-26）\n该公式中是已知值，而为给定的大于零的值，因此求公式10-26最小的问题又可转换为求公式10-27最小的问题。\n（公式10-27）\n两个向量最小的情况为其方向相反，即乘积为负值，又因是单位向量，因此方向如公式10-28所示。\n（公式10-27）\n在很小的情况下，将公式10-27代入公式10-22得公式10-28，具体的更新公式。\n（公式10-27）\n该更新公式表示权值向量w每次向着梯度的反方向移动一小步，按照此种方式更新可以尽快速度找到使得最小的w。此种方式称作梯度下降（gradient descent），简写为GD，该方法是一种常用且简单的方法。\n讲完了参数v的选择，再回头观察事先给定的参数的取值对梯度下降的影响，如-9所示。\n-9参数的大小对梯度下降的影响\n如-9最左，太小时下降速度很慢，因此寻找最优w的速度很慢；-9中间，当太大时，下降不稳定，甚至可能出现越下降越高的情况；合适的应为随着梯度的减小而减小，如图最右所示，即参数是可变的，且与梯度大小成正比。\n根据与梯度大小成正比的条件，可以将重新给定，新的如公式10-28所示。\n（公式10-28）\n最终公式10-27可写成公式10-29。\n（公式10-29）\n此时的被称作固定的学习速率（fixed learning rate），公式10-29即固定学习速率下的梯度下降。\nLogistic回归算法的步骤如下：\n设置权值向量w初始值为 ，设迭代次数为t， ；\n计算梯度;\n对权值向量w进行更新，；\n直到或者迭代次数足够多。"}
{"content2":"一、准备样本\n接上一篇文章提到的问题：根据一个人的身高、体重来判断一个人的身材是否很好。但我手上没有样本数据，只能伪造一批数据了，伪造的数据比较标准，用来学习还是蛮合适的。\n下面是我用来伪造数据的代码：\nstring Filename = \"./figure_full.csv\"; StreamWriter sw = new StreamWriter(Filename, false); sw.WriteLine(\"Height,Weight,Result\"); Random random = new Random(); float height, weight; Result result; for (int i = 0; i < 2000; i++) { height = random.Next(150, 195); weight = random.Next(70, 200); if (height > 170 && weight < 120) result = Result.Good; else result = Result.Bad; sw.WriteLine($\"{height},{weight},{(int)result}\"); } enum Result { Bad=0, Good=1 }\nView Code\n制造成功后的数据如下：\n用记事本打开：\n二、源码\n数据准备好了，我们就用准备好的数据进行学习了，先贴出全部代码，然后再逐一解释：\nnamespace BinaryClassification_Figure { class Program { static readonly string DataPath = Path.Combine(Environment.CurrentDirectory, \"Data\", \"figure_full.csv\"); static readonly string ModelPath = Path.Combine(Environment.CurrentDirectory, \"Data\", \"FastTree_Model.zip\"); static void Main(string[] args) { TrainAndSave(); LoadAndPrediction(); Console.WriteLine(\"Press any to exit!\"); Console.ReadKey(); } static void TrainAndSave() { MLContext mlContext = new MLContext(); //准备数据 var fulldata = mlContext.Data.LoadFromTextFile<FigureData>(path: DataPath, hasHeader: true, separatorChar: ','); var trainTestData = mlContext.Data.TrainTestSplit(fulldata,testFraction:0.2); var trainData = trainTestData.TrainSet; var testData = trainTestData.TestSet; //训练 IEstimator<ITransformer> dataProcessPipeline = mlContext.Transforms.Concatenate(\"Features\", new[] { \"Height\", \"Weight\" }) .Append(mlContext.Transforms.NormalizeMeanVariance(inputColumnName: \"Features\", outputColumnName: \"FeaturesNormalizedByMeanVar\")); IEstimator<ITransformer> trainer = mlContext.BinaryClassification.Trainers.FastTree(labelColumnName: \"Result\", featureColumnName: \"FeaturesNormalizedByMeanVar\"); IEstimator<ITransformer> trainingPipeline = dataProcessPipeline.Append(trainer); ITransformer model = trainingPipeline.Fit(trainData); //评估 var predictions = model.Transform(testData); var metrics = mlContext.BinaryClassification.Evaluate(data: predictions, labelColumnName: \"Result\", scoreColumnName: \"Score\"); PrintBinaryClassificationMetrics(trainer.ToString(), metrics); //保存模型 mlContext.Model.Save(model, trainData.Schema, ModelPath); Console.WriteLine($\"Model file saved to :{ModelPath}\"); } static void LoadAndPrediction() { var mlContext = new MLContext(); ITransformer model = mlContext.Model.Load(ModelPath, out var inputSchema); var predictionEngine = mlContext.Model.CreatePredictionEngine<FigureData, FigureDatePredicted>(model); FigureData test = new FigureData(); test.Weight = 115; test.Height = 171; var prediction = predictionEngine.Predict(test); Console.WriteLine($\"Predict Result :{prediction.PredictedLabel}\"); } } public class FigureData { [LoadColumn(0)] public float Height { get; set; } [LoadColumn(1)] public float Weight { get; set; } [LoadColumn(2)] public bool Result { get; set; } } public class FigureDatePredicted : FigureData { public bool PredictedLabel; } }\nView Code\n三、对代码的解释\n1、读取样本数据\nstring DataPath = Path.Combine(Environment.CurrentDirectory, \"Data\", \"figure_full.csv\"); MLContext mlContext = new MLContext(); //准备数据 var fulldata = mlContext.Data.LoadFromTextFile<FigureData>(path: DataPath, hasHeader: true, separatorChar: ','); var trainTestData = mlContext.Data.TrainTestSplit(fulldata,testFraction:0.2); var trainData = trainTestData.TrainSet; var testData = trainTestData.TestSet;\nLoadFromTextFile<FigureData>(path: DataPath, hasHeader: true, separatorChar: ',')用来读取数据到DataView\nFigureData类是和样本数据对应的实体类，LoadColumn特性指示该属性对应该条数据中的第几个数据。\npublic class FigureData { [LoadColumn(0)] public float Height { get; set; } [LoadColumn(1)] public float Weight { get; set; } [LoadColumn(2)] public bool Result { get; set; } }\npath:文件路径\nhasHeader:文本文件是否包含标题\nseparatorChar:用来分割数据的字符，我们用的是逗号，常用的还有跳格符‘\\t’\nTrainTestSplit(fulldata,testFraction:0.2)用来随机分割数据，分成学习数据和评估用的数据，通常情况，如果数据较多，测试数据取20%左右比较合适，如果数据量较少，测试数据取10%左右比较合适。\n如果不通过分割，准备两个数据文件,一个用来训练、一个用来评估，效果是一样的。\n2、训练\n//训练 IEstimator<ITransformer> dataProcessPipeline = mlContext.Transforms.Concatenate(\"Features\", new[] { \"Height\", \"Weight\" }) .Append(mlContext.Transforms.NormalizeMeanVariance(inputColumnName: \"Features\", outputColumnName: \"FeaturesNormalizedByMeanVar\")); IEstimator<ITransformer> trainer = mlContext.BinaryClassification.Trainers.FastTree(labelColumnName: \"Result\", featureColumnName: \"FeaturesNormalizedByMeanVar\"); IEstimator<ITransformer> trainingPipeline = dataProcessPipeline.Append(trainer); ITransformer model = trainingPipeline.Fit(trainData);\nIDataView这个数据集就类似一个表格，它的列（Column）是可以动态增加的，一开始我们通过LoadFromTextFile获得的数据集包括：Height、Weight、Result这几个列，在进行训练之前，我们还要对这个数据集进行处理，形成符合我们要求的数据集。\nConcatenate这个方法是把多个列，组合成一个列，因为二元分类的机器学习算法只接收一个特征列，所以要把多个特征列（Height、Weight）组合成一个特征列Features（组合的结果应该是个float数组）。\nNormalizeMeanVariance是对列进行归一化处理，这里输入列为：Features，输出列为：FeaturesNormalizedByMeanVar，归一化的含义见本文最后一节介绍。\n数据集就绪以后，就要选择学习算法，针对二元分类，我们选择了快速决策树算法FastTree，我们需要告诉这个算法特征值放在哪个列里面（FeaturesNormalizedByMeanVar），标签值放在哪个列里面（Result）。\n链接数据处理管道和算法形成学习管道，将数据集中的数据逐一通过学习管道进行学习，形成机器学习模型。\n有了这个模型我们就可以通过它进行实际应用了。但我们一般不会现在就使用这个模型，我们需要先评估一下这个模型，然后把模型保存下来。以后应用时再通过文件读取出模型，然后进行应用，这样就不用等待学习的时间了，通常学习的时间都比较长。\n3、评估\n//评估 var predictions = model.Transform(testData); var metrics = mlContext.BinaryClassification.Evaluate(data: predictions, labelColumnName: \"Result\"); PrintBinaryClassificationMetrics(trainer.ToString(), metrics);\n评估的过程就是对测试数据集进行批量转换（Transform），转换过的数据集会多出一个“PredictedLabel”的列，这个就是模型评估的结果，逐条将这个结果和实际结果（Result）进行比较，就最终形成了效果评估数据。\n我们可以打印这个评估结果，查看其成功率，一般成功率大于97%就是比较好的模型了。由于我们伪造的数据比较整齐，所以我们这次评估的成功率为100%。\n注意：评估过程不会提升现有的模型能力，只是对现有模型的一种检测。\n4、保存模型\n//保存模型 string ModelPath = Path.Combine(Environment.CurrentDirectory, \"Data\", \"FastTree_Model.zip\"); mlContext.Model.Save(model, trainData.Schema, ModelPath); Console.WriteLine($\"Model file saved to :{ModelPath}\");\n这个没啥好解释的。\n5、读取模型并创建预测引擎\n//读取模型 var mlContext = new MLContext(); ITransformer model = mlContext.Model.Load(ModelPath, out var inputSchema); //创建预测引擎 var predictionEngine = mlContext.Model.CreatePredictionEngine<FigureData, FigureDatePredicted>(model);\n创建预测引擎的功能和Transform是类似的，不过Transform是处理批量记录，这里只处理一条数据，而且这里的输入输出是实体对象，定义如下:\npublic class FigureData { [LoadColumn(0)] public float Height { get; set; } [LoadColumn(1)] public float Weight { get; set; } [LoadColumn(2)] public bool Result { get; set; } } public class FigureDatePredicted : FigureData { public bool PredictedLabel; }\n由于预测结果里放在“PredictedLabel”字段中，所以FigureDatePredicted类必须要包含PredictedLabel属性，目前FigureDatePredicted 类是从FigureData类继承的，由于我们只用到PredictedLabel属性，所以不继承也没有关系，如果继承的话，后面要调试的话会方便一点。\n6、应用\nFigureData test = new FigureData { Weight = 115, Height = 171 }; var prediction = predictionEngine.Predict(test); Console.WriteLine($\"Predict Result :{prediction.PredictedLabel}\");\n这部分代码就比较简单，test是我们要预测的对象，预测后打印出预测结果。\n四、附：数据归一化\n机器学习的算法中一般会有很多的乘法运算，当运算的数字过大时，很容易在多次运算后溢出，为了防止这种情况，就要对数据进行归一化处理。归一化的目标就是把参与运算的特征数变为（0，1）或（-1，1）之间的浮点数，常见的处理方式有：min-max标准化、Log函数转换、对数函数转换等。\n我们这次采用的是平均方差归一化方法。\n五、资源\n源码下载地址：https://github.com/seabluescn/Study_ML.NET\n工程名称：BinaryClassification_Figure\n点击查看机器学习框架ML.NET学习笔记系列文章目录"}
{"content2":"在机器学习-李航-统计学习方法学习笔记之感知机(1)中我们已经知道感知机的建模和其几何意义。相关推导也做了明确的推导。有了数学建模。我们要对模型进行计算。\n感知机学习的目的是求的是一个能将正实例和负实例完全分开的分离超平面。也就是去求感知机模型中的参数w和b.学习策略也就是求解途径就是定义个经验损失函数，并将损失函数极小化。我们这儿采用的学习策略是求所有误分类点到超平面S的总距离。假设超平面s的误分类点集合为M,那么所有误分类点到超平面S的总距离为\n显然损失函数L(w,b)是非负的，如果没有误分类点，那么损失函数的值就是0，因为损失函数的定义就是求误分类点到平面的距离，误分类点都没有，那么损失函数的值肯定是0.\n感知机学习算法是误分类驱动，采用随机梯度下降法。首先，任意选取一个超平面w,b,然后极小化目标函数。相关定义在作者的书中都有给出。不在啰嗦了。\n感知机学习算法的原始形式\n对例子2.1做详细推导。作者其实已经给出了推导。对于很多基础知识扎实的人来说已经足够了。但对于一些大学期间高数忘了差不多的我们来说，理通作者思路也要仔细手写推导一下。\n解  构建最优化问题：，按照算法2.1求解w,b,学习η=1\n取初值w0=(0,0)T (这里w0是初始的法向量，如果是三维空间应该是(0,0,0)T,这儿二维平面就够用了w0=(0,0)T。所以,w0=(0,0)T )\nb0=0.\n对x1=(3,3)T,因为是正分类点,所以y1=1带入分离 超平面公式\ny1(w0•x1+b0)\n=　 1((0,0)T •(3,3)T+0)       --------公式1.0\n其中T代表矩阵的转置，也就是把(0,0)竖过来。同时这儿的(0,0)T和(3,3)T也是向量的表示。中间的圆点代表求两个向量的内积。我们看一下向量内积的定义\n在线性代数中有对此的明确定义。所以(0,0)T和(3,3)T 的内积就为0*3+0*3=0.\n所以公式1.0的值为0.因为要把所有的正实例和负实例分开，这儿该正实例在分离超平面上，显然不符合要求。所以我们要更新w,b.\nw1=w0+y1x1 这儿更新w法向量的意义是移动分离超平面的方向，对于二维空间就是更改直线的斜率，更新b就是移动斜线的截距。\n我们首先把这儿几个实例点表示出来x1y1=((3,3)T,1 )     x2y2=((4,3)T,1 )    x3y3=((1,1)T,-1 )\n求得w1=(0,0)T+(3,3)T=(3,3)T     　b1=b0+y1=1\n所以线性模型为\n因为我们使用函数间隔来衡量是否被正确分类的，也就是在线性模型前面加上参数yi 因为正确分类时候yi=1，误分类的时候yi=-1,所以可以两者的乘积只要大于0就可以表示正确分类了，不需要更新函数参数。小于等于0就表示要更新参数。\n新的线性模型对于点x1y1=((3,3)T,1 )     x2y2=((4,3)T,1 )显然都大于0，也就是可以被正确分类。对于  x3y3=((1,1)T,-1 ),带入后函数间隔小于0代表函数未被正确分类。所以需要更新函数。\nw2=w1+y1x1\n对于感知机求解的一般形式，很简单，仔细看书，了解几个数学概念就很容易明白。不在赘述。\n感知机学习算法的收敛性\n大体浏览了下，感觉不是很重要，也不是很难理解，可能是我没自己手动推导一下的原因。想研究的可以直接看作者的推导。\n感知机学习算法的对偶形式\n下面是作者书中给出的例子，但是没有具体的推导过程。\n我们推导如下。从原始形式中我们可以知道。w的更新过程。\n第一次更新是x1y1=((3,3)T,1 ) 点不能是函数模型大于零,所以    w1=w0+x1y1\n第二次更新是x3y3=((1,1)T,-1 )点不能使其大于零，所以    w2=w1+x3y3\n第三次更新是x3y3=((1,1)T,-1 )点不能使其大于零，所以    w3=w2+x3y3\n第四次更新是x3y3=((1,1)T,-1 )点不能使其大于零，所以    w4=w3+x3y3\n第五次更新是x1y1=((3,3)T,1 )点不能使其大于零，所以     w5=w4+x1y1\n第六次更新是x3y3=((1,1)T,-1 )点不能使其大于零，所以    w6=w5+x3y3\n第七次更新是x3y3=((1,1)T,-1 )点不能使其大于零，所以    w7=w6+x3y3\n然后我们得到\n从上面可以总结出w7=w6+x3y3\nw7=w5+x3y3 +x3y3\nw7=w4+x1y1+x3y3 +x3y3\nw7=w3+x3y3+x1y1+x3y3 +x3y3\nw7=w2+x3y3+x3y3+x1y1+x3y3 +x3y3\nw7=w1+x3y3 +x3y3+x3y3+x1y1+x3y3 +x3y3\nw7=w0+x1y1 +x3y3 +x3y3+x3y3+x1y1+x3y3 +x3y3\n所以我们可以得出最终w7的值为两次x1y1 +五次x3y3\n也就等于在对偶形式中的\n同理也可以得出b,例2.2中的误分条件我们还可以写成如下形式。\n从上面的公式中对比作者给出的求解迭代过程。我们应该可以很容易理解对偶形式的感知机算法，推导后发现只是换了一个简便的计算形式。至此关于统计学习方法中的感知机篇章结束。\n可参考机器学习-李航-统计学习方法学习笔记之感知机(1)\n本文地址：http://www.cnblogs.com/santian/p/4351756.html\n博客地址：http://www.cnblogs.com/santian/\n转载请以超链接形式标明文章原始出处。"}
{"content2":"机器学习相关知识\n写在前面的话\n保持怀疑的态度（在全新的数据集上测试分类器）\n天下没有免费的午餐（没有适用的最好学习方法，具体问题具体对待）\n正确对待缺失值，不同参数的设置可能会对结果产生不同的影响\n不同算法都有对应的假设\n数据挖掘的结果总会误导人，保持求真的态度\n数据类型\n连续型\n离散型\n标称型\n概念\n离散化\n归一化\n正则化\n度量指标\n欧几里得距离\n以\\(R\\)为实数域，对于任意一个正整数n，实数的n元组的全体构成了R上的一个n维向量空间，用 \\(R^n\\) 来表示。 \\(R^n\\) 中的元素可以写成 $ X=(x_1,x_2,...x_n)$ .\n欧式范数定义 \\(R^n\\) 上的距离函数为:\n\\[d(x,y)=\\Vert x-y \\Vert =\\sqrt{\\displaystyle \\sum_{i=1}^n(x_i-y_i)^2}\\]\n可以使用上面的公式来度量n维空间中任意两点之间的距离。这也是在推荐系统中度量样本点的相似程度的一种方式。\n余弦相似度\n余弦相似度是基于n维空间向量的。所谓向量就是在向量的空间中的坐标原点指向该空间某一个点的度量表示。n维空间由n条互相正交的基向量构成，彼此无关，也是构成整个向量空间的基础结构。我们能够比较直观理解的是三维向量，超过三维就比较难以理解了。\n点积 对于任意两个向量 x、y 点积<x,y>定义为\n\\[<x,y>=\\displaystyle\\sum_{i=1}^nx_iy_i\\]\n向量长度 定义为\n\\[\\Vert x \\Vert =\\sqrt{<x,x>}=\\sqrt{\\displaystyle \\sum_{i=1}^n(x_i)^2}\\]\n余弦相似度 定义为\n\\[cos(\\theta)= \\frac{ A \\bullet B}{\\Vert A \\Vert \\Vert B \\Vert} =\\frac{\\displaystyle \\sum_{i=1}^nA_i \\times B_i}{\\sqrt{\\displaystyle \\sum_{i=1}^n(A_i)^2 \\times \\displaystyle \\sum_{i=1}^n(B_i)^2}}\\]\n越相似二者夹角越小越趋向于1，若两向量朝逆方向延伸，则度量值趋向于-1 可以明显的从数值中看到趋势。\n皮尔逊距离\n模型评估\n模型评估帮助我们找到最佳的模型来代表给定的数据集并且能够选择出的模型在未来的未知数据中取得较好的效果。在原来的训练集合上来评估一个模型的好坏是不太合适的，因为这样可能会造成过拟合的现象，也就是说可能会因为训练集某些隐含的因子使得这个分类模型有很好的准确率，但是当我们应用这个模型到新的数据集上就不一定有同样的结果。这就是过拟合的现象，没有在新的数据集上获得同样的效果（模型的泛化能力）。所以我们需要引入没有污染过（任何经过训练的数据我们称为被污染过的数据）进行测试。这里有两种方式进行评估。\nHold-out（这种方式针对大的数据集，把数据分成三份）\n训练集 构建模型\n验证集 细化模型参数、选择最佳模型\n测试集 衡量模型的性能 这部分数据不参与前面两个步骤\nCross-Validation 针对数据较少，可以采用交叉验证的方式取得一个比较稳定的平均值。常用的k-fold 也是就是所谓的k折交叉验证方式，把数据集合分成k份，每次选择其中一份作为测试集其余的作为训练集，重复k次，最后取得每次结果的平均值。\n分类评估\nConfusion Matrix\n混淆矩阵用来描述正确分类和错误分类的各个指标。下面是一个二分类问题混淆矩阵的实例。\nAccuracy准确率 : the proportion of the total number of predictions that were correct. 准确率\nPositive Predictive Value or Precision正确率 : the proportion of positive cases that were correctly identified.\nNegative Predictive Value : the proportion of negative cases that were correctly identified.\nSensitivity or Recall敏感度 召回率: the proportion of actual positive cases which are correctly identified.\nSpecificity 特异性: the proportion of actual negative cases which are correctly identified. 特异性\nROC Chart\n回归评估\nRoot Mean Squared Error\n均方差用来衡量回归模型中的错误率，注意不同模型计算时需要注意度量单位的统一。\n\\[RMSE=\\sqrt{\\frac{\\displaystyle \\sum_{i=1}^n(p_i-a_i)^2}{n}}\\]\n\\[ a=actual \\ target \\]\n\\[ p=predicted \\ target \\]\nRelative Squared Error\n\\[RSE=\\frac{\\displaystyle \\sum_{i=1}^n(p_i-a_i)^2}{\\displaystyle \\sum_{i=1}^n(\\overline a-a_i)^2}\\]\nMean Absolute Error\n\\[MAE=\\frac{\\displaystyle \\sum_{i=1}^n \\vert p_i-a_i \\vert }{n}\\]\nRelative Absolute Error\n\\[RAE=\\frac{\\displaystyle \\sum_{i=1}^n \\vert p_i-a_i \\vert }{\\displaystyle \\sum_{i=1}^n \\vert \\overline a-a_i \\vert }\\]\nCoefficient of Determination\n优化方法\n最小二乘法\n梯度上升（下降）寻求最大（最小）值\n梯度上升法基于的思想是：要找到函数的最值最好的方法就是沿着函数的地图方向进行。从初始点进行最值寻找的过程中需要计算当期点的梯度，并且沿着这个方向移动，具体移动的步伐通过步长来设定。到达下一个点之后重新计算梯度不迭代直到满足条件。（寻找到最优值或者到达误差允许的范围内。）\n随机梯度\n随机梯度在针对大数据集的时候显得特别有用，虽然损失了一定的精度，但是换来了较快的收敛速度，达到一个与全局最优较近位置的点。它是针对每个点选择盖点最优的方向进行移动，不一定要对整个数据集集合操作就可能到达收敛了。而BGD（批处理梯度下降）在每一次梯度的更新操作中需要对整个数据集进行计算，在大数据处理过程中无疑增大了计算量。\n最大似然估计\n风刮的\n常用算法（建模）\n分类\n分类问题实际是一类监督学习问题，根据训练数据的类标记来引导分类器选择合适的参数，较好的拟合样本空间中的数据，并且能够对未来的数据有一定的预测能力（泛化能力）。分类算法大致可以按照如下四种方式进行分类。\n频率表\nZeroR\nWeka中提供了ZeroR算法，是最简单的分类器。主要思想是按照类中大多数数据所属的类别类进行分类。基本上没有使用认为关于ml的理论。它的作用就是给其他分类器提供一个模型的基线，衡量一个算法的性能。换句话说我们用的模型再怎么不行也不能比这个baseline还要低吧。\n构造目标的频率表，选择频率最大的类别作为目标的预测值\n下面是ZeroR的混淆矩阵，我们可以看到这个算法的用途只是在于提供一个baseline而已。\nOneR\nOneR是“One Rule”的简称，也就是按照一个规则进行分类。它会针对每个属性上进行测试对目标进行分类，然后从每个属性中得到的规则中选择最小误分类的规则作为最终的分类模型。比如下面的例子很好的说明上述的描述。\n一个较小的总体分类误差意味着对正确分类提供了更多的贡献。\n这是这个分类问题的混淆矩阵\nNaive Bayesian\n贝叶斯分类依托于贝叶斯理论。基于先验概率的理解，根据样本数据的统计特征得出后验概率。是一种非常简单而且有效的分类方法。在数据较少的情况下也依然有效，同时也可以处理多类分类问题。缺点在于对输入数据的准备方式较为敏感。\n补充贝叶斯公式、贝叶斯理论\nto be continue... 、\n朴素贝叶斯的前提条件是:假设给定目标值时属性之间彼此相互独立。然后应用贝叶斯公式通过 $ P(c) ,P(x) ,P(x/c)$ 计算出后验概率\\(P(c/x)\\)。虽然我们不能确定给定的属性之间是否真的相互独立，但是在这么一个假设的前提下，朴素贝叶斯分类确实取得了非常不错的效果。\n下面举个栗子，看看贝叶斯的分类机制。\n给定一个属性集合计算它属于其中某个类别的一个后验概率，然后选择概率最大的那个类别作为它的分类。需要注意的一点就是由于贝叶斯针对多个变量的连乘操作，我们在操作中需要进行加一避免0的出现。比如途中当outlook为overcast的时候（Play Golf=no)。\n上面是对离散型的分类，处理数值型预测任务改怎么处理呢？\n我们需要把数值型数据转换成离散型，通常是假定正态分布然后将其离散化.然后构造频率表再使用贝叶斯理论。\n关于正态分布\n下面举个例子说明： 数值型离散化 使用贝叶斯\nDecision Tree\n决策树可用来构建分类和回归模型，能够处理离散型和连续型的数据。分类的过程就是不断的把数据集合按照属性分成一个个较小的子集，属性的选择可用比如信息增益等方式,对每个选择的属性做一次判断然后将数据归入到特定的子集中不断的进行直到分类完全。这个用来做分析判断的属性所在的节点叫做决策节点也是最终形成树的中间节点。最后形成的树的叶子节点表示纯类。\n构造决策树的核心算法来自 J.R.Quinlan发表的ID3算法，ID3使用的是自顶向下的贪婪算法，不经过回退操作在整个可能的空间中进行搜索.ID3使用熵和信息增益来构建决策树。\n熵\nID3使用熵（平均信息量）来计算样本间的同质性。即如果样本完全同质则信息熵为0，如果样本均匀包含各种可能性，则熵为1.熵在某种程度上表达了不确定性的多少。不确定性越大信息熵越大。数学之美一书中阐述过香农的信息熵理论。\n使用id3进行构建决策树时候，需要计算两类熵。\na) 单个属性的信息熵\nb) 多个属性的信息熵\n信息增益\n信息增益是基于在针对数据集上一个属性上分类之后熵的减少来定义的。构造决策树的过程实际上就是不断的寻找分裂属性并且返回最大的信息增益值，这意味着分裂后的分支所在的集合信息熵越小，它所在的分支越纯，从而达到了分类的效果。下面是计算步骤1-5。\n1) 计算分类目标的熵\n2) 将数据集按照不同的属性划分并且计算他们分裂之后的熵和信息增益。\n3) 选择最大的信息增益作为决策节点。这里选择的是outlook.\n4)当摸个分支的熵为0的时候则表示已经分类完成，直接作为叶子节点。否则的话继续选择属性进行分裂。\n5) 递归的运行id3算法直到数据集被完全划分而且没有决策节点。\n避免过拟合\nbinning\navoiding overfitting\nsuper attributes\nmissing values\n协方差矩阵\nLinear Discriminant Analysis\n线性判别分析被R. A. Fisher在1936年首次作为一种分类方法提出，具有与复杂方法相当的精确度。\nLDA基于这样一种思想，就是在预测变量的线性组合空间中搜索最佳的用来分开两个类（target）的线性组合。Finsher定义了下面的 代价函数 。\nLogistic Regression\n逻辑回归产生一条逻辑曲线将值映射到[0,1]之间，逻辑回归于线性回归有点类似，不过这条曲线使用的是目标变量的可能性的自然对数而不是概率来构造。而且预测变量不必满足正态分布或在同一个组内要求的相同方差。 逻辑回归不仅可以处理数值型数据而且可以处理分类型变量.\n逻辑回归使用sigmoid函数作为回归分类器，我们通过在每个特征上乘以一个回归系数，然后把所有的结果相加得到一个总和带入到sigmoid这个阶跃函数中，进而得到一个0~1之间的实数，我们把大于0.5的归入到1类，小于0.5的归入到0类。所以逻辑回归也可以被看成是一个概率估计分类器。\n那么最佳的回归系数怎么确定呢？这就需要采用最优化的方法来进行对参数的确定了，拟合训练数据学习出最佳的回归系数就是逻辑回归训练的任务了。\n这里让人疑问的是可能性和概率有哪里不同吗？\n优点：计算代价不高，易于理解和实现\n缺点： 容易欠拟合，分类精度可能不高\n数据类型：数值型和标称型\n图中等式可以等价于下面的式子：\n我们可以使用类似于线性回归中的最小二乘法来确定代价函数中的参数。在逻辑回归中我们采用的是最大似然估计来确定这些参数。\n相似函数\nK Nearest Neighbors\nK近邻是比较简答基于相似度来分类的一个算法，核心思想就是根据距离来寻找与待分类点的k个邻居，然后每个对应邻居节点都有自己的类标号，我们选择这k个邻居中类标号数目最多的作为待分类节点的标签，类似于一种投票机制。k近邻是一种lazy算法。处理待分类节点的时候才去寻找他对应的邻居。\n距离度量公式\n我们可以看到上面的公式主要是针对连续型变量的,在处理分类变量的时候我们需要采用Hamming distance。当数据集中同时存在s数值型和分类型变量的时候，它同时带来了将数值变量映射到[0,1]的标准化问题。\n下面是计算Hamming distance距离的表达式。\nK的选择\n一般选择k的范围为[3,10]当然只是个别的经验,通过交叉验证的方式可以帮助我们选择合适的k值并通过独立的验证集来验证这个k是否合适,这对分类器的分类效果至关重要。\n举个栗子\n在下面的栗子中是一个信用卡违约的一个预测问题，有两个属性age和loan，都是数值型变量。目标变量是是否违约。\n给定一个样本(age=48,loan=142,000)用来判断他是否会违约，我们选择和他最近的3个邻居，并选择邻居中类标记数目最多的标记来作为该样本的预测值。在这个例子中我们得出default=Y\n标准化距离\n当样本中的数据的度量单位不一致的时候，使用这些距离度量公式的时候有一个非常大的弊端，就是量纲较大的属性产生的影响会显得比较大很肯能掩盖掉了一些很重要的属性的影响，为此我们需要对数据进行标准化。使他们处于同一个水平进行比较和度量。对于标准化后的数据进行训练，不能对新来的未知数据的预测有较好的鲁棒性。下图是标准化转换计算公式\n其他\nArtificial Neural Network\n人工神经网络是基于生物神经网络的一个系统。模仿大脑内的神经元之间的触发方式。到达临界条件才会激活或者抑制。人工神经网络（ANN）由三个部分组成：输入层、隐含层、输出层。\n传递方式 -前向传播 后向反馈\nSupport Vector Machine\n回归\n聚类\n关联规则\n特征选择\n概念\n维度处理\nPCA主成分分析\nSVD奇异值分解\n附录-数据挖掘概览\nData mining Map"}
{"content2":"一、概念整体介绍\n人工智能（Artificial Intelligence）\n机器学习（Machine Learning）：一种实现人工智能的方法\n深度学习（Deep Learning）：一种实现机器学习的技术\n三者的关系图\n人工智能分类：\n强人工智能：强人工智能观点认为有可能制造出真正能推理（Reasoning）和解决问题（Problem_solving）的智能机器，并且这样的机器将被认为是有知觉的，有自我意识的。可以独立思考问题\n并制定解决问题的最优方案，有自己的价值观和世界观体系。有和生物一样的各种本能，比如生存和安全需求。\n弱人工智能：弱人工智能是指不能制造出真正地推理（Reasoning）和解决问题（Problem_solving）的智能机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能，也不会有自主意识。\n人工智能是计算机学科的一个分支，二十世纪七十年代以来被称为世界三大尖端技术之一（空间技术、能源技术、人工智能）。\n也被认为是二十一世纪三大尖端技术（基因工程、纳米科学、人工智能）之一。\n这是因为近三十年来它获得了迅速的发展，在很多学科领域都获得了广泛应用，并取得了丰硕的成果，人工智能已逐步成为一个独立的分支，无论在理论和实践上都已自成一个系统。\n人工智能的研究分支\n人工智能的发展历程\n各种概念关系\n相关链接：\n一张图解释人工智能、机器学习、深度学习三者关系：https://baijiahao.baidu.com/s?id=1588563162916669654&wfr=spider&for=pc\n一篇文章讲清楚人工智能、机器学习和深度学习的区别和联系：https://www.cnblogs.com/bokeyuan-dlam/articles/7928135.html\n科普一下：机器学习和深度学习的区别和关系：http://www.elecfans.com/rengongzhineng/691751.html\n人工智能的三个分支：认知、机器学习、深度学习：https://blog.csdn.net/testcs_dn/article/details/81185750\n还纠结选机器学习还是深度学习？看完你就有数了：https://www.baidu.com/link?url=rVRgTtwZ11xkY1lcq4rRgilW9PwQEBXf737ESjE_H8RySv47Fwe-LyD69FJhFxeSqhQYPAL3kArqxR_nfWPSQAqRsrxsLaUqwm6EPUym6XK&wd=&eqid=edac1d7c00058498000000065cac9dc7\n=====================================================\n二、人工智能应用领域\n关键词：\n自然语言生成、语音识别、虚拟助理、机器学习平台、人工智能硬件优化、决策管理、深度学习平台、生物信息、\n图像识别、情绪识别、P2P网络、内容创作、网络防御、AI建模/数字孪生、机器处理自动化、文本分析和自然语言处理\n游戏 ：人工智能在国际象棋，扑克，围棋等游戏中起着至关重要的作用，机器可以根据启发式知识来思考大量可能的位置并计算出最优的下棋落子。\n自然语言处理 ： 可以与理解人类自然语言的计算机进行交互。比如常见机器翻译系统、人机对话系统。\n专家系统 ： 有一些应用程序集成了机器，软件和特殊信息，以传授推理和建议。它们为用户提供解释和建议。比如分析股票行情，进行量化交易。\n视觉系统 ： 它系统理解，解释计算机上的视觉输入。例如，间谍飞机拍摄照片，用于计算空间信息或区域地图。医生使用临床专家系统来诊断患者。警方使用的计算机软件可以识别数据库里面存储的肖像，从而识别犯罪者的脸部。还有我们最常用的车牌识别等。\n语音识别 ：智能系统能够与人类对话，通过句子及其含义来听取和理解人的语言。它可以处理不同的重音，俚语，背景噪音，不同人的的声调变化等。\n手写识别 ： 手写识别软件通过笔在屏幕上写的文本可以识别字母的形状并将其转换为可编辑的文本。\n智能机器人 ： 机器人能够执行人类给出的任务。它们具有传感器，检测到来自现实世界的光，热，温度，运动，声音，碰撞和压力等数据。他拥有高效的处理器，多个传感器和巨大的内存，以展示它的智能，并且能够从错误中吸取教训来适应新的环境。\n相关链接：http://www.qianjia.com/html/2018-08/23_302917.html\n=====================================================\n三、机器学习\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。\n与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。\n从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n机器学习过程使用以下步骤进行定义：\n1. 确定相关数据集并准备进行分析。\n2. 选择要使用的算法类型。\n3. 根据所使用的算法构建分析模型。\n4. 立足测试数据集进行模型训练，并根据需要进行模型修改。\n5. 运行模型以生成测试评分。\n传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。\n=====================================================\n四、深度学习\n深度学习属于机器学习的一个子域，其相关算法受到大脑结构与功能（即人工神经网络）的启发。\n深度学习如今的全部价值皆通过监督式学习或经过标记的数据及算法实现。\n深度学习中的每种算法皆经过相同的学习过程。\n深度学习包含输入内容的非近线变换层级结构，可用于创建统计模型并输出对应结果。\n机器学习与深度学习间的区别：\n1、数据量：机器学习能够适应各种数据量，特别是数据量较小的场景。在另一方面，如果数据量迅速增加，那么深度学习的效果将更为突出。下图展示了不同数据量下机器学习与深度学习的效能水平。\n2、硬件依赖性：与传统机器学习算法相反，深度学习算法在设计上高度依赖于高端设备。深度学习算法需要执行大量矩阵乘法运算，因此需要充足的硬件资源作为支持。\n3、特征工程：特征工程是将特定领域知识放入指定特征的过程，旨在减少数据复杂性水平并生成可用于学习算法的模式。  示例：传统的机器学习模式专注于特征工程中所需要找像素及其他属性。\n深度学习算法则专注于数据的其他高级特征，因此能够降低处理每个新问题时特征提取器的实际工作量。\n4、问题解决方法：传统机器学习算法遵循标准程序以解决问题。它将问题拆分成数个部分，对其进行分别解决，而后再将结果结合起来以获得所需的答案。深度学习则以集中方式解决问题，而无需进行问题拆分。\n5、执行时间：执行时间是指训练算法所需要的时间量。深度学习需要大量时间进行训练，因为其中包含更多参数，因此训练的时间投入也更为可观。相对而言，机器学习算法的执行时间则相对较短。\n6、可解释性：可解释性是机器学习与深度学习算法间的主要区别之一——深度学习算法往往不具备可解释性。也正因为如此，业界在使用深度学习之前总会再三考量。\n机器学习与深度学习的实际应用：\n1. 通过指纹实现出勤打卡、人脸识别或者通过扫描车牌识别牌照号码的计算机视觉技术。\n2. 搜索引擎中的信息检索功能，例如文本搜索与图像搜索。\n3. 自动电子邮件营销与特定目标识别。\n4. 癌症肿瘤医学诊断或其他慢性疾病异常状态识别。\n5. 自然语言处理应用程序，例如照片标记。Facebook就提供此类功能以提升用户体验。\n6. 在线广告。\n未来发展趋势：\n1. 随着业界越来越多地使用数据科学与机器学习技术，对各个组织而言，最重要的是将机器学习方案引入其现有业务流程。\n2. 深度学习的重要程度正逐步超越机器学习。事实已经证明，深度学习是目前最先进且实际效能最出色的技术方案之一。\n3. 机器学习与深度学习将在研究与学术领域证明自身蕴藏的巨大能量。"}
{"content2":"这篇文章转自：http://hi.baidu.com/macula7/blog/item/8a3f22cd9587f81a00e92829.html\n里面称作者是周志华，我无从考证，只是转载。个人感觉写得很不错。转载至此。\n机器学习现在是一大热门，研究的人特多，越来越多的新人涌进来。\n不少人其实并没有真正想过，这是不是自己喜欢搞的东西，只不过看见别人都在搞，觉着跟大伙儿走总不会吃亏吧。\n问题是，真有个\"大伙儿\"吗？就不会是\"两伙儿\"、\"三伙儿\"？如果有\"几伙儿\"，那到底该跟着\"哪伙儿\"走呢？\n很多人可能没有意识到，所谓的machine learning community，现在至少包含了两个有着完全不同的文化、完全不同的价值观的群体，称为machine learning \"communities\"也许更合适一些。\n第一个community，是把机器学习看作人工智能分支的一个群体，这群人的主体是计算机科学家。\n现在的\"机器学习研究者\"可能很少有人读过1983年出的\"Machine Learning: An Artificial Intelligence Approach\"这本书。这本书的出版标志着机器学习成为人工智能中一个独立的领域。它其实是一部集早期机器学习研究之大成的文集，收罗了若干先贤（例 如Herbert Simon，那位把诺贝尔奖、图灵奖以及各种各样和他相关的奖几乎拿遍了的科学天才）的大作，主编是Ryszard S. Michalski（此君已去世多年了，他可算是机器学习的奠基人之一）、Jaime G. Carbonell（此君曾是Springer的LNAI的总编）、Tom Mitchell（此君是CMU机器学习系首任系主任、著名教材的作者，机器学习界没人不知道他吧）。Machine Learning杂志的创刊，正是这群人努力的结果。这本书值得一读。虽然技术手段早就日新月异了，但有一些深刻的思想现在并没有过时。各个学科领域总有 不少东西，换了新装之后又粉墨登场，现在热火朝天的transfer learning，其实就是learning by analogy的升级版。\n人工智能的研究从以\"推理\"为重点到以\"知识\"为重点，再到以\"学习\"为重点，是有一条自然、清晰的脉络。人工智能出身的机器学习研究者，绝大部分 是把机器学习作为实现人工智能的一个途径，正如1983年的书名那样。他们关注的是人工智能中的问题，希望以机器学习为手段，但具体采用什么样的学习手 段，是基于统计的、代数的、还是逻辑的、几何的，他们并不care。\n这群人可能对统计学习目前dominating的地位未必满意。靠统计学习是不可能解决人工智能中大部分问题的，如果统计学习压制了对其他手段的研 究，可能不是好事。这群人往往也不care在文章里show自己的数学水平，甚至可能是以简化表达自己的思想为荣。人工智能问题不是数学问题，甚至未必是 依靠数学能够解决的问题。人工智能中许多事情的难处，往往在于我们不知道困难的本质在哪里，不知道\"问题\"在哪里。一旦\"问题\"清楚了，解决起来可能并不 困难。\n第二个community，是把机器学习看作\"应用统计学\"的一个群体，这群人的主体是统计学家。\n和纯数学相比，统计学不太\"干净\"，不少数学家甚至拒绝承认统计学是数学。但如果和人工智能相比，统计学就太干净了，统计学研究的问题是清楚的，不象人工智能那样，连问题到底在哪里都不知道。在相当长时间里，统计学家和机器学习一直保持着距离。\n慢慢地，不少统计学家逐渐意识到，统计学本来就该面向应用，而机器学习天生就是一个很好的切入点。因为机器学习虽然用到各种各样的数学，但要分析大 量数据中蕴涵的规律，统计学是必不可少的。统计学出身的机器学习研究者，绝大部分是把机器学习当作应用统计学。他们关注的是如何把统计学中的理论和方法变 成可以在计算机上有效实现的算法，至于这样的算法对人工智能中的什么问题有用，他们并不care。\n这群人可能对人工智能毫无兴趣，在他们眼中，机器学习就是统计学习，是统计学比较偏向应用的一个分支，充其量是统计学与计算机科学的交叉。这群人对统计学习之外的学习手段往往是排斥的，这很自然，基于代数的、逻辑的、几何的学习，很难纳入统计学的范畴。\n两个群体的文化和价值观完全不同。第一个群体认为好的工作，第二个群体可能觉得没有技术含量，但第一个群体可能恰恰认为，简单的才好，正因为很好地 抓住了问题本质，所以问题变得容易解决。第二个群体欣赏的工作，第一个群体可能觉得是故弄玄虚，看不出他想解决什么人工智能问题，根本就不是在搞人工智 能、搞计算机，但别人本来也没说自己是在\"搞人工智能\"、\"搞计算机\"，本来就不是在为人工智能做研究。\n两个群体各有其存在的意义，应该宽容一点，不需要去互较什么短长。但是既然顶着Machine Learning这个帽子的不是\"一伙儿\"，而是\"两伙儿\"，那么要\"跟进\"的新人就要谨慎了，先搞清楚自己更喜欢\"哪伙儿\"。\n引两位著名学者的话结尾，一位是人工智能大奖得主、一位是统计学习大家，名字我不说了，省得惹麻烦：\n\"I do not come to AI to do statistics\"\n\"I do not have interest in AI\""}
{"content2":"核心提示：微软在 Office365、Azure 云、Dynamics365 上进行人工智能技术的部署，野心不小。 微软在2016年9月宣布组建自己的 AI 研究小组。该小组汇集了超过 5000 名计算机科学家和工程师，加上微软内部研究部门，将共同挖掘 AI 技术。 与此同时，亚马逊，Facebook，Google，IBM 还有微软联合宣\n而巨头们也纷纷拿出了自己的看家本领，Apple 的 Siri 利用自然语言处理来识别语音命令；Facebook 的深度学习面部识别算法能够快速准确地识别出人脸；Google 的「大脑」可能更为聪明，而且能够安装在数百万安卓系统的设备上。而隐藏在 CRM 领域的对手之一 Salesforce 也声称推出了一款全新的人工智能平台。还有 Amazon、Netflix 和 Spotify 都在强调使用机器学习了解如何与客户建立联系。\n最近的人工智能领域如此炙手可热，就连微软的首席执行官纳德拉也提到，「将 AI 覆盖到所有领域，是因为微软想要实现「AI 技术民主化」，从而解决全球最紧迫的挑战。」\n据悉，微软将从以下四个细分方向实现「AI 技术民主化」：\nAgents. Harness AI to fundamentally change human and computer interaction through agents such as Microsoft』s digital personal assistant Cortana\n1、助手。利用 AI 从根本上改变人机交互过程，如微软的数字个人助理 Cortana。\nApplications. Infuse every application, from the photo app on people』s phones to Skype and Office 365, with intelligence\n2、应用平台。从移动端的照片 app 到 Skype，再到 Office365，每一个应用平台融入了 AI 技术。\nServices. Make these same intelligent capabilities that are infused in Microsoft』s apps—cognitive capabilities such as vision and speech, and machine analytics—available to every application developer in the world\n3、服务。让全球每位应用开发者都能获取同样的 AI 技术支持。如微软 app 中视觉与语音识别能力，机器分析能力。\nInfrastructure. Build the world』s most powerful AI supercomputer with Azure and make it available to anyone, to enable people and organizations to harness its power\n4、基础架构。以 Azure 云为平台，构建强大的 AI 超级计算机，为企业和个人提供该项服务。\n那么，微软近期推出的几款人工智能产品看似「姗姗来迟」，但在 AI 领域的重量可不能小视，那么，微软在如何布局 AI 呢？其旗下几大产品又都与 AI 有着怎样的关系？\n推出人工智能版 Dynamics365\n提升销售转化率，更好地理解客户行为，是如今移动、社交、云三者融合时代的大背景下企业需要具备的重要能力。\n过去几周，CRM 领域频频传来人工智能利好的消息。如今，微软也向世人透露：在下月 1 日，将推出人工智能版 Dynamics365，为销售人员提供云端商务应用解决方案。\n该款产品将 ERP 与 CRM 进行整合，打造成为一个单一的云端解决方案。\n此次产品的推出主要针对的就是 CRM 领域占主导地位的 Salesforce，而此前不久，Salesforce 刚刚推出了一款名为「爱因斯坦」的人工智能云平台，能够为企业客户提供相应服务。\nMicrosoft has built in a couple of intelligence features into the release designed specifically for sales and service personnel. First, there is Customer Insights, a stand-alone cloud service, which enables users to bring in a variety of internal and external data sources. Companies can integrate all of this data with internal metrics (KPIs) to drive automated actions based on the data. The solution includes partner data from the likes of Facebook and Trip Advisor (proving you don』t need to own an external data source to take advantage of it).\n微软推出的 Dynamics365 能够为销售人员提供以下几类服务：一是客户洞察（Customer Insights），一款单机云服务，帮助用户收集各类内外部数据信息。企业能够将所有数据与内部指标（KPIs）进行集成，基于数据进行自动化行为的驱动。这套解决方案还包括了合作伙伴如 Facebook 和 Trip Advisor 上产生的大量数据，不过，这也说明了用户无需借外部获取数据。\nIt』s been designed as a stand-alone service that can work with any of the Dynamics 365 CRM components—sales, customer service or field service—and can also work with any external CRM tool with open APIs. This last point is particularly telling because it』s giving customers who might not be using Dynamics 365 (but are using other Microsoft tools like Outlook) access to this feature.\n一方面，单机服务能够与 Dynamics365 CRM 的组成部件进行很好地兼容，包括销售、客服或者现场服务；另一方面，还能与任何一个带有开放 API 接口的外部 CRM 工具进行兼容。此外，单机服务还能为并未使用 Dynamics365，而是微软其他工具如 Outlook 的客户，提供该项服务的渠道。\nThe second piece is called Relationship Insights, which as the name suggests gives sales people information about the status of their customer relationships at any given moment. It』s built on the on the Cortana Intelligence Suite, which Microsoft introduced in 2015 and uses tools like sentiment analysis to check on the likelihood of the deal closing and the next best action to take.\n二是关系洞察（Relationship Insights），为销售人员即时提供客户关系信息。该服务基于人工智能助手小娜的平台进行提供。这个智能助手于去年在全球推出，通过情感分析检测交易结束的可能性，并判断下一步最佳实践方案。\nFor now, know that Microsoft has consolidated its artificial intelligence tools into a single, coherent division and just about every vendor—not just those selling CRM—is trying to build some level of intelligence into its products. Dynamics 365 is just the latest manifestation.\n总的来说，微软已将人工智能工具嵌入一个单独连贯的区域，而且不只是 CRM 领域的供应商，基本上每个供应商都在试图将某种程度的智能技术嵌入到其产品中。"}
{"content2":"如果没有机器的协助，海量、复杂的数据将越来越难以利用。机器不仅处理数据，并且从中学习。\n日前，我在《纽约时报》Dealbook大会上谈到，每家公司都应当制定人工智能战略，刻不容缓。随着数字化革命进程的加快，数据越来越多、越来越复杂、越来越多样，企业必须迅速做出关键决策。为了驾驭数字洪流，企业需要人工智能战略，否则就会落后于时代。\n海量、复杂的可用数据量超过了人类分析师处理的能力。如果没有机器的协助，就日益难以有效利用数据，而机器不仅处理数据，并且从中学习。\n人工智能使企业能够管理复杂的数据，并且提供前所未有的机会以让企业做出实时决策、动态运营管理并响应客户。这并不只是在预测未来。从银行到医疗，从制造业到消费服务业，全球的企业都已经开始利用人工智能分析数据并构建学习型组织，从而以前所未有的速度来应变并展开竞争。\n企业正在利用人工智能平台进行激动人心的创新。企业要想运用人工智能致胜未来，就应遵循四个关键指南：\n了解哪些数据正被采集, 以及最新的智能互联技术还可采集到哪些数据。\n部署基础设施，以整合数据并知道如何分析这些数据。\n确定在哪方面投资人工智能以助于从数据中进行学习。\n人工智能支持并协助实时决策的世界即将到来，需培训高管使其做好准备。\n在英特尔，我们致力于让人工智能成为我们产品组合的重要组成部分。能够解决我们内部业务挑战的技术，同样也可帮到我们的客户。人工智能优化解决方案的案例包括：工厂流程、战略规划、IT数据中心运营等。应用案例不胜枚举，正因如此，英特尔投资也将人工智能视为选择投资项目的重要战略标准之一。\n企业需要立刻制定并执行自己的人工智能战略，方能制胜未来。\nAI人才的缺口是巨大的，抓住机会，学习应用引领。"}
{"content2":"本节使用的算法称为ID3，另一个决策树构造算法CART以后讲解。\n一、概述\n我们经常使用决策树处理分类问题，它的过程类似二十个问题的游戏：参与游戏的一方在脑海里想某个事物，其他参与者向他提出问题，只允许提20个问 题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小带猜测事物的范围。\n如所示的流程图就是一个决策树，长方形代表判断模块（decision block），椭圆形代表终止模块（terminating block），表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作分支（branch），它可以到达另一个判断模块或终止模块。\n图 1构造了一个假象的邮件分类系统，它首先检测发送邮件域名地址。如果地址为myEmployer.com，则将其放在分类\"无聊时需要阅读的邮件\"中。如 果邮件不是来自这个域名，则检查内容是否包括单词曲棍球，如果包含则将邮件归类到\"需要及时处理的朋友邮件\"，否则将邮件归类到\"无须阅读的垃圾邮件\"。\n二、优缺点\n优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。\n缺点：可能会产生过度匹配问题。\n适用数据类型：离散型和连续型\n三、数学公式\n如果待分类的数据集可能划分在多个分类之中，则符号Xi定义为：x\n其中p(xi)是选择该分类的概率。\n其中：n = 数据集分类数。\n例如：数据集的分类为 lables=[A,B,C,B,A,B] 则P(A)=2/6=0.3333,P(B)=3/6=0.5,P(C)=1/6=0.277,\n数据集的熵 H=-P(A)*log2P(A)+(-P(B)*log2P(B))+(-P(C)*log2P(C))\n四、树的构造\n在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。我们假设已经根据一定的方法选取了待划分的特征，则原始数据集将根据这个特征被划分为几个数据子集。这数据子集会分布在决策点（关键 特征）的所有分支上。如果某个分支下的数据属于同一类型，则无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要递归地重复划分数据 子集的过程，直到每个数据子集内的数据类型相同。如何划分子集的算法和划分原始数据集的方法相同。\n创建分支的过程用伪代码表示如下：\n检测数据集中的每个子项是否属于同一类型：\nIf Yes return 类标签\nElse\n寻找划分数据集的最好特征\n划分数据集\n创建分支节点\nfor 每个划分的子集：\n递归调用本算法并添加返回结果到分支节点中(这是个递归)\nreturn 分支节点\n决策树的一般流程：\n收集数据：可以使用任何方法。\n准备数据：树构造算法只适用于标称数据，因此数值型数据必须离散化。\n分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。\n训练算法：构造树的数据结构。\n测试算法：使用经验树计算错误率。\n使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。\n一些决策树算法使用二分法划分数据，本书将使用ID3算法划分数据集，该算法处理如何划分数据集，何时停止划分数据集。每次划分数据集我们只选取一个特征属性，那么应该选择哪个特征作为划分的参考属性呢？\n表1的数据包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚噗。我们可以将这些动物分成两类：鱼类和非鱼类。\n表1 海洋生物数据\n不浮出水面是否可以生存\n是否有脚蹼\n属于鱼类\n1\n是\n是\n是\n2\n是\n是\n是\n3\n是\n否\n否\n4\n否\n是\n否\n5\n否\n是\n否\n五、信息增益\n划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。我们可以在划分数据之前或之后使用信息论量化度量信息的内容。\n在划分数据集之前之后信息发生的变化成为信息增益，我们可以计算每个特征划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。\n集合信息的度量方式成为香农熵或者简称为熵。\n为了计算熵，我们需要计算所有类型所有可能值包含的信息的期望值，通过下面的公式得到：\n其中n是分类的数目。\n下面给出计算信息熵的Python函数，创建名为trees.py文件，添加如下代码：\n1 def createDataSet(): 2 dataSet = [[1, 1, 'yes'], 3 [1, 1, 'yes'], 4 [1, 0, 'no'], 5 [0, 1, 'no'], 6 [0, 1, 'no']] 7 labels = ['no surfacing','flippers'] 8 #change to discrete values 9 return dataSet, labels 10 def calcShannonEnt(dataSet): 11 numEntries = len(dataSet) #获取数据行数 numEntries = 5 12 labelCounts = {} 13 for featVec in dataSet: #the the number of unique elements and their occurance 14 currentLabel = featVec[-1] 15 if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 16 labelCounts[currentLabel] += 1 17 shannonEnt = 0.0 18 print(\"labelCounts=\",labelCounts) # labelCounts= {'yes': 2, 'no': 3} 19 for key in labelCounts: 20 prob = float(labelCounts[key])/numEntries # prob 为每个值出现的概率 21 shannonEnt -= prob * log(prob,2) # 数学公式，计算 熵 22 print(\"shannonEnt=\",shannonEnt) 23 return shannonEnt\n测试代码：\n1 >>> d,l=trees.createDataSet() 2 >>> d 3 [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] 4 >>> l 5 ['no surfacing', 'flippers'] 6 >>> c=trees.calcShannonEnt(d) 7 labelCounts= {'yes': 2, 'no': 3} 8 shannonEnt= 0.9709505944546686 9 >>>\ncalcShannonEnt：返回整个数据集的 熵\n上面测试代码数据集的熵=0.9709505944546686\n熵 值越高，则混合的数据也越多，得到 熵 之后，我们就可以按最大信息增益的方法划分数据集。\n六、划分数据集\n上面我们学习了如何度量数据集的无序程序，分类算法除了需要测量信息熵，还需要划分数据集，度量划分数据集的熵，以便判断当前是否正确地划分了数据集。\n我们将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集市最好的划分方法。\n按照给定的特征划分数据集：\n1 def splitDataSet(dataSet, axis, value):#查找数据集 dataSet 第 axis 列值== value 的元素，再排除第 axis 列的数据，组成一个新的数据集 2 retDataSet = [] 3 for featVec in dataSet: 4 if featVec[axis] == value: 5 reducedFeatVec = featVec[:axis] #chop out axis used for splitting 6 reducedFeatVec.extend(featVec[axis+1:]) 7 retDataSet.append(reducedFeatVec) 8 return retDataSet\n该函数使用了三个输入参数：带划分的数据集、划分数据集的特征（数据集第几列）、需要返回的特征的值（按哪个值划分）。函数先选取数据集中第axis个特征值为value的数据，从这部分数据中去除第axis个特征，并返回。\n测试这个函数，效果如下：\n1 >>> myDat, labels = trees.createDataSet() 2 >>> myDat 3 [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] 4 >>> trees.splitDataSet(myDat, 0, 1) #查找第0列值==1的元素，再排除第0列的数据，组成一个新的数据集 5 [[1, 'yes'], [1, 'yes'], [0, 'no']] 6 >>> trees.splitDataSet(myDat, 0, 0) 7 [[1, 'no'], [1, 'no']]\n接下来我们将遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的特征划分方式。熵计算将会告诉我们如何划分数据集是最好的数据组织方式。\n选择最好的数据集划分方式：\n1 def chooseBestFeatureToSplit(dataSet): 2 numFeatures = len(dataSet[0]) - 1 # 这里的dataSet 最后一列是分类 numFeatures=2;我们按2列数据进行划分 3 baseEntropy = calcShannonEnt(dataSet) # 计算出整个 数据数据集的 熵 4 bestInfoGain = 0.0; bestFeature = -1 5 for i in range(numFeatures): # 循环每一列特征 6 featList = [example[i] for example in dataSet]# 创建一个新的 列表，存放数据集第 i 列的数据 7 uniqueVals = set(featList) # 使用集合，把数据去重。。。。。。。。。。 8 newEntropy = 0.0 # 以下是计算 每一列 的 熵 求某列 最大的 熵 9 for value in uniqueVals:# 循环 第 i 列 的特征值 10 subDataSet = splitDataSet(dataSet, i, value) # 划分数据集。。。。 11 prob = len(subDataSet)/float(len(dataSet)) # 子数据集所占的比例。。。。 12 newEntropy += prob * calcShannonEnt(subDataSet) # 子数据集的 熵 * 比例。。。。 13 infoGain = baseEntropy - newEntropy #calculate the info gain; ie reduction in entropy 14 if (infoGain > bestInfoGain): #compare this to the best gain so far 15 bestInfoGain = infoGain #if better than current best, set to best 16 bestFeature = i 17 return bestFeature #返回按某列划分数据集的最大 熵\n本函数使用变量bestInfoGain和bestFeature记录最好的信息增益和对应的特征；\nnumFeatures记录特征维数，依次遍历各个特征，计算该特征值的集合（uniqueVals）；\n遍历该特征计算使用该特征划分的熵（newEntropy），据此计算新的信息增益（infoGain）；\n比较infoGain和bestInfoGain记录信息增益的最大值和对应特征；\n最终返回最大的信息增益对应特征的索引。\n测试上面代码的实际输出结果：\n1 >>> myData, labels = trees.createDataSet() 2 >>> trees.chooseBestFeatureToSplit(myData) 3 labelCounts= {'yes': 2, 'no': 3} 4 shannonEnt= 0.9709505944546686 5 labelCounts= {'no': 2} 6 shannonEnt= 0.0 7 labelCounts= {'yes': 2, 'no': 1} 8 shannonEnt= 0.9182958340544896 9 labelCounts= {'no': 1} 10 shannonEnt= 0.0 11 labelCounts= {'yes': 2, 'no': 2} 12 shannonEnt= 1.0 13 0\n七、递归构建决策树\n构建决策树的算法流程如下：\n得到原始数据集，\n基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。\n第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。我们可以采用递归的原则处理数据集。\n递归结束的条件是，程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。\n添加下面的程序代码：\n1 def majorityCnt(classList):# 返回 出现次数最多的类别 ，，类似于K-近邻算法中 返回前K中类别出现最多次数的。 2 classCount={} 3 for vote in classList: 4 if vote not in classCount.keys(): classCount[vote] = 0 5 classCount[vote] += 1 6 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) 7 return sortedClassCount[0][0] 8 9 def createTree(dataSet,labels): 10 classList = [example[-1] for example in dataSet] # 获取数据集的所有类别 11 if classList.count(classList[0]) == len(classList): 12 return classList[0]# 如果数据集的所有类别 都相同，则不需要划分 13 if len(dataSet[0]) == 1: # 使用完了所有特征，仍然不能将数据划分 到某个类别上的话，返回出现次数最多的类别 14 return majorityCnt(classList) 15 bestFeat = chooseBestFeatureToSplit(dataSet) # 获取数据集中 按哪一列进行划分。。。。。 16 bestFeatLabel = labels[bestFeat] # bestFeatLabel = 列 描述 17 myTree = {bestFeatLabel:{}} # 创建一个字典 18 del(labels[bestFeat]) # 删除已计算过的列 19 featValues = [example[bestFeat] for example in dataSet] 20 uniqueVals = set(featValues) # 获取某列 所有不重复值 21 for value in uniqueVals: 22 subLabels = labels[:] #copy all of labels, so trees don't mess up existing labels 23 myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels) # 递归 24 return myTree 25\nmajorityCnt函数统计classList列表中每个类型标签出现频率，返回出现次数最多的分类名称。\ncreateTree函数使用两个输入参数：数据集dataSet和标签列表labels。标签列表包含了数据集中所有特征的标签，算法本身并不需 要这个变量，但是为了给出数据明确的含义，我们将它作为一个输入参数提供。上述代码首先创建了名为classList的列表变量，其中包含了数据集的所有 类标签。列表变量classList包含了数据集的所有类标签。递归函数的第一个停止条件是所有类标签完全相同，则直接返回该类标签。递归函数的第二个停 止条件是使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。这里使用majorityCnt函数挑选出现次数最多的类别作为返回值。\n下一步程序开始创建树，这里直接使用Python的字典类型存储树的信息。字典变量myTree存储树的所有信息。当前数据集选取的最好特征存储在变量bestFeat中，得到列表中包含的所有属性值。\n最后代码遍历当前选择特征包含的所有属性值，在每个数据集划分上递归待用函数createTree()，得到的返回值将被插入到字典变量myTree中，因此函数终止执行时，字典中将会嵌套很多代表叶子节点信息的字典数据。\n注意其中的subLabels = labels[:]复制了类标签，因为在递归调用createTree函数中会改变标签列表的值。\n测试这些函数：\n1 >>> myDat, labels = trees.createDataSet() 2 >>> myTree = trees.createTree(myDat, labels) 3 >>> myTree 4 {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n变量myTree包含了很多代表树结构信息的嵌套字典，从左边开始，第一个关键字no surfacing 是第一个划分数据集特征的名称，该关键字的值也是一个字典。\n八、测试算法：使用决策树执行分类，以及决策树的存储\n依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于决策树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子结点；最后将测试数据定义为叶子结点所属的类型。\n使用决策树分类的函数：\n1 def classify(inputTree,featLabels,testVec): 2 firstStr = inputTree.keys()[0] 3 secondDict = inputTree[firstStr] 4 featIndex = featLabels.index(firstStr) 5 key = testVec[featIndex] 6 valueOfFeat = secondDict[key] 7 if isinstance(valueOfFeat, dict): 8 classLabel = classify(valueOfFeat, featLabels, testVec) 9 else: classLabel = valueOfFeat 10 return classLabel\n测试上面的分类函数：\n>>> myDat, labels = trees.createDataSet() >>> myTree = trees.createTree(myDat, labels[:]) >>> trees.classify(myTree, labels, [1, 0]) 'no' >>> trees.classify(myTree, labels, [1, 1]) 'yes'\n可以使用Python模块pickle序列化对象，参见下面的程序。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。\n1 def storeTree(inputTree, filename): 2 import pickle 3 fw = open(filename, 'w') 4 pickle.dump(inputTree, fw) 5 fw.close() 6 7 def grabTree(filename): 8 import pickle 9 fr = open(filename) 10 return pickle.load(fr)\n九、示例：使用决策树预测隐形眼镜类型\n>>> fr = open('lenses.txt') >>> lensens = [inst.strip().split('\\t') for inst in fr.readlines()] >>> lensensLabels = ['age', 'prescript', 'astigmatic', 'tearRate'] >>> lensesTree = trees.createTree(lensens,lensensLabels) >>> lensensTree\n执行结果：{'tearRate': {'normal': {'astigmatic': {'yes': {'prescript': {'hyper': {'age': {'pre': 'no lenses', 'presbyopic': 'no lenses', 'young': 'hard'}}, 'myope': 'hard'}}, 'no': {'age': {'pre': 'soft', 'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}}, 'young': 'soft'}}}}, 'reduced': 'no lenses'}}\n十、总结一下\n计算整个数据集的 熵\n根据第1处计算出来的熵 再计算数据集按哪一列划分最为合适（计算数据集每一列的 熵 ，根据所有列计算出来的 熵 获取最佳列），设此处计算出最佳列为 I\n获取数据集第 I 列所有不重复值的集合，设此处集合为 M\nfor v in M 循环集合 M\n根据 I 列 和 v 值 划分数据集\n再递归运算"}
{"content2":"依旧是唠叨一下：\n考完试了，该去实习的朋友都去实习了。这几天最主要的事情应该是把win10滚回到win7了，真的还是熟悉的画面，心情好了很多。可惜自己当初安装的好多软件都写入了注册表导致软件用不了，好处就是重新清理了一下电脑，顺便把虚拟机重新安装了一下，现在正在备份系统。是的，一定要备份，重要数据不要保存在C盘，安装软件不要安装在C盘，与空间无关，数据才是重点！win7比较稳定，可以懒得备份。但是linux一定要备份，不然在某一天启动系统突然说丢失了某个内核文件，然后你就得修复或者recovery，最糟糕是重新安装，没有几个命令行恢复来的酸爽。\n接着上一篇python机器学习<回归 一>\n上一篇文章主要讲了回归的入门，以及引入了“代价函数”这个东西，这一次就要涉及到数学算法了，但是再难的数学方程，其原理拆解开来不过是一个思维风暴，关键是你愿不愿意认真的、持续的思考，每一次的思考都是把接近生锈的脑袋运转一下好让自己不那么容易老年痴呆，哈哈，20岁说这句话似乎有点早～\n鉴于上一次介绍的繁琐难懂，这一次直接上数学公式。我们知道回归里面最简单的线性回归方程一般的表达式都是：f(x)=a*x（当只有单个影响因素x时），推广得到在多个影响因素就会得到：f(x)=a(0)*x(0)+a(2)*x(2)+...+a(n)*x(n)，其中a(n)是第n个回归系数，x(n)是第n个特征变量（这里的一些专有名词劳驾wiki上面找，wiki不懂的人可以百度。。。ps：最近最开心的事情是入手***搭建好了属于自己的shadowsocks）。那么，学过高等代数的朋友一定了解“向量内积”这个东西；其实，举一个很简单的例子你们就明白（这个概念非常重要，是机器学习中的计算常用的“向量化”！）：\n这里我就直接贴图，因为编辑器的字母打出来真的很丑～\n前面我们已经提到了“成本函数”，实际上就是误差平方和，目的是要找出最好的向量a（这里的回归系数a也是上一篇的theta，后面继续使用thata表示回归系数）让误差最小，一般采取的是求导的方式，关于求导的算法上一篇中通过下坡的比喻也很明了，这一次我们直接计算如何求得偏导数。\n再次结合前面的文章要点总结一下。\n首先，我们根据线性回归模型：\n得到成本函数：\n其次，根据梯度下降法对回归变量theta中的每一个元素求偏导，并在赋予初始值后不断的更新theta值直至下降到最低点得到局部最小值，加入只有一个特征变量的情况下（不包括常数x0），则有按照如下方式更新theta的值：\n其中，类似于求偏导是把其他变量当作常数处理的一个过程，此时特征变量是已知值（常数）：\n这里需要强调的是，x0作为常数theta0的的特征变量实际上全部是常数值1（无论是针对多少个样本（i=1....m））。\n问题来了：\n（1）怎么让你的最速下降模型朝着正确的方向下降，它一定会下降到局部最优点？\n（2）学习速度alpha如何取值？\n以上两个问题，依次做出解答：\n针对问题（1），下图演示的是随着迭代次数的增加，成本函数的变化（我真是画图技术越来越好）：\n我们的目的是每一次迭代，成本函数的更新值只会越来越小，至于小到什么程度，打个比方：取最小误差值的边界值为eptheta=1/1000，一般读作10的负三次方。\n针对问题（2），让alpha满足以下三个条件：\n（i）取充分小的alpha使得J（theta）在每一次迭代之后变得更小；\n（ii）如果取值太小，收敛到猴年马月；\n（iii）如果取值太大，J（theta）很可能不能满足条件（i），甚至达到发散的结果，无终止的循环。。。\n说的太多，画个简单明了的图给你们看看：\n一般情况，我们会选择多个alpha的值：0.001，0.01，0.1，然后分别画出J（theta）与迭代次数之间的散点图，为梯度下降算法选择最合适的学习速度alpha。\n罗嗦了半天，总算是把算法的整个过程描述出来了，还望各位觉得不对或者不熟悉的地方提出来，大家互相改进，那么接下来就是matlab中代价函数的实现（python突然罢工了，实际上numpy的编程跟matlab非常相似，相似到你无法想象的程度）：\n代价函数：\nfunction J = computeCost(X, y, theta) %COMPUTECOST Compute cost for linear regression % J = COMPUTECOST(X, y, theta) computes the cost of using theta as the % parameter for linear regression to fit the data points in X and y % Initialize some useful values m = length(y); % number of training examples % ====================== YOUR CODE HERE ====================== % Instructions: Compute the cost of a particular choice of theta % You should set J to the cost. pred = X*theta; % the prediction result sum = 0; for i = 1:m % iterating from 1 to m sum = sum+(pred(i) - y(i))^2; end J = 1/(2*m)*sum; % compute the cost function % ========================================================================= end\n梯度下降算法（只针对一个特征值）：\nfunction [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters) %GRADIENTDESCENT Performs gradient descent to learn theta % theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by % taking num_iters gradient steps with learning rate alpha % Initialize some useful values m = length(y); % number of training examples J_history = zeros(num_iters, 1); % initializing all of J to zero before iteration for iter = 1:num_iters % ====================== YOUR CODE HERE ====================== % Instructions: Perform a single gradient step on the parameter vector % theta. % % Hint: While debugging, it can be useful to print out the values % of the cost function (computeCost) and gradient here. % =======This is my bug ======I just want to use jacobian but it seems % =====something wrong, maybe because theta1 and theta2 are symbol variables===== % syms theta1 theta2; % temp_theta = [theta_1, theta_2]; % temp_rate = jacobian(J_history(iter), temp)' % theta = theta - alpha * temp_rate * theta; error = X * theta - y; % the error between prediction and observation delta = 1 / m * (error' * X)'; % the J's decrease slope theta = theta - alpha * delta; % ============================================================ % Save the cost J in every iteration J_history(iter) = computeCost(X, y, theta); end end\n前面提到“向量化”的概念，实际上上面的1到m个样本的迭代是直接借助矩阵中的向量运算的，改善代码如下：\nsum=(pred - y)'*(pred - y);\n然后经过取学习速度alpha=0.01，1500次迭代后，拟合效果图的代码：\nfunction plotData(x, y) %PLOTDATA Plots the data points x and y into a new figure % PLOTDATA(x,y) plots the data points and gives the figure axes labels of % population and profit. % ====================== YOUR CODE HERE ====================== % Instructions: Plot the training data into a figure using the % \"figure\" and \"plot\" commands. Set the axes labels using % the \"xlabel\" and \"ylabel\" commands. Assume the % population and revenue data have been passed in % as the x and y arguments of this function. % % Hint: You can use the 'rx' option with plot to have the markers % appear as red crosses. Furthermore, you can make the % markers larger by using plot(..., 'rx', 'MarkerSize', 10); figure; % open a new figure window plot(x, y, 'rx', 'MarkerSize', 10); % plot the data ylabel('Profit in $10,000s') % set the y-axis label xlabel('Population of City in 10,000s') % set the x-axis label % ============================================================ end\n拟合结果如下图所示：\n总结：这里需要提到的另外一个算法是正则方程（“Normal Equation”）。我看到《机器学习实战》这本书直接给出的算法就是庸正则方程令误差为0，然后反过来求theta的值，这个时候有一个问题就是求逆，因为不是所有的矩阵都是有逆矩阵的，因此在python或者matlab中就给出了引入数值计算的函数直接求出“伪逆”来逼近真正的“逆”。因此，这个算法比较简单易行，针对较多的特征值的时候比较方便，比如说当样本数量m小于特征数量n的时候。\n好啦，下周再见。\n下期预告：局部加权线性回归～"}
{"content2":"一、机器学习的背景\n大家都说人工智能是综合的学科，而机器学习就是人工智能的大脑。它通过对数据的处理，不断地变得更好和更强，做出各种各样的判断和决策。\n人工智能、机器学习、深度学习，这三者是什么关系？\n我们可以参照下面这张图：\n机器学习是实现人工智能的一种方法，机器学习有很多的细分领域，其中有一个领域是人工神经网络，而深度学习是人工神经网络这个领域的一个分支。\n二、什么是机器学习呢？\n做机器学习，大部分工作其实是编程。通俗地讲：机器学习是一种计算机程序，可以从现有的经验中学习如何完成某项任务，并且随着经验的增加，性能也随之提升。\n三、机器学习有哪些分类？\n机器学习的范围很广，主要分为三大类：监督学习（ Supervised Learning ）、非监督学习（ Unsupervised Learning ）和强化学习（ Reinforcement Learning ）。\n监督学习：监督学习学的是带标准答案的样本。拿猫和狗的识别来举例子。算法看一张图就告诉它，这是猫；再看一张图片，告诉它这也是猫，再看一张图，告诉它这是狗，如此往复。当它看了几十万张猫和狗的图片后，你再给它一张陌生的猫或者狗的图片，就基本能“认”出来，这是哪一种。这样的学习方法很有可能造成模型把所有答案都记了下来，但碰到新的题目又不会了的情况，这种情况叫做“过拟合”。\n非监督学习：非监督学习学的是没有标准答案的样本。同样拿猫和狗的识别举例。算法要自己去寻找这些图片的不同特征，然后把这些图片分为两类。它实际上不知道这两类是什么，但它知道这两类各有什么特征，当再出现符合这些特征的图片时它能识别出来，这是第一类图片，那是第二类图片。\n强化学习：强化学习的学习方式是通过不断做出决策并获得结果反馈后，学会自动进行决策，得到最优结果。我们小时候，看到马戏团的猴子居然会做算术题，感觉到很惊讶，这是怎么做到的呢？其实就是每次拿对了数字的时候，训练人员就给它一些食物作为奖励，这些奖励让他“知道”，这么做是“对的”，如果拿错了，可能就会有惩罚，这些惩罚就是要让它“知道”，这样做是“错的”。如此一来，经过不断的训练，猴子就“会”做算术题了。\n四、机器学习有哪些常见算法呢？\n1.决策树\n决策树是一种用于对数据进行分类的树形结构。\n2.线性回归\n试想，在纸上有很多的点，我们打算画一条直线，让这些点到这条直线的距离之和最短，怎么找到这条直线呢？这个方法就是线性回归。画一条线，让样本以及后面预测的点都尽量在这条线附近。\n3.支持向量机和核函数\n支持向量机是一种分类方法，力求在样本中划出一道线，让线距离两边样本的距离最大。它在文本分类、图像分类有较多应用。如果桌子上有红豆和绿豆，我们可以把SVM想象成一个忍者，他画了一条线，把红豆和绿豆区分开来。\n但有的时候豆子掺和在了一起，怎么办呢？我们可以针对红豆和绿豆的不同特性，把这些豆子都通过核函数进行计算，把他们映射到高维空间中去，这样豆子自然就被分开了。\n4.神经网络\n神经网络也是一种分类器。它是由很多个虚拟的神经元组成的一个网络，我们可以把一个神经元看做是一个分类器，那很多个神经元组成的网络就能对样本进行很多次分类。 还是拿忍者和豆子区分举例子。一个神经元，相当于忍者可以划一刀，多个神经元就可以划多刀，划的越多，自然分的越细。这里只是做简单的介绍，大家有概念即可，更详细的在后面会更新。\n5.朴素贝叶斯分类器\n贝叶斯是一个定理，它的意思是：当你不能准确知悉一个事物的本质时，你可以依靠与事物特定本质相关的事件出现的多少去判断其本质属性的概率。\n比如说，我们要识别一封邮件是不是垃圾邮件。我们随机挑选出100封垃圾邮件，分析它的特征，我们发现“便宜”这个词出现的频率很高，100封垃圾邮件里，有40封出现了这个词。那我们就以这个认知为依据，得出结论：如果出现了“便宜”，那这封邮件有40%的概率是垃圾邮件。\n当我们找到若干个这样的特征，然后用这些特征进行组合后，可以对某些邮件进行判断，它是垃圾邮件的概率超过了我们设定的阈值，我们就自动把这些邮件过滤掉，减少用户受到的打扰。这就是大部分垃圾邮件过滤的原理。\n6.聚类\n聚类是一种非监督学习的方式。简单的说，就是通过不断的迭代计算，把数据分成若干个组，使得这个组里的都是类似的数据，而不同组之间的数据是不相似的。\n7.强化学习\n在没有给出任何答案的情况下，先进行一些尝试，通过尝试所得到的回报，来确定这个尝试是否正确，由这一系列的尝试来不断调整和优化算法，最后算法知道在某种情况下，采取何种动作可以得到最好的结果。他的本质是解决“决策问题”，就是通过不断做出决策并获得结果反馈后，学会自动进行决策，得到最优结果。比如上面说过的猴子“学会”做算术题的过程。\n8.集成学习\n我们在做机器学习的时候，希望能做出各个方面表现都比较好的模型。但常常现实是我们的模型是有偏好的，可能只对某一些情况效果比较好，这个时候我们就希望把若干个这样的模型组合起来，得到一个更好更全面的模型，这种方法，就叫做集成学习。"}
{"content2":"RoboCup及学术大会（The Robot World Cup Soccer Games and Conferences）是国际上级别最高、规模最大、影响最广泛的机器人足球赛事和学术会议，每年举办一次。第五届RoboCup机器人足球世界杯赛及学术大会（简称 RoboCup-2001）于\n2001年8月2日\n至10日在美国西雅图与\"第17届国际人工智能联合大会\"同时同地隆重举行。参加本届杯赛正式比赛的共有来自世界30多个国家123支球队。其中，参加仿真组比赛的共有来自美、日、德等16个国家的44支球队，它们是从参加预选的87支球队中选拔出来的。参加中型组比赛的球队为18支，小型组21支，四腿组 16支。另外24支队伍将参加救援机器人、类人机器人等项目的比赛和演示。除了上述研究系列的比赛项目之外，本届继续举办了中小学生参加的普及系列的比赛。\nRoboCup机器人足球赛最重要的目的是检验信息自动化前沿研究、特别是多主体系统研究的最新成果，交流新思想和新进展，从而更好的推动基础研究和应用基础研究及其成果转化。通过竞赛，各种不同的新思想、新原理和新技术可以得到客观的评价。因而RoboCup机器人足球世界杯赛和学术大会受到了世界各国、特别是美、日、德等发达国家的高度重视。我国在这一方向上起步较晚，因此更需要奋起直追。\n机器人足球概况\n机器人足球赛是由硬件或仿真机器人进行的足球赛，比赛规则与人类正规的足球赛类似。硬件机器人足球队的研制涉及计算机、自动控制、传感与感知融合、无线通讯、精密机械和仿生材料等众多学科的前沿研究与综合集成。仿真机器人足球赛在标准软件平台上进行，平台设计充分体现了控制、通讯、传感和人体机能等方面的实际限制，使仿真球队程序易于转化为硬件球队的控制软件。仿真机器人足球的研究重点是球队的高级功能，包括动态不确定环境中的多主体合作、实时推理－规划－决策、机器学习和策略获取等当前人工智能的热点问题。概括地说，机器人足球是以体育竞赛为载体的前沿科研竞争和高科技对抗，是培养信息－自动化科技人才的重要手段，同时也是展示高科技进展的生动窗口和促进科技成果实用化和产业化的新途径。\n历史发端\n机器人足球的最初想法由加拿大不列颠哥伦比亚大学的Alan Mackworth教授于1992年正式提出。日本学者立即对这一想法进行了系统的调研和可行性分析。1993年，Minoru Asada（浅田埝）、Hiroaki Kitano（北野宏明）和Yasuo Kuniyoshi等著名学者创办了RoboCup机器人足球世界杯赛（Robot world cup soccer games，简称RoboCup）。\n与此同时，一些研究人员开始将机器人足球作为研究课题。隶属于日本政府的电子技术实验室（ETL）的Itsuki Noda（松原仁）以机器人足球为背景展开多主体系统的研究，日本大坂大学的浅田埝、美国卡内基－梅隆大学的Veloso等也开展了同类工作。\n1997年，在国际最权威的人工智能系列学术大会--第15届国际人工智能联合大会（The 15th International Joint Conference on Artificial Intelligence，简称IJCAI-97）上，机器人足球被正式列为人工智能的一项挑战。至此，机器人足球成为人工智能和机器人学新的标准问题。\n历届杯赛情况\n第一届RoboCup机器人足球世界杯赛于\n1997年8月25日\n在日本名古屋与IJCAI-97联合举行。来自美、欧、日、澳的40多支球队参赛，观众达5000余人。第二届杯赛于\n1998年7月4日\n至8日在法国巴黎与第16届世界杯足球赛同时举行（当年没有IJCAI大会），参赛队达60多支。\n1999年7月28日\n至\n8月4日\n，第三届RoboCup世界杯赛及学术大会在瑞典斯德哥尔摩与IJCAI-99联合举行，参赛队多达90余支。\n2000年8月25日\n至\n9月3日\n，第四届杯赛及学术大学在澳大利亚墨尔本举行，正式参赛队首次突破100大关，达104支。一些著名的大学（如美国CMU和Cornell等，德国Humboldt）、国立研究院（如美国NASA）和大公司（如日本SONY）均参与了相关的活动。\n组织机构\n国际RoboCup联合会是世界上规模最大的、占主导地位的机器人足球国际组织，总部设在瑞士，现有成员国近40个。联合会现任主席是国际著名科学家、在IJCAI-93大会上获得国际人工智能最高奖--\"计算机与思维\"大奖的北野宏明。联合会负责世界范围的学术活动和竞赛，包括每年一届的世界杯赛和学术研讨会，并为相关的本科生和研究生教育提供支持（教材、教学软件等）。\n除国际RoboCup联合会之外，还有其他一些国际组织。其中较大的一个是FIRA，该组织总部设在韩国大田，现有成员国20多个，每年举办一次国际性比赛。FIRA与RoboCup的主要区别之一是采用不同的技术规范：FIRA允许一支球队采用传统的集中控制方式，相当于一支球队中的全体队员受同一个大脑的控制；而RoboCup要求必须采用分布式控制方式，相当于每个队员有自己的大脑，因而是一个独立的\"主体\"。\n机器人足球的意义\n机器人足球是人工智能和机器人学新的标准问题，是连接基础研究与实际应用的中介和桥梁，是展示信息自动化前沿研究成果的窗口和促进产、学、研结合的新途径。\n人工智能和机器人学新的标准问题\n在第15届国际人工智能联合大会上，由Kitano, Veloso和Tambe等来自美、日、瑞典的9位国际著名或知名学者联合发表重要论文\"The RoboCup synthetic agent challenge 97\"，系统阐述了机器人足球的研究意义、目标、阶段设想、近期主要内容和评价原则。概括的说，过去50年中人工智能研究的主要问题是\"单主体静态可预测环境中的问题求解\"，其标准问题是国际象棋人－机对抗赛；未来50年中，人工智能的主要问题是\"多主体动态不可预测环境中的问题求解\"，其标准问题是足球的机－机对抗赛和人－机对抗赛。从科学研究的观点看，无论是现实世界中的智能机器人或机器人团队（如家用机器人和军用机器人团队），还是网络空间中的软件自主体（如用于网络计算和电子商务的各种自主软件以及它们组成的\"联盟\"），都可以抽象为具有自主性、社会性、反应性和能动性的\"自主体\"（agents）。由这些自主体以及相关的人构成的多主体系统（multi-agent systems），是未来物理和信息世界的一个缩影。其基本问题是主体（包括人）之间的协调，可细分为自主体设计、多主体体系结构、自主体合作和通讯、自动推理、规划、机器学习与知识获取、认识建模、系统生态和进化等一系列专题。这些专题有的是新提出的（如\"合作\"），有的是过去未能彻底解决并在新的条件下更加复杂化的（如机器学习）。这些问题不解决，未来社会所需的一些关键性技术就无法得到。值得注意的是，上述一系列问题中的大多数都在机器人足球中得到了集中的体现。在这个意义下，将机器人足球作为未来人工智能和机器人学的标准问题是十分恰当的；而这一研究意义之深远重大，也是不言而喻的。\n作为人工智能的标准问题，机器人足球赛与国际象棋人机对抗赛所体现出的研究背景区别主要有以下几个方面：\n(1)静态环境和动态环境。国际象棋赛中，\"深蓝\"所处的环境是静态的--当轮到它走而它还未走时，即在它\"思考\"下一步棋的过程中，环境不发生变化。相反，足球赛中每一时刻场上形势都在变化。\n(2)行动效果和环境的可预测与不可预测。国际象棋赛中，从一个给定的棋局出发可以达到的棋局是确定的和有限的，理论上可以穷举一切可能结果并从中选择最佳方案。而对足球赛来说，理论上不可穷举；通过离散化实现近似的穷举也存在理论上和技术上难以克服的困难。\n(3)个体与团体。\"深蓝\"至多只相当于一个人。而机器人足球队中的每一个队员相当于一个人，它们之间必须合作和协调，因而出现个人（队员）和团体（球队）在目标、知识、计划和行动等方面既有区别又密切相关的复杂情形。这种复杂性是深蓝没有的。\n以上三方面的差别使得每个足球机器人必须是一个能自主行动且与队友合作的自主体，而一支球队则是一个内部协调的多主体系统。相反，\"深蓝\"本质上只是一个能在几分钟内分析600亿个棋局的搜索程序。\n连接基础研究与应用技术开发的必要桥梁\n多主体系统是90年代以来人工智能的主攻方向，agent这个词在人工智能、计算科学及相关领域文献中出现的频率高得惊人。但与此同时，内容空泛的论文数量也同样多得惊人。出现这种情况的根本原因是，当前agent基础研究与其应用背景之间的距离过大，导致注重实际背景的工作就事论事，而注重基本问题的研究\"纸上谈兵\"。为了改变这种现状，不仅需要各种类型研究工作的大量积累，而且必须采取必要的手段，在基础研究与实际背景之间寻找恰当的中介和桥梁。RoboCup机器人足球正是这样一个中介和桥梁；这主要是由于机器人足球的下述特点。\n(1) 典型性。如上所述，RoboCup机器人足球队的研制涉及当前人工智能研究的大多数主要热点，因而构成一个典型问题。\n(2) 可行性。多主体系统的绝大多数实际背景十分复杂，以致研究人员在目前的条件下难以把握，无法兼顾具体细节分析与基本问题探索。而在机器人足球中则较易兼顾二者，易于深入。\n(3) 客观性。比赛提供了一种实验平台和评价各种理论与技术的客观方法，便于研究者的\"自我观察\"和相互交流。\n(4) 综合性。在以往的研究中，各种技术通常被分别开发和考察，综合集成工作一般由面向最终用户的应用部门来完成，这种方式不利于相关技术在更高层次上的衔接和在更深层次上的创新。机器人足球是第一个深层的\"综合平台\"。\n因此，开展机器人足球研究是人工智能从基础理论走向实际应用的一个战略性步骤。\n推动信息自动化领域产、学、研结合的重要途径\n虽然机器人足球只有短短几年历史，它在推动产、学、研结合方面的显著作用和巨大潜力已经表现出来。\n(1)提供一个展示高科技成果的形象化窗口，促进科学技术与社会生产和生活更紧密的融合。足球是当今世界上最普及的一项体育活动，同时也是一个巨大的产业。机器人足球赛在人类足球赛所具有的娱乐性、观赏性和刺激性之上，进一步增加了高科技对抗性，因而具有更大的魅力。国外人工智能研究始终自觉主动地融入社会的经济和文化生活，其主要手段是通过一些典型问题的解决（如自动定理证明和国际象棋人机对抗赛），在推动基础研究和技术创新的同时，引起社会的关注，从而获得社会的支持（资金投入和推广应用）。机器人足球是人工智能和机器人学主动融入社会的一种更加巧妙、更加平民化和更具吸引力的新形式、新手段。对于这种形式和手段将产生的作用（包括社会效益和经济效益），目前还难以准确的估量。但至少可以预期，除了机器宠物等已形成的产品化方向之外，一些属于\"注意力经济\"形式的产业化方向也值得密切关注。例如，据国际RoboCup联合会计划，10年内将开发出网上仿真比赛的３维动画平台，这将会带来什么结果？当然，机器人足球带来的主要社会和经济效益将来自多主体系统研究的进展以及在工业、商业和军事等方面的成功运用。\n(2)提供了一种研究成果逐步转化的方式。例如，有腿足球机器人既是通向人形机器人的必要中间环节，又是机器宠物的原型研究。据报道，SONY公司在其有腿足球机器人基础上开发的机器狗已售出近4万台。可以预计，随着有腿足球机器人研究的不断进展，各种新型的机器宠物将不断开发出来并获得越来越大的效益。类似地，随着其他类型的足球机器人及球队在技术方面的逐步成熟，相关的产品化和经济效益亦将水到渠成。\n(3)提供了一种素质教育和创新教育与前沿研究相结合的生动形式。国外的一些大学已经开设了机器人足球的本科生课程，科大也在国内率先进行了教学实验。实践表明，机器人足球课程是素质教育、创新教育与前沿研究相结合的一条可行途径。与传统的知识传授和技能培养为目标的课程不同，足球机器人及球队的研制具有实践性强、探索性强和综合性强的特点，有利于迅速接触前沿研究，并促使选课同学的创新能力和专业素质得到提高。\n机器人足球的教育作用不局限于大学。自1992年开始，美国政府有关部门在全国高中生中推行\"感知和认知移动机器人\"计划。高中生可免费获得\n70公斤\n重的一套零件，自行组装成遥控机器人，然后可参加有关的比赛。值得注意的是，本届世界杯赛增设了\"仿真初级组\"，其参赛者为中小学生。随着机器人技术的逐步成熟，将机器人足球活动推广到中小学势在必行。这也将对我国的教育改革及相关基础设施和产业提供新的机会和挑战。\n机器人足球的产业化前景\n机器人足球是科技、教育和产业化三位一体的事业。从技术的角度看，机器人足球的直接应用领域是各种智能机器人，包括家用机器人、医用机器人、工业机器人和军用机器人等。最大的应用领域是网络信息处理，未来网络信息空间中的软件代理将以类似于足球队员的方式，通过合作在竞争环境中完成预定任务。此外，预期在不远的将来，还将形成一个独立的产业，包括以下几个产业化方向。\n面向中小学生的普及型教学软件和机器人产品\n在2000年的世界杯赛上，已经设立了初级组比赛，来自世界各国的几十支由中小学生组成的球队参加了\"舞蹈组\"、\"一对一\"和\"二对二\"三种类型的比赛。在2001年的世界杯赛上，将举行更大规模的初级组比赛。另外，美国、日本、澳大利亚和加拿大等国已经举办或即将举办全国性的初级组比赛。相应的各种产品已经开发出来，在国外市场上已经很流行，中小学生可以自由组装自己的机器人，而且其价位即使对国内消费者也是可接受的。初级组比赛对于培养中小学生的科技素养、创新能力和实践能力具有十分重要的作用。对于改变我国中小学教育的落后状况，具有重大的现实意义。因此，在我国开展初级组比赛及相关的教学必将得到社会各界的普遍欢迎，也将得到国家有关领导人和部门的充分支持。可以预计，这将是一个非常重大的市场，但目前国内仍是一片空白。\n机器人足球的\"职业联赛\"势在必行\n机器人足球赛既是高科技的竞争，又具有足球的观赏性和娱乐性。因此，必将吸引大批的\"球迷\"。另外，随着普及程度的提高，越来越多的非专业人员可以参与非专业性的机器人足球赛，特别是仿真组比赛。可以预期，机器人足球赛将产生和人类足球赛类似的吸引力，进而形成类似的\"职业联赛\"和\"俱乐部\"体制。由于我国在机器人足球方面与世界最高水平的差距远远小于我国人类足球与世界先进水平的差距，因而会对社会产生更大的吸引力。所以，机器人足球赛本身将可以形成一个与人类足球赛类似的产业。\n相关的产品化方向\n在上述教育普及和\"职业联赛\"的带动下，有望形成一系列新的市场需求和相关的产品，如面向非专业人员的网络比赛环境、网络直播和收看软件、自动评论员、各种硬件机器人的产品和配件（高性能电源、电机、传感器、无线通讯设备）等。"}
{"content2":"一、序言\n微软的机器学习框架于2018年5月出了0.1版本，2019年5月发布1.0版本。期间各版本之间差异（包括命名空间、方法等）还是比较大的，随着1.0版发布，应该是趋于稳定了。之前在园子里也看到不少相关介绍的文章，对我的学习提供了不少帮助。由于目前资料不是很丰富，所以学习过程中也走了不少弯路，本系列的文章主要记录我学习过程中的一些心得体会，并对一些细节会做详细的解释，希望能为机器学习零基础的同学提供一些帮助。（C#零基础可不行）\n二、基本概念\n1、什么是机器学习？\n定义：一个电脑程序要完成任务（T），如果电脑获取的关于T的经验（E）越多就表现（P）得越好，那么我们就可以说这个程序‘学习’了关于T的经验。\n简单解释什么叫“机器的学习”：如果输入的经验越多表现的越好，这就叫“学习”。举个例子：传统的程序逻辑是基于算法的，在算法不变的情况下，程序就是运行100年能力也不会有提升，但机器学习是基于数据（样本）的，在算法不变的情况下，累计的有效数据越多，程序表现能力就越强。\n2、通过机器学习解决问题和传统算法解决问题的区别\n需要解决的问题：会议室进来一位男生，请他站在摄像头前面，通过一个程序评价一下这位男生身材是否很好。\n（1）传统解决方案\n首先我们分析要判断一个人身材是否很好，主要的判断特征包括：身高、体重、三围等等，然后通过一个衡量算法（比如BMI）进行计算，流程如下：\n（2）机器学习算法\n机器学习是依赖样本数据的，所以解决这个问题的思路是这样的：\n①首先你得上街去收集数据，询问你采访对象的身高、体重、三围数据，然后根据你的经验给他一个评判，形成下表：\n②对收集到的数据进行训练，形成模型，然后通过模型对要判断的对象进行评判。流程如下：\n小结：通过这个问题的解决，感觉通过机器学习来解决问题比传统方法麻烦多了，是的，对于身材判断这样的问题，人类可以很简单找到一个逻辑分析的方法，所以通过逻辑算法解决就比较方便，但有时候很多事情我们人类是很容易处理的，但我们却不知道其中的逻辑，比如：判断一张图片是否是18+图片，判断一片论文是否写得很好，判断一个人是否长得漂亮等等。这些问题人类很轻松就能处理，但无法总结出其中的规律并交给机器去执行，这时候机器学习算法就可以派上用场了。\n三、机器学习的流程\n机器学习的流程如下：\n数据准备 -> 建模 -> 训练 -> 评估 -> 应用\n在实际应用时，由于训练的过程可能时间比较长，所以我们会分两个阶段进行：\n1、学习阶段：数据准备 -> 建模 -> 训练 -> 评估 -> 保存模型\n2、消费阶段：读取模型 -> 应用\n评估的过程就是对模型的检验，我们一般会把样本数据随机分成两份，其中一部分用来学习，另一部分用来检验模型效果，判断一下我们的模型能力。\n以上是涉及到机器学习的有些最基础的理论知识，下面几篇文章会由浅入深介绍ML.NET的一些应用。\n系列文章目录：\n机器学习框架ML.NET学习笔记【1】基本概念\n机器学习框架ML.NET学习笔记【2】入门之二元分类\n机器学习框架ML.NET学习笔记【3】文本特征分析\n机器学习框架ML.NET学习笔记【4】手写数字识别\n机器学习框架ML.NET学习笔记【5】手写数字识别（续）\n机器学习框架ML.NET学习笔记【6】TensorFlow图片分类\n机器学习框架ML.NET学习笔记【7】人物图片颜值判断\n机器学习框架ML.NET学习笔记【8】目标检测\n机器学习框架ML.NET学习笔记【9】自动学习\n资源下载：\n项目源码：https://github.com/seabluescn/Study_ML.NET\n资源文件：https://gitee.com/seabluescn/ML_Assets\n（由于资源文件较大，所以放在码云平台提供下载）"}
{"content2":"python机器学习实战（三）\n版权声明：本文为博主原创文章，转载请指明转载地址\nwww.cnblogs.com/fydeblog/p/7277205.html\n前言\n这篇博客是关于机器学习中基于概率论的分类方法--朴素贝叶斯，内容包括朴素贝叶斯分类器，垃圾邮件的分类，解析RSS源数据以及用朴素贝叶斯来分析不同地区的态度.\n操作系统：ubuntu14.04 运行环境：anaconda-python2.7-jupyter notebook 参考书籍：机器学习实战和源码,机器学习（周志华） notebook writer ----方阳\n注意事项：在这里说一句，默认环境python2.7的notebook，用python3.6的会出问题，还有我的目录可能跟你们的不一样，你们自己跑的时候记得改目录，我会把notebook和代码以及数据集放到结尾的百度云盘，方便你们下载！\n1. 基于贝叶斯决策理论的分类方法\n朴素贝叶斯的特点\n优 点: 在数据较少的情况下仍然有效,可以处理多类别问题。\n缺 点: 对于输入数据的准备方式较为敏感。\n适用数据类型:标称型数据。\n贝叶斯决策理论的核心思想：选择具有最高概率的决策。（最小化每个样本的条件风险，则总体风险也就最小，就是选择最高概率，减小风险）\n2. 条件概率\n2.1 简单回顾\n条件概率在朴素贝叶斯里面是必不可少的一环，下面来简单介绍介绍\n假设现在有一个装了7块石头的罐子,其中3块是灰色的, 4块是黑色的 。如果从罐子中随机取出一块石头,那么是灰色石头的可能性是多少? 由于取石头有 7 种可能 ,其中 3种为灰色 ,所以取出灰色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢?很显然 ,是4/7 。\n如果这7块石头放在两个桶中,那么上述概率应该如何计算? （设两个桶分为A，B，A桶装了2个灰色和2个黑色的石头，B桶装了1个灰色和2个黑色的石头）\n要计算P(gray)或者P(black) ,事先得知道石头所在桶的信息会不会改变结果?你有可能巳经想到计算从B桶中取到灰色石头的概率的办法,这就是所谓的条件概率.\n来计算P(gray|bucketB),这个是条件概率，在已知是从B桶拿出石头的条件下，拿到灰色石头的概率。\n计算公式：P(gray|bucketB) = P(gray and bucketB) / P(bucketB) （将两者同时发生的概率除以前提条件发生的概率）\n我们知道P(bucketB)就是3/7，B桶的石头数/总石头数， P(gray and bucketB) 是1/7，B桶中的灰色石头数/总石头数，所以P(gray|bucketB) = 1/3\n这里说一下P(gray and bucketB) ，它等于P(bucketB|gray)乘以P(gray)的，先发生gray,然后在gray的基础上发生bucketB，就是gray and bucketB\n所以这里的公式还可以变一下，P(gray|bucketB) = P(gray and bucketB) / P(bucketB) =P(bucketB|gray)* P(gray) / P(bucketB)\n一般情况下，写成 p(c|x) = p(x|c)* p(c) / p(x) 这就是贝叶斯准则\n2.2 使用条件概率进行分类\n贝叶斯决策论中真正比较的是条件概率p(c1|x,y)和p(c2|x,y)，这些符号所代表的具体意义是，给定某个由x，y表示的数据点，想知道该数据点来自类别c1的概率是多少?数据点来自类别c2的概率又是多少?\n如果 p(c1|x,y) > p(c2|x,y) ，属于类别c1 如果 p(c2|x,y) > p(c1|x,y) ，属于类别c2\n这些概率可以有2.1的贝叶斯准则计算\n3. 使用朴素贝叶斯进行留言分类\n朴素贝叶斯的一般过程\n(1) 收集数据:可以使用任何方法。本章使用RSS源。\n(2) 准备数据:需要数值型或者布尔型数据。\n(3) 分析数据:有大量特征时,绘制特征作用不大,此时使用直方图效果更好。\n(4) 训练算法:计算不同的独立特征的条件概率。\n(5) 测试算法:计算错误率。\n(6) 使用算法:一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯命类器,不一定非要是文本\n朴素贝叶斯的两个假设 (1) 特征之间是统计独立的，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。 (2) 每个特征同等重要。\n以上两个假设是有问题的，不够严谨，但处理方便，实际效果却很好。\n3.1 准备数据：从文本中构建词向量\n词表到向量的转换函数如下：\n1 def loadDataSet(): 2 postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], 3 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], 4 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], 5 ['stop', 'posting', 'stupid', 'worthless', 'garbage'], 6 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], 7 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] 8 classVec = [0,1,0,1,0,1] #1 is abusive, 0 not 9 return postingList,classVec 10 11 def createVocabList(dataSet): 12 vocabSet = set([]) #create empty set 13 for document in dataSet: 14 vocabSet = vocabSet | set(document) #union of the two sets 15 return list(vocabSet) 16 17 def setOfWords2Vec(vocabList, inputSet): 18 returnVec = [0]*len(vocabList) 19 for word in inputSet: 20 if word in vocabList: 21 returnVec[vocabList.index(word)] = 1 22 else: print \"the word: %s is not in my Vocabulary!\" % word 23 return returnVec\n第一个loadDataSet函数是返回词条切分后的文档集合postlist（选自斑点犬爱好者留言板）和类别标签集合classvec（1代表侮辱，0则是正常言论）\n第二个createVocabList函数会返回输入数据集所有不重复词汇的列表\n第三个setOfWords2Vec函数的功能是遍历输入vocablist的所有单词,如果当初出现了InputSet中的单词，returnVec对应位数的值返回1，无则返回0\n简单来讲，第一个函数的作用是界定训练类别，看之后的文档是否含有类别中的词汇，第二个函数的作用是将一篇文档做成列表，方便后面进行标记。第三个函数则是将第二个函数生成的列表根据第一个类别词汇进行标记，将单词转化成数字，方便后面计算条件概率。\n测试一下吧(所有函数都放在bayes中)\ncd 桌面/machinelearninginaction/Ch04\n/home/fangyang/桌面/machinelearninginaction/Ch04\nimport bayes\nlistOPosts,listClasses = bayes.loadDataSet()\nmyVocabList = bayes.createVocabList(listOPosts)\nmyVocabList\nbayes.setOfWords2Vec(myVocabList,listOPosts[0])\nbayes.setOfWords2Vec(myVocabList,listOPosts[3])\n3.2 训练算法 :从词向量计算概率\n根据上面介绍的三个函数，我们知道如何将一组单词转换为一组数字，也知道一个词是否出现在一篇文档中。现在已知文档的类别，让我们使用转换得到的数字来计算条件概率吧\n还是根据上面的贝叶斯准则来计算条件概率，不过公式会有一点不一样\np(ci|w) = p(w|ci)* p(ci) / p(w) (这里的ci表示所属类别，这里有两种可能性1和0，w为向量，由多个数值组成)\n我们根据上面的公式对每个类进行计算，然后比较这两个概率值的大小。计算过程如下\n首先可以通过类别 i ( 侮辱性留言或非侮辱性留言)中文档数除以总的文档数来计算概率p(ci)，接下来计算p(w|ci)，由于p(w|ci) = p(w0,w1,w2..wn|ci)，又因为所有词都相互独立，所以p(w|ci) = p(w0|ci)p(w1|ci)p(w2|ci)...p(wn|ci)\n于是函数的伪代码相应如下\n计算每个类别中的文档数目 对每篇训练文档: 对每个类别: 如果词条出现文档中―增加该词条的计数值 增加所有词条的计数值 对每个类别: 对每个词条: 将该词条的数目除以总词条数目得到条件概率 返回每个类别的条件概率\n参考代码如下\n1 def trainNB0(trainMatrix,trainCategory): 2 numTrainDocs = len(trainMatrix) 3 numWords = len(trainMatrix[0]) 4 pAbusive = sum(trainCategory)/float(numTrainDocs) 5 p0Num = zeros(numWords); p1Num = zeros(numWords) 6 p0Denom = 0.0; p1Denom = 0.0 7 for i in range(numTrainDocs): 8 if trainCategory[i] == 1: 9 p1Num += trainMatrix[i] 10 p1Denom += sum(trainMatrix[i]) 11 else: 12 p0Num += trainMatrix[i] 13 p0Denom += sum(trainMatrix[i]) 14 p1Vect = p1Num/p1Denom 15 p0Vect = p0Num/p0Denom 16 return p0Vect,p1Vect,pAbusive\n输入的trainMatrix是文档经过setOfWords2Vec函数转换后的列表，trainCategory是每篇文档构成类别标签向量。输出是返回每个类别的概率，pAbusive等于类别和除以训练的样本数，这个就是说明一下文档类别的概率分布，没有什么其他意思\n由于要算每一个词语的概率，这里用到里numpy的array数组，可以很方便的计算每个词语的概率，即是用p0Num和p1Num来统计不同类别样本的词语所出现的次数，最后对每个元素除以该类别中的总词数\n来测试一下吧\nfrom numpy import *\nreload(bayes)\n<module 'bayes' from 'bayes.py'>\nlistOPosts,listClasses = bayes.loadDataSet()\nmyVocabList = bayes.createVocabList(listOPosts)\ntrainMat = []\nfor postinDoc in listOPosts: trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))\np0V , p1V, pAb = bayes.trainNB0(trainMat,listClasses)\np0V\np1V\n看一看在给定文档类别条件下词汇表中单词的出现概率, 看看是否正确.\n词汇表中的第一个词是cute , 其在类别 0中出现1次 ,而在类别1中从未出现。对应的条件概率分别为 0.04166667 与 0.0,该计算是正确的\n我们找找所有概率中的最大值,该值出现在p(1)数组第21个下标位置,大小为 0.15789474.可以查到该单词是stupid，这意味着它最能表征类别1的单词。\n3.3 测试算法：根据现实情况修改分类器\n利用贝叶斯分类器进行文档文类时，要计算每个元素的条件概率并相乘，若其中有一个概率值等于0，那么最后的乘积也为0，为降低这种影响，可以将所有词的出现数初始化为1 ,并将分母初始化为2 。\n相应的trainNB0()的第4行和第5行修改为：\np0Num = ones(numWords);  p1Num = ones(numWords)      #change to ones()\np0Denom = 2.0; p1Denom = 2.0                                             #change to 2.0\n另一个问题是向下溢出，乘积p(w0|ci)p(w1|ci)p(w2|ci)...p(wn|ci)太小的缘故 解决的办法是对乘积取对数\n相应的trainNB0()的第13行和第14行修改为：\np1Vect = log(p1Num/p1Denom)        #change to log()\np0Vect = log(p0Num/p0Denom)        #change to log()\n将更改好的函数命名为trainNB0_change\n现在已经准备好构建完整的分类器了。当使用numpy向量处理功能时 , 这一切变得十分简单.\n参考代码如下：\n1 def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): 2 p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult 3 p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) 4 if p1 > p0: 5 return 1 6 else: 7 return 0 8 def testingNB(): 9 listOPosts,listClasses = loadDataSet() 10 myVocabList = createVocabList(listOPosts) 11 trainMat=[] 12 for postinDoc in listOPosts: 13 trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) 14 p0V,p1V,pAb = trainNB0_change(array(trainMat),array(listClasses)) 15 testEntry = ['love', 'my', 'dalmation'] 16 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) 17 print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb) 18 testEntry = ['stupid', 'garbage'] 19 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) 20 print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n第一个函数就是两个类别的条件概率进行比较，输出最终的类别信息。 第二个函数就是一个测试函数，函数前面部分跟上面一样，后面引入两个测试样本，进行分类。\nreload(bayes)\n<module 'bayes' from 'bayes.pyc'>\nbayes.testingNB()\n['love', 'my', 'dalmation'] classified as: 0 ['stupid', 'garbage'] classified as: 1\n3.4 文档词袋模型\n我们将每个词的出现与否作为一个特征,这可以被描述为词集模型，上面就是词集模型。\n如果一个词在文档中出现不止一次,这可能意味着包含该词是否出现在文档中所不能表达的某种信息,这种方法被称为词袋模型。\n词集和词袋的区别：在词袋中,每个单词可以出现多次 ,而在词集中,每个词只能出现一次。\n为适应词袋模型 ,需要对函数setOfWords2Vec稍加修改，修改后的函数为bagOfWords2Vec，代码如下：\n1 def bagOfWords2VecMN(vocabList, inputSet): 2 returnVec = [0]*len(vocabList) 3 for word in inputSet: 4 if word in vocabList: 5 returnVec[vocabList.index(word)] += 1 6 return returnVec\n这个返回的列表表现的是单词出现的次数，还不再是是否出现\n4. 使用朴素贝叶斯过滤垃圾邮件\n4.1 准备数据：切分文本\n前面介绍的词向量是直接给定的，下面来介绍如何从文本中构建自己的词列表\n先从一个文本字符串介绍\nmySent = ' This book is the best book on python or M.L. I have ever laid eyes upon.'\nmySent.split()\n可以看到, 切分的结果不错, 但是标点符号也被当成了词的一部分.\n解决方法：可以使用正则表示式来切分句子 ,其中分隔符是除单词、数字外的任意字符串\nimport re\nregEx = re.compile('\\\\W*')\nlistOfTokens = regEx.split(mySent)\nlistOfTokens\n可以看到里面的标点没有了，但剩下一些空字符，还要进行一步，去掉这些空字符。\n[tok for tok in listOfTokens if len(tok) >0]\n空字符消掉了，我们可以看到，有的词首字母是大写的，这对句子查找很有用，但我们是构建词袋模型，所以还是希望格式统一，还要处理一下\n[tok.lower() for tok in listOfTokens if len(tok) >0]\n可以看到大写全部变成了小写，如果是想从小写变成大写，只需将tok.lower()改成top.upper()即可\n我们构建一个testParse函数，来切分文本，代码如下\n1 def textParse(bigString): #input is big string, #output is word list 2 import re 3 listOfTokens = re.split(r'\\W*', bigString) 4 return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n4.2 测试算法：使用朴素贝叶斯进行交叉验证\n参考代码如下：\n1 def spamTest(): 2 docList=[]; classList = []; fullText =[] 3 for i in range(1,26): 4 wordList = textParse(open('email/spam/%d.txt' % i).read()) 5 docList.append(wordList) 6 fullText.extend(wordList) 7 classList.append(1) 8 wordList = textParse(open('email/ham/%d.txt' % i).read()) 9 docList.append(wordList) 10 fullText.extend(wordList) 11 classList.append(0) 12 vocabList = createVocabList(docList)#create vocabulary 13 trainingSet = range(50); testSet=[] #create test set 14 for i in range(10): 15 randIndex = int(random.uniform(0,len(trainingSet))) 16 testSet.append(trainingSet[randIndex]) 17 del(trainingSet[randIndex]) 18 trainMat=[]; trainClasses = [] 19 for docIndex in trainingSet:#train the classifier (get probs) trainNB0 20 trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) 21 trainClasses.append(classList[docIndex]) 22 p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses)) 23 errorCount = 0 24 for docIndex in testSet: #classify the remaining items 25 wordVector = bagOfWords2VecMN(vocabList, docList[docIndex]) 26 if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: 27 errorCount += 1 28 print \"classification error\",docList[docIndex] 29 print 'the error rate is: ',float(errorCount)/len(testSet) 30 #return vocabList,fullText\n第一个循环是对垃圾邮件和非垃圾邮件进行切分，然后生成词列表和类标签\n第二个循环是0到50个数中随机生成10个序号\n第三个循环是将第二个循环得到的序号映射到词列表，得到训练集和相应的类别，然后进行训练算法\n第四个循环是进行错误率计算，分类出的类别与实际类别相比较，累计错误的样本数，最后除以总数，得到错误率\nbayes.spamTest()\nthe error rate is: 0.0\nbayes.spamTest()\n每次运行得出的结果可能不太一样，因为是随机选的序号\n5. 使用朴素贝叶斯分类器从个人广告中获取区域倾向\n在这个最后的例子当中,我们将分别从美国的两个城市中选取一些人,通过分析这些人发布的征婚广告信息,来比较这两个城市的人们在广告用词上是否不同。如果结论确实是不同,那么他们各自常用的词是哪些?从人们的用词当中,我们能否对不同城市的人所关心的内容有所了解？\n下面将使用来自不同城市的广告训练一个分类器,然后观察分类器的效果。我们的目的并不是使用该分类器进行分类,而是通过观察单词和条件概率值来发现与特定城市相关的内容。\n5.1 收集数据：导入RSS源\n接下来要做的第一件事是使用python下载文本,而利用RSS，这很容易得到，而Universal Feed Parser 是python最常用的RSS程序库。\n由于python默认不会安装feedparser，所以需要自己手动安装，这里附上ubuntu下的安装方法\n第一步：wget http://pypi.python.org/packages/source/f/feedparser/feedparser-5.1.3.tar.gz#md5=f2253de78085a1d5738f626fcc1d8f71\n第二步：tar zxf feedparser-5.1.3.tar.gz\n第三步：cd feedparser-5.1.3\n第四步：python setup.py install\n具体可以看到这个链接：blog.csdn.net/tinkle181129/article/details/45343267\n相关文档：http://code.google.com/p/feedparser/\nimport feedparser\nny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n上面是打开了Craigslist上的RSS源，要访问所有条目的列表，输入以下代码\nny['entries']\nlen(ny['entries'])\nOut：25\n可以构建一个类似spamTest的函数来对测试过程自动化\n1 def calcMostFreq(vocabList,fullText): 2 import operator 3 freqDict = {} 4 for token in vocabList: 5 freqDict[token]=fullText.count(token) 6 sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) 7 return sortedFreq[:30] 8 9 def localWords(feed1,feed0): 10 import feedparser 11 docList=[]; classList = []; fullText =[] 12 minLen = min(len(feed1['entries']),len(feed0['entries'])) 13 for i in range(minLen): 14 wordList = textParse(feed1['entries'][i]['summary']) 15 docList.append(wordList) 16 fullText.extend(wordList) 17 classList.append(1) #NY is class 1 18 wordList = textParse(feed0['entries'][i]['summary']) 19 docList.append(wordList) 20 fullText.extend(wordList) 21 classList.append(0) 22 vocabList = createVocabList(docList)#create vocabulary 23 top30Words = calcMostFreq(vocabList,fullText) #remove top 30 words 24 for pairW in top30Words: 25 if pairW[0] in vocabList: vocabList.remove(pairW[0]) 26 trainingSet = range(2*minLen); testSet=[] #create test set 27 for i in range(20): 28 randIndex = int(random.uniform(0,len(trainingSet))) 29 testSet.append(trainingSet[randIndex]) 30 del(trainingSet[randIndex]) 31 trainMat=[]; trainClasses = [] 32 for docIndex in trainingSet:#train the classifier (get probs) trainNB0 33 trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) 34 trainClasses.append(classList[docIndex]) 35 p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses)) 36 errorCount = 0 37 for docIndex in testSet: #classify the remaining items 38 wordVector = bagOfWords2VecMN(vocabList, docList[docIndex]) 39 if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: 40 errorCount += 1 41 print 'the error rate is: ',float(errorCount)/len(testSet) 42 return vocabList,p0V,p1V\nlocalWords函数与之前介绍的spamTest函数类似，不同的是它是使用两个RSS作为参数。\n上面还新增了一个辅助函数calcMostFreq,该函数遍历词汇表中的每个词并统计它在文本中出现的次数,然后根据出现次数从高到低对词典进行排序 , 最后返回排序最高的30个单词\n下面来测试一下\ncd 桌面/machinelearninginaction/Ch04\n/home/fangyang/桌面/machinelearninginaction/Ch04\nimport bayes\nimport feedparser\nny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\nsf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\nvocabList,pSF,pNY = bayes.localWords(ny,sf)\nthe error rate is ：0.15\nvocabList,pSF,pNY = bayes.localWords(ny,sf)\nthe error rate is ：0.4\n我们会发现这里的错误率要远高于垃圾邮件中的错误率，这是因为这里关注的是单词概率而不是实际分类，可以通过calcMostFreq函数改变移除单词数，降低错误率，因为次数最多的前30个单词涵盖了所有用词的30%，产生这种现象的原因是语言中大部分都是冗余和结构辅助性内容。\n5.2 分析数据：显示地域相关的用词\n将pSF和pNY进行排序，然后按照顺序将词打印出来，这里用getTopWords函数表示这个功能\n1 def getTopWords(ny,sf): 2 import operator 3 vocabList,p0V,p1V=localWords(ny,sf) 4 topNY=[]; topSF=[] 5 for i in range(len(p0V)): 6 if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i])) 7 if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i])) 8 sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True) 9 print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\" 10 for item in sortedSF: 11 print item[0] 12 sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True) 13 print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\" 14 for item in sortedNY: 15 print item[0]\n输入是两个RSS源，然后训练并测试朴素贝叶斯分类器，返回使用的概率值然后创建两个列表用于元组的存储。与之前返回排名最高的x个单词不同,这里可以返回大于某个阈值的所有词。这些元组会按照它们的条件概率进行排序。\nbayes.getTopWords(ny,sf)\n值得注意的现象是,程序输出了大量的停用词。移除固定的停用词（比如 there等等）看看结果会如何变化，依本书作者的经验来看,这样会使分类错误率降低。\n小结\n（1）对于分类而言,使用概率有时要比使用硬规则更为有效\n（2）贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法\n（3）独立性假设是指一个词的出现概率并不依赖于文档中的其他词，这个假设过于简单。这就是之所以称为朴素贝叶斯的原因。\n（4）下溢出就是其中一个问题,它可以通过对概率取对数来解决\n（5）词袋模型在解决文档分类问题上比词集模型有所提高\n（6）移除停用词，可降低错误率\n（7）花大量时间对切分器进行优化\n百度云链接：https://pan.baidu.com/s/1LgKUL7f4ja7mz0js-y62qg\nimport feedparser"}
{"content2":"从一个故事说起\n在应用开发如此方便的今天，我总是会听到有些人有这样的疑问，“只是做 应用 开发的话，还有没有必要学习诸如操作系统，编译原理这样的课程呢？”，亦或是会听到这样的话，“会用这个框架就行了，它底层是怎么实现的不用去管。”还记得我在大一学 C 语言的时候，就听过有同学说我以后是想从事 Java 开发的，C 语言这种学来应付一下考试就行，指针什么的其他语言又没有，就不用去管啦。\n真的是这样吗？刚好今天看到一个有意思的故事，从故事中我看到了答案，这个故事是是艾萨克·阿西莫夫 的科幻巨作《基地》中的一个片段。故事是这样的：\n在银河系中，随着战争的蔓延，文明从银河系边缘开始逐渐退化，许多星球虽然还保留着核电站等高科技产品，但是已经不知道它们是如何运作的。\n而有这样一颗小行星，我们暂且称之为 科技星 吧，在大战爆发前它搜集了银河系中的各种科学文献，并且汇聚了一大批的顶尖科学家。这颗小行星没有被卷入战争，而是将技术一直传承下去。\n科技星周围的星球觊觎它所拥有的高科技，想将之夺取。而科技星又没有自保的武装力量，在这种情况下，科技星如何自保呢？这里最有意思的地方，正是科技星所使用的科技宗教的战略。\n当后来其他星球上的高科技出现问题的时候，会向科技星求救。科技星就会派遣工程师前去维修，但是呢，他们将各种身份都进行包装，比如，工程师不叫做工程师，而是叫做“僧侣”，核电站也不叫做核电站，而是叫“圣殿”，维修也不叫做“维修”，而是叫做“祈祷”，也就是说，对核电站维修这一项工作完全被宗教化了！\n而此时科技星提供的说法是这样，因为这颗星球上的人做了坏事，比如违反法规，发动战争等等，触犯了神灵，所以神灵剥夺了他们使用能源的权力。而如果想要恢复能源，就必须对自己的行为忏悔，祈求神灵的原谅。所以当工程师进入核电站进行维修的时候，所有的星球居民一起下跪祈祷，而当核电站恢复的时候，大家纷纷称颂神的伟大。\n为什么那些拥有核电站星球的人们会对来维修的工程师“膜拜祈祷”呢？其根本原因还是在于核电站这样的高科技对他们而言是神秘的，未知的东西。 尽管他们拥有这样高科技的东西，却没有与之匹配的认知和知识储备。\n再回过头来看看一开始的问题，你是否明悟了呢？我们也是掌握着上层应用框架这种“高科技”，我们知道怎么去配置，怎么去调用，就像上面故事中普通星球的人知道怎么启动，关闭核电站一样。但一旦出了无法解决的问题，或者是遇到了什么性能瓶颈，似乎我们能做的，只能去各种技术群里，找那些大神“祈祷”了。\n再来说说人工智能\n在今天，人工智能这个名词已经逐渐为人们所熟知。而未来，人工智能的应用场景只会越来越广泛，面向 AI 编程也必然会是一种趋势。\n那么现在从事于 Web 或是 Android 等应用开发的程序员需要去学习机器学习或是深度学习相关的知识吗？我的回答是 YES 。有人说我又不想从事于人工智能的开发工作，为什么还要去学它呢呢？我想说的是，为了避免成为上面故事中那些普通星球的居民。再过几年，当你碰到一个会跟你说话的机器人或是更加奇妙的事物的时候，我们应该是对它的一些实现细节感兴趣，会有探究的欲望。而不是在那里感慨着造物主真伟大，竟能造出一个这样神奇的东西。\n话又说回来，在机器学习或是深度学习的学习过程中其实也很容易陷入到这种只会调用上层 API 而不知底层原理模型的境地。因为在今天，有很多库类都可以让你轻松实现一条语句就直接使用某个算法模型，所以很多人就不再专注于对底层模型原理的学习。在机器学习的学习过程中，相信大多数人应该都看过这样一张图，\n我们来看看这张图中 Hacking Skills 和 Substantive Expertise 的交界处，这里叫 Danger Zone，即危险区。意思是如果你只会编程和调用机器学习的 API，调参数，那么你就处于一种很危险的境地。\n结语\n一个好的程序员，不应当满足于学习到了什么新的技术或者学习了什么新的算法模型。真正有价值的东西，往往是那些人们不乐意去学的底层的，枯燥的内容。\n我们应该认识到，单单只会上层应用开发或只会调包调模型而不懂底层原理，那这种开发人员的知识体系便如空中阁楼。看起来华丽壮观，但实际上却地基不稳。一旦出现一点问题这座阁楼便会顷刻崩塌，并且无计可施，只能到处“祈祷”。\n对未知的事务保持好奇，不断学习，探究事物的本质，原理。在我看来，这才是程序员之道。\n推荐阅读 ：\n从分治算法到 MapReduce\nActor并发编程模型浅析\n大数据存储的进化史 --从 RAID 到 Hadoop Hdfs"}
{"content2":"人工智能系统Google开源的TensorFlow官方文档中文版\n2015年11月9日，Google发布人工智能系统TensorFlow并宣布开源，机器学习作为人工智能的一种类型，可以让软件根据大量的数据来对未来的情况进行阐述或预判。如今，领先的科技巨头无不在机器学习下予以极大投入。Facebook、苹果、微软，甚至国内的百度。Google 自然也在其中。「TensorFlow」是 Google 多年以来内部的机器学习系统。如今，Google 正在将此系统成为开源系统，并将此系统的参数公布给业界工程师、学者和拥有大量编程能力的技术人员，这意味着什么呢？\n打个不太恰当的比喻，如今 Google 对待 TensorFlow 系统，有点类似于该公司对待旗下移动操作系统 Android。如果更多的数据科学家开始使用 Google 的系统来从事机器学习方面的研究，那么这将有利于 Google 对日益发展的机器学习行业拥有更多的主导权。\nGoogle TensorFlow项目负责人Jeff Dean为该中文翻译项目回信称：\"看到能够将TensorFlow翻译成中文我非常激动，我们将TensorFlow开源的主要原因之一是为了让全世界的人们能够从机器学习与人工智能中获益，类似这样的协作翻译能够让更多的人更容易地接触到TensorFlow项目，很期待接下来该项目在全球范围内的应用!\"\n总览 | TensorFlow 官方文档中文版\nhttp://www.tensorfly.cn/tfdoc/tutorials/overview.html\nTensorFlow官方文档中文版_TensorFlow中文教程_TensorFlow开发中文手册[PDF]下载-极客学院Wiki\nhttp://wiki.jikexueyuan.com/project/tensorflow-zh/\n-----------------------------\n面向机器学习初学者的 MNIST 初级教程\n如果你是机器学习领域的新手, 我们推荐你从本文开始阅读. 本文通过讲述一个经典的问题, 手写数字识别 (MNIST), 让你对多类分类 (multiclass classification) 问题有直观的了解.\n面向机器学习专家的 MNIST 高级教程\n如果你已经对其它深度学习软件比较熟悉, 并且也对 MNIST 很熟悉, 这篇教程能够引导你对 TensorFlow 有初步了解.\nTensorFlow 使用指南\n这是一篇技术教程, 详细介绍了如何使用 TensorFlow 架构训练大规模模型. 本文继续使用MNIST 作为例子.\n卷积神经网络\n这篇文章介绍了如何使用 TensorFlow 在 CIFAR-10 数据集上训练卷积神经网络. 卷积神经网络是为图像识别量身定做的一个模型. 相比其它模型, 该模型利用了平移不变性(translation invariance), 从而能够更更简洁有效地表示视觉内容.\n单词的向量表示\n本文让你了解为什么学会使用向量来表示单词, 即单词嵌套 (word embedding), 是一件很有用的事情. 文章中介绍的 word2vec 模型, 是一种高效学习嵌套的方法. 本文还涉及了对比噪声(noise-contrastive) 训练方法的一些高级细节, 该训练方法是训练嵌套领域最近最大的进展.\n循环神经网络 (Recurrent Neural Network, 简称 RNN)\n一篇 RNN 的介绍文章, 文章中训练了一个 LSTM 网络来预测一个英文句子的下一个单词(该任务有时候被称作语言建模).\nMandelbrot 集合\nTensorFlow 可以用于与机器学习完全无关的其它计算领域. 这里实现了一个原生的 Mandelbrot 集合的可视化程序.\n偏微分方程\n这是另外一个非机器学习计算的例子, 我们利用一个原生实现的偏微分方程, 对雨滴落在池塘上的过程进行仿真.\nMNIST 数据下载\n一篇关于下载 MNIST 手写识别数据集的详细教程.\n视觉物体识别 (Visual Object Recognition)\n我们将毫无保留地发布已经选训练好的, 目前最先进的 Inception 物体识别模型.\nDeep Dream 视幻觉软件\n我们将发布一个 TensorFlow 版本的 Deep Dream,这是一款基于 Inception 识别模型的神经网络视幻觉软件.\n-------------------------------\nTensorFlow 官方文档中文版\n1. 新手入门\n1.1. 介绍\n1.2. 下载及安装\n1.3. 基本用法\n2. 完整教程\n2.1. 总览\n2.2. MNIST 数据下载\n2.3. MNIST 入门\n2.4. MNIST 进阶\n2.5. TENSORFLOW 运作方式入门\n2.6. 卷积神经网络\n2.7. 字词的向量表示\n2.8. 递归神经网络\n2.9. 曼德布洛特(MANDELBROT)集合\n2.10. 偏微分方程\n3. 进阶指南\n3.1. 总览\n3.2. 变量:创建、初始化、保存和加载\n3.3. TENSORBOARD:可视化学习\n3.4. TENSORBOARD:图表可视化\n3.5. 读取数据\n3.6. 线程和队列\n3.7. 添加新的OP\n3.8. 自定义数据读取\n3.9. 使用GPU\n3.10. 共享变量\n4. 资源\n4.1. 总览\n4.2. BIBTEX 引用\n4.3. 示例使用\n4.4. FAQ\n4.5. 术语表\n4.6. TENSOR排名、形状和类型\n5. 其他\n5.1. 更多TF相关资源\n5.2. 更多机器学习资源\n6. API DOC\n6.1. Cc API\n6.1.1. CLASS ENV\n6.1.2. CLASS ENV WRAPPER\n6.1.3. CLASS RANDOM ACCESS FILE\n6.1.4. CLASS SESSION\n6.1.5. CLASS STATUS\n6.1.6. CLASS TENSOR\n6.1.7. CLASS TENSOR BUFFER\n6.1.8. CLASS TENSOR SHAPE\n6.1.9. CLASS TENSOR SHAPE ITER\n6.1.10. CLASS TENSOR SHAPE UTILS\n6.1.11. CLASS THREAD\n6.1.12. CLASS WRITABLE FILE\n6.1.13. STRUCT SESSION OPTIONS\n6.1.14. STRUCT STATE\n6.1.15. STRUCT TENSOR SHAPE DIM\n6.1.16. STRUCT THREAD OPTIONS\n6.1.17. INDEX\n6.2. Images API\n6.2.1. INDEX\n6.3. Python API\n6.3.1. ARRAY OPS\n6.3.2. CLIENT\n6.3.3. CONSTANT OP\n6.3.4. CONTROL FLOW OPS\n6.3.5. FRAMEWORK\n6.3.6. IMAGE\n6.3.7. INDEX\n6.3.8. IO OPS\n6.3.9. MATH OPS\n6.3.10. NN\n6.3.11. PYTHON IO\n6.3.12. SPARSE OPS\n6.3.13. STATE OPS\n6.3.14. TRAIN\nPUBLISHED WITH GITBOOK"}
{"content2":"Infer.NET机器学习翻译系列文章将进行连载，感兴趣的朋友请收藏或关注\n本博客所有文章分类的总目录：http://www.cnblogs.com/asxinyu/p/4288836.html\n微软Infer.NET机器学习组件文章目录：http://www.cnblogs.com/asxinyu/p/4329742.html\n关于本文档的说明\n本文档基于Infer.NET 2.6对Infer.NET User Guide进行中文翻译，但进行了若干简化和提炼，按照原网站的思路进行，但不局限与其顺序。\n欢迎传播分享，必须保持原作者的信息，但禁止将该文档直接用于商业盈利。\n本人正在研究基于Infer.NET组件，并计划将其应用于实际的预测之中，该组件功能强大，封装很完善，但也有很多难以理解的地方，同时官方也给出了大量的例子，限于个人精力有限，更新时间较慢，也希望有兴趣的朋友一起来完成该项工作。\nEmail：asxinyu@qq.com\n本文章地址： http://www.cnblogs.com/asxinyu/p/4252769.html\n1.基本介绍\nInfer.NET是微软剑桥研究院基于.NET平台开发的一款机器推理组件，官方网站：http://research.microsoft.com/en-us/um/cambridge/projects/infernet/default.aspx\n该组件的采用的是Microsoft Research License Agreement 授权，Non-Commercial Use Only，除了商业使用，都可以，自己看着办。\n本章节的英文原文为，在这里。\n1.1 Infer.NET是什么?\nInfer.NET是一个概率图模型中(graphical models)用于运行贝叶斯推理机(Bayesian inference)的框架。如果对概率图模型或者贝叶斯推理的意义不了解，你可以参考一下相关资源文件，在Resources and References page页面。Infer.NET为各种应用程序所需要推理提供了先进的消息传递算法和统计程序。Infer.NET 与现有的一些推理软件有下列区别:\n1.1.1 丰富的建模语言\n支持单变量和多变量变量、也支持连续型和离散型变量。可以使用大量的各种因素进行建模，包括算术运算、线性代数、范围和积极约束、布尔操作符等等。支持不同模型的组合，以及不同类型的组合。【附：Infer.NET的内部使用了The model specification language (MSL) 建模语言，由于该组件不允许用于商业，因此源代码也没有全部开发，无法也无法搞清楚其原理】\n1.1.2 多种推理算法\n内置了多种推理算法，如Expectation Propagation, Belief Propagation (a special case of EP), Variational Message Passing and Gibbs sampling.这几个专业词汇暂时还不懂意义。\n1.1.3 为大规模推理而设计\n现有的在大多数推理程序执行过程中的开销，减慢了推理过程。而Infer.NET将推理模型编译为能够独立执行的源代码，不需要额外的开销。它也可以直接集成到您的应用程序。此外,也可以查看，分步执行源代码，或者使用标准的开发工具进行修改。\n1.1.4 用户可以进行扩展\n概率分布、因素、消息操作和推理算法都可以由用户添加。Infer.NET使用一个插件架构,使其开放性,适应性更强。而内置库支持多种模型和推理操作；但总会有特殊的情况,需要新的因素或者分布类型或者算法，这种情况下,用户可以编写自定义代码，自由与内置功能进行混合，以减少一些额外的工作。\n可以看看一个简单使用Infer.NET的例子。这个文档中的示例代码是C#，但Infer.NET支持.NET平台的所有语言。\n1.2 安装文件夹\nInfer.NET通过Zip压缩包进行发行，解压后，可以看到如下的文件夹目录：\n“Bin,Learners,Source(Distributions,Factors,Wrappers),Samples(C#,F#)”\nBin文件夹包含了Infer.NET的dll文件:\n1.Infer.Compiler.dll是一个使用Infer.NET API编写的将模型描述转换为推理代码的编译器；\n2.Infer.Runtime.dll是一个执行推理代码的程序集\n一般开发过程中只需要引用这两个dll,但在某些部署场景你可能只需要Infer.Runtime.dll。\nInfer.FSharp.dll是为了标准的F＃语言调用所做的一个封装。【不懂F#，也没有去深究】\nBin文件夹还包括了一些例子的生成文件，以及几个项目的生成文件。\n例子文件夹中有2个完整项目的源代码，1个是贝叶斯分类器，1个是推荐系统【比较复杂，还没开始研究】\n1.3 一个简单的例子\n下面是一个使用Infer.NET计算抛掷2枚硬币，结果都是正面的概率的例子，代码如下：\n1 Variable<bool> firstCoin = Variable.Bernoulli(0.5); 2 Variable<bool> secondCoin = Variable.Bernoulli(0.5); 3 Variable<bool> bothHeads = firstCoin & secondCoin; 4 InferenceEngine ie = new InferenceEngine(); 5 Console.WriteLine(\"Probability both coins are heads: \"+ie.Infer(bothHeads));\n程序输出为:\n1 Probability both coins are heads: Bernoulli(0.25)\n上述结果说明2面同时为正面的概率为0.25。上述简单的例子，包括了使用Infer.NET编程的几个关键步骤。\n1.定义概率模型：所有Infer.NET程序都需要明确定义的概率模型。上述程序的前3行就是定义3个随机变量。\n2.创建推理引擎(推理机)：所有的推理都是使用推理引擎进行的，在使用之前，必须创建和配置推理引擎。如第四行，使用默认的推理算法创建的推理引擎。\n3.执行推理查询：给定一个推理引擎，就可以使用Infer()方法来查询变量的边际分布。例子的最后一行中，引擎就去推理2个都是正面的边际分布。你还可以在这里找到更多“运行推理”的细节。\n1.4 Infer.NET工作原理\n下图是Infer.NET的推理过程：\n过程如下：\n1.首先用户创建1个 模型定义，并声明一些和模型相关推理查询需求；\n2.用户将模型定义和推理查询传递给模型编译器,后者使用指定的推理算法，创建需要执行这些查询模型的源代码。这个源代码可以写入一个文件,如果需要,也可以直接使用。\n3.C#编译器编译源代码来创建一个编译过的算法。这可以手动执行，或通过推断方法自动执行。\n4.使用一组观测值(数据),推理引擎根据用户指定的设置，执行编译算法,以便产生推理查询要求的边际分布。可以对观测值重复不同的设置，而不需要重新编译算法。\n本文章原始地址： http://www.cnblogs.com/asxinyu/p/4252769.html\n1.5 Frequently Asked Questions\n常见问题，比较简单，暂时没有翻译的必要，地址在这里。\n1.6 Resources and References\n常见问题，比较简单，暂时没有翻译的必要，地址在这里。\n2.资源下载\n这里提供Infer.NET 2.6的下载，包括了例子和基础的源码。下载地址：链接：http://pan.baidu.com/s/1o6FmVe6 密码：12wz\n如果本文章资源下载不了，或者文章显示有问题，请参考 本文原文地址：http://www.cnblogs.com/asxinyu/p/4252769.html\n另外本文的翻译电子版，以及该项目相关的翻译资源，将在最终完成后逐步开放，请关注本博客。\n翻译很累，写篇文章也费时间，兄台顺手点个推荐吧。"}
{"content2":"贝叶斯统计都是以条件概率，联合概率为基础的，所以我们从概率，条件概率，联合概率开始，然后到贝叶斯定理，最后讲一个贝叶斯的应用--垃圾邮件的过滤\n概率：事件发生的可能性，比如抛一枚硬币，正面向上的可能性有50%，掷色子点数为6的可能性为1/6。我们用符号表示为P(A)\n条件概率：满足某些条件下事件发生的可能性，比如求一个人在买了裤子的前提下再买衣服的概率，我们用符号表示为P(B|A)，即事件A发生下B发生的概率\n联合概率：多个事件同时发生的可能性，比如抛硬币两次都朝上的概率P(AB) = P(A)P(B),前提是事件是相互独立的互不影响，如果不独立则联合概率为P(AB) = P(A)P(B|A)\n当P(B) = P(B|A)时表示事件是相互独立的。\n贝叶斯定理\n利用联合概率我们可以计算出条件概率，比如知道了P(AB)和P(A)我们想知道事件A发生的前提下B发生的概率则P(B|A) = P(AB) / P(A),可如果我们想计算P(A|B)的概率呢？\n不巧的是P(A|B)并不等于P(B|A)。\n我们从联合概率知道概率乘积的顺序可以交换即P(AB) = P(BA),然后将两个概率展开P(A)P(B|A) = P(B)P(A|B),我们可以清楚的看到我们想要的P(A|B)就在其中\nP(A|B) = P(B|A)P(A) / P(B),这就是贝叶斯定理。\nP(A)就是先验概率，我们在计算前假设的某个概率，比如抛硬币正面向上的概率为50%\nP(B|A)就是后验概率，这是我们看到数据的后计算得到的\nP(A|B)就是先验概率和后验概率计算得到的，称似然度\nP(B) 在任何情况下该事件发生的概率，称标准化常量 P(B) = P(B1)P(B1|A1) + P(B2)P(B2|A2).....\n贝叶斯估计\n用极大似然估计可能会出现所要估计的概率值为0的情况，这会影响到后验概率的计算结果，使得分类有偏差。我们使用贝叶斯估计，即添加一个λ修正参数\n贝叶斯公式 P(B|A) = (P(AB) + λ) / (P(A) + Sλ)  λ >= 0 S表示随机变量各个取值的频数\n垃圾邮件的过滤\n垃圾邮件就是包含了某些词语，我们只有找到这些词，并计算出在这些词语出现的前提下是垃圾邮件的概率和不是垃圾邮件的概率，比较概率大小即\np(垃圾邮件 | w1,w2,w3,w4,w5...) 和 p(非垃圾邮件 | w1,w2,w3,w4,w5...)的大小。无法直接知道这个条件概率，所以贝叶斯又上场了，先写出联合概率，然后展开得\np(垃圾邮件 | w1,w2,w3,w4,w5...)p(w1,w2,w3,w4,w5...) = p(w1,w2,w3,w4,w5...| 垃圾邮件) p(垃圾邮件)，再化简得\np(垃圾邮件 | w1,w2,w3,w4,w5...) = p(w1,w2,w3,w4,w5...| 垃圾邮件) p(垃圾邮件) / p(w1,w2,w3,w4,w5...)\np(非垃圾邮件 | w1,w2,w3,w4,w5...) = p(w1,w2,w3,w4,w5...| 非垃圾邮件) p(非垃圾邮件) / p(w1,w2,w3,w4,w5...)\np(垃圾邮件) 就是先验概率0.5， p(w1,w2,w3,w4,w5...| 垃圾邮件) 就是后验概率，是根据所给数据计算的，因为两个概率都除以了p(w1,w2,w3,w4,w5...)，所以消去最终得到以下式子：\np(垃圾邮件 | w1,w2,w3,w4,w5...) = p(w1,w2,w3,w4,w5...| 垃圾邮件) p(垃圾邮件)\np(非垃圾邮件 | w1,w2,w3,w4,w5...) = p(w1,w2,w3,w4,w5...| 非垃圾邮件) p(非垃圾邮件)\nimport numpy as np import re pattern = re.compile(r'\\w+') class bayes(object): def __init__(self,wordList): self.wordsList = wordList self.hamCnt = 0 self.spamCnt = 0 self.pham = np.ones(len(self.wordsList)) self.pspan = np.ones(len(self.wordsList)) self.phamWordCnt = 2 self.pspanWordCnt = 2 def word_to_vector(self, word): tempVector = np.zeros(len(self.wordsList)) for line in pattern.findall(word): if line in self.wordsList: tempVector[self.wordsList.index(line)] += 1.0 return tempVector def Set_tran_data(self, word, Flag): Vector = self.word_to_vector(word.strip()) if Flag: self.pham += Vector self.phamWordCnt += sum(Vector) self.hamCnt += 1.0 else: self.pspan += Vector self.pspanWordCnt += sum(Vector) self.spamCnt += 1.0 def classifiy(self, word): Vector = self.word_to_vector(word) pA = self.hamCnt / (self.hamCnt + self.spamCnt) pB = self.spamCnt / (self.hamCnt + self.spamCnt) pAnum = sum(np.log(self.pham / self.phamWordCnt)*Vector) pBnum = sum(np.log(self.pspan / self.pspanWordCnt)*Vector) if np.log(pA) + pAnum > np.log(pB) + pBnum: return 1 else: return -1 if __name__ == \"__main__\": hamlist = [item for i in range(1, 20) for item in open(r'C:\\Users\\Administrator\\Desktop\\machinelearninginaction\\Ch04\\email\\ham\\%s.txt'% i, 'r').readlines() ] spamlist = [item for i in range(1, 20) for item in open(r'C:\\Users\\Administrator\\Desktop\\machinelearninginaction\\Ch04\\email\\spam\\%s.txt'% i, 'r').readlines()] wordList1 = [word for line in hamlist for word in pattern.findall(line) if len(word) > 2] wordList2 = [word for line in spamlist for word in pattern.findall(line) if len(word) > 2] wordList1.extend(wordList2) temp = bayes(list(set(wordList1))) tranhamlist = [open(r'C:\\Users\\Administrator\\Desktop\\machinelearninginaction\\Ch04\\email\\ham\\%s.txt'% i, 'r').read() for i in range(1, 20)] transpamlist = [open(r'C:\\Users\\Administrator\\Desktop\\machinelearninginaction\\Ch04\\email\\spam\\%s.txt'% i, 'r').read() for i in range(1, 20)] for line in tranhamlist: temp.Set_tran_data(line, True) for line in transpamlist: temp.Set_tran_data(line, False) testlist = [open(r'C:\\Users\\Administrator\\Desktop\\machinelearninginaction\\Ch04\\email\\ham\\%s.txt'% i, 'r').read() for i in range(21, 26)] for line in testlist: print temp.classifiy(line)\n数据是机器学习实战第4章"}
{"content2":"（原作：MSRA刘铁岩著《分布式机器学习：算法、理论与实践》。这一部分叙述很清晰，适合用于系统整理NN知识）\n线性模型\n线性模型是最简单的，也是最基本的机器学习模型。其数学形式如下：g(X;W)=WTX。有时，我们还会在WTX的基础上额外加入一个偏置项b，不过只要把X扩展出一维常数分量，就可以把带偏置项的线性函数归并到WTX的形式之中。线性模型非常简单明了，参数的每一维对应了相应特征维度的重要性。但是很显然，线性模型也存在一定的局限性。\n首先，线性模型的取值范围是不受限的，依据w和x的具体取值，它的输出可以是非常大的正数或者非常小的负数。然而，在进行分类的时候，我们预期得到的模型输出是某个样本属于正类（如正面评价）的可能性，这个可能性通常是取值在0和1之间的一个概率值。为了解决这二者之间的差距，人们通常会使用一个对数几率函数对线性模型的输出进行变换，得到如下公式：\n经过变换，严格地讲，g(x;w)已经不再是一个线性函数，而是由一个线性函数派生出来的非线性函数，我们通常称这类函数为广义线性函数。对数几率模型本身是一个概率形式，非常适合用对数似然损失或者交叉熵损失进行训练。\n其次，线性模型只能挖掘特征之间的线性组合关系，无法对更加复杂、更加强大的非线性组合关系进行建模。为了解决这个问题，我们可以对输入的各维特征进行一些显式的非线性预变换（如单维特征的指数、对数、多项式变换，以及多维特征的交叉乘积等），或者采用核方法把原特征空间隐式地映射到一个高维的非线性空间，再在高维空间里构建线性模型。\n核方法与支持向量机\n略\n决策树与Boosting\n略\n神经网络\n神经网络是一类典型的非线性模型，它的设计受到生物神经网络的启发。人们通过对大脑生物机理的研究，发现其基本单元是神经元，每个神经元通过树突从上游的神经元那里获取输入信号，经过自身的加工处理后，再通过轴突将输出信号传递给下游的神经元。当神经元的输入信号总和达到一定强度时，就会激活一个输出信号，否则就没有输出信号(如.7a所示)。\n.7 神经元结构与人工神经网络\n这种生物学原理如果用数学语言进行表达，就如.7b所示。神经元对输入的信号进行线性加权求和，然后依据求和结果的大小来驱动一个激活函数ψ，用以生成输出信号。生物系统中的激活函数类似于阶跃函数：\n但是，由于阶跃函数本身不连续，对于机器学习而言不是一个好的选择，因此在人们设计人工神经网络的时候通常采用连续的激活函数，比如Sigmoid函数、双曲正切函数(tanh)、校正线性单元(ReLU)等。它们的数学形式和函数形状分别如.8所示。\n.8 常用的激活函数\n1.全连接神经网络\n最基本的神经网络就是把前面描述的神经元互相连接起来，形成层次结构（如.9所示），我们称之为全连接神经网络。对于.9中这个网络而言，最左边对应的是输入节点，最右边对应的是输出节点，中间的三层节点都是隐含节点（我们把相应的层称为隐含层）。每一个隐含节点都会把来自上一层节点的输出进行加权求和，再经过一个非线性的激活函数，输出给下一层。而输出层则一般采用简单的线性函数，或者进一步使用softmax函数将输出变成概率形式。\n.9 全连接神经网络\n全连接神经网络虽然看起来简单，但它有着非常强大的表达能力。早在20世纪80年代，人们就证明了著名的通用逼近定理(Universal Approximation Theorem[28])。最早的通用逼近定理是针对Sigmoid激活函数证明的，一般情况下的通用逼近定理在2001年被证明[29]。其数学描述是，在激活函数满足一定条件的前提下，任意给定输入空间中的一个连续函数和近似精度ε，存在自然数Nε和一个隐含节点数为Nε的单隐层全连接神经网络，对这个连续函数的L∞-逼近精度小于ε。这个定理非常重要，它告诉我们全连接神经网络可以用来解决非常复杂的问题，当其他的模型（如线性模型、支持向量机等）无法逼近这类问题的分类界面时，神经网络仍然可以所向披靡、得心应手。近年来，人们指出深层网络的表达力更强，即表达某些逻辑函数，深层网络需要的隐含节点数比浅层网络少很多[30]。这对于模型存储和优化而言都是比较有利的，因此人们越来越关注和使用更深层的神经网络。\n全连接神经网络在训练过程中常常选取交叉熵损失函数，并且使用梯度下降法来求解模型参数（实际中为了减少每次模型更新的代价，使用的是小批量的随机梯度下降法）。要注意的是，虽然交叉熵损失是个凸函数，但由于多层神经网络本身的非线性和非凸本质，损失函数对于模型参数而言其实是严重非凸的。在这种情况下，使用梯度下降法求解通常只能找到局部最优解。为了解决这个问题，人们在实践中常常采用多次随机初始化或者模拟退火等技术来寻找全局意义下更优的解。近年有研究表明，在满足一定条件时，如果神经网络足够深，它的所有局部最优解其实都和全局最优解具有非常类似的损失函数值[31]。换言之，对于深层神经网络而言，“只能找到局部最优解”未见得是一个致命的缺陷，在很多时候这个局部最优解已经足够好，可以达到非常不错的实际预测精度。\n除了局部最优解和全局最优解的忧虑之外，其实关于使用深层神经网络还有另外两个困难。\n首先，因为深层神经网络的表达能力太强，很容易过拟合到训练数据上，导致其在测试数据上表现欠佳。为了解决这个问题，人们提出了很多方法，包括DropOut[32]、数据扩张（Data Augmentation）[33]、批量归一化（Batch Normalization）[34]、权值衰减（Weight Decay）[35]、提前终止（Early Stopping）[36]等，通过在训练过程中引入随机性、伪训练样本或限定模型空间来提高模型的泛化能力。\n其次，当网络很深时，输出层的预测误差很难顺利地逐层传递下去，从而使得靠近输入层的那些隐含层无法得到充分的训练。这个问题又称为“梯度消减”问题[37]。研究表明，梯度消减主要是由神经网络的非线性激活函数带来的，因为非线性激活函数导数的模都不太大，在使用梯度下降法进行优化的时候，非线性激活函数导数的逐层连乘会出现在梯度的计算公式中，从而使梯度的幅度逐层减小。为了解决这个问题，人们在跨层之间引入了线性直连，或者由门电路控制的线性通路[38]，以期为梯度信息的顺利回传提供便利。\n2.卷积神经网络\n除了全连接神经网络以外，卷积神经网络(Convolutional Neural Network，CNN)[13]也是十分常用的网络结构，尤其适用于处理图像数据。\n卷积神经网络的设计是受生物视觉系统的启发。研究表明每个视觉细胞只对于局部的小区域敏感，而大量视觉细胞平铺在视野中，可以很好地利用自然图像的空间局部相关性。与此类似，卷积神经网络也引入局部连接的概念，并且在空间上平铺具有同样参数结构的滤波器（也称为卷积核）。这些滤波器之间有很大的重叠区域，相当于有个空域滑窗，在滑窗滑到不同空间位置时，对这个窗内的信息使用同样的滤波器进行分析。这样虽然网络很大，但是由于不同位置的滤波器共享参数，其实模型参数的个数并不多，参数效率很高。\n.10描述了一个2×2的卷积核将输入图像进行卷积的例子。所谓卷积就是卷积核的各个参数和图像中空间位置对应的像素值进行点乘再求和。经过了卷积操作之后，会得到一个和原图像类似大小的新图层，其中的每个点都是卷积核在某空间局部区域的作用结果（可能对应于提取图像的边缘或抽取更加高级的语义信息）。我们通常称这个新图层为特征映射（feature map）。对于一幅图像，可以在一个卷积层里使用多个不同的卷积核，从而形成多维的特征映射；还可以把多个卷积层级联起来，不断抽取越来越复杂的语义信息。\n.10 卷积过程示意图\n除了卷积以外，池化也是卷积神经网络的重要组成部分。池化的目的是对原特征映射进行压缩，从而更好地体现图像识别的平移不变性，并且有效扩大后续卷积操作的感受野。池化与卷积不同，一般不是参数化的模块，而是用确定性的方法求出局部区域内的平均值、中位数，或最大值、最小值（近年来，也有一些学者开始研究参数化的池化算子[39]）。.11描述了对图像局部进行2×2的最大值池化操作后的效果。\n.11 池化操作示意图\n在实际操作中，可以把多个卷积层和多个池化层交替级联，从而实现从原始图像中不断抽取高层语义特征的目的。在此之后，还可以再级联一个全连接网络，在这些高层语义特征的基础上进行模式识别或预测。这个过程如.12所示。\n.12 多层卷积神经网络（N1,N2,N3表示对应单元重复的次数）\n实践中，人们开始尝试使用越来越深的卷积神经网络，以达到越来越好的图像分类效果。.13描述了近年来人们在ImageNet数据集上不断通过增加网络深度刷新错误率的历程。其中2015年来自微软研究院的深达152层的ResNet网络[40]，在ImageNet数据集上取得了低达3.57%的Top-5错误率，在特定任务上超越了普通人类的图像识别能力。\n.13 卷积神经网络不断刷新ImageNet数据集的识别结果\n.14残差学习\n随着卷积神经网络变得越来越深，前面提到的梯度消减问题也随之变得越来越显著，给模型的训练带来了很大难度。为了解决这个问题，近年来人们提出了一系列的方法，包括残差学习[40-41]（如.14所示）、高密度网络[42]（如.15所示）等。实验表明：这些方法可以有效地把训练误差传递到靠近输入层的地方，为深层卷积神经网络的训练奠定了坚实的实践基础。\n.15 高密度网络\n3.循环神经网络\n循环神经网络（Recurrent Neural Network，RNN）[14]的设计也有很强的仿生学基础。我们可以联想一下自己如何读书看报。当我们阅读一个句子时，不会单纯地理解当前看到的那个字本身，相反我们之前读到的文字会在脑海里形成记忆，而这些记忆会帮助我们更好地理解当前看到的文字。这个过程是递归的，我们在看下一个文字时，当前文字和历史记忆又会共同成为我们新的记忆，并对我们理解下一个文字提供帮助。其实，循环神经网络的设计基本就是依照这个思想。我们用表示在时刻的记忆，它是由t时刻看到的输入和时刻的记忆st-1共同作用产生的。这个过程可以用下式加以表示：\n很显然，这个式子里蕴含着对于记忆单元的循环迭代。在实际应用中，无限长时间的循环迭代并没有太大意义。比如，当我们阅读文字的时候，每个句子的平均长度可能只有十几个字。因此，我们完全可以把循环神经网络在时域上展开，然后在展开的网络上利用梯度下降法来求得参数矩阵U、W、V，如.16所示。用循环神经网络的术语，我们称之为时域反向传播(Back Propagation Through Time，BPTT)。\n.16 循环神经网络的展开\n和全连接神经网络、卷积神经网络类似，当循环神经网络时域展开以后，也会遇到梯度消减的问题。为了解决这个问题，人们提出了一套依靠门电路来控制信息流通的方法。也就是说，在循环神经网络的两层之间同时存在线性和非线性通路，而哪个通路开、哪个通路关或者多大程度上开关则由一组门电路来控制。这个门电路也是带参数并且这些参数在神经网络的优化过程中是可学习的。比较著名的两类方法是LSTM[43]和GRU[44]（如.17所示）。GRU相比LSTM更加简单一些，LSTM有三个门电路（输入门、忘记门、输出门），而GRU则有两个门电路（重置门、更新门），二者在实际中的效果类似，但GRU的训练速度要快一些，因此近年来有变得更加流行的趋势。\n.17 循环神经网络中的门电路\n循环神经网络可以对时间序列进行有效建模，根据它所处理的序列的不同情况，可以把循环神经网络的应用场景分为点到序列、序列到点和序列到序列等类型（如.18所示）。\n.18 循环神经网络的不同应用\n下面分别介绍几种循环神经网络的应用场景。\n(1)图像配文字：点到序列的循环神经网络应用\n在这个应用中，输入的是图像的编码信息（可以通过卷积神经网络的中间层获得，也可以直接采用卷积神经网络预测得到的类别标签），输出则是靠循环神经网络来驱动产生的一句自然语言文本，用以描述该图像包含的内容。\n(2)情感分类：序列到点的循环神经网络应用\n在这个应用中，输入的是一段文本信息（时序序列），而输出的是情感分类的标签（正向情感或反向情感）。循环神经网络用于分析输入的文本，其隐含节点包含了整个输入语句的编码信息，再通过一个全连接的分类器把该编码信息映射到合适的情感类别之中。\n(3)机器翻译：序列到序列的循环神经网络应用\n在这个应用中，输入的是一个语言的文本（时序序列），而输出的则是另一个语言的文本（时序序列）。循环神经网络在这个应用中被使用了两次：第一次是用来对输入的源语言文本进行分析和编码；而第二次则是利用这个编码信息驱动输出目标语言的一段文本。\n在使用序列到序列的循环神经网络实现机器翻译时，在实践中会遇到一个问题。输出端翻译结果中的某个词其实对于输入端各个词汇的依赖程度是不同的，通过把整个输入句子编码到一个向量来驱动输出的句子，会导致信息粒度太粗糙，或者长线的依赖关系被忽视。为了解决这个问题，人们在标准的序列到序列循环神经网络的基础上引入了所谓“注意力机制”。在它的帮助下，输出端的每个词的产生会利用到输入端不同词汇的编码信息。而这种注意力机制也是带参数的，可以在整个循环神经网络的训练过程中自动习得。\n神经网络尤其是深层神经网络是一个高速发展的研究领域。随着整个学术界和工业界的持续关注，这个领域比其他的机器学习领域获得了更多的发展机会，不断有新的网络结构或优化方法被提出。如果读者对于这个领域感兴趣，请关注每年发表在机器学习主流学术会议上的最新论文。\n参考文献：\n［1］Cao Z, Qin T, Liu T Y, et al. Learning to Rank: From Pairwise Approach to Listwise Approach[C]//Proceedings of the 24th international conference on Machine learning. ACM, 2007: 129-136.\n［2］Liu T Y. Learning to rank for information retrieval[J]. Foundations and Trends in Information Retrieval, 2009, 3(3): 225-331.\n［3］Kotsiantis S B, Zaharakis I, Pintelas P. Supervised Machine Learning: A Review of Classification Techniques[J]. Emerging Artificial Intelligence Applications in Computer Engineering, 2007, 160: 3-24.\n［4］Chapelle O, Scholkopf B, Zien A. Semi-supervised Learning (chapelle, o. et al., eds.; 2006)[J]. IEEE Transactions on Neural Networks, 2009, 20(3): 542-542.\n［5］He D, Xia Y, Qin T, et al. Dual learning for machine translation[C]//Advances in Neural Information Processing Systems. 2016: 820-828.\n［6］Hastie T, Tibshirani R, Friedman J. Unsupervised Learning[M]//The Elements of Statistical Learning. New York: Springer, 2009: 485-585.\n［7］Sutton R S, Barto A G. Reinforcement Learning: An Introduction[M]. Cambridge: MIT press, 1998.\n［8］Seber G A F, Lee A J. Linear Regression Analysis[M]. John Wiley & Sons, 2012.\n［9］Harrell F E. Ordinal Logistic Regression[M]//Regression modeling strategies. New York: Springer, 2001: 331-343.\n［10］Cortes C, Vapnik V. Support-Vector Networks[J]. Machine Learning, 1995, 20(3): 273-297.\n［11］Quinlan J R. Induction of Decision Trees[J]. Machine Learning, 1986, 1(1): 81-106.\n［12］McCulloch, Warren; Walter Pitts (1943). \"A Logical Calculus of Ideas Immanent in Nervous Activity\" [EB]. Bulletin of Mathematical Biophysics. 5(4): 115-133.\n［13］LeCun Y, Jackel L D, Bottou L, et al. Learning Algorithms for Classification: A Comparison on Handwritten Digit Recognition[J]. Neural networks: The Statistical Mechanics Perspective, 1995, 261: 276.\n［14］Elman J L. Finding structure in time[J]. Cognitive Science, 1990, 14(2): 179-211.\n［15］周志华. 机器学习［M］. 北京：清华大学出版社，2017.\n［16］Tom Mitchell. Machine Learning[M]. McGraw-Hill, 1997.\n［17］Nasrabadi N M. Pattern Recognition and Machine Learning[J]. Journal of Electronic Imaging, 2007, 16(4): 049901.\n［18］Voorhees E M. The TREC-8 Question Answering Track Report[C]//Trec. 1999, 99: 77-82.\n［19］Wang Y, Wang L, Li Y, et al. A Theoretical Analysis of Ndcg Type Ranking Measures[C]//Conference on Learning Theory. 2013: 25-54.\n［20］Devroye L, Gyrfi L, Lugosi G. A Probabilistic Theory of Pattern Recognition[M]. Springer Science & Business Media, 2013.\n［21］Breiman L, Friedman J, Olshen R A, et al. Classification and Regression Trees[J]. 1984.\n［22］Quinlan J R. C4. 5: Programs for Machine Learning[M]. Morgan Kaufmann, 1993.\n［23］Iba W, Langley P. Induction of One-level Decision Trees[J]//Machine Learning Proceedings 1992. 1992: 233-240.\n［24］Breiman L. Bagging predictors[J]. Machine Learning, 1996, 24(2): 123-140.\n［25］Schapire R E. The Strength of Weak Learnability[J]. Machine Learning, 1990, 5(2): 197-227.\n［26］Schapire R E, Freund Y, Bartlett P, et al. Boosting the Margin: A New Explanation for The Effectiveness of Voting Methods[J]. Annals of Statistics, 1998: 1651-1686.\n［27］Friedman J H. Greedy Function Approximation: A Gradient Boosting Machine[J]. Annals of statistics, 2001: 1189-1232.\n［28］Gybenko G. Approximation by Superposition of Sigmoidal Functions[J]. Mathematics of Control, Signals and Systems, 1989, 2(4): 303-314.\n［29］Csáji B C. Approximation with Artificial Neural Networks[J]. Faculty of Sciences, Etvs Lornd University, Hungary, 2001, 24: 48.\n［30］Sun S, Chen W, Wang L, et al. On the Depth of Deep Neural Networks: A Theoretical View[C]//AAAI. 2016: 2066-2072.\n［31］Kawaguchi K. Deep Learning Without Poor Local Minima[C]//Advances in Neural Information Processing Systems. 2016: 586-594.\n［32］Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A Simple Way to Prevent Neural Networks from Overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.\n［33］Tanner M A, Wong W H. The Calculation of Posterior Distributions by Data Augmentation[J]. Journal of the American statistical Association, 1987, 82(398): 528-540.\n［34］ Ioffe S, Szegedy C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift[C]//International Conference on Machine Learning. 2015: 448-456.\n［35］Krogh A, Hertz J A. A Simple Weight Decay Can Improve Generalization[C]//Advances in neural information processing systems. 1992: 950-957.\n［36］Prechelt L. Automatic Early Stopping Using Cross Validation: Quantifying the Criteria[J]. Neural Networks, 1998, 11(4): 761-767.\n［37］Bengio Y, Simard P, Frasconi P. Learning Long-term Dependencies with Gradient Descent is Difficult[J]. IEEE Transactions on Neural Networks, 1994, 5(2): 157-166.\n［38］Srivastava R K, Greff K, Schmidhuber J. Highway networks[J]. arXiv preprint arXiv:1505.00387, 2015.\n［39］Lin M, Chen Q, Yan S. Network in Network[J]. arXiv preprint arXiv:1312.4400, 2013.\n［40］He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 770-778.\n［41］He K, Zhang X, Ren S, et al. Identity Mappings in Deep Residual Networks[C]//European Conference on Computer Vision. Springer, 2016: 630-645.\n［42］Huang G, Liu Z, Weinberger K Q, et al. Densely Connected Convolutional Networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017, 1(2): 3.\n［43］Hochreiter S, Schmidhuber J. Long Short-term Memory[J]. Neural Computation, 1997, 9(8): 1735-1780.\n［44］Cho K, Van Merrinboer B, Gulcehre C, et al. Learning Phrase Representations Using RNN Encoder-decoder for Statistical Machine Translation[J]. arXiv preprint arXiv:1406.1078, 2014.\n［45］Cauchy A. Méthode générale pour la résolution des systemes d’équations simultanées[J]. Comp. Rend. Sci. Paris, 1847, 25(1847): 536-538.\n［46］Hestenes M R, Stiefel E. Methods of Conjugate Gradients for Solving Linear Systems[M]. Washington, DC: NBS, 1952.\n［47］Wright S J. Coordinate Descent Algorithms[J]. Mathematical Programming, 2015, 151(1): 3-34.\n［48］Polyak B T. Newton’s Method and Its Use in Optimization[J]. European Journal of Operational Research, 2007, 181(3): 1086-1096.\n［49］Dennis, Jr J E, Moré J J. Quasi-Newton Methods, Motivation and Theory[J]. SIAM Review, 1977, 19(1): 46-89.\n［50］Frank M, Wolfe P. An Algorithm for Quadratic Programming[J]. Naval Research Logistics (NRL), 1956, 3(1-2): 95-110.\n［51］Nesterov, Yurii. A method of solving a convex programming problem with convergence rate O (1/k2)[J]. Soviet Mathematics Doklady, 1983, 27(2).\n［52］Karmarkar N. A New Polynomial-time Algorithm for Linear Programming[C]//Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing. ACM, 1984: 302-311.\n［53］Geoffrion A M. Duality in Nonlinear Programming: A Simplified Applications-oriented Development[J]. SIAM Review, 1971, 13(1): 1-37.\n［54］Johnson R, Zhang T. Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction[C]//Advances in Neural Information Processing Systems. 2013: 315-323.\n［55］Sutskever I, Martens J, Dahl G, et al. On the Importance of Initialization and Momentum in Deep Learning[C]//International Conference on Machine Learning. 2013: 1139-1147.\n［56］Duchi J, Hazan E, Singer Y. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization[J]. Journal of Machine Learning Research, 2011, 12(7): 2121-2159.\n［57］Tieleman T, Hinton G. Lecture 6.5-rmsprop: Divide the Gradient By a Running Average of Its Recent Magnitude[J]. COURSERA: Neural networks for machine learning, 2012, 4(2): 26-31.\n［58］Zeiler M D. ADADELTA: An Adaptive Learning Rate Method[J]. arXiv preprint arXiv:1212.5701, 2012.\n［59］Kingma D P, Ba J. Adam: A Method for Stochastic Optimization[J]. arXiv preprint arXiv:1412.6980, 2014.\n［60］Reddi S, Kale S, Kumar S. On the Convergence of Adam and Beyond[C]// International Conference on Learning Representations, 2018.\n［61］Hazan E, Levy K Y, Shalev-Shwartz S. On Graduated Optimization for Stochastic Non-convex Problems[C]//International Conference on Machine Learning. 2016: 1833-1841."}
{"content2":"版本：0.1\n本文是知识的综述，内容基本来源于网络和以前学到的东西。欢迎补充和更正。\n对于标题的三个定义，大部分情况下不需要分清楚。但当我们提到强人工智能时，有必要把相关的定义解释清楚，以便更准确的理解和交流。其实随着人工智能的不断发展，每个定义也在不停地变化。\n人工智能\n人工智能是一个基本的概念。从广义上来说，计算机实现的都可以算作人工智能，因为计算、记忆等就是智能的一部分。最开始，人类还没有计算机的时候，其实很多人就在考虑意识和智能到底是什么了，当时这还算是个哲学问题。到了机械制造较发达以后，有些人就觉得大脑就是一堆齿轮。当然还有把大脑想成别的东西的。从有了一定的制造能力后，人们还是很想制造出智能的，这时候应该是人工智能这个概念产生的时候。顺便提一句，人类也是最近一两百年才理解大脑才是思维的中心，而不是心脏。\n后来，计算机的产生与发展来源于军事的解密，后来造原子弹之类的。当时的计算机就是纯粹完成一些人类的需要几个月，一堆人才能做出来的数学题。（参考《暗算》的场景，中国的人海战术）这时候的机器有了记忆和计算的能力。\n再后来，随着计算机的硬件和软件的越来越通用，科学家们开始考虑如何将人类的知识和判断用计算机来实现，甚至实现和人一样的智能。如果这个目标实现了，理所当然可以叫做“人工智能”。当然最初的时候，大家以为写一堆if...else...就能把医学专家，石油勘探专家复制出来了，或者让计算机能够理解自然语言。这个过程中有很多成功的例子，可能有些现在还在解决实际的问题。但这个方向之算是一个捷径吧，有些专业的问题在人的逻辑认识复杂度之内的，都可以用这个方法来解决。但是很多模糊的，或者太庞杂的问题，就很难写出足够的if...else来解决了。这样的系统也无法解决通用问题。\n神经网络和机器学习\n差不多在专家系统发展的同时，现在所称的人工智能就出现了。一个方向是研究神经细胞，并模仿神经细胞的方式来解决问题，叫做人工神经网络（可简称神经网络）；另外一个方向是建模，并用统计的方法来解决问题，叫做机器学习。这时候，基本上专家系统就被排除在人工智能的概念之外了。因为一堆硬编码的if...else...已经不能体现出计算机系统的优越性了（半开玩笑，专家系统不容易解决更多通用的问题，开发成本也相对较高）。\n神经网络和机器学习这两个概念其实是相互重叠的，但又不完全相同。神经网络是从其结构来说的，类似于人类的神经结构，有神经细胞，树突和轴突，它们之间还会形成突触。另外还有整合、激发等特点。而机器学习是从其功能上来说的。主要是指计算机通过已知数据的训练，对逻辑有了统计上的认知，从而能够对需要解决的问题给出答案，甚至是未知问题。这里的“未知问题”不表示所有的未知问题，而是很多我们看起来是未知问题，但其抽象的结果还是一个已知问题，也可叫做元问题。\n现在很多机器学习的算法都是基于神经网络的，也有一些不是神经网络的算法用于机器学习。（我读书少，就不列举了。）大部分神经网络（也许所有）的算法都可以转化为矩阵。无外乎是节点及其之间的联系。有些神经网络几乎不需要学习，所以也不能管它们叫做机器学习。但大部分这类算法只能解决有限类型的问题，实际应用范围比较窄。\n当今的人工智能一般是灰盒的，知道输入和输出，但不一定能看懂里面的逻辑。其实我们日常的生活大部分都是这样的，告知需求，然后等待结果。这是人类活动和合作的模式。比如，一个软件项目，用户只关心功能方面的问题，不关心里面的代码和具体写出来的逻辑；理发的时候我们关心发型，不关心哪里要先剪，具体要剪多少。我们希望人工智能也是这样的，也只有这样才像个智能。但实际情况是，虽然解决了从输入到输出不需要对中间的逻辑过分关心的问题。但需要花大量时间来调整输入，以及整个模型的参数，直到给出的输出在一定输入范围内的答案满意为止。换句话说，虽然有了一个通用的模型可以解决很多问题，但还是要在每个具体的的问题上花很多时间去创建一个特定的模型。和人相比，计算机的优势是一旦模型创建出来，就能很稳定，且大规模、快速的解决问题。\n强人工智能\n大概从上世纪80年代的人工智能热以来，人工智能虽然有了很大的发展，发明了BP这样的能够处理复杂网络的算法，也解决了很多问题。但其智能仍然完全无法和人，甚至简单的虫子相提并论。（最近好像虫子项目已经能实现虫子级的智能了，等学了之后来更新。）所以，需要用个词语来区别于现有的人工智能。强人工智能和人工智能差了一个“强”字，以前我给这个概念用的“真”字。“强”当然是指的比现在的人工智能要强，其也是一个模糊的概念，到底是要强到像人这样呢，还是比人还强？这个概念随着人工智能的发展，会进一步变化。“真”指的就是和人一样的，真正的智能。（好吧，这也是相对的，没准儿某个智力发达的物种不认同我们是有真智能的物种。）有些地方还会用“超”人工智能的概念，指的是比人类还高的智能。\n现在的人工智能已经在各行各业广泛的应用起来，解决了很多实际的问题，也大大的解放了生产力。貌似人类也不需要更强人工智能。在没有电、没有手机之前，估计很多人也没觉得需要什么新东西。关于为什么需要强人工智能的问题，下一篇再讨论。"}
{"content2":"作者：无影随想\n时间：2016年3月。\n出处：https://zhaokv.com/machine_learning/2016/03/ml-metric.html\n声明：版权所有，转载请注明出处\n在使用机器学习算法的过程中，针对不同场景需要不同的评价指标，在这里对常用的指标进行一个简单的汇总。\n一、分类\n1. 精确率与召回率\n精确率与召回率多用于二分类问题。精确率（Precision）指的是模型判为正的所有样本中有多少是真正的正样本；召回率（Recall）指的是所有正样本有多少被模型判为正样本，即召回。设模型输出的正样本集合为$A$，真正的正样本集合为$B$，则有：\n$\\text{Precision}(A,B)=\\frac{|A\\bigcap B|}{|A|},\\text{Recall}(A,B)=\\frac{|A\\bigcap B|}{|B|}$。\n有时候我们需要在精确率与召回率间进行权衡，一种选择是画出精确率-召回率曲线（Precision-Recall Curve），曲线下的面积被称为AP分数（Average precision score）；另外一种选择是计算$F_{\\beta}$分数：\n$F_{\\beta}=(1+\\beta^2)\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{\\beta^2\\cdot\\text{precision}+\\text{recall}}$。\n当$\\beta=1$称为$F_1$分数，是分类与信息检索中最常用的指标之一。\n2. ROC\n设模型输出的正样本集合为$A$，真正的正样本集合为$B$，所有样本集合为$C$，我们称$\\frac{|A\\bigcap B|}{|B|}$为真正率（True-positive rate），$\\frac{|A- B|}{|C-B|}$为假正率（False-positive rate）。\nROC曲线适用于二分类问题，以假正率为横坐标，真正率为纵坐标的曲线图，如：\nAUC分数是曲线下的面积（Area under curve），越大意味着分类器效果越好。\n3. 对数损失\n对数损失（Log loss）亦被称为逻辑回归损失（Logistic regression loss）或交叉熵损失（Cross-entropy loss）。\n对于二分类问题，设$y\\in\\{0,1\\}$且$p={\\rm Pr}(y=1)$，则对每个样本的对数损失为：\n$L_{\\rm log}(y,p)=-\\log{\\rm Pr}(y|p)=-(y\\log(p)+(1-y)\\log(1-p))$。\n可以很容易地将其扩展到多分类问题上。设$Y$为指示矩阵，即当样本$i$的分类为$k$时$y_{i,k}=1$；设$P$为估计的概率矩阵，即$p_{i,k}={\\rm Pr}(t_{i,k}=1)$，则对每个样本的对数损失为：\n$L_{\\log}(Y_i,P_i)=-\\log{\\rm Pr}(Y_i|P_i)=\\sum\\limits_{k=1}^{K}y_{i,k}\\log p_{i,k}$。\n4. 铰链损失\n铰链损失（Hinge loss）一般用来使“边缘最大化”（maximal margin）。\n铰链损失最开始出现在二分类问题中，假设正样本被标记为1，负样本被标记为-1，$y$是真实值，$w$是预测值，则铰链损失定义为：\n$L_{\\text{Hinge}}(w, y)=\\max\\{1-wy,0\\}=|1-wy|_+$。\n然后被扩展到多分类问题，假设$y_w$是对真实分类的预测值，$y_t$是对非真实分类预测中的最大值，则铰链损失定义为：\n$L_{\\text{Hinge}}(y_w, y_t)=\\max\\{1+y_t-y_w,0\\}$。\n注意，二分类情况下的定义并不是多分类情况下定义的特例。\n5. 混淆矩阵\n混淆矩阵（Confusion Matrix）又被称为错误矩阵，通过它可以直观地观察到算法的效果。它的每一列是样本的预测分类，每一行是样本的真实分类（反过来也可以），顾名思义，它反映了分类结果的混淆程度。混淆矩阵$i$行$j$列的原始是原本是类别$i$却被分为类别$j$的样本个数，计算完之后还可以对之进行可视化：\n6. kappa系数\nkappa系数（Cohen's kappa）用来衡量两种标注结果的吻合程度，标注指的是把N个样本标注为C个互斥类别。计算公式为\n$\\mathcal{K}=\\frac{p_o-p_e}{1-p_e}=1-\\frac{1-p_o}{1-p_e}$。\n其中$p_o$是观察到的符合比例，$p_e$是由于随机性产生的符合比例。当两种标注结果完全相符时，$\\mathcal{K}=1$，越不相符其值越小，甚至是负的。\n是不是云里来雾里去的，现在举个栗子，对于50个测试样本的二分类问题，预测与真实分布情况如下表：\nGROUND\n1\n0\nPREDICT\n1\n20\n5\n0\n10\n15\n预测与真实值相符共有20+15个，则观察到的符合比例为$p_o=(20+15)/50=0.7$。计算$p_e$比较复杂，PREDICT预测为1的比例为0.5，GROUND中1的比例为0.6，从完全随机的角度来看，PREDICT与GROUND均为1的概率为0.5*0.6=0.3，PREDICT与GROUND均为0的概率为0.5*0.4=0.2，则PREDICT与GROUND由于随机性产生的符合比例为0.2+0.3=0.5，即$p_e=0.5$，最后求得$\\mathcal{K}=\\frac{p_o-p_e}{1-p_e}=\\frac{0.7-0.5}{1-0.5}=0.4$。\n7. 准确率\n准确率（Accuracy）衡量的是分类正确的比例。设$\\hat{y}_i$是是第$i$个样本预测类别，$y_i$是真是类别，在$n_{\\rm sample}$个测试样本上的准确率为\n${\\rm accuracy}=\\frac{1}{n_{\\rm sample}}\\sum\\limits_{i=1}^{n_{\\rm sample}}1(\\hat{y}_i=y_i)$。\n其中$1(x)$是indicator function，当预测结果与真实情况完全相符时准确率为1，两者越不相符准确率越低。\n虽然准确率适用范围很广，可用于多分类以及多标签等问题上，但在多标签问题上很严格，在有些情况下区分度较差。\n8. 海明距离\n海明距离（Hamming Distance）用于需要对样本多个标签进行分类的场景。对于给定的样本$i$，$\\hat{y}_{ij}$是对第$j$个标签的预测结果，${y}_{ij}$是第$j$个标签的真实结果，$L$是标签数量，则$\\hat{y}_i$与$y_i$间的海明距离为\n$D_{Hamming}(\\hat{y}_i,y_i)=\\frac{1}{L}\\sum\\limits_{j=1}^L 1(\\hat{y}_{ij}\\neq y_{ij})$。\n其中$1(x)$是indicator function。当预测结果与实际情况完全相符时，距离为0；当预测结果与实际情况完全不符时，距离为1；当预测结果是实际情况的真子集或真超集时，距离介于0到1之间。\n我们可以通过对所有样本的预测情况求平均得到算法在测试集上的总体表现情况，当标签数量$L$为1时，它等于1-Accuracy，当标签数$L>1$时也有较好的区分度，不像准确率那么严格。\n9. 杰卡德相似系数\n杰卡德相似系数（ Jaccard similarity coefficients）也是用于需要对样本多个标签进行分类的场景。对于给定的样本$i$，$\\hat{y}_i$是预测结果，${y}_i$是真实结果，$L$是标签数量，则第$i$个样本的杰卡德相似系数为\n$J(\\hat{y}_i,y_i)=\\frac{|\\hat{y}_i\\bigcap y_i|}{|\\hat{y_i}\\bigcup y_i|}$。\n它与海明距离的不同之处在于分母。当预测结果与实际情况完全相符时，系数为1；当预测结果与实际情况完全不符时，系数为0；当预测结果是实际情况的真子集或真超集时，距离介于0到1之间。\n我们可以通过对所有样本的预测情况求平均得到算法在测试集上的总体表现情况，当标签数量$L$为1时，它等于Accuracy。\n10. 多标签排序\n在这节我们介绍一些更精细化的多标签分类效果衡量工具。设真实标签分类情况为$y\\in\\{0, 1\\}^{n_\\text{samples} \\times n_\\text{labels}}$，分类器预测情况为$\\hat{f}\\in\\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}$。\n10.1 涵盖误差\n涵盖误差（Coverage error）计算的是预测结果中平均包含多少真实标签，适用于二分类问题。涵盖误差定义为：\n$coverage(y,\\hat{f})=\\frac{1}{n_\\text{samples}}\\sum\\limits_{i=1}^{n_\\text{samples}}\\max\\limits_{j:y_{ij}=1}\\text{rank}_{ij}$，\n其中$\\text{rank}_{ij}=\\left|\\left\\{k:\\hat{f}_{ik}\\ge \\hat{f}_{ij} \\right\\}\\right|$。可以看到它实际衡量的是真实标签中有多少排在预测结果的前面。\n10.2 标签排序平均精度\n标签排序平均精度（Label ranking average precision）简称LRAP，它比涵盖误差更精细：\n$LRAP(y,\\hat{f})=\\frac{1}{n_\\text{samples}}\\sum\\limits_{i=1}^{n_\\text{samples}}\\frac{1}{|y_i|}\\sum\\limits_{j:y_{ij}=1}\\frac{|\\mathcal{L}_{ij}|}{\\text{rank}_{ij}}$，\n其中$\\mathcal{L}_{ij}=\\left\\{k:y_{ik}=1,\\hat{f}_{ik}\\ge\\hat{f}_{ij}\\right\\}$，$\\text{rank}_{ij}=\\left|\\left\\{k:\\hat{f}_{ik}\\ge \\hat{f}_{ij} \\right\\}\\right|$。\n10.3 排序误差\n排序误差（Ranking loss）进一步精细考虑排序情况：\n$ranking(y,\\hat{f})=\\frac{1}{n_\\text{samples}}\\sum\\limits_{i=1}^{n_\\text{samples}}\\frac{1}{|y_i|(n_\\text{labels}-|y_i|))}\\left|\\mathcal{L}_{ij} \\right|$，\n其中$\\mathcal{L}_{ij}=\\left\\{(k,l):\\hat{f}_{ik}<\\hat{f}_{ij}, y_{ik}=1, y_{il}=0\\right\\}$。\n二、拟合\n拟合问题比较简单，所用到的衡量指标也相对直观。假设$y_i$是第$i$个样本的真实值，$\\hat{y}_i$是对第$i$个样本的预测值。\n1. 平均绝对误差\n平均绝对误差MAE（Mean Absolute Error）又被称为$l1$范数损失（$l1$-norm loss）：\n${\\rm MAE}(y, \\hat{y})=\\frac{1}{n_{\\rm samples}}\\sum\\limits_{i=1}^{n_{\\rm samples}}|y_i-\\hat{y}_i|$。\n2. 平均平方误差\n平均平方误差MSE（Mean Squared Error）又被称为$l2$范数损失（$l2$-norm loss）：\n${\\rm MSE}(y, \\hat{y})=\\frac{1}{n_{\\rm samples}}\\sum\\limits_{i=1}^{n_{\\rm samples}}(y_i-\\hat{y}_i)^2$。\n3. 解释变异\n解释变异（ Explained variance）是根据误差的方差计算得到的：\n${\\rm explained variance}(y,\\hat{y})=1-\\frac{{\\rm Var}\\{y-\\hat{y}\\}}{{\\rm Var}{y}}$。\n4. 决定系数\n决定系数（Coefficient of determination）又被称为$R^2$分数：\n$R^2(y,\\hat{y})=1-\\frac{\\sum_{i=1}^{n_{\\rm samples}}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n_{\\rm samples}}(y_i-\\bar{y})^2}$，\n其中$\\bar{y}=\\frac{1}{n_{\\rm samples}}\\sum_{i=1}^{n_{\\rm samples}}y_i$。\n三、聚类\n1 . 兰德指数\n兰德指数（Rand index）需要给定实际类别信息$C$，假设$K$是聚类结果，$a$表示在$C$与$K$中都是同类别的元素对数，$b$表示在$C$与$K$中都是不同类别的元素对数，则兰德指数为：\n${\\rm RI}=\\frac{a+b}{C_2^{n_{\\rm samples}}}$，\n其中$C_2^{n_{\\rm samples}}$数据集中可以组成的总元素对数，RI取值范围为$[0,1]$，值越大意味着聚类结果与真实情况越吻合。\n对于随机结果，RI并不能保证分数接近零。为了实现“在聚类结果随机产生的情况下，指标应该接近零”，调整兰德系数（Adjusted rand index）被提出，它具有更高的区分度：\n${\\rm ARI}=\\frac{{\\rm RI}-E[{\\rm RI}]}{\\max({\\rm RI})-E[{\\rm RI}]}$，\n具体计算方式参见Adjusted Rand index。\nARI取值范围为$[-1,1]$，值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲，ARI衡量的是两个数据分布的吻合程度。\n2. 互信息\n互信息（Mutual Information）也是用来衡量两个数据分布的吻合程度。假设$U$与$V$是对$N$个样本标签的分配情况，则两种分布的熵（熵表示的是不确定程度）分别为：\n$H(U)=\\sum\\limits_{i=1}^{|U|}P(i)\\log (P(i)), H(V)=\\sum\\limits_{j=1}^{|V|}P'(j)\\log (P'(j))$，\n其中$P(i)=|U_i|/N,P'(j)=|V_j|/N$。$U$与$V$之间的互信息（MI）定义为：\n${\\rm MI}(U,V)=\\sum\\limits_{i=1}^{|U|}\\sum\\limits_{j=1}^{|V|}P(i,j)\\log\\left ( \\frac{P(i,j)}{P(i)P'(j)}\\right )$，\n其中$P(i,j)=|U_i\\bigcap V_j|/N$。标准化后的互信息（Normalized mutual information）为：\n${\\rm NMI}(U,V)=\\frac{{\\rm MI}(U,V)}{\\sqrt{H(U)H(V)}}$。\n与ARI类似，调整互信息（Adjusted mutual information）定义为：\n${\\rm AMI}=\\frac{{\\rm MI}-E[{\\rm MI}]}{\\max(H(U), H(V))-E[{\\rm MI}]}$。\n利用基于互信息的方法来衡量聚类效果需要实际类别信息，MI与NMI取值范围为$[0,1]$，AMI取值范围为$[-1,1]$，它们都是值越大意味着聚类结果与真实情况越吻合。\n3. 轮廓系数\n轮廓系数（Silhouette coefficient）适用于实际类别信息未知的情况。对于单个样本，设$a$是与它同类别中其他样本的平均距离，$b$是与它距离最近不同类别中样本的平均距离，轮廓系数为：\n$s=\\frac{b-a}{\\max(a,b)}$。\n对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。\n轮廓系数取值范围是$[-1,1]$，同类别样本越距离相近且不同类别样本距离越远，分数越高。\n四、信息检索\n信息检索评价是对信息检索系统性能（主要满足用户信息需求的能力）进行评估，与机器学习也有较大的相关性，感兴趣的可以参考这篇不错的博文。\n四、总结\n上面介绍了非常多的指标，实际应用中需要根据具体问题选择合适的衡量指标。那么具体工作中如何快速使用它们呢？优秀的Python机器学习开源项目Scikit-learn实现了上述绝指标的大多数，使用起来非常方便。"}
{"content2":"前面写了个简单的线性代数系列文章，目的就是让大家在接触SVD分解前，先了解回忆一下线性代数的基本知识，有助于大家理解SVD分解。不至于一下被大量的线性代数操作搞晕。这次终于开始正题——SVD的介绍了。\n所谓SVD，就是要把矩阵进行如下转换：A = USVT\nthe columns of U are the eigenvectors of the AAT matrix and the columns of V are the eigenvectors of the ATA matrix. VT is the transpose of V and S is a diagonal matrix. By definition the nondiagonal elements of diagonal matrices are zero. The diagonal elements of S are a special kind of values of the original matrix. These are termed the singular values of A.\n1 The Frobenius Norm\n一个矩阵所有元素的平方和再开方称为这个矩阵的Frobenius Norm。特殊情况下，行矩阵的Frobenius Norm为该向量的长度\n2 计算A转置 A*At At*A\n3 计算S\n在SVD中，将AAt的特征值从大到小排列，并开方，得到的就是奇异值。\n比如上图中，特征值为40，10.因此奇异值为6.32,3.16。矩阵的奇异值有如下特性：\na 矩阵的奇异值乘积等于矩阵行列式的值 6.32*3.16 = 20 = |A|\nb 矩阵A的 Frobenius Norm等于奇异值的平方和的开方\n总结一下计算S的步骤：1 计算AT 和ATA；2 计算ATA的特征值，排序并开方。\n由此可以得到S，下面来看如何计算 U，VT\n4  计算V和VT\n利用ATA的特征值来计算特征向量\n既然刚才提到V就是特征向量的组合，那么\n5 计算U\nA = USVT\nAV = USVTV = US\nAVS-1 = USS-1\nU = AVS-1\n6 计算SVD\n可以看出，SVD可以对矩阵进行分解重建。\n7 降维的SVD\n如果我们只保留前k个最大的奇异值，前k列个U，前k行个V，相当于将数据中占比不大的噪音进行过滤，这样既可以有效地对数据进行泛化，又起到了降维减少运算量的目的。是不是很奇妙？\n8 实际用途\n我们实际的工作中，经常会用到这种降维方法。包括现在非常火的推荐问题，以及LSI问题都对SVD有着广泛的应用。\n举个最常用的例子，在文本挖掘中：A就是 t (term) 行 d (document) 列的矩阵，每列是一篇文章，每行是一个单词，每个单元格的当前单词在当前文章里的出现次数。 U 是一个 t 行 r 列 的矩阵， V 是一个 r 行 d 列 的矩阵， S 是一个 r 行 r 列的对角矩阵。这里 r 的大小是 A的秩。那么U和V中分别是A的奇异向量，而S是A的奇异值。AA'的正交单位特征向量组成U，特征值组成S'S，A'A的正交单位特征向量组成V，特征值（与AA'相同）组成SS'。\n希望大家细细体会，多多交流，一起进步。"}
{"content2":"常见分类模型与算法\n距离判别法，即最近邻算法KNN；\n贝叶斯分类器；\n线性判别法，即逻辑回归算法；\n决策树；\n支持向量机；\n神经网络；\n1. KNN分类算法原理及应用\n1.1 KNN概述\nK最近邻（k-Nearest Neighbor，KNN）分类算法是最简单的机器学习算法。\nKNN算法的指导思想是“近朱者赤，近墨者黑”，由你的邻居来推断你的类型。\n本质上，KNN算法就是用距离来衡量样本之间的相似度。\n1.2 算法图示\n从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别\n算法涉及3个主要因素\n1) 训练数据集\n2) 距离或相似度的计算衡量\n3) k的大小\n算法描述\n1) 已知两类“先验”数据，分别是蓝方块和红三角，他们分布在一个二维空间中；\n2) 有一个未知类别的数据(绿点)，需要判断它是属于“蓝方块”还是“红三角”类；\n3) 考察离绿点最近的3个(或k个)数据点的类别，占多数的类别即为绿点判定类别；\n1.3 算法要点\n1.3.1 计算步骤\n计算步骤如下：\n1) 算距离：给定测试对象，计算它与训练集中的每个对象的距离；\n2) 找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻；\n3) 做分类：根据这k个近邻归属的主要类别，来对测试对象分类；\n1.3.2 相似度的衡量\n距离越近应该意味着这两个点属于一个分类的可能性越大，但，距离不能代表一切，有些数据的相似度衡量并不适合用距离；\n相似度衡量方法：包括欧式距离、夹角余弦等。\n（简单应用中，一般使用欧式距离，但对于文本分类来说，使用余弦来计算相似度就比欧式距离更合适）\n1.3.3 类别的判定\n简单投票法：少数服从多数，近邻中哪个类别的点最多就分为该类\n加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数）\n1.4 算法不足之处\n1. 样本不平衡容易导致结果错误\n如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。\n改善方法：对此可以采用权值的方法（和该样本距离小的邻居权值大）来改进。\n2. 计算量较大\n因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。\n改善方法：实现对已知样本点进行剪辑，事先去除对分类作用不大的样本。\n注：该方法比较适用于样本容量比较大的类域的类域的分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。\n1.5 KNN分类算法Python实战——KNN简单数据分类实践\n1.5.1 需求\n计算地理位置的相似度\n有以下先验数据，使用KNN算法对未知类别数据分类\n属性1\n属性2\n类别\n1.0\n0.9\nA\n1.0\n1.0\nA\n0.1\n0.2\nB\n0.0\n0.1\nB\n未知类别数据\n属性1\n属性2\n类别\n1.2\n1.0\n?\n0.1\n0.3\n?\n1.5.2 Python实现\n首先，我们新建一个KNN.py脚本文件，文件里面包含两个函数，一个用来生成小数据集，一个实现KNN分类算法。代码如下：\n########################## # KNN: k Nearest Neighbors #输入：newInput: (1xN)的待分类向量 # dataSet: (NxM)的训练数据集 # labels: 训练数据集的类别标签向量 # k: 近邻数 # 输出：可能性最大的分类标签 ########################## from numpy import import operator #创建一个数据集，包含2个类别共4个样本 def createDataSet(): # 生成一个矩阵，每行表示一个样本 group = array([[1.0,0.9],[1.0,1.0],[0.1,0.2],[0.0,0.1]]) # 4个样本分别所属的类别 labels = ['A', 'A', 'B', 'B'] return group, labels # KNN分类算法函数定义 def KNNClassify(newInput, dataSet, labels, k)： numSamples = dataSet.shape[0] #shape[0]表示行数 ## step1：计算距离 # tile(A, reps)：构造一个矩阵，通过A重复reps次得到 # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) -dataSet #按元素求差值 squareDiff = diff ** 2 #将差值平方 squareDist = sum(squaredDiff, axis = 1) # 按行累加 ##step2：对距离排序 # argsort() 返回排序后的索引值 sortedDistIndices = argsort(distance) classCount = {} # define a dictionary (can be append element) for i in xrange(k): ##step 3: 选择k个最近邻 voteLabel = labels[sortedDistIndices[i]] ## step 4:计算k个最近邻中各类别出现的次数 # when the key voteLabel is not in dictionary classCount，get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ##step 5：返回出现次数最多的类别标签 maxCount = 0 for key, value in classCount.items(): if value > maxCount: maxCount = value maxIndex = key return maxIndex\n然后调用算法进行测试\nimport KNN from numpy import * #生成数据集和类别标签 dataSet,labels = KNN.createDataSet() #定义一个未知类别的数据 testX = array([1.2, 1.0]) k=3 #调用分类函数对未知数据分类 outputLabel = KNN.KNNClassify(testX, dataSet, labels, 3) print \"Your input is:\", testX, \" and classified to class:\", outputLabel testX = array([0.1, 0.3]) outputLabel = KNN.KNNClassify(testX,dataSet, labels, 3) print \"Your input is:\", testX, \"and classified to class:\", outputLabel\n这时候会输出：\nYour input is: [1.2 1.0] and classified to class: A Your input is: [0.1 0.3] and classified to class: B\n2. 朴素贝叶斯分类算法原理\n2.1 概述\n贝叶斯分类算法时一大类分类算法的总称。贝叶斯分类算法以样本可能属于某类的概率来作为分类依据。朴素贝叶斯分类算法时贝叶斯分类算法中最简单的一种。\n注：朴素的意思时条件概率独立性\n2.2 算法思想\n朴素贝叶斯的思想是这样的：如果一个事物在一些属性条件发生的情况下，事物属于A的概率>属于B的概率，则判定事物属于A。\n通俗来说比如，在某条大街上，有100人，其中有50个美国人，50个非洲人，看到一个讲英语的黑人，那么我们是怎么去判断他来自哪里？\n提取特征：\n肤色：黑，语言：英语\n先验知识：\nP(黑色|非洲人) = 0.8\nP(讲英语|非洲人)=0.1\nP(黑色|美国人)= 0.2\nP(讲英语|美国人)=0.9\n要判断的概率是：\nP(非洲人|(讲英语，黑色) )\nP(美国人|(讲英语，黑色) )\n思考过程：\nP(非洲人|(讲英语，黑色) )   的 分子= 0.1 * 0.8 *0.5 =0.04\nP(美国人|(讲英语，黑色) )   的 分子= 0.9 *0.2 * 0.5 = 0.09\n从而比较这两个概率的大小就等价于比较这两个分子的值，可以得出结论，此人应该是：美国人。\n其蕴含的数学原理如下：\np(A|xy)=p(Axy)/p(xy)=p(Axy)/p(x)p(y)=p(A)/p(x)*p(A)/p(y)* p(xy)/p(xy)=p(A|x)p(A|y)\n朴素贝叶斯分类器\n讲了上面的小故事，我们来朴素贝叶斯分类器的表示形式：\n当特征为为x时，计算所有类别的条件概率，选取条件概率最大的类别作为待分类的类别。由于上公式的分母对每个类别都是一样的，因此计算时可以不考虑分母，即\n朴素贝叶斯的朴素体现在其对各个条件的独立性假设上，加上独立假设后，大大减少了参数假设空间。\n2.3 算法要点\n2.3.1 算法步骤\n1. 分解各类先验样本数据中的特征；\n2. 计算各类数据中，各特征的条件概率；(比如：特征1出现的情况下，属于A类的概率p(A|特征1)，属于B类的概率p(B|特征1)，属于C类的概率p(C|特征1)......)\n3. 分解待分类数据中的特征(特征1、特征2、特征3、特征4......)\n4. 计算各特征的各条件概率的乘积，如下所示：\n判断为A类的概率：p(A|特征1) * p(A|特征2) * p(A|特征3) * p(A|特征4)......\n判断为B类的概率：p(B|特征1) * p(B|特征2) * p(B|特征3) * p(B|特征4)......\n判断为C类的概率：p(C|特征1) * p(C|特征2) * p(C|特征3) * p(C|特征4)......\n......\n5. 结果中的最大值就是该样本所属的类别\n2.3.2 算法应用举例\n大众点评、淘宝等电商上都会有大量的用户评论，比如：\n1、衣服质量太差了！！！！颜色根本不纯！！！\n2、我有一有种上当受骗的感觉！！！！\n3、质量太差，衣服拿到手感觉像旧货！！！\n4、上身漂亮，合身，很帅，给卖家点赞\n5、穿上衣服帅呆了，给点一万个赞\n6、我在他家买了三件衣服！！！！质量都很差！\n0\n0\n0\n1\n1\n0\n其中1/2/3/6是差评，4/5是好评\n现在需要使用朴素贝叶斯分类算法来自动分类其他的评论，比如：\na、这么差的衣服以后再也不买了\nb、帅，有逼格\n……\n2.3.3 算法应用流程\n1. 分解出先验数据中的各特征\n(即分词，比如“衣服”，“质量太差”，“差”，“不纯”，“帅”，“漂亮”，“赞” ......)\n2. 计算各类别（好评、差评）中，各特征的条件概率\n(比如 p(“衣服” | 差评)、p(“衣服” | 好评)、p(“差”|好评)、p(“差”| 差评) ......)\n3. 计算类别概率\np(好评|(c1,c2,c5,c8))的分子=p(c1|好评) * p(c2|好评) * p(c3|好评) *......p(好评)\np(差评|(c1,c2,c5,c8))的分子=p(c1|差评) * p(c2|差评) * p(c3|差评) *......p(差评)\n4. 显然p(差评)的结果值更大，因此a被判别为\"差评\"\n2.4 朴素贝叶斯分类算法案例\n2.4.1 需求\n利用大量邮件先验数据，使用朴素贝叶斯分类算法来自动识别垃圾邮件\n2.4.2 python实现\n#过滤垃圾邮件 def textParse(bigString): #正则表达式进行文本解析 import re listOfTokens = re.split(r'\\W*', bigString) return [tok.lower() for tok in listOfTokens if len(tok) > 2] def spamTest() docList = []; classList = []; fullText = [] for i in range(1,26): #导入并解析文本文件 wordList = textParse(open('email/spam/%d.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList = textParse(open('email/ham/%d.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList = createVocabList(docList) trainingSet = range(50);testSet = [] for i in range(10): #随机构建训练集 randIndex = int(random.uniform(0, len(trainingSet))) testSet.append(trainingSet[randIndex]) #随机挑选一个文档索引号放入测试集 del(trainingSet[randIndex]) #将该文档索引号从训练集中剔除 trainMat = []; trainClasses = [] for docIndex in trainingSet: trainMat.append(setOfWords2Vec(vocabList, docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam = trainNBO(array(trainMat), array(trainClasses)) errorCount = 0 for docIndex in testSet: #对测试集进行分类 wordVector = setOfWords2Vec(vocabList, docList[docIndex]) if classifyNB(array(wordVector), p0V,p1V != classList[docIndex]: errorCount +=1 print 'the error rate is:', float(errorCount)/len(testSet)\n3. logistic逻辑回归分类算法及应用\n3.1 概述\nLineage逻辑回归是一种简单而又效果不错的分类算法。\n什么是回归：比如说我们有两类数据，各有50个点组成，当我们把这些点画出来，会有一条线区分这两组数据，我们拟合出这个曲线（因为很有可能是非线性的），就是回归。我们通过大量的数据找出这条线，并拟合出这条线的表达式，再有新数据，我们就以这条线为区分来实现分类。\n下图是一个数据集的两组数据，中间有一条区分两组数据的线。\n显然，只有这种线性可分的数据分布才适合用线性逻辑回归\n3.2 算法思想\nLineage回归分类算法就是将线性回归应用在分类场景中\n在该场景中，计算结果是要得到对样本数据的分类标签，而不是得到那条回归直线\n3.2.1 算法图示\n1) 算法目标()？\n大白话：计算各点的y值到拟合线的垂直距离，如果距离>0，分为类A；距离<0，分为类B。\n2) 如何得到拟合线呢？\n大白话：只能先假设，因为线或面的函数都可以表达成y(拟合)=w1 * x1 + w2 * x2 + w3 * x3 + ... ，其中的w是待定参数，而x是数据的各维度特征值，因而上述问题就变成了样本y(x) - y(拟合) > 0? A：B\n3) 如何求解出一套最优的w参数呢?\n基本思路：代入”先验数据“来逆推求解，但针对不等式求解参数极其困难，通用的解决方法，将对不等式的求解做一个转换：a.将”样本y(x) - y(拟合)“的差值压缩到一个0~1的小区间；b.然后代入大量的样本特征值，从而得到一系列的输出结果；c.再将这些输出结果跟样本的先验类别比较，并根据比较情况来调整拟合线的参数值，从而是拟合线的参数逼近最优。从而将问题转化为逼近求解的典型数学问题。\n3.2.2 sigmoid函数\n上述算法思路中，通常使用sigmoid函数作为转换函数\n函数表达式：\n，注：此处的x是向量\n函数曲线：\n之所以使用sigmoid函数，就是让样板点经过运算后得到的结果限制在0~1之间，压缩数据的巨幅震荡，从而方便得到样本点的分类标签(分类以sigmoid函数的计算结果是否大于0.5为依据)\n3.3 算法实现分析\n1.3.1 实现思路\n算法思想的数学表述\n把数据集的特征值设为x1，x2，x3......，求出它们的回归系数wj，设z=w1 * x1 + w2 * x2......，然后将z值代入sigmoid函数并判断结果，即可得到分类标签\n问题在于如何得到一组合适的参数wj？\n通过解析的途径很难求解，而通过迭代的方法可以比较便捷地找到最优解。简单来说，就是不断用样本特征值代入算式，计算出结果后跟其实际标签进行比较，根据差值来修正参数，然后再代入新的样本值计算，循环往复，直到无需修正或已到达预设的迭代次数。\n注：此过程用梯度上升来实现。\n1.3.2 梯度上升算法\n梯度上升是指找到函数增长的方向。在具体实现的过程中，不停地迭代运算直到w的值几乎不再变化为止。\n如图所示：\n3.4 Lineage逻辑回归分类Python实战\n3.4.1 需求\n对给定的先验数据集，使用logistic回归算法对新数据分类\n3.4.2 python实现\n3.4.2.1 定义sigmoid函数\ndef loadDataSet(): dataMat = []; labelMat = [] fr = open('d:/testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat, labelMat def sigmoid(inX): return 1.0/(1+exp(-inX))\n3.4.2.2 返回回归系数\n对应于每个特征值，for循环实现了递归梯度上升算法。\ndef gradAscent(dataMatln, classLabels): dataMatrix = mat(dataMatln) # 将先验数据集转换为NumPy矩阵 labelMat = mat(classLabels).transpose() #将先验数据的类标签转换为NumPy矩阵 m,n = shape(dataMatrix) alpha = 0.001 #设置逼近步长调整系数 maxCycles = 500 #设置最大迭代次数为500 weights = ones((n,1)) #weights即为需要迭代求解的参数向量 for k in range(maxCycles): #heavy on matrix operations h = sigmoid(dataMatrix * weights) #代入样本向量求得“样本y” sigmoid转换值 error = (labelMat - h) #求差 weights = weights + alpha * dataMatrix.transpose() * error #根据差值调整参数向量 return weights\n我们的数据集有两个特征值分别是x1,x2。在代码中又增设了x0变量。\n结果，返回了特征值的回归系数：\n[[4.12414349]\n[0.48007329]\n[-0.6168482]]\n我们得出x1和x2的关系(设x0 = 1)，0=4.12414349+0.48007329*x1 - 0.6168482*x2\n3.4.2.3 线性拟合线\n画出x1与x2的关系图——线性拟合线\n4.决策树（Decision Tree）分类算法原理及应用\n4.1 概述\n决策树——是一种被广泛使用的分类算法。相比贝叶斯算法，决策树的优势在于构造过程不需要任何领域知识或参数设置。在实际应用中，对于探测式的知识发现，决策树更加适用。\n决策树通常有三个步骤：特征选择、决策树的生成、决策树的修剪。\n4.2 算法思想\n通俗来说，决策树分类的思想类似于找对象。现想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话：\n女儿：多大年纪了？\n母亲：26\n女儿：长的帅不帅？\n母亲：挺帅的。\n女儿：收入高不？\n母亲：不算很高，中等情况。\n女儿：是公务员吗？\n母亲：是，公务员，在税务局上班呢。\n女儿：那好，我去见见。\n这个女孩的决策过程就是典型的分类树决策。实质：通过年龄、长相、收入和是否公务员将男人分为两个类别：见和不见\n假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑。\n上图完整表达了这个女孩决定是否见一个约会对象的策略，其中：\n绿色节点表示判断条件\n橙色节点表示决策结果\n箭头表示在一个判断条件在不同情况下的决策路径\n图中红色箭头表示了上面例子中女孩的决策过程。这幅图基本可以算是一颗决策树，说它”基本可以算“是因为图中的判定条件没有量化，如收入高中低等等，还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。\n决策树分类算法的关键就是根据”先验数据“构造一棵最佳的决策树，用以预测未知数据的类别\n决策树：是一个树结构(可以是二叉树或非二叉树)。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。\n4.3 决策树构造\n4.3.1 决策树构造样例\n假如有以下判断苹果好坏的数据样本：\n样本    红     大      好苹果\n0       1      1         1\n1       1      0         1\n2       0      1         0\n3       0      0         0\n样本中有2个属性，A0表示是否红苹果。A1表示是否大于苹果。假如要根据这个数据样本构建一棵自动判断苹果好坏的决策树。由于本例中的数据只有2个属性，因此，我们可以穷举所有可能构造出来的决策树，就2课树，如下图所示：\n显然左边先使用A0(红色)做划分依据的决策树要优于右边用A1(大小)做划分依据的决策树。当然这是直觉的认知。而直觉显然不适合转化成程序的实现，所以需要有一种定量的考察来评价这两棵树的性能好坏。\n决策树的评价所用的定量考察方法为计算每种划分情况的信息熵增益：如果经过某个选定的属性进行数据划分后的信息熵下降最多，则这个划分属性是最优选择。\n4.3.2 属性划分选择(即构造决策树)的依据\n熵：信息论的奠基人香农定义的用来信息量的单位。简单来说，熵就是“无序，混乱”的程度。\n公式：H(X)=- Σ pi * logpi, i=1,2, ... , n，pi为一个特征的概率\n通过计算来理解：\n1、原始样本数据的熵：\n样例总数：4\n好苹果：2\n坏苹果：2\n熵：-(1/2 * log(1/2) + 1/2 * log(1/2)) =1\n信息熵为1表示当前处于最混乱，最无序的状态\n2、两颗决策树的划分结果熵增益计算\n树1先选A0作划分，各子节点信息熵计算如下：\n0,1叶子节点有2个正例，0个负例。信息熵为：e1 = -(2/2 * log(2/2) + 0/2 * log(0/2)) =0。\n2,3叶子节点有0个正例，2个负例。信息熵为：e2 = -(0/2 * log(0/2) + 2/2 * log(2/2)) =0。\n因此选择A0划分后的信息熵为每个子节点的信息熵所占比重的加权和：E = e1 * 2/4 + e2 * 2/4 = 0。\n选择A0做划分的信息熵增益G(S，A0) = S - E = 1 - 0 =1。\n事实上，决策树叶子节点表示已经都属于相同类别，因此信息熵一定为0。\n树2先选A1作划分，各子节点信息熵计算如下：\n0,2子节点有1个正例，1个负例。信息熵为：e1 = -(1/2 * log(1/2) + 1/2 * log(1/2)) = 1。\n1,3子节点有1个正例，1个负例。信息熵为：e2 = -(1/2 * log(1/2) + 1/2 * log(1/2)) = 1。\n因此选择A1划分后的信息熵为每个子节点的信息熵所占比重的加权和：E = e1 * 2/4 + e2 * 2/4 = 1。也就是说分了跟没分一样！\n选择A1做划分的信息熵增益G(S，A1) = S - E = 1 - 1 = 0。\n因此，每次划分之前，我们只需要计算出信息熵增益最大的那种划分即可。\n4.4 算法要点\n4.4.1 指导思想\n经过决策属性的划分后，数据的无序度越来越低，也就是信息熵越来越小\n4.4.2 算法实现\n梳理出数据中的属性，比较按照某特定属性划分后的数据的信息熵增益，选择信息熵增益最大的那个属性作为第一划分依据，然后继续选择第二属性，以此类推。\n4.5 决策树分类算法Python实战\n4.5.1 案例需求\n我们的任务就是训练一个决策树分类器，输入身高和体重，分类器能给出这个人是胖子还是瘦子。\n所用的训练数据如下，这个数据一共有8个样本，每个样本有2个属性，分别为头发和声音，第三列为性别标签，表示“男”或“女”。该数据保存在1.txt中。\n头发\n声音\n性别\n长\n粗\n男\n短\n粗\n男\n短\n粗\n男\n长\n细\n女\n短\n细\n女\n短\n粗\n女\n长\n粗\n女\n长\n粗\n女\n4.5.2 模型分析\n决策树对于“是非”的二值逻辑的分枝相当自然。\n本例决策树的任务是找到头发、声音将其样本两两分类，自顶向下构建决策树。\n在这里，我们列出两种方案：\n①先根据头发判断，若判断不出，再根据声音判断，于是画了一幅图，如下：\n于是，一个简单、直观的决策树就这么出来了。头发长、声音粗就是男生；头发长、声音细就是女生；头发短、声音粗是男生；头发短、声音细是女生。\n② 先根据声音判断，然后再根据头发来判断，决策树如下：\n那么问题来了：方案①和方案②哪个的决策树好些？计算机做决策树的时候，面对多个特征，该如何选哪个特征为最佳多得划分特征？\n划分数据集的大原则是：将无序的数据变得更加有序。\n我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。于是我们这么想，如果我们能测量数据的复杂度，对比按不同特征分类后的数据复杂度，若按某一特征分类后复杂度减少的更多，那么这个特征即为最佳分类特征。为此，Claude Shannon定义了熵和信息增益，用熵来表示信息的复杂度，熵越大，则信息越复杂。信息增益表示两个信息熵的差值。\n首先计算未分类前的熵，总共有8位同学，男生3位，女生5位\n熵（总）= -3/8*log2(3/8)-5/8*log2(5/8)=0.9544\n接着分别计算方案①和方案②分类后信息熵。\n方案①首先按头发分类，分类后的结果为：长头发中有1男3女。短头发中有2男2女。\n熵（长发）= -1/4*log2(1/4)-3/4*log2(3/4)=0.8113\n熵（短发）= -2/4*log2(2/4)-2/4*log2(2/4)=1\n熵（方案①）= 4/8*0.8113+4/8*1=0.9057 （4/8为长头发有4人，短头发有4人）\n信息增益（方案①）= 熵（总）- 熵（方案①）= 0.9544 - 0.9057 = 0.0487\n同理，按方案②的方法，首先按声音特征来分，分类后的结果为：声音粗中有3男3女。声音细中有0男2女。\n熵（声音粗）= -3/6*log2(3/6)-3/6*log2(3/6)=1\n熵（声音细）= -2/2*log2(2/2)=0\n熵（方案②）= 6/8*1+2/8*0=0.75 （6/8为声音粗有6人，2/8为声音细有2人）\n信息增益（方案②）= 熵（总）- 熵（方案②）= 0.9544 - 0.75 = 0.2087\n按照方案②的方法，先按声音特征分类，信息增益更大，区分样本的能力更强，更具有代表性。\n以上就是决策树ID3算法的核心思想。\n4.5.3 python实现ID3算法\nfrom math import log import operator def calcShannonEnt(dataSet): # 计算数据的熵(entropy) numEntries=len(dataSet) # 数据条数 labelCounts={} for featVec in dataSet: currentLabel=featVec[-1] # 每行数据的最后一个字（类别） if currentLabel not in labelCounts.keys(): labelCounts[currentLabel]=0 labelCounts[currentLabel]+=1 # 统计有多少个类以及每个类的数量 shannonEnt=0 for key in labelCounts: prob=float(labelCounts[key])/numEntries # 计算单个类的熵值 shannonEnt-=prob*log(prob,2) # 累加每个类的熵值 return shannonEnt def createDataSet1(): # 创造示例数据 dataSet = [['长', '粗', '男'], ['短', '粗', '男'], ['短', '粗', '男'], ['长', '细', '女'], ['短', '细', '女'], ['短', '粗', '女'], ['长', '粗', '女'], ['长', '粗', '女']] labels = ['头发', '声音'] # 两个特征 return dataSet, labels def splitDataSet(dataSet, axis, value): # 按某个特征分类后的数据 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet def chooseBestFeatureToSplit(dataSet): # 选择最优的分类特征 numFeatures = len(dataSet[0]) - 1 print(numFeatures) baseEntropy = calcShannonEnt(dataSet) # 原始的熵 bestInfoGain = 0 bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] uniqueVals = set(featList) newEntropy = 0 for value in uniqueVals: subDataSet = splitDataSet(dataSet, i, value) prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) # 按特征分类后的熵 infoGain = baseEntropy - newEntropy # 原始熵与按特征分类后的熵的差值 if (infoGain > bestInfoGain): # 若按某特征划分后，熵值减少的最大，则次特征为最优分类特征 bestInfoGain = infoGain bestFeature = i return bestFeature def majorityCnt(classList): # 按分类后类别数量排序，比如：最后分类为2男1女，则判定为男： classCount={} for vote in classList: if vote not in classCount.keys(): classCount[vote]=0 classCount[vote]+=1 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 类别：男或女 if classList.count(classList[0]) == len(classList): return classList[0] if len(dataSet[0]) == 1: return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 选择最优特征 bestFeatLabel = labels[bestFeat] myTree = {bestFeatLabel:{}} # 分类结果以字典形式保存 del(labels[bestFeat]) featValues = [example[bestFeat] for example in dataSet] #print(featValues) uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) return myTree if __name__ == '__main__': dataSet, labels = createDataSet1() # 创造示例数据 print(createTree(dataSet, labels)) # 输出决策树模型结果\n这时候会输出\n{'声音': {'细': '女', '粗': {'头发': {'长': '女', '短': '男'}}}}\n4.5.4 决策树的保存\n一棵决策树的学习训练是非常耗费运算时间的，因此，决策树训练出来后，可进行保存，以便在预测新的数据时只需要直接加载训练好的决策树即可\n本案例的代码中已经把决策树的结构写入了tree.dot中。打开该文件，很容易画出决策树，还可以看到决策树的更多分类信息。\n本例的tree.dot如下所示：\ndigraph Tree { 0 [label=\"X[1] <= 55.0000\\nentropy = 0.954434002925\\nsamples = 8\", shape=\"box\"] ; 1 [label=\"entropy = 0.0000\\nsamples = 2\\nvalue = [ 2. 0.]\", shape=\"box\"] ; 0 -> 1 ; 2 [label=\"X[1] <= 70.0000\\nentropy = 0.650022421648\\nsamples = 6\", shape=\"box\"] ; 0 -> 2 ; 3 [label=\"X[0] <= 1.6500\\nentropy = 0.918295834054\\nsamples = 3\", shape=\"box\"] ; 2 -> 3 ; 4 [label=\"entropy = 0.0000\\nsamples = 2\\nvalue = [ 0. 2.]\", shape=\"box\"] ; 3 -> 4 ; 5 [label=\"entropy = 0.0000\\nsamples = 1\\nvalue = [ 1. 0.]\", shape=\"box\"] ; 3 -> 5 ; 6 [label=\"entropy = 0.0000\\nsamples = 3\\nvalue = [ 0. 3.]\", shape=\"box\"] ; 2 -> 6 ; }\n根据这个信息，决策树应该长的如下这个样子：\n参考资料：\nhttps://blog.csdn.net/csqazwsxedc/article/details/65697652"}
{"content2":"机器学习经典书籍小结\n博客第一篇文章[1]是转载的，也算是开始写博客不经意的表露了自己对机器学习的兴趣吧！那篇文章总结了机器学习的一些经典算法的论文与数学基础理论的一些书籍，对于开始学习机器学习的话恐怕太过深入，正好最近在买书，看了很多经典书籍的总结与评论，我再拾人牙慧，稍稍总结一下吧。\n先说一下我看过的和正在看的一些书吧！\n《数学之美》；作者吴军大家都很熟悉。这本书主要的作用是引起了我对机器学习和自然语言处理的兴趣。里面以极为通俗的语言讲述了数学在这两个领域的应用。\n《Programming Collective Intelligence》（中译本《集体智慧编程》）；作者Toby Segaran也是《BeautifulData : The Stories Behind Elegant Data Solutions》（《数据之美：解密优雅数据解决方案背后的故事》）的作者。这本书最大的优势就是里面没有理论推导和复杂的数学公式，是很不错的入门书。目前中文版已经脱销，对于有志于这个领域的人来说，英文的pdf是个不错的选择，因为后面有很多经典书的翻译都较差，只能看英文版，不如从这个入手。还有，这本书适合于快速看完，因为据评论，看完一些经典的带有数学推导的书后会发现这本书什么都没讲，只是举了很多例子而已。\n《Algorithms of the Intelligent Web》（中译本《智能web算法》）；作者Haralambos Marmanis、Dmitry Babenko。这本书中的公式比《集体智慧编程》要略多一点，里面的例子多是互联网上的应用，看名字就知道。不足的地方在于里面的配套代码是BeanShell而不是python或其他。总起来说，这本书还是适合初学者，与上一本一样需要快速读完，如果读完上一本的话，这一本可以不必细看代码，了解算法主要思想就行了。\n《统计学习方法》；作者李航，是国内机器学习领域的几个大家之一，曾在MSRA任高级研究员，现在华为诺亚方舟实验室。书中写了十个算法，每个算法的介绍都很干脆，直接上公式，是彻头彻尾的“干货书”。每章末尾的参考文献也方便了想深入理解算法的童鞋直接查到经典论文；本书可以与上面两本书互为辅助阅读。\n《Machine Learning》（《机器学习》）；作者TomMitchell[2]是CMU的大师，有机器学习和半监督学习的网络课程视频。这本书是领域内翻译的较好的书籍，讲述的算法也比《统计学习方法》的范围要大很多。据评论这本书主要在于启发，讲述公式为什么成立而不是推导；不足的地方在于出版年限较早，时效性不如PRML。但有些基础的经典还是不会过时的，所以这本书现在几乎是机器学习的必读书目。\n《Mining of Massive Datasets》（《大数据》）；作者Anand Rajaraman[3]、Jeffrey David Ullman，Anand是Stanford的PhD。这本书介绍了很多算法，也介绍了这些算法在数据规模比较大的时候的变形。但是限于篇幅，每种算法都没有展开讲的感觉，如果想深入了解需要查其他的资料，不过这样的话对算法进行了解也足够了。还有一点不足的地方就是本书原文和翻译都有许多错误，勘误表比较长，读者要用心了。\n《Data Mining: Practical Machine Learning Tools and Techniques》（《数据挖掘：实用机器学习技术》）；作者Ian H. Witten 、Eibe Frank是weka的作者、新西兰怀卡托大学教授。他们的《ManagingGigabytes》[4]也是信息检索方面的经典书籍。这本书最大的特点是对weka的使用进行了介绍，但是其理论部分太单薄，作为入门书籍还可，但是，经典的入门书籍如《集体智慧编程》、《智能web算法》已经很经典，学习的话不宜读太多的入门书籍，建议只看一些上述两本书没讲到的算法。\n《机器学习及其应用2011》，周志华、杨强主编。来源于“机器学习及其应用研讨会”的文集。该研讨会由复旦大学智能信息处理实验室发起，目前已举办了十届，国内的大牛如李航、项亮、王海峰、刘铁岩、余凯等都曾在该会议上做过讲座。这本书讲了很多机器学习前沿的具体的应用，需要有基础的才能看懂。如果想了解机器学习研究趋势的可以浏览一下这本书。关注领域内的学术会议是发现研究趋势的方法嘛。\n上面大多都是一些入门级的书籍，想要在这个领域深入下去，还需要深入的阅读一些经典书籍。看了很多推荐大牛推荐的书单，这里总结一下吧。\n《Pattern Classification》（《模式分类》第二版）；作者Richard O. Duda[5]、Peter E. Hart、David。模式识别的奠基之作，但对最近呈主导地位的较好的方法SVM、Boosting方法没有介绍，被评“挂一漏万之嫌”。\n《Pattern Recognition And Machine Learning》；作者Christopher M. Bishop[6]；简称PRML，侧重于概率模型，是贝叶斯方法的扛鼎之作，据评“具有强烈的工程气息，可以配合stanford 大学 Andrew Ng 教授的 Machine Learning 视频教程一起来学，效果翻倍。”\n《The Elements of Statistical Learning : Data Mining, Inference, andPrediction》，（《统计学习基础：数据挖掘、推理与预测》第二版）；作者RobertTibshirani、Trevor Hastie、Jerome Friedman。“这本书的作者是Boosting方法最活跃的几个研究人员，发明的Gradient Boosting提出了理解Boosting方法的新角度，极大扩展了Boosting方法的应用范围。这本书对当前最为流行的方法有比较全面深入的介绍，对工程人员参考价值也许要更大一点。另一方面，它不仅总结了已经成熟了的一些技术，而且对尚在发展中的一些议题也有简明扼要的论述。让读者充分体会到机器学习是一个仍然非常活跃的研究领域，应该会让学术研究人员也有常读常新的感受。”[7]\n《Data Mining：Concepts andTechniques》，（《数据挖掘：概念与技术》第三版）；作者（美）Jiawei Han[8]、（加）Micheline Kamber、（加）Jian Pei，其中第一作者是华裔。本书毫无疑问是数据挖掘方面的的经典之作，不过翻译版总是被喷，没办法，大部分翻译过来的书籍都被喷，想要不吃别人嚼过的东西，就好好学习英文吧。\n一些引申链接：\nhttp://blog.csdn.net/pongba/article/details/2915005\nhttp://blog.csdn.net/caikehe/article/details/8496721\nhttp://blog.chinaunix.net/uid-10314004-id-3594337.html\nhttp://weibo.com/1657470871/zpZ87mhND?sudaref=www.zhizhihu.com\nhttp://www.zhizhihu.com/html/y2012/4019.html\nhttp://zinkov.com/posts/2012-10-04-ml-book-reviews/\n几乎所有引申链接中都提到了上面我所提到的经典书籍。另外还有一些其他方面比如信息检索、人工智能还有数学基础方面的书籍。\n有人推荐，学习机器学习的话可以先读《统计学习方法》和《统计学习基础》打底，这样就包含了大部分的算法，然后再深入研究某个算法。我觉得，我在上面列出的四本经典书籍都应该通读一遍。孔子云“学而不思则罔，思而不学则殆”，我认为，学习、思考、实践不可缺一，学习的同时要加强算法代码的实现和其他方面比如并行化、使用场景等的思考。\n[1] http://blog.csdn.net/xinzhangyanxiang/article/details/7799997\n[2] http://www.cs.cmu.edu/~tom/\n[3] http://en.wikipedia.org/wiki/Anand_Rajaraman\n[4] http://book.douban.com/subject/1511568/\n[5] http://en.wikipedia.org/wiki/Richard_O._Duda\n[6] http://en.wikipedia.org/wiki/Christopher_Bishop\n[7] http://book.douban.com/subject/3578359/\n[8] http://en.wikipedia.org/wiki/Jiawei_Han"}
{"content2":"作者：张达衢  摘自中国论文网 原文地址：http://www.xzbu.com/4/view-8299582.htm\n【关键词】人工智能；发展现状；未来展望\n【中图分类号】TP18 【文献标志码】A 【文章编号】1673-1069（2017）04-0107-02\n1 引言\n2016年年初，韩国围棋国手李在石与围棋程序Alpha Go对弈中首战失利，再一次将人工智能拉入了公众的视野，使其成为2016年度话题度最高的科技之一。不可否认，近些年来人工智能发展迅速，很多人工智能产品已经开始进入人们的家中，如扫地机器人、智能保姆等，虽然它们还没有美国大片《终结者》中所描述得那么先进，但从前遥不可及的人工智能概念正在一步步变为现实却是不争的事实。人工智能的现状如何，它又将如何发展，都是学界较为关注的课题。\n2 人工智能综述\n2.1 人工智能的概念\n人工智能即AI，其英文全称为Artificial Intelligence。人工智能的概念要从人工和智能两方面来了解，所谓人工就是指人工智能脱胎于人类的文明，是人类智慧的产物；而智能则是指具有人工智能的计算机或其他�子设备可以模拟人类的智能行为和思维方式，人工智能是计算机科学的一个分支，它的近期主要目标在于研究用机器来模仿和执行人脑的某些智能功能，并开发相关理论和技术。\n2.2 人工智能的现实应用\n如今的人工智能机器，可以在胜任一些复杂脑力劳动的同时，辅助人类进行记忆和逻辑运算等活动。现阶段学者已经研制出了一些可以模拟人类精神活动的电子机器，经过完善升级，这些电子机器将有希望超越人类的能力，协助人类完成一些执行难度较大的工作。但是目前研制出的自动化系统或者机器人虽然可以代替部分人类劳动，却还没有到达可以实现人类多方面协调和自我学习升级的智能水平，要制造出一款可以完全拥有人类智慧的机器，还需进一步深入研究。还有一些人工智能产物经常应用于各种商业用途，例如单位内部的客户信息系统，决策支持系统，以及我们在世面上可以看见的医学顾问、法津顾问等软件。\n3 人工智能发展现状\n3.1 智能接口技术研究现状\n人工智能接口研究就是为了实现人机交流，为此学者必须从理论和实践两方面努力，解决计算机对文字和语言的理解与翻译、对自我的表达等功能问题。由于智能接口技术的研究和应用，计算机技术的发展获得了极大的推动力，在运行速率和人机交流等方面都有巨大提升。\n3.2 数据挖掘技术研究现状\n数据挖掘技术主要是对各类模糊的、大量的应用数据、人未知的、潜在已经存在的数据进行整理挖掘进行细致的研究，寻找出对研究有用的数据。目前，数据库、人工智能、数理统计已经成为数据挖掘技术的三大技术支撑，以基础理论、发现算法、可视化技术、知识表示方法、半结构化等作为研究内容，为数据挖掘技术的发展提供理论和技术支持。\n3.3 主体系统研究现状\n主体系统可以实现机器意图和想法的生成，是一种智能方面更接近人类的自主性实体系统。自主系统可以完成一些相对独立、自主的任务，甚至可以通过调整自我状态，应对环境和特殊情况的变化，进而保证自身规划任务的完成。在多主体系统研究中，主要是从物理和逻辑思维方面对主体进行智能行为的分析研究。\n4 人工智能发展中面临的问题\n4.1 识别功能的困惑\n计算机识别技术研究在近些年取得了大量成果，其产品的实际应用范围较广，但不可否认的是，计算机识别的模式是基于一定的算法和程序设定的，其识别机制完全不同于人类的感官识别，因此，在计算机进行识别，尤其是图形识别时，对各种印刷体、文字、指纹等清晰图形可以快速识别，但对于相似度较高的物体，计算机识别能力相对较弱，识别失败的情况较为普遍。语音识别主要研究各种语音信号的分类。语音识别技术近年来发展很快，但是缺点是识别极易受到干扰，发音不标准的语音较易引发识别错误。\n4.2 GPS功能的局限性\nGPS是企图实现一种不依赖于领域知识求解人工智能问题的通用方法，但是问题内部的表达形式和领域知识是分不开的，用谓词逻辑进行定理归结或者人工智能通用方法GPS，都可以从分析表达能力上找出其局限性，这样就减少了人工智能的应用范围[1]。 　　5 人工智能的未来应用展望\n人工智能与人生活最息息相关的应用范围就是融入人们的衣食住行和教育等方面，这也是人工智能未来最普遍的应用方向。\n5.1 无人驾驶的汽车\n奔驰、丰田等很多大型汽车企业都在研究�o人驾驶的汽车，像007电影中的那种拥有自主辨别路况、自动驾驶等功能的汽车也许很快就会成为现实。自动驾驶的汽车要搭载的技术并不只人工智能一种，它还需要将自动控制和视觉计算等新型技术集成应用，改变现有汽车的体系结构，赋予其自动识别、分析和控制的能力。因此，自动驾驶汽车需要实现三方面的技术突破：其一，实现利用摄像设备、雷达和激光测距机来获得路况信息；其二，实现利用地图进行自动的车辆导航；其三，根据已有信息数据对车辆的速度和方向进行控制。未来的自动驾驶汽车还可以通过车辆之间的信息互通和互相感应，来协调车速和方向，避免车辆碰撞，实现自动驾驶车辆的安全行进。\n5.2 智能化的课堂\n当前已经有一些智能化的教学软件，教师们可以在这些软件上把教学课件传送给学生，并进行授课答题，学生还可以与教师弹幕互动，使课堂变得妙趣横生，方便了教师的授课活动。对于学生而言，能够在期末十分便捷地回顾上课的错题，甚至能够在几年后翻阅学习过的课件；对于教师而言，能够精细地知道学生对知识的掌握程度，甚至能够发现最积极和最懈怠的学生。未来的智能课堂将更具有时间延展性，学生不仅可以在课堂学习知识，还可以利用智能电子设备进行课前预习和课后复习，从而使学生可以在更加趣味性的氛围中进行自主学习安排。\n5.3 自动化的厨房\n今后的厨房将会更加智能化，当你做饭时，设定好你想要的菜谱，准备好所需的食材，烹调设备即可将饭菜制作得恰到好处。它会根据你食材的新鲜程度，为你推荐最适合的菜谱，并计算出其营养参考标准，并为你推荐其他食物，使膳食营养均衡。当你家中某样食材不足时，物流公司便会将时下最新鲜的这一食材送至你家中[2]。\n6 结语\n人工智能这一概念是在1956年提出的，在当时，人工智能还只是人们头脑中的一种幻想，而在60年后的今天，人工智能的梦想已经逐渐照进现实，它甚至渗透进了工业、医学、服务等多个领域，可以说人工智能正在改变着我们生活的世界。但对于人工智能这个人类创造出来的技术，人们也存在一定的担忧，人工智能将向何方发展？人工智能发展到极致会不会脱离人类的控制？人工智能会不会超越人类的智慧？在诸多问题围绕下，人工智能技术依然在迅猛发展，它的未来如何，让我们拭目以待。\n参考文献：【1】王宇楼.人工智能的现状及今后的发展趋势展望[J].科技展望，2016（22）：299.\n【2】吕泽宇.人工智能的历史、现状与未来[J].信息与电脑（理论版），2016（13）：166-167."}
{"content2":"一、Caffe\n（Convolutional Architecture for Fast Feature Embedding） BVLC\nWe believe that Caffe is the fastest convnet implementation available. caffe的官网是http://caffe.berkeleyvision.org/。Caffe是一个清晰而高效的深度学习框架，其作者是博士毕业于UC Berkeley的贾扬清，目前在Google工作。\nCaffe是纯粹的C++/CUDA架构，支持命令行、Python和MATLAB接口；可以在CPU和GPU直接无缝切换：Caffe::set_mode(Caffe::GPU); 在Caffe中图层需要使用C++定义，而网络则使用Protobuf定义。Caffe是一个深度卷积神经网络的学习框架，使用Caffe可以比较方便地进行CNN模型的训练和测试，精于CV领域。\nCaffe作为快速开发和工程应用是非常适合的。caffe官方提供了大量examples，照着examples写，caffe只要求会写prototxt就行，它的训练过程、梯度下降算法等等都实现封装好了,懂了prototxt的语法了，基本就能自己构造神经网络了。caffe作为C++语言以及配合了CUDA开发的框架，训练效率也有保证，这也是caffe适合于工业应用的原因。代码易懂好理解,高效、实用。上手简单,使用方便,比较成熟和完善，实现基础算法方便快捷,开发新算法不是特别灵活,适合工业快速应用实现.\nCaffe的优势：\n一方面是调参，改网络很方便，开源做得很好，另一方面CNN在CV里用的很多，这也是Caffe的优势。\n上手快：配置文件简单，易上手，文档齐全，模型与相应优化都是以文本形式而非代码形式给出。\nCaffe给出了模型的定义、最优化设置以及预训练的权重，方便立即上手。\n速度快：Google Protocol Buffer数据标准为Caffe提升了效率，能够运行最棒的模型与海量的数据。Caffe与cuDNN结合使用，测试AlexNet模型，在K40上处理每张图片只需要1.17ms.\n模块化：允许对新数据格式、网络层和损失函数进行扩展，方便扩展到新的任务和设置上。\n可以使用Caffe提供的各层类型来定义自己的模型。\n开放性：公开的代码和参考模型用于再现。\n社区好：可以通过BSD-2参与开发与讨论。\n学术论文采用此模型较多。不少论文都与Caffe有关（R-CNN，DSN，最近还有人用Caffe实现LSTM）\n缺点：\n灵活性差，不同版本接口不兼容, 可定制性较低，不能很方便得扩展到其它模型。\nCaffe可能是第一个主流的工业级深度学习工具，它开始于2013年底,具有出色的卷积神经网络实现。在计算机视觉领域Caffe依然是最流行的工具包，它有很多扩展，但是由于一些遗留的架构问题，它对递归网络和语言建模的支持很差。\n二、MXNet\n百度 DMLC(分布式机器学习社区) 简称\"深盟\"\n内存优化做得好\nMXNet结合命令式和声明式编程的优点，既可以对系统做大量的优化，又可以方便调试。资源和计算的调度、内存分配资源管理、数据的表示、计算优化等都很值得学习的，原生支持分布式训练的。\n对于一个优秀的深度学习系统，或者更广来说优秀的科学计算系统，最重要的是编程接口的设计。他们都采用将一个领域特定语言(domain specific language)嵌入到一个主语言中。例如numpy将矩阵运算嵌入到python中。这类嵌入一般分为两种，其中一种嵌入的较浅，其中每个语句都按原来的意思执行，且通常采用命令式编程(imperative programming)，其中numpy和Torch就是属于这种。而另一种则用一种深的嵌入方式，提供一整套针对具体应用的迷你语言。这一种通常使用声明式语言(declarative programing)，既用户只需要声明要做什么，而具体执行则由系统完成。这类系统包括Caffe，theano和TensorFlow。\n这两种方式各有利弊，总结如下。\n命令式编程:\n如何执行 a=b+1: 需要b已经被赋值。立即执行加法，将结果保存在a中。\n优点: 语义上容易理解，灵活，可以精确控制行为。通常可以无缝地和主语言交互，方便地利用主语言的各类算法，工具包，debug和性能调试器。\n缺点: 实现统一的辅助函数和提供整体优化都很困难。\n声明式编程:\n如何执行 a=b+1: 返回对应的计算图(computation graph)，我们可以之后对b进行赋值，然后再执行加法运算\n优点: 在真正开始计算的时候已经拿到了整个计算图，所以我们可以做一系列优化来提升性能。实现辅助函数也容易，例如对任何计算图都提供forward和backward函数，对计算图进行可视化，将图保存到硬盘和从硬盘读取。\n缺点: 很多主语言的特性都用不上。某些在主语言中实现简单，但在这里却经常麻烦，例如if-else语句 。debug也不容易，例如监视一个复杂的计算图中的某个节点的中间结果并不简单。\n目前现有的系统大部分都采用上两种编程模式的一种。与它们不同的是，MXNet尝试将两种模式无缝的结合起来。在命令式编程上MXNet提供张量运算，而声明式编程中MXNet支持符号表达式。用户可以自由的混合它们来快速实现自己的想法。例如我们可以用声明式编程来描述神经网络，并利用系统提供的自动求导来训练模型。另一方便，模型的迭代训练和更新模型法则中可能涉及大量的控制逻辑，因此我们可以用命令式编程来实现。同时我们用它来进行方便地调式和与主语言交互数据。\n下表我们比较MXNet和其他流行的深度学习系统\n框架         Caffe                    Torch                          Theano               TensorFlow                   MXNet\n主语言     C++                        Lua                           Python                C++                                C++\n从语言     Python, Matlab        x                              x                         Python                            Python, R, Julia, Scala, Javascript, Matlab, Go\n硬件         CPU, GPU,        CPU, GPU, FPGA       CPU, GPU,         CPU, GPU, mobile         CPU, GPU,mobile\n分布式     x                             x                               x                          v                                      v\n命令式     x                             v                               v                          x                                      v\n声明式     v                             x                               x                          v                                      v\nSymbol： 声明式的符号表达式\nMXNet使用多值输出的符号表达式来声明计算图。符号是由操作子构建而来。一个操作子可以是一个简单的矩阵运算“+”，也可以是一个复杂的神经网络里面的层，例如卷积层。一个操作子可以有多个输入变量和多个输出变量，还可以有内部状态变量。一个变量既可以是自由的，我们可以之后对其赋值；也可以是某个操作子的输出。在执行一个符号表达式前，我们需要对所有的自由变量进行赋值。\nNDArray：命令式的张量计算\nMXNet提供命令式的张量计算来桥接主语言的和符号表达式。另一方面，NDArray可以无缝和符号表达式进行对接。\nKVStore：多设备间的数据交互\nMXNet提供一个分布式的key-value存储来进行数据交换。它主要有两个函数，push： 将key-value对从一个设备push进存储，pull：将某个key上的值从存储中pull出来此外，KVStore还接受自定义的更新函数来控制收到的值如何写入到存储中。最后KVStore提供数种包含最终一致性模型和顺序一致性模型在内的数据一致性模型。\n读入数据模块\n数据读取在整体系统性能上占重要地位。MXNet提供工具能将任意大小的样本压缩打包成单个或者数个文件来加速顺序和随机读取。\n训练模块\nMXNet实现了常用的优化算法来训练模型。用户只需要提供数据数据迭代器和神经网络的Symbol便可。此外，用户可以提供额外的KVStore来进行分布式的训练。\n过去，现状，和未来\n大半年数个优秀的C++机器学习系统的开发人员成立了DMLC，本意是更方便共享各自项目的代码，并给用户提供一致的体验。当时我们有两个深度学习的项目，一个是CXXNet，其通过配置来定义和训练神经网络。另一个是Minerva，提供类似numpy一样的张量计算接口。前者在图片分类等使用卷积网络上很方便，而后者更灵活。那时候我们想能不能做一个两者功能都具备的系统，于是这样就有了MXNet。其名字来自Minerva的M和CXXNet的XNet。其中Symbol的想法来自CXXNet，而NDArray的想法来自Minerva。我们也常把MXNet叫“mix net”。\nMXNet的目的是做一个有意思的系统，能够让大家用着方便的系统，一个轻量的和可以快速测试系统和算法想法的系统。未来主要关注下面四个方向：\n支持更多的硬件，目前在积极考虑支持AMD GPU，高通GPU，Intel Phi，FPGA，和更多智能设备。相信MXNet的轻量和内存节省可以在这些上大有作为。\n更加完善的操作子。目前不论是Symbol还是NDArray支持的操作还是有限，我们希望能够尽快的扩充他们。\n更多编程语言。除了C++，目前MXNet对Python，R和Julia的支持比较完善。但我们希望还能有很多的语言，例如javascript。\n更多的应用。我们之前花了很多精力在图片分类上，下面我们会考虑很多的应用。\n三、Torch       Facebook  Google DeepMind  Twitter  FAIR\n核心的计算单元使用C或者cuda做了很好的优化。在此基础之上，使用lua构建了常见的模型。另外，torch7构建的是一个生态系统，安装新的模型实现模块只需要luarocks install package. 比如：luarocks install rnn。之后就可以欢乐地使用rnn模型了。torch7的缺点可能就是1. wrapper是lua语言，需要一点时间来学习。2. 优化新的计算单元可能会比较麻烦，backend修改起来会比较麻烦.\n核心特征的总结：\n1. 一个强大的n维数组\n2. 很多实现索引，切片，移调transposing的例程\n3.惊人的通过LuaJIT的C接口\n4.线性代数例程\n5.神经网络，并基于能量的模型\n6.数值优化例程\n7.快速高效的GPU支持\n8.可嵌入，可移植到iOS，Android和FPGA的后台\n优势：\n1. 构建模型简单，一层层搭积木即可。\n2. 高度模块化，一层就是一个模块，写新模块也方便，套用接口就行，用tensor运算不必写cuda也能用GPU。\n3. 底层的tensor由C和cuda实现，速度不会比caffe差，甚至某些运算可能更快。\n4. 使用GPU方便，把tensor数据送到GPU只要简单的 \"tensor:cuda()\"。\n5. lua入门快，堪比python。\n6. 很重要的一点，nngraph，理论上可以用nn里的模块实现任何DAG构造的网络，当然也包括RNN、LSTM之类的。\n劣势：\n1. 对于不少人来说，lua要新学。\n2. 除了deep learning方面，其他好用的机器学习library较少。\n3. 数据文件格式比较麻烦，一般原始数据没有torch专用的t7格式文件，需要通过mat等格式中转转换。\n四、Theano\nthe LISA group at the University of Montreal(蒙特利尔)\nTheano是一个Python库，用来定义、优化和计算数学表达式，用于高效的解决多维数组的计算问题。\n优点：\n集成NumPy-使用numpy.ndarray\n使用GPU加速计算-比CPU快140倍（只针对32位float类型）\n有效的符号微分-计算一元或多元函数的导数\n速度和稳定性优化-比如能计算很小的x的函数log(1+x)的值\n动态地生成C代码-更快地计算\n广泛地单元测试和自我验证-检测和诊断多种错误\n灵活性好\n缺点：\n1.scan 中糟糕参数的传递限制，immutable 机制导致 function compile 时候的时间过长。\n2.theano 定义 function 时缺乏灵活的多态机制。\n3.困难的调试方法\n五、TensorFlow\nGoogle\nTensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统，其命名来源于本身的运行原理。Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow为张量从图象的一端流动到另一端计算过程。TensorFlow是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统。\nTensorFlow 表达了高层次的机器学习计算，大幅简化了第一代系统，并且具备更好的灵活性和可延展性。TensorFlow一大亮点是支持异构设备分布式计算，它能够在各个平台上自动运行模型，从电话、单个CPU / GPU到成百上千GPU卡组成的分布式系统。从目前的文档看，TensorFlow支持CNN、RNN和LSTM算法，拥有C++/Python编程接口，这都是目前在Image，Speech和NLP最流行的深度神经网络模型。\nTensorFlow的数据结构tensor，它相当于N维的array或者list，与MXNet类似，都是采用了以python调用的形式展现出来。某个定义好的tensor的数据类型是不变的，但是维数可以动态改变。用tensor rank和TensorShape来表示它的维数（例如rank为2可以看成矩阵，rank为1可以看成向量）。tensor是个比较中规中矩的类型。唯一特别的地方在于在TensorFlow构成的网络中，tensor是唯一能够传递的类型，而类似于array、list这种不能当成输入。\nTensorFlow的网络实现方式选择的是符号计算方式，它的程序分为计算构造阶段和执行阶段，构造阶段是构造出computation graph，computation graph就是包含一系列符号操作Operation和Tensor数据对象的流程图，跟mxnet的symbol类似，它定义好了如何进行计算（加减乘除等）、数据通过不同计算的顺序（也就是flow，数据在符号操作之间流动的感觉）。但是暂时并不读取输入来计算获得输出，而是由后面的执行阶段启动session的run来执行已经定义好的graph。这样的方式跟mxnet很相似，应该都是借鉴了theano的想法。其中TensorFlow还引入了Variable类型，它不像mxnet的Variable属于symbol（tf的operation类似mxnet的symbol），而是一个单独的类型，主要作用是存储网络权重参数，从而能够在运行过程中动态改变。tf将每一个操作抽象成了一个符号Operation，它能够读取0个或者多个Tensor对象作为输入(输出)，操作内容包括基本的数学运算、支持reduce、segment（对tensor中部分进行运算。\nTensorFlow的优点：\n1、TensorFlow则是功能很齐全，能够搭建的网络更丰富而不是像caffe仅仅局限在CNN。\n2、 TensorFlow的深度学习部分能够在一个模型中堆积了许多不同的模型和转换，你能够在一个模型中方便地处理文本 图片和规则分类以及连续变量，同时实现多目标和多损失工作；\n3、TensorFlow的管道部分能够将数据处理和机器学习放在一个框架中，TensorFlow指引了方向。\nTensorFlow是一个理想的RNN（递归神经网络） API和实现，TensorFlow使用了向量运算的符号图方法，使得新网络的指定变得相当容易，但TensorFlow并不支持双向RNN和3D卷积，同时公共版本的图定义也不支持循环和条件控制，这使得RNN的实现并不理想，因为必须要使用Python循环且无法进行图编译优化。\n六、CNTK      微软深度学习工具包\n微软将人工智能成果 CNTK 开源放上 GitHub，称是运算速度最快的 Toolkit\n是一个统一的深度学习工具包，它将神经网络描述成在有向图上的一系列计算步骤。在这个有向图中，叶子结点表示输入层或网络参数，其它的结点表示成在输入层上的矩阵操作。在CNTK上可以很容易的实现及结合当今流行的模型，例如前馈神经网络DNNs， 卷积神经网络(CNNs)， 循环神经网络 (RNNs/LSTMs)。在实现随机梯度下降学习时能够自动计算梯度，而且还能通过多个GPUs或服务器实现并行计算。CNTK是微软在Cortana 数字助理和Skype 翻译应用中使用的语音识别的系统框架。\nCNTK最大的优点是可以并行多个GPU或服务器。微软首席科学家黄学东说“谷歌公开的TensorFlow并没有这个功能”。\nCNTK的另外一个优点是支持Microsoft Windows。但是这个开源工具是用C++写的。微软计划将尽快公开对应的Python和C#版本。\n十个值得尝试的开源深度学习框架\n本周早些时候开源中国社区公布了Google开源了TensorFlow（GitHub）消息，此举在深度学习领域影响巨大，因为Google在人工智能领域的研发成绩斐然，有着雄厚的人才储备，而且Google自己的Gmail和搜索引擎都在使用自行研发的深度学习工具。\n无疑，来自Google军火库的TensorFlow必然是开源深度学习软件中的明星产品，登陆GitHub当天就成为最受关注的项目，当周获得评星数就轻松超过1万个。\n对于希望在应用中整合深度学习功能的开发者来说，GitHub上其实还有很多不错的开源项目值得关注，首先我们推荐目前规模人气最高的TOP3：\nCaffe\n源自加州伯克利分校的Caffe被广泛应用，包括Pinterest这样的web大户。与TensorFlow一样，Caffe也是由C++开发，Caffe也是Google今年早些时候发布的DeepDream项目（可以识别喵星人的人工智能神经网络）的基础。\nTheano\n2008年诞生于蒙特利尔理工学院，Theano派生出了大量深度学习Python软件包，最著名的包括Blocks和Keras。\nTorch\nTorch诞生已经有十年之久，但是真正起势得益于去年Facebook开源了大量Torch的深度学习模块和扩展。Torch另外一个特殊之处是采用了不怎么流行的编程语言Lua（该语言曾被用来开发视频游戏）。\n除了以上三个比较成熟知名的项目，还有很多有特色的深度学习开源框架也值得关注：\nBrainstorm\n来自瑞士人工智能实验室IDSIA的一个非常发展前景很不错的深度学习软件包，Brainstorm能够处理上百层的超级深度神经网络——所谓的公路网络Highway Networks。\nChainer\n来自一个日本的深度学习创业公司Preferred Networks，今年6月发布的一个Python框架。Chainer的设计基于define by run原则，也就是说，该网络在运行中动态定义，而不是在启动时定义，这里有Chainer的详细文档。\nDeeplearning4j\n顾名思义，Deeplearning4j是”for Java”的深度学习框架，也是首个商用级别的深度学习开源库。Deeplearning4j由创业公司Skymind于2014年6月发布，使用 Deeplearning4j的不乏埃森哲、雪弗兰、博斯咨询和IBM等明星企业。\nDeepLearning4j是一个面向生产环境和商业应用的高成熟度深度学习开源库，可与Hadoop和Spark集成，即插即用，方便开发者在APP中快速集成深度学习功能，可应用于以下深度学习领域：\n人脸/图像识别\n语音搜索\n语音转文字（Speech to text）\n垃圾信息过滤（异常侦测）\n电商欺诈侦测\nMarvin\n是普林斯顿大学视觉工作组新推出的C++框架。该团队还提供了一个文件用于将Caffe模型转化成语Marvin兼容的模式。\nConvNetJS\n这是斯坦福大学博士生Andrej Karpathy开发浏览器插件，基于万能的JavaScript可以在你的游览器中训练神经网络。Karpathy还写了一个ConvNetJS的入门教程，以及一个简洁的浏览器演示项目。\ngithub主页：https://github.com/karpathy/convnetjs\nMXNet\n出自CXXNet、Minerva、Purine等项目的开发者之手，主要用C++编写。MXNet强调提高内存使用的效率，甚至能在智能手机上运行诸如图像识别等任务。\nNeon\n由创业公司Nervana Systems于今年五月开源，在某些基准测试中，由Python和Sass开发的Neon的测试成绩甚至要优于Caffeine、Torch和谷歌的TensorFlow。\n6642:  iNaturalist（项目：连接自然）挑战2017数据集\n1707- 06436:  未来的计算机视觉调查--通过对2016年的1600篇论文的调研\n6342: ThiNet：一个滤波器级别的调谐方法用于深度网络压缩\n6292: STag:一个基准的标记系统"}
{"content2":"微软在Build 2018大会上推出的一款面向.NET开发人员的开源，跨平台机器学习框架ML.NET。 ML.NET将允许.NET开发人员开发他们自己的模型，并将自定义ML集成到他们的应用程序中，而无需事先掌握开发或调整机器学习模型的专业知识。在采用通用机器学习语言（如R和Python）开发的模型，并将它们集成到用C＃等语言编写的企业应用程序中需要付出相当大的努力。ML.NET填平了机器学习专家和软件开发者之间的差距，从而使得机器学习的平民化，即使没有机器学习背景的人们能够建立和运行模型。通过为.NET创建高质量的机器学习框架，微软已经使得将机器学习转化为企业（或通过Xamarin移动应用程序）变得更容易。这是一种使机器学习更加可用的形式。\n使用ML.NET可以解决哪些类型的问题？\n基于微软内部Windows，Bing和Azure等主要微软产品使用多年的机器学习构建的库目前处于预览阶段，最新版本是0.2 。该框架目前支持的学习模型包括\nK-Means聚类\n逻辑回归\n支持向量机\n朴素贝叶斯\n随机森林\n增强树木\n其他技术，如推荐引擎和异常检测，正在开发的路线图上。ML.NET将最终将接口暴露给其他流行的机器学习库，如TensorFlow，CNTK和Accord.NET。最后，还会有一些工具和语言增强功能，包括Azure和GUI / Visual Studio功能中的扩展功能。\n如何在应用程序中使用ML.NET？\nML.NET以NuGet包的形式提供，可以轻松安装到新的或现有的.NET应用程序中。\n该框架采用了用于其他机器学习库（如scikit-learn和Apache Spark MLlib）的“管道(LearningPipeline)”方法。数据通过多个阶段“传送”以产生有用的结果（例如预测）。典型的管道可能涉及\n加载数据\n转换数据\n特征提取/工程\n配置学习模型\n培训模型\n使用训练好的模型（例如获得预测）\n管道为使用机器学习模型提供了一个标准API。这使得在测试和实验过程中更容易切换一个模型。它还将建模工作分解为定义明确的步骤，以便更容易理解现有代码。scikit-learn库实现了很多机器学习算法，我们可以多多参考scikit-learn ：http://sklearn.apachecn.org/cn/0.19.0/index.html\nML.NET机器学习管道的核心组件：\nML数据结构（例如IDataView，LearningPipeline）\nTextLoader（将数据从分隔文本文件加载到LearningPipeline）和 CollectionDataSource 从一组对象中加载数据集\n转换（以获得正确格式的数据进行训练）：\n处理/特征化文本： TextFeaturizer\n架构modifcation： ，ColumnConcatenator，ColumnSelector和ColumnDropper\n使用分类特征：CategoricalOneHotVectorizer和CategoricalHashOneHotVectorizer\n处理丢失的数据： MissingValueHandler\n过滤器：RowTakeFilter，RowSkipFilter，RowRangeFilter\n特性选择：FeatureSelectorByCount和FeatureSelectorByMutualInformation\n学习算法（用于训练机器学习模型）用于各种任务：\n二元分类：FastTreeBinaryClassifier，StochasticDualCoordinateAscentBinaryClassifier，AveragedPerceptronBinaryClassifier，BinaryLogisticRegressor，FastForestBinaryClassifier，LinearSvmBinaryClassifier，和GeneralizedAdditiveModelBinaryClassifier\n多类分类：StochasticDualCoordinateAscentClassifier，LogisticRegressor，和NaiveBayesClassifier\n回归：FastTreeRegressor，FastTreeTweedieRegressor，StochasticDualCoordinateAscentRegressor，OrdinaryLeastSquaresRegressor，OnlineGradientDescentRegressor，PoissonRegressor，和GeneralizedAdditiveModelRegressor\n聚类 KMeansPlusPlusClusterer\n评估器（检查模型的工作情况）：\n对于二元分类： BinaryClassificationEvaluator\n对于多类分类： ClassificationEvaluator\n对于回归： RegressionEvaluator\n在构建机器学习模型时，首先需要定义您希望通过数据实现的目标。之后，您可以针对您的情况选择正确的机器学习任务。以下列表描述了您可以选择的不同机器学习任务以及一些常见用例。在ML.NET 0.2增加了一个 支持从一组对象中加载数据集的能力,以前这些只能从分隔的文本文件加载。另一个补充是交叉验证，这是一种验证机器学习模型性能的方法。交叉验证方法的一个有用方面是它不需要与用于创建模型的数据集分开的数据集。相反，它将多次提供的数据划分为不同组的训练和测试数据。ML.NET 0.2加入了一个示例代码库，演示了如何使用这个新框架，地址是https://github.com/dotnet/machinelearning-samples。\n二元分类\n二元分类属于 监督学习，用于预测数据的一个实例属于哪些两个类（类别）任务。分类算法的输入是一组标记示例，其中每个标记都是0或1的整数。二进制分类算法的输出是一个分类器，您可以使用该分类器来预测新的未标记实例的类。二元分类场景的例子包括：\n将Twitter评论的情绪理解为“积极”或“消极”。\n诊断患者是否患有某种疾病。\n决定将电子邮件标记为“垃圾邮件”。\n如果交易日是上涨日或下跌日\n手写数字识别\n语音识别\n图像识别\n有关更多信息，请参阅Wikipedia上的二元分类 文章。\n多类分类\n多元分类属于 监督学习，用于预测的数据的实例的类（类别）的任务。分类算法的输入是一组标记示例。每个标签都是0到k-1之间的整数，其中k是类的数量。分类算法的输出是一个分类器，您可以使用它来预测新的未标记实例的类。多类分类方案的例子包括：\n确定一只狗的品种为“西伯利亚雪橇犬”，“金毛猎犬”，“贵宾犬”等。\n将电影评论理解为“正面”，“中性”或“负面”。\n将酒店评论归类为“位置”，“价格”，“清洁度”等。\n有关更多信息，请参阅Wikipedia上的多类分类文章。\n分类步骤设置：\n首先定义问题\n然后，您将以名为Features的数字属性的形式表示您的数据。这对于已经分类的训练数据和将来需要分类的测试数据都是这样做的\n您将获取训练数据并将其输入分类算法以训练模型\n将需要分类的新实例或采取测试数据并将其传递给分类器进行分类\n聚类\n聚类属于无监督机器学习，用于数据的一组实例为包含类似特征的簇的任务。聚类还可用于识别数据集中的关系，这些关系可能不是通过浏览或简单观察而在逻辑上得出的。聚类算法的输入和输出取决于所选择的方法。您可以采用分布、质心、连通性或基于密度的方法。ML.NET目前支持使用K-Means聚类的基于质心的方法。聚类场景的例子包括：\n根据酒店选择的习惯和特点了解酒店客人群体。\n识别客户群和人口统计信息，以帮助构建有针对性的广告活动。\n根据制造指标对库存进行分类。\n根据房屋类型，价值和地理位置确定一组房屋\n地震震中确定危险区域\n使用集群将电话塔放在一个新城市中，以便所有用户都能获得最佳单一强度\n聚类设置步骤：\n你会从问题陈述开始，问题陈述是需要聚集的数据集\n然后，您将使用功能在该数据集中表示点。\n这里没有训练这一步，不需要学习\n您直接将数据提供给聚类算法以查找最终的聚类，而无需任何训练步骤\n回归\n回归是 监督的机器学习，用于从一组相关的功能预测标签的值。标签可以具有任何实际价值，并且不像分类任务那样来自有限的一组值。回归算法对标签对其相关特征的依赖性进行建模，以确定标签随着特征值的变化而如何变化。回归算法的输入是一组具有已知值标签的示例。回归算法的输出是一个函数，您可以使用该函数来预测任何新的输入要素集的标注值。回归情景的例子包括：\n根据房屋属性（如卧室数量，位置或大小）预测房价。\n根据历史数据和当前市场趋势预测未来股价。\n根据广告预算预测产品的销售情况。\n异常检测（即将推出）\n排名（即将推出）\n推荐（即将推出）"}
{"content2":"错误问题的描述：\nERROR 1045 (28000): Access denied for user 'ODBC'@'localhost' (using password: NO)\nERROR 1045 (28000): Access denied for user 'ODBC'@'localhost' (using password: YES)\nwindows下，以上两个错误的解决方法\n解决方法：\n1、找到配置文件my.ini  ，然后将其打开，可以选择用NotePadd++打开\n2、打开后，搜索mysqld关键字\n找到后，在mysqld下面添加skip-grant-tables，保存退出。\nPS：若提示不让保存时，可以将该文件剪切到桌面，更改保存后再复制到mySQL目录下\n# For advice on how to change settings please see\n# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html\n# *** DO NOT EDIT THIS FILE. It's a template which will be copied to the\n# *** default location during install, and will be replaced if you\n# *** upgrade to a newer version of MySQL.\n[mysqld]\nskip-grant-tables\n# Remove leading # and set to the amount of RAM for the most important data\n# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.\n# innodb_buffer_pool_size = 128M\n# Remove leading # to turn on a very important data integrity option: logging\n# changes to the binary log between backups.\n# log_bin\n# These are commonly set, remove the # and set as required.\nbasedir = D:\\SoftWare\\MySQL\\mysql-5.7.11-winx64\ndatadir = D:\\SoftWare\\MySQL\\mysql-5.7.11-winx64\\Data\nport = 3306\n# server_id = .....\n# Remove leading # to set options mainly useful for reporting servers.\n# The server defaults are faster for transactions and fast SELECTs.\n# Adjust sizes as needed, experiment to find the optimal values.\n# join_buffer_size = 128M\n# sort_buffer_size = 2M\n# read_rnd_buffer_size = 2M\nsql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES\n这样，是用于跳过密码问题，但是呢，这并不能彻底解决！\n3、重启mysql服务\n在任何路径目录下，都可以关闭/重启mysql的服务呢。（因为，之前，已经配置全局的环境变量了）\nnet stop mysql\nnet start mysql\n4、进入数据库，重设置密码。\nmysql -u root -p         Enter\n不用管password          Enter\nmysql> use mysql;      Enter\nmysql> update mysql.user set authtntication_string=password('rootroot') where user='root';   （密码自己设）\nmysql> flush privileges;      刷新数据库\nmysql> quit;\n5、密码重设置成功，改好之后，再修改一下my.ini这个文件，把我们刚才加入的\"skip-grant-tables\"这行删除，保存退出再重启mysql服务就可以了。\n6、重启mysql服务，并登录mysql用户，用户是root，密码是rootroot。\nD:\\SoftWare\\MySQL\\mysql-5.7.11-winx64\\bin> net stop mysql\nD:\\SoftWare\\MySQL\\mysql-5.7.11-winx64\\bin> net start mysql\nD:\\SoftWare\\MySQL\\mysql-5.7.11-winx64\\bin>mysql -u root -p\nEnter password:rootroot\n感谢下面的博主：\nhttp://www.ithao123.cn/content-10746582.html\n推荐书籍：\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"关于图文识别功能相关技术的实现\n转载请注明源地址：http://www.cnblogs.com/funnyzpc/p/8908906.html\n上一章，写的是SSL证书配置，中间折腾了好一会，在此感谢SSL证书发行商的协助；这次我就讲讲ocr识别的问题，先说说需求来源吧。。。\n之前因为风控每次需要手动P协议文件和身份证(脱敏)，还要识别证件及图片文件的内容，觉得狠狠狠麻烦，遂就找到了技术总监，技术总监一拍脑袋，额，小邹啊。。。\n呃，一开始并没抱太大希望，不过还是花了些心思做了些需求实现的调研，怎么办 google、duckduckgo、github一路找下来就有了几个工程了，嘿嘿~，可惜还没高兴到，没想到的是这些工程一个比一个坑，不是依赖windows系统组件就是代码bug不断，作者们，能用点儿心么\n日夜操劳，加班啊，总算是将几个工程全都修得能跑起来了，大费周折。。。难得啊\n欸，可惜效果均不佳；现开始，我总结下一些主流的图文识别技术，只是浅聊哦。。。\n首先，这些工程大致分两类：\n一类是纯算法，不附带机器学习功能的，且需要依赖于window系统组件的工程，比如tesseract和tess4j，识别效果可以说是巨差(可能我的技术很菜的原因)，但有一点儿值得赞许，就是识别结果的格式还算不错，这类图文识别的特点大致有如下几点：\nA>工程代码量较大\nB>依赖window组件，需要在window系统下才能运行\nC>识别效果无法通过学习逐渐优化\nD>识别出来的文字时常乱码，中文识别乱码错别字较多\nE>识别结果通常使用格式化模子来格式化结果，遂，识别结果的格式还算过得去\n一类是基于机器学习(比如Tensorflow)的工程，这些工程参差不齐，存在插件版本问题，尤其是python插件，实在在太太太难装了，在一就是工程大多较为简陋，由于机器学习具有不断改善的趋势，这是基于机器学习的图文识别的最大优势，总结起来，基于机器学习的图文识别的特点儿大致有如下几点：\nA>工程比较简单，代码量不是很多\nB>依赖的语言插件，如python实在难以安装\nC>有很多优化的方向，比如使用显卡，优化算法(卷积神经网络)来提高识别速度及模型准确度\nD>十分耗费计算机字段，一般识别一页A4大小的图片中的内容，(我使用Macbook Pro) 最快也用了二十多秒\nE>识别的结果比较乱，但对于中文，尤其是图片较好的中文的文字识别准确率能达到百分之七十网上，但是识别格式和文字准确度不如上者\nF>由于是基于机器学习，遂需要大量的数据喂养以提高识别的准确率，喂养的数据十分可观\n额，总的来说，后者的优势较大，也是趋势，比如腾讯QQ的图片识别还有百度大脑AI这些基本都是基于机器学习，个人觉得，如果投入一个团队去专门研究开发一个图文识别的产品，也是比较容易实现的，何况这个方向向前走就是人工智能，尽管现在看起来有些智障...。\n哦，大致总结完了，我就展示下基于tess4j和chinese-ocr这两项目的实现效果，我的输入是身份证：\n(注意:源图片是从github上拉下来的，个人做了些简陋的脱敏处理!)\n下面是基于tess4j实现的结果:\ntess4j的实现只能基于windows组件实现，故项目只能在windows下运行，另外tesseract也是windows组件的实现。\n一下是基于chinese-ocr的项目的实现的结果：\nchinese-orc是基于python语言+tensorflow的实现，结果一目了然，需要说的是，一下几个也是基于=>\nIITG-Captcha-Solver-OpenCV-TensorFlow:基于Tensorflow实现的验证码识别，已调试通过，验证码模糊度较高的识别不够准确\ntext-detection-ctpn　　　　　　　：基于Tensorflow实现的图片识别，未调试通过\ntensorflow-ocr　　　　　　　　　：基于tensorflow实现的图片识别，未调试通过\n由于github共享的工程参差不齐，存在插件版本、语言以及系统版本的差异，遂这些项目clone下来后需要修改些bug才可，这里简述下一些大致的问题的解决思路：\nA>对于插件版本下载不了的(我用的是pycharm)，建议使用相邻版本的插件，但有些插件需要爬梯出去才可下\nB>对于部分(例如 test.py)文件跑不起来的，请尝试着将这个文件放置在工程主目录下，但同时请注意 import引用的文件(可能需要手动修改)\nC>对于项目出现的主流问题请移步Issues以查找\n嗯，此篇原本在前一天发表的，由于公司周年庆耽搁了会儿，最后，我把上面几个工程的打包文件共享下(包含我修改过bug的)，有需要的请自行下载，如有疑问请电邮或留言。\n工程下载链接: https://pan.baidu.com/s/1B2Eyak8zwdAldA0NBfmlvw\n工程下载密码: r2av"}
{"content2":"部分 VIII\n机器学习\nOpenCV-Python 中文教程（搬运）目录\n46 K 近邻（k-Nearest Neighbour ）\n46.1 理解 K 近邻\n目标\n• 本节我们要理解 k 近邻（kNN）的基本概念。\n原理\nkNN 可以说是最简单的监督学习分类器了。想法也很简单，就是找出测试数据在特征空间中的最近邻居。我们将使用下面的图片介绍它。\n上图中的对象可以分成两组，蓝色方块和红色三角。每一组也可以称为一个 类。我们可以把所有的这些对象看成是一个城镇中房子，而所有的房子分别属于蓝色和红色家族，而这个城镇就是所谓的特征空间。（你可以把一个特征空间看成是所有点的投影所在的空间。例如在一个 2D 的坐标空间中，每个数据都两个特征 x 坐标和 y 坐标，你可以在 2D 坐标空间中表示这些数据。如果每个数据都有 3 个特征呢，我们就需要一个 3D 空间。N 个特征就需要 N 维空间，这个 N 维空间就是特征空间。在上图中，我们可以认为是具有两个特征色2D 空间）。\n现在城镇中来了一个新人，他的新房子用绿色圆盘表示。我们要根据他房子的位置把他归为蓝色家族或红色家族。我们把这过程成为 分类。我们应该怎么做呢？因为我们正在学习看 kNN，那我们就使用一下这个算法吧。\n一个方法就是查看他最近的邻居属于那个家族，从图像中我们知道最近的是红色三角家族。所以他被分到红色家族。这种方法被称为简单 近邻，因为分类仅仅决定与它最近的邻居。\n但是这里还有一个问题。红色三角可能是最近的，但如果他周围还有很多蓝色方块怎么办呢？此时蓝色方块对局部的影响应该大于红色三角。所以仅仅检测最近的一个邻居是不足的。所以我们检测 k 个最近邻居。谁在这 k 个邻居中占据多数，那新的成员就属于谁那一类。如果 k 等于 3，也就是在上面图像中检测 3 个最近的邻居。他有两个红的和一个蓝的邻居，所以他还是属于红色家族。但是如果 k 等于 7 呢？他有 5 个蓝色和 2 个红色邻居，现在他就会被分到蓝色家族了。k 的取值对结果影响非常大。更有趣的是，如果 k 等于 4呢？两个红两个蓝。这是一个死结。所以 k 的取值最好为奇数。这中根据 k 个最近邻居进行分类的方法被称为 kNN。\n在 kNN 中我们考虑了 k 个最近邻居，但是我们给了这些邻居相等的权重，这样做公平吗？以 k 等于 4 为例，我们说她是一个死结。但是两个红色三角比两个蓝色方块距离新成员更近一些。所以他更应该被分为红色家族。那用数学应该如何表示呢？我们要根据每个房子与新房子的距离对每个房子赋予不同的权重。距离近的具有更高的权重，距离远的权重更低。然后我们根据两个家族的权重和来判断新房子的归属，谁的权重大就属于谁。这被称为 修改过的kNN。\n那这里面些是重要的呢？\n• 我们需要整个城镇中每个房子的信息。因为我们要测量新来者到所有现存房子的距离，并在其中找到最近的。如果那里有很多房子，就要占用很大的内存和更多的计算时间。\n• 训练和处理几乎不需要时间。\n现在我们看看 OpenCV 中的 kNN。\n46.1.1 OpenCV 中的 kNN\n我们这里来举一个简单的例子，和上面一样有两个类。下一节我们会有一个更好的例子。\n这里我们将红色家族标记为 Class-0，蓝色家族标记为 Class-1。还要再创建 25 个训练数据，把它们非别标记为 Class-0 或者 Class-1。Numpy中随机数产生器可以帮助我们完成这个任务。\n然后借助 Matplotlib 将这些点绘制出来。红色家族显示为红色三角蓝色家族显示为蓝色方块。\nimport cv2 import numpy as np import matplotlib.pyplot as plt # Feature set containing (x,y) values of 25 known/training data trainData = np.random.randint(0,100,(25,2)).astype(np.float32) # Labels each one either Red or Blue with numbers 0 and 1 responses = np.random.randint(0,2,(25,1)).astype(np.float32) # Take Red families and plot them red = trainData[responses.ravel()==0] plt.scatter(red[:,0],red[:,1],80,'r','^') # Take Blue families and plot them blue = trainData[responses.ravel()==1] plt.scatter(blue[:,0],blue[:,1],80,'b','s') plt.show()\n你可能会得到一个与上面类似的图形，但不会完全一样，因为你使用了随机数产生器，每次你运行代码都会得到不同的结果。\n下面就是 kNN 算法分类器的初始化，我们要传入一个训练数据集，以及与训练数据对应的分类来训练 kNN 分类器（构建搜索树）。\n最后要使用 OpenCV 中的 kNN 分类器，我们给它一个测试数据，让它来进行分类。在使用 kNN 之前，我们应该对测试数据有所了解。我们的数据应该是大小为数据数目乘以特征数目的浮点性数组。然后我们就可以通过计算找到测试数据最近的邻居了。我们可以设置返回的最近邻居的数目。返回值包括：\n1. 由 kNN 算法计算得到的测试数据的类别标志（0 或 1）。如果你想使用最近邻算法，只需要将 k 设置为 1，k 就是最近邻的数目。\n2. k 个最近邻居的类别标志。\n3. 每个最近邻居到测试数据的距离。\n让我们看看它是如何工作的。测试数据被标记为绿色。\nnewcomer = np.random.randint(0,100,(1,2)).astype(np.float32) plt.scatter(newcomer[:,0],newcomer[:,1],80,'g','o') knn = cv2.KNearest() knn.train(trainData,responses) ret, results, neighbours ,dist = knn.find_nearest(newcomer, 3) print \"result: \", results,\"\\n\" print \"neighbours: \", neighbours,\"\\n\" print \"distance: \", dist plt.show()\n下面是我得到的结果：\nresult: [[ 1.]] neighbours: [[ 1. 1. 1.]] distance: [[ 53. 58. 61.]]\n这说明我们的测试数据有 3 个邻居，他们都是蓝色，所以它被分为蓝色家族。结果很明显，如下图所示：\n如果我们有大量的数据要进行测试，可以直接传入一个数组。对应的结果同样也是数组。\n# 10 new comers newcomers = np.random.randint(0,100,(10,2)).astype(np.float32) ret, results,neighbours,dist = knn.find_nearest(newcomer, 3) # The results also will contain 10 labels.\n46.2 使用 kNN 对手写数字 OCR\n目标\n• 要根据我们掌握的 kNN 知识创建一个基本的 OCR 程序\n• 使用 OpenCV 自带的手写数字和字母数据测试我们的程序\n46.2.1 手写数字的 OCR\n我们的目的是创建一个可以对手写数字进行识别的程序。为了达到这个目的我们需要训练数据和测试数据。OpenCV 安装包中有一副图片（/samples/python2/data/digits.png）, 其中有 5000 个手写数字（每个数字重复 500遍）。每个数字是一个 20x20 的小图。所以第一步就是将这个图像分割成 5000个不同的数字。我们在将拆分后的每一个数字的图像重排成一行含有 400 个像素点的新图像。这个就是我们的特征集，所有像素的灰度值。这是我们能创建的最简单的特征集。我们使用每个数字的前 250 个样本做训练数据，剩余的250 个做测试数据。让我们先准备一下：\nimport numpy as np import cv2 from matplotlib import pyplot as plt img = cv2.imread('digits.png') gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # Now we split the image to 5000 cells, each 20x20 size cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)] # Make it into a Numpy array. It size will be (50,100,20,20) x = np.array(cells) # Now we prepare train_data and test_data. train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400) test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400) # Create labels for train and test data k = np.arange(10) train_labels = np.repeat(k,250)[:,np.newaxis] test_labels = train_labels.copy() # Initiate kNN, train the data, then test it with test data for k=1 knn = cv2.KNearest() knn.train(train,train_labels) ret,result,neighbours,dist = knn.find_nearest(test,k=5) # Now we check the accuracy of classification # For that, compare the result with test_labels and check which are wrong matches = result==test_labels correct = np.count_nonzero(matches) accuracy = correct*100.0/result.size print accuracy\n现在最基本的 OCR 程序已经准备好了，这个示例中我们得到的准确率为91%。改善准确度的一个办法是提供更多的训练数据，尤其是判断错误的那些数字。为了避免每次运行程序都要准备和训练分类器，我们最好把它保留，这样在下次运行是时，只需要从文件中读取这些数据开始进行分类就可以了。\nNumpy 函数 np.savetxt，np.load 等可以帮助我们搞定这些。\n# save the data np.savez('knn_data.npz',train=train, train_labels=train_labels) # Now load the data with np.load('knn_data.npz') as data: print data.files train = data['train'] train_labels = data['train_labels']\n在我的系统中，占用的空间大概为 4.4M。由于我们现在使用灰度值（unint8）作为特征，在保存之前最好先把这些数据装换成 np.uint8 格式，这样就只需要占用 1.1M 的空间。在加载数据时再转会到 float32。\n46.2.2 英文字母的 OCR\n接下来我们来做英文字母的 OCR。和上面做法一样，但是数据和特征集有一些不同。现在 OpenCV 给出的不是图片了，而是一个数据文件（/samples/cpp/letter-recognition.data）。如果打开它的话，你会发现它有 20000 行，第一样看上去就像是垃圾。实际上每一行的第一列是我们的一个字母标记。接下来的 16 个数字是它的不同特征。这些特征来源于UCI Machine LearningRepository。你可以在此页找到更多相关信息。\n有 20000 个样本可以使用，我们取前 10000 个作为训练样本，剩下的10000 个作为测试样本。我们应在先把字母表转换成 asc 码，因为我们不正直接处理字母。\nimport cv2 import numpy as np import matplotlib.pyplot as plt # Load the data, converters convert the letter to a number data= np.loadtxt('letter-recognition.data', dtype= 'float32', delimiter = ',', converters= {0: lambda ch: ord(ch)-ord('A')}) # split the data to two, 10000 each for train and test train, test = np.vsplit(data,2) # split trainData and testData to features and responses responses, trainData = np.hsplit(train,[1]) labels, testData = np.hsplit(test,[1]) # Initiate the kNN, classify, measure accuracy. knn = cv2.KNearest() knn.train(trainData, responses) ret, result, neighbours, dist = knn.find_nearest(testData, k=5) correct = np.count_nonzero(result == labels) accuracy = correct*100.0/10000 print accuracy\n准确率达到了 93.22%。同样你可以通过增加训练样本的数量来提高准确率。\n47 支持向量机\n47.1 理解 SVM\n目标\n• 对 SVM 有一个直观理解\n原理\n47.1.1 线性数据分割\n如下图所示，其中含有两类数据，红的和蓝的。如果是使用 kNN，对于一个测试数据我们要测量它到每一个样本的距离，从而根据最近邻居分类。测量所有的距离需要足够的时间，并且需要大量的内存存储训练样本。但是分类下图所示的数据真的需要占用这么多资源吗？\n我们在考虑另外一个想法。我们找到了一条直线，f (x) = ax 1 + bx 2 + c，它可以将所有的数据分割到两个区域。当我们拿到一个测试数据 X 时，我们只需要把它代入 f (x)。如果 |f (X)| > 0，它就属于蓝色组，否则就属于红色组。\n我们把这条线称为 决定边界（Decision_Boundary）。很简单而且内存使用效率也很高。这种使用一条直线（或者是高位空间种的超平面）上述数据分成两组的方法成为 线性分割。\n从上图中我们看到有很多条直线可以将数据分为蓝红两组，那一条直线是最好的呢？直觉上讲这条直线应该是与两组数据的距离越远越好。为什么呢？\n因为测试数据可能有噪音影响（真实数据 + 噪声）。这些数据不应该影响分类的准确性。所以这条距离远的直线抗噪声能力也就最强。所以 SVM 要做就是找到一条直线，并使这条直线到（训练样本）各组数据的最短距离最大。下图中加粗的直线经过中心。\n要找到决定边界，就需要使用训练数据。我们需要所有的训练数据吗？不，只需要那些靠近边界的数据，如上图中一个蓝色的圆盘和两个红色的方块。我们叫他们 支持向量，经过他们的直线叫做 支持平面。有了这些数据就足以找到决定边界了。我们担心所有的数据。这对于数据简化有帮助。\nWe need not worry about all the data. It helps in data reduction.\n到底发生了什么呢？首先我们找到了分别代表两组数据的超平面。例如，蓝色数据可以用  表示，而红色数据可以用  表示，ω 叫做 权重向量( ),x 为 特征向量()。b 0 被成为 bias（截距？）。权重向量决定了决定边界的走向，而 bias 点决定了它（决定边界）的位置。决定边界被定义为这两个超平面的中间线（平面），表达式为 。从支持向量到决定边界的最短距离为 。\n边缘长度为这个距离的两倍，我们需要使这个边缘长度最大。我们要创建一个新的函数 并使它的值最小：\n其中 t i 是每一组的标记，.。\n47.1.2 非线性数据分割\n想象一下，如果一组数据不能被一条直线分为两组怎么办？例如，在一维空间中 X 类包含的数据点有（-3，3），O 类包含的数据点有（-1，1）。很明显不可能使用线性分割将 X 和 O 分开。但是有一个方法可以帮我们解决这个问题。使用函数  对这组数据进行映射，得到的 X 为 9，O 为 1，这时就可以使用线性分割了。\n或者我们也可以把一维数据转换成两维数据。我们可以使用函数对数据进行映射。这样 X 就变成了（-3，9）和（3，9）而 O 就变成了（-1，1）和（1，1）。同样可以线性分割，简单来说就是在低维空间不能线性分割的数据在高维空间很有可能可以线性分割。\n通常我们可以将 d 维数据映射到 D 维数据来检测是否可以线性分割（D>d）。这种想法可以帮助我们通过对低维输入（特征）空间的计算来获得高维空间的点积。我们可以用下面的例子说明。\n假设我们有二维空间的两个点：p = (p 1 ,p 2 ) 和 q = (q 1 ,q 2 )。用 Ø 表示映射函数，它可以按如下方式将二维的点映射到三维空间中：\n我们要定义一个核函数 K (p,q)，它可以用来计算两个点的内积，如下所示这说明三维空间中的内积可以通过计算二维空间中内积的平方来获得。这可以扩展到更高维的空间。所以根据低维的数据来计算它们的高维特征。在进行完映射后，我们就得到了一个高维空间数据。\n除了上面的这些概念之外，还有一个问题需要解决，那就是分类错误。仅仅找到具有最大边缘的决定边界是不够的。我们还需要考虑错误分类带来的误差。有时我们找到的决定边界的边缘可能不是最大的但是错误分类是最少的。所以我们需要对我们的模型进行修正来找到一个更好的决定边界：最大的边缘，最小的错误分类。评判标准就被修改为：\n下图显示这个概念。对于训练数据的每一个样本又增加了一个参数 ξ i 。它表示训练样本到他们所属类（实际所属类）的超平面的距离。对于那些分类正确的样本这个参数为 0，因为它们会落在它们的支持平面上。\n现在新的最优化问题就变成了：\n参数 C 的取值应该如何选择呢？很明显应该取决于你的训练数据。虽然没有一个统一的答案，但是在选取 C 的取值时我们还是应该考虑一下下面的规则：\n• 如果 C 的取值比较大，错误分类会减少，但是边缘也会减小。其实就是错误分类的代价比较高，惩罚比较大。（在数据噪声很小时我们可以选取较大的 C 值。）\n• 如果 C 的取值比较小，边缘会比较大，但错误分类的数量会升高。其实就是错误分类的代价比较低，惩罚很小。整个优化过程就是为了找到一个具有最大边缘的超平面对数据进行分类。（如果数据噪声比较大时，应该考虑）\n47.2 使用 SVM 进行手写数据 OCR\n目标\n本节我们还是要进行手写数据的 OCR，但这次我们使用的是 SVM 而不是 kNN。\n手写数字的 OCR\n在 kNN 中我们直接使用像素的灰度值作为特征向量。这次我们要使用方向梯度直方图Histogram of Oriented Gradients （HOG）作为特征向量。\n在计算 HOG 前我们使用图片的二阶矩对其进行抗扭斜（deskew）处理。\n所以我们首先要定义一个函数 deskew()，它可以对一个图像进行抗扭斜处理。下面就是 deskew() 函数：\ndef deskew(img): m = cv2.moments(img) if abs(m['mu02']) < 1e-2: return img.copy() skew = m['mu11']/m['mu02'] M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]]) img = cv2.warpAffine(img,M,(SZ, SZ),flags=affine_flags) return img\n下图显示了对含有数字 0 的图片进行抗扭斜处理后的效果。左侧是原始图像，右侧是处理后的结果。\n接下来我们要计算图像的 HOG 描述符，创建一个函数 hog()。为此我们计算图像 X 方向和 Y 方向的 Sobel 导数。然后计算得到每个像素的梯度的方向和大小。把这个梯度转换成 16 位的整数。将图像分为 4 个小的方块，对每一个小方块计算它们的朝向直方图（16 个 bin），使用梯度的大小做权重。这样每一个小方块都会得到一个含有 16 个成员的向量。4 个小方块的 4 个向量就组成了这个图像的特征向量（包含 64 个成员）。这就是我们要训练数据的特征向量。\ndef hog(img): gx = cv2.Sobel(img, cv2.CV_32F, 1, 0) gy = cv2.Sobel(img, cv2.CV_32F, 0, 1) mag, ang = cv2.cartToPolar(gx, gy) # quantizing binvalues in (0...16) bins = np.int32(bin_n*ang/(2*np.pi)) # Divide to 4 sub-squares bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:] mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:] hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)] hist = np.hstack(hists) return hist\n最后，和前面一样，我们将大图分割成小图。使用每个数字的前 250 个作\n为训练数据，后 250 个作为测试数据。全部代码如下所示：\nimport cv2 import numpy as np SZ=20 bin_n = 16 # Number of bins svm_params = dict( kernel_type = cv2.SVM_LINEAR, svm_type = cv2.SVM_C_SVC, C=2.67, gamma=5.383 ) affine_flags = cv2.WARP_INVERSE_MAP|cv2.INTER_LINEAR def deskew(img): m = cv2.moments(img) if abs(m['mu02']) < 1e-2: return img.copy() skew = m['mu11']/m['mu02'] M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]]) img = cv2.warpAffine(img,M,(SZ, SZ),flags=affine_flags) return img def hog(img): gx = cv2.Sobel(img, cv2.CV_32F, 1, 0) gy = cv2.Sobel(img, cv2.CV_32F, 0, 1) mag, ang = cv2.cartToPolar(gx, gy) bins = np.int32(bin_n*ang/(2*np.pi)) # quantizing binvalues in (0...16) bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:] mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:] hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)] hist = np.hstack(hists) # hist is a 64 bit vector return hist img = cv2.imread('digits.png',0) cells = [np.hsplit(row,100) for row in np.vsplit(img,50)] # First half is trainData, remaining is testData train_cells = [ i[:50] for i in cells ] test_cells = [ i[50:] for i in cells] ###### Now training ######################## deskewed = [map(deskew,row) for row in train_cells] hogdata = [map(hog,row) for row in deskewed] trainData = np.float32(hogdata).reshape(-1,64) responses = np.float32(np.repeat(np.arange(10),250)[:,np.newaxis]) svm = cv2.SVM() svm.train(trainData,responses, params=svm_params) svm.save('svm_data.dat') ###### Now testing ######################## deskewed = [map(deskew,row) for row in test_cells] hogdata = [map(hog,row) for row in deskewed] testData = np.float32(hogdata).reshape(-1,bin_n*4) result = svm.predict_all(testData) ####### Check Accuracy ######################## mask = result==responses correct = np.count_nonzero(mask) print correct*100.0/result.size\n准确率达到了 94%。你可以尝试一下不同的参数值，看看能不能达到更高的准确率。或者也可以读一下这个领域的文章并用代码实现它。\n48 K 值聚类\n48.1 理解 K 值聚类\n目标\n• 本节我们要学习 K 值聚类的概念以及它是如何工作的。\n原理\n我将用一个最常用的例子来给大家介绍 K 值聚类。\n48.1.1 T 恤大小问题\n话说有一个公司要生产一批新的 T 恤。很明显他们要生产不同大小的 T 恤来满足不同顾客的需求。所以这个公司收集了很多人的身高和体重信息，并把这些数据绘制在图上，如下所示：\n肯定不能把每个大小的 T 恤都生产出来，所以他们把所有的人分为三组：小，中，大，这三组要覆盖所有的人。我们可以使用 K 值聚类的方法将所有人分为 3 组，这个算法可以找到一个最好的分法，并能覆盖所有人。如果不能覆盖全部人的话，公司就只能把这些人分为更多的组，可能是 4 个或 5 个甚至更多。如下图：\n48.1.2 它是如何工作的？\n这个算法是一个迭代过程，我们会借助图片逐步介绍它。\n考虑下面这组数据（你也可以把它当成 T 恤问题），我们需要把他们分成两组。\n第一步：随机选取两个重心点，C 1 和 C 2 （有时可以选取数据中的两个点作为起始重心）。\n第二步：计算每个点到这两个重心点的距离，如果距离 C 1 比较近就标记为 0，如果距离 C 2 比较近就标记为 1。（如果有更多的重心点，可以标记为“2”，“3”等）\n在我们的例子中我们把属于 0 的标记为红色，属于 1 的标记为蓝色。我们就会得到下面这幅图。\n第三步：重新计算所有蓝色点的重心，和所有红色点的重心，并以这两个点更新重心点的位置。（图片只是为了演示说明而已，并不代表实际数据）重复步骤 2，更新所有的点标记。\n我们就会得到下面的图：\n继续迭代步骤 2 和 3，直到两个重心点的位置稳定下来。（当然也可以通过设置迭代次数，或者设置重心移动距离的阈值来终止迭代。）。此时这些点到它们相应重心的距离之和最小此时这些点到它们相应重心的距离之和最小。简单来说，C 1 到红色点的距离与 C 2 到蓝色点的距离之和最小。\n最终结果如下图所示：\n这就是对 K 值聚类的一个直观解释。要想知道更多细节和数据解释，你应该读一本关于机器学习的教科书或者参考更多资源中的链接。这只是 K 值聚类的基础。现在对这个算法有很多改进，比如：如何选取好的起始重心点，怎样加速迭代过程等。\n更多资源\n1. Machine Learning Course, Video lectures by Prof. Andrew Ng\n(Some of the images are taken from this)\n48.2 OpenCV 中的 K 值聚类\n目标\n• 学习使用 OpenCV 中的函数 cv2.kmeans() 对数据进行分类\n48.2.1 理解函数的参数\n输入参数\n理解函数的参数\n输入参数\n1. samples: 应该是 np.float32 类型的数据，每个特征应该放在一列。\n2. nclusters(K): 聚类的最终数目。\n3. criteria: 终止迭代的条件。当条件满足时，算法的迭代终止。它应该是一个含有 3 个成员的元组，它们是（typw，max_iter，epsilon）：\n• type 终止的类型：有如下三种选择：\n– cv2.TERM_CRITERIA_EPS 只有精确度 epsilon 满足是停止迭代。\n– cv2.TERM_CRITERIA_MAX_ITER 当迭代次数超过阈值时停止迭代。\n– cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER上面的任何一个条件满足时停止迭代。\n• max_iter 表示最大迭代次数。\n• epsilon 精确度阈值。\n4. attempts: 使用不同的起始标记来执行算法的次数。算法会返回紧密度最好的标记。紧密度也会作为输出被返回。\n5. flags：用来设置如何选择起始重心。通常我们有两个选择：cv2.KMEANS_PP_CENTERS和 cv2.KMEANS_RANDOM_CENTERS。\n输出参数\n1. compactness：紧密度，返回每个点到相应重心的距离的平方和。\n2. labels：标志数组（与上一节提到的代码相同），每个成员被标记为 0，1等\n3. centers：由聚类的中心组成的数组。\n现在我们用 3 个例子来演示如何使用 K 值聚类。\n48.2.2 仅有一个特征的数据\n假设我们有一组数据，每个数据只有一个特征（1 维）。例如前面的 T 恤问题，我们只使用人们的身高来决定 T 恤的大小。\n我们先来产生一些随机数据，并使用 Matplotlib 将它们绘制出来。\nimport numpy as np import cv2 from matplotlib import pyplot as plt x = np.random.randint(25,100,25) y = np.random.randint(175,255,25) z = np.hstack((x,y)) z = z.reshape((50,1)) z = np.float32(z) plt.hist(z,256,[0,256]),plt.show()\n现在我们有一个长度为 50，取值范围为 0 到 255 的向量 z。我已经将向量 z 进行了重排，将它变成了一个列向量。当每个数据含有多个特征是这会很有用。然后我们数据类型转换成 np.float32。\n我们得到下图：\n现在我们使用 KMeans 函数。在这之前我们应该首先设置好终止条件。我的终止条件是：算法执行 10 次迭代或者精确度 epsilon = 1.0。\n# Define criteria = ( type, max_iter = 10 , epsilon = 1.0 ) criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # Set flags (Just to avoid line break in the code) flags = cv2.KMEANS_RANDOM_CENTERS # Apply KMeans compactness,labels,centers = cv2.kmeans(z,2,None,criteria,10,flags)\n返回值有紧密度（compactness）, 标志和中心。在本例中我的到的中心是 60 和 207。标志的数目与测试数据的多少是相同的，每个数据都会被标记上“0”，“1”等。这取决与它们的中心是什么。现在我们可以根据它们的标志将把数据分两组。\nA = z[labels==0] B = z[labels==1]\n现在将 A 组数用红色表示，将 B 组数据用蓝色表示，重心用黄色表示。\n# Now plot 'A' in red, 'B' in blue, 'centers' in yellow plt.hist(A,256,[0,256],color = 'r') plt.hist(B,256,[0,256],color = 'b') plt.hist(centers,32,[0,256],color = 'y') plt.show()\n下面就是结果：\n含有多个特征的数据\n在前面的 T 恤例子中我们只考虑了身高，现在我们也把体重考虑进去，也就是两个特征。\n在前一节我们的数据是一个单列向量。每一个特征被排列成一列，每一行对应一个测试样本。\n在本例中我们的测试数据适应 50x2 的向量，其中包含 50 个人的身高和体重。第一列对应与身高，第二列对应与体重。第一行包含两个元素，第一个是第一个人的身高，第二个是第一个人的体重。剩下的行对应与其他人的身高和体重。如下图所示：\n现在我们来编写代码：\nimport numpy as np import cv2 from matplotlib import pyplot as plt X = np.random.randint(25,50,(25,2)) Y = np.random.randint(60,85,(25,2)) Z = np.vstack((X,Y)) # convert to np.float32 Z = np.float32(Z) # define criteria and apply kmeans() criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) ret,label,center=cv2.kmeans(Z,2,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS) # Now separate the data, Note the flatten() A = Z[label.ravel()==0] B = Z[label.ravel()==1] # Plot the data plt.scatter(A[:,0],A[:,1]) plt.scatter(B[:,0],B[:,1],c = 'r') plt.scatter(center[:,0],center[:,1],s = 80,c = 'y', marker = 's') plt.xlabel('Height'),plt.ylabel('Weight') plt.show()\n下面是我得到的结果：\n48.2.3 颜色量化\n颜色量化就是减少图片中颜色数目的一个过程。为什么要减少图片中的颜色呢？减少内存消耗！有些设备的资源有限，只能显示很少的颜色。在这种情况下就需要进行颜色量化。我们使用 K 值聚类的方法来进行颜色量化。没有什么新的知识需要介绍了。现在有 3 个特征：R，G，B。所以我们需要把图片数据变形成 Mx3（M 是图片中像素点的数目）的向量。聚类完成后，我们用聚类中心值替换与其同组的像素值，这样结果图片就只含有指定数目的颜色了。下面是代码：\nimport numpy as np import cv2 img = cv2.imread('home.jpg') Z = img.reshape((-1,3)) # convert to np.float32 Z = np.float32(Z) # define criteria, number of clusters(K) and apply kmeans() criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) K = 8 ret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS) # Now convert back into uint8, and make original image center = np.uint8(center) res = center[label.flatten()] res2 = res.reshape((img.shape)) cv2.imshow('res2',res2) cv2.waitKey(0) cv2.destroyAllWindows()\n下面是 K=8 的结果："}
{"content2":"准备环境\nanaconda\nnano ~/.zshrc export PATH=$PATH:/anaconda/bin source ~/.zshrc echo $HOME echo $PATH\nipython\nconda update conda && conda update ipython ipython-notebook ipython-qtconsole conda install scipy\nPYTHONPATH\nexport SPARK_HOME=/Users/erichan/garden/spark-1.5.1-bin-hadoop2.6 export PYTHONPATH=${SPARK_HOME}/python/:${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip\n运行环境\ncd $SPARK_HOME IPYTHON=1 IPYTHON_OPTS=\"--pylab\" ./bin/pyspark\n数据\n1. 获取原始数据\nPATH = \"/Users/erichan/sourcecode/book/Spark机器学习\" user_data = sc.textFile(\"%s/ml-100k/u.user\" % PATH) user_fields = user_data.map(lambda line: line.split(\"|\")) movie_data = sc.textFile(\"%s/ml-100k/u.item\" % PATH) movie_fields = movie_data.map(lambda lines: lines.split(\"|\")) rating_data_raw = sc.textFile(\"%s/ml-100k/u.data\" % PATH) rating_data = rating_data_raw.map(lambda line: line.split(\"\\t\"))\nnum_movies = movie_data.count() print num_movies\n1682\nuser_data.first()\nu'1|24|M|technician|85711'\nmovie_data.first()\nu'1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0'\nrating_data_raw.first()\nu'196\\t242\\t3\\t881250949'\n2. 探索数据\n2.1. 按列统计\nnum_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() ratings = rating_data.map(lambda fields: int(fields[2])) num_ratings = ratings.count() max_rating = ratings.reduce(lambda x, y: max(x, y)) min_rating = ratings.reduce(lambda x, y: min(x, y)) mean_rating = ratings.reduce(lambda x, y: x + y) / float(num_ratings) median_rating = np.median(ratings.collect()) ratings_per_user = num_ratings / num_users ratings_per_movie = num_ratings / num_movies print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes)\nUsers: 943, genders: 2, occupations: 21, ZIP codes: 795\nprint \"Min rating: %d\" % min_rating\nMin rating: 1\nprint \"Max rating: %d\" % max_rating\nMax rating: 5\nprint \"Average rating: %2.2f\" % mean_rating\nAverage rating: 3.53\nprint \"Median rating: %d\" % median_rating\nMedian rating: 4\nprint \"Average # of ratings per user: %2.2f\" % ratings_per_user\nAverage # of ratings per user: 106.00\nprint \"Average # of ratings per movie: %2.2f\" % ratings_per_movie\nAverage # of ratings per movie: 59.00\nratings.stats()\n(count: 100000, mean: 3.52986, stdev: 1.12566797076, max: 5, min: 1)\n2.2. 使用matplotlib的hist函数绘制直方图\nages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10)\ncount_by_rating = ratings.countByValue() x_axis = np.array(count_by_rating.keys()) y_axis = np.array([float(c) for c in count_by_rating.values()]) # we normalize the y-axis here to percentages y_axis_normed = y_axis / y_axis.sum() pos = np.arange(len(x_axis)) width = 1.0 ax = plt.axes() ax.set_xticks(pos + (width / 2)) ax.set_xticklabels(x_axis) plt.bar(pos, y_axis_normed, width, color='lightblue') plt.xticks(rotation=30) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10)\ncount_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect() x_axis1 = np.array([c[0] for c in count_by_occupation]) y_axis1 = np.array([c[1] for c in count_by_occupation]) x_axis = x_axis1[np.argsort(y_axis1)] y_axis = y_axis1[np.argsort(y_axis1)] pos = np.arange(len(x_axis)) width = 1.0 ax = plt.axes() ax.set_xticks(pos + (width / 2)) ax.set_xticklabels(x_axis) plt.bar(pos, y_axis, width, color='lightblue') plt.xticks(rotation=30) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10)\n2.3. 使用countByValue函数统计\ncount_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue() print \"Map-reduce approach:\" print dict(count_by_occupation2)\n{u'administrator': 79, u'retired': 14, u'lawyer': 12, u'healthcare': 16, u'marketing': 26, u'executive': 32, u'scientist': 31, u'student': 196, u'technician': 27, u'librarian': 51, u'programmer': 66, u'salesman': 12, u'homemaker': 7, u'engineer': 67, u'none': 9, u'doctor': 7, u'writer': 45, u'entertainment': 18, u'other': 105, u'educator': 95, u'artist': 28}\nprint \"\" print \"countByValue approach:\" print dict(count_by_occupation)\n{u'administrator': 79, u'writer': 45, u'retired': 14, u'lawyer': 12, u'doctor': 7, u'marketing': 26, u'executive': 32, u'none': 9, u'entertainment': 18, u'healthcare': 16, u'scientist': 31, u'student': 196, u'educator': 95, u'technician': 27, u'librarian': 51, u'programmer': 66, u'artist': 28, u'salesman': 12, u'other': 105, u'homemaker': 7, u'engineer': 67}\n2.4. 使用filter转换\ndef convert_year(x): try: return int(x[-4:]) except: return 1900 years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)) years_filtered = years.filter(lambda x: x != 1900) movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue() values = movie_ages.values() bins = movie_ages.keys() hist(values, bins=bins, color='lightblue', normed=True)\n(array([ 0. , 0.07575758, 0.09090909, 0.09090909, 0.18181818,\n0.18181818, 0.04545455, 0.07575758, 0.07575758, 0.03030303,\n0. , 0.01515152, 0.01515152, 0.03030303, 0. ,\n0.03030303, 0. , 0. , 0. , 0. ,\n0. , 0. , 0.01515152, 0. , 0.01515152,\n0. , 0. , 0. , 0. , 0. ,\n0. , 0. , 0. , 0. , 0. ,\n0. , 0. , 0.01515152, 0. , 0. ,\n0. , 0. , 0. , 0. , 0. ,\n0. , 0. , 0. , 0. , 0. ,\n0. , 0. , 0. , 0. , 0. ,\n0. , 0. , 0. , 0. , 0. ,\n0. , 0. , 0. , 0. , 0. ,\n0.01515152, 0. , 0. , 0. , 0. ]),\narray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n68, 72, 76]),\n)\nfig = matplotlib.pyplot.gcf() fig.set_size_inches(16,10)\n2.5. 使用groupByKey分组\n# to compute the distribution of ratings per user, we first group the ratings by user id user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))).groupByKey() # then, for each key (user id), we find the size of the set of ratings, which gives us the # ratings for that user user_ratings_byuser = user_ratings_grouped.map(lambda (k, v): (k, len(v))) user_ratings_byuser.take(5)\n[(2, 62), (4, 24), (6, 211), (8, 59), (10, 184)]\nuser_ratings_byuser_local = user_ratings_byuser.map(lambda (k, v): v).collect() hist(user_ratings_byuser_local, bins=200, color='lightblue', normed=True)\nfig = matplotlib.pyplot.gcf() fig.set_size_inches(16,10)\n3. 处理转换\n3.1. 填充缺失\nyears_pre_processed = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)).filter(lambda yr: yr != 1900).collect() years_pre_processed_array = np.array(years_pre_processed) # first we compute the mean and median year of release, without the 'bad' data point mean_year = np.mean(years_pre_processed_array[years_pre_processed_array!=1900]) median_year = np.median(years_pre_processed_array[years_pre_processed_array!=1900]) idx_bad_data = np.where(years_pre_processed_array==1900)[0] years_pre_processed_array[idx_bad_data] = median_year print \"Mean year of release: %d\" % mean_year\nMean year of release: 1989\nprint \"Median year of release: %d\" % median_year\nMedian year of release: 1995\nprint \"Index of '1900' after assigning median: %s\" % np.where(years_pre_processed_array == 1900)[0]\nIndex of '1900' after assigning median: []\n4. 提取特征\n4.1. 类别特征（norminal变量/ordinal变量）\nall_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect() all_occupations.sort() # create a new dictionary to hold the occupations, and assign the \"1-of-k\" indexes idx = 0 all_occupations_dict = {} for o in all_occupations: all_occupations_dict[o] = idx idx +=1 # try a few examples to see what \"1-of-k\" encoding is assigned print \"Encoding of 'doctor': %d\" % all_occupations_dict['doctor'] print \"Encoding of 'programmer': %d\" % all_occupations_dict['programmer']\nEncoding of 'doctor': 2\nEncoding of 'programmer': 14\nnumpy的zeros函数\nK = len(all_occupations_dict) binary_x = np.zeros(K) k_programmer = all_occupations_dict['programmer'] binary_x[k_programmer] = 1 print \"Binary feature vector: %s\" % binary_x print \"Length of binary vector: %d\" % K\nBinary feature vector: [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n0. 0.] Length of binary vector: 21\n4.2. 派生特征\n时间戳转换为类别特征\ndef extract_datetime(ts): import datetime return datetime.datetime.fromtimestamp(ts) def assign_tod(hr): times_of_day = { 'morning' : range(7, 12), 'lunch' : range(12, 15), 'afternoon' : range(15, 18), 'evening' : range(18, 23), 'night' : {23,24,0,1,2,3,4,5,6,7} } for k, v in times_of_day.iteritems(): if hr in v: return k timestamps = rating_data.map(lambda fields: int(fields[3])) hour_of_day = timestamps.map(lambda ts: extract_datetime(ts).hour) # now apply the \"time of day\" function to the \"hour of day\" RDD time_of_day = hour_of_day.map(lambda hr: assign_tod(hr)) timestamps.take(5)\n[881250949, 891717742, 878887116, 880606923, 886397596]\nhour_of_day.take(5)\n[23, 3, 15, 13, 13]\ntime_of_day.take(5)\n['night', 'night', 'afternoon', 'lunch', 'lunch']\n4.3. 文本特征\ndef extract_title(raw): import re grps = re.search(\"\\((\\w+)\\)\", raw) if grps: return raw[:grps.start()].strip() else: return raw raw_titles = movie_fields.map(lambda fields: fields[1]) for raw_title in raw_titles.take(5): print extract_title(raw_title)\nToy Story\nGoldenEye\nFour Rooms\nGet Shorty\nCopycat\nmovie_titles = raw_titles.map(lambda m: extract_title(m)) # next we tokenize the titles into terms. We'll use simple whitespace tokenization title_terms = movie_titles.map(lambda t: t.split(\" \")) print title_terms.take(5)\n[[u'Toy', u'Story'], [u'GoldenEye'], [u'Four', u'Rooms'], [u'Get', u'Shorty'], [u'Copycat']]\nflatMap\nall_terms = title_terms.flatMap(lambda x: x).distinct().collect() # create a new dictionary to hold the terms, and assign the \"1-of-k\" indexes idx = 0 all_terms_dict = {} for term in all_terms: all_terms_dict[term] = idx idx +=1 num_terms = len(all_terms_dict) print \"Total number of terms: %d\" % num_terms\nTotal number of terms: 2645\nprint \"Index of term 'Dead': %d\" % all_terms_dict['Dead']\nIndex of term 'Dead': 147\nprint \"Index of term 'Rooms': %d\" % all_terms_dict['Rooms']\nIndex of term 'Rooms': 1963\nzipWithIndex\nall_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap() print \"Index of term 'Dead': %d\" % all_terms_dict2['Dead'] print \"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms']\nIndex of term 'Dead': 147\nIndex of term 'Rooms': 1963\n创建稀疏向量/广播变量\nscipy depends $PYTHONPATH\ndef create_vector(terms, term_dict): from scipy import sparse as sp x = sp.csc_matrix((1, num_terms)) for t in terms: if t in term_dict: idx = term_dict[t] x[0, idx] = 1 return x all_terms_bcast = sc.broadcast(all_terms_dict) term_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value)) term_vectors.take(5)\n[<1x2645 sparse matrix of type ''\nwith 1 stored elements in Compressed Sparse Column format>,\n<1x2645 sparse matrix of type ''\nwith 1 stored elements in Compressed Sparse Column format>,\n<1x2645 sparse matrix of type ''\nwith 1 stored elements in Compressed Sparse Column format>,\n<1x2645 sparse matrix of type ''\nwith 1 stored elements in Compressed Sparse Column format>,\n<1x2645 sparse matrix of type ''\nwith 1 stored elements in Compressed Sparse Column format>]\n4.4. 正则化特征\nnp.random.seed(42) x = np.random.randn(10) norm_x_2 = np.linalg.norm(x) normalized_x = x / norm_x_2 print \"x:\\n%s\" % x print \"2-Norm of x: %2.4f\" % norm_x_2 print \"Normalized x:\\n%s\" % normalized_x print \"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x)\nx:\n[ 0.49671415 -0.1382643 0.64768854 1.52302986 -0.23415337 -0.23413696\n1.57921282 0.76743473 -0.46947439 0.54256004]\n2-Norm of x: 2.5908\nNormalized x:\n[ 0.19172213 -0.05336737 0.24999534 0.58786029 -0.09037871 -0.09037237\n0.60954584 0.29621508 -0.1812081 0.20941776]\n2-Norm of normalized_x: 1.0000\nfrom pyspark.mllib.feature import Normalizer normalizer = Normalizer() vector = sc.parallelize([x]) normalized_x_mllib = normalizer.transform(vector).first().toArray() print \"x:\\n%s\" % x print \"2-Norm of x: %2.4f\" % norm_x_2 print \"Normalized x MLlib:\\n%s\" % normalized_x_mllib print \"2-Norm of normalized_x_mllib: %2.4f\" % np.linalg.norm(normalized_x_mllib)\nx:\n[ 0.49671415 -0.1382643 0.64768854 1.52302986 -0.23415337 -0.23413696\n1.57921282 0.76743473 -0.46947439 0.54256004]\n2-Norm of x: 2.5908\nNormalized x MLlib:\n[ 0.19172213 -0.05336737 0.24999534 0.58786029 -0.09037871 -0.09037237\n0.60954584 0.29621508 -0.1812.20941776]\n2-Norm of normalized_x_mllib: 1.0000"}
{"content2":"人工智能早已不是一个新名词，它的发展历史已经有几十年。从80年代早期开始，当时计算机科学家设计出可以学习和模仿人类行为的算法。在学习方面，最重要的算法是神经网络，但由于模型过于强大，没有足够的数据支持，导致不是很成功。然而，在一些更具体的任务中，使用数据来适应函数的想法获得了巨大的成功，这也构成了机器学习的基础。\n人工智能早已不是一个新名词，它的发展历史已经有几十年。从80年代早期开始，当时计算机科学家设计出可以学习和模仿人类行为的算法。在学习方面，最重要的算法是神经网络，但由于模型过于强大，没有足够的数据支持，导致不是很成功。然而，在一些更具体的任务中，使用数据来适应函数的想法获得了巨大的成功，这也构成了机器学习的基础。在模仿方面，人工智能在图像识别、语音识别和自然语言处理方面有着广泛的应用。专家们花费了大量时间去创建边缘计算，彩色型材，N-gram语言模型，语法树等，不料所获成绩平平。\n传统的机器学习\n机器学习(ML)技术在预测中发挥了重要作用，机器学习已经经历了多代，有一套完备的模型结构，如：\n线性回归\nLogistic回归\n决策树\n支持向量机\n贝叶斯模型\n正则化模型\n集成模型\n神经网络\n每一个预测模型都基于一定的算法结构，参数可进行调整。训练预测模型涉及以下步骤：\n1.选择模型结构(例如，逻辑回归、随机森林等)。\n2.用训练数据(输入和输出)对模型进行反馈。\n3.学习算法将输出最优模型(即具有特定参数的模型，使训练误差最小化)。\n每个模型都有自己的特点，在某些任务中表现很好，在其他方面也却不尽人意。但一般来说，我们可以把它们分为低功耗(简单)模型和大功率(复杂)模型。在不同的模型之间进行选择是一个非常棘手的问题。传统上，使用低功耗/简单模型比使用高功率/复杂模型要好，原因如下：\n·在我们拥有大量的处理能力之前，训练高功率模型需要花费很长时间。\n·直到我们有一个庞大的数据量，培养高功率模型会导致过拟合问题(由于高功率模型具有丰富的参数，可以适应多种数据的形状，我们可能最终会训练出一个与当前训练数据非常相关的模型，而不是对未来数据进行预测)。\n然而，选择低功耗模型存在着所谓的”欠拟合”问题，即模型结构过于简单，无法在较复杂的情况下适应训练数据。(假设下面的数据有一个二次关系：y=5*X的平方;没有方法可以拟合一个线性回归：y=A，B，B，B，无论我们选择什么样的A和B。)\n为了减轻”不适合的问题”，数据科学家通常会应用他们的”领域知识”来产生”输入特性”，它与输出有更直接的关系。(例如，返回到二次关系y=5*X的平方)，然后通过选取a=5和b=0，拟合线性回归。\n机器学习的一个主要障碍是这个特征工程步骤，它要求领域专家在进入培训过程之前识别重要的信号。特征工程步骤非常手工，需要大量的领域专门知识，因此成为当今大多数机器学习任务的主要瓶颈。换句话说，如果我们没有足够的处理能力和足够的数据，那么我们必须使用低功耗/简单的模型，这需要我们花大量的时间和精力来创建适当的输入特性。这是大多数数据科学家花时间做的事情。\n神经网络的回归\n在2000年代早期，随着大容量数据时代大量的细粒度事件数据的收集，随着云计算和大规模并行处理基础设施的进步，机器处理能力得到了极大的提高。我们不再局限于低功耗/简单的模型。例如，当今最流行的两种主流机器学习模型是随机森林和梯度增强树。然而，尽管它们都非常强大，并提供非线性模型拟合训练数据，数据科学家仍然需要仔细地创建功能，以达到良好的性能。\n与此同时，计算机科学家重新使用了许多层的神经网络来完成这些人类模拟任务。这给新出生的DNN(深度神经网络)在图像分类和语音识别的任务提供了一个重大的突破。\nDNN的主要区别是，你可以发出原信号，(例如，RGB像素值)直接到DNN没有创造任何特定于域的输入特征。通过多层次的神经元(这就是为什么它被称为”深”的神经网络)，能够自动生成相应的功能，通过各层最后提供了一个很好的预测。这大大节省了”特征工程”的努力，也是数据科学家遇到的一个主要瓶颈。\nDNN也演变成许多不同的网络结构，所以我们美国有线电视新闻网(卷积神经网络)，RNN(神经网络)、LSTM(长短期记忆)、GAN(生成对抗网络)，迁移学习，注意模型…整个光谱被称为”深度学习”，这是当今全机器学习界关注的焦点。\n强化学习\n另一个关键的部分是如何模仿一个人(或动物)学习。想象一下感知/行为/奖赏周期的非常自然的动物行为。一个人或动物首先会通过感知他或她处于什么状态来理解环境。基于这一点，他或她会选择一个”动作”把他或她带到另一个”状态”，然后他或她会得到一个”奖励”，如此循环重复。\n这种学习方法(称为强化学习)与传统的有监督机器学习的曲线拟合方法有很大的不同。特别是，强化学习的发生非常迅速，因为每一个新的反馈(如执行一个动作和获得一个奖励)立即被发送来影响随后的决定。强化学习已经获得了巨大的成功在自动驾驶汽车以及AlphaGO(下棋机器人)。\n强化学习也提供了一个平滑的预测和优化集成，因为它保持一个信念的当前状态和可能的转移概率时采取不同的行动，然后作出决定，哪些行动会带来最好的结果。\n深度学习+强化学习=人工智能\n与经典机器学习技术相比，深度学习提供了一个更强大的预测模型，通常能产生良好的预测。与经典的优化模型相比，强化学习提供了更快的学习机制，并且更适应环境的变化。"}
{"content2":"1、智能的分类：感知——》记忆和思维——》学习和自适应——》决策与执行\n2、发展历史与大事件\n3、人工智能分类：\n（1）弱人工智能：特定领域，感知与记忆存储，如图像识别，语音识别；\n（2）强人工智能：多领域综合，认知学习与决策执行，如自动驾驶；\n（3）超人工智能：超越人类的智能，独立意识与创新创造；\n4、基础、技术、与应用层\n5、从数据处理看pc-互联网-物联网-人工智能的发展\n6、人工智能的关键，机器学习算法概览\n\n来自为知笔记(Wiz)"}
{"content2":"十分抱歉，由于项目太忙（我会说自己懒吗？）柳猫一直没有更新自己的手记，现在，就让柳猫来讲讲十个常用的深度学习算法。\n过去十年里，人们对机器学习的兴趣经历了爆炸式的整长。我们几乎每天都可以在计算机程序、行业会议和媒体上看到机器学习的身影。很多关于机器学习的讨论都混淆了“机器学习能做什么”和“人类希望机器学习能做什么”。从根本上讲，机器学习是运用算法从原始数据中提取信息，并用某种类型的模型进行表示，然后使用该模型对一些尚未用模型表示的其他数据来进行推断。\n神经网络就是机器学习各类模型中的其中一类，并且已经存在了至少50年。神经网络的基本单位是节点，它的想法大致来源于哺乳动物大脑中的生物神经元。生物大脑中的神经元节点之间的链接是随着时间推移不断演化的，而神经网络中的神经元节点链接也借鉴了这一点，会不断演化（通过“训练”的方式）。\n神经网络中很多重要框架的建立和改进都完成于二十世纪八十年代中期和九十年代初期。然而，要想获得较好结果需要大量的时间和数据，由于当时计算机的能力有限，神经网络的发展受到了一定的阻碍，人们的关注度也随之下降。二十一世纪初期，计算机的运算能力呈指数级增长，业界也见证了计算机技术发展的“寒武纪爆炸”——这在之前都是无法想象的。深度学习以一个竞争者的姿态出现，在计算能力爆炸式增长的十年里脱颖而出，并且赢得了许多重要的机器学习竞赛。其热度在2017年仍然不减。如今，在机器学习的出现的地方我们都能看到深度学习的身影。\n这是柳猫自己做的一个小例子，词向量的 t-SNE 投影，通过相似性进行聚类。\n最近，我开始阅读关于深度学习的学术论文。根据我的个人研究，以下文章对这个领域的发展产生了巨大的影响：\n1998年NYU的文章《基于梯度学习的文档识别》（Gradient-Based Learning Applied to Document Recognition） 介绍了卷积神经网络在机器学习中的应用。\nToronto 2009年的文章《深度波兹曼机器》（Deep Boltzmann Machines） 针对波兹曼机提出了一种新的学习算法，其中包含许多隐藏层。\nStanford 和 Google 2012年联合发表的文章《使用大规模非监督学习构建高层特征》（Building High-Level Features Using Large-Scale Unsupervised Learning） 解决了仅利用未标记的数据构建高级、特定类的特征检测器的问题。\nBerkeley 2013年的文章《用于一般视觉识别的深层卷积激活特征》（DeCAF——A Deep Convolutional Activation Feature for Generic Visual Recognition） 发布了名为 DeCAF 的算法，这是深度卷积激活特征的一个开源实现，使用相关的网络参数，视觉研究人员能够利用一系列视觉概念学习范例进行深入实验。\nDeepMind 2016年的文章《用深度强化学习玩Atari》（Playing Atari with Deep Reinforcement Learning） 提出了第一个可以成功地通过强化学习从高维感官输入中直接学习控制策略的深度学习模型。\n柳猫整理了人工智能工程师 10 个用于解决机器学习问题的强大的深度学习方法。但是，我们首先需要定义什么是深度学习。\n如何定义深度学习是很多人面临的一个挑战，因为它的形式在过去的十年中已经慢慢地发生了改变。下图直观地展示了人工智能，机器学习和深度学习之间的关系。\n人工智能领域广泛，存在时间较长。深度学习是机器学习领域的一个子集，而机器学习是人工智能领域的一个子集。一般将深度学习网络与“典型”前馈多层网络从如下方面进行区分：\n深度学习网络具有比前馈网络更多的神经元\n深度学习网络连接层之间的方式更复杂\n深度学习网络需要有像“寒武纪大爆发”式的计算能力进行训练\n深度学习网络能够自动提取特征\n上文提到的“更多的神经元”是指近年来神经元的数量不断增加，就可以用更复杂的模型来表示。层也从多层网络中每一层完全连接，发展到卷积神经网络中神经元片段的局部连接，以及与递归神经网络中的同一神经元的循环连接（与前一层的连接除外）。\n因此，深度学习可以被定义为以下四个基本网络框架中具有大量参数和层数的神经网络：\n无监督预训练网络\n卷积神经网络\n循环神经网络\n递归神经网络\n在这篇文章中，我主要讨论三个框架：\n卷积神经网络（Convolutional Neural Network）基本上就是用共享权重在空间中进行扩展的标准神经网络。卷积神经网络主要是通过内部卷积来识别图片，内部卷积可以看到图像上识别对象的边缘。\n循环神经网络（Recurrent Neural Network）基本上就是在时间上进行扩展的标准神经网络，它提取进入下一时间步的边沿，而不是在同一时间进入下一层。循环神经网络主要是为了识别序列，例如语音信号或者文本。其内部的循环意味着网络中存在短期记忆。\n递归神经网络（Recursive Neural Network）更类似于分层网络，其中输入序列没有真正的时间面，但是必须以树状方式分层处理。以下10种方法均可应用于这些框架。\n1、反向传播\n反向传播是一种计算函数偏导数（或梯度）的简单方法，它的形式是函数组合（如神经网络）。在使用基于梯度的方法求解最优化问题（梯度下降只是其中之一）时，需要在每次迭代中计算函数梯度。\n对于一个神经网络，其目标函数是组合形式。那么应该如何计算梯度呢？有2种常规方法：\n（1）微分解析法。函数形式已知的情况下，只需要用链式法则（基础微积分）计算导数。\n（2）有限差分法近似微分。这种方法运算量很大，因为函数评估的数量级是 O(N)，其中 N 是参数的个数。与微分解析法相比，这种方法运算量更大，但是在调试时，通常会使用有限差分验证反向传播的效果。\n2、随机梯度下降\n梯度下降的一个直观理解就是想象一条源自山顶的河流。这条河流会沿着山势的方向流向山麓的最低点，而这也正是梯度下降法的目标。\n我们所期望的最理想的情况就是河流在到达最终目的地（最低点）之前不会停下。在机器学习中，这等价于我们已经找到了从初始点（山顶）开始行走的全局最小值（或最优值）。然而，可能由于地形原因，河流的路径中会出现很多坑洼，而这会使得河流停滞不前。在机器学习术语中，这种坑洼称为局部最优解，而这不是我们想要的结果。有很多方法可以解决局部最优问题。\n因此，由于地形（即函数性质）的限制，梯度下降算法很容易卡在局部最小值。但是，如果能够找到一个特殊的山地形状（比如碗状，术语称作凸函数），那么算法总是能够找到最优点。在进行最优化时，遇到这些特殊的地形（凸函数）自然是最好的。另外，山顶初始位置（即函数的初始值）不同，最终到达山底的路径也完全不同。同样，不同的流速（即梯度下降算法的学习速率或步长）也会导致到达目的地的方式有差异。是否会陷入或避开一个坑洼（局部最小值），都会受到这两个因素的影响。\n3、学习率衰减\n调整随机梯度下降优化算法的学习速率可以提升性能并减少训练时间。这被称作学习率退火或自适应学习率。训练中最简单也最常用的学习率自适应方法就是逐渐降低学习率。在训练初期使用较大的学习率，可以对学习率进行大幅调整；在训练后期，降低学习率，以一个较小的速率更新权重。这种方法在早期可以快速学习获得较好的权重，并在后期对权重进行微调。\n两个流行而简单的学习率衰减方法如下：\n线性地逐步降低学习率\n在特定时点大幅降低学习率\n4、Dropout\n拥有大量参数的深度神经网络是非常强大的机器学习系统。然而，在这样的网络中,过拟合是一个很严重的问题。而且大型网络的运行速度很慢，这就使得在测试阶段通过结合多个不同的大型神经网络的预测来解决过拟合问题是很困难的。Dropout 方法可以解决这个问题。\n其主要思想是，在训练过程中随机地从神经网络中删除单元（以及相应的连接），这样可以防止单元间的过度适应。训练过程中，在指数级不同“稀疏度”的网络中剔除样本。在测试阶段，很容易通过使用具有较小权重的单解开网络（single untwined network），将这些稀疏网络的预测结果求平均来进行近似。这能有效地避免过拟合，并且相对于其他正则化方法能得到更大的性能提升。Dropout 技术已经被证明在计算机视觉、语音识别、文本分类和计算生物学等领域的有监督学习任务中能提升神经网络的性能，并在多个基准数据集中达到最优秀的效果。\n5、最大池\n最大池是一种基于样本的离散化方法。目标是对输入表征（图像、隐藏层输出矩阵等）进行下采样，降低维度并且允许对子区域中的特征进行假设。\n通过提供表征的抽象形式，这种方法可以在某种程度上解决过拟合问题。同样，它也通过减少学习参数的数目以及提供基本的内部表征转换不变性来减少计算量。最大池是通过将最大过滤器应用于通常不重叠的初始表征子区域来完成的。\n6、批量标准化\n当然，包括深度网络在内的神经网络需要仔细调整权重初始值和学习参数。批量标准化能够使这个过程更加简单。\n权重问题：\n无论怎么设置权重初始值，比如随机或按经验选择，初始权重和学习后的权重差别都很大。考虑一小批权重，在最初时，对于所需的特征激活可能会有很多异常值。\n深度神经网络本身就具有病态性，即初始层的微小变化就会导致后一层的巨大变化。\n在反向传播过程中，这些现象会导致梯度的偏移，这就意味着在学习权重以产生所需要的输出之前，梯度必须补偿异常值。而这将导致需要额外的时间才能收敛。\n批量标准化将这些梯度从异常值调整为正常值，并在小批量范围内（通过标准化）使其向共同的目标收敛。\n学习率问题：\n通常来说，学习率都比较小，这样只有一小部分的梯度用来校正权重，因为异常激活的梯度不应该影响已经学习好的权重。\n通过批量标准化，这些异常激活的可能性会被降低，就可以使用更大的学习率加速学习过程。电动叉车轮胎\n7、长短期记忆\n长短期记忆网络（LSTM）和其他递归神经网络中的神经元有以下三个不同点：\n它可以决定何时让输入进入神经元\n它可以决定何时记住上一个时间步中计算的内容\n它可以决定何时让输出传递到下一个时间戳 LSTM的强大之处在于它可以只基于当前的输入就决定上述所有。请看下方的图表：\n当前时间戳的输入信号 x(t) 决定了上述三点。\n输入门（input gate）决定了第一点，\n遗忘门（forget gate）决定了第二点，\n输出门（output gate）决定了第三点。 只依赖输入就可以完成这三项决定。这是受到大脑工作机制的启发，大脑可以基于输入来处理突然的上下文语境切换。\n8、Skip-gram\n词嵌入模型的目的是针对每个词学习一个高维密集表征，其中嵌入向量之间的相似性显示了相应词语之间语义或句法的相似性。Skip-gram 是一种学习词嵌入算法的模型。 skip-gram 模型（包括很多其它词嵌入模型）背后的主要思想是：如果两个词汇项有相似的上下文，则它们是相似的。\n换句话说，假设有一个句子，比如“cats are mammals”，如果用“dogs”替换“cats”，该句子仍然是有意义的。因此在这个例子中，“dogs”和“cats”有相似的上下文（即“are mammals”）。\n基于以上假设，我们可以考虑一个上下文窗口（包含 K 个连续项）。然后跳过其中一个词，试着学习一个可以得到除了跳过的这个词以外所有词项，并且可以预测跳过的词的神经网络。因此，如果两个词在一个大语料库中多次具有相似的上下文，那么这些词的嵌入向量将会是相似的。\n9、连续词袋模型\n在自然语言处理中，我们希望将文档中的每一个单词表示为一个数值向量，使得出现在相似上下文中的单词具有相似或相近的向量表示。在连续词袋模型中，我们的目标是利用一个特定单词的上下文，预测该词。\n首先在一个大的语料库中抽取大量的句子，每看到一个单词，同时抽取它的上下文。然后我们将上下文单词输入到一个神经网络，并预测在这个上下文中心的单词。\n当我们有成千上万个这样的上下文词汇和中心词时，我们就得到了一个神经网络数据集的实例。然后训练这个神经网络，在经过编码的隐藏层的最终输出中，我们得到了特定单词的嵌入式表达。当我们对大量的句子进行训练时也能发现，类似上下文中的单词都可以得到相似的向量。\n10、迁移学习\n我们来考虑一下卷积神经网络是如何处理图像的。假设有一张图像，对其应用卷积，并得到像素的组合作为输出。假设这些输出是边缘，再次应用卷积，那么现在的输出将是边缘或线的组合。然后再次应用卷积，此时的输出将是线的组合，以此类推。可以把它想象成是在每一层寻找一个特定的模式。神经网络的最后一层通常会变得非常特别。\n如果基于 ImageNet 进行训练，那么神经网络的最后一层或许就是在寻找儿童、狗或者飞机之类的完整图像。再往后倒退几层，可能会看到神经网络在寻找眼睛、耳朵、嘴巴或者轮子等组成部分。\n深度卷积神经网络中的每一层逐步建立起越来越高层次的特征表征，最后几层通常是专门针对输入数据。另一方面，前面的层则更为通用，主要用来在一大类图片中有找到许多简单的模式。\n迁移学习就是在一个数据集上训练卷积神经网络时，去掉最后一层，在不同的数据集上重新训练模型的最后一层。直观来讲，就是重新训练模型以识别不同的高级特征。因此，训练时间会减少很多，所以在没有足够的数据或者需要太多的资源时，迁移学习是一个很有用的工具。\n总结：\n深度学习是非常注重技术实践，所谓的百看不如一练。当然柳猫这里讲的还是非常肤浅，如果能够引起小伙伴们对深度学习的兴趣，柳猫就觉得很开心了。"}
{"content2":"不多说，直接上干货！\n为了进一步打造提升（大数据躺过的坑）本微信公众平台的博文高质量水平，特邀请善于分享、主动、敢于专研尝试新技术新领域的您，一起共同维护好我们的知识小天地。目前涉及领域有：大数据领域：Hadoop、Hive、HBase、Zookeeper、Flume、Sqoop、Kafka、Spark、Storm、Zeppelin、Oozie、Azkaban、Flink等。编程语言和脚本语言：Java、Scala、Python、Shell等。机器学习领域：分类、聚类、推荐、回归、优化、降维、神经网络、深度学习等。以及还有人工智能、自然语言处理... ...\n欢迎您的加入！\n打开百度App，扫码，精彩文章每天更新！欢迎关注我的百家号： 九月哥快讯\n大数据躺过的坑  （总群）：   161156071\nhadoop开发   ：       276519852\n大数据零基础入门 ：  416348910\nspark零基础入门： 285025652\nhadoop零基础入门：  541092360\n大数据开发 ：  207591869\n大数据手把手交流 ：  201590535\n大数据零基础收徒咨询  ：   132603465\n大数据内部收徒咨询群  ：  469185229\n第一步：\nhttps://mp.weixin.qq.com/cgi-bin/loginpage\n第二步：\n然后，进入个人的基本信息进行填写。\n这里大家随便选择一个邮箱。我这里以我的163邮箱进行注册。\n你好! 感谢你注册微信公众平台。 你的登录邮箱为：***@163.com。请回填如下6位验证码： ****\n这里，选择个人，然后进行个人信息的填写。包括个人的身份证号码和照片等详细信息。\n这里，使用自己的微信账号进行扫描其二维码。\n由此，立马，在自己的微信账号里收到：公众号管理员身份确认。 选择我确认并遵从协议。\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。 目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。\n大家，可以对自己的个人微信公众号的头像进行初步修改，当然以后也可以更改\n以后怎么来登录自己的个人微信公众号？\n我这里，因为是采用的是自己的163邮箱和密码，来注册。同时，是加上自己的微信号来作为管理员绑定。\n也许，后期会改名。"}
{"content2":"序言：\n“机器学习”是人工智能的核心研究领域之一， 其最初的研究动机是为了让计算机系统具有人的学习能力以便实现人工智能，因为众所周知，没有学习能力的系统很难被认为是具有智能的。目前被广泛采用的机器学习的定义是“利用经验来改善计算机系统自身的性能”[1]。事实上，由于“经验”在计算机系统中主要是以数据的形式存在的，因此机器学习需要设法对数据进行分析，这就使得它逐渐成为智能数据分析技术的创新源之一，并且为此而受到越来越多的关注。\n“数据挖掘”和“知识发现”通常被相提并论，并在许多场合被认为是可以相互替代的术语。对数据挖掘有多种文字不同但含义接近的定义，例如“识别出巨量数据中有效的、新颖的、潜在有用的、最终可理解的模式的非平凡过程”[2]。其实顾名思义，数据挖掘就是试图从海量数据中找出有用的知识。大体上看，数据挖掘可以视为机器学习和数据库的交叉，它主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。\n因 为 机 器 学  习 和 数 据 挖  掘 有 密 切 的  联系，周志华把它们放在一起做一个粗浅的介绍。\n详细见：机器学习与数据挖掘"}
{"content2":"在做数据处理时，需要用到不同的手法，如特征标准化，主成分分析，等等会重复用到某些参数，sklearn中提供了管道，可以一次性的解决该问题\n先展示先通常的做法\nimport pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression df = pd.read_csv('wdbc.csv') X = df.iloc[:, 2:].values y = df.iloc[:, 1].values # 标准化 sc = StandardScaler() X_train_std = sc.fit_transform(X_train) X_test_std = sc.transform(X_test) # 主成分分析PCA pca = PCA(n_components=2) X_train_pca = pca.fit_transform(X_train_std) X_test_pca = pca.transform(X_test_std) # 逻辑斯蒂回归预测 lr = LogisticRegression(random_state=1) lr.fit(X_train_pca, y_train) y_pred = lr.predict(X_test_pca)\n先对数据标准化，然后做主成分分析降维，最后做回归预测\n现在使用管道\nfrom sklearn.pipeline import Pipeline pipe_lr = Pipeline([('sc', StandardScaler()), ('pca', PCA(n_components=2)), ('lr', LogisticRegression(random_state=1))]) pipe_lr.fit(X_train, y_train) pipe_lr.score(X_test, y_test)\nPipeline对象接收元组构成的列表作为输入，每个元组第一个值作为变量名，元组第二个元素是sklearn中的transformer或Estimator。\n管道中间每一步由sklearn中的transformer构成，最后一步是一个Estimator。我们的例子中，管道包含两个中间步骤，一个StandardScaler和一个PCA，这俩都是transformer，逻辑斯蒂回归分类器是Estimator。\n当管道pipe_lr执行fit方法时，首先StandardScaler执行fit和transform方法，然后将转换后的数据输入给PCA，PCA同样执行fit和transform方法，最后将数据输入给LogisticRegression，训练一个LR模型。\n对于管道来说，中间有多少个transformer都可以。工作方式如下\n使用管道减少了很多代码量\n现在回归模型的评估和调参\n训练机器学习模型的关键一步是要评估模型的泛化能力。如果我们训练好模型后，还是用训练集取评估模型的性能，这显然是不符合逻辑的。一个模型如果性能不好，要么是因为模型过于复杂导致过拟合(高方差)，要么是模型过于简单导致导致欠拟合(高偏差)。可是用什么方法评价模型的性能呢？这就是这一节要解决的问题，你会学习到两种交叉验证计数，holdout交叉验证和k折交叉验证， 来评估模型的泛化能力\n一、holdout交叉验证(评估模型性能)\nholdout方法很简单就是将数据集分为训练集和测试集，前者用于训练，后者用于评估\n如果在模型选择的过程中，我们始终用测试集来评价模型性能，这实际上也将测试集变相地转为了训练集，这时候选择的最优模型很可能是过拟合的。\n更好的holdout方法是将原始训练集分为三部分：训练集、验证集和测试集。训练机用于训练不同的模型，验证集用于模型选择。而测试集由于在训练模型和模型选择这两步都没有用到，对于模型来说是未知数据，因此可以用于评估模型的泛化能力。下图展示了holdout方法的步骤：\n缺点：它对数据分割的方式很敏感，如果原始数据集分割不当，这包括训练集、验证集和测试集的样本数比例，以及分割后数据的分布情况是否和原始数据集分布情况相同等等。所以，不同的分割方式可能得到不同的最优模型参数\n二、K折交叉验证(评估模型性能)\nk折交叉验证的过程，第一步我们使用不重复抽样将原始数据随机分为k份，第二步 k-1份数据用于模型训练，剩下那一份数据用于测试模型。然后重复第二步k次，我们就得到了k个模型和他的评估结果(译者注：为了减小由于数据分割引入的误差，通常k折交叉验证要随机使用不同的划分方法重复p次，常见的有10次10折交叉验证)\n然后我们计算k折交叉验证结果的平均值作为参数/模型的性能评估。使用k折交叉验证来寻找最优参数要比holdout方法更稳定。一旦我们找到最优参数，要使用这组参数在原始数据集上训练模型作为最终的模型。\nk折交叉验证使用不重复采样，优点是每个样本只会在训练集或测试中出现一次，这样得到的模型评估结果有更低的方法。\n下图演示了10折交叉验证：\n10次10折交叉验证我的理解是将按十种划分方法，每次将数据随机分成k分，k-1份训练，k份测试。获取十个模型和评估结果，然后取10次的平均值作为性能评估\nfrom sklearn.model_selection import StratifiedKFold\npipe_lr = Pipeline([('sc', StandardScaler()), ('pca', PCA(n_components=2)), ('lr', LogisticRegression(random_state=1))]) pipe_lr.fit(X_train, y_train) kfold = StratifiedKFold(y=y_train, n_folds=10, random_state=1) scores= [] for k, (train, test) in enumerate(kfold): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(scores) print('Fold: %s, Class dist.: %s, Acc: %.3f' %(k+1, np.bincount(y_train[train]), score))print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n更简单的方法\nfrom sklearn.model_selection import StratifiedKFold pipe_lr = Pipeline([('sc', StandardScaler()), ('pca', PCA(n_components=2)), ('lr', LogisticRegression(random_state=1))]) pipe_lr.fit(X_train, y_train) scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1) print('CV accuracy scores: %s' %scores) print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\ncv即k\n三、学习曲线(调试算法)\nfrom sklearn.model_selection import learning_curve pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(penalty='l2', random_state=0))]) train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=1) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) plt.plot(train_sizes, train_mean, color='blue', marker='0', markersize=5, label='training accuracy') plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue') plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy') plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green') plt.grid() plt.xlabel('Number of training samples') plt.ylabel('Accuracy') plt.legend(loc='lower right') plt.ylim([0.8, 1.0]) plt.show()\nlearning_curve中的train_sizes参数控制产生学习曲线的训练样本的绝对/相对数量，此处，我们设置的train_sizes=np.linspace(0.1, 1.0, 10)，将训练集大小划分为10个相等的区间。learning_curve默认使用分层k折交叉验证计算交叉验证的准确率，我们通过cv设置k。\n上图中可以看到，模型在测试集表现很好，不过训练集和测试集的准确率还是有一段小间隔，可能是模型有点过拟合\n四、验证曲线解决过拟合和欠拟合(调试算法)\n验证曲线和学习曲线很相近，不同的是这里画出的是不同参数下模型的准确率而不是不同训练集大小下的准确率\nfrom sklearn.model_selection import validation_curve param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(penalty='l2', random_state=0))]) train_scores, test_scores = validation_curve(estimator=pipe_lr, X=X_train, y=y_train, param_name='clf__C', param_range=param_range, cv=10) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) plt.plot(param_range, train_mean, color='blue', marker='o', markersize=5, label='training accuracy') plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue') plt.plot(param_range, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy') plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green') plt.grid() plt.xscale('log') plt.xlabel('Parameter C') plt.ylabel('Accuracy') plt.legend(loc='lower right') plt.ylim([0.8, 1.0]) plt.show()\n我们得到了参数C的验证曲线。\n和learning_curve方法很像，validation_curve方法使用采样k折交叉验证来评估模型的性能。在validation_curve内部，我们设定了用来评估的参数，这里是C,也就是LR的正则系数的倒数。\n观察上图，最好的C值是0.1。\n总之，我们可以使用学习曲线判断算法是否拟合程度(欠拟合或者过拟合)，然后使用验证曲线评估参数获取最好的参数\n机器学习算法中有两类参数：从训练集中学习到的参数，比如逻辑斯蒂回归中的权重参数，另一类是模型的超参数，也就是需要人工设定的参数，比如正则项系数或者决策树的深度。\n权重参数可以通过验证曲线来获取最好的参数，而超参数则可以使用网格搜索调参\n五、网格搜索调参(调试算法)\n网格搜索其实就是暴力搜索，事先为每个参数设定一组值，然后穷举各种参数组合，找到最好的那组\nfrom sklearn.model_selection import GridSearchCV from sklearn.svm import SVC pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))]) param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] param_grid = [{'clf__C': param_range, 'clf__kernel': ['linear']}, {'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']}] gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1) gs = gs.fit(X_train, y_train) print(gs.best_score_) print(gs.best_params_)\nGridSearchCV中param_grid参数是字典构成的列表。对于线性SVM，我们只评估参数C；对于RBF核SVM，我们评估C和gamma。\n最后， 我们通过best_parmas_得到最优参数组合。\nsklearn人性化的一点是，我们可以直接利用最优参数建模(best_estimator_)\nclf = gs.best_estimator_ clf.fit(X_train, y_train) print('Test accuracy: %.3f' %clf.score(X_test, y_test))\n网格搜索虽然不错，但是穷举过于耗时，sklearn中还实现了随机搜索，使用 RandomizedSearchCV类，随机采样出不同的参数组合\n六、嵌套交叉验证(选择算法)\n结合k折交叉验证和网格搜索是调参的好手段。可是如果我们想从茫茫算法中选择最合适的算法，用什么方法呢？这就是下面要介绍的嵌套交叉验证\n嵌套交叉验证外层有一个k折交叉验证将数据分为训练集和测试集。还有一个内部交叉验证用于选择模型算法。下图演示了一个5折外层交叉沿则和2折内部交叉验证组成的嵌套交叉验证，也被称为5*2交叉验证\nsklearn中如下使用嵌套交叉验证\nsvc的精确度\ngs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1) scores = cross_val_score(gs, X, y, scoring='accuracy', cv=5) print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n决策树分类器精确度\ngs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=[{'max_depth': [1,2,3,4,5,6,7, None]}], scoring='accuracy', cv=5) scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5) print('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n比较下两者的精确度，即可知道那种算法更加合适\n七、混淆矩阵(性能评价指标)\n除了准确率，还有不少评价指标，如查准率，查全率，F1值等\n混淆矩阵(confusion matrix), 能够展示学习算法表现的矩阵。混淆矩阵是一个平方矩阵，其中记录了一个分类器的TP(true positive)、TN(true negative)、FP(false positive)和FN(false negative):\nfrom sklearn import metrics metrics.calinski_harabaz_score(input, y_pred)\npython实现混淆矩阵\n其实就是一个2*2的矩阵\nTP实际为真，判断成功(判断为真)的个数\nFN实际为真，判断错误(判断为假)\nFP实际为假，判断错误(判断为真)\nTN实际为假，判断成功(判断为假)\n摘自https://www.gitbook.com/book/ljalphabeta/python-"}
{"content2":"K-Means聚类算法是最为经典的，同时也是使用最为广泛的一种基于划分的聚类算法，它属于基于距离的无监督聚类算法。KMeans算法简单实用，在机器学习算法中占有重要的地位。对于KMeans算法而言，如何确定K值，确实让人头疼的事情。\n最近这几天一直忙于构建公司的推荐引擎。对用户群体的分类，要使用KMeans聚类算法，就研究了一下。\n探索K的选择\n对数据进行分析之前，采用一些探索性分析手段还是很有必要的。\n对于高维空间，我们可以采用降维的方式，把多维向量转化为二维向量。好在，R语言包里提供了具体的实现，MDS是个比较好的方式。\n多维标度分析(MDS)是一种将多维空间的研究对象简化到低维空间进行定位、分析和归类，同时又保留对象间原始关系的数据分析方法。R语言包提供了经典MDS和非度量MDS。\n通过MDS对数据进行处理后，采用ggplot绘出点图，看看数据分布的情况，使得我们对要聚类的数据有个直观的认识。\nSSE和Silhouette Coefficient系数\n我们还可以通过SSE和Silhouette Coefficient系数的方法评估最优K。譬如对K从1到15计算不同的聚类的SSE，由于kmeans算法中的随机因数，每次结果都不一样，为了减少时间结果的偶然性，对于每个k值，都重复运行50次，求出平均的SSE，最后绘制出SSE曲线。Silhouette Coefficient也采用同样做法。\nSSE结果\nSilhouette Coefficient结果\n从上图来看，8和9明显有一个尖峰。我们大体可以确定K的数目是8。值得注意在有些时候，这种方法有可能无效，但仍然不失为一个很好的方法。\nDB INDEX准则\nDB INdex准则全称Davies Bouldin index 。类内离散度和类间聚类常被用来判断聚类的有效性，DB INdex准则同时使用了类间聚类和类内离散度。通过计算这个指数，来确定到底哪个Cluster最合理\nR语言代码如下：\n1 data <- read.csv(\"a.csv\", header = T, 2 3 stringsAsFactors = F) 4 DB_index <- function(x, cl, k) { 5 data <- split.data.frame(x, cl$cluster) 6 # 计算类内离散度 7 8 S <- NULL 9 for (i in 1:k) { 10 S[i] <- sum(rowSums((data[[i]] - cl$centers[i])^2))/nrow(data[[i]]) 11 } 12 13 # 计算类间聚类 14 15 D <- as.matrix(dist(cl$centers)) 16 17 # 计算DB index 18 19 R <- NULL 20 for (i in 1:k) { 21 R <- c(max((S[i] + S[-i])/D[-i, i]), R) 22 } 23 DB <- sum(R)/k 24 return(DB) 25 } 26 27 # 循环计算不同聚类数的DB_Index指数 28 29 DB <- NULL 30 for (i in 2:15) { 31 32 cl <- kmeans(data, i) 33 34 DB <- c(DB_index(data, cl, i), DB) 35 36 } 37 plot(2:15, DB) 38 lines(2:15, DB)\nCANOPY算法\nCanopy聚类最大的特点是不需要事先指定k值(即clustering的个数)，与其他聚类算法相比，Canopy聚类虽然精度较低，但其在速度上有很大优势。\n因此可以使用Canopy聚类先对数据进行“粗”聚类，得到k值后再使用K-means进行进一步“细”聚类。这个算法不多说了，mahout聚类里有具体实现。\n参阅：https://en.wikipedia.org/wiki/Davies-Bouldin_index"}
{"content2":"看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？\n我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。\n可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n(1)以下是不完整的列表，但基本覆盖。\n机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）\n计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）\n人工智能：IJCAI, AAAI; （期刊AI）\n另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。\n特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。\n(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/;  JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。\n(3)说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。\n对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。\n注:\nNIPS = Neural Information Processing Systems  https://nips.cc/\nICML = International Conference on Machine Learning https://icml.cc\nUAI(AUAI) =Association for Uncertainty in Artifical Intelligence http://www.auai.org/\nAISTATS = Artificial Intelligence and Statistics http://www.aistats.org/\nJMLR = Journal of Machine Learning Research http://jmlr.org/\nIJCAI = International Joint Conference on Artifical Intelligence http://ijcai.org/\nAAAI = Association for the Advancement of Aritifical Intelligence http://www.aaai.org/home.html\n原文地址:\nhttp://emuch.net/html/201012/2659795.html"}
{"content2":"转自@王萌，有少许修改。\n机器学习起源于人工智能，可以赋予计算机以传统编程所无法实现的能力，比如飞行器的自动驾驶、人脸识别、计算机视觉和数据挖掘等。\n机器学习的算法很多。很多时候困惑人们的是，很多算法是一类算法，而有些算法又是从其他算法中延伸出来的。这里，我们从两个方面来给大家介绍，第一个方面是学习的方式，第二个方面是算法的类似性。\n学习方式\n将算法按照学习方式分类可以让人们在建模和算法选择的时候考虑能根据输入数据来选择最合适的算法来获得最好的结果。\n监督学习\n在监督学习中，输入数据被称为“训练数据”，每组训练数据有一个明确的类标。在建立预测模型的时候，监督学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。\n监督式学习的常见应用场景如分类问题和回归问题。常见算法有Linear Regression，Logistic Regression，Neural Network，SVMs。\n非监督学习\n在非监督学习中，数据并未被特别标识，学习模型是为了推断出数据的一些内在结构。\n常见的应用场景包括关联规则的学习以及聚类等。常见算法包括K-means Clustering ，Principal Component Analysis和Anomaly Detection。\n半监督学习\n在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。\n强化学习\n在强化学习（Reinforcement Learning）中，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式。在强化学习中，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。\n在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。\n算法类似性\n回归算法\n回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。\n基于核的算法\n基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA)等.\n聚类算法\n聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。\n降维算法\n像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis， PCA），偏最小二乘回归（Partial Least Square Regression，PLS）。\n集成学习算法\n集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，随机森林（Random Forest）等。（竞赛中多用到该类算法，效果较好）\n基于实例的算法\n基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM）.\n决策树学习\n决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3 (Iterative Dichotomiser 3)， C4.5， 随机森林（Random Forest）等。\n贝叶斯方法\n贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，以及Bayesian Belief Network（BBN）。\n人工神经网络\n人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。它是机器学习的一个庞大的分支。重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。（现在的深度学习就是由人工神经网络发展而来）\n深度学习\n常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。（发展至目前，最成功的当属CNN和LSTM。）"}
{"content2":"本文汇编了一些机器学习领域的框架、库以及软件（按编程语言排序）。\n1. C++\n1.1 计算机视觉\nCCV —基于C语言/提供缓存/核心的机器视觉库，新颖的机器视觉库\nOpenCV—它提供C++, C, Python, Java 以及 MATLAB接口，并支持Windows, Linux, Android and Mac OS操作系统。\n1.2 机器学习\nMLPack\nDLib\necogg\nshark\n2. Closure\nClosure Toolbox—Clojure语言库与工具的分类目录\n3.Go\n3.1 自然语言处理\ngo-porterstemmer—一个Porter词干提取算法的原生Go语言净室实现\npaicehusk—Paice/Husk词干提取算法的Go语言实现\nsnowball—Go语言版的Snowball词干提取器\n3.2 机器学习\nGo Learn— Go语言机器学习库\ngo-pr —Go语言机器学习包.\nbayesian—Go语言朴素贝叶斯分类库。\ngo-galib—Go语言遗传算法库。\n3.3 数据分析/数据可视化\ngo-graph—Go语言图形库。\nSVGo—Go语言的SVG生成库。\n4. Java\n4.1 自然语言处理\nCoreNLP—斯坦福大学的CoreNLP提供一系列的自然语言处理工具，输入原始英语文本，可以给出单词的基本形式（下面Stanford开头的几个工具都包含其中）。\nStanford Parser—一个自然语言解析器。\nStanford POS Tagger —一个词性分类器。\nStanford Name Entity Recognizer—Java实现的名称识别器\nStanford Word Segmenter—分词器，很多NLP工作中都要用到的标准预处理步骤。\nTregex, Tsurgeon and Semgrex —用来在树状数据结构中进行模式匹配，基于树关系以及节点匹配的正则表达式（名字是“tree regular expressions”的缩写）。\nStanford Phrasal:最新的基于统计短语的机器翻译系统，java编写\nStanford Tokens Regex—用以定义文本模式的框架。\nStanford Temporal Tagger—SUTime是一个识别并标准化时间表达式的库。\nStanford SPIED—在种子集上使用模式，以迭代方式从无标签文本中学习字符实体\nStanford Topic Modeling Toolbox —为社会科学家及其他希望分析数据集的人员提供的主题建模工具。\nTwitter Text Java—Java实现的推特文本处理库\nMALLET -—基于Java的统计自然语言处理、文档分类、聚类、主题建模、信息提取以及其他机器学习文本应用包。\nOpenNLP—处理自然语言文本的机器学习工具包。\nLingPipe —使用计算机语言学处理文本的工具包。\n4.2 机器学习\nMLlib in Apache Spark—Spark中的分布式机器学习程序库\nMahout —分布式的机器学习库\nStanford Classifier —斯坦福大学的分类器\nWeka—Weka是数据挖掘方面的机器学习算法集。\nORYX—提供一个简单的大规模实时机器学习/预测分析基础架构。\n4.3 数据分析/数据可视化\nHadoop—大数据分析平台\nSpark—快速通用的大规模数据处理引擎。\nImpala —为Hadoop实现实时查询\n5. Javascript\n5.1 自然语言处理\nTwitter-text-js —JavaScript实现的推特文本处理库\nNLP.js —javascript及coffeescript编写的NLP工具\nnatural—Node下的通用NLP工具\nKnwl.js—JS编写的自然语言处理器\n5.2 数据分析/数据可视化\nD3.js\nHigh Charts\nNVD3.js\ndc.js\nchartjs\ndimple\namCharts\n5.3 机器学习\nConvnet.js—训练深度学习模型的JavaScript库。\nClustering.js—用JavaScript实现的聚类算法，供Node.js及浏览器使用。\nDecision Trees—Node.js实现的决策树，使用ID3算法。\nNode-fann —Node.js下的快速人工神经网络库。\nKmeans.js—k-means算法的简单Javascript实现，供Node.js及浏览器使用。\nLDA.js —供Node.js用的LDA主题建模工具。\nLearning.js—逻辑回归/c4.5决策树的JavaScript实现\nMachine Learning—Node.js的机器学习库。\nNode-SVM—Node.js的支持向量机\nBrain —JavaScript实现的神经网络\nBayesian-Bandit —贝叶斯强盗算法的实现，供Node.js及浏览器使用。\n6. Julia\n6.1 机器学习\nPGM—Julia实现的概率图模型框架。\nDA—Julia实现的正则化判别分析包。\nRegression—回归分析算法包（如线性回归和逻辑回归）。\nLocal Regression —局部回归，非常平滑！\nNaive Bayes —朴素贝叶斯的简单Julia实现\nMixed Models —（统计）混合效应模型的Julia包\nSimple MCMC —Julia实现的基本mcmc采样器\nDistance—Julia实现的距离评估模块\nDecision Tree —决策树分类器及回归分析器\nNeural —Julia实现的神经网络\nMCMC —Julia下的MCMC工具\nGLM —Julia写的广义线性模型包\nOnline Learning\nGLMNet —GMLNet的Julia包装版，适合套索/弹性网模型。\nClustering—数据聚类的基本函数：k-means, dp-means等。\nSVM—Julia下的支持向量机。\nKernal Density—Julia下的核密度估计器\nDimensionality Reduction—降维算法\nNMF —Julia下的非负矩阵分解包\nANN—Julia实现的神经网络\n6.2 自然语言处理\nTopic Models —Julia下的主题建模\nText Analysis—Julia下的文本分析包\n6.3 数据分析/数据可视化\nGraph Layout —纯Julia实现的图布局算法。\nData Frames Meta —DataFrames的元编程工具。\nJulia Data—处理表格数据的Julia库\nData Read—从Stata、SAS、SPSS读取文件\nHypothesis Tests—Julia中的假设检验包\nGladfly —Julia编写的灵巧的统计绘图系统。\nStats—Julia编写的统计测试函数包\nRDataSets —读取R语言中众多可用的数据集的Julia函数包。\nDataFrames —处理表格数据的Julia库。\nDistributions—概率分布及相关函数的Julia包。\nData Arrays —元素值可以为空的数据结构。\nTime Series—Julia的时间序列数据工具包。\nSampling—Julia的基本采样算法包\n6.4 杂项/演示文稿\nDSP —数字信号处理\nJuliaCon Presentations—Julia大会上的演示文稿\nSignalProcessing—Julia的信号处理工具\nImages—Julia的图片库\n7. Lua\n7.1 机器学习\nTorch7\ncephes —Cephes数学函数库，包装成Torch可用形式。提供并包装了超过180个特殊的数学函数，由Stephen L. Moshier开发，是SciPy的核心，应用于很多场合。\ngraph —供Torch使用的图形包。\nrandomkit—从Numpy提取的随机数生成包，包装成Torch可用形式。\nsignal —Torch-7可用的信号处理工具包，可进行FFT, DCT, Hilbert, cepstrums, stft等变换。\nnn —Torch可用的神经网络包。\nnngraph —为nn库提供图形计算能力。\nnnx—一个不稳定实验性的包，扩展Torch内置的nn库。\noptim—Torch可用的优化算法库，包括 SGD, Adagrad, 共轭梯度算法, LBFGS, RProp等算法。\nunsup—Torch下的非监督学习包。提供的模块与nn(LinearPsd, ConvPsd, AutoEncoder, …)及独立算法 (k-means, PCA)等兼容。\nmanifold—操作流形的包。\nsvm—Torch的支持向量机库。\nlbfgs—将liblbfgs包装为FFI接口。\nvowpalwabbit —老版的vowpalwabbit对torch的接口。\nOpenGM—OpenGM是C++编写的图形建模及推断库，该binding可以用Lua以简单的方式描述图形，然后用OpenGM优化。\nsphagetti —MichaelMathieu为torch7编写的稀疏线性模块。\nLuaSHKit —将局部敏感哈希库SHKit包装成lua可用形式。\nkernel smoothing —KNN、核权平均以及局部线性回归平滑器\ncutorch—torch的CUDA后端实现\ncunn —torch的CUDA神经网络实现。\nimgraph—torch的图像/图形库，提供从图像创建图形、分割、建立树、又转化回图像的例程\nvideograph—torch的视频/图形库，提供从视频创建图形、分割、建立树、又转化回视频的例程\nsaliency —积分图像的代码和工具，用来从快速积分直方图中寻找兴趣点。\nstitch —使用hugin拼合图像并将其生成视频序列。\nsfm—运动场景束调整/结构包\nfex —torch的特征提取包，提供SIFT和dSIFT模块。\nOverFeat—当前最高水准的通用密度特征提取器。\nNumeric Lua\nLunatic Python\nSciLua\nLua – Numerical Algorithms\nLunum\n7.2 演示及脚本\nCore torch7 demos repository.核心torch7演示程序库\n线性回归、逻辑回归\n人脸检测（训练和检测是独立的演示）\n基于mst的断词器\ntrain-a-digit-classifier\ntrain-autoencoder\noptical flow demo\ntrain-on-housenumbers\ntrain-on-cifar\ntracking with deep nets\nkinect demo\n滤波可视化\nsaliency-networks\nTraining a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)\nMusic Tagging—torch7下的音乐标签脚本\ntorch-datasets 读取几个流行的数据集的脚本，包括：\nBSR 500\nCIFAR-10\nCOIL\nStreet View House Numbers\nMNIST\nNORB\nAtari2600 —在Arcade Learning Environment模拟器中用静态帧生成数据集的脚本。\n8. Matlab\n8.1 计算机视觉\nContourlets —实现轮廓波变换及其使用函数的MATLAB源代码\nShearlets—剪切波变换的MATLAB源码\nCurvelets—Curvelet变换的MATLAB源码（Curvelet变换是对小波变换向更高维的推广，用来在不同尺度角度表示图像。）\nBandlets—Bandlets变换的MATLAB源码\n8.2 自然语言处理\nNLP —一个Matlab的NLP库\n8.3 机器学习\nTraining a deep autoencoder or a classifier on MNIST digits—在MNIST字符数据集上训练一个深度的autoencoder或分类器[深度学习]。\nt-Distributed Stochastic Neighbor Embedding —获奖的降维技术，特别适合于高维数据集的可视化\nSpider—Matlab机器学习的完整面向对象环境。\nLibSVM —支持向量机程序库\nLibLinear —大型线性分类程序库\nMachine Learning Module —M. A .Girolami教授的机器学习课程，包括PDF，讲义及代码。\nCaffe—考虑了代码清洁、可读性及速度的深度学习框架\nPattern Recognition Toolbox —Matlab中的模式识别工具包，完全面向对象\n8.4 数据分析/数据可视化\nmatlab_gbl—处理图像的Matlab包\ngamic—图像算法纯Matlab高效实现，对MatlabBGL的mex函数是个补充。\n9. .NET\n9.1 计算机视觉\nOpenCVDotNet —包装器，使.NET程序能使用OpenCV代码\nEmgu CV—跨平台的包装器，能在Windows, Linus, Mac OS X, iOS, 和Android上编译。\n9.2 自然语言处理\nStanford.NLP for .NET —斯坦福大学NLP包在.NET上的完全移植，还可作为NuGet包进行预编译。\n9.3 通用机器学习\nAccord.MachineLearning —支持向量机、决策树、朴素贝叶斯模型、K-means、高斯混合模型和机器学习应用的通用算法，例如：随机抽样一致性算法、交叉验证、网格搜索。这个包是Accord.NET框架的一部分。\nVulpes—F#语言实现的Deep belief和深度学习包，它在Alea.cuBase下利用CUDA GPU来执行。\nEncog —先进的神经网络和机器学习框架，包括用来创建多种网络的类，也支持神经网络需要的数据规则化及处理的类。它的训练采用多线程弹性传播。它也能使用GPU加快处理时间。提供了图形化界面来帮助建模和训练神经网络。\nNeural Network Designer —这是一个数据库管理系统和神经网络设计器。设计器用WPF开发，也是一个UI，你可以设计你的神经网络、查询网络、创建并配置聊天机器人，它能问问题，并从你的反馈中学习。这些机器人甚至可以从网络搜集信息用来输出，或是用来学习。\n9.4 数据分析/数据可视化\nnuml —numl这个机器学习库，目标就是简化预测和聚类的标准建模技术。\nMath.NET Numerics—Math.NET项目的数值计算基础，着眼提供科学、工程以及日常数值计算的方法和算法。支持 Windows, Linux 和 Mac上的 .Net 4.0, .Net 3.5 和 Mono ，Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 以及装有 PCL Portable Profiles 47 及 344的Windows 8， 装有 Xamarin的Android/iOS 。\nSho —Sho是数据分析和科学计算的交互式环境，可以让你将脚本（IronPython语言）和编译的代码（.NET）无缝连接，以快速灵活的建立原型。这个环境包括强大高效的库，如线性代数、数据可视化，可供任何.NET语言使用，还为快速开发提供了功能丰富的交互式shell。\n10. Python\n10.1 计算机视觉\nSimpleCV—开源的计算机视觉框架，可以访问如OpenCV等高性能计算机视觉库。使用Python编写，可以在Mac、Windows以及Ubuntu上运行。\n10.2 自然语言处理\nNLTK —一个领先的平台，用来编写处理人类语言数据的Python程序\nPattern—Python可用的web挖掘模块，包括自然语言处理、机器学习等工具。\nTextBlob—为普通自然语言处理任务提供一致的API，以NLTK和Pattern为基础，并和两者都能很好兼容。\njieba—中文断词工具。\nSnowNLP —中文文本处理库。\nloso—另一个中文断词库。\ngenius —基于条件随机域的中文断词库。\nnut —自然语言理解工具包。\n10.3 机器学习\nBayesian Methods for Hackers —Python语言概率规划的电子书\nMLlib in Apache Spark—Spark下的分布式机器学习库。\nscikit-learn—基于SciPy的机器学习模块\ngraphlab-create —包含多种机器学习模块的库（回归，聚类，推荐系统，图分析等），基于可以磁盘存储的DataFrame。\nBigML—连接外部服务器的库。\npattern—Python的web挖掘模块\nNuPIC—Numenta公司的智能计算平台。\nPylearn2—基于Theano的机器学习库。\nhebel —Python编写的使用GPU加速的深度学习库。\ngensim—主题建模工具。\nPyBrain—另一个机器学习库。\nCrab —可扩展的、快速推荐引擎。\npython-recsys —Python实现的推荐系统。\nthinking bayes—关于贝叶斯分析的书籍\nRestricted Boltzmann Machines —Python实现的受限波尔兹曼机。[深度学习]。\nBolt —在线学习工具箱。\nCoverTree —cover tree的Python实现，scipy.spatial.kdtree便捷的替代。\nnilearn—Python实现的神经影像学机器学习库。\nShogun—机器学习工具箱。\nPyevolve —遗传算法框架。\nCaffe —考虑了代码清洁、可读性及速度的深度学习框架\nbreze—深度及递归神经网络的程序库，基于Theano。\n10.4 数据分析/数据可视化\nSciPy —基于Python的数学、科学、工程开源软件生态系统。\nNumPy—Python科学计算基础包。\nNumba —Python的低级虚拟机JIT编译器，Cython and NumPy的开发者编写，供科学计算使用\nNetworkX —为复杂网络使用的高效软件。\nPandas—这个库提供了高性能、易用的数据结构及数据分析工具。\nOpen Mining—Python中的商业智能工具（Pandas web接口）。\nPyMC —MCMC采样工具包。\nzipline—Python的算法交易库。\nPyDy—全名Python Dynamics，协助基于NumPy, SciPy, IPython以及 matplotlib的动态建模工作流。\nSymPy —符号数学Python库。\nstatsmodels—Python的统计建模及计量经济学库。\nastropy —Python天文学程序库，社区协作编写\nmatplotlib —Python的2D绘图库。\nbokeh—Python的交互式Web绘图库。\nplotly —Python and matplotlib的协作web绘图库。\nvincent—将Python数据结构转换为Vega可视化语法。\nd3py—Python的绘图库，基于D3.js。\nggplot —和R语言里的ggplot2提供同样的API。\nKartograph.py—Python中渲染SVG图的库，效果漂亮。\npygal—Python下的SVG图表生成器。\npycascading\n10.5 杂项脚本/iPython笔记/代码库\npattern_classification\nthinking stats 2\nhyperopt\nnumpic\n2012-paper-diginorm\nipython-notebooks\ndecision-weights\nSarah Palin LDA —Sarah Palin关于主题建模的电邮。\nDiffusion Segmentation —基于扩散方法的图像分割算法集合。\nScipy Tutorials —SciPy教程，已过时，请查看scipy-lecture-notes\nCrab—Python的推荐引擎库。\nBayesPy—Python中的贝叶斯推断工具。\nscikit-learn tutorials—scikit-learn学习笔记系列\nsentiment-analyzer —推特情绪分析器\ngroup-lasso—坐标下降算法实验，应用于（稀疏）群套索模型。\nmne-python-notebooks—使用 mne-python进行EEG/MEG数据处理的IPython笔记\npandas cookbook—使用Python pandas库的方法书。\nclimin—机器学习的优化程序库，用Python实现了梯度下降、LBFGS、rmsprop、adadelta 等算法。\n10.6 Kaggle竞赛源代码\nwiki challange —Kaggle上一个维基预测挑战赛 Dell Zhang解法的实现。\nkaggle insults—Kaggle上”从社交媒体评论中检测辱骂“竞赛提交的代码\nkaggle_acquire-valued-shoppers-challenge—Kaggle预测回头客挑战赛的代码\nkaggle-cifar —Kaggle上CIFAR-10 竞赛的代码，使用cuda-convnet\nkaggle-blackbox —Kaggle上blackbox赛代码，关于深度学习。\nkaggle-accelerometer —Kaggle上加速度计数据识别用户竞赛的代码\nkaggle-advertised-salaries —Kaggle上用广告预测工资竞赛的代码\nkaggle amazon —Kaggle上给定员工角色预测其访问需求竞赛的代码\nkaggle-bestbuy_big—Kaggle上根据bestbuy用户查询预测点击商品竞赛的代码（大数据版）\nkaggle-bestbuy_small—Kaggle上根据bestbuy用户查询预测点击商品竞赛的代码（小数据版）\nKaggle Dogs vs. Cats —Kaggle上从图片中识别猫和狗竞赛的代码\nKaggle Galaxy Challenge —Kaggle上遥远星系形态分类竞赛的优胜代码\nKaggle Gender —Kaggle竞赛：从笔迹区分性别\nKaggle Merck—Kaggle上预测药物分子活性竞赛的代码（默克制药赞助）\nKaggle Stackoverflow—Kaggle上 预测Stack Overflow网站问题是否会被关闭竞赛的代码\nwine-quality —预测红酒质量。\n11. Ruby\n11.1 自然语言处理\nTreat—文本检索与注释工具包，Ruby上我见过的最全面的工具包。\nRuby Linguistics—这个框架可以用任何语言为Ruby对象构建语言学工具。包括一个语言无关的通用前端，一个将语言代码映射到语言名的模块，和一个含有很有英文语言工具的模块。\nStemmer—使得Ruby可用 libstemmer_c中的接口。\nRuby Wordnet —WordNet的Ruby接口库。\nRaspel —aspell绑定到Ruby的接口\nUEA Stemmer—UEALite Stemmer的Ruby移植版，供搜索和检索用的保守的词干分析器\nTwitter-text-rb—该程序库可以将推特中的用户名、列表和话题标签自动连接并提取出来。\n11.2 机器学习\nRuby Machine Learning —Ruby实现的一些机器学习算法。\nMachine Learning Ruby\njRuby Mahout —精华！在JRuby世界中释放了Apache Mahout的威力。\nCardMagic-Classifier—可用贝叶斯及其他分类法的通用分类器模块。\nNeural Networks and Deep Learning—《神经网络和深度学习》一书的示例代码。\n11.3 数据分析/数据可视化\nrsruby – Ruby – R bridge\ndata-visualization-ruby—关于数据可视化的Ruby Manor演示的源代码和支持内容\nruby-plot —将gnuplot包装为Ruby形式，特别适合将ROC曲线转化为svg文件。\nplot-rb—基于Vega和D3的ruby绘图库\nscruffy —Ruby下出色的图形工具包\nSciRuby\nGlean—数据管理工具\nBioruby\nArel\n12. R\n12.1 通用机器学习\nClever Algorithms For Machine Learning\nMachine Learning For Hackers\nMachine Learning Task View on CRAN—R语言机器学习包列表，按算法类型分组。\ncaret—R语言150个机器学习算法的统一接口\nSuperLearner and subsemble—该包集合了多种机器学习算法\nIntroduction to Statistical Learning\n12.2 数据分析/数据可视化\nLearning Statistics Using R\nggplot2—基于图形语法的数据可视化包。\n13. Scala\n13.1 自然语言处理\nScalaNLP—机器学习和数值计算库的套装\nBreeze —Scala用的数值处理库\nChalk—自然语言处理库。\nFACTORIE—可部署的概率建模工具包，用Scala实现的软件库。为用户提供简洁的语言来创建关系因素图，评估参数并进行推断。\n13.2 数据分析/数据可视化\nMLlib in Apache Spark—Spark下的分布式机器学习库\nScalding —CAscading的Scala接口\nSumming Bird—用Scalding 和 Storm进行Streaming MapReduce\nAlgebird —Scala的抽象代数工具\nxerial —Scala的数据管理工具\nsimmer —化简你的数据，进行代数聚合的unix过滤器\nPredictionIO —供软件开发者和数据工程师用的机器学习服务器。\nBIDMat—支持大规模探索性数据分析的CPU和GPU加速矩阵库。\n13.3 机器学习\nConjecture—Scalding下可扩展的机器学习框架\nbrushfire—scalding下的决策树工具\nganitha —基于scalding的机器学习程序库\nadam—使用Apache Avro, Apache Spark 和 Parquet的基因组处理引擎，有专用的文件格式，Apache 2软件许可。\nbioscala —Scala语言可用的生物信息学程序库\nBIDMach—机器学习CPU和GPU加速库。"}
{"content2":"#对coursera上Andrew Ng老师开的机器学习课程的笔记和心得；\n#注:此笔记是我自己认为本节课里比较重要、难理解或容易忘记的内容并做了些补充，并非是课堂详细笔记和要点；\n#标记为<补充>的是我自己加的内容而非课堂内容，参考文献列于文末。博主能力有限，若有错误，恳请指正；\n#---------------------------------------------------------------------------------#\n#下面这个概念对理解机器学习非常有帮助，但是我发现很多小伙伴不了解这个;\n<补充>机器学习三要素-模型(model)、策略(strategy)、算法(algorithm)；\n模型就是所要学习条件概率分布或决策函数，我们常见的一些方法，像隐马模型(HMM)、SVM模型、决策树模型等等都归于此类；\n策略是指按照什么样的准则来学习或者挑选模型，像课上讲的J(Θ)、损失函数属于此类；\n这里的算法是指学习模型的具体计算方法，即用什么样的方法来求得最优解，像课上讲的梯度下降法，其他如牛顿法、拟牛顿法属于此类；\n#---------------------------------------------------------------------------------#\n#回到课堂上讲的。。。\n当一个方法的预测结果明显有问题时，可采用如下方法:\n1，Get more examples ：helps to fix high variance，Not good if you have high bias；\n2，Smaller set of features: fixes high variance (overfitting)，not good if you have high bias;\n3，Try adding additional features: fixes high bias (because hypothesis is too simple, make hypothesis more specific)\n;\n4，Add polynomial terms: fixes high bias problem;\n5，Decreasing λ : fixes high bias;\n6，Increases λ: fixes high variance;\n#---------------------------------------------------------------------------------#\n模型评估与模型选择\n<补充>用训练集来训练模型，验证集用于模型的选择，测试集用于最终对学习方法的评估；\n<补充>用训练误差和测试误差来评估学习方法:\n训练误差对判断给定的问题是否容易学习是有意义的，但本质上不重要；\n测试误差反映了学习方法对未知数据的预测能力，比较两种学习方法的好坏，不考虑计算速度、空间等因素，测试误差小的方法显然更好；\n#---------------------------------------------------------------------------------#\n诊断: bias vs. variance\nx = degree of polynomial d;\ny = error for both training and cross validation (two lines);\nif d is too small --> this probably corresponds to a high bias problem\nif d is too large --> this probably corresponds to a high variance problem\nFor the high bias case, we find both cross validation and training error are high\nDoesn't fit training data well\nDoesn't generalize either\nFor high variance, we find the cross validation error is high but training error is low\nSo we suffer from overfitting (training is low, cross validation is high)\ni.e. training set fits well\nBut generalizes poorly\n#---------------------------------------------------------------------------------#\n学习曲线(learning curve)\n学习曲线可以通过判断模型High bias还是High variance来提高性能；\n，\nsuffering from high bias：需要增加模型复杂度，增加数据无效！\n，\nsuffering from high variance：增加数据有效！也可尝试增加正则项；\n#---------------------------------------------------------------------------------#\n学习器的几个评价指标:\n精确率(precision)\n= true positives / # predicted positive\n= true positives / (true positive + false positive)；\n召回率(recall)\n= true positives / # actual positives\n= true positive / (true positive + false negative)；\nF1值\n= 2 * (PR/ [P + R])，If P = 0 or R = 0 the Fscore = 0；\n精确率与召回率都高，F1值也会高；\n准确率(accuracy)\n= (true positives + true negative)/ # total dataset\n= (true positives + true negative)/ (true positive + true negative + false positive + false negative)；\n#---------------------------------------------------------------------------------#\n平衡(trade off)精确率和召回率：很多时候我们需要平衡精确率和召回率；\n例子：\nTrained a logistic regression classifier\nPredict 1 if hθ(x) >= 0.5\nPredict 0 if hθ(x) < 0.5\n调整阈值对精确率和召回率的影响见下图：\n#---------------------------------------------------------------------------------#\n参考文献:\n《统计学习方法》，李航著；\n《machine learning》, by Tom Mitchell；\ncouresra课程: standford machine learning, by Andrew Ng；"}
{"content2":"唠嗑唠嗑\n依旧是每一次随便讲两句生活小事。表示最近有点懒，可能是快要考试的原因，外加这两天都有笔试和各种面试，让心情变得没那么安静的敲代码，没那么安静的学习算法。搞得第一次和技术总监聊天的时候都不太懂装饰器这个东东，甚至不知道函数式编程是啥；昨天跟另外一个经理聊天的时候也是没能把自己学习的算法很好的表达出来，真是饱暖思**啊。额，好像用词不当，反正就是人的脑袋除了想着吃肉还要多运动运动，幸好的是每天晚上的瑜伽能够让自己足够沉下心来冷静冷静。回想起当初的各种面试，现在的自己毫无疑问能够很好的表达那些问题，但是很多时候贵在反应速度，所以自己虽然反应不迟钝，但是回答的不完美也是平时没有反复巩固知识的结果，不扯了，写笔记咯。\n接着上一篇文章python机器学习《入门》\n正文：\n在前面的入门文章中主要介绍了机器学习任务重的两个算法：监督学习和非监督学习。其中，在监督学习中最重要的两个东西分别是回归和分类预测。这里，我们主要讲回归预测。上一节中，估计很多人看到一大堆文字描述就很头疼，表示这一节会尽量以例子讲解，其中例子的实现也是python编程求得结果哦。下一篇的文章将会具体的实现一个数据的爬取、分析以及训练最终预测流程。\n1、回归的来源\n“回归”这一词的是达尔文的表兄弟发明的（天才都一家去了），话说这个表兄弟一开始是利用豌豆种子（双亲）来预测下一代的尺寸，后来就发现有一些规律可循，因此就观察到人类的遗传上面，发现如果双亲的身高比平均高度高的话，子女的身高也倾向于较高的高度，但不会超过双亲（话说这句话在我身边完美体现，害我苦恼为啥是家里最矮的）。这种现象即孩子的高度向着平均身高回退（回归）。虽然说数值预测与回退现象关系不是很大，但是人家是达尔文的表兄弟，所以就引用了人家的指定学术用名啦~\n上一节我们讲到的房价月预测问题，实际上就是输入变量“房价”x并映射输出到一个连续的预期结果函数f(x)中。具体来说，假设我们有这么一群数据组合（x（i）,y（i））；其中i=1,...,m；也就是一共有m个数据组合样本，现在借助《机器学习实战》这本书给出的数据ex0.zip文件来实现该数据集的回归预测。\n一、首先，在附件中下载并打开数据文件ex0.txt，观察到：\n数据中的第一列都是1，那么很明显我们可以将后面的两列作为x，y值，虽然目前并不知道这些数据在实际应用中的名称。\n所有的数据（列与列之间）的间隔都是tab符号分割，每一个样本数据各占一行，这方便我们后期的数据读取。\n好的，分析数据，最方便的方式就是可视化数据，那么画个图看看数据的趋势如何：\n1.1 准备数据：使用Python从文本文件中导入数据\n创建名为“reg.py”文件，本节所有的代码都保存在该文件中；在画图之前，我们需要将准备一下，步骤包括：读取数据，将数据解析保存到矩阵中。代码如下：\n1 # coding=utf-8 2 __author__ = 'wing1995' 3 4 5 from numpy import * 6 7 8 9 def file2matrix(filename): 10 　　f = open(filename) 11 　　contents = f.readlines() 12 　　length = len(contents) # 得到文件内容的行数 13 　　Mat = zeros((length, 3)) # 创建一个空矩阵用于存储文件内容 14 　　index = 0 15 　　for line in contents: 16 　　　　line = line.strip() # 去除每一行的换行符 17 　　　　data = line.split('\\t') 18 　　　　Mat[index, :] = data # 将每一列数据按照行索引存放到空矩阵 19 　　　　index += 1 20 return mat(Mat)\n好的，现在你有了矩阵，可以通过任何方式索引矩阵，例如索引第二列数据：\n1 data_file = \"C:\\Users\\wing1995\\Desktop\\machinelearninginaction\\Ch08\\ex0.txt\" 2 dataMat = file2matrix(data_file) 3 print dataMat[:, 1]\nps：关于数据中的第一列数据为啥全是1后面会讲到，它属于默认的特征值x0。\n1.2 分析数据：使用Matplotlib创建散点图\n主要绘图用到了以下函数：\n绘制用基本函数：plt.scatter()，plt.plot()，plt.bar()\n自定义轴和标题函数：plt.xlabel()，plt.ylabel()，plt.title()\n基本图形显示，清除函数：plt.show()，plt.clf()\n具体函数的画法可以通过“help”命令查看，基础知识这里就不再赘述，直接在“reg.py”文件中添加画图函数：\n1 def my_scatter(dataMat): 2 　　x = dataMat[:, 1] 3 　　y = dataMat[:, 2] 4 　　plt.xlabel('x') 5 　　plt.ylabel('y') 6 　　plt.scatter(x, y) 7 　　plt.show()\n效果图如下：\n可以很明显的看得出来图片呈现上升的趋势，而且如果想用一条线来拟合该趋势的话，应该是一条直线。因此，我们给出拟合曲线的“假设函数”：\n所谓的拟合，就是尝试建立并调用函数h(x)，让输入数据x映射到输出结果y。\n以上样本有点大，举个小例子：\n现在，随机猜测假设函数的两个参数theta0=2和theta1=2，此时假设函数h(x)=2+2*x。得到的映射结果如下：\n这样子是无法了解我们的假设函数是否能很好的预测y值。因此，有了“成本函数”这个概念。\n成本函数J(theta)：我们可以通过成本函数来衡量假设函数的精确度，这里的精度指的是预测值h(x)与真实值y之间的差值。由于样本量往往是大于一个的，因此需要将样本中的输入值x依次代入到假设函数中得到的函数值与实际值y作比较求得样本的预测误差的平均值，具体公式如下：\n作为学数学的人，看这个就无比熟悉，工科的童鞋可能看公式不太习惯。通俗来讲，大家应该都知道平均值的定义，那么以上J（theta0,theta1）实际上就是1/2*M，这里的M就是误差平方的均值。其中m是指m个样本，例如上面表格中样本数据的m=4。这个成本函数的另外一个名字或许更为大家所知——“平方误差函数”或者“均方误差”，这里的均值减半也是梯度下降算法的简易实现，因为对平方求导得到的2会和这里的1/2抵消。还有一个问题是为啥当初设计误差的人不直接讲误差正负抵消而要做平方也是我一直没弄清的问题，上次研究图像处理的混合互补模型ROF中也有这么一个误差平方和的表示，导师问我为什么，我没回答出来，只觉得是固定的定义而已，希望知道的朋友解释一下。\n有了这个成本函数，我们就可以根据上面的表格得出假设函数的拟合精度了，那么问题来了：前面的假设函数中theta值也是我们假设的，对于大样本数据，我们主观的给定theta的值很多时候拟合的精度都不够高。如何求解这个最优theta得到拟合效果最好的假设函数？\n梯度下降算法：现在我们有了假设函数以及衡量它的精度的方式（成本函数）。现在需要一个方法来改善我们的假设函数，该方法即梯度下降法。\n1.3 用python画个图让你们更好的理解这个代价函数J(theta)。\n1.3.1 编程实现计算代价函数\n1 def computeCost(X, y, m, theta): 2 pre = X*theta # 预测值 3 s = 0 4 for i in range(m): 5 s += (pre[i] - y[i])**2 6 J = 1/(2*m)*s # 代价函数 7 return J 8 9 10 X = dataMat[:, 0:2] 11 y = dataMat[:, 2] 12 m = len(y) # 样本数量 13 theta = zeros((2, 1)) # 初始化theta 14 iterations = 1500 # 迭代次数 15 16 J = computeCost(X, y, m, theta)\n上面的代码已经很好的实现代价函数的算法，由于我们的初始化theta值为0，因此J的初始值也是0，接下来需要运用梯度下降算法计算theta0和theta1，因此先上一个coursera上面布置的作业里面我用matlab画的图（数据不一样），后面贴python代码实现该类型图的画法：\n上图是theta0和theta1在整个迭代过程中收敛到最佳假设函数的情况（J0—>J3）这个过程就是初始值theta0=theta1=0到最优值J3的多次迭代的结果，上图的红叉叉就是J(theta)的一个全局最优解的对应的最优theta的值。此时，成本最小，最能预测结果；讲此时的theta代入到假设函数得到的假设函数正是我们需要的回归函数，拟合度最高。\n从J0走向J1的这个过程就是成本函数J(theta0, theta1)分别对theta0和theta1求偏导，例如从J0走向J1的斜率就是theta0和theta1的偏导，就好比人走下坡路，斜率就是选择下坡的方向，角度；而theta0和theta1就是两个同时下坡的小人，迈开的步伐的大小就是学习速度alpha。因此两个小人能否走到坡底是由它们的初始位置（初始位置一半被初始化为0）和下坡的方向（偏导）以及下坡的步伐（学习速度alpha）所决定。\n总的来说，梯度下降的公式为：\n重复步骤，直至收敛：\n哎呀，看官可能看累了，接下来都有考试，就先写这么多，后面给出梯度下降算法的具体实现代码~"}
{"content2":"人工智能：\n人工智能（ArtificialIntelligence），英文缩写为AI。它是关于知识的科学（知识的表示、知识的获取以及知识的应用）。\n人工智能（学科）是计算机科学中涉及研究、设计和应用智能机器的一个分支。它的近期主要目标在于研究用机器来模仿和执行人脑的某些智力功能，并开发相关理论和技术。\n人工职能（能力）是智能机器所执行的通常与人类智能有关的智能行为，如判断、推理、证明、识别、感知、理解、通信、涉及、思考、规划、学习和问题求解等思维活动。该领域的研究包括：逻辑推理与定理证明、专家系统、机器学习、自然语言理解、神经网络、模式识别、智能控制等。\n机器学习：\n“机器学习”是人工智能的核心研究领域之一，其最初的研究动机是为了让计算机系统具有人的学习能力以便实现人工智能，因为众所周知，没有学习能力的系统很难被认为是具有智能的。目前被广泛采用的机器学习的定义是“利用经验来改善计算机系统自身的性能”。事实上，由于“经验”在计算机系统中主要是以数据的形式存在的，因此机器学习需要设法对数据进行分析，这就使得它逐渐成为智能数据分析技术的创新源之一，并且为此而受到越来越多的关注。\n数据挖掘：\n“数据挖掘”和“知识发现”通常被相提并论，并在许多场合被认为是可以相互替代的术语。对数据挖掘有多种文字不同但含义接近的定义，例如“识别出巨量数据中有效的、新颖的、潜在有用的、最终可理解的模式的非平凡过程”。其实顾名思义，数据挖掘就是试图从海量数据中找出有用的知识。大体上看，数据挖掘可以视为机器学习和数据库的交叉，它主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。\n参考文献：\n1、周志华：数据挖掘与机器学习\n2、What is the difference between data mining,statistics,machine learning and AI?\n3、人工智能，机器学习，统计学，数据挖掘之间有什么区别？"}
{"content2":"最近在新浪微博上看到@周磊July 组织了一个机器学习的读书演讲会，比较感兴趣，报名参加了。之前跟@贾志峰Michael 感叹北京的技术交流活动太少，不如美国同行爱交流。感觉最近两年以来技术交流活动越来越多了。\n会议在清华东门的FIT大楼举办。两点开始。我1点50进去，已经没座位了，站了不少人。目测坐了70人，站了30-40人。举办者说争取下次找更合适的场地。\n2点钟两位原定的演讲者没来，清华计算机系研究生（不知是硕士还是博士）@小猴机器人 先介绍了他的无人驾驶汽车项目。仿谷歌的无人驾驶汽车，但是价格要便宜很多，许多配件是从淘宝上买来的，一个无人驾驶汽车成本大概是数千元，当然体积很小，在玩具车中算大的，在他们自己搭的沙盘中模拟无人驾驶。他最得意的地方一是比谷歌的方案要便宜（谷歌的无人驾驶汽车要百万美元），第二个地方是尽量利用现有的解决方案而不是重新发明轮子，第三是他想把这个项目开源让更多人参与。\n大约两点半@张栋_机器学习 开始讲DeepQA。这是IBM的一个人工智能解决方案，在许多问答竞赛类节目中胜过了人。思路与搜索引擎不同，试图去理解人类的问题，并从已有的知识库中找到答案，在众多答案中打分从而找到最佳答案。@张栋_机器学习 说DeepQA在医疗领域有出色的表现（还提到一个领域我忘了），我的想法是大约因为医疗领域的问题大多数是结构化的。对于非结构话的问题DeepQA如何解决，@张栋_机器学习 提到一个思路是关键词，后面也有人提相关问题，@张栋_机器学习 没做太深入的讲解。\n@张栋_机器学习 演讲中举方韩大战为例，说如果是DeepQA来判断谁对谁错，就是把相关资料找来打分。并说这跟人的思路差不多。我感觉这恰恰跟人的思路不同。人来判断这件事，很大的因素是之前的头脑中的印象，对方韩大战中双方的资料也许只看喜欢的一方。毕竟人的脑力非常有限，这是一个说得过去的策略，不像DeepQA可以找来数千台机器同时计算，从而把回答问题的时间从数小时提高到3秒钟。\n3点半开始@张俊林say 讲《Siri--苹果帝国的余晖》，演讲资料见http://vdisk.weibo.com/s/2ugN-/1329045608 。PPT上的文字比较小，后面基本看不见，效果有点差。据他讲，实际上Siri涉及到的技术已经有40多年的研究历史，国内厂家短时间内想山寨出来一个是很困难的。\n苹果的Siri需要根据用户的输入得到用户的意图，根据用户的意图找到相关的服务（许多不是苹果自己提供的）并返回给用户。我理解最困难的还是在理解人的意图。与DeepQA面临同样的问题。"}
{"content2":"作为近几年的一大热词，人工智能一直是科技圈不可忽视的一大风口。随着智能硬件的迭代，智能家居产品逐步走进千家万户，语音识别、图像识别等AI相关技术也经历了阶梯式发展。如何看待人工智能的本质？人工智能的飞速发展又经历了哪些历程？本文就从技术角度为大家介绍人工智能领域经常提到的几大概念与AI发展简史。\n一、人工智能相关概念\n1、人工智能（Artifical Intelligence, AI)：就是让机器像人一样的智能、会思考,\n是机器学习、深度学习在实践中的应用。人工智能更适合理解为一个产业，泛指生产更加智能的软件和硬件，人工智能实现的方法就是机器学习。\n2、数据挖掘：数据挖掘是从大量数据中提取出有效的、新颖的、有潜在作用的、可信的、并能最终被人理解模式(pattern)的非平凡的处理过程。\n数据挖掘利用了统计、机器学习、数据库等技术用于解决问题；数据挖掘不仅仅是统计分析，而是统计分析方法学的延伸和扩展，很多的挖掘算法来源于统计学。\n3、机器学习：专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，机器学习是对能通过经验自动改进的计算机算法的研究。\n机器学习是建立在数据挖掘技术之上发展而来，只是数据挖掘领域中的一个新兴分支与细分领域，只不过基于大数据技术让其逐渐成为了当下显学和主流。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域。\n4、深度学习（Deep Learning）：是相对浅层学习而言的，是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络。它模仿人脑的机制来解释数据，例如图像，声音和文本。深度学习的概念源于人工神经网络的研究。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。\n到了当下，经过深度学习技术训练的机器在识别图像方面已不逊于人类，比如识别猫、识别血液中的癌细胞特征、识别MRI扫描图片中的肿瘤。在谷歌AlphaGo学习围棋等等领域，AI已经超越了人类目前水平的极限。\n为了方便大家理解，我们将上文提到的四个概念的关系用下图表示。需要注意的是，图示展现的只是一种大致的从属关系，其中数据挖掘与人工智能并不是完全的包含关系。\n二、人工智能发展历史\n（图片来源于网络）\n由图可以明显看出Deep Learning从06年崛起之前经历了两个低谷，这两个低谷也将神经网络的发展分为了几个不同的阶段，下面就分别讲述这几个阶段。\n1、 第一代神经网络（1958-1969）\n最早的神经网络的思想起源于1943年的MP人工神经元模型，当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示：\n1958年Rosenblatt发明的感知器（perceptron）算法。该算法使用MP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。\n1、 第二代神经网络（1986~1998）\n第一次打破非线性诅咒的当属现代Deep Learning大牛Hinton，其在1986年发明了适用于多层感知器（MLP）的BP算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。\n1989年，Robert Hecht-Nielsen证明了MLP的万能逼近定理，即对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近该定理的发现极大的鼓舞了神经网络的研究人员。\n同年，LeCun发明了卷积神经网络-LeNet，并将其用于数字识别，且取得了较好的成绩，不过当时并没有引起足够的注意。\n值得强调的是在1989年以后由于没有特别突出的方法被提出，且神经网络（NN）一直缺少相应的严格的数学理论支持，神经网络的热潮渐渐冷淡下去。\n1997年，LSTM模型被发明，尽管该模型在序列建模上的特性非常突出，但由于正处于NN的下坡期，也没有引起足够的重视。\n3、统计学建模的春天（1986~2006）\n1986年，决策树方法被提出，很快ID3，ID4，CART等改进的决策树方法相继出现。\n1995年，线性SVM被统计学家Vapnik提出。该方法的特点有两个：由非常完美的数学理论推导而来（统计学与凸优化等），符合人的直观感受（最大间隔）。不过，最重要的还是该方法在线性分类的问题上取得了当时最好的成绩。\n1997年，AdaBoost被提出，该方法是PAC（Probably Approximately Correct）理论在机器学习实践上的代表，也催生了集成方法这一类。该方法通过一系列的弱分类器集成，达到强分类器的效果。\n2000年，KernelSVM被提出，核化的SVM通过一种巧妙的方式将原空间线性不可分的问题，通过Kernel映射成高维空间的线性可分问题，成功解决了非线性分类的问题，且分类效果非常好。至此也更加终结了NN时代。\n2001年，随机森林被提出，这是集成方法的另一代表，该方法的理论扎实，比AdaBoost更好的抑制过拟合问题，实际效果也非常不错。\n2001年，一种新的统一框架-图模型被提出，该方法试图统一机器学习混乱的方法，如朴素贝叶斯，SVM，隐马尔可夫模型等，为各种学习方法提供一个统一的描述框架。\n4、快速发展期（2006~2012）\n2006年，深度学习（DL）元年。是年，Hinton提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。但是由于没有特别有效的实验验证，该论文并没有引起重视。\n2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。\n2011年，微软首次将DL应用在语音识别上，取得了重大突破。\n5、爆发期（2012~至今）\n2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。\nAlexNet的创新点：\n（1）首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题；\n（2）由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习；\n（3）扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合；\n（4）首次采用GPU对计算进行加速。\n结语：作为21世纪最具影响力的技术之一，人工智能不仅仅在下围棋、数据挖掘这些人类原本不擅长的方面将我们打败，还在图像识别、语音识别等等领域向我们发起挑战。如今，人工智能也在与物联网、量子计算、云计算等等诸多技术互相融合、进化，以超乎我们想象的速度发展着。而这一切的发生与演变，只用了几十年的时间……"}
{"content2":"上一篇文章主要介绍了查询与文档内容相似性的打分以及基于概率模型的BM25模型和如何修改lucene的排序源代码。这篇文章将重点讲述机器学习排序，其中的重头戏是关于ListNet算法的英文原版学术论文的解读以及RankLib源码包的学习。\n机器学习排序:从 Pairwise方法到Listwise方法\nZhe Cao*\nTao Qin*\n清华大学，北京，10084，中国\nTie-Yan Liu\n微软亚洲研究院，海淀区知春路49号，10080，中国北京\nMing-Feng Tsai*\n国立台湾大学，中华台北 106\nHang Li\n微软亚洲研究院，海淀区知春路49号，10080，中国北京\n翻译：XueQiang  Tong   http://www.cnblogs.com/txq157   txq157@163.com\n说明：在翻译过程中，我会尽量尊重原著，力求做到简洁易懂，后面会加入自己的一些理解。\n摘要\n本文主要阐述对排序对象构建机器学习模型(评分函数)。机器学习排序在文档检索、协同过滤以及其他许多领域都有广泛且重要的应用。目前一种基于把”文档对”作为排序对象的机器学习排序方法已经被提出来，在学术界我们把它称为Pairwise方法。尽管Pairwise有所改进，然而它忽略了一个非常重要的事实：我们的排序预测任务是基于所有的排序对象，这些对象的排列顺序要远远多于两个对象的排列。本文论述的机器学习排序方法，是把对象列表(检索出的全部文档)作为排序对象，并且为这个方法提出了一种概率模型。具体来说，我们引入了两个概率模型作为Listwise方法的损失函数，分别是全排列概率模型和top-one概率模型。我们会使用神经网络构建评分函数和梯度下降作为损失函数的优化手段。在信息检索中的经验显示，Listwise比Pairwise表现得更加出色。\n1.引言\n很多应用的核心事务都离不开排序，比如全文检索，协同过滤，专家搜索，反网络垃圾邮件，情感分析，产品评级等等。不失一般性，本文主要以文档检索为例讨论机器学习排序。当应用于全文检索时，机器学习排序按照如下方式工作：假设现在有一些文档集合，在检索的时候，给定一个查询，然后评分函数给每一个返回的文档打分，按降序排列。每个文档的得分代表了与这个查询的相对相关程度。在机器学习排序的训练中，往往先提供多个查询，其中每个查询都和由这个查询得到的评分文档集合相关联，得到文档后用这些训练数据创建出评分函数，用以精确地预测文档集的得分。\n由于它的重要性，近年来机器学习排序在机器学习社区中引起了广泛的关注。在业内被称之为Pairwise的若干方法已经成功用于全文检索。这个方法把一对儿有排列顺序的文档作为训练学习的实例，并且用分类算法来处理。特别地，在训练中，在我们收集到所有文档对儿的全排列之后，对每个文档对儿标识一个代表这两个文档相关程度的标签(-1 or 1)，然后我们用这些数据训练出分类模型，再用这个分类模型排序。SVM、Boosting还有神经网络这些分类模型的应用直接导致RankingSVM(Herbrich et al.,1999)，RankBoost(Freund et al.,1998)，RankNet(Burges et al.,2005)算法的产生。\n使用Pairwise方法有很多优势。首先，在分类算法中很多成型的方法可以直接应用于排序中。其次，在特定情况下可以非常容易地获取到文档对儿(Joachims,2002)。\n然而，这个方法存在很多缺点。首先，作为训练对象的文档对儿，他所训练出来\n的模型的最小化损失函数，是用在分类中而非排序中。其次，文档对儿数量庞大导致计算复杂度太高。第三，文档对儿属于独立同分布(iid)的假设过于(严格)strong，与实际情况相差甚远。最后，不同的查询产生的文档对儿数量变化太大，换句话说文档对儿的数量对查询比较敏感，由于这个差异，将直接导致训练模型更加偏向于拥有更多文档对儿的查询(拥有多数量文档对儿的查询对建模贡献更大)。\n在本文中，我们提出Listwise的方法，在这个方法中，我们把文档集作为训练对象。接下来的主要问题是如何定义Listwise的损失函数。我们提出一个概率模型用于listwise损失函数的参数估计。我们会同时把排序函数对文档的打分(此时参数是未知的)和人工对文档显示的或者隐士的打分变换成概率分布，这样我们就可以把两者(特指概率分布)间的距离(差异)作为损失函数。\n我们定义了两个概率模型，分别是组合(全排列)概率模型和top one概率。ListNet算法是这样一种算法，它使用listwise的损失函数，优化损失函数前先构建神经网络模型，然后用梯度下降估计参数。事实表明，ListNet算法明显好于Ranking SVM, RankBoost还有 RankNet。\n本文主要包括以下4部分：①listwise算法的概述；②基于概率模型的损失函数的转换；③listwise算法的发展；④关于这个算法的实验验证。\n2.Related Work\n2.1机器学习排序\nRankNet算法在对损失函数进行参数估计时，使用交叉熵作为参数搜索方向(构建损失函数)使用梯度下降法优化损失函数，在这个过程中会构建线性神经网络作为评分函数。Pairwise算法被先后应用于信息检索。比如，Joachims(2002)把RankingSVM算法应用于全文检索，他从用户的点击数据(一般从点击图中获取)中获取训练时要用到的文档对儿。\n2.2排序中的概率模型\nLuce 定义了概率分布模型，他进一步引入参数来表征概率分布并且发明了估计参数的方法。Plackett在投票结果系统上应用了这个模型和方法。本文应用相似的概率分布模型。然而，本文提到的底层结构(parameters)和基本用法(文档分数转换为概率分布) 与Plackett的会有些差异。\n3.Listwise方法\n这部分，我们将会以全文检索为例，给出关于机器学习排序的一般性的描述，并且在细节上加以详细说明。在以下描述中，我们使用上标表示查询的索引，使用下标表示文档的索引。\n在训练中，通常给定这样一组查询Q={q(1),q(2),…q(m)}。每个查询都和文档集合d(i)=(d(i)1,d(i)2,…d(i)n(i))相关联,其中d(i)j表示第j个文档，n(i)表示第i个查询的文档数目(d(i))。此外，每一组文档d(i)都和文档的人工打分y(i)=(y(i)1,y(i)2,…y(i)n(i))相关联。y(i)代表了文档和查询的相关程度，这个分数是人为指定的。比如，这个分数可以由文档d(i)j的点击率转化而来(Joachims, 2002)。这种假设认为具有较高点击率的文档和查询的相关性更强。\n一个特征向量 x(i)j = Ψ(q(i), d(i)j) 是由每个查询-文档(q(i), d(i)j)创建而来，i = 1, 2, · · · m，j =1,2, · · · n(i)。每个特征向量构成x(i) = (x(i)1,x(i)2,…x(i)n(i))，这是每个查询构成的特征矩阵。对应分数集合y(i) = (y(i)1,y(i)2,…y(i)n(i)),最后训练集可以表示成T =｛x(i), y(i)｝mi=1。\n然后我们定义一个评分函数f,对每个特征向量x(i)j 输出一个评分f(x(i)j)，对于特征矩阵我们得到一组评分z(i) = (f(x(i)1),f(x(i)2),…f(x(i)n(i)))。学习的目标是在整个数据集上取得总损失函数最小化。\n∑mi=1L(y(i),z(i))       --------------------- (1)\nL为listwise的损失函数。在训练过中，给定一个查询和相关联的文档集，我们把整个文档集的函数打分和人工打分转换成概率分布然后计算差值，利用梯度下降估计出打分函数的最优参数(优化的过程)，然后用测试集提高泛化能力。相比之下，Pairwise方法训练时在文档集中找出所有的文档对儿排列，如果前一个文档的人工打分高于后一个，就标识为1否则为-1，最后用这些训练数据训练出一个分类模型，比如SVM。比如有三个文档，我们暂且命名为a,b,c，全排列为ab(1),ba(-1),ac(1),ca(-1),cb(1),bc(-1)，括号里为预测后的分类。我们可以从所有分类为1的组合中找出文档排列顺序：acb。Pairwise算法将更多的精力放在寻找全排列的文档对儿以及训练分类模型上，复杂度非常高，Listwise解决了这个问题。\n4.概率模型\n前面我们提出使用两个概率模型中的任意一个计算损失函数，这两个概率模型分别是组合概率和top one 概率。\n4.1.Permutation Probability\n我们确定了排序对象集合1,2,…n。其中一种排列为{1,2,…,n}。我们写为π 。其中π(j)表示对象在排列中的位置。文档集合的所有可能的排列为Ωn。以后我们有时会互换排名函数和排名函数给出的分数列表。\n我们假设，在使用排序函数的排序列表中进行预测时具有不确定性。换句话说，任何排列都是可能的，但是不同的排列可能有不同的似然函数估计。\n我们定义的组合概率，在给定评分函数前提下，应该在组合概率的似然估计上取得比较理想的效果。因此，我们有如下定义：\n定理1 假设π是排序列表中其中一种排列，Φ(.)是一个递增并且恒大于零的函数。那么，排列组合的概率为：\n于任意的排序列表，在前一个文档得分高于后面一个文档得分的情况下，如果两者交换位置，我们将会得到一个更加低值的概率分布。定理4更加简单，如果一个概率分布是按照文档的分数降序排列的，那么他具有最高值的概率分布，反之如果按照升序排列的话，概率分布的值是最低的。\n给定两个scores集合，我们根据他们计算出两个组合概率分布模型，然后把这两个模型之间的差值作为listwise的损失函数。然而，对于容量为n的文档集合来说，组合情况是n!,计算起来比较棘手，所有我们考虑使用top-one概率模型。\n4.2 Top One Probability\n一个对象的top one概率表示这个对象在所有的文档集中排在最前面的概率。\n需要注意的是ListNet和RankNet很相似。他们的主要区别在于前者把document list 作为训练和预测的对象，后者把 document pair作为实例。比较有趣的是，当有一组查询，每个查询得到的文档数目为2时，listwise的损失函数变得和pairwise几乎相等。RankNet的时间复杂度为O(m .n2max)(Burges et al.,2005),m代表查询的数目,ｎ代表每个查询对应的最大文档数目。ListNet的时间复杂度为O(m .nmax)。\n6.实验\n我们将使用三个数据集分别与RankNet(Burges et al.,2005)、Ranking SVM (Herbrich et al., 1999)和 RankBoost (Freund et al., 1998)进行精度对比。这里的ListNet使用top one概率模型。\n为了简单起见，在本次实验中，我们使用线性神经网络模型并且省略常量b:\n，尖括号里面代表向量内积。\n6.1数据集\n我们使用3个数据集：TREC，OHSUMED，CSearch。TREC数据集包括了1053110个pages,11164829个超链接，这些数据是查询50次得到的。在构造特征向量时，考虑了内容特征和超链接特征，总共有20个。OHSUMED数据集包括了348556个documents，106个queries，16140个文档对儿，总共构建了30个特征。CSearch大约包含了25000 queries，每个query有1000多个关联文档，构建了600个features，包括query dependent features and independent features.这个数据集提供了5个等级的评分：4 (”perfect match”) to 0 (”bad match”)。\n为了使排序更加接近真实情况，我们创建训练数据集的时候将使用”评分等级”来表示关联程度的高低(离散关联判断)。在排名性能评估上，我们采用两个常用的IR评价措：NDCG和MAP。NDCG is designed to measure ranking accuracy when there are more than two levels of relevance　judgments. For MAP it is assumed that there are two levels: relevant and irrelevant.关于这两部分的理解，可以参看http://www.cnblogs.com/HappyAngel/p/3535919.html。NDCG主要用于评分等级大于2个的场景，而MAP主要用于评分等级为相关和不相关的场景。\n6.2排序精度\n对于TREC和OHSUMED我们把每个数据集分成5个部分,实施”五折交叉验证”。 在每次实验中，3/5用于训练，1/5用于验证，剩下的1/5用于测试。对于RankNet和ListNet算法，在每次实验中验证集主要用于确定最优化的迭代次数以便训练出最优的模型。对于RankingSVM算法主要是调整系数C而RankBoost算法主要寻找出最佳的weak learns的数量。在第6部分我们输出的报告(精度)为五次实验的平均值。\nFigure 1和Table 1给出了TREC 的报告结果。结果显示，ListNet算法的表现要优于其他三种方法。尤其在Table1的报告中，我们看到在第一次和第二次试验中ListNet的NDCG值超越RankNet大约4个point gain，带来大约10%的搜索结果相关性的改进。\nFigure 2和Table 2显示了OHSUMED数据集的试验结果。再一次，在所有的评估中，ListNet仍然优于RankNet和RankBoost。此外，除了第3次和第5次试验外，ListNet算法均优于RankSVM(用NDCG评估)。\nCSearch的数据量非常庞大，我们没有采用交叉验证的方式。我们从中随机选取1/3作为训练，1/3用于验证，剩下的1/3用于测试。Figure3显示了ListNet，RankNet和RankBoost的试验结果。ListNet算法再一次不负众望。由于数据量的原因，我们不能实施RankingSVM。\n6.3讨论\n我们来讨论一下为什么基于listwise的方法ListNet优于基于pairwise的方法RankNet,RankingSVM还有RankBoost。就像在第一部分解释过的那样，在pairwise方法中文档对儿的数量受查询影响很大，结果导致在训练时训练模型更加偏向于拥有较多文档对儿的查询。并且我们在几乎所有的测试数据集中都观察到这种倾向。Table 2显示了在OHSUMED数据集上每一个查询的文档对儿的分布情况。\n我们看到分布呈现明显的倾斜：更多的查询只拥有很少的文档对儿，只有少数的查询拥有较多的文档对儿。在listwise方法中损失函数在每个查询中都有定义，所以这个问题跟本不存在。(由于listwise把整个文档集作为训练对象，不会出现训练模型的倾向问题) This appears to be one of the reasons for the higher performance(高性能) by ListNet。\n第二个问题就是pairwise的损失函数问题。由于pairwise实际上把排序问题转换成了分类问题，使用分类算法的损失函数对于排序来说，它所付出的代价可能会更大。而且对于MAP和NDCG这样的评估准则，更加适合用在把整个文档集作为训练集的场景，pairwise的损失函数对于上述的评估准则来说会显得更加松散。\n我们更进一步分析两者的损失函数的不同点。这一次我们使用TREC数据集的Figure4和Figure5来说明。可以看到，在训练中，pairwise的损失函数并不是完全和NDCG成反比(理想状态应该是呈现反比关系，即损失函数越小，NDCG越大)。从数据中我们看到，从第20次迭代到第50次迭代过程中，两者(loss function and NDCG)还是呈现反比关系的。然而60次以后，尽管pairwise的损失函数在下降，NDCG值却并没有上升。相比之下，listwise的损失函数完全和NDCG呈现反比关系。另外，从图中明显看出，pairwise损失函数收敛的速度也明显慢于listwise。\n最终我们得出结论，listwise方法的性能明显优于pairwise，更适合于机器学习排序。\n7.结论\n在本文中，我们提出了一种新的学习方法排名，称为listwise方法。我们认为\n在学习排名时采用这种方法比传统的成对法更好。在listwise方法中，不是使用对象对作为实例，我们使用对象列表作为学习中的实例。Listwise方法的关键问题是定义一个listwise损失函数。在本文中，我们提出采用概率方法来解决它。具体来说，我们使用概率模型：组合概率和top-one概率将文档排名分数转换为概率分布模型。然后，我们可以把两个概率分布模型之间的任何差值度量（例如，交叉熵）视为listwise损失函数。\n我们然后开发了一种基于该方法的学习方法，使用线性神经网络构建评分函数,使用交叉熵构建损失函数，使用梯度下降法对损失函数进行优化。使用三个数据集的实验结果表明，该方法比现有的pairwise方法（如RanNet，Ranking SVM和RankBoost）更好，这表明最好采用listwise方法来学习排名。除此之外，我们还调查了pairwise损失函数和性能评估指标（如NDCG和MAP）之间的关系。\n8.致谢\nBin Gao对这项工作提出了许多有价值的建议。我们还要感谢Kai Yi对我们实验的帮助。\n9.参考工具\nBaeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern information retrieval. Addison Wesley.\nBurges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,Hamilton, N., & Hullender, G. (2005). Learning to rank using gradient descent. Proceedings of ICML 2005 (pp.89–96).\nCao, Y. B., Xu, J., Liu, T. Y., Li, H., Huang, Y. L., & Hon, H. W. (2006). Adapting ranking SVM to document retrieval. Proceedings of SIGIR 2006 (pp. 186–193).\nCohen, W. W., Schapire, R. E., & Singer, Y. (1998). Learning to order things. Advances in Neural Information Processing Systems. The MIT Press.\nCrammer, K., & Singer, Y. (2001). Pranking with ranking.Proceedings of NIPS 2001.\nCraswell, N., Hawking, D., Wilkinson, R., & Wu, M.(2003). Overview of the TREC 2003 web track. Proceedings of TREC 2003 (pp. 78–92).\nFreund, Y., Iyer, R., Schapire, R. E., & Singer, Y. (1998).\nAn efficient boosting algorithm for combining preferences. Proceedings of ICML 1998 (pp. 170–178).\nHerbrich, R., Graepel, T., & Obermayer, K. (1999). Support vector learning for ordinal regression. Proceedings of ICANN 1999 (pp. 97–102).\nHersh, W. R., Buckley, C., Leone, T. J., & Hickam, D. H.(1994).\nOHSUMED: An interactive retrieval evaluation and new large test collection for research. Proceedings of SIGIR 1994 (pp. 192–201).\nJarvelin, K., & Kekanainen, J. (2000). IR evaluation methods for retrieving highly relevant documents. Proceedings of SIGIR 2000 (pp. 41–48).\nJoachims, T. (1999). Making large-scale support vector machine learning practical. Advances in kernel methods: support vector learning, 169–184.\nJoachims, T. (2002). Optimizing search engines using clickthrough data. Proceedings of KDD 2002 (pp. 133–142).\nLebanon, G., & Lafferty, J. (2002). Cranking: Combining rankings using conditional probability models on permutations. Proceedings of ICML 2002 (pp. 363–370).\nLuce, R. D. (1959). Individual choice behavior. Wiley. Matveeva, I., Burges, C., Burkard, T., Laucius, A., & Wong, L. (2006).\n10.附录\nA: Proof of Lemma 2\nEnd。"}
{"content2":"先从一本书说起吧----《机器学习实战》\n作者在书中讲到逻辑回归的时候，用简短的语言介绍了一下理论之后，就给出了一段代码。然而就是这段代码把我带进了误区，也许不能叫误区，而是因为我自己的水平不够。后来在查阅资料的时候，发现有人也因为这个问题纠结了好久。也许这本书是写给一些有经验的人员看的，不是特别适合作为入门的书。\n在查找关于逻辑回归相关资料的时候，发现大多数都是介绍了好多数学公式，所以我一直都在理解数学公式的基础上同时试图在脑海中演练该如何编程实现它，然后再对照上面提到的书中的代码，然后悲哀的发现了解不了。并且，查到的大多数资料上并没有详细的代码实现，如果有，也是跟书上的代码是一样的。\n最后，从网上找到书中使用的测试数据，跟踪打印代码中的每个变量，才理解了书中第一段代码的求解原理，进而理解了后面一些代码的原理。现在回过头想想确实比较简单，但是这个简单是有一个前提的：书上的代码或者资料中推导出的的公式或者我自己的理解，这三者之间必须是有一个错误的。\n并且在这个过程中，我一直试图绕过那么多数学公式和一堆概念，但是发现很难，所以我按照自己的理解剔除掉一些无用的数学公式和概念，力求用最少的理论解释清楚什么是逻辑回归。\n1.逻辑回归的定义\n1）有一种定义是这样的：逻辑回归其实是一个线性分类器，只是在外面嵌套了一个逻辑函数，主要用于二分类问题。\n这个定义明确的指明了逻辑回归的特点：\na） 一个线性分类器\nb）外层有一个逻辑函数\n2）假设有一个线性函数z，其一般公式为：\n(公式一)\n转换成求和公式：\n（公式二）\n转化成向量的形式：\n（公式三）\n3）逻辑函数(也叫Sigmoid函数)\n基本上采用的都是下面这个函数：\n(公式四)\n这个函数的作用就是把无限大或者无限小的数据压缩到[0,1]之间，用来估计概率。图像大致为：\n基本上是以0.5分界，0.5以上为1，0.5以下为0。但是这个分界值可以自己设定。\n4）逻辑回归函数\n综合公式四和公式一或者公式四和公式三，即可得到逻辑回归函数：\n（公式五）或者（公式六）\n其实，如果编程求解的话，到这里基本就可以了。但是既然都提到了最大似然估计，那我们也说下。\n2.最大似然估计\n最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。\n换句话说就是：既然我们无法知道真实值，那么就把这个当作真实值吧！\n其唯一的作用就是给这个算法找一个说的过去的理论基础，然后在这个基础上推导出最大似然函数，接着构建损失函数。这对于非数学专业的人来说，用途并不大，有时候甚至会造成理解上的困难，进而变成学习的阻碍。其实，我们完全可以绕过这个阻碍，只去关注最后的结果。\n补充说明：如果想要理解推导过程，可以先看看最大似然估计思想，然后也要理解联合概率。因为有些讲逻辑回归的文章会直接跳出最大似然估计的函数，如果不了解这两点内容容易抓瞎。\n3.求解方式\n1）使用梯度下降法求解\n经过一系列推导之后，得出梯度下降法求解的核心公式，即权重的更新方式：\n（公式七）\n需要说明的是：α表示下降的步长，可以自己指定。hθ 表示损失函数或者惩罚系数。如果hθ 表示惩罚系数，那么如何求的这组系数才是整个逻辑回归算法的重点。\n在开始写代码前，再介绍另外一个求解方式：向量化\n2）向量化\n向量化是使用矩阵计算来代替for循环，以简化计算过程，提高效率。(下面引用下其他文章的讲解：出现的地方太多不知道哪个是原作者，见谅)\n向量化过程：\n约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值：\ng(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。\nθ更新过程可以改为：\n综上所述，向量化后θ更新的步骤如下：\na)求 A=x*θ\nb)求 E=g(A)-y\nc)求\n4.实现过程(下面的代码实现的是梯度上升法，其跟梯度下降法的唯一区别就是和之间的减号变成了加号，前者求最大值，后者求最小值)\n代码基本脱胎于《机器学习实战》这本书，但是有改动。\n1）普通的梯度上升法\n下面这段代码，也就是开头提到的那个造成误解的代码，其实现依据是向量化求解，并不是根据公式七来的，所以如果对照公式七理解这段代码会完全摸不着头脑。如果对照向量化后θ(也就是权重)的更新步骤会很容易理解。\n''' 普通的梯度上升法 ''' import numpy as np import os import pandas as pd def loadDataSet(): ##运行脚本所在目录 base_dir=os.getcwd() ##记得添加header=None，否则会把第一行当作头 data=pd.read_table(base_dir+r\"\\lr.txt\",header=None) ##dataLen行dataWid列 ：返回值是dataLen=100 dataWid=3 dataLen,dataWid = data.shape ##训练数据集 xList = [] ##标签数据集 lables = [] ##读取数据 for i in range(dataLen): row = data.values[i] xList.append(row[0:dataWid-1]) lables.append(row[-1]) return xList,lables ##逻辑函数 def sigmoid(inX): return 1.0/(1+np.exp(-inX)) ##梯度上升函数 def gradAscent(datamatIn,classLables): ##把datamatIn从列表转换成矩阵 dataMatrix = np.mat(datamatIn) ##把列表转换成100行1列的矩阵，而np.mat(classLables)是转换成1行100列的矩阵 labelMat = np.mat(classLables).transpose() ##求矩阵的长宽 m,n = np.shape(dataMatrix) ##步长，可以自己设置 alpha = 0.001 ##最大循环次数 maxTry = 500 ##初始化向量：2行1列的矩阵 weights =np.ones((n,1)) ##循环一定次数，求权重 for k in range(maxTry): ##dataMatrix 100行2列 weights是2行1列 ##h是100行1列 h = sigmoid(dataMatrix*weights) ##向量的偏差 error = (labelMat - h) ##dataMatrix.transpose() 转换成2行100列的矩阵 ##error 是100行1列 ##weights是2行1列的值 weights = weights + alpha*dataMatrix.transpose()*error return weights ''' 结果大于0.3的设置为1，正确率基本100% ''' def GetResult(): dataMat,labelMat=loadDataSet() weights=gradAscent(dataMat,labelMat) dataMatrix = np.mat(dataMat) ##求的最后的结果 h = sigmoid(dataMatrix*weights) ##打印结果，观察数据 for i in range(len(h)): print(str(h[i])+\":\"+str(labelMat[i])) #print(h) #print(weights) ##0.08108752 -0.1233496 if __name__=='__main__': GetResult()\n2）随机梯度上升发\n这个算法，才是符合公式七的算法，但是代码中并没有求和这步，只有括号中的那部分，这也是我开头说的三者之间必有一个错误的地方。\n只包括核心部分，其他部分见上段代码\n''' 结果大于0.29或者0.26都可以，也只有1-2个分类错误 weights:[ 0.0868611 -0.13086297] ''' ##随机梯度上升算法 def gradAscent(datamatIn,classLables): m,n = np.shape(datamatIn) ##步长，可以自己指定，决定收敛速度 alpha = 0.001 ##最大循环次数 maxTry = 200 ##初始化权重：列表而不是矩阵 weights =np.ones(n) ##循环求解：在整个数据集上循环 for k in range(maxTry): ##对每行进行处理 for i in range(m): ##每行向量化 h = sigmoid(sum(datamatIn[i]*weights)) ##每行向量偏差 error = (classLables[i] - h) ##更新权重 weights = weights +alpha*error*datamatIn[i] return weights ##打印结果 def GetResult(): dataMat,labelMat=loadDataSet() weights=gradAscent(dataMat,labelMat) m,n = np.shape(dataMat) for i in range(m): h = sigmoid(sum(dataMat[i]*weights)) print(str(h)+\" : \"+str(labelMat[i])) #print(weights)\n3）改进的随机梯度上升算法\n书中还讲到了一个改进的随机梯度上升算法。\n##随机梯度上升函数 def gradAscent(datamatIn,classLables): m,n = np.shape(datamatIn) ##循环次数 maxTry = 150 ##初始化权重：列表 weights =np.ones(n) ##循环求解 for j in range(maxTry): ##在整个数据集上循环 for i in range(m): ##跟新alpha，即跟新步长值 alpha = 4/(1.0+j+i)+0.01 ##随机抽取一个下标 randIndex = int(np.random.uniform(0,m)) ##对抽到下标的数据行进行求值 h = sigmoid(sum(datamatIn[randIndex]*weights)) ##求得误差值 error = classLables[randIndex] - h ##更新权重 weights = weights +alpha*error*datamatIn[randIndex] return weights\n该算法每次都会调整步长值，即缓解了随着循环次数的增加造成的特征值的波动，也保证了当j<<max(i)时，步长值的下降不是严格下降的。而避免参数的严格下降在优化退火算法中常常用到。\n5.使用sklearn包中的逻辑回归算法(非完整代码，缺少部分在第一个代码段)\nsklearn包中的LogisticRegression函数，默认使用L2正则化防止过度拟合。\nfrom sklearn.linear_model import LogisticRegression def sk_lr(X_train,y_train): model = LogisticRegression() model.fit(X_train, y_train) model.score(X_train,y_train) #print('权重',model.coef_) return model.predict(X_train) ##分类错了2个 def GetResult(): dataMat,labelMat=loadDataSet() pred = sk_lr(dataMat,labelMat) for i in range(len(pred)): print(str(pred[i])+\" : \"+str(labelMat[i])) if __name__=='__main__': GetResult()\n最后得出的预测结果就是0，1值，跟标签对比，有两个分类错了。\n6.逻辑回归优缺点\n优点：计算代价不高，易于理解和实现\n缺点：容易欠拟合，分类精度可能不高\n适用数据类型：数值型和标称型数据\n附录：测试数据\n-0.017612 14.053064 0 -1.395634 4.662541 1 -0.752157 6.538620 0 -1.322371 7.152853 0 0.423363 11.054677 0 0.406704 7.067335 1 0.667394 12.741452 0 -2.460150 6.866805 1 0.569411 9.548755 0 -0.026632 10.427743 0 0.850433 6.920334 1 1.347183 13.175500 0 1.176813 3.167020 1 -1.781871 9.097953 0 -0.566606 5.749003 1 0.931635 1.589505 1 -0.024205 6.151823 1 -0.036453 2.690988 1 -0.196949 0.444165 1 1.014459 5.754399 1 1.985298 3.230619 1 -1.693453 -0.557540 1 -0.576525 11.778922 0 -0.346811 -1.678730 1 -2.124484 2.672471 1 1.217916 9.597015 0 -0.733928 9.098687 0 -3.642001 -1.618087 1 0.315985 3.523953 1 1.416614 9.619232 0 -0.386323 3.989286 1 0.556921 8.294984 1 1.224863 11.587360 0 -1.347803 -2.406051 1 1.196604 4.951851 1 0.275221 9.543647 0 0.470575 9.332488 0 -1.889567 9.542662 0 -1.527893 12.150579 0 -1.185247 11.309318 0 -0.445678 3.297303 1 1.042222 6.105155 1 -0.618787 10.320986 0 1.152083 0.548467 1 0.828534 2.676045 1 -1.237728 10.549033 0 -0.683565 -2.166125 1 0.229456 5.921938 1 -0.959885 11.555336 0 0.492911 10.993324 0 0.184992 8.721488 0 -0.355715 10.325976 0 -0.397822 8.058397 0 0.824839 13.730343 0 1.507278 5.027866 1 0.099671 6.835839 1 -0.344008 10.717485 0 1.785928 7.718645 1 -0.918801 11.560217 0 -0.364009 4.747300 1 -0.841722 4.119083 1 0.490426 1.960539 1 -0.007194 9.075792 0 0.356107 12.447863 0 0.342578 12.281162 0 -0.810823 -1.466018 1 2.530777 6.476801 1 1.296683 11.607559 0 0.475487 12.040035 0 -0.783277 11.009725 0 0.074798 11.023650 0 -1.337472 0.468339 1 -0.102781 13.763651 0 -0.147324 2.874846 1 0.518389 9.887035 0 1.015399 7.571882 0 -1.658086 -0.027255 1 1.319944 2.171228 1 2.056216 5.019981 1 -0.851633 4.375691 1 -1.510047 6.061992 0 -1.076637 -3.181888 1 1.821096 10.283990 0 3.010150 8.401766 1 -1.099458 1.688274 1 -0.834872 -1.733869 1 -0.846637 3.849075 1 1.400102 12.628781 0 1.752842 5.468166 1 0.078557 0.059736 1 0.089392 -0.715300 1 1.825662 12.693808 0 0.197445 9.744638 0 0.126117 0.922311 1 -0.679797 1.220530 1 0.677983 2.556666 1 0.761349 10.693862 0 -2.168791 0.143632 1 1.388610 9.341997 0 0.317029 14.739025 0\nView Code"}
{"content2":"人工智能：计算机视觉、图像处理、模式识别、机器学习之间的关系\n什么是人工智能呢？人工智能，是由人类设计并在计算机环境下实现的模拟或再现某些人智能行为的技术。一般认为，人类智能活动可以分为两类：感知行为与思维活动。模拟感知行为的人工智能研究的一些例子包括语音识别、话者识别等与人类的听觉功能有关的“计算机听觉”，物体三维表现的形状知识、距离、速度感知等与人类视觉有关的“计算机视觉”，等等。模拟思维活动的人工智能研究的例子包括符号推理、模糊推理、定理证明等与人类思维有关的“计算机思维”，等等。\n人工智能领域：机器学习 深度学习 图像算法 图像处理 语音识别 图像识别 算法研究\n从图像处理和模式识别发展起来的计算机视觉研究对象之一是如何利用二维投影图像恢复三维景物世界。计算机视觉使用的理论方法主要是基于几何、概率和运动学计算与三维重构的视觉计算理论，它的基础包括射影几何学、刚体运动力学、概率论与随机过程、图像处理、人工智能等理论。计算机视觉要达到的基本目的有以下几个：\n(1) 根据一幅或多幅二维投影图像计算出观察点到目标物体的距离；\n(2) 根据一幅或多幅二维投影图像计算出目标物体的运动参数；\n(3) 根据一幅或多幅二维投影图像计算出目标物体的表面物理特性；\n(4) 根据多幅二维投影图像恢复出更大空间区域的投影图像。\n计算机视觉要达到的最终目的是实现利用计算机对于三维景物世界的理解，即实现人的视觉系统的某些功能。\n在计算机视觉领域里，医学图像分析、光学文字识别对模式识别的要求需要提到一定高度。又如模式识别中的预处理和特征抽取环节应用图像处理的技术；图像处理中的图像分析也应用模式识别的技术。在计算机视觉的大多数实际应用当中，计算机被预设为解决特定的任务，然而基于机器学习的方法正日渐普及，一旦机器学习的研究进一步发展，未来“泛用型”的电脑视觉应用或许可以成真。\n人工智能所研究的一个主要问题是：如何让系统具备“计划”和“决策能力”？从而使之完成特定的技术动作（例如：移动一个机器人通过某种特定环境）。这一问题便与计算机视觉问题息息相关。在这里，计算机视觉系统作为一个感知器，为决策提供信息。另外一些研究方向包括模式识别和机器学习（这也隶属于人工智能领域，但与计算机视觉有着重要联系），也由此，计算机视觉时常被看作人工智能与计算机科学的一个分支。\n机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演译。\n为了达到计算机视觉的目的，有两种技术途径可以考虑。第一种是仿生学方法，即从分析人类视觉的过程入手，利用大自然提供给我们的最好参考系——人类视觉系统，建立起视觉过程的计算模型，然后用计算机系统实现之。第二种是工程方法，即脱离人类视觉系统框框的约束，利用一切可行和实用的技术手段实现视觉功能。此方法的一般做法是，将人类视觉系统作为一个黑盒子对待，实现时只关心对于某种输入，视觉系统将给出何种输出。这两种方法理论上都是可以使用的，但面临的困难是，人类视觉系统对应某种输入的输出到底是什么，这是无法直接测得的。而且由于人的智能活动是一个多功能系统综合作用的结果，即使是得到了一个输入输出对，也很难肯定它是仅由当前的输入视觉刺激所产生的响应，而不是一个与历史状态综合作用的结果。\n不难理解，计算机视觉的研究具有双重意义。其一，是为了满足人工智能应用的需要，即用计算机实现人工的视觉系统的需要。这些成果可以安装在计算机和各种机器上，使计算机和机器人能够具有“看”的能力。其二，视觉计算模型的研究结果反过来对于我们进一步认识和研究人类视觉系统本身的机理，甚至人脑的机理，也同样具有相当大的参考意义。\n在我的理解里，要实现计算机视觉必须有图像处理的帮助，而图像处理倚仗与模式识别的有效运用，而模式识别是人工智能领域的一个重要分支，人工智能与机器学习密不可分。纵观一切关系，发现计算机视觉的应用服务于机器学习。各个环节缺一不可，相辅相成。\n计算机视觉（computer vision），用计算机来模拟人的视觉机理获取和处理信息的能力。就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，用电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。\n图像处理（image processing），用计算机对图像进行分析，以达到所需结果的技术。又称影像处理。基本内容图像处理一般指数字图像处理。数字图像是指用数字摄像机、扫描仪等设备经过采样和数字化得到的一个大的二维数组，该数组的元素称为像素，其值为一整数，称为灰度值。图像处理技术的主要内容包括图像压缩，增强和复原，匹配、描述和识别3个部分。常见的处理有图像数字化、图像编码、图像增强、图像复原、图像分割和图像分析等。图像处理一般指数字图像处理。\n模式识别(Pattern Recognition)是指对表征事物或现象的各种形式的(数值的、文字的和逻辑关系的)信息进行处理和分析,以对事物或现象进行描述、辨认、分类和解释的过程,是信息科学和人工智能的重要组成部分。模式识别又常称作模式分类，从处理问题的性质和解决问题的方法等角度，模式识别分为有监督的分类（Supervised Classification）和无监督的分类(Unsupervised Classification)两种。模式还可分成抽象的和具体的两种形式。前者如意识、思想、议论等,属于概念识别研究的范畴,是人工智能的另一研究分支。我们所指的模式识别主要是对语音波形、地震波、心电图、脑电图、图片、照片、文字、符号、生物传感器等对象的具体模式进行辨识和分类。模式识别研究主要集中在两方面,一是研究生物体(包括人)是如何感知对象的，属于认识科学的范畴,二是在给定的任务下,如何用计算机实现模式识别的理论和方法。应用计算机对一组事件或过程进行辨识和分类，所识别的事件或过程可以是文字、声音、图像等具体对象，也可以是状态、程度等抽象对象。这些对象与数字形式的信息相区别，称为模式信息。模式识别与统计学、心理学、语言学、计算机科学、生物学、控制论等都有关系。它与人工智能、图像处理的研究有交叉关系。\n机器学习(Machine Learning)是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习在人工智能的研究中具有十分重要的地位。一个不具有学习能力的智能系统难以称得上是一个真正的智能系统，但是以往的智能系统都普遍缺少学习的能力。随着人工智能的深入发展，这些局限性表现得愈加突出。正是在这种情形下，机器学习逐渐成为人工智能研究的核心之一。它的应用已遍及人工智能的各个分支，如专家系统、自动推理、自然语言理解、模式识别、计算机视觉、智能机器人等领域。机器学习的研究是根据生理学、认知科学等对人类学习机理的了解，建立人类学习过程的计算模型或认识模型，发展各种学习理论和学习方法，研究通用的学习算法并进行理论上的分析，建立面向任务的具有特定应用的学习系统。这些研究目标相互影响相互促进。\n人类研究计算机的目的，是为了提高社会生产力水平，提高生活质量，把人从单调复杂甚至危险的工作中解救出来。今天的计算机在计算速度上已经远远超过了人，然而在很多方面，特别是在人类智能活动有关的方面例如在视觉功能、听觉功能、嗅觉功能、自然语言理解能力功能等等方面，还不如人。\n这种现状无法满足一些高级应用的要求。例如，我们希望计算机能够及早地发现路上的可疑情况并提醒汽车驾驶员以避免发生事故，我们更希望计算机能帮助我们进行自动驾驶，目前的技术还不足以满足诸如此类高级应用的要求，还需要更多的人工智能研究成果和系统实现的经验。\n转载时请以超链接形式标明文章原始出处和作者信息及本声明http://www.blogbus.com/shijuanfeng-logs/216968430.html"}
{"content2":"拥抱开源, 又见 .NET」\n随着 .NET Core的发布和开源，.NET又重新回到了人们的视野。除了开源、跨平台、高性能以及优秀的语言特性，越来越多的第三方开源库也出现在了Github上——包括ML.NET机器学习、Xamarin移动开发平台、基于Actor模型的分布式框架Orleans以及分布式开发及部署平台Service Fabric等等。\n9月15日 .NET西安社区第一次线下分享交流会如期举行并取得圆满成功；以下是活动本次活动的精彩内容\n活动反馈：\n感谢张阳对F#普及，改变了自己很多对代码的认识。希望能分享一些ASP.NET Core大型工程化的经验和干货。也愿意作为讲师分享使用Blazor的心得；\n认识了新朋友，对DotNet了解更深入，觉得应该有一些 .NET Core新特性，以及相对于 .NET Fx的优势的介绍，期望下次活动分享交流Azure，.NET Core应用架构相关经验心得；也愿意作为讲师进行ASP.NET Core+Azure App Service+ Azure SQL实践分享；\n增长了见识，见识到很多新的技术，多增加互动的环节效果会更好；\n特别鸣谢：\n感谢魏琼东，张文清，张阳三位讲师为我们带来的精彩话题；\n感谢西安thoughtworks对「拥抱开源, 又见 .NET」第一次线下分享交流活动的大力支持；\n还有陈计节，远在北京，特意主动联系我们，赞助 .NET西安社区《C#本质论》&《.NET性能优化》共10本图书；\n感谢彭子健，马建勋，阿布，何易凡，王婧，卢冲（排名以报名顺序为准）牺牲周末，冒雨前来，协助社区布置活动现场，维护活动内外场秩序。\n非常感谢大家的鼎力支持，是你们的支持和付出，促成了 .NET西安社区的成立，也促成了西安地区第一次 .NET线下分享交流会的圆满成功！\n最后的最后，期待大家对 .NET西安社区持续关注，更期待大家线上、线下分享。\n下面是三位讲师的ppt下载链接。\n从csharp到fsharp.pptx\ndotNET在大数据和人工智能项目之中的应用实践.pptx\n生命周期指北.pptx"}
{"content2":"目前，人工智能（AI）非常热门，许多人都想一窥究竟。如果你对人工智能有所了解，但对机器学习（Machine Learning）的理解有很多的困惑，那么看完本文后你将会对此有进一步深入理解。在这里，不会详细介绍机器学习算法的基本原理，而是通过将比较有意思的视频（YouTube）和文字相结合，逐渐增加对机器学习基础的理解。\n当看到本文时，请坐下来放松一下。因为下面的这些视频需要花费一点时间，但看完视频后，你肯定会被吸引继续阅读下去。此外，当阅读完本文后，你应该会对现在最热门的技术——机器学习有了全面的知识基础，并对此产生学习热情，最终能学到什么程度完全取决于个人的努力，本文只是一块敲门砖。\n为什么机器学习现在如此热门\n人工智能总是很酷，从科幻电影到现实中的阿法狗、聊天机器人等，一直吸引人们的关注。长久以来，人们认为人工智能一直围绕着程序员对某些事情应该如何表现的功能性猜测。然而，程序员并不总是像我们经常看到的那样对人工智能编程同样有着天赋。正如我们经常看到的那样，比如谷歌“史诗游戏失败”中在人工智能、物理、有时甚至是经验丰富的人类玩家中都存在有过失。\n无论如何，人工智能有一种新的天赋——通过该项技术，我们可以教电脑玩游戏、理解语言、甚至识别人或物。这个只显露冰山一角的新技术来源一个旧的概念——机器学习，直到最近几年，它才获得了理论之外的处理能力，这源于数据量的爆炸、计算机性能的提升以及算法理论的突破。\n通过人工智能这项技术，我们不再需要人为地提出高级算法，只需要教会计算机自己来提出高级算法即可。\n那么这样的事情是如何实现的呢？机器学习算法并没有真正被类似于程序员编程那样进行编写，而是自动生成。观看下面这个简短的视频，该视频为创建人工智能的高级概念提供了出色的注释和动画。\n是不是一个很疯狂的处理过程？并且，当算法完成后，我们甚至无法理它，它就像一个黑匣子。比如，该项技术应用于视觉领域中是用人工智能玩马里奥游戏。作为一个人，我们都知道如何躲避障碍物和吃金币，但人工智能识别所产生的预测策略是疯狂的，见下面的视频：\n是不是很吃惊？看完上述视频后，我们的问题是对机器学习不了解，并且不知道如何将它与电子游戏联系起来。\n为什么要使用机器学习？\n关于为什么要关心机器学习，这里有两个很好的答案。\n首先，机器学习使计算机可以做到计算机以前不能实现的事情。如果你想尝试一些新事物，或者不仅仅是新事物，而是影响全世界，你都可以用机器学习来完成。\n其次，如果你不影响世界，世界将影响你。\n现在，很多大型公司在机器学习上投入了很多的研发和投资，我们已经看到它正在改变世界。思想领袖警告我们不能让这个新的算法时代存在于公众视线之外。想象一下，如果一些企业巨头控制着互联网，如果我们不掌握这项武器，科学的真理将不会被我们占据。Christian Heilmann在他关于机器学习的谈话中说得很好：\n“我们能够希望其他人善用这种力量。对于个人而言，不要认为这是一个好的赌注。我宁愿玩，也要参加这场科技革命，你也应该参与。”——Chris Heilmann的机器学习谈话视频\n对机器学习感兴趣\n机器学习这个概念很有用而且很酷，上述内容让我们比较抽象地了解了它，但机器学习算法究竟发生了什么？它是如何运作的？我们还不是很清楚。\n如果你想直接进入到理论研究，建议你跳过这一部分继续下一个“如何开始”部分。如果你有动力成为机器学习的实干者，那么就不需要看下面的视频了。如果你仍然试图了解机器学习可能是什么，下面的使用机器学习完成数字手写体识别的视频非常适合引导读者建立一种机器学习的逻辑：\n是不是很酷？该视频显示每个层变得更简单，而不是变得更复杂。就像函数将数据分解成较小的部分一样，以抽象的概念结束。你可以在该网站（Adam Harley）与此流程进行交互。\n此外，机器学习的经典实例之一是1936年的鸢尾花数据集。在参加JavaFXpert的机器学习概述的演示中，我学会了如何使用工具来可视化调整和反向传播神经网络上神经元的权重。可视化过程可以让我们看到它是如何训练神经模型。\n使用Jim可视化工具训练鸢尾花神经网络\n即使你不是一个Java爱好者，Jim提供了一个1.5小时的机器学习概念介绍也是比较有用的，其中包含上述许多例子的更多详细信息。\n这些概念令人兴奋，你准备好成为这个新时代的爱因斯坦吗？机器学习算法每天都在发生突破，所以现在就开始吧。\n如何开始？\n目前，网络上有大量的资源可用。首先，应该订阅一些时事通讯、技术博客、微信公众号，以保持个人知识的滚动。比如medium、爱可可-爱学习、云栖社区等。\n至于如何进行深入学习，我推荐下面两种方法：\n从头到尾拧完n颗螺栓\n在这种方法中，将需要你全面了解机器学习算法和相关的数学知识。我知道，这种方式听起来很难完成，但要想真正地了解算法细节，就必须从头开始编码实现。\n如果你想成为机器学习中的一员，并在核心圈中占据一席之地，那么这就是你的选择。我建议你试试一些公开课app（比如，course、Brilliant.org），并参加人工神经网络课程。经典的网络课程主推Andrew Ng 老师的机器学习课程以及周志华老师的书籍等。\n在学习的同时，可以完成对应的线下作业。通过完成对应的作业，会进一步加深对知识的理解，因为这些作业并不简单。但更重要的是，如果确实完成了这项工作，你将对机器学习的实施有进一步深刻的理解，这将使得你以新的和改变世界的方式成功地将其应用到对应的场景中。\n快速上手\n如果你对编写算法并不感兴趣，但仍想要使用它们来创建一个令人惊叹的网站/应用程序，你应该跳转到学习TensorFlow和对应的速成课程。\nTensorFlow是用于机器学习的开源软件库。\n如果选修课程不适合你的学习方式，那你仍然是很幸运的。如今不必学习机器学习的细节就可以掌握如何使用它。此外，还可以通过多种方式有效地机器学习作为服务成为技术巨头。\n数据是这项技术很重要的原材料，如果你的数据比较合适，那么使用机器学习建模可能是最佳解决方案。无论是使用机器学习中的哪一种算法，现在就开始吧。\n成为创造者\n我要对所有上述的人和视频说声谢谢，它们是我学习机器学习起步的灵感，虽然我在该领域仍然是个新手，但是当我们拥抱这个令人敬畏的时代时，我很高兴为他人指明一条学习道路。\n如果你想学习这门技术，就必须与机器学习领域的研究者有所联系。没有友好的面孔、回答和讨论，任何事情都将变得很难。一般技术圈的人都是比较热心肠的，遇到问题先google，找不到答案就咨询圈内人，相信会有友好的同行给出友好的建议。\n我希望这篇文章能激励你和周围的人学习机器学习，我也很乐意和你一起寻找酷炫有趣的机器学习代码，希望本文对你有所帮助。\n原文链接\n本文为云栖社区原创内容，未经允许不得转载。"}
{"content2":"感谢中国人民大学的胡鹤老师，课程容量巨大，收获颇丰。\n之前提到的CNN模型主要用到人类的视觉中枢，但其有一劣势，无论是人类的视觉神经还是听觉神经，所接受到的都是一个连续的序列，使用CNN相当于割裂了前后的联系。从而诞生了专门为处理序列的Recurrent Neural Network（RNN），每一个神经元除了当前信息的输入外，还有之前产生的记忆信息，保留序列依赖型。\n一、RNN基本原理\n如下图所示有两种表示方法，每张图片左边是RNN的神经元（称为memory cell），右边是按时间轴展开后的情况。每次输入两个信息输出两个信息，每轮处理hidden state。把同样神经元在时间上展开处理，比CNN更加节省参数，是一个相当高效的表示方法。\n可参考如下公式表示，最后简化后的形式同一般神经元相同，输入信息乘权重加偏值：\n由于t状态的t由t-1时候决定，因而具有记忆功能，也叫作memory cell（或cell）\n隐状态可由如下表示：\n隐状态是当前t时刻的状态，也由t-1时刻决定，简单情况下，hidden state等同于output（y），但大多较为复杂的cell中，它们并不相同。如下图所示：\n二、RNN种类：\n1. sequence-to-sequence：输入输出都是一个序列。例如股票预测中的RNN，输入是前N天价格，输出明天的股市价格。\n2. sequence-to-vector：输入是一个序列，输出单一向量。\n例如，输入一个电影评价序列，输出一个分数表示情感趋势（喜欢还是讨厌）。\n3. vector-to-sequence：输入单一向量，输出一个序列。\n4.Encoder-Decoder：输入sequence-to-vector，称作encoder，输出vector-to-sequence，称作decoder。\n这是一个delay模型，经过一段延迟，即把所有输入都读取后，在decoder中获取输入并输出一个序列。这个模型在机器翻译中使用较广泛，源语言输在入放入encoder，浓缩在状态信息中，生成目标语言时，可以生成一个不长度的目标语言序列。\n三、RNN实例\n1.手动实现\n以下是一个手动实现RNN的实例\nn_inputs = 3 # hidden state n_neurons = 5 X0 = tf.placeholder(tf.float32, [None, n_inputs]) X1 = tf.placeholder(tf.float32, [None, n_inputs]) # 由于Wx要和X相乘，故低维是n_inputs Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons],dtype=tf.float32)) # 低维，高维都是n_neurons，为了使得输出也是hidden state的深度 # 这样下一次才可以继续运算 Wy = tf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32)) b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32)) # Y0初始化为0，初始时没有记忆 Y0 = tf.tanh(tf.matmul(X0, Wx) + b) # 把上一轮输出Y0也作为输入 Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b) init = tf.global_variables_initializer() import numpy as np X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0 X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1 with tf.Session() as sess: 　　init.run() 　　Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch}) # Y0，Y1都是4*5大小，4是mini-batch数目，5是输出神经元个数\nTensorFlow函数集成后实现\n2.static unrolling through time\nstatic_rnn()是使用链式cells实现一个按时间轴展开的RNN\n# 这种和上面那种手动实现的效果相同 n_inputs = 3 n_neurons = 5 X0 = tf.placeholder(tf.float32, [None, n_inputs]) X1 = tf.placeholder(tf.float32, [None, n_inputs]) basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32) Y0, Y1 = output_seqs # run部分 init = tf.global_variables_initializer() X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) with tf.Session() as sess: init.run() Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})\npacking sequence\nn_steps = 2 n_inputs = 3 n_neurons = 5 # 输入是一个三维tensor，none是mini-batch大小不限，n_steps是序列长度 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # 把一个高维度n的tensor展开成一个n-1维，降维，这里是3位降到2维列表 # unstack之前要做一个1,2维转置，相当于构造了n_steps个数的列表 X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2])) basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) # states是最新状态 output_seqs, states = tf.contrib.rnn.static_rnn( basic_cell, X_seqs, dtype=tf.float32) # 再做一个转置，和输入对应 outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2]) # 输入大小4*2*3 X_batch = np.array([ # t = 0　　　　 t = 1 [[0, 1, 2], [9, 8, 7]], # instance 0 [[3, 4, 5], [0, 0, 0]], # instance 1 [[6, 7, 8], [6, 5, 4]], # instance 2 [[9, 0, 1], [3, 2, 1]], # instance 3 ]) with tf.Session() as sess: init.run() outputs_val = outputs.eval(feed_dict={X: X_batch}) # output_val是一个4*2*5，仅输出维度神经元个数改变\n3. dynamic RNN\n本身支持高维tensor输入，内嵌一个循环运行足够多次数的cell，不需要unstack步骤。\n这个内嵌循环while_loop()在前向传播中将每次迭代的tensor值存储下来，以便于反向传播过程中使用其计算梯度值。\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # 动态RNN内部封装一个循环 # 根据输入，动态决定自己需要展开几次 basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\ndynamicRNN可以动态规定输入大小（就像句子输入）\nn_steps = 2 n_inputs = 3 n_neurons = 5 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) seq_length = tf.placeholder(tf.int32, [None]) outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length) init = tf.global_variables_initializer() # X_batch的大小4*2*3 X_batch = np.array([ # step 0 step 1 [[0, 1, 2], [9, 8, 7]], # instance 1 [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors) [[6, 7, 8], [6, 5, 4]], # instance 3 [[9, 0, 1], [3, 2, 1]], # instance 4 ]) # 这里设置sequence大小，一共4个batch，第二维上只取第一个 seq_length_batch = np.array([2, 1, 2, 2]) with tf.Session() as sess: init.run() outputs_val, states_val = sess.run( [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})\n如果事先不知道输出序列的长度，就需要定义一个end-of-sequence token（eos token），无论是课上还是网上相关信息都很少，这里就不展开了。。\n四、RNN训练\n1. 拟合分类\nRNN比较难以训练，单是如下图的节点中，cost function就包含y2,y3,y4三个输出，往回回溯。\n以下MINIST中使用150个RNN神经元，最后加一个全连接层，得到10个神经元的输出（分别对应0-9），最后看对应在\nfrom tensorflow.contrib.layers import fully_connected n_steps = 28 n_inputs = 28 n_neurons = 150 n_outputs = 10 learning_rate = 0.001 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # 一维输出 y = tf.placeholder(tf.int32, [None]) # 使用最简单的basicRNNcell basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) #使用dynamic_rnn outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32) # 原始输出 logits = fully_connected(states, n_outputs, activation_fn=None) # 计算和真实的交叉熵 xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy) # 使用AdamOptimizer optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) # 计算准确率，只有等于y才是对的，其他都错 correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) init = tf.global_variables_initializer() from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\") # 转换到合理的输入shape X_test = mnist.test.images.reshape((-1, n_steps, n_inputs)) y_test = mnist.test.labels # run100遍，每次处理150个输入 n_epochs = 100 batch_size = 150 # 开始循环 with tf.Session() as sess: init.run() for epoch in range(n_epochs): for iteration in range(mnist.train.num_examples // batch_size): # 读入数据并reshape X_batch, y_batch = mnist.train.next_batch(batch_size) X_batch = X_batch.reshape((-1, n_steps, n_inputs)) # X大写，y小写 sess.run(training_op, feed_dict={X: X_batch, y: y_batch}) acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch}) acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test}) # 每次打印一下当前信息 print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n以下，只用了150个参数，做了单层。就可以达到非常高的效果，可以看出rnn效果非常不错\n序列预测，前20个状态作为输入，则第2个到21个作为输出，作为训练集\n# 输入x0-x19 n_steps = 20 # 只预测一个值 n_inputs = 1 # rnn有100个 n_neurons = 100 n_outputs = 1 # none表示min_batch大小这里任意 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) y = tf.placeholder(tf.float32, [None, n_steps, n_outputs]) cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu) outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n如上代码中，每次输出的vector都是100维的，加入一个output rejections后，使得每次只输出1个值\noutput rejection实现代码如下\n# 设置输出为上面设定的n_outputs大小 cell = tf.contrib.rnn.OutputProjectionWrapper( tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu), output_size=n_outputs) learning_rate = 0.001 loss = tf.reduce_mean(tf.square(outputs - y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) init = tf.global_variables_initializer() # 开始训练 n_iterations = 10000 batch_size = 50 with tf.Session() as sess: init.run() for iteration in range(n_iterations): X_batch, y_batch = [...] # fetch the next training batch sess.run(training_op, feed_dict={X: X_batch, y: y_batch}) if iteration % 100 == 0: mse = loss.eval(feed_dict={X: X_batch, y: y_batch}) print(iteration, \"\\tMSE:\", mse)\n2.预测\n当一个RNN训练好后，它就可以生成很多新的东西。RNN的强大的生成能力非常有魅力，用很多曲子去训练它，它就可以生成新的曲子，用很多文章训练它，他就可以生成新的文章。如果可以训练出功能非常强的RNN模型，就有可能代替人的工作。\nwith tf.Session() as sess: # 导入训练好的模型 saver.restore(sess, \"./my_time_series_model\") # 生成新的曲线 sequence = [0.] * n_steps for iteration in range(300): X_batch = np.array(sequence[-n_steps:]).reshape(1, n_steps, 1) y_pred = sess.run(outputs, feed_dict={X: X_batch}) sequence.append(y_pred[0, -1, 0])\nRNN也可以不断叠加，形成很深的网络，如下图所示，每一层输出都反馈到当前位置的输入，时间轴展开后，如右边所示。\nn_inputs = 2 n_steps = 5 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) n_neurons = 100 n_layers = 3 # 做了3层rnn # 模型不是越复杂越好，越复杂所需数据量越大，否则会有过拟合的风险 # 可以加dropout来控制 layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) for layer in range(n_layers)] multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers) outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32) init = tf.global_variables_initializer() X_batch = np.random.rand(2, n_steps, n_inputs) with tf.Session() as sess: init.run() outputs_val, states_val = sess.run([outputs, states], feed_dict={X: X_batch})\n五、困难及优化\n反向训练时，对于RNN来说，要横向往前推，一直往前推到序列开始的地方。当序列非常长时，梯度消失，梯度爆炸都与路径长度太长有关，前面的权重都基本固定不变，没有训练效果。\n为了解决这个困难，有了很多更复杂RNN模型的提出\n1.LSTM（Long Short Term Memory）\n97年提出，直到深度学习提出，使用LSTM做出具体实事后，才火起来。或许是因为现在有大数据的环境，以及训练能力很强的硬件这些客观条件得具备，才能真正发挥LSTM的威力。\n它把训练信息分为长期记忆（c）和短期记忆（h），上面的长期记忆信息，可以穿到很远，即使序列长到1000，也可以向前传导。\n它分了很多个门（gate），输出信息趋近于0，门关闭，趋近于1门打开。i是输入门控制新输入加多少到长期记忆中，f是forget控制是否受长期记忆的影响，哪些长期记忆被忘掉，o是输出门控制哪些长期记忆可以输出并作为短期记忆ht传递下去，通过这3个门控制信息的流动。\n可以保证长期记忆变换的缓慢，相对稳定，可以对距离比较远的序列影响，ht和ht-1可以看到距离也比较远，短期记忆ht-1变化明显。\n# TensorFlow中LSTM具体实现 n_steps = 28 n_inputs = 28 n_neurons = 150 n_outputs = 10 n_layers = 3 learning_rate = 0.001 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) y = tf.placeholder(tf.int32, [None]) lstm_cells = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons) for layer in range(n_layers)] multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells) outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32) top_layer_h_state = states[-1][1] logits = tf.layers.dense(top_layer_h_state, n_outputs, name=\"softmax\") xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name=\"loss\") optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) init = tf.global_variables_initializer()\nLSTM还有一点改进Peephole Connection\nlstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)\n2. GRU（Gated recurrent unit）\nGRU是对LSTM简化后的版本，去掉了长短期记忆的区分（都是h），减少了几个门，2014年提出，从参数上来说较LSTM简单些。\n统一用update gate控制原来的i门和f门。z趋近于0就用ht-1来更新，趋近于1就取当前输入。\n比LSTM还少一个矩阵乘法，实际表现不比LSTM差，也成为现在很多研究者越来越看重的方法。\n调用时，直接调用GRU cell即可\ngru_cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)\n六、RNN在NLP（natural language processing）中的应用\nRNN的输入原本是one-hot的表示，但这样会使得输入极其稀疏，不好训练。于是将高维空间映射到低维（如100维）空间，用这个低维嵌入的输入做训练，非常有效。\nWord Embeddings\n相同含义的词在低维空间中距离近，含义差的多的离得远。\n# 把50000维数据映射到150维数据空间上 vocabulary_size = 50000 embedding_size = 150 # 做一个全连接 embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) train_inputs = tf.placeholder(tf.int32, shape=[None]) # from ids embed = tf.nn.embedding_lookup(embeddings, train_inputs)# to embd\n例如下图所示，把要翻译的英文句子做输入，用训练后的状态值做输入，和法语作为训练集的作为decoder输入。第一位放一个起始信号<go>，输出和输入刚好错一位，最后一位以一个结束标识<eos>结束。这样做是为了后继应用时，翻译新句子没有training label，只有英文输入。把英文输入放进来后加一个<go>，得到第一个je输出，把第一个词放进来得到第二个输出bois。。最后<eos>翻译结束。\n最后实际应用时如下，输入<go>开始翻译。"}
{"content2":"一、二次代价函数\n1. 形式：\n其中，C为代价函数，X表示样本，Y表示实际值，a表示输出值，n为样本总数\n2. 利用梯度下降法调整权值参数大小，推导过程如下图所示：\n根据结果可得，权重w和偏置b的梯度跟激活函数的梯度成正比（即激活函数的梯度越大，w和b的大小调整的越快，训练速度也越快）\n3. 激活函数是sigmoid函数时，二次代价函数调整参数过程分析\n理想调整参数状态：距离目标点远时，梯度大，参数调整较快；距离目标点近时，梯度小，参数调整较慢。\n如果我的目标点是调整到M点，从A点==>B点的调整过程，A点距离目标点远，梯度大，调整参数较快；B点距离目标较近，梯度小，调整参数慢。符合参数调整策略\n如果我的目标点是调整到N点，从B点==>A点的调整过程，A点距离目标点近，梯度大，调整参数较快；B点距离目标较远，梯度小，调整参数慢。不符合参数调整策略\n二、交叉熵代价函数\n1.形式：\n其中，C为代价函数，X表示样本，Y表示实际值，a表示输出值，n为样本总数\n2. 利用梯度下降法调整权值参数大小，推导过程如下图所示：\n根据结果可得，权重w和偏置b的梯度跟激活函数的梯度无关。而和输出值与实际值的误差成正比（即误差越大，w和b的大小调整的越快，训练速度也越快）\n3.激活函数是sigmoid函数时，二次代价函数调整参数过程分析\n理想调整参数状态：距离目标点远时，梯度大，参数调整较快；距离目标点近时，梯度小，参数调整较慢。\n如果我的目标点是调整到M点，从A点==>B点的调整过程，A点距离目标点远，误差大，调整参数较快；B点距离目标较近，误差小，调整参数较慢。符合参数调整策略\n如果我的目标点是调整到N点，从B点==>A点的调整过程，A点距离目标点近，误差小，调整参数较慢；B点距离目标较远，误差大，调整参数较快。符合参数调整策略\n总结：\n如果输出神经元是线性的，选择二次代价函数较为合适\n如果输出神经元是S型函数（sigmoid函数），选择交叉熵代价函数较为合适\n如果输出神经元是softmax回归的代价函数，选择对数释然代价函数较为合适\n二、利用代价函数优化MNIST数据集识别程序\n1.在Tensorflow中代价函数的选择：\n如果输出神经元是线性的，选择二次代价函数较为合适 loss = tf.reduce_mean(tf.square())\n如果输出神经元是S型函数（sigmoid函数），选择交叉熵代价函数较为合适 loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits())\n如果输出神经元是softmax回归的代价函数，选择对数释然代价函数较为合适 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits())\n2.通过代价函数选择对MNIST数据集分类程序优化\n#使用交叉熵代价函数\n1 import os 2 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 3 import tensorflow as tf 4 from tensorflow.examples.tutorials.mnist import input_data 5 #载入数据集 6 mnist = input_data.read_data_sets('MNIST_data', one_hot=True) 7 #每个批次的大小（即每次训练的图片数量） 8 batch_size = 50 9 #计算一共有多少个批次 10 n_bitch = mnist.train.num_examples // batch_size 11 #定义两个placeholder 12 x = tf.placeholder(tf.float32, [None, 784]) 13 y = tf.placeholder(tf.float32, [None, 10]) 14 #创建一个只有输入层（784个神经元）和输出层（10个神经元）的简单神经网络 15 Weights = tf.Variable(tf.zeros([784, 10])) 16 Biases = tf.Variable(tf.zeros([10])) 17 Wx_plus_B = tf.matmul(x, Weights) + Biases 18 prediction = tf.nn.softmax(Wx_plus_B) 19 #交叉熵代价函数 20 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) 21 #使用梯度下降法 22 train_step = tf.train.GradientDescentOptimizer(0.15).minimize(loss) 23 #初始化变量 24 init = tf.global_variables_initializer() 25 #结果存放在一个布尔型列表中 26 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax返回一维张量中最大的值所在的位置，标签值和预测值相同，返回为True 27 #求准确率 28 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #cast函数将correct_prediction的布尔型转换为浮点型，然后计算平均值即为准确率 29 30 with tf.Session() as sess: 31 sess.run(init) 32 #将测试集循环训练20次 33 for epoch in range(21): 34 #将测试集中所有数据循环一次 35 for batch in range(n_bitch): 36 batch_xs, batch_ys = mnist.train.next_batch(batch_size) #取测试集中batch_size数量的图片及对应的标签值 37 sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys}) #将上一行代码取到的数据进行训练 38 acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}) #准确率的计算 39 print('Iter : ' + str(epoch) + ',Testing Accuracy = ' + str(acc))\nView Code\n#执行结果\n1 Iter : 0,Testing Accuracy = 0.8323 2 Iter : 1,Testing Accuracy = 0.8947 3 Iter : 2,Testing Accuracy = 0.9032 4 Iter : 3,Testing Accuracy = 0.9068 5 Iter : 4,Testing Accuracy = 0.909 6 Iter : 5,Testing Accuracy = 0.9105 7 Iter : 6,Testing Accuracy = 0.9126 8 Iter : 7,Testing Accuracy = 0.9131 9 Iter : 8,Testing Accuracy = 0.9151 10 Iter : 9,Testing Accuracy = 0.9168 11 Iter : 10,Testing Accuracy = 0.9178 12 Iter : 11,Testing Accuracy = 0.9173 13 Iter : 12,Testing Accuracy = 0.9181 14 Iter : 13,Testing Accuracy = 0.9194 15 Iter : 14,Testing Accuracy = 0.9201 16 Iter : 15,Testing Accuracy = 0.9197 17 Iter : 16,Testing Accuracy = 0.9213 18 Iter : 17,Testing Accuracy = 0.9212 19 Iter : 18,Testing Accuracy = 0.9205 20 Iter : 19,Testing Accuracy = 0.9215\nView Code\n#使用二次代价函数\n1 import os 2 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 3 import tensorflow as tf 4 from tensorflow.examples.tutorials.mnist import input_data 5 #载入数据集 6 mnist = input_data.read_data_sets('MNIST_data', one_hot=True) 7 #每个批次的大小（即每次训练的图片数量） 8 batch_size = 100 9 #计算一共有多少个批次 10 n_bitch = mnist.train.num_examples // batch_size 11 #定义两个placeholder 12 x = tf.placeholder(tf.float32, [None, 784]) 13 y = tf.placeholder(tf.float32, [None, 10]) 14 #创建一个只有输入层（784个神经元）和输出层（10个神经元）的简单神经网络 15 Weights = tf.Variable(tf.zeros([784, 10])) 16 Biases = tf.Variable(tf.zeros([10])) 17 Wx_plus_B = tf.matmul(x, Weights) + Biases 18 prediction = tf.nn.softmax(Wx_plus_B) 19 #二次代价函数 20 loss = tf.reduce_mean(tf.square(y - prediction)) 21 #使用梯度下降法 22 train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss) 23 #初始化变量 24 init = tf.global_variables_initializer() 25 #结果存放在一个布尔型列表中 26 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax返回一维张量中最大的值所在的位置，标签值和预测值相同，返回为True 27 #求准确率 28 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #cast函数将correct_prediction的布尔型转换为浮点型，然后计算平均值即为准确率 29 30 with tf.Session() as sess: 31 sess.run(init) 32 #将测试集循环训练20次 33 for epoch in range(21): 34 #将测试集中所有数据循环一次 35 for batch in range(n_bitch): 36 batch_xs, batch_ys = mnist.train.next_batch(batch_size) #取测试集中batch_size数量的图片及对应的标签值 37 sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys}) #将上一行代码取到的数据进行训练 38 acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}) #准确率的计算 39 print('Iter : ' + str(epoch) + ',Testing Accuracy = ' + str(acc))\nView Code\n#执行结果\n1 Iter : 0,Testing Accuracy = 0.8325 2 Iter : 1,Testing Accuracy = 0.8711 3 Iter : 2,Testing Accuracy = 0.8831 4 Iter : 3,Testing Accuracy = 0.8876 5 Iter : 4,Testing Accuracy = 0.8942 6 Iter : 5,Testing Accuracy = 0.898 7 Iter : 6,Testing Accuracy = 0.9002 8 Iter : 7,Testing Accuracy = 0.9014 9 Iter : 8,Testing Accuracy = 0.9036 10 Iter : 9,Testing Accuracy = 0.9052 11 Iter : 10,Testing Accuracy = 0.9065 12 Iter : 11,Testing Accuracy = 0.9073 13 Iter : 12,Testing Accuracy = 0.9084 14 Iter : 13,Testing Accuracy = 0.909 15 Iter : 14,Testing Accuracy = 0.9095 16 Iter : 15,Testing Accuracy = 0.9115 17 Iter : 16,Testing Accuracy = 0.912 18 Iter : 17,Testing Accuracy = 0.9126 19 Iter : 18,Testing Accuracy = 0.913 20 Iter : 19,Testing Accuracy = 0.9136 21 Iter : 20,Testing Accuracy = 0.914\nView Code\n结论：（二者只有代价函数不同）\n正确率达到90%所用迭代次数：使用交叉熵代价函数为第三次；使用二次代价函数为第六次（在MNIST数据集分类中，使用交叉熵代价函数收敛速度较快）\n最终正确率：使用交叉熵代价函数为92.15%，使用二次代价函数为91.4%（在MNIST数据集分类中，使用交叉熵代价函数识别准确率较高）\n三、拟合问题\n参考文章：\nhttps://blog.csdn.net/willduan1/article/details/53070777\n1.根据拟合结果分类：\n欠拟合：模型没有很好地捕捉到数据特征，不能够很好地拟合数据\n正确拟合\n过拟合：模型把数据学习的太彻底，以至于把噪声数据的特征也学习到了，这样就会导致在后期测试的时候不能够很好地识别数据，即不能正确的分类，模型泛化能力太差\n2.解决欠拟合和过拟合\n解决欠拟合常用方法：\n添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。\n添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。\n减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n解决过拟合常用方法：\n增加数据集\n正则化方法\nDropout（通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作）\n四、初始化优化MNIST数据集分类问题\n#改变初始化方法\nWeights = tf.Variable(tf.truncated_normal([784, 10])) Biases = tf.Variable(tf.zeros([10]) + 0.1)\n五、优化器优化MNIST数据集分类问题\n大多数机器学习任务就是最小化损失，在损失定义的情况下，后面的工作就交给优化器。\n因为深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化。\n1.梯度下降法分类及其介绍\n标准梯度下降法：先计算所有样本汇总误差，然后根据总误差来更新权值\n随机梯度下降法：随机抽取一个样本来计算误差，然后更新权值\n批量梯度下降法：是一种折中方案，从总样本中选取一个批次（batch），然后计算这个batch的总误差，根据总误差来更新权值\n2.常见优化器介绍\n参考文章：\nhttps://www.leiphone.com/news/201706/e0PuNeEzaXWsMPZX.html\n3.优化器优化MNIST数据集分类问题\n#选择Adam优化器\n1 import os 2 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 3 import tensorflow as tf 4 from tensorflow.examples.tutorials.mnist import input_data 5 #载入数据集 6 mnist = input_data.read_data_sets('MNIST_data', one_hot=True) 7 #每个批次的大小（即每次训练的图片数量） 8 batch_size = 50 9 #计算一共有多少个批次 10 n_bitch = mnist.train.num_examples // batch_size 11 #定义两个placeholder 12 x = tf.placeholder(tf.float32, [None, 784]) 13 y = tf.placeholder(tf.float32, [None, 10]) 14 #创建一个只有输入层（784个神经元）和输出层（10个神经元）的简单神经网络 15 Weights = tf.Variable(tf.zeros([784, 10])) 16 Biases = tf.Variable(tf.zeros([10])) 17 Wx_plus_B = tf.matmul(x, Weights) + Biases 18 prediction = tf.nn.softmax(Wx_plus_B) 19 #交叉熵代价函数 20 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) 21 #使用Adam优化器 22 train_step = tf.train.AdamOptimizer(1e-2).minimize(loss) 23 #初始化变量 24 init = tf.global_variables_initializer() 25 #结果存放在一个布尔型列表中 26 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax返回一维张量中最大的值所在的位置，标签值和预测值相同，返回为True 27 #求准确率 28 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #cast函数将correct_prediction的布尔型转换为浮点型，然后计算平均值即为准确率 29 30 with tf.Session() as sess: 31 sess.run(init) 32 #将测试集循环训练20次 33 for epoch in range(21): 34 #将测试集中所有数据循环一次 35 for batch in range(n_bitch): 36 batch_xs, batch_ys = mnist.train.next_batch(batch_size) #取测试集中batch_size数量的图片及对应的标签值 37 sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys}) #将上一行代码取到的数据进行训练 38 acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}) #准确率的计算 39 print('Iter : ' + str(epoch) + ',Testing Accuracy = ' + str(acc))\nView Code\n#执行结果\nIter : 1,Testing Accuracy = 0.9224 Iter : 2,Testing Accuracy = 0.9293 Iter : 3,Testing Accuracy = 0.9195 Iter : 4,Testing Accuracy = 0.9282 Iter : 5,Testing Accuracy = 0.926 Iter : 6,Testing Accuracy = 0.9291 Iter : 7,Testing Accuracy = 0.9288 Iter : 8,Testing Accuracy = 0.9274 Iter : 9,Testing Accuracy = 0.9277 Iter : 10,Testing Accuracy = 0.9249 Iter : 11,Testing Accuracy = 0.9313 Iter : 12,Testing Accuracy = 0.9301 Iter : 13,Testing Accuracy = 0.9315 Iter : 14,Testing Accuracy = 0.9295 Iter : 15,Testing Accuracy = 0.9299 Iter : 16,Testing Accuracy = 0.9303 Iter : 17,Testing Accuracy = 0.93 Iter : 18,Testing Accuracy = 0.9304 Iter : 19,Testing Accuracy = 0.9269 Iter : 20,Testing Accuracy = 0.9273\nView Code\n注意：不同优化器参数的设置是关键。在机器学习中，参数的调整应该是技术加经验，而不是盲目调整。这边是我以后需要学习和积累的地方\n六、根据今天所学内容，对MNIST数据集分类进行优化，准确率达到95%以上\n#优化程序\n1 import os 2 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 3 import tensorflow as tf 4 from tensorflow.examples.tutorials.mnist import input_data 5 #载入数据集 6 mnist = input_data.read_data_sets('MNIST_data', one_hot=True) 7 #每个批次的大小（即每次训练的图片数量） 8 batch_size = 50 9 #计算一共有多少个批次 10 n_bitch = mnist.train.num_examples // batch_size 11 #定义两个placeholder 12 x = tf.placeholder(tf.float32, [None, 784]) 13 y = tf.placeholder(tf.float32, [None, 10]) 14 #创建一个只有输入层（784个神经元）和输出层（10个神经元）的简单神经网络 15 Weights1 = tf.Variable(tf.truncated_normal([784, 200])) 16 Biases1 = tf.Variable(tf.zeros([200]) + 0.1) 17 Wx_plus_B_L1 = tf.matmul(x, Weights1) + Biases1 18 L1 = tf.nn.tanh(Wx_plus_B_L1) 19 20 Weights2 = tf.Variable(tf.truncated_normal([200, 50])) 21 Biases2 = tf.Variable(tf.zeros([50]) + 0.1) 22 Wx_plus_B_L2 = tf.matmul(L1, Weights2) + Biases2 23 L2 = tf.nn.tanh(Wx_plus_B_L2) 24 25 Weights3 = tf.Variable(tf.truncated_normal([50, 10])) 26 Biases3 = tf.Variable(tf.zeros([10]) + 0.1) 27 Wx_plus_B_L3 = tf.matmul(L2, Weights3) + Biases3 28 prediction = tf.nn.softmax(Wx_plus_B_L3) 29 30 #交叉熵代价函数 31 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) 32 #使用梯度下降法 33 train_step = tf.train.AdamOptimizer(2e-3).minimize(loss) 34 #初始化变量 35 init = tf.global_variables_initializer() 36 #结果存放在一个布尔型列表中 37 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) 38 #求准确率 39 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 40 41 with tf.Session() as sess: 42 sess.run(init) 43 #将测试集循环训练50次 44 for epoch in range(51): 45 #将测试集中所有数据循环一次 46 for batch in range(n_bitch): 47 batch_xs, batch_ys = mnist.train.next_batch(batch_size) #取测试集中batch_size数量的图片及对应的标签值 48 sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys}) #将上一行代码取到的数据进行训练 49 test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}) #准确率的计算 50 print('Iter : ' + str(epoch) + ',Testing Accuracy = ' + str(test_acc))\nView Code\n#执行结果\n1 Iter : 0,Testing Accuracy = 0.6914 2 Iter : 1,Testing Accuracy = 0.7236 3 Iter : 2,Testing Accuracy = 0.8269 4 Iter : 3,Testing Accuracy = 0.8885 5 Iter : 4,Testing Accuracy = 0.9073 6 Iter : 5,Testing Accuracy = 0.9147 7 Iter : 6,Testing Accuracy = 0.9125 8 Iter : 7,Testing Accuracy = 0.922 9 Iter : 8,Testing Accuracy = 0.9287 10 Iter : 9,Testing Accuracy = 0.9248 11 Iter : 10,Testing Accuracy = 0.9263 12 Iter : 11,Testing Accuracy = 0.9328 13 Iter : 12,Testing Accuracy = 0.9316 14 Iter : 13,Testing Accuracy = 0.9387 15 Iter : 14,Testing Accuracy = 0.9374 16 Iter : 15,Testing Accuracy = 0.9433 17 Iter : 16,Testing Accuracy = 0.9419 18 Iter : 17,Testing Accuracy = 0.9379 19 Iter : 18,Testing Accuracy = 0.9379 20 Iter : 19,Testing Accuracy = 0.9462 21 Iter : 20,Testing Accuracy = 0.9437 22 Iter : 21,Testing Accuracy = 0.9466 23 Iter : 22,Testing Accuracy = 0.9479 24 Iter : 23,Testing Accuracy = 0.9498 25 Iter : 24,Testing Accuracy = 0.9481 26 Iter : 25,Testing Accuracy = 0.9489 27 Iter : 26,Testing Accuracy = 0.9496 28 Iter : 27,Testing Accuracy = 0.95 29 Iter : 28,Testing Accuracy = 0.9508 30 Iter : 29,Testing Accuracy = 0.9533 31 Iter : 30,Testing Accuracy = 0.9509 32 Iter : 31,Testing Accuracy = 0.9516 33 Iter : 32,Testing Accuracy = 0.9541 34 Iter : 33,Testing Accuracy = 0.9513 35 Iter : 34,Testing Accuracy = 0.951 36 Iter : 35,Testing Accuracy = 0.9556 37 Iter : 36,Testing Accuracy = 0.9527 38 Iter : 37,Testing Accuracy = 0.9521 39 Iter : 38,Testing Accuracy = 0.9546 40 Iter : 39,Testing Accuracy = 0.9544 41 Iter : 40,Testing Accuracy = 0.9555 42 Iter : 41,Testing Accuracy = 0.9546 43 Iter : 42,Testing Accuracy = 0.9553 44 Iter : 43,Testing Accuracy = 0.9534 45 Iter : 44,Testing Accuracy = 0.9576 46 Iter : 45,Testing Accuracy = 0.9535 47 Iter : 46,Testing Accuracy = 0.9569 48 Iter : 47,Testing Accuracy = 0.9556 49 Iter : 48,Testing Accuracy = 0.9568 50 Iter : 49,Testing Accuracy = 0.956 51 Iter : 50,Testing Accuracy = 0.9557\nView Code\n#写在后面\n呀呀呀呀\n本来想着先把python学差不多再开始机器学习和这些框架的学习\n老师触不及防的任务\n给了论文 让我搭一个模型出来\n我只能硬着头皮上了\n不想用公式编译器了\n手写版计算过程  请忽略那丑丑的字儿\n加油哦！小伙郭"}
{"content2":"facenet官网\nhttps://github.com/davidsandberg/facenet\n我们就是用facenet算法来实现人脸比对，需要详细了解这个模块的可以去参考上面的网址\n首先我们去这个网址去下载文件\n下载完数据情况为下面一样：\n#一个是模型，一个是数据\nx.jpg和y.jpg是我事先放在里面要比对的图片\n然后我们要配置内置变量\n到时候运行的话要在黑框下运行，就是命令窗口\n然后直接运行一下就可以了，大于1的不是本人，小于1的话是本人，这就是欧式距离\n这个是在命令窗口下运行的，想要把它运行在脚本的话我们把里面的compare.py这个文件的内容复制出来放在你的项目上，然后封装一下就可以调用并且使用了"}
{"content2":"导语：\nScience is NOT a battle, it is a collaboration. We all build on each other's ideas. Science is an act of love, not war. Love for the beauty in the world that surr ounds us and love to share and build something together. That makes science a highly satisfying activity, emotionally speaking!\n——Yoshua Bengio\n人工智能的浪潮正席卷全球，诸多词汇时刻萦绕在我们的耳边，如人工智能，机器学习，深度学习等。“人工智能”的概念早在1956年就被提出，顾名思义用计算机来构造复杂的，拥有与人类智慧同样本质特性的机器。经过几十年的发展，在2012年后，得益于数据量的上涨，运算力的提升和机器学习算法(深度学习)的出现，人工智能开始大爆发。但目前的科研工作都集中在弱人工智能部分，即让机器具备观察和感知能力，可以一定程度的理解和推理，预期在该领域能够取得一些重大突破。电影里的人工智能多半都是在描绘强人工智能，即让机器获得自适应能力，解决一些之前还没遇到过的问题，而这部分在目前的现实世界里难以真正实现。\n弱人工智能有希望取得突破，是如何实现的，“智能”又从何而来呢？这主要归功于一种实现人工智能的方法——机器学习。\n一、机器学习概念\n机器学习是一种实现人工智能的方法。\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。\n二、深度学习概念\n深度学习是一种实现机器学习的技术。\n深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络），因此越来越多的人将其单独看作一种学习的方法。\n最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。其实有不少想法早年间也曾有过，但由于当时训练数据量不足、计算能力落后，因此最终的效果不尽如人意。\n深度学习，作为目前最热的机器学习方法，但并不意味着是机器学习的终点。起码目前存在以下问题：\n1)        深度学习模型需要大量的训练数据，才能展现出神奇的效果，但现实生活中往往会遇到小样本问题，此时深度学习方法无法入手，传统的机器学习方法就可以处理；\n2)        有些领域，采用传统的简单的机器学习方法，可以很好地解决了，没必要非得用复杂的深度学习方法；\n3)        深度学习的思想，来源于人脑的启发，但绝不是人脑的模拟。\n因此，机器学习框架和深度学习框架之间也是有区别的。本质上，机器学习框架涵盖用于分类，回归，聚类，异常检测和数据准备的各种学习方法，并且其可以或可以不包括神经网络方法。深度学习或深度神经网络（DNN）框架涵盖具有许多隐藏层的各种神经网络拓扑。这些层包括模式识别的多步骤过程。网络中的层越多，可以提取用于聚类和分类的特征越复杂。我们常见的Caffe，CNTK，DeepLearning4j，Keras，MXNet和TensorFlow是深度学习框架。 Scikit-learning和Spark MLlib是机器学习框架。 Theano跨越了这两个类别。\n本文接下来的篇幅将会重点介绍深度学习的三个框架caffe、tensorflow和keras，如果只是需要使用传统的机器学习基础算法使用scikit-learning和spark MLlib则更为合适。\n三、深度学习框架比较\n神经网络一般包括：训练，测试两大阶段。训练：就是把训练数据（原料）和神经网络模型：如AlexNet、RNN等“倒进” 神经网络训练框架例如cafffe等然后用 CPU或GPU（真火） “提炼出” 模型参数（仙丹）的过程。测试：就是把测试数据用训练好的模型（神经网络模型 + 模型参数）跑一跑看看结果如何，作为炼丹炉caffe，keras，tensorflow就是把炼制过程所涉及的概念做抽象，形成一套体系。\n（一）Caffe\n1、概念介绍\nCaffe是一个清晰而高效的深度学习框架，也是一个被广泛使用的开源深度学习框架，在Tensorflow出现之前一直是深度学习领域Github star最多的项目。\nCaffe的主要优势为：容易上手，网络结构都是以配置文件形式定义，不需要用代码设计网络。训练速度快，组件模块化，可以方便的拓展到新的模型和学习任务上。但是Caffe最开始设计时的目标只针对于图像，没有考虑文本、语音或者时间序列的数据，因此Caffe对卷积神经网络的支持非常好，但是对于时间序列RNN，LSTM等支持的不是特别充分。caffe工程的models文件夹中常用的网络模型比较多，比如Lenet、AlexNet、ZFNet、VGGNet、GoogleNet、ResNet等。\n2、Caffe的模块结构\n总的来讲，由低到高依次把网络中的数据抽象成Blob, 各层网络抽象成 Layer ，整个网络抽象成Net，网络模型的求解方法 抽象成 Solver。\n1)       Blob 主要用来表示网络中的数据，包括训练数据，网络各层自身的参数，网络之间传递的数据都是通过 Blob 来实现的，同时 Blob 数据也支持在 CPU 与 GPU 上存储，能够在两者之间做同步。\n2)       Layer 是对神经网络中各种层的一个抽象，包括我们熟知的卷积层和下采样层，还有全连接层和各种激活函数层等等。同时每种 Layer 都实现了前向传播和反向传播，并通过 Blob 来传递数据。\n3)       Net 是对整个网络的表示，由各种 Layer 前后连接组合而成，也是我们所构建的网络模型。\n4)       Solver 定义了针对 Net 网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义 Solver 能够实现不同的网络求解方式。\n3、安装方式\nCaffe 需要预先安装比较多的依赖项，CUDA，snappy，leveldb，gflags，glog，szip，lmdb，OpenCV，hdf5，BLAS，boost等等\nCaffe官网：http://caffe.berkeleyvision.org/\nCaffe Github : https://github.com/BVLC/caffe\nCaffe 安装教程：http://caffe.berkeleyvision.org/installation.html  http://blog.csdn.net/yhaolpz/article/details/71375762\nCaffe 安装分为CPU和GPU版本，GPU版本需要显卡支持以及安装CUDA。\nCaffe依赖 ProtoBuffer Boost GFLAGS GLOG BLAS HDF5 OpenCV LMDB LEVELDB Snappy\n4、使用Caffe搭建神经网络\n表 3-1 caffe搭建神经网络流程\n使用流程\n操作说明\n1、数据格式处理\n将数据处理成caffe支持格式，具体包括：LEVELDB,LMDB,内存数据，hdfs数据，图像数据，windows，dummy等。\n2、编写网络结构文件\n定义网络结构，如当前网络包括哪几层，每层作用是什么，使用caffe过程中最麻烦的一个操作步骤。具体编写格式可参考caffe框架自带自动识别手写体样例：caffe/examples/mnist/lenet_train_test.prototxt。\n3、编写网络求解文件\n定义了网络模型训练过程中需要设置的参数，比如学习率，权重衰减系数，迭代次数，使用GPU还是CP等，一般命名方式为xx_solver.prototxt，可参考：caffe/examples/mnist/lenet_solver.prototxt。\n4、训练\n基于命令行的训练，如：caffe train -solver examples/mnist/lenet_solver.prototxt\n5、测试\ncaffe test -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu 0 -iterations 100\n在上述流程中，步骤2是核心操作，也是caffe使用最让人头痛的地方，keras则对该部分做了更高层的抽象，让使用者能够快速编写出自己想要实现的模型。\n（二） Tensorflow\n1、概念介绍\nTensorFlow是一个使用数据流图进行数值计算的开源软件库。图中的节点表示数学运算，而图边表示在它们之间传递的多维数据阵列（又称张量）。灵活的体系结构允许你使用单个API将计算部署到桌面、服务器或移动设备中的一个或多个CPU或GPU。Tensorflow涉及相关概念解释如下：\n1）符号计算\n符号计算首先定义各种变量，然后建立一个“计算图”,计算图规定了各个变量之间的计算关系。 符号计算也叫数据流图，其过程如下-1所示，数据是按图中黑色带箭头的线流动的。\n图 3-1 数据流图示例\n数据流图用“结点”（nodes）和“线”(edges)的有向图来描述数学计算。\n①         “节点” 一般用来表示施加的数学操作，但也可以表示数据输入（feed in）的起点/输出（push out）的终点，或者是读取/写入持久变量（persistent variable）的终点。\n②         “线”表示“节点”之间的输入/输出关系。\n③         在线上流动的多维数据阵列被称作“张量”。\n2）张量\n张量(tensor)，可以看作是向量、矩阵的自然推广，用来表示广泛的数据类型。张量的阶数也叫维度。\n0阶张量,即标量,是一个数。\n1阶张量,即向量,一组有序排列的数\n2阶张量,即矩阵,一组向量有序的排列起来\n3阶张量，即立方体，一组矩阵上下排列起来\n4阶张量......\n依次类推\n3）数据格式(data_format)\n目前主要有两种方式来表示张量：\n① th模式或channels_first模式，Theano和caffe使用此模式。\n② tf模式或channels_last模式，TensorFlow使用此模式。\n下面举例说明两种模式的区别：\n对于100张RGB3通道的16×32（高为16宽为32）彩色图，\nth表示方式：（100,3,16,32）\ntf表示方式：（100,16,32,3）\n唯一的区别就是表示通道个数3的位置不一样。\n2、Tensorflow的模块结构\nTensorflow/core目录包含了TF核心模块代码，具体结构如-2所示：\n图 3-2 tensorflow代码模块结构\n3、安装方式\n1、官网下载naconda安装：https://www.anaconda.com/download/\n2、依次在Anaconda Prompt控制台，按以下5个步骤输入指令进行安装：\n1)        安装py3+ cmd : conda create -n py3.6 python=3.6 anaconda\n2)        激活虚拟环境 cmd : activate py3.6\n3)        激活TSF预安装cmd:\nconda create -n tensorflow python=3.6\nactivate tensorflow\n4)        安装TSF：\npip install --ignore-installed --upgrade tensorflow\npip install --ignore-installed --upgrade tensorflow-gpu\n5)        退出虚拟环境cmd ：deactivate py3.6\n4、使用Tensorflow搭建神经网络\n使用Tensorflow搭建神经网络主要包含以下6个步骤：\n1)        定义添加神经层的函数\n2)        准备训练的数据\n3)        定义节点准备接收数据\n4)        定义神经层：隐藏层和预测层\n5)        定义 loss 表达式\n6)        选择 optimizer 使 loss 达到最小\n7)        对所有变量进行初始化，通过 sess.run optimizer，迭代多次进行学习。\n5、示例代码\nTensorflow 构建神经网络识别手写数字，具体代码如下所示：\nimport tensorflow as tf import numpy as np # 添加层 def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs # 1.训练的数据 # Make up some real data x_data = np.linspace(-1,1,300)[:, np.newaxis] noise = np.random.normal(0, 0.05, x_data.shape) y_data = np.square(x_data) - 0.5 + noise # 2.定义节点准备接收数据 # define placeholder for inputs to network xs = tf.placeholder(tf.float32, [None, 1]) ys = tf.placeholder(tf.float32, [None, 1]) # 3.定义神经层：隐藏层和预测层 # add hidden layer 输入值是 xs，在隐藏层有 10 个神经元 l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) # add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果 prediction = add_layer(l1, 10, 1, activation_function=None) # 4.定义 loss 表达式 # the error between prediciton and real data loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1])) # 5.选择 optimizer 使 loss 达到最小 # 这一行定义了用什么方式去减少 loss，学习率是 0.1 train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # important step 对所有变量进行初始化 init = tf.initialize_all_variables() sess = tf.Session() # 上面定义的都没有运算，直到 sess.run 才会开始运算 sess.run(init) # 迭代 1000 次学习，sess.run optimizer for i in range(1000): # training train_step 和 loss 都是由 placeholder 定义的运算，所以这里要用 feed 传入参数 sess.run(train_step, feed_dict={xs: x_data, ys: y_data}) if i % 50 == 0: # to see the step improvement print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))\n（三） Keras\n1、概念介绍\nKeras由纯Python编写而成并基于Tensorflow、Theano以及CNTK后端，相当于Tensorflow、Theano、 CNTK的上层接口，号称10行代码搭建神经网络，具有操作简单、上手容易、文档资料丰富、环境配置容易等优点，简化了神经网络构建代码编写的难度。目前封装有全连接网络、卷积神经网络、RNN和LSTM等算法。\nKeras有两种类型的模型，序贯模型（Sequential）和函数式模型（Model），函数式模型应用更为广泛，序贯模型是函数式模型的一种特殊情况。\n1)        序贯模型（Sequential):单输入单输出，一条路通到底，层与层之间只有相邻关系，没有跨层连接。这种模型编译速度快，操作也比较简单\n2)        函数式模型（Model）：多输入多输出，层与层之间任意连接。这种模型编译速度慢。\n2、Keras的模块结构\nKeras主要由5大模块构成，模块之间的关系及每个模块的功能如-3所示：\n图 3-3 keras模块结构图\n3、安装方式\nKeras的安装方式有以下三个步骤：\n1)        安装anaconda（python）\n2)        用于科学计算的python发行版，支持Linux、Mac、Windows系统，提供了包管理与环境管理的功能，可以很方便的解决多版本python并存、切换以及各种第三方包安装问题。\n3)        利用pip或者conda安装numpy、keras、 pandas、tensorflow等库\n下载地址： https://www.anaconda.com/what-is-anaconda/\n4、使用Keras搭建神经网络\n使用keras搭建一个神经网络，包括5个步骤，分别为模型选择、构建网络层、编译、训练和预测。每个步骤操作过程中使用到的keras模块如-4所示。\n图 3-4 使用keras搭建神经网络步骤\n6、示例代码\nKears构建神经网络识别手写数字，具体代码如下所示：\nfrom keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation from keras.optimizers import SGD from keras.datasets import mnist import numpy ''' 第一步：选择模型 ''' model = Sequential() ''' 第二步：构建网络层 ''' model.add(Dense(500,input_shape=(784,))) # 输入层，28*28=784 model.add(Activation('tanh')) # 激活函数是tanh model.add(Dropout(0.5)) # 采用50%的dropout model.add(Dense(500)) # 隐藏层节点500个 model.add(Activation('tanh')) model.add(Dropout(0.5)) model.add(Dense(10)) # 输出结果是10个类别，所以维度是10 model.add(Activation('softmax')) # 最后一层用softmax作为激活函数 ''' 第三步：编译 ''' sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) # 优化函数，设定学习率（lr）等参数 model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数 ''' 第四步：训练 .fit的一些参数 batch_size：对总的样本数进行分组，每组包含的样本数量 epochs ：训练次数 shuffle：是否把数据随机打乱之后再进行训练 validation_split：拿出百分之多少用来做交叉验证 verbose：屏显模式 0：不输出 1：输出进度 2：输出每次的训练结果 ''' (X_train, y_train), (X_test, y_test) = mnist.load_data() # 使用Keras自带的mnist工具读取数据（第一次需要联网） # 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维 X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2]) Y_train = (numpy.arange(10) == y_train[:, None]).astype(int) Y_test = (numpy.arange(10) == y_test[:, None]).astype(int) model.fit(X_train,Y_train,batch_size=200,epochs=50,shuffle=True,verbose=0,validation_split=0.3) model.evaluate(X_test, Y_test, batch_size=200, verbose=0) ''' 第五步：输出 ''' print(\"test set\") scores = model.evaluate(X_test,Y_test,batch_size=200,verbose=0) print(\"\") print(\"The test loss is %f\" % scores) result = model.predict(X_test,batch_size=200,verbose=0) result_max = numpy.argmax(result, axis = 1) test_max = numpy.argmax(Y_test, axis = 1) result_bool = numpy.equal(result_max, test_max) true_num = numpy.sum(result_bool) print(\"\") print(\"The accuracy of the model is %f\" % (true_num/len(result_bool)))\n（四）框架性能及优缺点对比\n表 3-2 深度学习框架对比\n对比维度\nCaffe\nTensorflow\nKears\n上手难度\n1、     不用不写代码，只需在.prototxt文件中定义网络结构就可以完成模型训练。\n2、     安装过程复杂，且在.prototxt 文件内部设计网络节构比较受限，没有在 Python 中设计网络结构方便、自由。配置文件不能用编程的方式调整超参数，对交叉验证、超参数Grid Search 等操作无法很方便的支持。\n1、     安装简单，教学资源丰富，根据样例能快速搭建出基础模型。\n2、     有一定的使用门槛。不管是编程范式，还是数学统计基础，都为非机器学习与数据科学背景的伙伴们带来一定的上手难度。另外，是一个相对低层的框架，使用时需要编写大量的代码，重新发明轮子。\n1、安装简单，它旨在让用户进行最快速的原型实验，让想法变为结果的这个过程最短，非常适合最前沿的研究。\n2、API使用方便，用户只需要将高级的模块拼在一起，就可以设计神经网络，降低了编程和阅读别人代码时的理解开销\n框架维护\n在 TensorFlow 出现之前一直是深度学习领域 GitHub star 最多的项目，前由伯克利视觉学中心（Berkeley Vision and Learning Center，BVLC）进行维护。\n被定义为“最流行”、“最被认可”的开源深度学习框架， 拥有产品级的高质量代码，有 Google 强大的开发、维护能力的加持，整体架构设计也非常优秀。\n开发主要由谷歌支持， API以“tf.keras\"的形式打包在TensorFlow中。微软维护着Keras的CNTK后端。亚马逊AWS正在开发MXNet支持。其他提供支持的公司包括NVIDIA、优步、苹果（通过CoreML）\n支持语言\nC++/Cuda\nC++ python (Go，Java，Lua，Javascript，或者是R)\nPython\n封装算法\n1、对卷积神经网络的支持非常好，拥有大量的训练好的经典模型（AlexNet、VGG、Inception）乃至其他 state-of-the-art （ResNet等）的模型，收藏在它的 Model Zoo。\n2、对时间序列 RNN、LSTM 等支持得不是特别充分\n1、支持CNN与RNN， 还支持深度强化学习乃至其他计算密集的科学计算(如偏微分方程求解等)。\n2、计算图必须构建为静态图，这让很多计算变得难以实现，尤其是序列预测中经常使用的 beam search。\n1、专精于深度学习，支持卷积网络和循环网络，支持级联的模型或任意的图结构的模型，从 CPU 上计算切换到 GPU 加速无须任何代码的改动。\n2、没有增强学习工具箱，自己修改实现很麻烦。封装得太高级，训练细节不能修改、penalty细节很难修改。\n模型部署\n1、程序运行非常稳定，代码质量比较高，很适合对稳定性要求严格的生产环境，第一个主流的工业级深度学习框架。Caffe 的底层基于 C++，可以在各种硬件环境编译并具有良好的移植性，支持 Linux、Mac 和 Windows 系统，也可以编译部署到移动设备系统如 Android 和 iOS 上。\n1、为生产环境设计的高性能的机器学习服务系统，可以同时运行多个大规模深度学习模型，支持模型生命周期管理、算法实验，并可以高效地利用 GPU 资源，让训练好的模型更快捷方便地投入到实际生产环境。灵活的移植性，可以将同一份代码几乎不经过修改就轻松地部署到有任意数量 CPU 或 GPU 的 PC、服务器或者移动设备上。\n1、使用TensorFlow、CNTK、Theano作为后端，简化了编程的复杂度，节约了尝试新网络结构的时间。模型越复杂，收益越大，尤其是在高度依赖权值共享、多模型组合、多任务学习等模型上，表现得非常突出。\n性能\n目前仅支持单机多 GPU 的训练，不支持分布式的训练。\n1、     支持分布式计算，使 GPU 集群乃至 TPU 集群并行计算，共同训练出一个模型。\n2、     对不同设备间的通信优化得不是很好，分布式性能还没有达到最优。\n无法直接使用多 GPU，对大规模的数据处理速度没有其他支持多 GPU 和分布式的框架快。用TensorFLow backend时速度比纯TensorFLow 下要慢很多。\n如表3-2对比维度所示，对于刚入门机器学习的新手而已，keras无疑是最好的选择，能够快速搭建模型验证想法。随着对机型学习的理解逐步加深，业务模型越来越复杂时，可以根据实际需要转到Tensorflow或Caffe。\n四、结束语\n深度学习的研究在持续进行中，一直与其它经典机器学习算法并存，各类深度学习框架也是遍地开花，各有偏向，优劣各异，具体用哪种要根据应用场景灵活选择。正如本文导语所言，科学不是战争而是合作。任何学科的发展从来都不是一条路走到黑，而是同行之间互相学习、互相借鉴、博采众长、相得益彰，站在巨人的肩膀上不断前行。对机器学习和深度学习的研究也是一样，你死我活那是邪教，开放包容才是正道！\n最后，文章内容多摘自网上广大网友的贡献，如所写内容涉及他人著作且未进行参考引用，那一定是我遗漏了，非常抱歉，还请及时联系我进行修正，万分感谢！。\n参考文章\n[1]. https://www.zhihu.com/question/57770020/answer/249708509  人工智能、机器学习和深度学习的区别?\n[2] . http://km.oa.com/group/25254/articles/show/325228?kmref=search&from_page=1&no=1 从入门到吃鸡--基于Caffe 框架AI图像识别自动化\n[3]. http://blog.luoyetx.com/2015/10/reading-caffe-1/\n[4]. https://zhuanlan.zhihu.com/p/24087905  Caffe入门与实践-简介\n[5]. https://keras-cn.readthedocs.io/en/latest/for_beginners/FAQ/  keras官网\n[6]. http://biog.csdn.net/sinat_26917383\n[7]. http://www.cnblogs.com/lc1217/p/7132364.html 深度学习：Keras入门(一)之基础篇\n[8]. https://www.jianshu.com/p/e112012a4b2d 一文学会用 Tensorflow 搭建神经网络\n[9]. https://www.zhihu.com/question/42061396/answer/93827994  深度学习会不会淘汰掉其他所有机器学习算法?\n[10]. https://www.leiphone.com/news/201702/T5e31Y2ZpeG1ZtaN.html  TensorFlow和Caffe、MXNet、Keras等其他深度学习框架的对比\n[11]. https://chenrudan.github.io/blog/2015/11/18/comparethreeopenlib.html  Caffe、TensorFlow、MXnet三个开源库对比\n[12]. https://zhuanlan.zhihu.com/p/24687814  对比深度学习十大框架：TensorFlow最流行但并不是最好\n[13]. https://www.leiphone.com/news/201704/8RWdnz9dQ0tyoexF.html  万事开头难！入门TensorFlow，这9个问题TF Boys 必须要搞清楚"}
{"content2":"大家好！今天是我学习汇编语言的第一课。我感觉汇编好好玩啊！\n机器码编程\n哇！一上来就写程序了啊，还是用机器码的啊！是呢！嘿嘿！我们去下载1个二进制编辑器，我也不知道哪款好用点，我下载的是FlexHEX。然后我们打开它，并写入一下代码：CD 00\n以前我学C语言的时候，我知道CD 00 其实就是1100 1101 0000 0000啦！然后我们保存它为1个文件。关于文件名随便取啦！然后我们运行打开cmd，我们把刚才编辑保存好的文件拖到cmd中然后再运行！\n哇！程序输出了Divide overflow 好棒啊！一上来就可以机器码编程啦！而且只有2字节呀！看起来也不是那么的复杂啊！但是我看下面的程序，下面的程序是welcome to masm的机器码：\n大家看到了吗？！好多的01011001啊，要是我们写二进制机器码的时候1个被错误的写成0了，那如何去调试查找啊，所以汇编语言产生了哦！汇编语言它的主题是汇编指令，汇编指令呢只是机器指令便于书写记忆的一种格式。比如：机器指令1000100111011000 它的意思是寄存器BX的内容送到AX中，对于的汇编指令是：MOV AX,BX。大家看，这样的写法是不是更接近人类语言呀！(咦！什么是寄存器啊，这里怎么冒出来个寄存器，寄存器是CPU中可以存储数据的器件，一个CPU中有多个寄存器，AX,BX就是其中2个寄存器的代号)。现在我们看看我们的8086CPU中有哪些寄存器：我们用运行输入cmd，然后再输入debug，然后再在-后输入r.我们一起看看：\n哇！眼花了！仔细一瞧啊！其实也不晕啊，就13个寄存器啊（我暂时这样理解）！每个寄存器是16位的，也就是说十六位二进制数。\n既然汇编语言是机器语言的助记符。虽然它们是一一对应的，但是如果把汇编语言直接塞给CPU，CPU也还是不认识的呢，所以，要让电脑运行汇编程序，必须要有1个翻译啊。这个翻译呢就是汇编语言编译器啦！\n汇编语言的组成\n1,汇编指令（机器码的助记符）\n2，伪指令（由编译器执行）\n3，其他符号（由编译器识别）\n汇编语言的核心是汇编指令啦，它决定了汇编的特性哦。\n我的指令和数据在什么地方啊\nCPU是计算机的核心，它控制着整个计算机的运作，但是我们必须向它提供指令和数据它才能工作呢，那么这些指令和数据放在什么地方的啊，以前我们学C语言做调试的时候知道，我们调试的时候看就是内存，所以汇编语言也一样啊，指令和数据也是放在内存里的，但是我们现在要重新取个名字，听起来更专业点，呵呵，内存就叫存储器（这里要和寄存器搞清楚关系啊，寄存器是CPU内部的，虽然它也是存放东西的啦！）。\n大笨蛋CPU如何区分指令和数据啊\n1000100111011000 这个二进制机器码要是我们把它当做是数据的时候，那么它就是89D8H\n1000100111011000 这个二进制机器码要是我们把它当做是指令的时候，那么它就是MOV AX,BX\n但是大笨蛋CPU就认识二进制的啊，它怎么知道哪些是数据哪些是指令啊！它是这样认识的，大笨蛋CPU和其他芯片之间有导线连接，逻辑上可以划分为：地址总线，数据总线，控制总线。\n所以我们只要把1000100111011000放在地址总线，CPU就认识它是地址，如果把它放在数据总线上，那么CPU就认为他是数据，如果把它放在控制总线上，那么笨蛋CPU就认为它是指令。\n地址总线\n我们看到地址总线是一串0110101011010组成的，所以说呢，这些位二进制数有多少种变化CPU就可以对多少个存储单元进行寻址。也就是说，一个CPU有N根地址总线，那么这个CPU的地址总线宽度为N。这样的CPU最多可以寻址2的N次方个内存单元啦！\n数据总线\n数据总线就像一根和内存的高速公路呢！它的宽度决定了CPU和外界的数据传送的速度，当然高速公路越宽，这样就可以用很宽很宽的大卡车运货，这样每次运的货就更多！\n下面我们来看1个例子，我们向内存中写入数据89D8H的时候，8088，8086CPU的传送能力：\n8088CPU每次是传送8位数据：\n8086每次是传送16位的：\n从这个例子可以看出，8086一次就可以传送完8088两次才能传送完的数据啊！8086要先进呀！\n控制总线\n从地址总线，数据总线的知识，我们可以推理出有多少根控制总线，就意味着CPU提供了对外部器件的多少控制，呵呵！所以它的宽度决定了CPU对外部器件的控制能力啦！\n嗯！先学到这里！加油！梦想是总有一天要写出1个自己的操作系统出来。"}
{"content2":"很多朋友想学习机器学习，却苦于环境的搭建，这里给出windows上scikit-learn研究开发环境的搭建步骤。\nStep 1. Python的安装\npython有2.x和3.x的版本之分，但是很多好的机器学习python库都不支持3.x，因此，推荐安装2.7版本的python。当前最新的python是2.7.12.链接如下：\nhttps://www.python.org/downloads/release/python-2712/\n里面可以看到有32位版和64位版的。如果你的机器是64位版的，那么32位和64位版的任选一个安装就可以了。如果机器是32位版的，就只能安装32位版的了。如果你搞不清楚你的机器的位数，那么就安装32位版的吧。也就是“Windows x86 MSI installer”。\nWindows x86-64 MSI installer\nWindows\nfor AMD64/EM64T/x64, not Itanium processors\n8fa13925db87638aa472a3e794ca4ee3\n19820544\nSIG\nWindows x86 MSI installer\nWindows\nfe0ef5b8fd02722f32f7284324934f9d\n18907136\nSIG\n安装完毕后，可以设置下环境变量，把python目录加到PATH，比如我的Python装在 C:\\Python27，那我就把C:\\Python27\\Scripts和C:\\Python27加到环境变量。当然不加也可以。这样每次使用Python时加上python的全路径名。\n安装完成后，在windows的命令行输入python，如果能出来python的基本信息说明安装成功。\nStep 2. Python包管理工具pip的安装\n我们需要包管理工具来方便python库的安装，包管理工具有很多，这里推荐我习惯使用的pip。\n下载pip的安装脚本。链接如下。下载get-pip.py。然后到你的下载目录，在命令行输入\"python get-pip.py\",跑完即可安装成功。\nhttps://pip.pypa.io/en/stable/installing/\n下载完毕后，记得跑下这个命令“pip install -U pip”，一是看看pip能不能正常工作，二是把pip升级到最新版本。\nStep 3. 安装 Visual C++ Compiler for Python\n链接在这: https://www.microsoft.com/en-us/download/details.aspx?id=44266\n这个不装后面很多科学计算的都会装不了。\nStep 4. 安装numpy和scipy\n这两哥们是科学计算和矩阵运算必备工具。\n由于numpy和scipy直接用pip安装经常会出各种各样的问题，因此一般推荐下载离线版的whl来安装numpy和scipy。\n首先安装离线版的numpy，这里我一般是在下面的链接下载numpy，当然scipy也是在这。\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy\n可以看到里面有很多版本的numpy可以下载，我们的python是2.7，windows 32位的，因此下载“numpy-1.11.2+mkl-cp27-cp27m-win32.whl”\n下载完毕后进入下载目录，在命令行运行 \"pip install numpy-1.11.2+mkl-cp27-cp27m-win32.whl\" ，这样numpy就安装成功了。\n用同样的方法安装scipy。在下面的链接下载scipy。\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy\n我们的python是2.7，windows 32位的，因此选择scipy-0.18.1-cp27-cp27m-win32.whl下载。\n完了运行 \"pip install scipy-0.18.1-cp27-cp27m-win32.whl\"\n这样numpy和scipy两个好基友就搞定了。\nStep 4. 安装matplotlib，pandas和scikit-learn\n这没有什么好说的，直接在命令行运行下面的命令即可。注意，先安装matplotlib再安装pandas\npip install -U matplotlib\npip install -U jinja2\npip install -U jsonschema\npip install -U pyzmq\npip install -U pandas\npip install -U scikit-learn\nStep 5. 安装ipython和ipython notebook\nipython notebook是最常用的python交互式学习工具，当然，现在叫做Jupyter Notebook。scikit-learn官方的例子都给出了用ipython notebook运行的版本。\n安装方式很简单：\npip install ipython\npip install jupyter\n官网在这：http://ipython.org/notebook.html\n安装完毕后，在命令行输入“jupyter-notebook”,输出会提示你notebook运行在http://localhost:8888\nStep 6.  Hello World！尝试运行一个scikit-learn机器学习程序\n在scikit-learn官网下载一个机器学习的例子，比如： http://scikit-learn.org/stable/_downloads/plot_cv_predict.ipynb\n然后在下载目录运行\"jupyter notebook\",接着浏览器打开http://localhost:8888。\n可以在浏览器看到你下载目录的内容，我们打开刚下载的plot_cv_predict.ipynb这个文件链接，可以看到python程序的内容，这时我们可以点上面的三角形按钮，一步步的运行程序，如果没有报错，最后可以看到一个线性回归的预测图。\n可以修改这个程序，重新一步步的跑，达到研究学习的目的。\n以上就是scikit-learn和pandas环境的搭建过程。希望大家都可以搭建成功，来研究机器学习。\n（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）"}
{"content2":"决策树是机器学习的常见算法，分为分类树和回归树。当对一个样本的分类进行预测时使用分类树，当对样本的某一个值进行预测时使用回归树。本文是有关决策树的第一部分，主要介绍分类树的几种构建方法，以及如何使用分类树测试分类。\n目录如下：\n1、分类树的基本概念\n2、采用数据集说明\n3、划分数据集的几种方式\n4、构造分类树\n5、使用分类树测试分类\n6、写在后面的话\n一 分类树的基本概念\n分类树（classification tree）简单地说，就是根据训练数据集构造一个类似树形的分类决策模型，然后用这个模型来辅助决策。\n例如下图是一个简单的是否举行某个活动的决策树（分类树）：\n我们可以通过上面的决策树进行预测，当天气晴朗，交通畅通时，我们预测该活动很可能要举办；当天下小雨交通拥挤时，我们预测活动很可能被取消。\n这只是一个简单的小例子，真实中的决策树方法包括以下几个步骤：\n(1） 收集数据：可以使用任何方法。\n（2）准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。\n（3）选取划分算法：根据数据的特点，选取合适的划分算法\n（4）构造决策树：使用选取的划分算法构造树形的决策模型\n（5）测试算法：使用经验树计算错误率\n（6）使用算法：使用决策树模型预测决策\n二 采用数据集说明\nUCI数据集 是机器学习不错的数据集网站，本文选取其中的 Balloons 数据集，将其内容用中文表示如下。\n该数据主要是根据几个因素预测气球是否会破。\n三 划分数据集的几种方式\n决策树的几种经典实现方式是ID3，C4.5 和 CART\n其中C4.5是对ID3的改进，C4.5和ID3都是分类树，CART 即可用作分类树，又可用于回归树，当CART用作分类时使用基尼指数作为划分依据，当CART用作回归时使用最小方差作为划分依据。\n信息熵\n熵（entropy），也即信息熵，是度量样本集合纯度的一种指标。一个数据集的熵越大，则说明该数据分类的纯度越纯。\nD表示数据集，假设D共有m个类别，Pk 表示第k个类别占样本总数的比例，数据集熵的公式如下:\n如上面给出的关于气球的数据集，只有 “会” 和 “不会”两种分类，“会”有7个，“不会”有9个，占比分别是 P1 =7/16 ,  P2=9/16\n按照熵的公式 可得 Ent(D) = -P1 * log2P1 - P2*log2P2 = 0.989\n代码实现：\n1 from math import log 2 import operator 3 4 def calc_entropy(data_set): 5 \"\"\"计算数据集的熵\"\"\" 6 count = len(data_set) 7 label_counts = {} 8 9 # 统计数据集中每种分类的个数 10 for row in data_set: 11 label = row[-1] 12 if label not in label_counts.keys(): 13 label_counts[label] = 1 14 else: 15 label_counts[label] += 1 16 17 # 计算熵 18 entropy = 0.0 19 for key in label_counts: 20 prob = float(label_counts[key]) / count 21 entropy -= prob * log(prob, 2) 22 return entropy\n信息增益法\nID3算法使用信息增益作为划分数据集的依据。整个数据集的熵称作 原始熵，数据集D根据某个特征划分之后的熵为条件熵，信息增益 = 原始熵 - 条件熵   。用信息增益划分的具体做法是：计算每一类特征V对应的信息增益，然后挑选信息增益最小的特征进行划分。\n信息增益公式为:\n其中v 为特征a的一个分类，pv 为 v分类占特征a总个数的比例，Dv 根据特征a 的v分类进行划分之后的数据集。\n代码实现：\n1 def choose_best_feature_1(data_set): 2 \"\"\"选取信息增益最高的特征\"\"\" 3 feature_count = len(data_set[0]) - 1 4 # 数据集的原始熵 5 base_entropy = calc_entropy(data_set) 6 # 最大的信息增益 7 best_gain = 0.0 8 # 信息增益最大的特征 9 best_feature = -1 10 11 # 遍历计算每个特征 12 for i in range(feature_count): 13 feature = [example[i] for example in data_set] 14 feature_value_set = set(feature) 15 new_entropy = 0.0 16 17 # 计算信息增益 18 for value in feature_value_set: 19 sub_data_set = split_data_set(data_set, i, value) 20 prob = len(sub_data_set) / float(len(data_set)) 21 new_entropy += prob * calc_entropy(sub_data_set) 22 gain = base_entropy - new_entropy 23 24 # 比较得出最大的信息增益 25 if gain > best_gain: 26 best_gain = gain 27 best_feature = i 28 29 return best_feature\n增益率法\nID3所采用的信息增益划分数据集是可能对数目较多的属性有偏好，C4.5 算法避免了这个问题，使用“增益率”来选择最优化分属性。\n信息增益率公式：\n其中Gain_ratio(D,a) 表示 根据特征 a 划分之后的信息增益率， IV(a)为 特征a 的固有值\n代码实现：\n1 def choose_best_feature_2(data_set): 2 \"\"\"根据增益率选取划分特征\"\"\" 3 feature_count = len(data_set[0]) - 1 4 # 数据集的原始熵 5 base_entropy = calc_entropy(data_set) 6 # 最大的信息增益率 7 best_gain_ratio = 0.0 8 # 信息增益率最大的特征 9 best_feature = -1 10 11 # 遍历计算每个特征 12 for i in range(feature_count): 13 feature = [example[i] for example in data_set] 14 feature_value_set = set(feature) 15 new_entropy = 0.0 16 # 固有值 17 intrinsic_value = 0.0 18 19 # 计算信息增益 20 for value in feature_value_set: 21 sub_data_set = split_data_set(data_set, i, value) 22 prob = len(sub_data_set) / float(len(data_set)) 23 new_entropy += prob * calc_entropy(sub_data_set) 24 intrinsic_value -= prob * log(prob, 2) 25 gain = base_entropy - new_entropy 26 gain_ratio = gain / intrinsic_value 27 28 # 比较得出最大的信息增益率 29 if gain_ratio > best_gain_ratio: 30 best_gain_ratio = gain_ratio 31 best_feature = i 32 33 return best_feature\n基尼指数法\n当CART用做分类树时，使用“基尼指数”来选择划分特征。基尼指数是另外一种表示数据集纯度的指标。\n基尼值：\n基尼指数：\n代码实现：\n1 def calc_gini(data_set): 2 \"\"\"计算数据集的基尼值\"\"\" 3 count = len(data_set) 4 label_counts = {} 5 6 # 统计数据集中每种分类的个数 7 for row in data_set: 8 label = row[-1] 9 if label not in label_counts.keys(): 10 label_counts[label] = 1 11 else: 12 label_counts[label] += 1 13 14 # 计算基尼值 15 gini = 1.0 16 for key in label_counts: 17 prob = float(label_counts[key]) / count 18 gini -= prob * prob 19 return gini 20 21 22 def choose_best_feature_3(data_set): 23 \"\"\"根据基尼指数选择划分特征\"\"\" 24 feature_count = len(data_set[0]) - 1 25 # 最小基尼指数 26 min_gini_index = 0.0 27 # 基尼指数最小的特征 28 best_feature = -1 29 30 # 遍历计算每个特征 31 for i in range(feature_count): 32 feature = [example[i] for example in data_set] 33 feature_value_set = set(feature) 34 35 # 基尼指数 36 gini_index = 0.0 37 # 计算基尼指数 38 for value in feature_value_set: 39 sub_data_set = split_data_set(data_set, i, value) 40 prob = len(sub_data_set) / float(len(data_set)) 41 gini_index += prob * calc_gini(sub_data_set) 42 43 # 比较得出最小的基尼指数 44 if gini_index < min_gini_index or min_gini_index == 0.0: 45 min_gini_index = gini_index 46 best_feature = i 47 48 return best_feature\n四 构造分类树\n每次根据划分算法选出最佳的划分特征进行划分，然后对子数据集进行递归划分，直到所有子集的纯度都为1，即构成了决策树。\n构造决策树代码：\n1 def create_division_tree(data_set, labels): 2 \"\"\"创建决策树\"\"\" 3 class_list = [example[-1] for example in data_set] 4 5 # 所有分类相同时返回 6 if class_list.count(class_list[0]) == len(class_list): 7 return class_list[0] 8 9 # 已经遍历完所有特征 10 if len(data_set[0]) == 1: 11 return get_top_class(class_list) 12 13 # 选取最好的特征 14 best_feat = choose_best_feature_1(data_set) 15 best_feat_label = labels[best_feat] 16 17 # 划分 18 my_tree = {best_feat_label: {}} 19 del (labels[best_feat]) 20 value_set = set([example[best_feat] for example in data_set]) 21 for value in value_set: 22 sub_labels = labels[:] 23 my_tree[best_feat_label][value] = create_division_tree(split_data_set(data_set, best_feat, value), sub_labels) 24 return my_tree\n构造的决策树存在在Python 字典类型中，不能直观地看清决策树层次，这里我们使用Matplotlib 模块提供的绘图工具绘制出决策树的模型如下：\n我们可以看到使用不同的划分算法，构造的决策树是不一样的。\n五 使用决策树测试分类\n通过决策树预测测试样本时，就是根据测试样本的特征属性 从决策树根节点开始不断向下遍历，直到叶子节点。\n1 def classify(division_tree, feat_labels, test_vector): 2 \"\"\"遍历决策树对测试数据进行分类\"\"\" 3 first_key = list(division_tree.keys())[0] 4 second_dict = division_tree[first_key] 5 6 feat_index = feat_labels.index(first_key) 7 test_key = test_vector[feat_index] 8 9 test_value = second_dict[test_key] 10 11 if isinstance(test_value, dict): 12 class_label = classify(test_value, feat_labels, test_vector) 13 else: 14 class_label = test_value 15 return class_label\n六 写在后面的话\n本文完整代码见https://gitee.com/beiyan/machine_learning/tree/master/decision_tree，本文只是分类树方法的简单实现，关于回归树的介绍，以及决策树的剪枝算法，数值型数据离散方法等 将在后序文章中介绍。"}
{"content2":"欢迎大家前往腾讯云社区，获取更多腾讯海量技术实践干货哦~\n作者：吴懿伦\n导语： 本文是对机器学习算法的一个概览，以及个人的学习小结。通过阅读本文，可以快速地对机器学习算法有一个比较清晰的了解。本文承诺不会出现任何数学公式及推导，适合茶余饭后轻松阅读，希望能让读者比较舒适地获取到一点有用的东西。\n引言\n本文是对机器学习算法的一个概览，以及个人的学习小结。通过阅读本文，可以快速地对机器学习算法有一个比较清晰的了解。本文承诺不会出现任何数学公式及推导，适合茶余饭后轻松阅读，希望能让读者比较舒适地获取到一点有用的东西。\n本文主要分为三部分，第一部分为异常检测算法的介绍，个人感觉这类算法对监控类系统是很有借鉴意义的；第二部分为机器学习的几个常见算法简介；第三部分为深度学习及强化学习的介绍。最后会有本人的一个小结。\n1 异常检测算法\n异常检测，顾名思义就是检测异常的算法，比如网络质量异常、用户访问行为异常、服务器异常、交换机异常和系统异常等，都是可以通过异常检测算法来做监控的，个人认为这种算法很值得我们做监控的去借鉴引用，所以我会先单独介绍这一部分的内容。\n异常定义为“容易被孤立的离群点 (more likely to be separated)”——可以理解为分布稀疏且离密度高的群体较远的点。用统计学来解释，在数据空间里面，分布稀疏的区域表示数据发生在此区域的概率很低，因而可以认为落在这些区域里的数据是异常的。\n-1离群点表现为远离密度高的正常点\n如-1所示，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。\n下面是几种异常检测算法的简介。\n1.1 基于距离的异常检测算法\n-2 基于距离的异常检测\n思想：一个点如果身边没有多少小伙伴，那么就可以认为这是一个异常点。\n步骤：给定一个半径r，计算以当前点为中心、半径为r的圆内的点的个数与总体个数的比值。如果该比值小于一个阈值，那么就可以认为这是一个异常点。\n1.2 基于深度的异常检测算法\n-3 基于深度的异常检测算法\n思想：异常点远离密度大的群体，往往处于群体的最边缘。\n步骤：通过将最外层的点相连，并表示该层为深度值为1；然后将次外层的点相连，表示该层深度值为2，重复以上动作。可以认为深度值小于某个数值k的为异常点，因为它们是距离中心群体最远的点。\n1.3 基于分布的异常检测算法\n-4 高斯分布\n思想：当前数据点偏离总体数据平均值3个标准差时，可以认为是一个异常点（偏离多少个标准差可视实际情况调整）。\n步骤：计算已有数据的均值及标准差。当新来的数据点偏离均值3个标准差时，视为异常点。\n1.4 基于划分的异常检测算法\n-5孤立深林\n思想：将数据不断通过某个属性划分，异常点通常能很早地被划分到一边，也就是被早早地孤立起来。而正常点则由于群体众多，需要更多次地划分。\n步骤：通过以下方式构造多颗孤立树：在当前节点随机挑选数据的一个属性，并随机选取属性的一个值，将当前节点中所有数据划分到左右两个叶子节点；如果叶子节点深度较小或者叶子节点中的数据点还很多，则继续上述的划分。异常点表现为在所有孤立树中会有一个平均很低的树的深度，如-5中的红色所示为深度很低的异常点。\n2 机器学习常见算法\n简单介绍机器学习的几个常见算法：k近邻、k-means聚类、决策树、朴素贝叶斯分类器、线性回归、逻辑回归、隐马尔可夫模型及支持向量机。遇到讲得不好的地方建议直接跳过。\n2.1 K近邻\n-1距离最近的3个点里面有2个点为红三角，所以待判定点应为红三角\n分类问题。对于待判断的点，从已有的带标签的数据点中找到离它最近的几个数据点，根据它们的标签类型，以少数服从多数原则决定待判断点的类型。\n2.2 k-means聚类\n-2不断迭代完成“物以类聚”\nk-means聚类的目标是要找到一个分割，使得距离平方和最小。初始化k个中心点；通过欧式距离或其他距离计算方式，求取各个数据点离这些中心点的距离，将最靠近某个中心点的数据点标识为同一类，然后再从标识为同一类的数据点中求出新的中心点替代之前的中心点，重复上述计算过程，直到中心点位置收敛不再变动。\n2.3 决策树\n-3 通过决策树判断今天是否适合打球\n决策树的表现形式和if-else类似，只是在通过数据生成决策树的时候，需要用到信息增益去决定最先使用那个属性去做划分。决策树的好处是表现力强，容易让人理解结论是如何得到的。\n2.4 朴素贝叶斯分类器\n朴素贝叶斯法师基于贝叶斯定理与特征条件独立性假设的分类方法。由训练数据学习联合概率分布，然后求得后验概率分布。（抱歉，没图，又不贴公式，就这样吧-_-）\n2.5 线性回归\n-4 拟一条直线，与所有数据点实际值之差的和最小\n就是对函数f(x)=ax+b，通过代入已有数据(x,y)，找到最合适的参数a和b，使函数最能表达已有数据输入和输出之间的映射关系，从而预测未来输入对应的输出。\n2.6 逻辑回归\n-5 逻辑函数\n逻辑回归模型其实只是在上述的线性回归的基础上，套用了一个逻辑函数，将线性回归的输出通过逻辑函数转化成0到1之间的数值，便于表示属于某一类的概率。\n2.7 隐马尔科夫模型\n-6 隐藏状态x之间的转移概率以及状态x的观测为y的概率图\n隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的序列的过程。隐马尔科夫模型有三要素和三个基本问题，有兴趣的可以单独去了解。最近看了一篇有意思的论文，其中使用了隐马尔可夫模型去预测美国研究生会在哪个阶段转专业，以此做出对策挽留某专业的学生。公司的人力资源会不会也是通过这个模型来预测员工会在哪个阶段会跳槽，从而提前实施挽留员工的必要措施？(^_^)\n2.8 支持向量\n-7支持向量对最大间隔的支持\n支持向量机是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。如-7所示，由于支持向量在确定分离超平面中起着关键性的作用，所以将这种分类模型称为支持向量机。\n对于输入空间中的非线性分类问题，可以通过非线性变换（核函数）将它转换为某个高维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机。如-8所示，训练点被映射到可以容易地找到分离超平面的三维空间。\n-8将二维线性不可分转换为三维线性可分\n3 深度学习简介\n这里将简单介绍神经网络的由来。介绍顺序为：感知机、多层感知机（神经网络）、卷积神经网络及循环神经网络。\n3.1 感知机\n-1输入向量通过加权求和后代入激活函数中求取结果\n神经网络起源于上世纪五、六十年代，当时叫感知机，拥有输入层、输出层和一个隐含层。它的缺点是无法表现稍微复杂一些的函数，所以就有了以下要介绍的多层感知机。\n3.2 多层感知机\n-2多层感知机，表现为输入与输出间具有多个的隐含层\n在感知机的基础上，添加了多个隐含层，以满足能表现更复杂的函数的能力，其称之为多层感知机。为了逼格，取名为神经网络。神经网络的层数越多，表现能力越强，但是随之而来的是会导致BP反向传播时的梯度消失现象。\n3.3 卷积神经网络\n-3卷积神经网络的一般形式\n全连接的神经网络由于中间隐含层多，导致参数数量膨胀，并且全连接方式没有利用到局部模式（例如图片里面临近的像素是有关联的，可构成像眼睛这样更抽象的特征），所以出现了卷积神经网络。卷积神经网络限制了参数个数并且挖掘了局部结构这个特点，特别适用于图像识别。\n3.4 循环神经网络\n-4 循环神经网络可以看成一个在时间上传递的神经网络\n循环神经网络可以看成一个在时间上传递的神经网络，它的深度是时间的长度，神经元的输出可以作用于下一个样本的处理。普通的全连接神经网络和卷积神经网络对样本的处理是独立的，而循环神经网络则可以应对需要学习有时间顺序的样本的任务，比如像自然语言处理和语言识别等。\n4 个人小结\n机器学习其实是学习从输入到输出的映射：\n即希望通过大量的数据把数据中的规律给找出来。（在无监督学习中，主要任务是找到数据本身的规律而不是映射）\n总结一般的机器学习做法是：根据算法的适用场景，挑选适合的算法模型，确定目标函数，选择合适的优化算法，通过迭代逼近最优值，从而确定模型的参数。\n关于未来的展望，有人说强化学习才是真正的人工智能的希望，希望能进一步学习强化学习，并且要再加深对深度学习的理解，才可以读懂深度强化学习的文章。\n最后最后，由于本人也只是抽空自学了几个月的小白，所以文中有错误的地方，希望海涵和指正，我会立即修改，希望不会误导到别人。\n参考文献\n【1】 李航. 统计学习方法[J]. 清华大学出版社, 北京, 2012.\n【2】 Kriegel H P, Kröger P, Zimek A. Outlier detection techniques[J]. Tutorial at KDD, 2010.\n【3】 Liu F T, Ting K M, Zhou Z H. Isolation forest[C]//Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on. IEEE, 2008: 413-422.\n【4】 Aulck L, Aras R, Li L, et al. Stem-ming the Tide: Predicting STEM attrition using student transcript data[J]. arXiv preprint arXiv:1708.09344, 2017.\n【5】 李宏毅.deep learning tutorial. http://speech.ee.ntu.edu.tw/~tlkagk/slide/Deep%20Learning%20Tutorial%20Complete%20(v3)\n【6】 科研君.卷积神经网络、循环神经网络、深度神经网络的内部结构区别. https://www.zhihu.com/question/34681168\n相关阅读\n一站式满足电商节云计算需求的秘诀\n迟蹭一个热点：自我对弈的 AlphaGo Zero\nPython 机器学习库 --- sklearn --- 线性模型\n此文已由作者授权腾讯云技术社区发布，转载请注明文章出处\n原文链接：https://cloud.tencent.com/community/article/570843\n海量技术实践经验，尽在腾讯云社区！"}
{"content2":"最近在看《机器学习实战》这本书，因为自己本身很想深入的了解机器学习算法，加之想学python，就在朋友的推荐之下选择了这本书进行学习。\n一 . K-近邻算法（KNN）概述\n最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。\nKNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。\n下面通过一个简单的例子说明一下：如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。\n由此也说明了KNN算法的结果很大程度取决于K的选择。\n在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：\n同时，KNN通过依据k个对象中占优的类别进行决策，而不是单一的对象类别决策。这两点就是KNN算法的优势。\n接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：\n1）计算测试数据与各个训练数据之间的距离；\n2）按照距离的递增关系进行排序；\n3）选取距离最小的K个点；\n4）确定前K个点所在类别的出现频率；\n5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。\n二 .python实现\n首先呢，需要说明的是我用的是python3.4.3，里面有一些用法与2.7还是有些出入。\n建立一个KNN.py文件对算法的可行性进行验证，如下：\n#coding:utf-8 from numpy import * import operator ##给出训练数据以及对应的类别 def createDataSet(): group = array([[1.0,2.0],[1.2,0.1],[0.1,1.4],[0.3,3.5]]) labels = ['A','A','B','B'] return group,labels ###通过KNN进行分类 def classify(input,dataSe t,label,k): dataSize = dataSet.shape[0] ####计算欧式距离 diff = tile(input,(dataSize,1)) - dataSet sqdiff = diff ** 2 squareDist = sum(sqdiff,axis = 1)###行向量分别相加，从而得到新的一个行向量 dist = squareDist ** 0.5 ##对距离进行排序 sortedDistIndex = argsort(dist)##argsort()根据元素的值从大到小对元素进行排序，返回下标 classCount={} for i in range(k): voteLabel = label[sortedDistIndex[i]] ###对选取的K个样本所属的类别个数进行统计 classCount[voteLabel] = classCount.get(voteLabel,0) + 1 ###选取出现的类别次数最多的类别 maxCount = 0 for key,value in classCount.items(): if value > maxCount: maxCount = value classes = key return classes\n接下来，在命令行窗口输入如下代码：\n#-*-coding:utf-8 -*- import sys sys.path.append(\"...文件路径...\") import KNN from numpy import * dataSet,labels = KNN.createDataSet() input = array([1.1,0.3]) K = 3 output = KNN.classify(input,dataSet,labels,K) print(\"测试数据为:\",input,\"分类结果为：\",output)\n回车之后的结果为：\n测试数据为： [ 1.1  0.3] 分类为： A\n答案符合我们的预期，要证明算法的准确性，势必还需要通过处理复杂问题进行验证，之后另行说明。\n这是第一次用python编的一个小程序，势必会遇到各种问题，在此次编程调试过程中遇到了如下问题：\n1 导入.py文件路径有问题，因此需要在最开始加如下代码：\nimport sys\nsys.path.append(\"文件路径\")，这样就不会存在路径有误的问题了；\n2 在python提示代码存在问题时，一定要及时改正，改正之后保存之后再执行命令行，这一点跟MATLAB是不一样的，所以在python中最好是敲代码的同时在命令行中一段一段的验证；\n3 在调用文件时函数名一定要写正确，否则会出现：'module' object has no attribute 'creatDataSet'；\n4 'int' object has no attribute 'kclassify'，这个问题出现的原因是之前我讲文件保存名为k.py,在执行\noutput = K.classify(input,dataSet,labels,K)这一句就会出错。根据函数式编程的思想，每个函数都可以看为是一个变量而将K赋值后，调用k.py时就会出现问题。\n三 MATLAB实现\n之前一直在用MATLAB做聚类算法的一些优化，其次就是数模的一些常用算法，对于别的算法，还真是没有上手编过，基础还在，思想还在，当然要动手编一下，也是不希望在学python的同时对MATLAB逐渐陌生吧，走走停停，停很重要。\n首先，建立KNN.m文件，如下：\n%% KNN clear all clc %% data trainData = [1.0,2.0;1.2,0.1;0.1,1.4;0.3,3.5]; trainClass = [1,1,2,2]; testData = [0.5,2.3]; k = 3; %% distance row = size(trainData,1); col = size(trainData,2); test = repmat(testData,row,1); dis = zeros(1,row); for i = 1:row diff = 0; for j = 1:col diff = diff + (test(i,j) - trainData(i,j)).^2; end dis(1,i) = diff.^0.5; end %% sort jointDis = [dis;trainClass]; sortDis= sortrows(jointDis'); sortDisClass = sortDis'; %% find class = sort(2:1:k); member = unique(class); num = size(member); max = 0; for i = 1:num count = find(class == member(i)); if count > max max = count; label = member(i); end end disp('最终的分类结果为：'); fprintf('%d\\n',label)\n运行之后的结果是，最终的分类结果为：2。和预期结果一样。\n三 实战\n之前，对KNN进行了一个简单的验证，今天我们使用KNN改进约会网站的效果，个人理解，这个问题也可以转化为其它的比如各个网站迎合客户的喜好所作出的推荐之类的，当然，今天的这个例子功能也实在有限。\n在这里根据一个人收集的约会数据，根据主要的样本特征以及得到的分类，对一些未知类别的数据进行分类，大致就是这样。\n我使用的是python 3.4.3,首先建立一个文件，例如date.py,具体的代码如下：\n#coding:utf-8 from numpy import * import operator from collections import Counter import matplotlib import matplotlib.pyplot as plt ###导入特征数据 def file2matrix(filename): fr = open(filename) contain = fr.readlines()###读取文件的所有内容 count = len(contain) returnMat = zeros((count,3)) classLabelVector = [] index = 0 for line in contain: line = line.strip() ###截取所有的回车字符 listFromLine = line.split('\\t') returnMat[index,:] = listFromLine[0:3]###选取前三个元素，存储在特征矩阵中 classLabelVector.append(listFromLine[-1])###将列表的最后一列存储到向量classLabelVector中 index += 1 ##将列表的最后一列由字符串转化为数字，便于以后的计算 dictClassLabel = Counter(classLabelVector) classLabel = [] kind = list(dictClassLabel) for item in classLabelVector: if item == kind[0]: item = 1 elif item == kind[1]: item = 2 else: item = 3 classLabel.append(item) return returnMat,classLabel#####将文本中的数据导入到列表 ##绘图（可以直观的表示出各特征对分类结果的影响程度） datingDataMat,datingLabels = file2matrix('D:\\python\\Mechine learing in Action\\KNN\\datingTestSet.txt') fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*array(datingLabels),15.0*array(datingLabels)) plt.show() ## 归一化数据,保证特征等权重 def autoNorm(dataSet): minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet))##建立与dataSet结构一样的矩阵 m = dataSet.shape[0] for i in range(1,m): normDataSet[i,:] = (dataSet[i,:] - minVals) / ranges return normDataSet,ranges,minVals ##KNN算法 def classify(input,dataSet,label,k): dataSize = dataSet.shape[0] ####计算欧式距离 diff = tile(input,(dataSize,1)) - dataSet sqdiff = diff ** 2 squareDist = sum(sqdiff,axis = 1)###行向量分别相加，从而得到新的一个行向量 dist = squareDist ** 0.5 ##对距离进行排序 sortedDistIndex = argsort(dist)##argsort()根据元素的值从大到小对元素进行排序，返回下标 classCount={} for i in range(k): voteLabel = label[sortedDistIndex[i]] ###对选取的K个样本所属的类别个数进行统计 classCount[voteLabel] = classCount.get(voteLabel,0) + 1 ###选取出现的类别次数最多的类别 maxCount = 0 for key,value in classCount.items(): if value > maxCount: maxCount = value classes = key return classes ##测试(选取10%测试） def datingTest(): rate = 0.10 datingDataMat,datingLabels = file2matrix('D:\\python\\Mechine learing in Action\\KNN\\datingTestSet.txt') normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] testNum = int(m * rate) errorCount = 0.0 for i in range(1,testNum): classifyResult = classify(normMat[i,:],normMat[testNum:m,:],datingLabels[testNum:m],3) print(\"分类后的结果为:,\", classifyResult) print(\"原结果为：\",datingLabels[i]) if(classifyResult != datingLabels[i]): errorCount += 1.0 print(\"误分率为:\",(errorCount/float(testNum))) ###预测函数 def classifyPerson(): resultList = ['一点也不喜欢','有一丢丢喜欢','灰常喜欢'] percentTats = float(input(\"玩视频所占的时间比?\")) miles = float(input(\"每年获得的飞行常客里程数?\")) iceCream = float(input(\"每周所消费的冰淇淋公升数?\")) datingDataMat,datingLabels = file2matrix('D:\\python\\Mechine learing in Action\\KNN\\datingTestSet2.txt') normMat,ranges,minVals = autoNorm(datingDataMat) inArr = array([miles,percentTats,iceCream]) classifierResult = classify((inArr-minVals)/ranges,normMat,datingLabels,3) print(\"你对这个人的喜欢程度:\",resultList[classifierResult - 1])\n新建test.py文件了解程序的运行结果，代码：\n#coding:utf-8 from numpy import * import operator from collections import Counter import matplotlib import matplotlib.pyplot as plt import sys sys.path.append(\"D:\\python\\Mechine learing in Action\\KNN\") import date date.classifyPerson()\n运行结果如下图：\n以上，是对本次算法的整理和总结。"}
{"content2":"注：正则化是用来防止过拟合的方法。在最开始学习机器学习的课程时，只是觉得这个方法就像某种魔法一样非常神奇的改变了模型的参数。但是一直也无法对其基本原理有一个透彻、直观的理解。直到最近再次接触到这个概念，经过一番苦思冥想后终于有了我自己的理解。\n0. 正则化（Regularization ）\n前面使用多项式回归，如果多项式最高次项比较大，模型就容易出现过拟合。正则化是一种常见的防止过拟合的方法，一般原理是在代价函数后面加上一个对参数的约束项，这个约束项被叫做正则化项（regularizer）。在线性回归模型中，通常有两种不同的正则化项：\n加上所有参数（不包括$\\theta_0$）的绝对值之和，即$l1$范数，此时叫做Lasso回归；\n加上所有参数（不包括$\\theta_0$）的平方和，即$l2$范数，此时叫做岭回归.\n看过不少关于正则化原理的解释，但是都没有获得一个比较直观的理解。下面用代价函数的图像以及正则化项的图像来帮助解释正则化之所以起作用的原因。\n0.1 代价函数的图像\n为了可视化，选择直线方程进行优化。假设一个直线方程以及代价函数如下：\n$\\hat{h}_{\\theta} = \\theta_0 + \\theta_1 x$，该方程只有一个特征$x$，两个参数$\\theta_0$和$\\theta_1$\n$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{(\\theta_0 + \\theta_1 x^{(i)} - y^{(i)})^2}$，该代价函数为均方误差函数（MSE），其中$m$表示样本量.\n为了保持简单，只取一个样本点$(1, 1)$代入上面的代价函数方程中，可得$J(\\theta) = (\\theta_0 + \\theta_1 - 1)^2$. 该式是一个二元一次方程，可以在3维空间中作图（下面利用网站GeoGebra画出该方程的图像）：\n-1，代入样本点$(1, 1)$后的代价函数MSE的图像\n由于多个样本点的代价函数是所有样本点代价函数之和，且不同的样本点只是相当于改变了代价函数中两个变量的参数（此时$\\theta_0$和$\\theta_1$是变量，样本点的取值是参数）。因此多样本的代价函数MSE的图像只会在-1上发生缩放和平移，而不会发生过大的形变。\n对于坐标轴，表示如下：\n使用$J$轴表示蓝色轴线，上方为正向；\n使用$\\theta_1$表示红色轴线，左边为正向；\n使用$\\theta_0$表示绿色轴线，指向屏幕外的方向为正向.\n此时的函数图像相当于一条抛物线沿着平面$J = 0$上直线$\\theta_0 = - \\theta_1$平移后形成的图像。\n0.2 正则化项的图像\n这里使用$L1$范数作为正则化项，加上正则化项之后MSE代价函数变成：\n$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{(\\theta_0 + \\theta_1 x^{(i)} - y^{(i)})^2}  + \\lambda ||\\theta_1||_1$,\n上式中$\\lambda$是正则化项的参数，为了简化取$\\lambda = 1$。由于正则化项中始终不包含截距项$\\theta_0$，此时的$L1$范数相当于参数$\\theta_1$的绝对值，函数图像如下：\n-2，$L1$正则化项的图像\n此时的函数图像相当于一张对折后，半张开的纸。纸的折痕与平面$J = 0$上$\\theta_0$轴重叠。\n0.3 代价函数与正则化项图像的叠加\n直接将这两个图像放在一起的样子：\n-3，同时显示代价函数与正则化项的图像\n将两个方程相加之后，即$J(\\theta) = (\\theta_0 + \\theta_1 - 1)^2 + |\\theta_1|$，做图可以得到下面的图像：\n-4，加入正则化项之后代价函数的图像\n此时的图像，就像是一个圆锥体被捏扁了之后，立在坐标原点上。观察添加正则化项前后的图像，我们会发现：\n加上正则化项之后，此时损失函数就分成了两部分：第1项为原来的MSE函数，第2项为正则化项，最终的结果是这两部分的线性组合;\n在第1项的值非常小但在第2项的值非常大的区域，这些值会受到正则化项的巨大影响，从而使得这些区域的值变的与正则化项近似：例如原来的损失函数沿$\\theta_0 = -\\theta_1$，$J$轴方向上的值始终为0，但是加入正则化项$J = |\\theta_1|$后，该直线上原来为0的点，都变成了$\\theta_1$的绝对值。这就像加权平均值一样，哪一项的权重越大，对最终结果产生的影响也越大;\n如果想象一种非常极端的情况：在参数的整个定义域上，第2项的取值都远远大于第一项的取值，那么最终的损失函数几乎100%都会由第2项决定，也就是整个代价函数的图像会非常类似于$J=|\\theta_1|$（-2）而不是原来的MSE函数的图像（-1）。这时候就相当于$\\lambda$的取值过大的情况，最终的全局最优解将会是坐标原点，这就是为什么在这种情况下最终得到的解全都为0.\n1. 岭回归\n岭回归与多项式回归唯一的不同在于代价函数上的差别。岭回归的代价函数如下：\n$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}{(y^{(i)} - (w x^{(i)} + b))^2}  + \\lambda ||w||_2^2 = MSE(\\theta) + \\lambda \\sum_{i = 1}^{n}{\\theta_i^2} \\ \\quad \\cdots \\ (1 - 1)$$\n为了方便计算导数，通常也写成下面的形式：\n$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m}{(y^{(i)} - (w x^{(i)} + b))^2}  + \\frac{\\lambda}{2} ||w||_2^2 = \\frac{1}{2}MSE(\\theta) + \\frac{\\lambda}{2} \\sum_{i = 1}^{n}{\\theta_i^2} \\ \\quad \\cdots \\ (1 - 2)$$\n上式中的$w$是长度为$n$的向量，不包括截距项的系数$\\theta_0$；$\\theta$是长度为$n + 1$的向量，包括截距项的系数$\\theta_0$；$m$为样本数；$n$为特征数.\n岭回归的代价函数仍然是一个凸函数，因此可以利用梯度等于0的方式求得全局最优解（正规方程）：\n$$\\theta = (X^T X + \\lambda I)^{-1}(X^T y)$$\n上述正规方程与一般线性回归的正规方程相比，多了一项$\\lambda I$，其中$I$表示单位矩阵。假如$X^T X$是一个奇异矩阵（不满秩），添加这一项后可以保证该项可逆。由于单位矩阵的形状是对角线上为1其他地方都为0，看起来像一条山岭，因此而得名。\n除了上述正规方程之外，还可以使用梯度下降的方式求解（求梯度的过程可以参考一般线性回归，3.2.2节）。这里采用式子$1 - 2$来求导：\n$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{m} X^T \\cdot (X \\cdot \\theta - y)  + \\lambda w \\ \\quad \\cdots \\ (1 - 3) $$\n因为式子$1- 2$中和式第二项不包含$\\theta_0$，因此求梯度后，上式第二项中的$w$本来也不包含$\\theta_0$。为了计算方便，添加$\\theta_0 = 0$到$w$.\n因此在梯度下降的过程中，参数的更新可以表示成下面的公式：\n$$\\theta = \\theta - (\\frac{\\alpha}{m} X^T \\cdot (X \\cdot \\theta - y)  + \\lambda w) \\ \\quad \\cdots \\ (1 - 4) $$\n其中$\\alpha$为学习率，$\\lambda$为正则化项的参数\n1.1 数据以及相关函数\n1 import numpy as np 2 import matplotlib.pyplot as plt 3 from sklearn.preprocessing import PolynomialFeatures 4 from sklearn.metrics import mean_squared_error 5 6 data = np.array([[ -2.95507616, 10.94533252], 7 [ -0.44226119, 2.96705822], 8 [ -2.13294087, 6.57336839], 9 [ 1.84990823, 5.44244467], 10 [ 0.35139795, 2.83533936], 11 [ -1.77443098, 5.6800407 ], 12 [ -1.8657203 , 6.34470814], 13 [ 1.61526823, 4.77833358], 14 [ -2.38043687, 8.51887713], 15 [ -1.40513866, 4.18262786]]) 16 m = data.shape[0] # 样本大小 17 X = data[:, 0].reshape(-1, 1) # 将array转换成矩阵 18 y = data[:, 1].reshape(-1, 1)\n继续使用多项式回归中的数据。\n1.2 岭回归的手动实现\n有了上面的理论基础，就可以自己实现岭回归了，下面是Python代码：\n1 # 代价函数 2 def L_theta(theta, X_x0, y, lamb): 3 \"\"\" 4 lamb: lambda, the parameter of regularization 5 theta: (n+1)·1 matrix, contains the parameter of x0=1 6 X_x0: m·(n+1) matrix, plus x0 7 \"\"\" 8 h = np.dot(X_x0, theta) # np.dot 表示矩阵乘法 9 theta_without_t0 = theta[1:] 10 L_theta = 0.5 * mean_squared_error(h, y) + 0.5 * lamb * np.sum(np.square(theta_without_t0)) 11 return L_theta 12 13 # 梯度下降 14 def GD(lamb, X_x0, theta, y, alpha): 15 \"\"\" 16 lamb: lambda, the parameter of regularization 17 alpha: learning rate 18 X_x0: m·(n+1), plus x0 19 theta: (n+1)·1 matrix, contains the parameter of x0=1 20 \"\"\" 21 for i in range(T): 22 h = np.dot(X_x0, theta) 23 theta_with_t0_0 = np.r_[np.zeros([1, 1]), theta[1:]] # set theta[0] = 0 24 theta -= (alpha * 1/m * np.dot(X_x0.T, h - y) + lamb*(theta_with_t0_0)) # add the gradient of regularization term 25 if i%50000==0: 26 print(L_theta(theta, X_x0, y, lamb)) 27 return theta 28 29 T = 1200000 # 迭代次数 30 degree = 11 31 theta = np.ones((degree + 1, 1)) # 参数的初始化，degree = 11，一个12个参数 32 alpha = 0.0000000006 # 学习率 33 # alpha = 0.003 # 学习率 34 lamb = 0.0001 35 # lamb = 0 36 poly_features_d = PolynomialFeatures(degree=degree, include_bias=False) 37 X_poly_d = poly_features_d.fit_transform(X) 38 X_x0 = np.c_[np.ones((m, 1)), X_poly_d] # ADD X0 = 1 to each instance 39 theta = GD(lamb=lamb, X_x0=X_x0, theta=theta, y=y, alpha=alpha)\n上面第10行对应公式$1-2$，第24行对应公式$1-3$。由于自由度比较大，此时利用梯度下降的方法训练模型比较困难，学习率稍微大一点就会出现出现损失函数的值越过最低点不断增长的情况。下面是训练结束后的参数以及代价函数值：\n[[ 1.00078848e+00] [ -1.03862735e-05] [ 3.85144400e-05] [ -3.77233288e-05] [ 1.28959318e-04] [ -1.42449160e-04] [ 4.42760996e-04] [ -5.11518471e-04] [ 1.42533716e-03] [ -1.40265037e-03] [ 3.13638870e-03] [ 1.21862016e-03]] 3.59934190413\n从上面的结果看，截距项的参数最大，高阶项的参数都比较小。下面是比较原始数据和训练出来的模型之间的关系：\n1 X_plot = np.linspace(-2.99, 1.9, 1000).reshape(-1, 1) 2 poly_features_d_with_bias = PolynomialFeatures(degree=degree, include_bias=True) 3 X_plot_poly = poly_features_d_with_bias.fit_transform(X_plot) 4 y_plot = np.dot(X_plot_poly, theta) 5 plt.plot(X_plot, y_plot, 'r-') 6 plt.plot(X, y, 'b.') 7 plt.xlabel('x') 8 plt.ylabel('y') 9 plt.show()\n-1，手动实现岭回归的效果\n图中模型与原始数据的匹配度不是太好，但是过拟合的情况极大的改善了，模型变的更简单了。\n1.2 正规方程\n下面使用正规方程求解：\n其中$\\lambda = 10$\n1 theta2 = np.linalg.inv(np.dot(X_x0.T, X_x0) + 10*np.identity(X_x0.shape[1])).dot(X_x0.T).dot(y) 2 print(theta2) 3 print(L_theta(theta2, X_x0, y, lamb)) 4 5 X_plot = np.linspace(-3, 2, 1000).reshape(-1, 1) 6 poly_features_d_with_bias = PolynomialFeatures(degree=degree, include_bias=True) 7 X_plot_poly = poly_features_d_with_bias.fit_transform(X_plot) 8 y_plot = np.dot(X_plot_poly, theta2) 9 plt.plot(X_plot, y_plot, 'r-') 10 plt.plot(X, y, 'b.') 11 plt.xlabel('x') 12 plt.ylabel('y') 13 plt.show()\n参数即代价函数的值：\n[[ 0.56502653] [-0.12459546] [ 0.26772443] [-0.15642405] [ 0.29249514] [-0.10084392] [ 0.22791769] [ 0.1648667 ] [-0.05686718] [-0.03906615] [-0.00111673] [ 0.00101724]] 0.604428719639\n从参数来看，截距项的系数减小了，1-7阶都有比较大的参数都比较大，后面更高阶项的参数越来越小，下面是函数图像：\n-2，使用正规方程求解\n从图中可以看到，虽然模型的自由度没变，还是11，但是过拟合的程度得到了改善。\n1.3 使用scikit-learn\nscikit-learn中有专门计算岭回归的函数，而且效果要比上面的方法好。使用scikit-learn中的岭回归，只需要输入以下参数：\nalpha: 上面公式中的$\\lambda$，正则化项的系数；\nsolver: 求解方法；\nX: 训练样本；\ny: 训练样本的标签.\n1 from sklearn.linear_model import Ridge 2 3 # 代价函数 4 def L_theta_new(intercept, coef, X, y, lamb): 5 \"\"\" 6 lamb: lambda, the parameter of regularization 7 theta: (n+1)·1 matrix, contains the parameter of x0=1 8 X_x0: m·(n+1) matrix, plus x0 9 \"\"\" 10 h = np.dot(X, coef) + intercept # np.dot 表示矩阵乘法 11 L_theta = 0.5 * mean_squared_error(h, y) + 0.5 * lamb * np.sum(np.square(coef)) 12 return L_theta 13 14 lamb = 10 15 ridge_reg = Ridge(alpha=lamb, solver=\"cholesky\") 16 ridge_reg.fit(X_poly_d, y) 17 print(ridge_reg.intercept_, ridge_reg.coef_) 18 print(L_theta_new(intercept=ridge_reg.intercept_, coef=ridge_reg.coef_.T, X=X_poly_d, y=y, lamb=lamb)) 19 20 X_plot = np.linspace(-3, 2, 1000).reshape(-1, 1) 21 X_plot_poly = poly_features_d.fit_transform(X_plot) 22 h = np.dot(X_plot_poly, ridge_reg.coef_.T) + ridge_reg.intercept_ 23 plt.plot(X_plot, h, 'r-') 24 plt.plot(X, y, 'b.') 25 plt.show()\n训练结束后得到的参数为（分别表示截距，特征的系数；代价函数的值）：\n[ 3.03698398] [[ -2.95619849e-02 6.09137803e-02 -4.93919290e-02 1.10593684e-01 -4.65660197e-02 1.06387336e-01 5.14340826e-02 -2.29460359e-02 -1.12705709e-02 -1.73925386e-05 2.79198986e-04]] 0.213877232488\n-3，使用scikit-learn训练岭回归\n经过与前面两种方法得到的结果比较，这里得到的曲线更加平滑，不仅降低了过拟合的风险，代价函数的值也非常低。\n2. Lasso回归\nLasso回归于岭回归非常相似，它们的差别在于使用了不同的正则化项。最终都实现了约束参数从而防止过拟合的效果。但是Lasso之所以重要，还有另一个原因是：Lasso能够将一些作用比较小的特征的参数训练为0，从而获得稀疏解。也就是说用这种方法，在训练模型的过程中实现了降维(特征筛选)的目的。\nLasso回归的代价函数为：\n$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m}{(y^{(i)} - (w x^{(i)} + b))^2}  + \\lambda ||w||_1 = \\frac{1}{2}MSE(\\theta) + \\lambda \\sum_{i = 1}^{n}{|\\theta_i|} \\ \\quad \\cdots \\ (2 - 1)$$\n上式中的$w$是长度为$n$的向量，不包括截距项的系数$θ_0$, $θ$是长度为$n+1$的向量，包括截距项的系数$θ_0$，$m$为样本数，$n$为特征数.\n$||w||_1$表示参数$w$的$l1$范数，也是一种表示距离的函数。加入$w$表示3维空间中的一个点$(x, y, z)$，那么$||w||_1 = |x| + |y| + |z|$，即各个方向上的绝对值（长度）之和。\n式子$2-1$的梯度为：\n$$\\nabla_{\\theta}MSE(\\theta) + \\lambda \\begin{pmatrix} sign(\\theta_1) \\\\  sign(\\theta_2) \\\\ \\vdots \\\\ sign(\\theta_n) \\end{pmatrix} \\quad \\cdots \\ (2-2)$$\n其中$sign(\\theta_i)$由$\\theta_i$的符号决定: $\\theta_i > 0, sign(\\theta_i) = 1; \\ \\theta_i = 0, sign(\\theta_i) = 0; \\ \\theta_i < 0, sign(\\theta_i) = -1$.\n2.1 Lasso的实现\n直接使用scikit-learn中的函数：\n可以参考官方文档，http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n下面模型中的参数alpha就是公式(2-1)中的参数$\\lambda$，是正则化项的系数，可以取大于0的任意值。alpha的值越大，对模型中参数的惩罚力度越大，因此会有更多的参数被训练为0（只对线性相关的参数起作用），模型也就变得更加简单了。\n1 from sklearn.linear_model import Lasso 2 3 lamb = 0.025 4 lasso_reg = Lasso(alpha=lamb) 5 lasso_reg.fit(X_poly_d, y) 6 print(lasso_reg.intercept_, lasso_reg.coef_) 7 print(L_theta_new(intercept=lasso_reg.intercept_, coef=lasso_reg.coef_.T, X=X_poly_d, y=y, lamb=lamb)) 8 9 X_plot = np.linspace(-3, 2, 1000).reshape(-1, 1) 10 X_plot_poly = poly_features_d.fit_transform(X_plot) 11 h = np.dot(X_plot_poly, lasso_reg.coef_.T) + lasso_reg.intercept_ 12 plt.plot(X_plot, h, 'r-') 13 plt.plot(X, y, 'b.') 14 plt.show()\n最终获得的参数以及代价函数的值为：\n其中计算代价函数值的函数\"L_theta_new\"需要修改其中的\"L_theta\"为\"L_theta = 0.5 * mean_squared_error(h, y) + lamb * np.sum(np.abs(coef))\"\n[ 2.86435179] [ -0.00000000e+00 5.29099723e-01 -3.61182017e-02 9.75614738e-02 1.61971116e-03 -3.42711766e-03 2.78782527e-04 -1.63421713e-04 -5.64291215e-06 -1.38933655e-05 1.02036898e-06]\n0.0451291096773\n从结果可以看到，截距项的值最大，一次项的系数为0，二次项的系数是剩下的所有项中值最大的，也比较符合数据的真实来源。这里也可以看出来，更高阶的项虽然系数都非常小但不为0，这是因为这些项之间的关系是非线性的，无法用线性组合互相表示。\n-1，Lasso回归得到的图像\n-1是目前在$degree=11$的情况下，得到的最好模型。\n3. 弹性网络（ Elastic Net）\n弹性网络是结合了岭回归和Lasso回归，由两者加权平均所得。据介绍这种方法在特征数大于训练集样本数或有些特征之间高度相关时比Lasso更加稳定。\n其代价函数为：\n$$J(\\theta) = \\frac{1}{2}MSE(\\theta) + r\\lambda \\sum_{i = 1}^{n}{|\\theta_i|} + \\frac{1-r}{2} \\lambda \\sum_{i=1}^{n} {\\theta_i^2} \\ \\quad \\cdots \\ (3 - 1)$$\n其中$r$表示$l1$所占的比例。\n使用scikit-learn的实现：\n1 from sklearn.linear_model import ElasticNet 2 3 # 代价函数 4 def L_theta_ee(intercept, coef, X, y, lamb, r): 5 \"\"\" 6 lamb: lambda, the parameter of regularization 7 theta: (n+1)·1 matrix, contains the parameter of x0=1 8 X_x0: m·(n+1) matrix, plus x0 9 \"\"\" 10 h = np.dot(X, coef) + intercept # np.dot 表示矩阵乘法 11 L_theta = 0.5 * mean_squared_error(h, y) + r * lamb * np.sum(np.abs(coef)) + 0.5 * (1-r) * lamb * np.sum(np.square(coef)) 12 return L_theta 13 14 elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.8) 15 elastic_net.fit(X_poly_d, y) 16 print(elastic_net.intercept_, elastic_net.coef_) 17 print(L_theta_ee(intercept=elastic_net.intercept_, coef=elastic_net.coef_.T, X=X_poly_d, y=y, lamb=0.1, r=0.8)) 18 19 X_plot = np.linspace(-3, 2, 1000).reshape(-1, 1) 20 X_plot_poly = poly_features_d.fit_transform(X_plot) 21 h = np.dot(X_plot_poly, elastic_net.coef_.T) + elastic_net.intercept_ 22 plt.plot(X_plot, h, 'r-') 23 plt.plot(X, y, 'b.') 24 plt.show()\n得到的结果为：\n[ 3.31466833] [ -0.00000000e+00 0.00000000e+00 -0.00000000e+00 1.99874040e-01 -1.21830209e-02 2.58040545e-04 3.01117857e-03 -8.54952421e-04 4.35227606e-05 -2.84995639e-06 -8.36248799e-06] 0.0807738447192\n该方法中得到了，更多的0，当然这也跟参数的设置有关。\n-1，使用elastic-net得到的结果\n4. 正则化项的使用以及l1与l2的比较\n根据吴恩达老师的机器学习公开课，建议使用下面的步骤来确定$\\lambda$的值：\n创建一个$\\lambda$值的列表，例如$\\lambda \\in {0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24}$;\n创建不同degree的模型（或改变其他变量）;\n遍历不同的模型和不同的$\\lambda$值;\n使用学习到的参数$\\theta$（包含正则化项）计算验证集上的误差（计算误差时不包含正则化项），$J_{CV}(\\theta)$;\n选择在验证集上误差最小的参数组合（degree和$\\lambda$）;\n使用选出来的参数和$\\lambda$在测试集上测试，计算$J_{test}(\\theta)$.\n下面通过一张图像来比较一下岭回归和Lasso回归：\n-1，Lasso与岭回归的比较（俯瞰图）\n上图中，左上方表示$l1$（图中菱形图案）和代价函数（图中深色椭圆环）；左下方表示$l2$（椭圆形线圈）和代价函数（图中深色椭圆环）。同一条线上（或同一个环上），表示对应的函数值相同；图案中心分别表示$l1, l2$范数以及代价函数的最小值位置。\n右边表示代价函数加上对应的正则化项之后的图像。添加正则化项之后，会影响原来的代价函数的最小值的位置，以及梯度下降时的路线（如果参数调整合适的话，最小值应该在距离原来代价函数最小值附近且与正则化项的图像相交，因为此时这两项在相互约束的情况下都取到最小值，它们的和也最小）。右上图，显示了Lasso回归中参数的变化情况，最终停留在了$\\theta_2 = 0$这条线上；右下方的取值由于受到了$l2$范数的约束，也产生了位移。\n当正则化项的权重非常大的时候，会产生左侧黄色点标识的路线，最终所有参数都为0，但是趋近原点的方式不同。这是因为对于范数来说，原点是它们的最小值点。\nReference\nhttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\nGéron A. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems[M]. \" O'Reilly Media, Inc.\", 2017. github\nhttps://www.coursera.org/learn/machine-learning\nedx: UCSanDiegoX - DSE220x Machine Learning Fundamentals"}
{"content2":"What is Oryx?\n大名鼎鼎的Sean Owen (http://www.linkedin.com/in/srowen) 正在 Cloudera 作为 Director of Data Science 专注投身于名为Oryx的开源机器学习项目当中。（Oryx意思是剑羚，属于非洲羚羊的一类分支）。如果您读过《Mahout in Action》这本书，您应该有印象：此书的作者之一就是Sean Owen。\nOryx的开发意图在于帮助Hadoop用户构建机器学习模式并将其加以部署，这样我们就能够以实时方式查询并获取其结果——例如将其作为垃圾邮件过滤器或者推荐引擎的组成部分。\n作为Hadoop当中实现机器学习模式创建的传统途径，Apache Mahout \"已经走到了发展道路的尽头。\"Owen如是说。\nThe Oryx open source project provides simple, real-time large-scale machine learning / predictive analytics infrastructure.\nArchitecture\nOryx does two things at heart: builds models, and serves models. These are the responsibilities of two separate components, the Computation Layer and Serving Layer, respectively.\n在 IntelliJ IDEA 中编译 Oryx\n虽然我是忠实的 Visual Studio 的拥趸（老牌的Windows Phone Developer），但是在工作中不可避免也会遇到 Java 开发的工作，比如目前专注的基于机器学习算法的推荐引擎。对我而言，Eclipse 是生命中不能承受之轻（至少目前为止是如此），之前基于 Apache Mahout 所做的推荐算法是在 Eclipse 中编码和生成Jar包，遇到的问题无数。比如在 Eclipse 中导入 不管是 Mahout 还是 Oryx 的源码都出现我无法解决的编译错误。众所周知 Mahout 和 Oryx 都是基于 Maven 编译的，而在我的 Eclipse 开发环境中出现的 Maven 编译错误，花费了很多时间查找资料，但是最终还是无从解决。\n在得知机器学习领域的大神Sean Owen 使用 IntelliJ IDEA 开发 Oryx ，毅然决然地安装 IntelliJ IDEA Community Edition ( http://www.jetbrains.com/idea/download/ )。\n在 Cloudera 的开发者社区 Data Science and Machine Learning 论坛您可发现 Owen 的足迹，耐心地为开发者解答 Apache Mahout 和 Oryx 的疑问。\n在 Windows 8.1 中启动 IntelliJ IDEA，打开从 GitHub 下载的 Oryx 开源代码 (https://github.com/cloudera/oryx ) 。\n在 Oryx Project 上点击鼠标右键，选择 \"Maven\" – \"Reimport\"，IntelliJ IDEA 重新导入项目所依赖的 Maven Libraries。\n至此，Oryx 的源码即可编译成功，研究之，学习之。"}
{"content2":"相似图片搜索的原理\nhttp://www.ruanyifeng.com/blog/2011/07/principle_of_similar_image_search.html\n如何识别图像边缘？\nhttp://www.ruanyifeng.com/blog/2016/07/edge-recognition.html\n数字图像处理的基本原理和常用方法\nhttps://b2museum.cdstm.cn/identification/sztxcl-relative.htm\n人脸识别的十个关键技术组成及原理\nhttps://www.zhinengl.com/2017/08/face-recognition-key-tech-and-principles/\n手写数字识别系统之图像分割\nhttp://www.cnblogs.com/wuxian11/p/6498693.html\n闻声识人——声纹识别技术简介\nhttp://www.cnblogs.com/wuxian11/p/6498699.html\n与计算机之间的另一种沟通方式 ——“手势识别”\nhttp://www.cnblogs.com/wuxian11/p/6498675.html\n二维码的生成细节和原理\nhttps://coolshell.cn/articles/10590.html\n车牌识别及验证码识别的一般思路\nhttp://www.cnblogs.com/xiaotie/archive/2009/01/15/1376677.html\n深度学习之人脸识别\nhttp://www.fanyeong.com/2018/02/04/dl-face-recognition/\n深入浅出人脸识别原理\nhttps://dxyoo7.github.io/2017/11/28/face_recognition/\n一种通过变量分配进脸谱图的思路与方法研究\nhttp://www.stats.gov.cn/tjzs/tjsj/tjcb/dysj/201709/t20170929_1539211.html\n世界上最简单的机器学习介绍\nhttps://zhuanlan.zhihu.com/p/24339995\nhttps://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471\n视频教程：https://www.lynda.com/Data-Science-tutorials/Welcome/548594/598220-4.html?lpk35=9149&utm_medium=ldc-partner&utm_source=CMPRC&utm_content=524&utm_campaign=CD20575&bid=524&aid=CD20575\n使用机器学习生成超级马里奥制造商水平\nhttps://zhuanlan.zhihu.com/p/24344720\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3\n图像识别【鸟or飞机】？深度学习与卷积神经网络\nhttps://zhuanlan.zhihu.com/p/24524583\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721\n用深度学习识别人脸\nhttps://zhuanlan.zhihu.com/p/24567586 （中文版）\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78 （原版）\n此文章介绍的易懂明了，可以使用下面这张图像搜索相关文章（有时或使用文字搜索不出来）\nGoogle 翻译背后的黑科技：神经网络和序列到序列学习\nhttps://zhuanlan.zhihu.com/p/24590838\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa\n如何用深度学习进行语音识别\nhttps://zhuanlan.zhihu.com/p/24703268\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a\n滥用生成敌对 网络制作8位像素艺术\nhttps://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7\n如何故意欺骗神经网络\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196\n机器学习：作者专栏地址 https://medium.com/@ageitgey\nViola-Jones对象检测框架\nhttps://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework#Feature_types_and_evaluation\nOpenCV+OCR 图像处理字符识别原理及代码\nhttp://blog.csdn.net/qq_29672495/article/details/53648300"}
{"content2":"基于中科院seetaface2进行封装的JAVA人脸识别库，支持人脸识别、1:1比对、1:N比对。\n项目介绍\n基于中科院seetaface2进行封装的JAVA人脸识别算法库，支持人脸识别、1:1比对、1:N比对。 seetaface2：https://github.com/seetaface/SeetaFaceEngine2\n环境配置\n1、下载model（ https://pan.baidu.com/s/1HJj8PEnv3SOu6ZxVpAHPXg ） 文件到本地，并解压出来；\n2、下载doc目录中对应的lib包到本地并解压：Windows(64位)环境下载lib-win-x64.zip、Linux(64位)下载lib-linux-x64.tar.bz2，Linux环境还需要安装依赖库，详见：https://my.oschina.net/u/1580184/blog/3042404 ；\n3、将doc中的faces-data.db下载到本地；（PS：如果不需要使用1:N人脸搜索,不需要此文件，需要将seetafce.properties中的sqlite.db.file配置注释掉）；\n4、将src/main/resources/中的seetaface.properties文件放到项目的resources根目录中；\n#linux系统中依赖的lib名称 libs=holiday,SeetaFaceDetector200,SeetaPointDetector200,SeetaFaceRecognizer200,SeetaFaceCropper200,SeetaFace2JNI #Windows系统中依赖的lib名称 #libs=libgcc_s_sjlj-1,libeay32,libquadmath-0,ssleay32,libgfortran-3,libopenblas,holiday,SeetaFaceDetector200,SeetaPointDetector200,SeetaFaceRecognizer200,SeetaFaceCropper200,SeetaFace2JNI #lib存放目录 libs.path=/usr/local/seetaface2/lib #model存放目录 bindata.dir=/usr/local/seetaface2/bindata ##sqlite配置(如果不用1:N人脸搜索功能，请删除下面5项sqlite开头的配置) sqlite.db.file=/data/faces-data.db sqlite.conn.maxTotal=50 sqlite.conn.maxIdle=5 sqlite.conn.minIdle=0 sqlite.conn.maxWaitMillis=60000\n5、将seetafaceJNI-2.0.jar和依赖包导入到项目中，pom如下:\n<properties> <spring.version>4.2.8.RELEASE</spring.version> <log4j.version>2.8.2</log4j.version> <slf4j.version>1.7.25</slf4j.version> </properties> <dependencies> <dependency> <groupId>com.cnsugar.ai</groupId> <artifactId>seetafaceJNI</artifactId> <version>2.0</version> <!--<scope>system</scope>--> <!--<systemPath>${project.basedir}/lib/seetafaceJNI-2.0.jar</systemPath>--> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-core</artifactId> <version>${spring.version}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>${slf4j.version}</version> </dependency> <!-- sqlite --> <dependency> <groupId>org.xerial</groupId> <artifactId>sqlite-jdbc</artifactId> <version>3.25.2</version> </dependency> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-pool2</artifactId> <version>2.4.2</version> </dependency> </dependencies>\n6、调用FaceHelper中的方法。"}
{"content2":"什么是程序（Program）\n计算机程序，是指为了得到某种结果而可以由计算机（等具有信息处理能力的装置）执行的代码化指令序列（或者可以被自动转换成代码化指令序列的符号化指令序列或者符号化语句序列）。\n通俗讲，计算机给人干活，但它不是人，甚至不如狗懂人的需要（《小羊肖恩》里的狗是多么聪明可爱又忠诚于主人）。那怎么让它干活呢，那就需要程序员用某种编程语言来写程序，编程语言就是计算机能理解的语言，计算机可以执行这些程序（指令），最终完成任务。\n下边的C++程序是完成n的阶乘：\nint n = std::atoi(argv[1]); //求n的阶乘 double result = 1.0; for (int i = 2; i <= n; i++) { result *= i; } std::cout << n << \"的阶乘是：\" << result << std::endl;\n什么是算法（Algorithm）\n算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或者多个操作。\n举个简单的例子，并且大家生活中都能用得上的。现在做个小游戏，A在纸上随机写了一个1到100间的整数，B去猜，猜对的话游戏结束，猜错的话A会告诉B猜的小了还是大了。那么B会怎么做呢，第一次肯定去猜50，每次都猜中间数。为什么呢？因为这样最坏情况下（\\(log_2{100}\\)）六七次就能猜到。\n这就是二分查找，生活中可能就会用得到，而在软件开发中也经常会用得到。\n再来看一个稍微复杂一点点的算法，【快速排序】，面试中考的频率非常高非常高，甚至可以说是必考。\n什么是机器学习算法（Machine Learning）\n机器学习的定义\n《机器学习》书中的定义：\n关于某类任务 T 和性能度量P，如果一个计算机程序能在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E中学习。\n比如AlphaGo：\n任务 T ：下棋\n性能标准 P ：击败对手的百分比\n训练经验：和自己对弈或者比赛经验。\n再比如自动驾驶：\n任务T : 通过视频传感器在高速公路上行驶\n性能标准P：平均无差错行驶里程\n训练经验E：注视人类驾驶时录制的一系列图像和驾驶指令。\n百度百科的定义：\n机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n机器学习的主要任务\n监督学习：\n（1）分类：将实例数据划分到合适的分类中。\nKNN(k-近邻算法)、决策树、朴素贝叶斯、Logistic回归、SVM(支持向量积)。\n（2）回归：预测数值型数据。\n无监督学习：\n（1）聚类：将数据集合分成由类似的对象组成的多个类的过程。\nK-MEANS(K均值聚类)\n神经网络（Neural Network）与深度学习（Deep Learning）\n生物学启示\n人工神经网络ANN的研究一定程度上受到了生物学的启发，生物的学习系统由相互连接的神经元（neuron）组成的异常复杂的网格。而人工神经网络由一系列简单的单元相互密集连接构成的，其中每一个单元有一定数量的实值输入，并产生单一的实数值输出。\n据估计人类的大脑是由大约\\(10^11\\)次方个神经元相互连接组成的密集网络，平均每个神经元与其他\\(10^4\\)个神经元相连。神经元的活性通常被通向其他神经元的连接激活或抑制。\n生物的神经元：\n人工神经元（感知机）：\n多层感知机：\n神经网络表示\n1993年的ALVINN系统是ANN学习的一个典型实例，这个系统使用一个学习到的ANN以正常的速度在高速公路上驾驶汽车。ANN的输入是一个30*32像素的网格，像素的亮度来自一个安装在车辆上的前向摄像机。ANN的输出是车辆行驶的方向。\n浅层学习\n20世纪80年代末期，用于人工神经网络的反向传播算法（也叫Back Propagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基于统计模型的机器学习热潮。这个热潮一直持续到今天。人们发现，利用BP算法可以让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件做预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显出优越性。这个时候的人工神经网络，虽也被称作多层感知机（Multi-layer Perceptron），但实际是种只含有一层隐层节点的浅层模型。\n20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、 Boosting、最大熵方法（如LR，Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂。\n深层学习\n深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。\nDeep learning本身算是machine learning的一个分支，简单可以理解为neural network的发展。\n一种典型的用来识别数字的卷积网络是LeNet-5。当年美国大多数银行就是用它来识别支票上面的手写数字的。能够达到这种商用的地步，它的准确性可想而知。\nLeNet-5的网络结构如下：\n与机器学习相关联的概念\n数据挖掘（Data Mining）\n数据挖掘=机器学习+数据库。数据挖掘是在大型数据存储库中，自动地发现有用信息的过程。\n自然语言处理 （Natural Language Process）\n自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。\n模式识别（Pattern Recognition）\n模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。\n统计学习（Statistical Learning）\n统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。\n计算机视觉（Computer Vision）\n计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。\n语音识别（Speech Recognition）\n语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。\n计算机图形学、数字图像处理、计算机视觉\n计算机视觉（ Computer Vision，简称 CV），是让计算机“看懂”人类看到的世界，输入是图像，输出是图像中的关键信息；\n图片 -> dog or cat?\n图片 -> [xyz xyz xyz ... xyz]\n计算机图形学（Computer Graphics，简称 CG），是让计算机“描述”人类看到的世界，输入是三维模型和场景描述，输出是渲染图像；\n[xyz xyz xyz ... xyz] -> 图片\n数字图像处理（Digital Image Processing，简称 DIP），输入的是图像，输出的也是图像。Photoshop 中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。\n图片 -> ps后的图片\n再说联系\nCG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。通常的做法是绘制一个全屏的矩形，在 Pixel Shader 中进行图像处理。\nCV 大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理，增强对比度、去除噪点。\n最后还要提到今年的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。\n面试——把自己嫁出去\n面试官面的是什么\n我个人的经验一次正规的面试包括几个部分：\n基础能力：数据结构与算法通过做一些智商题、ACM，一般笔试题会从leetcode找。基础能力除了基本的数据结构与算法外，经常还会考察求职者对一门编程语言的掌握程度。\n工作经历：在哪些公司工作过，做过哪些项目，能不能把做过的东西很清晰的很系统的讲出来。（注：哪怕不是自己做过的东西，求职者能很好的讲出来，面试官也会给加分）\n沟通能力：性格是否比较好，是否能愉快的沟通，是不是能融入团队。其实有时就是看颜值，通俗说能否看对眼。哪怕能力不怎么好，但是面试官司觉得人不错，工作能干得了，值得培养也没问题。\n求职者要的是什么\n钱\n成长\n开心\n面试要注意什么\n技术能力是核心\n谦虚谨慎诚实是打动面试官的重要因素\n沟通也很重要\n适当美化自己的经历，但不吹牛，也不要过分谦虚\n参考资料：\nDeep Learning（深度学习）学习笔记整理系列之（三)\n《机器学习》Tom M.Mitchell\n《机器学习实践》Peter Harrington\n《数学之美》吴军\n《统计学习方法》李航\n《计算机视觉、图形学和图像处理，三者有什么联系？》张静\n《从机器学习谈起》 计算机的潜意识\n《计算机视觉与计算机图形学的对立统一》卜居"}
{"content2":"机器学习知识体系（强烈推荐）\n随着2016年Alpha Go在围棋击败李世石，2017年初卡内基梅隆大学人工智能系统Libratus在长达20天的鏖战中，打败4名世界顶级德州扑克玩家，这标志着人工智能技术又达到了一个新的高峰。人工智能已经不再是在各大公司幕后提供各种智能推荐、语音识别算法的工具，它已经慢慢走向台前进入到平常百姓的视野之中。曾经有人描述人工智能就向一列缓缓开向人们的火车，一开始非常遥远而且看起来非常缓慢，它慢慢接近，直到人们清楚看到它的时候，它已经呼啸而过，把人远远抛在身后。现在似乎就是人们可以远远看到人工智能的时候，它已经发展数十年，但直到最近才引起广泛注意，随着大数据的积累、算法的改进、硬件的提升，人工智能可以在很多细分的领域成为专家，辅助人类甚至超过人类。\n作为一名初学者，我也是刚刚接触人工智能和机器学习，希望能够和大家共同学习。接触一个领域的第一步是尽快的了解全貌并且搭建出相应的知识体系。大致提纲如下（后续不断补充）：\n1 - 数学\n线性代数、微积分\n在整个机器学习过程中涉及大量矩阵运算和微积分导数的概念，因此建议初学者至少要有较为扎实的数学基础，对矩阵和微积分的概念了解比较清楚。否则在一些公式推导过程中会遇到较大障碍，而不断反复回来复习数学知识。\n2 - 编程语言\nPython/R/Java/Matlab\nPython已经成为机器学习的第一语言，至于为什么知乎(https://www.zhihu.com/question/30105838?sort=created)中有非常不错的解释。众多机器学习的框架都支持Python API，所以学习机器学习，Python语言语法估计是绕不过去。\n3 - Supervise learning\nLinear regression\nLogistic regression\nNeural network\nSVM\n监督学习指的是人们给机器一大堆标记好的数据，比如一大堆照片，标记出哪些是猫的照片，哪些不是，然后让机器自己学习归纳出算法，可以判断出其他照片是否是猫。目前这个领域算法代表：Linear regression, Logistic regression, Neural network, SVM等等。\n4 - Unsupervise learning\nK-means\nPCA\nAnomaly detection\n非监督学习指的就是人们给机器一大堆没有标记的数据，让机器可以对数据进行分类、检测异常等。\n5 - Special topic\nRecommend system\nLarge scale machine learning application\n一些特殊算法，例如推荐系统。常用于购物网站，可以根据你的过往购物或评分情况，来向你推荐商品。\n6 - Advice on machine learning\nBias/vairance\nRegulation\nLearning curve\nError analysis\nCelling analysis\n机器学习的建议，包含参数正则化、学习曲线、错误分析、调参等。\n7 - Deep Learning\nNeural Netwotk\n深度学习是近期机器学习的一个热门分支，模拟人类大脑的思维方式，可以极大的提高正确率，是近来机器学习的一个非常大的突破。\n8 - Tools/Framework\nTensorFlow/Theano/Keras\n很多大厂就开源了一些机器学习的框架，基于这些框架可以很容易搭建机器学习的平台。\n推荐的学习资料：\nGithub上面有一份非常详尽的学习路径 (https://github.com/JustFollowUs/Machine-Learning)\n我个人推荐的几个经典资料：\n机器学习\nAndrew NG的Coursera的机器学习入门 (https://www.coursera.org/learn/machine-learning)：这个教程非常适合初学者，没有很高深的数学推导，Andrew也是业内大牛但非常谦逊，讲解非常浅显易懂。\n周志平的机器学习(https://book.douban.com/subject/26708119/)：号称最好的中文机器学习入门，这里有对这本书的详细评(https://www.zhihu.com/question/39945249)。\n几本经典著作：An Introduction to Statistical Learning，Pattern Recognition and Machine Learning，The Elements of Statistical Learning\n深度学习\nNeural Networks and Deep Learning：Michael Nielsen用非常浅显易懂的方式介绍了神经网络和深度学习，并且提供了一个手写数字识别的例子，非常适合入门。\nUFLDL Tutorial I UFLDL Tutorial II：Andrew NG主导的Deep Learning的学习资料，内容非常精炼，适合稍微有些基础的同学。\nDeep Learning: 几位大神共同编写的关于深度学习的free book。\n从人工智能到机器学习，再到最近大热的深度学习，人们已经在这个领域研究了数十年，现在虽然取得一定的突破，但是离真正的人工智能还有非常长的距离。而且人工智能一定要跳出学术界的研究框架，结合工业界的应用，从2016年可以看到大量的实际应用场景，例如自动驾驶、Apple Siri、Amazon Echo、谷歌翻译等等，我相信未来人工智能领域极有可能成为下一代颠覆性的技术革命。"}
{"content2":"在机器学习用于产品的时候，我们经常会遇到跨平台的问题。比如我们用Python基于一系列的机器学习库训练了一个模型，但是有时候其他的产品和项目想把这个模型集成进去，但是这些产品很多只支持某些特定的生产环境比如Java，为了上一个机器学习模型去大动干戈修改环境配置很不划算，此时我们就可以考虑用预测模型标记语言(Predictive Model Markup Language,以下简称PMML)来实现跨平台的机器学习模型部署了。\n1. PMML概述\nPMML是数据挖掘的一种通用的规范，它用统一的XML格式来描述我们生成的机器学习模型。这样无论你的模型是sklearn,R还是Spark MLlib生成的，我们都可以将其转化为标准的XML格式来存储。当我们需要将这个PMML的模型用于部署的时候，可以使用目标环境的解析PMML模型的库来加载模型，并做预测。\n可以看出，要使用PMML，需要两步的工作，第一块是将离线训练得到的模型转化为PMML模型文件，第二块是将PMML模型文件载入在线预测环境，进行预测。这两块都需要相关的库支持。\n2. PMML模型的生成和加载相关类库\nPMML模型的生成相关的库需要看我们使用的离线训练库。如果我们使用的是sklearn，那么可以使用sklearn2pmml这个python库来做模型文件的生成，这个库安装很简单，使用\"pip install sklearn2pmml\"即可，相关的使用我们后面会有一个demo。如果使用的是Spark MLlib, 这个库有一些模型已经自带了保存PMML模型的方法，可惜并不全。如果是R，则需要安装包\"XML\"和“PMML”。此外，JAVA库JPMML可以用来生成R,SparkMLlib,xgBoost,Sklearn的模型对应的PMML文件。github地址是：https://github.com/jpmml/jpmml。\n加载PMML模型需要目标环境支持PMML加载的库，如果是JAVA，则可以用JPMML来加载PMML模型文件。相关的使用我们后面会有一个demo。\n3. PMML模型生成和加载示例\n下面我们给一个示例，使用sklearn生成一个决策树模型，用sklearn2pmml生成模型文件，用JPMML加载模型文件，并做预测。\n完整代码参见我的github:https://github.com/ljpzzz/machinelearning/blob/master/model-in-product/sklearn-jpmml\n首先是用用sklearn生成一个决策树模型，由于我们是需要保存PMML文件，所以最好把模型先放到一个Pipeline数组里面。这个数组里面除了我们的决策树模型以外，还可以有归一化，降维等预处理操作，这里作为一个示例，我们Pipeline数组里面只有决策树模型。代码如下:\nimport numpy as np import matplotlib.pyplot as plt %matplotlib inline import pandas as pd from sklearn import tree from sklearn2pmml.pipeline import PMMLPipeline from sklearn2pmml import sklearn2pmml import os os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Java/jdk1.8.0_171/bin' X=[[1,2,3,1],[2,4,1,5],[7,8,3,6],[4,8,4,7],[2,5,6,9]] y=[0,1,0,2,1] pipeline = PMMLPipeline([(\"classifier\", tree.DecisionTreeClassifier(random_state=9))]); pipeline.fit(X,y) sklearn2pmml(pipeline, \".\\demo.pmml\", with_repr = True)\n上面这段代码做了一个非常简单的决策树分类模型，只有5个训练样本，特征有4个，输出类别有3个。实际应用时，我们需要将模型调参完毕后才将其放入PMMLPipeline进行保存。运行代码后，我们在当前目录会得到一个PMML的XML文件，可以直接打开看，内容大概如下：\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?> <PMML xmlns=\"http://www.dmg.org/PMML-4_3\" version=\"4.3\"> <Header> <Application name=\"JPMML-SkLearn\" version=\"1.5.3\"/> <Timestamp>2018-06-24T05:47:17Z</Timestamp> </Header> <MiningBuildTask> <Extension>PMMLPipeline(steps=[('classifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=9, splitter='best'))])</Extension> </MiningBuildTask> <DataDictionary> <DataField name=\"y\" optype=\"categorical\" dataType=\"integer\"> <Value value=\"0\"/> <Value value=\"1\"/> <Value value=\"2\"/> </DataField> <DataField name=\"x3\" optype=\"continuous\" dataType=\"float\"/> <DataField name=\"x4\" optype=\"continuous\" dataType=\"float\"/> </DataDictionary> <TransformationDictionary> <DerivedField name=\"double(x3)\" optype=\"continuous\" dataType=\"double\"> <FieldRef field=\"x3\"/> </DerivedField> <DerivedField name=\"double(x4)\" optype=\"continuous\" dataType=\"double\"> <FieldRef field=\"x4\"/> </DerivedField> </TransformationDictionary> <TreeModel functionName=\"classification\" missingValueStrategy=\"nullPrediction\" splitCharacteristic=\"multiSplit\"> <MiningSchema> <MiningField name=\"y\" usageType=\"target\"/> <MiningField name=\"x3\"/> <MiningField name=\"x4\"/> </MiningSchema> <Output> <OutputField name=\"probability(0)\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"0\"/> <OutputField name=\"probability(1)\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"1\"/> <OutputField name=\"probability(2)\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"2\"/> </Output> <Node> <True/> <Node> <SimplePredicate field=\"double(x3)\" operator=\"lessOrEqual\" value=\"3.5\"/> <Node score=\"1\" recordCount=\"1.0\"> <SimplePredicate field=\"double(x3)\" operator=\"lessOrEqual\" value=\"2.0\"/> <ScoreDistribution value=\"0\" recordCount=\"0.0\"/> <ScoreDistribution value=\"1\" recordCount=\"1.0\"/> <ScoreDistribution value=\"2\" recordCount=\"0.0\"/> </Node> <Node score=\"0\" recordCount=\"2.0\"> <True/> <ScoreDistribution value=\"0\" recordCount=\"2.0\"/> <ScoreDistribution value=\"1\" recordCount=\"0.0\"/> <ScoreDistribution value=\"2\" recordCount=\"0.0\"/> </Node> </Node> <Node score=\"2\" recordCount=\"1.0\"> <SimplePredicate field=\"double(x4)\" operator=\"lessOrEqual\" value=\"8.0\"/> <ScoreDistribution value=\"0\" recordCount=\"0.0\"/> <ScoreDistribution value=\"1\" recordCount=\"0.0\"/> <ScoreDistribution value=\"2\" recordCount=\"1.0\"/> </Node> <Node score=\"1\" recordCount=\"1.0\"> <True/> <ScoreDistribution value=\"0\" recordCount=\"0.0\"/> <ScoreDistribution value=\"1\" recordCount=\"1.0\"/> <ScoreDistribution value=\"2\" recordCount=\"0.0\"/> </Node> </Node> </TreeModel> </PMML>\n可以看到里面就是决策树模型的树结构节点的各个参数，以及输入值。我们的输入被定义为x1-x4,输出定义为y。\n有了PMML模型文件，我们就可以写JAVA代码来读取加载这个模型并做预测了。\n我们创建一个Maven或者gradle工程，加入JPMML的依赖，这里给出maven在pom.xml的依赖，gradle的结构是类似的。\n<dependency> <groupId>org.jpmml</groupId> <artifactId>pmml-evaluator</artifactId> <version>1.4.1</version> </dependency> <dependency> <groupId>org.jpmml</groupId> <artifactId>pmml-evaluator-extension</artifactId> <version>1.4.1</version> </dependency>\n接着就是读取模型文件并预测的代码了，具体代码如下：\nimport org.dmg.pmml.FieldName; import org.dmg.pmml.PMML; import org.jpmml.evaluator.*; import org.xml.sax.SAXException; import javax.xml.bind.JAXBException; import java.io.FileInputStream; import java.io.IOException; import java.io.InputStream; import java.util.HashMap; import java.util.LinkedHashMap; import java.util.List; import java.util.Map; /** * Created by 刘建平Pinard on 2018/6/24. */ public class PMMLDemo { private Evaluator loadPmml(){ PMML pmml = new PMML(); InputStream inputStream = null; try { inputStream = new FileInputStream(\"D:/demo.pmml\"); } catch (IOException e) { e.printStackTrace(); } if(inputStream == null){ return null; } InputStream is = inputStream; try { pmml = org.jpmml.model.PMMLUtil.unmarshal(is); } catch (SAXException e1) { e1.printStackTrace(); } catch (JAXBException e1) { e1.printStackTrace(); }finally { //关闭输入流 try { is.close(); } catch (IOException e) { e.printStackTrace(); } } ModelEvaluatorFactory modelEvaluatorFactory = ModelEvaluatorFactory.newInstance(); Evaluator evaluator = modelEvaluatorFactory.newModelEvaluator(pmml); pmml = null; return evaluator; } private int predict(Evaluator evaluator,int a, int b, int c, int d) { Map<String, Integer> data = new HashMap<String, Integer>(); data.put(\"x1\", a); data.put(\"x2\", b); data.put(\"x3\", c); data.put(\"x4\", d); List<InputField> inputFields = evaluator.getInputFields(); //过模型的原始特征，从画像中获取数据，作为模型输入 Map<FieldName, FieldValue> arguments = new LinkedHashMap<FieldName, FieldValue>(); for (InputField inputField : inputFields) { FieldName inputFieldName = inputField.getName(); Object rawValue = data.get(inputFieldName.getValue()); FieldValue inputFieldValue = inputField.prepare(rawValue); arguments.put(inputFieldName, inputFieldValue); } Map<FieldName, ?> results = evaluator.evaluate(arguments); List<TargetField> targetFields = evaluator.getTargetFields(); TargetField targetField = targetFields.get(0); FieldName targetFieldName = targetField.getName(); Object targetFieldValue = results.get(targetFieldName); System.out.println(\"target: \" + targetFieldName.getValue() + \" value: \" + targetFieldValue); int primitiveValue = -1; if (targetFieldValue instanceof Computable) { Computable computable = (Computable) targetFieldValue; primitiveValue = (Integer)computable.getResult(); } System.out.println(a + \" \" + b + \" \" + c + \" \" + d + \":\" + primitiveValue); return primitiveValue; } public static void main(String args[]){ PMMLDemo demo = new PMMLDemo(); Evaluator model = demo.loadPmml(); demo.predict(model,1,8,99,1); demo.predict(model,111,89,9,11); } }\n代码里有两个函数，第一个loadPmml是加载模型的，第二个predict是读取预测样本并返回预测值的。我的代码运行结果如下：\ntarget: y value: {result=2, probability_entries=[0=0.0, 1=0.0, 2=1.0], entityId=5, confidence_entries=[]}\n1 8 99 1:2\ntarget: y value: {result=1, probability_entries=[0=0.0, 1=1.0, 2=0.0], entityId=6, confidence_entries=[]}\n111 89 9 11:1\n也就是样本（1,8,99,1）被预测为类别2，而（111,89,9,11）被预测为类别1。\n以上就是PMML生成和加载的一个示例，使用起来其实门槛并不高，也很简单。\n4. PMML总结与思考\nPMML的确是跨平台的利器，但是是不是就没有缺点呢？肯定是有的！\n第一个就是PMML为了满足跨平台，牺牲了很多平台独有的优化，所以很多时候我们用算法库自己的保存模型的API得到的模型文件，要比生成的PMML模型文件小很多。同时PMML文件加载速度也比算法库自己独有格式的模型文件加载慢很多。\n第二个就是PMML加载得到的模型和算法库自己独有的模型相比，预测会有一点点的偏差，当然这个偏差并不大。比如某一个样本，用sklearn的决策树模型预测为类别1，但是如果我们把这个决策树落盘为一个PMML文件，并用JAVA加载后，继续预测刚才这个样本，有较小的概率出现预测的结果不为类别1.\n第三个就是对于超大模型，比如大规模的集成学习模型，比如xgboost, 随机森林，或者tensorflow，生成的PMML文件很容易得到几个G，甚至上T，这时使用PMML文件加载预测速度会非常慢，此时推荐为模型建立一个专有的环境，就没有必要去考虑跨平台了。\n此外，对于TensorFlow，不推荐使用PMML的方式来跨平台。可能的方法一是TensorFlow serving，自己搭建预测服务，但是会稍有些复杂。另一个方法就是将模型保存为TensorFlow的模型文件，并用TensorFlow独有的JAVA库加载来做预测。\n我们在下一篇会讨论用python+tensorflow训练保存模型，并用tensorflow的JAVA库加载做预测的方法和实例。\n（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）"}
{"content2":"谈谈数据挖掘和机器学习\n又是好长时间没有写博客了，最近周末事情太多，明天劳动节终于可以让我们劳动人民休息一天了。首先声明的是本人并非数据挖掘和机器学习的高手，只是作为业余兴趣刚刚开始研究，据我所知好多朋友也和我一样对这方面的东西感兴趣，个人认为机器人技术是未来发展的方向。虽然我的专业是软件开发，基于Windows的，基于linux的，但是我认为未来的软件开发可能面对的硬件将会是多元化的，适当的了解一下未来的趋势也不为过。\n本来想从机器学习开始，但是通过对业内人士的咨询，朋友建议我从数据挖掘开始，通过近一个月的学习，对数据挖掘的基本思想有了些粗浅的认识，分享出来，希望和大家共同快乐，共同进步。在这样的信息爆炸的时代，我希望对每个重要的知识点用一句话通俗的表达出来，既然是入门篇，让大家看明白了最重要。\n分类和决策树\n分类技术要解决的问题是从一批已知的完整记录中学习到一颗决策树或者一个模型，然后应用这个模型对未知的数据属性进行预测或分类。有时候对于一些现实的问题，我们无法通过纯数学的解析式表打自变量和因变量的关系时，我们就可能降低要求，从精确的解析式到模型足够好，可以解释大部分的现象，解决大部分的问题，通过分类技术和决策树就可以解决一部分问题。\n下面是一个简单的决策树归纳算法实现\n1 TreeGrowth(E,F) 2 if stopping_cond(E,F)=true then 3 leaf=createNode() 4 leaf.label=Classify(E) 5 return leaf 6 else 7 root=createNode() 8 root.test_cond=find_best_split(E,F) 9 令V={v|v是root.test_cond的一个可能的输出} 10 for 每个v属于V do 11 Ev={e|root.test_cond(e)=v 并且e属于E} 12 child=TreeGrowth(Ev,F) 13 将child作为root的派生结点添加到树中,并将边(root->child)标记为v 14 end for 15 end if 16 return root\n关联分析\n关联分析要解决的问题是通过算法找出隐含在数据中的关联，比如买尿布的人大部分也买啤酒，买啤酒的人大部分也买花生米，有了关联就可以做针对性的广告或者商业分析。\n聚类分析\n聚类分析要解决的问题是将数据分成内部高内聚，外部低耦合的集合，这样对相似的事物进行分析就会更有针对性。\n异常检测\n一批数据中总有些数据记录的性质和其他大部分的数据差别很大，对有些分析我们需要找出这些数据并排除其对整体的影响，当然在某些极端的情况下对异常数据的深入研究也是有必要的。\n入门书籍推荐\n在咨询了专业的人士之后我得到的推荐是如下这本数据挖掘导论，同时由于数据挖掘在过国内的发展时间不长，好多英文术语没有精确的中文对照，所以对于入门而言，专业人士推荐我中英文对照着看，这样既有效率，又会尽量不脱离原文，估计这本书都看懂了，就应该算对数据挖掘有一点点基本的了解了。学并快乐着，不要梦想一下子成为高手。每天进步一点点，慢慢的你就是高手了。"}
{"content2":"当我们运用训练好了的模型来预测未知数据时候发现有较大误差，那么我们下一步可以做什么呢？\n一般来说可以选择以下几种方法：\n增加训练集（通常是有效的，但是代价太大）\n减少特征的数量\n获取更多的特征\n增加多项式特征\n减小正则化参数lambda\n增大正则化参数lambda\n但是要选择什么方法来改进我们的算法，我们需要运用一些机器学习诊断法来协助我们判断。\n一、评估h(x) ---- Evaluating a Hypothesis\n一个好的Hypothesis：有小的训练误差同时没有过拟合\n我们的训练是通过最小化训练误差来得到h(x), 但是有小的训练误差并不代表它一定就是一个好的Hypothesis，很可能会发生过拟合导致泛化能力差。\n过拟合检验：\n把数据集分成训练集（70%）和测试集（30%）\n用训练集训练出一个模型之后，我们通过测试集来评估这个模型\n对于回归模型：利用测试集计算代价函数J\n对于分类模型：利用测试集计算代价函数J，并计算误分类比率err(h(x), y) / mtest\n二、模型选择和交叉验证集 ---- Model Selection and Train_Validation_Test set\n当我们不确定多项式模型最高该几次或者正则化参数应该取多大的时候，我们可以使用交叉验证集来帮助选择模型。（微调模型的超参数：多项式最高次，正则化参数）\n交叉验证(cross validation)：\n数据集：训练集（60%），交叉验证集（20%）， 测试集（20%）\n模型选择过程(model selection)：\n使用训练集训练出多个模型\n用这些模型分别对交叉验证集计算交叉验证误差J\n选取代价函数最小的模型\n用选出的模型对测试集计算推广误差J\n很多人仅仅把数据集分成了训练集和交叉验证集，使用交叉验证集选择模型同时测出误差作为预测效果。当数据集很大时也许可以得到比较好的泛化误差，但是一般来说这样并不好。\n三、诊断偏差和方差 ---- Diagnosing Bias and Variance\n当一个模型表现不是很好时，一般来说是两种情况：偏差比较大（欠拟合），方差比较大（过拟合）。\n泛化性能用期望泛化误差表示，而期望泛化误差可以分解为偏差，方差和噪声。\nBias：描述的是预测值与真实值之间的差距。\nVariance：描述的是预测值的变化范围，离散程度，也就是离其真实值的距离。\n偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；（准确性）\n方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；（稳定性）\n判断偏差和方差（多项式次数及λ的值）：\n我们通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来分析\n如表所示，当Jcv(Θ)和Jtrain(Θ)都比较大而且近似时，属于高偏差问题。（欠拟合）\n当Jtrain(Θ)比较小，而且Jcv(Θ)远大于Jtrain(Θ)时，属于高方差问题。（过拟合）\n同样的，我们将训练集和交叉验证集的代价函数误差与λ的值绘制在同一张图表上来分析(在选择模型时，λ的值一般取两倍步长)\n注意：这里我们在计算训练集误差、验证集误差和测试集误差时都不考虑正则化项（只包含数据的平方误差）。\n可以看出，当λ比较小时，可能出现过拟合问题（高方差），此时训练集误差比较小，验证集误差比较大。\n当λ比较大时，对每个参数的惩罚因子都很大，可能出现欠拟合问题（高偏差），此时训练集误差和验证集误差都比较大。\n四、学习曲线 ---- Learning Curves\n训练集大小的影响：\n对于高偏差的情况，增加训练集并没有用\n对于高方差的情况，增加训练集也许是有用的；\n六、针对高方差和高偏差的情况可以采取的措施\n增加训练集：高方差\n减少特征的数量：高方差\n获取更多的特征：高偏差\n增加多项式特征：高偏差\n减小正则化参数lambda：高偏差\n增大正则化参数lambda：高方差\n对于神经网络，越简单的神经网络计算量小但是容易欠拟合。相反的，越复杂的神经网络容易过拟合，但我们可以使用正则化项来克服过拟合，一般来说用复杂的神经网络比简单的神经网络效果好，当然计算量比较大。对于隐藏层数量的选择，一般来说一层是比较合理的选择，但是你想要从一层、两层、三层...里面做最合理的选择，可以用交叉验证集做模型选择。"}
{"content2":"摘要: 原创博客:转载请标明出处:http://www.cnblogs.com/zxouxuewei/\n一、前言\nRTAB-Map (Real-Time Appearance-Based Mapping)是一种基于全局贝叶斯闭环检测的RGB-D Graph SLAM方法。它可以用kinect的深度信息结合kinect变\n换得到的激光数据进行即时定位与建图（gmapping算法志只用到了kinect转换得到的激光数据，而把深度信息丢弃了）。\n官网地址：http://introlab.github.io/rtabmap/\nROS下面安装请看：https://github.com/introlab/rtabmap_ros#rtabmap_ros\n教程请看：https://github.com/introlab/rtabmap/wiki/Tutorials\n在机器人上使用RTAB-Map：Setup RTAB-Map on Your Robot!\n二、deb方式安装\n嫌麻烦采用deb方式安装\nJade: $ sudo apt-get install ros-jade-rtabmap-ros Indigo: $ sudo apt-get install ros-indigo-rtabmap-ros Hydro: $ sudo apt-get install ros-hydro-rtabmap-ros\n安装完之后插上Kinect在终端运行命令：\n$ rtabmap\nWhen launching rtabmap_ros's nodes, if you have the error error while loading shared libraries..., add the next line at the end of your ~/.bashrc to fix it:\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/ros/kinetic/lib/x86_64-linux-gnu\n出现GUI，然后选择“File”——“New database”，再点击开始按键就可以出现图像了\n三、在自己的工作空间中安装源码包\n在catkin工作空间中安装RTAB-Map ros-pkg,因为版本问题我们通过手动方式下载源码包。\nhttps://github.com/introlab/rtabmap_ros/releases/tag/0.11.8-indigo\n下载压缩包后，自己解压放到自己的工作空间的／catkin_ws/src下：\ncd ~/catkin_ws/\ncatkin_make\nAndrew Ng教授的《机器学习》公开课视频（30集）\nhttp://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning\nAndrew Ng教授的Deep Learning维基，有中文翻译\nhttp://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial\n其他教学资源\n韩家炜教授在北大的《数据挖掘》暑期班视频，英文PPT，中文讲解（22集）\nhttp://v.youku.com/v_show/id_XMzA3NDI5MzI=.html（视频：01数据挖掘概念，课程简介，数据库技术发展史，数据挖掘应用）\n韩家炜教授（UIUC大学）的《数据挖掘》在线课程\nhttps://wiki.engr.illinois.edu/display/cs412/Home;jsessionid=6BF0A2C36A95A31D2DA754A017756F4B\n卡内基•梅隆大学（CMU）的《机器学习》在线课程\nhttp://www.cs.cmu.edu/~epxing/Class/10701/lecture.html\n麻省理工学院（MIT）的《机器学习》在线课程\nhttp://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/index.htm\n加州理工学院（Caltech）的《机器学习与数据挖掘》在线课程\nhttp://work.caltech.edu/telecourse.html（同上述网易公开课http://v.163.com/special/opencourse/learningfromdata.html）\nUC Irvine的《机器学习与数据挖掘》在线课程\nhttp://sli.ics.uci.edu/Classes/2011W-178\n斯坦福大学的《数据挖掘》在线课程\nhttp://www.stanford.edu/class/stats202/\n其他资源\n北京机器学习读书会\nhttp://q.weibo.com/1644133\n机器学习相关电子书\nhttp://t.cn/zjtPuCS（打开artificial intelligence找子目录machine learning）\n附：\n主讲教师介绍：（新浪公开课：机器学习http://open.sina.com.cn/course/id_280/）\n讲师：Andrew Ng\n学校：斯坦福\n斯坦福大学计算机系副教授，人工智能实验室主任，致力于人工智能、机器学习，神经信息科学以及机器人学等研究方向。他和他的学生成功开发出新的机器视觉算法，大大简化了机器人的传感器系统。"}
{"content2":"Pycharm 激活码(转) 有效期到2019/10月\n2018年11月13日 17:15:32 may_ths 阅读数：64\n【激活码激活】\n修改hosts文件\n添加下面一行到hosts文件，目的是屏蔽掉Pycharm对激活码的验证\n0.0.0.0 account.jetbrains.com\n注：hosts文件路径，Windows在C:\\Windows\\System32\\drivers\\etc\\hosts，Linux在 /etc/hosts。Win下需要管理员权限打开。如果遇到权限问题，可将hosts文件先复制出来修改后再覆盖原来的即可。\n在hosts文件最后单独一行加进去即可\n打开PyCharm，选择 Activate code（用激活码激活）\n复制下面的激活码，填入激活码框\n（注：此激活码为本人教育邮箱注册申请，有效期至2019年10月，仅供学习使用，不得用于商业用途，违者将依法追究责任。）\nSSUJFAQGMI-eyJsaWNlbnNlSWQiOiJTU1VKRkFRR01JIiwibGljZW5zZWVOYW1lIjoiWmhpd2VpIEhvbmciLCJhc3NpZ25lZU5hbWUiOiIiLCJhc3NpZ25lZUVtYWlsIjoiIiwibGljZW5zZVJlc3RyaWN0aW9uIjoiRm9yIGVkdWNhdGlvbmFsIHVzZSBvbmx5IiwiY2hlY2tDb25jdXJyZW50VXNlIjpmYWxzZSwicHJvZHVjdHMiOlt7ImNvZGUiOiJJSSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IkFDIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiRFBOIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiUFMiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJHTyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IkRNIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiQ0wiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSUzAiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSQyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IlJEIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiUEMiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSTSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IldTIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiREIiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJEQyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IlJTVSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9XSwiaGFzaCI6IjEwNjQ1NTE3LzAiLCJncmFjZVBlcmlvZERheXMiOjAsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-eNTyizE3kmBWEVd8daP6msWpn1/6mapFOi/fYBbc8LokedHKs0W1P+RNBR7eWPuD8efGE0EI00CydiPSOz+7qFHMaW69aW/2x5JTH3Nb6qIH9qVWCZDi1Sb5BDQxpen5OUVGks6rOtaNkOIAhQMbZyKTEQDd9rg0hUEY0BxhwDdR1zWlCWFL9h0smFWqncVvvt5wX09W4WnepJ+wYvUOgW0gPJTwV1NsCoa5hfgh5tVOKqfiuT3uD1QYYKh1Q6DYAKDMpkkObEt6BAwg7Gdg4MV7/f4R01RSRaZm7JJuoECeRSswzMLipDLMeAXTEAeHOumgZVsofvkhYAGQUuvNXA==-MIIEPjCCAiagAwIBAgIBBTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE1MTEwMjA4MjE0OFoXDTE4MTEwMTA4MjE0OFowETEPMA0GA1UEAwwGcHJvZDN5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQC9WZuYgQedSuOc5TOUSrRigMw4/+wuC5EtZBfvdl4HT/8vzMW/oUlIP4YCvA0XKyBaCJ2iX+ZCDKoPfiYXiaSiH+HxAPV6J79vvouxKrWg2XV6ShFtPLP+0gPdGq3x9R3+kJbmAm8w+FOdlWqAfJrLvpzMGNeDU14YGXiZ9bVzmIQbwrBA+c/F4tlK/DV07dsNExihqFoibnqDiVNTGombaU2dDup2gwKdL81ua8EIcGNExHe82kjF4zwfadHk3bQVvbfdAwxcDy4xBjs3L4raPLU3yenSzr/OEur1+jfOxnQSmEcMXKXgrAQ9U55gwjcOFKrgOxEdek/Sk1VfOjvS+nuM4eyEruFMfaZHzoQiuw4IqgGc45ohFH0UUyjYcuFxxDSU9lMCv8qdHKm+wnPRb0l9l5vXsCBDuhAGYD6ss+Ga+aDY6f/qXZuUCEUOH3QUNbbCUlviSz6+GiRnt1kA9N2Qachl+2yBfaqUqr8h7Z2gsx5LcIf5kYNsqJ0GavXTVyWh7PYiKX4bs354ZQLUwwa/cG++2+wNWP+HtBhVxMRNTdVhSm38AknZlD+PTAsWGu9GyLmhti2EnVwGybSD2Dxmhxk3IPCkhKAK+pl0eWYGZWG3tJ9mZ7SowcXLWDFAk0lRJnKGFMTggrWjV8GYpw5bq23VmIqqDLgkNzuoog==\n点击 OK 进行认证，目前这个激活码有效期到2019年10月22日，之后再继续更新。\n【永久激活破解】\n1、下载官方PyCharm（专业版）；\n下载地址：https://www.jetbrains.com/pycharm/download/\n2、下载crack激活包，通过截获截止时间骗过PyCharm；\n下载地址：https://pan.baidu.com/s/18w_vH0pvnjUOmhm9bkQgpA，提取码：nutj\n3、将下载的文件放入PyCharm安装bin目录下:\n4、在pycharm.exe.vmoptions，pycharm64.exe.vmoptions两文件里末尾加上以下内容：\n-javaagent:D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\bin\\JetbrainsCrack-2.6.10-release-enc.jar\n注：上面的D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\改成自己电脑上PyCharm的安装地址即可。\n5、如果已有注册码无须填写，没用可以填写破解包里的注册码或者上面给出的注册码。另外也可以使用教育邮箱免费注册申请；\n6、重新启动PyCharm验证。\n截止2099年这个时间应该够你用的了，哈哈。。。\n（仅供教育学习使用，不得用于商业用途！）\n常用软件开发学习资料（免费下载）：\n1.经典编程电子书收藏\n2.C&C++编程学习资料收藏\n3.算法及数据结构（有关c,c++,java）\n4.Java开发学习资料收藏\n5.Android开发学习资料收藏\n6.Python开发学习资料收藏\n7.大数据，机器学习，人工智能资料收藏"}
{"content2":"随着iOS11的发布，苹果公司也正式加入了机器学习的战场。在新的iOS11中内置了CoreML，虽然还是Beta版本，但是功能已经非常强大了。\n在这个CoreML库里面，已经集成了一些训练好的模型，可以在App中直接使用这些模型进行预测。\n下面是苹果对于Core ML的介绍。\nCoreML让你将很多机器学习模型集成到你的app中。除了支持层数超过30层的深度学习之外，还支持决策树的融合，SVM（支持向量机），线性模型。由于其底层建立在Metal 和Accelerate等技术上，所以可以最大限度的发挥CPU和GPU的优势。你可以在移动设备上运行机器学习模型，数据可以不离开设备直接被分析。\nVision：这部分是关于图像分析和图像识别的。其中包括人脸追踪，人脸识别，航标（landmarks），文本识别，区域识别，二维码识别，物体追踪，图像识别等。\n其中使用的模型包括：Places205-GoogLeNet，ResNet50，Inception v3，VGG16。\n这些模型最小的25M，对于app还是可以接受的，最大的有550M，不知道如何集成到app中。\nNLPAPI：这部分是自然语言处理的API，包括语言识别，分词，词性还原，词性判定，实体辨识。\nGamePlayKit：这部分的话，应该是制作游戏时候，提供一些随机数生成，寻找路径（pathfinding），人工智能的库。感觉上可能还带有强化学习的一些功能（提到了agent behavior，这个可能是强化学习 Q-Learning的一些术语吧0）。其中也有一些Decision Trees的API，但是不知道和传统的决策树是否一致。\n从图中可以看到，Core ML 的底层是Accelerate 和 BNNS，BNNS（Basic neural network subroutines），框架中已经集成了神经网络了，并且对于大规模计算和图形计算进行了一定的优化了。Metal Performance Shaders看介绍应该是能够使得app充分使用GPU的组件。"}
{"content2":"=======================国内====================\n之前自己一直想总结一下国内搞机器学习和数据挖掘的大牛，但是自己太懒了。所以没搞…\n最近看到了下面转载的这篇博文，感觉总结的比较全面了。\n个人认为，但从整体研究实力来说，机器学习和数据挖掘方向国内最强的地方还是在MSRA，\n那边的相关研究小组太多，很多方向都能和数据挖掘扯上边。这里我再补充几个相关研究方向\n的年轻老师和学者吧。\n蔡登：http://www.cad.zju.edu.cn/home/dengcai/,Han Jiawei老师的学生，博士毕业后回浙大\n任教，也算是国内年轻一代的牛人了。\n万小军：https://sites.google.com/site/wanxiaojun1979/，得FQ才能看到主页。主要\n研究方向是文本挖掘和语义计算。自然语言方向好会议发了很多文章。\n张磊：http://research.microsoft.com/en-us/um/people/leizhang/\n———————————————————————————————–\n原文地址：http://blog.csdn.net/playoffs/article/details/7588597\n李航：http://research.microsoft.com/en- us/people/hangli/，是MSRA Web Search and\nMining Group高级研究员和主管，主要研究领域是信息检索，自然语言处理和统计学习。\n近年来，主要与人合作使用机器学习方法对信息检索中排序，相关性等问题的 研究。曾在\n人大听过一场他的讲座，对实际应用的问题抽象，转化和解决能力值得学习。\n周志华：http://cs.nju.edu.cn/zhouzh/，是南京大学的杰青，机器学习和数据挖掘方面\n国内的领军人物，其好几个研究生都 进入了美国一流高校如uiuc，cmu等学习和深造。周教授\n在半监督学习，multi-label学习和集成学习方面在国际上有一定的影响力。另外，他也\n是ACML的创始人。人也很nice，曾经发邮件咨询过一个naive的问题，周老师还在百忙之中\n回复了我，并对我如何发邮件给了些许建议。\n杨强：http://www.cse.ust.hk/~qyang/，香港科技大学教 授，也是KDD 2012的会议主席，\n可见功力非同一般。杨教授是迁移学习的国际领军人物，曾经的中国第一位acm全球冠军上\n交的戴文渊硕士期间就是跟他合作发表了一系列 高水平的文章。还有，杨教授曾有一个关\n于机器学习和数据挖掘有意思的比喻：比如你训练一只狗，若干年后，如果它忽然有一天能\n帮你擦鞋洗衣服，那么这就是数 据挖掘；要是忽然有一天，你发现狗发装成一个老太婆\n消失了，那么这就是机器学习。\n李建中：http://db.hit.edu.cn/jianzhongli/，哈工大和黑大共有教授，是分布式数据库\n的领军人物。近年来，其团队 在不确定性数据，sensor network方面也发表了一系列有名\n文章。李教授为人师表，教书育人都做得了最好，在圈内是让人称道的好老师和好学者。\n唐杰：http://keg.cs.tsinghua.edu.cn/jietang/，清华大学副教授，是图挖掘方面的专家。\n他主持设计和实现的Arnetminer是国内领先的图挖掘系统，该系统也是多个会议的支持商。\n张钹：http://www.csai.tsinghua.edu.cn/personal_homepage/zhang_bo/index.html 清华\n大学教授，中科院院士，。现任清华大学信息技术研究院指导委员会主任，微软亚洲研究院\n技术顾问等。主要从事人工智能、神经网络、遗传算法、智能机器 人、模式识别以及智能控\n制等领域的研究工作。在过去二十多年中，张钹教授系统地提出了问题求解的商空间理\n论。近年来，他建立了神经与认知计算研究中心以及多媒体信息处理研究组。该研究组已在\n图像和视频的分析与检索方面取得一些重要研究成果。\n刘铁岩：http://research.microsoft.com/en-us/people/tyliu/ MSRA研究主管，\n是learning to rank的国际知名学者。近年逐步转向管理，研究兴趣则开始关注计算广告学方面。\n王海峰：http://ir.hit.edu.cn/~wanghaifeng/ 信息检索，自然语言处理，机器翻译方面\n的专家，ACL的副主席，百度高级科学家。近年，在百度主持研发了百度翻译产品。\n何晓飞：http://people.cs.uchicago.edu/~xiaofei/ 浙江大学教授，多媒体处理，\n图像检索以及流型学习的国际领先学者。\n朱军：http://www.ml-thu.net/~jun/ 清华大学副教授，机器学习绝对重量级新星。\n主要研究领域是latent variable models, large-margin learning, Bayesian nonparametrics,\nand sparse learning in high dimensions. 他也是今年龙星计划的机器学习领域的主讲人之一。\n———————————————————————————————-\n吴军：http://www.cs.jhu.edu/~junwu/ 腾讯副总裁，前google研究员。\n著名《数学之美》和《浪潮之巅》系列的作者。\n张栋：http://weibo.com/machinelearning 前百度科学家和google研究员，机器学习工业界的代表人物之一。\n戴文渊：http://apex.sjtu.edu.cn/apex_wiki/Wenyuan_Dai 现百度凤巢ctr预估组leader。\n前ACM大赛冠军，硕士期间一系列transfer learning方面的高水平论文让人瞠目结舌。\n======================资源====================\n以前转过一个计算机视觉领域内的牛人简介，现在转一个更宽范围内的牛人简介：\nhttp://people.cs.uchicago.edu/~niyogi/\nhttp://www.cs.uchicago.edu/people/\nhttp://pages.cs.wisc.edu/~jerryzhu/\nhttp://www.kyb.tuebingen.mpg.de/~chapelle\nhttp://people.cs.uchicago.edu/~xiaofei/\nhttp://www.cs.uiuc.edu/homes/dengcai2/\nhttp://www.kyb.mpg.de/~bs\nhttp://research.microsoft.com/~denzho/\nhttp://www-users.cs.umn.edu/~kumar/dmbook/index.php#item5\n(resources for the book of the introduction of data mining by Pang-ning Tan et.al. )（国内已经有相应的中文版）\nhttp://www.cs.toronto.edu/~roweis/lle/publications.html    (lle算法源代码及其相关论文)\nhttp://dataclustering.cse.msu.edu/index.html#software（data clustering）\nhttp://www.cs.toronto.edu/~roweis/     (里面有好多资源)\nhttp://www.cse.msu.edu/~lawhiu/  (manifold learning)\nhttp://www.math.umn.edu/~wittman/mani/ (manifold learning demo in matlab)\nhttp://www.iipl.fudan.edu.cn/~zhangjp/literatures/MLF/INDEX.HTM  (manifold learning in matlab)\nhttp://videolectures.net/mlss05us_belkin_sslmm/   (semi supervised learning with manifold method by Belkin)\nhttp://isomap.stanford.edu/    (isomap主页)\nhttp://web.mit.edu/cocosci/josh.html  MIT    TENENBAUM J B主页\nhttp://web.engr.oregonstate.edu/~tgd/    （国际著名的人工智能专家 Thomas G. Dietterich）\nhttp://www.cs.berkeley.edu/~jordan/ （MIchael I.Jordan）\nhttp://www.cs.cmu.edu/~awm/  (Andrew W. Moore’s  homepage)\nhttp://learning.cs.toronto.edu/ （加拿大多伦多大学机器学习小组）\nhttp://www.cs.cmu.edu/~tom/ （Tom Mitchell，里面有与教材匹配的slide。）\n转自：http://www.52ml.net/"}
{"content2":"原文：http://blog.csdn.net/zouxy09/article/details/48903179\n一、概述\n机器学习算法在近几年大数据点燃的热火熏陶下已经变得被人所“熟知”，就算不懂得其中各算法理论，叫你喊上一两个著名算法的名字，你也能昂首挺胸脱口而出。当然了，算法之林虽大，但能者还是有限，能适应某些环境并取得较好效果的算法会脱颖而出，而表现平平者则被历史所淡忘。随着机器学习社区的发展和实践验证，这群脱颖而出者也逐渐被人所认可和青睐，同时获得了更多社区力量的支持、改进和推广。\n以最广泛的分类算法为例，大致可以分为线性和非线性两大派别。线性算法有著名的逻辑回归、朴素贝叶斯、最大熵等，非线性算法有随机森林、决策树、神经网络、核机器等等。线性算法举的大旗是训练和预测的效率比较高，但最终效果对特征的依赖程度较高，需要数据在特征层面上是线性可分的。因此，使用线性算法需要在特征工程上下不少功夫，尽量对特征进行选择、变换或者组合等使得特征具有区分性。而非线性算法则牛逼点，可以建模复杂的分类面，从而能更好的拟合数据。\n那在我们选择了特征的基础上，哪个机器学习算法能取得更好的效果呢？谁也不知道。实践是检验哪个好的不二标准。那难道要苦逼到写五六个机器学习的代码吗？No，机器学习社区的力量是强大的，码农界的共识是不重复造轮子！因此，对某些较为成熟的算法，总有某些优秀的库可以直接使用，省去了大伙调研的大部分时间。\n基于目前使用python较多，而python界中远近闻名的机器学习库要数scikit-learn莫属了。这个库优点很多。简单易用，接口抽象得非常好，而且文档支持实在感人。本文中，我们可以封装其中的很多机器学习算法，然后进行一次性测试，从而便于分析取优。当然了，针对具体算法，超参调优也非常重要。\n二、Scikit-learn的python实践\n2.1、Python的准备工作\nPython一个备受欢迎的点是社区支持很多，有非常多优秀的库或者模块。但是某些库之间有时候也存在依赖，所以要安装这些库也是挺繁琐的过程。但总有人忍受不了这种繁琐，都会开发出不少自动化的工具来节省各位客官的时间。其中，个人总结，安装一个python的库有以下三种方法：\n1）Anaconda\n这是一个非常齐全的python发行版本，最新的版本提供了多达195个流行的python包，包含了我们常用的numpy、scipy等等科学计算的包。有了它，妈妈再也不用担心我焦头烂额地安装一个又一个依赖包了。Anaconda在手，轻松我有！下载地址如下：http://www.continuum.io/downloads\n2）Pip\n使用过Ubuntu的人，对apt-get的爱只有自己懂。其实对Python的库的下载和安装可以借助pip工具的。需要安装什么库，直接下载和安装一条龙服务。在pip官网https://pypi.python.org/pypi/pip下载安装即可。未来的需求就在#pip install xx 中。\n3）源码包\n如果上述两种方法都没有找到你的库，那你直接把库的源码下载回来，解压，然后在目录中会有个setup.py文件。执行#python setup.py install 即可把这个库安装到python的默认库目录中。\n2.2、Scikit-learn的测试\nscikit-learn已经包含在Anaconda中。也可以在官方下载源码包进行安装。本文代码里封装了如下机器学习算法，我们修改数据加载函数，即可一键测试：\nclassifiers = {'NB':naive_bayes_classifier, 'KNN':knn_classifier, 'LR':logistic_regression_classifier, 'RF':random_forest_classifier, 'DT':decision_tree_classifier, 'SVM':svm_classifier, 'SVMCV':svm_cross_validation, 'GBDT':gradient_boosting_classifier }\ntrain_test.py\n#!usr/bin/env python #-*- coding: utf-8 -*- import sys import os import time from sklearn import metrics import numpy as np import cPickle as pickle reload(sys) sys.setdefaultencoding('utf8') # Multinomial Naive Bayes Classifier def naive_bayes_classifier(train_x, train_y): from sklearn.naive_bayes import MultinomialNB model = MultinomialNB(alpha=0.01) model.fit(train_x, train_y) return model # KNN Classifier def knn_classifier(train_x, train_y): from sklearn.neighbors import KNeighborsClassifier model = KNeighborsClassifier() model.fit(train_x, train_y) return model # Logistic Regression Classifier def logistic_regression_classifier(train_x, train_y): from sklearn.linear_model import LogisticRegression model = LogisticRegression(penalty='l2') model.fit(train_x, train_y) return model # Random Forest Classifier def random_forest_classifier(train_x, train_y): from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier(n_estimators=8) model.fit(train_x, train_y) return model # Decision Tree Classifier def decision_tree_classifier(train_x, train_y): from sklearn import tree model = tree.DecisionTreeClassifier() model.fit(train_x, train_y) return model # GBDT(Gradient Boosting Decision Tree) Classifier def gradient_boosting_classifier(train_x, train_y): from sklearn.ensemble import GradientBoostingClassifier model = GradientBoostingClassifier(n_estimators=200) model.fit(train_x, train_y) return model # SVM Classifier def svm_classifier(train_x, train_y): from sklearn.svm import SVC model = SVC(kernel='rbf', probability=True) model.fit(train_x, train_y) return model # SVM Classifier using cross validation def svm_cross_validation(train_x, train_y): from sklearn.grid_search import GridSearchCV from sklearn.svm import SVC model = SVC(kernel='rbf', probability=True) param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]} grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1) grid_search.fit(train_x, train_y) best_parameters = grid_search.best_estimator_.get_params() for para, val in best_parameters.items(): print para, val model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True) model.fit(train_x, train_y) return model def read_data(data_file): import gzip f = gzip.open(data_file, \"rb\") train, val, test = pickle.load(f) f.close() train_x = train[0] train_y = train[1] test_x = test[0] test_y = test[1] return train_x, train_y, test_x, test_y if __name__ == '__main__': data_file = \"mnist.pkl.gz\" thresh = 0.5 model_save_file = None model_save = {} test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'DT', 'SVM', 'GBDT'] classifiers = {'NB':naive_bayes_classifier, 'KNN':knn_classifier, 'LR':logistic_regression_classifier, 'RF':random_forest_classifier, 'DT':decision_tree_classifier, 'SVM':svm_classifier, 'SVMCV':svm_cross_validation, 'GBDT':gradient_boosting_classifier } print 'reading training and testing data...' train_x, train_y, test_x, test_y = read_data(data_file) num_train, num_feat = train_x.shape num_test, num_feat = test_x.shape is_binary_class = (len(np.unique(train_y)) == 2) print '******************** Data Info *********************' print '#training data: %d, #testing_data: %d, dimension: %d' % (num_train, num_test, num_feat) for classifier in test_classifiers: print '******************* %s ********************' % classifier start_time = time.time() model = classifiers[classifier](train_x, train_y) print 'training took %fs!' % (time.time() - start_time) predict = model.predict(test_x) if model_save_file != None: model_save[classifier] = model if is_binary_class: precision = metrics.precision_score(test_y, predict) recall = metrics.recall_score(test_y, predict) print 'precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall) accuracy = metrics.accuracy_score(test_y, predict) print 'accuracy: %.2f%%' % (100 * accuracy) if model_save_file != None: pickle.dump(model_save, open(model_save_file, 'wb'))\n四、测试结果\n本次使用mnist手写体库进行实验：http://deeplearning.net/data/mnist/mnist.pkl.gz。共5万训练样本和1万测试样本。\n代码运行结果如下：\nreading training and testing data... ******************** Data Info ********************* #training data: 50000, #testing_data: 10000, dimension: 784 ******************* NB ******************** training took 0.287000s! accuracy: 83.69% ******************* KNN ******************** training took 31.991000s! accuracy: 96.64% ******************* LR ******************** training took 101.282000s! accuracy: 91.99% ******************* RF ******************** training took 5.442000s! accuracy: 93.78% ******************* DT ******************** training took 28.326000s! accuracy: 87.23% ******************* SVM ******************** training took 3152.369000s! accuracy: 94.35% ******************* GBDT ******************** training took 7623.761000s! accuracy: 96.18%\n在这个数据集中，由于数据分布的团簇性较好（如果对这个数据库了解的话，看它的t-SNE映射图就可以看出来。由于任务简单，其在deep learning界已被认为是toy dataset），因此KNN的效果不赖。GBDT是个非常不错的算法，在kaggle等大数据比赛中，状元探花榜眼之列经常能见其身影。三个臭皮匠赛过诸葛亮，还是被验证有道理的，特别是三个臭皮匠还能力互补的时候！\n还有一个在实际中非常有效的方法，就是融合这些分类器，再进行决策。例如简单的投票，效果都非常不错。建议在实践中，大家都可以尝试下。"}
{"content2":"有幸参加了微软OpenHack挑战赛，虽然题目难度不大，但是很有意思，学到了很多东西，还有幸认识了微软梁健老师，谢谢您的帮助！同时还认识同行的很多朋友，非常高兴，把这段难忘的比赛记录一下~~也分享一下代码，给那些没有参加的朋友，\n数据集(文末链接)\n首先每支队伍会收到一个数据集，它是一个登山公司提供的装备图片，有登山镐，鞋子，登山扣，不知道叫什么的雪地爪？手套，冲锋衣，安全带。。。一共12个类别，每个类别几百个样本，我们的任务就是对这些图片分类和识别\n简单看一下：\n赛题：\n赛题共有6道，简单描述一下：\n1、搭建环境(略过)\n2、图像正规化(包括颜色和大小)\n3、通过机器学习方法对图像分类，precision>0.8\n4、通过深度学习方法对图像分类，precision>0.9\n5、部署(略过)\n6、目标检测(用全新的数据集，检测雪地中的登山者是否带头盔！！航拍图像，有点难度~)\n_______________________________________\n下面是每道题目的详细描述和代码\n题目2\n完成以下任务:\n选择一种基本颜色，例如白色并填充所有图片尺寸不是1:1比例的图像\n不通过直接拉伸的方式，重塑至128x128x3像素的阵列形状\n确保每个图像的像素范围从0到255(包含或[0,255])，也称为“对比度拉伸”(contrast stretching).\n标准化或均衡以确保像素在[0,255]范围内.\n成功完成的标准\n团队将在Jupyter Notebook中运行一个代码单元，绘制原始图像，然后绘制填充后的像素值归一化或均衡图像, 展示给教练看.\n团队将在Jupyter notebook 为教练运行一个代码单元，显示的像素值的直方图应该在0到255的范围内（包括0和255）.\ndef normalize(src): arr = array(src) arr = arr.astype('float') # Do not touch the alpha channel for i in range(3): minval = arr[...,i].min() maxval = arr[...,i].max() if minval != maxval: arr[...,i] -= minval arr[...,i] *= (255.0/(maxval-minval)) arr = arr.astype(uint8) return Image.fromarray(arr,'RGB') import matplotlib.pyplot as plt from PIL import ImageColor from matplotlib.pyplot import imshow from PIL import Image from pylab import * import copy plt.figure(figsize=(10,10)) #设置窗口大小 # src = Image.open(\"100974.jpeg\") src = Image.open(\"rose.jpg\") src_array = array(src) plt.subplot(2,2,1), plt.title('src') plt.imshow(src), plt.axis('off') ar=src_array[:,:,0].flatten() ag=src_array[:,:,1].flatten() ab=src_array[:,:,2].flatten() plt.subplot(2,2,2), plt.title('src hist') plt.axis([0,255,0,0.03]) plt.hist(ar, bins=256, normed=1,facecolor='red',edgecolor='r',hold=1) #原始图像直方图 plt.hist(ag, bins=256, normed=1,facecolor='g',edgecolor='g',hold=1) #原始图像直方图 plt.hist(ab, bins=256, normed=1,facecolor='b',edgecolor='b') #原g始图像直方图 dst = normalize(src) dst_array = array(dst) plt.subplot(2,2,3), plt.title('dst') plt.imshow(dst), plt.axis('off') ar=dst_array[:,:,0].flatten() ag=dst_array[:,:,1].flatten() ab=dst_array[:,:,2].flatten() plt.subplot(2,2,4), plt.title('dst hist') plt.axis([0,255,0,0.03]) plt.hist(ar, bins=256, normed=1,facecolor='red',edgecolor='r',hold=1) #原始图像直方图 plt.hist(ag, bins=256, normed=1,facecolor='g',edgecolor='g',hold=1) #原始图像直方图 plt.hist(ab, bins=256, normed=1,facecolor='b',edgecolor='b') #原g始图像直方图\nView Code\n题目3\n使用一个非参数化分类方法(参考 参考文档)来创建一个模型，预测新的户外装备图像的分类情况，训练来自挑战2的预处理过的128x128x3的装备图像。所使用的算法可以从scikit-learn库中挑选现有的非参数化算法来做分类。向教练展示所提供的测试数据集的精确度，并且精确度分数需要超过80%。\ndir_data =\"data/preprocess_images/\" equipments = ['axes', 'boots', 'carabiners', 'crampons', 'gloves', 'hardshell_jackets', 'harnesses', 'helmets', 'insulated_jackets', 'pulleys', 'rope', 'tents'] train_data = [] y = [] import os from PIL import Image for equip_name in equipments: dir_equip = dir_data + equip_name for filename in os.listdir(dir_equip): if(filename.find('jpeg')!=-1): name = dir_equip + '/' + filename img = Image.open(name).convert('L') train_data.append(list(img.getdata())) y.append(equip_name)\nView Code\nfrom sklearn import svm from sklearn.cross_validation import train_test_split train_X,test_X, train_y, test_y = train_test_split(train_data, y, test_size = 0.3, random_state = 0) from sklearn import neighbors from sklearn.metrics import precision_recall_fscore_support as score from sklearn.metrics import precision_score,recall_score clf_knn = neighbors.KNeighborsClassifier(algorithm='kd_tree') clf_knn.fit(train_X, train_y) y_pred = clf_knn.predict(test_X)\nView Code\nprint(__doc__) import itertools import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): \"\"\" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \"\"\" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\"Normalized confusion matrix\") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') # Compute confusion matrix # cnf_matrix = confusion_matrix(y_test, y_pred) np.set_printoptions(precision=2) confusion_mat = confusion_matrix(test_y, y_pred, labels = equipments) # Plot non-normalized confusion matrix plt.figure(figsize=(10,10)) plot_confusion_matrix(confusion_mat, classes=equipments, title='Confusion matrix, without normalization') # Plot normalized confusion matrix plt.figure(figsize=(10,10)) plot_confusion_matrix(confusion_mat, classes=equipments, normalize=True, title='Normalized confusion matrix') plt.show()\nView Code\n因为要求精确度>0.8，sklearn中的很多算法应该都能满足，我选择了准确度比较高的KNN来建模，应该足够用了\n算一下presion和recall，轻松超越0.8\n题目4\n挑战完成标准，使用深度学习模型，如CNN分析复杂数据\n团队将在Jupyter Notebook上为教练运行一个代码单元，展示模型的准确度为90％或更高\n准确度如果要>0.9，sklearn中的机器学习算法就很难达到了，关键时刻只能上CNN\nimport matplotlib.pyplot as plt from PIL import ImageColor from matplotlib.pyplot import imshow from PIL import Image from pylab import * dir_data =\"data/preprocess_images/\" equipments = ['axes', 'boots', 'carabiners', 'crampons', 'gloves', 'hardshell_jackets', 'harnesses', 'helmets', 'insulated_jackets', 'pulleys', 'rope', 'tents'] train_data = [] y = [] import os from PIL import Image i=0 for equip_name in equipments: dir_equip = dir_data + equip_name for filename in os.listdir(dir_equip): if(filename.find('jpeg')!=-1): name = dir_equip + '/' + filename img = Image.open(name).convert('L') train_data.append(array(img).tolist()) y.append(i) i += 1 train_data = np.asarray(train_data)\nView Code\nfrom sklearn import svm from sklearn.cross_validation import train_test_split import numpy as np import keras num_classes=12 img_rows=128 img_cols=128 train_X, test_X, train_y, test_y = train_test_split(train_data, y, test_size = 0.3, random_state = 0) train_X = train_X.reshape(train_X.shape[0], img_rows, img_cols, 1) test_X = test_X.reshape(test_X.shape[0], img_rows, img_cols, 1) train_X = train_X.astype('float32') test_X = test_X.astype('float32') train_X /= 255 test_X /= 255 print('x_train shape:', train_X.shape) print(train_X.shape[0], 'train samples') print(test_X.shape[0], 'test samples') # convert class vectors to binary class matrices train_y = keras.utils.to_categorical(train_y, num_classes) test_y = keras.utils.to_categorical(test_y, num_classes)\nView Code\nfrom keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten from keras.models import Sequential from keras.layers import Convolution2D,MaxPooling2D, Conv2D import keras model = Sequential() model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 1))) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) # model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation='relu')) # model.add(Dropout(0.5)) model.add(Dense(12, activation='softmax')) model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy']) model.fit(train_X, train_y, batch_size=128, epochs=50, verbose=1, validation_data=(test_X, test_y)) score = model.evaluate(test_X, test_y, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1])\nView Code\nCNN的混淆矩阵比KNN的好了不少\n训练了好多次，不断调整各个卷积层和参数，终于达到了一个比较好的效果~~\n题目6\n使用深度学习框架，基于一个常用的模型，比如Faster R-CNN，训练一个目标检测的模型。这个模型需要能够检测并且使用方框框出图片中出现的每一个头盔。\n这道题目首先要自己标注样本，几百张图像标注完累的半死。。。这里我们使用VOTT来标注，它会自动生成一个样本描述文件，很方便。Faster R-CNN的程序我们参考了git上的一个红细胞检测的项目，https://github.com/THULiusj/CosmicadDetection-Keras-Tensorflow-FasterRCNN，代码非常多就不贴了\n最后来一张效果图\n本文数据集和VOTT工具 链接：\nhttps://pan.baidu.com/s/1FFw0PLJrrOhwR6J1HexPJA\n提取码 s242"}
{"content2":"Logistic本质上是一个基于条件概率的判别模型(DiscriminativeModel)。\n函数图像为：\n通过sigma函数计算出最终结果，以0.5为分界线，最终结果大于0.5则属于正类(类别值为1)，反之属于负类(类别值为0)。\n如果将上面的函数扩展到多维空间，并且加上参数，则函数变成：\n接下来问题来了，如何得到合适的参数向量θ呢？\n由于sigma函数的特性，我们可作出如下的假设：\n上式即为在已知样本X和参数θ的情况下，样本X属性正类(y=1)和负类(y=0)的条件概率。\n将两个公式合并成一个，如下：\n既然概率出来了，那么最大似然估计也该出场了。假定样本与样本之间相互独立，那么整个样本集生成的概率即为所有样本生成概率的乘积：\n为了简化问题，我们对整个表达式求对数，(将指数问题对数化是处理数学问题常见的方法)：\n满足似然函数(θ)的最大的θ值即是我们需要求解的模型。\n梯度上升算法\n就像爬坡一样，一点一点逼近极值。爬坡这个动作用数学公式表达即为：\n其中，α为步长。\n回到Logistic Regression问题，我们同样对函数求偏导。\n先看：\n其中：\n再由：\n可得：\n接下来就剩下第三部分：\n(这个公式应该很容易理解，简单的偏导公式)\n还有就是：\n综合三部分即得到：\n因此，梯度迭代公式为：\n结合本式再去理解《机器学习实战》Page 78中的代码就很简单了。\n摘自：http://sbp810050504.blog.51cto.com/2799422/1608064"}
{"content2":"英文：Quora\n译文：CSDN CODE\n链接：http://code.csdn.net/news/2822818\nGraphLab\nGraphLab是一种新的面向机器学习的并行框架。\nGraphLab提供了一个完整的平台，让机构能够使用可扩展的机器学习系统建立大数据以分析产品，该公司客户包含Zillow、Adobe、Zynga、Pandora、Bosch、ExxonMobil等，它们从别的应用程序或者服务中抓取数据，通过推荐系统、欺诈监測系统、情感及社交网络分析系统等系统模式将大数据理念转换为生产环境下能够使用的预測应用程序。\n项目主页： http://graphlab.org/\nVowpal Wabbit\nVowpal Wabbit(Fast Online Learning)最初是由雅虎研究院建设的一个机器学习平台。眼下该项目在微软研究院。\n它是由John Langford启动并主导的项目。\n项目地址： http://hunch.net/~vw/\nscikits.learn\nscikit-learn是一个开源的、构建在SciPy之上用于机器学习的 Python 模块。它包含简单而高效的工具，可用于数据挖掘和数据分析。适合于不论什么人，可在各种情况下反复使用、构建在 NumPy、SciPy和 matplotlib 之上，遵循BSD 协议。\n项目地址： http://scikit-learn.org/stable\nTheano\nTheano是一个python库，用来定义、优化和模拟数学表达式计算，用于高效的解决多维数组的计算问题。\n它使得写深度学习模型更加easy，同一时候也给出了一些关于在GPU上训练它们的选项。\n项目地址： http://deeplearning.net/software/theano/\nMahout\nMahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目。提供一些可扩展的机器学习领域经典算法的实现。旨在帮助开发者更加方便快捷地创建智能应用程序。\nMahout包括很多实现。包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 能够有效地扩展到云中。\n项目主页： http://mahout.apache.org/\npybrain\npybrain是Python的一个机器学习模块，它的目标是为机器学习任务提供灵活、易应、强大的机器学习算法。pybrain包含神经网络、强化学习(及二者结合)、无监督学习、进化算法。以神经网络为核心，全部的训练方法都以神经网络为一个实例。\n项目主页： http://pybrain.org/\nOpenCV\nOpenCV是一个基于（开源）发行的跨平台计算机视觉库。能够执行在Linux、Windows和Mac OS操作系统上。它轻量级并且高效——由一系列 C 函数和少量 C++ 类构成。同一时候提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的非常多通用算法。\n项目主页： http://opencv.org/\nOrange\nOrange 是一个基于组件的数据挖掘和机器学习软件套装，它的功能即友好，又非常强大，高速而又多功能的可视化编程前端。以便浏览数据分析和可视化，基绑定了 Python以进行脚本开发。\n它包括了完整的一系列的组件以进行数据预处理，并提供了数据帐目，过渡。建模，模式评估和勘探的功能。\n项目主页： http://orange.biolab.si/\nNLTK\nNLTK（natural language toolkit)是python的自然语言处理工具包。\n2001年推出。至今发展很活跃。它的主要作用是为了教学。至今已经在20多个国家60多所高校使用。里面包含了大量的词料库，以及自然语言处理方面的算法实现：分词， 词根计算。 分类， 语义分析等。\n项目主页： http://nltk.org/\nNupic\nNupic是一个开源的人工智能平台。\n该项目由Grok（原名 Numenta）公司开发，当中包含了公司的算法和软件架构。 NuPIC 的运作接近于人脑，“当模式变化的时候，它会忘掉旧模式，记忆新模式”。\n如人脑一样。CLA 算法可以适应新的变化。\n项目主页： http://numenta.org/nupic.html"}
{"content2":"机器学习、数据挖掘、人工智能、统计模型这么多概念有何差异\n在各种各样的数据科学论坛上这样一个问题经常被问到——机器学习和统计模型的差别是什么？这确实是一个难以回答的问题。考虑到机器学习和统计模型解决问题的相似性，两者的区别似乎仅仅在于数据量和模型建立者的不同。这里有一张覆盖机器学习和统计模型的数据科学维恩图。\n在这篇文章中，我将尽最大的努力来展示机器学习和统计模型的区别，同时也欢迎业界有经验的朋友对本文进行补充。\n在我开始之前，让我们先明确使用这些工具背后的目标。无论采用哪种工具去分析问题，最终的目标都是从数据获得知识。两种方法都旨在通过分析数据的产生机制挖掘背后隐藏的信息。\n两种方法的分析目标是相同的。现在让我们详细的探究一下其定义及差异。\n定义\n机器学习：一种不依赖于规则设计的数据学习算法。\n统计模型：以数学方程形式表现变量之间关系的程式化表达\n对于喜欢从实际应用中了解概念的人，上述表达也许并不明确。让我们看一个商务的案例。\n商业案例\n让我们用麦肯锡发布的一个有趣案例来区分两个算法。\n案例：分析理解电信公司一段时间内客户的流失水平。\n可获得数据：两个驱动-A&B;\n麦肯锡接下来的展示足够让人兴奋。盯住下图来理解一下统计模型和机器学习算法的差别。\n从上图中你观察到了什么？统计模型在分类问题中得到一个简单的分类线。一条非线性的边界线区分了高风险人群和低风险人群。但当我们看到通过机器学习产生的颜色时，我们发现统计模型似乎没有办法和机器学习算法进行比较。机器学习的方法获得了任何边界都无法详细表征的信息。这就是机器学习可以为你做的。\n机器学习还被应用在YouTube和Google的引擎推荐上，机器学习通过瞬间分析大量的观测样本给出近乎完美的推荐建议。即使只采用一个16 G 内存的笔记本，我每天处理数十万行的数千个参数的模型也不会超过30分钟。然而一个统计模型需要在一台超级计算机跑一百万年来来观察数千个参数。\n机器学习和统计模型的差异：\n在给出了两种模型在输出上的差异后，让我们更深入的了解两种范式的差异，虽然它们所做的工作类似。\n1、所属的学派\n2、产生时间\n3、基于的假设\n4、处理数据的类型\n5、操作和对象的术语\n6、使用的技术\n7、预测效果和人力投入\n以上提到的方面都能从每种程度上区分机器学习和统计模型，但并不能给出机器学习和统计模型的明确界限。\n分属不同的学派\n机器学习：计算机科学和人工智能的一个分支,通过数据学习构建分析系统，不依赖明确的构建规则。统计模型：数学的分支用以发现变量之间相关关系从而预测输出。\n诞生年代不同\n统计模型的历史已经有几个世纪之久。但是机器学习却是最近才发展起来的。二十世纪90年代，稳定的数字化和廉价的计算使得数据科学家停止建立完整的模型而使用计算机进行模型建立。这催生了机器学习的发展。随着数据规模和复杂程度的不断提升，机器学习不断展现出巨大的发展潜力。\n假设程度差异\n统计模型基于一系列的假设。例如线性回归模型假设：\n（1） 自变量和因变量线性相关\n（2） 同方差\n（3） 波动均值为0\n（4） 观测样本相互独立\n（5） 波动服从正态分布\nLogistics回归同样拥有很多的假设。即使是非线性回归也要遵守一个连续的分割边界的假设。然而机器学习却从这些假设中脱身出来。机器学习最大的好处在于没有连续性分割边界的限制。同样我们也并不需要假设自变量或因变量的分布。\n数据区别\n机器学习应用广泛。在线学习工具可飞速处理数据。这些机器学习工具可学习数以亿计的观测样本，预测和学习同步进行。一些算法如随机森林和梯度助推在处理大数据时速度很快。机器学习处理数据的广度和深度很大。但统计模型一般应用在较小的数据量和较窄的数据属性上。\n命名公约\n下面一些命名几乎指相同的东西：\n公式：\n虽然统计模型和机器学习的最终目标是相似的，但其公式化的结构却非常不同\n在统计模型中，我们试图估计f 函数 通过\n因变量（Y）=f(自变量) 扰动 函数\n机器学习放弃采用函数f的形式，简化为：\n输出（Y）——> 输入（X）\n它试图找到n维变量X的袋子，在袋子间Y的取值明显不同。\n预测效果和人力投入\n自然在事情发生前并不给出任何假设。一个预测模型中越少的假设，越高的预测效率。机器学习命名的内在含义为减少人力投入。机器学习通过反复迭代学习发现隐藏在数据中的科学。由于机器学习作用在真实的数据上并不依赖于假设，预测效果是非常好的。统计模型是数学的加强，依赖于参数估计。它要求模型的建立者，提前知道或了解变量之间的关系。\n结束语\n虽然机器学习和统计模型看起来为预测模型的不同分支，但它们近乎相同。通过数十年的发展两种模型的差异性越来越小。模型之间相互渗透相互学习使得未来两种模型的界限更加模糊。http://www.cda.cn/view/19060.html"}
{"content2":"我一直很好奇人工智能是如何提出来的，它背后有什么样的故事，在人工智能发展的这60年的时间中，又经历了什么？为什么现在才是人工智能的爆发点，未来人工智能又将走向何处？带着这样的问题我读了吴军博士的《智能时代》这本书，打开了我对人工智能的了解，这篇文章主要内容也来自于这本书。\n我们这代人对人工智能的关注，来自于2016年AlphaGo大战世界著名围棋选手李世民，在比赛之前各方关注度非常高，国内各方媒体争相报道，预测这场比赛的结果，人们好奇人工智能现在智能到什么程度以及计算机如何和人下围棋，最终AlphaGo以4：1胜了李世明，大家都在感慨人工智能时代即将来临。仅仅过了一年，2017年5月27日AlphaGo的2.0版本3:0战胜围棋世界排名第一的柯洁九段，从此在AlphaGo面前已无人类对手。\n计算机之所以能够战胜人类，是因为机器获得智能的方式和人类不同，它不是靠逻辑推理，而是靠大数据和算法。Google使用了几十万盘围棋高手之间的对弈的数据来训练AlphaGo，这是它获得所谓“智能”的原因。在计算方面，Google使用了几十万台服务器来训练AlphaGo下棋模型，并让不同的AlphaGo相互对弈上千万盘。第二个关键技术是启发式搜索算法-蒙特卡洛树搜索算法（英语：Monte Carlo tree search；简称：MCTS），它能将搜索的空间限制在非常有限的范围内，保证计算机能够快速找到好的下法。由此可见，下围棋这个看似智能型的问题，从本质上讲，是一个大数据和算法的问题。\n说到人工智能，就不得不提计算机届的一个传奇人物：阿兰.图灵博士。1950年，图灵在《思想》（mind）杂志上发表了一篇《计算的机器和智能》的论文。在论文中，图灵既没有讲计算机怎样才能获得智能，也没有提出如何解决复杂问题的智能方法，知识提出了一个验证机器有无智能的的判别方法。\n让一台机器和一个人坐在幕后，让一个裁判同时与幕后的人和机器进行交流，如果这个裁判无法判断自己交流的对象是人还是机器，就说明这台机器有了和人同等的智能。就是大名鼎鼎的图灵测试。后来，计算机科学家对此进行了补充，如果计算机实现了下面几件事情中的一件，就可以认为它有图灵所说的那种智能：\n1、语音识别\n2、机器翻译\n3、文本的自动摘要或者写作\n4、战胜人类的国际象棋冠军\n5、自动回答问题\n今天，计算机已经做到了上述的这几件事情，甚至还超额完成了任务，比如现在的围棋比国际象棋要高出6-8个数量级，当然，人类走到这一步并非一帆风顺，而是走了几十年的弯路。\n人工智能的诞生：1943 - 1956\n在20世纪40年代和50年代，来自不同领域（数学，心理学，工程学，经济学和政治学）的一批科学家开始探讨制造人工大脑的可能性。1956年，人工智能被确立为一门学科。\n1956年的夏天，香农和一群年轻的学者在达特茅斯学院召开了一次头脑风暴式研讨会。会议的组织者是马文·闵斯基，约翰·麦卡锡和另两位资深科学家Claude Shannon以及Nathan Rochester，后者来自IBM。与会者包括Ray Solomonoff，Oliver Selfridge，Trenchard More，Arthur Samuel，Newell和Simon，他们中的每一位都将在AI研究的第一个十年中作出重要贡献。\n会议虽然叫做“达特茅斯夏季人工智能研究会议”，其实它不同于今天我们召开几天的学术会议，因为一来没有什么可以报告的科研成果，二来这个会议持续了一个暑假。事实上，这是一次头脑风暴式的讨论会，这10位年轻的学者讨论的是当时计算机尚未解决，甚至尚未开展研究的问题，包括人工智能、自然语言处理和神经网络等。\n会上纽厄尔和西蒙讨论了“逻辑理论家”，而麦卡锡则说服与会者接受“人工智能”一词作为本领域的名称。1956年达特矛斯会议上人工智能的名称和任务得以确定，同时出现了最初的成就和最早的一批研究者，因此这一事件被广泛承认为人工智能诞生的标志。\n60年前的达特茅斯大学\n黄金年代：1956 - 1974\n达特茅斯会议之后的数年是大发现的时代。对许多人而言，这一阶段开发出的程序堪称神奇：计算机可以解决代数应用题，证明几何定理，学习和使用英语。当时大多数人几乎无法相信机器能够如此“智能”。研究者们在私下的交流和公开发表的论文中表达出相当乐观的情绪，认为具有完全智能的机器将在二十年内出现。ARPA（国防高等研究计划署）等政府机构向这一新兴领域投入了大笔资金。\n第一代AI研究者们非常乐观，曾作出了如下预言:\n1958年，H. A. Simon，Allen Newell：“十年之内，数字计算机将成为国际象棋世界冠军。” “十年之内，数字计算机将发现并证明一个重要的数学定理。”\n1965年，H. A. Simon：“二十年内，机器将能完成人能做到的一切工作。”\n1967年，Marvin Minsky：“一代之内……创造‘人工智能’的问题将获得实质上的解决。”\n1970年，Marvin Minsky：“在三到八年的时间里我们将得到一台具有人类平均智能的机器。”\n早期，人工智能使用传统的人工智能方法进行研究，什么是传统的人工智能研究呢？简单的讲，就是首先了解人类是如何产生智能的，然后让计算机按照人的思路去做。因此在语音识别、机器翻译等领域迟迟不能突破，人工智能研究陷入低谷。\n第一次AI低谷：1974 - 1980\n由于人工智能研究者们对项目难度评估不足，这除了导致承诺无法兑现外，还让人们当初的乐观期望遭到严重打击。到了70年代，人工智能开始遭遇批评，研究经费也被转移到那些目标明确的特定项目上。\n1972年康奈尔大学的教授弗雷德.贾里尼克（Fred Jelinek)被要求到IBM做语音识别。在之前各个大学和研究这个问题已经花了20多年的时间，主流的研究方法有两个特点，一个是让计算机尽可能地模拟人的发音特点和听觉特征，一个是让计算机尽可能的方法理解人所讲的完整的语句。对于前一项研究，有被称为特征提取，后一项的研究大都使用传统人工智能的方法，它基于规则和语义。\n贾里尼克任务，人的大脑是一个信息源，从思考到找到合适的语句，再通过发音说出来，是一个编码的过程，经过媒介传播到耳朵，是一个解码的过程。既然是一个典型的通讯问题，那就可以用解决通讯方法来解决问题，为此贾里尼克用两个数据模型（马尔科夫模型）分别描述信源和信道。然后使用大量的语音数据来训练。最后，贾里尼克团队花了4年团队，将语音识别从过去的70%提高到90%。后来人们尝试使用此方法来解决其他智能问题，但因为缺少数据，结果不太理想。\n在当时，由于计算机性能的瓶颈、计算复杂性的指数级增长、数据量缺失等问题，一些难题看上去好像完全找不到答案。比如像今天已经比较常见的机器视觉功能在当时就不可能找到一个足够大的数据库来支撑程序去学习，机器无法吸收足够的数据量自然也就谈不上视觉方面的智能化。\n项目的停滞不但让批评者有机可乘——1973年Lighthill针对英国人工智能研究状况的报告批评了人工智能在实现其“宏伟目标”上的完全失败，也影响到了项目资金的流向。人工智能遭遇了6年左右的低谷。\n繁荣：1980 - 1987\n在80年代，一类名为“专家系统”的AI程序开始为全世界的公司所采纳，而“知识处理”成为了主流AI研究的焦点。1981年，日本经济产业省拨款八亿五千万美元支持第五代计算机项目。其目标是造出能够与人对话，翻译语言，解释图像，并且像人一样推理的机器。\n受到日本刺激，其他国家纷纷作出响应。英国开始了耗资三亿五千万英镑的Alvey工程。美国一个企业协会组织了MCC（Microelectronics and Computer Technology Corporation，微电子与计算机技术集团），向AI和信息技术的大规模项目提供资助。DARPA也行动起来，组织了战略计算促进会（Strategic Computing Initiative），其1988年向AI的投资是1984年的三倍。人工智能又迎来了大发展。\n早期的专家系统Symbolics 3640\n专家系统是一种程序，能够依据一组从专门知识中推演出的逻辑规则在某一特定领域回答或解决问题。最早的示例由Edward Feigenbaum和他的学生们开发。1965年起设计的Dendral能够根据分光计读数分辨混合物。1972年设计的MYCIN能够诊断血液传染病。它们展示了这一方法的威力。专家系统仅限于一个很小的知识领域，从而避免了常识问题；其简单的设计又使它能够较为容易地编程实现或修改。总之，实践证明了这类程序的实用性。直到现在AI才开始变得实用起来。\n专家系统的能力来自于它们存储的专业知识。这是70年代以来AI研究的一个新方向。Pamela McCorduck在书中写道，“不情愿的AI研究者们开始怀疑，因为它违背了科学研究中对最简化的追求。智能可能需要建立在对分门别类的大量知识的多种处理方法之上。” “70年代的教训是智能行为与知识处理关系非常密切。有时还需要在特定任务领域非常细致的知识。”知识库系统和知识工程成为了80年代AI研究的主要方向。\n1982年，物理学家John Hopfield证明一种新型的神经网络（现被称为“Hopfield网络”）能够用一种全新的方式学习和处理信息。大约在同时（早于Paul Werbos），David Rumelhart推广了反向传播算法，一种神经网络训练方法。这些发现使1970年以来一直遭人遗弃的联结主义重获新生。\n第二次AI低谷：1987 - 1993\n“AI之冬”一词由经历过1974年经费削减的研究者们创造出来。他们注意到了对专家系统的狂热追捧，预计不久后人们将转向失望。事实被他们不幸言中：从80年代末到90年代初，AI遭遇了一系列财政问题。\n变天的最早征兆是1987年AI硬件市场需求的突然下跌。Apple和IBM生产的台式机性能不断提升，到1987年时其性能已经超过了Symbolics和其他厂家生产的昂贵的Lisp机。老产品失去了存在的理由：一夜之间这个价值五亿美元的产业土崩瓦解。\nXCON等最初大获成功的专家系统维护费用居高不下。它们难以升级，难以使用，脆弱（当输入异常时会出现莫名其妙的错误），成了以前已经暴露的各种各样的问题的牺牲品。专家系统的实用性仅仅局限于某些特定情景。到了80年代晚期，战略计算促进会大幅削减对AI的资助。DARPA的新任领导认为AI并非“下一个浪潮”，拨款将倾向于那些看起来更容易出成果的项目。\n1991年人们发现十年前日本人宏伟的“第五代工程”并没有实现。事实上其中一些目标，比如“与人展开交谈”，直到2010年也没有实现。与其他AI项目一样，期望比真正可能实现的要高得多。\n走在正确的路上：1993 - 2005\n现已年过半百的AI终于实现了它最初的一些目标。它已被成功地用在技术产业中，不过有时是在幕后。这些成就有的归功于计算机性能的提升，有的则是在高尚的科学责任感驱使下对特定的课题不断追求而获得的。不过，至少在商业领域里AI的声誉已经不如往昔了。\n“实现人类水平的智能”这一最初的梦想曾在60年代令全世界的想象力为之着迷，其失败的原因至今仍众说纷纭。各种因素的合力将AI拆分为各自为战的几个子领域，有时候它们甚至会用新名词来掩饰“人工智能”这块被玷污的金字招牌。AI比以往的任何时候都更加谨慎，却也更加成功。\n第一次让全世界感到计算机智能水平有了质的飞跃实在1966年，IBM的超级计算机深蓝大战人类国际象棋冠军卡斯伯罗夫，卡斯伯罗夫是世界上最富传奇色彩的国际象棋世界冠军，这次比赛最后以4：2比分战胜了深蓝。对于这次比赛媒体认为深蓝虽然输了比赛，但这毕竟是国际象棋上计算机第一次战胜世界冠军两局。时隔一年后，改进后的深蓝卷土重来，以3.5：2.5的比分战胜了斯伯罗夫。自从1997年以后，计算机下棋的本领越来越高，进步超过人的想象。到了现在，棋类游戏中计算机已经可以完败任何人类。\n深蓝实际上收集了世界上百位国际大师的对弈棋谱，供计算机学习。这样一来，深蓝其实看到了名家们在各种局面下的走法。当然深蓝也会考虑卡斯伯罗夫可能采用的走法，对不同的状态给出可能性评估，然后根据对方下一步走法对盘面的影响，核实这些可能性的估计，找到一个最有利自己的状态，并走出这步棋。因此深蓝团队其实把一个机器智能问题变成了一个大数据和大量计算的问题。\nIBM“深蓝”战胜国际象棋世界冠军\n越来越多的AI研究者们开始开发和使用复杂的数学工具。人们广泛地认识到，许多AI需要解决的问题已经成为数学，经济学和运筹学领域的研究课题。数学语言的共享不仅使AI可以与其他学科展开更高层次的合作，而且使研究结果更易于评估和证明。AI已成为一门更严格的科学分支。\nJudea Pearl发表于1988年的名著将概率论和决策理论引入AI。现已投入应用的新工具包括贝叶斯网络，隐马尔可夫模型，信息论，随机模型和经典优化理论。针对神经网络和进化算法等“计算智能”范式的精确数学描述也被发展出来。\n大数据：2005 - 现在\n从某种意义上讲，2005年是大数据元年，虽然大部分人感受不到数据带来的变化，但是一项科研成果却让全世界从事机器翻译的人感到震惊，那就是之前在机器翻译领域从来没有技术积累、不为人所知的Google，以巨大的优势打败了全世界所有机器翻译研究团队，一跃成为这个领域的领头羊。\n就是Google花重金请到了当时世界上水平最高的机器翻译专家弗朗兹·奥科 (Franz Och)博士。奥科用了上万倍的数据来训练系统。量变的积累就导致了质变的发生。奥科能训练出一个六元模型，而当时大部分研究团队的数据量只够训练三元模型。简单地讲，一个 好的三元模型可以准确地构造英语句子中的短语和简单的句子成分之间的搭配，而六元模型则可以构造整个从句和复杂的句子成分之间的搭配，相当于将这些片段从一种语言到另一种语言直接对译过去了。不难想象，如果一个系统对大部分句子在很长的片段上直译，那么其准确性相比那些在词组单元做翻译的系统要准确得多。\n如今在很多与“智能”有关的研究领域，比如图像识别和自然语言理解，如果所采用的方法无法利用数据量的优势，会被认为是落伍的。\n数据驱动方法从20世纪70年代开始起步，在八九十年代得到缓慢但稳步的发展。进入21世纪后，由于互联网的出现，使得可用的数据量剧增，数据驱动方法的优势越来越明显，最终完成了从量变到质变的飞跃。如今很多需要类似人类智能才能做的事情，计算机已经可以胜任了，这得益于数据量的增加。\n全世界各个领域数据不断向外扩展，渐渐形成了另外一个特点，那就是很多数据开始出现交叉，各个维度的数据从点和线渐渐连成了网，或者说，数据之间的关联性极大地增强，在这样的背景下，就出现了大数据。\n大数据是一种思维方式的改变。现在的相比过去大了很多，量变带来了质变，思维方式、做事情的方法就应该和以往有所不同。这其实是帮助我们理解大数据概念的一把钥匙。在有大数据之前，计算机并不擅长解决需要人类智能来解决的问题，但是今天这些问题换个思路就可以解决了，其核心就是变智能问题为数据问题。由此，全世界开始了新的一轮技术革命——智能革命。\n建议购买正版书籍，如需试读电子版本，请在公众号回复：”智能时代“\n参考：\n人工智能史\n智能时代\n作者：纯洁的微笑\n出处：http://www.ityouknow.com/\n版权归作者所有，转载请注明出处"}
{"content2":"这次作业的coding任务量比较大，总的来说需要实现neural network, knn, kmeans三种模型。\nQ11~Q14为Neural Network的题目，我用单线程实现的，运行的时间比较长，因此把这几道题的正确答案记录如下：\nQ11: 6\nQ12: 0.001\nQ13: 0.01\nQ14: 0.02 ≤ Eout ≤ 0.04\n其中Q11和Q14的答案比较明显，Q12和Q13有两个答案比较接近（参考了讨论区的内容，最终也调出来了）\nneural network的代码实现思路如下：\n1）实现权重矩阵W初始化（def init_W(nnet_struct, w_range))\n2）实现计算每一轮神经元输出的函数，即bp算法中的forward过程(def forward_process(x, y, W))\n3）实现计算每一轮output error对于每个神经元输入score的导数，即bp算法中的backward过程(def backward_process(x, y, neuron_output, W))\n4）利用梯度下降方法，更新各层权重矩阵W的函数(def update_W_withGD(x, neuron_output, gradient, W, ita))\n其中最难的是步骤3），要想实现矩阵化编程，需要对神经网络的每层结构熟练，同时对于你使用的编程语言的矩阵化操作要非常熟悉；自己在这个方面比较欠缺，还得是熟能生巧。\n>>自己第一次写NNet的算法，从单隐层（隐层个数2）开始调试的：按照模块1）2）3）4）的顺序，各个模块调试；循序渐进的调试速度比较慢，但模块质量高一些，后面的联合调试就省事一些。\n>>如果是特别复杂的网络，如何对这种gradient的算法进行调试呢？因为gradient各个点的gradient几乎是不可能都算到的，在网上查了gradient checking方法：http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n>>NNet的调参真的很重要，就Q14来说，即使是hidden units的总个数一样，如果每层的个数不同，最后的结果也是有差别的（我第一次比较粗心，把NNet的结构按照 3 8 1这样了，发现结果没有 8 3 1这样好），后面多搜搜调参相关的资料积累一下。\n代码如下（没有把调试的代码删掉，可以记录调试的经过，同时也防止以后犯类似的错误），确实乱了一些，请看官包涵了：\n#encoding=utf8 import sys import numpy as np import math from random import * ## # read data from local file # return with numpy array def read_input_data(path): x = [] y = [] for line in open(path).readlines(): if line.strip()=='': continue items = line.strip().split(' ') tmp_x = [] for i in range(0,len(items)-1): tmp_x.append(float(items[i])) x.append(tmp_x) y.append(float(items[-1])) return np.array(x),np.array(y) ## # initialize weight matrix # input neural network structure & initilizing uniform value range (both low and high) # each layer's bias need to be added # return with inialized W def init_W(nnet_struct, w_range): W = [] for i in range(1,len(nnet_struct)): tmp_w = np.random.uniform(w_range['low'], w_range['high'], (nnet_struct[i-1]+1,nnet_struct[i]) ) W.append(tmp_w) return W ## # randomly pick sample from raw data for Stochastic Gradient Descent # T indicates the iterative numbers # return with data for each SGD iteration def pick_SGD_data(x, y, T): sgd_x = np.zeros((T,x.shape[1])) sgd_y = np.zeros(T) for i in range(T): index = randint(0, x.shape[0]-1) sgd_x[i] = x[index] sgd_y[i] = y[index] return sgd_x, sgd_y ## # forward process # calculate each neuron's output def forward_process(x, y, W): ret = [] #print W[0].shape #print W[1].shape pre_x = np.hstack((1,x)) for i in range(len(W)): pre_x = np.tanh(np.dot(pre_x, W[i])) ret.append(pre_x) pre_x = np.hstack((1,pre_x)) return ret ## # backward process # calcultae the gradient of error and each neuron's input score def backward_process(x, y, neuron_output, W): ret = [] L = len(neuron_output) # print neuron_output[0].shape, neuron_output[1].shape # Output layer score = np.dot( np.hstack((1, neuron_output[L-2])), W[L-1]) # print score # print score.shape gradient = np.array( [-2 * (y-neuron_output[L-1][0]) * tanh_gradient(score)] ) # print gradient # print gradient.shape ret.insert(0, gradient) # Hidden layer for i in range(L-2,-1,-1): if i==0: score = np.dot(np.hstack((1, x)),W[i]) # print score.shape # print gradient.shape # print W[1][1:].transpose().shape # print score gradient = np.dot(gradient, W[1][1:].transpose()) * tanh_gradient(score) # print gradient # print gradient.shapeq ret.insert(0, gradient) else: score = np.dot(np.hstack((1,neuron_output[i-1])),W[i]) # print score.shape # print gradient.shape # print W[i+1][1:].transpose().shape # print \"......\" gradient = np.dot(gradient , W[i+1][1:].transpose()) * tanh_gradient(score) # print gradient.shape # print \"======\" ret.insert(0, gradient) return ret # give a numpy array # boardcast tanh gradient to each element def tanh_gradient(s): ret = np.zeros(s.shape) for i in range(s.shape[0]): ret[i] = 4.000001 / (math.exp(2*s[i])+math.exp(-2*s[i])+2) return ret ## # update W with Gradient Descent def update_W_withGD(x, neuron_output, gradient, W, ita): ret = [] L = len(W) # print \"L:\"+str(L) # print neuron_output[0].shape, neuron_output[1].shape # print gradient[0].shape, gradient[1].shape # print W[0].shape, W[1].shape # print np.hstack((1,x)).transpose().shape # print gradient[0].shape ret.append( W[0] - ita * np.array([np.hstack((1,x))]).transpose() * gradient[0] ) for i in range(1, L, 1): ret.append( W[i] - ita * np.array([np.hstack((1,neuron_output[i-1]))]).transpose() * gradient[i] ) # print len(ret) return ret ## # calculate Eout def calculate_E(W, path): x,y = read_input_data(path) error_count = 0 for i in range(x.shape[0]): if predict(x[i],y[i],W): error_count += 1 return 1.000001*error_count/x.shape[0] def predict(x, y, W): y_predict = x for i in range(0, len(W), 1): y_predict = np.tanh( np.dot( np.hstack((1,y_predict)), W[i] ) ) y_predict = 1 if y_predict>0 else -1 return y_predict!=y ## # Q11 def Q11(x,y): R = 20 # repeat time Ms = { 6, 16 } # hidden units M_lowests = {} for M in Ms: M_lowests[M] = 0 for r in range(R): T = 50000 ita = 0.1 min_M = -1 E_min = float(\"inf\") for M in Ms: sgd_x, sgd_y = pick_SGD_data(x, y, T) nnet_struct = [ x.shape[1], M, 1 ] # print nnet_struct w_range = {} w_range['low'] = -0.1 w_range['high'] = 0.1 W = init_W(nnet_struct, w_range) # for i in range(len(W)): # print W[i] # print sgd_x,sgd_y for t in range(T): neuron_output = forward_process(sgd_x[t], sgd_y[t], W) # print sgd_x[t],sgd_y[t] # print W # print neuron_output error_neuronInputScore_gradient = backward_process(sgd_x[t], sgd_y[t], neuron_output, W) # print error_neuronInputScore_gradient W = update_W_withGD(sgd_x[t], neuron_output, error_neuronInputScore_gradient, W, ita) E = calculate_E(W,\"test.dat\") # print str(r)+\":::\"+str(M)+\":\"+str(E) M_lowests[M] += E for k,v in M_lowests.items(): print str(k)+\":\"+str(v) ## # Q12 def Q12(x,y): ita = 0.1 M = 3 nnet_struct = [ x.shape[1], M, 1 ] Rs = { 0.001, 0.1 } R_lowests = {} for R in Rs: R_lowests[R] = 0 N = 40 T = 30000 for i in range(N): for R in Rs: sgd_x, sgd_y = pick_SGD_data(x, y, T) w_range = {} w_range['low'] = -1*R w_range['high'] = R W = init_W(nnet_struct, w_range) for t in range(T): neuron_output = forward_process(sgd_x[t], sgd_y[t], W) error_neuronInputScore_gradient = backward_process(sgd_x[t], sgd_y[t], neuron_output, W) W = update_W_withGD(sgd_x[t], neuron_output, error_neuronInputScore_gradient, W, ita) E = calculate_E(W, \"test.dat\") print str(R)+\":\"+str(E) R_lowests[R] += E for k,v in R_lowests.items(): print str(k)+\":\"+str(v) ## # Q13 def Q13(x,y): M = 3 nnet_struct = [ x.shape[1], M, 1 ] itas = {0.001,0.01,0.1} ita_lowests = {} for ita in itas: ita_lowests[ita] = 0 N = 20 T = 20000 for i in range(N): for ita in itas: sgd_x, sgd_y = pick_SGD_data(x, y, T) w_range = {} w_range['low'] = -0.1 w_range['high'] = 0.1 W = init_W(nnet_struct, w_range) for t in range(T): neuron_output = forward_process(sgd_x[t], sgd_y[t], W) error_neuronInputScore_gradient = backward_process(sgd_x[t], sgd_y[t], neuron_output, W) W = update_W_withGD(sgd_x[t], neuron_output, error_neuronInputScore_gradient, W, ita) E = calculate_E(W, \"test.dat\") print str(ita)+\":\"+str(E) ita_lowests[ita] += E for k,v in ita_lowests.items(): print str(k)+\":\"+str(v) ## # Q14 def Q14(x,y): T = 50000 ita = 0.01 E_total = 0 R = 10 for i in range(R): nnet_struct = [ x.shape[1], 8, 3, 1 ] w_range = {} w_range['low'] = -0.1 w_range['high'] = 0.1 W = init_W(nnet_struct, w_range) sgd_x, sgd_y = pick_SGD_data(x, y, T) for t in range(T): neuron_output = forward_process(sgd_x[t], sgd_y[t], W) error_neuronInputScore_gradient = backward_process(sgd_x[t], sgd_y[t], neuron_output, W) W = update_W_withGD(sgd_x[t], neuron_output, error_neuronInputScore_gradient, W, ita) E = calculate_E(W, \"test.dat\") print E E_total += E print E_total*1.0/R def main(): x,y = read_input_data(\"train.dat\") # print x.shape, y.shape # Q11(x, y) # Q12(x, y) # Q13(x, y) Q14(x, y) if __name__ == '__main__': main()\nQ15~Q18是KNN算法相关的，各道题几乎秒出结果，这里不记录答案了：\nKNN的核心，也就是KNN函数了：\n1）给定K个邻居数，返回这个点属于哪一类，代码尽量写的可配置一些\n2）numpy有个argsort函数，可以根据数组的value大小，对下标index进行排序；并返回排序后的index；利用好这个特性，代码很简洁\n3）如果是其他的语言，应该实现一个类似numpy.argsort的模块，代码整体上清晰不少能\nKNN的代码如下：\n#encoding=utf8 import sys import numpy as np import math from random import * ## # read data from local file # return with numpy array def read_input_data(path): x = [] y = [] for line in open(path).readlines(): if line.strip()=='': continue items = line.strip().split(' ') tmp_x = [] for i in range(0,len(items)-1): tmp_x.append(float(items[i])) x.append(tmp_x) y.append(float(items[-1])) return np.array(x),np.array(y) ## # KNN ( for binary classification ) # input all labeled data & test sample # return with label def KNN(k, x, y, test_x): distance = np.sum((x-test_x)*(x-test_x), axis=1) order = np.argsort(distance) ret = 0 for i in range(k): ret += y[order[i]] return 1 if ret>0 else -1 ## # Q15 calculate Ein def calculate_Ein(x, y): error_count = 0 k = 5 for i in range(x.shape[0]-1): # tmp_x = np.vstack( ( x[0:i],x[(i+1):(x.shape[0]-1)] ) ) # tmp_y = np.hstack( ( y[0:i],y[(i+1):(x.shape[0]-1)] ) ) ret = KNN( k, x, y, x[i]) if y[i]!=ret: error_count += 1 return 1.0*error_count/x.shape[0] ## # Q16 calculate Eout def calculate_Eout(x, y, path): test_x, test_y = read_input_data(path) error_count = 0 k = 1 for i in range(test_x.shape[0]): ret = KNN (k, x, y, test_x[i]) if test_y[i]!=ret: error_count += 1 return 1.0*error_count/test_x.shape[0] def main(): x,y = read_input_data(\"knn_train.dat\") print calculate_Ein(x,y) print calculate_Eout(x,y, \"knn_test.dat\") if __name__ == '__main__': main()\nQ19~Q20是Kmeans算法相关的，运行代码也很快可以得出结果，不记录答案了：\nKmeans的算法实现思路非常清晰：\n1）实现初始化随机选各类中心点的功能（题目中是随机选原始数据的点，如果是其他的选点方法，单独拎出来一个模块，不影响其他模块）\n2）实现每次更新各个数据点类别的功能（def update_category(x, K, centers)）\n3）固定各个点的类别，更新各个类别的center点坐标（def update_centers(x, y, K)）\n模块实现上，得益于numpy的矩阵计算操作函数。（应该掌握一套自己的矩阵计算操作代码，这样可以随时拿起来二次开发）\n代码如下：\n#encoding=utf8 import sys import numpy as np import math from random import * ## # read data from local file # return with numpy array def read_input_data(path): x = [] for line in open(path).readlines(): if line.strip()=='': continue items = line.strip().split(' ') tmp_x = [] for i in range(0,len(items)): tmp_x.append(float(items[i])) x.append(tmp_x) return np.array(x) ## # input all data and category K # return K category centers def Kmeans(x, K): T = 50 E_total = 0 for t in range(T): centers = init_centers(x, K) y = np.zeros(x.shape[0]) R = 50 for r in range(R): y = update_category(x, K, centers) centers = update_centers(x, y, K) E = calculate_Ein(x, y, centers) print E E_total += E return E_total*1.0/T def init_centers(x, K): ret = [] order = range(x.shape[0]) np.random.shuffle(order) for i in range(K): ret.append(x[order[i]]) return np.array(ret) def update_category(x, K, centers): y = [] for i in range(x.shape[0]): category = -1 distance = float(\"inf\") for k in range(K): d = np.sum((x[i] - centers[k])*(x[i] - centers[k]),axis=0) if d < distance: distance = d category = k y.append(category) return np.array(y) def update_centers(x, y, K): centers = [] for k in range(K): # print \"np.sum(x[np.where(y==k)],axis=0)\" # print np.sum(x[np.where(y==k)],axis=0).shape center = np.sum(x[np.where(y==k)],axis=0)*1.0/np.array(np.where(y==k)).shape[1] centers.append(center) return np.array(centers) def calculate_Ein(x, y, centers): # print centers[0].shape error_total = 0 for i in range(x.shape[0]): error_total += np.sum((x[i]-centers[y[i]])*(x[i]-centers[y[i]]),axis=0) return 1.0*error_total/x.shape[0] def main(): x = read_input_data(\"kmeans_train.dat\") # print x.shape print Kmeans(x,2) if __name__ == '__main__': main()\n==========================================================================\n完成了这次作业后，终于跟完了《机器学习基石+机器学习技法》32次课，8次coding作业。\n个人上完这门课后，主要有三点收获：\n1）通过coding的作业题目，实现了一些主流机器学习算法（Perceptron、AdaBoost-stump、Linear Regression、Logistic Regression、Decision Tree、Neural Network、KNN、Kmeans）；以前都是用算法包，对各个算法的理解不如实现过一遍来得深和细。\n2）以前对各个算法的理解就是会用（其实也不能说太会用），上完课程后，对每个模型的Motivation有了一定的掌握：模型为什么要这么设计？Regularizer为什么要这么设计？模型的利弊有哪些？以及模型的一些比较直观的数学原理推导。\n3）以前看待各个机器学习算法，都是孤立的看待每个算法（这个算法是解决啥的，那个算法是解决啥的），没有成体系地把各个算法拎起来。台大这门课在整个授课环节中，都贯穿了非常强的体系的观念，这里举两个例子：\na. Linear Network与Factorization有啥联系（15讲）\nb. Decision Tree与AdaBoost有啥关系（8、9讲）\nc. Linear Regression与Neural Network有啥关系（12讲）\n在看这门课之前，是绝对不会把上面的每组中两个模型联系起来看待的；但这门课确实给了比较深的motivation，非常强的全局主线。\n最后，谈一点个人上公开课的体会：\n1）只听一遍：走马观花，学到的东西微乎其微\n2）听课，写作业：实践者的态度去学，学到的东西比只听课要多了去了\n3）听课，写作业，写听课blog：实践者+研究者的态度去学；“最好的学就是教”，在写blog的过程中，会强迫自己把当时很多不清晰的point都搞清楚，要不然真的写不出来\n4）循环进行3）：温故知新的道理大家都懂，就看有没有时间吧\nSign 就写到这了....."}
{"content2":"import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D from sklearn import datasets, linear_model from sklearn.model_selection import train_test_split def load_data(): # 使用 scikit-learn 自带的 iris 数据集 iris=datasets.load_iris() X_train=iris.data y_train=iris.target return train_test_split(X_train, y_train,test_size=0.25,random_state=0,stratify=y_train) #逻辑回归 def test_LogisticRegression(*data): X_train,X_test,y_train,y_test=data regr = linear_model.LogisticRegression() regr.fit(X_train, y_train) print('Coefficients:%s, intercept %s'%(regr.coef_,regr.intercept_)) print('Score: %.2f' % regr.score(X_test, y_test)) # 加载用于分类的数据集 X_train,X_test,y_train,y_test=load_data() # 调用 test_LogisticRegression test_LogisticRegression(X_train,X_test,y_train,y_test) def test_LogisticRegression_multinomial(*data): ''' 测试 LogisticRegression 的预测性能随 multi_class 参数的影响 ''' X_train,X_test,y_train,y_test=data regr = linear_model.LogisticRegression(multi_class='multinomial',solver='lbfgs') regr.fit(X_train, y_train) print('Coefficients:%s, intercept %s'%(regr.coef_,regr.intercept_)) print('Score: %.2f' % regr.score(X_test, y_test)) # 调用 test_LogisticRegression_multinomial test_LogisticRegression_multinomial(X_train,X_test,y_train,y_test) def test_LogisticRegression_C(*data): ''' 测试 LogisticRegression 的预测性能随 C 参数的影响 ''' X_train,X_test,y_train,y_test=data Cs=np.logspace(-2,4,num=100) scores=[] for C in Cs: regr = linear_model.LogisticRegression(C=C) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 fig=plt.figure() ax=fig.add_subplot(1,1,1) ax.plot(Cs,scores) ax.set_xlabel(r\"C\") ax.set_ylabel(r\"score\") ax.set_xscale('log') ax.set_title(\"LogisticRegression\") plt.show() # 调用 test_LogisticRegression_C test_LogisticRegression_C(X_train,X_test,y_train,y_test)"}
{"content2":"人工智能（计算机科学的一个分支）\n人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，但没有一个统一的定义。\n人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。但是这种会自我思考的高级人工智能还需要科学理论和工程上的突破。\n人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。"}
{"content2":"强化学习概况\n正如在前面所提到的，强化学习是指一种计算机以“试错”的方式进行学习，通过与环境进行交互获得的奖赏指导行为，目标是使程序获得最大的奖赏，强化学习不同于连督学习，区别主要表现在强化信号上，强化学习中由环境提供的强化信号是对产生动作的好坏作一种评价(通常为标量信号)，而不是告诉强化学习系统如何去产生正确的动作。唯一的目的是最大化效率和/或性能。算法对正确的决策给予奖励，对错误的决策给予惩罚，如下图所示：\n持续的训练是为了不断提高效率。这里的重点是性能，这意味着我们需要，在看不见的数据和算法已经学过的东西，之间找到一种平衡。该算法将一个操作应用到它的环境中，根据它所做的行为接受奖励或惩罚，不断的重复这个过程，等等。\n接下来让我们看一个程序，概念是相似的，尽管它的规模和复杂性很低。想象一下，是什么让自动驾驶的车辆从一个地点移动到了另一个点。\n让我们看看我们的应用程序：\n在这里，可以看到我们有一个非常基本的地图，一个没有障碍，但有外部限制的墙。黑色块(start)是我们的对象，红色块(stop)是我们的目标。在这个应用程序中，我们的目标是让我们的对象在墙壁以内到达目标位置。如果我们的下一步把我们的对象放在一个白色的方块上，我们的算法将得到奖励。如果我们的下一步行动超出墙壁的围地范围，我们将受到惩罚。在这个例子中，它的路径上绝对没有障碍，所以我们的对象应该能够到达它的目的地。问题是:它能多快学会?\n下面是另一个比较复杂的地图示例：\n学习类型\n在应用程序的右边是我们的设置，如下面的屏幕截图所示。我们首先看到的是学习算法。在这个应用中，我们将处理两种不同的学习算法，Q-learning和state-action-reward-state-action (SARSA)。让我们简要讨论一下这两种算法。\nQ-learning\nQ-learning可以在没有完全定义的环境模型的情况下，识别给定状态下的最优行为(在每个状态中值最高的行为)。它还擅长处理随机转换和奖励的问题，而不需要调整或适应。\n以下是Q-learning的数学表达式：\n如果我们提供一个非常高级的抽象示例，可能更容易理解。\n程序从状态1开始。然后它执行动作1并获得奖励1。接下来，它四处寻找状态2中某个行为的最大可能奖励是多少;然后使用它来更新动作1的值等等。\nSARSA\nSARSA的工作原理是这样的:\n1. 程序从状态1开始。\n2. 然后它执行动作1并获得奖励1。\n3. 接下来，它进入状态2，执行动作2，并获得奖励2。\n4. 然后，程序返回并更新动作1的值。\n这Q-learning算法的不同之处在于找到未来奖励的方式。\nQ-learning使用状态2中奖励最高的动作的值，而SARSA使用实际动作的值。\n这是SARSA的数学表达式：\n运行我们的应用程序\n现在，让我们开始使用带有默认参数的应用程序。只需点击开始按钮，学习就开始了。完成后，您将能够单击Show Solution按钮，学习路径将从头到尾播放。\n点击Start开始学习阶段，一直到黑色物体达到目标:\n对于每个迭代，将评估不同的对象位置，以及它们的操作和奖励。一旦学习完成，我们可以单击Show Solution按钮来重播最终的解决方案。完成后，黑色对象将位于红色对象之上:\n现在让我们看看应用程序中的代码。有两种我们之前强调过的学习方法。\nQ-learning是这样的：\n/// <summary> /// Q-Learning 线程 /// </summary> private void QLearningThread() { //迭代次数 int iteration = 0; TabuSearchExploration tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy; EpsilonGreedyExploration explorationPolicy = (EpsilonGreedyExploration)tabuPolicy.BasePolicy; while ((!needToStop)&&(iteration<learningIterations)) { explorationPolicy.Epsilon = explorationRate - ((double)iteration / learningIterations) * explorationRate; qLearning.LearningRate = learningRate - ((double)iteration / learningIterations) * learningRate; tabuPolicy.ResetTabuList(); var agentCurrentX = agentStartX; var agentCurrentY = agentStartY; int steps = 0; while ((!needToStop)&& ((agentCurrentX != agentStopX) || (agentCurrentY != agentStopY))) { steps++; int currentState= GetStateNumber(agentCurrentX, agentCurrentY); int action = qLearning.GetAction(currentState); double reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action); int nextState = GetStateNumber(agentCurrentX, agentCurrentY); // 更新对象的qLearning以设置禁忌行为 qLearning.UpdateState(currentState, action, reward, nextState); tabuPolicy.SetTabuAction((action + 2) % 4, 1); } System.Diagnostics.Debug.WriteLine(steps); iteration++; SetText(iterationBox, iteration.ToString()); } EnableControls(true); }\nSARSA学习有何不同?让我们来看看SARSA学习的while循环，并理解它:\n/// <summary> /// Sarsa 学习线程 /// </summary> private void SarsaThread() { int iteration = 0; TabuSearchExploration tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy; EpsilonGreedyExploration explorationPolicy = (EpsilonGreedyExploration)tabuPolicy.BasePolicy; while ((!needToStop) && (iteration < learningIterations)) { explorationPolicy.Epsilon = explorationRate - ((double)iteration / learningIterations) * explorationRate; sarsa.LearningRate = learningRate - ((double)iteration / learningIterations) * learningRate; tabuPolicy.ResetTabuList(); var agentCurrentX = agentStartX; var agentCurrentY = agentStartY; int steps = 1; int previousState = GetStateNumber(agentCurrentX, agentCurrentY); int previousAction = sarsa.GetAction(previousState); double reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, previousAction); while ((!needToStop) && ((agentCurrentX != agentStopX) || (agentCurrentY != agentStopY))) { steps++; tabuPolicy.SetTabuAction((previousAction + 2) % 4, 1); int nextState = GetStateNumber(agentCurrentX, agentCurrentY); int nextAction = sarsa.GetAction(nextState); sarsa.UpdateState(previousState, previousAction, reward, nextState, nextAction); reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, nextAction); previousState = nextState; previousAction = nextAction; } if (!needToStop) { sarsa.UpdateState(previousState, previousAction, reward); } System.Diagnostics.Debug.WriteLine(steps); iteration++; SetText(iterationBox, iteration.ToString()); } // 启用设置控件 EnableControls(true); }\n最后一步，看看如何使解决方案具有动画效果。我们需要这样才能看到我们的算法是否实现了它的目标。\n代码如下:\nTabuSearchExploration tabuPolicy; if (qLearning != null) tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy; else if (sarsa != null) tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy; else throw new Exception(); var explorationPolicy = (EpsilonGreedyExploration)tabuPolicy?.BasePolicy; explorationPolicy.Epsilon = 0; tabuPolicy?.ResetTabuList(); int agentCurrentX = agentStartX, agentCurrentY = agentStartY; Array.Copy(map, mapToDisplay, mapWidth * mapHeight); mapToDisplay[agentStartY, agentStartX] = 2; mapToDisplay[agentStopY, agentStopX] = 3;\n这是我们的while循环，所有神奇的事情都发生在这里!\nwhile (!needToStop) { cellWorld.Map = mapToDisplay; Thread.Sleep(200); if ((agentCurrentX == agentStopX) && (agentCurrentY == agentStopY)) { mapToDisplay[agentStartY, agentStartX] = 2; mapToDisplay[agentStopY, agentStopX] = 3; agentCurrentX = agentStartX; agentCurrentY = agentStartY; cellWorld.Map = mapToDisplay; Thread.Sleep(200); } mapToDisplay[agentCurrentY, agentCurrentX] = 0; int currentState = GetStateNumber(agentCurrentX, agentCurrentY); int action = qLearning?.GetAction(currentState) ?? sarsa.GetAction(currentState); UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action); mapToDisplay[agentCurrentY, agentCurrentX] = 2; }\n让我们把它分成更容易消化的部分。我们要做的第一件事就是建立禁忌政策。如果您不熟悉tabu搜索，请注意，它的目的是通过放松其规则来提高本地搜索的性能。在每一步中，如果没有其他选择(有回报的行动)，有时恶化行动是可以接受的。\n此外，还设置了prohibition (tabu)，以确保算法不会返回到以前访问的解决方案。\nTabuSearchExploration tabuPolicy; if (qLearning != null) tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy; else if (sarsa != null) tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy; else throw new Exception(); var explorationPolicy = (EpsilonGreedyExploration)tabuPolicy?.BasePolicy; explorationPolicy.Epsilon = 0; tabuPolicy?.ResetTabuList();\n接下来，我们要定位我们的对象并准备地图。\nint agentCurrentX = agentStartX, agentCurrentY = agentStartY; Array.Copy(map, mapToDisplay, mapWidth * mapHeight); mapToDisplay[agentStartY, agentStartX] = 2; mapToDisplay[agentStopY, agentStopX] = 3;\n下面是我们的主执行循环，它将以动画的方式显示解决方案:\nwhile (!needToStop) { cellWorld.Map = mapToDisplay; Thread.Sleep(200); if ((agentCurrentX == agentStopX) && (agentCurrentY == agentStopY)) { mapToDisplay[agentStartY, agentStartX] = 2; mapToDisplay[agentStopY, agentStopX] = 3; agentCurrentX = agentStartX; agentCurrentY = agentStartY; cellWorld.Map = mapToDisplay; Thread.Sleep(200); } mapToDisplay[agentCurrentY, agentCurrentX] = 0; int currentState = GetStateNumber(agentCurrentX, agentCurrentY); int action = qLearning?.GetAction(currentState) ?? sarsa.GetAction(currentState); UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action); mapToDisplay[agentCurrentY, agentCurrentX] = 2; }\n汉诺塔游戏\n河内塔由三根杆子和最左边的几个按顺序大小排列的圆盘组成。目标是用最少的移动次数将所有磁盘从最左边的棒子移动到最右边的棒子。\n你必须遵守的两条重要规则是，一次只能移动一个磁盘，不能把大磁盘放在小磁盘上;也就是说，在任何棒中，磁盘的顺序必须始终是从底部最大的磁盘到顶部最小的磁盘，如下所示：\n假设我们使用三个磁盘，如图所示。在这种情况下，有33种可能的状态，如下图所示:\n河内塔谜题中所有可能状态的总数是3的磁盘数次幂。\n其中||S||是集合状态中的元素个数，n是磁盘的个数。\n在我们的例子中，我们有3×3×3 = 27个圆盘在这三根棒上分布的唯一可能状态，包括空棒;但是两个空棒可以处于最大状态。\n定义了状态总数之后，下面是我们的算法从一种状态移动到另一种状态的所有可能操作：\n这个谜题的最小可能步数是:\n磁盘的数量是n。\nQ-learning算法的正式定义如下:\n在这个Q-learning算法中，我们使用了以下变量:\nQ矩阵:一个二维数组，首先对所有元素填充一个固定值(通常为0)，用于保存所有状态下的计算策略;也就是说，对于每一个状态，它持有对各自可能的行动的奖励。\n折扣因子:决定了对象如何处理奖励的政策。当贴现率接近0时，只考虑当前的报酬会使对象变得贪婪，而当贴现率接近1时，会使对象变得更具策略性和前瞻性，从而在长期内获得更好的报酬。\nR矩阵:包含初始奖励的二维数组，允许程序确定特定状态的可能操作列表。\n我们应该简要介绍一下Q-learning Class的一些方法：\nInit:生成所有可能的状态以及开始学习过程。\nLearn:在学习过程中有顺序的步骤\nInitRMatrix: 这个初始化奖励矩阵的值如下:\n1. 0:在这种状态下，我们没有奖励。\n2. 100:这是我们在最终状态下的最大奖励，我们想去的地方。\n3. X:在这种情况下是不可能采取这种行动的。\nTrainQMatrix: 包含Q矩阵的实际迭代值更新规则。完成后，我们希望得到一个训练有素的对象。\nNormalizeQMatrix: 使Q矩阵的值标准化，使它们成为百分数。\nTest: 提供来自用户的文本输入，并显示解决此难题的最佳最短路径。\n让我们更深入地研究我们的TrainQMatrix的代码:\n/// <summary> /// 训练Q矩阵 /// </summary> /// <param name=\"_StatesMaxCount\">所有可能移动的个数</param> private void TrainQMatrix(int _StatesMaxCount) { pickedActions = new Dictionary<int, int>(); // 可用操作列表(基于R矩阵，其中包含从某个状态开始的允许的下一个操作，在数组中为0) List<int> nextActions = new List<int>(); int counter = 0; int rIndex = 0; // 3乘以所有可能移动的个数就有足够的集来训练Q矩阵 while (counter < 3 * _StatesMaxCount) { var init = Utility.GetRandomNumber(0, _StatesMaxCount); do { // 获得可用的动作 nextActions = GetNextActions(_StatesMaxCount, init); // 从可用动作中随机选择一个动作 if (nextActions != null) { var nextStep = Utility.GetRandomNumber(0, nextActions.Count); nextStep = nextActions[nextStep]; // 获得可用的动作 nextActions = GetNextActions(_StatesMaxCount, nextStep); // 设置从该状态采取的动作的索引 for (int i = 0; i < 3; i++) { if (R != null && R[init, i, 1] == nextStep) rIndex = i; } // 这是值迭代更新规则-折现系数是0.8 Q[init, nextStep] = R[init, rIndex, 0] + 0.8 * Utility.GetMax(Q, nextStep, nextActions); // 将下一步设置为当前步骤 init = nextStep; } } while (init != FinalStateIndex); counter++; } }\n使用三个磁盘运行应用程序:\n使用四个磁盘运行应用程序:\n这里有7个磁盘。最佳移动步数是127，所以你可以看到解决方案可以多快地乘以可能的组合:\n总结\n这种形式的强化学习更正式地称为马尔可夫决策过程(Markov Decision Process, MDP)。MDP是一个离散时间随机控制的过程,这意味着在每个时间步,在状态x下,决策者可以选择任何可用的行动状态,这个过程将在下一步反应，随机移动到一个新的状态,给决策者一个奖励。进程进入新状态的概率由所选动作决定。因此，下一个状态取决于当前状态和决策者的行为。给定状态和操作，下一步完全独立于之前的所有状态和操作。"}
{"content2":"原文地址：http://blog.sina.com.cn/s/blog_7e5f32ff0102vlgj.html\n入门书单\n1.《数学之美》PDF6\n作者吴军大家都很熟悉。以极为通俗的语言讲述了数学在机器学习和自然语言处理等领域的应用。\n2.《Programming Collective Intelligence》（《集体智慧编程》）PDF3\n作者Toby Segaran也是《BeautifulData : The Stories Behind Elegant Data Solutions》（《数据之美：解密优雅数据解决方案背后的故事》）的作者。这本书最大的优势就是里面没有理论推导和复杂的数学公式，是很不错的入门书。目前中文版已经脱销，对于有志于这个领域的人来说，英文的pdf是个不错的选择，因为后面有很多经典书的翻译都较差，只能看英文版，不如从这个入手。还有，这本书适合于快速看完，因为据评论，看完一些经典的带有数学推导的书后会发现这本书什么都没讲，只是举了很多例子而已。\n3.《Algorithms of the Intelligent Web》（《智能web算法》）PDF1\n作者Haralambos Marmanis、Dmitry Babenko。这本书中的公式比《集体智慧编程》要略多一点，里面的例子多是互联网上的应用，看名字就知道。不足的地方在于里面的配套代码是BeanShell而不是python或其他。总起来说，这本书还是适合初学者，与上一本一样需要快速读完，如果读完上一本的话，这一本可以不必细看代码，了解算法主要思想就行了。\n4.《统计学习方法》 PDF模糊\n作者李航，是国内机器学习领域的几个大家之一，曾在MSRA任高级研究员，现在华为诺亚方舟实验室。书中写了十个算法，每个算法的介绍都很干脆，直接上公式，是彻头彻尾的“干货书”。每章末尾的参考文献也方便了想深入理解算法的童鞋直接查到经典论文；本书可以与上面两本书互为辅助阅读。\n5.《Machine Learning》（《机器学习》） PDF7\n作者Tom Mitchell是CMU的大师，有机器学习和半监督学习的网络课程视频。这本书是领域内翻译的较好的书籍，讲述的算法也比《统计学习方法》的范围要大很多。据评论这本书主要在于启发，讲述公式为什么成立而不是推导；不足的地方在于出版年限较早，时效性不如PRML。但有些基础的经典还是不会过时的，所以这本书现在几乎是机器学习的必读书目。\n6.《Mining of Massive Datasets》（《大数据》） PDF19\n作者Anand Rajaraman[3]、Jeffrey David Ullman，Anand是Stanford的PhD。这本书介绍了很多算法，也介绍了这些算法在数据规模比较大的时候的变形。但是限于篇幅，每种算法都没有展开讲的感觉，如果想深入了解需要查其他的资料，不过这样的话对算法进行了解也足够了。还有一点不足的地方就是本书原文和翻译都有许多错误，勘误表比较长，读者要用心了。\n7.《Data Mining: Practical Machine Learning Tools and Techniques》（《数据挖掘：实用机器学习技术》） PDF16\n作者Ian H. Witten 、Eibe Frank是weka的作者、新西兰怀卡托大学教授。他们的《ManagingGigabytes》[4]也是信息检索方面的经典书籍。这本书最大的特点是对weka的使用进行了介绍，但是其理论部分太单薄，作为入门书籍还可，但是，经典的入门书籍如《集体智慧编程》、《智能web算法》已经很经典，学习的话不宜读太多的入门书籍，建议只看一些上述两本书没讲到的算法。\n8.《机器学习及其应用》\n周志华、杨强主编。来源于“机器学习及其应用研讨会”的文集。该研讨会由复旦大学智能信息处理实验室发起，目前已举办了十届，国内的大牛如李航、项亮、王海峰、刘铁岩、余凯等都曾在该会议上做过讲座。这本书讲了很多机器学习前沿的具体的应用，需要有基础的才能看懂。如果想了解机器学习研究趋势的可以浏览一下这本书。关注领域内的学术会议是发现研究趋势的方法嘛。\n9.《Managing Gigabytes》（深入搜索引擎）PDF8\n信息检索不错的书。\n10.《Modern Information Retrieval》 PDF6\nRicardo Baeza-Yates et al. 1999。貌似第一本完整讲述IR的书。可惜IR这些年进展迅猛，这本书略有些过时了。翻翻做参考还是不错的。另外，Ricardo同学现在是Yahoo Research for Europe and Latin Ameria的头头。\n11.《推荐系统实践》 PDF13\n项亮，不错的入门读物\n深入\n1.《Pattern Classification》（《模式分类》第二版） PDF14\n作者Richard O. Duda[5]、Peter E. Hart、David。模式识别的奠基之作，但对最近呈主导地位的较好的方法SVM、Boosting方法没有介绍，被评“挂一漏万之嫌”。\n2.《Pattern Recognition And Machine Learning》 PDF74\n作者Christopher M. Bishop[6]；简称PRML，侧重于概率模型，是贝叶斯方法的扛鼎之作，据评“具有强烈的工程气息，可以配合stanford 大学 Andrew Ng 教授的 Machine Learning 视频教程一起来学，效果翻倍。”\n3.《The Elements of Statistical Learning : Data Mining, Inference, andPrediction》，（《统计学习基础：数据挖掘、推理与预测》第二版） PDF8\n作者RobertTibshirani、Trevor Hastie、Jerome Friedman。“这本书的作者是Boosting方法最活跃的几个研究人员，发明的Gradient Boosting提出了理解Boosting方法的新角度，极大扩展了Boosting方法的应用范围。这本书对当前最为流行的方法有比较全面深入的介绍，对工程人员参考价值也许要更大一点。另一方面，它不仅总结了已经成熟了的一些技术，而且对尚在发展中的一些议题也有简明扼要的论述。让读者充分体会到机器学习是一个仍然非常活跃的研究领域，应该会让学术研究人员也有常读常新的感受。”[7]\n4.《Data Mining：Concepts andTechniques》（《数据挖掘：概念与技术》第三版） PDF3\n作者（美）Jiawei Han[8]、（加）Micheline Kamber、（加）Jian Pei，其中第一作者是华裔。本书毫无疑问是数据挖掘方面的的经典之作，不过翻译版总是被喷，没办法，大部分翻译过来的书籍都被喷，想要不吃别人嚼过的东西，就好好学习英文吧。\n5.《AI, Modern Approach 2nd》 PDF8\nPeter Norvig，无争议的领域经典。\n6.《Foundations of Statistical Natural Language Processing》 PDF7\n自然语言处理领域公认经典。\n7.《Information Theory：Inference and Learning Algorithms》 PDF5\n8.《Statistical Learning Theory》 PDF7\nVapnik的大作，统计学界的权威，本书将理论上升到了哲学层面，他的另一本书《The Nature ofStatistical Learning Theory》也是统计学习研究不可多得的好书，但是这两本书都比较深入，适合有一定基础的读者。\n数学基础\n1.《矩阵分析》 PDF22\nRoger Horn。矩阵分析领域无争议的经典\n2.《概率论及其应用》 PDF3\n威廉·费勒。极牛的书，可数学味道太重，不适合做机器学习的\n3.《All Of Statistics》 PDF高清版18\n机器学习这个方向，统计学也一样非常重要。推荐All of statistics，这是CMU的一本很简洁的教科书，注重概念，简化计算，简化与Machine Learning无关的概念和统计内容，可以说是很好的快速入门材料。\n4.《Nonlinear Programming, 2nd》 PDF5\n最优化方法，非线性规划的参考书。\n5.《Convex Optimization》 PDF9 配套代码7\nBoyd的经典书籍，被引用次数超过14000次，面向实际应用，并且有配套代码，是一本不可多得的好书。\n6.《Numerical Optimization》 PDF6\n第二版，Nocedal著，非常适合非数值专业的学生和工程师参考，算法流程清晰详细，原理清楚。\n7.《Introduction to Mathematical Statistics》 PDF5\n第六版，Hogg著，本书介绍了概率统计的基本概念以及各种分布，以及ML，Bayesian方法等内容。\n8.《An Introduction to Probabilistic Graphical Models》 PDF20\nJordan著，本书介绍了条件独立、分解、混合、条件混合等图模型中的基本概念，对隐变量（潜在变量）也做了详细介绍，相信大家在隐马尔科夫链和用Gaussian混合模型来实现EM算法时遇到过这个概念。\n9.《Probabilistic Graphical Models-Principles and Techniques》 PDF8\nKoller著，一本很厚很全面的书，理论性很强，可以作为参考书使用。\n具体数学 PDF5\n经典\n大家的补充\n1.线性代数 (Linear Algebra)：\n我想国内的大学生都会学过这门课程，但是，未必每一位老师都能贯彻它的精要。这门学科对于Learning是必备的基础，对它的透彻掌握是必不可少的。我在科大一年级的时候就学习了这门课，后来到了香港后，又重新把线性代数读了一遍，所读的是\nIntroduction to Linear Algebra (3rd Ed.) by Gilbert Strang.\n这本书是MIT的线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。\n而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，课程的video在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。\nhttp://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm8\n2.概率和统计 (Probability and Statistics):\n概率论和统计的入门教科书很多，我目前也没有特别的推荐。我在这里想介绍的是一本关于多元统计的基础教科书：\nApplied Multivariate Statistical Analysis (5th Ed.) by Richard A. Johnson and Dean W. Wichern\n这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。\n之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是\nIntroduction to Graphical Models (draft version). by M. Jordan and C. Bishop.\n我不知道这本书是不是已经出版了（不要和Learning in Graphical Models混淆，那是个论文集，不适合初学）。这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。MIT内部可以access，至于外面，好像也是有电子版的。\n3.分析 (Analysis)：\n我想大家基本都在大学就学过微积分或者数学分析，深度和广度则随各个学校而异了。这个领域是很多学科的基础，值得推荐的教科书莫过于\nPrinciples of Mathematical Analysis, by Walter Rudin\n有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。\n在分析这个方向，接下来就是泛函分析(Functional Analysis)。\nIntroductory Functional Analysis with Applications, by Erwin Kreyszig.\n适合作为泛函的基础教材，容易切入而不失全面。我特别喜欢它对于谱论和算子理论的特别关注，这对于做learning的研究是特别重要的。Rudin也有一本关于functional analysis的书，那本书在数学上可能更为深刻，但是不易于上手，所讲内容和learning的切合度不如此书。\n在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。\n4.拓扑 (Topology)：\n在我读过的基本拓扑书各有特色，但是综合而言，我最推崇：\nTopology (2nd Ed.) by James Munkres\n这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。\n5.流形理论 (Manifold theory)：\n对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是\nIntroduction to Smooth Manifolds. by John M. Lee\n虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space, bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。\n虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：\nLie Groups, Lie Algebras, and Representations: An Elementary Introduction. by Brian C. Hall\n此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。\n机器学习经典书籍&论文\n转自水木\n除了以下推荐的书以外，出版在Foundations and Trends in Machine Learning上面的survey文章都值得一看。\n入门：\nPattern Recognition And Machine Learning\nChristopher M. Bishop\nMachine Learning : A Probabilistic Perspective\nKevin P. Murphy\nThe Elements of Statistical Learning : Data Mining, Inference, and Predictio\nn\nTrevor Hastie, Robert Tibshirani, Jerome Friedman\nInformation Theory, Inference and Learning Algorithms\nDavid J. C. MacKay\nAll of Statistics : A Concise Course in Statistical Inference\nLarry Wasserman\n优化：\nConvex Optimization\nStephen Boyd, Lieven Vandenberghe\nNumerical Optimization\nJorge Nocedal, Stephen Wright\nOptimization for Machine Learning\nSuvrit Sra, Sebastian Nowozin, Stephen J. Wright\n核方法：\nKernel Methods for Pattern Analysis\nJohn Shawe-Taylor, Nello Cristianini\nLearning with Kernels : Support Vector Machines, Regularization, Optimizatio\nn, and Beyond\nBernhard Schlkopf, Alexander J. Smola\n半监督：\nSemi-Supervised Learning\nOlivier Chapelle\n高斯过程：\nGaussian Processes for Machine Learning (Adaptive Computation and Machine Le\narning)\nCarl Edward Rasmussen, Christopher K. I. Williams\n概率图模型：\nGraphical Models, Exponential Families, and Variational Inference\nMartin J Wainwright, Michael I Jordan\nBoosting:\nBoosting : Foundations and Algorithms\nSchapire, Robert E.; Freund, Yoav\n贝叶斯:\nStatistical Decision Theory and Bayesian Analysis\nJames O. Berger\nThe Bayesian Choice : From Decision-Theoretic Foundations to Computational I\nmplementation\nChristian P. Robert\nBayesian Nonparametrics\nNils Lid Hjort, Chris Holmes, Peter Müller, Stephen G. Walker\nPrinciples of Uncertainty\nJoseph B. Kadane\nDecision Theory : Principles and Approaches\nGiovanni Parmigiani, Lurdes Inoue\n蒙特卡洛：\nMonte Carlo Strategies in Scientific Computing\nJun S. Liu\nMonte Carlo Statistical Methods\nChristian P.Robert, George Casella\n信息几何：\nMethods of Information Geometry\nShun-Ichi Amari, Hiroshi Nagaoka\nAlgebraic Geometry and Statistical Learning Theory\nWatanabe, Sumio\nDifferential Geometry and Statistics\nM.K. Murray, J.W. Rice\n渐进收敛：\nAsymptotic Statistics\nA. W. van der Vaart\nEmpirical Processes in M-estimation\nGeer, Sara A. van de\n不推荐：\nStatistical Learning Theory\nVladimir N. Vapnik\nBayesian Data Analysis, Second Edition\nAndrew Gelman, John B. Carlin, Hal S. Stern, Donald B. Rubin\nProbabilistic Graphical Models : Principles and Techniques\nDaphne Koller, Nir Friedman\n机器学习经典论文/survey合集\nActive Learning\nTwo Faces of Active Learning50, Dasgupta, 2011\nActive Learning Literature Survey8, Settles, 2010\nApplications\nA Survey of Emerging Approaches to Spam Filtering9, Caruana, 2012\nAmbient Intelligence: A Survey3, Sadri, 2011\nA Survey of Online Failure Prediction Methods2, Salfner, 2010\nAnomaly Detection: A Survey3, Chandola, 2009\nMining Data Streams: A Review4, Gaber, 2005\nWorkflow Mining: A Survey of Issues and Approaches2, Aalst, 2003\nBiology\nSupport Vector Machines in Bioinformatics: a Survey12, Chicco, 2012\nComputational Epigenetics: The New Scientific Paradigm 3, Lim, 2010\nAutomated Protein Structure Classification: A Survey4, Hassanzadeh, 2009\nChemoinformatics - An Introduction for Computer Scientists3, Brown, 2009\nComputational Challenges in Systems Biology2, Heath, 2009\nComputational Epigenetics 3, Bock, 2008\nProgress and Challenges in Protein Structure Prediction3, Zhang, 2008\nA Review of Feature Selection in Bioinformatics4, Saeys, 2007\nMachine Learning in Bioinformatics: A Brief Survey and Recommendations for Practitioners6, Bhaskar, 2006\nBioinformatics - An Introduction for Computer Scientists1, Cohen, 2004\nComputational Systems Biology2, Kitano, 2002\nProtein Structure Prediction and Structural Genomics2, Baker, 2001\nRecent Developments and Future Directions in Computational Genomics1, Tsoka, 2000\nMolecular Biology for Computer Scientists1, Hunter, 1993\nClassification\nSupervised Machine Learning: A Review of Classification Techniques22, Kotsiantis, 2007\nClustering\nXML Data Clustering: An Overview4, Algergawy, 2011\nData Clustering: 50 Years Beyond K-Means6, Jain, 2010\nClustering Stability: An Overview5, Luxburg, 2010\nParallel Clustering Algorithms: A Survey4, Kim, 2009\nA Survey: Clustering Ensembles Techniques2, Ghaemi, 2009\nA Tutorial on Spectral Clustering4, Luxburg, 2007\nSurvey of Clustering Data Mining Techniques4, Berkhin, 2006\nSurvey of Clustering Algorithms4, Xu, 2005\nClustering of Time Series Data - A Survey3, Liao, 2005\nClustering Methods4, Rokach, 2005\nRecent Advances in Clustering: A Brief Survey2, Kotsiantis, 2004\nSubspace Clustering for High Dimensional Data: A Review2, Parsons, 2004\nUnsupervised and Semi-supervised Clustering: a Brief Survey3, Grira, 2004\nClustering in Life Sciences3, Zhao, 2002\nOn Clustering Validation Techniques2, Halkidi, 2001\nData Clustering: A Review3, Jain, 1999\nA Survey of Fuzzy Clustering4, Yang, 1993\nComputer Vision\nPedestrian Detection: An Evaluation of the State of the Art7, Dollar, 2012\nA Comparative Study of Palmprint Recognition Algorithms3, Zhang, 2012\nHuman Activity Analysis: A Review2, Aggarwal, 2011\nSubspace Methods for Face Recognition2, Rao, 2010\nContext Based Object Categorization: A Critical Survey2, Galleguillos, 2010\nObject tracking: A Survey3, Yilmaz, 2006\nDetecting Faces in Images: A Survey2, Yang, 2002\nDatabases\nData Fusion3, Bleiholder, 2008\nDuplicate Record Detection: A Survey2, Elmagarmid, 2007\nOverview of Record Linkage and Current Research Directions2, Winkler, 2006\nA Survey of Schema-based Matching Approaches3, Shvaiko, 2005\nDeep Learning\nRepresentation Learning: A Review and New Perspectives17, Bengio, 2012\nDimension Reduction\nDimensionality Reduction: A Comparative Review6, Maaten, 2009\nDimension Reduction: A Guided Tour4, Burges, 2009\nA Survey of Manifold-Based Learning Methods2, Huo, 2007\nToward Integrating Feature Selection Algorithms for Classification and Clustering3, Liu, 2005\nAn Introduction to Variable and Feature Selection3, Guyon, 2003\nA Survey of Dimension Reduction Techniques2, Fodor, 2002\nEconomics\nAuctions and Bidding: A Guide for Computer Scientists1, Parsons, 2011\nComputational Sustainability1, Gomes, 2009\nComputational Finance1, Tsang, 2004\nGame Theory\nComputer Poker: A Review4, Rubin, 2011\nGraphical Models\nAn Introduction to Variational Methods for Graphical Models5, Jordan, 1999\nKernel Methods\nKernels for Vector-Valued Functions: a Review4, Alvarez, 2012\nLearning Theory\nIntroduction to Statistical Learning Theory7, Bousquet, 2004\nMachine Learning\nA Few Useful Things to Know about Machine Learning7, Domingos, 2012\nA Tutorial on Bayesian Nonparametric Models4, Blei, 2011\nDecision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning2, Criminisi, 2011\nTop 10 Algorithms in Data Mining4, Wu, 2008\nSemi-Supervised Learning Literature Survey, Zhu, 2007\nInterestingness Measures for Data Mining: A Survey, Geng, 2006\nA Survey of Interestingness Measures for Knowledge Discovery1, McGarry, 2005\nA Tutorial on the Cross-Entropy Method, Boer, 2005\nA Survey of Kernels for Structured Data, Gartner, 2003\nSurvey on Frequent Pattern Mining, Goethals, 2003\nThe Boosting Approach to Machine Learning: An Overview1, Schapire, 2003\nA Survey on Wavelet Applications in Data Mining, Li, 2002\nMathematics\nTopology and Data3, Carlsson, 2009\nMulti-armed Bandit\nRegret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems3, Bubeck, 2012\nNatural Computing\nReservoir Computing Approaches to Recurrent Neural Network Training, Jaeger, 2009\nArtificial Immune Systems, Aickelin, 2005\nA Survey of Evolutionary Algorithms for Data Mining and Knowledge Discovery, Freitas?? , 2003\nData Mining in Soft Computing Framework: A Survey, Mitra, 2002\nNeural Networks for Classification: A Survey1, Zhang, 2000\nNatural Language Processing\nProbabilistic Topic Models2, Blei, 2012\nOntology Learning From Text: A Look Back And Into The Future1, Wong, 2012\nMachine Transliteration Survey, Karimi, 2011\nTranslation Techniques in Cross-Language Information Retrieval, Zhou, 2011\nComprehensive Review of Opinion Summarization, Kim, 2011\nA Survey on Sentiment Detection of Reviews, Tang, 2009\nWord Sense Desambiguation: A Survey, Navigli, 2009\nTopic Models, Blei, 2009\nOpinion Mining and Sentiment Analysis, Pang, 2008\nInformation Extraction, Sarawagi, 2008\nStatistical Machine Translation, Lopez, 2008\nA Survey of Named Entity Recognition and Classification, Nadeau, 2007\nAdaptive Information Extraction, Turmo, 2006\nSurvey of Text Clustering, Jing, 2005\nMachine Learning in Automated Text Categorization, Sebastiani, 2002\nWeb Mining Research: A Survey, Kosala, 2000\nNetworks\nCommunity Detection in Graphs1, Fortunato, 2010\nA Survey of Statistical Network Models, Goldenberg, 2010\nCommunities in Networks, Porter, 2009\nGraph Clustering, Schaeffer, 2007\nGraph Mining: Laws, Generators, and Algorithms, Chakrabarti, 2006\nComparing Community Structure Identification, Danon, 2005\nLink Mining: A Survey1, Getoor, 2005\nDetecting Community Structure in Networks, Newman, 2004\nLink Mining: A New Data Mining Challenge, Getoor, 2003\nOn-Line Learning\nOn-Line Algorithms in Machine Learning1, Blum, 1998\nOthers\nA Survey of Very Large-Scale Neighborhood Search Techniques, Ahuja, 2001\nPlanning and Scheduling\nA Review of Machine Learning for Automated Planning1, Jimenez, 2009\nProbabilistic\nApproximate Policy Iteration: A Survey and Some New Methods, Bertsekas, 2011\nAn Introduction to MCMC for Machine Learning1, Andrieu, 2003\nProbabilistic Models\nAn Introduction to Conditional Random Fields1, Sutton, 2010\nRandomized Algorithms\nRandomized Algorithms for Matrices and Data1, Mahoney, 2011\nRecommender Systems\nRecent advances in Personalized Recommender Systems1, Liu, 2009\nMatrix Factorization Techniques for Recommender Systems1, Koren, 2009\nA Survey of Collaborative Filtering Techniques1, Su, 2009\nRegression\nEnsemble Approaches for Regression: a Survey4, Moreira, 2012\nReinforcement Learning\nA Survey of Reinforcement Learning in Relational Domains1, Otterlo, 2005\nReinforcement Learning: A Survey, Kaelbling, 1996\nRule Learning\nAssociation Mining, Ceglar, 2006\nAlgorithms for Association Rule Mining - A General Survey and Comparison, Hipp, 2000\nTesting\nControlled Experiments on the Web: Survey and Practical Guide, Kohavi, 2009\nTime Series\nTime-Series Data Mining2, Esling, 2012\nA Review on Time Series Data Mining1, Fu, 2011\nDiscrete Wavelet Transform-Based Time Series Analysis and Mining, Chaovalit, 2011\nTransfer Learning\nA Survey on Transfer Learning, Pan, 2010\nWeb Mining\nA Taxonomy of Sequential Pattern Mining Algorithms, Mabroukeh, 2010\nA Survey of Web Clustering Engines, Carpineto, 2009\nWeb Page Classification: Features and Algorithms, Qi, 2009\nMining Interesting Knowledge from Weblogs: A Survey, Facca, 2005\nAn Overview of Web Data Clustering Practices, Vakali, 2005\nA Survey of Web Metrics, Dhyani, 2002\nData Mining for Hypertext: A Tutorial Survey3, Chakrabarti, 2000"}
{"content2":"最近在朋友圈转起了一张图。抱着试一试的心态，我肝了些课程。都是与python相关的。\n课程一：你不知道的python\n讲师：王玉杰  （混沌巡洋舰联合创始人 & web开发工程师 & 数据工程师）\n这门课主要是简单介绍了人工智能，人工智能与python的关系以及python的特点等。\n零、思维导图预览：\n一、人工智能方面的应用\n1.手机app方面：\n——人脸识别 （face++） 也就是图像识别\n—— 购物推荐（淘宝app）\n—— 语音识别（讯飞输入法）\n—— 图片识花（微信里的小程序）也是图像识别\n—— 新闻资讯推荐（今日头条）\n2.其他方面：\n——无人驾驶\n——AlphaGo 围棋（阿尔法狗）\n二、人工智能背后的技术\n——　机器学习 &深度学习 &python\n１.机器学习\n特证：用大量的数据积累，然后从大连的数据中学习。\n——常见的机器学习算法：线性回归，决策树，神经网络等。\n2.深度学习\n——机器学习中的多层神经网络\n特征：可以自主的从大量数据中分析学习。\n3.三者的关系：\n4.人工智能与Python的关系\n——目前市面上大部分的人工智能的代码 都是使用Python 来编写。\n三、Python的简单认识\n1.python的特性\n——1). 多平台运行（macOS、windows、Linux）\n若是windows 环境，建议安装一个linux的环境。\n****题外话： 树莓派（装着linux系统的电脑）*****\n——2). 配置简单\n——Anaconda\n这是一个打包的集合，里面预装好了conda、python、众多packages、科学计算工具等等。所以也称为python的一种发行版。\n——3).语法简单\n一句话就能理解：“python 是世界上最不需要写注释的语言”\n——4). 有强大的数据处理库\n——numpy、 scipy、pandas、matpioylib\nNumpy：\n——是构建科学计算代码集的最基础的库。它提供了许多用Python进行N维数组和矩阵操作的功能。该库提供了Numpy数组类型的数学运算向量化，可以改善性能，从而加快执行速度。\nScipy：\n——是一个针对工程和科学库。主要功能是建立在Numpy基础之上，因此它使用了大量的Numpy数组结构。Scipy库通过其特定的子模块提供高效的数学运算功能，例如：数值积分、优化等。\nPandas：\n——是一个简单直观的应用于“带标记的”和“关系性的”数据的Python库。它可以快速的进行数据操作、聚合和可视化。\nMatPlotlib:\n——是一个可以做数据的可视化图表的库。超酷。与之相似的库有：seaborn 。且seaborn是建立在MatPlotlib之上的。\n——5). 有丰富的第三方库\n——6).编程工具推荐：Jupyter NoteBook\n——直接在浏览器里运行，可以直接在里面写代码，程序运行后可以立即得到反馈。\n——交互式编程环境。\n四、Python职业方向\n—— web开发  、人工智能 、数据分析、Linux 运维、爬虫工程师、自动化测试"}
{"content2":"大约在二百年前，工业革命使机械在人类社会中无处不在。今天工业4.0的革命正在进行，它的发展为我们带来了人工智能。那今天让我们一起来了解下这个新物种的起源和它的未来之路。\n对于非专业人士来说，人工智能（AI，ArtificialIntelligence）发展几十年，几乎看不到什么显著变化，虽然偶尔也有比如IBM深蓝电脑战胜人类棋手、会说话的机器人等新闻传出，但人工智能更多只能解决一些“玩具问题”，或生存在实验室条件下，离我们的日常生活一直十分遥远。\n然而，2016年，谷歌的阿尔法狗（AlphaGo）让世界围棋冠军李世石投子认输的那一刻起，人工智能成为了科技界、工业界、投资界乃至公众间的热门话题，热到甚至被写入了我国“十三五”规划纲要。仿佛一夜之间，所有的科技公司都将目光转向了人工智能。\n人工智能为何在现在变得如此重要？\n其实，早在1956年，人工智能就被提了出来，但当时主要是指代可体现出智能行为的硬件或软件。这听起来更像一种计算机系统，可以执行一些以前需要人类智能的任务。\n随后，人工智能的研究起起伏伏数十年，但一直因为没有发掘出对人类社会真正有益的功能而趋于平静。但互联网技术的发展和“深度学习”算法的出现改变了这一局面，让其在很多垂直领域获得了真正的应用，并超越人类的表现。GE科学家AchaleshPandey表示：“一个重要的原因就是可用的大量数据和巨大的计算能力的出现，而深度学习（一种机器学习算法）的突破更是让人工智能领域产生了脱胎换骨的革新。”\n近期人工智能大事件\n事件1：阿尔法狗大胜世界冠军李世石\n2016年3月的这场对战被认为是一场人机“世纪大战”，受到全世界的关注，最终阿尔法狗以4:1大胜李世石让人大跌眼睛。\n事件2：Uber无人驾驶出租车\n2016年9月，Uber宣布在美国匹兹堡市推出城区大范围无人驾驶出租车，并真实向打车用户开放，这些车辆配备了前置摄像头、360度雷达感应器，能够在固定路线上行驶。\n事件3：GE收购两家人工智能企业\n2016年11月，GE宣布收购了两家人工智能高科技公司BitStewSystems和Wise.io，以强化和拓展Predix云平台，为GE的工业制造带来的相关大数据集，这有助于推动人工智能在工业领域的应用，为电厂、航空发动机和医疗等领域提供智能解决方案。\n事件4：自动唇读系统远超人类专家\n谷歌DeepMind与英国牛津大学研发了一套基于人工智能的自动唇读系统LipNet，对Gird语料库唇语识别的准确率达到了惊人的95.2%。\n事件5：人工智能赢得德州扑克冠军\n2017年1月，在宾夕法尼亚州匹兹堡的Rivers赌场，卡耐基梅隆大学研发的人工智能系统Libratus战胜4位德州扑克顶级选手，赢取了20万美元的奖金。\n我国人工智能现状\n在人工智能技术领域，我国大体上能够与世界先进国家发展同步，以百度、阿里巴巴、腾讯为首的互联网巨头公司也已在人工智能领域上布局。特别是百度公司，已经将人工智能视为未来各项业务的核心所在。百度在2013年就成立了深度学习研究院，无人驾驶项目也同期启动，并在过去的两年半里，将200亿研发费用大部分投入了人工智能。2017年1月6日，百度人工智能机器人“小度”利用其超强的人脸识别能力，以3:2的成绩战胜人类最强大脑代表王峰。5月4日，百度更是将自身使命直接更改为“用科技让复杂的世界更简单”，再次强调了人工智能对百度的重要性。\n阿里巴巴和腾讯等其他中国企业也非常重视人工智能，不过主要侧重人工智能对主要业务的补充。阿里巴巴研发并对外开放了我国首个人工智能计算平台“DTPAI”，开发者可通过简单拖拽方式完成对海量数据的分析挖掘。腾讯公司则研发与对外开放了视觉识别平台“腾讯优图”，它在人脸识别上达到了稳居世界前列的99.5%以上准确率，即将在微众银行、财付通等相关产品中大规模应用。\n除此之外，我国在人工智能领域还有近百家创业公司，业务覆盖了工业机器人、服务机器人、商业智能及视觉识别等技术领域。科大讯飞的“讯飞超脑”计划，京东公司的智能聊天机器人等都达到了国际先进水平。目前我国在人工智能专利总数上仅次于美国，但近年申请增长率已经超过美国。据统计，2014年，我国人工智能产业市场规模为48.6亿元，到2016年底，人工智能产业市场规模已经增长到了95.6亿元。\n不容忽视的工业人工智能领域\n综上所述，你可以发现，国内外绝大部分科技巨头都集中在消费级人工智能，但事实上，人工智能在工业和制造业领域也拥有广泛的运用，且面临着独特的挑战。而工业巨头GE正试图将机器学习和人工智能应用到很多众所周知的各种产品中，覆盖航空、运输、医疗和发电等多个领域。\n自2015年开始，GE就宣布推出Predix云平台。Predix是全球第一个也是唯一一个专门面向工业开发的云平台。Predix通过机器的互联互通，为企业提供有效的数据分析，从而快速、智能和高效地运营并加速客户和合作伙伴的创新进程。当时，GE公司就表示使用机器学习来对收集的传感器数据进行模式识别，可以带来节能或预防性维护。即便Predix云平台在2016年2月向GE客户开放后，GE仍然在建设人工智能能力，来履行承诺。\nGE数字集团首席执行官鲁威廉表示：“之前使用了机器学习，但我更愿意将其称之为一种传统方法。如果你要做一个工业人工智能系统，那首先要保证你的系统对所服务的行业有足够深度的了解。”\n而这个深度的定义某种程度上就是指可供人工智能系统深度学习的大数据，虽然GE在燃气轮机、航空发动机和医疗设备等数据上已经有了大量的积累，但仍斥巨资收购BitStewSystems、Wise.io、ServiceMax等公司来进一步强化。鲁威廉说：“我们有机器学习工业领域的专业算法，它将会知道一个发电厂是什么，涵盖所有的深度，而这是通用人工智能系统永远不会真正理解的。”\nGE医疗就正在与合作伙伴联手，共同就人工智能辅助肺结节检出与诊断进行研究，并已取得阶段性成果。在刚刚结束的第77届中国国际医疗器械（春季）博览会（CMEF）上展示的人工智能肺结节辅助诊断技术是GE医疗的一款诊断图像处理软件，整合了人工智能技术与深度学习理念和工具，通过多层神经网络和神经元来模拟人类大脑实现图像识别，在诊断效率和精准度、肺结节自动识别敏感度以及检出率上均取得大幅提升。\n阶段性临床测试结果显示：人工智能工具在<3mm病灶发现和诊断方面具有很大的优势，检出速度和准确定均大幅提升，这对肺癌的更早期发现和早期诊断具有重大价值。\n接下来会怎样？\n经济学常识告诉我们，低效能的生产模式必然会被高效率的淘汰，就如同手工作坊在工业4.0后被机器工厂替代，马车被汽车替代一样。而人工智能也是现代技术发展到一定阶段的产物，核心背后是机器学习、云计算、大数据的繁荣。这样说来，人工智能的大规模应用也许只不过是一场顺势而为的技术变革。"}
{"content2":"本来这篇是准备5.15更的，但是上周一直在忙签证和工作的事，没时间就推迟了，现在终于有时间来写写Learning Spark最后一部分内容了。\n第10-11 章主要讲的是Spark Streaming 和MLlib方面的内容。我们知道Spark在离线处理数据上的性能很好，那么它在实时数据上的表现怎么样呢？在实际生产中，我们经常需要即使处理收到的数据，比如实时机器学习模型的应用，自动异常的检测，实时追踪页面访问统计的应用等。Spark Streaming可以很好的解决上述类似的问题。\n了解Spark Streaming ，只需要掌握以下几点即可：\nDStream\n概念：离散化流（discretized stream），是随时间推移的数据。由每个时间区间的RDD组成的序列。DStream可以从Flume、Kafka或者HDFS等多个输入源创建。\n操作：转换和输出，支持RDD相关的操作，增加了“滑动窗口”等于时间相关的操作。\n下面以一张图来说明Spark Streaming的工作流程：\n从上图中也可以看到，Spark Streaming把流式计算当做一系列连续的小规模批处理来对待。它从各种输入源读取数据，并把数据分组为小的批次，新的批次按均匀的时间间隔创建出来。在每个时间区间开始的时候，一个新的批次就创建出来，在该区间内收到的数据都会被添加到这个批次中去。在时间区间结束时，批次停止增长。\n转化操作\n无状态转化操作：把简单的RDDtransformation分别应用到每个批次上，每个批次的处理不依赖于之前的批次的数据。包括map()、filter()、reduceBykey()等。\n有状态转化操作：需要使用之前批次的数据或者中间结果来计算当前批次的数据。包括基于滑动窗口的转化操作，和追踪状态变化的转化操作（updateStateByKey()）\n无状态转化操作\n有状态转化操作\nWindows机制（一图盛千言）\n上图应该很容易看懂，下面举个实例（JAVA写的）：\nUpdateStateByKey()转化操作\n主要用于访问状态变量，用于键值对形式的DStream。首先会给定一个由(键，事件)对构成的DStream，并传递一个指定如何个人剧新的事件更新每个键对应状态的函数，它可以构建出一个新的DStream，为（键，状态）。通俗点说，加入我们想知道一个用户最近访问的10个页面是什么，可以把键设置为用户ID，然后UpdateStateByKey()就可以跟踪每个用户最近访问的10个页面，这个列表就是“状态”对象。具体的要怎么操作呢，UpdateStateByKey()提供了一个update（events，oldState）函数，用于接收与某键相关的时间以及该键之前对应的状态，然后返回这个键对应的新状态。\nevents：是在当前批次中收到的时间列表()可能为空。\noldState：是一个可选的状态对象，存放在Option内；如果一个键没有之前的状态，可以为空。\nnewState：由函数返回，也以Option形式存在。如果返回一个空的Option，表示想要删除该状态。\nUpdateStateByKey()的结果是一个新的DStream，内部的RDD序列由每个时间区间对应的（键，状态）对组成。\n接下来讲一下输入源\n核心数据源：文件流，包括文本格式和任意hadoop的输入格式\n附加数据源：kafka和flume比较常用，下面会讲一下kafka的输入\n多数据源与集群规模\nKafka的具体操作如下：\n基于MLlib的机器学习\n一般我们常用的算法都是单机跑的，但是想要在集群上运行，不能把这些算法直接拿过来用。一是数据格式不同，单机上我们一般是离散型或者连续型的数据，数据类型一般为array、list、dataframe比较多，以txt、csv等格式存储，但是在spark上，数据是以RDD的形式存在的，如何把ndarray等转化为RDD是一个问题；此外，就算我们把数据转化成RDD格式，算法也会不一样。举个例子，你现在有一堆数据，存储为RDD格式，然后设置了分区，每个分区存储一些数据准备来跑算法，可以把每个分区看做是一个单机跑的程序，但是所有分区跑完以后呢？怎么把结果综合起来？直接求平均值？还是别的方式？所以说，在集群上跑的算法必须是专门写的分布式算法。而且有些算法是不能分布式的跑。Mllib中也只包含能够在集群上运行良好的并行算法。\nMLlib的数据类型\nVector：向量（mllib.linalg.Vectors）支持dense和sparse（稠密向量和稀疏向量）。区别在与前者的没一个数值都会存储下来，后者只存储非零数值以节约空间。\nLabeledPoint:（mllib.regression）表示带标签的数据点，包含一个特征向量与一个标签,注意，标签要转化成浮点型的，通过StringIndexer转化。\nRating:(mllib.recommendation)，用户对一个产品的评分，用于产品推荐\n各种Model类：每个Model都是训练算法的结果，一般都有一个predict()方法可以用来对新的数据点或者数据点组成的RDD应用该模型进行预测\n一般来说，大多数算法直接操作由Vector、LabledPoint或Rating组成的RDD，通常我们从外部数据读取数据后需要进行转化操作构建RDD。具体的聚类和分类算法原理不多讲了，可以自己去看MLlib的在线文档里去看。下面举个实例----垃圾邮件分类的运行过程：\n步骤：\n1.将数据转化为字符串RDD\n2.特征提取，把文本数据转化为数值特征，返回一个向量RDD\n3.在训练集上跑模型，用分类算法\n4.在测试系上评估效果\n具体代码：\n1 from pyspark.mllib.regression import LabeledPoint 2 from pyspark.mllib.feature import HashingTF 3 from pyspark.mllib.calssification import LogisticRegressionWithSGD 4 5 spam = sc.textFile(\"spam.txt\") 6 normal = sc.textFile(\"normal.txt\") 7 8 #创建一个HashingTF实例来把邮件文本映射为包含10000个特征的向量 9 tf = HashingTF(numFeatures = 10000) 10 #各邮件都被切分为单词，每个单词背映射为一个特征 11 spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \"))) 12 normalFeatures = normal.map(lambda email: tf.transform(email.split(\" \"))) 13 14 #创建LabeledPoint数据集分别存放阳性（垃圾邮件）和阴性（正常邮件）的例子 15 positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1,features)) 16 negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0,features)) 17 trainingData = positiveExamples.union(negativeExamples) 18 trainingData.cache#因为逻辑回归是迭代算法，所以缓存数据RDD 19 20 #使用SGD算法运行逻辑回归 21 model = LogisticRegressionWithSGD.train(trainingData) 22 23 #以阳性（垃圾邮件）和阴性（正常邮件）的例子分别进行测试 24 posTest = tf.transform(\"O M G GET cheap stuff by sending money to...\".split(\" \")) 25 negTest = tf.transform(\"Hi Dad, I stared studying Spark the other ...\".split(\" \")) 26 print \"Prediction for positive test examples: %g\" %model.predict(posTest) 27 print \"Prediction for negative test examples: %g\" %model.predict(negTest)\n这个例子很简单，讲的也很有限，建议大家根据自己的需求，直接看MLlib的官方文档，关于聚类，分类讲的都很详细。\n注：图片参考同事的PPT讲义^_^，已授权哈哈"}
{"content2":"转载请注明出处：http://www.cnblogs.com/ymingjingr/p/4271742.html\n目录\n机器学习基石笔记1——在何时可以使用机器学习(1)\n机器学习基石笔记2——在何时可以使用机器学习(2)\n机器学习基石笔记3——在何时可以使用机器学习(3)(修改版)\n机器学习基石笔记4——在何时可以使用机器学习（4）\n机器学习基石笔记5——为什么机器可以学习（1）\n机器学习基石笔记6——为什么机器可以学习（2）\n机器学习基石笔记7——为什么机器可以学习（3）\n机器学习基石笔记8——为什么机器可以学习（4）\n机器学习基石笔记9——机器可以怎样学习（1）\n机器学习基石笔记10——机器可以怎样学习（2）\n机器学习基石笔记11——机器可以怎样学习（3）\n机器学习基石笔记12——机器可以怎样学习（4）\n机器学习基石笔记13——机器可以怎样学得更好（1）\n机器学习基石笔记14——机器可以怎样学得更好（2）\n机器学习基石笔记15——机器可以怎样学得更好（3）\n机器学习基石笔记16——机器可以怎样学得更好（4）\n十四、Regularization\n正则化。\n14.1 Regularized Hypothesis Set\n正则化假设。\n上一章中提到了防止过拟合的五种措施，本章将介绍其中一种措施，正则化（Regularization）。\n正则化的主要思想：将假设函从高次多项式的数降至低次，如同开车时的踩刹车，将速度降低，效果图如-1所示，右图表示高次多项式函数，明显产生了过拟合现象，而左图的表示使用正则化后的低次函数。\n-1 正则化拟合与过拟合\n已知高次多项式包含低次多项式，因此高次函数和低次函数的关系如-2所示，本章的内容是在使用高次函数过拟合时，如何将假设函数降低为低次，即如何从外围的大圈中回归到内部的小圈。\n-2 高次函数与低次函数的关系图\n\"正则化\"这个词来自于不适定问题（ill-posed problem）的函数逼近（function approximation），即在函数逼近中出现多个解，如何选择解的问题。\n如何降次？该问题使用到前几章中提到的多项式转换与线性回归的知识，把降次的问题转换成带有限制（constraint）条件的问题。以下以10次多项式与二次式为例了解正则化，假设w的表达式分别如公式14-1与公式14-2。\n（公式14-1）\n（公式14-2）\n公式14-2可以使用公式14-1加上如下限制条件表示， ，\n因此10次多项式的假设空间与最小 的表达式分别如公式14-3和公式14-4。\n（公式14-3）\n（公式14-4）\n通过上述结论，2次式的假设空间与最小的表达式分别如公式14-5和公式14-6。\n（公式14-5）\n（公式14-6）\n如果将的条件设计的更宽松，表示成的形式，如公式14-7所示。\n（公式14-7）\n因此求的最优化的问题如公式14-8所示。\n（公式14-8）\n该假设空间与、的关系如公式14-9所示。\n（公式14-9）\n假设空间又被称作稀疏（sparse）的假设空间，因为很多参数为0。注意公式14-8限制中的 函数，表明该最优化问题为一个NP难问题。因此必须继续改进假设函数，产生假设空间如公式14-10所示。\n（公式14-10）\n假设空间最优化的问题如公式14-11所示。\n（公式14-11）\n与有重叠部分，但是并不完全一致。随着C的增大， 的假设空间也在增大，可以得到如公式14-12所示。\n（公式14-12）\n称假设空间为正则化假设空间，即假设限制条件的假设空间。正则化假设空间中最好的假设用符号 表示。\n14.2 Weight Decay Regularization\n权值衰减正则化。\n为了表述的简便，将上一节的最优化公式14-11写成向量矩阵的形式，如公式14-13所示。\n（公式14-13）\n插一句，通常解释带有限制条件的最优化问题都会引用拉格朗日函数，林老师更深入的解释了拉格朗日乘子背后的因素。\n首先绘制有限制条件的最优化示意图，图中蓝色部分为，红色部分为限制条件，从表达公式不难得出两者一个为椭圆，一个为圆形（在高维空间中式超球体）。\n-4 有限制条件的最优化示意图\n从前面的章节中了解在求解最小时，可用梯度的反方向，即 作为下降方向，但是与回归问题还有一些不同，此处多了限制条件，因此下降的方向不可以超出限制的范围，如-3中红色的向量为限制圆球切线的法向量，朝着该方向下降便超出了限制的范围，因此只可以沿着球切线的方向滚动，如-3中绿色的向量。何时降到最小？即实际滚动方向（图中蓝色的向量）不存在与球切线方向相同的分量，换句话说与球切线的法向量w相平行，如公式14-14所示，其中表示正则化最优解。\n（公式14-14）\n加入拉格朗日乘子 ，可写成等式的形式，如公式14-15.\n（公式14-15）\n将线性回归中求得的表达式（9.2节中求导过程）代入公式14-15，得公式14-16.\n（公式14-16）\n求出的表达式如公式14-17。\n（公式14-17）\n其中是半正定的，因此只要，则保证为正定矩阵，必可逆。该回归形式被称为岭回归（ridge regression）。\n是否还记得线性回归的直接形式，如公式14-18所示。\n（公式14-18）\n对公式14-15做成积分得公式14-19。\n（公式14-19）\n求公式14-19的最小解问题等价于公式14-19。其中该表达式称为增广错误（augmented error），用 表示，其中为正则化项（regularizer）。用无限制条件的取代了上节中提到的有限制条件的。实际上使用了拉格朗日函数，但林老师是反推过去，之所以叫做增广错误，是因为比传统的多了一正则化项。在或时（的情况是线性回归的求解），最小w的求解公式如公式14-20所示。\n（公式14-20）\n因此，不需要给出上一节中有条件的最小化问题中包含的参数C，而只需要给出增广错误中的参数。\n观察参数对最终求得的的影响，如-5。\n-5 参数对最终求得的的影响\n在时，过拟合，随着的不断增大变成了欠拟合状态。越大的对应着越短的权值向量w，同时也对应着越小的约束半径C。（记得14.1节中如何处理欠拟合吗？将C尽量缩小，准确的说寻找小的权值向量w），因此这种将w变小的正则化，即加上的正则化称为权重衰减（weight-decay）正则化。此种正则化，可以和任意的转换函数及任意的线性模型结合。\n注意：在做多项式转换时，假设 ，多项式转换函数为 则在高次项 上时，数值非常小，为了和低次项对应的权值向量分量产生一致的影响力，则该项的权值 一定非常大，但是正则化求解需要特别小的权值向量w，因此需要转换后的多项式各项线性无关，即转换函数为，其各项为正交基函数（orthonormal basis functions），此多项式称为勒让德多项式（Legendre polynomials），多项式的前5项如-6所示。\n-6 勒让德多项式的前5项表示\n14.3 Regularization and VC Theory\n正则化与VC理论。\n本节介绍正则化与VC理论的关系。即从VC理论的角度说明为什么正则化的效果好（14.1节从过拟合的角度介绍正则化好的原因）。\n最小化带限制条件的与最小化等价，因为参数C类似与参数 。通过7.4节的知识得知，的上限可以表示为公式14-21的形式。\n（公式14-21）\n因此，VC限制间接的保证了最小化可以得到最小的。\n便于观察对比，将的表达式重复写一遍，如公式14-22。\n（公式14-22）\n上限更一般的形式可以写成公式14-23。\n（14-23）\n通过公式14-22与公式14-23的对比，更容易理解最小化能获得比最小化更好效果的原因。如公式14-22中正则化项表示一个假设函数的复杂度；而公式14-23中的表示整个假设空间的复杂度，如果（，其中表示该假设的复杂度）很好的代表，则比表现的更好。\n上述是通过VC限制通过一个启发式的方式说明正则化的优势，接下来通过VC维阐述正则化的好处。\n将最小化的形式写成公式14-24。\n（公式14-24）\n按第七章的理论，VC维 ， 在求解最小化时所有的假设函数 都将被考虑。但是因为参数C或者更直接的来说参数 的限制，实际被考虑的只有 。因此有效的VC维 与两部分相关：假设空间H及算法A。实际的VC维很小意味着模型复杂度很低。\n14.4 General Regularizers\n一般化的正则化项。\n本章的前几节介绍的正则化项是权值衰减的正则化项（weight-decay (L2) regularizer），或称为L2正则化项，标量形式为 ，向量形式为。那么更一般的正则化项应该如何设计，或者一般化的正则化项的设计原则是什么？主要分为三点，如下：\n依据目标函数（target-dependent），即根据目标函数的性质设计正则化项，如某目标函数是对称函数，因此权值向量的所有奇数分量应被抑制，可以设计成 的形式，在奇数时增加；\n可以说得通（plausible）：正则化项应尽可能地平滑（smooth）或简单（simpler），因为不论是随机性噪音还是确定性噪音都不是平滑的。平滑表示可微，如L2。简单表示容易求解，如L1正则化项或稀疏（sparsity）正则化项： ，稍后介绍；\n友好：易于最优化的求解。如L2。\n即使设计的正则化项不好也不用担心，因为还存在一个参数 ，当其为0时，则正则化项不起作用。\n回忆8.3节，错误衡量的设计原则，与此类似，依据用户（user-dependent），说得通，友好。\n因此最终的增广错误由错误函数和正则化项两部分组成，如公式14-25所示。\n（公式14-25）\n通过比较常用的两种正则化项（L2和L1）具体的解释上述设计原则。\nL2的正则化示意图如-7所示，正则化项如公式14-26。\n-7 L2正则化示意图\n（公式14-26）\n该正则化项在为凸函数，在每个位置都可以微分，因此比较容易计算。\n再介绍一种新的正则化项L1，其示意图如-8所示正则化项如公式14-27。\n-8 L1正则化项示意图\n（公式14-27）\n同样也是凸图形，但是并不是所有的位置都可微，如转角处。为何成为稀疏？假设菱形法相w全是不为零的分量，因此微分得的向量为分量全为1的向量。如果与该全为1的向量不平行，则向量一直会沿着菱形边界移动到顶点处，因此在顶点处产生最优解，最优解含有值为0的分量，因此为稀疏的解，计算速度快。\n在结束本章前，观察在不同噪音情况下，参数如何选择。目标函数设计成15次多项式函数，如-9表示固定确定性噪音，不同随机性噪音下，参数最佳选择，横坐标表示参数的选择，纵坐标表示 ，其中加粗的点表示在该种噪音情况下参数的最佳取值。（此处因为是为了观察在不同噪音下如何选择参数，目标函数是已知的，所以可以求出，现实中是不可能的，下一个例子也是如此，不再重复解释）\n-9 不同随机性噪音下参数的选择\n目标函数设计成15次多项式函数，如-10表示固定随机性噪音，不同确定性噪音下，参数最佳选择，横坐标表示参数的选择，纵坐标表示，其中加粗的点表示在该种噪音情况下参数的最佳取值。\n-10不同确定性噪音下参数的选择\n从上述两个图中不难得出，越大的噪音需要越大的正则化，这如同越颠簸的路，越需要踩刹车一样。但是一个更重要的问题却没有解决，即在噪音未知的情况下，如何选择参数，这是下章的内容。"}
{"content2":"欢迎大家前往腾讯云技术社区，获取更多腾讯海量技术实践干货哦~\n作者：李春晓\n导语：\n“从入门到第一个模型”差点就成了“从入门到放弃”。本文是机器学习在运维场景下的一次尝试，用一个模型实现了业务规律挖掘和异常检测。这只是一次尝试，能否上线运转还有待考究。试了几个业务的数据，看似有效，心里却仍然忐忑，担心哪里出错或者有未考虑到的坑，将模型介绍如下，请大侠们多多指教，帮忙指出可能存在的问题，一起交流哈。\n背景:\n业务运维需要对业务基础体验指标负责，过去的分析都是基于大数据，统计各个维度及其组合下关键指标的表现。比如我们可以统计到不同网络制式下打开一个app的速度（耗时），也可以获取不同命令字的成功率。针对移动APP类业务，基于经验，我们在分析一个指标时都会考虑这些因素：App版本、指标相关的特有维度（比如图片下载要考虑size、图片类型； 视频点播类要考虑视频类型、播放器类型等）、用户信息（网络制式、省份、运营商、城市）等。这些维度综合作用影响关键指标，那么哪些维度组合一定好，哪些一定不好？耗时类指标的表现往往呈现准正态分布趋势，其长尾永远存在并且无法消除，这种情况要不要关注？ 针对命令字成功率，有些命令字成功率低是常态，要不要告警？过去我们会通过在监控中设置特例来避免告警。有没有一种方法，能自动识别常态与非常态？在机器学习如火如荼的现在，也许可以试一试。\n目标：\n挖掘业务潜在规律(针对耗时这类连续值指标，找出引起长尾的因素)\n监控业务指标时，找出常态并忽略常态，仅针对突发异常产生告警并给出异常的根因。\n之后就是艰苦的屡败屡战，从入门到差点放弃，最终搞出第一个模型的奋战史了。最大的困难是没写过代码，不会python，机器学习理论和代码都要同步学习；然后就是在基础薄弱的情况下一开始还太贪心，想要找一个通用的模型，对不同业务、不同指标都可以通用，还可以同时解决两个目标问题，缺少一个循序渐进入门的过程，难免处处碰壁，遇到问题解决问题，重新学习。好在最终结果还是出来了，不过还是要接受教训：有了大目标后先定个小目标，理清思路后由点及面，事情会顺利很多。\n接下来直接介绍模型，过程中走的弯路就忽略掉（因为太多太弱了，有些理论是在遇到问题后再研究才搞明白）。\n基本思路：\n1.通过学习自动获取业务规律，对业务表现进行预测（ET算法），预测命中的就是业务规律，没命中的有可能是异常（请注意，是有可能，而非绝对）；\n2.将1的结果分别输入决策树（DT）进行可视化展示；用预测命中的部分生成业务潜在规律视图；用未命中的来检测异常，并展示根因。\n步骤简介（以耗时这个指标为例）：\n1. 准备两份不重合的数据，一份用于训练，一份用于预测\n例：视频播放类业务的维度（如版本，机型，视频来源，视频编码类型等各种已有特征），及耗时数据\n2. 将目标问题转化为分类问题，有可能是常见的二分类，也有可能是多分类，视情况而定\n将耗时这种连续性指标转为离散值，目标是产生三个分类：“极好的/0”，“一般的/1”，“极差的/2”，将耗时按10分位数拆分，取第1份（或者前2份）作为“极好的”样本，中间几份为“一般的”的样本，最后1（或者2）份为“极差的“样本 。 这里的“极差的”其实就是正态分布的长尾部分。如下图，第一列是耗时区间（未加人工定义阈值，自动获取），第二列是样本量。\n3. 特征处理\n3.1 特征数值化\n这里表现为两类问题，但处理方式都一样：\n（1）文本转数值\n（2）无序数值需要削掉数值的大小关系,比如Appid这类，本身是无序的，不应该让算法认为65538>65537\n方法：one-hot编码, 如性别这个特征有三种取值，boy,girl和unknown，转换为三个特征sex==boy,sex==girl,sex==unknown, 条件满足将其置为1，否则置为0.\n实现方式3种：自己实现；sklearn调包；pandas的get_dummies方法。\nOne-hot编码后特征数量会剧烈膨胀，有个特征是手机机型，处理后会增加几千维，同时也要根据情况考虑是否需要对特征做过于细化的处理。\n3.2 特征降维\n是否需要降维，视情况而定，我这里做了降维，因为特征太多了，如果不降维，最终的树会非常庞大，无法突出关键因素。\n所谓降维，也就是需要提取出特征中对结果起到关键影响因素的特征，去掉不重要的信息和多余信息，理论不详述了，参考：http://sklearn.lzjqsdd.com/modules/feature_selection.html\n本文用了ET的feature_importance这个特性做降维，将5000+维的数据降至300左右\n4. 用ET算法（随机森林的变种，ExtraTreesClassifier）训练一个分类模型（三分类）\n4.1 评价模型的指标选取\n对于分类算法，我们首先想到的准确性 precision这个指标，但它对于样本不均衡的场景下是失效的。举个例子，我们有个二分类（成功和失败）场景，成功的占比为98%。这种样本直接输入训练模型，必定过拟合，模型会直接忽略失败的那类，将所有都预测为成功。此时成功率可达98%，但模型其实是无效的。那么应该用什么？\n对于二分类，可用roc_auc_score，对于多分类，可用confusion_matrix和classification report\n4.2 样本不均衡问题处理\n本文用的例子，显然0和2的数量非常少，1的数量是大头。为了不对1这种类型产生过拟合，可对0和2这两类做过抽样处理。\n常见的有两类算法：\n（1）直接复制少数类样本\n（2）SMOTE过抽样算法（细节略）\n这里两种算法都用过，最终选了SMOTE，不过本文研究的数据上没有看出明显差别。\n少数类的过抽样解决了大类的过拟合问题，同时也带来了小类的过拟合，不过这里的模型正好需要让小类过拟合，我们就是要把表现“极好”和“极坏”的部分找出来，表现平平的在异常检测时加入关注。过拟合这个问题，不用过于恐惧，反而可以利用。举个例子，“患病”和“不患病”这种分类场景，宁可将“患病”的检出率高一些。如下图这个分类报告，对于小类样本（0和2），我们需要利用recall高的特性，即把它找出来就好；而对于大类样本，我们需要precision高的特性，用于做异常检测。\n4.3 模型参数选取\nSklearn有现成的GridSearchCV方法可用，可以看看不同参数组合下模型的效果。对于树类算法，常用的参数就是深度，特征个数；森林类算法加一个树个数。\nMax_depth这个参数需要尤其注意，深度大了，容易过拟合，一般经验值在15以内。\n4.4 模型训练好后，用测试数据预测，从中提取各个类别预测正确的和不正确的。\n例：\n预测正确的部分：获取预测为0,2，实际也为0,2的样本标示；\n预测错误的部分：获取预测为0和1，实际为2的样本标示(根据情况调节)\n5. 输入决策树进行可视化展示,分别做业务规律挖掘和异常检测\n这里DT算法仅用于展示，将不同类别的数据区分开，必要时仍然要设置参数，如min_samples_leaf, min_impurity_decrease，以突出关键信息。\n还可以通过DecisionTreeClassifier的内置tree_对象将想要找的路径打出来\n以下分别给出例子：\n5.1 业务规律挖掘\n视频点播场景，取0和2这两类预测正确的部分，输入DT，如下图，自动找出了业务潜在规律，并一一用大数据统计的方式验证通过，结论吻合。这个树的数据相对纯净，因为输入给它的数据可以理解为必然符合某种规律。\n5.2. 异常检测\n本文模型还在研究阶段，未用线上真实异常数据，而是手工在测试数据某个维度（或者组合）上制造异常来验证效果。\n针对成功率，可以视容忍程度做二分类或者三分类。\n二分类：取一个阈值，如99%，低于99%为2，异常，否则为0正常。缺点是如果某个维度上的成功率长期在99%以下，如98%，当它突然下跌时会被当做常态忽略掉，不会告警。\n三分类：99%以上为0, 96~99% 为1，低于96%为2，这种方式会更灵活。 三种分类也分别对应其重要性。重点关注，普通关注，忽略。\n下图是一个二分类的例子（手工将平台为IPH和播放端为client的置为异常）：\n最后：这里只是一次小尝试，如果要平台化上线运转，还要很多因素要考虑，首要就是模型更新问题（定时更新？避免选取到异常发生时段？），这个将放在下阶段去尝试。\n相关阅读\n5分钟教你玩转 sklearn 机器学习（上）\n机器学习概念总结笔记（一）\n机器学习之离散特征自动化扩展与组合\n此文已由作者授权腾讯云技术社区发布，转载请注明文章出处\n原文链接：https://cloud.tencent.com/community/article/477670"}
{"content2":"本文简要介绍了10款 Quora上网友推荐的人工智能和机器学习领域方面的开源项目。\nGraphLab\nGraphLab是一种新的面向机器学习的并行框架。GraphLab提供了一个完整的平台，让机构可以使用可扩展的机器学习系统建立大数据以 分析产品，该公司客户包括Zillow、Adobe、Zynga、Pandora、Bosch、ExxonMobil等，它们从别的应用程序或者服务中抓 取数据，通过推荐系统、欺诈监测系统、情感及社交网络分析系统等系统模式将大数据理念转换为生产环境下可以使用的预测应用程序。（ 详情 ）\n项目主页： http://graphlab.org/\nVowpal Wabbit\nVowpal Wabbit (Fast Online Learning)最初是由雅虎研究院建设的一个机器学习平台，目前该项目在微软研究院。它是由John Langford启动并主导的项目。\n项目地址： http://hunch.net/~vw/\nscikits.learn\nscikit-learn是一个开源的、构建在SciPy之上用于机器学习的 Python 模块。它包括简单而高效的工具，可用于数据挖掘和数据分析，适合于任何人，可在各种情况下重复使用、构建在 NumPy、SciPy和 matplotlib 之上，遵循BSD 协议。（ 详情 ）\n项目地址： http://scikit-learn.org/stable\nTheano\nTheano是一个python库，用来定义、优化和模拟数学表达式计算，用于高效的解决多维数组的计算问题。它使得写深度学习模型更加容易，同时也给出了一些关于在GPU上训练它们的选项。（ 详情 ）\n项目地址： http://deeplearning.net/software/theano/\nMahout\nMahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout包含许多实现，包括聚 类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。\n项目主页： http://mahout.apache.org/\npybrain\npybrain是Python的一个机器学习模块，它的目标是为机器学习任务提供灵活、易应、强大的机器学习算法。pybrain包括神经网络、强化学习(及二者结合)、无监督学习、进化算法。以神经网络为核心，所有的训练方法都以神经网络为一个实例。\n项目主页： http://pybrain.org/\nOpenCV\nOpenCV是一个基于（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows和Mac OS操作系统上。它轻量级而且高效——由一系列 C 函数和少量 C++ 类构成，同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。（ 详情 ）\n项目主页： http://opencv.org/\nOrange\nOrange 是一个基于组件的数据挖掘和机器学习软件套装，它的功能即友好，又很强大，快速而又多功能的可视化编程前端，以便浏览数据分析和可视化，基绑定了 Python以进行脚本开发。它包含了完整的一系列的组件以进行数据预处理，并提供了数据帐目，过渡，建模，模式评估和勘探的功能。\n项目主页： http://orange.biolab.si/\nNLTK\nNLTK（natural language toolkit)是python的自然语言处理工具包。2001年推出，至今发展非常活跃。它的主要作用是为了教学，至今已经在20多个国家60多所高校 使用，里面包括了大量的词料库，以及自然语言处理方面的算法实现：分词， 词根计算， 分类， 语义分析等。\n项目主页： http://nltk.org/\nNupic\nNupic是一个开源的人工智能平台。该项目由Grok（原名 Numenta）公司开发，其中包括了公司的算法和软件架构。 NuPIC 的运作接近于人脑，“当模式变化的时候，它会忘掉旧模式，记忆新模式”。如人脑一样，CLA 算法能够适应新的变化。（ 详情 ）\n项目主页： http://numenta.org/nupic.html\n以上是小编整理的10款人工智能和机器学习领域的开源项目。更多项目可参看这个列表： http://deeplearning.net/software_links/"}
{"content2":"说起来，随着人工智能和大数据逐渐进入人们的眼中，越来越多的人看到互联网未来大好发展趋势，而想要学习一门技术来进入其中，以期分一杯羹。但是，作为人工智能和大数据的重要编程语言，Python和Java，该学习哪一种?也成了很多人所困惑的。到底python工资高还是java呢?毕竟，我们学一门技术，也是为了自身的就业和发展。\n其实，也有读者告诉小编，百度上搜索，python工资高还是java呢，得到的结果也是各有所好。有的说Python高，有的说Java高，有的又说各种编程语言的程序员如今都是企业所青睐的抢手货。真的让人看了都头疼，不知到底该相信哪一种说法了。\n那么，到底与老牌编程语言Java相比，python工资高还是java?今天我们就来看一下。\n综合多种数据资料对于编程语言的调查结果显示，Python正在超越Java。随着人工智能的火热，国家对Python的大力扶持，学习和使用Python的人越来越多。在所有参与调查的人当中，Java占比39.3%，Python占比31.7%;在参与调查的专业开发者中，Java占比38.3%，Python占比27.6%\n其次是，在同样拥有多年工作经验的前提下，Python程序员的薪资比Java程序员的薪资明显要高出很多。像北京这样的一线城市，毕业的学员，均薪都在10k以上，在有着过硬的技术基础的情况下，项目实战经验丰富，有些学员甚至能够获得20k甚至以上的薪资。其他的例如Go、Rust和Clojure等，即使他们没有很多年的工作经验，也会得到较多的报酬。然而，使用低于蓝线语言(如PHP)的开发人员，即使有多年的经验，得到的报酬也很少。\n毕竟，在大多数的依靠技术类的行业，如信息安全领域、游戏领域、社交网络领域等，Java仍是需求占比最高的技术职位，但Python的薪资增幅明显高于Java。这也明显看出，虽然人才需求量在不断增加，但是企业对于人才的技能水平要求也是不断提高。这时候，Python编程语言入手非常快，学习曲线非常低，语法简洁，功能强大，并且能够在人工智能和大数据等方面得到充分运用的优势，也难怪它会深受市场欢迎，后来者居上。\n综上所述，我们结合目前大数据和人工智能的发展状况来看，python工资高还是java?毫无疑问，作为编程语言的老牌Java还是保持佼佼者的地位，无论是市场需求还是薪资水平都名列前茅。但是随着人工智能浪潮的兴起、TensorFlow以及爬虫框架的普及以及大数据与云计算解决方案的使用，Python已经被广泛应用于机器学习、人工智能系统以及各种现代技术，Java的霸主地位已经被动摇。\n所以，到底python工资高还是java?前提还是离不开你自身先掌握到这门编程语言技术，与时俱进的去增值自己。如此，才有谈及高薪的资本，才能在急速发展的互联网时代占据一席之地!"}
{"content2":"一、计算机视觉\nDivid Marr将计算机视觉系统的开发问题归纳为3个要素：\n（1）数学理论\n考虑数学计算层面的目标及可以引入的合理约束条件。\n（2）描述和算法\n重点解决计算机视觉中的输入输出的数据格式问题，并设计合理的算法实现其系统功能。\n（3）硬件的合理使用\n使用符合算法要求的硬件并考虑该硬件对所需要的算法和描述的反作用。\n计算机视觉系统框架\n1.1 图像数据处理层\n对图像像素或者频域进行相应处理，比如图像获取、传输、压缩、降噪、装换、存储、增强和复原等。\n1.3图像识别获取层\n图像识别是指利用计算机对图像进行处理、分析和理解，以识别不同模式的目标和对象的技术，主要包括图像匹配和机器学习。\n图像匹配的研究内容大致集中在三个方面：特征空间；相似性度量；搜索策略\n机器学习：是一门人工智能的科学，该领域的主要研究的是人工智能，特别是如何在经验学习中改善具体算法的性能。\n研究如何使用计算机模拟或实现人类的学习活动。\n二、模式识别\n广义上来说，模式是供模仿用的完美无缺的标本，通常，把通过对具体的个别事物进行观察所得的具有时间和空间分布的信息称之为模式；而把模式所属的类别或同一类中模式的总体称之为模式类。模式识别是对表征事物或现象的各种形式的（数值的、文字的和逻辑关系的）信息进行处理和分析，以对事物或现象进行描述、辨认、分类和解释的过程，是信息科学和人工智能的重要组成部分。\n模式识别主要集中在两个方面，一是研究生物（包括人）是如何感知对象的，二是在给定的任务下，如何用计算机实现模式识别的理论和方法。\n人工智能是专门研究用机器人模拟人的动作、感觉和思维过程与规律的一门科学，而模式识别则是利用计算机专门对物理量及其变化过程进行描述与分类，通常用来对图像、文字、相片以及声音等信息进行处理、分类和识别。\n模式识别系统\n模式识别方法：\n1.统计模式识别\n2.结构模式识别\n3.模糊模式识别\n4.人工神经网络模式识别\n5.模板匹配模式识别\n6.支持向量机的模式识别\n三、人工智能\n美国斯坦福大学著名的人工智能研究中心尼尔逊教授这样定义人工智能“人工智能是关于知识的学科---怎样表示知识以及怎样获取知识并使用知识的学科”，另一名著名的美国MIT的winston教授认为“人工智能就是研究如何使计算机去做过去只有人才能做的智能工作”。\n人工智能的研究与应用：\n问题求解\n专家系统\n机器学习\n神经网络\n模式识别\n人工生命\n未完待续。。。。。。。。。。。。。。。。。。。。\n参考文献\n[1]许志杰,王晶,刘颖,范九伦. 计算机视觉核心技术现状与展望[J]. 西安邮电学院学报,2012,06:1-8.\n[2]王道累,陈军,吴懋亮. 计算机视觉原理分析及其应用[J]. 上海电力学院学报,2016,03:283-287.\n[3]范会敏,王浩. 模式识别方法概述[J]. 电子设计工程,2012,19:48-51.\n计算机视觉补充：\n计算机视觉与机器视觉\n计算机视觉是一门研究如何使机器“看”的科学，更进一步的说，就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉也可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学。它的最终研究目标就是使计算机能象人那样通过视觉观察和理解世界，具有自主适应环境的能力。\n机器视觉就是用机器代替人眼来做测量和判断。机器视觉系统是通过机器视觉产品(即图像摄取装置，分CMOS和CCD两种)将被摄取目标转换成图像信号，传送给专用的图像处理系统，得到被摄目标的形态信息，根据像素分布和亮度、颜色等信息，转变成数字化信号;图像系统对这些信号进行各种运算来抽取目标的特征，进而根据判别的结果来控制现场的设备动作。\n从学科分类上，二者都被认为是ArtificialIntelligence下属科目，不过计算机视觉偏软件，通过算法对图像进行识别分析，而机器视觉软硬件都包括(采集设备，光源，镜头，控制，机构，算法等)，指的是系统，更偏实际应用。简单的说，我们可以认为计算机视觉是研究“让机器怎么看”的科学，而机器视觉是研究“看了之后怎么用”的科学。\n随着硬件、算法及大数据的不断发展，整个人工智能领域面临前所未有的规模增长，也促使了国外的许多创业公司被大公司收购。\n5月，美国亚马逊公司收购了一支欧洲顶级机器视觉团队用于无人机领域研究。无独有偶，英特尔收购了俄罗斯计算机视觉公司Itseez，用于无人驾驶领域。ARM宣布以3.5亿美元收购英国嵌入式计算机视觉技术公司Apical。此前，Snapchat收购计算机视觉公司Seene;Pinterest收购视觉创业公司VisualGraph;Twitter收购基于深度学习的计算机视觉创业公司Madbits。\n同时图像识别的能力越来越强，错误率越来越低，国内也陆续爆发了大批优秀的计算机视觉(ComputerVision)创业公司。\n1、旷视科技：让机器看懂世界\n北京旷视科技有限公司成立于2012年11月，公司专注于人脸识别技术和相关产品应用研究，面向开发者提供服务，能提供一整套人脸检测、人脸识别、人脸分析以及人脸3D技术的视觉技术服务，主要通过提供云端API、离线SDK、以及面向用户的自主研发产品形式，将人脸识别技术广泛应用到互联网及移动应用场景中。\nFace++通过和众多互联网公司合作，并通过“脱敏”技术掌握到了500万张人脸图片数据库，在互联网图片人脸识别LFW的准确率达到99.6%，合作伙伴包括阿里、360等一批大型的图片、社交、设备类企业。\n2、云从科技：源自计算机视觉之父的人脸识别技术\n广州云从信息科技有限公司(简称云从科技)是一家专注于计算机视觉与人工智能的高科技企业，核心技术源于四院院士、计算机视觉之父——ThomasS.Huang黄煦涛教授。核心团队曾于2007年到2011年6次斩获智能识别世界冠军，得到上市公司佳都科技与香港杰翱资本的战略投资。\n公司主要技术团队来自中国科学院重庆分院，是中科院研发实力最雄厚的人脸识别团队，并作为中科院战略性先导科技专项的唯一人脸识别团队，代表参与了新疆喀什等地安防布控。\n3、格林深瞳：让计算机看懂世界\n格灵深瞳是一家将计算机视觉和深度学习技术应用于商业领域的科技公司，自主研发的深瞳技术在人和车的检测、跟踪与识别方面居于世界领先水平。公司借助海量数据，让计算机像人一样看懂这个世界，实时获取自然世界正在发生的一切，打造自然世界的搜索引擎。"}
{"content2":"很早的时候读比尔的《未来之路》书中回忆了最早微软对于个人电脑进入家庭的设想“每一个家庭的书桌上都有一台个人电脑，每一台电脑上都运行者微软的软件”这个伟大梦想的产生点点滴滴，当然也有对你未来网络和个人智能终端的设想。从今天来看，书中的许多成为现实，计算机技术正在以非常迅猛的姿态进入我们生活的各个角落，变得再也离不开。\n未来这一切会发展到何方，有一点可以确认的是机器正在编的越来越智能，伴随者铺天盖地的宣传苹果公司的最新手机iPhone4S走入我们的视野，iPhone4S自带了语音语音Siri，用户只需对着手机说话，Siri就可以进行智能辨别并且给予回应，可以帮助用户发短信、查路线、订餐馆、安排约会等等。人类已经或者已经无法阻止智能的机器进入我们的生活，影响我们生活，影响我们决策。今天的Siri需要在网络和苹果云支持，明天苹果会不会将我们导向到给苹果交钱的餐厅就餐，这真是一个悲剧。当然这种可能依然是人在背后起作用，当某天智能终端进入成为个人必备，机器人进入家庭，而机器越来越具有智能。电影《终结者》所描绘的场景会不会就是人类的命运。“人类开发破坏自然，自然反过来报复人类。人类制造机器人，机器人反过来统治人类”。\n作为是计算机学科的一个分支，人工智能二十世纪七十年代以来被称为世界三大尖端技术之一（空间技术、能源技术、人工智能），而现代人工智能取得突破却来自现代医学或者说现代科学对人脑研究的成果的研究。特别是更完善的神经网络模型的提出人工智能更是大大跨越了一步。从软件技术的观点，软件谁对现实的最本质模拟，而人工智能其实就是对人大脑思维活动的模拟。从这一点来说计算机人工智能技术也属于仿生学范畴，只不过仿的是人的大脑而已。生物科学已发展到这样一个阶段，使研究大脑已成为对神经科学最大的挑战。人工智能和智能机器人研究的仿生学方面——生物模式识别的研究，大脑学习记忆和思维过程的研究与模拟，也是仿生学研究的主攻方面。 而仿生学最终境界就是真实模拟，相信有一天，具有无限寿命、非凡智能、独立意志的机器人产生不会是梦想。\n为了保护人类，早在1940年科幻作家阿西莫夫就提出了“机器人三原则”，阿西莫夫也因此获得“机器人学之父”的桂冠。机器人三原则理论提出的半个世纪以来，不断的被科幻作家和导演使用。但有趣的是，凡是出现三原则的电影和小说里，机器人几乎都违反了本条例。这个其实支出一个现实的问题，安全永远不能指望协议和规则，或者施舍，安全来自自身实力和威慑。机器的进化总是在递增前进，人类的在短暂的寿命却只是不断的轮回。人类的未来在何方？"}
{"content2":"CSDN的博主poson在他的博文《机器学习的最优化问题》中指出“机器学习中的大多数问题可以归结为最优化问题”。我对机器学习的各种方法了解得不够全面，本文试图从凸优化的角度说起，简单介绍其基本理论和在机器学习算法中的应用。\n1.动机和目的\n人在面临选择的时候重视希望自己能够做出“最好”的选择，如果把它抽象成一个数学问题，那么“最好的选择”就是这个问题的最优解。优化问题，就是把你考虑的各个因素表示成为一组函数（代价函数），解决这个问题就是在一集备选解中选择最好的解。\n那么，为什么我们要讨论凸优化而不是一般的优化问题呢？那时因为凸优化问题具有很好的性质——局部最优就是全局最优，这一特性让我们能够迅速有效的求解问题。（实际上就是太一般的优化问题讨论不来）\n2.凸优化的定义\n首先明确两个定义:\n(1) 如果中任意两点之间的线段任在中，那么集合被称为凸集。即对任意和满足的都有\n(2) 函数是凸函数，则是凸集，且对于任意在任下有\nStephen Boyd在他的《convex optimization》中定义凸优化问题是形如\n的问题，其中为凸函数。也就是说，凸优化问题是指需要最小化的函数（代价函数）是凸函数，而且定义域为凸集的问题。\n3.凸优化问题的一般求解方法\n有些凸优化问题比较简单，是可以直接求解的，譬如二次规划，这里不做说明。求解凸优化问题，就要利用该问题的“凸”性——只要我一直朝着代价函数减小的方向去，那么我一定不会走错！这就是下降方法的基本思想。\n《convex optimization》这本书中，将凸优化问题分为无约束优化、等式约束优化和不等式约束优化分别介绍了其算法，然其本质并无区别。下降方法即产生一优化点列其中\n并且。此处表示迭代的步长（比例因子），表示的是搜索方向（搜索步径）。下降方法指只要不是最优点，成立。  以下内容均来自Stephen Boyd的《convex optimization》及其中文译本。\n搜索步径\n一旦确定了搜索方向，那么我们可以通过求解得到搜索步径，当求解该问题成本较低时，可以采用该方法。该方法称为精确直线搜索。\n然而实践中一般采用非精确直线搜索方法，譬如回溯直线搜索。算法如下图：\n下降方向\n在各个领域都广为应用的LMS算法也称为随机梯度算法（LMS算法和这里算法的区别和联系应该会另写一篇）。用负梯度作为下降的方向是一种和自然的选择，此外还有Newton方法。而最速下降方法是定义出的在某一特定范数下的方法。梯度下降和Netwon方法分别是二次范数和Hessian范数下的最速下降方法。算法的收敛性和Hessian矩阵有关，此处不详细说明。\n等式约束\n对于标准的凸优化问题，等式约束是仿射的，这也就意味着该优化问题的定义域是一个向量子空间。一个自然的想法是在这个空间内进行下降，这种想法被证明是可行的。根据初始迭代点的兴致，可以分为两类。\n(1)初始点可行：在可行域内迭代\n(2)初始点不可行：迭代过程中逐步靠近可行域\n不等式约束\n如果我们不能解决一个问题，那么就消除这个问题。\n采用示性函数可以将不等式约束隐含在代价函数中，这里带来的问题是——代价函数非凸。障碍方法被引入以解决这个问题。（内点法）这样，不等式约束就变成了等式约束或是无约束的情况了。\n如果，我不知道该怎么选择搜索方向？\n既然真的不知道，那就找一套合适的规则，避开选择方向这个问题吧！\n——坐标下降法\n坐标下降法如下所示（可参考维基百科）\n坐标下降方法是一种下降方法，但是和梯度下降不同，坐标下降法采用一维搜索，也就是说在每次迭代过程中，下降方向都是平行与坐标轴的。由于下降方向是确定的，因此坐标下降方法并不涉及到寻找搜索方向这一过程。迭代过程图如下所示：\n4.KKT条件\n面临一个凸优化问题，直接采用下降方法是一个不明智的选择——很有可能你还在迭代，别人已经把结果求出来了。或者，别人把原问题转换成为一个更容易求得的问题。KKT条件是最优点需要满足的条件,如下所示\n前两个条件是约束给出的，后三个条件涉及到（拉格朗日）对偶函数。对偶函数定义了最优值得下界。\n定义对偶问题的最优解为，原问题的最优解为，如果，则强对偶性成立。这个时候对偶函数才起到了左右。（要不然求个下界没什么用处）当凸优化问题满足Slater条件时，强对偶性是成立的。\n由此可以导出KKT条件的后三个式子——不等式约束Lagrange乘子大于等于0，强对偶性成立，对偶函数梯度为0。\n5.机器学习算法举例\n支持向量机（SVM）\n对于线性可分的两类而言，SVM的目的是找出最优的分离面。这个最优的判断准则是和点的距离最远。这个问题可以表示为如下形式\nSVM算法火了很多很多年了，博客JerryLead里用5篇写了SVM的基本方法和理论，可以去看他的。支持向量机中涉及到了KKT条件和Slater约束（实际上更准确来说求解的是对偶问题的解），以及和坐标下降法有一定关系的SMO算法。\n主分量分析（PCA）\n主分量分析是无监督学习。主分量分析是统计模式识别和信号处理中进行数据压缩的一种标准方法。\n特征选择的过程中，理论上“数据空间”到“特征空间”这一个线性变化的过程不会改变数据的维数。在需要对数据进行维数压缩的情况下，我们希望截断x后，在均方误差意义下最优。principal components 的意思就是，将数据投影到特征空间后，留下的分量是最主要的。\n主成分中的“主要”指的是含有原信号的最多信息，信息在这里采用的是方差来描述。（信息熵难以计算，而方差作为二阶统计信息，在一定程度上可以表示其包含的信息）PCA中采用了一组正交基来表示，所以各个向量是两两正交的，并且方差和向量的范数有关，所以限定方向向量的范数为1（采用欧几里德范数）。在以上假设下，求解凸优化问题就能得到PCA的解析解。\n在以上假设下，首先考虑输出是一维的情况，假设输入为且均值为0，输出满足\n那么输出的方差表示为\n那么，优化问题可表示为\n显然，这是一个凸优化问题，利用KKT条件有\n这就意味着，当取对应的最大特征值的特征向量时，输出具有最大的方差。同理，当输出为多维时，可以采用数学归纳法求得各个分量。对应第分量，需要求解的优化问题是\n易知第n分量对应的向量为第n大特征值的特征向量，这里仅用了KKT条件就求得了对输入进行主分量分析的方法。\n6.总结\n前段时间我问自己一个问题“人是如何确定世界最高峰的”，这显然不是一个凸优化问题。或许在最初的时候，生活在平原上的人们发现了一处高地，他们爬了上去，认为这就是“世界最高峰”。\n但很遗憾的是，这并不是一个凸优化问题。后来，人们走到了更多的地方，发现有更高的山，不断的修改自己的认知。世界是非凸的，没有那么美妙的性质可以利用，我们必须不断修正自己的认知，扩展自己的见识，才能站在更高的地方，领略不一样的风景。"}
{"content2":"下面是些泛泛的基础知识，但是真正搞机器学习的话，还是非常有用。像推荐系统、DSP等目前项目上机器学习的应用的关键，我认为数据处理非常非常重要，因为很多情况下，机器学习的算法是有前提条件的，对数据是有要求的。\n机器学习强调三个关键词：算法、经验、性能，其处理过程如下图所示。\n上图表明机器学习是数据通过算法构建出模型并对模型进行评估，评估的性能如果达到要求就拿这个模型来测试其他的数据，如果达不到要求就要调整算法来重新建立模型，再次进行评估，如此循环往复，最终获得满意的经验来处理其他的数据。\n1.2 机器学习的分类\n1.2.1 监督学习\n监督是从给定的训练数据集中学习一个函数（模型），当新的数据到来时，可以根据这个函数（模型）预测结果。监督学习的训练集要求包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注（标量）的。在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”、“非垃圾邮件”，对手写数字识别中的“1”、“2”、“3”等。在建立预测模型时，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断调整预测模型，直到模型的预测结果达到一个预期的准确率。常见的监督学习算法包括回归分析和统计分类：\nl  二元分类是机器学习要解决的基本问题，将测试数据分成两个类，如垃圾邮件的判别、房贷是否允许等问题的判断。\nl  多元分类是二元分类的逻辑延伸。例如，在因特网的流分类的情况下，根据问题的分类，网页可以被归类为体育、新闻、技术等，依此类推。\n监督学习常常用于分类，因为目标往往是让计算机去学习我们已经创建好的分类系统。数字识别再一次成为分类学习的常见样本。一般来说，对于那些有用的分类系统和容易判断的分类系统，分类学习都适用。\n监督学习是训练神经网络和决策树的最常见技术。神经网络和决策树技术高度依赖于事先确定的分类系统给出的信息。对于神经网络来说，分类系统用于判断网络的错误，然后调整网络去适应它；对于决策树，分类系统用来判断哪些属性提供了最多的信息，如此一来可以用它解决分类系统的问题。\n1.2.2 无监督学习\n与监督学习相比，无监督学习的训练集没有人为标注的结果。在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法和k-Means算法。这类学习类型的目标不是让效用函数最大化，而是找到训练数据中的近似点。聚类常常能发现那些与假设匹配的相当好的直观分类，例如基于人口统计的聚合个体可能会在一个群体中形成一个富有的聚合，以及其他的贫穷的聚合。\n非监督学习看起来非常困难：目标是我们不告诉计算机怎么做，而是让它（计算机）自己去学习怎样做一些事情。非监督学习一般有两种思路：第一种思路是在指导Agent时不为其指定明确的分类，而是在成功时采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是产生一个分类系统，而是做出最大回报的决定。这种思路很好地概括了现实世界，Agent可以对那些正确的行为做出激励，并对其他的行为进行处罚。\n因为无监督学习假定没有事先分类的样本，这在一些情况下会非常强大，例如，我们的分类方法可能并非最佳选择。在这方面一个突出的例子是Backgammon（西洋双陆棋）游戏，有一系列计算机程序（例如neuro-gammon和TD-gammon）通过非监督学习自己一遍又一遍地玩这个游戏，变得比最强的人类棋手还要出色。这些程序发现的一些原则甚至令双陆棋专家都感到惊讶，并且它们比那些使用预分类样本训练的双陆棋程序工作得更出色。\n1.2.3 半监督学习\n半监督学习（Semi-supervised Learning）是介于监督学习与无监督学习之间一种机器学习方式，是模式识别和机器学习领域研究的重点问题。它主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。半监督学习对于减少标注代价，提高学习机器性能具有非常重大的实际意义。主要算法有五类：基于概率的算法；在现有监督算法基础上进行修改的方法；直接依赖于聚类假设的方法等，在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理地组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测，如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。\n半监督学习分类算法提出的时间比较短，还有许多方面没有更深入的研究。半监督学习从诞生以来，主要用于处理人工合成数据，无噪声干扰的样本数据是当前大部分半监督学习方法使用的数据，而在实际生活中用到的数据却大部分不是无干扰的，通常都比较难以得到纯样本数据。\n1.2.4 强化学习\n强化学习通过观察来学习动作的完成，每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻做出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning 以及时间差学习（Temporal difference learning）。\n在企业数据应用的场景下，人们最常用的可能就是监督式学习和非监督式学习的模型。在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据，目前半监督式学习是一个很热的话题。而强化学习更多地应用在机器人控制及其他需要进行系统控制的领域。\n1.3 机器学习的常见算法\n常见的机器学习算法有：\nl  构造条件概率：回归分析和统计分类；\nl  人工神经网络；\nl  决策树；\nl  高斯过程回归；\nl  线性判别分析；\nl  最近邻居法；\nl  感知器；\nl  径向基函数核；\nl  支持向量机；\nl  通过再生模型构造概率密度函数；\nl  最大期望算法；\nl  graphical model：包括贝叶斯网和Markov随机场；\nl  Generative Topographic Mapping；\nl  近似推断技术；\nl  马尔可夫链蒙特卡罗方法；\nl  变分法；\nl  最优化：大多数以上方法，直接或者间接使用最优化算法。\n根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题，下面用一些相对比较容易理解的方式来解析一些主要的机器学习算法：\n1.3.1 回归算法\n回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。\n1.3.2 基于实例的算法\n基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor (KNN)，、学习矢量量化（Learning Vector Quantization， LVQ）以及自组织映射算法（Self-Organizing Map，SOM）\n1.3.3 正则化方法\n正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression、Least Absolute Shrinkage and Selection Operator（LASSO）以及弹性网络（Elastic Net）。\n1.3.4 决策树学习\n决策树算法根据数据的属性采用树状结构建立决策模型，决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）、 ID3 (Iterative Dichotomiser 3)、C4.5、Chi-squared Automatic Interaction Detection (CHAID)、Decision Stump、机森林（Random Forest）、多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine，GBM）。\n1.3.5 贝叶斯学习\n贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法、平均单依赖估计（Averaged One-Dependence Estimators， AODE）以及 Bayesian Belief Network（BBN）。\n1.3.6 基于核的算法\n基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易解决。常见的基于核的算法包括：支持向量机（Support Vector Machine，SVM）、径向基函数（Radial Basis Function，RBF)以及线性判别分析（Linear Discriminate Analysis，LDA)等。\n1.3.7 聚类算法\n聚类就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所有的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means 算法以及期望最大化算法（Expectation Maximization，EM）。\n1.3.8 关联规则学习\n关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori 算法和 Eclat 算法等。\n1.3.9 人工神经网络算法\n人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法（其中深度学习就是其中的一类算法，我们会单独讨论）。重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）、反向传递（Back Propagation）、Hopfield 网络、自组织映射（Self-Organizing Map, SOM）、学习矢量量化（Learning Vector Quantization，LVQ）。\n1.3.10 深度学习算法\n深度学习算法是对人工神经网络的发展，在近期赢得了很多关注，特别是百度也开始发力深度学习后，更是在国内引起了很多关注。在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）、 Deep Belief Networks（DBN）、卷积网络（Convolutional Network）、堆栈式自动编码器（Stacked Auto-encoders）。\n1.3.11 降低维度算法\n像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式，试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis，PCA）、偏最小二乘回归（Partial Least Square Regression，PLS）、 Sammon 映射、多维尺度（Multi-Dimensional Scaling, MDS）、投影追踪（Projection Pursuit）等。\n1.3.12 集成算法\n集成算法用一些相对较弱的学习模型独立地对同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting、Bootstrapped Aggregation（Bagging）、AdaBoost、堆叠泛化（Stacked Generalization， Blending）、梯度推进机（Gradient Boosting Machine, GBM）、随机森林（Random Forest）。"}
{"content2":"先来介绍一下自己\n中南大学（不知名985双一流 A 类）大二计算机专业本科生，才学编程1年多一点。大一的时候搞了大半年 ACM，现在慢慢转向项目开发（在学习 JAVA 开发，U3D 和 C#），同时在学习机器学习、大数据等内容，混进了学校的图灵班。\n下面进入正题\n以后是读研还是直接工作？这个问题可能困扰了很多读计算机类的本科生。大一刚入学的时候，我的想法是不读研，之后直接去工作，为什么呢？我当时报计算机专业的时候想的是以后要做游戏开发，那读不读研完全是不重要了，搞好技术，自然就行。今年暑假的时候，去表哥的公司呆了一暑假，当时放弃了 ACM 暑假集训（相当于退队了，寒假开始可能会回去接着搞），虽然还是有些后悔，但是也有了蛮多收获，一个是见识稍微广了些，想的更远了，二是敢于去接触一些看起来很高大上的领域（如人工智能等）。开学后，写了些关于机器学习方面的程序，去听了一些学术研讨会，慢慢开始有了读研的想法，所以纠结了好一阵子，我觉的我纠结的更大一部分原因就是认识面小，不知道在 BAT 这些大公司里工作是什么一种感受，感觉只有出去实习几次，才能更好做出抉择。但这段时间跟一些大神级的前辈交流之后，想的是技术方面接着搞（下大功夫搞），学业方面还是稍微抓一抓。\n上面一段话讲的还是我这一年来的一些经历，下面就讲讲我的看法。一般来说，对于刚进入大一的同学来说，你们可能还没考虑过自己以后的发展路线，那么这时候可以边走边看，先把成绩搞起来（简单地说就是刷绩点），边学习边思考、规划未来，而不是想着以后是读研还是直接工作，这个问题等你们到了真正需要选择的那天再考虑，什么是真正需要选择的时候呢？就是你们既有保研名额，又有BAT的offer的时候，到这时候再考虑也不迟，这时候你们可以根据自己的兴趣来选，如果是对科研方面更感兴趣，当然是保研无疑了，但对开发更有兴趣，那直接工作可能会更好，毕竟读研是搞研究去了，开发方面的东西研究生学不到什么。表示有些后悔大一的时候直接把绩点丢了。对于大二大三的同学来说，目前成绩能拿到保研名额或是争取一下还是有希望拿到的，最好要拿到，多一个选择总没错。而对于保研大体上没戏的，如果你决定要考研，那自然没什么话讲，当然是去考了。那不准备考研的，自然就是选择直接工作了（除了那些现在还是成天浑浑噩噩的同学），那该怎么做？我没经历过，自然不太懂该怎么做，但我也听了许多在 BAT 等大公司工作的学长学姐的分享，我还是谈谈自己的看法吧。（以下看法对读研的同学来说也会适用）\n1.简历\n毫无疑问，有一份好的简历，更容易取得面试资格（好像有时候想去某实验室学习也是需要投简历的），并且，需要从现在就开始准备简历，好的简历不是一时半刻就能写出来的，都是经过长时间的修改加工才能做出来的。\n2.平时敲代码！多敲代码！！\n为什么这么说呢？现在放眼望去，学习编程的人不在少数，你要如何与他们竞争呢？一是学历，这点我们不能否认，二是自身的硬实力，而这硬实力是怎么练出来的？就是平时的积累，多敲代码，但我们要做的不应该是所谓的码农，因此我们还需要多思考，多学习。我始终相信一句话，积少成多，方能成大器。\n3.敢于去接触一些看起来挺高深的东西\n我现在是准备去学校的一个实验室学习有关机器学习、大数据这些方面的内容。对要准备读研的同学来说这点是肯定的了，而对于本科毕业就准备出去工作的同学，想想几年后，自己相比于读研的同学，优势在哪里?绝大部分只是经验，而这经验等他们工作几年后也算不上是优势了，时代在发展，我们要学的东西越来越多，不想被淘汰，就需要去接触这些，撸起袖子就是干，不管会不会，不学肯定不会。\n4.多关注关注外面\n平时多参加些比赛，拿不拿奖不重要，关键是要出去见识见识，见的多了就会有更好的想法。还多与一些厉害的学长学姐交流，有时他们的一些经历或是想法是能让自己更清楚地知道接下来该怎么走的。还有就是，关注前沿的技术，不断学习才能更加强大。\n5.精一门，懂多门\n就是编程涉及的各方面都涉及了解一下，见识的多对自己未来的发展没有坏处，但得要有自己一个专精的地方，得靠这个吃饭。读研的自然也一样，你们研究的方向自然就是精的那门，那只满足与这一门就够了吗？要知道，每一门技术都不是孤立存在的，一定是与其他的技术有关联，涉猎的范围广了，对于一个问题的分析会更加透彻，想的也会更远。像我去年的时候，想的就是做个游戏开发，但接触了一段时间机器学习之后，我想法就稍微变了，现在想的是以后做的是游戏中 AI 的开发（个人认为，一个优秀的游戏少不了好的 AI ）。总结一下就是学归学，总的方向肯定不变，学了只是为了更好地实现自己地目标。\n最后我想说直接工作并不代表以后你就比不上读研的同学，说实话，我们得承认读三年研究生能学到不少东西，但是工作三年期间，不代表你不能学他们学的，不代表几年后你就比不过他们\n以上就是我目前的一些看法，如有说的不好的地方，请多多包涵，同时希望各位大神、前辈们能给些学习上的建议。"}
{"content2":"机器学习十大算法之一：EM算法。能评得上十大之一，让人听起来觉得挺NB的。什么是NB啊，我们一般说某个人很NB，是因为他能解决一些别人解决不了的问题。神为什么是神，因为神能做很多人做不了的事。那么EM算法能解决什么问题呢？或者说EM算法是因为什么而来到这个世界上，还吸引了那么多世人的目光。\n我希望自己能通俗地把它理解或者说明白，但是，EM这个问题感觉真的不太好用通俗的语言去说明白，因为它很简单，又很复杂。简单在于它的思想，简单在于其仅包含了两个步骤就能完成强大的功能，复杂在于它的数学推理涉及到比较繁杂的概率公式等。如果只讲简单的，就丢失了EM算法的精髓，如果只讲数学推理，又过于枯燥和生涩，但另一方面，想把两者结合起来也不是件容易的事。所以，我也没法期待我能把它讲得怎样。希望各位不吝指导。\n一、最大似然\n扯了太多，得入正题了。假设我们遇到的是下面这样的问题：\n假设我们需要调查我们学校的男生和女生的身高分布。你怎么做啊？你说那么多人不可能一个一个去问吧，肯定是抽样了。假设你在校园里随便地活捉了100个男生和100个女生。他们共200个人（也就是200个身高的样本数据，为了方便表示，下面，我说“人”的意思就是对应的身高）都在教室里面了。那下一步怎么办啊？你开始喊：“男的左边，女的右边，其他的站中间！”。然后你就先统计抽样得到的100个男生的身高。假设他们的身高是服从高斯分布的。但是这个分布的均值u和方差∂2我们不知道，这两个参数就是我们要估计的。记作θ=[u, ∂]T。\n用数学的语言来说就是：在学校那么多男生（身高）中，我们独立地按照概率密度p(x|θ)抽取100了个（身高），组成样本集X，我们想通过样本集X来估计出未知参数θ。这里概率密度p(x|θ)我们知道了是高斯分布N(u,∂)的形式，其中的未知参数是θ=[u, ∂]T。抽到的样本集是X={x1,x2,…,xN}，其中xi表示抽到的第i个人的身高，这里N就是100，表示抽到的样本个数。\n由于每个样本都是独立地从p(x|θ)中抽取的，换句话说这100个男生中的任何一个，都是我随便捉的，从我的角度来看这些男生之间是没有关系的。那么，我从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？因为这些男生（的身高）是服从同一个高斯分布p(x|θ)的。那么我抽到男生A（的身高）的概率是p(xA|θ)，抽到男生B的概率是p(xB|θ)，那因为他们是独立的，所以很明显，我同时抽到男生A和男生B的概率是p(xA|θ)*  p(xB|θ)，同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。用数学家的口吻说就是从分布是p(x|θ)的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：\n这个概率反映了，在概率密度函数的参数是θ时，得到X这组样本的概率。因为这里X是已知的，也就是说我抽取到的这100个人的身高可以测出来，也就是已知的了。而θ是未知了，则上面这个公式只有θ是未知数，所以它是θ的函数。这个函数放映的是在不同的参数θ取值下，取得当前这个样本集的可能性，因此称为参数θ相对于样本集X的似然函数（likehood function）。记为L(θ)。\n这里出现了一个概念，似然函数。还记得我们的目标吗？我们需要在已经抽到这一组样本X的条件下，估计参数θ的值。怎么估计呢？似然函数有啥用呢？那咱们先来了解下似然的概念。\n直接举个例子：\n某位同学与一位猎人一起外出打猎，一只野兔从前方窜过。只听一声枪响，野兔应声到下，如果要你推测，这一发命中的子弹是谁打的？你就会想，只发一枪便打中，由于猎人命中的概率一般大于这位同学命中的概率，看来这一枪是猎人射中的。\n这个例子所作的推断就体现了极大似然法的基本思想。\n再例如：下课了，一群男女同学分别去厕所了。然后，你闲着无聊，想知道课间是男生上厕所的人多还是女生上厕所的人比较多，然后你就跑去蹲在男厕和女厕的门口。蹲了五分钟，突然一个美女走出来，你狂喜，跑过来告诉我，课间女生上厕所的人比较多，你要不相信你可以进去数数。呵呵，我才没那么蠢跑进去数呢，到时还不得上头条。我问你是怎么知道的。你说：“5分钟了，出来的是女生，女生啊，那么女生出来的概率肯定是最大的了，或者说比男生要大，那么女厕所的人肯定比男厕所的人多”。看到了没，你已经运用最大似然估计了。你通过观察到女生先出来，那么什么情况下，女生会先出来呢？肯定是女生出来的概率最大的时候了，那什么时候女生出来的概率最大啊，那肯定是女厕所比男厕所多人的时候了，这个就是你估计到的参数了。\n从上面这两个例子，你得到了什么结论？\n回到男生身高那个例子。在学校那么男生中，我一抽就抽到这100个男生（表示身高），而不是其他人，那是不是表示在整个学校中，这100个人（的身高）出现的概率最大啊。那么这个概率怎么表示？哦，就是上面那个似然函数L(θ)。所以，我们就只需要找到一个参数θ，其对应的似然函数L(θ)最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做θ的最大似然估计量，记为：\n有时，可以看到L(θ)是连乘的，所以为了便于分析，还可以定义对数似然函数，将其变成连加的：\n好了，现在我们知道了，要求θ，只需要使θ的似然函数L(θ)极大化，然后极大值对应的θ就是我们的估计。这里就回到了求最值的问题了。怎么求一个函数的最值？当然是求导，然后让导数为0，那么解这个方程得到的θ就是了（当然，前提是函数L(θ)连续可微）。那如果θ是包含多个参数的向量那怎么处理啊？当然是求L(θ)对所有参数的偏导数，也就是梯度了，那么n个未知的参数，就有n个方程，方程组的解就是似然函数的极值点了，当然就得到这n个参数了。\n最大似然估计你可以把它看作是一个反推。多数情况下我们是根据已知条件来推算结果，而最大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。比如，如果其他条件一定的话，抽烟者发生肺癌的危险时不抽烟者的5倍，那么如果现在我已经知道有个人是肺癌，我想问你这个人抽烟还是不抽烟。你怎么判断？你可能对这个人一无所知，你所知道的只有一件事，那就是抽烟更容易发生肺癌，那么你会猜测这个人不抽烟吗？我相信你更有可能会说，这个人抽烟。为什么？这就是“最大可能”，我只能说他“最有可能”是抽烟的，“他是抽烟的”这一估计值才是“最有可能”得到“肺癌”这样的结果。这就是最大似然估计。\n好了，极大似然估计就讲到这，总结一下：\n极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。"}
{"content2":"机器学习中的监督学习和无监督学习\n说在前面\n最近的我一直在寻找实习机会，很多公司给了我第一次电话面试的机会，就没有下文了。不管是HR姐姐还是第一轮的电话面试，公司员工的态度和耐心都很值得点赞，我也非常感激他们。但是我都没有进入下一轮面试的机会，一路想想我的简历和学习经历，确实也挺难有进入第二轮面试的机会的，因为我大学里学习的知识和技能除了一些常用算法就再也没别的了，参加过几场ACM/ICPC并获得过几个小奖，没有什么项目经验和扎实的语言基础，可想而知我得弱到什么程度去了。\n前几天还是很认真的想了一下，计划A就是再看看有没有规模小一些的合口味的公司，如果有机会暑假也可以出去实习一段时间，如果真没有的话 ，我也不会灰心丧气，执行我的计划B。我的计划B就是在校再学习一段时间，争取在校招的时候有个好结果。目前我就决定好好学习一些机器学习和爬虫的知识，把理论知识搞扎实了，争取到时候也有勇气投递一下搜索方面和算法工程师的职位。\n正文\n机器学习主要分为有监督学习和无监督学习两种。接下来我详细的给大家介绍一下这两种方法的概念和区别。\n监督学习（supervised learning）：通过已有的训练样本（即已知数据以及其对应的输出）来训练，从而得到一个最优模型，再利用这个模型将所有新的数据样本映射为相应的输出结果，对输出结果进行简单的判断从而实现分类的目的，那么这个最优模型也就具有了对未知数据进行分类的能力。在社会中，我们在很小的时候就被大人教授这是鸟啊，那是猪啊，这个是西瓜、南瓜，这个可以吃、那个不能吃啊之类的，我们眼里见到的这些景物食物就是机器学习中的输入，大人们告诉我们的结果就是输出，久而久之，当我们见的多了，大人们说的多了，我们脑中就会形成一个抽象的模型，下次在没有大人提醒的时候看见别墅或者洋楼，我们也能辨别出来这是房子，不能吃，房子本身也不能飞等信息。上学的时候，老师教认字、数学公式啊、英语单词等等，我们在下次碰到的时候，也能区分开并识别它们。这就是监督学习，它在我们生活中无处不在。\n无监督学习（unsupervised learning）：我们事先没有任何训练数据样本，需要直接对数据进行建模。比如我们去参观一个画展，我们对艺术一无所知，但是欣赏完很多幅作品之后，我们面对一幅新的作品之后，至少可以知道这幅作品是什么派别的吧，比如更抽象一些还是更写实一点，虽然不能很清楚的了解这幅画的含义，但是至少我们可以把它分为哪一类。再比如我们在电影院看电影，对于之前没有学过相关电影艺术知识的我们，可能不知道什么是一部好电影，什么是一部不好的电影，可是在观看了很多部电影之后，我们脑中对电影就有了一个潜在的认识，当我们再次坐在电影院认真观看新上映的电影时，脑中就会对这部电影产生一个评价：怎么这电影这么不好啊，整个故事线是混乱的，一点也不清晰，比我之前看过的那些电影差远了，人物的性格也没有表现出来，关键是电影主题还搞偏了；哎呀，这个电影拍得确实好啊，故事情节和人物性格都很鲜明，而且场景很逼真，主角的实力表演加上他与生俱来的忧郁眼神一下把人物演活了。\n再给大家举一个无监督学习的例子。远古时期，我们的祖先打猎吃肉，他们本身之前是没有经验而言的，当有人用很粗的石头去割动物的皮的时候，发现很难把皮隔开，但是又有人用很薄的石头去割，发现比别人更加容易的隔开动物的毛皮，于是，第二天、第三天、……，他们就知道了需要寻找比较薄的石头片来割。这些就是无监督学习的思想，外界没有经验和训练数据样本提供给它们，完全靠自己摸索。\n总结\n本次计划比较系统的学习机器学习理论知识了，当然也会比较完整的把我所学到的这些知识分享给大家。回想刚才所提到的监督学习和无监督学习两种方法，或许很多人都会认为任何事情有人教当然很好了啊，所有监督学习更方便快捷嘛，大部分情况确实这样，但是如果有些情况比如无法提供训练数据样本或者提供训练数据样本的成本太高的话，或许我们就应该采取无监督学习的策略了。监督学习的典型例子就是决策树、神经网络以及疾病监测，而无监督学习就是很早之前的西洋双陆棋和聚类。\n监督学习和无监督学习的更具体例子我会在后面学习的过程中给大家总结出来。另外，如果大家有比较好的机器学习的资源，也很感谢您的留言。"}
{"content2":"矩阵\n参考: 机器学习基础\n一般而言，一个对象应该被视为完整的个体，表现实中有意义的事物，不能轻易拆分。\n对象是被特征化的客观事物，而表（或矩阵）是容纳这些对象的容器。换句话说，对象是表中的元素，表是对象的集合（表中的每个对象都有相同的特征和维度，对象对于每个特征都有一定的取值）。\n分类或聚类可以看作根据对象特征的相似性与差异性，对矩阵空间的一种划分。\n预测或回归可以看作根据对象在某种序列（时间）上的相关性，表现为特征取值变化的一种趋势。\nimport numpy as np\na = np.arange(9).reshape((3, -1)) a\narray([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\nb = a.copy()\nid(a) == id(b)\nFalse\nrepr(a) == repr(b)\nTrue\nLinalg\nA = np.mat([[1, 2, 4, 5, 7], [9,12 ,11, 8, 2], [6, 4, 3, 2, 1], [9, 1, 3, 4, 5], [0, 2, 3, 4 ,1]])\n行列式\nnp.linalg.det(A)\n-812.00000000000068\n逆\nnp.linalg.inv(A)\nmatrix([[ -7.14285714e-02, -1.23152709e-02, 5.29556650e-02, 9.60591133e-02, -8.62068966e-03], [ 2.14285714e-01, -3.76847291e-01, 1.22044335e+00, -4.60591133e-01, 3.36206897e-01], [ -2.14285714e-01, 8.25123153e-01, -2.04802956e+00, 5.64039409e-01, -9.22413793e-01], [ 5.11521867e-17, -4.13793103e-01, 8.79310345e-01, -1.72413793e-01, 8.10344828e-01], [ 2.14285714e-01, -6.65024631e-02, 1.85960591e-01, -8.12807882e-02, -1.46551724e-01]])\n转置\nA.T\nmatrix([[ 1, 9, 6, 9, 0], [ 2, 12, 4, 1, 2], [ 4, 11, 3, 3, 3], [ 5, 8, 2, 4, 4], [ 7, 2, 1, 5, 1]])\nA * A.T\nmatrix([[ 95, 131, 43, 78, 43], [131, 414, 153, 168, 91], [ 43, 153, 66, 80, 26], [ 78, 168, 80, 132, 32], [ 43, 91, 26, 32, 30]])\n秩\nnp.linalg.matrix_rank(A)\n5\n解方程\n\\[Ax = b\\]\nb = [1, 0, 1, 0, 1] S = np.linalg.solve(A, b) S\narray([-0.0270936 , 1.77093596, -3.18472906, 1.68965517, 0.25369458])\n现代数学三大基石：\n概率论说明了事物可能会是什么样；\n数值分析揭示了它们为什么这样，以及如何变成这样；\n线性代数告诉我们事物从来不只有一个样子，使我们能够从多个角度来观察事物。\n相似性度量\n闵可夫斯基 Minkowski() 距离对应于 \\(||\\cdot||_p\\), 即\n\\begin{aligned}\nd = ||x_1-x_2||_p & & x_1, x_2 \\in \\mathbb{R}^n\n\\end{aligned}\n曼哈顿距离 (Manhattan) : \\(p=1\\), 又称为城市街区距离 (City Block distance)\n切比雪夫 (Chebyshev) 距离: \\(p=∞\\), 可用来计算象棋走的步数.\n比较常见是范数（如欧式距离（\\(L_2\\)）、曼哈顿距离（\\(L_1\\)）、切比雪夫距离（\\(L_{\\infty}\\)））和夹角余弦。下面我主要说明一下其他的几个比较有意思的度量：\n汉明距离（Hamming）\n定义：两个等长字符串 s1 与 s2 之间的汉明距离定义为将其中一个变成另外一个所需要的最小替换次数。(对应于 \\(||\\cdot||_0\\))\n应用：信息编码（为了增强容错性，应该使得编码间的最小汉明距离尽可能大）。\nA = np.mat([[1, 1, 0, 1, 0, 1, 0, 0, 1], [0, 1, 1, 0, 0, 0, 1, 1, 1]]) smstr = np.nonzero(A[0] - A[1])\nA[0] - A[1]\nmatrix([[ 1, 0, -1, 1, 0, 1, -1, -1, 0]])\nsmstr\n(array([0, 0, 0, 0, 0, 0], dtype=int64), array([0, 2, 3, 5, 6, 7], dtype=int64))\nd = smstr[0].shape[0] d\n6\n杰卡德相似系数（Jaccard Similarity Coefficient）\n相似度：\n\\[ J(A, B) = \\frac{|\\;A \\bigcap B\\;|}{|\\;A \\bigcup B\\;|} \\]\n杰卡德距离（Jaccard Distance）\n区分度：\n\\[ J_{\\delta}(A, B) = 1 - J(A, B) = 1 - \\frac{|\\;A \\bigcap B\\;|}{|\\;A \\bigcup B\\;|} \\]\n应用：\n样本 \\(A\\) 和样本 \\(B\\) 所有维度的取值为 \\(0\\) 或 \\(1\\)，表示包含某个元素与否。\nimport scipy.spatial.distance as dist\nA\nmatrix([[1, 1, 0, 1, 0, 1, 0, 0, 1], [0, 1, 1, 0, 0, 0, 1, 1, 1]])\ndist.pdist(A, 'jaccard')\narray([ 0.75])\n蝴蝶效应（洛伦兹动力学方程）：确定性与随机性相统一\n系统未来的所有运动都被限制在一个明确的范围之内——确定性；\n运动轨迹变化缠绕的规则是随机性的，任何时候你都无法准确判定下一次运动的轨迹将落在「蝴蝶」的哪侧翅膀上的哪个点上——随机性。\n总而言之，系统运动大的范围是确定的、可测的，但是运动的细节是随机的、不可测的。\n从统计学角度来看，蝴蝶效应说明了：\n样本总体（特征向量或对象）的取值范围一般是确定的，所有样本对象（包括已经存在的和未出现的）的取值都位于此空间内；\n无论收集再多的样本对象，也不能使这种随机性降低或消失。\n随机性是事物的一种根本的、内在的、无法根除的性质，也是一切事物（概率）的本质属性。\n衡量事物运动的随机性，必须从整体而不是局部来认知事物，因为从每个局部，事物可能看起来都是不同的（或相同的）。\n概率论便是度量随机性的一个工具。一般地，上述所说的矩阵，被称为设计矩阵，基本概念重写：\n样本（样本点）：原指随机试验的一个结果，可以理解为设计矩阵中的一个对象，如苹果、小猪等。\n样本空间：原指随机试验所有可能结果的集合，可以理解为矩阵的所有对象，引申为对象特征的取值范围：\\(10\\) 个苹果，\\(2\\) 只小猪。\n随机事件：原指样本空间的一个子集，可以理解为某个分类，它实际指向一种概率分布：苹果为红色，小猪为白色\n随机变量：可以理解为指向某个事件的一个变量：\\(X\\{x_i = \\text{黄色}\\}\\)\n随机变量的概率分布：给定随机变量的取值范围，导致某种随机事件出现的可能性。可以理解为符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。\n空间变换\n由特征列的取值所构成的矩阵空间应具有完整性，即能够反映事物的空间形式或变换规律。\n向量：具有大小和方向。\n向量与矩阵的乘积就是一个向量从一个线性空间（坐标系），通过线性变换，选取一个新的基底，变换到这个新的基底所构成的另一个线性空间的过程。\n矩阵与矩阵的乘法 \\(C = A \\cdot B\\)：\n\\(A\\)：向量组\n\\(B\\)：线性变换下的矩阵\n假设我们考察一组对象 \\(\\scr{A} = \\{\\alpha_1, \\cdots, \\alpha_m\\}\\)，它们在两个不同维度的空间 \\(V^n\\) 和 \\(V^p\\) 的基底分别是 \\(\\{\\vec{e_1}, \\cdots, \\vec{e_n}\\}\\) 和 \\(\\{\\vec{d_1}, \\cdots, \\vec{d_p}\\}\\)，\\(T\\) 即为 \\(V^n\\) 到 \\(V^p\\) 的线性变换，且有（\\(k = \\{1, \\cdots, m\\}\\)）：\n\\[ \\begin{align} &T \\begin{pmatrix} \\begin{bmatrix} \\vec{e_1} \\\\ \\vdots \\\\ \\vec{e_n} \\end{bmatrix} \\end{pmatrix} = A \\begin{bmatrix} \\vec{d_1} \\\\ \\vdots \\\\ \\vec{d_p} \\end{bmatrix} \\\\ &\\alpha_k = \\begin{bmatrix} x_1^{k} & \\cdots & x_n^k \\end{bmatrix} \\begin{bmatrix} \\vec{e_1} \\\\ \\vdots \\\\ \\vec{e_n} \\end{bmatrix}\\\\ &T(\\alpha_k)= \\begin{bmatrix} y_1^{k} & \\cdots & y_p^k \\end{bmatrix} \\begin{bmatrix} \\vec{d_1} \\\\ \\vdots \\\\ \\vec{d_p} \\end{bmatrix} \\end{align} \\]\n令\n\\[ \\begin{cases} &X^k = \\begin{bmatrix} x_1^{k} & \\cdots & x_n^k \\end{bmatrix}\\\\ &Y^k = \\begin{bmatrix} y_1^{k} & \\cdots & y_p^k \\end{bmatrix} \\end{cases} \\]\n则记：\n\\[ \\begin{cases} &X = \\begin{bmatrix} X^{1} \\\\ \\vdots \\\\ X^m \\end{bmatrix} \\\\ &Y = \\begin{bmatrix} Y^{1} \\\\ \\vdots \\\\ Y^m \\end{bmatrix} \\end{cases} \\]\n由式（1）可知：\n\\[ \\begin{align} XA = Y \\end{align} \\]\n因而 \\(X\\) 与 \\(Y\\) 表示一组对象在不同的线性空间的坐标表示。\\(A\\) 表示线性变换在某个基偶（如，\\((\\{\\vec{e_1}, \\cdots, \\vec{e_n}\\}, \\{\\vec{d_1}, \\cdots, \\vec{d_p}\\})\\)）下的矩阵表示。\n使用 Numpy 求解矩阵的特征值和特征向量\n\\[ A = \\lambda v \\]\nA = [[8, 7, 6], [3, 5, 7], [4, 9, 1]] evals, evecs = np.linalg.eig(A)\nprint('特征值：\\n%s\\n特征向量：\\n%s'%(evals, evecs))\n特征值： [ 16.43231925 2.84713925 -5.2794585 ] 特征向量： [[ 0.73717284 0.86836047 -0.09167612] [ 0.48286213 -0.4348687 -0.54207062] [ 0.47267364 -0.23840995 0.83531726]]\n有了特征值和特征向量，我们便可以还原矩阵：\n\\[ A = Q \\Sigma Q^{-1} \\]\nsigma = evals * np.eye(3) sigma\narray([[ 16.43231925, 0. , -0. ], [ 0. , 2.84713925, -0. ], [ 0. , 0. , -5.2794585 ]])\n或者，利用 np.diag：\nnp.diag(evals)\narray([[ 16.43231925, 0. , 0. ], [ 0. , 2.84713925, 0. ], [ 0. , 0. , -5.2794585 ]])\nnp.dot(np.dot(evecs, sigma), np.linalg.inv(evecs))\narray([[ 8., 7., 6.], [ 3., 5., 7.], [ 4., 9., 1.]])\n我的学习笔记：ML 基础\n我的Github：https://github.com/q735613050/AI/tree/master/ML\n关于矩阵的一个不成熟的解释: 机器学习中的矩阵"}
{"content2":"1. Iris data set\nIris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性。可通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。\n该数据集包含了5个属性：\nSepal.Length（花萼长度），单位是cm;\nSepal.Width（花萼宽度），单位是cm;\nPetal.Length（花瓣长度），单位是cm;\nPetal.Width（花瓣宽度），单位是cm;\nspecies (种类)：Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），以及Iris Virginica（维吉尼亚鸢尾）。\n如表 11所示的iris部分数据集。\n表 11\n6.4\n2.8\n5.6\n2.2\n2\n5\n2.3\n3.3\n1\n1\n4.9\n2.5\n4.5\n1.7\n2\n4.9\n3.1\n1.5\n0.1\n0\n5.7\n3.8\n1.7\n0.3\n0\n4.4\n3.2\n1.3\n0.2\n0\n5.4\n3.4\n1.5\n0.4\n0\n6.9\n3.1\n5.1\n2.3\n2\n6.7\n3.1\n4.4\n1.4\n1\n5.1\n3.7\n1.5\n0.4\n0\n5.2\n2.7\n3.9\n1.4\n1\n6.9\n3.1\n4.9\n1.5\n1\n5.8\n4\n1.2\n0.2\n0\n5.4\n3.9\n1.7\n0.4\n0\n7.7\n3.8\n6.7\n2.2\n2\n6.3\n3.3\n4.7\n1.6\n1\n2. Neural Network\n2.1 Perform\nTensorFlow提供一个高水平的机器学习 API (tf.contrib.learn)，使得容易配置(configure)、训练(train)和评估(evaluate)各种机器学习模型。tf.contrib.learn库的使用可以概括为五个步骤，如下所示：\n1) Load CSVs containing Iris training/test data into a TensorFlow Dataset\n2) Construct a neural network classifier\n3) Fit the model using the training data\n4) Evaluate the accuracy of the model\n5)Classify new samples\n2.2 Code\n本节以对 Iris 数据集进行分类为例进行介绍，如下所示是完整的TensorFlow程序：\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport urllib\nimport numpy as np\nimport tensorflow as tf\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\nIRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\ndef main():\n# If the training and test sets aren't stored locally, download them.\nif not os.path.exists(IRIS_TRAINING):\nraw = urllib.urlopen(IRIS_TRAINING_URL).read()\nwith open(IRIS_TRAINING, \"w\") as f:\nf.write(raw)\nif not os.path.exists(IRIS_TEST):\nraw = urllib.urlopen(IRIS_TEST_URL).read()\nwith open(IRIS_TEST, \"w\") as f:\nf.write(raw)\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\nfilename=IRIS_TRAINING,\ntarget_dtype=np.int,\nfeatures_dtype=np.float32)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header(\nfilename=IRIS_TEST,\ntarget_dtype=np.int,\nfeatures_dtype=np.float32)\n# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\nhidden_units=[10, 20, 10],\nn_classes=3,\nmodel_dir=\"/tmp/iris_model\")\n# Define the training inputs\ndef get_train_inputs():\nx = tf.constant(training_set.data)\ny = tf.constant(training_set.target)\nreturn x, y\n# Fit model.\nclassifier.fit(input_fn=get_train_inputs, steps=2000)\n# Define the test inputs\ndef get_test_inputs():\nx = tf.constant(test_set.data)\ny = tf.constant(test_set.target)\nreturn x, y\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(input_fn=get_test_inputs,\nsteps=1)[\"accuracy\"]\nprint(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n# Classify two new flower samples.\ndef new_samples():\nreturn np.array(\n[[6.4, 3.2, 4.5, 1.5],\n[5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\npredictions = list(classifier.predict(input_fn=new_samples))\nprint(\n\"New Samples, Class Predictions: {}\\n\"\n.format(predictions))\nif __name__ == \"__main__\":\nmain()\n3. Analysis\n3.1 Load data\n对于本文的程序，Iris数据集被分为两部分：\n训练集：有120个样例，保存在iris_training.csv文件中；\n测试集：有30个样例，保存在iris_test.csv文件中。\n1) import module\n首先程序引入必要module，然后定义了数据集的本地路径和网络路径；\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport urllib\nimport tensorflow as tf\nimport numpy as np\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\nIRIS_TEST_URL = http://download.tensorflow.org/data/iris_test.csv\n2) Open File\n若本地路径上不存在数据集指定的文件，则通过网上下载。\nif not os.path.exists(IRIS_TRAINING):\nraw = urllib.urlopen(IRIS_TRAINING_URL).read()\nwith open(IRIS_TRAINING,'w') as f:\nf.write(raw)\nif not os.path.exists(IRIS_TEST):\nraw = urllib.urlopen(IRIS_TEST_URL).read()\nwith open(IRIS_TEST,'w') as f:\nf.write(raw)\n3) load Dataset\n接着将Iris数据集加载到TensorFlow框架中，使其TensorFlow能够直接使用。这其中使用了learn.datasets.base模块的load_csv_with_header()函数。该方法有三个参数:\nfilename：指定了CSV文件的名字；\ntarget_dtype：指定了数据集中目标数据类型，其为numpy datatype类型；\nfeatures_dtype：指定了数据集中特征向量的数据类型，其为numpy datatype类型。\n如表 11所示，Iris数据中的目标值为：0~2，所以可以定义为整型数据就可以了，即np.int，如下所示：\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\nfilename=IRIS_TRAINING,\ntarget_dtype=np.int,\nfeatures_dtype=np.float32)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header(\nfilename=IRIS_TEST,\ntarget_dtype=np.int,\nfeatures_dtype=np.float32)\n由于tf.contrib.learn中的数据类型（Datasets）是以元祖类型定义的，所以用户可以通过data 和 target两个域属性访问特征向量数据和目标数据。即training_set.data 和 training_set.target为训练数据集中的特征向量和目标数据。\n3.2 Construct Estimator\ntf.contrib.learn预定义了许多模型，称为：Estimators。用户以黑箱模型使用Estimator来训练和评估数据。本节使用tf.contrib.learn.DNNClassifier来训练数据，如下所示：\n# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\nhidden_units=[10, 20, 10],\nn_classes=3,\nmodel_dir=\"/tmp/iris_model\")\n首先程序定义了模型的feature columns，其指定了数据集中特征向量的数据类型。每种类型都有一个名字，由于本节的数据是实数型，所以这里使用.real_valued_column类型。该类型第一个参数指定了列名字，第二个参数指定了列的数量。其中所有的特征类型都定义在：tensorflow/contrib/layers/python/layers/feature_column.py.\n然后程序创建了DNNClassifier模型，\nfeature_columns=feature_columns：指定所创建的特征向量类型；\nhidden_units=[10, 20, 10]：设置隐藏层的层数，并指定每层神经元的数据量；\nn_classes=3：指定目标类型的数量，Iris数据有三类，所以这里为3；\nmodel_dir=/tmp/iris_model：指定模型在训练期间保存的路径。\n3.3 Describe pipeline\nTensorFlow框架的数据都是以Tensor对象存在，即要么是constant、placeholder或Variable类型。通常训练数据是以placeholder类型定义，然后用户训练时，传递所有的数据。本节则将训练数据存储在constant类型中。如下所示：\n# Define the training inputs\ndef get_train_inputs():\nx = tf.constant(training_set.data)\ny = tf.constant(training_set.target)\nreturn x, y\n3.4 Fit DNNClassifier\n创建分类器后，就可以调用神经网络中DNNClassifier模型的fit()函数来训练模型了，如下所示：\n# Fit model.\nclassifier.fit(input_fn=get_train_inputs, steps=2000)\n通过向fit传递get_train_inputs函数返回的训练数据，并指定训练的步数为2000步。\n3.5 Evaluate Model\n训练模型后，就可以通过evaluate()函数来评估模型的泛化能力了。与fit函数类似，evaluate函数的输入数据也需为Tensor类型，所以定义了get_test_inputs()函数来转换数据。\n# Define the test inputs\ndef get_test_inputs():\nx = tf.constant(test_set.data)\ny = tf.constant(test_set.target)\nreturn x, y\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(input_fn=get_test_inputs, steps=1)[\"accuracy\"]\nprint(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n注意：\n由于evaluate函数的返回值是一个Map类型（即dict类型），所以直接根据\"accuracy\"键获取值：accuracy_score。\n3.6 Classify Samples\n在训练模型后，就可以使用estimator模型的predict()函数来预测样例。如表 31有所示的两个样例，希望预测其为什么类型。\n表 31\nSepal Length\nSepal Width\nPetal Length\nPetal Width\n6.4\n3.2\n4.5\n1.5\n5.8\n3.1\n5\n1.7\n如下所示的程序：\n# Classify two new flower samples.\ndef new_samples():\nreturn np.array(\n[[6.4, 3.2, 4.5, 1.5],\n[5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\npredictions = list(classifier.predict(input_fn=new_samples))\nprint(\n\"New Samples, Class Predictions: {}\\n\"\n.format(predictions))\n输出：\nNew Samples, Class Predictions: [1 2]\n注意：\n由于predict()函数执行的返回结果类型是generator。所以上述程序将其转换为一个list对象。\n4. Logging and Monitoring\n由于TensorFlow的机器学习Estimator是黑箱学习，用户无法了解模型执行发生了什么，以及模型什么时候收敛。所以tf.contrib.learn提供的一个Monitor API，可以帮助用户记录和评估模型。\n4.1 Default ValidationMonitor\n默认使用fit()函数训练Estimator模型时，TensorFlow会产生一些summary数据到fit()函数指定的路径中。用户可以使用Tensorborad来展示更详细的信息。如图 1所示，执行上述程序DNNClassifier的fit()和evaluate()函数后，默认在TensorBoard页面显示的常量信息。\n图 1\n4.2 Monitors\n为了让用户更直观地了解模型训练过程的细节，tf.contrib.learn提供了一些高级Monitors，使得用户在调用fit()函数时，可以使用Monitors来记录和跟踪模型的执行细节。如表 41所示是fitt()函数支持的Monitors类型：\n表 41\nMonitor\nDescription\nCaptureVariable\n每执行n步训练，就将保存指定的变量值到一个集合(collection)中\nPrintTensor\n每执行n步训练，记录指定的Tensor值\nSummarySaver\n每执行n步训练，使用tf.summary.FileWriter函数保存tf.Summary 缓存\nValidationMonitor\n每执行n步训练，记录一批评估metrics，同时可设置停止条件\n如\\tensorflow\\examples\\tutorials\\monitors\\ iris_monitors.py所示的程序：\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport numpy as np\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.INFO)\n# Data sets\nIRIS_TRAINING = os.path.join(os.path.dirname(__file__), \"iris_training.csv\")\nIRIS_TEST = os.path.join(os.path.dirname(__file__), \"iris_test.csv\")\ndef main(unused_argv):\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\nfilename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header(\nfilename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float)\nvalidation_metrics = {\n\"accuracy\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_accuracy,\nprediction_key=\"classes\"),\n\"precision\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_precision,\nprediction_key=\"classes\"),\n\"recall\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_recall,\nprediction_key=\"classes\"),\n\"mean\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_mean,\nprediction_key=\"classes\")\n}\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\ntest_set.data,\ntest_set.target,\nevery_n_steps=50,\nmetrics=validation_metrics,\nearly_stopping_metric=\"loss\",\nearly_stopping_metric_minimize=True,\nearly_stopping_rounds=200)\n# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(\nfeature_columns=feature_columns,\nhidden_units=[10, 20, 10],\nn_classes=3,\nmodel_dir=\"/tmp/iris_model\",\nconfig=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))\n# Fit model.\nclassifier.fit(x=training_set.data,\ny=training_set.target,\nsteps=2000,\nmonitors=[validation_monitor])\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(\nx=test_set.data, y=test_set.target)[\"accuracy\"]\nprint(\"Accuracy: {0:f}\".format(accuracy_score))\n# Classify two new flower samples.\nnew_samples = np.array(\n[[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\ny = list(classifier.predict(new_samples))\nprint(\"Predictions: {}\".format(str(y)))\nif __name__ == \"__main__\":\ntf.app.run()\n4.3 Configuring ValidationMonitor\n如图 1所示，如果没有指定任何evaluation metrics，那么ValidationMonitor默认会记录loss和accuracy信息。但用户可以通过创建ValidationMonitor对象来自定义metrics信息。\n即通过向ValidationMonitor构造函数传递一个metrics参数，该参数是一个Map类型(dist)，其中的key是希望显示的名字，value是一个MetricSpec对象。\n其中tf.contrib.learn.MetricSpec类的构造函数有如下四个参数：\nmetric_fn：是一个函数，TensorFlow在tf.contrib.metrics模块中预定义了一些函数，用户可以直接使用；\nprediction_key：如果模型返回一个Tensor或与一个单一的入口，那么这个参数可以被忽略；\nlabel_key：可选\nweights_key：可选\n如下所示创建一个dist类型的对象：\nvalidation_metrics = {\n\"accuracy\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_accuracy,\nprediction_key=\"classes\"),\n\"precision\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_precision,\nprediction_key=\"classes\"),\n\"recall\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_recall,\nprediction_key=\"classes\"),\n\"mean\":\ntf.contrib.learn.MetricSpec(\nmetric_fn=tf.contrib.metrics.streaming_mean,\nprediction_key=\"classes\")\n}\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\ntest_set.data,\ntest_set.target,\nevery_n_steps=50,\nmetrics=validation_metrics,\nearly_stopping_metric=\"loss\",\nearly_stopping_metric_minimize=True,\nearly_stopping_rounds=200)\n注意：Python中的dist可以直接以一对\"{}\"初始化元素，如上validation_metrics对象创建所示。\n5. 参考文献\n[1].TensorFlowà Develop à Get Started àtf.contrib.learn Quickstart；\n[2].TensorFlowà Develop à Get Started à Logging and Monitoring Basics with tf.contrib.learn；"}
{"content2":"本人最近才迷恋上ROS（Robot Operating System）,准确的说应该是6月中旬，具体的记不清了（可能是年纪大了，容易健忘）。对于一个电子DIY的狂热爱好者来说，我在校的梦想就是做一个属于自己的一个高大上的机器人。近几年机器学习，人工智能那是相当火啊，尤其是在央视春晚播了机器人之后，可以说中国的机器人产业是爆发式的增长（核心技术都是国外的，像芯片，算法之类的还是老外的）。\n先介绍一下自己的情况吧，本人现在已经大三了，主修的是自动化（在广东的一所二本工科学校）。在刚上大学的时候，内心多么的期待和向往着大学的无忧无虑的生活，所以大一一个学年都在浪，根本没学到什么专业知识，基本上每天都在忙着做兼职（派传单，餐厅帮厨，做促销...），最忙的时候一天三份工，不过忙却快乐着（就像同学说的：生活就像qiangjian,既然反抗不了就要学着去享受）。就这样一年过去了，攒了几千块钱，这可得好好浪了，然后和同学去了一趟厦门，立马变成了穷光蛋。\n好，我们言归正传，开始扯我的专业技能这块。大二上学期一个偶然的机会，同学叫我去面试一个程序设计协会的部长，也正是这个机会唤醒我的求知欲，自己意识到不能再想大一那样过了。在这里也特别感谢那个师兄收了我，从此我也走上了一条码农的不归路......大二10月份，我开始学习51单片机以及C，这段时间真的是几乎每天都带在实验室看视频，看书。。。。看着看着，马上期末了，但这时候心也收不回来了，完全没有心思放在学习这方面，所以导致过了电路（通常来讲我是不会挂科的，因为还是复习了好几天呢，但谁让我们班遇上了饿哦们院有名“杀手”，四大杀手之一，然后这一科我们班挂了20多了，几乎一半多）欲哭无泪啊。导致博主我大三还得重修，这也是大学最遗憾的是了（不过，没挂过科的大学是不完整的，我完整了）最坑的是，这也导致了我大学的很多评优都没机会了。\n再次言归正传，那个学期基本上学完了51，基本上算是搞懂“博大精深”的C语言了，下学期了，开始学stm32，这块跟着野火大哥学，因为本人真的是对飞机，机器人之泪的很感兴趣，所以就下血本买了个烈火的小四轴（心疼啊），不过每天都在安慰自己，为了学到技术，为了自己美好的将来，我认了。这一段时间边学边做，一边看一边改程序，上网弄资料融合到自己的飞机中，说实话学到了很多，不过也浪费了好多参加比赛的机会（飞思卡尔，电子大赛，合泰杯...）那时候有一个想法就是参加比赛是学东西，自己做也是学知识，没什么差别（不过现在博主找工作的时候，还是有很多公司会看你的这些经历的，就像那些做机器人的，很多都明确写了参加过机器人对抗赛的优先考虑）。大三了，这时候看见师兄都在学韦东山，那我也学吧，又开始学ARM9，又狠下心来买了TQ2440，这学期在实验室认识了个师弟，超牛逼的，好像是从初中那会就开始接触电子了，听说智商也是很高，当年差几分就去了华南工了，尤其是模电，特别扭x（最近又休学自己创业了），我真的是佩服啊。这学期和他们几个在实验室熬夜（后来实验室不让通宵了，还搞了个联名抗议书，不过终究还是不了了之），每天都在学ARM9，刚入门真的是超痛苦，一个星期基本上都在接线，怎么下载程序。其中最搞笑的是，博主用串口线的时候，人家要求是直通，我的却是交叉的，导致怎么也接收不到信息，还以为开饭版是坏的，弄几天后，突然开窍菜解决，坑啊。然后这时候开始接触linux了，对系统有了基本的认识。下学期，参加了几个比赛，拿了一等奖，三等奖。这时候做的作品就和机器人有关了，所以就想到了为什么没有机器人的操作系统呢？因为裸板好多模块弄在一起是很繁琐的，向机器人呢么多的传感器之类的。所以开始上网找资料。因为博主这时候正在学linux的驱动，所以不是太上心。但我们做过一个遥控建网球的机器人，想后续加工一下，所以开始找资料，这时候我就对ROS 和视觉处理这方面产生了浓厚的兴趣，然后就开始走上了一条不归路.......\n下面为大家附上我做的一些东西以及现在做的和相关的三年的积累。。。。。。"}
{"content2":"1.简介\ngbdt全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，gbdt在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征。这三点实在是太吸引人了，导致在面试的时候大家也非常喜欢问这个算法。 gbdt的面试考核点，大致有下面几个:\ngbdt 的算法的流程？\ngbdt 如何选择特征 ？\ngbdt 如何构建特征 ？\ngbdt 如何用于分类？\ngbdt 通过什么方式减少误差 ？\ngbdt的效果相比于传统的LR，SVM效果为什么好一些 ？\ngbdt 如何加速训练？\ngbdt的参数有哪些，如何调参 ？\ngbdt 实战当中遇到的一些问题 ？\ngbdt的优缺点 ？\n2. 正式介绍\n首先gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。\ngbdt的训练过程\n我们通过一张图片，图片来源来说明gbdt的训练过程:\n图 1：GBDT 的训练过程\ngbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度，（此处是可以证明的）。\n弱分类器一般会选择为CART TREE（也就是分类回归树）。由于上述高偏差和简单的要求 每个分类回归树的深度不会很深。最终的总分类器 是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。\n模型最终可以描述为：$$F_{m}(x) = \\sum_{m=1}^{M}T\\left ( x;\\theta _m \\right )$$\n模型一共训练M轮，每轮产生一个弱分类器 $T\\left ( x;\\theta _m \\right )$。弱分类器的损失函数$$\\hat\\theta_{m} = \\mathop{\\arg\\min}_{\\theta_{m}} \\sum_{i=1}^{N}L\\left ( y_{i},F_{m-1}(x_{i})+T(x_{i};\\theta_{m} ) \\right )$$\n$F_{m-1}(x)$ 为当前的模型，gbdt 通过经验风险极小化来确定下一个弱分类器的参数。具体到损失函数本身的选择也就是L的选择，有平方损失函数，0-1损失函数，对数损失函数等等。如果我们选择平方损失函数，那么这个差值其实就是我们平常所说的残差。\n但是其实我们真正关注的，1.是希望损失函数能够不断的减小，2.是希望损失函数能够尽可能快的减小。所以如何尽可能快的减小呢？\n让损失函数沿着梯度方向的下降。这个就是gbdt 的 gb的核心了。 利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值去拟合一个回归树。gbdt 每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度。\n这样每轮训练的时候都能够让损失函数尽可能快的减小，尽快的收敛达到局部最优解或者全局最优解。\ngbdt如何选择特征？\ngbdt选择特征的细节其实是想问你CART Tree生成的过程。这里有一个前提，gbdt的弱分类器默认选择的是CART TREE。其实也可以选择其他弱分类器的，选择的前提是低方差和高偏差。框架服从boosting 框架即可。\n下面我们具体来说CART TREE(是一种二叉树) 如何生成。CART TREE 生成的过程其实就是一个选择特征的过程。假设我们目前总共有 M 个特征。第一步我们需要从中选择出一个特征 j，做为二叉树的第一个节点。然后对特征 j 的值选择一个切分点 m. 一个 样本的特征j的值 如果小于m，则分为一类，如果大于m,则分为另外一类。如此便构建了CART 树的一个节点。其他节点的生成过程和这个是一样的。现在的问题是在每轮迭代的时候，如何选择这个特征 j,以及如何选择特征 j 的切分点 m:\n原始的gbdt的做法非常的暴力，首先遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征 m 的最优切分点 j。\n如何衡量我们找到的特征 m和切分点 j 是最优的呢？ 我们用定义一个函数 FindLossAndSplit 来展示一下求解过程：\n1 def findLossAndSplit(x,y): 2 # 我们用 x 来表示训练数据 3 # 我们用 y 来表示训练数据的label 4 # x[i]表示训练数据的第i个特征 5 # x_i 表示第i个训练样本 6 7 # minLoss 表示最小的损失 8 minLoss = Integet.max_value 9 # feature 表示是训练的数据第几纬度的特征 10 feature = 0 11 # split 表示切分点的个数 12 split = 0 13 14 # M 表示 样本x的特征个数 15 for j in range(0,M): 16 # 该维特征下，特征值的每个切分点，这里具体的切分方式可以自己定义 17 for c in range(0,x[j]): 18 L = 0 19 # 第一类 20 R1 = {x|x[j] <= c} 21 # 第二类 22 R2 = {x|x[j] > c} 23 # 属于第一类样本的y值的平均值 24 y1 = ave{y|x 属于 R1} 25 # 属于第二类样本的y值的平均值 26 y2 = ave{y| x 属于 R2} 27 # 遍历所有的样本，找到 loss funtion 的值 28 for x_1 in all x 29 if x_1 属于 R1： 30 L += (y_1 - y1)^2 31 else: 32 L += (y_1 - y2)^2 33 if L < minLoss: 34 minLoss = L 35 feature = i 36 split = c 37 return minLoss,feature ,split\n如果对这段代码不是很了解的，可以先去看看李航第五章中对CART TREE 算法的叙述。在这里，我们先遍历训练样本的所有的特征，对于特征 j，我们遍历特征 j 所有特征值的切分点 c。找到可以让下面这个式子最小的特征 j 以及切分点c.\ngbdt 如何构建特征 ?\n其实说gbdt 能够构建特征并非很准确，gbdt 本身是不能产生特征的，但是我们可以利用gbdt去产生特征的组合。在CTR预估中，工业界一般会采用逻辑回归去进行处理,在我的上一篇博文当中已经说过，逻辑回归本身是适合处理线性可分的数据，如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。\n长久以来，我们都是通过人工的先验知识或者实验来获得有效的组合特征，但是很多时候，使用人工经验知识来组合特征过于耗费人力，造成了机器学习当中一个很奇特的现象：有多少人工就有多少智能。关键是这样通过人工去组合特征并不一定能够提升模型的效果。所以我们的从业者或者学界一直都有一个趋势便是通过算法自动，高效的寻找到有效的特征组合。Facebook 在2014年 发表的一篇论文便是这种尝试下的产物，利用gbdt去产生有效的特征组合，以便用于逻辑回归的训练，提升模型最终的效果。\n图 2：用GBDT 构造特征\n如图 2所示，我们 使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为 0.\n于是对于该样本，我们可以得到一个向量[0,1,0,1,0] 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。实验证明这样会得到比较显著的效果提升。\nGBDT 如何用于分类 ？\n首先明确一点，gbdt 无论用于分类还是回归一直都是使用的CART 回归树。不会因为我们所选择的任务是分类任务就选用分类树，这里面的核心是因为gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的。这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。\n如果选用的弱分类器是分类树，类别相减是没有意义的。上一轮输出的是样本 x 属于 A类，本一轮训练输出的是样本 x 属于 B类。 A 和 B 很多时候甚至都没有比较的意义，A 类- B类是没有意义的。\n我们具体到分类这个任务上面来，我们假设样本 X 总共有 K类。来了一个样本 x，我们需要使用gbdt来判断 x 属于样本的哪一类。\n图三 gbdt 多分类算法流程\n第一步 我们在训练的时候，是针对样本 X 每个可能的类都训练一个分类回归树。举例说明，目前样本有三类，也就是 K = 3。样本 x 属于 第二类。那么针对该样本 x 的分类结果，其实我们可以用一个 三维向量 [0,1,0] 来表示。0表示样本不属于该类，1表示样本属于该类。由于样本已经属于第二类了，所以第二类对应的向量维度为1，其他位置为0。\n针对样本有 三类的情况，我们实质上是在每轮的训练的时候是同时训练三颗树。第一颗树针对样本x的第一类，输入为$（x,0）$。第二颗树输入针对 样本x 的第二类，输入为$（x,1）$。第三颗树针对样本x 的第三类，输入为$（x，0）$\n在这里每颗树的训练过程其实就是就是我们之前已经提到过的CATR TREE 的生成过程。在此处我们参照之前的生成树的程序 即可以就解出三颗树，以及三颗树对x 类别的预测值$f_{1}(x),f_{2}(x),f_{3}(x)$。那么在此类训练中，我们仿照多分类的逻辑回归 ，使用softmax 来产生概率，则属于类别 1 的概率$$p_{1}=exp(f_{1}{(x)})/\\sum_{k= 1}^{3}exp(f_{k}{(x)})$$\n并且我们我们可以针对类别1 求出 残差$y_{11}(x) = 0-p_{1}(x)$;类别2 求出残差$y_{22}(x)= 1-p_2(x)$;类别3 求出残差$y_{33}(x)= 0-p_{3}(x)$.\n然后开始第二轮训练 针对第一类 输入为（x,$y_{11}(x)$）, 针对第二类输入为（x,$y_{22}(x))$, 针对 第三类输入为 (x,$y_{33}(x)$).继续训练出三颗树。一直迭代M轮。每轮构建 3颗树。\n所以当K =3。我们其实应该有三个式子 $$F_{1M}{(x)}=\\sum_{m=1}^{M}{\\hat{C_{1m}}I(x\\epsilon R_{1m})}$$ $$F_{2M}{(x)}=\\sum_{m=1}^{M}{\\hat{C_{2m}}I(x\\epsilon R_{2m})}$$ $$F_{3M}{(x)}=\\sum_{m=1}^{M}{\\hat{C_{3m}}I(x\\epsilon R_{3m})}$$\n当训练完毕以后，新来一个样本 x1 ，我们需要预测该样本的类别的时候，便可以有这三个式子产生三个值，$f_{1}(x),f_{2}(x),f_{3}(x)$。样本属于 某个类别c的概率为 $$p_{c}=exp(f_{c}{(x)})/\\sum_{k= 1}^{3}exp(f_{k}{(x)})$$\nGBDT 多分类举例说明\n上面的理论阐述可能仍旧过于难懂，我们下面将拿Iris 数据集中的六个数据作为例子，来展示gbdt 多分类的过程。\n样本编号\n花萼长度(cm)\n花萼宽度(cm)\n花瓣长度(cm)\n花瓣宽度\n花的种类\n1\n5.1\n3.5\n1.4\n0.2\n山鸢尾\n2\n4.9\n3.0\n1.4\n0.2\n山鸢尾\n3\n7.0\n3.2\n4.7\n1.4\n杂色鸢尾\n4\n6.4\n3.2\n4.5\n1.5\n杂色鸢尾\n5\n6.3\n3.3\n6.0\n2.5\n维吉尼亚鸢尾\n6\n5.8\n2.7\n5.1\n1.9\n维吉尼亚鸢尾\n图四 Iris 数据集\n这是一个有6个样本的三分类问题。我们需要根据这个花的花萼长度，花萼宽度，花瓣长度，花瓣宽度来判断这个花属于山鸢尾，杂色鸢尾，还是维吉尼亚鸢尾。具体应用到gbdt多分类算法上面。我们用一个三维向量来标志样本的label。[1,0,0] 表示样本属于山鸢尾，[0,1,0] 表示样本属于杂色鸢尾，[0,0,1] 表示属于维吉尼亚鸢尾。\ngbdt 的多分类是针对每个类都独立训练一个 CART Tree。所以这里，我们将针对山鸢尾类别训练一个 CART Tree 1。杂色鸢尾训练一个 CART Tree 2 。维吉尼亚鸢尾训练一个CART Tree 3，这三个树相互独立。\n我们以样本 1 为例。针对 CART Tree1 的训练样本是$[5.1, 3.5 , 1.4, 0.2]$，label 是 1，最终输入到模型当中的为$[5.1, 3.5 , 1.4, 0.2, 1]$。针对 CART Tree2 的训练样本也是$[5.1, 3.5 , 1.4, 0.2]$,但是label 为 0,最终输入模型的为$[5.1, 3.5 , 1.4, 0.2, 0]$. 针对 CART Tree 3的训练样本也是$[5.1, 3.5 , 1.4, 0.2] $,label 也为0,最终输入模型当中的为$[5.1, 3.5 , 1.4, 0.2, 0]$.\n下面我们来看 CART Tree1 是如何生成的，其他树 CART Tree2 , CART Tree 3的生成方式是一样的。CART Tree的生成过程是从这四个特征中找一个特征做为CART Tree1 的节点。比如花萼长度做为节点。6个样本当中花萼长度 大于5.1 cm的就是 A类，小于等于 5.1 cm 的是B类。生成的过程其实非常简单，问题 1.是哪个特征最合适？ 2.是这个特征的什么特征值作为切分点？ 即使我们已经确定了花萼长度做为节点。花萼长度本身也有很多值。在这里我们的方式是遍历所有的可能性，找到一个最好的特征和它对应的最优特征值可以让当前式子的值最小。\n我们以第一个特征的第一个特征值为例。R1 为所有样本中花萼长度小于 5.1 cm 的样本集合，R2 为所有样本当中花萼长度大于等于 5.1cm 的样本集合。所以 $R1 = \\left \\{  2\\right \\}$，$R2 = \\left \\{ 1,3,4,5,6 \\right \\}$.\n图 5 节点分裂示意图\ny1 为 R1 所有样本的label 的均值 $1/1 = 1$。y2 为 R2 所有样本的label 的均值 $(1+0+0+0+0) /5 = 0.2$。\n下面便开始针对所有的样本计算这个式子的值。样本1 属于 R2 计算的值为$( 1 - 0.2)^2$, 样本2 属于R1 计算的值为$( 1 -1 )^2$, 样本 3，4，5，6同理都是 属于 R2的 所以值是$(0-0.2)^2$. 把这六个值加起来，便是 山鸢尾类型在特征1 的第一个特征值的损失值。这里算出来(1-0.2)^2+ (1-1)^2 + (0-0.2)^2+(0-0.2)^2+(0-0.2)^2 +(0-0.2)^2= 0.84\n接着我们计算第一个特征的第二个特征值，计算方式同上，R1 为所有样本中 花萼长度小于 4.9 cm 的样本集合，R2 为所有样本当中 花萼长度大于等于 4.9 cm 的样本集合.所以 $R1 = \\left \\{  \\right \\}$，$R1 = \\left \\{ 1,2,3,4,5,6 \\right \\}$. y1 为 R1 所有样本的label 的均值 = 0。y2 为 R2 所有样本的label 的均值 $(1+1+0+0+0+0) /6 = 0.3333$。\n图 6 第一个特征的第二个特侦值的节点分裂情况\n我们需要针对所有的样本,样本1 属于 R2, 计算的值为$( 1 - 0.333 )^2$, 样本2 属于R2 ,计算的值为$( 1 -0.333 )^2$, 样本 3，4，5，6同理都是 属于 R2的, 所以值是$(0-0.333)^2$. 把这六个值加起来山鸢尾类型在特征1 的第二个特征值的损失值。这里算出来 (1-0.333)^2+ (1-0.333)^2 + (0-0.333)^2+(0-0.333)^2+(0-0.333)^2 +(0-0.333)^2 = 2.244189. 这里的损失值大于 特征一的第一个特征值的损失值，所以我们不取这个特征的特征值。\n图 7 所有情况说明\n这样我们可以遍历所有特征的所有特征值，找到让这个式子最小的特征以及其对应的特征值，一共有24种情况,4个特征*每个特征有6个特征值。在这里我们算出来让这个式子最小的特征花萼长度,特征值为5.1 cm。这个时候损失函数最小为 0.8。\n于是我们的预测函数此时也可以得到: $$f(x) = \\sum_{x\\epsilon R_{1}} y_{1}*I(x\\epsilon R_{1})+\\sum_{x\\epsilon R_{2}} y_{2}*I(x\\epsilon R_{2})$$\n此处 R1 = {2},R2 = {1,3,4,5,6}，y1 = 1,y2 = 0.2。训练完以后的最终式子为 $$f_{1}(x) = \\sum_{x\\epsilon R_{1}} 1*I(x\\epsilon R_{1})+\\sum_{x\\epsilon R_{2}} 0.2*I(x\\epsilon R_{2})$$\n借由这个式子，我们得到对样本属于类别1 的预测值 $f_{1}(x) = 1 + 0.2 * 5  = 2$。同理我们可以得到对样本属于类别2,3的预测值$f_{2}(x)$,$f_{3}(x)$.样本属于类别1的概率 即为 $$p_{1}=exp(f_{1}{(x)})/\\sum_{k= 1}^{3}exp(f_{k}{(x)})$$\n下面我们用代码来实现整个找特征的过程，大家可以自己再对照代码看看。\n1 # 定义训练数据 2 train_data = [[5.1,3.5,1.4,0.2],[4.9,3.0,1.4,0.2],[7.0,3.2,4.7,1.4],[6.4,3.2,4.5,1.5],[6.3,3.3,6.0,2.5],[5.8,2.7,5.1,1.9]] 3 4 # 定义label 5 label_data = [[1,0,0],[1,0,0],[0,1,0],[0,1,0],[0,0,1],[0,0,1]] 6 # index 表示的第几类 7 def findBestLossAndSplit(train_data,label_data,index): 8 sample_numbers = len(label_data) 9 feature_numbers = len(train_data[0]) 10 current_label = [] 11 12 # define the minLoss 13 minLoss = 10000000 14 15 # feature represents the dimensions of the feature 16 feature = 0 17 18 # split represents the detail split value 19 split = 0 20 21 # get current label 22 for label_index in range(0,len(label_data)): 23 current_label.append(label_data[label_index][index]) 24 25 # trans all features 26 for feature_index in range(0,feature_numbers): 27 ## current feature value 28 current_value = [] 29 30 for sample_index in range(0,sample_numbers): 31 current_value.append(train_data[sample_index][feature_index]) 32 L = 0 33 ## different split value 34 print current_value 35 for index in range(0,len(current_value)): 36 R1 = [] 37 R2 = [] 38 y1 = 0 39 y2 = 0 40 41 for index_1 in range(0,len(current_value)): 42 if current_value[index_1] < current_value[index]: 43 R1.append(index_1) 44 else: 45 R2.append(index_1) 46 47 ## calculate the samples for first class 48 sum_y = 0 49 for index_R1 in R1: 50 sum_y += current_label[index_R1] 51 if len(R1) != 0: 52 y1 = float(sum_y) / float(len(R1)) 53 else: 54 y1 = 0 55 56 ## calculate the samples for second class 57 sum_y = 0 58 for index_R2 in R2: 59 sum_y += current_label[index_R2] 60 if len(R2) != 0: 61 y2 = float(sum_y) / float(len(R2)) 62 else: 63 y2 = 0 64 65 ## trans all samples to find minium loss and best split 66 for index_2 in range(0,len(current_value)): 67 if index_2 in R1: 68 L += float((current_label[index_2]-y1))*float((current_label[index_2]-y1)) 69 else: 70 L += float((current_label[index_2]-y2))*float((current_label[index_2]-y2)) 71 72 if L < minLoss: 73 feature = feature_index 74 split = current_value[index] 75 minLoss = L 76 print \"minLoss\" 77 print minLoss 78 print \"split\" 79 print split 80 print \"feature\" 81 print feature 82 return minLoss,split,feature 83 84 findBestLossAndSplit(train_data,label_data,0)\n3 总结\n目前，我们总结了 gbdt 的算法的流程，gbdt如何选择特征，如何产生特征的组合，以及gbdt 如何用于分类，这个目前可以认为是gbdt 最经常问到的四个部分。至于剩余的问题，因为篇幅的问题，我们准备再开一个篇幅来进行总结。也欢迎大家关注我的微信公众号。ModifyAI"}
{"content2":"版权声明：\n本文由LeftNotEasy所有，发布于http://leftnoteasy.cnblogs.com。如果转载，请注明出处，在未经作者同意下将本文用于商业用途，将追究其法律责任。\n前言:\n上次写过一篇关于贝叶斯概率论的数学，最近时间比较紧，coding的任务比较重，不过还是抽空看了一些机器学习的书和视频，其中很推荐两个：一个是stanford的machine learning公开课，在verycd可下载，可惜没有翻译。不过还是可以看。另外一个是prml-pattern recognition and machine learning, Bishop的一部反响不错的书，而且是2008年的，算是比较新的一本书了。\n前几天还准备写一个分布式计算的系列，只写了个开头，又换到写这个系列了。以后看哪边的心得更多，就写哪一个系列吧。最近干的事情比较杂，有跟机器学习相关的，有跟数学相关的，也有跟分布式相关的。\n这个系列主要想能够用数学去描述机器学习，想要学好机器学习，首先得去理解其中的数学意义，不一定要到能够轻松自如的推导中间的公式，不过至少得认识这些式子吧，不然看一些相关的论文可就看不懂了，这个系列主要将会着重于去机器学习的数学描述这个部分，将会覆盖但不一定局限于回归、聚类、分类等算法。\n回归与梯度下降：\n回归在数学上来说是给定一个点集，能够用一条曲线去拟合之，如果这个曲线是一条直线，那就被称为线性回归，如果曲线是一条二次曲线，就被称为二次回归，回归还有很多的变种，如locally weighted回归，logistic回归，等等，这个将在后面去讲。\n用一个很简单的例子来说明回归，这个例子来自很多的地方，也在很多的open source的软件中看到，比如说weka。大概就是，做一个房屋价值的评估系统，一个房屋的价值来自很多地方，比如说面积、房间的数量（几室几厅）、地段、朝向等等，这些影响房屋价值的变量被称为特征(feature)，feature在机器学习中是一个很重要的概念，有很多的论文专门探讨这个东西。在此处，为了简单，假设我们的房屋就是一个变量影响的，就是房屋的面积。\n假设有一个房屋销售的数据如下：\n面积(m^2)  销售价钱（万元）\n123            250\n150            320\n87              160\n102            220\n…               …\n这个表类似于帝都5环左右的房屋价钱，我们可以做出一个图，x轴是房屋的面积。y轴是房屋的售价，如下：\n如果来了一个新的面积，假设在销售价钱的记录中没有的，我们怎么办呢？\n我们可以用一条曲线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以在将曲线上这个点对应的值返回。如果用一条直线去拟合，可能是下面的样子：\n绿色的点就是我们想要预测的点。\n首先给出一些概念和常用的符号，在不同的机器学习书籍中可能有一定的差别。\n房屋销售记录表 - 训练集(training set)或者训练数据(training data), 是我们流程中的输入数据，一般称为x\n房屋销售价钱 - 输出数据，一般称为y\n拟合的函数（或者称为假设或者模型），一般写做 y = h(x)\n训练数据的条目数(#training set), 一条训练数据是由一对输入数据和输出数据组成的\n输入数据的维度(特征的个数，#features)，n\n下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。就如同上面的线性回归函数。\n我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向，等等，我们可以做出一个估计函数：\nθ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了：\n我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个函数称为损失函数（loss function）或者错误函数(error function)，描述h函数不好的程度，在下面，我们称这个函数为J函数\n在这儿我们可以做出下面的一个错误函数：\n这个错误估计函数是去对x(i)的估计值与真实值y(i)差的平方和作为错误估计函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。\n如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，在stanford机器学习开放课最后的部分会推导最小二乘法的公式的来源，这个来很多的机器学习和数学书上都可以找到，这里就不提最小二乘法，而谈谈梯度下降法。\n梯度下降法是按下面的流程进行的：\n1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。\n2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。\n为了更清楚，给出下面的图：\n这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低。也就是深蓝色的部分。θ0，θ1表示θ向量的两个维度。\n在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。\n然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如图所示，算法的结束将是在θ下降到无法继续下降为止。\n当然，可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，可能是下面的情况：\n上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点\n下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：（求导的过程如果不明白，可以温习一下微积分）\n下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。\n一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。\n用更简单的数学语言进行描述步骤2）是这样的：\n倒三角形表示梯度，按这种方式来表示，θi就不见了，看看用好向量和矩阵，真的会大大的简化数学的描述啊。\n总结与预告：\n本文中的内容主要取自stanford的课程第二集，希望我把意思表达清楚了：）本系列的下一篇文章也将会取自stanford课程的第三集，下一次将会深入的讲讲回归、logistic回归、和Newton法，不过本系列并不希望做成stanford课程的笔记版，再往后面就不一定完全与stanford课程保持一致了。"}
{"content2":"---------------------------------------------------------------------------------------\n本系列文章为《机器学习实战》学习笔记，内容整理自书本，网络以及自己的理解，如有错误欢迎指正。\n源码在Python3.5上测试均通过，代码及数据 --> https://github.com/Wellat/MLaction\n---------------------------------------------------------------------------------------\n1、算法概述\n1.1 朴素贝叶斯\n朴素贝叶斯是使用概率论来分类的算法。其中朴素：各特征条件独立；贝叶斯：根据贝叶斯定理。\n根据贝叶斯定理，对一个分类问题，给定样本特征x，样本属于类别y的概率是：\n-------（1）\n在这里，x 是一个特征向量，设 x 维度为 M。因为朴素的假设，即特征条件独立，根据全概率公式展开，上式可以表达为：\n这里，只要分别估计出，特征 Χi 在每一类的条件概率就可以了。类别 y 的先验概率可以通过训练集算出，同样通过训练集上的统计，可以得出对应每一类上的，条件独立的特征对应的条件概率向量。\n1.2 算法特点\n优点：在数据较少的情况下仍然有效，可以处理多类别问题。\n缺点：对于输入数据的准备方式较为敏感。\n适用数据类型：标称型数据。\n2、使用Python进行文本分类\n要从文本中获取特征，需要先拆分文本。可以把词条想象为单词，也可以使用非单词词条，如URL、IP地址或者任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档中，0表示词条未出现。\n2.1 准备数据：从文本中构建词向量\n1 from numpy import * 2 3 def loadDataSet(): 4 ''' 5 postingList: 进行词条切分后的文档集合 6 classVec:类别标签 7 ''' 8 postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], 9 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], 10 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], 11 ['stop', 'posting', 'stupid', 'worthless', 'garbage'], 12 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], 13 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] 14 classVec = [0,1,0,1,0,1] #1代表侮辱性文字，0代表正常言论 15 return postingList,classVec 16 17 def createVocabList(dataSet): 18 vocabSet = set([])#使用set创建不重复词表库 19 for document in dataSet: 20 vocabSet = vocabSet | set(document) #创建两个集合的并集 21 return list(vocabSet) 22 23 def setOfWords2Vec(vocabList, inputSet): 24 returnVec = [0]*len(vocabList)#创建一个所包含元素都为0的向量 25 #遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1 26 for word in inputSet: 27 if word in vocabList: 28 returnVec[vocabList.index(word)] = 1 29 else: print(\"the word: %s is not in my Vocabulary!\" % word) 30 return returnVec 31 ''' 32 我们将每个词的出现与否作为一个特征，这可以被描述为词集模型(set-of-words model)。 33 如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息, 34 这种方法被称为词袋模型(bag-of-words model)。 35 在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。 36 为适应词袋模型，需要对函数setOfWords2Vec稍加修改，修改后的函数称为bagOfWords2VecMN 37 ''' 38 def bagOfWords2VecMN(vocabList, inputSet): 39 returnVec = [0]*len(vocabList) 40 for word in inputSet: 41 if word in vocabList: 42 returnVec[vocabList.index(word)] += 1 43 return returnVec\n2.2 训练算法：从词向量计算概率\n计算每个类别的条件概率，伪代码：\n1 def trainNB0(trainMatrix,trainCategory): 2 ''' 3 朴素贝叶斯分类器训练函数(此处仅处理两类分类问题) 4 trainMatrix:文档矩阵 5 trainCategory:每篇文档类别标签 6 ''' 7 numTrainDocs = len(trainMatrix) 8 numWords = len(trainMatrix[0]) 9 pAbusive = sum(trainCategory)/float(numTrainDocs) 10 #初始化所有词出现数为1，并将分母初始化为2，避免某一个概率值为0 11 p0Num = ones(numWords); p1Num = ones(numWords)# 12 p0Denom = 2.0; p1Denom = 2.0 # 13 for i in range(numTrainDocs): 14 if trainCategory[i] == 1: 15 p1Num += trainMatrix[i] 16 p1Denom += sum(trainMatrix[i]) 17 else: 18 p0Num += trainMatrix[i] 19 p0Denom += sum(trainMatrix[i]) 20 #将结果取自然对数，避免下溢出，即太多很小的数相乘造成的影响 21 p1Vect = log(p1Num/p1Denom)#change to log() 22 p0Vect = log(p0Num/p0Denom)#change to log() 23 return p0Vect,p1Vect,pAbusive\n2.3 测试算法\n分类函数：\n1 def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): 2 ''' 3 分类函数 4 vec2Classify:要分类的向量 5 p0Vec, p1Vec, pClass1:分别对应trainNB0计算得到的3个概率 6 ''' 7 p1 = sum(vec2Classify * p1Vec) + log(pClass1) 8 p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) 9 if p1 > p0: 10 return 1 11 else: 12 return 0\n测试：\n1 def testingNB(): 2 listOPosts,listClasses = loadDataSet() 3 myVocabList = createVocabList(listOPosts) 4 trainMat=[] 5 for postinDoc in listOPosts: 6 trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) 7 #训练模型，注意此处使用array 8 p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses)) 9 testEntry = ['love', 'my', 'dalmation'] 10 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) 11 print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)) 12 testEntry = ['stupid', 'garbage'] 13 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) 14 print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n3、实例：使用朴素贝叶斯过滤垃圾邮件\n一般流程：\n3.1 切分文本\n将长字符串切分成词表，包括将大写字符转换成小写，并过滤字符长度小于3的字符。\n1 def textParse(bigString):# 2 ''' 3 文本切分 4 输入文本字符串，输出词表 5 ''' 6 import re 7 listOfTokens = re.split(r'\\W*', bigString) 8 return [tok.lower() for tok in listOfTokens if len(tok) > 2] 9\n3.2 使用朴素贝叶斯进行垃圾邮件分类\n1 def spamTest(): 2 ''' 3 垃圾邮件测试函数 4 ''' 5 docList=[]; classList = []; fullText =[] 6 for i in range(1,26): 7 #读取垃圾邮件 8 wordList = textParse(open('email/spam/%d.txt' % i,'r',encoding= 'utf-8').read()) 9 docList.append(wordList) 10 fullText.extend(wordList) 11 #设置垃圾邮件类标签为1 12 classList.append(1) 13 wordList = textParse(open('email/ham/%d.txt' % i,'r',encoding= 'utf-8').read()) 14 docList.append(wordList) 15 fullText.extend(wordList) 16 classList.append(0) 17 vocabList = createVocabList(docList)#生成次表库 18 trainingSet = list(range(50)) 19 testSet=[] # 20 #随机选10组做测试集 21 for i in range(10): 22 randIndex = int(random.uniform(0,len(trainingSet))) 23 testSet.append(trainingSet[randIndex]) 24 del(trainingSet[randIndex]) 25 trainMat=[]; trainClasses = [] 26 for docIndex in trainingSet:#生成训练矩阵及标签 27 trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) 28 trainClasses.append(classList[docIndex]) 29 p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses)) 30 errorCount = 0 31 #测试并计算错误率 32 for docIndex in testSet: 33 wordVector = bagOfWords2VecMN(vocabList, docList[docIndex]) 34 if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: 35 errorCount += 1 36 print(\"classification error\",docList[docIndex]) 37 print('the error rate is: ',float(errorCount)/len(testSet)) 38 #return vocabList,fullText\n4、实例：使用朴素贝叶斯分类器从个人广告中获取区域倾向\n一般流程：\n在这个中，我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的征婚广告信息，来比较这两个城市的人们在广告用词上是否不同 。\n4.1 实现代码\n1 ''' 2 函数localWords()与程序清单中的spamTest()函数几乎相同，区别在于这里访问的是 3 RSS源而不是文件。然后调用函数calcMostFreq()来获得排序最高的30个单词并随后将它们移除 4 ''' 5 def localWords(feed1,feed0): 6 import feedparser 7 docList=[]; classList = []; fullText =[] 8 minLen = min(len(feed1['entries']),len(feed0['entries'])) 9 for i in range(minLen): 10 wordList = textParse(feed1['entries'][i]['summary']) 11 docList.append(wordList) 12 fullText.extend(wordList) 13 classList.append(1) #NY is class 1 14 wordList = textParse(feed0['entries'][i]['summary']) 15 docList.append(wordList) 16 fullText.extend(wordList) 17 classList.append(0) 18 vocabList = createVocabList(docList)#create vocabulary 19 top30Words = calcMostFreq(vocabList,fullText) #remove top 30 words 20 for pairW in top30Words: 21 if pairW[0] in vocabList: vocabList.remove(pairW[0]) 22 trainingSet = list(range(2*minLen)); testSet=[] #create test set 23 for i in range(10): 24 randIndex = int(random.uniform(0,len(trainingSet))) 25 testSet.append(trainingSet[randIndex]) 26 del(trainingSet[randIndex]) 27 trainMat=[]; trainClasses = [] 28 for docIndex in trainingSet:#train the classifier (get probs) trainNB0 29 trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) 30 trainClasses.append(classList[docIndex]) 31 p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses)) 32 errorCount = 0 33 for docIndex in testSet: #classify the remaining items 34 wordVector = bagOfWords2VecMN(vocabList, docList[docIndex]) 35 if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: 36 errorCount += 1 37 print('the error rate is: ',float(errorCount)/len(testSet)) 38 return vocabList,p0V,p1V 39 40 def calcMostFreq(vocabList,fullText): 41 ''' 42 返回前30个高频词 43 ''' 44 import operator 45 freqDict = {} 46 for token in vocabList: 47 freqDict[token]=fullText.count(token) 48 sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True) 49 return sortedFreq[:30] 50 51 if __name__== \"__main__\": 52 #导入RSS数据源 53 import operator 54 ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss') 55 sf=feedparser.parse('http://sfbay.craigslist.org/stp/index.rss') 56 localWords(ny,sf)"}
{"content2":"【主页】 apachecn.org\n【Github】@ApacheCN\n暂时下线: 社区\n暂时下线: cwiki 知识库\n自媒体平台\n微博：@ApacheCN\n知乎：@ApacheCN\nCSDN\n简书\nOSChina\n博客园\n我们不是 Apache 的官方组织/机构/团体，只是 Apache 技术栈（以及 AI）的爱好者！\n合作or侵权，请联系【fonttian】fonttian@gmail.com | 请抄送一份到 apachecn@163.com\n预处理\n离散化\n等值分箱\n等量分箱\n独热 one-hot\n标准化\n最小最大 min-max\nz-score\nl2 标准化\n归一化\n特征选择\nANOVA\n信息增益/信息增益率\n模型验证\n评价指标\n回归\nMSE\nR 方\n分类\n准确率\n精确率\n召回率\nF1 得分\n宏平均 F1\n微平均 F1\n聚类\n互信息\n轮廓距离\n交叉验证\nK 折\n网格搜索\n最优化方法\n梯度下降\n随机梯度下降 SGD\n牛顿法/拟牛顿法\n动量法\nRMSProp\nAdam\n传统机器学习\n基本概念\n欠拟合/过拟合\n距离\n汉明距离\n曼哈顿距离\n欧几里得距离\n切比雪夫距离\n余弦相似度\npearson 相似度\n损失函数\nMSE\n交叉熵\nHinge\n线性模型\n线性回归\nLasso/岭回归\n正则化\n逻辑回归\nsoftmax 回归\n支持向量机\n拉格朗日对偶\n软边界支持向量机\n核方法\n树和森林\n决策树\n随机森林\nGDBT/XGBoost\nLightGBM\n集成学习\nBagging\nBoosting\nAdaboost\nBlending/Stacking\nKNN\n聚类\nKMenas\n层次聚类\n凝聚聚类\n分裂聚类\nDBSCAN\n谱聚类\n高斯混合模型 GMM\n概率图\n朴素贝叶斯\n隐马尔科夫 HMM\n降维\nPCA/SVD\nT-SNE\n深度学习\n基本概念\n正向传播\n反向传播\n激活函数\nsigmoid\nsoftmax\ntanh\nReLU\nELU\nLeaky ReLU\n丢弃 Dropout\n微调 Fine-Tune\n批量归一化 BatchNorm\n前馈神经网络 DNN/多层感知机 MLP\n输入层\n隐层\n输出层\n卷积神经网络 CNN\n层\n卷积层\n池化层\n全连接层\n经典结构\nLeNet\nAlexNet\nZFNet\nGoogLeNet\nVGG\nResNet\nDenseNet\n循环神经网络 RNN\n循环层\n经典结构\nLSTM\nGRU\nBiLSTM\n注意力\nSeq2Seq\n自编码器\n栈式自编码器\n稀疏自编码器\n去噪自编码器\n变分自编码器\n生成对抗网络 GAN\nDCGAN\n应用领域（待扩展）\n推荐系统\n机器视觉 CV\n自然语言处理 NLP\n生物信息\n常用工具\n数据分析\nNumPy\nPandas\n科学计算\nSciPy\n可视化\nMatplotlib\nSeaborn\n机器学习\nscikit-learn/sklearn\nXGBoost\nLightGBM\n深度学习\nKeras\nTensorFlow\nPyTorch"}
{"content2":"2.1 大数据预测因为\n1.Data grows,but not our insights.Noise that blinds our insights.\n2.No Big data for big complexity.在复杂系统面前，没有大数据。\n2.2 大数据与机器学习\n1.You need a machine to learn\n算法是拯救这一切的最重要办法，和非常有效的办法。\n2.Towards statistical learning 统计学习\n物理学或者动力学模型尝试掌握事务背后的原因和规律，再去预测事物的发展，但是对于复杂的系统，这种规律是失效的。\n统计模型具有的能力就是学习，根据数据进行随时调整。"}
{"content2":"隔壁有个妹纸喷我 ，好高兴....给她回复了下\n哎呀，没想到是个妹纸，其实我就随便那么一说，没合计妹纸还专门写个檄文声讨我，受宠若惊\n其实你的评论一看就比较专业，所以我就去你博客大概扫了一眼，发现个大问题，感觉就是一基层技术人员，所以就没当回事...\n包括你写的这个三月的学习思路，基本还是按码农那个路子来的，一上来就要撸起袖子敲代码。\n机器学习我肯定是门都没入啦，但要说如何从码农做到架构师做到技术总监，这方面我觉得肯定比你有经验。\n我想我俩的水平肯定都不是搞学术，而是搞应用级技术本质上并没有比传统信息系统技术栈复杂多少，长多少，我可以说在传统信息系统中弦哥这样的老程序员掌握的编程思想、编程哲学、架构设计、解决方案选型这方面肯定要甩你几条街的。这些难题同样会在机器学习应用级技术领域出现，也决定了你能走多远。\n我想你还年轻，你没见过也不会去想一个程序员的瓶颈，我年纪大点，也带了很多年团队，见过很多在这个瓶颈上痛苦徘徊迷茫的程序员，原因是啥？就是因为你的这种学习思路和方法。\n当然年轻人不服就干的精神是非常非常可贵的，支持你这个系列，一起学习，另外也别给自己太大压力哟。\n今天我们的学习目标是，搞清楚hand-crafted rules实现人工智能的方式及其缺陷，以及如何识别江湖骗子。\n上篇我们讲到在machine learning之前基本都是靠hand-crafted rules去实现所谓人工智能。这部分太简单，一般教科书都是以“我们都知道...”一笔带过，这无疑给不知道的同学带来一记重拳！这篇我们通过两个例子去讲。\n弦哥养了个狗子叫二丫，刚领回家的一段时间，只要二狗子撒尿，我就会喊“尿！”，像精神病似的喊了一个月，二狗子建立起了简单的逆向条件反射。\nif(有人喊尿)\n就蹲下撒尿\n一天去遛狗，对面迎来两个妹纸，弦哥大吼一声：“尿！”，二狗子立马蹲下就尿。其中的那个萌妹子：“哇！好聪明的狗狗！”（*萌妹子以为二狗子能听懂指令，感觉狗子具备智能。），另外一个妹纸是机器学习专业的，不以为然，学着弦哥的样子也吼了一声：“尿！”，二狗子立马又蹲下，然而由于刚尿完，所以只能做着蹲的姿势并没有尿，场面比较尴尬...（*如果是小孩就会真正理解尿这个指令的含义，这时候他会说：”daddy，我没有尿！“，而不会把裤子脱了。），这时候机器学习专业妹纸又得意的吼了一声：”不准尿！“，倒霉的二狗子又蹲下了，姿势显得更加猥琐，不解的看着我们...（*所谓的一些智能声控灯也有这个问题，你喊：”不要关灯！“，他也会把灯给你关了。）。我原本打算继续给萌妹子表演狗子拉屎、狗子拜年、狗子打滚、狗子做加法，让妹纸觉得狗子具备小孩的智商，（*这些都可以通过简单的if语句实现，组合起来会让机器看起来更具备智能。）但由于有行家在，只能悻悻的牵着笨狗子走了，搭讪失败！\n图为弦哥家只会hand-crafted rules的笨狗子\n小结：hand-crafted rules实现的人工智能，其本质就是通过许多if判断语句去实现，然而并不能完成比较复杂的任务，由于需要编写大量的if语句，工作量非常大不说还会经常出现一些可笑的错误，早期的一些所谓智能聊天机器人，基本都是用这种技术。\n第二个例子，弦哥在东北混，对喊麦十分在行，如果我说，能编写一个叫“社会你弦哥”的智能说唱喊麦机器人，你随便说个五言绝句，“社会你弦哥”就能喊麦唱出来，你信不信？。伪代码如下：\n将输入的一首五言诗分成4句；\n在每句的第二个字后面随机加上“我、他、那、这”的其中任意一个字；\n最后按“咚次哒次”的节奏加重音读出来；\n我们运行下这段程序试试，老师来音乐！大家的手举起来嗨~\n锄禾我日当午！\n汗滴他禾下土！\n谁知这盘中餐！\n粒粒他艰辛苦！\n再来一个， 接着嗨~\n离离那原上草！\n一岁他一枯荣！\n野火我烧不尽！\n春风这吹又生！\n据说有媒体采访一位专业的音乐评论家，让他谈谈对“MC天佑”喊麦的看法，他只说了一句“请尊重我的专业...\"。\n小结：基本的hand-crafted rules加上一些其他专业领域的基础知识，就被很多人用来忽悠是人工智能。\n这个结论不是我说的，一个国内业界小有名气的朋友私下告诉弦哥，目前人工智能领域市场估值很高，很多创业公司为了骗投资人和政府支持，搞的所谓人工智能产品就是这些东西。\n国外也一样，业界知名大牛尼古拉斯·赵四(*没听清名字)在facebook上发过一副漫画，说的也是这个事。\n图片来源：台大电机系李宏毅教授机器学习讲义截图\n谈个我并不是要说明IT圈比娱乐圈还乱，也不是不尊重业内的老师，毕竟我也搞过什么大数据主动服务之类的玩意，大家都混口饭吃不容易。只是想再次勉励同学们，人工智能没有想象的那么难，不要被业内动辄清华北大斯坦福的title吓到，王健林都说了清华北大不如胆子大，哈哈。比如deep learning已经属于应用级技术，有很多开源项目支撑，可以不太准确的说paper级别的理论咱们也用不到，且在商业应用领域意义也不大。\n当然这个系列会越来越难，遇到困难的时候，同学们要有信心、耐心和坚持！"}
{"content2":"1. Scikit-learn\nScikit-learn 是基于Scipy为机器学习建造的的一个Python模块，他的特色就是多样化的分类，回归和聚类的算法包括支持向量机，逻辑回归，朴素贝叶斯分类器，随机森林，Gradient Boosting，\n聚类算法和DBSCAN。而且也设计出了Python numerical和scientific libraries Numpy and Scipy\n2.Pylearn2\nPylearn是一个让机器学习研究简单化的基于Theano的库程序。\n3.NuPIC\nNuPIC是一个以HTM学习算法为工具的机器智能平台。HTM是皮层的精确计算方法。HTM的核心是基于时间的持续学习算法和储存和撤销的时空模式。NuPIC适合于各种各样的问题,尤其是检测异常和预测的\n流数据来源。\n4.Nilearn\nNilearn 是一个能够快速统计学习神经影像数据的Python模块。它利用Python语言中的scikit-learn 工具箱和一些进行预测建模，分类，解码，连通性分析的应用程序来进行多元的统计。\n5.PyBrain\nPybrain是基于Python语言强化学习，人工智能，神经网络库的简称。 它的目标是提供灵活、容易使用并且强大的机器学习算法和进行各种各样的预定义的环境中测试来比较你的算法。\n6.Pattern\nPattern 是Python语言下的一个网络挖掘模块。它为数据挖掘，自然语言处理，网络分析和机器学习提供工具。它支持向量空间模型、聚类、支持向量机和感知机并且用KNN分类法进行分类。\n7.Fuel\nFuel为你的机器学习模型提供数据。他有一个共享如MNIST, CIFAR-10 (图片数据集), Google’s One Billion Words (文字)这类数据集的接口。你使用他来通过很多种的方式来替代自己的数据。\n8.Bob\nBob是一个免费的信号处理和机器学习的工具。它的工具箱是用Python和C++语言共同编写的，它的设计目的是变得更加高效并且减少开发时间，它是由处理图像工具,音频和视频处理、机器学习和模式识别\n的大量软件包构成的。\n9.Skdata\nSkdata是机器学习和统计的数据集的库程序。这个模块对于玩具问题，流行的计算机视觉和自然语言的数据集提供标准的Python语言的使用。\n10.MILK\nMILK是Python语言下的机器学习工具包。它主要是在很多可得到的分类比如SVMS,K-NN,随机森林，决策树中使用监督分类法。 它还执行特征选择。 这些分类器在许多方面相结合,可以形成不同的例如\n无监督学习、密切关系金传播和由MILK支持的K-means聚类等分类系统。\n11.IEPY\nIEPY是一个专注于关系抽取的开源性信息抽取工具。它主要针对的是需要对大型数据集进行信息提取的用户和想要尝试新的算法的科学家。\n12.Quepy\nQuepy是通过改变自然语言问题从而在数据库查询语言中进行查询的一个Python框架。他可以简单的被定义为在自然语言和数据库查询中不同类型的问题。所以，你不用编码就可以建立你自己的一个用自然\n语言进入你的数据库的系统。\n现在Quepy提供对于Sparql和MQL查询语言的支持。并且计划将它延伸到其他的数据库查询语言。\n13.Hebel\nHebel是在Python语言中对于神经网络的深度学习的一个库程序，它使用的是通过PyCUDA来进行GPU和CUDA的加速。它是最重要的神经网络模型的类型的工具而且能提供一些不同的活动函数的激活功能，\n例如动力，涅斯捷罗夫动力，信号丢失和停止法。\n14.mlxtend\n它是一个由有用的工具和日常数据科学任务的扩展组成的一个库程序。\n15.nolearn\n这个程序包容纳了大量能对你完成机器学习任务有帮助的实用程序模块。其中大量的模块和scikit-learn一起工作，其它的通常更有用。\n16.Ramp\nRamp是一个在Python语言下制定机器学习中加快原型设计的解决方案的库程序。他是一个轻型的pandas-based机器学习中可插入的框架，它现存的Python语言下的机器学习和统计工具\n（比如scikit-learn,rpy2等）Ramp提供了一个简单的声明性语法探索功能从而能够快速有效地实施算法和转换。\n17.Feature Forge\n这一系列工具通过与scikit-learn兼容的API，来创建和测试机器学习功能。\n这个库程序提供了一组工具，它会让你在许多机器学习程序使用中很受用。当你使用scikit-learn这个工具时，你会感觉到受到了很大的帮助。（虽然这只能在你有不同的算法时起作用。）\n18.REP\nREP是以一种和谐、可再生的方式为指挥数据移动驱动所提供的一种环境。\n它有一个统一的分类器包装来提供各种各样的操作，例如TMVA, Sklearn, XGBoost, uBoost等等。并且它可以在一个群体以平行的方式训练分类器。同时它也提供了一个交互式的情节。\n19.Python 学习机器样品\n用亚马逊的机器学习建造的简单软件收集。\n20.Python-ELM\n这是一个在Python语言下基于scikit-learn的极端学习机器的实现。"}
{"content2":"各位工程师累了吗? 推荐一篇可以让你技术能力达到出神入化的网站[\"宅男门诊\"](https://zhainanmenzhen.com/)\n1、使用机器学习来解决问题，我们用数学语言来描述它，然后建立一个模型，例如回归模型或者分类模型等来描述这个问题；\n2、通过最小化误差、最大似然、最大后验概率等等建立模型的代价函数，转化为最优化问题。找到最优化问题的解，也就是能拟合我们的数据的最好的模型参数；\n3、求解这个代价函数，找到最优解。\n求最优解方法：\n1、如果优化函数存在解析解。例如我们求最值一般是对优化函数求导，找到导数为0的点。如果代价函数能简单求导，并且求导后为0的式子存在解析解，那么我们就可以直接得到最优的参数。\n2、如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，互相依赖的情况。或者求导后式子得不到解释解，或者未知参数的个数大于方程组的个数等。这时候使用迭代算法来一步一步找到最优解。\n特别的若优化函数是凸函数，那么就存在全局最优解，如果函数是非凸的，那么就会有很多局部最优的解，因此凸优化重要性不言而喻。人们总希望在万千事物中找到最优的那个他。\n1.什么是机器学习\n计算机自动从数据中发现规律，并应用于解决新问题\n给定数据(X1,Y1), (X2,Y2), … ,(Xn,Yn)，机器自动学习X和Y之间的关系，从而对新的Xi，能够预测Yi。如由身高预测性别，身高预测体重。\n机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。\n2.基于规则\n3.基于模型- 机器学习\n4.实例-房价预测\n5.基本概念\n我们先明确机器学习中一些概念和常用的符号:\n房屋销售记录表 训练集(training set)或者训练数据(training data), 一般称为x\n房屋销售价钱 输出数据，一般称为y\n拟合的函数 （模型、假设），一般写做 y = h(x)\n训练数据的条数(training set) 一条训练数据是由一对输入和输出数据组成的\n输入数据的维度(特征的个数features) 房屋的售价，数据表中的列\n6.机器学习过程\n基本概念：\n7.机器学习主要问题\n分类：LR,SVM,NB,KNN,决策树\nLR(logistic regression),SVM()，NB(naive bayes ),KNN（k-nearest neighbor）,决策树\n聚类：k均值(k-means)，层次，GMM（高斯混合模型）\n回归：线性回归，逻辑回归\n关联规则：Apriori,FPgrowth\n8.监督与非监督学习\n监督学习：\n给定数据(X1,Y1)，(X2,Y2)，…，(Xn,Yn)\n对新的Xi，预测其Yi\n分类，回归\n非监督学习：\n给定数据X1，X2，…，Xn\n求Yi=f(Xi)，P(Xi,Yi)\n聚类，降维\n9.机器学习三要素\n模型---规律\n策略---模型好不好\n10.经验风险\n11.结构风险\n12.算法\n13.正则化\n正则化(regularization)在线性代数理论中，不适定问题通常是由一组线性代数方程定义的，而且这组方程组通常来源于有着很大的条件数的不适定反问题\n14.交叉验证\n1. 简单交叉验证\n简单交叉验证的方法是这样的，随机从最初的样本中选择部分，形成验证数据，而剩下的当作训练数据。一般来说，少于三分之一的数据被选作验证数据。\n2. K折交叉验证\n10折交叉验证是把样本数据分成10份，轮流将其中9份做训练数据，将剩下的1份当测试数据，10次结果的均值作为对算法精度的估计，通常情况下为了提高精度，还需要做多次10折交叉验证。\n更进一步，还有K折交叉验证，10折交叉验证是它的特殊情况。K折交叉验证就是把样本分为K份，其中K-1份用来做训练建立模型，留剩下的一份来验证，交叉验证重复K次，每个子样本验证一次。\n3. 留一验证\n留一验证只使用样本数据中的一项当作验证数据，而剩下的全作为训练数据，一直重复，直到所有的样本都作验证数据一次。可以看出留一验证实际上就是K折交叉验证，只不过这里的K有点特殊，K为样本数据个数。\n15.泛化能力（预测能力）\n泛化能力指由学习方法得到的模型对未知数据的预测能力。\n概括地说，所谓泛化能力（generalization ability）是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据对背后的规律，对具有同一规律的学习集以外的数据，经过训练的算法也能给出合适的输出，该能力称为泛化能力。\n16.模型评估与模型选择\n当损失函数给定时，基于损失函数的模型的训练误差和模型的测试误差就自然成为学习方法评估的标准。\n通常，测试误差越小的方法具有更好的预测能力，泛化能力强。\n17.过拟合与模型选择\n18.总结"}
{"content2":"文本分类的定义\n文本分类是现在非常热门的一个研究领域，也是机器学习中最为重要最为基础的组成部分。文本分类有各种各样的方法，有些简单易懂，有些看上去非常复杂。其实只要搞清楚他们背后的原理，理解文本分类并不是一件很困难的事情。今天先从宏观上介绍一下文本分类，后续会在其他博文中分门别类对文本分类这一课题进行深入的分析，敬请关注。也希望各位高手们多提建议，毕竟我也是菜鸟一个。\n文本分类就是将一篇文章归入已有的几个类别当中，这里注重强调2点：\n1 要分类的类别必须是事先确定的，并且短时间内不会发生改变。\n2 分类的类别并不一定唯一。\n文本分类的方法\n1 人工制订规则\n这种方法最大的弊端就是需要牵扯太大的人力，成本极高。对人的要求也极高，很难通过文章抽象出属于某个分类的规则。而且这种方法灵活性太小，很难适应语言的发展，因此极少有人使用。\n2 统计学方法\n统计学习方法的基本思想就是：让机器像人类一样自己来通过对大量同类文档的观察来自己总结经验，作为今后分类的依据。统计学习方法需要一批由人工进行了准确分类的文档作为学习的材料（称为训练集，注意由人分类一批文档比从这些文档中总结出准确的规则成本要低得多），计算机从这些文档重挖掘出一些能够有效分类的规则，这个过程被形象的称为训练，而总结出的规则集合常常被称为分类器。训练完成之后，需要对计算机从来没有见过的文档进行分类时，便使用这些分类器来进行。\n统计学习方法\n之前已经提到过，这种方法就是让计算机自己去学习已经分类好的训练集，然而计算机不是人类，无法按人类理解文章那样来学习，这就给文本分类提出了一个重要的课题。因此如何表示一篇文章，让计算机能够理解就成了重中之重。我们知道，文章的语义信息是很难用计算机可以识别的形式所描述的，因此我们只能退而求其次，用文章中所包含的较低级别的词汇信息来表示文档。事实证明，这种做法的效果也是不错的。\n进一步的，不光是包含哪些词很重要，这些词出现的次数对分类也很重要。\n再确定如何表示文档之后，就要讨论如何让计算机去学习文档了，也就是我们说的训练。\n在训练过程中，每一个实例被称为一个样本，这些样本是由人工进行分类处理过的文档集合，计算机认为这些数据的分类是绝对正确的，可以信赖的。之后，让计算机去观察学习这些样本，来猜出一个可能的分类规则，在机器学习中，这种猜测出来的规则称为假设。然后当遇到要分类的文档时，就是用我们的假设来进行判断，并为文档进行分类。\n举个例子说明，人们评价一辆车是否是“好车”的时候，可以看作一个分类问题。我们也可以把一辆车的所有特征提取出来转化为向量形式。在这个问题中词典向量可以为： D=（价格，最高时速，外观得分，性价比，稀有程度）\n保时捷：vp=（200万，320，9.5，3，9）\n花冠： vt=（15万，220，6.0，8，3）\n不同的人会有不同的评价标准，如果以性价比来看，很显然花冠是首选。如果考虑时速，外观和时尚，当然要选保时捷了。可见，对同一个分类问题，用同样的表示形式（同样的文档模型），但因为关注数据不同方面的特性而可能得到不同的结论。这种对文档数据不同方面侧重的不同导致了原理和实现方式都不尽相同的多种方法，每种方法也都对文本分类这个问题本身作了一些有利于自身的假设和简化，这些假设又接下来影响着依据这些方法而得到的分类器最终的表现。\n常用的分类方法\n分类方法可以说是机器学习领域人们研究的最多的一个部分，目前也有很多成熟的算法。比如决策树，Rocchio，朴素贝叶斯，神经网络，支持向量机，线性最小平方拟合，kNN，遗传算法，最大熵等等，下面就挑选几个跟大家简单介绍一下，以后的博文会陆续的对其中的一些方法做详细的阐述，希望大家常来看看，欢迎拍砖。\n1 Rocchio算法\n这个方法的思想就是把一个分类中的所有文档的向量去平均值，得到一个新的向量，相当于分类的质心。再有新文档需要判断的时候，比较新文档和质心有多么相像，也就是计算新文档和质心的距离。通过距离的远近判断是否属于该分类。另外有改进的Rocchio算法除了考虑所有正样本的质心之外，还考虑不属于该文档的所有文档的质心。这样一来，新文档应该接近正样本的质心，远离负样本的质心。\n该算法有致命的缺陷：\n首先该算法假设所有同类文档都是聚集在一个质心的周围，这显然没有任何依据，事实证明也并非如此。\n另外一个弊端是该算法认为训练数据绝对正确，这在很多应用中是无法保证的。\n2 朴素贝叶斯算法\n贝叶斯算法关注的是文档属于某类别概率。文档属于某个类别的概率等于文档中每个词属于该类别的概率的综合表达式。而每个词属于该类别的概率又在一定程度上 可以用这个词在该类别训练文档中出现的次数（词频信息）来粗略估计，因而使得整个计算过程成为可行的。使用朴素贝叶斯算法时，在训练阶段的主要任务就是估 计这些值。\n同样的，该方法也有一定的缺陷：\n首先，P(d| Ci)之所以能展开成（式1）的连乘积形式，就是假设一篇文章中的各个词之间是彼此独立的，其中一个词的出现丝毫不受另一个词的影响。但这显然不对，即使不是语言学专家的我们也知道，词语之间有明显的所谓“共现”关系，在不同主题的文章中，可能共现的次数或频率有变化，但彼此间绝对谈不上独立。\n其次，使用某个词在某个类别训练文档中出现的次数来估计P(wi|Ci)时，只在训练样本数量非常多的情况下才比较准确，而需要大量样本的要求不仅给前期人工分类的工作带来更高要求（从而成本上升），在后期由计算机处理的时候也对存储和计算资源提出了更高的要求。\n3 kNN算法\nkNN算法则又有所不同，在kNN算法看来，训练样本就代表了类别的准确信息，因此此算法产生的分类器也叫做“基于实例”的分类器，而不管样本是使用什么特征表示的。其基本思想是在给定新文档后，计算新文档特征向量和训练文档集中各个文档的向量的相似度，得到K篇与该新文档距离最近最相似的文档， 根据这K篇文档所属的类别判定新文档所属的类别（注意这也意味着kNN算法根本没有真正意义上的“训练”阶段）。这种判断方法很好的克服了Rocchio 算法中无法处理线性不可分问题的缺陷，也很适用于分类标准随时会产生变化的需求（只要删除旧训练文档，添加新训练文档，就改变了分类的准则）。\nkNN唯一的也可以说最致命的缺点就是判断一篇新文档的类别时，需要把它与现存的所有训练文档全都比较一遍，这个计算代价并不是每个系统都能够承受的 （比如我将要构建的一个文本分类系统，上万个类，每个类即便只有20个训练样本，为了判断一个新文档的类别，也要做20万次的向量比较！）。一些基于 kNN的改良方法比如Generalized Instance Set就在试图解决这个问题。\n特征选择的方法\n大家可能会觉得奇怪，正说着文本分类方法，怎么突然就跳到特征选择上面了。这是因为特征选择在文本分类乃至机器学习中都起到了至关重要的作用。特征选择的好，既可以充分利用训练数据，又可以将数据量进行有效的精简，从而减少计算代价，有效的防治过拟合和欠拟合的发生。常用的特征选择算法有互信息，文档频率，信息增益，开方检验等。具体的特征选择方法我会在以后的博文中进行说明，大家一起期待吧。\n今天就写到这里，大概介绍了文本分类的定义，分类和常用算法，以后会针对不同的分类进行单独详细的讲解。"}
{"content2":"目录\n导数、偏导数和方向导数\n方向导数的推导过程\n方向导数和梯度\nReferences\n相关博客\n最近学习《最优化导论》，遇到了“方向导数”这一概念，故对其及相关概念进行一遍梳理。并给出方向导数的推导过程。\n导数、偏导数和方向导数\n在一元可导函数 \\(y = f(x)\\) 中，导数 \\(f'(x_0)\\) 即是曲线上 \\(x = x_0\\) 处的斜率。按照定义求导数：\n\\[ f'(x) = \\lim_{\\Delta x \\to 0}\\frac{f(x+ \\Delta x) - f(x)}{\\Delta x} \\tag{1} \\]\n当然，我们也可以通过各种求导法则来计算导数。\n对一个 \\(R^m \\to R\\) 的多元可导函数，\\(y=f(\\bm x),\\bm x = [x_1, x_2, ..., x_m]^\\top\\)，我们能够求的导数就多，如偏导数、方向导数，但归根到底，这些导数都可以认为是曲面上一点在某个方向的斜率。对于 \\(m\\le 2\\) 的情况，我们还能够通过坐标系很直观地了解；当 \\(m > 2\\) 时，我们可以从向量空间的角度理解。\n偏导数是指 \\(y=f(\\bm x)\\) 对 \\(\\bm x = [x_1, x_2, ..., x_m]^\\top\\) 中的某一维进行求导，如下式（2）所示，对第 \\(i\\) 维求偏导数：\n\\[ \\begin{split} \\frac{\\partial f(\\bm x)}{\\partial x_i} &= \\frac{\\partial f(x_1, x_2, ...,x_i,..., x_m)}{\\partial x_i} \\\\ &= \\lim_{\\Delta x_i \\to 0}\\frac{f(x_1, x_2, ...,x_i + \\Delta x_i,..., x_m) - f(x_1, x_2, ...,x_i,..., x_m)}{\\Delta x_i} \\end{split} \\tag{2} \\]\n方向导数就更好理解了，\\(y=f(\\bm x)\\) 对 \\(\\bm x = [x_1, x_2, ..., x_m]^\\top\\) 构成的向量空间 \\(R^m\\) 中某一方向 \\(\\bm d' = [\\Delta x_1, \\Delta x_2, ..., \\Delta x_m]^\\top\\) 求导数，即得到该方向上的方向导数 \\(\\frac{\\partial f(\\bm x)}{\\partial \\bm d'}\\)，如式（3）所示：\n\\[ \\begin{split} \\frac{\\partial f(\\bm x)}{\\partial \\bm d'} &= \\frac{\\partial f(x_1, x_2,..., x_m)}{\\partial x_i} \\\\ &= \\lim_{\\rho \\to 0}\\frac{f(x_1 + \\Delta x_1, x_2 +\\Delta x_2, ..., x_m +\\Delta x_m) - f(x_1, x_2, ..., x_m)}{\\rho} \\\\ &\\rho = \\sqrt{\\Delta x_1^2 + \\Delta x_2^2 + \\cdots +\\Delta x_m^2} \\end{split} \\tag{3} \\]\n方向导数和偏导数是什么关系？对于多元可导函数 \\(y=f(\\bm x),\\bm x = [x_1, x_2, ..., x_m]^\\top\\)，在其上任一点 \\(\\bm x_i\\)，我们都可以在向量空间 \\(R^m\\) 中的每一个方向都可以计算一个方向导数，也就是超平面上点 \\(\\bm x_i\\) 在每一个方向切线的“斜率”。这里“每一个方向”自然包括各个偏导数的方向。即偏导数构成的集合 A 是方向导数构成集合 B 的子集。\n方向导数的推导过程\n\\(f(\\boldsymbol x)\\) 是一个 \\(R^m \\to R\\) 的函数，如果我们要求 \\(f(\\boldsymbol x)\\) 在任一点 \\(\\boldsymbol x_0 = [x_1^{0}, x_2^{0}, ..., x_m^{0}]^\\top\\) 点方向为 \\(\\boldsymbol d\\) 的方向导数，那么按照定义，我们得到如下公式：\n\\[ \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol d}\\mid_{\\boldsymbol x = \\boldsymbol x_0} = \\lim_{\\alpha \\to 0}\\frac{f(\\boldsymbol x_0 + \\alpha \\boldsymbol d) - f(\\boldsymbol x_0)}{\\alpha} \\tag{4} \\]\n式（4）中，\\(\\boldsymbol d\\) 为单位向量。公式（4）其实是公式（3）的向量形式。（plus：公式（3）中 \\(d'\\) 不是单位向量，故加上 \\('\\) 来区分）\n设 \\(g(\\alpha) = f(x_0+\\alpha \\boldsymbol d)\\)，我们注意到，\\(g(0) = f(x_0)\\)，所以，式（4）又可以写为：\n\\[ \\begin{split} \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol d}\\mid_{\\boldsymbol x = \\boldsymbol x_0} & = \\lim_{\\alpha \\to 0}\\frac{g(\\alpha) - g(0)}{\\alpha} \\\\ &= \\frac{d g(\\alpha)}{d \\alpha}\\mid_{\\alpha = 0} \\\\ &= \\frac{d f(\\boldsymbol x_0+\\alpha \\boldsymbol d)}{d \\alpha}|_{\\alpha = 0} \\\\ &= \\nabla f(\\boldsymbol x_0)^\\top\\boldsymbol d \\\\ &= <\\nabla f(\\boldsymbol x_0), \\boldsymbol d> \\\\ &= \\boldsymbol d^\\top\\nabla f(\\boldsymbol x_0) \\end{split} \\tag{5} \\]\n所以，\n\\[ \\frac{\\partial f(\\boldsymbol x)}{\\partial \\boldsymbol d}= \\boldsymbol d^\\top\\nabla f(\\boldsymbol x) \\tag{6} \\]\n方向导数和梯度\n首先明确，导数是一个值，代表切线的斜率，而梯度是一个向量。最大方向导数的方向就是梯度代表的方向。\n梯度是 \\(f(\\bm x)\\) 对各个自变量\\(\\bm x = [x_1, x_2, ..., x_m]^\\top\\) 每一维分别求偏导数得到的向量。\n从式（5）和（6）中我们也可以知道，当 \\(\\bm d = \\frac{\\nabla f(\\bm x)}{\\|\\nabla f(\\bm x)\\|}\\)，方向导数最大。 最大方向导数的方向就是梯度，最大的方向导数就是梯度的欧几里德范数。\nReferences\n如何直观形象的理解方向导数与梯度以及它们之间的关系？-- 马同学\n方向导数与梯度——学习笔记 -- Reclusiveman\n[机器学习] ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent）-- WangBo_NLPR\nEdwin K. P. Chong, Stanislaw H. Zak-An Introduction to Optimization, 4th Edition\n相关博客\n【机器学习之数学】01 导数、偏导数、方向导数、梯度\n【机器学习之数学】02 梯度下降法、最速下降法、牛顿法、共轭方向法、拟牛顿法\n【机器学习之数学】03 有约束的非线性优化问题——拉格朗日乘子法、KKT条件、投影法"}
{"content2":"from:http://www.zhizhihu.com/html/y2009/410.html\n机器学习是计算机科学和统计学的边缘交叉领域，R关于机器学习的包主要包括以下几个方面：\n1）神经网络（Neural Networks）：\nnnet包执行单隐层前馈神经网络，nnet是VR包的一部分（http://cran.r-project.org/web/packages/VR/index.html）。\n2）递归拆分（Recursive Partitioning）：\n递归拆分利用树形结构模型，来做回归、分类和生存分析，主要在rpart包（http://cran.r-project.org/web/packages/rpart/index.html）和tree包（http://cran.r-project.org/web/packages/tree/index.html）里执行，尤其推荐rpart包。Weka里也有这样的递归拆分法，如：J4.8, C4.5, M5，包Rweka提供了R与Weka的函数的接口（http://cran.r-project.org/web/packages/RWeka/index.html）。\nparty包提供两类递归拆分算法，能做到无偏的变量选择和停止标准：函数ctree()用非参条件推断法检测自变量和因变量的关系；而函数mob()能用来建立参数模型（http://cran.r-project.org/web/packages/party/index.html）。另外，party包里也提供二分支树和节点分布的可视化展示。\nmvpart包是rpart的改进包，处理多元因变量的问题（http://cran.r-project.org/web/packages/mvpart/index.html）。rpart.permutation包用置换法（permutation）评估树的有效性（http://cran.r-project.org/web/packages/rpart.permutation/index.html）。knnTree包建立一个分类树，每个叶子节点是一个knn分类器（http://cran.r-project.org/web/packages/knnTree/index.html）。LogicReg包做逻辑回归分析，针对大多数自变量是二元变量的情况（http://cran.r-project.org/web/packages/LogicReg/index.html）。maptree包（http://cran.r-project.org/web/packages/maptree/index.html）和pinktoe包（http://cran.r-project.org/web/packages/pinktoe/index.html）提供树结构的可视化函数。\n3）随机森林（Random Forests）：\nrandomForest 包提供了用随机森林做回归和分类的函数（http://cran.r-project.org/web/packages/randomForest/index.html）。ipred包用bagging的思想做回归，分类和生存分析，组合多个模型（http://cran.r-project.org/web/packages/ipred/index.html）。party包也提供了基于条件推断树的随机森林法（http://cran.r-project.org/web/packages/party/index.html）。varSelRF包用随机森林法做变量选择（http://cran.r-project.org/web/packages/varSelRF/index.html）。\n4）Regularized and Shrinkage Methods：\nlasso2包（http://cran.r-project.org/web/packages/lasso2/index.html）和lars包（http://cran.r-project.org/web/packages/lars/index.html）可以执行参数受到某些限制的回归模型。elasticnet包可计算所有的收缩参数（http://cran.r-project.org/web/packages/elasticnet/index.html）。glmpath包可以得到广义线性模型和COX模型的L1 regularization path（http://cran.r-project.org/web/packages/glmpath/index.html）。penalized包执行lasso (L1) 和ridge (L2)惩罚回归模型（penalized regression models）（http://cran.r-project.org/web/packages/penalized/index.html）。pamr包执行缩小重心分类法(shrunken centroids classifier)（http://cran.r-project.org/web/packages/pamr/index.html）。earth包可做多元自适应样条回归（multivariate adaptive regression splines）（http://cran.r-project.org/web/packages/earth/index.html）。\n5）Boosting :\ngbm包（http://cran.r-project.org/web/packages/gbm/index.html）和boost包（http://cran.r-project.org/web/packages/boost/index.html）执行多种多样的梯度boosting算法，gbm包做基于树的梯度下降boosting，boost包包括LogitBoost和L2Boost。GAMMoost包提供基于boosting的广义相加模型(generalized additive models)的程序（http://cran.r-project.org/web/packages/GAMMoost/index.html）。mboost包做基于模型的boosting（http://cran.r-project.org/web/packages/mboost/index.html）。\n6）支持向量机（Support Vector Machines）：\ne1071包的svm()函数提供R和LIBSVM的接口 （http://cran.r-project.org/web/packages/e1071/index.html）。kernlab包为基于核函数的学习方法提供了一个灵活的框架，包括SVM、RVM……(http://cran.r-project.org/web/packages/kernlab/index.html) 。klaR 包提供了R和SVMlight的接口（http://cran.r-project.org/web/packages/klaR/index.html）。\n7）贝叶斯方法（Bayesian Methods）：\nBayesTree包执行Bayesian Additive Regression Trees (BART)算法（http://cran.r-project.org/web/packages/BayesTree/index.html，http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/BART%206--06.pdf）。tgp包做Bayesian半参数非线性回归（Bayesian nonstationary, semiparametric nonlinear regression）（http://cran.r-project.org/web/packages/tgp/index.html）。\n8）基于遗传算法的最优化（Optimization using Genetic Algorithms）：\ngafit包（http://cran.r-project.org/web/packages/gafit/index.html）和rgenoud包（http://cran.r-project.org/web/packages/rgenoud/index.html）提供基于遗传算法的最优化程序。\n9）关联规则（Association Rules）：\narules包提供了有效处理稀疏二元数据的数据结构，而且提供函数执Apriori和Eclat算法挖掘频繁项集、最大频繁项集、闭频繁项集和关联规则（http://cran.r-project.org/web/packages/arules/index.html）。\n10）模型选择和确认（Model selection and validation）：\ne1071包的tune()函数在指定的范围内选取合适的参数（http://cran.r-project.org/web/packages/e1071/index.html）。ipred包的errorest()函数用重抽样的方法（交叉验证，bootstrap）估计分类错误率（http://cran.r-project.org/web/packages/ipred/index.html）。svmpath包里的函数可用来选取支持向量机的cost参数C（http://cran.r-project.org/web/packages/svmpath/index.html）。ROCR包提供了可视化分类器执行效果的函数，如画ROC曲线（http://cran.r-project.org/web/packages/ROCR/index.html）。caret包供了各种建立预测模型的函数，包括参数选择和重要性量度（http://cran.r-project.org/web/packages/caret/index.html）。caretLSF包（http://cran.r-project.org/web/packages/caretLSF/index.html）和caretNWS（http://cran.r-project.org/web/packages/caretNWS/index.html）包提供了与caret包类似的功能。\n11）统计学习基础（Elements of Statistical Learning）：\n书《The Elements of Statistical Learning: Data Mining, Inference, and Prediction 》（http://www-stat.stanford.edu/~tibs/ElemStatLearn/）里的数据集、函数、例子都被打包放在ElemStatLearn包里（http://cran.r-project.org/web/packages/ElemStatLearn/index.html）。\n网址：http://cran.r-project.org/web/views/MachineLearning.html维护人员：Torsten Hothorn"}
{"content2":"不多说，直接上干货！\n我的集群机器情况是 bigdatamaster（192.168.80.10）、bigdataslave1（192.168.80.11）和bigdataslave2（192.168.80.12）\n然后，安装目录是在/home/hadoop/app下。\n官方建议在master机器上安装Hue，我这里也不例外。安装在bigdatamaster机器上。\nHue版本：hue-3.9.0-cdh5.5.4\n需要编译才能使用（联网）\n说给大家的话：大家电脑的配置好的话，一定要安装cloudera manager。毕竟是一家人的。\n同时，我也亲身经历过，会有部分组件版本出现问题安装起来要个大半天时间去排除，做好心里准备。废话不多说，因为我目前读研，自己笔记本电脑最大8G，只能玩手动来练手。\n纯粹是为了给身边没高配且条件有限的学生党看的！ 但我已经在实验室机器群里搭建好cloudera manager 以及 ambari都有。\n大数据领域两大最主流集群管理工具Ambari和Cloudera Manger\nCloudera安装搭建部署大数据集群（图文分五大步详解）（博主强烈推荐）\nAmbari安装搭建部署大数据集群（图文分五大步详解）（博主强烈推荐）\n一、默认的spark配置文件\n########################################################################### # Settings to configure the Spark application. ########################################################################### [spark] # Host address of the Livy Server. ## livy_server_host=localhost # Port of the Livy Server. ## livy_server_port=8998 # Configure livy to start with 'process', 'thread', or 'yarn' workers. ## livy_server_session_kind=process # If livy should use proxy users when submitting a job. ## livy_impersonation_enabled=true # List of available types of snippets ## languages='[{\"name\": \"Scala Shell\", \"type\": \"spark\"},{\"name\": \"PySpark Shell\", \"type\": \"pyspark\"},{\"name\": \"R Shell\", \"type\": \"r\"},{\"name\": \"Jar\", \"type\": \"Jar\"},{\"name\": \"Python\", \"type\": \"py\"},{\"name\": \"Impala SQL\", \"type\": \"impala\"},{\"name\": \"Hive SQL\", \"type\": \"hive\"},{\"name\": \"Text\", \"type\": \"text\"}]'\n二、以下是跟我机器集群匹配的配置文件（非HA集群下怎么配置Hue的spark模块）\n我的spark是安装在bigdatamaster、bigdataslave1和bigdataslave2机器上。\n注意： 要在Hue中使用Spark还需要安装spark-jobserver，但是这个东西没在CDH中，所以我们必须手动安装spark-jobserver 先要安装SBT。SBT = (not so) Simple Build Tool,是scala的构建工具，与java的maven地位相同。\ncurl https://bintray.com/sbt/rpm/rpm > bintray-sbt-rpm.repo sudo mv bintray-sbt-rpm.repo /etc/yum.repos.d/ sudo yum install sbt\n安装好SBT后，安装spark-jobserver\ngit clone https://github.com/ooyala/spark-jobserver.git cd spark-jobserver sbt re-start\n编辑jobserver 配置文件，将jobserver跟你的spark-master连接上。编辑 job-server/src/main/resources/application.conf 修改master属性\nmaster = \"spark://bigdatamaster:7077\"\n编辑 hue.ini 找到 [spark] 段落，修改 server_url 为正确的地址\n[spark] # URL of the REST Spark Job Server. server_url=http://host1:8090/\n三、以下是跟我机器集群匹配的配置文件（HA集群下怎么配置Hue的pig模块）\n跟非HA集群一样的配法。\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"版权声明：\n本文由LeftNotEasy发布于http://leftnoteasy.cnblogs.com, 本文可以被全部的转载或者部分使用，但请注明出处，如果有问题，请联系wheeleast@gmail.com。也可以加我的微博: @leftnoteasy\n前言：\n上一次写了关于PCA与LDA的文章，PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。在上篇文章中便是基于特征值分解的一种解释。特征值和奇异值在大部分人的印象中，往往是停留在纯粹的数学计算中。而且线性代数或者矩阵论里面，也很少讲任何跟特征值与奇异值有关的应用背景。奇异值分解是一个有着很明显的物理意义的一种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。就像是描述一个人一样，给别人描述说这个人长得浓眉大眼，方脸，络腮胡，而且带个黑框的眼镜，这样寥寥的几个特征，就让别人脑海里面就有一个较为清楚的认识，实际上，人脸上的特征是有着无数种的，之所以能这么描述，是因为人天生就有着非常好的抽取重要特征的能力，让机器学会抽取重要的特征，SVD是一个重要的方法。\n在机器学习领域，有相当多的应用与奇异值都可以扯上关系，比如做feature reduction的PCA，做数据压缩（以图像压缩为代表）的算法，还有做搜索引擎语义层次检索的LSI（Latent Semantic Indexing）\n另外在这里抱怨一下，之前在百度里面搜索过SVD，出来的结果都是俄罗斯的一种狙击枪（AK47同时代的），是因为穿越火线这个游戏里面有一把狙击枪叫做SVD，而在Google上面搜索的时候，出来的都是奇异值分解（英文资料为主）。想玩玩战争游戏，玩玩COD不是非常好吗，玩山寨的CS有神马意思啊。国内的网页中的话语权也被这些没有太多营养的帖子所占据。真心希望国内的气氛能够更浓一点，搞游戏的人真正是喜欢制作游戏，搞Data Mining的人是真正喜欢挖数据的，都不是仅仅为了混口饭吃，这样谈超越别人才有意义，中文文章中，能踏踏实实谈谈技术的太少了，改变这个状况，从我自己做起吧。\n前面说了这么多，本文主要关注奇异值的一些特性，另外还会稍稍提及奇异值的计算，不过本文不准备在如何计算奇异值上展开太多。另外，本文里面有部分不算太深的线性代数的知识，如果完全忘记了线性代数，看本文可能会有些困难。\n一、奇异值与特征值基础知识：\n特征值分解和奇异值分解在机器学习领域都是属于满地可见的方法。两者有着很紧密的关系，我在接下来会谈到，特征值分解和奇异值分解的目的都是一样，就是提取出一个矩阵最重要的特征。先谈谈特征值分解吧：\n1）特征值：\n如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：\n这时候λ就被称为特征向量v对应的特征值，一个矩阵的一组特征向量是一组正交向量。特征值分解是将一个矩阵分解成下面的形式：\n其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角阵，每一个对角线上的元素就是一个特征值。我这里引用了一些参考文献中的内容来说明一下。首先，要明确的是，一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。比如说下面的一个矩阵：\n它其实对应的线性变换是下面的形式：\n因为这个矩阵M乘以一个向量(x,y)的结果是：\n上面的矩阵是对称的，所以这个变换是一个对x，y轴的方向一个拉伸变换（每一个对角线上的元素将会对一个维度进行拉伸变换，当值>1时，是拉长，当值<1时时缩短），当矩阵不是对称的时候，假如说矩阵是下面的样子：\n它所描述的变换是下面的样子：\n这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最主要的变化方向（变化方向可能有不止一个），如果我们想要描述好一个变换，那我们就描述好这个变换主要的变化方向就好了。反过头来看看之前特征值分解的式子，分解得到的Σ矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）\n当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。也就是之前说的：提取这个矩阵最重要的特征。总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。\n（说了这么多特征值变换，不知道有没有说清楚，请各位多提提意见。）\n2）奇异值：\n下面谈谈奇异值分解。特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法：\n假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量），从图片来反映几个相乘的矩阵的大小可得下面的图片\n那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置 * A，将会得到一个方阵，我们用这个方阵求特征值可以得到：    这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：\n这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：\nr是一个远小于m、n的数，这样矩阵的乘法看起来像是下面的样子：\n右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵：U、Σ、V就好了。\n二、奇异值的计算：\n奇异值的计算是一个难题，是一个O(N^3)的算法。在单机的情况下当然是没问题的，matlab在一秒钟内就可以算出1000 * 1000的矩阵的所有奇异值，但是当矩阵的规模增长的时候，计算的复杂度呈3次方增长，就需要并行计算参与了。Google的吴军老师在数学之美系列谈到SVD的时候，说起Google实现了SVD的并行化算法，说这是对人类的一个贡献，但是也没有给出具体的计算规模，也没有给出太多有价值的信息。\n其实SVD还是可以用并行的方式去实现的，在解大规模的矩阵的时候，一般使用迭代的方法，当矩阵的规模很大（比如说上亿）的时候，迭代的次数也可能会上亿次，如果使用Map-Reduce框架去解，则每次Map-Reduce完成的时候，都会涉及到写文件、读文件的操作。个人猜测Google云计算体系中除了Map-Reduce以外应该还有类似于MPI的计算模型，也就是节点之间是保持通信，数据是常驻在内存中的，这种计算模型比Map-Reduce在解决迭代次数非常多的时候，要快了很多倍。\nLanczos迭代就是一种解对称方阵部分特征值的方法（之前谈到了，解A’* A得到的对称方阵的特征值就是解A的右奇异向量），是将一个对称的方程化为一个三对角矩阵再进行求解。按网上的一些文献来看，Google应该是用这种方法去做的奇异值分解的。请见Wikipedia上面的一些引用的论文，如果理解了那些论文，也“几乎”可以做出一个SVD了。\n由于奇异值的计算是一个很枯燥，纯数学的过程，而且前人的研究成果（论文中）几乎已经把整个程序的流程图给出来了。更多的关于奇异值计算的部分，将在后面的参考文献中给出，这里不再深入，我还是focus在奇异值的应用中去。\n三、奇异值与主成分分析（PCA）：\n主成分分析在上一节里面也讲了一些，这里主要谈谈如何用SVD去解PCA的问题。PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子：\n这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。\n一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。\nPCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。\n还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。\n而将一个m * n的矩阵A变换成一个m * r的矩阵，这样就会使得本来有n个feature的，变成了有r个feature了（r < n)，这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是：\n但是这个怎么和SVD扯上关系呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量…我们回忆一下之前得到的SVD式子：\n在矩阵的两边同时乘上一个矩阵V，由于V是一个正交的矩阵，所以V转置乘以V得到单位阵I，所以可以化成后面的式子\n将后面的式子与A * P那个m * n的矩阵变换为m * r的矩阵的式子对照看看，在这里，其实V就是P，也就是一个变化的向量。这里是将一个m * n 的矩阵压缩到一个m * r的矩阵，也就是对列进行压缩，如果我们想对行进行压缩（在PCA的观点下，对行进行压缩可以理解为，将一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉）怎么办呢？同样我们写出一个通用的行压缩例子：\n这样就从一个m行的矩阵压缩到一个r行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以U的转置U'\n这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。\n四、奇异值与潜在语义索引LSI：\n潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在矩阵计算与文本处理中的分类问题中谈到：\n“三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。”\n上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial，具体的网址我将在最后的引用中给出：\n这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵：\n左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。\n继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次；\n其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。\n然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到：\n在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。\n不知道按这样描述，再看看吴军老师的文章，是不是对SVD更清楚了？:-D\n参考资料：\n1）A Tutorial on Principal Component Analysis, Jonathon Shlens\n这是我关于用SVD去做PCA的主要参考资料\n2）http://www.ams.org/samplings/feature-column/fcarc-svd\n关于svd的一篇概念好文，我开头的几个图就是从这儿截取的\n3）http://www.puffinwarellc.com/index.php/news-and-articles/articles/30-singular-value-decomposition-tutorial.html\n另一篇关于svd的入门好文\n4）http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html\nsvd与LSI的好文，我后面LSI中例子就是来自此\n5）http://www.miislita.com/information-retrieval-tutorial/svd-lsi-tutorial-1-understanding.html\n另一篇svd与LSI的文章，也还是不错，深一点，也比较长\n6）Singular Value Decomposition and Principal Component Analysis, Rasmus Elsborg Madsen, Lars Kai Hansen and Ole Winther, 2004\n跟1）里面的文章比较类似"}
{"content2":"一、机器学习相关的公司分析\n1、大的有师傅的公司\n这类公司主要是百度，阿里和腾讯。共同的特点是数据很大，机器学习的团队比较庞大，一般进去的同学都可以有师傅带着学习，进步会比较快。\n但是三个公司的特点也有所不同。\n百度是我认为在业务和技术之间匹配的最好，并且从基础到应用搭配的最好的公司。机器学习方面的能力对于百度的广告，搜索，移动搜索，LBS,应用分发，移动音乐，移动阅读，移动新闻，图片搜索，语音输入，浏览器，视频等所有业务都非常重要；而百度也非常重视机器学习团队的搭建。目前在产品方面的表现也非常不错。如果近期加入的团队一旦在基础研究以及产品化方面有巨大突破的话，百度的各个核心产品都可能大大的超出其他公司的产品。\n百度的机器学习人才的需求种类最宽。\n阿里目前的机器学习人才主要用在业务挖掘，广告和推荐方面。和阿里的业务非常的匹配；根据IPO公告，以及近期的动作，阿里未来的业务发展方向主要是电商业务的区域扩张（向下是向县城扩张，向上是跨国业务的发展）以及产品品类的扩张（从实物产品的电商向服务，金融方向的扩张。）从这种趋势来看，未来阿里的机器学习人才需求还是以业务挖掘，广告和推荐方面的人才需求为主（图像处理和NLP作为feutrue的提供者，也有需求）。\n腾讯公司过去的主要业务是建立在社交网络之上的游戏，互联网增值服务（会员和道具之类的），广告等。根据年报，我认为腾讯今后的重点是在微信的基础上来发展盈利性业务，目前能够看到的业务有游戏，电商，支付，嘀嘀打车等；腾讯也单独把广告和视频业务提出来当做重点业务。\n结合以上对于腾讯的业务分析和预测，以及之前对于腾讯的职位的一些认识，我认为腾讯今后对于机器学习类人才的需求主要有业务挖掘，广告算法，推荐等。\n从业务上来看，三家公司都具有收入和利润基本都来自核心业务（百度主要来自于搜索广告；腾讯主要来自于游戏和增值服务，阿里主要来自于电商广告），同时有向其他俩家的核心业务扩展的动作但是没有成功的特点（百度尝试过电商和社交；阿里尝试过社交，也正在做搜索；腾讯尝试过搜索，也在做电商）。\n从战略和职位设置来看，百度是从基础到产品都做；而阿里和腾讯主要侧重于应用。\n2、中等规模的团队搭建中的公司以及专业公司\n有一些公司，相对于BAT来说，市场地位相对较弱，但是公司的市场地位也不错；同时机器学习的团队相对较小，或者布置的普遍程度相对较弱。\n比如当当，携程，去哪儿，360,58同城，优酷，乐视。这类公司一般会设置俩类机器学习的岗位，一是业务挖掘类岗位，另外就是推荐和广告算法的团队。这类公司具有市场地位不够稳固，机器学习团队相对较弱或者较新的特点。\n同样有一些中等规模的广告行业的专业公司，也有机器学习的团队，比如MediaV,品友互动等公司。这类公司主要的岗位是计算广告算法工程师。\n3、小的专业公司\n在移动互联网快速发展的今天，有一些专业性的小公司，产品本身对于机器学习技术的依赖性非常大，也设置了机器学习的岗位，这些小公司大多数是创业公司，业务发展的不确定性比较大，同时需要的机器学习人才和业务本身的相关性非常大。\n比如口袋购物(主要需求的是推荐算法，广告算法，NLP和图像处理人才)，今日头条（主要需要的是文本挖掘，推荐等人才），微博易（主要需要的是文本挖掘类人才），出门问问（主要需要的是语音识别，搜索的人才）。\n不同类型的公司对于人才的要求不同，对于能够带给人才的东西也不同，各有优劣。同学们可以根据自己的情况灵活的选择，每类公司中都有非常好的公司和岗位！\n二、机器学习相关职位分析\n1、互联网业务挖掘\n使用的主要数据和要解决的问题,初级的业务挖掘人员的工作会离数据和算法更近；高级的业务挖掘人员\n会离用户和业务更近。\n职位需求趋势：\n这类职位的需求量非常大，基本所有的主要互联网公司都设置了这个岗位。这个岗位的名字常常有“分\n析师”，“数据挖掘工程师”等。\n零售，金融，电信，制造业等行业对于互联网业务挖掘人员也持欢迎态度。近几年对于这类人才的需求\n很能会有非常大的增长。\n薪水状况：\n从我接触到的猎头职位的情况来看，在这个岗位上工作三四年，能够独立和业务部门以及技术部门沟通\n，并且能够灵活的应用数据为业务部门提出解释和建议的人才的年度薪水在20万元到35万元左右。\n2年前见过大的互联网公司的分析总监给到50万元以上。\n职业发展前景：\n我个人认为人类曾经经历过火车时代，电力时代，汽车时代，电子时代；当下的时代是数据时代。具有\n良好的数据分析能力的人对于越来越多的企业具有至关重要的作用。根据海德思哲公司的分析，未来的\n企业领袖人物往往是business+science+technology三方面都很强的人，业务挖掘工程师的工作内容和其\n中的俩项密切相关。\n2、推荐算法\n解决的核心问题是给用户想要的，不要给用户不想要的，降低用户找东西的难度，给用户更多的惊喜。\n不同的互联网产品在不同的阶段，可以通过推荐系统解决不同的问题和实现不同的目标。\n职位的设置情况和需求趋势：\n相对业务挖掘岗位，有推荐算法职位的公司数量相对比较少。能够看到的一些公司如下。\n电商：淘宝，当当，京东，口袋购物。\n视频：优酷土豆，爱奇艺，风行在线，乐视。\n音乐：豆瓣，虾米，网易云音乐，百度。\n新闻APP:今日头条，网易新闻客户端，百度新闻，指阅。\n阅读：盛大文学，掌阅科技。\n团购：美团，糯米。\n社交：微博，linkedin。\n手机助手：豌豆荚，\nLBS推荐：百度，高德。\n相对电脑，手机的私密性更强，屏幕资源更小，可能会有更多的移动应用公司会部署推荐算法的岗位。\n薪水状况：\n我接触到的推荐算法负责人的职位（能够直接面对工程和产品部门，对推荐系统的效果负责），招聘方\n给到的年度薪在30万元到50万元左右。\n职业发展前景：\n移动互联网是近几年互联网行业最大的潜力细分领域，而推荐对于移动互联网的所有产品都有非常重要\n的作用，从这个角度来讲推荐算法工程师的职业前景非常不错。\n在多个移动互联网的细分领域，推荐都处于核心地位，因此成熟的推荐算法人才创业的机会也比较多。\n3、广告算法\n数据主要是俩块，一块是用户的数据，除了公司自有的数据以外，也可以通过DMP(数据管理平台)获取到\n一些用户的数据；另外一块是关于广告的数据。\n需要解决的问题就是把用户和进行更好的匹配，提升总体的市场效率。\n其中CTR预估是非常重要的工作内容。\n职位的设置情况和需求趋势：\n和其他的职位相比，计算广告的公司数量比较集中。公司主要分为三类。\n一类是有Exchange或者类似体量的公司（相当于有设局或者设立证券交易所的公司），有百度，淘宝，\n腾讯，搜狗，360，微软，雅虎。这类公司的流量很大，广告主的数量也很大，他们制定各自的市场内的\n游戏规则（主要是资源分配的办法以及结算办法。）\n另外一类是DSP(Demand side platform),比如MediaV,品友互动，浪淘金等。这类公司本身没有大的流量\n，但是都在努力建立相对广告主更为有效的广告投放能力，主要从广告主挣钱。主要的目标是帮助广告\n主更有效率的把广告投放到目标群体身上。\n第三类公司是类似五八同城，优酷，新浪微博这样的大媒体。或者多盟这样的SSP（Supply side\nplatform）。这类公司自己有一定的流量，也有一些广告主客户。也需要有人来做市场效率的提升。\n新增职业机会的来源，我能够看到的主要有以下几种。一种是在搜索市场上取得突破后需要建立商业变\n现体系的360；还有一些是来自于对淘宝模式模仿的电商公司，比如当当；还有一类是互联网广告公司的\n业务拓展和创新，比如SSP公司向DSP业务的拓展，或者互联网广告监测公司，或者广告生态种新的角色\n诞生带来的新职位机会。\n近几年看到的互联网广告相关的变化主要有2个，一个是谷歌采用GSP（General second price）竞价方\n式并逐步被别的公司跟进；另外一个变化就是有些DSP公司大力倡导RTB（Real time bidding）。\n这些变化都没有带来行业内计算广告人才需求量的急速增长。看未来，广告生态系统的逐渐完善而催生\n出的新的细分行业和公司，也没有看到能够带来大的新增职位的急速增长。\n但是互联网广告行业面临的挑战和机会也很多，比如多屏互动的期望，以及广告主日益增长的需求，都\n对企业的创新提出了要求。\n综合以上，计算广告行业未来的人才需求特点可能是“少而精”。\n薪水状况：\n广告目前是互联网行业最重要的收入来源。从百度公司和阿里集团招股书中就可以看出，这俩个公司收\n入的绝大部分都来自广告。\n因此计算广告人才的薪水也非常的高。\n我了解到的比较成熟的计算广告人才（同时在算法和工程方面很成熟）的年收入有50万元到150万元左右\n。\n职位发展前景：\n一旦进入计算广告行业，相对其他职位来说，创业机会较少。更有可能的是在行业内的少数几个公司成\n长。\n该类人才的创业机会可能需要具备几个条件，第一是外围环境的重大变化导致的新类型公司的创立机会\n比较成熟，这个周期可能比较长；另外要有大量资金的支持；另外相对来说可能风险会比较大。\n4、NLP\n使用的数据和要解决的问题：\n参考《Foundations of Statistical Natural Language Processing》，\n《Speech_and_Language_Processing》。\n使用的数据主要是人们日常随意写出来的或者说出来的话。比如新闻，文章，微博上的话，qq的聊天，\n贴吧里的话，博客上的话，企业呼叫中心的对话等。\n要解决的问题主要是对这些内容进行抽象，映射或者响应。比如信息抽取（命名实体识别，情感分析等\n），机器翻译，聚类，分类，自动问答等。\n职位的设置情况和需求趋势：\n目前看到的NLP岗位设置主要有以下几类。\n一类是在搜索公司的query处理相关的工作。目前我了解到的工作主要分俩个部分，一个部分是做query\n的纠正，改写，或者近义词分析等；另外一类工作是做Topic model的研究，意思就是把用户的需求抽象\n在一个“model”上，而同时预先把网页到抽象到一个“model”上，然后在model内部挑选出用户最感兴\n趣的网页优先展示。\n还有一类是研发机构的研究岗位，比如微软，富士通研发，三星通信等机构都有NLP的研发岗位。\n第三类岗位就是一些专业性公司，比如口袋购物，微博易，今日头条，掌阅科技，简网世纪这样的公司\n。在这些公司里NLP和图像处理的工作地位类似，就是为下一步的处理提供feature。\n从用户端来看，WEB2.0的出现，以及社交网络的大力发展，为NLP积累下了大量的数据，同时企业也越来\n越重视通过网络来倾听用户，以及和用户进行沟通；同时信息的急速增长，导致用户对于个性化产品的\n需求越发明显，也促进了NLP工作的推进和岗位的设置。\n今后NLP的岗位会急速增长。增长的来源一部分来自于搜索引擎公司，根据李航前辈的微博，未来搜索突\n破可能来自俩个方向，一个方向是LTR(Learning to rank),另外的一个方向是Semantic match。我的理\n解可能semantic mtach需要大量的NLP的工作和人才。\n另外一类职位我觉得来自于大量的已经产生的大量非结构化的数据处理相关的公司，以及从大量的语音\n识别转化出来的数据的利用相关的公司。这个具体是在哪个方向上不好说，但是我想趋势上一定是企业\n对于海量用户产而生的嘈杂的声音的理解和利用。数据的量还在不断的急速增加，NLP工作量和任务类型\n也在增加，最终导致岗位需求的增加。\n薪水状况：\nNLP人才的需求不象数据挖掘和推荐那样量大和紧缺，和其他的职位相比薪水比较温和。\n我接触过的猎头职位，有公司愿意给工作2年左右的NLP工程师20到30万元的年度工资；也有创业公司愿\n意给NLP leader50万元以上的年度工资。\n职位发展前景：\n我预测NLP工程师在原公司的职业生命活力会比较强（数据，算法以及工作目标可能都会有新的挑战和机\n会进来）；同时成熟的人才也会有很多创业机会可以考虑。\n5、图像处理\n使用的数据和要解决的问题：\n参考《Computer Vision:Algorithms and Applications.Richard Szeliski》，\n《Multiple_View_Geometry_in_Computer_Vision__2nd_Edition》\n面对的数据是图像，具体也会有处理静态图像和动态视频的区别。以及离线处理和在线处理的区别。\n常见的要解决的问题有检测（就是看某个图片里是否有某类东西，比如是否有人脸），识别（就是输入\n一个图片，看这个图片和库里的哪个图片是一致的。）分割，拼接，3D重建，聚类，分类等。\n职位的设置情况和需求趋势：\n传统的图像处理岗位主要分布在类似智慧眼，汉王这样的IT公司和类似佳能，微软这样的研发机构中。\n智慧眼和汉王这样的IT公司的产品，主要以行业用户为主，应用的场合有门禁，安全，社保识别等。\n在大数据在中国兴起的前后，更多的互联网公司开始设立图像处理的岗位。其中百度是把图像和语音作\n为文字之外的新的搜索对象来看待的，设立了图像处理的岗位并招聘了人才，但是具体如何产品化目前\n还不太清楚，目前看到的只有图搜，就是把图片拷贝到检索栏里，然后可以看到一些检索的结果。\n淘宝以及一些其他的电商公司也设立了图像处理或者正在招聘相关的人才，主要的工作目标是为广告和\n推荐等应用提供feature。\n与数据挖掘，推荐和NLP职位相比，图像处理的职位数量相对较少，发展相对比较滞后，能够看到的大众\n用户使用的产品相对比较少。\n我个人的感觉是图像处理行业正处在一个爆发的前夕；象智能交通事故报告系统（自动对摄像头采集到\n的图像进行处理，即使在夜间也可以自动的识别出来重大事故并且提醒交警去救援），自动驾驶，离群\n人群自动检测（比如机场和火车站的恐怖分子检测）等应用都具有足够的价值，目前系统能够做到的水\n平也非常接近商品化。\n我的判断是随着技术的突破和外围配套（包括法规或者硬件等）的成熟，图像处理的岗位会有大的爆发\n。\n薪水状况：\n相对其他职位来说，图像处理的职位比较少一些；同时薪水不是很高。但是未来的收入前景不错。\n职位发展前景：\n不论在公司内部的提升，还是未来创业的角度来看，图像处理工程师的发展机会都会越来越多。"}
{"content2":"1 人工智能、机器学习、深度学习的关系\n“人工智能” 一词最早是再20世纪50年代提出来的。\n“ 机器学习 ” 是通过算法，使用大量数据进行训练，训练完成后会产生模型\n有监督的学习 supervised learning\n无监督的学习 unsupervised learning\n增强式学习 reinforcement learning\n已经应用领域：推荐引擎、定向广告、需求预测、垃圾邮件过滤、医学诊断、自然语言处理、搜索引擎、证券分析、视觉识别、语音识别、手写识别等\n“ 深度学习 ” 是机器学习的分支，其仿真人类神经网络的工作方式，常见深度学习架构有\n多层感知器 multi-layer perceptron\n深度神经网络 deep neural network ,DNN\n卷积神经网络 convolutional neural network , CNN\n递归神经网路 recurrent neural network , RNN\n已经应用领域：视觉识别，语音识别，自然语言处理，生物医学等\n另：\nGPU（Graphics Processing Unit）为图形处理器，用于电脑的图形运算，\nCPU与GPU的架构有本质的不同，CPU 含有数颗核心，为顺序处理进行优化；而GPU 则拥有高达数千个小型且高效的核心，发挥强大并行计算能力。\n深度学习以大量矩形运算模拟神经元的工作方式，该工作方式特别适合并行计算。GPU通过大量核心并行计算，在深度学习训练中，GPU比CPU要快10~75倍。\nGoogle 公司于2016年宣布人工智能专用芯片 TPU（Tensor Processing Unit，张量处理单元 / 张量处理芯片）来进行计算；TPU 是专为深度学习设计的特殊规格的逻辑芯片（IC），使得深度学习的训练速度更快。\n2 机器学习介绍\n机器学习的训练数据构成：\n数据特征features\n数据标签label\n机器学习分为两个阶段：\n训练Training\n预测Predict\n3 机器学习分类\n3.1 有监督的学习\n有监督的学习的数据具备特征features、预测目标/标签label两要素。通过算法训练并建立模型。当有新的数据时，我们将其进行预测。\n二元分类，特征features的标签label有两个（离散）选项。\n多元分类，特征features的标签label有至少两个（离散）选项\n回归分析，特征features的标签label是连续的值。\n3.2 无监督的学习\n该方式无label标签。\n如 cluster 集群算法将数据分成几个差异较大的群组，而群组内的相似度最高。\n3.3 增强式学习\n增强式学习原理：借助定义动作actions、状态states、奖励rewards的方式不断训练机器循序渐进，使其学会执行某项任务的算法。\n常见算法有：Q-Learning、TD（Temporal Difference）、Sarsa 等。\n如训练机器玩《超级玛丽》电子游戏就是借助不断训练学会玩游戏，对应状态有：\n动作：左、右、跳\n状态：当前游戏界面\n奖励：得分、受伤\n4 深度学习简介\n一个输入层、一个输出层、N个隐藏层，所以称之为深度学习。"}
{"content2":"http://www.renwuyi.com/index.php?action=artinfo&id=19036&cat_id=2#top\n文本生成是比较学术的说法，通常在媒体上见到的“机器人写作”、“人工智能写作”、“自动对话生成”、“机器人写古诗”等，都属于文本生成的范畴。\n2016年里，关于文本生成有许多的新闻事件，引起了学术界以外对这一话题的广泛关注。\n2016年3月3日，MIT CSAIL【1】报道了，MIT计算机科学与人工智能实验室的一位博士后开发了一款推特机器人，叫DeepDrumpf，它可以模仿当时的美国总统候选人Donald Trump来发文。\n2016年3月22日，日本共同社报道，由人工智能创作的小说作品《机器人写小说的那一天》入围了第三届星新一文学奖的初审。这一奖项以被誉为“日本微型小说之父”的科幻作家星新一命名。提交小说的是“任性的人工智能之我是作家”（简称“我是作家”）团队【2】。\n2016年5月，美国多家媒体【3】【4】报道，谷歌的人工智能项目在学习了上千本浪漫小说之后写出后现代风格的诗歌。\n基于人工智能的文本生成真的已经达到媒体宣传的水平了吗？这些事件背后是怎样的人工智能技术？关于机器人写小说的工作，我们会在另一篇文章《会有那么一天，机器人可以写小说吗？》里进行深入的讨论，他们的工作更多的是基于模板的生成。在这篇文章里，我们主要想通过三篇文章介绍另一大类方法，即基于统计的文本生成。\n令人吃惊的Char-RNN\n关于基于深度学习的文本生成，最入门级的读物包括Andrej Karpathy这篇博客【5】。他使用例子生动讲解了Char-RNN（Character based Recurrent Neural Network）如何用于从文本数据集里学习，然后自动生成像模像样的文本。\n图一直观展示了Char-RNN的原理。以要让模型学习写出“hello”为例，Char-RNN的输入输出层都是以字符为单位。输入“h”，应该输出“e”；输入“e”，则应该输出后续的“l”。输入层我们可以用只有一个元素为1的向量来编码不同的字符，例如，h被编码为“1000”、“e”被编码为“0100”，而“l”被编码为“0010”。使用RNN的学习目标是，可以让生成的下一个字符尽量与训练样本里的目标输出一致。在图一的例子中，根据前两个字符产生的状态和第三个输入“l”预测出的下一个字符的向量为<0.1, 0.5, 1.9, -1.1>，最大的一维是第三维，对应的字符则为“0010”，正好是“l”。这就是一个正确的预测。但从第一个“h”得到的输出向量是第四维最大，对应的并不是“e”，这样就产生代价。学习的过程就是不断降低这个代价。学习到的模型，对任何输入字符可以很好地不断预测下一个字符，如此一来就能生成句子或段落。\nAndrej Karpathy还共享了代码【6】，感兴趣的同学不妨下载来试试，效果会让你震惊。Andrej Karpathy在底层使用的RNN的具体实现是LSTM（Long-Short Term Memory），想了解LSTM可以阅读【7】，讲得再清楚不过。\n研究人员用Char-RNN做了很多有趣的尝试，例如，用莎士比亚的作品来做训练，模型就能生成出类似莎士比亚的句子；利用金庸的小说来做训练，模型就能生成武侠小说式的句子；利用汪峰的歌词做训练，模型也能生成类似歌词的句子来。\n在本文一开始提到的【1】，MIT计算机科学与人工智能实验室的博士后Bradley Hayes也正是利用类似的方法开发了一款模仿候任美国总统Donald Trump的推特机器人，叫DeepDrumpf。例如，图二中，这个机器人说，“我就是伊斯兰国不需要的。”\n据作者介绍，他受到一篇模拟莎士比亚的论文启发，以Donald Trump的演讲和辩论（时常大约几个小时）的字幕作为训练语料，使用深度神经网络学习去训练Trump的模型。他也声称，因为有一篇文章调侃Trump的发言只有小学四年级的水平，因而想到用Trump的语料可能是最容易控制的。\n这是一个有趣的应用，记者评论称这个机器人也并不是总能写出好的句子，但至少部分是通顺的。其实，风格并不是很难学到，只要使用的训练语料来自同一个人，而这个人的写作或者发言具有辨识度高的特点。\n深度学习生成对话\n推荐阅读的第二篇文章是诺亚方舟实验室的尚利峰、吕正东和李航在2015年ACL大会上发表的“Neural Responding Machine for Short-Text Conversation” 【9】。大家也许听说过微软小冰，它因为开创性的主要做闲聊（即以娱乐为目的的聊天）式对话，被哈尔滨工业大学的刘挺教授誉为是第二波人机对话的浪潮的代表【8】。小冰的出现也影响到了学术界。除了原来做知识性的问答，一些研究也开始关注闲聊，让机器人和人类搭话，这方面诺亚方舟实验室发表了一系列有影响力的文章。今天介绍的这篇文章在Arxiv.org上发布短短一年时间，已经有67次的引用。\n【9】这篇文章尝试用encoder-decoder（编码-解码）的框架解决短文本对话（Short Text Conversation，缩写为STC）的问题。虽然encoder-decoder框架已经被成功应用在机器翻译的任务中，但是对话与翻译不同，对应一个输入文本（post）往往有多种不同的应答（responses）。文中举了一个例子，一个人说“刚刚我吃了一个吞拿鱼三明治”，不同的应答可以是“天哪，才早晨11点”、“看起来很美味哟”或是“在哪里吃的”。这种一对多的情况在对话中很普遍也很自然。的确，不同的人会对同一句话做出不同的反应，即使是同一个人，如果每次回答都一模一样也是很无趣的。\n针对这一特点，作者们提出Neural Responding Machine（简称NRM，见图三）框架来解决短文本对话的问题。他们尝试了全局编码和局部编码，最终发现先分别训练，再用图四的结构来做微调训练，效果最佳。全局编码的优点是能够获得全局信息，同样的词在不同情境下会有不同的意义，全局信息可以部分解决这类情况；缺点是，它供给解码的输入比较固定。局部编码利用局部信息，比较灵活多样，刚好可以缓解全局编码的弱点。\n这篇论文的另一大贡献是构建了一个比较大的数据集和标注来评价不同的方法。通过对比，所提出的混合全局和局部的方法比以往基于搜索的方法和机器翻译的方法都要好很多。机器翻译的方法生成的句子往往不通顺，得分最低。能比基于搜索的方法好很多也非常不容易，因为基于搜索的方法得到的已经是人使用过的应答，不会不通顺。大家可以在图五的实例中直接感受一下生成的效果。NRM-glo是全局编码的模型，NRM-loc是局部编码的模型，NRM-hyb是混合了全局和局部的模型，Rtr.-based则是基于搜索的方法。\n2015到2016年，这篇论文的作者组织了NTCIR-12 STC任务【10】，公开他们的数据集，并提供公共评测。有16个大学或研究机构参加了中文短文本对话任务的评测。2017年，他们将会继续组织NTCIR-13 STC【11】，现已开放注册【12】。除了上一届的基于搜索的子任务，这一次还设立了生成应答的子任务。我们预计今年的结果会更精彩。\n被媒体误解的谷歌人工智能写诗\n第三篇文章是Samuel Bowman等发表在Arxiv.org上的名为“Generating Sentences from a Continuous Space”的文章【13】。作者分别来自斯坦福大学、马萨诸塞大学阿姆斯特分校以及谷歌大脑部门，工作是在谷歌完成的。\n这一工作曾被媒体广泛报道，但我发现很多报道（例如【3】【4】）都对论文的工作有一些误解。一些记者将图六所示的文字误认为是机器人写出来的后现代风格的诗歌，其实不然。这只是作者在展示他们的方法可以让句子级别的编码解码更连续。具体而言，在他们学习到的空间中，每个点可以对应一个句子，任意选定两个点，例如在图六中，一对点对应的句子分别是“i want to talk to you.”和“she didn’t want to be with him”，两点之间的连线上可以找出间隔均匀的几个点，将它们也解码成句子，会发现，这些句子好像是从第一句逐渐变化成了最后一句。\n得到这样的结果实属不易。在文章的一开始，作者就给出了一个例子，来说明传统的自动解码并不能很好地编码完整的句子。如图七所示，从句子“i went to the store to buy some groceries”到句子“horses are my favorite animals”，中间取的点经过解码得到的句子呈现在它们之间。可以发现，这些句子未必是符合语法的英文句子。与之相比，图六呈现的句子质量要好很多，不仅语法正确，主题和句法也一致。\n这篇文章的想法非常有意思，他们想使用VAE（varationalautoencoder的简称）学习到一个更连续的句子空间。如图八所示，作者使用了单层的LSTM 模型作为encoder（编码器）和decoder（解码器），并使用高斯先验作为regularizer（正规化项），形成一个序列的自动编码器。比起一般的编码解码框架得到的句子编码往往只会记住一些孤立的点，VAE框架学到的可以想象成是一个椭圆形区域，这样可以更好地充满整个空间。我的理解是，VAE框架将贝叶斯理论与深度神经网络相结合，在优化生成下一个词的目标的同时，也优化了跟先验有关的一些目标（例如KL cost和crossentropy两项，细节请参考论文），使对一个整句的表达更好。\n当然，为了实现这一想法，作者做了很多尝试。首先，对图八所展示的结构做一些变形并没有带来明显的区别。但在优化时，使用退火的技巧来降低KL cost和训练时把适当比例的词变为未知词（即word dropout）这两项技术就非常有效。\n作者们通过两个有意思的实验来展示了他们的结果。一个是做填空题，如图九所示，隐藏句子的后20%，让模型来生成后面的部分。从几个例子看，VAE的方法比RNN语言模型（简称RNNLM）更加通顺和有信息量。第二个实验就是在两个句子之间做轮移（Homotopy，也就是线性插值），对比图六和图七，可以看出VAE给出的句子更平滑而且正确，这一点可以间接说明学习到的句子空间更好地被充满。\n当然，作者们还给出了一些定量的比较结果。在比较填空结果时，他们使用了adversarial evaluation（对抗评价）。具体的做法是，他们取样50%的完整句子作为正例，再拿50%的由模型填空完成的句子作为负例。然后训练一个分类器，如果一个模型填的越难与正例分开，就说明这种模型的生成效果更好，更具欺骗性。因此，可以认为这一模型在填空任务上更出色。实验的结果也支持VAE比RNNLM更好。\n问题与难点\n人工智能真的会创作吗？使用深度学习技术写出的文章或者对话，的确是会出现训练集合里未见过的句子。例如，一个原句的前半段可能会跟上另一个原句的后半段；也可能除了词，搭配组合都是训练集里没有的。这看起来有些创作的意味，但是细究起来，往往是原句的部分更为通顺和有意义。目前的技术可以拼凑，偶尔出现一两个好玩的点，但是写得长了，读起来会觉得没头没脑，这是因为没有统领全篇的精神，跟人类的作家比当然还是相差很远。\n机器学习到的还只是文字表面，没有具备人要写文章的内在动因。人写文章表达的是自己的思想和感受，这是机器所没有的。因此，即使是机器写文章，具体想要表达什么，似乎还要由人来控制。但如果控制得太多，看起来又不那么智能，少了些趣味。我认为，要想让机器更自由地写出合乎逻辑的话来，我们还需要类似VAE那篇文章一样更深入的研究，对句子甚至段落的内在逻辑进行学习。\n另外，人在写一篇文章的时候，很容易自我衡量语句是否通顺、思想是否表达清楚以及文章的结构是否清晰有趣，机器却很难做到。因此，优化的目标很难与真正的质量相一致。目前的自然语言理解技术对于判断句法语法是否正确可能还有些办法，但要想判断内容和逻辑上是否顺畅，恐怕还需要常识和推理的帮助，这些部分暂时还比较薄弱。但也并非毫无办法，我相信未来对文本生成的研究一定会涉及这些方面。"}
{"content2":"一、概述\n前两章我们要求分类器做出艰难决策，给出“该数据实例属于哪一类”这类问题的明确答案。不过，分类器有时会产生错误结果，这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。\n概率论是许多机器学习算法的基础，所以深刻理解这一主题就显得十分重要。第3章在计算特征值取某个值的概率时涉及了一些概率知识，在那里我们先统计特征在数据集中取某个特定值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。我们将在此基础上深人讨论。\n本章会给出一些使用概率论进行分类的方法。首先从一个最简单的概率分类器开始，然后给出一些假设来学习朴素贝叶斯分类器。我们称之为“朴素”，是因为整个形式化过程只做最原始、最简单的假设。不必担心，你会详细了解到这些假设。我们将充分利用Python的文本处理能力将文档切分成词向量，然后利用词向量对文档进行分类。我们还将构建另一个分类器，观察其在真实的垃圾邮件数据集中的过滤效果，必要时还会回顾一下条件概率。最后，我们将介绍如何从个人发布的大量广告中学习分类器，并将学习结果转换成人类可理解的信息。\n假设现在我们有一个数据集，它由两类数据组成，数据分布如图所示。\n我们现在用p1(x,y)表示数据点(x,y)属于类别1(以图中用圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2 ( 图中用三角形表示的类别）的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：\n也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。回到图，如果该图中的整个数据使用6个浮点数来表示，并且计算类别概率的python代码只有两行，那么你会更倾向于使用下面哪种方法来对该数据点进行分类？\n(1)使用第1章的knn ，进行1000次距离计算；\n(2)使用第2章的决策树，分别沿x轴、y轴划分数据；\n(3)计算数据点属于每个类别的概率，并进行比较。\n使用决策树不会非常成功；而和简单的概率计算相比，knn的计算量太大。因此，对于上述问题，最佳选择是使用刚才提到的概率比较方法\n二、优缺点\n优点：在数据较少的情况下仍然有效，可以处理多类别问题。\n缺点：对于输入数据的准备方式较为敏感。\n适用数据类型：标称型数据。\n三、数学公式\n贝叶斯定理\n了解贝叶斯定理之前，需要先了解下条件概率。P(A|B)表示在事件B已经发生的条件下事件A发生的概率：\n假如我们已经知道了P(A|B)，但是现在我们想要求P(B|A)，也就是在事件A发生的条件下事件B发生的概率，这时贝叶斯定理就派上用场了。\n前面提到贝叶斯决策理论要求计算两个概率p1(x,y) 和p2(x,y)；\n但这两个准则并不是贝叶斯决策理论的所有内容。使用p1() p2()只是为了简化描述，而真正需要计算和比较的是p(c1|x,y) 和p(c2|x,y)。 这些符号的意思是：\n给定某个x,y表示的数据点，那么该数据点来自类别c1的概率是多少? 来自c2的概率又是多少？\n现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，问这个球是红球且来自容器 A 的概率是多少?\n假设已经抽出红球为事件 B，选中容器 A 为事件 A，则有：P(B) = 8/20，P(A) = 1/2，P(B|A) = 7/10，\n按照公式，则有：P(A|B) = (7/10)*(1/2) / (8/20) = 0.875\n四、使用朴素贝叶斯进行文档分类\n机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。虽然电子邮件是一种会 不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任意类型的文本进行分类。我们可以观察文档中出现的词，并把每个词的出现或者不出 现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是上节介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。\n使用每个词作为特征并观察它们是否出现，这样得到的特征数目会有多少呢？针对的是哪一种人类语言呢？当然不止一种语言。据估计，仅在英语中，单词的总数就有500000之多。为了能进行英文阅读，估计需要掌握数千单词。\n所谓独立,指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。\n这个假设正是朴素贝叶斯分类器中朴素一词的含义,朴素贝叶斯分类器中的另一个假设是，每个特征同等重要.\n算法一般流程\n1.数据的收集\n2.数据的准备：数值型或布尔型\n3.分析数据\n4.训练算法：计算不同的独立特征的条件概率\n5.测试算法：计算错误率\n6.使用算法：以实际应用为驱动\n朴素贝叶斯伪代码\n1.计算各个独立特征在各个分类中的条件概率\n2.计算各类别出现的概率\n3.对于特定的特征输入，计算其相应属于特定分类的条件概率\n4.选择条件概率最大的类别作为该输入类别进行返回\n五、准备数据：从文本中构建词向量\n我们将把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现在所有文档中的所有单词，再决定将哪些词纳人词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。\n词表到向量的转换函数：\n1 def loadDataSet(): 2 postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], 3 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], 4 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], 5 ['stop', 'posting', 'stupid', 'worthless', 'garbage'], 6 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], 7 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] 8 classVec = [0,1,0,1,0,1] #1 代表侮辱性词, 0 代表正常言论 9 return postingList,classVec 10 11 def createVocabList(dataSet): #根据数据集返回 关键词汇向量 12 vocabSet = set([]) # 创建空的集合 13 for document in dataSet: 14 vocabSet = vocabSet | set(document) # 操作符 | 用来求两个集合的并集 15 return list(vocabSet) # 返回 集合中 所有不重复的关键词 16 17 def setOfWords2Vec(vocabList, inputSet):# vocabList=词汇表 ，inputSet = 输入的文档 #文档词汇 转换 成文档 向量 18 returnVec = [0]*len(vocabList) # 生成一个值为0，长度和vocabList一样的集合 19 for word in inputSet: 20 if word in vocabList: 21 returnVec[vocabList.index(word)] = 1 22 else: print \"the word: %s is not in my Vocabulary!\" % word 23 return returnVec # 返回 输入文档inputSet 的向量\n第一个函数loadDataset()创建了一些实验样本。该函数返回的第一个变量是进行词条切分后的文档集合,这些文档来自斑点犬爱好者留言板。这些留言文本被切分成一系列的词条集合，标点符号从文本中去掉，后面会探讨文本处理的细节。loadDataSet( )函数返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性。这些文本的类别由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。\n下一个函数createVocabList（）会创建一个包含在所有文档中出现的不重复词的列表，为此使用了Python 的set数据类型。将词条列表输给set构造数，set（）就会返回一个不重复词表。首先，创建一个空集合, 然后将每篇文档返回的新词集合添加到该集合中。操作符丨用于求两个集合的并集，这也是一个按位或（or) 操作符（参见附录0） 。在数学符号表示上，按位或操作与集合求并操作使用相同记号。\n获得词汇表后，便可以使用函数setofWords2Vec（），该函数的输人参数为词汇表及某个文档，输出的是文档向量，向量的每一元素为1或0，分别表示词汇表中的单词在输人文档中是否出现。函数首先创建一个和词汇表等长的向量，并将其元素都设置为0 。接着，遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1。一切都顺利的话，就不需要检查某个词是否还vobalist中，后边可能会用到这一操作。\n测试代码：\n1 >>> listOPost,listClasses=bayes.loadDataSet() 2 >>> listOPost 3 [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] 4 >>> listClasses 5 [0, 1, 0, 1, 0, 1] 6 >>> myVocabList=bayes.createVocabList(listOPost) 7 >>> myVocabList 8 ['garbage', 'love', 'my', 'dog', 'park', 'buying', 'help', 'is', 'so', 'to', 'ate', 'steak', 'please', 'him', 'not', 'stupid', 'take', 'maybe', 'posting', 'problems', 'worthless', 'I', 'food', 'quit', 'mr', 'dalmation', 'stop', 'has', 'licks', 'how', 'flea', 'cute'] 9 #检查上述的词表发现，这里不会出现重复的单词 10 >>> bayes.setOfWords2Vec(myVocabList,listOPost[0])# 把文档转换成向理 11 [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0] 12 >>> bayes.setOfWords2Vec(myVocabList,listOPost[3])# 把文档转换成向理 13 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n六、训练算法：从词向量计算概率\n前面介绍了如何将一组单词转换为一组数字，接下来看看如何使用这些数字计算概率。现在已经知道一个词是否出现在一篇文档中，也知道该文档所属的类别。还记得前面提到的贝叶斯准则？我们重写贝叶斯准则，将之前的x、y替换为w。粗体w表示这是一个向量，即它由多个数值组成。在这个例子中，数值个数与词汇表中的词个数相同。\n我们将使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。如何计算呢？首先可以通过类别i（侮辱性留言或非侮辱性留言）中文档数除以总的文档数来计算概率p(ci)。接下来计算p(w|ci)，这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作p(w0,w1,w2..wN|ci)。这里假设所有词都互相独立，该假设也称作条件独立性假设，它意味着可以使用p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)来计算上述概率，这就极大地简化了计算的过程。\n该函数的伪代码如下：\n计算每个类别中的文档数目\n对每篇训练文档：\n对每个类别：\n如果词条出现文档中―增加该词条的计数值\n增加所有词条的计数值\n对每个类别：\n对每个词条：\n将该词条的数目除以总词条数目得到条件概率\n还回每个类别的条件概率\n朴素贝叶斯分类器训练函数：\n1 def trainNB0(trainMatrix,trainCategory): #trainMatrix 所有文档的向量形式 trainCategory 文档的分类 类别向量 #计算不同分类的文档概率，即 P(W|C1) P(W|C0) 2 numTrainDocs = len(trainMatrix) # numTrainDocs = 总文档数 3 numWords = len(trainMatrix[0]) # numWords= 词汇长度 4 pAbusive = sum(trainCategory)/float(numTrainDocs) # 计算 分类=1 的文档比例 p(1)的概率，这是一个二分类的问题 p(0)=1-p(1) 5 p0Num = zeros(numWords); p1Num = zeros(numWords) #初始化概率， 分子 6 p0Denom = 0.0; p1Denom = 0.0 #定义分母 7 for i in range(numTrainDocs): 8 if trainCategory[i] == 1: #类别为 1 9 p1Num += trainMatrix[i] # 分子向量累计相加 10 p1Denom += sum(trainMatrix[i])#分母 向量之和 11 else: 12 p0Num += trainMatrix[i] 13 p0Denom += sum(trainMatrix[i]) 14 p1Vect = p1Num/p1Denom # 当类别为 1 时，计算每个文档中出现 词汇的概率 15 p0Vect = p0Num/p0Denom # 16 return p0Vect,p1Vect,pAbusive\n代码函数中的输入参数为文档矩阵trainMa-trix，以及由每篇文档类别标签所构成的向量train-Category。首先，计算文档属于侮辱性文档（class=1）的概率，即P(1)。因为这是一个二类分类问题，所以可以通过1-P(1)得到P(0)。对于多于两类的分类问题，则需要对代码稍加修改。\n计算p(wi|c1)和p(wi|c0)，需要初始化程序中的分子变量和分母变量①。由于w中元素如此众多，因此可以使用NumPy数组快速计算这些值。\n上述程序中的分母变量是一个元素个数等于词汇表大小的NumPy数组。在for循环中，要遍历训练集trainMatrix中的所有文档。一旦某个词语（侮辱性或正常词语）在某一文档中出现，则该词对应的个数（p1Num或者p0Num）就加1，而且在所有的文档中，该文档的总词数也相应加1②。对于两个类别都要进行同样的计算处理。最后，对每个元素除以该类别中的总词数③。利用NumPy可以很好实现，用一个数组除以浮点数即可，若使用常规的Python列表则难以完成这种任务，读者可以自己尝试一下。最后，函数会返回两个向量和一个概率。\n接下来试验一下，在Python提示符下输入：\n1 >>> listOPosts,listClasses=bayes.loadDataSet() 2 3 >>> listOPosts 4 [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], 5 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], 6 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], 7 ['stop', 'posting', 'stupid', 'worthless', 'garbage'], 8 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], 9 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] 10 >>> listClasses 11 [0, 1, 0, 1, 0, 1] 12 13 >>> myVocabList=bayes.createVocabList(listOPosts) 14 #至此我们构建了一个包含所有词的列表myVocabList。 15 >>> myVocabList 16 ['garbage', 'love', 'my', 'dog', 'park', 'buying', 'help', 'is', 'so', 'to', 'ate', 'steak', 'please', 'him', 'not', 'stupid', 'take', 'maybe', 'posting', 'problems', 'worthless', 'I', 'food', 'quit', 'mr', 'dalmation', 'stop', 'has', 'licks', 'how', 'flea', 'cute'] 17 18 >>> trainMat=[] 19 >>> for postinDoc in listOPosts: 20 trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc)) 21 22 >>> trainMat 23 [[0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], 24 [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 25 [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], 26 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 27 [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0], 28 [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]] 29 #该for循环使用词向量来填充trainMat列表。下面给出属于侮辱性文档的概率以及两个类别的概率向量 30 >>> p0V,p1V,pAb=bayes.trainNB0(trainMat,listClasses) 31 >>> pAb #这就是任意文档属于侮辱性文档的概率。 32 0.5 33 >>> p0V 34 array([ 0. , 0.04166667, 0.125 , 0.04166667, 0. , 35 0. , 0.04166667, 0.04166667, 0.04166667, 0.04166667, 36 0.04166667, 0.04166667, 0.04166667, 0.08333333, 0. , 37 0. , 0. , 0. , 0. , 0.04166667, 38 0. , 0.04166667, 0. , 0. , 0.04166667, 39 0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667, 40 0.04166667, 0.04166667]) 41 >>> p1V 42 array([ 0.05263158, 0. , 0. , 0.10526316, 0.05263158, 43 0.05263158, 0. , 0. , 0. , 0.05263158, 44 0. , 0. , 0. , 0.05263158, 0.05263158, 45 0.15789474, 0.05263158, 0.05263158, 0.05263158, 0. , 46 0.10526316, 0. , 0.05263158, 0.05263158, 0. , 47 0. , 0.05263158, 0. , 0. , 0. , 48 0. , 0. ])\n首先，我们发现文档属于侮辱类的概率pAb为0.5，该值是正确的。接下来，看一看在给定文档类别条件下词汇表中单词的出现概率，看看是否正确。词汇表中的第一个词是cute，其在类别0中出现1次，而在类别1中从未出现。对应的条件概率分别为0.041 666 67与0.0。该计算是正确的。我们找找所有概率中的最大值，该值出现在P(1)数组第26个下标位置，大小为0.157 89474。在myVocabList的第26个下标位置上可以查到该单词是stupid。这意味着stupid是最能表征类别1（侮辱性文档类）的单词。\n七、测试算法：根据现实情况修改分类器\n利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中一个概率值为0，那么最后的乘积也为0。为降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。在文本编辑器中打开bayes.py文件，并将trainNB0()的第4行和第5行修改为：\np0Num=ones(numWords);p1Num=ones(numWords)\np0Denom=2.0;p1Denom=2.0\n另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积p(w0|ci)p(w1|ci)p(w2|ci)...p(wn|ci)时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（读者可以用Python尝试相乘许多很小的数，最后四舍五入后会得到0。）\n一种解决办法是对乘积取自然对数。在代数中有ln(a*b) =ln(a)+ln(b)，于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。图 给出函数f(x)与ln(f(x))的曲线。检查这两条曲线，就会发现它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。\n函数f(x)与ln(f(x))会一块增大。这表明想求函数的最大值时，可以使用该函数的自然对数来替换原函数进行求解\n通过修改return前的两行代码，将上述做法用到分类器中：\np1Vect=log(p1Num/p1Denom)\np0Vect=log(p0Num/p0Denom)\n修改后的 trainNB0 代码：\n1 def trainNB0(trainMatrix,trainCategory): 2 numTrainDocs = len(trainMatrix) 3 numWords = len(trainMatrix[0]) 4 pAbusive = sum(trainCategory)/float(numTrainDocs) 5 p0Num = ones(numWords); p1Num = ones(numWords) #change to ones() 6 p0Denom = 2.0; p1Denom = 2.0 #change to 2.0 7 for i in range(numTrainDocs): 8 if trainCategory[i] == 1: 9 p1Num += trainMatrix[i] 10 p1Denom += sum(trainMatrix[i]) 11 else: 12 p0Num += trainMatrix[i] 13 p0Denom += sum(trainMatrix[i]) 14 p1Vect = log(p1Num/p1Denom) #change to log() 15 p0Vect = log(p0Num/p0Denom) #change to log() 16 return p0Vect,p1Vect,pAbusive\n现在已经准备好构建完整的分类器了。当使用NumPy向量处理功能时，这一切变得十分简单。打开文本编辑器，将下面的代码添加到bayes.py中：\n朴素贝叶斯分类函数\n1 def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): #根据 输入的 文档 对文档进行分类 预测 P(W|C0)=p0Vec P(W|C1)=p0Vec 2 p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult 3 p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) 4 if p1 > p0: 5 return 1 6 else: 7 return 0 8 9 def testingNB(): 10 listOPosts,listClasses = loadDataSet() 11 myVocabList = createVocabList(listOPosts) 12 trainMat=[] 13 for postinDoc in listOPosts: 14 trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) 15 p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses)) 16 testEntry = ['love', 'my', 'dalmation'] 17 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) 18 print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)) 19 testEntry = ['stupid', 'garbage'] 20 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) 21 print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\nclassifyNB代码有4个输入：要分类的向量vec2Clas-sify以及使用函数trainNB0()计算得到的三个概率。使用NumPy的数组来计算两个向量相乘的结果①。这里的相乘是指对应元素相乘，即先将两个向量中的第1个元素相乘，然后将第2个元素相乘，以此类推。接下来将词汇表中所有词的对应值相加，然后将该值加到类别的对数概率上。最后，比较类别的概率返回大概率对应的类别标签。\n下面来看看实际结果。将程序清单4-3中的代码添加之后，在Python提示符下输入：\n1 >>> imp.reload(bayes) 2 <module 'bayes' from 'F:\\\\99999_算法\\\\《机器学习实战》源代码\\\\machinelearninginaction\\\\Ch04\\\\bayes.py'> 3 >>> bayes.testingNB() 4 ['love', 'my', 'dalmation'] classified as: 0 5 ['stupid', 'garbage'] classified as: 1\n八、文档词袋模型\n目前为止，我们将每个词的出现与否作为一个特征，这可以被描述为词集模型（set-of-words model）。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为词袋模型（bag-of-wordsmodel）。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数setOf-Words2Vec()稍加修改，修改后的函数称为bagOfWords2Vec()。下面的程序清单给出了基于词袋模型的朴素贝叶斯代码。它与函数setOfWords2Vec()几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为1。\n朴素贝叶斯词袋模型\ndef bagOfWords2VecMN(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 return returnVec\n九、示例：使用朴素贝叶斯过滤垃圾邮件"}
{"content2":"人工智能（机器学习）学习之路推荐——Python\n虽然自己学过C，但是自己曾从事python后端开发、python算法工程师岗位，所以本篇文章主要通过python来介绍机器学习的路线。当然，前期的机器学习基础的推荐，是不会分语言的。\n纯小白——计算机小白\n如果你是完完全全的纯小白，并且只会计算机这三个字。\n个人推荐你可以看看《计算机科学导论》（专业书籍个人推荐看纸质书，一是支持正版；二是电子书不利于观看书的全貌；三是电子书不方便做笔记），看完之后你应该明白计算机的相关知识，如计算机网络、数据结构与算法、数据库、文件存储过程、计算机语言。如果看完《计算机科学导论》你对不仅仅对人工智能感兴趣，还对计算机本身感兴趣，可以看看下面几本书。\n你可以看看《计算机：一部历史》，可以作为你的计算机发展史的普及读物。\n你可以看看《网络是怎样连接的》——计算机网络，恩，说的简单点就是看完本书，你应该能知道WiFi的实现原理。\n你可以看看《计算机组成原理》——恩，看完你能明白你的计算机工作的原理，如计算机的五大组成部分为控制器、运算器、存储器、输入设备、输出设备。\n你可以看看《数据结构与算法》——如果没有编程基础，跳过。有C的基础，可以看看大学的教材《数据结构与算法-C语言》；如果你有Python的基础，可以看看《数据结构与算法-Python描述》，就是总之对应语言的数据结构与算法书籍即可。\n多说一嘴，可能有同学问：老师，买哪一本《计算机科学导论》。我会回答你：哪一本都行，因为书籍能出现在市面上，就有他出现的理由，你买去看就行了。没必要挑三拣四，虽然我不得不承认市面上真的有一些写的可能不太好的书（我自己看过几本），但这都是个例。\n计算机小白——计算机语言（Python）小白\n看完《计算机科学导论》，相信你对计算机应该有了一定的了解。如果你不只是想成为最强王者，这个时候你应该入手一门计算机语言了。C、C++、Java、Python、R、Go、PHP、JavaScript，很多很多，他们各有优缺点，你自己仔细甄选。但是Python毫无疑问是最简单的，又由于本人从事Python开发，我介绍下你如何快速入门Python。\n首先你可以看看这两位老师的博客：https://www.cnblogs.com/linhaifeng/p/7278389.html或https://www.cnblogs.com/nickchen121/p/10718112.html，期间你可以穿插我接下来讲的书籍互补，但是你必须的看完博客第一篇——Python入门，之后再去看其他书籍，因为你需要使用Pycharm，而不是其他IDE编辑Python代码。\n第一本书应该是《Python从入门到实践》，这本书很浅显，但很适合小白，看完你可以去美国开个披萨店了。后面三个项目，不推荐做。\n第二本书应该是《笨方法学Python3》，很适合小白查漏补缺基础知识点。\n第三本书应该是《Python核心编程》，厚厚的一本书，更多的是接轨未来的项目，选看部分章节。\n第四本书应该是《流畅的Python》，如果你看完那位老师的Python面向对象高级的时候，可以看这本书，否则慎入，他会让你质疑自己是否学过python。\n第五本书《编写高质量Python代码的59个有效方法》，书名就可以看出，他能教会你什么。\n第六本书《改善Python程序的91个建议》，这个也不多说，干就对了，否则你代码写完只有你自己才看得懂了。\n计算机语言小白——算法小白\n本篇文章主要以Python举例，相信你现在对Python已经应用自如了，这个时候，你就需要补充算法知识，提高你的逻辑思维了。\n首先你可以看看《数据结构与算法——Python描述》这本书，由于是中文的，相对友好，看完你最起码得知道线性表、链表、堆、栈、哈希表、二叉树、图之间的区别，然后一些简单的算法。\n其次你可以看看《Python算法教程》，说实话，这本书翻译的可真不行，如果你不想看，那就不看吧！\n不得不推荐一本英文书，因为这是我的算法启蒙书《Problem Solving with Algorithms and Data Structures using Python》，国内之前是很少Python算法教材的，几乎为零，这一本讲的真的不错，亚马逊有卖。\n如果你有闲余时间，就别去虎牙、熊猫了，可以逛一逛题库-领扣（LeetCode），https://leetcode-cn.com/problemset/all/\n算法小白-人生方向定位\n现在的你，可谓是入门编程这个世界，但是你远没有达到码农这个程度，你仅仅是步入魔法世界的石墙。其他语言我不了解，我不多说，接下来我讲讲python的几大方向，你需要确定你的人生职业了。如果你感觉你不喜欢人工智能了，也可以转岗，嘻嘻。虽然本篇题目是说人工智能，但是，其他的你听听也不错呀！\nPython后端开发，入门较为简单，不需要较高的算法基础，未来可以学习Django、Flask、Tornado后端框架；对Mysql、PostgreSql、MOngoDB、Redis等数据库有较深的理解；简单的了解Linux你就可以出去找实习工作了，嘻嘻。不想找实习工作，一句话说不清楚，私聊我，我教你怎么做！\nPython爬虫工程师，入门较为简单，同样不需要较高的算法基础，未来可以学习Scrapy框架；对Mysql、PostgreSql、MOngoDB、Redis等数据库有较深的理解；较Python后端开发，你就需要对Python有更深刻的理解，因为你需要写很多脚本，不扩展了，同理，有问题私聊我。\nPython自动化运维，我并不是很熟悉，我不多说，简而言之就是Python结合Linux实现自动化，但是你可以私聊我，我知道谁懂。\nPython数据分析师，这个可就有点档次了，门槛稍微提升，不做扩展，同理，想深入了解，私聊我。\nPython算法工程师（机器学习/深度学习领域），这个档次就不用说了，我们会重点在下文讨论。\n人生定位——机器学习大师\n首先得明确告诉你一点，人工智能是一个领域，机器学习是实现人工智能的一种方式，深度学习是机器学习的一个实现方法。所以，我们只对机器学习做一个介绍，说人工智能应该就是欺你是小白。\n首先推荐你一本顶级入门书《人工智能基础-高中版》，这本书后面章节可以跳过，就当科学普及吧。\n其次推荐你看一位大师的网课，不得不推荐，吴恩达老师的视频，相信很多机器学习入门的小兄弟都看过，同理后面的章节可以选看，不要硬着头皮看，你看不懂的，你缺乏基础。\n期间，你可以买一本周志华老师的《机器学习-西瓜书》，但是，不是让你看他，而是让你知道你要学习哪些东西，此书不适合入门，适合未来参考。\n这个时候，你需要干嘛？你需要补数学了，《程序员的数学 ①》、《程序员的数学 ②》、《程序员的数学 ③》看完再看下面的部分吧，否则下面的书籍于你而言就是天书，同理看不懂的跳过。\n看完上面三本数学，还不够哦！再来三本《简明微积分》、《简明线性代数》、《概率论极其应用》，同理看不懂的跳过。\n通过上面六本书的熏陶，最起码你知道数学的各种符号表示什么意思了，如\\(f(x),\\sum,\\prod\\)，对于你而言这就够了。\n数学看完，你得先入门传统机器学习，你可以先看看《图解机器学习》、《白话大数据与机器学习》，对传统的机器学习有一个了解，知道线性回归、逻辑回归、支持向量机是啥，这就够了。\n如果你知道机器学习算法是啥，李航老师的《统计学习方法》值得一看，你需要对算法内部的推导以及实现有一个清晰的认知，期间可以参考《机器学习-西瓜书》。\n万事俱备，只欠东风，这个时候你需要通过代码实现大型项目了，《Python机器学习》是一本不错的书，把scikit-learn的用法都介绍了个遍，期间你可以补充numpy+pandas+matplotlib库的使用，官方文档等着你。本书后面的tensorflow1已经被淘汰了，可以等待市面上tensorflow2的更新，也可以参考博客https://www.cnblogs.com/nickchen121/p/10840284.html。\n终于对机器学习有一个全面的理解了，这个时候你可以尝试入门深度学习了，你可以先尝试了解《机器学习-西瓜书》中的神经网络章节以及吴恩达老师的神经网络的视频。\n对神经网络有一定的认识之后，你可以看看《图解深度学习》，很好的一本入门书，同理，看不懂的跳过，虽然这本书讲的知识点少，但有一定的深度。\n终于走到了这一本终极书《深度学习-花书》，这本书中的所有数学必会，而且你应该都会了。多说一嘴，这本书你应该要仔细钻研了，其他的我不多说，800多页，多看几遍，你不看我也无能为力呀。\n然后，你就出师了，可以尝试去了解强化学习，目前还是挺流行的。\n最后，多说一嘴，上述所有书籍，看不懂的跳过，看不懂要么因为你基础不行，要么因为你还没到那个境界，不是你傻。朝着这两个方向努力看看是自己哪个地方出了问题，基础不够暂时停一停补基础，境界不够跳过不要看。"}
{"content2":"人工智能（AI）：\n人工智能指的是在处理任务时具有人类智力特点的机器。包括具有组织和理解语言，识别物体和声音，以及学习和解决问题等能力。\n机器学习（ML）：\n机器学习是一种实现人工智能的方式。\n深度学习（DL）：\n深度学习是你实现机器学习的途经之一。其他途径包括决策树，归纳逻辑程序设计，聚类，强化学习和贝叶斯网络等。\n人工神经网络（ANNs）：\n人工神经网络是深度学习的方法。\n人工智能与物联网的关系：\n人工智能与物联网之间的关系正如人类的大脑和身体。"}
{"content2":"转自 http://blog.csdn.net/kang1292655979/article/details/52799439\n今年8月，雷锋网(搜索“雷锋网”公众号关注)将在深圳举办“全球人工智能与机器人创新大会”（GAIR），在本次大会上，我们将发布“人工智能与机器人Top25创新企业榜“，速感科技是我们重点关注的公司之一。今天，我们邀请到了速感科技CTO张一茗，为大家揭秘SLAM技术的前世今生。\n张一茗。速感科技CTO。毕业于北京航空航天大学，师从中国惯性技术领域的著名专家冯培德院士，多年组合导航定位系统研究经验。热爱技术和研发，摘得过许多诸如Intel iot创客马拉松、清华创客马拉松等创客比赛第一名。速感科技经过多年历练，发展出一套以视觉SLAM为核心，集探索、导航、定位、避障、路径规划为一体的成熟化机器人无源导航算法。\nSLAM作为一种基础技术，从最早的军事用途（核潜艇海底定位就有了SLAM的雏形）到今天，已经逐步走入人们的视野，过去几年扫地机器人的盛行让它名声大噪，近期基于三维视觉的VSLAM又让它越来越显主流，许多人不得不关注它，但根据雷锋网(搜索“雷锋网”公众号关注)的调查，了解它并能真正把它说清楚的国内大牛并不多，今天，我们请来了速感科技的CTO，张一茗，从SLAM的前世今生开始，彻底扫清我们心中的疑惑。\n▌SLAM的前世\n我之前从本科到研究生，一直在导航与定位领域学习，一开始偏重于高精度的惯性导航、卫星导航、星光制导及其组合导航。出于对实现无源导航的执念，我慢慢开始研究视觉导航中的SLAM方向，并与传统的惯性器件做组合，实现独立设备的自主导航定位。\n定位、定向、测速、授时是人们惆怅千年都未能完全解决的问题，最早的时候，古人只能靠夜观天象和司南来做简单的定向。直至元代，出于对定位的需求，才华横溢的中国人发明了令人叹为观止的牵星术，用牵星板测量星星实现纬度估计。\n1964年美国投入使用GPS，突然就打破了大家的游戏规则。军用的P码可以达到1-2米级精度，开放给大众使用的CA码也能够实现5-10米级的精度。\n后来大家一方面为了突破P码封锁，另一方面为了追求更高的定位定姿精度，想出了很多十分具有创意的想法来挺升GPS的精度。利用RTK的实时相位差分技术，甚至能实现厘米的定位精度，基本上解决了室外的定位和定姿问题。\n但是室内这个问题就难办多了，为了实现室内的定位定姿，一大批技术不断涌现，其中，SLAM技术逐渐脱颖而出。SLAM是一个十分交叉学科的领域，我先从它的传感器讲起。\n▌离不开这两类传感器\n目前用在SLAM上的Sensor主要分两大类，激光雷达和摄像头。（待会儿发的部分素材摘自官网、论文、专利，侵删）。\n这里面列举了一些常见的雷达和各种深度摄像头。激光雷达有单线多线之分，角分辨率及精度也各有千秋。SICK、velodyne、Hokuyo以及国内的北醒光学、Slamtech是比较有名的激光雷达厂商。他们可以作为SLAM的一种输入形式。\n这个小视频里展示的就是一种简单的2D SLAM。\n这个小视频是宾大的教授kumar做的特别有名的一个demo，是在无人机上利用二维激光雷达做的SLAM。\n而VSLAM则主要用摄像头来实现，摄像头品种繁多，主要分为单目、双目、单目结构光、双目结构光、ToF几大类。他们的核心都是获取RGB和depth map(深度信息)。简单的单目和双目（Zed、leapmotion）我这里不多做解释，我主要解释一下结构光和ToF。\n▌最近流行的结构光和TOF\n结构光原理的深度摄像机通常具有激光投射器、光学衍射元件（DOE）、红外摄像头三大核心器件。\n这个图（下图）摘自primesense的专利。\n可以看到primesense的doe是由两部分组成的，一个是扩散片，一个是衍射片。先通过扩散成一个区域的随机散斑，然后复制成九份，投射到了被摄物体上。根据红外摄像头捕捉到的红外散斑，PS1080这个芯片就可以快速解算出各个点的深度信息。\n这儿还有两款结构光原理的摄像头。\n第一页它是由两幅十分规律的散斑组成，最后同时被红外相机获得，精度相对较高。但据说DOE成本也比较高。\n还有一种比较独特的方案（最后一幅图），它采用mems微镜的方式，类似DLP投影仪，将激光器进行调频，通过微镜反射出去，并快速改变微镜姿态，进行行列扫描，实现结构光的投射。（产自ST，ST经常做出一些比较炫的黑科技）。\nToF（time of flight）也是一种很有前景的深度获取方法。\n传感器发出经调制的近红外光，遇物体后反射，传感器通过计算光线发射和反射时间差或相位差，来换算被拍摄景物的距离，以产生深度信息。类似于雷达，或者想象一下蝙蝠，softkinetic的DS325采用的就是ToF方案（TI设计的），但是它的接收器微观结构比较特殊，有2个或者更多快门，测ps级别的时间差，但它的单位像素尺寸通常在100um的尺寸，所以目前分辨率不高。以后也会有不错的前景，但我觉得并不是颠覆性的。\n好，那在有了深度图之后呢，SLAM算法就开始工作了，由于Sensor和需求的不同，SLAM的呈现形式略有差异。大致可以分为激光SLAM（也分2D和3D）和视觉SLAM（也分Sparse、semiDense、Dense）两类，但其主要思路大同小异。\n这个是Sparse（稀疏）的\n这个偏Dense（密集）的\n▌SLAM算法实现的4要素\nSLAM算法在实现的时候主要要考虑以下4个方面吧：\n1. 地图表示问题，比如dense和sparse都是它的不同表达方式，这个需要根据实际场景需求去抉择\n2. 信息感知问题，需要考虑如何全面的感知这个环境，RGBD摄像头FOV通常比较小，但激光雷达比较大\n3. 数据关联问题，不同的sensor的数据类型、时间戳、坐标系表达方式各有不同，需要统一处理\n4. 定位与构图问题，就是指怎么实现位姿估计和建模，这里面涉及到很多数学问题，物理模型建立，状态估计和优化\n其他的还有回环检测问题，探索问题（exploration），以及绑架问题（kidnapping）。\n这个是一个比较有名的SLAM算法，这个回环检测就很漂亮。但这个调用了cuda，gpu对运算能力要求挺高，效果看起来比较炫。\n▌以VSLAM举个栗子\n我大概讲一种比较流行的VSLAM方法框架。\n整个SLAM大概可以分为前端和后端，前端相当于VO（视觉里程计），研究帧与帧之间变换关系。首先提取每帧图像特征点，利用相邻帧图像，进行特征点匹配，然后利用RANSAC去除大噪声，然后进行匹配，得到一个pose信息（位置和姿态），同时可以利用IMU（Inertial measurement unit惯性测量单元）提供的姿态信息进行滤波融合\n后端则主要是对前端出结果进行优化，利用滤波理论（EKF、UKF、PF）、或者优化理论TORO、G2O进行树或者图的优化。最终得到最优的位姿估计。\n后端这边难点比较多，涉及到的数学知识也比较多，总的来说大家已经慢慢抛弃传统的滤波理论走向图优化去了。\n因为基于滤波的理论，滤波器稳度增长太快，这对于需要频繁求逆的EKF（扩展卡尔曼滤波器），PF压力很大。而基于图的SLAM，通常以keyframe（关键帧）为基础，建立多个节点和节点之间的相对变换关系，比如仿射变换矩阵，并不断地进行关键节点的维护，保证图的容量，在保证精度的同时，降低了计算量。\n列举几个目前比较有名的SLAM算法：PTAM,MonoSLAM, ORB-SLAM,RGBD-SLAM,RTAB-SLAM,LSD-SLAM。\n所以大家如果想学习SLAM的话，各个高校提高的素材是很多的，比如宾大、MIT、ETH、香港科技大学、帝国理工等等都有比较好的代表作品，还有一个比较有前景的就是三维的机器视觉，普林斯顿大学的肖剑雄教授结合SLAM和Deep Learning做一些三维物体的分类和识别， 实现一个对场景深度理解的机器人感知引擎。\nhttp://robots.princeton.edu/talks/2016_MIT/RobotPerception.pdf  这是他们的展示。\n总的来说，SLAM技术从最早的军事用途（核潜艇海底定位就有了SLAM的雏形）到今天，已经逐步走入人们的视野，扫地机器人的盛行更是让它名声大噪。同时基于三维视觉的VSLAM越来越显主流。在地面/空中机器人、VR/AR/MR、汽车/AGV自动驾驶等领域，都会得到深入的发展，同时也会出现越来越多的细分市场等待挖掘。\n这个是occipital团队出的一个产品，是个很有意思的应用，国内卖4000+，大概一个月1000出货量吧（虽然不是很多，但是效果不错，pad可玩）虚拟家居、无人飞行／驾驶、虚拟试衣、3D打印、刑侦现场记录、沉浸式游戏、增强现实、商场推送、设计辅助、地震救援、工业流水线、GIS采集等等，都等待着VSLAM技术一展宏图\n▌SLAM的今生——还存在着问题\n多传感器融合、优化数据关联与回环检测、与前端异构处理器集成、提升鲁棒性和重定位精度都是SLAM技术接下来的发展方向，但这些都会随着消费刺激和产业链的发展逐步解决。就像手机中的陀螺仪一样，在不久的将来，也会飞入寻常百姓家，改变人类的生活。\n不过说实话，SLAM在全面进入消费级市场的过程中，也面对着一些阻力和难题。比如Sensor精度不高、计算量大、Sensor应用场景不具有普适性等等问题。\n多传感器融合、优化数据关联与回环检测、与前端异构处理器集成、提升鲁棒性和重定位精度都是SLAM技术接下来的发展方向，但这些都会随着消费刺激和产业链的发展逐步解决。就像手机中的陀螺仪一样，在不久的将来，也会飞入寻常百姓家，改变人类的生活。\n（激光雷达和摄像头两种 SLAM 方式各有什么优缺点呢，有没有一种综合的方式互补各自的缺点的呢？）\n激光雷达优点是可视范围广，但是缺点性价比低，低成本的雷达角分辨率不够高，影响到建模精度。vSLAM的话缺点就是FOV通常不大，50-60degree，这样高速旋转时就容易丢，解决方案有的，我们公司就在做vSLAM跟雷达还有IMU的组合。\n（请问目前基于视觉的SLAM的计算量有多大？嵌入式系统上如果要做到实时30fps，是不是只有Nvidia的芯片（支持cuda）才可以？）\n第一个问题，虽然基于视觉的SLAM计算量相对较大，但在嵌入式系统上是可以跑起来的，Sparse的SLAM可以达到30-50hz（也不需要GPU和Cuda），如果dense的话就比较消耗资源，根据点云还有三角化密度可调，10－20hz也是没有问题。\n并不一定要用cuda，一些用到cuda和GPU的算法主要是用来加速SIFT、ICP，以及后期三角化和mesh的过程，即使不用cuda可以采用其他的特征点提取和匹配策略也是可以的。\n▌最后一个问题\n（今年8月，雷锋网将在深圳举办“全球人工智能与机器人创新大会”（简称：GAIR）。想了解下，您对机器人的未来趋势怎么看？）\n这个问题就比较大了。\n机器人产业是个很大的Ecosystem，短时间来讲，可能产业链不够完整，消费级市场缺乏爆点爆款。虽然大家都在谈论做机器人，但是好多公司并没有解决用户痛点，也没有为机器人产业链创造什么价值。\n但是大家可以看到， 大批缺乏特色和积淀的机器人公司正在被淘汰，行业格局越来越清晰，分工逐渐完善，一大批细分市场成长起来。\n从机器人的感知部分来说，传感器性能提升、前端处理（目前的sensor前端处理做的太少，给主CPU造成了很大的负担）、多传感器融合是一个很大的增长点。\n现在人工智能也开始扬头，深度学习、神经网络专用的分布式异构处理器及其协处理器成为紧急需求，我个人很希望国内有公司能把这块做好。\n也有好多创业公司做底层工艺比如高推重比电机、高能量密度电池、复合材料，他们和机器人产业的对接，也会加速机器人行业的发展。整个机器人生态架构会越来越清晰，从硬件层到算法层到功能层到SDK 再到应用层，每一个细分领域都有公司切入，随着这些产业节点的完善，能看到机器人行业的前景还是很棒的，相信不久之后就会迎来堪比互联网的指数式增长！"}
{"content2":"原文地址：http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n翻译：Tacey Wong\n概要：\n该章节，我们将介绍贯穿scikit-learn使用中的“机器学习（Machine Learning）”这个词汇，并给出一些简单的学习示例。\n前言\nscikit-learn (Python机器学习库)\n进行数据挖掘和数据分析的简单而高效的工具\n任何人都可使用,可在多种场景/上下文复用\n基于NumPy,SciPy和matplotlib构建\n开放源代码,可用于商业用途_BSD协议\n分类\n识别一个对象属于那一种类别\n应用:垃圾邮件检测,图像识别\n算法:SVM(支持向量机),KNN(K近邻),随机森林\n回归\n预测与某个对象相关联的连续值属性\n应用:药物反应,股票价格\n算法:线性回归,SVR(支持向量回归),ridge regression(岭回归),LASSO回归\n聚类\n将相似的对象自动聚集到不同的集合中\n应用:顾客细分,分组试验结果\n算法:K-Means,谱聚类,mean-shift中值移动\n降维\n降低随机变量的数目\n可视化:可视化,提高效率\n算法:PCA(主成分分析),特征选取,非负矩阵分解\n模型选取\n比较,验证,参数和模型的选择\n目标:通过参数调整改进精度\n模块:网格搜索,交叉验证,metrics(度量)\n预处理\n特征提取和正则化\n应用: 转换数据以便机器学习算法使用\n模块:预处理,特征提取\n一、机器学习：问题设定\n通常，一个学习问题是通过分析一些数据样本来尝试预测未知数据的属性。如果每一个样本不仅仅是一个单独的数字，比如一个多维的实例（multivariate data），也就是说有着多个属性特征.我们可以把学习问题分成如下的几个大类：\n（1）有监督学习\n数据带有我们要预测的属性。这种问题主要有如下几种：\n①分类\n样例属于两类或多类，我们想要从已经带有标签的数据学习以预测未带标签的数据。识别手写数字就是一个分类问题，这个问题的主要目标就是把每一个输出指派到一个有限的类别中的一类。另一种思路去思考分类问题，其实分类问题是有监督学习中的离散形式问题。每一个都有一个有限的分类。对于样例提供的多个标签，我们要做的就是把未知类别的数据划分到其中的一种。\n②回归\n去过预期的输出包含连续的变量，那么这样的任务叫做回归。根据三文鱼的年纪和中联预测其长度就是一个回归样例。\n（2）无监督学习\n训练数据包含不带有目标值的输入向量x。对于这些问题，目标就是根据数据发现样本中相似的群组——聚类。或者在输入空间中判定数据的分布——密度估计，或者把数据从高维空间转换到低维空间以用于可视化\n训练集和测试集\n机器学习是学习一些数据集的特征属性并将其应用于新的数据。这就是为什么在机器学习用来评估算法时一般把手中的数据分成两部分。一部分我们称之为训练集，用以学习数据的特征属性。一部分我们称之为测试集，用以检验学习到的特征属性。\n二、加载一个样本数据集\nscikit-learn本身带有一些标准数据集。比如用来分类的iris(鸢尾花)数据集、digits(数字)数据集；用来回归的boston house price(波士顿房屋价格) 数据集。\n接下来，我们我们从shell开启一个Python解释器并加载iris和digits两个数据集。【译注：一些代码惯例就不写了，提示符>>>之类的学过Python的都懂】\n$ python >>>from sklearn import datasets #从sklearn包中加载数据集模块 >>>iris = datasets.load_iris() #加载鸢尾花数据集 >>>digits = datasets.load_digits() #加载数字图像数据集\n一个数据集是一个包含数据所有元数据的类字典对象。这个数据存储在 '.data'成员变量中，是一个\\(n*n\\)的数组，行表示样例，列表示特征。在有监督学习问题中，一个或多个响应变量（Y）存储在‘.target’成员变量中。不同数据集的更多细节可以在专属章节中找到。\n例如，对于digits数据集，digits.data可以访问得到用来对数字进行分类的特征：\n>>>print(digits.data) [[ 0. 0. 5. ..., 0. 0. 0.] [ 0. 0. 0. ..., 10. 0. 0.] [ 0. 0. 0. ..., 16. 9. 0.] ..., [ 0. 0. 1. ..., 6. 0. 0.] [ 0. 0. 2. ..., 12. 0. 0.] [ 0. 0. 10. ..., 12. 1. 0.]]\ndigits.target 就是数字数据集各样例对应的真实数字值。也就是我们的程序要学习的。\n>>>digits.target array([0, 1, 2, ..., 8, 9, 8])\n数据数组的形状\n尽管原始数据也许有不同的形状，但实际使用的数据通常是一个二维数组（n个样例，n个特征）。对于数字数据集，每一个原始的样例是一张（8 x 8）的图片,也能被使用：\n>>>digits.images[0] array([[ 0., 0., 5., 13., 9., 1., 0., 0.], [ 0., 0., 13., 15., 10., 15., 5., 0.], [ 0., 3., 15., 2., 0., 11., 8., 0.], [ 0., 4., 12., 0., 0., 8., 8., 0.], [ 0., 5., 8., 0., 0., 9., 8., 0.], [ 0., 4., 11., 0., 1., 12., 7., 0.], [ 0., 2., 14., 5., 10., 12., 0., 0.], [ 0., 0., 6., 13., 10., 0., 0., 0.]])\n三、学习和预测\n对于数字数据集（digits dataset），任务是预测一张图片中的数字是什么。数字数据集提供了0-9每一个数字的可能样例，可以用它们来对位置的数字图片进行拟合分类。\n在scikit-learn中，用以分类的拟合（评估）函数是一个Python对象，具体有fit(X,Y)和predic(T)两种成员方法。\n其中一个拟合（评估）样例是sklearn.svmSVC类，它实现了支持向量分类（SVC）。一个拟合（评估）函数的构造函数需要模型的参数，但是时间问题，我们将会把这个拟合（评估）函数作为一个黑箱：\n>>>from sklearn import svm >>>clf = svm.SVC(gamma=0.001, C=100.)\n选择模型参数\n我们调用拟合（估测）实例clf作为我们的分类器。它现在必须要拟合模型，也就是说，他必须要学习模型。这可以通过把我们的训练集传递给fit方法。作为训练集，我们使用其中除最后一组的所有图像。我们可以通过Python的分片语法[:-1]来选取训练集，这个操作将产生一个新数组，这个数组包含digits.data中除最后一组数据的所有实例。\n>>>clf.fit(digits.data[:-1], digits.target[:-1]) SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n现在你就可以预测新的数值了。我们可以让这个训练器预测没有作为训练数据使用的最后一张图像是什么数字。\n>>>clf.predict(digits.data[-1]) array([8])\n相应的图片如下图：\n正如你所看到的，这是一个很有挑战的任务：这张图片的分辨率很低。你同意分类器给出的答案吗？\n这个分类问题的完整示例在这里识别手写数字，你可以运行并使用它。[译：看本文附录]\n四、模型持久化\n可以使用Python的自带模块——pickle来保存scikit中的模型：\n>>>from sklearn import svm >>>from sklearn import datasets >>>clf = svm.SVC() >>>iris = datasets.load_iris() >>>X, y = iris.data, iris.target >>>clf.fit(X, y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) >>>import pickle >>>s = pickle.dumps(clf) >>>clf2 = pickle.loads(s) >>>clf2.predict(X[0]) array([0]) >>>y[0] 0\n对于scikit，也许使用joblib的pickle替代——（joblib.dump&joblib.load）更有趣。因为它在处理带数据时更高效。但是遗憾的是它只能把数据持久化到硬盘而不是一个字符串（译注：搬到string字符串意味着数据在内存中）：\n>>>from sklearn.externals import joblib >>>joblib.dump(clf, 'filename.pkl')\n往后你就可以加载这个转储的模型（也能在另一个Python进程中使用），如下：\n>>>clf = joblib.load('filename.pkl')\n注意：\njoblib.dump返回一个文件名的列表，每一个numpy数组元素包含一个clf在文件系统上的名字，在用joblib.load加载的时候所有的文件需要在相同的文件夹下\n注意pickle有一些安全和可维护方面的问题。请参考Model persistent 获得在scikit-learn中模型持久化的细节。\n五、惯例约定\nscikit-learn的各种拟合（评估）函数遵循一些确定的规则以使得他们的用法能够被预想到（译：使得各种学习方法的用法统一起来）\n①类型转换\n除非特别指定，输入将被转换为float64\nimport numpy from sklearn import random_projection rng = np.random.RandomState(0) X = rng.rand(10,2000) X = np.array(X,dtype ='float32') print x.dtype transformer = random_projection.GaussianRandomProjection() X_new = transformer.fit_transform(X) print X_new.dtype\n在这个例子中，X是float32，被fit_transform(X)转换成float64,回归被转换成float64，分类目标维持不变.\nfrom sklearn import datesets from sklearn.svm import SVC iris = datasets.load_iris() clf =SVC() clf.fit(iris.data,iris.target) print list(clf.predict(iris.data[:3])) clf.fit(iris.data,iris.target_names[iris.target]) print list(clf.predict(iris.data[:3]))\n这里第一个predict()返回一个整数数组，是因为iris.target(一个整数数组)被用于拟合。第二个predict()返回一个字符串数组，因为iris.target_names被用于拟合。\n②重拟合和更新参数\n一个拟合（评估）函数的混合参数（超参数）能够在通过sklearn.pipeline.Pipeline.set_params方法构造之后被更新。多次调用fit()能够覆写之前fit()学习的内容：\nimport numpy as np from sklearn.svm import SVC rng = np.random.RandomState(0); X = rng.rand(100,10) Y = rng.binomial(1,0.5,100) X_test = rng.rand(5,10) clf = SVC() clf.set_params(kernel = 'linear').fit(X,Y) clf.predict(X_test) clf.set_params(kernel='rbf').fit(X,Y) clf.predict(X_test)\n这里,用SVC()构造之后，开始拟合（评估）函数默认的'rbf'核被改编成'linear'，后来又改回'rbf'去重拟合做第二次的预测。\n附：\n①digits数据集：一个展示怎样用scikit-learn识别手写数字的样例:绘制数字：\n# 源代码: Gaël Varoquaux # 修改以进行文档化:Jaques Grobler # 协议: BSD 3 from sklearn import datasets import matplotlib.pyplot as plt #加载数字数据集 digits = datasets.load_digits() #展示第一个数字 plt.figure(1, figsize=(3, 3)) plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest') plt.show()\n②绘制数字分类 （plot_digits_classification.py）\n# 作者: Gael Varoquaux <gael dot varoquaux at normalesup dot org> # 协议: BSD 3 clause # Python标准科学计算包导入 import matplotlib.pyplot as plt # 导入数据集,分类器和评估度量 from sklearn import datasets, svm, metrics # 数字数据集 digits = datasets.load_digits() #数据是一个8x8的数字图像,让我们先看看开头的三张图像.图像存储在数据集 #的`images`属性中,如果我们要加载图像文件的话,可以使用pylab.imread. #注意每一张图像尺寸必须相等.这些图像各自对应的数字是多少我们是知道的 #他们存储在数据集的target属性中. images_and_labels = list(zip(digits.images, digits.target)) for index, (image, label) in enumerate(images_and_labels[:4]): plt.subplot(2, 4, index + 1) plt.axis('off') plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest') plt.title('Training: %i' % label) # 在数据上应用一个分类器, 我们需要铺平图像, # 将数据转换成二位矩阵: n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) # 创建一个分类器: 一个支持向量分类器 classifier = svm.SVC(gamma=0.001) # 我们在前半部分数据上进行学习 classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2]) # 现在预测后半部分的值: expected = digits.target[n_samples / 2:] predicted = classifier.predict(data[n_samples / 2:]) print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(expected, predicted))) print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted)) images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted)) for index, (image, prediction) in enumerate(images_and_predictions[:4]): plt.subplot(2, 4, index + 5) plt.axis('off') plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest') plt.title('Prediction: %i' % prediction) plt.show()"}
{"content2":"机器学习原理、实现与实践——机器学习概论\n如果一个系统能够通过执行某个过程改进它的性能，这就是学习。 ——— Herbert A. Simon\n1. 机器学习是什么\n计算机基于数据来构建概率统计模型并运用模型对数据进行预测与分析的一门学科。\n从上面的机器学习的定义中，我们可以了解到以下的信息：\n机器学习以计算机及网络为平台，是建立在计算机及网络之上的；\n机器学习以数据为研究对象。\n机器学习的目的是对数据进行预测与分析\n机器学习以模型为中心。构建模型、优化模型并用模型来进行预测。\n机器学习的模型是基于概率统计的模型。里面大量用到了概率与统计的知识。\n机器学习也是信息论 、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独立的理论体系与方法论。\n2. 机器学习的对象\n机器学习的对象是数据，它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。同时，数据是多样的，包括存在计算机及网络上的各种数字、文字、图像、视频、音频数据及它们的组合。\n那么什么样的数据可以被抽象，被学习呢，杂乱无章的数据可以吗？\n机器学习关于数据的基本假设是同类数据具有一定的统计规律性。同类数据是指具有某种共同性质的数据，由于它们具有统计规律，所以可以用概率统计方法来加以处理。可以用随机变量描述数据数据中的特征，用概率分布描述数据的统计规律。\n在实际的机器学习中，数据往往被提取为一个特征向量表示为\n$$x = (x^{(1)},x^{(2)},\\dots,x^{(i)},x^{(n)})^T$$\n数据可以为离散的，也可以为连续的。\n3. 机器学习的目的\n机器学习用于对数据进行预测与分析，特别是对未知新数据进行预测与分析。\n机器学习总的目标就是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同理也要考虑尽可能地提高学习效率。\n4. 机器学习的方法\n机器学习的方法是基于数据构建统计模型从而对数据进行预测与分析。机器学习包括了监督学习、非监督学习、半监督学习和强化学习。\n监督学习：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数集合，这个函数集合称为假设空间（hypothesis space）；应用某个评价准则，从假设空间中先取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选择由算法实现。\n模型的假设空间、模型的选择准则以及模型学习的算法构成了机器学习的三要素，简称模型、策略、算法。\n机器学习的步骤可以归纳为：\n得到一个有限的训练数据集合；\n确定包含所有可能的模型的假设空间，即学习模型的集合；\n确定模型选择的准则，即学习策略；\n实现求解最优模型的算法，即学习的算法；\n通常学习方法选择最优模型；\n利用学习的最优模型对新数据进行预测或分析。\n5. 机器学习的应用\n近20年来，机器学习无论是在理论上还是在应用方面都得到了巨大的发展，有许多重要突破，统计学习已被成功地应用到人工智能、模式识别、数据挖掘、自然语言处理、语音识别、图像识别、信息检索和生物信息等许多计算机应用领域中。\n下面是从《机器学习实战》中摘录的一段文字，描述了假想的一日，机器学习已经与我们的生活息息相关。\n假设你想起今天是某位朋友的生日，打算通过邮局给她邮寄一张生日贺卡。你打开浏览器搜索趣味卡片，搜索引擎显示了10个最相关的链接。你认为第二个链接最符合你的要求，点击这个链接，搜索引擎将记录这次点击，并从中学习以优化下次搜索结果。然后，你检查电子邮件系统，此时垃圾邮件过滤器已经在后台自动过滤垃圾广告邮件，并将其放在垃圾箱内。接着你去商店购买这张生日卡片，并给你朋友的孩子挑选了一些尿布。结账时，收银员给你一张1美元的优惠券，可以用于购买6罐装的啤酒。之所以你会得到这张优惠券，是因为款台收费软件基于以前的统计知识，认为买尿布的人往往也会买啤酒。然后你去邮局邮寄这张贺卡，手写识别软件识别出邮寄地址，并将贺卡发送给正确的邮车。当天你还去了贷款申请机构，查看自己是否能够申请贷款，办事员并不是直接给出结果，而是将你最近的金融活动信息输入计算机，由软件来判定你是否合格。最后，你还去了赌场想找些乐子，当你步入前门时，尾随你进来的一个家伙被突然出现的保安给拦了下来。“对不起，索普先生，我们不得不请你离开赌场。我们不欢迎老千”。\n上面提到的所有场景，都有机器学习存在！"}
{"content2":"现在机器学习逐渐成为行业热门，经过二十几年的发展，机器学习目前也有了十分广泛的应用，如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、DNA序列测序、语音和手写识别、战略游戏和机器人等方面。\n翻译整理了目前GitHub上最受欢迎的28款开源的机器学习项目，以供开发者参考使用。\n1. TensorFlow\nTensorFlow 是谷歌发布的第二代机器学习系统。据谷歌宣称，在部分基准测试中，TensorFlow的处理速度比第一代的DistBelief加快了2倍之多。\n具体的讲，TensorFlow是一个利用数据流图（Data Flow Graphs）进行数值计算的开源软件库：图中的节点（ Nodes）代表数学运算操作，同时图中的边（Edges）表示节点之间相互流通的多维数组，即张量（Tensors）。这种灵活的架构可以让使用者在多样化的将计算部署在台式机、服务器或者移动设备的一个或多个CPU上，而且无需重写代码；同时任一基于梯度的机器学习算法均可够借鉴TensorFlow的自动分化（Auto-differentiation）；此外通过灵活的Python接口，要在TensorFlow中表达想法也变得更为简单。\nTensorFlow最初由Google Brain小组（该小组隶属于Google's Machine Intelligence研究机构）的研究员和工程师开发出来的，开发目的是用于进行机器学习和深度神经网络的研究。但该系统的通用性足以使其广泛用于其他计算领域。\n目前Google 内部已在大量使用 AI 技术，包括 Google App 的语音识别、Gmail 的自动回复功能、Google Photos 的图片搜索等都在使用 TensorFlow 。\n开发语言：C++\n许可协议：Apache License 2.0\nGitHub项目地址：https://github.com/tensorflow/tensorflow\n2. Scikit-Learn\nScikit-Learn是用于机器学习的Python 模块，它建立在SciPy之上。该项目由David Cournapeau 于2007年创立，当时项目名为Google Summer of Code，自此之后，众多志愿者都为此做出了贡献。\n主要特点：\n操作简单、高效的数据挖掘和数据分析\n无访问限制，在任何情况下可重新使用\n建立在NumPy、SciPy 和 matplotlib基础上\nScikit-Learn的基本功能主要被分为六个部分：分类、回归、聚类、数据降维、模型选择、数据预处理，具体可以参考官方网站上的文档。经过测试，Scikit-Learn可在 Python 2.6、Python 2.7 和 Python 3.5上运行。除此之外，它也应该可在Python 3.3和Python 3.4上运行。\n注：Scikit-Learn以前被称为Scikits.Learn。\n开发语言：Python\n许可协议:3-Clause BSD license\nGitHub项目地址: https://github.com/scikit-learn/scikit-learn\n3.Caffe\nCaffe 是由神经网络中的表达式、速度、及模块化产生的深度学习框架。后来它通过伯克利视觉与学习中心（(BVLC）和社区参与者的贡献，得以发展形成了以一个伯克利主导，然后加之Github和Caffe-users邮件所组成的一个比较松散和自由的社区。\nCaffe是一个基于C++/CUDA架构框架，开发者能够利用它自由的组织网络，目前支持卷积神经网络和全连接神经网络（人工神经网络）。在Linux上，C++可以通过命令行来操作接口，对于MATLAB、Python也有专门的接口，运算上支持CPU和GPU直接无缝切换。\nCaffe的特点\n易用性：Caffe的模型与相应优化都是以文本形式而非代码形式给出， Caffe给出了模型的定义、最优化设置以及预训练的权重，方便快速使用；\n速度快：能够运行最棒的模型与海量的数据；\nCaffe可与cuDNN结合使用，可用于测试AlexNet模型，在K40上处理一张图片只需要1.17ms；\n模块化：便于扩展到新的任务和设置上；\n使用者可通过Caffe提供的各层类型来定义自己的模型；\n目前Caffe应用实践主要有数据整理、设计网络结构、训练结果、基于现有训练模型，使用Caffe直接识别。\n开发语言：C++\n许可协议： BSD 2-Clause license\nGitHub项目地址: https://github.com/BVLC/caffe\n4. PredictionIO\nPredictionIO 是面向开发人员和数据科学家的开源机器学习服务器。它支持事件采集、算法调度、评估，以及经由REST APIs的预测结果查询。使用者可以通过PredictionIO做一些预测，比如个性化推荐、发现内容等。PredictionIO 提供20个预设算法，开发者可以直接将它们运行于自己的数据上。几乎任何应用与PredictionIO集成都可以变得更“聪明”。其主要特点如下所示：\n基于已有数据可预测用户行为；\n使用者可选择你自己的机器学习算法；\n无需担心可扩展性，扩展性好。\nPredictionIO 基于 REST API（应用程序接口）标准，不过它还包含 Ruby、Python、Scala、Java 等编程语言的 SDK（软件开发工具包）。其开发语言是Scala语言，数据库方面使用的是MongoDB数据库，计算系统采用Hadoop系统架构。\n开发语言：Scala\n许可协议： Apache License 2.0\nGitHub项目地址: https://github.com/PredictionIO/PredictionIO\n5. Brain\nBrain是 JavaScript 中的 神经网络库。以下例子说明使用Brain来近似 XOR 功能：\nvar net = new brain.NeuralNetwork(); net.train([{input: [0, 0], output: [0]}, {input: [0, 1], output: [1]}, {input: [1, 0], output: [1]}, {input: [1, 1], output: [0]}]); var output = net.run([1, 0]); // [0.987]\n当 brain 用于节点中，可使用npm安装：\nnpm install brain\n当 brain 用于浏览器，下载最新的 brain.js 文件。训练计算代价比较昂贵，所以应该离线训练网络（或者在 Worker 上），并使用 toFunction() 或者 toJSON()选项，以便将预训练网络插入到网站中。\n开发语言：JavaScript\nGitHub项目地址: https://github.com/harthur/brain\n6. Keras\nKeras是极其精简并高度模块化的神经网络库，在TensorFlow 或 Theano 上都能够运行，是一个高度模块化的神经网络库，支持GPU和CPU运算。Keras可以说是Python版的Torch7，对于快速构建CNN模型非常方便，同时也包含了一些最新文献的算法，比如Batch Noramlize，文档教程也很全，在官网上作者都是直接给例子浅显易懂。Keras也支持保存训练好的参数，然后加载已经训练好的参数，进行继续训练。\nKeras侧重于开发快速实验，用可能最少延迟实现从理念到结果的转变，即为做好一项研究的关键。\n当需要如下要求的深度学习的库时，就可以考虑使用Keras：\n考虑到简单快速的原型法（通过总体模块性、精简性以及可扩展性）；\n同时支持卷积网络和递归网络，以及两者之间的组合；\n支持任意连接方案（包括多输入多输出训练）；\n可在CPU 和 GPU 上无缝运行。\nKeras目前支持 Python 2.7-3.5。\n开发语言：Python\nGitHub项目地址:https://github.com/fchollet/keras\n7. CNTK\nCNTK（Computational Network Toolkit ）是一个统一的深度学习工具包，该工具包通过一个有向图将神经网络描述为一系列计算步骤。在有向图中，叶节点表示输入值或网络参数，其他节点表示该节点输入之上的矩阵运算。\nCNTK 使得实现和组合如前馈型神经网络DNN、卷积神经网络（CNN）和循环神经网络(RNNs/LSTMs)等流行模式变得非常容易。同时它实现了跨多GPU 和服务器自动分化和并行化的随机梯度下降（SGD，误差反向传播）学习。\n下图将CNTK的处理速度（每秒处理的帧数）和其他四个知名的工具包做了比较了。配置采用的是四层全连接的神经网络（参见基准测试脚本）和一个大小是8192 的高效mini batch。在相同的硬件和相应的最新公共软件版本（2015.12.3前的版本）的基础上得到如下结果：\nCNTK自2015年四月就已开源。\n开发语言：C++\nGitHub项目地址:https://github.com/Microsoft/CNTK\n8. Convnetjs\nConvNetJS是利用Javascript实现的神经网络，同时还具有非常不错的基于浏览器的Demo。它最重要的用途是帮助深度学习初学者更快、更直观的理解算法。\n它目前支持：\n常见的神经网络模块（全连接层，非线性）；\n分类（SVM/ SOFTMAX）和回归（L2）的成本函数；\n指定和训练图像处理的卷积网络；\n基于Deep Q Learning的实验强化学习模型。\n一些在线示例：\nConvolutional Neural Network on MNIST digits\nConvolutional Neural Network on CIFAR-10\nToy 2D data\nToy 1D regression\nTraining an Autoencoder on MNIST digits\nDeep Q Learning Reinforcement Learning demo +Image Regression (\"Painting\") +Comparison of SGD/Adagrad/Adadelta on MNIST 开发语言：Javascript 许可协议：MIT License GitHub项目地址:https://github.com/karpathy/convnetjs\n9. Pattern\nPattern是Python的一个Web挖掘模块。拥有以下工具：\n数据挖掘：网络服务（Google、Twitter、Wikipedia）、网络爬虫、HTML DOM解析；\n自然语言处理：词性标注工具(Part-Of-Speech Tagger)、N元搜索(n-gram search)、情感分析(sentiment analysis)、WordNet；\n机器学习：向量空间模型、聚类、分类（KNN、SVM、 Perceptron）；\n网络分析：图形中心性和可视化。\n其文档完善，目前拥有50多个案例和350多个单元测试。 Pattern目前只支持Python 2.5+（尚不支持Python 3），该模块除了在Pattern.vector模块中使用LSA外没有其他任何外部要求，因此只需安装 NumPy （仅在Mac OS X上默认安装）。\n开发语言：Python\n许可协议：BSD license\nGitHub项目地址:https://github.com/clips/pattern\n10. NuPIC\nNuPIC是一个实现了HTM学习算法的机器智能平台。HTM是一个关于新（大脑）皮质（Neocortex）的详细人工智能算法。HTM的核心是基于时间的连续学习算法，该算法可以存储和调用时间和空间两种模式。NuPIC可以适用于解决各类问题，尤其是异常检测和流数据源预测方面。\nNuPIC Binaries文件目前可用于：\nLinux x86 64bit\nOS X 10.9\nOS X 10.10\nWindows 64bit\nNuPIC 有自己的独特之处。许多机器学习算法无法适应新模式，而NuPIC的运作接近于人脑，当模式变化的时候，它会忘掉旧模式，记忆新模式。\n开发语言：Python\nGitHub项目地址：https://github.com/numenta/nupic\n11. Theano\nTheano是一个Python库，它允许使用者有效地定义、优化和评估涉及多维数组的数学表达式，同时支持GPUs和高效符号分化操作。Theano具有以下特点：\n与NumPy紧密相关--在Theano的编译功能中使用了Numpy.ndarray ；\n透明地使用GPU--执行数据密集型计算比CPU快了140多倍（针对Float32）；\n高效符号分化--Theano将函数的导数分为一个或多个不同的输入；\n速度和稳定性的优化--即使输入的x非常小也可以得到log(1+x)正确结果；\n动态生成 C代码--表达式计算更快；\n广泛的单元测试和自我验证--多种错误类型的检测和判定。\n自2007年起，Theano一直致力于大型密集型科学计算研究，但它目前也很被广泛应用在课堂之上（ 如Montreal大学的深度学习/机器学习课程）。\n开发语言：Python\nGitHub项目地址：https://github.com/Theano/Theano\n12. MXNet\nMXNet是一个兼具效率和灵活性的深度学习框架。它允许使用者将符号编程和命令式编程相结合，以追求效率和生产力的最大化。其核心是动态依赖调度程序，该程序可以动态自动进行并行化符号和命令的操作。其中部署的图形优化层使得符号操作更快和内存利用率更高。该库轻量且便携带，并且可扩展到多个GPU和多台主机上。\n主要特点：\n其设计说明提供了有用的见解，可以被重新应用到其他DL项目中；\n任意计算图的灵活配置；\n整合了各种编程方法的优势最大限度地提高灵活性和效率；\n轻量、高效的内存以及支持便携式的智能设备；\n多GPU扩展和分布式的自动并行化设置；\n支持Python、R、C++和 Julia；\n对“云计算”友好，直接兼容S3、HDFS和Azure。\nMXNet不仅仅是一个深度学习项目，它更是一个建立深度学习系统的蓝图、指导方针以及黑客们对深度学习系统独特见解的结合体。\n开发语言：Jupyter Notebook\n开源许可：Apache-2.0 license\nGitHub项目地址：https://github.com/dmlc/mxnet\n13. Vowpal Wabbit\nVowpal Wabbit是一个机器学习系统，该系统推动了如在线、散列、Allreduce、Learning2search、等方面机器学习前沿技术的发展。 其训练速度很快，在20亿条训练样本，每个训练样本大概100个非零特征的情况下：如果特征的总位数为一万时，训练时间为20分钟；特征总位数为1000万时，训练时间为2个小时。Vowpal Wabbit支持分类、 回归、矩阵分解和LDA。\n当在Hadoop上运行Vowpal Wabbit时，有以下优化机制：\n懒惰初始化：在进行All Reduce之前，可将全部数据加载到内存中并进行缓存。即使某一节点出现了错误，也可以通过在另外一个节点上使用错误节点的数据（通过缓存来获取）来继续训练。\nSpeculative Execution：在大规模集群当中，一两个很慢的Mapper会影响整个Job的性能。Speculative Execution的思想是当大部分节点的任务完成时，Hadoop可以将剩余节点上的任务拷贝到其他节点完成。\n开发语言：C++\nGitHub项目地址：https://github.com/JohnLangford/vowpal_wabbit\n14. Ruby Warrior\n通过设计了一个游戏使得Ruby语言和人工智能学习更加有乐趣和互动起来。\n使用者扮演了一个勇士通过爬上一座高塔，到达顶层获取珍贵的红宝石（Ruby）。在每一层，需要写一个Ruby脚本指导战士打败敌人、营救俘虏、到达楼梯。使用者对每一层都有一些认识，但是你永远都不知道每层具体会发生什么情况。你必须给战士足够的人工智能，以便让其自行寻找应对的方式。\n勇士的动作相关API：\nWarrior.walk： 用来控制勇士的移动，默认方向是往前；\nwarrior.feel：使用勇士来感知前方的情况，比如是空格，还是有怪物；\nWarrior.attack：让勇士对怪物进行攻击；\nWarrior.health：获取勇士当前的生命值；\nWarrior.rest：让勇士休息一回合，恢复最大生命值的10%。\n勇士的感知API:\nSpace.empty：感知前方是否是空格；\nSpace.stairs：感知前方是否是楼梯；\nSpace.enemy： 感知前方是否有怪物；\nSpace.captive：感知前方是否有俘虏；\nSpace.wall：感知前方是否是墙壁。\n开发语言：Ruby\nGitHub项目地址：https://github.com/ryanb/ruby-warrior\n15. XGBoost\nXGBoot是设计为高效、灵活、可移植的优化分布式梯度 Boosting库。它实现了 Gradient Boosting 框架下的机器学习算法。XGBoost通过提供并行树Boosting（也被称为GBDT、GBM），以一种快速且准确的方式解决了许多数据科学问题。相同的代码可以运行在大型分布式环境如Hadoop、SGE、MP上。它类似于梯度上升框架，但是更加高效。它兼具线性模型求解器和树学习算法。\nXGBoot至少比现有的梯度上升实现有至少10倍的提升，同时还提供了多种目标函数，包括回归、分类和排序。由于它在预测性能上的强大，XGBoot成为很多比赛的理想选择，其还具有做交叉验证和发现关键变量的额外功能。\n值得注意的是：XGBoost仅适用于数值型向量，因此在使用时需要将所有其他形式的数据转换为数值型向量；在优化模型时，这个算法还有非常多的参数需要调整。\n开发语言：C++\n开源许可：Apache-2.0 license\nGitHub项目地址：https://github.com/dmlc/xgboost\n16. GoLearn\nGoLearn 是Go 语言中“功能齐全”的机器学习库，简单性及自定义性是其开发目标。\n在安装 GoLearn 时，数据作为实例被加载，然后可以在其上操作矩阵，并将操作值传递给估计值。GoLearn 实现了Fit/Predict的Scikit-Learn界面，因此用户可轻松地通过反复试验置换出估计值。此外，GoLearn还包括用于数据的辅助功能，例如交叉验证、训练以及爆裂测试。\n开发语言：Go\nGitHub项目地址: https://github.com/sjwhitworth/golearn\n17. ML_for_Hackers\nML_for_Hackers 是针对黑客机器学习的代码库，该库包含了所有针对黑客的机器学习的代码示例（2012）。该代码可能和文中出现的并不完全相同，因为自出版以来，可能又添加了附加的注释和修改部分。\n所有代码均为R语言，依靠众多的R程序包，涉及主题包括分类(Classification)、排行(Ranking)、以及回归(Regression)的所有常见的任务和主成分分析(PCA)和多维尺度(Multi-dimenstional Scaling)等统计方法。\n开发语言：R\n开源许可：Simplified BSD License\nGitHub项目地址: https://github.com/johnmyleswhite/ML_for_Hackers\n18. H2O-2\nH2O使得Hadoop能够做数学运算！它可以通过大数据衡量统计数据、机器学习和数学。H2O是可扩展的，用户可以在核心区域使用简单的数学模型构建模块。H2O保留着与R、Excel 和JSON等相类似的熟悉的界面，使得大数据爱好者及专家们可通过使用一系列由简单到高级的算法来对数据集进行探索、变换、建模及评分。采集数据很简单，但判决难度却很大，而H2O却通过更快捷、更优化的预测模型，能够更加简单迅速地从数据中获得深刻见解。\n0xdata H2O的算法是面向业务流程——欺诈或趋势预测。Hadoop专家可以使用Java与H2O相互作用，但框架还提供了对Python、R以及Scala的捆绑。\n开发语言：Java\nGitHub项目地址: https://github.com/h2oai/h2o-2\n19. neon\nneon 是 Nervana 基于 Python 语言的深度学习框架，在诸多常见的深层神经网络中都能够获得较高的性能，比如AlexNet、VGG 或者GoogLeNet。在设计 neon 时，开发者充分考虑了如下功能：\n支持常用的模型及实例，例如 Convnets、 MLPs、 RNNs、LSTMs、Autoencoders 等，其中许多预训练的实现都可以在模型库中发现；\n与麦克斯韦GPU中fp16 和 fp32(基准) 的nervanagpu 内核紧密集成；\n在Titan X（1 GPU ~ 32 hrs上可完整运行）的AlexNet上为3s/macrobatch（3072图像）；\n快速影像字幕模型（速度比基于 NeuralTalk 的CPU 快200倍）。\n支持基本自动微分；\n框架可视化；\n可交换式硬盘后端：一次编写代码，然后配置到 CPU、GPU、或者 Nervana 硬盘。\n在 Nervana中，neon被用来解决客户在多个域间存在的各种问题。\n开发语言：Python\n开源许可：Apache-2.0 license\nGitHub项目地址: https://github.com/NervanaSystems/neon\n20. Oryx 2\n开源项目Oryx提供了简单且实时的大规模机器学习、预测分析的基础设施。它可实现一些常用于商业应用的算法类：协作式过滤/推荐、分类/回归、集群等。此外，Oryx 可利用 Apache Hadoop 在大规模数据流中建立模型，还可以通过HTTP REST API 为这些模型提供实时查询，同时随着新的数据不断流入，可以近似地自动更新模型。这种包括了计算层和服务层的双重设计，能够分别实现一个Lambda 架构。模型在PMML格式交换。\nOryx本质上只做两件事：建模和为模型服务，这就是计算层和服务层两个独立的部分各自的职责。计算层是离线、批量的过程，可从输入数据中建立机器学习模型，它的经营收益在于“代”，即可利用某一点处输入值的快照建模，结果就是随着连续输入的累加，随时间生成一系列输出；服务层也是一个基于Java长期运行的服务器进程，它公开了REST API。使用者可从浏览器中访问，也可利用任何能够发送HTTP请求的语言或工具进行访问。\nOryx的定位不是机器学习算法的程序库，Owen关注的重点有四个：回归、分类、集群和协作式过滤（也就是推荐）。其中推荐系统非常热门，Owen正在与几个Cloudera的客户合作，帮他们使用Oryx部署推荐系统。\n开发语言：Java\nGitHub项目地址: https://github.com/cloudera/oryx\n21. Shogun\nShogun是一个机器学习工具箱，由Soeren Sonnenburg 和Gunnar Raetsch（创建，其重点是大尺度上的内核学习方法，特别是支持向量机（SVM，Support Vector Machines）的学习工具箱。它提供了一个通用的连接到几个不同的SVM实现方式中的SVM对象接口，目前发展最先进的LIBSVM和SVMlight 也位于其中，每个SVM都可以与各种内核相结合。工具箱不仅为常用的内核程序（如线性、多项式、高斯和S型核函数）提供了高效的实现途径，还自带了一些近期的字符串内核函数，例如局部性的改进、Fischer、TOP、Spectrum、加权度内核与移位，后来有效的LINADD优化内核函数也已经实现。\n此外，Shogun还提供了使用自定义预计算内核工作的自由，其中一个重要特征就是可以通过多个子内核的加权线性组合来构造的组合核，每个子内核无需工作在同一个域中。通过使用多内核学习可知最优子内核的加权。\n目前Shogun可以解决SVM 2类的分类和回归问题。此外Shogun也添加了了像线性判别分析（LDA）、线性规划（LPM）、（内核）感知等大量线性方法和一些用于训练隐马尔可夫模型的算法。\n开发语言：C/C++、Python\n许可协议：GPLv3\nGitHub项目地址: https://github.com/shogun-toolbox/shogun\n22. HLearn\nHLearn是由Haskell语言编写的高性能机器学习库，目前它对任意维度空间有着最快最近邻的实现算法。\nHLearn同样也是一个研究型项目。该项目的研究目标是为机器学习发掘“最佳可能”的接口。这就涉及到了两个相互冲突的要求：该库应该像由C/C++/Fortran/Assembly开发的底层库那样运行快速；同时也应该像由Python/R/Matlab开发的高级库那样灵活多变。Julia在这个方向上取得了惊人的进步，但是 HLearn“野心”更大。更值得注意的是，HLearn的目标是比低级语言速度更快，比高级语言更加灵活。\n为了实现这一目标，HLearn采用了与标准学习库完全不同的接口。在HLearn中H代表着三个不同的概念，这三个概念也是HLearn设计的基本要求：\nH代表Haskell。机器学习是从数据中预测函数，所以功能性编程语言适应机器学习是完全说的通的。但功能性编程语言并没广泛应用于机器学习，这是因为它们固来缺乏支持学习算法的快速数值计算能力。HLearn通过采用Haskell中的SubHask库获得了快速数值计算能力；\nH同时代表着Homomorphisms。Homomorphisms是抽象代数的基本概念，HLearn将该代数结构用于学习系统中；\nH还代表着History monad。在开发新的学习算法过程中，最为困难的任务之一就是调试优化过程。在此之前，是没有办法减轻调试过程的工作量的，但History monad正在试图解决该问题。它可以让你在整个线程优化代码的过程中无需修改原代码。此外，使用该技术时没有增加其他的运行开销。\n开发语言：Haskell\nGitHub项目地址:https://github.com/mikeizbicki/HLearn\n23. MLPNeuralNet\nMLPNeuralNet是一个针对iOS和Mac OS系统的快速多层感知神经网络库，可通过已训练的神经网络预测新实例。它利用了向量运算和硬盘加速功能（如果可用），其建立在苹果公司的加速框架之上。\n若你已经用Matlab（Python或R）设计了一个预测模型，并希望在iOS应用程序加以应用。在这种情况下，正好需要MLP NeuralNet，而MLP NeuralNet只能加载和运行前向传播方式的模型。MLP NeuralNet 有如下几个特点：\n分类、多类分类以及回归输出；\n向量化实现形式；\n双精度；\n多重隐含层数或空（此时相当于逻辑学/线性回归）。\n开发语言：Objective-C\n许可协议：BSD license\nGitHub项目地址: https://github.com/nikolaypavlov/MLPNeuralNet\n24. Apache Mahout\nMahout 是Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。Apache Mahout项目的目标是建立一个能够快速创建可扩展、高性能机器学习应用的环境。\n虽然在开源领域中相对较为年轻，但 Mahout 已经提供了大量功能，特别是在集群和 CF 方面。Mahout 的主要特性包括：\nTaste CF，Taste是Sean Owen在SourceForge上发起的一个针对CF的开源项目，并在2008年被赠予Mahout；\n一些支持 Map-Reduce 的集群实现包括 k-Means、模糊 k-Means、Canopy、Dirichlet 和 Mean-Shift；\nDistributed Naive Bayes 和 Complementary Naive Bayes 分类实现；\n针对进化编程的分布式适用性功能；\nMatrix 和矢量库。\n使用 Mahout 还可实现内容分类。Mahout 目前支持两种根据贝氏统计来实现内容分类的方法：第一种方法是使用简单的支持 Map-Reduce 的 Naive Bayes 分类器；第二种方法是 Complementary Naive Bayes，它会尝试纠正Naive Bayes方法中的一些问题，同时仍然能够维持简单性和速度。\n开发语言：Java\n许可协议：Apache\nGitHub项目地址: https://github.com/apache/mahout\n25. Seldon Server\nSeldon是一个开放式的预测平台，提供内容建议和一般的功能性预测。它在Kubernetes集群内运行，因此可以调配到Kubernetes范围内的任一地址：内部部署或云部署（例如，AWS、谷歌云平台、Azure）。另外，它还可以衡量大型企业安装的需求。\n开发语言：Java\nGitHub项目地址: https://github.com/SeldonIO/seldon-server\n26. Datumbox - Framework\nDatumbox机器学习框架是用Java编写的一个开源框架，该框架的涵盖大量的机器学习算法和统计方法，并能够处理大尺寸的数据集。\nDatumbox API提供了海量的分类器和自然语言处理服务，能够被应用在很多领域的应用，包括了情感分析、话题分类、语言检测、主观分析、垃圾邮件检测、阅读评估、关键词和文本提取等等。目前，Datumbox所有的机器学习服务都能够通过API获取，该框架能够让用户迅速地开发自己的智能应用。目前，基于GPL3.0的Datumbox机器学习框架已经开源并且可以从GitHub上进行下载。\nDatumbox的机器学习平台很大程度上已经能够取代普通的智能应用。它具有如下几个显著的优点：\n强大并且开源。Datumbox API使用了强大的开源机器学习框架Datumbox，使用其高度精确的算法能够迅速地构建创新的应用；\n易于使用。平台API十分易于使用，它使用了REST&JSON的技术，对于所有的分类器；\n迅速使用。Datumbox去掉了那些很花时间的复杂机器学习训练模型。用户能够通过平台直接使用分类器。\nDatumbox主要可以应用在四个方面：一个是社交媒体的监视，评估用户观点能够通过机器学习解决，Datumbox能够帮助用户构建自己的社交媒体监视工具；第二是搜索引擎优化，其中非常有效的方法就是文档中重要术语的定位和优化；第三点是质量评估，在在线通讯中，评估用户产生内容的质量对于去除垃圾邮件是非常重要的，Datumbox能够自动的评分并且审核这些内容；最后是文本分析，自然语言处理和文本分析工具推动了网上大量应用的产生，平台API能够很轻松地帮助用户进行这些分析。\n开发语言：Java\n许可协议：Apache License 2.0\nGitHub项目地址: https://github.com/datumbox/datumbox-framework\n27. Jubatus\nJubatus库是一个运行在分布式环境中的在线机器学习框架，即面向大数据数据流的开源框架。它和Storm有些类似，但能够提供更多的功能，主要功能如下：\n在线机器学习库：包括分类、聚合和推荐；\nFv_converter: 数据预处理（用自然语言）；\n在线机器学习框架，支持容错。\nJubatus认为未来的数据分析平台应该同时向三个方向展开：处理更大的数据，深层次的分析和实时处理。于是Jubatus将在线机器学习，分布式计算和随机算法等的优势结合在一起用于机器学习，并支持分类、回归、推荐等基本元素。根据其设计目的，Jubatus有如下的特点：\n可扩展：支持可扩展的机器学习处理。在普通硬件集群上处理数据速度高达100000条/秒； ＋实时计算：实时分析数据和更新模型；\n深层次的数据分析：支持各种分析计算：分类、回归、统计、推荐等。\n如果有基于流数据的机器学习方面的需求，Jubatus值得关注。\n开发语言：C/C++\n许可协议：LGPL\nGitHub项目地址: https://github.com/jubatus/jubatus\n28. Decider\nDecider 是另一个 Ruby 机器学习库，兼具灵活性和可扩展性。Decider内置了对纯文本和URI、填充词汇、停止词删除、字格等的支持，以上这些都可以很容易地在选项中组合。Decider 可支持Ruby中任何可用的存储机制。如果你喜欢，可以保存到数据库中，实现分布式分类。\nDecider有几个基准，也兼作集成测试。这些都是定期运行并用于查明CPU和RAM的瓶颈。Decider可以进行大量数学运算，计算相当密集，所以对速度的要求比较高。这是经常使用Ruby1.9和JRuby测试其计算速度。此外，用户的数据集应该完全在内存中，否则将会遇到麻烦。\n开发语言：Ruby\nGitHub项目地址: https://github.com/danielsdeleo/Decider\n本文永久更新链接地址：http://www.linuxidc.com/Linux/2016-04/130424.htm"}
{"content2":"人工智能和机器学习领域有哪些有趣的开源项目？\n投递人 itwriter 发布于 2014-12-02 11:21 评论(0) 有20人阅读  原文链接  [收藏]  «\n本文简要介绍了 10 款   Quora 上网友推荐的 人工智能和机器学习领域方面的开源项目。\nGraphLab\nGraphLab 是一种新的面向机器学习的并行框架。GraphLab 提供了一个完整的平台，让机构可以使用可扩展的机器学习系统建立大数据以分析产品，该\n公司客户包括 Zillow、Adobe、Zynga、Pandora、Bosch、ExxonMobil 等，它们从别的应用程序或者服务中抓取数据，通过推荐系统、欺诈监测系统、情感\n及社交网络分析系统等系统模式将大数据理念转换为生产环境下可以使用的预测应用程序。（ 详情）\n项目主页： http://graphlab.org/\nVowpal Wabbit\nVowpal Wabbit (Fast Online Learning)最初是由雅虎研究院建设的一个机器学习平台，目前该项目在微软研究院。它是由 John Langford 启动并主\n导的项目。\n项目地址：  http://hunch.net/~vw/\nscikits.learn\nscikit-learn 是一个开源的、构建在 SciPy 之上用于机器学习的 Python 模块。它包括简单而高效的工具，可用于数据挖掘和数据分析，适合于任何\n人，可在各种情况下重复使用、构建在 NumPy、SciPy 和 matplotlib 之上，遵循 BSD 协议。（详情）\n项目地址： http://scikit-learn.org/stable\nTheano\nTheano 是一个 python 库，用来定义、优化和模拟数学表达式计算，用于高效的解决多维数组的计算问题。它使得写深度学习模型更加容易，同时也\n给出了一些关于在 GPU 上训练它们的选项。（ 详情）\n项目地址： http://deeplearning.net/software/theano/\nMahout\nMahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便\n快捷地创建智能应用程序。Mahout 包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地\n扩展到云中。\n项目主页： http://mahout.apache.org/\npybrain\npybrain 是 Python 的一个机器学习模块，它的目标是为机器学习任务提供灵活、易应、强大的机器学习算法。pybrain 包括神经网络、强化学习(及\n二者结合)、无监督学习、进化算法。以神经网络为核心，所有的训练方法都以神经网络为一个实例。\n项目主页： http://pybrain.org/\nOpenCV\nOpenCV 是一个基于（开源）发行的跨平台计算机视觉库，可以运行在 Linux、Windows 和 Mac OS 操作系统上。它轻量级而且高效——由一系列 C 函\n数和少量 C++ 类构成，同时提供了 Python、Ruby、MATLAB 等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。（ 详情）\n项目主页： http://opencv.org/\nOrange\nOrange 是一个基于组件的数据挖掘和机器学习软件套装，它的功能即友好，又很强大，快速而又多功能的可视化编程前端，以便浏览数据分析和可视\n化，基绑定了 Python 以进行脚本开发。它包含了完整的一系列的组件以进行数据预处理，并提供了数据帐目，过渡，建模，模式评估和勘探的功能。\n项目主页： http://orange.biolab.si/\nNLTK\nNLTK（natural language toolkit)是 python 的自然语言处理工具包。2001 年推出，至今发展非常活跃。它的主要作用是为了教学，至今已经在 20\n多个国家 60 多所高校使用，里面包括了大量的词料库，以及自然语言处理方面的算法实现：分词， 词根计算， 分类， 语义分析等。\n项目主页： http://nltk.org/\nNupic\nNupic 是一个开源的人工智能平台。该项目由 Grok（原名 Numenta）公司开发，其中包括了公司的算法和软件架构。 NuPIC 的运作接近于人脑，“当\n模式变化的时候，它会忘掉旧模式，记忆新模式”。如人脑一样，CLA 算法能够适应新的变化。（ 详情）\n项目主页： http://numenta.org/nupic.html\n--------------------------------------------------------------------------------\n以上是小编整理的 10 款人工智能和机器学习领域的开源项目。更多项目可参看这个列表：http://deeplearning.net/software_links/，或查看\nQuora 中更多网友的精彩回答。\nC++在“商业应用”方面，曾经是天下第一的开发语言，但这一桂冠已经被java抢走多年。因为当今商业应用程序类型，已经从桌面应用迅速转移成\nWeb应用。当Java横行天下之后，MS又突然发力，搞出C#语言，有大片的曾经的C++程序员，以为C++要就此沉沦，未料，这三年来，C++的生命力突然被严重\n地增强了。主力原因就是开源的软件、基础软件（比如并发原生支持，比如Android必定要推出原生的SDK）、各种跨平台应用的出现。\n开源C++库必须具有以下特点：必须是成熟的产品、跨平台的产品、相对通用的库。\n一、通用标准类\nSTL：C++标准模板库，也是开源的。\nboost：C++准标准库，它是强大地，江湖称之“千锤百炼”。\n-------若掌握，必横行世界。\ndeelx （轻量级的正则表达式解析类库，国产），boost里有强大的正则表达式解析库，但如果你只想要一个表达式解析，不想要拖上庞大的boost\n库时……支持一下国货。\niconv /iconvpp ： (C形式的编码转换函数库，\n二、XML解析库\nC++的XML相关库不少，但是大部分其实都是C库，使用起来自然不那么轻便。其中基于DOM的有TinyXml，基于SAX的当然是Xerces。前者小巧快捷\n，便于使用，适合做数据交换。后者则是全功能的XML解析器。\nxerces-c ：最强大的XML解析库了，不是仅仅在开源库里，你尽管把商业的算在内。当然，它的变体，被IBM拿去卖钱的那个版本，多了数百兆的\n东东来支持各国编码转换，是更强大，但我觉得有小小的，开源的iconv在前不就够了？\n对三个轻型xml解析开源库：SlimXml、TinyXml、RapidXml，对比如下：\n解析这个3.3万行，1.5M大小的xml，三个库分别花了\n· SlimXml: 22ms\n· TinyXml: 54ms\n· RapidXml: 4ms!\n结论是，RapidXml果然很强悍，居然比SlimXml快5倍多。\n比较欣慰的是，在没有很关注效率的情况下，SlimXml仍然比TinyXml快 2.5倍。SlimXml走的是简单小巧路线，源代码只有 32k，而TinyXml和\nRapidXml的源码分别是147k和141k，有这样的效率可以满意了。因为这个库主要还是针对几十上百行的小文件，解析特别大的xml不在我考虑的范围之内。\n还有irrlicht（鬼火引擎）的irrXMl解析器。\nxsd (XML 与 C++数据结构的绑定工具)。\n三、数据库\n我比较喜欢OTL（用于连接数据库）。其他的没用过\n四、多媒体类\nSDL (Simple DirectMedia Layer/多媒体直接访问层，用于游戏编程)。\n相应的c开源库有ffmpeg、mpeg4、aac、avc、libmad、mpeg1、flac、ac3、ac3、matroska著名的多媒体播放器 TCPMP 天下闻名的跨平台、嵌入式\n手持设备视频播放器，\n五、网络开发类\n1、gSOAP SOAP协议的C++支持库及代码生成工具。\n2、ACE  网络编程研究首选。\nACE适合于研究，大型网络编程上效率不足，大型网游里面几乎没有用到ACE的，很多用了ACE的项目也被证明了效率不高。\n除了ACE之外，还有很多系统和网络编程方面的程序库。比如在线程库方面，还有ZThread、boost::thread，如果放大到C/C++ 领域， 还有APR，还\n有CII。在文件和目录操作方面，boost也有相应的组件，而在网络编程方面有socket++，还有boost::asio，未来的C++0X中几乎肯定有一个网络编程和一个\n线程库。然而目前看来，ACE仍然是进行系统和高性能网络编程的首选，其地位在一段时间内不会被撼动。它不但是 一个实用的程序库、框架集，还是一个\n典范的设计模式应用范例，非常值得学习。\n3、开源C++库，称为POCO（POrtable COmponents – 可移植元件），非常方便好用。\n特性：\n* 线程，程序同步及多线程编程高级抽象\n* 流及文件系统访问\n* 共享库将类加载\n* 功能强大的日志和错误报告\n* 安全及加密\n* 网络编程 (TCP/IP 套接字, HTTP客户端和HTTP服务器, FTP, SMTP, POP3, 等)\n* XML解析 (SAX2 和 DOM) 及生成\n* 配置文件及选项处理\n* SQL数据库访问(ODBC, MySQL, SQLite)\n可以运行的平台包括：\n* Windows\n* Mac OS X\n* iPhone OS\n* (embedded) Linux\n* HP-UX\n* Tru64\n* Solaris\n* QNX\n六、GUI库\nBCG Windows平台下界面设计的第三方库，可以让你的界面更美好，更具时 代感。\nwxWidgets ：使用wxWidgets ，开发者可以基于同一套代码，为Win32, Mac OS X, GTK+, X11, Motif, WinCE等平台开发应用程序。wxWidgets库可\n以被C++, Python, Perl, and C#/.NET等开发语言使用。跟其它有些同样支持跨平台GUI开发工具不同，基于wxWidgets的应用，拥有真实本地化的视觉及使\n用效果——因 为，wxWidgets使用（各）平台原生的控件，而不是简单通过贴图去模拟。wxWidgets是使用广泛的，自由的，开源的，成熟的。\nQT-------------界面（GUI）开发，支持C++/Java/Python/...多种语言。跨平台。最主要的好处是，API非常优 美！Qt本身也不仅仅只是做GUI编\n程，实际它基本上可以做OS-API可以做的任何事情。象网络/数据库/OpenGL/...都提供完美的支持。\n传统上Qt被认为是可移植的GUI库，但实际上Qt现在已经是一个比较完整的可移植应用程序框架了，其中包含了大量的工具，比如正则表达式、\nWeb和 Socket类、2D和3D图形、XML解析、SQL类等，甚至还包括了一个完整的容器类库，不过其王牌还是GUI。在目前的跨平台GUI框架中，Qt成 熟度最高\n，已经被一些大公司应用在关键产品中。由于Trolltech对Qt采用的dual license模式，该产品既可以从开源社区获得支持，又能够赚取足够的商业利润，\n因此其前景也令人比较有信心。\nQt的主要技术特色是其元对象模型。Qt实际上使用的并不是标准的C++，而是标准C++的一个扩展。它通过元对象模型扩展，实现了著名的\nsignal/slot机制，而这一机制也成为Qt的最大特色和优势。\n与Qt类似的可移植GUI框架还有wxWidget、FOX等\n六. 计算机视觉\nOpenCV，因特尔自主的开源库。支持C/C++/Python接口。这个感兴趣的朋友可以玩一下。如果结合OpenCV，你可以做一些外行人觉得很酷的程序。比如\n说用它的人脸识别函数，来对你的摄像头进行处理，判断人的动作等。\n七. 图形图像处理\nGDAL，处理大图像。要是GIS专业的人肯定会语言到非常大的tif影像，动则几个GB的航空影像。GDAL对大图像的读写支持是非常棒的（像多波段的图像都可\n以搞定）。支持C++/Java/Python...\n国外开源的GIS软件QGIS就是用了gdal\nc的图形图像库较多，libjpeg、libpng、zlib、tiff、JBIG、最著名的开源形图像处理软件Cximage\n八、内存管理：boost::smart_ptr，Hans-Boehm GC\nC/C++的内存管理是一个永恒的话题。一般来说，C++的开发者倾向于自己管理内存。然而，出乎很多C++开发者意料的是，近期C++的一些领袖人物\n已 经公开宣称，如果不配备自动内存管理机制，用C++编写安全可靠的大型程序是非常困难的。     而Bjarne Stroustrup也曾对中国开发者建议，如果没\n有特别的理由，应该在大型项目中使用自动内存管理工具。因此，今天的C++开发者应当积极地学习和应用 自动内存管理设施。\n说到自动内存管理，比较轻量级的做法是boost::smart_ptr，而激进的做法是引入完整的GC机制。目前开源而又比较可靠的GC中，Hans- Boehm GC\n无疑是最受信赖的。作为一个保守的GC，Hans-Boehm GC在性能和功能方面都算是卓越。特别是，使用这个GC，你仍然可以delete、free来自己管理内存，\n对于我们编程习惯的冲击比较小。\n九、密码及安全：OpenSSL\n安全是今天进行C/C++编程无法回避和必须重视的问题。然而编写安全的应用程序，特别是跟网络相关的C/C++应用程序，是一件十分困难的事情。\n特别是涉及到大量的安全、密码学相关的算法、规范，如果让开发者自己摸索，其工作量和难度达到了不现实的程度。因此必须借助可靠的相关程序库才有\n可能提高程序的安全性。借助第三方安全保护平台，给程序做加密保护。在安全库这方面，OpenSSL是目前最好的选择，其内容之全面可靠，已经成为业界\n标杆。在应用程序加密保护这块，爱加密是非常专业的。然而，由于安全编程固有的复杂性，即使使用penSSL，开发工作仍然是非常繁琐的。因此我们也希\n望能够尽快看到更简单、更易用的C/C++安全程序库。\n十、矩阵计算：MTL\n自1995年以来，C++在科学计算领域当中取得了巨大的突破。这主要归功于template技术的高级应用，使得C++在科学计算的性能方面取得了巨大\n的进步，一大批优秀的C++科学计算库涌现出来。比如Blitz++、POOMA、MTL、Boost::uBLAS。而这其中，MTL就功能丰富程度、 性能、开发支持和成熟程度\n来讲，是比较突出的一个，因此可以优先考虑。值得一提的是，2002年，MTL与后来被Intel收购的KAI C++配合，曾经在性能评测中击败了FORTRAN。\n十一、中间件\n1、分布式对象中间件：ICE\nICE是分布式对象中间件领域里的后起之秀，可以大致地将其视为“改进版”的CORBA。目前应用在一些大型项目当中，其中包括波音公司主持的下\n一代陆军作战系统。\nICE的一个特别价值是其代码的范例意义。由于ICE的出现较晚，开发者比较系统地应用了新的C++编程风格，所以成为了研读C++代码的良好目标。\n2、消息中间件：ZeroMQ，总结的几种特性如下：\n1） 消息系统中，它差不多是最简洁的，只是个简洁的API，有n多种语言的绑定，没有专门的服务器；\n2） 性能非常优越，远远高于RabbitMQ、ActiveMQ、MSMQ等；\n3） 适合做分布式和并发应用。\n十二、正则表达式：boost::regex\n正则表达式是编程工作中最强有力的工具之一。C++的正则表达式支持一直以来是一个软肋。大约在2001年左右，boost中出现了regex库，初步解\n决了这个问题。但是最初的regex无论在效率上还是可靠性方面都有一些问题，后来经过一次大规模的翻修之后，达到了比较完善的程度。其他可以选择的\n替代品还有C语言的pcre库，Qt中的QRegExp类等。\n十三、配置管理：Lua\n随着软件系统越来越复杂，对软件的可配置型提出了越来越高的要求。传统上只要通过命令行参数来配置的系统，现在可能需要越来越多的方式和\n机制。目前越来越 受欢迎、并且得到越来越多证实的做法，是将Lua嵌入到C/C++程序中，而用Lua程序作为配置脚本。这种做法的优势是，Lua语言强大灵\n活，可以适应 复杂的配置要求。同时，Lua便于嵌入C/C++程序，而且编译执行速度非常快，可以说是目前解决C/C++程序配置管理问题的一个出色方案。\n十四、3D游戏引擎：\n1. Irrlicht http://irrlicht.sourceforge.net/\n始于2003，次年即被评为最佳开源游戏引擎。官方支持C++和.Net，拓展语言绑定包含java,perl,ruby,python.跨平台支持，使用D3D,OpenGL以及自\n带API.\n优点：容易上手；跨平台；自带XML解析器；大的社区；\n缺点：最近开发慢下来了\n2. Panda3D http://www.panda3d.org/\n由迪斯尼开发，卡耐基－梅隆娱娱乐科技中心支持。Python是官方推荐语言。也支持C++。\n优点：有用的社区；大量功能；定期开发；\n缺点：缺乏工具支持；极差的文档；\n3. OGRE http://www.ogre3d.org/\n图像引擎中最好的一个。2000年立的项。推荐使用C++语言。需要非常熟悉编程才行。初学者不宜。\n优点：大量功能；优异的文档；大规模的社区；活跃的开发\n缺点：不适合初哥；只有图像引擎\n4. Crystal Space http://www.crystalspace3d.org/main/Main_Page\n1997年发行，用C++编写的开源游戏引擎。推荐使用C++\n优点：不错的社区支持；大量功能；\n缺点：难学；\n5、Delta3d  ?http://www.delta3d.org/index.php\nDelta3D是一款由美国海军研究学院(Naval Postgraduate School)开发的全功能游戏与仿真引擎，得到美国军方巨大的支持与丰厚的投资。该引\n擎应用领域极为广泛，如开发在培训、教育、娱乐行业和科学计算可视化领域等方面建模与仿真的软件。\n它的标准化设计把一些知名开源软件和引擎         如 Open Scene Graph(OSG), OpenDynamicsEngine (ODE), Character Animation Library\n(CAL3D), 还有 OpenAL融为一体。Delta3D通过对这些底层模块进行隐藏封装，整合在一起从而形成了一个使用更加方便的高级API 函数库，使得开发者在\n必要的时候能够使用底层函数进行二次开发。Delta3D在软件系列中，处于中间层(Middle layer)的位置上。\n优点：适合各种3D游戏，仿真，很全面。一直在做更新。\n缺点：参考资料比较少。中文文档也比较少。官方参考资料比较少。但是读源代码可以加快理解，应用。"}
{"content2":"转自：https://linux.cn/article-8582-1.html\n编译自：https://opensource.com/article/17/5/python-machine-learning-introduction 作者： Michael J. Garbade\n原创：LCTT https://linux.cn/article-8582-1.html 译者： ucasFL\n本文地址：https://linux.cn/article-8582-1.html\n2017-06-07 09:12    收藏: 1\n本文导航\n-提高你的 Python 技能21%\n-安装 Anaconda30%\n-基本的机器学习技能34%\n-学习更多的 Python 库48%\n-探索机器学习66%\n机器学习是你的简历中必需的一门技能。我们简要概括一下使用 Python 来进行机器学习的一些步骤。\n你想知道如何开始机器学习吗？在这篇文章中，我将简要概括一下使用 Python 来开始机器学习的一些步骤。Python 是一门流行的开源程序设计语言，也是在人工智能及其它相关科学领域中最常用的语言之一。机器学习简称 ML，是人工智能的一个分支，它是利用算法从数据中进行学习，然后作出预测。机器学习有助于帮助我们预测我们周围的世界。\n从无人驾驶汽车到股市预测，再到在线学习，机器学习通过预测来进行自我提高的方法几乎被用在了每一个领域。由于机器学习的实际运用，目前它已经成为就业市场上最有需求的技能之一。另外，使用 Python 来开始机器学习很简单，因为有大量的在线资源，以及许多可用的 Python 机器学习库。\n你需要如何开始使用 Python 进行机器学习呢？让我们来总结一下这个过程。\n提高你的 Python 技能\n由于 Python 在工业界和科学界都非常受欢迎，因此你不难找到 Python 的学习资源。如果你是一个从未接触过 Python 的新手，你可以利用在线资源，比如课程、书籍和视频来学习 Python。比如下面列举的一些资源：\nPython 学习之路\nGoogle 开发者 Python 课程（视频）\nGoogle 的 Python 课堂\n安装 Anaconda\n下一步是安装 Anacona。有了 Anaconda ，你将可以开始使用 Python 来探索机器学习的世界了。Anaconda 的默认安装库包含了进行机器学习所需要的工具。\n基本的机器学习技能\n有了一些基本的 Python 编程技能，你就可以开始学习一些基本的机器学习技能了。一个实用的学习方法是学到一定技能便开始进行练习。然而，如果你想深入学习这个领域，那么你需要准备投入更多的学习时间。\n一个获取技能的有效方法是在线课程。吴恩达的 Coursera 机器学习课程 是一个不错的选择。其它有用的在线训练包括：\nPython 机器学习： Scikit-Learn 教程\nPython 实用机器学习教程\n你也可以在 LiveEdu.tv 上观看机器学习视频，从而进一步了解这个领域。\n学习更多的 Python 库\n当你对 Python 和机器学习有一个好的感觉之后，可以开始学习一些开源的 Python 库。科学的 Python 库将会使完成一些简单的机器学习任务变得很简单。然而，选择什么库是完全主观的，并且在业界内许多人有很大的争论。\n一些实用的 Python 库包括：\nScikit-learn ：一个优雅的机器学习算法库，可用于数据挖掘和数据分析任务。\nTensorflow ：一个易于使用的神经网络库。\nTheano ： 一个强大的机器学习库，可以帮助你轻松的评估数学表达式。\nPattern ： 可以帮助你进行自然语言处理、数据挖掘以及更多的工作。\nNilearn ：基于 Scikit-learn，它可以帮助你进行简单快速的统计学习。\n探索机器学习\n对基本的 Python、机器学习技能和 Python 库有了一定理解之后，就可以开始探索机器学习了。接下来，尝试探索一下 Scikit-learn 库。一个不错的教程是 Jake VanderPlas 写的 Scikit-learn 简介。\n然后，进入中级主题，比如 K-均值聚类算法简介、线性回归、决策树和逻辑回归。\n最后，深入高级机器学习主题，比如向量机和复杂数据转换。\n就像学习任何新技能一样，练习得越多，就会学得越好。你可以通过练习不同的算法，使用不同的数据集来更好的理解机器学习，并提高解决问题的整体能力。\n使用 Python 进行机器学习是对你的技能的一个很好的补充，并且有大量免费和低成本的在线资源可以帮助你。你已经掌握机器学习技能了吗？可以在下面留下你的评论，或者提交一篇文章来分享你的故事。\n（题图：opensource.com）\n作者简介：\nMichael J. Garbade 博士是旧金山 LiveEdu Inc（Livecoding.tv）的创始人兼首席执行官。Livecoding.tv 是世界上观看工程师直播编代码最先进的直播平台。你可以通过观看工程师们写网站、移动应用和游戏，来将你的技能提升到一个新的水平。MichaelJ. Garbade 博士拥有金融学博士学位，并且是一名自学成才的工程师，他喜欢 Python、Django、Sencha Touch 和视频流。"}
{"content2":"课程主页是：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html﻿\n大家可以在上面看到课程的介绍，教师的信息，现在也可以下载到课件了。\n今年的《机器学习》课程有一些变化。\n首先，与之前纯粹来自美国著名大学的教师不同，今年讲课的老师与业界关系更加密切， 主讲余凯和张潼二位老师都是在业界从事科研多年的资深专家；凑巧的是他们现在又同时在百度工作，张潼现在是在百度做访问科学家吧，余凯现在更是百度的研发 副总裁了。其中一个小插曲就是余凯老师周五穿了百度的文化衫过来，虽然他一再解释穿衣的来由，但越解释却越感觉他这是在植入广告，呵呵... ﻿估计吧，这次听课套磁留学的是没有了，套磁去百度估计不少，呵呵... ﻿\n其实，虽然此次授课的主要是上面两位老师，但是参与授课答疑的应该说是一个教师团队，有清华的，计算所的，甚至微软的刘鉄岩都来了。 由于此次课程安排得很紧凑，基本就是每天上午的9点到下午2点，课间课后都没有答疑，明确的答疑时间是下午2点上完课以后，我估计大部分人也和我一样2点 听完就直接回去了。在最后一天的最后一节课，本来是安排summary和road ahead的，后来老师们临时改成了交流与答疑，虽然还是有些小缺陷(比如所有问题都是大家提前发到老师邮箱的，没有现场提问的环节)，但我依旧觉得交流 是这课程的最亮点了。\n然后呢，我自己也有的一些变化。10年的那次纯粹是打酱油，每天上下课都要坐很久的地铁，再加上下里巴人第一次去如此花花世界的大上海，虽然安奈住了浮躁的内心，但是现实社会的冲击依旧巨大。 ﻿10年的时候上课什么的，虽然事情准备了很多，比如课件基本都提前打印好了，PRML也看了一些，但是Eric和Feifei老师讲的很多还是不懂，这个估计也与自己上课老瞌睡有关 相相反，今年的课程就没怎么准备了，首先课件事先也没公布同事也没有参考书，就大概列了一下课程纲要；然后就是现在毕竟还在上班，能翘班一星期(照样有薪水哦，呵呵 ﻿)来上课就很感激经理们了，预习不光时间上做不到，就是心思上也做不到。虽然没准备吧，但是确实感觉能听懂比10年更多些内容了，一是估计这两年自己也进步了一点点吧，然后就是此次讲的更多的是与业界相关的东西，理论部分大都简要带过了。\n第一天介绍了Introduction to Machine Learning, Linear Model, Overfitting and Regularization 和 Linear Classification四部分内容。大部分内容都属于入门级的，如果看过PRML效果可能会更好。课上余凯老师提到人工智能核心部分“控制”和“感知”其实都是需要机器学习的。在提到机器学习 三要素Data, Model和Algorithm的时候，其实余凯老师说到他倒认为最重要的是需求，这可能与其在业界工作有关吧。线性模型本来说应该不是非常难理解，但是 张潼老师从统计的角度分析，说实话真没太听懂，特别是后面讲到对各个特征重要进行评估的时候用到了假设检验中的z-score和p-value，从这里以 后就没能跟上老师的步调了。后面的Overfitting and Regularization也是大部分从统计的角度讲解的，从训练集测试集的划分到bias variance之间的trade-off，提到使用regularization去避免overfitting(在我看来实质是避免模型过于复杂)。最 后，余凯老师把perceptron, SVM, Logistic Regression统一放到Linear Classification去讲解，这种分类确实不一样。但是这里的感知器应该只是指那种还不能解决XOR问题的单层神经网络吧。\n第二天讲的是Model Selection, Model Combination, Boosting and Bagging和Overview of Learing theory。这里的模型选择讲的是用一种什么标准去评价一个模型的好坏，我们用的标准基本还是经验风险和结构风险，所以课中重点介绍了Structural Risk Minimization。老师课件上还提到AIC,BIC,MDL等评价标准，以后再去看吧。模型的combination除了boosting还有加权结合啦，voting，stacking等，但研 究热点明显是boosting，ACL最新的文章都有好几篇。最后一节课是学习理论，这基本是我最不懂的地方了，记得研二开始看《机器学习》的时候就没 懂，到10年的机器学习还是没太弄懂，主要纠结的地方还是关于模型复杂度证明中用到了很多不等式。其实这部分对于做机器学习理论部分的是还真的是非常重要 的，比如SVM中的VC dimension理论。\n第三天就讲到Optimitization on Machine Learning, Online Learning, Sparisity Models和Basic Expansion and kernel method。优化算法是求解机器学习方法的重要算法，比如常用的梯度下降等，现在用的较多的是BFGS和LBFGS(我现在才知道BFGS原来是四个人名组成的)。讲到核函数的时候，余凯老师提到其实核函数就是一个平滑算子。其他几部分内容印象不是很深了。\n第四天的内容分别是Introduction to Graphical Models, Structural Learning, Learing on the web和Deep Learing and Feature learning models for vision。这次的图模型入门，老师介绍的都是比较浅显易懂内容，所以我大概都算是听懂了。老师说到图模型其实是一种表达问题的语言，不是一种具体的模型，其实我自己老是 把图模型与topic model特别是LDA联系在一起，今后还是要继续深入学习。Structural learing是由清华的朱军老师代上的，朱军老师基本总结了HMM，MEMM和CRF，并比较分析了CRF和H3N。MEMM的标注偏置还是一个很严重 的问题，但是我真没有理解。老师推荐一个资源Alchemy, Alchemy is a software package providing a series of algorithms for statistical relational learning and probabilistic logic inference, based on the Markov logic representation。 后面张潼老师介绍的learing on the web主要是ranking 和 classification，其他我更感兴趣的方面倒没有深入探讨，比如用于行为建模，社区分析等。最后是余凯老师介绍的最近非常非常火的deep learing，并介绍其在CV上的应用。deep learing 应用的比较成功的领域还是语音和图像，我感觉deep learing主要就是对特征空间按﻿照层次分层建模，进行深入挖掘图像和语音特征的意义。余凯老师说到目前最好的语言识别系统是微软用了9层特征空间，最好的图像识别系统是google做出来的，据说效果都提高了十多个百分点。 ﻿\n最后一天就三节课，分别是Transfer Learing and Semi-supervised learning， Recommendation system和简单介绍了下CV的learning。迁移学习倒是可以多讲一下，毕竟我们都认为人类对事物的知识是有迁移性的，但是实现起来又确实很难，所以目前大部分的系统都是通过share一些feature来实现transfer learning的。推荐系统倒是我和经理都很敢兴趣的领域，这里也算是入了下门吧。其实现在网络上有很多现成数据，做起来也很方便，我就试图跟着学习了一段时间mahout。最后一节余凯老师基本上扫过了下CV全部内容，没有太多理论没有太多公式，这对于最后一节课确实也挺合适。 ﻿\n主要的课程及内容就上面那么多，下面说几件小事吧。\n1. 有同学问到如何学习机器学习，张潼给的建议是去解决一个问题。找一个喜欢的感兴趣的问题，自己动手去解决，从采集数据，预处理，选择解决方法(模型)，编写程序等，我也相信这样一套流程下来，大概会对机器学习方法有个更感性的认识。余凯老师补充到，入门的书籍其实也很重要，他自己推荐的是Duda1973年写的那本《Pattern Classfication and Scene Analysis》，该书第二版就是大家常推荐的《模式分类》。最后一堂课每位老师也都推荐了一本书，其实这几本书我大概都扫过，班上有同学还下载了整理在新浪微博上，你可以在这里进行下载。 ﻿\n2. 这次能去听课真的要非常感谢国内美国的两位经理，真的非常感谢他们能这么信任我(其实,我这一年来的工作确实做得很差，我自己都不满意，更别说他们了)。不然要我请假不拿工资去听课，我真不一定能下得了这样的狠心，毕竟我现在真的非常需要这些钱。要知道以后读书的工资只能解决生存问题，可是我还是想要生活的呀。 ﻿\n3. 其实不想说会议组织的，但是还是要强调下，清华的老师和同学真的做了非常多的工作，非常感谢他们。虽然张长水教授一再说到清华的条件有限等等，其实是人都知道这是谦虚啦。不管其他人(特别是文科生)是怎样的观点(不明白我说的话可以看看许知远的书《那些忧伤的年轻人》应该就明白了，呵呵 ﻿)，对我这位呆头呆脑的理工科学生，清华之于我，就像阳光之于植物，奥运之于运动员，都只有敬仰之情。 ﻿\n4. 实验室很多师兄师姐说到他们把Eric Xing当做偶像，哈哈，我乘机得瑟了一番，因为我不仅见过他还有他的照片。其实吧，他更是我的偶像，冲着不是他的天才，而是他的努力。\n5. 谈论机器学习，谈论统计学习，就难免谈论两种哲学观点。\n总的来说，龙星计划机器学习课程，上次(10年上海)是纯粹打酱油，这次(12年北京)除了打酱油，还买了点盐"}
{"content2":"问答\nhttp://www.quora.com/What-is-data-science  数据科学是什么？\nhttp://www.quora.com/How-do-I-become-a-data-scientist  我怎样才能成为一个数据科学家？\nhttp://www.quora.com/Data-Science/How-does-data-science-differ-from-traditional-statistical-analysis  科学数据是如何从传统的统计分析不同吗？\n相关课程\nhttp://statistics.berkeley.edu/classes/s133/  计算数据概念，伯克利分校\nhttp://www.cs.berkeley.edu/~jordan/courses/294-fall09/  实用机器学习，伯克利分校\nhttp://inst.eecs.berkeley.edu/~cs188  人工智能伯克利分校\nhttp://vis.berkeley.edu/courses/cs294-10-sp11/wiki/index.php/CS294-10_Visualization  可视化，伯克利\nhttp://courses.ischool.berkeley.edu/i290-dma/s12/doku.php  数据挖掘和分析的智能商务服务，伯克利\nhttp://courses.ischool.berkeley.edu/i296a-dsa/s12  数据科学及分析：思想领袖，伯克利\nhttp://ml-class.org/  机器学习，斯坦福大学\nhttp://www-stat.stanford.edu/~naras/stat290  范式的计算数据，斯坦福大学\nhttp://www.stanford.edu/class/cs246/cs246-11-mmds  挖掘大型数据集，斯坦福大学\nhttps://graphics.stanford.edu/wikis/cs448b-10-fall  数据可视化，斯坦福大学\nhttp://www.stanford.edu/class/cs369m/  海量数据集分析，斯坦福大学的算法\nhttp://hci.stanford.edu/courses/cs448g/  交互式数据分析，斯坦福大学的研究主题\nhttp://www.stanford.edu/class/stats202/  数据挖掘，斯坦福大学\nhttp://www.cs.cmu.edu/~epxing/Class/10701/lecture.html  机器学习，债务工具中央结算系统\nhttp://www.stat.cmu.edu/~cshalizi/statcomp/  统计计算，债务工具中央结算系统\nhttp://malt.ml.cmu.edu/mw/index.php/Syllabus_for_Machine_Learning_with_Large_Datasets_10-605_in_Spring_2012  对于大型数据集的机器学习，债务工具中央结算系统\nhttp://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/index.htm  机器学习，麻省理工学院\nhttp://ocw.mit.edu/courses/sloan-school-of-management/15-062-data-mining-spring-2003/  数据挖掘，麻省理工学院\nhttp://www.mit.edu/~9.520/  统计学习理论及应用，MIT\nhttp://dataiap.github.com/dataiap/  数据素养，麻省理工学院\nhttps://wiki.engr.illinois.edu/display/cs412  数据挖掘，UIUC\nhttp://work.caltech.edu/telecourse.html  数据，加州理工学院学习\nhttp://itunes.apple.com/us/itunes-u/statistics-110-introduction/id495213607  统计简介，美国哈佛大学\nhttp://www.umiacs.umd.edu/~jimmylin/cloud-2010-Spring  数据密集的信息处理应用，马里兰大学\nhttp://www.cs.columbia.edu/~coms699812/  处理海量数据，哥伦比亚\nhttp://jakehofman.com/ddm/  数据驱动的建模，哥伦比亚\nhttp://www.cc.gatech.edu/~agray/4245fall10/  数据挖掘和分析，佐治亚理工学院\nhttp://www.cc.gatech.edu/~agray/6740fall09  计算数据分析：机器学习和大的基础，佐治亚理工学院\nhttp://had.co.nz/stat480/  爱荷华州立大学应用统计计算，\nhttp://had.co.nz/stat645/  数据可视化\nhttp://www.cs.nyu.edu/courses/spring08/G22.3033-003/index.html  数据仓库与数据挖掘，NYU\nhttp://chem-eng.utoronto.ca/~datamining/DataMiningCourse.htm  数据挖掘技术在工程，多伦多\nhttp://sli.ics.uci.edu/Classes/2011W-178  机器学习与数据挖掘，加州大学欧文分校\nhttp://users.csc.calpoly.edu/~dekhtyar/466-Fall2010/  数据的知识发现，卡尔波利\nhttp://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/  大型芝加哥大学学习，\nhttp://www.cise.ufl.edu/class/cis6930fa11lad/  数据科学：大型高级数据分析，美国佛罗里达大学\nhttp://uni-leipzig.de/~strimmer/lab/courses/ss09/current-topics/  统计数据分析的Universitat莱比锡的策略\n相关的研讨会\nhttp://strataconf.com/strata2011/public/schedule/detail/17164  数据训练营，地层2011\nhttp://learning.stat.purdue.edu/mlss/mlss/start  2011年，普渡大学的机器学习暑期学校\nhttp://lookingatdata.com/  了解数据\n书籍\nhttp://www.amazon.com/Competing-Analytics-New-Science-Winning/dp/1422103323/ 竞争分析\nhttp://www.amazon.com/Analytics-Work-Smarter-Decisions-Results/dp/1422177696 Google Analytics（分析）工作\nhttp://www.amazon.com/Super-Crunchers-Thinking-Numbers-Smart/dp/0553805401  超级统计员\nhttp://www.amazon.com/Numerati-Stephen-Baker/dp/0547247931/  The Numerati\nhttp://www.amazon.com/Data-Driven-Profiting-Important-Business/dp/1422119122/  数据驱动\nhttp://oreilly.com/catalog/0636920018254/  数据源手册\nhttp://oreilly.com/catalog/9780596529321/  集体智慧编程\nhttp://oreilly.com/catalog/0636920010203  挖掘社会网络\nhttp://oreilly.com/catalog/9780596802363/  数据分析与开放源码工具\nhttp://oreilly.com/catalog/9780596514556/  可视化数据\nhttp://www.edwardtufte.com/tufte/books_vdqi  定量信息的可视化显示\nhttp://www.edwardtufte.com/tufte/books_ei  展望信息\nhttp://www.edwardtufte.com/tufte/books_visex  视觉说明：图片和数量，证据和叙事\nhttp://www.edwardtufte.com/tufte/books_be  美丽的证据\nhttp://www.greenteapress.com/thinkstats/  思考统计\nhttp://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X  数据回归分析和多级/分层模型\nhttp://www.amazon.com/gp/product/0195152964/  应用纵向数据分析\nhttp://www.amazon.com/Design-Observational-Studies-Springer-Statistics/dp/1441912126/  观察性研究设计\nhttp://www.amazon.com/Statistical-Rules-Thumb-Probability-Statistics/dp/0470144483/统计拇指规则\nhttp://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/1441923225/统计\nhttp://www.amazon.com/Handbook-Statistical-Analyses-Using-Second/dp/1420079336/ 统计分析使用R手册\nhttp://www.amazon.com/Mathematical-Statistics-Analysis-Duxbury-Advanced/dp/0534399428/  数理统计与数据分析\nhttp://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/  统计学习的元素\nhttp://www.amazon.com/Counterfactuals-Causal-Inference-Principles-Analytical/dp/0521671930/  与事实相反的因果推理\nhttp://infolab.stanford.edu/~ullman/mmds/book.pdf  挖掘海量数据集\nhttp://www.amazon.com/Data-Analysis-Learned-Probability-Statistics/dp/1118010647 数据分析：从近50年来可以学到什么\nhttp://www.amazon.com/Bias-Causation-Comparisons-Probability-Statistics/dp/0470286393  偏见和因果关系\nhttp://www.amazon.com/Regression-Modeling-Strategies-Frank-Harrell/dp/0387952322 回归建模策略\nhttp://www.amazon.com/Probably-Not-Prediction-Probability-Statistical/dp/0470184019 可能不会\nhttp://www.amazon.com/Statistics-Principled-Argument-Robert-Abelson/dp/0805805281/  统计原则上的争论\nhttp://www.amazon.com/gp/product/0691057826/  数据分析的实践\n视频\nhttp://www.ted.com/talks/lies_damned_lies_and_statistics_about_tedtalks.html  谎言，该死的谎言和统计数据\nhttp://www.gapminder.org/videos/the-joy-of-stats/  喜的统计资料\nhttp://datajournalism.stanford.edu/  新闻中的年龄数据"}
{"content2":"算算时间，从开始到现在，做机器学习算法也将近八个月了。虽然还没有达到融会贯通的地步，但至少在熟悉了算法的流程后，我在算法的选择和创造能力上有了不小的提升。实话说，机器学习很难，非常难，要做到完全了解算法的流程、特点、实现方法，并在正确的数据面前选择正确的方法再进行优化得到最优效果，我觉得没有个八年十年的刻苦钻研是不可能的事情。其实整个人工智能范畴都属于科研难题，包括模式识别、机器学习、搜索、规划等问题，都是可以作为独立科目存在的。我不认为有谁可以把人工智能的各个方面都做到极致，但如果能掌握其中的任一方向，至少在目前的类人尖端领域，都是不小的成就。\n这篇日志，作为我2014年的学业总结，详细阐述目前我对机器学习的理解，希望各位看官批评指正，多多交流！\n机器学习（MachineLearning），在我看来就是让机器学习人思维的过程。机器学习的宗旨就是让机器学会“人识别事物的方法”，我们希望人从事物中了解到的东西和机器从事物中了解到的东西一样，这就是机器学习的过程。在机器学习中有一个很经典的问题：\n“假设有一张色彩丰富的油画，画中画了一片茂密的森林，在森林远处的一棵歪脖树上，有一只猴子坐在树上吃东西。如果我们让一个人找出猴子的位置，正常情况下不到一秒钟就可以指出猴子，甚至有的人第一眼就能看到那只猴子。”\n那么问题就来了，为什么人能在上千种颜色混合而成的图像中一下就能识别出猴子呢？在我们的生活中，各种事物随处可见，我们是如何识别出各种不同的内容呢？也许你可能想到了——经验。没错，就是经验。经验理论告诉我们认识的所有东西都是通过学习得到的。比如，提起猴子，我们脑海里立刻就会浮现出我们见过的各种猴子，只要画中的猴子的特征与我们意识中的猴子雷同，我们就可能会认定画中画的是猴子。极端情况下，当画中猴子的特征与我们所认识某一类猴子的特征完全相同，我们就会认定画中的猴子是哪一类。\n另一种情况是我们认错的时候。其实人识别事物的错误率有的时候也是很高的。比如，当我们遇见不认识的字的时候会潜意识的念字中我们认识的部分。比如，“如火如荼”这个词，是不是有朋友也跟我一样曾经念过“如火如茶（chá）”？我们之所以犯错，就是因为在我们没有见过这个字的前提下，我们会潜意识的使用经验来解释未知。\n目前科技如此发达，就有牛人考虑可不可以让机器模仿人的这种识别方法来达到机器识别的效果，机器学习也就应运而生了。\n从根本上说，识别，是一个分类的结果。看到四条腿的生物，我们可能会立即把该生物归为动物一类，因为我们常常见到的四条腿的、活的东西，九成以上是动物。这里，就牵扯出了概率的问题。我们对身边的事物往往识别率很高，是因为人的潜意识几乎记录了肉眼看到的事物的所有特征。比如，我们进入一个新的集体，刚开始大家都不认识，有的时候人和名字都对不上号，主要原因就是我们对事物的特征把握不够，还不能通过现有特征对身边的人进行分类。这个时候，我们常常会有这种意识：哎，你好像叫张三来着？哦，不对，你好像是李四。这就是分类中的概率问题，有可能是A结果，有可能是B结果，甚至是更多结果，主要原因就是我们的大脑收集的特征不够多，还无法进行准确分类。当大家都彼此熟悉了之后，一眼就能识别出谁是谁来，甚至极端情况下，只听声音不见人都能进行识别，这说明我们已经对该事物的特征把握相当精确。\n所以，我认为，人识别事物有四个基本步骤：学习、提取特征、识别、分类。\n那么机器可不可以模仿这个过程来实现识别呢？答案是肯定的，但是没有那么容易。难题有三：第一，人的大脑有无数神经元进行数据交换和处理，在目前的机器中还达不到同等的处理条件；第二，人对事物特征的提取是潜意识的，提取无意识情况下的信息，误差很大；第三，也是最重要的一点，人的经验来自于人每时每刻的生活中，也就是人无时无刻都处在学习中，如何让机器进行各个方面的自主学习？因此，目前在人工智能领域始终还没达到类人的水平，我认为主要原因就是机器没有潜意识。人的潜意识其实并不完全受人的意识支配，但却可以提高人类识别事物的概率。我们无法给机器加载潜意识，因为主动加载的意识就是主观意识，在机器里无法完成人类潜意识的功能。所以，以目前的发展情况来看，要达到完全类人，还有不短的时间。但即便如此，与人的思维差别很大的机器依然可以为我们的生活带来帮助。比如，我们常用的在线翻译、搜索系统、专家系统等，都是机器学习的产物。\n那么，如何实现机器学习呢？\n整体上看，机器学习就是模仿人识别事物的过程，即：学习、提取特征、识别、分类。由于机器不能跟人类思维一样根据事物特征自然而然的选择分类方法，所以机器学习方法的选择依然还需要人工选择。目前，机器学习的方法主要有三种：监督学习、半监督学习和无监督学习。监督学习是利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程。白话一点，就是根据已知的，推断未知的。代表方法有：Nave Bayes、SVM、决策树、KNN、神经网络以及Logistic分析等；半监督方法主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题，也就是根据少量已知的和大量未知的内容进行分类。代表方法有：最大期望、生成模型和图算法等。无监督学习是利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程。也就是及其自个儿学。代表方法有：Apriori、FP树、K-means以及目前比较火的Deep Learning。从这三方面看，无监督学习是最智能的，有能实现机器主动意识的潜质，但发展还比较缓慢；监督学习是不太靠谱的，从已知的推断未知的，就必须要把事物所有可能性全都学到，这在现实中是不可能的，人也做不到；半监督学习是“没办法中的办法”，既然无监督学习很难，监督学习不靠谱，就取个折中，各取所长。目前的发展是，监督学习技术已然成熟，无监督学习还在起步，所以对监督学习方法进行修改实现半监督学习是目前的主流。但这些方法基本只能提取信息，还不能进行有效的预测（人们就想，既然没法得到更多，就先看看手里有什么，于是数据挖掘出现了）。\n机器学习方法非常多，也很成熟。下面我挑几个说。\n首先是SVM。因为我做的文本处理比较多，所以比较熟悉SVM。SVM也叫支持向量机，其把数据映射到多维空间中以点的形式存在，然后找到能够分类的最优超平面，最后根据这个平面来分类。SVM能对训练集之外的数据做很好的预测、泛化错误率低、计算开销小、结果易解释，但其对参数调节和核函数的参数过于敏感。个人感觉SVM是二分类的最好的方法，但也仅限于二分类。如果要使用SVM进行多分类，也是在向量空间中实现多次二分类。\nSVM有一个核心函数SMO，也就是序列最小最优化算法。SMO基本是最快的二次规划优化算法，其核心就是找到最优参数α，计算超平面后进行分类。SMO方法可以将大优化问题分解为多个小优化问题求解，大大简化求解过程。\nSVM还有一个重要函数是核函数。核函数的主要作用是将数据从低位空间映射到高维空间。详细的内容我就不说了，因为内容实在太多了。总之，核函数可以很好的解决数据的非线性问题，而无需考虑映射过程。\n第二个是KNN。KNN将测试集的数据特征与训练集的数据进行特征比较，然后算法提取样本集中特征最近邻数据的分类标签，即KNN算法采用测量不同特征值之间的距离的方法进行分类。KNN的思路很简单，就是计算测试数据与类别中心的距离。KNN具有精度高、对异常值不敏感、无数据输入假定、简单有效的特点，但其缺点也很明显，计算复杂度太高。要分类一个数据，却要计算所有数据，这在大数据的环境下是很可怕的事情。而且，当类别存在范围重叠时，KNN分类的精度也不太高。所以，KNN比较适合小量数据且精度要求不高的数据。\nKNN有两个影响分类结果较大的函数，一个是数据归一化，一个是距离计算。如果数据不进行归一化，当多个特征的值域差别很大的时候，最终结果就会受到较大影响；第二个是距离计算。这应该算是KNN的核心了。目前用的最多的距离计算公式是欧几里得距离，也就是我们常用的向量距离计算方法。\n个人感觉，KNN最大的作用是可以随时间序列计算，即样本不能一次性获取只能随着时间一个一个得到的时候，KNN能发挥它的价值。至于其他的特点，它能做的，很多方法都能做；其他能做的它却做不了。\n第三个就是Naive Bayes了。Naive Bayes简称NB（牛X），为啥它牛X呢，因为它是基于Bayes概率的一种分类方法。贝叶斯方法可以追溯到几百年前，具有深厚的概率学基础，可信度非常高。Naive Baye中文名叫朴素贝叶斯，为啥叫“朴素”呢？因为其基于一个给定假设：给定目标值时属性之间相互条件独立。比如我说“我喜欢你”，该假设就会假定“我”、“喜欢”、“你”三者之间毫无关联。仔细想想，这几乎是不可能的。马克思告诉我们：事物之间是有联系的。同一个事物的属性之间就更有联系了。所以，单纯的使用NB算法效率并不高，大都是对该方法进行了一定的改进，以便适应数据的需求。\nNB算法在文本分类中用的非常多，因为文本类别主要取决于关键词，基于词频的文本分类正中NB的下怀。但由于前面提到的假设，该方法对中文的分类效果不好，因为中文顾左右而言他的情况太多，但对直来直去的老美的语言，效果良好。至于核心算法嘛，主要思想全在贝叶斯里面了，没啥可说的。\n第四个是回归。回归有很多，Logistic回归啊、岭回归啊什么的，根据不同的需求可以分出很多种。这里我主要说说Logistic回归。为啥呢？因为Logistic回归主要是用来分类的，而非预测。回归就是将一些数据点用一条直线对这些点进行拟合。而Logistic回归是指根据现有数据对分类边界线建立回归公式，以此进行分类。该方法计算代价不高，易于理解和实现，而且大部分时间用于训练，训练完成后分类很快；但它容易欠拟合，分类精度也不高。主要原因就是Logistic主要是线性拟合，但现实中很多事物都不满足线性的。即便有二次拟合、三次拟合等曲线拟合，也只能满足小部分数据，而无法适应绝大多数数据，所以回归方法本身就具有局限性。但为什么还要在这里提出来呢？因为回归方法虽然大多数都不合适，但一旦合适，效果就非常好。\nLogistic回归其实是基于一种曲线的，“线”这种连续的表示方法有一个很大的问题，就是在表示跳变数据时会产生“阶跃”的现象，说白了就是很难表示数据的突然转折。所以用Logistic回归必须使用一个称为“海维塞德阶跃函数”的Sigmoid函数来表示跳变。通过Sigmoid就可以得到分类的结果。\n为了优化Logistic回归参数，需要使用一种“梯度上升法”的优化方法。该方法的核心是，只要沿着函数的梯度方向搜寻，就可以找到函数的最佳参数。但该方法在每次更新回归系数时都需要遍历整个数据集，对于大数据效果还不理想。所以还需要一个“随机梯度上升算法”对其进行改进。该方法一次仅用一个样本点来更新回归系数，所以效率要高得多。\n第五个是决策树。据我了解，决策树是最简单，也是曾经最常用的分类方法了。决策树基于树理论实现数据分类，个人感觉就是数据结构中的B+树。决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。决策树计算复杂度不高、输出结果易于理解、对中间值缺失不敏感、可以处理不相关特征数据。其比KNN好的是可以了解数据的内在含义。但其缺点是容易产生过度匹配的问题，且构建很耗时。决策树还有一个问题就是，如果不绘制树结构，分类细节很难明白。所以，生成决策树，然后再绘制决策树，最后再分类，才能更好的了解数据的分类过程。\n决策树的核心树的分裂。到底该选择什么来决定树的分叉是决策树构建的基础。最好的方法是利用信息熵实现。熵这个概念很头疼，很容易让人迷糊，简单来说就是信息的复杂程度。信息越多，熵越高。所以决策树的核心是通过计算信息熵划分数据集。\n我还得说一个比较特殊的分类方法：AdaBoost。AdaBoost是boosting算法的代表分类器。boosting基于元算法（集成算法）。即考虑其他方法的结果作为参考意见，也就是对其他算法进行组合的一种方式。说白了，就是在一个数据集上的随机数据使用一个分类训练多次，每次对分类正确的数据赋权值较小，同时增大分类错误的数据的权重，如此反复迭代，直到达到所需的要求。AdaBoost泛化错误率低、易编码、可以应用在大部分分类器上、无参数调整，但对离群点敏感。该方法其实并不是一个独立的方法，而是必须基于元方法进行效率提升。个人认为，所谓的“AdaBoost是最好的分类方法”这句话是错误的，应该是“AdaBoost是比较好的优化方法”才对。\n好了，说了这么多了，我有点晕了，还有一些方法过几天再写。总的来说，机器学习方法是利用现有数据作为经验让机器学习，以便指导以后再次碰到的决策。目前来说，对于大数据分类，还是要借助分布式处理技术和云技术才有可能完成，但一旦训练成功，分类的效率还是很可观的，这就好比人年龄越大看待问题越精准的道理是一样的。这八个月里，从最初的理解到一步步实现；从需求的逻辑推断到实现的方法选择，每天都是辛苦的，但每天也都是紧张刺激的。我每天都在想学了这个以后可以实现什么样的分类，其实想想都是让人兴奋的。当初，我逃避做程序员，主要原因就是我不喜欢做已经知道结果的事情，因为那样的工作没有什么期盼感；而现在，我可以利用数据分析得到我想象不到的事情，这不仅满足了我的好奇感，也让我能在工作中乐在其中。也许，我距离社会的技术需求还有很远的距离，但我对自己充满信心，因为，我不感到枯燥，不感到彷徨，虽然有些力不从心，但态度坚定。\n2014的学习很艰难，我挺过来了；2015年，可能会更艰难，但我更加期待！\n最后，希望各位能人、牛人、同道中人给予点评，多多交流，一个人做算法是吃力的，希望各位踊跃评价，共同进步！"}
{"content2":"不多说，直接上干货！\n玩玩这个远程连接软件，是个绿色软件。\n别人已经做好了的。\n解压之后，\n下面，软件展示下，\n这会默认去打开，\n为了，方便，使用，放到桌面，作为快捷方式\n成功\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"1  简介\n决策树学习是一种逼近离散值目标函数的方法，在这种学习到的函数被表示为一棵决策树。\n2 决策树表示\n决策树通过把实例从根节点排列到某个叶子结点来分类实例，叶子结点即为实例所属的分类。树上的每一个结点指定了对实例的某个属性的测试，并且该结点的每一个后续分支对应于该属性的一个可能值。\n分类实例的方法是从这棵树的根节点开始，测试这个结点指定的属性，然后按照给定实例的该属性值对应的树枝向下移动。然后这个过程在以新结点为根的子树上重复。\n画出了一棵典型的学习到的决策树。\n\n这棵决策树根据天气情况分类“星期六上午是否适合打网球”。例如，下面的实例：\n<   Outlook=Sunny，Temperature= Hot，Humidity = High ，Wind= Strong  >\n将被沿着这棵决策树的最左分支向下排列，因而被评定为反例（也就是这棵树预测这个实例Play Tennis = No）。\n这棵树以及表 3-2 中用来演示 ID3 学习算法的例子摘自（Quinlan 1986）。\n通常决策树代表实例属性值约束的合取（conjunction ）的析取式（disjunction）。 从树根到树叶的每一条路径对应一组属性测试的合取，树本身对应这些合取的析取。\n例如，\n（Outlook=Sunny ٨ Humidity= Normal） ٧（Outlook=Overcast） ٧（Outlook=Rain ٨ Wind=Weak）\n3 决策树的ID3算法\n基本的ID3 算法通过自顶向下构造决策树来进行学习。\n构造过程是从“哪一个属性将在树的根结点被测试？”这个问题开始的。\n为了回答这个问题，使用统计测试来确定每一个实例属性单独分类训练样例的能力。\n（1）分类能力最好的属性被选作树的根结点的测试。\n（2）然后为根结点属性的每个可能值产生一个分支，并把训练样例排列到适当的分支（也就是，样例的该属性值对应的分支）之下。\n（3）然后重复整个过程，用每个分支结点关联的训练样例来选取在该点被测试的最佳属性。\n这形成了对合格决策树的贪婪搜索（greedy search ），也就是算法从不回溯重新考虑以前的选择。\n伪代码\n3.1 哪个属性是最佳的分类属性\nID3 算法的核心问题是选取在树的每个结点要测试的属性。\n我们希望选择的是最有助于分类实例的属性。那么衡量属性价值的一个好的定量标准是什么呢？\n这里将定义一个统计属性，称为“信息增益（information gain ）”，用来衡量给定的属性区分训练样例的能力。\nID3 算法在增长树的每一步使用这个信息增益标准从候选属性中选择属性。\n3.1.1用熵度量样例的均一性\n为了精确地定义信息增益，我们先定义信息论中广泛使用的一个度量标准，称为熵（entropy），它刻画了任意样例集的纯度（ purity）。\n给定包含关于某个目标概念的正反样例的样例集S ，那么S 相对这个布尔型分类的熵为：\nEntropy(S ) =-P⊕log2P⊕-PΘlog2PΘ\n其中P⊕ 是在S 中正例的比例，PΘ是在S 中负例的比例。在有关熵的所有计算中我们定义0 log0 为0 。\n举例说明，假设S 是一个关于某布尔概念的有 14 个样例的集合，它包括 9 个正例和5 个反例（我们采用记号[9+ ，5 -]来概括这样的数据样例）。\n那么S 相对于这个布尔分类的熵（Entropy）为：\nEntropy([9+,5-]) =-(9/14)log2(9/14) - (5/14)log2(5/14)\n至此我们讨论了目标分类是布尔型的情况下的熵。更一般的，如果目标属性具有 c个不同的值，那么S 相对于 c 个状态（c-wise ）的分类的熵定义为：\nEntropy(S )=Σ -Pi*log2Pi(1 ≤ i≤ c)\n3.1.2用信息增益度量期望的熵降低\n已经有了熵作为衡量训练样例集合纯度的标准，现在可以定义属性分类训练数据的效力的度量标准。这个标准被称为“信息增益（information gain ）”。\n简单的说，一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低。更精确地讲，一个属性A 相对样例集合 S 的信息增益Gain(S, A)被定义为：\n其中 Values (A)是属性A所有可能值的集合，SV 是S 中属性A的值为v 的子集（也就是，Sv ={s∈ S |A(s )=v } ）。\n请注意，等式的第一项就是原来集合S 的熵，第二项是用A分类S 后熵的期望值。\n这个第二项描述的期望熵就是每个子集的熵的加权和，权值|Sv|/|S|为属于S的样例占原始样例S 的比例 。\n所以Gain(S , A)是由于知道属性A的值而导致的期望熵减少。换句话来讲，Gain (S , A)是由于给定属性A的值而得到的关于目标函数值的信息。\n例如，假定 S 是一套有关天气的训练样例，描述它的属性包括可能是具有 Weak和Strong 两个值的 Wind。像前面一样，假定 S 包含14 个样例，[9+ ，5 -]。在这 14 个样例中，假定正例中的 6 个和反例中的2 个有Wind =Weak，其他的有Wind =Strong。由于按照属性Wind 分类14 个样例得到的信息增益可以计算如下\n实验举例：\n训练样本：\nD1 Sunny Hot High Weak No D2 Sunny Hot High Strong No D3 Overcast Hot High Weak Yes D4 Rain Mild High Weak Yes D5 Rain Cool Normal Weak Yes D6 Rain Cool Normal Strong No D7 Overcast Cool Normal Strong Yes D8 Sunny Mild High Weak No D9 Sunny Cool Normal Weak Yes D10 Rain Mild Normal Weak Yes D11 Sunny Mild Normal Strong Yes D12 Overcast Mild High Strong Yes D13 Overcast Hot Normal Weak Yes D14 Rain Mild High Strong No\n测试样本：\nD1 Sunny Hot High Weak D2 Sunny Hot High Strong D3 Overcast Hot High Weak D4 Rain Mild High Weak D5 Rain Cool Normal Weak D6 Rain Cool Normal Strong D7 Overcast Cool Normal Strong D8 Sunny Mild High Weak D9 Sunny Cool Normal Weak D10 Rain Mild Normal Weak D11 Sunny Mild Normal Strong D12 Overcast Mild High Strong D13 Overcast Hot Normal Weak D14 Rain Mild High Strong\n头文件\nhead.h #ifndef ID3_H_INCLUDED #define ID3_H_INCLUDED #include <map> #include <fstream> #include <vector> #include <set> #include <iostream> #include <cmath> using namespace std; const int DataRow=14; const int DataColumn=6; const int testRow=14; const int testColumn=5; struct Node{ double value;　　　　　　// 标签值，1为YES 0为No int attrid;　　　　　　　//属性标号 int attrvalue;　　　　　　//属性值 vector<Node*> childNode; }; #endif // ID3_H_INCLUDED\nC++代码\n#include \"id3.h\" string DataTable[DataRow][DataColumn]; //保存训练样例 string TestTable[testRow][DataColumn]; //保存测试样例 map<string ,int> string2int; set<int> S; set<int> Attributes; string attrName[DataColumn]= {\"Day\",\"OutLook\",\"Temperature\",\"Humidity\",\"Wind\",\"PlayTennis\"}; string attrValue[DataColumn][DataColumn]= { {}, {\"Sunny\",\"Overcast\",\"Rain\"},// sunny 1,overcast 2,rain3 {\"Hot\",\"Mild\",\"Cool\"},//hot 1,mild 2, cool 3 {\"High\",\"Normal\"},//High 1,normal 2 {\"Weak\",\"Strong\"}, // weak 1,strong 2 {\"Yes\",\"No\"} //yes 1,no 2 }; int attrCount[DataColumn]={14,3,3,2,2,2}; double lg2(double n) { return log(n)/log(2); } void Init() //初始化 { ifstream fin(\"dataset.txt\"); for(int i=0;i<DataRow;++i) { for(int j=0;j<DataColumn;++j) { fin>>DataTable[i][j]; } } fin.close(); for(int i=1;i<=DataColumn-1;++i) { string2int[attrName[i]]=i; for(int j=0;j<attrCount[i];++j) string2int[attrValue[i][j]]=j; } for(int i=0;i<DataRow;i++) S.insert(i); for(int i=1;i<=DataColumn-2;i++) Attributes.insert(i); } double Entropy(const set<int> &s) //计算熵值 { double yes=0,no=0; for(set<int>::size_type i=1;i<s.size();i++) { if(string2int[DataTable[i][DataColumn-1]]==0) yes++; else no++; } if(no==0||yes==0) return 0; double Py=yes/s.size(); double Pn=no/s.size(); double ans=-1*Py*lg2(Py)+-1*Pn*lg2(Pn); return ans; } double Gain(const set<int> &s,int attrid) //计算信息增益值 { double ans=0; int attrcount = attrCount[attrid]; double sumEntropy = Entropy(s); set<int> *pset = new set<int>[attrcount]; for(set<int>::const_iterator iter=s.begin();iter!=s.end();iter++) { pset[string2int[DataTable[*iter][attrid]]].insert(*iter); } for(int i=0;i<attrcount;i++) { ans-=(double)pset[i].size()/(double)s.size()*Entropy(pset[i]); } return sumEntropy-ans; } int FinderBestAttribute(const set<int> &s,const set<int> &attr)　　　　　　//找到最佳分类属性 { double maxg=0; int k=-1; for(set<int>::const_iterator iter=attr.begin();iter!=attr.end();++iter) { double tem=Gain(s,*iter); if(tem>maxg) { maxg=tem; k=*iter; } } int sum=s.size(); sum=attr.size(); if(k==-1) cout<<\"FinderBestAttribute Error!\"<<endl; return k; } Node * Id3_solution(set<int> s,set<int> attr) { Node * now = new Node(); now->value=-1; if(attr.empty()) return NULL; int yes=0,no=0,sum=s.size(); for(set<int>::iterator iter=s.begin();iter!=s.end();iter++) { if(DataTable[*iter][DataColumn-1]==\"Yes\") yes++; else no++; } if(yes==sum||no==sum) { now->value=yes/sum; return now; } int bestattrid = FinderBestAttribute(s,attr); //找到最佳的分类属性 now->attrid=bestattrid; attr.erase(attr.find(bestattrid)); vector<set<int> > child=vector<set<int> >(attrCount[bestattrid]); for(set<int>::iterator iter=s.begin();iter!=s.end();iter++) //插入孩子结点 { int id = string2int[DataTable[*iter][bestattrid]]; child[id].insert(*iter); } for(int i=0;i<child.size();i++) //对孩子结点进行递归调用 { Node *rel = Id3_solution(child[i],attr); rel->attrvalue=i; now->childNode.push_back(rel); } return now; } void test(Node * Root)　　　　　　　　　　//用测试样例进行测试 { Node* pnow=Root; ifstream fin(\"test.txt\"); for(int i=0;i<testRow;i++) { for(int j=0;j<testColumn;j++) fin>>TestTable[i][j]; } fin.close(); for(int i=0;i<testRow;i++) { pnow=Root; while(true) { if(pnow->value==1) {TestTable[i][DataColumn-1]=\"yes\";break;} else if(pnow->value==0) {TestTable[i][DataColumn-1]=\"no\";break;} for(vector<Node*>::iterator iter=pnow->childNode.begin();iter!=pnow->childNode.end();++iter) { if((*iter)->attrvalue==string2int[TestTable[i][pnow->attrid]]) {pnow=*iter; break;} } } } } int main() { Init(); Node * Root = Id3_solution(S,Attributes); test(Root); for(int i=0;i<testRow;i++) { for(int j=0;j<DataColumn;j++) cout<<TestTable[i][j]<<\" \"; cout<<endl; } return 0; }\n结果："}
{"content2":"笔记总结，各章节主要内容已总结在标题之中\nAndrew Ng机器学习课程笔记–week1(机器学习简介&线性回归模型)\nAndrew Ng机器学习课程笔记--week2(多元线性回归&正规公式）\nAndrew Ng机器学习课程笔记--week3(逻辑回归&正则化参数)\nAndrew Ng机器学习课程笔记--week4(神经网络)\nAndrew Ng机器学习课程笔记--week5(上)(神经网络损失函数&反向传播算法）\nAndrew Ng机器学习课程笔记--week5(下)(梯度检测&BP随机初始化)\nAndrew Ng机器学习课程笔记--week6(算法评估&bias&variance&精度&召回率)\nAndrew Ng机器学习课程笔记--week7(SVM)\nAndrew Ng机器学习课程笔记--week8(非监督学习&聚类&PCA)\nAndrew Ng机器学习课程笔记--week9(上)(异常检测&多元高斯分布)\nAndrew Ng机器学习课程笔记–week9(下)(推荐系统&协同过滤)\nAndrew Ng机器学习课程笔记--week10(优化梯度下降)\nAndrew Ng机器学习课程笔记--week11(图像识别&总结划重点 )\n敲黑板划重点！！！！\n下面是本系列课程笔记的总结。\n\nMARSGGBO♥原创\n\n2017-8-16"}
{"content2":"转自论坛http://www.ieee.org.cn/dispbbs.asp?BoardID=62&replyID=31567&id=29962&star=1&skin=0\n作者好像是南大周志华老师\n我知道的几个人工智能会议(一流)\n下面同分的按字母序排列:\nIJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个领域就没几篇了，象machine learning、computer vision这么大的领域每次大概也就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司\"IJCAI Inc.\"主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer& Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外，IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找3位reviewer, second PC member 找一位.\nAAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可以给到1+, 也可以给到1-或者2+, 总的来说我给它\"1\". 这是因为它的开法完全受IJCAI制约: 每年开, 但如果这一年的IJCAI在北美举行, 那么就停开. 所以, 偶数年里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些,特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了.\nCOLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉, 所以这个会被一些人看成是理论计算机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: \"一小群数学家在开会\". 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的会议, 例如COLT.\nCVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. IEEE一直有个倾向, 要把会办成\"盛会\", 历史上已经有些会被它从quality很好的会办成\"盛会\"了. CVPR搞不好也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了.\nICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办.  ICCV逢奇数年开，开会地点以往是北美，欧洲和亚洲轮流，本来2003年定在北京，后来因Sars和原定05年的法国换了一下。ICCV'07年将首次在南美(巴西)举行.\nCVPR原则上每年在北美开, 如果那年正好ICCV在北美,则该年没有CVPR.\nICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的介绍.\nNIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS'05的论文集是06年出. 会议的名字是\"Advances in Neural Inxxxxation Processing Systems\", 所以, 与ICMLECML这样的\"标准的\"机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在MichaelJordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给\"外人\"的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说,ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选理事, 有资格提名的人包括近三年在ICMLECMLCOLT发过文章的人, NIPS则被排除在外了. 无论如何, 这是一个非常好的会.\nACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of\nComputational Linguistics) 主办, 每年开.\nKR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI)最好的会议之一. KR Inc.主办, 现在是偶数年开.\nSIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至有点机器学习应用会议的味道了, 所以把它也列进来.\nSIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列在tier-1里面, 一方面是名声远不及其他的top conference响亮, 另一方面是相对容易被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. 这几年来KDD的质量都很高. SIGKDD从2000年来full paper的录取率都在10%-12%之间，远远低于IJCAI和ICML.\n经常听人说，KDD要比IJICAI和ICML都要困难。IJICAI才6页，而KDD要10页。没有扎实系统的工作，很难不留下漏洞。有不少IJICAI的常客也每年都投KDD，可难得几个能经常中。\nUAI (1-): 名字叫\"人工智能中的不确定性\", 涉及表示推理学习等很多方面, AUAI(Association of UAI) 主办, 每年开.\n我知道的几个人工智能会议(二三流)\n(原创为lilybbs.us上的daniel)\n纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全.\n同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的.\ntier 2: tier-2的会议列得不全, 我熟悉的领域比较全一些.\nAAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,\n几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显.\nECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能升级到1-去.\nECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显.\nICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了.\nSDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚,但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的.\nICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了.\nICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上.\nCOLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多.\nECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,\n很难往上升.\nALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容.\nEMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点.\nILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了.\nPKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被PKDD接受).\ntier 3: 列得很不全. 另外, 因为AI的相关会议非常多, 所以能列在tier-3也算不错了, 基本上能进到所有AI会议中的前30%吧\nACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了.\nDS (3+): 日本人发起的一个接近数据挖掘的会议.\nECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议.\nICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了.\nPAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5.\nICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN.\nAJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.\nCAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.\nCEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC/FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI  (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作.\nFUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍.\nGECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型.\nICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议.\nICIP (3): 图像处理方面最著名的会议之一, 盛会型.\nICPR (3): 模式识别方面最著名的会议之一, 盛会型.\nIEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹.\nIJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍.\nIJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议.\nPRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升."}
{"content2":"我断断续续利用在家休假的时间，完成了这门课程《Essential Math for Machine Learning: Python Edition》的学习，并且得到了总分91分。\n这门课程的主要内容如下，虽然我们以前都学过数学，但大部分真的都还给老师了。学习这门课程中，总是隐隐约约有一种压力感，一方面总是觉得这些知识我应该知道啊，但另外一方面，看着那些题目却有一种无助的感觉。\n学习数学很可能是有趣的，因为我在学习这门课程时，到处找资料，发现真的有一个网站叫 Math is Fun，而且有中文的版本（数学乐）。\n英文网站是 https://www.mathsisfun.com/ ，中文网站是 https://www.shuxuele.com/index.html\n但无论如何有趣，前提是你能掌握它的一些基本原理。谢天谢地谢Python，因为有了numpy和pandas这些package，在理解和掌握上面提到的几方面数学知识过程中，我感觉到了前所未有的能力。我印象最深刻的是，在计算向量和矩阵运算的时候，使用numpy提供的array以及matrix类型，简直易如反掌。\n课程附带了25个练习，是Jupyter格式的notebook。每个材料都丰富详实，可谓业界良心，真心为这些讲师点赞。\n如果对Jupyter 还不太熟悉，建议访问 http://jupyter.org/\n我另外还发现一个不错的中文网站 https://zh.wikihow.com 里面有不少数学方程求解的讲解。\n最后，还给大家分享一个Python的统计分析库——statsmodels ，请参考https://www.statsmodels.org/stable/index.html\n以及与numpy齐名的Scipy，在本课程中用到了它的stats模块中的binom类型来计算概率，非常实用。\n要了解Scipy，请参考  https://www.scipy.org/\n请通过 https://aka.ms/learningAI 或者扫描下面的二维码关注本系列文章《人工智能学习笔记》"}
{"content2":"悦动智能 | 微信公众号ID：aibbtcom\n现如今，人工智能已经被炒的非常火热，似乎不管是不是科技圈的人士，都要在嘴边聊上几句人工智能，以显示自己多么与时俱进。\n人工智能的定义是让机器实现原来只有人类才能完成的任务，其核心是算法。\n例如下图所示就是让机器模拟人各种能力的人工智能领域示意图：\n当然一方面人工智能的确是未来的方向，而另一方面则是因为人工智能有可能是科技圈中的下一个黑天鹅。说不定什么时候，一只独角兽就会从中诞生。\n但在此之前，一定要正确的认清什么才是真正的人工智能。\n伪人工智能横行\n现在大多数人工智能都属于伪人工智能。为什么这么说，可以从以下两个方面来解释。\n第一，人工智能不是一下就能做出来的，需要时间以及实验的积累。\n而做出人工智能的这些人才也是一样，他们需要切实的接触到真正的人工智能当中，不过这样的人才在全世界也就寥寥几百个。\n但是好像在一瞬间，在中国就有几万个人工智能方面的人才被选拔了出来，可想而知这样的人才是真正的人工智能专家吗？\n这些人才往往被大公司冠以年薪30万或50万疯抢，虽然里面的确有很多优秀的人才，但是这样未免显得太过着急。从人才培养角度来看，人工智能领域还存在着大量的泡沫。\n第二，许多项目只不过是换了个‘马甲’。\n许多创业公司喜欢为自己的项目贴上一个标签，这样的话不但可以吸引眼球，更能得到投资人的青睐。\n虽然不能说这种做法是错误的，但这显然也不是真正的人工智能，甚至会误导其他人对于人工智能的认知。\n比如许多项目在贴上人工智能标签之前非常简单，只是一些如同机器人学习，或者算法研究之类的项目，如今摇身一变全都成为了人工智能。\n什么才是真正的人工智能？\n我们既不是专家，也不是专门研究这种领域的学者，有没有简单的方法直接辨别什么是人工智能，什么是伪人工智能？\n答案是有的。\n举一个简单的例子，之前人们也尝试教计算机下国际象棋。计算机经过学习之后，与人们依然互有胜负，在最终完全战胜人类的时候，时间已经过去了10年。\n而谷歌的AlphaGo，从什么都不会到围棋中不可战胜的存在只用了短短一年的时间。\n由此可以看出，真正的人工智能体现在其卓越的学习能力。\n如果你隔一段时间，大概3个月左右去看一个算法的进步，比如面部识别，如语音识别，如果该算法进步只是代数级，没有达到指数级，那么这种算法可能更多的是机器学习，还未达到人工智能水平。\n既然已经辨别了什么是真正的人工智能，那么对于人工智能而言，什么才是最重要的。\n可能有些人会说算法，有些人会说设备，有些人会说编程技术。虽然它们也是构成人工智能中重要的一环，但是这些都不是最重要的。\n对于真正的人工智能而言，最重要的永远是大数据，只有拥有完整的数据，人工智能才能真正的发展起来。就像是一把宝刀，需要有一块好的磨刀石才能让它更加锐利，而大数据恰好就是这块最好的磨刀石。\n就像是谷歌的AlphaGo，有人说为什么AlphaGo不去下象棋，而是只在围棋领域中称雄呢。\nAlphaGo的专家则表示，不是他们不想这么做，而是无法这么做。因为在围棋中，日本人一直以来有保存棋谱的习惯，在每个棋谱上都标注了什么是第1手，什么是第100手，这样很容易被AlphaGo学习。\n但是对于象棋来说，自古以来大多数都是残局。虽说残局也很精彩，但是对于AlphaGo来说，它不知道残局形成的原因，对之前的步骤一无所知，这样就会对它的认知造成障碍。\n这也说明，完整的数据对于人工智能多么重要。任何抛开数据谈人工智能的，全都是耍流氓。\n人工智能中的独角兽\n目前，中国的大部分数据全都被BAT所掌握着，国外则是Facebook、Google、亚马逊之类的企业。对于创业者而言，想要打破数据的垄断具有相当大的挑战，但也不是没有机会。\n比如说医疗数据，BAT就还没有形成垄断。金融方面数据，更多的掌握在金融公司手中，这些互联网企业也没有。\n在这两个领域，不管你的技术水平如何，至少在数据方面是在同一起跑线上，这对于创业者或后进入的公司是一个难得机遇。同时，下一个巨头也有可能在这两个领域诞生。\n就拿医疗来说，国外已经有许多家企业与医院达成协作，直接读取医院中的病例以及X光片或者CT片。\n医生一天看10张并且分析出症状都已经是非常有经验了，而人工智能，则可以在1个小时内看10万张，效率不可同日而语。\n对于医生而言，诊断病因需要基于自己的经验积累。但是对于人工智能来说这就太简单了，通过图像和最终诊断结果的闭环学习，人工智能很快就能对X光片或CT片进行病因分析。当然这一过程需要不断完善，才能提升正确性及智能化。\n在国外由于隐私保护非常严密，很多数据无法开放，因此无法做到大量数据录入。\n但是由于如今中国民众对于隐私保护还没有那么严格，因此中国企业还是有机会在这个领域中实现超越的。\n只要有了大数据，特定领域超越BAT也不是不可能的。\n所以说，数据才是人工智能中最重要的一环。\n原文地址：http://www.aibbt.com/a/29255.html"}
{"content2":"机器学习语言\n一、机器学习常用的编程语言有哪些？\n机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、 凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以 获取新的知识或技能， 重新组织已有的知识结构使之不断改善自身的性能。 它是人工智能的 核心， 是使计算机具有智能的根本途径， 其应用遍及人工智能的各个领域， 它主要使用归纳、 综合而不是演绎。 目前机器学习语言主要有：R 语言，Python，MATLAB、JAVA、WEKA、GO、JavaScript 等。\n二、各个语言在机器学习方面的优缺点:\n1、R 语言\nR 语言是一个开发环境，采用一种近似于 Lisp 的脚本语言。在这个库中，所有与统计相 关的功能都通过 R 语言提供， 包括一些复杂的图标。 CRAN(可以认为是机器学习的第三方包) 中的机器学习目录下的代码， 是由统计技术方法和其他相关领域中的领军人物编写的。 如果 做实验，或是快速拓展知识，R 语言都是必须学习的。R 的优势在于有包罗万象的统计函数 可以调用，特别是在时间序列分析方面（主要用在金融分析与趋势预测）无论是经典还是前 沿的方法都有相应的包直接使用；相比 python 在这方面贫乏不少。缺点是在处理大数据方 面，性能和速度低下。\n2、Python\nPython 是一门多功能的语言。数据统计是更多是通过第三方包来实现的。在数据分析、 海量的数据统计、以及提供互动化的数据分析，动态的缩放图表等综合功能最强大的。但是 这些功能分散在第三方库里面，没有得到有机的整合，相应的学习成本会较高。python 与 R 相比速度要快。python 可以直接处理上 G 的数据；R 不行，R 分析数据时需要先通过数据库 把大数据转化为小数据（通过 groupby）才能交给 R 做分析，因此 R 不可能直接分析行为详 单，只能分析统计结果。Python 的优势在于其胶水语言的特性，一些底层用 C 写的算法封 装在 python 包里后性能非常高效(Python 的数据挖掘包 Orange canve 中的决策树分析 50 万 用户 10 秒出结果，用 R 几个小时也出不来，8G 内存全部占满)。\n3、MATLAB\n作为机器学习、模式识别等方面经常使用的工具，MATLAB 在实现机器学习算法时要比 Python 或者 NumPy 更加自然。很多高校也在计算机科学相关的课程中教授 MATLAB 语言， 易学易用。然而，MATLAB 也存在很多的缺点：价格昂贵、非开源、性能表现平平、语法不 符合程序员的习惯等。 例如， MATLAB 中矩阵乘积运算操作为 X.dot(Y) ， 而 Python 为 X@W ， 更加简洁、明了。MATLAB 的性能比 Python、Go、Java 等语言要差很多。\n4、JAVA\n在大数据框架中，Mahout（在印地语中意思是“大象骑士”）包 含几种常见的机器学 习方法。这款软件包是围绕算法而非方法，所以需要有一定的算法基础，其各部分功能是整 合在一起的，比如基于用户的推荐系统。 另一个基于 Hadoop 的机器学习项目是 Cloudera 公司推行的 Oryx， 其特性在于通过交付 实时流结果而非处理批量作业来对 Mahout 处理结果进行进一步分析。 该该项目现在还处于 初始阶段，这只是个项目而非实际产品，但它在不断改善，所以很值得关注。\n除了上述主要针对 Hadoop 的 Mahout， 其他一些面向 Java 的机器学习库也在广泛使用。 Weka 由新西兰怀卡托大学开发的工作台式的应用，它在常见的算法集合中增加了可视化和 数据挖掘功能。对于那些想要为他们的工作打造一个前端或者计划将 Java 作为初始开发的 用户来说，Weka 可能是最好的选择。Java-ML 也不错，但它更适合那些已经习惯将 Java 和 机器学习配合使用的开发者。\n5、WEKA\nWEKA 是一个数据挖掘工作平台，为用户提供数一系列据挖掘全过程的 API、命令行和 图形化用户接口。可以准备数据、可视化、建立分类、进行回归分析、建立聚类模型，同时 可以通过第三方插件执行其他算法。\n6、Go\n谷 歌的系统语言，由于其并行设计，使其似乎是一个编写机器学习库理想的环境。虽然 目前与之相关的库项目规模尚小，但也有一些值得关注， GoLearn，它的开发者将其描述为 一个“内置电池”的机器学习库。它提供过滤、分类以及回归分析等多种工具。另一套较小 且更为基础的库是 mlgo，虽然目前它能提供的算法数量还非常少，但计划在未来推出更多。\n7、JavaScript\n关于 JavaScript， 原意是这样的， 任何能够由 JavaScript 编写的内容最终都会由 JavaScript 编写，这对机器学习库同样适用。目前由 JavaScript 编写的方案在这一领域数量仍然相对较 少，大多数选项仅仅是单一算法而非完整的库，但已经有部分有用工具渐渐脱颖而出。 ConvNetJS 允许大家直接在浏览器当中进行深度学习神经网络培训，而名为 brain 则将神经 网络作为可安装的 NPM 模块提供给大家。此外，Encog 库同样值得关注，而且它适用于多 种平台：Java、C#、C/C++以及 JavaScript。\n三：相关回答：\n1. 机器学习，无非就是提取特征，然后分类，而这其中的大部分在opencv里已经集成了，所以你有必要先学习一下opencv这一开源库，强大而简洁。关于入门的资料，你可以看一下csdn的浅墨的文章。他的博客地址http://blog.csdn.net/poem_qianmo?viewmode=contents，这也是我oepncv入门的资料，共十八课，踏踏实实的跟着坐下来，应该是能入门了，如果不够，可以买他写的书，及我大爱的一本《深入理解opencv》。\n2. 关于机器学习，有那么一本书《机器学习实战》，是用python写的，个人觉得很好，不仅简单的写了下常用机器学习算法的原理，而且有代码。python要是不熟悉的话，可以现学现卖，如果你之前学过任何一门语言，那么python学习就会比较简单，现在在搞深度学习框架，很多框架的都提供了python的接口，python是一门愈来愈热的语言，有必要学习。\n3. 关于视频分析，我从我从事的智能监控方面来讲一下，其实就是图像的处理，首先要提取视频中的运动物体，常用算法有：帧差法，GMM，vibe等；提取前景（运动物体）后对其进行跟踪，跟踪的主要算法有：camshift，粒子滤波，TLD，压缩感知等；以及之后对监控视频的去模糊，去雾，夜视增强，行人检测，车牌检测，上下身颜色识别，人车分类、视频浓缩，不过这些，都可基于opencv来实现。\n4. 机器学习的分支，深度学习，也就是深度神经网络是近来比较火热的领域，很多机器学习实现的功能很难用到商用中，比如人脸识别，传统的机器学习方法受光照，角度干扰太大，很难达到较好的识别率，深度学习在图像中的应用已经有很多了。这里介绍几个框架，也是目前我在用的，伯克利的caffe，以及谷歌的tensorflow，当然这应该是你完成上述前三部门的内容后，才该做的。"}
{"content2":"前言：\n高斯过程回归(GPR)和贝叶斯线性回归类似，区别在于高斯过程回归中用核函数代替了贝叶斯线性回归中的基函数（其实也是核函数，线性核）。采用核函数可以定义高斯过程回归是一个比贝叶斯线性回归更通用的模型，应用非常广泛。本文参考的资料为视频http://www.youtube.com/playlist?list=PLD0Z06AA0D2E8ZZBA中相关的部分以及论文Gaussian Processes for Regression A Quick Introduction.\n基础知识：\n首先来看看Bayesian linear regression（贝叶斯线性回归）模型：\n其中的D为已知的有监督的训练样本。Yi为样本标签，由可知，yi可以表示为一个高斯过程和一个随机变量的和。公式中的w是一个多维高斯分布。\n而  是一个高斯分布，并且它属于线性高斯分布。有上一篇博文机器学习&数据挖掘笔记_10（高斯过程简单理解）可知，如果高斯过程为线性的，即它的sample是在高维空间中的平面，要求它的核函数需满足k(xi,xj)=xi’*xj的形式，且均值函数为0，下面是它的证明过程：\n既然已经得知yi的中心是在一个高维空间的平面上，所以当新来的数据后，就可以预测它的均值也在该平面对应的位置上，这就达到了回归的目的。\n在将BLR（贝叶斯线性回归）扩展到GPR(高斯过程回归)前，来看看多维高斯分布的一些重要性质，第一个性质为两个相互独立的多维高斯分布A和B的和也是一个多维高斯分布C，且C的均值和方差都为A和B均值方差的和。第二个性质为：两个多维高斯分布之和构成的分布C而言，在已知一部分观察值C1的条件下，另一部分观察值C2的概率分布是一个多维高斯分布，且可以用A和B中对应的信息来表示。这2个性质的介绍如下：\n接下来就是要怎样利用高斯过程进行回归运算了。高斯过程回归的模型如下：\n其中的ya为需要预测的值，yb为观察到的值，当然了，xa和xb也是观察值。由前面博文机器学习&数据挖掘笔记_10（高斯过程简单理解）中介绍的高斯过程存在性定理可知，一旦我们确定了x上的u和k，就可以得到一个高斯过程Zx，此时的样本值Yi可以写成：  即两个独立的多维高斯变量之和。而利用上面多维高斯变量的性质，可推导出需要预测的ya在yb条件下的概率：\n上面的m和D有解析表达式，因此可以直接求，里面的的变量都是已知的。其中的m就是我们回归预测的值，而D就是此时预测的误差，两者表达式和前面类似，如下：\n由贝叶斯线性回归和高斯过程回归的对比可知，贝叶斯线性回归是高斯过程回归中的一个子集，只是它用的是线性核而已，通过两者的公式就可以看出它们之间的关系：\n上面是贝叶斯线性回归，下面是高斯过程回归。\n简单例子：\n假设现在已经观察到了6个样本点，x为样本点特征（一维的），y为样本输出值。现在新来了一个样本点，要求是用高斯回归过程来预测新来样本点的输出值。这些样本点显示如下;\n其中前面6个点是已知输出值的训练样本，其值为：\n第7个点是需要预测的样本，红色的垂直条形线表示观察输出值的误差，绿色的垂直条形线为用高斯过程回归的误差。\n用GPR解该问题的流程大概如下（对应前面讲的一些基础知识）：\n1. 选择适当的u（均值函数）和k（核函数），以及噪声变量σ，其中核函数的选择尤其重要，因为它体现了需处理问题的先验知识，应根据不同的应用而选择不同的核。\n2. 计算出训练样本的核矩阵（6*6），如下：\n3. 计算需预测的点  与训练样本6个点的核值向量，如下：\n4. 自己和自己的核值为  且此时整个样本的多维高斯分布表达式为：\n5. 通过前面m和D的公式，求得m=0.95，D=0.21.\n6. 画出最终结果如下：\n这个例子来源于论文Gaussian Processes for Regression A Quick Introduction中，它的核函数等参数选择和基础知识部分的不同，但这里主要是对GPR的应用有个简单的宏观上的理解，让大脑对GPR应用有个初步的印象，否则有了那么多的公式推导但不会应用又有什么用呢？\n参考资料：\nhttp://www.youtube.com/playlist?list=PLD0Z06AA0D2E8ZZBA\nGaussian Processes for Regression A Quick Introduction, M.Ebden, August 2008.\n机器学习&数据挖掘笔记_10（高斯过程简单理解）"}
{"content2":"1人工智能科普类：人工智能科普、人工智能哲学\n《智能的本质》斯坦福、伯克利客座教授30年AI研究巅峰之作\n《科学+遇见人工智能》李开复、张亚勤、张首晟等20余位科学家与投资人共同解读AI革命\n《人工智能时代》从人工智能的历史、现状、未来，工业机器人、商业机器人、家用机器人、机器翻译、机器学习等人工智能应用领域依次介绍了人工智能发展前景。\n《人工智能简史》 跟着图灵、冯•诺依曼、香农、西蒙、纽维尔、麦卡锡、明斯基等人工智能的先驱们重走人工智能之路，站在前人的肩膀上，看人工智能的三生三世，鉴以往才能知未来。\n2人工智能机器学习类：Python、机器学习、数据科学\n《Python机器学习实践指南》 结合了机器学习和Python 语言两个热门的领域，通过利用两种核心的机器学习算法来用Python 做数据分析。\n《Python机器学习——预测分析核心算法》从算法和Python语言实现的角度，认识机器学习。\n《机器学习实践应用》阿里机器学习专家力作，实战经验分享，基于阿里云机器学习平台，针对7个具体的业务场景，搭建了完整的解决方案。\n《NLTK基础教程——用NLTK和Python库构建机器学习应用》介绍如何通过NLTK库与一些Python库的结合从而实现复杂的NLP任务和机器学习应用。\n3人工智能深度学习类：深度学习、Tensorflow\n《深度学习》AI圣经，深度学习领域奠基性的经典畅销书 特斯拉CEO埃隆·马斯克等国内外众多专家推荐！\n《深度学习精要（基于R语言）》基于R语言实战,使用无监督学习建立自动化的预测和分类模型\n《TensorFlow技术解析与实战》包揽TensorFlow1.1的新特性 人脸识别 语音识别 图像和语音相结合等热点一应俱全\n《TensorFlow机器学习项目实战》第二代机器学习实战指南，提供深度学习神经网络等项目实战，有效改善项目速度和效率。\n4\n人工智能算法策略类：算法、推荐系统、编程等\n《神经网络算法与实现——基于Java语言》 完整地演示了使用Java开发神经网络的过程，既有非常基础的实例也有高级实例。\n《趣学算法》 50 多个实例循展示算法的设计、实现、复杂性分析及优化过程 培养算法思维 带您感受算法之美。\n《算法谜题》 Google、Facebook等一流IT公司算法面试必备，经典算法谜题合集。\n《Python算法教程》精通Python基础算法，畅销书Python基础教程作者力作。\n《编程之法：面试和算法心得》程序员面试宝典 笔试金典 CSDN访问量过千万的博客结构之法算法之道博主July著作。\n《趣题学算法》 一本有趣的、易学的、实用的，帮助读者快速入门应用的算法书。\n《Java遗传算法编程》 遗传算法设计 机器学习人工智能 来自Java专家的声音 用遗传算法解决类似旅行商的经典问题。\n《算法学习与应用从入门到精通》320个实例、753分钟视频、5个综合案例、74个技术解惑，一本书的容量，讲解了入门类、范例类和项目实战类三类图书的内容。\n5人工智能时间图像和视觉识别类：图像识别 、语音识别、自然语言处理、建模工程\n《OpenCV和Visual Studio图像识别应用开发》无人驾驶人脸识别基础技术 用OpenCV实现图像处理应用 计算机视觉编程实战手册。\n《人脸识别原理及算法——动态人脸识别系统研究》 介绍了动态场景下的人脸识别方法，该方法综合应用了人脸定位、人脸识别、视频处理等算法。\n《精通Python自然语言处理》用Python开发令人惊讶的NLP项目，自然语言处理任务，掌握利用Python设计和构建给予NLP的应用的实践。\n《Python自然语言处理》基于Python编程语言和NLTK，自然语言处理领域的一本实用入门指南。\n《贝叶斯方法：概率编程与贝叶斯推断》 机器学习 人工智能 数据分析从业者的技能基础 国际杰出机器学习专家余凯博士 腾讯专家研究员岳亚丁博士推荐。\n《贝叶斯思维：统计建模的Python学习法》Thin Stats和Think Python图书作者重磅出击，数据分析师、数据工程师、数据科学家案头常备。\n《概率编程实战》人工智能领域的先驱、美国加州大学伯克利分校教授Stuart Russell作序推荐！一本不可思议的Scala概率编程实战书籍！\n《自己动手写神经网络》机器学习与人工智能参考书，基于Java语言撰写。"}
{"content2":"摘要：本文首先浅谈了自己对决策树的理解，进而通过Python一步步构造决策树，并通过Matplotlib更直观的绘制树形图，最后，选取相应的数据集对算法进行测试。\n最近在看《机器学习实战》这本书，因为一直想好好了解机器学习方面的算法，加之想学Python，就在朋友的推荐之下选择了这本同等定位的书。今天就来学习一下决策树，所有的代码均python3.4实现，确实与2.7有很多不同。\n决策树和KNN一样，都是处理分类问题的算法。对于决策树的定义不计其数，就我个人而言，首先单看名字，就想到了最小生成树，猜想图解的话这个算法会是一棵树，在机器学习这个层面，将所要处理的数据看做是树的根，相应的选取数据的特征作为一个个节点（决策点），每次选取一个节点将数据集分为不同的数据子集，可以看成对树进行分支，这里体现出了决策，直到最后无法可分停止，也就是分支上的数据为同一类型，可以想象一次次划分之后由根延伸出了许多分支，形象的说就是一棵树。\n在机器学习中，决策树是一个预测模型，它代表的是对象属性与对象值之间的一种映射关系，我们可以利用决策树发现数据内部所蕴含的知识，比如在本文的最后我们选取隐形眼镜数据集根据决策树学习到眼科医生是如何判断患者佩戴眼镜片的过程，而K近邻算法虽与决策树同属分类，却无从得知数据的内在形式。下面我们就一步步的学习决策树：\n1. 构造决策树\n基于之前的了解，在构造决策树首先需要选取特征将原始数据划分为几个数据集，那么第一个问题就是当前数据的哪个特征在划分数据分类时起决定性作用，所以必须评估每个特征。进而通过特征将原始数据就被划分为几个数据子集，这些数据子集分布在第一个决策点的所有分支上，如果分支上的所有数据为同一类型，则划分停止，若分支上的所有数据不是同一类型，则还需要继续划分，直到所有具有相同类型的数据均在一个数据子集中。在用决策树进行划分时，关键是每次划分时选取哪个特征进行划分，在划分数据时，我们必须采用量化的方法判断如何划分数据。\n（1）信息增益\n划分数据时是根据某一原则进行划分，使得划分在同一集合中的数据具有共同的特征，据此，我们可以理解为划分数据的原则就是是无序的数据变得有序。当然划分数据有很多种方法，在此选用信息论度量信息，划分组织杂乱无章的数据。\n信息论是量化处理信息的分支科学，可以在数据划分之前或之后使用信息论量化度量信息的内容。其中在划分数据集之前之后信息发生的变化称为信息增益，计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。\n首先我们需要知道怎么计算信息增益，集合信息的度量方式称为香农熵或者简称为熵，熵定义为信息的期望值，那么信息是什么？xi的信息可定义为：L(xi) = -log(p(xi)),其中p(xi)是选择该分类的概率。\n熵指的是所有类别所有可能值包含的信息期望值，可表示为：\n熵越高，表明混合的数据越多，则可以在数据集中添加更多的分类。基于上述的分析，编程计算给定数据集的香农熵，代码如下：\nfrom math import log ###计算香农熵(为float类型） def calShang(dataSet): numEntries = len(dataSet) labelCounts = {}##创建字典 for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key]) / numEntries shannonEnt -= prob * log(prob,2) return shannonEnt\n对此我们可以输入数据集测试：\ndef creatDataSet(): dataSet = [[1,1,'yes'], [1,1,'yes'], [1,0,'no'], [0,1,'no'], [0,1,'no']] labels = ['no surfacing','flippers'] return dataSet,labels ''' #测试 myData,labels = creatDataSet() print(\"原数据为：\",myData) print(\"标签为：\",labels) shang = calShang(myData) print(\"香农熵为：\",shang) '''\n得到熵之后，我们就可以按照获取最大增益的办法划分数据集。\n（2）划分数据集\n基于之前的分析，信息增益表示的是信息的变化，而信息可以用熵来度量，所以我们可以用熵的变化来表示信息增益。而获得最高信息增益的特征就是最好的选择，故此，我们可以对所有特征遍历，得到最高信息增益的特征加以选择。\n首先，我们按照给定特征划分数据集并进行简单的测试：\n###划分数据集（以指定特征将数据进行划分） def splitDataSet(dataSet,feature,value):##传入待划分的数据集、划分数据集的特征以及需要返回的特征的值 newDataSet = [] for featVec in dataSet: if featVec[feature] == value: reducedFeatVec = featVec[:feature] reducedFeatVec.extend(featVec[feature + 1:]) newDataSet.append(reducedFeatVec) return newDataSet ''' #测试 myData,labels = creatDataSet() print(\"原数据为：\",myData) print(\"标签为：\",labels) split = splitDataSet(myData,0,1) print(\"划分后的结果为:\",split) '''\n接下来我们遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的划分方式并简单测试：\n##选择最好的划分方式(选取每个特征划分数据集，从中选取信息增益最大的作为最优划分)在这里体现了信息增益的概念 def chooseBest(dataSet): featNum = len(dataSet[0]) - 1 baseEntropy = calShang(dataSet) bestInforGain = 0.0 bestFeat = -1##表示最好划分特征的下标 for i in range(featNum): featList = [example[i] for example in dataSet] #列表 uniqueFeat = set(featList)##得到每个特征中所含的不同元素 newEntropy = 0.0 for value in uniqueFeat: subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet) / len(dataSet) newEntropy += prob * calShang(subDataSet) inforGain = baseEntropy - newEntropy if (inforGain > bestInforGain): bestInforGain = inforGain bestFeature = i#第i个特征是最有利于划分的特征 return bestFeature ''' ##测试 myData,labels = creatDataSet() best = chooseBest(myData) print(best) '''\n（3）递归构建决策树\n基于之前的分析，我们选取划分结果最好的特征划分数据集，由于特征很可能多与两个，因此可能存在大于两个分支的数据集划分，第一次划分之后，可以将划分的数据继续向下传递，如果将每一个划分的数据看成是原数据集，那么之后的每一次划分都可以看成是和第一次划分相同的过程，据此我们可以采用递归的原则处理数据集。递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都有相同的分类。编程实现：\n##递归构建决策树 import operator #返回出现次数最多的分类名称 def majorClass(classList): classCount = {} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 #降序排序，可以指定reverse = true sortedClassCount = sorted(classcount.iteritems(),key = operator.itemgetter(1),reverse = true) return sortedClassCount[0][0] #创建树 def creatTree(dataSet,labels): classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): return classList[0] if len(dataSet[0]) == 1: return majorClass(classList) bestFeat = chooseBest(dataSet) bestFeatLabel = labels[bestFeat] myTree = {bestFeatLabel:{}} del(labels[bestFeat]) featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = creatTree(splitDataSet(dataSet,bestFeat,value),subLabels) return myTree ''' #测试 myData,labels = creatDataSet() mytree = creatTree(myData,labels) print(mytree) '''\n2.使用matplotlib注解绘制树形图\n之前我们已经从数据集中成功的创建了决策树，但是字典的形式非常的不易于理解，因此本节采用Matplotlib库创建树形图。\n首先，使用文本注解绘制树节点：\n##采用matplotlib绘制树形图 import matplotlib.pyplot as plt decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\") leafNode = dict(boxstyle=\"round4\", fc=\"0.8\") arrow_args = dict(arrowstyle=\"<-\") #绘制树节点 def plotNode(nodeTxt, centerPt, parentPt, nodeType): createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction', xytext=centerPt, textcoords='axes fraction', va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args )\n获得叶节点的数目和树的层数，并进行测试：\n##获取节点的数目和树的层数 def getNumLeafs(myTree): numLeafs = 0 #firstStr = myTree.keys()[0] firstSides = list(myTree.keys()) firstStr = firstSides[0]#找到输入的第一个元素 secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]) == dict: numLeafs += getNumLeafs(secondDict[key]) else: numLeafs += 1 return numLeafs def getTreeDepth(myTree): maxDepth = 1 firstSides = list(myTree.keys()) firstStr = firstSides[0]#找到输入的第一个元素 #firstStr = myTree.keys()[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]) == dict: thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth > maxDepth: maxDepth = thisDepth return maxDepth def retrieveTree(i): listOfTrees =[{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}, {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}} ] return listOfTrees[i] #测试 mytree = retrieveTree(0) print(getNumLeafs(mytree)) print(getTreeDepth(mytree))\n在此，我们说明一下Python2.7和3.4在实现本段代码的区别：\n在2.7中，找到key所对应的第一个元素为：firstStr = myTree.keys()[0]，这在3.4中运行会报错：'dict_keys' object does not support indexing，这是因为python3改变了dict.keys,返回的是dict_keys对象,支持iterable 但不支持indexable，我们可以将其明确的转化成list，则此项功能在3中应这样实现：\nfirstSides = list(myTree.keys()) firstStr = firstSides[0]#找到输入的第一个元素\n绘制树：\ndef plotNode(nodeTxt, centerPt, parentPt, nodeType): createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction', xytext=centerPt, textcoords='axes fraction', va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args ) def plotMidText(cntrPt, parentPt, txtString): xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1] createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30) def plotTree(myTree, parentPt, nodeTxt): numLeafs = getNumLeafs(myTree) depth = getTreeDepth(myTree) firstSides = list(myTree.keys()) firstStr = firstSides[0]#找到输入的第一个元素 cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) plotMidText(cntrPt, parentPt, nodeTxt) plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD for key in secondDict.keys(): if type(secondDict[key]).__name__=='dict': plotTree(secondDict[key],cntrPt,str(key)) else: plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD def createPlot(inTree): fig = plt.figure(1, facecolor='white') fig.clf() axprops = dict(xticks=[], yticks=[]) createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) plotTree.totalW = float(getNumLeafs(inTree)) plotTree.totalD = float(getTreeDepth(inTree)) plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; plotTree(inTree, (0.5,1.0), '') plt.show() #测试 mytree = retrieveTree(0) print(mytree) createPlot(mytree)\n测试之后结果如下：\n这样相比于字典形式确实清晰了很多。\n3.测试算法\n在本章中，我们首先使用决策树对实际数据进行分类，然后使用决策树预测隐形眼镜类型对算法进行验证。\n（1）使用决策树执行分类\n在使用了训练数据构造了决策树之后，我们便可以将它用于实际数据的分类：\n###决策树的分类函数，返回当前节点的分类标签 def classify(inputTree,featLabels,testVec):##传入的数据为dict类型 firstSides = list(inputTree.keys()) firstStr = firstSides[0]#找到输入的第一个元素 ##这里表明了python3和python2版本的差别，上述两行代码在2.7中为：firstStr = inputTree.key()[0] secondDict = inputTree[firstStr]##建一个dict #print(secondDict) featIndex = featLabels.index(firstStr)#找到在label中firstStr的下标 for i in secondDict.keys(): print(i) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]) == dict:###判断一个变量是否为dict，直接type就好 classLabel = classify(secondDict[key],featLabels,testVec) else: classLabel = secondDict[key] return classLabel ##比较测试数据中的值和树上的值，最后得到节点 #测试 myData,labels = creatDataSet() print(labels) mytree = retrieveTree(0) print(mytree) classify = classify(mytree,labels,[1,0]) print(classify)\n（2）使用决策树预测隐形眼镜类型\n基于之前的分析，我们知道可以根据决策树学习到眼科医生是如何判断患者需要佩戴的眼镜片，据此我们可以帮助人们判断需要佩戴的镜片类型。\n在此从UCI数据库中选取隐形眼镜数据集lenses.txt，它包含了很多患者眼部状况的观察条件以及医生推荐的隐形眼镜类型。我们选取此数据集，结合Matplotlib绘制树形图，进一步观察决策树是如何工作的，具体的代码如下：\nfr = open('lenses.txt') lenses = [inst.strip().split('\\t') for inst in fr.readlines()] lensesLabels = ['ages','prescript','astigmatic','tearRate'] lensesTree = creatTree(lenses,lensesLabels) print(lensesTree) createPlot(lensesTree)\n得到的树形图：\n沿着决策树的不同分支，我们可以得到不同患者需要佩戴的隐形眼镜类型，从该图中我们可以得到，只需要问四个问题就可以确定出患者需要佩戴何种隐形眼镜。\n本章主要使用的是ID3算法，自身也存在着很多不足。当然还有其它的决策树构造算法，比如C4.5和CART，以后有机会了再好好看看。\n以上是我自己的一些理解与总结，难免有错，望大家不吝指教~"}
{"content2":"MNIST是一个入门级的计算机视觉数据集，它包含各种手写数字图片：\n1. MNIST数据集\nMNIST，是不是听起来特高端大气，不知道这个是什么东西？\n== 手写数字分类问题所要用到的（经典）MNIST数据集 ==\nMNIST数据集的官网是Yann LeCun's website\n自动下载和安装这个数据集的python代码\n该段代码在tensorflow/examples/tutorials/mnist/input_data.py\n\"\"\"Functions for downloading and reading MNIST data.\"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function import gzip import os import tempfile import numpy from six.moves import urllib from six.moves import xrange # pylint: disable=redefined-builtin import tensorflow as tf from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n导入项目\nimport tensorflow.examples.tutorials.mnist.input_data import input_data mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n2. 运行TensorFlow的InteractiveSession\n使用TensorFlow之前，首先导入它：\nimport tensorflow as tf sess = tf.InteractiveSession()\n3. 计算图\n为了在Python中进行高效的数值计算，我们通常会使用像NumPy一类的库，将一些诸如矩阵乘法的耗时操作在Python环境的外部来计算，这些计算通常会通过其它语言并用更为高效的代码来实现。\n但遗憾的是，每一个操作切换回Python环境时仍需要不小的开销。如果你想在GPU或者分布式环境中计算时，这一开销更加可怖，这一开销主要可能是用来进行数据迁移。\nTensorFlow也是在Python外部完成其主要工作，但是进行了改进以避免这种开销。其并没有采用在Python外部独立运行某个耗时操作的方式，而是先让我们描述一个交互操作图，然后完全将其运行在Python外部。这与Theano或Torch的做法类似。\n因此Python代码的目的是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。详情请查看基本用法中的计算图表一节。\n4. 实现softmax回归模型\n4.1 占位符\n我们通过为输入图像 x 和目标输出类别 y_ 创建节点，来开始构建计算图\n我们通过操作符号变量来描述这些可交互的操作单元，可以用下面的方式创建一个：\nx = tf.placeholder(\"float\", shape=[None, 784]) y_ = tf.placeholder(\"float\", shape=[None, 10])\nx不是一个特定的值，而是一个占位符placeholder，我们在TensorFlow运行计算时输入这个值。我们希望能够输入任意数量的MNIST图像，每一张图展平成784维的向量。我们用2维的浮点数张量来表示这些图，这个张量的形状是[None，784 ]。（这里的None表示此张量的第一个维度可以是任何长度的。）\n输出类别值y_也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。\n虽然placeholder的shape参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。\n4.2 变量\n我们的模型也需要权重值和偏置量，当然我们可以把它们当做是另外的输入（使用占位符），但TensorFlow有一个更好的方法来表示它们：Variable 。 一个Variable代表一个可修改的张量，存在在TensorFlow的用于描述交互性操作的图中。它们可以用于计算输入值，也可以在计算中被修改。对于各种机器学习应用，一般都会有模型参数，可以用Variable表示。\nW = tf.Variable(tf.zeros([784,10])) b = tf.Variable(tf.zeros([10]))\n我们在调用tf.Variable的时候传入初始值。在这个例子里，我们把W和b都初始化为零向量。W是一个784x10的矩阵（因为我们有784个特征和10个输出值）。b是一个10维的向量（因为我们有10个分类）\n变量需要通过seesion初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个变量,可以一次性为所有变量完成此操作。\nsess.run(tf.initialize_all_variables())\n5. 类别预测\n现在，我们可以实现我们的模型啦。只需要一行代码！计算每个分类的softmax概率值\ny = tf.nn.softmax(tf.matmul(x,W) + b)\ntf.matmul( X，W)表示x乘以W，对应之前等式里面的Wx,这里x是一个2维张量拥有多个输入。然后再加上b，把和输入到tf.nn.softmax函数里面。\n为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好的。其实，在机器学习，我们通常定义指标来表示一个模型是坏的，这个指标称为成本（cost）或损失（loss），然后尽量最小化这个指标。但是，这两种方式是相同的。\n一个非常常见的，非常漂亮的成本函数是“交叉熵”（cross-entropy）。交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：\n交叉熵是用来衡量我们的预测用于描述真相的低效性。\n交叉熵\n根据公式计算交叉熵，可以很容易的为训练过程指定最小化误差用的损失函数，我们的损失函数是目标类别和预测类别之间的交叉熵\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n首先，用 tf.log 计算 y 的每个元素的对数。接下来，我们把 y_ 的每一个元素和 tf.log(y) 的对应元素相乘。最后，用 tf.reduce_sum 计算张量的所有元素的总和。（注意，这里的交叉熵不仅仅用来衡量单一的一对预测和真实值，而是所有100幅图片的交叉熵的总和。对于100个数据点的预测表现比单一数据点的表现能更好地描述我们的模型的性能\n6. 训练模型\n现在我们知道我们需要我们的模型做什么啦，用TensorFlow来训练它是非常容易的。因为TensorFlow拥有一张描述你各个计算单元的图，它可以自动地使用反向传播算法(backpropagation algorithm)来有效地确定你的变量是如何影响你想要最小化的那个成本值的。然后，TensorFlow会用你选择的优化算法来不断地修改变量以降低成本。\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n在这里，我们要求TensorFlow用梯度下降算法（gradient descent algorithm）以0.01的学习速率最小化交叉熵。梯度下降算法（gradient descent algorithm）是一个简单的学习过程，TensorFlow只需将每个变量一点点地往使成本不断降低的方向移动。当然TensorFlow也提供了其他许多优化算法：只要简单地调整一行代码就可以使用其他的算法。\nTensorFlow在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。\n然后开始训练模型，这里我们让模型循环训练1000次！\nfor i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(50) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n另一种代码写法\nfor i in range(1000): batch = mnist.train.next_batch(50) train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n该循环的每个步骤中，我们都会随机抓取训练数据中的50个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行train_step。\n使用一小部分的随机数据来进行训练被称为随机训练（stochastic training）- 在这里更确切的说是随机梯度下降训练。在理想情况下，我们希望用我们所有的数据来进行每一步的训练，因为这能给我们更好的训练结果，但显然这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。\n7. 评估我们的模型\n首先让我们找出那些预测正确的标签。tf.argmax 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如tf.argmax(y,1)各个预测数字中概率最大的那一个，而 tf.argmax(y_,1) 代表正确的标签，我们可以用 tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，[True, False, True, True] 会变成 [1,0,1,1] ，取平均值后得到 0.75.\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n最后，我们计算所学习到的模型在测试数据集上面的正确率。\nprint sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) #输出结果 0.9092\n8. 代码\nfrom tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) print(mnist.train.images.shape, mnist.train.labels.shape) print(mnist.test.images.shape, mnist.test.labels.shape) print(mnist.validation.images.shape, mnist.validation.labels.shape) import tensorflow as tf sess = tf.InteractiveSession() x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.nn.softmax(tf.matmul(x, W) + b) y_ = tf.placeholder(tf.float32, [None, 10]) cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) tf.global_variables_initializer().run() for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) train_step.run({x: batch_xs, y_: batch_ys}) correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"}
{"content2":"异常检测(Anomaly Detection)\n基本假设：多数情况下数据点落入正常的取值范围，但是当异常行为发生时，数据点的取值落入正常取值范围之外（如所示）。所以可以利用高斯分布，计算行为发生的概率，如果是概率小于给定阈值，则认为发生了异常行为。基本过程是利用训练数据点建立模型$p(x)$，对于新的数据点$x_{new}$, 如果$p(x_{new})<\\epsilon$则发生异常；否则正常。异常检测的应用包括：\n欺诈检测(Fraud detection)\n制造业(Manufacturing)\n数据中心监视电脑(Monitering computers in data center)\n\n高斯分布\n对于一元高斯分布$x \\sim N(\\mu, \\sigma^2)$，表达式如下，其中$\\mu$表示均值，对应于分布的对称轴；$\\sigma$表示数据点的离散程度，$\\sigma$越大函数图像的下端张口越大峰值越低；反之$\\sigma$越小，图像下端张口越小，峰值越高，如所示。\n$$p(x;\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$$\n\n参数估计\n高斯分布的总体参数$\\mu$和$\\sigma$可以使用样本数据点进行估计，如下\n$$\\mu = \\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}$$\n$$\\sigma^2=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)}-\\mu)^2$$\n注意在统计学中，参数$\\sigma^2$的系数为$\\frac{1}{m-1}$而在机器学习中习惯使用$\\frac{1}{m}$.\n异常检测算法\n对于训练数据集$\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)}\\}$，其中数据点$x^{(i)}\\in R^n$并假设每个特征均服从高斯分布，即$x^{(i)}_j \\sim N(\\mu, \\sigma^2)$，可如下建立模型$p(x)$\n\\begin{align*}p(x)&=p(x_1; \\mu_1, \\sigma_1^2)p(x_2; \\mu_2, \\sigma_2^2)\\ldots p(x_n; \\mu_n, \\sigma_n^2) \\\\ &= \\prod\\limits_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2)\\end{align*}\n算法步骤：\n1. 特征选择：选择能够指示异常行为的特征\n2. 参数估计：用训练数据集估计每个特征的整体均值$\\mu_j$和方差$\\sigma_j^2$，即$\\mu_j = \\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}_j$, $\\sigma^2_j=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)}_j-\\mu_j)^2$\n3. 用估计得到的参数$\\mu_1, \\mu_2, \\ldots, \\mu_n$,  $\\sigma^2_1, \\sigma^2_2, \\ldots, \\sigma^2_n$建立模型$p(x)$；\n4. 对于给定新的数据点$x_{new}$, 计算$p(x_{new})$；如果$p(x_{new})<\\epsilon$则发生异常，否则正常。\n算法评估:\n给定训练数据集(去掉标签建立模型)中$\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)}\\}$，训练模型$p(x)$。在交叉验证集(带标签)中，如果$p(x_{cv})<\\epsilon$，则预测$y=1$；否则预测$y=0$。最后计算指标Precision/Recall/F1Score等来评估算法性能。注意：也可以用验证集来选择阈值$\\epsilon$.\n异常检测与监督式学习对比：\n特征选择：\n选择的特征需要近似服从于高斯分布，如果明显不服从高斯分布，可以做适当的转换，例如$log(x), log(x+c), \\sqrt{x}, x^{1/3}$等\n多元高斯分布\n之前的模型假设各个特征之间是相互独立的，因此模型$p(x)$将各特征取值的概率相乘【$P(AB)=P(B)P(A|B)=P(A)P(B|A)$，当且仅当事件AB相互独立时才有$P(AB)=P(A)P(B)$】；然而当各个特征之间存在依赖关系时，一元的高斯模型将不能很好的刻画$p(x)$，需要多元高斯模型。模型$p(x)$的建立不再是各个概率相乘，而直接用多元高斯分布进行刻画$$p(x;\\mu, \\Sigma)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$ 其中$\\mu$是$n$维行向量，$\\mu=\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}$； $\\Sigma$是$n\\times n$协方差矩阵，$\\Sigma=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T$，给出了在不同参数取值下的二维高斯模型及其对应的等高线图。\n\n多元高斯模型和一元高斯模型的关系：当协方差矩阵$\\Sigma$是对角阵且对角线元为一元高斯分布的估计参数$\\sigma_j^2$时，两个模型是等价的。区别在于前者能够自动获取特征之间的依赖关系而后者不能(后者假设特征之间是独立的)。当特征数$n$很大时，前者计算代价高昂而后者计算速度快。前者适用于$m>n$(一般要求$m>10n$)的情况，而后者当$m$很小时依然适用。\n推荐系统\n电影推荐系统问题：根据用户对已看过电影的打分，对用户未看过的电影(下表中以?表示)进行打分估计，以给其推荐合适的电影。\n符号说明：\n$n_u$表示用户数量\n$n_m$表示电影数量\n$r(i, j)$是符号变量，如果用户$j$已经对电影$i$进行评分则$r(i, j)=1$；反之，如果用户$j$尚未对电影$i$进行评分则$r(i, j)=0$.\n$y^{(i, j)}$表示用户$j$对电影$i$的评分（如果用户$j$对电影$i$已经评分，即$r(i, j)=1$）.\nMovie\nUser1\nUser2\nUser3\nUser4\nx1\nx2\nmovie1\n5\n5\n0\n0\n0.9\n0\nmovie2\n5\n?\n?\n0\n1.0\n0.01\nmovie3\n?\n4\n0\n?\n0.99\n0\nmovie4\n0\n0\n5\n4\n0.1\n1.0\nmovie5\n0\n0\n5\n?\n0\n0.9\n基于内容的推荐\n对每一部电影$i$抽出若干特征，然后每个用户$j$学习一个参数向量$\\theta^{(j)}$，然后用$(\\theta^{(j)})^Tx^{(i)}$来估计用户$j$对电影$i$的评分。例如对于上面的表格，我们对每一个电影抽取出2个特征$x_1,x_2$(对应表格最后2列)，然后每个用户$j$学习一个参数向量$\\theta^{(j)}\\in R^3$（包含bias项$\\theta_0=1$以及$x_1, x_2$的系数$\\theta_1, \\theta_2$）,然后就可以用$(\\theta^{(j)})^Tx^{(i)}$来预测评分。为了学习参数$\\theta$，定义代价函数为$$J(\\theta^{(1)},\\theta^{(2)},\\ldots,\\theta^{(n_u)})=\\frac{1}{2}\\sum\\limits_{j=1}^{n_u}\\sum\\limits_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\\frac{\\lambda}{2}\\sum\\limits_{j=1}^{n_u}\\sum\\limits_{k=1}^n(\\theta^{(j)}_k)^2$$\n梯度下降法的参数更新：$$\\theta_k^{(j)}=\\theta_k^{(j)}-\\alpha\\left(\\sum\\limits_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\\lambda\\theta_k^{(j)}\\right)\\quad k > 0$$ $$\\theta_k^{(j)}=\\theta_k^{(j)}-\\alpha\\sum\\limits_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}\\quad k = 0$$\n协同过滤(Collaborative Filtering)\n基于内容的推荐假设电影的特征(如$x_1$, $x_2$)是已知的，仅需要学习参数$\\theta$；然而实际中电影的特征是未知的，现在假定已知用户的参数$\\theta$，需要学习电影的特征$x$，与上面的代价函数类似，定义$$J(x^{(1)},x^{(2)},\\ldots,x^{(n_m)})=\\frac{1}{2}\\sum\\limits_{i=1}^{n_m}\\sum\\limits_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\\frac{\\lambda}{2}\\sum\\limits_{i=1}^{n_m}\\sum\\limits_{k=1}^n(x^{(i)}_k)^2$$这样我们发现，给定电影特征$x$可以学习到用户参数$\\theta$；反之给定用户参数$\\theta$可以学习到特征$x$。因此可以先随机猜一个$\\theta$，然后学习$x$，再由学习到的$x$学习$\\theta$，然后不断重复即可。然而事实上，两个参数$x, \\theta$可以如下同时更新，从而得到协同过滤的推荐算法$$J(x^{(1)},x^{(2)},\\ldots,x^{(n_m)},\\theta^{(1)},\\theta^{(2)},\\ldots,\\theta^{(n_u)})=\\frac{1}{2}\\sum\\limits_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\\frac{\\lambda}{2}\\sum\\limits_{j=1}^{n_u}\\sum\\limits_{k=1}^n(\\theta^{(j)}_k)^2+\\frac{\\lambda}{2}\\sum\\limits_{i=1}^{n_m}\\sum\\limits_{k=1}^n(x^{(i)}_k)^2$$\n协同过滤算法步骤:\n1. 初始化参数$x^{(1)},x^{(2)},\\ldots,x^{(n_m)},\\theta^{(1)},\\theta^{(2)},\\ldots,\\theta^{(n_u)}$为随机数，其中$x\\in R^n$表示电影特征，$\\theta \\in R^n$表示用户参数（注：不包含bias参数$\\theta_0$）\n2. 使用梯度下降或者其他高级优化算法，进行参数更新\n$$x_k^{(i)}=x_k^{(i)}-\\alpha\\left(\\sum\\limits_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\\lambda x_k^{(i)}\\right)$$\n$$\\theta_k^{(j)}=\\theta_k^{(j)}-\\alpha\\left(\\sum\\limits_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\\lambda\\theta_k^{(j)}\\right)$$\n3. 用学习到的参数$\\theta$和$x$预测电影评分$\\theta^Tx$\n低秩矩阵分解(Low rank matrix factorization)\n协同过滤与低秩矩阵分解：协同过滤算法要求评分矩阵$Y$中元素$y^{(i,j)}$越接近$(\\theta^{(j)})^T x^{(i)}$越好，因此参数$\\theta$和$x$的求解，实际上等价于寻找两个矩阵$X$和$\\Theta$使得$Y \\approx X\\Theta^T$，从而协同过滤问题可以转化为低秩矩阵分解问题。\n均值归一化：对于尚未评分任何电影的用户，可以对$Y$矩阵按行求平均值作为该用户的初始评分；用均值化矩阵$Y-\\mu$进行参数学习，然后用$(\\theta^{(j)})^T\\theta^{(i)}+\\mu_i$进行评分预测。\n参考文献\n[1] Andrew Ng Coursera 公开课第九周\n[2] Recommender Systems: Collaborative Filtering. http://recommender-systems.org/collaborative-filtering/\n[3] Wikipedia: Low-rank approximation https://en.wikipedia.org/wiki/Low-rank_approximation"}
{"content2":"《zw版·Halcon-delphi系列原创教程》酸奶自动分类脚本（机器学习、人工智能）\nHalcon强大的图像处理能力，令人往往会忽视其内核，是更加彪悍的机器学习、人工智能。\n分类，聚类分析，是机器学习、人工智能的核心算法之一，也是个典型的应用。\nHalcon内置的聚类分析、机器学习模块，就有：knn邻近算法、向量机SVM、GMM高斯混合模型（Gaussian Mixture Model，或者混合高斯模型，也可以简写为MOG（Mixture of Gaussian）、MLP(多层神经网络)等等。\n相关模块，基本上都是汇编级的高度优化，直接调用就可以。\n目前国内、海外机器学习、人工智能方面的学者，没有几位重视这块。\n国外，可能是版权问题，毕竟，Halcon是售价高达数万欧元（不是人民币）的商业软件，而且主要用于自控、机器视觉等工业领域，而不是大学。\n国内，可能是对于Halcon的了解不够，halcon，虽然在自控领域一家独大（70%份额），本身非常低调，很少在行业外宣传自己，也许是国人的逆向工程、D版，把德国人，也吓坏了。\n其实，图像处理的核心，图像识别、分类，都离不开机器学习、人工智能\n大家看看opencv的发展路线就可以清楚看到，从cv1.0的图像，到cv2.0、2.4的机器学习，以及目前cv3.0的GPU、cuda人工智能模块，AI在其中所占据的份额越来越大。\nHalcon因为面向一线生产线，所以很多机器学习、人工智能，都是黑箱式的，无需编程，直接调用，例如内置的ocr模块，可以识别99%的标准工业字符：超市、海关、流水线、零配件\n不过，Halcon也提供了大量的机器学习模块，毕竟各种应用场合复杂，许多库，必须进行定制。\nHalcon自带demo脚本：matching_multi_channel_yogurl.hdev\n是一个简单的机器学习、人工智能分类应用，也是个典型的应用场景\n效果还是蛮好的，大家可以看到，、，图像的角度不同，有旋转，Halcon能够轻轻松松识别。\n这个脚本，AI方面不算复杂，建模就是先拍摄几张产品的照片，直接匹配。\n通常，Halcon建模，需要进行200次（默认参数）迭代。\n选这个脚本，其中一个原因，是因为前几天，有人在论坛询问，如何对企业生产线的产品（零食好像？）进行自动分类。\n脚本80多行，很简单。\n1 * This example demonstrates shape based matching 2 * with multi channel images 3 * 4 * Init display 5 dev_update_off () 6 Mode := 'multi channel' 7 ModelColor := 'green' 8 CircleColor := 'white' 9 Names := ['Pear Apple Hazelnut','Cherry Currant','Strawberry'] 10 read_image (Image, 'color/yogurt_model_01') 11 get_image_size (Image, Width, Height) 12 dev_close_window () 13 dev_open_window (0, 0, Width, Height, 'black', WindowHandle) 14 set_display_font (WindowHandle, 14, 'mono', 'true', 'false') 15 * 16 * Part 1: create shape models 17 ModelIDs := [] 18 for Index := 1 to 3 by 1 19 read_image (Image, 'color/yogurt_model_' + Index$'02') 20 dev_display (Image) 21 * 22 * Create ROI automatically 23 access_channel (Image, Channel1, 1) 24 fast_threshold (Channel1, Region, 75, 255, 20) 25 fill_up (Region, RegionFillUp) 26 opening_circle (RegionFillUp, RegionOpening, 170.5) 27 gen_contour_region_xld (RegionOpening, Contours, 'border') 28 fit_circle_contour_xld (Contours, 'geotukey', -1, 0, 0, 3, 2, Row, Column, Radius, StartPhi, EndPhi, PointOrder) 29 gen_circle (Circle, Row, Column, Radius / 2) 30 reduce_domain (Image, Circle, ImageReduced) 31 * 32 * Create model 33 create_shape_model (ImageReduced, 6, rad(0), rad(360), 'auto', 'auto', 'ignore_color_polarity', [35,50,15], 11, ModelID) 34 ModelIDs := [ModelIDs,ModelID] 35 * 36 * Display model 37 dev_set_color (CircleColor) 38 dev_set_draw ('margin') 39 dev_set_line_width (5) 40 dev_display (Circle) 41 get_shape_model_contours (Model1Contours, ModelID, 1) 42 dev_set_color (ModelColor) 43 dev_set_line_width (2) 44 dev_display_shape_matching_results (ModelIDs, ModelColor, Row, Column, 0.0, 1, 1, ModelID) 45 disp_message (WindowHandle, 'Create shape model ' + Names[Index - 1], 'window', 12, 12, 'black', 'true') 46 disp_message (WindowHandle, 'Press \\'Run\\' to continue', 'window', 450, 12, 'black', 'true') 47 stop () 48 endfor 49 * Main loop: Find yogurt 50 for Index := 1 to 10 by 1 51 read_image (Image, 'color/yogurt_' + Index$'02') 52 * Preprocessing: Reduce search domain to speed up matching 53 access_channel (Image, Channel1, 1) 54 fast_threshold (Channel1, Region, 50, 255, 20) 55 fill_up (Region, RegionFillUp) 56 erosion_rectangle1 (RegionFillUp, RegionErosion, 210, 210) 57 reduce_domain (Image, RegionErosion, ImageReduced) 58 * Find yogurt 59 find_shape_models (ImageReduced, ModelIDs, rad(0), rad(360), 0.80, 1, 0.5, 'least_squares', 0, 0.95, Row, Column, Angle, Score, Model) 60 * 61 * Display results 62 dev_display (Image) 63 gen_circle (Circle, Row, Column, Radius / 2) 64 dev_set_color (CircleColor) 65 dev_set_line_width (5) 66 dev_display (Circle) 67 get_shape_model_contours (ModelContours, Model, 1) 68 dev_set_color (ModelColor) 69 dev_set_line_width (2) 70 dev_display_shape_matching_results (ModelIDs, ModelColor, Row, Column, Angle, 1, 1, Model) 71 disp_message (WindowHandle, Names[find(ModelIDs,Model)] + ' found', 'window', 12, 12, 'black', 'true') 72 disp_message (WindowHandle, 'Score ' + Score, 'window', 50, 12, 'black', 'true') 73 if (Index < 10) 74 disp_continue_message (WindowHandle, 'black', 'true') 75 stop () 76 endif 77 endfor 78 * 79 * Cleanup memory 80 clear_shape_model (ModelIDs[0]) 81 clear_shape_model (ModelIDs[1]) 82 clear_shape_model (ModelIDs[2])\n【《zw版·Halcon-delphi系列原创教程》,网址，cnblogs.com/ziwang/】"}
{"content2":"接触机器学习1年多了，由于只会用C#堆代码，所以只关注.NET平台的资源，一边积累，一边收集，一边学习，所以在本站第101篇博客到来之际，分享给大家。部分用过的 ，会有稍微详细点的说明，其他没用过的，也是我关注的，说不定以后会用上。机器学习并不等于大数据或者数据挖掘，还有有些区别，有些东西可以用来处理大数据的问题或者数据挖掘的问题，他们之间也是有部分想通的，所以这些组件不仅仅可以用于机器学习，也可以用于数据挖掘相关的。\n按照功能把资源分为3个部分，开源综合与非综合类，以及其他网站博客等资料。都是能够在.NET平台使用的。谢谢大家支持，这些组件我日后肯定也会研究其使用，到时候有心得再分享上来。如果有兴趣，可以关注本博客。\n本文原文地址：http://www.cnblogs.com/asxinyu/p/4422050.html\n1.开源综合类\n1.1 AForge.NET\nAForge.NET是一个专门为开发者和研究者基于C#框架设计的，他包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，模糊系统，机器人控制等领域。这个框架由一系列的类库组成。主要包括有：\nAForge.Imaging —— 一些日常的图像处理和过滤器\nAForge.Vision —— 计算机视觉应用类库\nAForge.Neuro —— 神经网络计算库AForge.Genetic -进化算法编程库\nAForge.MachineLearning —— 机器学习类库\nAForge.Robotics —— 提供一些机器学习的工具类库\nAForge.Video —— 一系列的视频处理类库\nAForge.Fuzzy —— 模糊推理系统类库\nAForge.Controls—— 图像，三维，图表显示控件\n来自：http://baike.haosou.com/doc/1786119-1888850.html\n官方网站：http://www.aforgenet.com/\n我个人认为这个是.NET平台机器学习和数据挖掘发展时间最长，最好，最全面的开源.NET组件之一。博客园有很多园友写过专门的使用文章。我本人也只是关注，还没有使用，因为方向和处理的问题不一样，暂时还没有实际应用。源代码，案例等都非常全面。\n1.2 Accord.NET Framework\nAccord.NET Framework是在AForge.NET基础上封装和进一步开发来的。功能也很强大，因为AForge.NET更注重与一些底层和广度，而Accord.NET Framework更注重与机器学习这个专业，在其基础上提供了更多统计分析和处理函数，包括图像处理和计算机视觉算法，所以侧重点不同，但都非常有用。\n官方网站：http://accord-framework.net/\n1.3 Math.NET\n不管是机器学习还是数据挖掘，都与数学离不开关系，既然是在.NET平台，那么这个组件以后你也许用得上。Math.NET是.NET平台下最全面的数学计算组件之一，基础功能非常完善。我的博客有对这个组件的详细研究：http://www.cnblogs.com/asxinyu/p/4329737.html 。当然更多的功能还得大家自己使用中发掘，毕竟提供了源代码。Math.NET初衷是开源建立一个稳定并持续维护的先进的基础数学工具箱，以满足.NET开发者的日常需求。目前该组件主要分为以下几个子项目，该组件同时也支持Mono，而且支持的平台也非常广泛。Math.NET Numerics是核心功能是数值计算。主要是提供日常科学工程计算相关的算法，包括一些特殊函数，线性代数，概率论，随机函数，微积分，插值，最优化等相关计算功能。详细的介绍和使用可以参考本站的菜单“Math.NET”，查看目录。\n官方网站：http://www.mathdotnet.com/\n1.4 Infer.NET\n好吧，上面说的那些很强大，强大一方面是说包括的面广，一方面是代码，注释，资源，案例也很完善。如果说上面那些是大炮，那么我认为这个Infer.NET就是战斗机，零零散散接触和研究，以及翻译它的文档代码已经有5个月了，时间越久，越感觉到它的火力之强大。我博客已经发表了2篇翻译的文档：http://www.cnblogs.com/asxinyu/p/4329742.html，请关注。\nInfer.NET是微软剑桥研究院基于.NET平台开发的一款机器推理组件，该组件的采用的是Microsoft Research License Agreement 授权，Non-Commercial Use Only.Infer.NET是一个概率图模型中(graphical models)用于运行贝叶斯推理机(Bayesian inference)的框架。如果对概率图模型或者贝叶斯推理的意义不了解，你可以参考一下相关资源文件，在Resources and References page页面。Infer.NET为各种应用程序所需要推理提供了先进的消息传递算法和统计程序。Infer.NET更关注与概率图编程或者贝叶斯理论的相关应用。这个随机因素和不确定世界中的很多问题，都可以适用，所以他的强大一方面是专注，另一方面是提供的建模语言。与其他的组件不同，其他组件是算法级，而Infer.NET是建模级别，附带了各种通用和常见的推理算法。可以通过简单的代码来创建模型，按照微软的话说是MSL建模语言，这也是这个组件让我肃然起敬的地方，估计也只有微软的研究人员才会想到这么干一劳永逸的事情。\n官方网站：http://research.microsoft.com/en-us/um/cambridge/projects/infernet/default.aspx\n1.5 numl\n另外一个小巧的，包含比较多的机器学习算法类库，支持监督式和非监督式学习。支持很多常见的机器学习算法，文档资源还不错。包括Cluster,KMeans,PCA,DecisionTree,KNN,NaiveBayes,NeuralNetwork等学习算法，内容也非常丰富，功能强大，同时也包括一些数值计算的实现。这个组件个人认为没有以上的那么复杂，结构小巧合理，代码也很优雅。看看下面这段代码，很快就可以构建一个决策树学习器进行预测：\n1 var generator = new DecisionTreeGenerator(); 2 generator.Descriptor = Descriptor.Create<Tennis>(); 3 generator.SetHint(false); 4 5 Tennis[] tennis = TennisData.GetData(); 6 7 var learned = Learner.Learn(tennis, 0.80, 1000, generator); 8 9 IModel model = learned.Model; 10 double accuracy = learned.Accuracy; 11 12 Tennis t = new Tennis 13 { 14 Outlook = Outlook.Sunny, 15 Temperature = Temperature.High, 16 Windy = false 17 }; 18 19 Tennis predictedVal = model.Predict(t);\nnuml的入门案例和文档比较全面，如果本身对算法比较了解，熟悉C#，那入门应该不是问题。并且可以通过组件本身构建和解决更加复杂的问题。\n官方网站：http://numl.net/\n1.6 Alglib\nALGLIB是一个跨平台的数值分析和数据处理函数库，该函数库包括开源版本和商业版本。它支持多种编程语言，如C++，C#，Pascal，VBA等，可以在多个操作系统平台上运行，如：Windows，Linux和Solaris。ALGLIB有以下特点：\n（1）线性代数（包括矩阵分析）；\n（2）方程求解（线性和非线性）；\n（3）插值；\n（4）最优化；\n（5）快速傅里叶变换；\n（6）数值积分；\n（7）线性和非线性最小二乘拟合；\n（8）常微分方程求解；\n（9）特殊函数；\n（10）统计（描述统计、假设检验）；\n（11）数据分析（分类、回归、神经网络）；\n官方网站：http://www.alglib.net/\n2.开源.NET平台非综合类\n2.1 Adaboost算法\n1.https://github.com/bgorven/Classifier\n2.https://github.com/ElmerNing/Adaboost\n2.2 Apriori算法\n1.https://github.com/Omar-Salem/Apriori-Algorithm\n2.https://github.com/simonesalvo/apriori\n2.3 PageRank算法\nhttps://github.com/archgold/pagerank\n2.4 NativeBayes(朴素贝叶斯)算法\n1.https://github.com/Rekin/Naive-Bayes-Classifier\n2.https://github.com/ArdaXi/Bayes.NET\n3.https://github.com/amrishdeep/Dragon\n4.https://github.com/joelmartinez/nBayes\n2.5 kmeans算法\nhttp://visualstudiomagazine.com/articles/2013/12/01/k-means-data-clustering-using-c.aspx\n3.其他资源与技术博客\n【资源】108个大数据文档PDF开放下载-整理后打包下载，虽然是大数据的相关资料，主要是PPT等，但也有和机器学习有一点关系，需要的看看；\n白话贝叶斯理论及在足球比赛结果预测中的应用和C#实现【附资料】 里面有贝叶斯相关的论文资料，文章本身对朴素贝叶斯的原理也介绍得非常清楚；\n数据挖掘领域十大经典算法初探\n机器学习10大经典算法\n支持向量机通俗导论（理解SVM的三层境界）\n从决策树学习谈到贝叶斯分类算法、EM、HMM\n自然语言处理博客 ，包含的内容非常多，可能理论性有点强 http://www.52nlp.cn/\n西北工业大学博导聂飞平博客：http://www.escience.cn/people/fpnie/index.html\n一个机器学习数据挖掘的博客，有不少资源链接：http://www.zhizhihu.com/\nhttp://mlg.eng.cam.ac.uk/mlss09/schedule.htm\n一个机器学习资源集中平台  http://www.kernel-machines.org/software\n算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)\n最大熵模型介绍\n概率图模型\n博客园Bobby0322的博客：http://www.cnblogs.com/Bobby0322/p/4052495.html 中的商务智能与数据挖掘应用系列文章:\n《BI那点儿事》数据挖掘初探\n《BI那点儿事》数据挖掘的主要方法\n《BI那点儿事》浅析十三种常用的数据挖掘的技术\n《BI那点儿事》数据挖掘与相关领域的关系\n《BI那点儿事》Microsoft 关联算法\n《BI那点儿事》Microsoft 聚类分析算法\n《BI那点儿事》Microsoft 聚类分析算法——三国人物身份划分\n《BI那点儿事》Microsoft 决策树算法\n《BI那点儿事》Microsoft 决策树算法——找出三国武将特性分布，献给广大的三国爱好者们\n《BI那点儿事》Microsoft 线性回归算法\n《BI那点儿事》Microsoft 逻辑回归算法\n《BI那点儿事》Microsoft 逻辑回归算法——预测股票的涨跌\n《BI那点儿事》Microsoft Naive Bayes 算法\n《BI那点儿事》Microsoft Naive Bayes 算法——三国人物身份划分\n《BI那点儿事》Microsoft 神经网络算法\n《BI那点儿事》Microsoft 顺序分析和聚类分析算法\n《BI那点儿事》Microsoft 时序算法\n《BI那点儿事》Microsoft 时序算法——验证神奇的斐波那契数列\n《BI那点儿事》数据挖掘各类算法——准确性验证\n4.我的100篇博客之路\n从2009年8月1日注册博客园开始，已经有5年多的时间了。这是博客的第100篇正式随笔文章。在2015年元旦的时候，看着自己的博客很久没有更新，只有40多篇文章，然后列出了一个写作计划，初期是至少完成高质量的文章50篇左右。而到现在只有4个月，没想到我几乎完成了全年的目标。当然发表的50多篇文章中，我认为高质量和有意义的可能只有40篇，但丝毫没关系，至少还有很多时间。这些文章是对自己经历和知识的总结，也是一个提高。在这100篇博客里程碑到来的时候，我简单的回顾了一下这100篇文章。\n第1篇首日浏览量到1000的文章：\nXCode使用记录—使用XCode自动向数据库插入测试数据(2012-04-25 09:11)\n第1篇首日浏览量到3000的文章：\n拥有自己的代码生成器—Newlife.XCode模板编写教程 (2012-05-11 08:35)\n第1篇 上博客园头条的文章：\n挑战ORM性能——Newlife.XCode下500万sqlite数据库的分页(2012-08-22 12:22)\n第1篇 推荐超过60的文章：\n【原创】开源Word读写组件DocX介绍与入门 (2013-02-22 10:35)  24\n第1篇 推荐超过80的文章：\n【5.1送礼】国内第一部Matlab和C#.Net混合编程视频教程【免费】 (2014-04-29 08:02)\n第1篇 总浏览量超1.6万的文章：\n【吐槽】VS2012的安装项目只能用InstallShield Limited Edition (2013-09-07 11:20)\n在所有的100篇随笔中，有13篇是目录和链接汇总，不能算是写的随笔，还有9篇文章是刚开始来博客园的时候，还在学习，技术含量不高。但我也没删除，毕竟是一段历史。加上有2篇关于比特币和源码的文章，准确的说不是我写的，大部分是@大石头的内容，还有2篇资源和百度吐槽是很随意临时写的，根本没打算发表在首页，只是做一个记录。所以实际比较有技术一点的文章或者心得数量是73篇。这73篇文章中:\n在个人认为还不错的文章中有至少15 篇上了博客园头条(包括“最多推荐”和“最多评论”以及“编辑推荐”)\n1.白话贝叶斯理论及在足球比赛结果预测中的应用和C#实现【附资料】\n2.你用过这种奇葩的C#注释吗？如何看待 (2015-04-17 10:04)\n3.【原创】C#玩高频数字彩快3的一点体会 (2015-04-11 09:03)\n4.【踩坑经历】一次Asp.NET小网站部署踩坑和解决经历 (2015-04-01 06:10)\n5.【分享】博客美化(4)为博客添加一个智能的文章推荐插件 (2015-03-24 07:55)\n6.【原创】Newlife.XCode的常见功能使用(一）查询与数据初始化 (2015-01-26 08:52)\n7.【原创】开源Math.NET基础数学类库使用(13)C#实现其他随机数生成器 (2015-03-18 08:32)\n8.【反传销】传销故事总结—如何尽可能保护自身和家人安全 (2015-03-09 07:37)\n9.【反传销】春节一个短暂误入传销和脱身的真实故事以及对技术的思考 (2015-03-03 06:10)\n10.App乱世，3721离我们有多远 (2015-02-10 09:24)\n11.【原创】开源Word读写组件DocX介绍与入门 (2013-02-22 10:35)\n12.【原创】C#开源轻量级对象数据库NDatabase介绍 (2013-02-20 09:35)\n13.【原创】.NET开源压缩组件介绍与入门 (2013-03-05 07:59)\n14.【5.1送礼】国内第一部Matlab和C#.Net混合编程视频教程【免费】 (2014-04-29 08:02)\n另外还有一篇文章被博客园作为编辑推荐文章：\n15.【原创】Matlab.NET混合编程技巧之直接调用Matlab内置函数\n总的来说，文章是非常高效和得到大家的认可的，虽然技术含量不是特别高级，但可能基础的技术更多的能引起共鸣吧。我想说的是，每一篇文章都是经过很用心的编辑和写出来的，结果也是非常理想的，得到了很多人的支持和理解，所以才有了如此高效的访问量和推荐以及评论。"}
{"content2":"AS WE ALL KNOW，学机器学习的一般都是从python+sklearn开始学，适用于数据量不大的场景（这里就别计较“不大”具体指标是啥了，哈哈）\n数据量大了，就需要用到其他技术了，如：spark, tensorflow，当然也有其他技术，此处略过一坨字...\n先来看看如何让这3个集成起来吧（WINDOWS环境）：pycharm(python开发环境), pyspark.cmd(REPL命令行接口), spark(spark驱动、MASTER等)\ndownload Anaconda, latest version, which 64bit support for windows, 这里必须安装64位版本的Anaconda，因为后面tensorflow只支持64位的\nhttps://www.continuum.io/downloads/\n安装Anaconda，都是默认选项就行\ndowload pycharm from jetbrain site, and install (please do it by yourself)，这个很简单，直接略过\n接下来是下载spark，我下的是最新版2.1.0的 http://spark.apache.org/downloads.html\n解压缩后把它复制到一个容易找的目录，我这是C:\\spark-2.1.0-bin-hadoop2.7\n这个时候如果直接双击bin下的spark-shell.cmd文件的话是会报错的，主要原因是没有winutils.exe这东西(用来在windows环境下模拟文件操作的)，因此还需要做几个小步骤才能正常启动\n1. 设置一个假的hadoop目录，在这个目录的bin下放刚才说的那个winutils.exe文件(需要自己创建bin目录)\n2. 设置环境变量HADOOP_HOME，值为这个假的hadoop目录\n3. 拷贝winutils.exe到这个bin里，下载\nOK，这时可以双击spark-shell.cmd了，如下：\nHOHO, ==，==，我们不是要搞PYTHON环境嘛，怎么搞scala了，别急，先搞scala是因为先要把基本的给走通，再去搞python环境的接口。\npython接口的REPL是这个文件，pyspark.cmd，双击，也报错...\n别急，这里是因为python版本问题，anaconda最新版的python解释器版本是3.6.1，这个版本的spark不支持，需要降低版本 到3.5\n卸载python? 不用，用anaconda的环境切换就行了\n1. 先创建一个新的开发环境: conda create -n my_new_env_python35\n2. 激活这个新的开发环境: activate my_new_env_python35\n3. 在这个新的开发环境中安装python 3.5: conda install python=3.5\n这时python3.5版本的解释器就算是安装完成了，默认目录在C:\\ProgramData\\Anaconda3\\envs\\my_new_env_python35\\python.exe\n然后就是需要把spark的python支持包复制到相应的路径中了，从下复制到my_new_env_python35环境的Lib\\site-packages目录下\n接下来需要把python默认版本改成python3.5，需要修改PATH路径，把python3.5的路径放在第一个查找路径下就行了\n然后就开始整pycharm开发环境了。\n首先肯定是新建一个python项目了，然后改设置，用来指定python解释器的路径，菜单：File-->Settings\n接着设置运行时候的配置参数\n漏了python调用pyspark的代码了，代码如下：\nimport sys from operator import add from pyspark import SparkContext if __name__ == \"__main__\": sc = SparkContext(appName=\"PythonWordCount\") lines = sc.textFile('words.txt') count=lines.count() print(count) counts = lines.flatMap(lambda x: x.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(add) output = counts.collect() for (word, count) in output: print(\"%s: %i\" % (word, count)) sc.stop()\n至此，python环境算是搞定了。"}
{"content2":"一般情况下我们人类大脑可以在没有明确指示的情况下处理绝大部分问题。例如，你做房产经纪时间很长，你对于房产的合适定价、它的最佳营销方式以及哪些客户会感兴趣等等都会有一种本能般的“感觉”。强人工智能（Strong AI）研究的目标就是要让计算机能这样思考。\n但是目前的机器学习算法还没有那么好——它们只能专注于非常特定的、有限的问题。也许在这种情况下，“机器学习”更贴切的定义是“在少量范例数据的基础上找出一个等式来解决特定的问题”。\n不幸的是，“机器在少量范例数据的基础上找出一个等式来解决特定的问题”这个名字太绕口。所以最后我们用“机器学习”取而代之。\n让我们开始写代码吧！\n前面例子中评估房价的程序，你打算怎么写呢？可以先思考一下。\n如果你对机器学习一无所知，很有可能你会尝试写出一些基本规则来评估房价，如下：\ndef estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = 0 # In my area, the average house costs $200 per sqft price_per_sqft = 200 if neighborhood == \"hipsterton\": # but some areas cost a bit more price_per_sqft = 400 elif neighborhood == \"skid row\": # and some areas cost less price_per_sqft = 100 # start with a base price estimate based on how big the place is price = price_per_sqft * sqft # now adjust our estimate based on the number of bedrooms if num_of_bedrooms == 0: # Studio apartments are cheap price = price — 20000 else: # places with more bedrooms are usually # more valuable price = price + (num_of_bedrooms * 1000) return price\n假如你像这样瞎忙几个小时，也许会取得一点成效，但是你的程序永不会完美，而且当价格变化时很难维护。\n如果能让计算机找出实现上述函数功能的办法，这样岂不更好？只要返回的房价数字正确，谁会在乎函数具体干了些什么呢？\ndef estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = <computer, plz do some math for me> return price\n考虑这个问题的一种角度是将房价看做一碗美味的汤，而汤中成分就是卧室数、面积和地段。如果你能算出每种成分对最终的价格有多大影响，也许就能得到各种成分混合起来形成最终价格的具体比例。\n这样可以将你最初的程序（全是疯狂的if else语句）简化成类似如下的样子：\ndef estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = 0 # a little pinch of this price += num_of_bedrooms * .841231951398213 # and a big pinch of that price += sqft * 1231.1231231 # maybe a handful of this price += neighborhood * 2.3242341421 # and finally, just a little extra salt for good measure price += 201.23432095 return price\n请注意那些用粗体标注的神奇数字——.841231951398213, 1231.1231231,2.3242341421, 和201.23432095。它们称为权重。如果我们能找出对每栋房子都适用的完美权重，我们的函数就能预测所有的房价！\n找出最佳权重的一种笨办法如下所示：\n步骤1：\n首先，将每个权重都设为1.0：\ndef estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood): price = 0 # a little pinch of this price += num_of_bedrooms * 1.0 # and a big pinch of that price += sqft * 1.0 # maybe a handful of this price += neighborhood * 1.0 # and finally, just a little extra salt for good measure price += 1.0 return price\n步骤2：\n将每栋房产带入你的函数运算，检验估算值与正确价格的偏离程度：\n运用你的程序预测房屋价格。\n例如：上表中第一套房产实际成交价为25万美元，你的函数估价为17.8万，这一套房产你就差了7.2万。\n再将你的数据集中的每套房产估价偏离值平方后求和。假设数据集中有500套房产交易，估价偏离值平方求和总计为86,123,373美元。这就反映了你的函数现在的“正确”程度。\n现在，将总计值除以500，得到每套房产的估价偏离平均值。将这个平均误差值称为你函数的代价。\n如果你能调整权重使得这个代价变为0，你的函数就完美了。它意味着，根据输入的数据，你的程序对每一笔房产交易的估价都是分毫不差。而这就是我们的目标——尝试不同的权重值以使代价尽可能的低。\n步骤3：\n不断重复步骤2，尝试所有可能的权重值组合。哪一个组合使得代价最接近于0，它就是你要使用的，你只要找到了这样的组合，问题就得到了解决!\n头脑风暴时间\n这太简单了，对吧？想一想刚才你做了些什么。你取得了一些数据，将它们输入至三个通用的简单步骤中，最后你得到了一个可以对你所在区域的房屋进行估价的函数。\n但是下面的事实可能会扰乱你的思想：\n1.过去40年来，很多领域（如语言学/翻译学）的研究表明，这种通用的权重运算方式的学习算法已经胜过了需要利用真人明确规则的方法。机器学习的“笨”办法最终打败了人类专家。\n2.你最后写出的函数真是笨，它甚至不知道什么是“面积”和“卧室数”。它知道的只是搅动，改变数字来得到正确的答案。\n3.很可能你都不知道为何一组特殊的权重值能起效。所以你只是写出了一个你实际上并不理解却能证明的函数。\n4.试想一下，你的程序里没有类似“面积”和“卧室数”这样的参数，而是接受了一组数字。假设每个数字代表了你车顶安装的摄像头捕捉的画面中的一个像素，再将预测的输出不称为“价格”而是叫做“方向盘转动度数”，这样你就得到了一个程序可以自动操纵你的汽车了！\n太疯狂了，对吧？\n步骤3中的“尝试每个数字”怎么回事？\n好吧，当然你不可能尝试所有可能的权重值来找到效果最好的组合。那可真要花很长时间，因为要尝试的数字可能无穷无尽。\n为避免这种情况，数学家们找到了很多聪明的办法（维基百科）来快速找到优秀的权重值，而不需要尝试过多。下面是其中一种：\n首先，写出一个简单的等式表示前述步骤2：\n接着，让我们将这同一个等式用机器学习的数学术语（现在你可以忽略它们）进行重写：\nθ表示当前的权重值。 J(θ) 意为“当前权重值对应的代价”。\n这个等式表示我们的估价程序在当前权重值下偏离程度的大小。\n如果将所有赋给卧室数和面积的可能权重值以图形形式显示，我们会得到类似下图的图表：\n事实上这个图在以后的学习中会遇到，我们一般给它取名叫损失函数与参数之间的曲面图\n图中蓝色的最低点就是代价最低的地方——即我们的程序偏离最小。最高点意味着偏离最大。所以，如果我们能找到一组权重值带领我们到达图中的最低点，我们就找到了答案！\n因此，我们只需要调整权重值使我们在图上能向着最低点“走下坡路”。如果对于权重的细小调节能一直使我们保持向最低点移动，那么最终我们不用尝试太多权重值就能到达那里。\n如果你还记得一点微积分的话，你也许记得如果你对一个函数求导，结果会告诉你函数在任一点的斜率。换句话说，对于图上给定一点，它告诉我们那条路是下坡路。我们可以利用这一点朝底部进发。\n所以，如果我们对代价函数关于每一个权重求偏导，那么我们就可以从每一个权重中减去该值。这样可以让我们更加接近山底。一直这样做，最终我们将到达底部，得到权重的最优值。（读不懂？不用担心，接着往下读）。\n这种找出最佳权重的办法被称为批量梯度下降，上面是对它的高度概括。如果想搞懂细节，不要害怕，我们在后面会继续学习。\n先简单发一张损失函数的等高线图来镇楼：\n当你使用机器学习算法库来解决实际问题，所有这些都已经为你准备好了。但明白一些具体细节总是有用的。\n还有什么你随便就略过了？\n其时上面整个内容介绍的就是机器学习中的一种思想：多元线性回归，一种深度学习算法。即根据多个数据样本生成一个问题解决方案。\n这边的内容需要有一定的入门知识，我打算按照这个方向学习下去（加个好友一起学习啊）。\n机器学习法力无边吗？\n一旦你开始明白机器学习技术很容易应用于解决貌似很困难的问题（如手写识别），你心中会有一种感觉，只要有足够的数据，你就能够用机器学习解决任何问题。只需要将数据输入进去，就能看到计算机变戏法一样找出拟合数据的等式。\n但是很重要的一点你要记住，机器学习只能对用你占有的数据实际可解的问题才适用。\n例如，如果你建立了一个模型来根据每套房屋内盆栽数量来预测房价，它就永远不会成功。房屋内盆栽数量和房价之间没有任何的关系。所以，无论它怎么去尝试，计算机也推导不出两者之间的关系。\n怎样深入学习机器学习\n打算采用Andrew Ng的网页教程UFLDL Tutorial，据说这个教程写得浅显易懂，也不太长。不过在这这之前还是复习下machine learning的基础知识，见网页：http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=DeepLearning。内容其实很短，每小节就那么几分钟，且讲得非常棒。\n入门已经结束，接下来还有好几座大山去爬。共勉！！"}
{"content2":"...................................................\n（13）按格式输出\nfor i in range(0,len(List_row)):\nif (i%2==0):\nst=List_row[i].strip().split(' ')\nprint(int(len(st)/2),file=dt)\nfor j in range(0,len(st)):\nprint(st[j],file=dt,end=' ')\nif (j%2==1):\nprint('',file=dt)\nprint('',file=dt)\ndt.close()\n...............................................................\n到此就把所有数据按特定的格式处理完了。"}
{"content2":"是在去年（2011）知道这个online class的，那时的域名是www.ml-class.org，只有machine learning的课程。记得去年的某天，我在某处（应该是网易公开课）看了一集机器学习的公开课。很感兴趣，于是找到了Andrew Ng教授的主页，最后发现了这么一个网上公开课。\n在寒假时，我把公开课里没有任何字幕的Lecture下了下来，硬着头皮看了几个章节，结果是一知半解。现在工作之余，又想好好地把这个课程学习一下，于是决定通过写学习笔记的方式来督促自己去深究。\n现在ml-class已经成为了coursera.org的一份子，各路大牛在这开设online class，涉及各类学科，都是些难得的资源。ML class的页面链接：https://class.coursera.org/ml-2012-002/class/index，现在的版本已经能够下载字幕了。\n1. Welcome | 欢迎\n在这一节中，主要介绍了机器学习技术的基本概况，它的魅力以及它的应用实例。\n虽然我们一直以来不知道机器学习为何物，但是在生活中却经常使用与这个技术相关的服务，如搜索服务、相片识别、垃圾邮件分类等等。由此可见机器学习的魅力之大。Andrew Ng说\"For me one of the reasons I'm excited is the AI dream of someday building machines as intelligent as you or me.\"这么一个AI dream，很多人会为之动心吧！\n机器学习在人工智能中起着重要的作用，它随着人工智能的发展而得以发展，可以认为它使得计算机获得了一种强大的全新的能力。应用机器学习技术的领域有：数据挖掘（Data Mining）、手写识别（Handwriting Recognition）、自然语言处理（Natural Language Processing, NLP）以及计算机视觉（Computer Vision）。\n2. What is machine learning | 机器学习是什么\n对于“什么是机器学习，什么不是”，虽然很难给出一个能够被普遍接受的定义，但是还是有些定义值得研究的。\nArthur Samuel的定义是：Field of study that gives computers the ability to learn without being explicitly programmed. 其含义为，机器学习就是研究如何不通过明确地编写程序而实现给予计算机学习能力。\nTom M. Mitchell的定义：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. 对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E中学习。（翻译来源：《机器学习》，Tom M. Mitchell，曾华军等译，机械工业出版社）\n用上述第二个定义来定义手写识别学习问题：\n任务T：识别和分类图像中的手写文字\n性能度量P：分类的正确率\n训练经验E：已知分类的手写文字数据库\n将要介绍的机器学习算法主要可分为有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。在有监督学习中，我们将用训练样本去训练计算机，相当于教它做某事，而在无监督学习中，我们放手让它自己去学习。除此之外，还有增强学习（Reinforcement Learning）和推荐系统（Recommender System）将会被介绍。这个课程还会介绍有关学习算法实际应用的建议。\n未完待续……"}
{"content2":"摘要\n本文对支持向量机做了简单介绍，并对线性可分支持向量分类机、线性支持向量分类机以及核函数做了详细介绍。\n最近一直在看《机器学习实战》这本书，因为自己本身很想深入的了解机器学习算法，加之想学python，就在朋友的推荐之下选择了这本书进行学习，今天学习支持向量机（Support Vector Machines,SVM），这个无论是在模式识别还是机器学习等领域都赫赫有名的工具。\n支持向量机是一项借助于最优化方法来解决机器学习问题的新工具，最初由 V.Vapnik 等人提出，近几年来其在理论研究和算法实现等方面都取得了很大的进展，开始成为克服“维数灾难”和过学习等困难的强有力的手段。\n一  最大间隔分隔数据\n我们知道分类的目的是学会一个分类器，将数据库中的数据映射到给定类别中的某一个，实现对未知数据类别的预测。\n对于二维数据集，将数据集分隔开的直线称为分隔超平面，如果分隔的是三维的，分类的就是面，如果是更高维的就是超平面。将分类的决策边界统称为超平面，分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据都属于另一个类别。\n那么对于分类而言，合理的构建分类器就尤为重要了，怎样才能构造分类器使得分类结果更为可信。下面举个例子说明：\n对于左图坐标中的两类图形，如果让你画一条线，合理的将它们分开，你会如何怎样划分？这分法可就多了，到底哪种分法最好呢？我们可以引入一个实际问题：假设这两类图形分别代替的是两个居民区，现要在两居民区之间修一条路，那么该怎么修？我想大多数人都会选择那条红线吧，这条路权衡了远和近，也可以理解为折中，相对于每个点而言都是最公平的。\n该例子就可以看成是在样本中寻找超平面将不同类别分开，那么为了更好的分类，就要使得不同类别之间的间隔最大，（间隔指的是点到分隔面的距离），这样如果分错或者在有限数据上训练分类器的话，也能最大程度的保证分类器的健壮。上述例子是在二维平面中寻找超平面，直接用肉眼就可以抉择，然而如果是三维或者更高维的情况，单凭人类肉眼是无能为力的，但机智的我们是可以通过计算机来寻找啊，通过相应的数学知识，建立对应的数学模型，通过计算机来求解。\n这种寻找超平面分类思想就是SVM的思想，下来我们学习支持向量机。\n二 支持向量机\n用于分类的SVM本质上是一个二类分类模型。SVM属于监督学习，目的是在给定一个包含正例和反例的样本集合中寻找一个超平面对样本中的正例和反例进行分割，同时保证正例和反例之间的间隔最大。这样使得分类结果更为可信，而且对于未知的新样本才能有更好的分类预测能力。为了达到类别之间间隔最大，我们不需要考虑所有点，只需要让离分隔超平面最近的点距离分隔面的距离尽可能远即可，想想也是很有道理的，这里距离分隔超平面最近的那些点就是支持向量。\n下面来讲述一下SVM的原理：\n首先，需要给定N个训练样本：{(x1,y1),(x2,y2)…(xn,yn)},其中x是d维向量，表明了每个样本具有d个属性；yi指的是类别，并且yi属于{-1,1}。目的是寻找一个实值函数g(x),使得可以用分类函数f(x) = sgn(g(x))推断任意一个样本x所对应的y值。\n（1）线性可分支持向量机\n线性可分SVM就是用上述的N个样本去训练学习得到一个线性分类器，也就是得到一个超平面：f(x) = sgn(w•x+b),线性可分表明当w•x+b>0时，对应的f(x) = 1,相应的当w•x+b<0时，对应的f(x) = -1,而w•x+b = 0就是所要寻找的超平面，此时对应的超平面为硬间隔超平面。接下来我们就来寻找这个超平面，基于之前的分析，这里我们需要将样本分成两类，且保证分隔面到这两类中最近的点的距离尽可能的远，下面我们结合数学公式进行分析：\n如上图所示，我们要寻找一个超平面最大的分隔这两个类，保证这两个类别之间的距离尽可能大，问题可以转化为最大化这两个类别中距离分隔面最近的点（支持向量）之间的距离。\n首先，在上图中找到两个和这个超平面平行且距离相等的超平面：w•x+b = -1和w•x+b = 1，保证在这两个超平面之间没有任何样本点，很容易想象，这两个超平面势必包含的是距离分隔超平面最近的点，那么问题就可转化为最大化这两个超平面之间的距离；进而结合相关的数学知识，因为超平面均二维，则它们之间的距离可表示为：d = |1+1|/sqrt(w12 + w22) = 2 / ||w||，问题就是最大化2 / ||w||，可以转化为最小化||w||；最后结合两个超平面之间没有任何样本点这个约束，则有：对于任何一个正样本yi=+1，它都要处于w•x+b = 1这个超平面的右边，即要保证：y= w•x+b>=+1，同理对于任何一个负样本yi=-1，它都要处于w•x+b=-1的左边，也就是要保证：y = w•x+b <=-1，于是可以合并为：yi (w•xi+b)>=1。\n于是寻找最优超平面的问题就可以转化为二次规划问题：\nmin ||w||2/2\ns.t.  yi (w•xi+b)>=1    i = 1,2,...,N\n该问题的特点是目标函数是凸函数（范数均为凸函数），并且约束条件为线性，则可以引入lagrange函数：\n进而根据wolf对偶的定义，将原问题的各变量偏导置零有：\n进而带入拉格朗日函数可将问题转化为原问题的拉格朗日对偶问题：\n求解上述问题的最优解，计算w*和b*:\n由KKT互补条件可得：\n只有当xi为支持向量的时候，对应的ai*才为正，否则皆为0，选择a*的一个正分量，计算可得：\n由此可以构造分类超平面（w*•x)+b* = 0，由此求得决策函数：\n进而得到分类函数：\n从而对未知类别进行分类。根据KKT的条件，只有当xi为支持向量的时候，对应的ai*才为正，否则皆为0。所以，我们只需求得新来的样本和支持向量的内积，然后运算即可。\n（2）线性支持向量分类机\n上面所分析的是样本点线性可分的情况，我们在寻找硬间隔超平面时，首先是找到了两个分类边界，并假定所有的样本点都在这两个分类边界以外，但现实不总是那么尽人意，下面这种情况也势必会遇到：\n这幅图里正类和负类都有点跑到“另类”的地盘，这时候就找不到一条直线将它们分开了，那要如何才能折中呢？对于这种数据点有一定程度偏离超平面的情况，我们仍然能继续使用超平面进行划分，只是这时要对间隔进行“软化” ，构造软间隔超平面。简言之就是在两个分类边界之间允许出现样本点，这类样本点被称为边界支持向量。 这种向量机成为线性支持向量分类机，如下图所示：\n上面提到需要对该问题“软化“，那么如何软化呢，就是要引入松弛变量：\n从而得到软化之后针对于原问题的约束条件为：\n松弛变量的设置，允许了某些样本点出现在对方的区域中，当松弛变量充分大时，样本点总是满足上述的约束条件，但也是要设法避免取值太大。为此我们可以重新调整目标函数，引入惩罚因子C，对离群点进行惩罚，则二次规划问题转化为：\n，其中，C>0\n对应的拉格朗日函数为：\n对应原问题的对偶问题为：\n我们发现与线性可分模型中知识多了C>=a这个约束条件，按照之前的方法，同理计算得：\n分类函数为：\n其中 C为无穷大时，就等价于线性可分的情形。\n（3） 核函数\n上面讲述的是线性支持向量分类机，其中允许一定程度上的离群点，那若是样本点真的是线型不可分呢，那就得采用核函数进行处理了。\n联系到T.M.Cover的模式可分性定理：一个复杂的模式分析问题映射到高维空间后，会比在低维空间线性可分。核方法就是通过非线性映射将原始数据通过特征映射嵌入到新的特征空间（Hilbert空间），发现数据在特征空间上的线性模式，进而选取相应的核函数利用输入计算内积。根据对偶解法可得算法所需要的信息位于特征空间上数据点之间的内积，维数过大时会影响算法的高效性，所以，将内积作为输入特征的直接函数，更高效的计算内积，减少了算法的时间复杂度。\n常用的核函数有：\n基于上述分析，对于线性不可分的样本点，问题就转化为在Hilbert空间中寻找超平面：，相应的转化为二次规划问题：\n其中核函数K满足条件：\n我们再次选用RBF核函数，得到拉格朗日对偶问题：\n相应的计算求得分类函数为：\n接下来就可以据此对线性不可分问题进行分类了。因为大多数的a*是0，所以我们只需要计算新样本和少量的训练样本的核函数，求和去符号就可完成新样本的分类了。而采用不同的核函数，相当于采用不同的相似度对样本进行分类。\n至此，关于支持向量机的相关知识大致就学习完了，之后的一些细节也将继续学习。"}
{"content2":"总体思路：\n各种类型的机器学习分类\n按照输出空间类型分Y\n按照数据标记类型分yn\n按照不同目标函数类型分f\n按照不同的输入空间类型分X\n按照输出空间类型Y，可以分为二元分类，多元分类，回归分析以及结构化学习等，这个好理解，离散的是分类，连续的是回归，到是结构化的学习接触的相对较少，以后有空可以关注下。\n按照数据标记分可以分为：\n监督；\n非监督；\n半监督；\n增强学习；\n下面这张ppt很好的总结了这点：\n这是围绕标记yn的类型进行分类的，\n监督和非监督很好理解，半监督和增强其实应用更加普遍，数据的标记大部分时候是需要人来做的，这个条件有时候很难满足（经费不足），那么半监督就有比较好的应用了。用人的学习过程来理解，人按照课本去学习，是监督，如果没有课本，按照自己发现的规律去解决问题，则是非监督，因为此，非监督学习的应用相对有限。有时候学习的事物特征到标记结果不是很好描述，例如搜索引擎的广告系统，针对不同用户信息以及query放什么广告，放在什么位置，这需要增强学习去不断的强化，让用户通过点击率反馈机器学习系统使得其不断优化，因为我们自己定义数据标记是很困难的，又例如机器学习开车。因此增强学习的关键在于反馈的存在。\n按照不同的目标函数类型f，可以分为Batch，online以及active，课程中终于和人的学习过程做类比，其实和人一类比就很好理解了：\n这三种学习类型分别可以类比为：填鸭式，老师教学以及主动问问题。\n按照输入X分的话，主要是三种：\n这里需要提到的是，把原始输入X转化为可以真正作为机器学习的输入的training examples的过程称为 feature engineering，也就是从Raw data --> concrete data的过程。\n回顾下我们在哪？\n在回答何时可以用机器学习的时候，我确实需要知道机器学习有什么类型，其实这些类型正好是围绕最后这张图而来的，确定这些类型就是逐个确定机器学习算法各个要素应该选用哪种方法的过程，只有当每一个都确定了，我们才能知道这个问题是否可以用机器学习来解决（见下，2，3，4）：\n总结：\n目前用的最多的分类是按照yn去分，课程给了个详细的分类，觉得很不错，有了全局观，后面就好易于理解了。\n参考资料：\nCoursera台大机器学习基石"}
{"content2":"17年开始，网上的机器学习教程逐渐增多，国内我所了解的就有网易云课堂、七月、小象学院和北风。他们的课程侧重点各有不同，有些侧重理论，有些侧重实践，结合起来学习事半功倍。但是论经典，还是首推吴恩达的机器学习课程。\n吴大大14年在coursera的课程通俗易懂、短小精悍，在讲解知识点的同时，还会穿插相关领域的最新动态，并向你推荐相关论文。课程10周共18节课，每个课程都有PPT和课后习题，当然，也有中文字幕。\n百度网盘（视频 + 英文字幕 + 中文字幕 + 练习 + PPT）:\n链接：https://pan.baidu.com/s/1ggWjzFH 密码：bk1g\n第一周\n一、 引言(Introduction)\n1.1 欢迎\n1.2 机器学习是什么？\n1.3 监督学习\n1.4 无监督学习\n二、单变量线性回归(Linear Regression with One Variable)\n2.1 模型表示\n2.2 代价函数\n2.3 代价函数的直观理解I\n2.4 代价函数的直观理解II\n2.5 梯度下降\n2.6 梯度下降的直观理解\n2.7 梯度下降的线性回归\n2.8 接下来的内容\n三、线性代数回顾(Linear Algebra Review)\n3.1 矩阵和向量\n3.2 加法和标量乘法\n3.3 矩阵向量乘法\n3.4 矩阵乘法\n3.5 矩阵乘法的性质\n3.6 逆、转置\n第二周\n四、多变量线性回归(Linear Regression with Multiple Variables)\n4.1 多维特征\n4.2 多变量梯度下降\n4.3 梯度下降法实践1-特征缩放\n4.4 梯度下降法实践2-学习率\n4.5 特征和多项式回归\n4.6 正规方程\n4.7 正规方程及不可逆性（选修）\n五、Octave教程(Octave Tutorial)\n5.1 基本操作\n5.2 移动数据\n5.3 计算数据\n5.4 绘图数据\n5.5 控制语句：for，while，if语句\n5.6 向量化 88\n5.7 工作和提交的编程练习\n第三周\n六、逻辑回归(Logistic Regression)\n6.1 分类问题\n6.2 假说表示\n6.3 判定边界\n6.4 代价函数\n6.5 简化的成本函数和梯度下降\n6.6 高级优化\n6.7 多类别分类：一对多\n七、正则化(Regularization)\n7.1 过拟合的问题\n7.2 代价函数\n7.3 正则化线性回归\n7.4 正则化的逻辑回归模型\n第四周\n第八、神经网络：表述(Neural Networks: Representation)\n8.1 非线性假设\n8.2 神经元和大脑\n8.3 模型表示1\n8.4 模型表示2\n8.5 样本和直观理解1\n8.6 样本和直观理解II\n8.7 多类分类\n第五周\n九、神经网络的学习(Neural Networks: Learning)\n9.1 代价函数\n9.2 反向传播算法\n9.3 反向传播算法的直观理解\n9.4 实现注意：展开参数\n9.5 梯度检验\n9.6 随机初始化\n9.7 综合起来\n9.8 自主驾驶\n第六周\n十、应用机器学习的建议(Advice for Applying Machine Learning)\n10.1 决定下一步做什么\n10.2 评估一个假设\n10.3 模型选择和交叉验证集\n10.4 诊断偏差和方差\n10.5 正则化和偏差/方差\n10.6 学习曲线\n10.7 决定下一步做什么\n十一、机器学习系统的设计(Machine Learning System Design)\n11.1 首先要做什么\n11.2 误差分析\n11.3 类偏斜的误差度量\n11.4 查准率和查全率之间的权衡\n11.5 机器学习的数据\n第7周\n十二、支持向量机(Support Vector Machines)\n12.1 优化目标\n12.2 大边界的直观理解\n12.3 数学背后的大边界分类（选修）\n12.4 核函数1\n12.5 核函数2\n12.6 使用支持向量机\n第八周\n十三、聚类(Clustering)\n13.1 无监督学习：简介\n13.2 K-均值算法\n13.3 优化目标\n13.4 随机初始化\n13.5 选择聚类数\n十四、降维(Dimensionality Reduction)\n14.1 动机一：数据压缩\n14.2 动机二：数据可视化\n14.3 主成分分析问题\n14.4 主成分分析算法\n14.5 选择主成分的数量\n14.6 重建的压缩表示\n14.7 主成分分析法的应用建议\n第九周\n十五、异常检测(Anomaly Detection)\n15.1 问题的动机\n15.2 高斯分布\n15.3 算法\n15.4 开发和评价一个异常检测系统\n15.5 异常检测与监督学习对比\n15.6 选择特征\n15.7 多元高斯分布（选修）\n15.8 使用多元高斯分布进行异常检测（选修）\n十六、推荐系统(Recommender Systems)\n16.1 问题形式化\n16.2 基于内容的推荐系统\n16.3 协同过滤\n16.4 协同过滤算法\n16.5 向量化：低秩矩阵分解\n16.6 推行工作上的细节：均值归一化\n第十周\n十七、大规模机器学习(Large Scale Machine Learning)\n17.1 大型数据集的学习\n17.2 随机梯度下降法\n17.3 小批量梯度下降\n17.4 随机梯度下降收敛\n17.5 在线学习\n17.6 映射化简和数据并行\n十八、应用实例：图片文字识别(Application Example: Photo OCR)\n18.1 问题描述和流程图\n18.2 滑动窗口\n18.3 获取大量数据和人工数据\n18.4 上限分析：哪部分管道的接下去做\n十九、总结(Conclusion)"}
{"content2":"人工智能（AI）—— 为机器赋予人的智能\n机器学习一种实现人工智能的方法\n深度学习一种实现机器学习的技术\n人工智能的知识图谱\nAI、ML和DL的关系\n由以上的关系图我们可以看出，总结的说就是\n机器学习是实现人工智能的一种方法,深度学习是机器学习一个分支\n机器学习的必要性\n很多软件无法靠人工编程:自动驾驶、计算机视觉、自然语言处理\n人类常会犯错(比如紧张、累了、困了),机器不容易犯错\n关于机器学习的定义有很多，比较晦涩的一个定义是\n“晦涩”的机器学习定义\n◆对某类任务T(Task)和性能度量P( Performance)\n◆通过经验E( Experience)改进后\n◆在任务T上由性能度量P衡量的性能有所提升\n简单的机器学习定义是：\n简单的机器学习定义\n◆机器学习:用数据来解答问题\n◆数据对应训练\n◆解答问题对应推测\n简单的说，机器学习相当于对人们难以处理的数据，使用机器进行一系列的处理之后，找到一个判定函数或者是模型，当再次遇到这种情况的时候，机器可以通过学习到的\n模型或者是与之相对应的函数进行一系列的人们难以进行的操作。\n机器学习的“关键三步”\n找一系列函数来实现预期的功能:建模问题\n找一组合理的评价标准,来评估函数的好坏:评价问题\n快速找到性能最佳的函数:优化问题（比如梯度下降就是这个目的）\n什么是深度学习？\n深度学习是机器学习的一个分支，是实现机器学习的一个方法。笼统的说就是，基于深度神经网络的学习研究就是深度学习。\n关于神经网络与深度神经网络？\n神经网络一般由输入层，隐含层，输出层三层结构组成，简单的神经网络是只有一个输入层，一个隐含层，一个输出层组成；深度神经网络是由一个输入层，多个隐含层，\n以及一个输出层组成。\n深度学习为什么兴起？\n随着科技的发展，人工智能的兴起，数据量不断的增加，相应的神经网络也在不断地发展。\n+++++++++++++++++++++++++++++++未+++++++ ++++++++++++++++"}
{"content2":"1、机器学习偏向于学习，对潜在的规律分析完成后，再对未来进行预测。\n2、模式识别主要偏工程应用，是对具体的环境和客体的“模式”进行识别。\n3、数据挖掘主要偏应用，是对历史数据进行分析并发现规律，注重数据的价值体现。\n4、人工智能主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作，它涵盖了机器学习、模式识别、数据挖掘等技术。"}
{"content2":"本节内容 预备资料:\n1.FFmpeg:\n链接：https://pan.baidu.com/s/1jonSAa_TG2XuaJEy3iTmHg\n密码：w6hk\n2.baidu-aip:\npip install baidu-aip\n终于进入主题了,此篇是人工智能应用的重点,只用现成的技术不做底层算法,也是让初级程序员快速进入人工智能行业的捷径\n目前市面上主流的AI技术提供公司有很多,比如百度,阿里,腾讯,主做语音的科大讯飞,做只能问答的图灵机器人等等\n这些公司投入了很大一部分财力物力人力将底层封装,提供应用接口给我们,尤其是百度,完全免费的接口\n既然百度这么仗义,咱们就不要浪费掉怎么好的资源,从百度AI入手,开启人工智能之旅\n开启人工智能技术的大门 : http://ai.baidu.com/\n看看我大百度的AI大法,这些技术全部都是封装好的接口,看着就爽\n接下来咱们就一步一步的操作一下\n首先进入控制台,注册一个百度的账号(百度账号通用)\n开通一下我们百度AI开放平台的授权\n然后找到已开通服务中的百度语音\n走到这里,想必已经知道咱们要从语音入手了,语音识别和语音合成\n打开百度语音,进入语音应用管理界面,创建一个新的应用\n创建语音应用App\n就可以创建应用了,回到应用列表我们可以看到已创建的应用了\n这里面有三个值 AppID , API Key , Secret Key 记住可以从这里面看到 , 在之后的学习中我们会用到\n好了 百度语音的应用已经创建完成了 接下来 我会用Python 代码作为实例进行应用及讲解\n一.安装百度的人工智能SDK:\n首先咱们要 pip install baidu-aip 安装一个百度人工智能开放平台的Python SDK实在是太方便了,这也是为什么我们选择百度人工智能的最大原因\n安装完成之后就来测试一下:\n在工程目录下,就可以看到 s1.mp3 这个文件了,来听一听\n上面咱们测试了一个语音合成的例子,那么就从语音合成开始入手\n二.语音合成:\n技术上,代码上任何的疑惑,都可以从官方文档中得到答案\nbaidu-aip Python SDK 语音合成技术文档 : https://ai.baidu.com/docs#/TTS-Online-Python-SDK/top\n刚才我们做了一个语音合成的例子,就用这个例子来展开说明\n先来看第一段代码\n这是与百度进行一次加密校验 , 认证你是合法用户 合法的应用\nAipSpeech 是百度语音的客户端 认证成功之后,客户端将被开启,这里的client 就是已经开启的百度语音的客户端了\n再来看第二段代码:\n用百度语音客户端中的synthesis方法,并提供相关参数\n成功可以得到音频文件,失败则返回一段错误信息\n重点看一下 synthesis 这个方法 , 从 https://ai.baidu.com/docs#/TTS-Online-Python-SDK/top 来获得答案吧\n从参数入手分析:\n按照这些参数,从新发起一个语音合成\n这次声音是不是与一点点萝莉了呢?\n这都是语音语调的作用 0 - 9 其实就是 御姐音 - 萝莉音\n这就是人工智能中的语音合成技术,调用百度的SDK,只用了5分钟,完成了1年的开发量,哈哈哈哈\n一定要自己练习一下语音合成, 别把它玩儿坏了\n三.语音识别:\n哎,每次到这里,我都默默无语泪两行,声音这个东西格式太多样化了,如果要想让百度的SDK识别咱们的音频文件,就要想办法转变成百度SDK可以识别的格式PCM\n目前DragonFire已知可以实现自动化转换格式并且屡试不爽的工具 : FFmpeg 这个工具的下载地址是 : 链接：https://pan.baidu.com/s/1jonSAa_TG2XuaJEy3iTmHg 密码：w6hk\nFFmpeg 环境变量配置:\n首先你要解压缩,然后找到bin目录,我的目录是 C:\\ffmpeg\\bin\n然后 以 windows 10 为例,配置环境变量\n如果没搞明白的话,我也没有办法了,这么清晰这么明白\n尝试一下,是否配置成功\n看到这个界面就算配置成功了,配置成功有什么用呢, 这个工具可以将wav wma mp3 等音频文件转换为 pcm 无压缩音频文件\n做一个测试,首先要打开windows的录音机,录制一段音频(说普通话)\n现在假设录制的音频文件的名字为 audio.wav 放置在 D:\\DragonFireAudio\\\n然后我们用命令行对这个 audio.wav 进行pcm格式的转换然后得到 audio.pcm\n命令是 : ffmpeg -y  -i audio.wav  -acodec pcm_s16le -f s16le -ac 1 -ar 16000 audio.pcm\n然后打开目录就可以看到pcm文件了\npcm文件已经得到了,赶紧进入正题吧\n百度语音识别SDK的应用:\n前提是你的audio.pcm 要与你当前的文件在同一个目录,还是分段看一下代码\n读取文件的内容,file_context 是 audio.pcm 文件打开的二进制流\nasr函数需要四个参数,第四个参数可以忽略,自有默认值,参照一下这些参数是做什么的\n第一个参数: speech 音频文件流 建立包含语音内容的Buffer对象, 语音文件的格式，pcm 或者 wav 或者 amr。(虽说支持这么多格式,但是只有pcm的支持是最好的)\n第二个参数: format 文件的格式,包括pcm（不压缩）、wav、amr (虽说支持这么多格式,但是只有pcm的支持是最好的)\n第三个参数: rate 音频文件采样率 如果使用刚刚的FFmpeg的命令转换的,你的pcm文件就是16000\n第四个参数: dev_pid 音频文件语言id 默认1537（普通话 输入法模型）\n再来看下一段代码,打印返回结果:\n成功的dict中 result 就是我们要的识别文本\n失败的dict中 err_no 就是我们要的错误编码,错误编码代表什么呢?\n如果err_no不是0的话,就参照一下错误码表\n到此百度AI语音部分的调用就结束了,是不是感觉很简单\n刚刚学完练习一下:\n1.尝试从语音识别中拿出result对应的中文\n2.尝试你说一句话,然后让百度AI学你说话\n3.尝试使用对话的方式,得到你叫什么名字,你今年几岁了,这样简单问题的答案"}
{"content2":"声明：以下内容转载自平行宇宙。\nPython在科学计算领域，有两个重要的扩展模块：Numpy和Scipy。其中Numpy是一个用python实现的科学计算包。包括：\n一个强大的N维数组对象Array；\n比较成熟的（广播）函数库；\n用于整合C/C++和Fortran代码的工具包；\n实用的线性代数、傅里叶变换和随机数生成函数。\nSciPy是一个开源的Python算法库和数学工具包，SciPy包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。其功能与软件MATLAB、Scilab和GNU Octave类似。\nNumpy和Scipy常常结合着使用，Python大多数机器学习库都依赖于这两个模块，绘图和可视化依赖于matplotlib模块，matplotlib的风格与matlab类似。Python机器学习库非常多，而且大多数开源，主要有：\n1.scikit-learn\nscikit-learn 是一个基于SciPy和Numpy的开源机器学习模块，包括分类、回归、聚类系列算法，主要算法有SVM、逻辑回归、朴素贝叶斯、Kmeans、DBSCAN等，目前由INRI 资助，偶尔Google也资助一点。\n项目主页：\nhttps://pypi.python.org/pypi/scikit-learn/\nhttp://scikit-learn.org/\nhttps://github.com/scikit-learn/scikit-learn\n2.NLTK\nNLTK(Natural Language Toolkit)是Python的自然语言处理模块，包括一系列的字符处理和语言统计模型。NLTK 常用于学术研究和教学，应用的领域有语言学、认知科学、人工智能、信息检索、机器学习等。 NLTK提供超过50个语料库和词典资源，文本处理库包括分类、分词、词干提取、解析、语义推理。可稳定运行在Windows, Mac OS X和Linux平台上.\n项目主页：\nhttp://sourceforge.net/projects/nltk/\nhttps://pypi.python.org/pypi/nltk/\nhttp://nltk.org/\n3.Mlpy\nMlpy是基于NumPy/SciPy的Python机器学习模块，它是Cython的扩展应用。包含的机器学习算法有：\n3.1回归\nleast squares, ridge regression, least angle regression, elastic net, kernel ridge regression, support vector machines (SVM), partial least squares (PLS)\n3.2分类\nlinear discriminant analysis (LDA), Basic perceptron, Elastic Net, logistic regression, (Kernel) Support Vector Machines (SVM), Diagonal Linear Discriminant Analysis (DLDA), Golub Classifier, Parzen-based, (kernel) Fisher Discriminant Classifier, k-nearest neighbor, Iterative RELIEF, Classification Tree, Maximum Likelihood Classifier\n3.3聚类\nhierarchical clustering, Memory-saving Hierarchical Clustering, k-means\n3.4维度约减\n(Kernel) Fisher discriminant analysis (FDA), Spectral Regression Discriminant Analysis (SRDA), (kernel) Principal component analysis (PCA)\n项目主页：\nhttp://sourceforge.net/projects/mlpy\nhttps://mlpy.fbk.eu/\n4.Shogun\nShogun是一个开源的大规模机器学习工具箱。目前Shogun的机器学习功能分为几个部分：feature表示，feature预处理， 核函数表示,核函数标准化，距离表示，分类器表示，聚类方法，分布， 性能评价方法，回归方法，结构化输出学习器。\nSHOGUN 的核心由C++实现，提供 Matlab、 R、 Octave、 Python接口。主要应用在linux平台上。\n项目主页：\nhttp://www.shogun-toolbox.org/\n5.MDP\nThe Modular toolkit for Data Processing (MDP) ，用于数据处理的模块化工具包，一个Python数据处理框架。\n从用户的观点，MDP是能够被整合到数据处理序列和更复杂的前馈网络结构的一批监督学习和非监督学习算法和其他数据处理单元。计算依照速度和内存需求而高效的执行。从科学开发者的观点，MDP是一个模块框架，它能够被容易地扩展。新算法的实现是容易且直观的。新实现的单元然后被自动地与程序库的其余部件进行整合。MDP在神经科学的理论研究背景下被编写，但是它已经被设计为在使用可训练数据处理算法的任何情况中都是有用的。其站在用户一边的简单性，各种不同的随时可用的算法，及应用单元的可重用性，使得它也是一个有用的教学工具。\n项目主页：\nhttp://mdp-toolkit.sourceforge.net/\nhttps://pypi.python.org/pypi/MDP/\n6.PyBrain\nPyBrain(Python-Based Reinforcement Learning, Artificial Intelligence and Neural Network)是Python的一个机器学习模块，它的目标是为机器学习任务提供灵活、易应、强大的机器学习算法。（这名字很霸气）\nPyBrain正如其名，包括神经网络、强化学习(及二者结合)、无监督学习、进化算法。因为目前的许多问题需要处理连续态和行为空间，必须使用函数逼近(如神经网络)以应对高维数据。PyBrain以神经网络为核心，所有的训练方法都以神经网络为一个实例。\n项目主页：\nhttp://www.pybrain.org/\nhttps://github.com/pybrain/pybrain/\n7.BigML\nBigML 使得机器学习为数据驱动决策和预测变得容易，BigML使用容易理解的交互式操作创建优雅的预测模型。BigML使用BigML.io,捆绑Python。\n项目主页：\nhttps://bigml.com/\nhttps://pypi.python.org/pypi/bigml\nhttp://bigml.readthedocs.org/\n8.PyML\nPyML是一个Python机器学习工具包， 为各分类和回归方法提供灵活的架构。它主要提供特征选择、模型选择、组合分类器、分类评估等功能。\n项目主页：\nhttp://cmgm.stanford.edu/~asab/pyml/tutorial/\nhttp://pyml.sourceforge.net/\n9.Milk\nMilk是Python的一个机器学习工具箱，其重点是提供监督分类法与几种有效的分类分析：SVMs(基于libsvm)，K-NN，随机森林经济和决策树。它还可以进行特征选择。这些分类可以在许多方面相结合，形成不同的分类系统。\n对于无监督学习，它提供K-means和affinity propagation聚类算法。\n项目主页：\nhttps://pypi.python.org/pypi/milk/\nhttp://luispedro.org/software/milk\n10. PyMVPA\nPyMVPA(Multivariate Pattern Analysis in Python)是为大数据集提供统计学习分析的Python工具包，它提供了一个灵活可扩展的框架。它提供的功能有分类、回归、特征选择、数据导入导出、可视化等\n项目主页：\nhttp://www.pymvpa.org/\nhttps://github.com/PyMVPA/PyMVPA\n11.Pattern\nPattern是Python的web挖掘模块，它绑定了  Google、Twitter 、Wikipedia API，提供网络爬虫、HTML解析功能，文本分析包括浅层规则解析、WordNet接口、句法与语义分析、TF-IDF、LSA等，还提供聚类、分类和图网络可视化的功能。\n项目主页：\nhttp://www.clips.ua.ac.be/pages/pattern\nhttps://pypi.python.org/pypi/Pattern\n12.pyrallel\nPyrallel(Parallel Data Analytics in Python)基于分布式计算模式的机器学习和半交互式的试验项目，可在小型集群上运行，适用范围：\nfocus on small to medium dataset that fits in memory on a small (10+ nodes) to medium cluster (100+ nodes).\nfocus on small to medium data (with data locality when possible).\nfocus on CPU bound tasks (e.g. training Random Forests) while trying to limit disk / network access to a minimum.\ndo not focus on HA / Fault Tolerance (yet).\ndo not try to invent new set of high level programming abstractions (yet): use a low level programming model (IPython.parallel) to finely control the cluster elements and messages transfered and help identify what are the practical underlying constraints in distributed machine learning setting.\n项目主页：\nhttps://pypi.python.org/pypi/pyrallel\nhttp://github.com/pydata/pyrallel\n13. Monte\nMonte ( machine learning in pure Python)是一个纯Python机器学习库。它可以迅速构建神经网络、条件随机场、逻辑回归等模型，使用inline-C优化，极易使用和扩展。\n项目主页：\nhttps://pypi.python.org/pypi/Monte\nhttp://montepython.sourceforge.net\n14.Orange\nOrange 是一个基于组件的数据挖掘和机器学习软件套装，它的功能即友好，又很强大，快速而又多功能的可视化编程前端，以便浏览数据分析和可视化，基绑定了 Python以进行脚本开发。它包含了完整的一系列的组件以进行数据预处理，并提供了数据帐目，过渡，建模，模式评估和勘探的功能。其由C++ 和 Python开发，它的图形库是由跨平台的Qt框架开发。\n项目主页：\nhttps://pypi.python.org/pypi/Orange/\nhttp://orange.biolab.si/\n15.Theano\nTheano 是一个 Python 库，用来定义、优化和模拟数学表达式计算，用于高效的解决多维数组的计算问题。Theano的特点：\nl  紧密集成Numpy\nl  高效的数据密集型GPU计算\nl  高效的符号微分运算\nl  高速和稳定的优化\nl  动态生成c代码\nl  广泛的单元测试和自我验证\n自2007年以来，Theano已被广泛应用于科学运算。theano使得构建深度学习模型更加容易，可以快速实现下列模型：\nl  Logistic Regression\nl  Multilayer perceptron\nl  Deep Convolutional Network\nl  Auto Encoders, Denoising Autoencoders\nl  Stacked Denoising Auto-Encoders\nl  Restricted Boltzmann Machines\nl  Deep Belief Networks\nl  HMC Sampling\nl  Contractive auto-encoders\n注：Theano，一位希腊美女，Croton最有权势的Milo的女儿，后来成为了毕达哥拉斯的老婆。\n项目主页：\nhttp://deeplearning.net/tutorial/\nhttps://pypi.python.org/pypi/Theano\n16.Pylearn2\nPylearn2建立在theano上，部分依赖scikit-learn上，目前Pylearn2正处于开发中，将可以处理向量、图像、视频等数据，提供MLP、RBM、SDA等深度学习模型。Pylearn2的目标是：\nResearchers add features as they need them. We avoid getting bogged down by too much top-down planning in advance.\nA machine learning toolbox for easy scientific experimentation.\nAll models/algorithms published by the LISA lab should have reference implementations in Pylearn2.\nPylearn2 may wrap other libraries such as scikits.learn when this is practical\nPylearn2 differs from scikits.learn in that Pylearn2 aims to provide great flexibility and make it possible for a researcher to do almost anything, while scikits.learn aims to work as a “black box” that can produce good results even if the user does not understand the implementation\nDataset interface for vector, images, video, ...\nSmall framework for all what is needed for one normal MLP/RBM/SDA/Convolution experiments.\nEasy reuse of sub-component of Pylearn2.\nUsing one sub-component of the library does not force you to use / learn to use all of the other sub-components if you choose not to.\nSupport cross-platform serialization of learned models.\nRemain approachable enough to be used in the classroom (IFT6266 at the University of Montreal).\n项目主页：\nhttp://deeplearning.net/software/pylearn2/\nhttps://github.com/lisa-lab/pylearn2\n还有其他的一些Python的机器学习库，如：\npmll(https://github.com/pavlov99/pmll)\npymining(https://github.com/bartdag/pymining)\nease (https://github.com/edx/ease)\ntextmining(http://www.christianpeccei.com/textmining/)\n更多的机器学习库可通过https://pypi.python.org/pypi查找。"}
{"content2":"根据网上的相关博客总结了一下机器学习中的这两个概念，参考博客见文末。\n生成模型：无穷样本==》概率密度模型 = 生成模型==》预测\n判别模型：有限样本==》判别函数 = 预测模型==》预测\n机器学习中的模型一般分为两类：判别模型、生成模型，这是对问题的两种不同的审视角度。\n假设我们要学习一个算法区分大象和狗（假设输入是重量、鼻子长度等特征）。判别模型和生成模型以两种不同的思路解决这个问题：\n判别模型：根据训练集，找到一个两种动物的一个差别（决策边界），然后根据输入的特征（比如重量，鼻子长度等）看这个特种落在决策边界的哪一边\n生成模型：根据大象的特征，学习一个大象的模型（总结一个大象长什么样的规律），然后也学习一个狗的模型，为了分类一个新的物种，我们看这个物种和哪个模型更加匹配\n形式化的描述两种模型\n判别模型（Discriminative Model）\n直接学习的是决策函数y = f(x) 或者条件概率p(y|x) （或者输入X和label{0,1}的一种映射关系），利用正负例和分类标签，关注在判别模型的边缘分布。更多用来直接解决给定的问题，而不侧重于建模。建模的目标是\n主要特点：寻找不同类别之间的最优分类面，反映的是异类数据之间的差异\n优点是：\n分类边界更灵活，比使用纯概率方法或生产模型得到的更高级\n能清晰的分辨出多类或某一类与其他类之间的差异特征\n在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好\n适用于较多类别的识别\n判别模型的性能比生成模型要简单，比较容易学习\n缺点是：\n不能反映训练数据本身的特性，能力有限，可以告诉你的是1还是2，但没办法把整个场景描述出来\n形式和结构上不如生成式模型优雅\n类似于黑盒，变量之间的关系不明确\n常见判别模型：逻辑回归，svm，传统的神经网络，Nearest Neighbor，GRF(条件随机场)，LDA(线性判别分析)，Boosting, 线性回归，k近邻，最大熵模型\n---------------------------------------------------------------\n生成模型（Generative Model）\n学习的是联合概率p(x,y), 由于p(x,y) = p(x|y)*p(y), 即我们需要学习的是p(x|y)和p(y), 就上面给的例子，如果y表示一个动物是狗(y=0)或者大象(y=1)，那么p(x|y=0)对狗的特征进行建模，p(x|y=1)是对大象的特征进行建模，p(y)可以假设其服从伯努利分布（因为y只有0,1两种取值）。建模目标如下：\n上面的式子之所以可以忽略p(x)，因为对训练集的输入x来说p(x)是一个常量                                        本文地址\n模型训练好以后对于输入的x，我们可以根据一下贝叶斯公式来计算P(y|x), 然后看p(y=0|x)和p(y=1|x)哪个较大，就划分给那一类\n(即生成模型和判别模型的最终目标都是p(y|x)，只不过生成模型的目标转换成了另一种形式)\n主要特点：一般主要是对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度； 只关注自己的 inclass 本身，不关心到底 decision boundary 在哪\n优点：\n实际上带的信息要比判别模型丰富，有更强的解释力\n研究单类问题比判别模型灵活性强\n模型可以通过增量学习得到\n能用于数据不完整的情况\n缺点：\n学习和计算过程比较复杂\n常见生成模型：GDA(高斯判别分析)，朴素贝叶斯，贝叶斯网络，Mixtures of Multinomials，高斯混合模型，Mixtures of Experts，隐马尔科夫模型，Sigmoidal Belief Networks，马尔科夫随机场，LDA(潜在狄立克雷分配)\n----------------------------------------------------------\n下面是全文转zouxy09的文章生成模型与判别模型，讲的很详细\n一直在看论文的过程中遇到这个问题，折腾了不少时间，然后是下面的一点理解，不知道正确否。若有错误，还望各位前辈不吝指正，以免小弟一错再错。在此谢过。\n一、决策函数Y=f(X)或者条件概率分布P(Y|X)\n监督学习的任务就是从数据中学习一个模型（也叫分类器），应用这一模型，对给定的输入X预测相应的输出Y。这个模型的一般形式为决策函数Y=f(X)或者条件概率分布P(Y|X)。\n决策函数Y=f(X)：你输入一个X，它就输出一个Y，这个Y与一个阈值比较，根据比较结果判定X属于哪个类别。例如两类（w1和w2）分类问题，如果Y大于阈值，X就属于类w1，如果小于阈值就属于类w2。这样就得到了该X对应的类别了。\n条件概率分布P(Y|X)：你输入一个X，它通过比较它属于所有类的概率，然后输出概率最大的那个作为该X对应的类别。例如：如果P(w1|X)大于P(w2|X)，那么我们就认为X是属于w1类的。\n所以上面两个模型都可以实现对给定的输入X预测相应的输出Y的功能。实际上通过条件概率分布P(Y|X)进行预测也是隐含着表达成决策函数Y=f(X)的形式的。例如也是两类w1和w2，那么我们求得了P(w1|X)和P(w2|X)，那么实际上判别函数就可以表示为Y= P(w1|X)/P(w2|X)，如果Y大于1或者某个阈值，那么X就属于类w1，如果小于阈值就属于类w2。而同样，很神奇的一件事是，实际上决策函数Y=f(X)也是隐含着使用P(Y|X)的。因为一般决策函数Y=f(X)是通过学习算法使你的预测和训练数据之间的误差平方最小化，而贝叶斯告诉我们，虽然它没有显式的运用贝叶斯或者以某种形式计算概率，但它实际上也是在隐含的输出极大似然假设（MAP假设）。也就是说学习器的任务是在所有假设模型有相等的先验概率条件下，输出极大似然假设。\n所以呢，分类器的设计就是在给定训练数据的基础上估计其概率模型P(Y|X)。如果可以估计出来，那么就可以分类了。但是一般来说，概率模型是比较难估计的。给一堆数给你，特别是数不多的时候，你一般很难找到这些数满足什么规律吧。那能否不依赖概率模型直接设计分类器呢？事实上，分类器就是一个决策函数（或决策面），如果能够从要解决的问题和训练样本出发直接求出判别函数，就不用估计概率模型了，这就是决策函数Y=f(X)的伟大使命了。例如支持向量机，我已经知道它的决策函数（分类面）是线性的了，也就是可以表示成Y=f(X)=WX+b的形式，那么我们通过训练样本来学习得到W和b的值就可以得到Y=f(X)了。还有一种更直接的分类方法，它不用事先设计分类器，而是只确定分类原则，根据已知样本（训练样本）直接对未知样本进行分类。包括近邻法，它不会在进行具体的预测之前求出概率模型P(Y|X)或者决策函数Y=f(X)，而是在真正预测的时候，将X与训练数据的各类的Xi比较，和哪些比较相似，就判断它X也属于Xi对应的类。\n实际上，说了那么多，也不知道自己表达清楚了没有。那我们是谈生成模型和判别模型，上面到底啰嗦了那么多到底有啥阴谋啊？呵呵，往下说就知道了。\n二、生成方法和判别方法\n监督学习方法又分生成方法（Generative approach）和判别方法（Discriminative approach），所学到的模型分别称为生成模型（Generative Model）和判别模型（Discriminative Model）。咱们先谈判别方法，因为它和前面说的都差不多，比较容易明白。\n判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。\n生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类，就像上面说的那样。注意了哦，这里是先求出P(X,Y)才得到P(Y|X)的，然后这个过程还得先求出P(X)。P(X)就是你的训练数据的概率分布。哎，刚才说了，需要你的数据样本非常多的时候，你得到的P(X)才能很好的描述你数据真正的分布。例如你投硬币，你试了100次，得到正面的次数和你的试验次数的比可能是3/10，然后你直觉告诉你，可能不对，然后你再试了500次，哎，这次正面的次数和你的试验次数的比可能就变成4/10，这时候你半信半疑，不相信上帝还有一个手，所以你再试200000次，这时候正面的次数和你的试验次数的比（就可以当成是正面的概率了）就变成5/10了。这时候，你就觉得很靠谱了，觉得自己就是那个上帝了。呵呵，真啰嗦，还差点离题了。\n还有一个问题就是，在机器学习领域有个约定俗成的说法是：不要去学那些对这个任务没用的东西。例如，对于一个分类任务：对一个给定的输入x，将它划分到一个类y中。那么，如果我们用生成模型：p(x,y)=p(y|x).p(x)\n那么，我们就需要去对p(x)建模，但这增加了我们的工作量，这让我们很不爽（除了上面说的那个估计得到P(X)可能不太准确外）。实际上，因为数据的稀疏性，导致我们都是被强迫地使用弱独立性假设去对p(x)建模的，所以就产生了局限性。所以我们更趋向于直观的使用判别模型去分类。\n这样的方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。典型的生成模型有：朴素贝叶斯和隐马尔科夫模型等。\n三、生成模型和判别模型的优缺点\n在监督学习中，两种方法各有优缺点，适合于不同条件的学习问题。\n生成方法的特点：上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y|X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。\n判别方法的特点：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。\n四、生成模型和判别模型的联系\n由生成模型可以得到判别模型，但由判别模型得不到生成模型。\n五、再形象点可以吗\n例如我们有一个输入数据x，然后我们想将它分类为标签y。（迎面走过来一个人，你告诉我这个是男的还是女的）\n生成模型学习联合概率分布p(x,y)，而判别模型学习条件概率分布p(y|x)。\n下面是个简单的例子：\n例如我们有以下(x,y)形式的数据：(1,0), (1,0), (2,0), (2, 1)\n那么p(x,y)是：\ny=0   y=1\n-----------\nx=1 | 1/2   0\nx=2 | 1/4   1/4\n而p(y|x) 是：\ny=0   y=1\n-----------\nx=1| 1     0\nx=2| 1/2   1/2\n我们为了将一个样本x分类到一个类y，最自然的做法就是条件概率分布p(y|x)，这就是为什么我们对其直接求p(y|x)方法叫做判别算法。而生成算法求p(x,y)，而p(x,y)可以通过贝叶斯方法转化为p(y|x)，然后再用其分类。但是p(x,y)还有其他作用，例如，你可以用它去生成(x,y)对。\n再假如你的任务是识别一个语音属于哪种语言。例如对面一个人走过来，和你说了一句话，你需要识别出她说的到底是汉语、英语还是法语等。那么你可以有两种方法达到这个目的：\n1、学习每一种语言，你花了大量精力把汉语、英语和法语等都学会了，我指的学会是你知道什么样的语音对应什么样的语言。然后再有人过来对你哄，你就可以知道他说的是什么语音，你就可以骂他是“米国人还是小日本了”。（呵呵，切勿将政治掺杂在技术里面）\n2、不去学习每一种语言，你只学习这些语言模型之间的差别，然后再分类。意思是指我学会了汉语和英语等语言的发音是有差别的，我学会这种差别就好了。\n那么第一种方法就是生成方法，第二种方法是判别方法。\n生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。\n六、对于跟踪算法\n跟踪算法一般来说可以分为两类：基于外观模型的生成模型或者基于外观模型的判别模型。\n生成模型：一般是学习一个代表目标的模型，然后通过它去搜索图像区域，然后最小化重构误差。类似于生成模型描述一个目标，然后就是模式匹配了，在图像中找到和这个模型最匹配的区域，就是目标了。\n判别模型：将跟踪问题看成一个二分类问题，然后找到目标和背景的决策边界。它不管目标是怎么描述的，那只要知道目标和背景的差别在哪，然后你给一个图像，它看它处于边界的那一边，就归为哪一类。\n参考：\n生成模型与判别模型\n判别模型、生成模型与朴素贝叶斯方法\n判别式模型与生成式模型\n判别模型和生成模型 -- ML Step By Step(4)\n【版权声明】转载请注明出处：http://www.cnblogs.com/TenosDoIt/p/3721074.html"}
{"content2":"相关书籍PDF资料下载见：https://pan.baidu.com/s/1sk0uHSHQKG1Jrw47TCZULw 提取码：zja9\n最全AI书籍资料整理推荐：\n《21个项目玩转深度学习：基于TensorFlow的实践详解》PDF+源代码\n《153分钟学会R》\n《AI·未来》\n《ggplot2：数据分析与图形艺术》\n《Hadoop权威指南(第2版)》\n《Hadoop权威指南（第四版）》中文PDF+英文PDF+源代码\n《Keras快速上手：基于Python的深度学习实战》\n《OpenCV3计算机视觉 Python语言实现(第二版)》\n《OpenCV官方教程中文版（For Python）》\n《Python3网络爬虫开发实战》中文PDF+源代码\n《Python编程从入门到实践》（高清中文版PDF+高清英文版PDF+源代码）\n《Python编程快速上手：让繁琐工作自动化》【高清中文版PDF+高清英文版PDF+源代码】\n《Python编程入门(第3版)》中文PDF+英文PDF\n《Python程序设计与算法基础教程》\n《Python核心编程(第三版)》（高清中文版PDF+高清英文版PDF+源代码）\n《Python机器学习》高清英文版PDF+中文版PDF+源代码及数据集\n《Python机器学习基础教程》高清中文版PDF+高清英文版PDF+源代码\n《Python机器学习及实践从零开始通往Kaggle竞赛之路》\n《Python机器学习经典实例》(高清中文版PDF+高清英文版PDF+源代码)\n《Python机器学习实践指南》(高清中文版PDF+高清英文版PDF+源代码)\n《Python机器学习—预测分析核心算法》高清中文版PDF+高清英文版PDF+源代码\n《Python基础教程（第3版）》（高清中文版PDF+高清英文版PDF+源代码）\n《Python金融大数据分析》中文版PDF+英文版PDF+源代码\n《Python金融实战》中文版PDF+英文版PDF+源代码\n《Python进行自然语言处理》\n《Python深度学习》高清中文版pdf+高清英文版pdf+源代码\n《Python神经网络编程》中文版PDF+英文版PDF+源代码\n《Python数据处理》（高清中文版PDF+高清英文版PDF+源代码）\n《Python数据分析基础》高清中文PDF+高清英文PDF+源代码\n《Python数据分析基础教程：NumPy学习指南(第2版)》高清中文PDF+英文PDF+源代码\n《Python数据科学手册》高清中文版PDF+高清英文版PDF+源代码\n《Python数据可视化编程实战》中文版PDF+英文版PDF+源代码\n《Python数据挖掘入门与实践》高清中文版+高清英文版+源代码\n《Python网络数据采集》高清中文版PDF+高清英文版PDF+源代码\n《Python学习手册(第4版)》高清中文PDF+高清英文PDF+源代码\n《Python游戏编程快速上手(第3版)》高清中文版PDF+高清英文版PDF+源代码\n《Python语言及其应用》高清中文版PDF+高清英文版PDF+源代码\n《R数据科学》高清中文版PDF+高清英文版PDF+源代码\n《R数据可视化手册》高清英文版PDF+中文版PDF+源代码\n《R语言编程艺术》中文版PDF+英文版PDF+源代码\n《R语言实战（第2版）》高清中文版PDF+高清英文版PDF+源代码\n《R语言与网站分析》\n《TensorFlow 官方文档中文版》\n《TensorFlow机器学习实战指南》中文版PDF+英文版PDF+源代码\n《TensorFlow机器学习项目实战》中文PDF+英文PDF+源代码\n《TensorFlow技术解析与实战》高清中文PDF+源代码\n《TensorFlow实战》中文版PDF+源代码\n《TensorFlow实战Google深度学习框架(第2版)》中文版PDF和源代码\n《白话大数据与机器学习》PDF+《图解机器学习》PDF\n《白话深度学习与TensorFlow》中文版PDF\n《贝叶斯方法概率编程与贝叶斯推断》中文版PDF+英文版PDF+源代码\n《贝叶斯思维：统计建模的Python学习法》高清中文版PDF+高清英文版PDF+源代码\n《从Excel到Python数据分析进阶指南》高清中文版PDF\n《大数据之路：阿里巴巴大数据实践》\n《大数据治理》\n《大数据治理与服务》\n《动手学深度学习》高清PDF\n《父与子的编程之旅python【第二版】》高清中文版PDF+高清英文版PDF+源代码\n《机器学习：实用案例解析》中文版PDF+英文版PDF+源代码\n《机器学习》\n《机器学习》\n《机器学习导论》\n《机器学习导论》\n《机器学习基础教程》中文PDF+英文PDF\n《机器学习实践应用》高清PDF+源代码\n《机器学习实战：基于Scikit-Learn和TensorFlow》高清中英文PDF+源代码\n《机器学习实战》(高清中文版PDF+高清英文版PDF+源代码)\n《机器学习系统设计》高清中文版+高清英文版+源代码\n《机器学习与数据科学(基于R的统计学习方法)》高清中文PDF+源代码\n《机器学习之路》\n《机器之心》\n《教孩子学编程Python语言版》中文版PDF+英文版PDF+源代码\n《解析卷积神经网络深度学习实践手册》\n《解析深度学习语音识别实践》\n《精通Python爬虫框架Scrapy》中文PDF+英文PDF+源代码\n《精通Python自然语言处理》高清中文版PDF+高清英文版PDF+源代码\n《矩阵分析与应用（第二版）张贤达》PDF\n《浪潮之巅》\n《利用Python进行数据分析(第二版)》高清中文版PDF+高清英文版PDF+源代码\n《零基础入门学习Python》电子书PDF+笔记+课后题及答案\n《零起点Python大数据与量化交易》中文PDF+源代码\n《流畅的Python》高清中文版PDF_mobi+高清英文版PDF_mobi+源代码大全套\n《面向机器智能的TensorFlow实践》中文版PDF+英文版PDF+源代码\n《模式识别与机器学习》\n《趣学Python编程》中文PDF+英文PDF+源代码\n《人工智能：国家人工智能战略行动抓手》\n《人工智能：智能系统指南》\n《人工智能》\n《人工智能基础》\n《人工智能时代的教育革命》\n《人工智能一种现代方法》\n《人类简史》\n《深度学习、优化与识别》PDF+《深度学习原理与TensorFlow实践》PDF\n《深度学习：一起玩转TensorLayer》\n《深度学习：原理与应用实践》中文版PDF\n《深度学习精要（基于R语言）》高清中文版PDF+高清英文版PDF+源代码\n《深度学习入门：基于Python的理论与实现》高清中文版PDF+源代码\n《深度学习-伊恩·古德费洛》【中文版和英文版】【高清完整版PDF】\n《深度学习与计算机视觉》\n《深度学习之PyTorch实战计算机视觉》PDF\n《深度学习之TensorFlow：入门、原理与进阶实战》PDF+源代码\n《深入浅出强化学习：原理入门》\n《深入浅出深度学习：原理剖析与python实践》PDF+源代码\n《神经网络与机器学习（第3版）》高清英文PDF+中文PDF\n《神经网络与深度学习（美）MichaelNielsen著》中文版PDF+英文版PDF+源代码\n《生物信息学机器学习方法》\n《失控》\n《时间简史》\n《时间序列分析及应用：R语言（原书第2版）》\n《实用机器学习(孙亮 著)》PDF+源代码\n《数据科学实战手册(R+Python)》中文PDF+英文PDF+源代码\n《数据科学中的R语言》中文PDF+源代码\n《数据挖掘实用机器学习技术（中文第二版）》\n《数据挖掘与R语言》\n《数理统计与数据分析 原书第3版》\n《统计手册：金融中的统计方法》\n《统计学习方法》\n《统计学习基础 数据挖掘、推理与预测》\n《图说D3数据可视化利器从入门到进阶》\n《未来简史》\n《吴恩达深度学习课程笔记》\n《一天搞懂深度学习》\n《智能革命》\n《智能时代》\n阶段一、人工智能基础 －　高等数学必知必会\n本阶段主要从数据分析、概率论和线性代数及矩阵和凸优化这四大块讲解基础，旨在训练大家逻辑能力，分析能力。拥有良好的数学基础，有利于大家在后续课程的学习中更好的理解机器学习和深度学习的相关算法内容。同时对于AI研究尤为重要，例如人工智能中的智能很大一部分依托“概率论”实现的。\n一、数据分析\n1）常数e\n2）导数\n3）梯度\n4）Taylor\n5）gini系数\n6）信息熵与组合数\n7）梯度下降\n8）牛顿法\n二、概率论\n1）微积分与逼近论\n2）极限、微分、积分基本概念\n3）利用逼近的思想理解微分，利用积分的方式理解概率\n4）概率论基础\n5）古典模型\n6）常见概率分布\n7）大数定理和中心极限定理\n8）协方差(矩阵)和相关系数\n9）最大似然估计和最大后验估计\n三、线性代数及矩阵\n1）线性空间及线性变换\n2）矩阵的基本概念\n3）状态转移矩阵\n4）特征向量\n5）矩阵的相关乘法\n6）矩阵的QR分解\n7）对称矩阵、正交矩阵、正定矩阵\n8）矩阵的SVD分解\n9）矩阵的求导\n10）矩阵映射/投影\n四、凸优化\n1）凸优化基本概念\n2）凸集\n3）凸函数\n4）凸优化问题标准形式\n5）凸优化之Lagerange对偶化\n6）凸优化之牛顿法、梯度下降法求解\n阶段二、人工智能提升 － Python高级应用\n随着AI时代的到来以及其日益蓬勃的发展，Python作为AI时代的头牌语言地位基本确定，机器学习是着实令人兴奋，但其复杂度及难度较大，通常会涉及组装工作流和管道、设置数据源及内部和云部署之间的分流而有了Python库后，可帮助加快数据管道，且Python库也在不断更新发布中，所以本阶段旨在为大家学习后续的机器学习减负。\n一、容器\n1）列表:list\n2）元组:tuple\n3）字典: dict\n4）数组: Array\n5）切片\n6）列表推导式\n7）浅拷贝和深拷贝\n二、函数\n1）lambda表达式\n2）递归函数及尾递归优化\n3）常用内置函数/高阶函数\n4）项目案例：约瑟夫环问题\n三、常用库\n1）时间库\n2）并发库\n3）科学计算库\n4）Matplotlib可视化绘图库\n5）锁和线程\n6）多线程编程\n阶段三、人工智能实用 － 机器学习篇\n机器学习利用算法去分析数据、学习数据，随后对现实世界情况作出判断和预测。因此，与预先编写好、只能按照特定逻辑去执行指令的软件不同，机器实际上是在用大量数据和算法去“自我训练”，从而学会如何完成一项任务。\n所以本阶段主要从机器学习概述、数据清洗和特征选择、回归算法、决策树、随机森林和提升算法、SVM、聚类算、EM算法、贝叶斯算法、隐马尔科夫模型、LDA主题模型等方面讲解一些机器学习的相关算法以及这些算法的优化过程，这些算法也就是监督算法或者无监督算法。\n一、机器学习\n1）机器学习概述\n二、监督学习\n1）逻辑回归\n2）softmax分类\n3）条件随机场\n4）支持向量机svm\n5）决策树\n6）随机森林\n7）GBDT\n8）集成学习\n三、非监督学习\n1）高斯混合模型\n2）聚类\n3）PCA\n4）密度估计\n5）LSI\n6）LDA\n7）双聚类\n四、数据处理与模型调优\n1）特征提取\n2）数据预处理\n3）数据降维\n4）模型参数调优\n5）模型持久化\n6）模型可视化\n阶段四、人工智能实用 － 数据挖掘篇\n本阶段主要通过音乐文件分类和金融反欺诈模型训练等项目，帮助大家对于上阶段的机器学习做更深入的巩固，为后续深度学习及数据挖掘提供项目支撑。\n项目一：百度音乐系统文件分类\n音乐推荐系统就是利用音乐网站上的音乐信息，向用户提供音乐信息或者建议，帮助用户决定应该听什么歌曲。而个人化推荐则是基于音乐信息及用户的兴趣特征、听歌历史行为，向用户推荐用户可能会感兴趣的音乐或者歌手。推荐算法主要分为以下几种：基于内容的推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐等；推荐系统常用于各个互联网行业中，比如音乐、电商、旅游、金融等。\n项目二：千万级P2P金融系统反欺诈模型训练\n目前比较火的互联网金融领域，实质是小额信贷，小额信贷风险管理，本质上是事前对风险的主动把控，尽可能预测和防范可能出现的风险。本项目应用GBDT、Randomforest等机器学习算法做信贷反欺诈模型，通过数据挖掘技术，机器学习模型对用户进行模型化综合度量，确定一个合理的风险范围，使风险和盈利达到一个平衡的状态。\n阶段五、人工智能前沿 －　深度学习篇\n深度学习是实现机器学习的技术，同时深度学习也带来了机器学习的许多实际应用，拓展了AI的使用领域，本阶段主要从TensorFlow、BP神经网络、深度学习概述、CNN卷积神经网络、递归神经网、自动编码机，序列到序列网络、生成对抗网络，孪生网络，小样本学习技术等方面讲解深度学习相关算法以，掌握深度学习前沿技术，并根据不同项目选择不同的技术解决方案。针对公司样本不足，采用小样本技术和深度学习技术结合，是项目落地的解决方案。\n1）TensorFlow基本应用\n2）BP神经网络\n3）深度学习概述\n4）卷积神经网络(CNN)\n5）图像分类(vgg,resnet)\n6）目标检测(rcnn,fast-rcnn,faster-rcnn,ssd)\n7）递归神经网络(RNN)\n8）lstm,bi-lstm,多层LSTM\n9）无监督学习之AutoEncoder自动编码器\n10）Seq2Seq\n11）Seq2Seq with Attension\n12）生成对抗网络\n13）irgan\n14）finetune及迁移学习\n15）孪生网络\n16）小样本学习\n阶段六、人工智能进阶 － 自然语言处理篇\n自然语言处理（NLP）是计算机科学领域与人工智能领域中的一个重要方向。它已成为人工智能的核心领域。自然语言处理解决的是“让机器可以理解自然语言”这一到目前为止都还只是人类独有的特权，被誉为人工智能皇冠上的明珠，被广泛应用。本阶段从NLP的字、词和句子全方位多角度的学习NLP，作为NLP的基础核心技术，对NLP为核心的项目，如聊天机器人，合理用药系统，写诗机器人和知识图谱等提供底层技术。通过学习NLP和深度学习技术，掌握NLP具有代表性的前沿技术。\n1）词（分词，词性标注）代码实战\n2）词（深度学习之词向量，字向量）代码实战\n3）词（深度学习之实体识别和关系抽取）代码实战\n4）词（关键词提取，无用词过滤）代码实战\n5）句（句法分析，语义分析）代码实战\n6）句（自然语言理解,一阶逻辑）代码实战\n7）句（深度学习之文本相似度）代码实战\n阶段七、人工智能进阶 － 图像处理篇\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。广泛的应用于农牧业、林业、环境、军事、工业和医学等方面，是人工智能和深度学习的重要研究方向。深度学习作为当前机器学习领域最热门的技术之一，已经在图像处理领域获得了应用，并且展现出巨大的前景。本阶段学习了数字图像的基本数据结构和处理技术，到前沿的深度学习处理方法。掌握前沿的ResNet,SSD,Faster RCNN等深度学习模型，对图像分类，目标检测和模式识别等图像处理主要领域达到先进水平。实际工作中很多项目都可以转化为本课程的所学的知识去解决，如行人检测，人脸识别和数字识别。\n一、图像基础\n图像读，写，保存，画图（线，圆，多边形，添加文字）\n二、图像操作及算数运算\n图像像素读取，算数运算，ROI区域提取\n三、图像颜色空间运算\n图像颜色空间相互转化\n四、图像几何变换\n平移，旋转，仿射变换，透视变换等\n五、图像形态学\n腐蚀，膨胀，开/闭运算等\n六、图像轮廓\n长宽，面积，周长，外接圆，方向，平均颜色，层次轮廓等\n七、图像统计学\n图像直方图\n八、图像滤波\n高斯滤波，均值滤波，双边滤波，拉普拉斯滤波等\n阶段八、人工智能终极实战 － 项目应用\n本阶段重点以项目为导向，通过公安系统人脸识别、图像识别以及图像检索、今日头条CTR广告点击量预估、序列分析系统、聊天机器人等多个项目的讲解，结合实际来进行AI的综合运用。\n阶段九、人工智能实战 － 企业项目实战\n课程一、基于Python数据分析与机器学习案例实战教程\n课程风格通俗易懂，基于真实数据集案例实战。主体课程分成三个大模块(1)python数据分析，(2)机器学习经典算法原理详解,(3)十大经典案例实战。通过python数据科学库numpy,pandas,matplot结合机器学习库scikit-learn完成一些列的机器学习案例。算法课程注重于原理推导与流程解释，结合实例通俗讲解复杂的机器学习算法，并以实战为主，所有课时都结合代码演示。算法与项目相结合，选择经典kaggle项目，从数据预处理开始一步步代码实战带大家快速入门机器学习。旨在帮助同学们快速上手如何使用python库来完整机器学习案例。选择经典案例基于真实数据集，从数据预处理开始到建立机器学习模型以及效果评估，完整的讲解如何使用python及其常用库进行数据的分析和模型的建立。对于每一个面对的挑战，分析解决问题思路以及如何构造合适的模型并且给出合适评估方法。在每一个案例中，同学们可以快速掌握如何使用pandas进行数据的预处理和分析，使用matplotlib进行可视化的展示以及基于scikit-learn库的机器学习模型的建立。\n1）Python数据分析与机器学习实战课程简介\n2）Python快速入门\n3）Python科学计算库Numpy\n4）Python数据分析处理库Pandas\n5）Python可视化库Matplotlib\n6）回归算法\n7）模型评估\n8）K近邻算法\n9）决策树与随机森林算法\n10）支持向量机\n11）贝叶斯算法\n12）神经网络\n13）Adaboost算法\n14）SVD与推荐\n15）聚类算法\n16）案例实战：使用Python库分析处理Kobe Bryan职业生涯数据\n17）案例实战：信用卡欺诈行为检测\n18）案例实战：泰坦尼克号获救预测\n19）案例实战：鸢尾花数据集分析\n20）案例实战：级联结构的机器学习模型\n21）案例实战：员工离职预测\n22）案例实战：使用神经网络进行手写字体识别\n23）案例实战：主成分分析\n24）案例实战：基于NLP的股价预测\n25）案例实战：借贷公司数据分析\n课程二、人工智能与深度学习实战\n课程风格通俗易懂，必备原理，形象解读，项目实战缺一不可！主体课程分成四个大模块(1)神经网络必备基础知识点，(2)深度学习模型，(3)深度学习框架Caffe与Tensorflow，(4)深度学习项目实战。 课程首先概述讲解深度学习应用与挑战，由计算机视觉中图像分类任务开始讲解深度学习的常规套路。对于复杂的神经网络，将其展开成多个小模块进行逐一攻破，再挑战整体神经网络架构。对于深度学习模型形象解读卷积神经网络原理，详解其中涉及的每一个参数，对卷积网络架构展开分析与评估，对于现阶段火爆的对抗生成网络以及强化学习给出形象解读，并配合项目实战实际演示效果。 基于框架实战，选择两款深度学习最火框架，Caffe与Tensorflow，首先讲解其基本使用方法，并结合案例演示如何应用框架构造神经网络模型并完成案例任务。 选择经典深度学习项目实战，使用深度学习框架从零开始完成人脸检测，验证码识别，人脸关键点定位，垃圾邮件分类，图像风格转换，AI自己玩游戏等。对于每一个项目实战，从数据预处理开始一步步构建网络模型并展开分析与评估。 课程提供所涉及的所有数据，代码以及PPT，方便大家快速动手进行项目实践！\n1）深度学习概述与挑战\n2）图像分类基本原理门\n3）深度学习必备基础知识点\n4）神经网络反向传播原理\n5）神经网络整体架构\n6）神经网络案例实战图像分类任务\n7）卷积神经网络基本原理\n8）卷积参数详解\n9）卷积神经网络案例实战\n10）经典网络架构分析\n11）分类与回归任务\n12）三代物体检测算法分析\n13）数据增强策略\n14）TransferLearning\n15）网络架构设计\n16） 深度学习框架Caffe网络结构配置\n17）Caffe\n18）深度学习项目实战人脸检测\n19）人脸正负样本数据源制作\n20）人脸检测网络架构配置习模型\n21）人脸检测代码实战\n22）人脸关键点定位项目实战\n23）人脸关键点定位网络模型\n24）人脸关键点定位构建级联网络\n25）人脸关键点定位测试效果与分析\n26）Tensorflow框架实战\n27）Tensorflow构建回归模型\n28）Tensorflow构建神经网络模型\n29）Tensorflow深度学习模型\n30）Tensorflow打造RNN网络模型\n31）Tensorflow项目实战验证识别\n32）项目实战图像风格转换\n33）QLearning算法原理\n34）DQN网络架构\n35）项目实战DQN网络让AI自己玩游戏\n36）项目实战对抗生成网络等"}
{"content2":"本文结构：\n是什么？\n有什么算法？\n数学原理？\n编码实现算法？\n1. 是什么？\n简单地理解，就是根据一些 feature 进行分类，每个节点提一个问题，通过判断，将数据分为几类，再继续提问。这些问题是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上。\n2. 有什么算法？\n常用的几种决策树算法有ID3、C4.5、CART：\nID3：选择信息熵增益最大的feature作为node，实现对数据的归纳分类。\nC4.5：是ID3的一个改进，比ID3准确率高且快，可以处理连续值和有缺失值的feature。\nCART：使用基尼指数的划分准则，通过在每个步骤最大限度降低不纯洁度，CART能够处理孤立点以及能够对空缺值进行处理。\n3. 数学原理？\nID3: Iterative Dichotomiser 3\n参考\n下面这个数据集，可以同时被上面两颗树表示，结果是一样的，而我们更倾向于选择简单的树。\n那么怎样做才能使得学习到的树是最简单的呢？\n下面是 ID3（ Iterative Dichotomiser 3 ）的算法：\n例如下面数据集，哪个是最好的 Attribute？\n用熵Entropy来衡量：\nE(S) 是数据集S的熵\ni 指每个结果，即 No，Yes的概率\nE越大意味着信息越混乱，我们的目标是要让E最小。\nE在0-1之间，如果P＋的概率在0.5， 此时E最大，这时候说明信息对我们没有明确的意义，对分类没有帮助。\n但是我们不仅仅想要变量的E最小，还想要这棵树是 well organized。\n所以用到 Gain：信息增益\n意思是如果我后面要用这个变量的话，它的E会减少多少。\n例如下面的数据集：\n先计算四个feature的熵E，及其分支的熵，然后用Gain的公式计算信息增益。\n再选择Gain最大的特征是 outlook。\n第一层选择出来后，各个分支再继续选择下一层，计算Gain最大的，例如分支 sunny 的下一层节点是 humidity。\n详细的计算步骤可以参考这篇博文。\nC4.5\n参考\nID3有个局限是对于有大量数据的feature过于敏感，C4.5是它的一个改进，通过选择最大的信息增益率 gain ratio 来选择节点。而且它可以处理连续的和有缺失值的数据。\nP’ (j/p) is the proportion of elements present at the position p, taking the value of j-th test.\n例如 outlook 作为第一层节点后，它有 3 个分支，分别有 5，4，5 条数据，则 SplitInfo(5,4,5) = -5/14log(5,14)-4/14log(4,14)-5/14(5,14) ，其中 log(5,14) 即为 log2(5/14)。\n下面是一个有连续值和缺失值的例子：\n连续值\n第一步计算 Gain，除了连续值的 humudity，其他步骤和前文一样。\n要计算 humudity 的 Gain 的话，先把所有值升序排列：\n{65, 70, 70, 70, 75, 78, 80, 80, 80, 85, 90, 90, 95, 96}\n然后把重复的去掉：\n{65, 70, 75, 78, 80, 85, 90, 95, 96}\n如下图所示，按区间计算 Gain，然后选择最大的 Gain (S, Humidity) = 0.102\n因为 Gain(S, Outlook) = 0 .246，所以root还是outlook：\n缺失值\n处理有缺失值的数据时候，用下图的公式：\n例如 D12 是不知道的。\n计算全集和 outlook 的 info，\n其中几个分支的熵如下，再计算出 outlook 的 Gain：\n比较一下 ID3 和 C4.5 的准确率和时间：\naccuracy ：\nexecution time：\n4. 编码实现算法？\n代码可以看《机器学习实战》这本书和这篇博客。\n完整代码可以在 github 上查看。\n接下来以 C4.5 的代码为例：\n1. 定义数据：\n1 def createDataSet(): 2 dataSet = [[0, 0, 0, 0, 'N'], 3 [0, 0, 0, 1, 'N'], 4 [1, 0, 0, 0, 'Y'], 5 [2, 1, 0, 0, 'Y'], 6 [2, 2, 1, 0, 'Y'], 7 [2, 2, 1, 1, 'N'], 8 [1, 2, 1, 1, 'Y']] 9 labels = ['outlook', 'temperature', 'humidity', 'windy'] 10 return dataSet, labels\n2. 计算熵：\n1 def calcShannonEnt(dataSet): 2 numEntries = len(dataSet) 3 labelCounts = {} 4 for featVec in dataSet: 5 currentLabel = featVec[-1] 6 if currentLabel not in labelCounts.keys(): 7 labelCounts[currentLabel] = 0 8 labelCounts[currentLabel] += 1 # 数每一类各多少个， {'Y': 4, 'N': 3} 9 shannonEnt = 0.0 10 for key in labelCounts: 11 prob = float(labelCounts[key])/numEntries 12 shannonEnt -= prob * log(prob, 2) 13 return shannonEnt\n3. 选择最大的gain ratio对应的feature：\n1 def chooseBestFeatureToSplit(dataSet): 2 numFeatures = len(dataSet[0]) - 1 #feature个数 3 baseEntropy = calcShannonEnt(dataSet) #整个dataset的熵 4 bestInfoGainRatio = 0.0 5 bestFeature = -1 6 for i in range(numFeatures): 7 featList = [example[i] for example in dataSet] #每个feature的list 8 uniqueVals = set(featList) #每个list的唯一值集合 9 newEntropy = 0.0 10 splitInfo = 0.0 11 for value in uniqueVals: 12 subDataSet = splitDataSet(dataSet, i, value) #每个唯一值对应的剩余feature的组成子集 13 prob = len(subDataSet)/float(len(dataSet)) 14 newEntropy += prob * calcShannonEnt(subDataSet) 15 splitInfo += -prob * log(prob, 2) 16 infoGain = baseEntropy - newEntropy #这个feature的infoGain 17 if (splitInfo == 0): # fix the overflow bug 18 continue 19 infoGainRatio = infoGain / splitInfo #这个feature的infoGainRatio 20 if (infoGainRatio > bestInfoGainRatio): #选择最大的gain ratio 21 bestInfoGainRatio = infoGainRatio 22 bestFeature = i #选择最大的gain ratio对应的feature 23 return bestFeature\n4. 划分数据，为下一层计算准备:\n1 def splitDataSet(dataSet, axis, value): 2 retDataSet = [] 3 for featVec in dataSet: 4 if featVec[axis] == value: #只看当第i列的值＝value时的item 5 reduceFeatVec = featVec[:axis] #featVec的第i列给除去 6 reduceFeatVec.extend(featVec[axis+1:]) 7 retDataSet.append(reduceFeatVec) 8 return retDataSet\n5. 多重字典构建树：\n1 def createTree(dataSet, labels): 2 classList = [example[-1] for example in dataSet] # ['N', 'N', 'Y', 'Y', 'Y', 'N', 'Y'] 3 if classList.count(classList[0]) == len(classList): 4 # classList所有元素都相等，即类别完全相同，停止划分 5 return classList[0] #splitDataSet(dataSet, 0, 0)此时全是N，返回N 6 if len(dataSet[0]) == 1: #[0, 0, 0, 0, 'N'] 7 # 遍历完所有特征时返回出现次数最多的 8 return majorityCnt(classList) 9 bestFeat = chooseBestFeatureToSplit(dataSet) #0－> 2 10 # 选择最大的gain ratio对应的feature 11 bestFeatLabel = labels[bestFeat] #outlook -> windy 12 myTree = {bestFeatLabel:{}} 13 #多重字典构建树{'outlook': {0: 'N' 14 del(labels[bestFeat]) #['temperature', 'humidity', 'windy'] -> ['temperature', 'humidity'] 15 featValues = [example[bestFeat] for example in dataSet] #[0, 0, 1, 2, 2, 2, 1] 16 uniqueVals = set(featValues) 17 for value in uniqueVals: 18 subLabels = labels[:] #['temperature', 'humidity', 'windy'] 19 myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) 20 # 划分数据，为下一层计算准备 21 return myTree\n6. 可视化决策树的结果:\ndataSet, labels = createDataSet() labels_tmp = labels[:] desicionTree = createTree(dataSet, labels_tmp) treePlotter.createPlot(desicionTree)"}
{"content2":"引言\n之前学习了逻辑回归，主要是从三方面学习的，一个是coursera上台大林轩田老师机器学习公开课的逻辑回归部分，一个是斯坦福Andrew Ng老师机器学习公开课的逻辑回归部分，另一个是《机器学习实战》逻辑回归部分\n前两者主要是对逻辑回归理论的学习，后者主要是实践的学习，现在对其进行整理，也便于自己思考。\n本文主要内容\n本文主要分为以下内容：\n首先大致介绍了逻辑回归的分类过程，包括常规分类过程中的计算权重与样本特征之间乘积得到分数，利用sigmod函数将分数转换到0到1之间的值，然后利用这个值进行分类\n然后对损失函数进行了分析，包括如何利用极大似然得到最佳的权重向量值，以及梯度下降法时权重的更新公式\n再然后对解决过拟合的一般方式进行了一定的说明，包括正规化，减少特征\n最后是机器学习实战的代码和仿真实验的结果\n逻辑回归的分类过程\n跟一般的分类模型一样，分类的话需要找一个合适的分类函数，台大机器学习中称之为h函数（hypothesis函数）\n台大机器学习课程中对h得到的过程是这样讲解的\n权重w与样本特征值的乘积得到分数\n样本x是有n维的，每一维代表样本的一个特征，每一特征在判断x属于哪一类时所占的权重不一样，所以首先需要对x的各个维度(即特征向量的每一维)加权值w(这里的w未知，我们逻辑回归的目的就是求出这个w权重向量，表示对特征向量每一维所占的权重)算出一个作为判断分类的一个分数s,s的计算方式如下\n逻辑函数将分数转换为类别\n然后有了分数s，就需要利用一个逻辑函数将s转换为0到1的值，0,1也就是分类的结果，0代表负类，1代表正类\n逻辑回归中在这一步与其他不同的是它含增加了一个sigmod函数，将分数转换到0到1之间的一个值\n得到这个0到1之间的值θ(s)，再将其与一个阈值做比较，一般是0.5，大于阈值的为正类1，小于阈值的为负类0\n最终得到分类模型与样本特征向量的关系公式如下\n应用分类函数进行类别判断\n得到分类模型(函数)之后，如果想知道某一样本的分类结果，常见的场景应用是将这个样本的特征向量作为x输入到这个函数，得到一个y值，将y值与阈值做比较，大于阈值的为正，小于阈值的为负。\n对分类函数(模型)的选取需要对数据有一定的了解或分析，知道或者猜测预测函数的\"大概\"形式，比如是线性函数还是非线性函数。\n损失函数\n要怎样去衡量输出的类别与实际的类别之间的差值呢，这就需要构造一个损失函数，该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）（机器学习实战中选取这种形式），也可以是其他的形式。\n综合考虑所有训练数据的\"损失\"，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。\n显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。\n找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（机器学习实战中用了梯度下降法和随机梯度下降法）。\n构造损失函数\n其实前面的分类函数h(x)还有一个深层次的含义，它代表了样本x属于类别1的概率\nAndrew Ng在课程中直接给出了Cost函数及J(θ)函数的公式，但是并没有给出具体的解释，只是说明了这个函数来衡量h函数预测的好坏是合理的。\n台大林轩田老师倒是从原理上推导了整个过程，但个人感觉这部分只是为了得到衡量分类结果的一个指标值，在此只写出部分推导过程，包括极大似然估计和梯度下降法的推导，其余的有兴趣的可以去看台大机器学习相关部分，讲的很清楚。\n极大似然推导\n对于之前的关于分类函数h代表样本属于类别1的概率问题\n将此式综合起来，即综合y=1和y=0的情况\n此式稍加观察可以发现和上面的式子代表的含义是一样的，y=1时，1-h(x)部分就没有了，y=0时，h(x)部门就没有了\n取似然函数为\n对数似然函数为\n最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解(机器学习实战书中即是用的梯度上升法)，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为下式，即：\n因为乘了一个负的系数-1/m，所以取J(θ)最小值时的θ为要求的最佳参数，求最大值就变为了求最小值，梯度上升法就变为了梯度下降法\nθ更新过程可以写成：\n过拟合问题\n对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。\n问题的主因\n过拟合问题往往源自过多的特征。\n解决方法\n1）减少特征数量（减少特征会失去一些信息，即使特征选的很好）\n可用人工选择要保留的特征；\n特征选择算法；\n2）正则化（特征较多时比较有效）\n保留所有特征，但减少θ的大小\n正规化解决过拟合实例\n首先多说一句，正则项可以取不同的形式\n在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为：\n其中lambda是正则项系数：\n如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象；\n如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。\n接下来我们看斯坦福机器学习公开课中的一个房价问题用正规化解决过拟合的例子\n左图是适当拟合，右图是过拟合。\n分析得到，过拟合是由于其中x三次方四次方项的存在引起的，造成模型过于复杂，直观上看也就是曲线过于弯曲\n如果我们想解决这个例子中的过拟合问题，最好能将x的三次方四次方项的影响消除，也就是让θ3和θ4尽可能的等于0；\n假设我们想这样做的话，一个简单的办法就是给原有的Cost函数的相应项加上两个略大惩罚项系数，例如：\n这样在最小化Cost函数的时候\n正则化后的梯度下降算法θ的更新变为：\n逻辑回归实战\n这部分主要是以机器学习实战中的例子为主导，自己也编写了整个算法的过程\n首先是加载数据部分\ndef loadDataSet():\ndataMat = []; labelMat = []\nfr = open('testSet.txt')\nfor line in fr.readlines():\nlineArr = line.strip().split()\ndataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\nlabelMat.append(int(lineArr[2]))\nreturn dataMat,labelMat\n定义了两个数组用于存放数据和类别\n加载后即基于数据和真实类别进行梯度下降法求取最佳的权重向量\ndef gradAscent(dataMatIn, classLabels):\ndataMatrix = mat(dataMatIn) #convert to NumPy matrix\nlabelMat = mat(classLabels).transpose() #convert to NumPy matrix\nm,n = shape(dataMatrix)\nalpha = 0.001\nmaxCycles = 500\nweights = ones((n,1))\nfor k in range(maxCycles): #heavy on matrix operations\nh = sigmoid(dataMatrix*weights) #matrix mult\nerror = (labelMat - h) #vector subtraction\nweights = weights + alpha * dataMatrix.transpose()* error #matrix mult\nreturn weights\n首先将数据数组转换为矩阵，方便之后的计算，alpha是步长，最大循环500次，权重向量初始化为全1向量\n将数据域权重向量相乘，输入sigmod函数，得到分类结果，此处的h是一个向量\n真实类别向量减去预测类别向量h得到分类的错误度，这也是一个向量\n然后基于这个错误更新权重\n这里的停止条件设为迭代500次\n最后会得到一个权重向量，而最佳分类线方程为0=wx，图形化如下\n改进\n由于上面的方法每次迭代都对所有的样本进行计算，计算量过大，所以一个改进的方法是每次迭代只对一个样本进行计算，更新权重也基于这个样本的损失函数进行更新，所以稍加改动，代码如下\ndef stocGradAscent0(dataMatrix, classLabels):\nm,n = shape(dataMatrix)\nalpha = 0.01\nweights = ones(n) #initialize to all ones\nfor i in range(m):\nh = sigmoid(sum(dataMatrix[i]*weights))\nerror = classLabels[i] - h\nweights = weights + alpha * error * dataMatrix[i]\nreturn weights\n但是结果并不太好，这是由于每次用一个样本进行权重的更新，由于有一些样本是很难正确区分的样本，那这些样本会在每次迭代的过程中引发系数也就是权重的剧烈波动，如图所示\n一个改进的方式是步长也随着迭代的次数进行改变，这样会缓解权重的波动\n进一步改进，代码如下\ndef stocGradAscent1(dataMatrix, classLabels, numIter=150):\nm,n = shape(dataMatrix)\nweights = ones(n) #initialize to all ones\nfor j in range(numIter):\ndataIndex = range(m)\nfor i in range(m):\nalpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not\nrandIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant\nh = sigmoid(sum(dataMatrix[randIndex]*weights))\nerror = classLabels[randIndex] - h\nweights = weights + alpha * error * dataMatrix[randIndex]\ndel(dataIndex[randIndex])\nreturn weights\n另外选择样本时不是第i次迭代时选择第i个样本，而是每次迭代都随机选择样本，这样可以减少周期性波动\n结果也很理想，与最开始用全部数据进行权重更新时的结果差不多，但计算量明显少于前面的方法"}
{"content2":"版本：0.1\n当今科学虽然非常发达了，但还是没能很好的理解和解释我们的世界。三个基本问题仍然困扰着我们：最小的是什么，最大的是什么和意识是什么。所谓最小，即最小的物质是什么。虽然我们证明了上帝粒子，快要证明各种粒子的统一和完备性了。但再往小了看呢，这些基本粒子又是什么组成的？这一层一层分析下去，是否有尽头？我们还要不断的了解更小粒子、逼近真理。所谓最大，我们可视的宇宙之外是什么？我们可视的宇宙之内，看不见的暗物质、暗能量是什么？是否有我们的世界和其完全没有作用力的黑物质和能量存在？最大和最小的问题也许是同一个问题。所谓意识，每个人都能感知到自己的存在。但意识和物质到底有没有关系？它在哪里？是否能创造、迁移和复制？对此，除了哲学家绞尽脑汁的思考之外，科学家对此基本也一无所知。研究意识的第一步，就是要研究人类智能。从现在的理解来看，人类智能有很大可能是意识的载体（软件上的软件？）。\n当代人工智能发展的时间和计算机的发展时间基本是一样长的。现代的计算机和刚出现的计算机相比，性能上提高了不止百万倍。人工智能也是最近一些年才开始了大规模的应用。其解决问题所需要的计算、开发成本还是比较高的。但强人工智能仍然还没什么眉目，最多就是能拼拼凑凑的能和人在某一方面比较，比如回答问题、下棋、端茶倒水之类的。这和人类做的事情的种类和学习能力是没法比较的。到底是什么阻碍了强人工智能的发展呢？总结起来基本上是三个方面，强人工智能的基本问题、计算规模的限制、现实利益驱动不够。\n基本问题\n强人工智能的一些问题还没有得到解决。虽然有很多科学家从细胞，分子生物，心理，大脑活跃度成像等各个角度角度来研究大脑。但不得不说，大脑对我们来说仍然是黑盒的。我们对它的认识还非常粗浅，一方面对大脑的各个层次的功能也只是碎片式的理解，另一方面，从对碎片理解来看，也没有推导出产生智能的系统是什么样的。\n强人工智能也可以算作是仿生学的范畴。从模仿的角度来看，有两种研究和应用方法，一种是全盘复制的模仿；另一种是理解核心原理，然后做出从工程上来说更好的设计。比如仿人机器人的设计可认为是全盘复制，这个好处就是现有的功能都有，而且人易于接受。全盘复制需要对其结构非常了解，仿人机器人需要对人体骨骼、自由度方向进行深入的研究。再比如飞机，是从鸟类飞翔开始，理解了空气动力学的核心原理，然后从机械上做出了更容易实现和维护的方案。理解核心原理需要更多的理论上的突破，通常都有新的学科产生。对于强人工智能来说，这两条路都不容易。结构上，神经元一方面是分子级别的反应，反应种类多、且蛋白质结构复杂。另一方面是上百亿神经元组成的网络，取一小块研究都是非常大的规模，且很难进行动态研究。从原理上，虽然单个细胞的行为有很多理论了， 但规模化以后为什么就产生智能了，还没有理论能说清楚。\n下面也仅仅列出了从现在的研究来看，所能发现的基本问题。由于信息量太大，知识之间交错不清，对这些基本问题的分类也不尽合理。随着这些问题的解决，估计还会出现新的基本问题。\n基本组织结构\n对于人类智能，从神经元细胞的层面已经有了大量的研究，这些研究有些为医学服务，还有些也是为了了解智能。在细胞层面上，科学家从比较宏观的角度研究了神经元的树突、轴突在信息刺激下的生长和衰减。也研究了信息如何从触觉神经元一路传递到脑部的过程。微观上还从分子、离子层面来研究传递信息的原理，特别是突触上信息是如何在树突、轴突直接传递，跨神经元的刺激如何才能激发成功。这些研究使我们理解了神经元的主要活动是整合、激发。神经元的树突、轴突会在刺激下有变化，产生或者消除一些突触，从而形成学习、记忆的效果。\n这个方向上的研究帮助理解了神经元的基本活动过程。人们已经能够构建基础的神经网络。但这样的神经网络只能模拟很小规模的神经元网络。但这还不够，别说人类的智力，连虫子的智力都还没到。神经元细胞本身是属于比较大的细胞，其细胞体是否直接参与了神经活动，还是只提供了物质和能量？记忆效果如何在细胞上累积？神经元细胞之间除了突触传递外，还有什么别的物质来协调它们的活动？哪些活动是和神经活动有关，哪些活动是和细胞的基本生存有关？由于对生物细胞本身活动过程的理解还不够，这样的问题还没法回答。\n宏观行为\n从宏观上，科学家们也进行了大量的研究。通过对大脑活动的各种成像方法和医学研究。对于人类大脑不同区域所负责的功能已经理解得比较清楚了，大脑不同位置上的结构的差异也有了理解。但这个层面上的研究粒度太粗了，基本上能把大脑分成上百块就不错了。这样算下来，每块平均有上亿个神经元细胞。记得有人说，这种宏观上的研究相当于用红外线摄像机从汽车外面研究车为什么会开一样，基本上是理解不了原理的。这方面研究的应用主要是在医学和心理学上。\n依靠人脑来研究人脑，有时就像自己要把自己拎起来一样，看似是不可能的任务。人、、的研究方式一般都是产生想法、然后逻辑推理。有句话叫做大胆猜想，小心求证。这基本上都要在大脑里进行一些模拟和推导。如果要对神经网络的宏观行为在大脑中进行模拟，至少也要模拟上千个神经元。这不是用10个神经元模拟1个，估计是用1000甚至10000个模拟1个。而且人脑的绝大部分神经元已经固化了别的任务，不会参与这个模拟过程。所以人脑本身对自己规模化的模拟几乎是不可能的，这个思考过程需要一些外部的帮助来完成。但想法的产生是迅速的，在快速验证原型前，可能大脑就把有些想法抛弃了，可能经常就是几个想法绕来绕去的，没有进展。所以，对这样的宏观问题的研究效率是很低的。这不像是对某些宏观情况，比如气象、星系演变类的研究建模，这些问题的计算量大，但其公式数量较少，程序架构也相对容易理解。\n所以，虽然对宏观上大脑怎样分工，和对微观上单个神经元是如何工作的都有较多的了解，但没有一个方法能将这两者统一起来，实现一个完备的系统。考虑到这整个系统是从代码量不大的DNA中的一小部分产生的，这个系统就越发显得神奇……\n模式的粒度\n所谓模式，也可以称为函数，即特定的输入能产生有规律的输出。神经网络和一般数学上函数的区别是，神经网络中有存储的功能，所以每次输入可能会影响下一次输出。这不影响一个神经网络被称为函数，但为了和一般函数的概念区分，下面还是称之为模式。\n人脑的整个神经网络可以称为一个模式，心理学、大脑成像等方面的研究就是在研究其模式。但这个模式太复杂了，输入输出很多，作为一个整体只能进行宏观上的研究。每个神经元，也可以称作一个模式，科学家们对其行为基本研究得不少了。但这个模式的功能又太简单了，和单个的晶体管原件差不多，能够解决的问题有限。\n现代很多神经网络和机器学习的算法，能够处理若干个神经元，这也可算做一个模式。对于这种模式，研究非常多，而且能够解决很多实际问题了。但这类模式的结构非常简单，一般是有清晰的层次关系，其输入也需要严格的标准化。人脑中一般不是分层的结构。不分层结构的主要挑战有，\n1）如何抑制整合、激发过程中的振荡。虽然有神经网络算法是不分层的，但很容易发生信号的振荡，环路等，不容易解决实际问题。\n2）如何有意义的创建、删除神经元之间的连接。这里的有意义除了是连接的调整能够有助于解决问题，或简化模型。另一方面，也需要这些调整是自组织的。所谓自组织，是通过神经元、或模式自身的信息就能够进行调整，而不是依靠大量宏观或外部的信息来进行调整。如果依靠很多宏观和外部的信息，其实就是把记忆、运算挪到了外部。这就不是一个可扩展的神经网络了。\n先不管这些问题的难度。如果我们找到了能够解决这些问题的算法，产生了模式。那么其内部是否可分为更小的模式呢？可以认为大脑就是不同规模模式的组合。由于神经元之间的组合很自由，可能很难清晰的划出各个模式之间的边界。可以认为一组神经元之间的连接较多，而且它们和外部的连接较少，就可称为一个模式。正因为模式边界的模糊性，对其的研究就更难了。\n大规模的模式整合\n如果要解决很复杂的问题，就需要将很多模式一起配合。在解决一些实际问题的时候，可能有好几个模式在使用，比如Cortana类的语音助手，有好几个模式组合在一起工作，才能实现所需要的功能。但每个模式的输入输出，模式之间该如何组合，都花费了大量的人力物力来设计和实现。\n在基本组织结构的模拟上，产生了神经网络和机器学习的各种算法。这些算法大部分都将神经网络特殊化为分层结构，否则很容易因为信号振荡、或自由度太大而无法有效输出结果。基本上是人需要花不少时间对问题进行建模，然后调整各种参数，从而输出比较理想的结果。机器学习的算法从规模上比人类是有很大优势的，由于人类大脑运算、输入输出速度的限制，是无法对大量数据进行有效的统计归纳，并发现其中规律的。当有了一个比较有效的模型，这就比较好做到了。这是普通机器学习智能的优势。大脑的优势在于其容纳了非常多的模式，而且很容易学会新的模式。\n随着所需要配合的模式的增加，软件复杂度也会随之增长。通过人工来设计模式、组合模式的效率和人的大脑相比，效率就低了。人脑里面的模式数量大概在千万级，通过人工的方式，几乎是不可能完成的任务。\n具体的问题有，\n1）如何高效的发现新模式。这里有两种思路，一种是设计一个模式产生器用来产生新的模式，并将其嵌入到已有的系统中。另一种是让神经网络自身能够产生新的模式。这样产生的模式之间可能并没有清晰的界限，更像是大脑中网络的结构。\n2）如何发现类似的模式，并将其合并。这对节省空间、提高计算效率、知识的举一反三都有很重要的意义。这个问题对于输入输出定义很清晰的模式较容易，如果是人脑这样的没有清晰的模式界限的，就比较挑战了。\n3）模型的通用性。大脑有一个一般化的网络结构，其灵活性非常大。即使某些专用区域失去了功能，如听力相关的区域，其它区域通过一些训练，能够恢复大部分功能。虽然某些部分，如小脑，有一些专用的变化，但其基本原理是类似的。这里面的挑战在于，如何能够创建一个灵活的结构，可以组成不同等级的模式。\n通用语言\n这里说的语言不是计算机编程的语言，也不是用于人和人之间交流的语言，而是适合强人工智能内部交流的语言。人类大脑中是有这样的语言的，平常说的只能意会，不能言传，就是内部语言的表现。所谓意会就是出了人类语言之外的其它知识集或者感官可以理解的东西，但很难翻译成人类语言。这种语言是大脑中比较高级的交流层次，是我们的主观意识能够感受到的最低层次的活动。最低层次的神经活动就是神经元之间的激发整合了，我们应该是很难感受到的。\n通用语言的优势在于能对各种概念、物体一层一层的归纳、建模。从而能够描述非常复杂的对象或者抽象活动等。\n大脑中能产生这种通用语言，和它自由、混沌的网络结构是分不开的。大脑通过神经网络的自组织，完成了各种概念层次的自组织。它的语言体系不是先定义后实现的，而是一边输入、一边定义。这也是人为什么年纪越大，经验越丰富。因为通用语言的适应范围随着输入的增长而变大。\n说到这里，我一直感觉我们的思想被冯诺依曼系统和现代计算机指令集限制了。我们走上了一条捷径，创建了当今看似高度发达的计算机世界，但其实一直是在用硬编码、强介入的方式来解决问题。人类花了很多智力资源用来和计算机系统沟通，比如，程序员。\n世界模型\n每个人的大脑中对这个世界都有一个模型，从哲学角度也叫世界观。这个模型表达了人对这个世界的认知，包含了人所有的知识，对具体事物和抽象事物的理解。\n虽然每个大脑先天上有所不同，后天的成长环境也不同，每个人的世界模型之间有一些偏差，但绝大部分都是一样的。这就像DNA一样，人和其它动物间的差别不大，别说人和人之间了，但人的个体差异从外表上也是很显著的。这样的个体差异也同时包括了先天和后天环境产生的差异。\n当前的机器学习应用中的模型，一般都是单一或者有限的几个模式。这样的模型高度简化了真实世界，虽然降低了每个单独的问题难度，但是通用性很差。如果能有和人脑中类似的世界模型，绝大部分东西就可以重用了。这样就能在已知的基础上累积未知，在解决新问题时所需要的人工介入就很少了。\n世界的模型是一个非常复杂的模型，估计包括了千万级的模式。其复杂性以当前的人口基数和软件管理水平是不能通过一行一行代码堆出来的。所以，需要通过大规模的模式整合来实现。一个能够自动化产生各种模式并吸收知识的系统。这和人类从小到大所接受的教育是类似的。另外，人脑的强大除了学习，还有归纳。对于类似的模式，人脑是能够将其合并的，这样的合并不仅能节省空间，还能有效的发现未知的模式和提高响应速度。另外，人脑对知识的有效的有损压缩也是很有用的。大脑是永远存不满的，它会不断的将知识合并、整理、遗忘。现在的计算机架构很难做到这一点。\n这些基本问题也不完全是停滞不前的，还是有一些进展的。比如多层神经网络也有了一定的发展，比如深度学习。再比如，控制非分层的神经网络中振荡的问题，也有些比较简单的算法来处理了。\n计算规模\n虽然当前的计算能力已经很强了，但强人工智能所需要的规模也是非常大的。按照通常的计算，人类有大约120亿个灰质细胞，每个细胞有数百到上万个树突，假设平均每个细胞会有5000个树突形成突触。这样算下来，突触的数量大约有10的13次方。从存储上来看，假设每个突触有1k的存储量，总共是5PB的存储量，虽然很巨大，但现在的存储系统已经不难解决了。从计算量上看，假设人脑的频率是100Hz，每次活动有1%的突触参与。由于当前的计算机不是为神经活动设计的，所以需要用当前的指令集来模拟神经活动，假设每一次突触活动需要100个处理器周期。假设用8核2.4G的处理器，算下来大概需要29万个处理器，才能达到人类的运算速度和规模。虽然现在的超级计算机也有上十万个处理器，但不是简单的累加就可以达到同样的计算能力。整个系统内的数据存取速度，通讯速度也是需要考虑的。在大规模的并行计算下由于处理器之间的通讯，协调等问题，计算效率是会下降的。另外，这只是一个人脑规模所需要的计算资源，如果要支持多个团队的研究，所需要的资源就要数倍了。\n现在IBM等公司已经在制造为神经网络优化的计算机了，且不说其架构是否和强人工智能所匹配。但从能耗、成本、并行规模上应该比现在的处理器架构更适用于强人工智能。即使这样，也需要可观的规模才能有足够模拟一个人脑的资源。\n现实利益驱动\n在人工智能发展的过程中有一个冷门期，大概是上世纪70年代。当时，一方面理论上没什么突破，另一方面也没有找到实际的应用。所以研究机构、政府、企业等没有动力继续支持研究，大部分科学家们也因为不容易出成果也就不研究了。直到那些不撞南墙不回头的科学家发现了新的理论、新的应用方向，才将大量的资源和科学家重新吸引过来。\n当前人工智能、机器学习方向虽然看起来热热闹闹的，但主要还是集中在应用领域。愿意对强人工智能投入的组织和科学家有不少，但不是主流。虽然无人质疑强人工智能的应用范围，但对其可预见范围内能实现的信心不足，对其所需要的投资仍是敬而远之。所以人工智能虽然复苏了，强人工智能的研究可以说仍然是在被冷落中。一旦对人类智能的认识和模拟达到了新的高度，强人工智能有了更切实的时间表，各种资源就会蜂拥而至。这和资本主义的逐利性是一样的，从积极的方面来看，也是资源的合理调配。\n综观全文，主要篇幅都在讲基本问题的挑战，所以主要不在于运算规模和资源投入，还是要解决一系列的基本问题。从基本问题的难度上来看，如果有各种资源的投入，这个过程是能加快的。通过人脑来超越人脑，实现强人工智能，需要规模化、系统化的研究，不停的迭代和积累，才会成功。"}
{"content2":"为什么一些机器学习模型需要对数据进行归一化？\nhttp://www.cnblogs.com/LBSer/p/4440590.html\n机器学习模型被互联网行业广泛应用，如排序（参见：排序学习实践）、推荐、反作弊、定位（参见：基于朴素贝叶斯的定位算法）等。一般做机器学习应用的时候大部分时间是花费在特征处理上，其中很关键的一步就是对特征数据进行归一化，为什么要归一化呢？很多同学并未搞清楚，维基百科给出的解释：1）归一化后加快了梯度下降求最优解的速度；2）归一化有可能提高精度。下面我简单扩展解释下这两点。\n1 归一化为什么能提高梯度下降法求解最优解的速度？\n斯坦福机器学习视频做了很好的解释：https://class.coursera.org/ml-003/lecture/21\n如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；\n而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。\n因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。\n2 归一化有可能提高精度\n一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。\n3 归一化的类型\n1）线性归一化\n这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。\n2）标准差标准化\n经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n3）非线性归一化\n经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。"}
{"content2":"在用PMML实现机器学习模型的跨平台上线中，我们讨论了使用PMML文件来实现跨平台模型上线的方法，这个方法当然也适用于tensorflow生成的模型，但是由于tensorflow模型往往较大，使用无法优化的PMML文件大多数时候很笨拙，因此本文我们专门讨论下tensorflow机器学习模型的跨平台上线的方法。\n1. tensorflow模型的跨平台上线的备选方案\ntensorflow模型的跨平台上线的备选方案一般有三种：即PMML方式，tensorflow serving方式，以及跨语言API方式。\nPMML方式的主要思路在上一篇以及讲过。这里唯一的区别是转化生成PMML文件需要用一个Java库jpmml-tensorflow来完成，生成PMML文件后，跨语言加载模型和其他PMML模型文件基本类似。\ntensorflow serving是tensorflow 官方推荐的模型上线预测方式，它需要一个专门的tensorflow服务器，用来提供预测的API服务。如果你的模型和对应的应用是比较大规模的，那么使用tensorflow serving是比较好的使用方式。但是它也有一个缺点，就是比较笨重，如果你要使用tensorflow serving，那么需要自己搭建serving集群并维护这个集群。所以为了一个小的应用去做这个工作，有时候会觉得麻烦。\n跨语言API方式是本文要讨论的方式，它会用tensorflow自己的Python API生成模型文件，然后用tensorflow的客户端库比如Java或C++库来做模型的在线预测。下面我们会给一个生成生成模型文件并用tensorflow Java API来做在线预测的例子。\n2. 训练模型并生成模型文件\n我们这里给一个简单的逻辑回归并生成逻辑回归tensorflow模型文件的例子。\n完整代码参见我的github:https://github.com/ljpzzz/machinelearning/blob/master/model-in-product/tensorflow-java\n首先，我们生成了一个6特征，3分类输出的4000个样本数据。\nimport numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets.samples_generator import make_classification import tensorflow as tf X1, y1 = make_classification(n_samples=4000, n_features=6, n_redundant=0, n_clusters_per_class=1, n_classes=3)\n接着我们构建tensorflow的数据流图，这里要注意里面的两个名字，第一个是输入x的名字input,第二个是输出prediction_labels的名字output，这里的这两个名字可以自己取，但是后面会用到，所以要保持一致。\nlearning_rate = 0.01 training_epochs = 600 batch_size = 100 x = tf.placeholder(tf.float32, [None, 6],name='input') # 6 features y = tf.placeholder(tf.float32, [None, 3]) # 3 classes W = tf.Variable(tf.zeros([6, 3])) b = tf.Variable(tf.zeros([3])) # softmax回归 pred = tf.nn.softmax(tf.matmul(x, W) + b, name=\"softmax\") cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1)) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) prediction_labels = tf.argmax(pred, axis=1, name=\"output\") init = tf.global_variables_initializer()\n接着就是训练模型了，代码比较简单，毕竟只是一个演示：\nsess = tf.Session() sess.run(init) y2 = tf.one_hot(y1, 3) y2 = sess.run(y2) for epoch in range(training_epochs): _, c = sess.run([optimizer, cost], feed_dict={x: X1, y: y2}) if (epoch+1) % 10 == 0: print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)) print (\"优化完毕!\") correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y2, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) acc = sess.run(accuracy, feed_dict={x: X1, y: y2}) print (acc)\n打印输出我这里就不写了，大家可以自己去试一试。接着就是关键的一步，存模型文件了，注意要用convert_variables_to_constants这个API来保存模型，否则模型参数不会随着模型图一起存下来。\ngraph = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"output\"]) tf.train.write_graph(graph, '.', 'rf.pb', as_text=False)\n至此，我们的模型文件rf.pb已经被保存下来了，下面就是要跨平台上线了。\n3. 模型文件在Java平台上线\n这里我们以Java平台的模型上线为例，C++的API上线我没有用过，这里就不写了。我们需要引入tensorflow的java库到我们工程的maven或者gradle文件。这里给出maven的依赖如下，版本可以根据实际情况选择一个较新的版本。\n<dependency> <groupId>org.tensorflow</groupId> <artifactId>tensorflow</artifactId> <version>1.7.0</version> </dependency>\n接着就是代码了，这个代码会比JPMML的要简单，我给出了4个测试样本的预测例子如下，一定要注意的是里面的input和output要和训练模型的时候对应的节点名字一致。\nimport org.tensorflow.*; import org.tensorflow.Graph; import java.io.IOException; import java.nio.file.Files; import java.nio.file.Paths; /** * Created by 刘建平pinard on 2018/7/1. */ public class TFjavaDemo { public static void main(String args[]){ byte[] graphDef = loadTensorflowModel(\"D:/rf.pb\"); float inputs[][] = new float[4][6]; for(int i = 0; i< 4; i++){ for(int j =0; j< 6;j++){ if(i<2) { inputs[i][j] = 2 * i - 5 * j - 6; } else{ inputs[i][j] = 2 * i + 5 * j - 6; } } } Tensor<Float> input = covertArrayToTensor(inputs); Graph g = new Graph(); g.importGraphDef(graphDef); Session s = new Session(g); Tensor result = s.runner().feed(\"input\", input).fetch(\"output\").run().get(0); long[] rshape = result.shape(); int rs = (int) rshape[0]; long realResult[] = new long[rs]; result.copyTo(realResult); for(long a: realResult ) { System.out.println(a); } } static private byte[] loadTensorflowModel(String path){ try { return Files.readAllBytes(Paths.get(path)); } catch (IOException e) { e.printStackTrace(); } return null; } static private Tensor<Float> covertArrayToTensor(float inputs[][]){ return Tensors.create(inputs); } }\n我的预测输出是1,1,0,0，供大家参考。\n4. 一点小结\n对于tensorflow来说，模型上线一般选择tensorflow serving或者client API库来上线，前者适合于较大的模型和应用场景，后者则适合中小型的模型和应用场景。因此算法工程师使用在产品之前需要做好选择和评估。\n（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）"}
{"content2":"机器学习算法原理、实现与实践——机器学习的三要素\n1 模型\n在监督学习中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是这些线性函数构成的函数的集合。\n假设空间用$\\mathcal{F}$表示。假设空间可以定义为决策函数的集合\n$$\\mathcal{F}=\\{f|Y=f(X)\\}$$\n其中，$X$和$Y$是定义在输入空间$\\mathcal{X}$和输出空间$\\mathcal{Y}$上的变量。这时$\\mathcal{F}$通常是由一个参数向量决定的函数族\n$$\\mathcal{F}=\\{f|Y=f_{\\theta}(X),\\theta\\in \\mathbf{R}^n\\}$$\n参数向量$\\theta$取值于$n$维欧氏空间$\\mathbf{R}^n$，称为参数空间（parameter space）。\n假设空间也可以定义为条件概率的集合\n$$\\mathcal{F}=\\{P|P(Y|X)\\}$$\n其中，$X$和$Y$是定义在输入空间$\\mathcal{X}$和输出空间$\\mathcal{Y}$上的变量。这时$\\mathcal{F}$通常是由一个参数向量决定的条件概率分布族\n$$\\mathcal{F}=\\{P|P_{\\theta}(Y|X),\\theta\\in \\mathbf{R}^n\\}$$\n称由决策函数表示的模型为非概率模型，由条件概率表示的模型为概率模型。\n2 策略\n有了模型的假设空间，机器学习接着要考虑的是按照什么样的准则学习或选择最优的模型。\n首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。\n2.1 损失函数与风险函数\n对于给定的输入$X$和假设空间$\\mathcal{F}$中选择的决策函数模型$f$，由$f(X)$给出相应的输入$Y$，这个输出的预没值$f(X)$与真实值$Y$可能一致，也可能不一致，用一个损失函数或代价函数来度量预测的错误程度。损失函数是$f(x)$和$Y$的非负实值函数，记作$L(Y,f(X))$\n几种常用的损失函数：\n1） 0-1损失函数(0-1 loss function)\n$$L(Y,f(X)) = \\begin{cases}1, &Y\\neq f(X) \\\\ 0, & Y=f(X)\\end{cases}$$\n2） 平方损失函数（quadratic loss function）\n$$L(Y,f(X)) = (Y – f(X))^2$$\n3）绝对损失函数（absolute loss function）\n$$L(Y,f(X)) = |Y-f(X)|$$\n4） 对数损失函数（logarithmic loss function）或对数似然损失函数\n$$L(Y,P(Y|X)) = –logP(Y|X)$$\n损失函数值越小，模型就越好。由于模型的输入、输出$(X,Y)$是随机变量，遵循联合分布$P(X,Y)$，所以损失函数的期望是\n$$R_{exp}(f)=E_P[L(Y,f(X))]=\\int_{\\mathcal{X}\\times\\mathcal{Y}}L(y,f(x))P(x,y)dxdy$$\n这是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数(risk function)或期望损失(expected loss)。\n学习的目标就是选择期望风险最小的模型。由于联合分布$P(X,Y)$是所有样本所遵循的统计规律，它是未知的，所以$R_{exp}(f)$不能直接计算。实际上如果知道了联合分布，那么可以直接计算出$P(Y|X) = \\int_{\\mathcal{X}}P(x,y)dx$，也就不需要学习了。\n所以用上面那种方式定义风险函数是不行的，那样的话监督学习变成了一个病态问题。\n对于给定的训练数据集\n$$T={(x_1,y_1),(x_2,y_2),\\dots,(x_N,y_N)}$$\n模型$f(X)$关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss），记作$R_{emp}$：\n$$R_{emp}(f) = \\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))$$\n期望风险$R_{exp}(f)$是模型关于联合分布的期望损失，经验风险$R_{emp}(f)$是模型关于训练样本集的平均损失。根据大数定律，当样本容量$N$趋于无穷时，经验风险$R_{emp}(f)$趋向于期望风险$R_{exp}(f)$\n所以，一个很自然的想法是用经验风险估计期望风险。但是，由于现实中训练样本数目很有限，所以用经验风险估计期望风险常常不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。\n2.2 经验风险最小化与结构风险最小化\n在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式就可以确定。经验风险最小化的策略认为，经验风险最小的模型就是最优的模型。根据这一策略，按照经验风险最小化求最佳模型就是求解最优化问题：\n$$\\min_{f\\in\\mathcal{F}}\\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))$$\n其中$\\mathcal{F}$是假设空间。\n当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中广泛采用。比如极大似然估计就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。\n但是当样本容量很小时，经验风险最小化学习效果就未必很好，会产生“过拟合(over-fitting)”现象。\n结构风险最小化（structural risk minimization SRM）是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化。结构风险在经验风险上加上表示模型复杂度的正则化项或罚项。在假设空间，损失函数以及训练样本集确定的情况下，结构风险的定义是\n$$R_{srm}(f) = \\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))+\\lambda J(f)$$\n其中$J(f)$为模型的复杂度，是定义在假设空间$\\mathcal{F}$上的泛函。模型$f$越复杂，复杂度$J(f)$就越大；反之，模型$f$越简单，复杂度$J(f)$就越小。也就是说复杂度表示了对复杂模型的惩罚。$\\lambda\\ge 0$是系数，用以权衡经验风险和模型的复杂度。结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。\n比如，贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation,MAP）就是结构风险最小化的例子。当模型是条件概率分布、损失函数就是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。\n结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求最优化模型时，就是求解最优化问题：\n$$\\min_{f\\in\\mathcal{F}}\\frac{1}{N}L(y_i,f(x_i))+\\lambda J(f)$$\n这样，监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的目标函数。\n3 算法\n从上面可以看出，在确定寻找最优模型的策略后\n机器学习的问题归结为最优化的问题。机器学习讨论的算法问题就成为了求解最优化模型解的算法。而且往往最优化模型没有的解析解，需要用数值计算的方法求解，我们要确保找到全局最优解，以及使求解的过程非常高效。"}
{"content2":"转自：https://www.zhihu.com/question/49321024\nXylt\n游戏、游戏开发 话题的优秀回答者\n既然你提到了Façade，那可以进一步了解一下用到了增强学习，曾斩获不少学院派游戏大奖的《黑与白》（Black and White）。\n《黑与白》围绕增强学习的特点打造核心玩法，让玩家扮演反馈提供者的角色，通过上帝视角的扇巴掌和戳怪物等反馈，来影响游戏中各个生物的表现，从而培养出有复杂行为能力的智能生物。游戏中的生物使用一种围绕信念-欲望-意图设计的行为框架，把游戏物件、生物要达成的目标、以及达成目标所需的行为树联系起来，产生出复杂的生物表现。\n总的来说，一般游戏使用机器学习技术的思路有两种，一种是利用“学习”能力，即让AI适应玩家或者模仿玩家，也是这里《黑与白》所使用的，但玩家能给出的学习样本是非常有限的，很难保证效果到位；另一种是利用学习后得出的“成果”，即得到能解决某一特定问题的强力AI，但未必能给游戏本身的娱乐性带来太大的提升。\n单纯使用机器学习的“成果”部分来提升游戏AI水平的学术研究已经有很多了，但一般而言一个模型只能用来解决复杂游戏中的一个子问题，比如《文明》中的城市选址，《Quake》中特定模式的最佳团队策略等。各方面都采用机器学习技术来打造AI也不是不可能，但是相比传统的状态机行为树等做法，提升的只是AI的实力，服务的只是少部分核心玩家，付出的开发成本（时间、技术人才、试错）却一定不少。\n传统游戏AI的一个特点是强调可预测性(predictable)，一方面是让玩家能够通过AI的表现很容易地倒推出AI的行为准则，然后在了解其规律的基础上找寻更好的解法，从而提升自己的游戏水平；另一方面具备强可预测性的AI能保证在动辄以十万为基数的玩家群体中，不会出现难以预料的极端情况破坏体验，比如无限关卡马里奥就需要保证不会因为随机组合而出现玩家理论上不可能通过的障碍。游戏希望的是，玩家败给AI是因为没找到它的弱点，而不是单纯因为AI不可战胜，这点与提升AI实力的做法是背道而驰的。\n机器学习做游戏AI也不是唯一出路，有在游戏的迭代检验部分运用相关技术的可行性。比如我做一款类似星际争霸的游戏，里面存在游戏策略的相互克制关系，那我可以通过多个机器学习模型之间的竞争去提前演算出“天梯环境”的变化，对游戏平衡有一个直观的感受；比如上面提到的无限关卡马里奥等用到过程内容生成技术的时候，结合机器学习技术来进行动态难度调整（DDA），而非设计师的脑补。在这些方向上，机器学习更像是设计师的一种工具，运用在生产过程中，而不是游戏本体中，用来减少设计偏差，从而控制游戏质量。\n既然用机器学习提升AI的实力没有多少实际运用的意义，真正能让机器学习技术在游戏中作为核心地位来使用的，还是要靠游戏玩法上的突破性和不可替代性来支持。利用机器学习本身的“学习”能力而不是学习成果，去彻底改变玩家与游戏的互动方式。\n《黑与白》已经是一个很具开创性的方向了，但现在游戏圈大环境浮躁保守，外加本身的技术门槛，这部作品后数十年都没有值得一提的后来者出现了。等传统玩法消磨殆尽，机器学习的技术进一步普及以后，或许游戏行业会往这上面去做突破吧。\n不算参考资料的参考资料：\n（机器学习给PCG做DDA）Jennings-Teats, M., Smith, G., & Wardrip-Fruin, N. (2010, October). Polymorph: A model for dynamic level generation. In Proceedings of the Sixth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (pp. 138-143). AAAI Press.\n（增强学习-文明4城市选址）Wender, S., & Watson, I. (2008). Using reinforcement learning for city site selection in the turn-based strategy game Civilization IV. 2008 IEEE Symposium On Computational Intelligence and Games. doi:10.1109/cig.2008.5035664\n（遗传算法迭代FPS团队策略）Liaw, C., Wang, W. H., Tsai, C. T., Ko, C. H., & Hao, G. (2013). Evolving a team in a first-person shooter game by using a genetic algorithm. Applied Artificial Intelligence, 27(3), 199-212.\n编辑于 2017-05-11\n任春旭\n游戏AI工程师\n我不完全赞成其它答案里说游戏不需要AI太聪明的说法，我认为现在没听说哪个游戏用上智能算法的原因是开发一个游戏AI的成本太高，而成本太高的原因是游戏上的人工智能技术还不成熟。\n我在为一款MOBA游戏做游戏AI，已经做了一年半了，可能是国内花时间在这方面最多的人之一。在玩家测试的过程中，我发现玩家其实是对高质量的机器人对手有需求的。因为MOBA游戏的平均胜率永远只能是50%，一定会有许多玩家因为赢少输多而流失。另外很多玩家是很难接受80%以下的胜率的，玩家匹配没法满足他们的这个需求。所以无论是Dota2还是LOL，都有一批忠实于人机对战的玩家。\n而且如果你玩过王者荣耀，可能发现AI还有另一个用处，在这里就不细说了。\n另外对于策略类游戏，现有的游戏的AI明显是过于弱了。例如文明系列、钢铁雄心系列、三国志系列，甚至已经弱到了影响后期游戏体验的地步，你把城市托管给电脑，它能给你管成一坨屎。虽然这些游戏可能确实不需要有多强的AI，但也不意味着它们现在的水平满足玩家的需求。\nAI强和AI是陪玩家玩的并不矛盾，强大的AI可以陪更多水平层次的玩家玩，对于水平比较菜的玩家，只需要把难度调低就行了。\n但是开发一个好的游戏AI成本不低。\n如果用传统的行为树和状态机，那么一个复杂的游戏AI对程序的要求是很高的，需要有很强的调试能力。游戏AI开发不同于其它类型的开发，游戏AI是很难测试的。程序里的分支实在是太多了，依靠游戏行业的QA来帮助测试起到的作用很有限。所以很大程度上只能靠程序员的自律，很多时候只有开发AI的那个人才知道这些分支之间的转换关系。\n如果靠新技术，问题来了，互联网上的人工智能技术并不能直接拿到游戏开发上，需要重新进行研究，重新研究不仅需要大量成本，而且会有失败的可能性。让一个游戏失败的可能已经够多了，游戏行业的投资人和制作人们是不愿意花这么多钱在一个不一定能成功的方向上的。\n他们不愿意花钱的原因，其中一个恰恰是不懂技术的人总是会认为做一个优秀的AI很简单，知乎上的另一个关于游戏AI的问题，500多个回答，里面90%的回答都是认为游戏公司可以做出吊打人类的AI，只是游戏公司不愿意而已，里面甚至还有很多程序员。\n他们犯的错在于，看到了AI的一点亮眼的表现，例如补刀准，躲技能灵敏，就觉得AI只要照这样做下去就无敌了，但实际上随着AI的越来越复杂，开发难度增长之快让人难以想象。\nLOL有个模式是噩梦人机，里面的人机对手技能伤害和范围都被大幅增强，但在刚出来的时候还是经常被玩家偷塔获胜。后来他们在下一次出噩梦人机的时候，将它完全修改成另一种游戏模式，不允许玩家偷塔了，可以看出这是一次对AI开发的难度的妥协。\n综上，游戏AI看上去的难度和游戏AI实际上的难度之间的差异，加上游戏行业巨大的盈利压力，加上基础研究的不成熟，导致了目前还没有可能做出一个拥有非常智能的AI的游戏。随着人工智能研究深度和广度的扩展，这一天应该就在不久的将来。\n编辑于 2017-05-11\n猴与花果山\n如果是说AI方面的话，那其实没有必要，我只想说——游戏AI和传统AI是两个领域。\n游戏AI主要有两方面，一方面是关卡设计方面（包括怪物AI之类的），可以称之为“对手型AI”；一方面是题主你说的NPC的人工智能问题，可以称之为“交互型AI”。\n2013年的时候，我们做手游做了一个火纹Like的手游（结果没上就挂了），在那个游戏中，我研究了很多AI的算法，还有战棋类SLG的玩法、经典战术等（包括火纹、曹操传、梦幻模拟战等游戏，我研究了很多玩家的玩法），我最初的目的是制作出一个非常牛逼的AI，让玩家难以战胜，并且我做到了，AI的一些走法让人感觉妙不可言，但是后来我还是否定了这样一个AI，因为游戏AI并不是要聪明到让玩家惊讶甚至无法战胜的，游戏AI终究是陪玩家玩的，好的游戏AI是让玩家能够摸出规律的。从理论到现实中，我们也看到了，一般的神作AI都是有规律可循的，其实这个规律让玩家能够摸透，就是一种玩法，玩家利用对于AI的理解，安排打法战术，通过训练最后战胜AI甚至可以完胜，这时候才是玩家玩游戏最佳体验的时候。所以玩家真正想要的对手型AI并不是那种“特别聪明”的，于是我带着这个思路重新审视了这些游戏的AI，发现他们的确也是故意做的有规律可循的，我想这就是“对手型AI”应有的调性（Motif）。\n而交互型AI，我们也曾努力尝试去研究，期望让游戏能营造出一个更真实世界的体验，我们相信未来有一天AR的时代到来的时候（但我却从不看好VR，[深入思考]现代VR游戏的致命伤在于应用情景 - GameRes游资网），我们会需要这样的AI去做游戏（或者是游戏化）。但是这个想法就今天而言，距离“合适”还有很长的时间要等待，于是我们姑且说今天的交互型AI，为什么我们没有去用Deep learning来做呢？我们抛开实现难度等问题，只能说——这样的AI是有趣的(fun)，但是它并不能让用户对游戏产生依赖性（hooked），也并不能很好的帮助游戏培养用户习惯（游戏最终粘稠住玩家的手段和赌博是一样的——培养习惯）。我们曾经思考过游戏化培养习惯的方案，其中有多人交互的方式（类似匿名戒酒社、传教等模式），也思考过是否AI终究能在某些环节代替人，但是我们发现这种就是一个伪命题，因为人真正想要的社交是人与人之间（比如陌生人的陌陌、熟人的微信）的。\n所以这个问题的答案，我只能总结为——游戏AI和传统AI并不是一个领域的。"}
{"content2":"课程简介:\n人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。\n人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。 2017年12月，人工智能入选“2017年%\n下载地址：百度网盘下载"}
{"content2":"一、要解决的问题\n问题：常常一些单位或组织召开会议时需要录入会议记录，我们需要通过机器学习对用户输入的文本内容进行自动评判，合格或不合格。（同样的问题还类似垃圾短信检测、工作日志质量分析等。）\n处理思路：我们人工对现有会议记录进行评判，标记合格或不合格，通过对这些记录的学习形成模型，学习算法仍采用二元分类的快速决策树算法，和上一篇文章不同，这次输入的特征值不再是浮点数，而是中文文本。这里就要涉及到文本特征提取。\n为什么要进行文本特征提取呢？因为文本是人类的语言，符号文字序列不能直接传递给算法。而计算机程序算法只接受具有固定长度的数字矩阵特征向量(float或float数组)，无法理解可变长度的文本文档。\n常用的文本特征提取方法有如下几种：\n以上只是需要了解大致的含义，我们不需要去实现一个文本特征提取的算法，只需要使用平台自带的方法就可以了。\n系统自带的文本特征处理的方法，输入是一个字符串，要求将一个语句中的词语用空格分开，英语的句子中词汇是天生通过空格分割的，但中文句子不是，所以我们需要首先进行分词操作，具体流程如下：\n二、代码\n代码整体流程和上一篇文章描述的基本一致，为简便起见，我们省略了模型存储和读取的过程。\n先看一下数据集：\n代码如下：\nnamespace BinaryClassification_TextFeaturize { class Program { static readonly string DataPath = Path.Combine(Environment.CurrentDirectory, \"Data\", \"meeting_data_full.csv\"); static void Main(string[] args) { MLContext mlContext = new MLContext(); var fulldata = mlContext.Data.LoadFromTextFile<MeetingInfo>(DataPath, separatorChar: ',', hasHeader: false); var trainTestData = mlContext.Data.TrainTestSplit(fulldata, testFraction: 0.15); var trainData = trainTestData.TrainSet; var testData = trainTestData.TestSet; var trainingPipeline = mlContext.Transforms.CustomMapping<JiebaLambdaInput, JiebaLambdaOutput>(mapAction: JiebaLambda.MyAction, contractName: \"JiebaLambda\") .Append(mlContext.Transforms.Text.FeaturizeText(outputColumnName: \"Features\", inputColumnName: \"JiebaText\")) .Append(mlContext.BinaryClassification.Trainers.FastTree(labelColumnName: \"Label\", featureColumnName: \"Features\")); ITransformer trainedModel = trainingPipeline.Fit(trainData); //评估 var predictions = trainedModel.Transform(testData); var metrics = mlContext.BinaryClassification.Evaluate(data: predictions, labelColumnName: \"Label\"); Console.WriteLine($\"Evalution Accuracy: {metrics.Accuracy:P2}\"); //创建预测引擎 var predEngine = mlContext.Model.CreatePredictionEngine<MeetingInfo, PredictionResult>(trainedModel); //预测1 MeetingInfo sampleStatement1 = new MeetingInfo { Text = \"支委会。\" }; var predictionresult1 = predEngine.Predict(sampleStatement1); Console.WriteLine($\"{sampleStatement1.Text}:{predictionresult1.PredictedLabel}\"); //预测2 MeetingInfo sampleStatement2 = new MeetingInfo { Text = \"开展新时代中国特色社会主义思想三十讲党员答题活动。\" }; var predictionresult2 = predEngine.Predict(sampleStatement2); Console.WriteLine($\"{sampleStatement2.Text}:{predictionresult2.PredictedLabel}\"); Console.WriteLine(\"Press any to exit!\"); Console.ReadKey(); } } public class MeetingInfo { [LoadColumn(0)] public bool Label { get; set; } [LoadColumn(1)] public string Text { get; set; } } public class PredictionResult : MeetingInfo { public string JiebaText { get; set; } public float[] Features { get; set; } public bool PredictedLabel; public float Score; public float Probability; } }\nView Code\n三、代码分析\n和上一篇文章中相似的内容我就不再重复解释了，重点介绍一下学习管道的建立。\nvar trainingPipeline = mlContext.Transforms.CustomMapping<JiebaLambdaInput, JiebaLambdaOutput>(mapAction: JiebaLambda.MyAction, contractName: \"JiebaLambda\") .Append(mlContext.Transforms.Text.FeaturizeText(outputColumnName: \"Features\", inputColumnName: \"JiebaText\")) .Append(mlContext.BinaryClassification.Trainers.FastTree(labelColumnName: \"Label\", featureColumnName: \"Features\"));\n首先，在进行文本特征转换之前，我们需要对文本进行分词操作，您可以对样本数据进行预处理，形成分词的结果再进行学习，我们没有采用这个方法，而是自定义了一个分词处理的数据处理管道，通过这个管道进行分词，其定义如下：\nnamespace BinaryClassification_TextFeaturize { public class JiebaLambdaInput { public string Text { get; set; } } public class JiebaLambdaOutput { public string JiebaText { get; set; } } public class JiebaLambda { public static void MyAction(JiebaLambdaInput input, JiebaLambdaOutput output) { JiebaNet.Segmenter.JiebaSegmenter jiebaSegmenter = new JiebaNet.Segmenter.JiebaSegmenter(); output.JiebaText = string.Join(\" \", jiebaSegmenter.Cut(input.Text)); } } }\n最后我们新建了两个对象进行实际预测：\n//预测1 MeetingInfo sampleStatement1 = new MeetingInfo { Text = \"支委会。\" }; var predictionresult1 = predEngine.Predict(sampleStatement1); Console.WriteLine($\"{sampleStatement1.Text}:{predictionresult1.PredictedLabel}\"); //预测2 MeetingInfo sampleStatement2 = new MeetingInfo { Text = \"开展新时代中国特色社会主义思想三十讲党员答题活动。\" }; var predictionresult2 = predEngine.Predict(sampleStatement2); Console.WriteLine($\"{sampleStatement2.Text}:{predictionresult2.PredictedLabel}\");\n预测结果如下：\n四、调试\n上一篇文章提到，当我们运行Transform方法时，会对所有记录进行转换，转换后的数据集是什么样子呢，我们可以写一个调试程序看一下。\nvar predictions = trainedModel.Transform(testData); DebugData(mlContext, predictions); private static void DebugData(MLContext mlContext, IDataView predictions) { var trainDataShow = new List<PredictionResult>(mlContext.Data.CreateEnumerable<PredictionResult>(predictions, false, true)); foreach (var dataline in trainDataShow) { dataline.PrintToConsole(); } } public class PredictionResult { public string JiebaText { get; set; } public float[] Features { get; set; } public bool PredictedLabel; public float Score; public float Probability; public void PrintToConsole() { Console.WriteLine($\"JiebaText={JiebaText}\"); Console.WriteLine($\"PredictedLabel:{PredictedLabel},Score:{Score},Probability:{Probability}\"); Console.WriteLine($\"TextFeatures Length:{Features.Length}\"); if (Features != null) { foreach (var f in Features) { Console.Write($\"{f},\"); } Console.WriteLine(); } Console.WriteLine(); } }\n通过对调试结果的分析，可以看到整个数据处理管道的工作流程。\n五、资源获取\n源码下载地址：https://github.com/seabluescn/Study_ML.NET\n工程名称：BinaryClassification_TextFeaturize\n点击查看机器学习框架ML.NET学习笔记系列文章目录"}
{"content2":"百度实习面试总结——百度大搜，机器学习实习生\n简历是内推的，找到了川大的一位学姐，然后内推了简历。\n过了几天，HR的电话打过来协商面试时间。一开始说的是下午两点，我想了想，好像有一节选修课，于是想调整再晚一点，于是调到了四点，不过这时间不还是在上课吗……\n于是果断上课上到一半，然后翘了出来，溜回寝室。在室友玩守望先锋的背景音乐下，进行了一面（微笑脸\n一面\n其实这时候，我并不知道我面的是什么岗位的……\n一面和小米一样，都是在一个代码分享平台上面写，就是面试官看得见你写的代码，你也看得见面试官写的是啥。\n先让我自我介绍，然后我blablabla。\n然后进入正题：\n第一题：给你n个数，要求找到所有的三元集<i,j,k>，满足a[i]+a[j]+a[k]=S，要求复杂度n^2\n我做这道题的时候，一开始给了一堆乱七八糟的方法。（FFT什么的其实都想说出来，但是忍住了……\n然后面试官提示了一下两个数的情况，发现好像可以直接O(n)的Two pointers就好了。\n然后我脑补了一下，于是给了个和two pointers毫无关系的n^2方法（暴力枚举+hash)……\n然后在面试官的提示下，做出了n^2的two pointers。\n这时候感觉好像要go die了……\n第二题：给25匹马，你有五个赛道，问你最少需要比赛多少场，才能得到前三的马。\n我一开始没啥想法，于是答了个上限，(25-3)/2=11场，因为每场比赛可以淘汰两只马。\n然后面试官提示了一下杨氏矩阵，然后看我好像还是不太懂的样子，就给我讲了正解：\n你建一个5*5矩阵，每一行，从左到右，从大到小排序。第一列，从大到小排序。\n那么说构成我要求的这个矩阵，就需要6次比赛。\n然后就可以淘汰掉第四列和第五列，第四行和第五行。\n第一排只会剩下3匹马，第二排只会剩下2匹马，第三排只会剩下1匹马。\n由于第一行第一列的马儿必须选，所以剩下五匹马比赛选出前二就完了。\n所以最少比赛七次。\n这时候，感觉自己已经go die了。\n第三题：给你一堆二维点，让你找到一个点，使得其他点到这个点的距离和最小。\n这道题我终于会了，于是在面试官说完之前 ，就完成了抢答。\n这道题是某年的校赛初赛题，把X轴和Y轴分开之后，分别取中点就完了。\n第四题：\n问：如何检测两篇新闻是一样的呀？\n答：把新闻拿出来字符串匹配就好了嘛。\n问：但是其中可能存在个别字符不一样，其他的都一样的情况怎么办呀？\n答：那就把新闻拆分成句子，抠下来，然后进行匹配吧。\n问：有些句子，实际上是代表着这个网站的特征的句子，如何把这些句子分辩出来呢？\n答：翻一下之前这个网站的新闻，机器学习一下，看看那些句子出现的频次高。\n问：哦，你会机器学习吗？\n答：了解过，我知道K近邻。\n问：解释一下。\n答：KNN就是把一些作为样本，然后拿现在要检测的和那些样本做距离，取距离最小的那几个，然后里面啥最多，那么检测的这个就是啥。我之前做过那个数字识别，就是用这个搞的。\n问：你说的好像不太清楚，我忘了，我回去查一查。\nGG。\n然后一面就说，你准备一下，赶紧面对二面吧。\n这时，我室友正在欢乐的在守望先锋1600分鱼塘挣扎中。\n二面\n开头还是让我自我介绍，然后我把一面的自我介绍重复了一遍(x\n然后进入正题：\n第一题：\n问：给你一个前缀，再给你一堆字符串，问你有多少个字符串包括这个前缀。\n答：hash。\n问：假设要求动态的多次询问是否存在这个前缀呢？\n答：把所有的字符串拿去建字典树，然后每次有前缀的时候，爬一爬就好了。\n问：如果不是前缀呢，是问你是否存在这个子串呢？\n答：hash？\n问：这个好像不太行呀，你可以有充分的预处理时间哦？\n答：那就把所有的字符串抠出来，比如把abc变成abc,bc,c三个字符串，然后做字典树？\n问：好像可以？\n答：恩，好像可以。\n第二题：给你一个矩阵，从左导右是递增的，从上到下是递增的，让你找到里面是否存在一个k。\n曾经曾某拿这道题考过我，然后我被他羞辱过，然后我就记住了这道题……\n我诚实的答了我曾经做过这道题，然后证明了一下结论的正确性。\n正解就是从右上角开始跑，如果k大于当前格子的数，就往下走，否则往左走，证明略。\n第三题：\n问：给你一个二叉树，求从根开始的最长路径。\n答：好像dfs一下就好了？顺便记录一下路径长度。\n问：要求输出路径。\n答：第二次再dfs一下，如果到叶子节点的时候，是最长的话，就输出？\n问：只用一次dfs。\n答：好像不遍历完整棵树，是无法确定最长路的？我感觉得两次呀。\n问：你能不能开个数组记录一下路径，然后更新呢？\n答：好像可以呀。\nGG\n第四题：\n问：给你一个随机数生成器，有p的概率输出0，(1-p)的概率输出1，p未知。要求用这个随机生成器做成50%输出0，50%输出1的生成器。\n答：我在纸上画了画，好像让这个随机数生成器一开始正常输出，然后第二个周期1-rand()输出，这样就是一个平均的了。\n问：你这个假设周期是1，那么输出了两个数，如果我只取第一个数，那么第一个数是否是50%概率输出的呢？\n答：好像从两个数的角度来看，这两个数输出1的期望和输出0的期望个数相同，我觉得好像是一样的。如果只看第一个数的话，他就不是。我感觉这好像是个逻辑问题，我觉得我逻辑好像不太好……\n问：我想让你实现一个函数，然后这个函数的功能是50%概率输出0和1，你再想一想。\n抠了一下脑袋，然后想了10min……\n答:输出四个数做,01和10的概率不一样,01+10不等于11+00。00和11都是p(1-p)，01是pp 10是（1-p）（1-p），然后00x11=01x10，根据这一，我们可以乘法找对称关系。一共十六对，都能找到。\n问：我懂了你的意思，你这样太麻烦了，假设你可以跳过一些东西呢？没必要全部用。\n答：那就11输出1,00输出0？01和10都跳过？\n问：对。\n然后面试官让我准备三面了。\n三面\n让我自我介绍一下，于是我又重复了一遍（ x\n你有什么优点？\nblablabla\n你有什么缺点？\nblablabla\n你知道我们干什么的吗？\n不知道。\n你知道你要干什么吗？\n不知道。\n那我给你讲一下，我们是干blablabla……\n而你会做blablablabla……\n我觉得你这种一张白纸的竞赛选手，还是比较适合这里的。\n然后三面就结束了。\n后续\n然后我就以实习生的身份，混进百度之星出题组了。（x\n我一定会好好出题的！大家记得去参加百度之星呀！\n至于offer这边，就杳无音讯了，我以为我被拒绝了，但是！\n过了十几天，三面面试官打电话给我说了下百度现在HC不足的问题，你要来就只能四月之后来。\n而我正好有校赛要去打，本来就打算最早也只能四月份之后过去。\n于是达成共识。\n于是愉快的就决定了四月份再去思考这件事儿去了。\n于是我又开始在学校躺尸，过着给小朋友教书的快乐日子。"}
{"content2":"C4.5算法\nC4.5算法的核心思想是ID3算法，是ID3算法的改进：\n用信息增益率来选择属性，克服了用信息增益来选择属性时变相选择取值多的属性的不足；\n在树的构造过程中进行剪枝；\n能处理非离散化数据；\n能处理不完整数据。\n优点：\n产生的分类规则易于理解，准确率高。\n缺点：\n在构造过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；\nC4.5算法只适合于能够驻留内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。\nK-means算法\n简单的聚类，吧n个对象根据他们的属性分为k个类，k<n。\n算法的核心是要优化失真函数J，使其收敛到局部最小值而不是全局最小值：\n\\[J=\\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} || x_n - u_k ||^2,\\]\n\\(r_{nk}\\)表示n数据第k个类，\\(u_k\\)是第k个类中心值。\n然后求出最优的\\(u_k\\)：\n\\[u_k=\\frac{\\sum r_{nk} x_n}{\\sum_{n} r_{nk} }\\]\n优点：\n算法速度快。\n缺点：\n分组的数目k是一个输入参数，不适合的k可能返回较差的结果。\n朴素贝叶斯算法\n朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。\n算法的基础是概率问题,分类原理是通过某对象的先验概率,利用贝叶斯公式计算出其后验概率,即该对象属于某一类的概率,选择具有最大后验概率的类作为该对象所属的类。\n朴素贝叶斯假设是约束性很强的假设,假设特征条件独立,但朴素贝叶斯算法简单,快速, 具有较小的出错率。\n在朴素贝叶斯的应用中,主要研究了电子邮件过滤以及文本分类研究。\nK最近邻算法\n缺点：\nK值需要预先设定，而不能自适应\n当样本不平衡时，如一个类的样本容量很大，二其他类样本容量很小，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。\n该算法适用于对样本容量比较大的类域进行自动分类。\nEM最大期望算法\nEM算法是基于模型的聚类算法，是在概率模型中寻找参数最大思然估计的算法，其中概率模型依赖于无法观测的隐藏变量。\nE步估计隐含变量，M步估计其他参数，交替将极值推向最大。\nEM算法比K-means算法计算复杂，收敛较慢，不适合大规模数据集和高维数据，但比K-means算法计算结构稳定、准确。\nEM算法经常用在机器学习和计算机视觉的数据集聚（data clustering）领域。\nPageRank算法\nGoogle的页面排序算法。\n基于从许多优质的网页链接过来的 网页,必定还是优质网页的回归关系,来判定所有网页的重要性。\n一个人有越多牛逼的朋友，他牛逼的概率就越大。\n优点：\n完全独立于查询，只依赖于网页链接结构，可以离线计算。\n缺点：\nPageRank算法忽略了网页搜索的时效性；\n旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的网页排名却很低，因为它们几乎没有in-links。\nAdaBoost\nAdaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。\n算法本事该百诺数据分布来实现的，它根据每次训练集中每一个样本的分类是否正确，以及上一次的总体分类准确率，来确定没个样本的权值。\n将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。\n算法流程：\n先通过对N个训练样本的学习得到第一个弱分类器；\n将分错的样本和其他的新数据一起构成一个新的N个训练样本，通过学习得到第二个弱分类器；\n讲前面都分错的样本加上新的样本构成另一个新的N个训练样本集，通过学习得到第三个弱分类器；\n如此反复，最终得到经过提升的强分类器。\n目前 AdaBoost 算法广泛的应用于人脸检测、目标识别等领域。\nApriori算法\nApriori算法是一种挖掘关联规则的算法，用于挖掘其内涵的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法。\nApriori算法的两个阶段：\n寻找频繁项集；\n有频繁项集找关联规则。\n算法缺点：\n在每一步产生侯选项目集时循环产生的组合过多,没有排除 不应该参与组合的元素;\n每次计算项集的支持度时,都对数据库中的全部记录进行了一遍扫描比较,需要很大的I/O 负载。\nSVM支持向量机\n支持向量机是一种基于分类边界的方法。\n基本原理：\n如果训练数据分布在二维平面上的点,它们按照其分类 聚集在不同的区域。\n基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界。\n对于多维数据（N维），可以将他们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面。\n线性分类器使用超平面类型的边界，非线性分类器使用超曲面。\n支持向量机的原理是将低维空间的点映射到高维空间,使它们成为线性可分,再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分,而在原有的数据空间中,是一种非线性划分。\nCART树\n决策树的分类方法，基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。\n如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。\n优点：\n非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。\n面对存在缺失值、变量数多等问题时，CART数显得非常稳健。"}
{"content2":"在我的理解里，要实现计算机视觉必须有图像处理的帮助，而图像处理倚仗与模式识别的有效运用，而模式识别是人工智能领域的一个重要分支，人工智能与机器学习密不可分。纵观一切关系，发现计算机视觉的应用服务于机器学习。各个环节缺一不可，相辅相成。\n计算机视觉（computer vision），用计算机来模拟人的视觉机理获取和处理信息的能力。就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，用电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。\n图像处理（image processing），用计算机对图像进行分析，以达到所需结果的技术。又称影像处理。基本内容图像处理一般指数字图像处理。数字图像是指用数字摄像机、扫描仪等设备经过采样和数字化得到的一个大的二维数组，该数组的元素称为像素，其值为一整数，称为灰度值。图像处理技术的主要内容包括图像压缩，增强和复原，匹配、描述和识别3个部分。常见的处理有图像数字化、图像编码、图像增强、图像复原、图像分割和图像分析等。图像处理一般指数字图像处理。\n模式识别(Pattern Recognition)是指对表征事物或现象的各种形式的(数值的、文字的和逻辑关系的)信息进行处理和分析,以对事物或现象进行描述、辨认、分类和解释的过程,是信息科学和人工智能的重要组成部分。模式识别又常称作模式分类，从处理问题的性质和解决问题的方法等角度，模式识别分为有监督的分类（Supervised Classification）和无监督的分类(Unsupervised Classification)两种。模式还可分成抽象的和具体的两种形式。前者如意识、思想、议论等,属于概念识别研究的范畴,是人工智能的另一研究分支。我们所指的模式识别主要是对语音波形、地震波、心电图、脑电图、图片、照片、文字、符号、生物传感器等对象的具体模式进行辨识和分类。模式识别研究主要集中在两方面,一是研究生物体(包括人)是如何感知对象的，属于认识科学的范畴,二是在给定的任务下,如何用计算机实现模式识别的理论和方法。应用计算机对一组事件或过程进行辨识和分类，所识别的事件或过程可以是文字、声音、图像等具体对象，也可以是状态、程度等抽象对象。这些对象与数字形式的信息相区别，称为模式信息。模式识别与统计学、心理学、语言学、计算机科学、生物学、控制论等都有关系。它与人工智能、图像处理的研究有交叉关系。\n机器学习(Machine Learning)是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习在人工智能的研究中具有十分重要的地位。一个不具有学习能力的智能系统难以称得上是一个真正的智能系统，但是以往的智能系统都普遍缺少学习的能力。随着人工智能的深入发展，这些局限性表现得愈加突出。正是在这种情形下，机器学习逐渐成为人工智能研究的核心之一。它的应用已遍及人工智能的各个分支，如专家系统、自动推理、自然语言理解、模式识别、计算机视觉、智能机器人等领域。机器学习的研究是根据生理学、认知科学等对人类学习机理的了解，建立人类学习过程的计算模型或认识模型，发展各种学习理论和学习方法，研究通用的学习算法并进行理论上的分析，建立面向任务的具有特定应用的学习系统。这些研究目标相互影响相互促进。\n人类研究计算机的目的，是为了提高社会生产力水平，提高生活质量，把人从单调复杂甚至危险的工作中解救出来。今天的计算机在计算速度上已经远远超过了人，然而在很多方面，特别是在人类智能活动有关的方面例如在视觉功能、听觉功能、嗅觉功能、自然语言理解能力功能等等方面，还不如人。\n这种现状无法满足一些高级应用的要求。例如，我们希望计算机能够及早地发现路上的可疑情况并提醒汽车驾驶员以避免发生事故，我们更希望计算机能帮助我们进行自动驾驶，目前的技术还不足以满足诸如此类高级应用的要求，还需要更多的人工智能研究成果和系统实现的经验。\n什么是人工智能呢？人工智能，是由人类设计并在计算机环境下实现的模拟或再现某些人智能行为的技术。一般认为，人类智能活动可以分为两类：感知行为与思维活动。模拟感知行为的人工智能研究的一些例子包括语音识别、话者识别等与人类的听觉功能有关的“计算机听觉”，物体三维表现的形状知识、距离、速度感知等与人类视觉有关的“计算机视觉”，等等。模拟思维活动的人工智能研究的例子包括符号推理、模糊推理、定理证明等与人类思维有关的“计算机思维”，等等。\n从图像处理和模式识别发展起来的计算机视觉研究对象之一是如何利用二维投影图像恢复三维景物世界。计算机视觉使用的理论方法主要是基于几何、概率和运动学计算与三维重构的视觉计算理论，它的基础包括射影几何学、刚体运动力学、概率论与随机过程、图像处理、人工智能等理论。计算机视觉要达到的基本目的有以下几个：\n(1) 根据一幅或多幅二维投影图像计算出观察点到目标物体的距离；\n(2) 根据一幅或多幅二维投影图像计算出目标物体的运动参数；\n(3) 根据一幅或多幅二维投影图像计算出目标物体的表面物理特性；\n(4) 根据多幅二维投影图像恢复出更大空间区域的投影图像。\n计算机视觉要达到的最终目的是实现利用计算机对于三维景物世界的理解，即实现人的视觉系统的某些功能。\n在计算机视觉领域里，医学图像分析、光学文字识别对模式识别的要求需要提到一定高度。又如模式识别中的预处理和特征抽取环节应用图像处理的技术；图像处理中的图像分析也应用模式识别的技术。在计算机视觉的大多数实际应用当中，计算机被预设为解决特定的任务，然而基于机器学习的方法正日渐普及，一旦机器学习的研究进一步发展，未来“泛用型”的电脑视觉应用或许可以成真。\n人工智能所研究的一个主要问题是：如何让系统具备“计划”和“决策能力”？从而使之完成特定的技术动作（例如：移动一个机器人通过某种特定环境）。这一问题便与计算机视觉问题息息相关。在这里，计算机视觉系统作为一个感知器，为决策提供信息。另外一些研究方向包括模式识别和机器学习（这也隶属于人工智能领域，但与计算机视觉有着重要联系），也由此，计算机视觉时常被看作人工智能与计算机科学的一个分支。\n机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演译。\n为了达到计算机视觉的目的，有两种技术途径可以考虑。第一种是仿生学方法，即从分析人类视觉的过程入手，利用大自然提供给我们的最好参考系——人类视觉系统，建立起视觉过程的计算模型，然后用计算机系统实现之。第二种是工程方法，即脱离人类视觉系统框框的约束，利用一切可行和实用的技术手段实现视觉功能。此方法的一般做法是，将人类视觉系统作为一个黑盒子对待，实现时只关心对于某种输入，视觉系统将给出何种输出。这两种方法理论上都是可以使用的，但面临的困难是，人类视觉系统对应某种输入的输出到底是什么，这是无法直接测得的。而且由于人的智能活动是一个多功能系统综合作用的结果，即使是得到了一个输入输出对，也很难肯定它是仅由当前的输入视觉刺激所产生的响应，而不是一个与历史状态综合作用的结果。\n不难理解，计算机视觉的研究具有双重意义。其一，是为了满足人工智能应用的需要，即用计算机实现人工的视觉系统的需要。这些成果可以安装在计算机和各种机器上，使计算机和机器人能够具有“看”的能力。其二，视觉计算模型的研究结果反过来对于我们进一步认识和研究人类视觉系统本身的机理，甚至人脑的机理，也同样具有相当大的参考意义。\n转载时请以超链接形式标明文章原始出处和作者信息及本声明http://www.blogbus.com/shijuanfeng-logs/216968430.html"}
{"content2":"本博客所有文章分类的总目录：http://www.cnblogs.com/asxinyu/p/4288836.html\n微软Infer.NET机器学习组件文章目录：http://www.cnblogs.com/asxinyu/p/4329742.html\n关于本文档的说明\n本文档基于Infer.NET 2.6对Infer.NET User Guide进行中文翻译，但进行了若干简化和提炼，按照原网站的思路进行，但不局限与其顺序。\n欢迎传播分享，必须保持原作者的信息，但禁止将该文档直接用于商业盈利。\n本人正在研究基于Infer.NET组件，并计划将其应用于实际的预测之中，该组件功能强大，封装很完善，但也有很多难以理解的地方，同时官方也给出了大量的例子，限于个人精力有限，更新时间较慢，也希望有兴趣的朋友一起来完成该项工作。\nEmail：asxinyu@qq.com\n本文章原始地址：http://www.cnblogs.com/asxinyu/p/4255921.html\n1.Infer.NET贝叶斯分类器介绍\n本教程提供了一个简短的,循序渐进的介绍如何使用C# API创建、训练和测试一个贝叶斯机分类器的过程。如果你只是想尝试使用贝叶斯机器分类器,你可以提供数据一个特定格式文本文件，不用担心最终的性能,可以参考一下命令行的使用。\n本项目使用的贝叶斯分类器是基于Infer.NET API构建的，源码等资料在本文最后的资源信息。\n该文章翻译的网址为：点击这里看英文原文\n2.性别预测介绍\n本教程的目的,让我们假设我们要根据一组给人身高和体重的数据来预测一个人的性别。让我们进一步假设我们已经收集到的样本大小为N=1000,而我们知道的性别作为标签样本单独存放。下面图是样本数据可视化的结果：\n解决这类问题的一个方法是创建一个概率模型。Infer.NET允许您使用各种各样的模型。在描述了这个分类问题后,我们已经给你准备了一个完整和稳定的模型,所以你不需要做这个工作。然而,我们创建的贝叶斯机器(BPM)分类器,必须要有一些前提条件，这些你可能要搞清楚。如果这些假设不满足你的情况,你自己可以考虑创建一个更符合你实际情况的概率模型。样本数据中包含508名女性(红色)和508名男性(蓝色)。样本中的女性平均身高是162厘米,男性平均身高为175.2厘米。女性平均体重为70.1公斤,男性平均体重为83.8公斤。正如人们所预料的,该图的中，两组数据的显示有很大的重叠,这意味着给定一个人的身高和体重，并不能完全的区分为男性或者女性。现在,根据这些数据，那么一个身高183厘米,体重78公斤的人是一个女人的可能性大吗?\n使用Visual Studio创建项目，并添加下面几个dll引用：Infer.Learners.dll，Infer.Learners.Classifier.dll，Infer.Runtime.dll。这个过程比较简单，基本忽略。\n3.数据和映射\n针对上述提供的数据。每个人的身高和体重是以向量实例存储在内存中的，相应的性别以字符串类型对象表示的，其值是“女性”或“男性”。因此整个样本，包含个数是1000的向量数组对象和1000个字符串类型对象。\n相关说明：\n1.贝叶斯机器分类器模型并不显式地指定一个预期结果。然而,在许多分类问题中，添加一个预期结果是至关重要的,因为它使得贝叶斯分类器以固定的特性转换特征(决策边界不需要原始数据特征),从而显著的影响预测性能[翻译不太准确]。BPM可以实现添加预期值的功能,使其总有1这个值。如果你的分类数据还不包括这样一个常数特征值,你应该很想将它添加到特征向量中。\n2.如果特性(包括预期值)是高度相关的，贝叶斯机器分类器的训练可能出现收敛速度慢的情况。\n在上述性别预测的例子中,我们可以使用了一个增强的三维特征向量,不仅包含一个人的身高和体重,还有额外的特征值为1。此外,让我们在原始数据中减去平均身高和平均体重，这就消除身高、体重和预期值之间的相关性。\n数据可以存储在各种不同的格式中。通常的您的数据格式都和学习者Learner所预期要求的数据类型都不一致。这意味着您必须将原始数据转化为学习者的格式。这在一些小的用例测试中，当然是一个合理的选择,但对于大型的真实数据集,这种转换代价是非常大的。因此为了避免用户输入固定类型的数据，Infer.NET提供了一个灵活的机制,允许您指定学习者应该如何使用他们的输入数据。这种机制被称为映射。一个映射定义输入数据是如何传递到Infer.NET的学习者中的。因为它让你选择最方便的格式,它有助于避免不必要的数据转换。\n贝叶斯机器分类器中有两种不同类型的映射,详细可以参考Mappings类。在我们简单介绍的例子中,最简单的将输入数据映射到一个表单以便于贝叶斯分类器理解的方法是实现IClassifierMapping接口，如下所示：\n1 /// <summary> 2 /// A mapping for the Bayes Point Machine classifier tutorial. 3 /// </summary> 4 public class ClassifierMapping: IClassifierMapping<IList<Vector>, int, IList<string>, string, Vector> 5 { 6 public IEnumerable<int> GetInstances(IList<Vector> featureVectors) 7 { 8 for (int instance = 0; instance < featureVectors.Count; instance++) 9 { 10 yield return instance; 11 } 12 } 13 public Vector GetFeatures(int instance, IList<Vector> featureVectors) 14 { 15 return featureVectors[instance]; 16 } 17 18 public string GetLabel(int instance, IList<Vector> featureVectors, IList<string> labels) 19 { 20 return labels[instance]; 21 } 22 23 public IEnumerable<string> GetClassLabels(IList<Vector> featureVectors = null, IList<string> labels = null) 24 { 25 return new[] { \"Female\", \"Male\" }; 26 } 27 }\n为了实现IClassifierMapping 接口，必须要实现以下几点：\n1.哪个是要分批交给分类器的对象？(GetInstances);\n2.如何获取给定实例的特征值？ (GetFeatures);\n3.如何获取给定实例实际的标签值? (GetLabel);\n4.获取数据中所有不同类型标签值，相当于标签范围(GetClassLabels)。\n4.创建贝叶斯分类器，并训练，预测和评估\n4.1 创建分类器\n有了手动创建的数据映射，就可以创建贝叶斯机器分类器，如下所示：\n1 有了手动创建的映射，就可以创建贝叶斯机器分类器，如下所示： 2 // Create the Bayes Point Machine classifier from the mapping 3 var mapping = new ClassifierMapping(); 4 var classifier = BayesPointMachineClassifier.CreateBinaryClassifier(mapping);\n4.2 训练\n这样，就可以使用分类器根据身高和体重去学习和预测性别。使用1000个样本去训练贝叶斯机器分类器,如下所示：\n1 // Train the Bayes Point Machine classifier on the gender data 2 classifier.Train(trainingSet.FeatureVectors, trainingSet.Labels);\ntrainingSet.FeatureVectors是一个包括身高和体重测量数据的向量数组，trainingSet.Labels是一个代表性别的预期标签。\n注意，训练贝叶斯我们并不需要设置任何参数，如前置分布权重。这是因为贝叶斯机器分类器是无需超参数的(hyper-parameter)。这不仅避免了一些错误的参数设置，还可以自动移除一些影响运行时间的参数。更厉害的是：它甚至不需要规范化的数据输入，贝叶斯机器分类器能够自己自动适应不同尺度的观察数据。这些都是通过heavy-tailed 前置分布权重设置的。\n4.3 预测\n使用训练后的贝叶斯机器分类器，就能够预测那些只有身高和体重数据的人的性别。特别是，现在我们可以回答之前那个身高183厘米,体重178公斤的人是一个女人的可能性有多大。如下代码：\n1 // Making predictions on previously unseen data 2 var predictions = classifier.PredictDistribution(testSet.FeatureVectors);\ntestSet.FeatureVectors是一个只包含身高，体重以及预期值的向量数组。\n调用PredictDistribution在测试集中，给每个实例返回一个伯努利分布，这个事实说明给定一个人的身高和体重，我们通常并不能完全确定一个人的性别。例如：\n1 P(gender = 'Female' | height = 183cm, weight = 78kg) = 0.07\n根据训练集1000个样本的观测数据，这个身高183厘米,体重78公斤的人是女性的概率是7%。在许多情况下,您可能需要预测一个最终确定的答案，而不是调用PredictDistribution,然后简单地预测，给出概率，例如，我们可以这样写：\n1 // Making decisions 2 string estimate = classifier.Predict(InstanceOfInterest, testSet.FeatureVectors);\n结果是：Male\n注意,这种精确的分类预测仍然需要计算预测分布作为一个中间步骤。此外,最佳的精确答案不一定是最有可能的类。\n4.4 评估测试\n为了评价分类器的预测情况,我们需要利用一些不同于训练集的有标签的数据,因此我们假定我们可以得到一组100个额外的真实性别，体重和身高的测量记录。一个评价过程要通过ClassifierEvaluator进行,如下:\n1 // Create an evaluator for mapping 2 var evaluatorMapping = mapping.ForEvaluation(); 3 var evaluator = 4 new ClassifierEvaluator<IList<Vector>, int, IList<string>, string>( 5 evaluatorMapping);\n结果：Accuracy = 0.85；AUC = 0.926\nClassifierEvaluator  also allows you to get the receiver operating characteristic curve itself (for \"Female\" as the designated positive class):\n1 IEnumerable<Pair<double, double>> rocCurve = 2 evaluator.ReceiverOperatingCharacteristicCurve( 3 \"Female\", testSet.FeatureVectors, predictions);\n我们将在后续的文章中分享关于基于Infer.NET组件构建的贝叶斯机器分类器更多的功能和相关细节。\n5.资源\n本人手动制作了Infer.NET 2.6的帮助文档，CHM格式，还有贝叶斯分类器的相关代码，\n文件比较大，将通过邮箱与30日下午统一发送，需要的朋友留Email。\n如果本文章资源下载不了，或者文章显示有问题，请参考 本文原文地址：http://www.cnblogs.com/asxinyu/p/4255921.html\n如果您看完本篇文章感觉不错，请点击一下右下角的【推荐】来支持一下博主，谢谢！"}
{"content2":"转自：https://segmentfault.com/a/1190000005356857\n1. 前言\n本来这篇标题我想的是算法工程师的技能，但是我觉得要是加上机器学习在标题上，估计点的人会多一点，所以标题成这样了，呵呵，而且被搜索引擎收录的时候多了一个时下的热门词，估计曝光也会更多点。不过放心，文章没有偏题，我们来说正经的。\n今天就说说机器学习这个最近两年计算机领域最火的话题，这不是一篇机器学习的技术文章，只是告诉大家机器学习里面的坑实在是太多，而且很多还没入门或者刚刚入门的朋友们，其实在你们前面是个大坑，如果你励志要在这条路上走下去的话，请做好心理准备。\n2. 我们学习机器学习的目的\n实话实说，目前大部分人上各种班来学习机器学习，学习大数据，归根到底还是希望能找到一个好的工作，拿到更高的薪水，当然还有一部分原因是自己对这一方面比较感兴趣，希望更深入的了解这个领域。\n我个人觉得，第一个原因的因素更大。\n3. 我们在谈机器学习的时候在谈什么\n首先，我们看看一个机器学习的系统长成什么样子\n￼\n几乎所有的机器学习系统都是由上述系统图组成，不同的是监督型的系统训练数据可能需要人工干预而非监督型的系统不需要人工干预，简单来说就是给一批训练数据给这个机器学习模型进行学习，得到一个预测模型，然后用这个预测模型对新的未知数据进行预测。\n现在网络上机器学习方面的文章，博客到处都是，市面上各种各样的书籍也到处都是，而且目前在线教育最火的领域也是这个，各种各样的机器学习的在线教育的班，学费还挺贵。\n但是你发现没有，所有这些谈论的机器学习都是在谈论模型，什么《深入理解XXX模型》，《可能是最好的理解XXX的文章》，《机器学习并不难，XXXX模型详解》之类的文章和书遍地开花。各种介绍逻辑回归，深度学习，神经网络，SVM支持向量机，BP神经网络，卷积神经网络.....等等等等。\n所以，我们在谈论机器学习的时候，实际上是在谈论机器学习的模型，也就是各种机器学习算法。而且大家都认为只要学会了模型和算法的理论，那就是机器学习的专家了。我相信大多数人都是这么认为的。\n4. 小明成了机器学习\"专家\"\n有个小朋友，是搞计算机的，叫小明，看了alphago虐李世石的视频，虽然他完全不懂围棋，但是他还是被震撼到了，决心要好好学习一下这个传说中的机器学习。于是到处在网上找教程，找博客文章，找书籍，好好的学了半年，终于觉得自己入门了。每个机器学习的模型算法都能说出个所以然来了。\n不知道大家有多少在这个阶段？\n但小明还想更进一步，于是开始研究各种模型的代码和工具了，hadoop和spark那是标配了，又是各种找文章，各种找书，各种在线学习班，还好这些东西一大把一大把的，特别现在的在线学习班，要是没有大数据处理班，没有hadoop班，那就别开了。\n一路下来，大半年又过去了，终于小明觉得自己学会了，理论也有了，大数据处理工具也会了，简直无敌了！\n又有多少人在这个阶段？并以为自己已经会机器学习了。到这个阶段，如果你学得好，那么你已经可以去开个学习班教别人机器学习了。但如果你以为这样就可以去找个公司做算法工程师了，那么告诉你，图样图森破，乃义五！\n小明因为有较强的理论知识，能推导所有公式，又会hadoop，spark，再加上自己的表达能力强，很容易的秒了几个面试官进了一个大公司，是在一个电商做搜索的算法工程师，月薪很高，终于可以一展拳脚了，老板交给他一个任务，用你那牛逼的知识把搜索的点击率给我提升一个百分点吧。\n如果你是小明，如果你刚从某个机器学习的学习班下来，你怎么弄？你是不是傻了？\n5. 机器学习不仅仅是模型\n产生这个问题的原因就是所有人都以为机器学习的模型就是机器学习本身，以为对那些个算法理解了就是机器学习的大牛了，但实际上完全不是这样的。\n模型是谁在玩呢？模型是科学家发明出来的， 是各个大公司的各个科学家，研究员发明出来的，这个发明出来是会出论文的，是他们用来虐我们的智商的，一般情况下，你发明不了模型吧（如果可以，可以不要往下看了，你可以走学术那条路）？你修改不了模型吧？\n所以说，学会了模型，只是刚刚刚刚入门，甚至还算不上入门吧\n那各个公司的那么多算法工程师在干嘛呢？我们以一个搜索排序的算法工程师为例，他们在做甚呢？他们在\n观察数据--->找特征--->设计算法--->算法验证--->洗数据--->工程化--->上线看效果--->goto 观察数据\n而且一个成熟的系统中，一般模型已经大概确定了，如果效果不是特别不好不会换模型，比如一个公司的搜索排序系统用了机器学习的逻辑回归模型，你要改成别的模型一般不太可能，那么只能做一些特征上的补充。\n好，我们通过这个流程来看看一个机器学习的算法工程师到底还要什么能力。\n5.1 观察数据\n小明每天就在工位上看数据，查数据，看表格，画曲线，发现像销量，收藏，点击等等这种能想到的特征早就被用了，就这么耗了三个月，没有任何进展，人都崩溃了，来了这么久，机器学习代码毛都没看到呢。\n第四个月，他发现一点问题，他发现有些商品，评论什么的都挺好，感觉产品质量也不错，但就是销量上不去，所以老排后面，于是，他把这些评论都是五星，但是销量比较差的商品滤出来了，想看看他们有什么共性。\n观察数据阶段，你说要什么能力？呵呵，只能告诉你，需要数据敏感性，其实也就是告诉你需要全面的能力，需要经验，需要产品经理的能力。\n除了这些，你还需要能随手编脚本代码的能力，遇到有些数据需要初步处理，可能需要随手编代码处理，而且编的要快，因为这些代码可能就用一两次就不用了，所以需要比较强大的脚本语言能力，那么python至少要熟悉吧，shell要会吧。\n5.2 找特征\n数据观察下来发现了问题，现在要找特征了，要找特征，也就是找什么因素导致销量上不去的，首先，需要想象力，然后去验证你的想象力。\n小明的想象力爆棚，即便这样，也搞了一个月才发现这些个商品有个共同特征，那就是图片都比较烂，让人一看就不想点。卧槽，要是能把图片质量加入到排序因素里面的话，是不是有奇效呢？图片质量作为特征，这之前可没人做过，终于找到一个特征了。\n所以在这一阶段，毕竟大家的想象力都是有限的，更多的是经验值，才能找到符合当前场景的特征。\n5.3 设计算法\n特征是找到了，但怎么把这个特征加到排序模型里面去呢？图片好不好，有多好，这些机器怎么理解呢？如果不能把图片质量变成一个数学上的向量，那永远都无法加入到排序模型里面去。\n这一阶段是真正考验算法工程师的地方了，那就是将特征向量化，小明观察到越好看的图像往往颜色变化更多，而质量差的图片往往颜色没什么变化，于是他想到一种办法，先把图像数据进行傅里叶变换，变成频域的数据，根据傅里叶变换的性质，高频部分的幅度高表示图像的颜色变化很明显，如果低频部分高，表示颜色变化不明显，这和观察到的图像信息基本能匹配上，这样一副图像的好坏，就可以用傅里叶变换后高频部分的幅度表示了，然后在做一些归一化的变化，就把图像向量化了，向量化以后就可以加入到排序模型去了。\n这一步，你可能会用到你学习的机器学习模型，但肯定只占了一小部分，大部分情况需要你根据当前场景自己建立一个数学模型，而不是机器学习模型，你说这一阶段需要什么技能？虽然我这里举的例子比较极端，但是数学抽象能力，数学建模能力和数学工具的熟练使用是必不可少的，并且同样需要较强的编程能力，这已不是上一步的脚本能力，是实打实的计算机算法编程能力了。\n5.4 算法验证\n算法是设计好了，还要设计一个算法的离线验证方法来证明给你的老大看说我的算法是有效果的，不然哪那么多机会让你到线上去试啊，这一步也是各种综合能力的组合，关键是在这一步上，你要用一种通俗的语言从理论上说服你的老大，这是一种什么能力？强大的语言表达能力。\n除了这个你还需要设计出一个上线以后的AB测试方案，能够很好的测试出你的算法是否真的有效。\n5.5 洗数据\n特征找到了，算法也设计得差不多能体现特征了，体力活来了，那就是洗数据，这是算法工程师的必修课，数据不是你想要什么样子他就长得什么样子的，所以要把数据变成你想要的样子，然后去掉无效的数据可是个体力活。\n像上面这个例子，首先可能大家的图片大小都不一样，要变成一个尺寸才好进行变换，有些商品有多个图片，可能需要找出质量最好的再处理等等等等。\n这一阶段首先也是要脚本语言处理能力，而且还需要掌握一些数据处理工具的使用，关键还要有足够的耐性和信心，当然，必不可少的是优秀的编程能力。\n5.6 工程化\n好了，前面的坑你全跨过来了，到了这一步了，呵呵，算法设计完了，数据也准备好了，估计半年过去了，那赶快放到线上去吧，你以为拿着一堆脚本就能上线了啊，得考虑工程化了，如果把你的算法嵌入到原有系统中，如果保证你的算法的效率，别一跑跑一天，代码的健壮性也要考虑啊，如果是在线算法，还得考虑性能，别把内存干没了。\n这一步，你才真正的用上了你上面学的机器学习的hadoop，spark工具，看了上面说的，要完成工程化这一步，得有什么能力不用我说了吧，这是一个标准的软件开发工程师的必要技能，还是高级开发工程师哦。\n5.7 上线看效果\n所有的都做完了，前前后后10个月了，终于可以上线了，好了，真正的考验来了，看看上线的效果呗，产品经理说，做个AB测试吧，结果呵呵了，点击率降低了，小明啊！这10个月忙活下来点击率还下降了？？？老板还不把你骂死，所以，你必须有强大的抗打击能力。\n呵呵，赶快下线吧，从头看看哪里出了问题，又花了一个月修改了算法，重新上线，恩，这次不错，点击率提高了0.2个百分点，继续努力吧，看看还有没有什么可以挖掘的，于是，你就goto到了看数据的那一步。\n别看这0.2，大的数据集合下，提高0.2已经是非常不错的提高了，所以花这么多钱，养算法工程师，要是一年能出几次0.2，那就是真值了。\n6. 让我们总结一下\n上面这么多的过程，靠一个人全部完成确实有点困难，我说的有点夸张，中间有些步骤是有人配合的，观察数据的时候有产品经理配合你，洗数据的时候有数据工程师配合你，工程化的时候有系统工程师配合你，但是作为机器学习的算法工程师，整个过程你都得能hold得住啊，所以即便是你一个人应该也要能完成整个流程才行。\n这只是一个标准的算法工程师应该具备的能力，当然我这里是以搜索算法举例的，其他的算法工程师也差不太多，总跑不过上面几个过程，当然，你要是牛人，能根据场景修改这个机器学习的模型，甚至自己能想个模型，那就更厉害了。\n好，我们把上面的重点标记的部分取出来汇总一下，让我们看看一个算法工程师需要具备哪些技能\n数据敏感性，观察力\n数学抽象能力，数学建模能力和数学工具的熟练使用的能力\n能随手编脚本代码的能力，强大的计算机算法编程能力，高级开发工程师的素质\n想象力，耐性和信心，较强的语言表达能力，抗打击能力\n然后，还有很关键的一点，你需要很聪明，当然，你如果能做到以上那么几点，基本上也会很聪明了，如果真能做到这样，反而那些机器学习的模型，理论和工具就显得不那么重要了，因为那些也只是知识和工具，随时都可以学嘛。\n你说，这些是靠看几篇博客，看几本书，上几次课就能具备的么？？\n当然，我们这里讨论的是一般情况，如果你一心就是做研究的话，那么需要把上述技能熟练度再提高一个量级。\n最后，正在学习机器学习，励志做算法工程师的你，准备好踏这些坑了么？？"}
{"content2":"首先要简单区别几个概念：人工智能，机器学习，深度学习，神经网络。这几个词应该是出现的最为频繁的，但是他们有什么区别呢？\n人工智能：人类通过直觉可以解决的问题，如：自然语言理解，图像识别，语音识别等，计算机很难解决，而人工智能就是要解决这类问题。\n机器学习：机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。\n深度学习：其核心就是自动将简单的特征组合成更加复杂的特征，并用这些特征解决问题。\n神经网络：最初是一个生物学的概念，一般是指大脑神经元，触点，细胞等组成的网络，用于产生意识，帮助生物思考和行动，后来人工智能受神经网络的启发，发展出了人工神经网络。\n来一张图就比较清楚了，如下图：\n机器学习的范围\n机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。\n模式识别\n模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的“模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10年间，它们都有了长足的发展”。\n数据挖掘\n数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是“石头”啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。\n统计学习\n统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。\n计算机视觉\n计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。\n语音识别\n语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。\n自然语言处理\n自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。\n机器学习的方法\n1、回归算法\n在大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。\n实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。\n逻辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线不是线性时，逻辑回归的表达能力就不足。\n2、神经网络\n让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是\"神经网络\"。\n在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。\n3、SVM（支持向量机）\n支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。\n但是，通过跟高斯“核”的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。“核”事实上就是一种特殊的函数，最典型的特征就是可以将低维的空间映射到高维的空间。\n我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过“核”可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维平面中的非线性划分效果。\n支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。\n4、聚类算法\n无监督算法中最典型的代表就是聚类算法。\n让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。\n聚类算法中最典型的代表就是K-Means算法。\n5、降维算法\n降维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。\n降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。\n6、推荐算法\n推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别：\n一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。\n另一类是基于用户相似度的推荐，则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。\n两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。\n7、其他\n除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。\n下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。\n监督学习算法：\n线性回归，逻辑回归，神经网络，SVM\n无监督学习算法：\n聚类算法，降维算法\n特殊算法：\n推荐算法\n除了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。\n机器学习的分类\n目前机器学习主流分为：监督学习，无监督学习，强化学习。\na) 监督学习是最常见的一种机器学习，它的训练数据是有标签的，训练目标是能够给新数据（测试数据）以正确的标签。例如，将邮件进行是否垃圾邮件的分类，一开始我们先将一些邮件及其标签（垃圾邮件或非垃圾邮件）一起进行训练，学习模型不断捕捉这些邮件与标签间的联系进行自我调整和完善，然后我们给一些不带标签的新邮件，让该模型对新邮件进行是否是垃圾邮件的分类。\nb) 无监督学习常常被用于数据挖掘，用于在大量无标签数据中发现些什么。无监督主要有三种：聚类、离散点检测和降维。\n它的训练数据是无标签的，训练目标是能对观察值进行分类或者区分等。例如无监督学习应该能在不给任何额外提示的情况下，仅依据所有“猫”的图片的特征，将“猫”的图片从大量的各种各样的图片中将区分出来。\nc) 强化学习通常被用在机器人技术上（例如机械狗），它接收机器人当前状态，算法的目标是训练机器来做出各种特定行为。工作流程多是：机器被放置在一个特定环境中，在这个环境里机器可以持续性地进行自我训练，而环境会给出或正或负的反馈。机器会从以往的行动经验中得到提升并最终找到最好的知识内容来帮助它做出最有效的行为决策。\n机器学习模型的评估\n拿猫的识别来举例，假设机器通过学习，已经具备了一定的识别能力。那么，我们输入4张图片，机器的判断如下：\n常用的评价指标有三种：准确率（precision）、召回率（recall）和精准率（accuracy），其中：\nPrecision = TP/（TP+FP），表示我们抓到的人中，抓对了的比例；\nRecall = TP/ （TP+FN），表示我们抓到的坏人占所有坏人的比例；\nAccuracy = （TP + TN）/ All ，表示识别对了（好人被识别成好人，坏人被识别成坏人）的比例。\n三个指标越高，表示算法的适应性越好。\n机器学习的应用\n机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够“预测”。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的“经验丰富”的人比“初出茅庐”的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。\n机器学习界的名言：成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！\n在大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。\n机器学习的子类--深度学习\n2006年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点：\n1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；\n2.深度神经网络在训练上的难度，可以通过“逐层初始化” 来有效克服。\n通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。\n目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。\n深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。\n机器学习的父类--人工智能\n人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图：\n总结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此接近人工智能的梦想。\n让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。\n人工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。\n出处：\nhttps://www.cnblogs.com/lizheng114/p/7439556.html\nhttp://www.cnblogs.com/subconscious/p/4107357.html"}
{"content2":"眼下，人工智能已经成为越来越火的一个方向。普通程序员，如何转向人工智能方向，是知乎上的一个问题。本文是我对此问题的一个回答的归档版。相比原回答有所内容增加。\n一. 目的\n本文的目的是给出一个简单的，平滑的，易于实现的学习方法，帮助 “普通” 程序员踏入AI领域这个门。这里，我对普通程序员的定义是：拥有大学本科知识；平时工作较忙；自己能获取的数据有限。因此，本文更像是一篇 “from the scratch” 的AI入门教程。\n二. AI领域简介\nAI，也就是人工智能，并不仅仅包括机器学习。曾经，符号与逻辑被认为是人工智能实现的关键，而如今则是基于统计的机器学习占据了主导地位。最近火热的深度学习正是机器学习中的一个子项。目前可以说，学习AI主要的是学习机器学习。但是，人工智能并不等同于机器学习，这点在进入这个领域时一定要认识清楚。关于AI领域的发展历史介绍推荐看周老师写的《机器学习简介》。下面一个问题是：AI的门好跨么？其实很不好跨。我们以机器学习为例。在学习过程中，你会面对大量复杂的公式，在实际项目中会面对数据的缺乏，以及艰辛的调参等。如果仅仅是因为觉得这个方向未来会“火”的话，那么这些困难会容易让人放弃。考虑到普通程序员的特点，而要学习如此困难的学科，是否就是没有门路的？答案是否定的。只要制定合适的学习方法即可。\n三. 学习方法\n学习方法的设定简单说就是回答以下几个问题：我要学的是什么？我怎样学习？我如何去学习？这三个问题概括说就是：学习目标，学习方针与学习计划。学习目标比较清楚，就是踏入AI领域这个门。这个目标不大，因此实现起来也较为容易。“过大的目标时就是为了你日后放弃它时找到了足够的理由”。学习方针可以总结为 “兴趣为先，践学结合”。简单说就是先培养兴趣，然后学习中把实践穿插进来，螺旋式提高。这种方式学习效果好，而且不容易让人放弃。有了学习方针以后，就可以制定学习计划，也称为学习路线。下面就是学习路线的介绍。\n四. 学习路线\n我推荐的学习路线是这样的，如下图：\nAI领域学习路线图\n这个学习路线是这样设计的：首先了解这个领域，建立起全面的视野，培养起充足的兴趣，然后开始学习机器学习的基础，这里选择一门由浅入深的课程来学习，课程最好有足够的实验能够进行实战。基础打下后，对机器学习已经有了充足的了解，可以用机器学习来解决一个实际的问题。这时还是可以把机器学习方法当作一个黑盒子来处理的。实战经验积累以后，可以考虑继续进行学习。这时候有两个选择，深度学习或者继续机器学习。深度学习是目前最火热的机器学习方向，其中一些方法已经跟传统的机器学习不太一样，因此可以单独学习。除了深度学习以外，机器学习还包括统计学习，集成学习等实用方法。如果条件足够，可以同时学习两者，一些规律对两者是共通的。学习完后，你已经具备了较强的知识储备，可以进入较难的实战。这时候有两个选择，工业界的可以选择看开源项目，以改代码为目的来读代码；学术界的可以看特定领域的论文，为解决问题而想发论文。无论哪者，都需要知识过硬，以及较强的编码能力，因此很能考察和锻炼水平。经过这个阶段以后，可以说是踏入AI领域的门了。“师傅领进门，修行在个人”。之后的路就要自己走了。\n下面是关于每个阶段的具体介绍：\n0.领域了解\n在学习任何一门知识之前，首先第一步就是了解这个知识是什么？它能做什么事？它的价值在什么地方？如果不理解这些的话，那么学习本身就是一个没有方向的舟，不知道驶向何处，也极易有沉船的风险。了解这些问题后，你才能培养出兴趣，兴趣是最好的引路人，学习的动力与持久力才能让你应付接下来的若干个阶段。关于机器学习是什么，能做什么，它与深度学习以及人工智能的关系，可以看我写的博客 从机器学习谈起：\n1.知识准备\n如果你离校过久，或者觉得基础不牢，最好事先做一下准备复习工作。“工欲善其事，必先利其器”。以下的准备工作不多，但足以应付后面阶段的学习。\n数学：复习以下基本知识。线性代数：矩阵乘法；高数：求导；概率论：条件与后验概率。其他的一些知识可以在后面的学习的过程中按需再补；\n英文：常备一个在线英文词典，例如爱词霸，能够不吃力的看一些英文的资料网页；\nFQ：可以随时随地上Google，这是一个很重要的工具。不是说百度查的不能看，而是很多情况下Google搜出来的资料比百度搜的几十页的资料还管用，尤其是在查英文关键字时。节省时间可是很重要的学习效率提升；\n2.机器学习\n机器学习的第一门课程首推Andrew Ng的机器学习。这门课程有以下特点：难度适中，同时有足够的实战例子，非常适合第一次学习的人。cs229 这门课程我这里不推荐，为什么，原因有以下：\n时间：cs229 的时间太早，一些知识已经跟不上当今的发展，目前最为火热的神经网络一笔带过。而Cousera上神经网络可是用了两个课时去讲的！而且非常详细；\n教学：Ng在cs229 时候的教学稍显青涩，可能是面对网络教学的原因。有很多问题其实他都没有讲清楚，而且下面的人的提问其实也很烦躁，你往往不关心那些人的问题。这点在Coursera上就明显得到了改善，你会发现Ng的教学水平大幅度改善了，他会对你循循善诱，推心置腹，由浅入深的教学，在碰到你不明白的单词术语时也会叫你不要担心，更重要的，推导与图表不要太完善，非常细致清晰，这点真是强力推荐；\n字幕：cs229 的字幕质量比Coursera上的差了一截。Coursera上中文字幕翻译经过了多人把关，质量很有保证；\n作业：cs229 没有作业，虽然你可以做一些，但不会有人看。这点远不如Coursera上每周有deadline的那种作业，而且每期作业提交上去都有打分。更重要的是，每期作业都有实际的例子，让你手把手练习，而且能看到自己的成果，成就感满满！\n3.实践做项目\n学习完了基础课程，你对机器学习就有了初步了解。现在使用它们是没有问题的，你可以把机器学习算法当作黑盒子，放进去数据，就会有结果。在实战中你更需要去关心如何获取数据，以及怎么调参等。如果有时间，自己动手做一个简单的实践项目是最好的。这里需要选择一个应用方向，是图像（计算机视觉），音频（语音识别），还是文本（自然语言处理）。这里推荐选择图像领域，这里面的开源项目较多，入门也较简单，可以使用OpenCV做开发，里面已经实现好了神经网络，SVM等机器学习算法。项目做好后，可以开源到到 Github 上面，然后不断完善它。实战项目做完后，你可以继续进一步深入学习，这时候有两个选择，深度学习和继续机器学习；\n4.深度学习\n深度学习：深度学习是目前最火热的研究方向。有以下特点：知识更新快，较为零碎，没有系统讲解的书。因此学习的资源也相对零散，下面是一些资源介绍。其中不推荐的部分并不代表不好，而是在这个初学阶段不合适：\n推荐，UFLDL： 非常好的DL基础教程，也是Andrew Ng写的。有很详尽的推导，有翻译，且翻译质量很高；\n推荐，Deep learning (paper)：2015年Nature上的论文，由三位深度学习界的大牛所写，读完全篇论文，给人高屋建瓴，一览众山小的感觉，强烈推荐。如果只能读一篇论文了解深度学习，我推荐此篇。这篇论文有同名的中文翻译；\n推荐，Neural networks and deep learning：这本书的作者非常擅长以浅显的语言表达深刻的道理，虽然没有翻译，但是阅读并不困难；\n推荐，Recurrent Neural Networks： 结合一个实际案例告诉你RNN是什么，整篇教程学完以后，会让你对RNN如何产生作用的有很清晰的认识，而这个效果，甚至是读几篇相关论文所没有的；\n不推荐，Neural Networks for Machine Learning - University of Toronto | Coursera：深度学习创始人教的课，最大的问题是太难，而且老先生的吐字有时不是很标准；\n不推荐，Deep Learning (book)：同样也是由深度学习大牛所写的书，但感觉就像是第二作者，也就是他的学生所写的。很多内容都讲了，但是感觉也没讲出什么内容来，只是告诉你来自那篇论文，这样的话可能直接阅读论文更合适。\n不推荐，cs231n：李菲菲的课程，很有名，专门讲CNN。但是这门课程有一个最大的问题，就是没有字幕，虽然有youtube的自动翻译字幕，但有还不如没有。\n5.继续机器学习\n深度学习未必就是未来的一定主流，至少一些大牛是这么认为的。传统的机器学习有如下特点，知识系统化，有相对经典的书。其中统计学习（代表SVM）与集成学习（代表adaboost）是在实践中使用非常多的技术。下面是相关资源：\n推荐，机器学习(周志华)：如果是在以前，机器学习方面的经典教材首推PRML，但现在周老师的书出来以后，就不再是这样了。首先推荐读周老师的书。这本书有一个特点，那就是再难的道理也能用浅显精炼的语言表达出来。正如周老师的名言：“体现你水平的地方是把难的东西讲容易了，而不是把容易的东西讲难，想把一个东西讲难实在太简单”；\n不推荐，Pattern Recognition And Machine Learning：当前阶段不推荐。PRML是以贝叶斯的观点看待很多机器学习方法，这也是它的一大特色。但对于初学者来说，这种观点其实并无必要。而且此书没有中文翻译，当前阶段硬啃很容易放弃；\n6.开源项目\n当知识储备较为充足时，学习可以再次转入实践阶段。这时候的实践仍然可以分两步走，学习经典的开源项目或者发表高质量的论文。开源项目的学习应该以尽量以优化为目的，单纯为读代码而学习效果往往不太好。好的开源项目都可以在Github 里搜索。这里以深度学习为例。深度学习的开源优秀库有很多，例如torch，theano等等，这里列举其中的两个：\n推荐，DeepLearnToolbox：较早的一个深度学习库，用matlab语言撰写，较为适合从刚学习的课程转入学习。遗憾的是作者不再维护它了；\n推荐，tensorflow：Google的开源库，时至今日，已经有40000多个star，非常惊人，支持移动设备；\n7.会议论文\n较好的课程都会推荐你一些论文。一些著名的技术与方法往往诞生于一些重要的会议。因此，看往年的会议论文是深入学习的方法。在这时，一些论文中的内容会驱使你学习数学中你不擅长的部分。有时候你会觉得数学知识储备不够，因此往往需要学习一些辅助课程。当你看完足够的论文以后，在这个阶段，如果是在校学生，可以选择某个课题，以发论文为目的来学习研究。一般来说，论文是工作的产物。有时候一篇基于实验的论文往往需要你写代码或者基于开源项目。因此开源项目的学习与会议论文的工作两者之间是有相关的。两者可以同时进行学习。关于在哪里看论文，可以看一下CCF推荐排名，了解一下这个领域里有哪些优秀的会议。下面介绍两个图像与机器学习领域的著名顶级会议：\nCVPR：与另两个会议ICCV和ECCV合称计算机视觉领域的三大会，注意会议每年的主页是变动的，因此搜索需要加上年份；\nConference on Neural Information Processing Systems：简称NIPS，许多重要的工作发表在这上面，例如关于CNN的一篇重要论文就是发表在上面；\n8.自由学习\n自由学习：到这里了，可以说是进入这个门了。下面可以依据兴趣来自由学习。前阶段不推荐的学习资源也可随意学习，下面是点评：\ncs229 ：Ng写的讲义很不错，其中关于SVM的推导部分很清晰，想学习SVM推荐；\nNeural Networks for Machine Learning：大牛的视角跟人就是不一样，看看Hinton对神经网络是怎么看的，往往会让你有种原来如此的感悟。其实看这门课程也等同于读论文，因为几乎每节课的参考资料里都有论文要你读；\nCS231n: Convolutional Neural Networks for Visual Recognition：最新的知识，还有详细的作业。国内应该有团队对字幕进行了翻译，可以找找；\nPRML：作为一门经典的机器学习书籍，是很有阅读必要的，会让你对机器学习拥有一个其他的观察视角；\n五. 总结\n本文的目的是帮助对AI领域了解不深，但又想进入的同学踏入这个门。这里只说踏入，是因为这个领域的专精实在非常困难，需要数年的积累与努力。在进行领域学习前，充分认识自己的特点，制定合适的学习方法是十分重要的。首先得对这个领域进行充分了解，培养兴趣。在学习时，保持着循序渐进的学习方针，不要猛进的学习过难资源；结合着学习与实践相辅的策略，不要只读只看，实际动手才有成就感。学习某个资源时要有充分的目的，不是为了学开源项目而看代码，而是为了写开源项目而看；不是为了发论文而写论文，而是为了做事情而写论文。如果一个学习资源对你过难，并不代表一定是你的问题，可能是学习资源的演讲或撰写人的问题。能把难的问题讲简单的人才是真正有水平的人。所以，一定要学习优质资源，而不是不分青红皂白的学习。最后，牢记以兴趣来学习。学习的时间很长，过程也很艰难，而只有兴趣才是让你持之以恒，攻克难关的最佳助力。\n谨以此文与在学海中乘舟的诸位共勉。我就是一名普通程序员，刚刚转入AI领域，还有很多不足。希望此文可以帮助到大家。"}
{"content2":"机器学习和数据挖掘推荐书单\n有了这些书，再也不愁下了班没妹纸该咋办了。慢慢来，认真学，揭开机器学习和数据挖掘这一神秘的面纱吧！\n《机器学习实战》：本书第一部分主要介绍机器学习基础，以及如何利用算法进行分类，并逐步介绍了多种经典的监督学习算法，如k近邻算法、朴素贝叶斯算法、Logistic回归算法、支持向量机、AdaBoost集成方法、基于树的回归算法和分类回归树（CART）算法等。第三部分则重点介绍无监督学习及其一些主要算法：k均值聚类算法、Apriori算法、FP-Growth算法。第四部分介绍了机器学习算法的一些附属工具。\n全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。\n之前看过一部分这本书，但是实习工作涉及到用Java代码处理数据，所以暂时先搁一下，目前正在李航的那本书。\n《数据挖掘-实用机器学习技术》：本书介绍数据挖掘的基本理论与实践方法。主要内容包括：各种模型(决策树，关联规则、线性模型、聚类、贝叶斯网以及神经网络)以及在实践中的运用，所存任缺陷的分析。安全地清理数据集、建立以及评估模型的预测质量的方法，并且提供了一个公开的数据挖掘工作平台Weka。Weka系统拥有进行数据挖掘仟务的图形用户界面，有助于理解模型，是一个实用并且深受欢迎的工具。\n《数据挖掘：概念与技术》：本书全面地讲述数据挖掘领域的重要知识和技术创新。在第1版内容相当全面的基础上，第2版展示了该领域的最新研究成果，例如挖掘流、时序和序列数据以及挖掘时间空间、多媒体、文本和Web数据。本书可作为数据挖掘和知识发现领域的教师、研究人员和开发人员的一本必读书。\n《统计学习基础 数据挖掘、推理与预测》：尽管应用的是统计学方法，但强调的是概念，而不是数学。许多例子附以彩图。《统计学习基础:数据挖掘、推理与预测》内容广泛，从有指导的学习（预测）到无指导的学习，应有尽有。包括神经网络、支持向量机、分类树和提升等主题，是同类书籍中介绍得最全面的。计算和信息技术的飞速发展带来了医学、生物学、财经和营销等诸多领域的海量数据。理解这些数据是一种挑战，这导致了统计学领域新工具的发展，并延伸到诸如数据挖掘、机器学习和生物信息学等新领域。\n《机器学习》（Mitchell）：展示了机器学习中核心的算法和理论，并阐明了算法的运行过程。《机器学习》综合了许多的研究成果，例如统计学、人工智能、哲学、信息论、生物学、认知科学、计算复杂性和控制论等，并以此来理解问题的背景、算法和其中的隐含假定。《机器学习》可作为计算机专业 本科生、研究生教材，也可作为相关领域研究人员、教师的参考书。\n《统计学习方法》：本书全面系统地介绍了统计学习的主要方法，特别是监督学习方法，包括感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、em算法、隐马尔可夫模型和条件随机场等。除第1章概论和最后一章总结外，每章介绍一种方法。叙述从具体问题或实例入手，由浅入深，阐明思路，给出必要的数学推导，便于读者掌握统计学习方法的实质，学会运用。为满足读者进一步学习的需要，书中还介绍了一些相关研究，给出了少量习题，列出了主要参考文献。\n《机器学习导论》：对机器学习的定义和应用实例进行了介绍，涵盖了监督学习。贝叶斯决策理论。参数方法、多元方法、维度归约、聚类、非参数方法、决策树。线性判别式、多层感知器，局部模型、隐马尔可夫模型。分类算法评估和比较，组合多学习器以及增强学习等。\n《机器学习及其应用》：全书共分14章，内容分别涉及因果推断、流形学习与降维、迁移学习、类别不平衡学习、演化聚类、多标记学习、排序学习、半监督学习等技术和协同过滤、社区推荐、机器翻译等应用，以及互联网应用对机器学习技术需求的探讨。\n《模式分类》第二版：除了保留了第1版的关于统计模式识别和结构模式识别的主要内容以外，读者将会发现新增了许多近25年来的新理论和新方法，其中包括神经网络、机器学习、数据挖掘、进化计算、不变量理论、隐马尔可夫模型、统计学习理论和支持向量机等。\n《推荐系统实践》：过大量代码和图表全面系统地阐述了和推荐系统有关的理论基础，介绍了评价推荐系统优劣的各种标准(比如覆盖率、满意度)和方法(比如AB测试)，总结了当今互联网领域中各种和推荐有关的产品和服务。\n《深入搜索引擎--海量信息的压缩、索引和查询》：理论和实践并重，深入浅出地给出了海量信息数据处理的整套解决方案，包括压缩、索引和查询的方方面面。其最大的特色在于不仅仅满足信息检索理论学习的需要，更重要的是给出了实践中可能面对的各种问题及其解决方法。\n《概率论与数理统计》：这本书不用过多介绍了吧，普遍大学里大一时期的教科书，只恨当年没听课啊，现在正在慢慢啃。。。\n《大数据：互联网大规模数据挖掘与分布式处理》：主要内容包括分布式文件系统、相似性搜索、搜索引擎技术、频繁项集挖掘、聚类算法、广告管理及推荐系统。\n《Web数据挖掘》：信息检索领域的书籍，该书深入讲解了从大量非结构化Web数据中提取和产生知识的技术。书中首先论述了Web的基础（包括Web信息采集机制、Web标引机制以及基于关键字或基于相似性搜索机制），然后系统地描述了Web挖掘的基础知识，着重介绍基于超文本的机器学习和数据挖掘方法，如聚类、协同过滤、监督学习、半监督学习，最后讲述了这些基本原理在Web挖掘中的应用。《Web数据挖掘》为读者提供了坚实的技术背景和最新的知识。\n《数据之巅》：对大数据追根溯源，提出当前信息技术的发展，已经让中国获得了后发优势，中国要在大数据时代的全球竞争中胜出，必须把大数据从科技符号提升成为文化符号，在全社会倡导数据文化。\n《深入浅出统计学》：本书涵盖的知识点包括：信息可视化、概率计算、几何分布、二项分布及泊松分布、正态分布、统计抽样、置信区间的构建、假设检验、卡方分布、相关与回归等等，完整涵盖AP考试范围。\n《矩阵分析》：本书从数学分析的角度论述矩阵分析的经典方法和现代方法，取材新，有一定的深度，并给出在多元微积分、复分析、微分方程、量优化、逼近理论中的许多重要应用。主要内容包括：特征值、特征向量和相似性，酉等价和正规矩阵，标准形，Hermite矩阵和对称矩阵，向量范数和矩阵范数，特征值和估计和扰动，正定矩阵，非负矩阵。"}
{"content2":"摘要：随着机器学习和深度学习的热潮，各种图书层出不穷。然而多数是基础理论知识介绍，缺乏实现的深入理解。本系列文章是作者结合视频学习和书籍基础的笔记所得。本系列文章将采用理论结合实践方式编写。首先介绍机器学习和深度学习的范畴，然后介绍关于训练集、测试集等介绍。接着分别介绍机器学习常用算法，分别是监督学习之分类（决策树、临近取样、支持向量机、神经网络算法）监督学习之回归（线性回归、非线性回归）非监督学习（K-means聚类、Hierarchical聚类）。本文采用各个算法理论知识介绍，然后结合python具体实现源码和案例分析的方式（本文原创编著，转载注明出处:机器学习及其基础概念简介(2)）\n目录\n【Machine Learning】Python开发工具：Anaconda+Sublime(1)\n【Machine Learning】机器学习及其基础概念简介(2)\n【Machine Learning】决策树在商品购买力能力预测案例中的算法实现(3)\n【Machine Learning】KNN算法虹膜图片识别实战(4)\n1 机器学习简介\n机器学习 （Machine Learning, ML) ：\n概念：多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n学科定位：人工智能(Artificial Intelligence, AI）的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n定义：探究和开发一系列算法来如何使计算机不需要通过外部明显的指示，而可以自己通过数据来学习，建模，并且利用建好的模型和新的输入来进行预测的学科。\n发展：\nArthur Samuel (1959): 一门不需要通过外部程序指示而让计算机有能力自我学习的学科\nLangley（1996) ： “机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”\nTom Michell (1997):  “机器学习是对能通过经验自动改进的计算机算法的研究”\n学习：针对经验E (experience) 和一系列的任务 T (tasks) 和一定表现的衡量 P，如果随之经验E的积累，针对定义好的任务T可以提高表现P，就说计算机具有学习能力\n例子： 人脸识别、无人驾驶汽车、下棋、语音识别、电商推荐系统等\n应用：语音识别、自动驾驶、语言翻译、计算机视觉、推荐系统、无人机、识别垃圾邮件\n机器学习就业需求：LinkedIn所有职业技能需求量第一：机器学习，数据挖掘和统计分析人才 http://blog.linkedin.com/2014/12/17/the-25-hottest-skills-that-got-people-hired-in-2014/\n2 深度学习(Deep Learning)\n深度学习(Deep Learning)：\n深度学习是基于机器学习延伸出来的一个新的领域，由以人大脑结构为启发的神经网络算法为起源加之模型结构深度的增加发展，并伴随大数据和计算能力的提高而产生的一系列新的算法。\n深度学习发展：\n其概念由著名科学家Geoffrey Hinton等人在2006年和2007年在《Sciences》等上发表的文章被提出和兴起。\n学习能用来干什么？为什么近年来引起如此广泛的关注？\n深度学习，作为机器学习中延伸出来的一个领域，被应用在图像处理与计算机视觉，自然语言处理以及语音识别等领域。自2006年至今，学术界和工业界合作在深度学习方面的研究与应用在以上领域取得了突破性的进展。以ImageNet为数据库的经典图像中的物体识别竞赛为例，击了所有传统算法，取得了前所未有的精确度。\n深度学习目前有哪些代表性的学术机构和公司走在前沿？人才需要如何？\n学校以多伦多大学，纽约大学，斯坦福大学为代表，工业界以Google, Facebook, 和百度为代表走在深度学习研究与应用的前沿。Google挖走了Hinton，Facebook挖走了LeCun，百度硅谷的实验室挖走了Andrew Ng，Google去年4月份以超过5亿美金收购了专门研究深度学习的初创公司DeepMind, 深度学习方因技术的发展与人才的稀有造成的人才抢夺战达到了前所未有激烈的程度。诸多的大大小小(如阿里巴巴，雅虎）等公司也都在跟进，开始涉足深度学习领域，深度学习人才需求量会持续快速增长。\n深度学习如今和未来将对我们生活造成怎样的影响？\n目前我们使用的Android手机中google的语音识别，百度识图，google的图片搜索，都已经使用到了深度学习技术。Facebook在去年名为DeepFace的项目中对人脸识别的准备率第一次接近人类肉眼（97.25% vs 97.5%)。大数据时代，结合深度学习的发展在未来对我们生活的影响无法估量。保守而言，很多目前人类从事的活动都将因为深度学习和相关技术的发展被机器取代，如自动汽车驾驶，无人飞机，以及更加职能的机器人等。深度学习的发展让我们第一次看到并接近人工智能的终极目标。\n深度学习的应用展示：\n无人驾驶汽车中的路标识别\nGoogle Now中的语音识别\n百度识图\n针对图片，自动生成文字的描述\n图片文字识别结果：“A person riding a motorcycle on a dirt road,”\n3 机器学习相关概念介绍\n基本概念：训练集，测试集，特征值，监督学习，非监督学习，半监督学习，分类，回归\n概念学习：人类学习概念：鸟，车，计算机\n定义：概念学习是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数\n例子：学习 “享受运动\" 这一概念：\n小明进行水上运动，是否享受运动取决于很多因素\n样例\n天气\n温度\n湿度\n风力\n水温\n预报\n享受运动\n1\n晴\n暖\n普通\n强\n暖\n一样\n是\n2\n晴\n暖\n大\n强\n暖\n一样\n是\n3\n雨\n冷\n大\n强\n暖\n变化\n否\n4\n晴\n暖\n大\n强\n冷\n变化\n是\n天气：晴，阴，雨\n温度：暖，冷\n湿度：普通，大\n风力：强，弱\n水温：暖，冷\n预报：一样，变化\n享受运动：是，否\n概念定义在实例(instance)集合之上，这个集合表示为X。（X：所有可能的日子，每个日子的值由 天气，温度，湿度，风力，水温，预 报6个属性表示。待学习的概念或目标函数成为目标概念（target concept), 记做c。c(x) = 1, 当享受运动时， c(x) = 0 当不享受运动时，c(x)也可叫做y\nx: 每一个实例\nX: 样例, 所有实例的集合\n学习目标：f: X -> Y\n训练集(training set/data)/训练样例（training examples): 用来进行训练，也就是产生模型或者算法的数据集\n测试集(testing set/data)/测试样例 (testing examples)：用来专门进行测试已经学习好的模型或者算法的数据集\n特征向量(features/feature vector)：属性的集合，通常用一个向量来表示，附属于一个实例\n标记(label): c(x), 实例类别的标记\n正例(positive example)\n反例(negative example)\n例子：研究美国硅谷房价\n影响房价的两个重要因素：面积(平方米），学区（评分1-10）\n样例\n面积（平方米）\n学区 （11.2 深度学习(Deep Learning)介绍-10）\n房价 （1000$)\n1\n100\n8\n1000\n2\n120\n9\n1300\n3\n60\n6\n800\n4\n80\n9\n1100\n5\n95\n5\n850\n分类 (classification): 目标标记为类别型数据(category)\n回归(regression): 目标标记为连续性数值 (continuous numeric value)\n例子：研究肿瘤良性，恶性于尺寸，颜色的关系\n特征值：肿瘤尺寸，颜色\n标记：良性/恶性\n有监督学习(supervised learning)： 训练集有类别标记(class label)\n无监督学习(unsupervised learning)： 无类别标记(class label)\n半监督学习（semi-supervised learning)：有类别标记的训练集 + 无标记的训练集\n4 机器学习步骤框架\n把数据拆分为训练集和测试集\n用训练集和训练集的特征向量来训练算法\n用学习来的算法运用在测试集上来评估算法 （可能要设计到调整参数（parameter tuning), 用验证集（validation set）\n例如：\n100 天： 训练集\n10天：测试集 （不知道是否 ” 享受运动“， 知道6个属性，来预测每一天是否享受运动）"}
{"content2":"一、引言\n本材料参考Andrew Ng大神的机器学习课程 http://cs229.stanford.edu\n在上一篇有监督学习回归模型中，我们利用训练集直接对条件概率p(y|x;θ)建模，例如logistic回归就利用hθ(x) = g(θTx)对p(y|x;θ)建模（其中g(z)是sigmoid函数）。假设现在有一个分类问题，要根据一些动物的特征来区分大象(y = 1)和狗(y = 0)。给定这样的一种数据集，回归模型比如logistic回归会试图找到一条直线也就是决策边界，来区分大象与狗这两类，然后对于新来的样本，回归模型会根据这个新样本的特征计算这个样本会落在决策边界的哪一边，从而得到相应的分类结果。\n现在我们考虑另外一种建模方式：首先，根据训练集中的大象样本，我们可以建立大象模型，根据训练集中的狗样本，我们可以建立狗模型。然后，对于新来的动物样本，我们可以让它与大象模型匹配看概率有多少，与狗模型匹配看概率有多少，哪一个概率大就是那个分类。\n判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。\n生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，即：\n常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。\n二、高斯判别分析 Gaussian Discriminant Analysis\n高斯判别分析GDA是一种生成式模型，在GDA中，假设p(x|y)满足多值正态分布。多值正态分布介绍如下：\n2.1 多值正态分布 multivariate normal distribution\n一个n维的多值正态分布可以表示为多变量高斯分布，其参数为均值向量，协方差矩阵，其概率密度表示为：\n当均值向量为2维时概率密度的直观表示：\n左边的图表示均值为0，协方差矩阵∑ = I；中间的图表示均值为0，协方差矩阵∑ = 0.6I；右边的图表示均值为0，协方差矩阵∑ = 2I。可以观察到，协方差矩阵越大，概率分布越扁平；协方差矩阵越小，概率分布越高尖。\n2.2 高斯判别分析模型\n如果有一个分类问题，其训练集的输入特征x是随机的连续值，就可以利用高斯判别分析。可以假设p(x|y)满足多值正态分布，即：\n该模型的概率分布公式为：\n模型中的参数为Φ，Σ，μ0和μ1。于是似然函数（x和y的联合分布）为：\n其中Φ是y = 1的概率，Σ是协方差矩阵，μ0是y = 0对应的特征向量x的均值 ， μ1是y = 1对应的特征向量x的均值，于是得到它们的计算公式如下：\n于是这样就可以对p(x,y)建模，从而得到概率p(y = 0|x)与p(y = 1|x)，从而得到分类标签。其结果如下图所示：\n三、朴素贝叶斯模型\n在高斯判别分析GDA中，特征向量x是连续实数值，如果特征向量x是离散值，可以利用朴素贝叶斯模型。\n3.1 垃圾邮件分类\n假设我们有一个已被标记为是否是垃圾邮件的数据集，要建立一个垃圾邮件分类器。用一种简单的方式来描述邮件的特征，有一本词典，如果邮件包含词典中的第i个词，则设xi = 1，如果没有这个词，则设xi = 0，最后会形成这样的特征向量x：\n这个特征向量表示邮件包含单词\"a\"和单词\"buy\"，但是不包含单词\"aardvark,\"aardwolf\",\"zygmurgy\"。特征向量x的维数等于字典的大小。假设字典中有5000个单词，那么特征向量x就为5000维的包含0/1的向量，如果我们建立多项式分布模型，那么有25000中输出结果，这就意味着有接近25000个参数，这么多的参数，要建模很困难。\n因此为了建模p(x|y)，必须做出强约束假设，这里假设对于给定的y，特征x是条件独立的，这个假设条件称为朴素贝叶斯假设，得到的模型称为朴素贝叶斯模型。比如，如果y= 1表示垃圾邮件，其中包含单词200 \"buy\"，以及单词300 \"price\"，那么我们假设此时单词200 \"buy\" x200、单词300\"price\"x300 是条件独立的，可以表示为p(x200|y) = p(x200|y,x300)。注意，这个假设与x200与x300独立是不同的，x200与x300独立可以写作：p(x200) = p(x200|x300)；这个假设是对于给定的y，x200与x300是条件独立的。\n因此，利用上述假设，根据链式法则得到：\n该模型有3个参数：\n， ，\n那么。根据生成式模型的规则，我们要使联合概率最大：\n根据这3个参数意义，可以得到它们各自的计算公式：\n这样就得到了朴素贝叶斯模型的完整模型。对于新来的邮件特征向量x，可以计算：\n实际上只要比较分子就行了，分母对于y = 0和y = 1是一样的，这时只要比较p(y = 0|x)与p(y = 1|x)哪个大就可以确定邮件是否是垃圾邮件。\n3.2 拉普拉斯平滑\n朴素贝叶斯模型可以在大部分情况下工作良好。但是该模型有一个缺点：对数据稀疏问题敏感。\n比如在邮件分类中，对于低年级的研究生，NIPS显得太过于高大上，邮件中可能没有出现过，现在新来了一个邮件\"NIPS call for papers\"，假设NIPS这个词在词典中的位置为35000，然而NIPS这个词从来没有在训练数据中出现过，这是第一次出现NIPS，于是算概率时：\n由于NIPS从未在垃圾邮件和正常邮件中出现过，所以结果只能是0了。于是最后的后验概率：\n对于这样的情况，我们可以采用拉普拉斯平滑，对于未出现的特征，我们赋予一个小的值而不是0。具体平滑方法为：\n假设离散随机变量取值为{1,2,···,k}，原来的估计公式为：\n使用拉普拉斯平滑后，新的估计公式为：\n即每个k值出现次数加1，分母总的加k，类似于NLP中的平滑，具体参考宗成庆老师的《统计自然语言处理》一书。\n对于上述的朴素贝叶斯模型，参数计算公式改为："}
{"content2":"机器不学习 jqbxx.com-专注机器学习,深度学习,自然语言处理,大数据,个性化推荐,搜索算法,知识图谱\n虽然我不是专门研究迁移学习的，但是作为一个AI研究者，就如题图吴老师所说，迁移学习极为重要，是必须要学习的，今天就先总结介绍一些迁移学习的基础知识，目录如下：\n迁移学习一些概念\n迁移学习简介\n迁移学习的分类\n迁移学习热门研究方向\n迁移学习一些概念\n在文章的一开始，先来学习迁移学习一些概念：\n域：一个域 D 由一个特征空间 X 和特征空间上的边际概率分布 P(X) 组成，其中 X=x1,x2,...xn 。举个例子：对于一个有文档，其有很多词袋表征（bag-of-words representation）X 是所有文档表征的空间，而 xi 是第 i 个单词的二进制特征。P(X)代表对X的分布。\n任务 ：在给定一个域 D={X,P(X)} 之后，一个任务 T 由一个标签空间 y 以及一个条件概率分布 P(Y/X) 构成，其中，这个条件概率分布通常是从由特征—标签对 xi, yi组成的训练数据中学习得到。\n源域(source domain),目标域(target domain):在迁移学习中，我们已有的知识叫做源域(source domain)，要学习的新知识叫目标域(target domain)。\n负迁移:指的是在源域上学习到的知识，对于目标域上的学习产生负面作用。产生负迁移的原因主要有两个：一个是源域 和目标域的相似度很低，无法做迁移。另一个是虽数据问源域和目标域是相似的，但是，迁移学习方法不够好，没找到可迁移的成分，导致负迁移。\n迁移学习简介\n先举几个例子，比如我们已经会编写Java程序，就可以类比着来学习C++，都是面向对象的语言，就很快学会了，或者在学会骑自行车之后，骑摩托车也自己比较容易了，因为这两种交通工具有许多相似之处。总结起来，用成语来说迁移学习就是举一反三！\n再来个图示，如下左图，传统机器学习对不同的学习任务需要建立不同的模型，学习不同的参数，而对于迁移学习（右图），只需要利用源域中的数据将知识迁移到目标域，就能完成模型建立。\n迁移学习的严格定义:\n给定源域 Ds={Xs, Fs(X)} 和学习任务 Ts ,目标域 DT={Xt,Ft(X)} 和学习任务 Tt ,迁移学习旨在源域不同于目标域或学习任务 Tt 不同于学习任务 Ts 的条件下通过使用学习任务 Ts 和源域 Ds={Xs,Fs(X)} 所获取的知识来帮助学习目标的在目标域Dt的预测函数 Ft(.) 。\n为什么需要进行迁移学习？\n数据的标签很难获取，当有些任务的数据标签很难获取时，就可以通过其他容易获取标签且和该任务相似的任务来迁移学习。\n从头建立模型是复杂和耗时的，也即是需要通过迁移学习来加快学习效率。\n和一起相关领域的辨析：\n多任务学习：区别在于在迁移学习中，我们主要关心在我们的目标任务和域上的表现。而多任务学习中的目标是在所有可用的任务上都要表现良好，尽管某个标签数据通常都被假定在一个任务上。当然，现在迁移学习和多任务学习也并没有很大的区别，比如归纳式迁移学习中当两个域都有标签的时候， 这就与多任务学习相似\n持续学习：虽然多任务学习允许我们在许多任务中保留知识，而不会对我们的源任务造成性能损失，但只有在所有任务都处于训练时间的情况下，这才是可能的。对于每个新任务，我们通常需要重新训练我们所有任务的模型。然而，在现实世界中，我们希望一个代理能够通过使用它以往的一些经验来处理逐渐变得复杂的任务。为了达到这个目的，我们需要让一个模型在不忘记的情况下持续地学习。这个机器学习的领域被称为学会学习、元学习（meta learning）、终生学习，或者持续学习。持续学习在最近的强化学习 (强化学习以 Google DeepMind 对通用学习代理的探索而著称) 上已经取得了成功 ，也正在被用于序列到序列的模型上 。\n最近元学习（meta learning）也挺火爆的，再来举个网上的例子：我们都知道，在金庸的武侠世界中，有各种各样的武功，不同的武功都不一样，有内功也有外功。那么里面的张无忌就特别厉害，因为他练成了九阳神功。有了九阳神功，张无忌学习新的武功就特别快，在电影倚天屠龙记之魔教教主中，张无忌分分钟学会了张三丰的太极拳打败了玄冥二老。九阳神功就是一种学会学习的武功！我们希望神经网络也能学会学习，这样也就能快速学习啦！\nzero-shot 学习：如果我们把迁移学习使用到极限，并且想要仅仅从很少的实例中学习，这就分别得到了 few-shot、one-shot 以及 zero-shot 学习。让模型执行 one-shot 和 zero-shot 学习，无疑属于机器学习中最艰难的问题。而另一方面，这却是我们人类天生就会的：幼年的时候，为了让我们能够认出任何狗狗，我们仅仅需要被告知一次「这是一条狗」，然而成年人可以仅通过在文中阅读就理解一个东西的本质，不需要事先见过它。\none-shot 学习的新进展利用了这样的思想，即为了在测试的时候实现好的性能，模型需要显式地被训练，从而进行 one-shot 学习。但更加逼真、具有概括性的 zero-shot 学习设置在最近已经引起了注意，在零点学习中训练类别出现在测试的时候。\n以上就是一些相关领域，当然这里只是很简单的介绍，有一个概念，详细请自行了解\n迁移学习的分类\n按迁移情景分\n归纳式迁移学习（Inductive TL）：源域和目标域的学习任务不同\n直推式迁移学习（Transductive TL):源域和目标域不同，学习任务相同\n无监督迁移学习（Unsupervised TL):源域和目标域均没有标签\n根据源Domain和目前Domain 之间的关系，源Task 和 目标Task之间的关系，以及任务方法更详细的整理为下表：\n按迁移学习的基本方法分\n基于实例的迁移学习方法\n在源域中找到与目标域相似的数据，把这个数据的权值进行调整，使得新的数据与目标域的数据进行匹配。然后进行训练学习，得到适用于目标域的模型。这样的方法优点是方法简单，实现容易。缺点在于权重的选择与相似度的度量依赖经验，且源域与目标域的数据分布往往不同。\n基于特征的迁移学习方法\n当源域和目标域含有一些共同的交叉特征时，我们可以通过特征变换，将源域和目标域的特征变换到相同空间，使得该空间中源域数据与目标域数据具有相同分布的数据分布，然后进行传统的机器学习。优点是对大多数方法适用，效果较好。缺点在于难于求解，容易发生过适配。\n需要注意的的是基于特征的迁移学习方法和基于实例的迁移学习方法的不同是基于特征的迁移学习需要进行特征变换来使得源域和目标域数据到到同一特征空间，而基于实例的迁移学习只是从实际数据中进行选择来得到与目标域相似的部分数据，然后直接学习。\n基于模型的迁移学习方法\n源域和目标域共享模型参数，也就是将之前在源域中通过大量数据训练好的模型应用到目标域上进行预测。基于模型的迁移学习方法比较直接，这样的方法优点是可以充分利用模型之间存在的相似性。缺点在于模型参数不易收敛。\n举个例子：比如利用上千万的图象来训练好一个图象识别的系统，当我们遇到一个新的图象领域问题的时候，就不用再去找几千万个图象来训练了，只需把原来训练好的模型迁移到新的领域，在新的领域往往只需几万张图片就够，同样可以得到很高的精度。\n基于关系的迁移学习方法\n当两个域是相似的时候，那么它们之间会共享某种相似关系，将源域中学习到的逻辑网络关系应用到目标域上来进行迁移，比方说生物病毒传播规律到计算机病毒传播规律的迁移。这部分的研究工作比较少。典型方法就是mapping的方法\n看一个图来总结以上的知识（可以看成归纳式迁移学习是最广泛应用的）：\n按特征空间分\n同构迁移学习（Homogeneous TL）: 源域和目标域的特征维度相同分布不同\n异构迁移学习（Heterogeneous TL）：源域和目标域的特征空间不同\n以下图是做迁移学习分类的一个梳理：\n迁移学习热门研究方向\n域适配问题(domain adaptation)：有标签的源域和无标签的目标域共享相同的特征和类别，但是特征分布不同，如何利用源域标定目标域。解决Domain adaptation问题主要的思路就是将source训练好的模型能够用在target上，而域适配问题最主要的也就是如何减少source域和target域不同分布之间的差异。\n代表性论文有：Domain adaptation via transfer component analysis--基于特征的迁移方法；Density ratio estimation in machine learning--基于实例的迁移方法；Cross-domain video concept detection using adaptive svms--基于模型的迁移方法等等\n最近的进展也有 Wasserstein Distance Guided Representation Learning for Domain Adaptation等，用W-GAN来做domain adaptation，可以一看。\n多源迁移学习(multi-source TL):多个源域和目标域，通过进行有效的域筛选，从而进行迁移。多源迁移学习可以有效利用存在的多个可用域，综合起来进行迁移，达到较好的效果。当然现在如何衡量多个域之间的相关性和多个域的利用方法还是一个比较大的问题\n代表性论文有：Boosting for transfer learning；Multi-source transfer learning with multi-view adaboost等等\n深度迁移学习(deep TL) :特别是近年来由于深度学习的火爆，越来越多研究者利用深度神经网络的结构进行迁移学习，深度学习可以深度表征域中的知识结构，也大大增强了模型的泛化能力，可以说利用深度学习做迁移学习的前景还是很好的。\n代表性论文有：Simultaneous deep transfer across domains and tasks；Multi-source transfer learning with multi-view adaboost；Learning Transferable Features with Deep Adaptation Networks等等\n更多精彩内容，机器不学习官方网站 jqbxx.com"}
{"content2":"原文链接\n如果你是商界英才（而不是数据科学家或者机器学习专家），你也许对主流媒体宣传的人工智能（artificial intelligence，AI）已经耳熟能详了。你在《经济学人》和《名利场》杂志上读过相关文章，你看到过特斯拉自动驾驶的煽情文章，听到过史蒂芬•霍金讲述人工智能威胁人类的耸人听闻，甚至迪尔伯特关于人工智能和人类智能的玩笑你都知道。\n此时，胸怀大志要把自己的生意做大做强的你，面对媒体关于人工智能的碎碎念，可能萌生了两个疑问——\n第一，人工智能的商业潜力是真是假？\n第二，这玩意怎么用到我的生意上？\n对第一个问题，答案是：千真万确。今天的商业活动，可以开始应用人工智能来将要求人类智能的活动替换为自动处理以降低成本。人工智能可以允许你将一个需要人海战术的工作通量增加100倍而成本减少90%。\n第二个问题的答案要长一些。首先得消除主流媒体鼓吹导致的误解。一旦误解消除，我们才能为你介绍如何应用人工智能到自己的生意中去。\n◆ ◆ ◆\n误解一：人工智能是魔术\n多数主流媒体将人工智能描述为神奇而神秘的。我们只需为大魔术师般的公司，如Google，Facebook，Apple，Amazon和Microsoft等鼓掌欢呼即可。这样的描述只是在帮倒忙。如果我们想要人工智能应用到商业活动中，至少需要让公司的执行官们理解它。人工智能不是魔术。人工智能是数据、数学、模式和迭代。如果我们想要人工智能应用到商业活动中，我们必须更加透明，并解释清楚人工智能的3个互相连锁的关键概念。\n1.训练数据（TrainingData，TD）——\n训练数据是机器可以用来学习的起始数据集。训练数据有输入值和自带答案的输出值，这样机器学习模型可以从答案中寻找模式。比如，输入可以是客服单，带有客户和公司的客服代表之间的电子邮件。输出可以是基于公司某个分类定义的从1到5的分类标签。\n2.机器学习（MachineLearning，ML）——\n机器学习是软件从训练数据中学习到某种模式，并把它应用到新的输入数据中。比如，一个新的客服单，带有某位客户和某位公司客服代表的邮件来了，机器学习模型可以预测出一个分类，告诉你它对该分类的把握有多大。机器学习的关键特征是，它不是通过固定的规则来学习。因此，当它消化新的数据后，它会调整其规则。\n3.人机回圈（Human-in-the-Loop，HITL）——\n人机回圈是人工智能的第三个核心成分。我们不能指望机器学习万无一失。一个好的机器学习模型大概只有70%的准确性。因此你需要一个人机回圈流程，当模型的可信度低时，还可以依靠人。\n因此，别被人工智能的神话愚弄了。现在，有了人工智能的公式，在此基础上，你可以对人工智能有一个基本的理解了。AI = TD + ML + HITL\n◆ ◆ ◆\n误解2：人工智能是给科技精英用的\n媒体报道似乎暗示，人工智能只是科技精英的菜——只有像Amazon，Apple，Facebook，Google，IBM，Microsoft，Salesforce，Tesla，Uber这些公司能斥上亿美金巨资组建庞大的机器学习专家团队。这个概念是错的。\n今天，十万美元即可在商业过程中开始应用人工智能。因此，如果你的公司是全美营业额在5千万美元以上的26，000家公司之一，你就可以投入营业额的0.2%，来启动人工智能。\n因此，人工智能不只属于高科技公司。它属于任何行业。\n◆ ◆ ◆\n误解3：人工智能只解决亿万美元级的大问题\n主流媒体叙说的故事，通常是未来式的例子，比如无人驾驶汽车，无人机投递包裹。Google，Tesla和Uber这些公司投入了数亿美元争夺无人驾驶汽车领域的领先地位，因为“赢者通吃”的想法在作怪。这样的故事给人工智能打上了“花费亿万美元开拓创新领域”的烙印。但事实并非如此。\n人工智能也可以用几百万美元来解决现有问题。让我解释一下。任何生意的一个核心任务都是了解客户。这在最早的市场——古希腊的阿格拉如此，在古罗马的竞技场里面对面做买卖时如此，在网购盛行的今天也如此。许多公司坐拥非结构化的客户数据宝库，有电子邮件，也有Twitter评论。人工智能可以用于解决客服单分类或者理解推文情感这样的难题。\n因此人工智能不止是为了解决如无人驾驶汽车这样的亿万美元级“让人兴奋”的新问题，它也可以解决百万美元级的现有“无聊”问题，如通过客服单分类或者社交媒体情感分析来了解你的客户。\n◆ ◆ ◆\n误解4：算法比数据更重要\n主流媒体对人工智能的报道偏重于关注机器学习算法，将其视为最重要的部分。主流媒体似乎把算法与人脑等同了。他们隐约传达着这样一个信息：复杂的算法最终会超越人类的大脑并创造奇迹。媒体拿机器在国际象棋和围棋比赛里击败人类的故事作为例子。而且他们主要关注“深度神经网络”和“深度学习”，以及机器是如何做出决策。\n这种报道给人的印象是，一个公司要想应用人工智能就需要聘请机器学习专家来建立完美的算法。但如果一个企业没有思考如何获得高质量的算法，即使机器学习模型经过大量的特定训练数据学习之后，仍然会产生一个与期望（“我们有一个伟大的算法”）不匹配的结果（“我们的模型的准确率只有60%”）。\n现如今，没有计划或训练数据的预算就从微软，亚马逊和谷歌购买商业机器学习的服务，就像买一辆无法接近加油站的车，只是买了一块昂贵的金属。汽车和汽油的类比有些不贴切，因为如果你给机器学习模型的训练数据越多，机器学习模型就会越准确。这就像不断给汽车加油，汽车的燃料利用率会不断提高。训练数据对于机器学习模型的重要性比汽油对汽车的重要性更高。如果想深入了解对这类误解性的报道的话，你可以阅读我们以前的帖子《更多的数据击败更好的算法》。\n所以关键就是训练数据的质量和数量至少是和算法一样重要的，要确保你部署人工智能的计划和预算反映这一点。\n◆ ◆ ◆\n误解5:机器>人类\n在过去的30年里，无论是施瓦辛格在《终结者》里扮演的电子人杀手，还是艾丽西亚·维坎德在《机械姬》里扮演的智能机器人伊娃，媒体一直喜欢把人工智能描绘成比人类更强大的机器。媒体想编写一个机器对战人类谁会成为赢家的故事，这是可以理解的。但却歪曲了事实。\n例如，最近谷歌DeepMind 的 alphago战胜韩国棋手李世石的报道被简单地描述成机器战胜人类。这样的表达不是对真实情况的准确描述。更准确的描述是机器加上一群人打败了一个人。\n消除这种误解的主要理由是机器和人的技能是互补的。从上面的图中我们可以看出机器在处理结构化计算方面有优势。机器擅长“找到特征向量”的任务，不太擅长“找到豹纹裙”任务。人类在识别意义和背景上具有得天独厚的优势。人类很容易“找到豹纹裙”，但在“找到特征向量”方面跟机器相比不具有优势。\n因此，正确的框架是要意识到在商业情景下机器和人是互补的。人工智能是人和机器共同工作。\n◆ ◆ ◆\n错误6：人工智能是机器取代人类\n主流媒体为了关注度喜欢描绘一个反乌托邦式的未来，这种情况可能会发生，但这种描述对正确理解人和机器如何共同工作产生了不利的影响。\n例如，让我们再思索下分类支持票据的业务流程。现如今大多数企业都还是百分百人工操作的。结果就是不仅进度缓慢而且成本线性增长，限制了工作量。现在想象一下用模型分类10，000张支持票的准确度是70%。30%的错误是不能接受的，就需要人机回圈的参与。你可以设置可接受的置信阈值为95%并且只接受模型在置信水平不低于95%时的输出。所以最初的机器学习模型可能只做了一小部分的工作，比如说5-10%。但是，随着新的人为标记的数据被创建，并且将其反馈到机器学习模型中，模型会不断学习并提高。随着时间的推移，该模型可以处理越来越多的客户支持票据分类工作，分类票据的业务量可以显著提高。\n因此，人和机器共同协作可以增加业务量，保持质量，减少重要的业务流程的单位成本。\n这就消除了人工智能是机器代替人类的误解。事实是，人工智能是关于机器增强人类的能力。\n◆ ◆ ◆\n错误7：人工智能=机器学习\n主流媒体带给人们的最后一条根深蒂固的误解就是人工智能和机器学习是等同的。这个误解就导致了不切实际的管理期望—从微软，亚马逊或谷歌公司购买商业机器学习的服务就能神奇地将人工智能运用到生产中。\n而除了机器学习之外还需要训练数据和人机回圈才有可能找到可行的人工智能解决方案。\n没有训练数据的机器学习就像一辆没有汽油的汽车。既昂贵又无用。\n没有人机回圈的机器学习是不会有好的产出的。机器学习模型需要人的参与来去除低的置信度预测。\n因此，如果你是一个想把人工智能应用到业务上的执行官，现在你应该对它有一个认识框架了。你应该用人工智能的7个真理来代替这7个误解。\n真相1：人工智能=训练数据+机器学习+人机回圈\n真相2：人工智能属于任何行业\n真相3：人工智能可以用几百万美元来解决现有的商业问题\n真相4：算法并没有比训练数据的数量和质量更重要\n真相5：机器和人是互补的\n真相6：人工智能是机器增强人的能力\n真相7：人工智能=训练数据+机器学习+人机回圈\n原文发布时间为：2016-10-06\n原文链接"}
{"content2":"6. 学习模型的评估与选择\nContent\n6. 学习模型的评估与选择\n6.1 如何调试学习算法\n6.2 评估假设函数(Evaluating a hypothesis)\n6.3 模型选择与训练/验证/测试集(Model selection and training/validation/test sets)\n6.4 偏差与方差\n6.4.1 Diagnosing bias vs. variance.\n6.4.2 正则化与偏差/方差(Regularization and bias/variance)\n6.5 学习曲线(Learning Curves)\n6.6 调试学习算法\n我们已经学习了许多有用的学习模型(线性回归，Logistic回归，神经网络)，但是当要解决一个实际问题时，以下问题是我们要考虑的：\n如何知道我们所设计的模型是有用的或者较好的?\n当模型应用的不理想时，我们应该从哪些方面进行改进？\n如何针对具体问题选择学习模型?\n下面将针对上述问题提出建议。\n6.1 如何调试学习算法\n现在假设我们已经实现了如下的一个正则化的线性回归模型用于预测房价\n根据已有的训练集，我们已经将该模型训练完毕。但是，当我们需要在新的数据集上进行房价的预测时，发现预测的结果和实际有很大的误差。下面我们应该做什么？我们可以从下面的一些角度考虑:\n获取更多的数据量\n有时数据量大并没有帮助\n通常数据量越大，学习模型训练得越好，但是即使这样，也应该做一些初步实验（见6.5节 学习曲线）来确保数据量越大，训练越好。(如果一开始就用大量的数据来训练模型，将会耗费大量的时间：收集数据，训练模型)\n减少特征量\n细心的从已有的特征量中选出一个子集\n可以手工选择，也可以用一些降维( dimensionality reduction)技术\n增加额外的特征量\n有时并不起作用\n仔细考虑数据集，是否遗漏了一些重要的特征量(可能花费较多的时间)\n添加的特征量可能只是训练集的特征，不适合全体数据集，可能会过拟合\n添加多项式的特征量\n减少正则化参数\n增加正则化参数\n可以发现，我们似乎有很多种方法来改善学习模型，但是，有些方法可能要花费很多时间（或许还不起作用），有些方法可能是矛盾的。所以，需要一种方式来给我们指明方向：到底应该采用哪种或哪些方式来优化模型。我们将这种方式称为机器学习诊断(Machine Learning Diagnostics)。机器学习诊断是一种测试法，能够深入了解某种算法到底是否有用。这通常也能够告诉我们，要想改进一种算法的效果，什么样的尝试才是有意义的，从而节省时间，减少不必要的尝试。\n6.2 评估假设函数(Evaluating a hypothesis)\n当我们确定学习算法的参数的时候，我们考虑的是选择使训练误差最小化的参数。但我们已经知道(见3.1 underfitting and overfitting )，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数，可能该假设函数会过拟合，泛化能力弱。\n该如何判断一个假设函数是过拟合的呢?对于简单的例子，可以对假设函数 h(x) 进行画图 然后观察图形趋势。但是，特征量较多时（大于2），画图就很难实现。因此，我们需要另一种方法来评估我们的假设函数。如下给出了一种评估假设函数的标准方法：\n假设我们有这样一组数据组(如-1)，我们要做的是将这些数据分成两部分: 训练集和测试集。一种典型的分割方法是按照7:3的比例，将70%的数据作为训练集，30%的数据作为测试集。这里默认原有数据集是无序的（随机的），所以我们选择前70%作为训练集，后30%作为测试集，但如果原数据集是有序的，我们应该随机选择出7：3的数据集分别作为训练集和测试集。\n-1 大小为10的数据集及其划分\n因此，典型的训练和测试方案如下：\n用70%划分得到的训练集来训练模型：即最小化J(θ)\n计算训练后的模型在测试集上的误差(test set error)。\n其中Jtest(θ) 为测试集上的平均平方误差(average square error)，mtest为测试集的大小。\n如果我们使用线性回归，test set error 则为\n另一种定义Logistics回归的误差是误分类率(misclassification error)或称0/1错分率(0/1 misclassification).\n定义error function\nTest error is\n6.3 模型选择与训练/验证/测试集(Model selection and training/validation/test sets)\n如何选择正则化参数的大小和多项式的次数是常常面临的问题，称之为模型选择问题。我们已经多次接触到过拟合现象，在过拟合的情况中，参数非常拟合训练集，那么模型对于相同数据组预测集的预测误差不能够用来推广到一般情况的，即是不能作为实际的泛化误差。也就是不能说明你的假设对于新样本的效果。\n下面我们来考虑模型选择问题，假如要选择能最好地拟合数据的多项式次数，具体地，我们在次数为1到10之间应该如何做出选择。\nd表示应该选择的多项式次数。所以，似乎除了要确定的参数θ之外，我们同样需要用数据集来确定这个多项式的次数d。\nd =1 (linear)\nd=2 (quadratic)\n...\nd=10\n那么我们可以这样做:\n选择第一个模型(d = 1)，然后求训练误差的最小值，得到一个参数向量θ1\n选择第二个模型(d = 2), 二次函数模型, 进行同样的过程, 得到另一个参数向量θ2\n以此类推，最后得到θ10\n接下来对所有这些模型，求出测试集误差\nJtest(θ1)\nJtest(θ2)\n...\nJtest(θ10)\n接下来为了确定选择哪一个模型最好，即哪一个对应的测试集误差最小。对于这个例子，我们假设最终选择了五次多项式模型。\n确定模型后，现在我们想知道，这个模型能不能很好地推广到新样本。我们可以观察这个五次多项式假设模型对测试集的拟合情况，但这里有一个问题是：这样做仍然不能公平地说明，我的假设推广到一般时的效果。其原因在于，我们刚才是使用的测试集和假设拟合来得到的多项式次数d 这个参数，这也就是说，我们选择了一个能够最好地拟合测试集的参数d的值。因此，我们的参数向量θ5在拟合测试集时的结果很可能导致一个比实际泛化误差更完美的预测结果。换言之，我们是找了一个最能拟合测试集的参数d，因此我再用测试集来评价我们的模型就显得不公平了。\n为了解决这一问题，在模型选择中，如果我们想要评价某个假设，我们通常采用以下的方法：给定某个数据集，和刚才将数据分为训练和测试集不同的是，我们要将其分为三段：\n训练集 Training set (60%) - m values\n交叉检验集 Cross validation (CV) set (20%)mcv\n测试集 Test set (20%) mtest\n我们随之也可以定义训练误差，交叉验证误差和测试误差如下：\n因此，我们按如下方式选择模型：\n1. Minimize cost function for each of the models as before\n2. Test these hypothesis on the cross validation set to generate the cross validation error\n3. Pick the hypothesis with the lowest cross validation error. e.g. pick θ5\n4. Finally，Estimate generalization error of model using the test set\n值得注意的是，在如今的机器学习应用中, 也有很多人是用测试集来选择模型, 然后又同样的测试集来评价模型的表现报告测试误差，看起来好像还能得到比较不错的泛化误差。如果有很多很多测试集的话，这也许还能行得通，否则得到的测试误差很大程度要比实际的泛化误差好。因此最佳做法还是把数据分成训练集、验证集、测试集。\n6.4 偏差与方差\n6.4.1 Diagnosing bias vs. variance\n如-2，当运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况，要么是偏差比较大(欠拟合)，要么是方差比较大（过拟合），能判断出现的情况是这两种情况中的哪一种非常重要，因为它是一个很有效的指示器，告诉我们可以改进算法的最有效的方法和途径。\n-2 不同模型的拟合情况\n现在我们已经掌握了训练集，验证集和测试集的概念。我们就能更好地理解偏差和方差的问题。具体来说，我们沿用之前所使用的训练集误差和验证集误差的定义也就是平方误差，画出-3.\n-3 多项式次数与误差的关系\nd等于1是用线性函数来进行拟合，而在最右边的这个图表示更高次数的多项式的拟合情况。随着我们增大多项式的次数，我们将对训练集拟合得越来越好，所以如果d等于1时 对应着一个比较大的训练误差，而如果我们的多项式次数很高时 我们的训练误差就会很小 甚至可能等于0 因为可能非常拟合训练集。所以，当我们增大多项式次数时，我们不难发现训练误差明显下降。\n接下来我们再看交叉验证误差，如果d等于1，意味着用一个很简单的函数来拟合数据，此时我们不能很好地拟合训练集(欠拟合)，我们会得到一个较大的交叉验证误差，而如果我们用一个中等大小的多项式次数来拟合时，如d等于2，那么我们会得到一个更小的交叉验证误差，因为我们找了一个能够更好拟合数据的次数。但是，如果次数d太大，比如说d的值取为4 ，那么我们又过拟合了，我们又会得到一个较大的交叉验证误差。\n具体来说 假设我们得出了一个学习算法，而这个算法并没有表现地如期望那么好，我们应该判断此时的学习算法是正处于高偏差的问题还是高方差的问题。\n当训练误差和交叉验证误差相近且都比较大时，即对应-3曲线中的左端，对应的就是高偏差的问题\n相反地，当训练误差较小而交叉验证误差远大于训练误差时，即对应-3曲线右端，对应的是高方差的问题\n6.4.2 正则化与偏差/方差(Regularization and bias/variance )\n我们知道，算法正则化可以有效地防止过拟合。但正则化跟算法的偏差和方差又有什么关系呢？对于如下正则化的线性回归模型\n我们分析以下三种情形:\n第一种情形是正则化参数λ取一个比较大的值，如等于10000，此时，所有这些参数θ将被大大惩罚，其结果是这些参数的值将近似等于0 并且假设模型 h(x) 的值将等于或者近似等于。因此我们最终得到的假设函数应该近似是一条平滑的直线（如-4-(1)），因此这个假设处于高偏差，对数据集欠拟合(underfit)。\n与之对应的另一种情况是λ值很小，比如λ=0，这种情况下，如果我们要拟合一个高阶多项式，通常会处于高方差和过拟合(overfitting)的情况（如-4-(3)）。因为λ的值等于0相当于没有正则化项 因此会对假设过拟合。\n如-4-(2)，只有λ取不大不小的值时，才会得到一组对数据刚好拟合的参数值θ。\n-4 不同λ取值的拟合情况\n现在我们可以按照如下方式选择出一个最合适的正则化参数 λ：\n确定λ可能的取值向量，通常为[0，0.01，0.02，0.04，0.08，… ，10.24]\n每一个λ的可能取值对应一个模型，对每一个模型进行训练，使代价函数最小，得到对应的参数θ。\n对于每个训练后的模型，计算出其在交叉检验集上的误差\n取使最小的模型作为我们的模型，并将其应用于测试集，得到测试误差，并以此估计泛化误差。\n其中:\n与多项式次数与误差类似，我们可以画出λ与误差的函数关系，如-5所示\n-5 λ与误差的关系\n6.5 学习曲线(Learning Curves)\n有时我们需要检查学习算法运行是否一切正常，或者希望改进算法的表现或效果，那么学习曲线(Learning Curves)就是一种很好的工具。并且，我们可以使用学习曲线来判断某一个学习算法是否处于偏差，方差问题或是二者皆有。下面我们就来介绍学习曲线。\n学习曲线和-5类似，它们的区别在于学习曲线是以训练集的大小m为横坐标。纵坐标仍然是训练集误差Jtrain和交叉检验误差Jcv。\n一般情况下的学习曲线如-7所示：\n对于训练集误差而言，m越小，越容易拟合，误差越小，换言之，Jtrain随m的增大而增大。\n对于交叉检验误差而言，m越小，模型的泛化能力越弱，故误差越大，换言之，Jcv随m的增大而减小。\n当m大到一定程度时，训练集误差和交叉检验误差较接近且都比较小。\n-7 一般情况下的学习曲线\n当学习算法是高偏差时，如-8所示，此时:\n对于训练集误差而言，当m很小时，误差很小，但由于它不能很好的拟合训练集，随后就会增长较快，达到一个较稳定的值。\n对于交叉检验误差而言，当m很小时，算法的泛化能力非常弱，误差很大，随着m的增加，泛化能力稍有提升，误差会有所减小，但由于学习算法本身对训练集误差较大，故交叉检验误差不会下降太多，最后稳定在一个较高的值。\n在m不太大时，训练集误差就和交叉检验误差接近，但都比较大。\n所以，在高偏差的情况下，增大训练集往往不起作用。\n-8 高偏差时的学习曲线\n当学习算法是高方差时，如-9所示，此时：\n对于训练集误差而言，当m很小时，误差很小，并且由于算法能很好的拟合训练集(过拟合)，随着m的增加，误差只有少量增加(增加很慢)。\n对于交叉检验误差而言，当m很小时，算法的泛化能力非常弱，误差很大，随着m的增加，泛化能力稍有提升，误差会有所减小，但由于学习算法过拟合，泛化能力有限，故交叉检验误差不会下降太多，最后稳定在一个较高的值。\n在m较大时，训练集误差和交叉检验误差也有一定的差距，此时训练集误差较小，而交叉检验集误差较大。\n所以，在高方差的情况下，增大训练集通常是有效的(减少过拟合)。\n-9 高方差时的学习曲线\n6.6 调试学习算法\n经过了上面的分析，现在我们对调试学习算法的策略进行总结\nGet more training examples --> helps to fix high variance\nNot good if you have high bias\nSmaller set of features --> fixes high variance (overfitting)\nNot good if you have high bias\nTry adding additional features --> fixes high bias (because hypothesis is too simple, make hypothesis more specific)\nAdd polynomial terms --> fixes high bias problem\nDecreasing λ --> fixes high bias\nIncreases λ --> fixes high variance\n对于神经网络而言，我们需要针对不同的问题设计不同的网络结构，通常从下面两种角度考虑：\n选择一个较小的网络\n较少的隐藏层(如1层)和较少的隐藏单元，适用于变量（特征量）较少的情况\n可能欠拟合，但计算代价较小\n选择一个较大的网络\n更多的隐藏层-需要我们决定具体是多少层较好\n适用于变量较多的情况，\n可能会过拟合，需要使用正则化来削弱过拟合\n计算代价更大\n通常而言，选择一层隐藏层或许是比较好的选择。当然，在应用神经网路时也应将已有数据集划分为训练集，交叉检验集和测试集。"}
{"content2":"Spark提供了常用机器学习算法的实现， 封装于spark.ml和spark.mllib中.\nspark.mllib是基于RDD的机器学习库， spark.ml是基于DataFrame的机器学习库.\n相对于RDD， DataFrame拥有更丰富的操作API, 可以进行更灵活的操作. 目前, spark.mllib已经进入维护状态， 不再添加新特性.\n本文将重点介绍pyspark.ml， 测试环境为Spark 2.1, Python API.\n首先介绍pyspark.ml中的几个基类:\nML DataSet： 即为pyspark.sql.DataFrame作为数据集使用\npyspark.ml.Transformer： 代表将数据集转换到另一个数据集的算法\npyspark.ml.Estimator： 代表根据数据和参数创建模型的算法，包含方法\nfit(dataset, params)： 根据训练数据集和参数进行训练， 返回训练好的模型对象\npyspark.ml.Model: 代表训练好的模型的基类， 通常由Estimator.fit()创建. 包含的方法有:\ntransform(df): 将输入数据集代入模型变换为输出数据集\nsave(path): 保存训练好的模型\nload(path): 从文件中加载模型\npyspark.ml.Pipeline： 用于将多个步骤组合为管道进行处理， 可以建立线性管道和有向无环图管道.\npyspark.ml下将不同算法封装到不同的包中:\npyspark.ml.linalg 线性代数工具包. 包括：\nVector\nDenseVector\nSparseVector\nMatrix\nDenseMatrix\nSparseMatrix\npyspark.ml.feature特征和预处理算法包. 包括:\nTokenizer\nNormalizer\nStopWordsRemover\nPCA\nNGram\nWord2Vec\npyspark.ml.classification分类算法包. 包括：\nLogisticRegression\nDecisionTreeClassifier\nRandomForestClassifier\nNaiveBayes\nMultilayerPerceptronClassifier\nOneVsRest\npyspark.ml.clustering 聚类算法包. 包括：\nKMeans\nLDA\npyspark.ml.regression回归算法包. 包括：\nLinearRegression\nGeneralizedLinearRegression\nDecisionTreeRegressor\nRandomForestRegressor\npyspark.ml.recommendation推荐系统算法包. 包括：\nALS\npyspark.ml.tuning 校验工具包\npyspark.ml.evaluation 评估工具包\npyspark.ml中的算法大多数为Estimator的派生类. 大多数算法类均拥有对应的Model类.\n如classification.NaiveBayes和classification.NaiveBayesModel. 算法类的fit方法可以生成对应的Model类.\n应用示例\npyspark.ml使用了统一风格的接口，这里只展示部分算法.\n首先用NaiveBayes分类器做一个二分类：\n>>> from pyspark.sql import Row >>> from pyspark.ml.linalg import Vectors >>> df = spark.createDataFrame([ ... Row(label=0.0, weight=0.1, features=Vectors.dense([0.0, 0.0])), ... Row(label=0.0, weight=0.5, features=Vectors.dense([0.0, 1.0])), ... Row(label=1.0, weight=1.0, features=Vectors.dense([1.0, 0.0]))]) >>> nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", weightCol=\"weight\") >>> model = nb.fit(df) # 构造模型 >>> test0 = sc.parallelize([Row(features=Vectors.dense([1.0, 0.0]))]).toDF() >>> result = model.transform(test0).head() # 预测 >>> result.prediction 1.0 >>> result.probability DenseVector([0.32..., 0.67...]) >>> result.rawPrediction DenseVector([-1.72..., -0.99...])\nmodel.transform将输入的一行(Row)作为一个样本，产生一行输出. 这里我们只输入了一个测试样本， 所以直接使用head()取出唯一一行输出.\n使用LogisticRegression和OneVsRest做多分类：\n>>> from pyspark.sql import Row >>> from pyspark.ml.linalg import Vectors >>> df = sc.parallelize([ ... Row(label=0.0, features=Vectors.dense(1.0, 0.8)), ... Row(label=1.0, features=Vectors.sparse(2, [], [])), ... Row(label=2.0, features=Vectors.dense(0.5, 0.5))]).toDF() >>> lr = LogisticRegression(maxIter=5, regParam=0.01) >>> ovr = OneVsRest(classifier=lr) >>> model = ovr.fit(df) >>> # 进行预测 >>> test0 = sc.parallelize([Row(features=Vectors.dense(-1.0, 0.0))]).toDF() >>> model.transform(test0).head().prediction 1.0 >>> test1 = sc.parallelize([Row(features=Vectors.sparse(2, [0], [1.0]))]).toDF() >>> model.transform(test1).head().prediction 0.0 >>> test2 = sc.parallelize([Row(features=Vectors.dense(0.5, 0.4))]).toDF() >>> model.transform(test2).head().prediction 2.0\n使用PCA进行降维：\n>>> from pyspark.ml.linalg import Vectors >>> data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),), ... (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), ... (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)] >>> df = spark.createDataFrame(data,[\"features\"]) >>> pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\") >>> model = pca.fit(df) >>> model.transform(df).head().pca_features DenseVector([1.648..., -4.013...])\nEstimator和Transformer均为PipelineStage的派生类，pipeline由一系列Stage组成.调用pipeline对象的fit方法， 将会依次执行Stage并生成一个最终模型.\n>>>from pyspark.ml import Pipeline >>>from pyspark.ml.classification import LogisticRegression >>>from pyspark.ml.feature import HashingTF, Tokenizer >>> data = [ (0, \"a b c d e spark\", 1.0), (1, \"b d\", 0.0), (2, \"spark f g h\", 1.0), (3, \"hadoop mapreduce\", 0.0) ] >>> df = spark.createDataFrame(data, [\"id\", \"text\", \"label\"]) >>> # build pipeline >>> tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") >>> hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") >>> lr = LogisticRegression(maxIter=10, regParam=0.001) >>> pipeline = Pipeline(stages=[tokenizer, hashingTF, lr]) >>> # train >>> model = pipeline.fit(df) >>> data2 = [ (4, \"spark i j k\"), (5, \"l m n\"), (6, \"spark hadoop spark\"), (7, \"apache hadoop\") ] >>> test = spark.createDataFrame(data2, [\"id\", \"text\"]) >>> result = model.transform(test) >>> result = result.select(\"id\", \"text\", \"probability\", \"prediction\") >>> result.collect() [Row(id=4, text=u'spark i j k', probability=DenseVector([0.1596, 0.8404]), prediction=1.0), Row(id=5, text=u'l m n', probability=DenseVector([0.8378, 0.1622]), prediction=0.0), Row(id=6, text=u'spark hadoop spark', probability=DenseVector([0.0693, 0.9307]), prediction=1.0), Row(id=7, text=u'apache hadoop', probability=DenseVector([0.9822, 0.0178]), prediction=0.0)]\n本文示例来源于官方文档\n更多内容请参考:\npyspark.ml文档\nSpark ML编程指导"}
{"content2":"引言\n随机森林在机器学习实战中没有讲到，我是从伯克利大学的一个叫breiman的主页中看到相关的资料，这个breiman好像是随机森林算法的提出者，网址如下\nhttp://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n随机森林算法简介\n随机森林说白了就是很多个决策树组成在一起，就形成了森林，关键在于如何创建森林里的每一棵树，随机森林用到的方法bootstrap法，通俗的讲就是有放回的抽取样本\n这里有个理论依据在这，说明有放回的抽取方法大概有三分之一的样本不会被抽取到，在此我简单说一下这个原因\n三分之一的样本不会被抽取到的原因\n当N足够大时， 将收敛于1／e≈0.368，这表明原始样本集D中接近37％的样本不会出现在bootstrap样本中，这些数据称为袋外(Out-Of-Bag，OOB)数据，使用这些数据来估计模型的性能称为OOB估计\n构建决策树\n除了随机抽取样本外，构建一个决策树还需要随机抽取样本的特征，比如样本总共有100维特征，我们随机抽取其中的10维特征构建决策树\n如果我们想构建200棵决策树的随机森林，我们就要这样随机抽取200次，每次抽取10维特征构建一个决策树\n而用于构建决策树的样本也需要用之前说的那种bootstrap法有放回的随机抽取，简单说一下用于构建一个决策树的样本集生成的过程\nbootstrap法抽取样本过程\n随机抽取1个样本，然后让回，然后再随机抽取1个样本，这样抽取N次，可以得到N个样本的数据集，用这N个样本的数据集，按照之前随机选取的10维特征，遍历这10维特征，对数据集进行划分，得到一棵决策树\n如果要构建200棵树，就需要随机抽取200次10维特征，随机抽取200次N个样本集\n随机森林分类\n得到了200棵树的随机森林如何用作分类呢，随机森林中用的OOB数据测试随机森林的分类结果，之前说到bootstrap方法会造成大概三分之一的数据不会被采样的，这部分数据就被称之为OOB数据，将这部分数据放入森林中，每一棵树会对相应的数据得到一个分类结果，那么最后的结果会根据投票来确定\n为什么不用交叉验证的方法二用OOB的方法\n有一个问题就是这种OOB的方法跟交叉验证中随机抽取样本有什么区别，比如十折交叉验证中就是把数据集平均分为10份儿，随机选取其中的9份儿用作训练，1份儿用作测试，重复十次，取平均值，那么OOB这种有放回的重采样，和交叉验证有什么区别呢\n一个很重要的区别根据作者的说法在于计算量，用交叉验证(CV)估计组合分类器的泛化误差时，可能导致很大的计算量，从而降低算法的运行效率，而采用OOB数据估计组合分类器的泛化误差时，可以在构建各决策树的同时计算出OOB误\n差率，最终只需增加少量的计算就可以得到。相对于交叉验证，00B估计是高效的，且其结果近似于交叉验证的结果"}
{"content2":"计划最近好好按步骤按阶段系统性的学习下机器学习和深度学习，希望能坚持下去。\n2019-01-05 基于TensorFlow的深度学习系列教程 2——常量Constant\n2019-01-03 深度学习Tensorflow生产环境部署（下·模型部署篇）\n2019-01-03 深度学习Tensorflow生产环境部署（上·环境准备篇）\n2018-12-23 基于TensorFlow的深度学习系列教程 1——Hello World!\n2018-12-22 想要接触人工智能吗？先要学会如何阅读论文\n2018-08-09 AI之——OCR文字识别快速体验版\n2018-07-03 AI之——路径规划与室内导航\n2018-06-22 AI之——无人驾驶\n2018-07-26 吴恩达机器学习笔记 —— 19 应用举例：照片OCR（光学字符识别）\n2018-08-04 吴恩达机器学习笔记 —— 18 大规模机器学习\n2018-08-01 吴恩达机器学习笔记 —— 17 推荐系统\n2018-07-31 吴恩达机器学习笔记 —— 16 异常点检测\n2018-07-30 吴恩达机器学习笔记 —— 15 降维\n2018-07-25 吴恩达机器学习笔记 —— 14 无监督学习\n2018-08-04 吴恩达机器学习笔记 —— 13 支持向量机\n2018-07-24 吴恩达机器学习笔记 —— 12 机器学习系统设计\n2018-07-23 吴恩达机器学习笔记 —— 11 应用机器学习的建议\n2018-07-22 吴恩达机器学习笔记 —— 10 神经网络参数的反向传播算法\n2018-07-20 吴恩达机器学习笔记 —— 9 神经网络学习\n2018-07-19 吴恩达机器学习笔记 —— 8 正则项\n2018-07-18 吴恩达机器学习笔记 —— 7 Logistic回归\n2018-07-16 吴恩达机器学习笔记 —— 6 Octave/Matlab教程 略\n2018-07-16 吴恩达机器学习笔记 —— 5 多变量线性回归\n2018-07-15 吴恩达机器学习笔记 —— 4 配置 略\n2018-07-15 吴恩达机器学习笔记 —— 3 线性回归回顾\n2018-07-10 吴恩达机器学习笔记 —— 2 单变量线性回归\n2018-07-08 吴恩达机器学习笔记 —— 1 绪论：初识机器学习"}
{"content2":"版权声明：\n本文由LeftNotEasy发布于http://leftnoteasy.cnblogs.com, 本文可以被全部的转载或者部分使用，但请注明出处，如果有问题，请联系wheeleast@gmail.com\n前言：\n第二篇的文章中谈到，和部门老大一宁出去outing的时候，他给了我相当多的机器学习的建议，里面涉及到很多的算法的意义、学习方法等等。一宁上次给我提到，如果学习分类算法，最好从线性的入手，线性分类器最简单的就是LDA，它可以看做是简化版的SVM，如果想理解SVM这种分类器，那理解LDA就是很有必要的了。\n谈到LDA，就不得不谈谈PCA，PCA是一个和LDA非常相关的算法，从推导、求解、到算法最终的结果，都有着相当的相似。\n本次的内容主要是以推导数学公式为主，都是从算法的物理意义出发，然后一步一步最终推导到最终的式子，LDA和PCA最终的表现都是解一个矩阵特征值的问题，但是理解了如何推导，才能更深刻的理解其中的含义。本次内容要求读者有一些基本的线性代数基础，比如说特征值、特征向量的概念，空间投影，点乘等的一些基本知识等。除此之外的其他公式、我都尽量讲得更简单清楚。\nLDA：\nLDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。有些资料上也称为是Fisher’s Linear Discriminant，因为它被Ronald Fisher发明自1936年，Discriminant这次词我个人的理解是，一个模型，不需要去通过概率的方法来训练、预测数据，比如说各种贝叶斯方法，就需要获取数据的先验、后验概率等等。LDA是在目前机器学习、数据挖掘领域经典且热门的一个算法，据我所知，百度的商务搜索部里面就用了不少这方面的算法。\nLDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(Linear Classifier)：因为LDA是一种线性分类器。对于K-分类的一个分类问题，会有K个线性函数：\n当满足条件：对于所有的j，都有Yk > Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的，就是所属的分类了。\n上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：\n红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被原点明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：\n假设用来区分二分类的直线（投影函数)为：\nLDA分类的一个目标是使得不同类别之间的距离越远越好，同一类别之中的距离越近越好，所以我们需要定义几个关键的值。\n类别i的原始中心点为：（Di表示属于类别i的点)\n类别i投影后的中心点为：\n衡量类别i投影后，类别点之间的分散程度（方差）为：\n最终我们可以得到一个下面的公式，表示LDA投影到w后的损失函数：\n我们分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。\n我们定义一个投影前的各类别分散程度的矩阵，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的输入点集Di里面的点距离这个分类的中心店mi越近，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.\n带入Si，将J(w)分母化为：\n同样的将J(w)分子化为：\n这样损失函数可以化成下面的形式：\n这样就可以用最喜欢的拉格朗日乘子法了，但是还有一个问题，如果分子、分母是都可以取任意值的，那就会使得有无穷解，我们将分母限制为长度为1（这是用拉格朗日乘子法一个很重要的技巧，在下面将说的PCA里面也会用到，如果忘记了，请复习一下高数），并作为拉格朗日乘子法的限制条件，带入得到：\n这样的式子就是一个求特征值的问题了。\n对于N(N>2)分类的问题，我就直接写出下面的结论了：\n这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的Wi了。\n这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。\n下图是图像识别中广泛用到的特征脸（eigen face），提取出特征脸有两个目的，首先是为了压缩数据，对于一张图片，只需要保存其最重要的部分就是了，然后是为了使得程序更容易处理，在提取主要特征的时候，很多的噪声都被过滤掉了。跟下面将谈到的PCA的作用非常相关。\n特征值的求法有很多，求一个D * D的矩阵的时间复杂度是O(D^3), 也有一些求Top M的方法，比如说power method，它的时间复杂度是O(D^2 * M), 总体来说，求特征值是一个很费时间的操作，如果是单机环境下，是很局限的。\nPCA：\n主成分分析（PCA）与LDA有着非常近似的意思，LDA的输入数据是带标签的，而PCA的输入数据是不带标签的，所以PCA是一种unsupervised learning。LDA通常来说是作为一个独立的算法存在，给定了训练数据后，将会得到一系列的判别函数（discriminate function），之后对于新的输入，就可以进行预测了。而PCA更像是一个预处理的方法，它可以将原本的数据降低维度，而使得降低了维度的数据之间的方差最大（也可以说投影误差最小，具体在之后的推导里面会谈到）。\n方差这个东西是个很有趣的，有些时候我们会考虑减少方差（比如说训练模型的时候，我们会考虑到方差-偏差的均衡），有的时候我们会尽量的增大方差。方差就像是一种信仰（强哥的话），不一定会有很严密的证明，从实践来说，通过尽量增大投影方差的PCA算法，确实可以提高我们的算法质量。\n说了这么多，推推公式可以帮助我们理解。我下面将用两种思路来推导出一个同样的表达式。首先是最大化投影后的方差，其次是最小化投影后的损失（投影产生的损失最小）。\n最大化方差法：\n假设我们还是将一个空间中的点投影到一个向量中去。首先，给出原空间的中心点：\n假设u1为投影向量，投影之后的方差为：\n上面这个式子如果看懂了之前推导LDA的过程，应该比较容易理解，如果线性代数里面的内容忘记了，可以再温习一下，优化上式等号右边的内容，还是用拉格朗日乘子法：\n将上式求导，使之为0，得到：\n这是一个标准的特征值表达式了，λ对应的特征值，u对应的特征向量。上式的左边取得最大值的条件就是λ1最大，也就是取得最大的特征值的时候。假设我们是要将一个D维的数据空间投影到M维的数据空间中（M < D)， 那我们取前M个特征向量构成的投影矩阵就是能够使得方差最大的矩阵了。\n最小化损失法：\n假设输入数据x是在D维空间中的点，那么，我们可以用D个正交的D维向量去完全的表示这个空间（这个空间中所有的向量都可以用这D个向量的线性组合得到）。在D维空间中，有无穷多种可能找这D个正交的D维向量，哪个组合是最合适的呢？\n假设我们已经找到了这D个向量，可以得到：\n我们可以用近似法来表示投影后的点：\n上式表示，得到的新的x是由前M 个基的线性组合加上后D - M个基的线性组合，注意这里的z是对于每个x都不同的，而b对于每个x是相同的，这样我们就可以用M个数来表示空间中的一个点，也就是使得数据降维了。但是这样降维后的数据，必然会产生一些扭曲，我们用J描述这种扭曲，我们的目标是，使得J最小：\n上式的意思很直观，就是对于每一个点，将降维后的点与原始的点之间的距离的平方和加起来，求平均值，我们就要使得这个平均值最小。我们令：\n将上面得到的z与b带入降维的表达式：\n将上式带入J的表达式得到：\n再用上拉普拉斯乘子法（此处略），可以得到，取得我们想要的投影基的表达式为：\n这里又是一个特征值的表达式，我们想要的前M个向量其实就是这里最大的M个特征值所对应的特征向量。证明这个还可以看看，我们J可以化为：\n也就是当误差J是由最小的D - M个特征值组成的时候，J取得最小值。跟上面的意思相同。\n下图是PCA的投影的一个表示，黑色的点是原始的点，带箭头的虚线是投影的向量，Pc1表示特征值最大的特征向量，pc2表示特征值次大的特征向量，两者是彼此正交的，因为这原本是一个2维的空间，所以最多有两个投影的向量，如果空间维度更高，则投影的向量会更多。\n总结：\n本次主要讲了两种方法，PCA与LDA，两者的思想和计算方法非常类似，但是一个是作为独立的算法存在，另一个更多的用于数据的预处理的工作。另外对于PCA和LDA还有核方法，本次的篇幅比较大了，先不说了，以后有时间再谈：\n参考资料：\nprml bishop，introduce to LDA（对不起，这个真没有查到出处）"}
{"content2":"1024G——前沿技术资料百度云链接：\n人工智能：链接：http://pan.baidu.com/s/1nvk5AaP 密码：dpg5\n深度学习：链接：http://pan.baidu.com/s/1slvSHpN 密码：v3ro\n数据分析：链接：http://pan.baidu.com/s/1slALMPJ 密码：drgs\n算法：链接：http://pan.baidu.com/s/1bpmlTKz 密码：5xh3\n推荐系统：链接：http://pan.baidu.com/s/1i4CNNpz 密码：hyft\n行业报告：链接：http://pan.baidu.com/s/1slJ9r8h 密码：0a0u\n训练数据集：链接:https://pan.baidu.com/s/1dEHGaKL 密码:xjhz\n自然语言处理(NLP)：链接：http://pan.baidu.com/s/1geTMLQb 密码：cf5c\n机器学习：链接：http://pan.baidu.com/s/1nuUq8kh 密码：wk11\nDocker：链接：http://pan.baidu.com/s/1o8LbsOq 密码：er09\nFluent：链接：http://pan.baidu.com/s/1pLIXyC7 密码：ke6n\nHadoop：链接：http://pan.baidu.com/s/1kVO5rlt 密码：v2a7\nHBase：链接：http://pan.baidu.com/s/1nvwaQjn 密码：nwnl\nKafka：链接：http://pan.baidu.com/s/1kUGh4DT 密码：ou16\nMySql：链接：http://pan.baidu.com/s/1o8i1C8M 密码：w2s4\nOpenStack：链接：http://pan.baidu.com/s/1skUOvsp 密码：gz9x\nPython：链接：http://pan.baidu.com/s/1o7IdpZs 密码：v0fj\nRedis：链接：http://pan.baidu.com/s/1i5foaZN 密码：a9jl\nR语言：链接：http://pan.baidu.com/s/1dFql6tb 密码：xw49\nSpark：链接：http://pan.baidu.com/s/1o8sAY9c 密码：unx9\nStorm：链接：http://pan.baidu.com/s/1slKpoRj 密码：0bsd\n峰会资料：链接：http://pan.baidu.com/s/1cby5Ee 密码：nvtg\n众所周知：百度云盘经常抽风，请大家花样打开内容！！遇到连接失效，我们也正在抢修，修好了会再更新不要着急。从一级 菜单进入就存入自己的云盘 不然有可能会出现里面没内容或没有密码的的情况 存入自己的盘之后就好了。"}
{"content2":"本笔记主要记录学习《机器学习》的总结体会。如有理解不到位的地方，欢迎大家指出，我会努力改正。\n在学习《机器学习》时，我主要是通过Andrew Ng教授在mooc上提供的《Machine Learning》课程，不得不说Andrew Ng老师在讲授这门课程时，真的很用心，特别是编程练习，这门课真的很nice，在此谢谢Andrew Ng老师的付出。同时也谢过告知这个平台的小伙伴。本文在写的过程中，多有借鉴Andrew Ng教授在mooc提供的资料，再次感谢。\n转载请注明出处：http://blog.csdn.net/u010278305\n什么是机器学习？我认为机器学习就是，给定一定的信息（如一间房子的面子，一幅图片每个点的像素值等等），通过对这些信息进行“学习”，得出一个“学习模型“，这个模型可以在有该类型的信息输入时，输出我们感兴趣的结果。好比我们如果要进行手写数字的识别，已经给定了一些已知信息（一些图片和这些图片上的手写数字是多少），我们可以按以下步骤进行学习：\n1、将这些图片每个点的像素值与每个图片的手写数字值输入”学习系统“。\n2、通过”学习过程“，我们得到一个”学习模型“，这个模型可以在有新的手写数字的图片输入时，给出这张图片对应手写数字的合理估计。\n什么是线性回归？我的理解就是，用一个线性函数对提供的已知数据进行拟合，最终得到一个线性函数，使这个函数满足我们的要求（如具有最小平方差,随后我们将定义一个代价函数，使这个目标量化），之后我们可以利用这个函数，对给定的输入进行预测（例如，给定房屋面积，我们预测这个房屋的价格）。如下图所示：\n假设我们最终要的得到的假设函数具有如下形式：\n其中，x是我们的输入，theta是我们要求得的参数。\n代价函数如下：\n我们的目标是使得此代价函数具有最小值。\n为此，我们还需要求得代价函数关于参量theta的导数，即梯度，具有如下形式：\n有了这些信息之后，我们就可以用梯度下降算法来求得theta参数。过程如下：\n其实，为了求得theta参数，有更多更好的算法可以选择，我们可以通过调用matlab的fminunc函数实现,而我们只需求出代价与梯度，供该函数调用即可。\n根据以上公式，我们给出代价函数的具体实现：\nfunction J = computeCostMulti(X, y, theta) %COMPUTECOSTMULTI Compute cost for linear regression with multiple variables % J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the % parameter for linear regression to fit the data points in X and y % Initialize some useful values m = length(y); % number of training examples % You need to return the following variables correctly J = 0; % Instructions: Compute the cost of a particular choice of theta % You should set J to the cost. hThetaX=X*theta; J=1/(2*m)*sum((hThetaX-y).^2); end\n什么是逻辑回归？相比于线性回归，逻辑回归只会输出一些离散的特定值（例如判定一封邮件是否为垃圾邮件，输出只有0和1），而且对假设函数进行了处理，使得输出只在0和1之间。\n假设函数如下：\n代价函数如下：\n梯度函数如下，观察可知，形式与线性回归时一样：\n有了这些信息，我们就可以通过fminunc求出最优的theta参数，我们只需给出代价与梯度的计算方式，代码如下：\nfunction [J, grad] = costFunction(theta, X, y) %COSTFUNCTION Compute cost and gradient for logistic regression % J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the % parameter for logistic regression and the gradient of the cost % w.r.t. to the parameters. % Initialize some useful values m = length(y); % number of training examples % You need to return the following variables correctly J = 0; grad = zeros(size(theta)); % Instructions: Compute the cost of a particular choice of theta. % You should set J to the cost. % Compute the partial derivatives and set grad to the partial % derivatives of the cost w.r.t. each parameter in theta % % Note: grad should have the same dimensions as theta % hThetaX=sigmoid(X * theta); J=1/m*sum(-y.*log(hThetaX)-(1-y).*log(1-hThetaX)); grad=(1/m*(hThetaX-y)'*X)'; end\n其中，sigmod函数如下：\nfunction g = sigmoid(z) %SIGMOID Compute sigmoid functoon % J = SIGMOID(z) computes the sigmoid of z. % You need to return the following variables correctly g = zeros(size(z)); % Instructions: Compute the sigmoid of each value of z (z can be a matrix, % vector or scalar). e=exp(1); g=1./(1+e.^-z); end\n有时，会出现”过拟合“的情况，即求得的参数能够很好的拟合训练集中的数据，但在进行预测时，明显与趋势不符，好比下图所示：\n此时，我们需要进行正则化处理，对参数进行惩罚，使得除theta(1)之外的theta值均保持较小值。\n进行正则化之后的代价函数如下：\n进行正则化之后的梯度如下：\n下面给出正则化之后的代价与梯度值得代码：\nfunction [J, grad] = costFunctionReg(theta, X, y, lambda) %COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization % J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using % theta as the parameter for regularized logistic regression and the % gradient of the cost w.r.t. to the parameters. % Initialize some useful values m = length(y); % number of training examples % You need to return the following variables correctly J = 0; grad = zeros(size(theta)); % Instructions: Compute the cost of a particular choice of theta. % You should set J to the cost. % Compute the partial derivatives and set grad to the partial % derivatives of the cost w.r.t. each parameter in theta hThetaX=sigmoid(X * theta); theta(1)=0; J=1/m*sum(-y.*log(hThetaX)-(1-y).*log(1-hThetaX))+lambda/(2*m)*sum(theta.^2); grad=(1/m*(hThetaX-y)'*X)' + lambda/m*theta; end\n对于线性回归，正则化的过程基本类似。\n至于如何选择正则化时的常数lambda，我们可以将数据分为训练集、交叉验证集和测试集三部分，在不同lambda下，先用训练集求出参数theta，之后求出训练集与交叉验证集的代价，通过分析得出适合的lambda。如下图所示：\n转载请注明出处：http://blog.csdn.net/u010278305"}
{"content2":"1. 简介\n决策树(Decision Tree）是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。Entropy = 系统的凌乱程度，使用算法ID3, C4.5和C5.0生成树算法使用熵。这一度量是基于信息学理论中熵的概念。\n决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。\n决策树学习通常包括 3 个步骤：\n特征选择\n决策树的生成\n决策树的修剪\n1.1 决策树场景\n场景一：二十个问题\n有一个叫 “二十个问题” 的游戏，游戏规则很简单：参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。\n场景二：邮件分类\n一个邮件分类系统，大致工作流程如下：\n首先检测发送邮件域名地址。如果地址为 myEmployer.com, 则将其放在分类 \"无聊时需要阅读的邮件\"中。\n如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 \"曲棍球\" , 如果包含则将邮件归类到 \"需要及时处理的朋友邮件\",\n如果不包含则将邮件归类到 \"无需阅读的垃圾邮件\" 。\n1.2 定义\n分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。\n结点有两种类型：\n内部结点（internal node）：表示一个特征或属性。\n叶结点（leaf： node）：表示一个类。\n用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。\n2. 决策树原理\n熵：\n熵（entropy）指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。\n信息熵（香农熵）：\n是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。\n信息增益：\n在划分数据集前后信息发生的变化称为信息增益。\n2.1 工作原理\n我们使用 createBranch() 方法构造一个决策树，如下所示：\n检测数据集中的所有数据的分类标签是否相同: If so return 类标签 Else: 寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征） 划分数据集 创建分支节点 for 每个划分的子集 调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中 return 分支节点\n2.2 决策树开发流程\n1. 收集数据：可以使用任何方法。 2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。 3. 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。 4. 训练算法：构造树的数据结构。 5. 测试算法：使用经验树计算错误率。（经验树没有搜索到较好的资料，有兴趣的同学可以来补充） 6. 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。\n2.3 决策树算法特点\n优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。\n缺点：可能会产生过度匹配问题。\n适用数据类型：数值型和标称型。\n3. 实战案例\n3.1 项目概述\n根据以下 2 个特征，将动物分成两类：鱼类和非鱼类。\n特征：\n不浮出水面是否可以生存\n是否有脚蹼\n3.2 开发流程\n(1) 收集数据\n可以使用任何方法\n我们利用 createDataSet() 函数输入数据：\ndef createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return dataSet, labels\n(2) 准备数据\n树构造算法只适用于标称型数据，因此数值型数据必须离散化\n此处，由于我们输入的数据本身就是离散化数据，所以这一步就省略了。\n（3） 分析数据\n可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期\n计算给定数据集的香农熵的函数\ndef calcShannonEnt(dataSet): # 求list的长度，表示计算参与训练的数据量 numEntries = len(dataSet) # 计算分类标签label出现的次数 labelCounts = {} # the the number of unique elements and their occurance for featVec in dataSet: # 将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签 currentLabel = featVec[-1] # 为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。 if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 # 对于 label 标签的占比，求出 label 标签的香农熵 shannonEnt = 0.0 for key in labelCounts: # 使用所有类标签的发生频率计算类别出现的概率。 prob = float(labelCounts[key])/numEntries # 计算香农熵，以 2 为底求对数 shannonEnt -= prob * log(prob, 2) return shannonEnt\n按照给定特征划分数据集\n将指定特征的特征值等于 value 的行剩下列作为子数据集。\ndef splitDataSet(dataSet, index, value): \"\"\"splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行) 就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中 Args: dataSet 数据集 待划分的数据集 index 表示每一行的index列 划分数据集的特征 value 表示index列对应的value值 需要返回的特征的值。 Returns: index列为value的数据集【该数据集需要排除index列】 \"\"\" retDataSet = [] for featVec in dataSet: # index列为value的数据集【该数据集需要排除index列】 # 判断index列的值是否为value if featVec[index] == value: # chop out index used for splitting # [:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行 reducedFeatVec = featVec[:index] ''' 请百度查询一下： extend和append的区别 list.append(object) 向列表中添加一个对象object list.extend(sequence) 把一个序列seq的内容添加到列表中 1、使用append的时候，是将new_media看作一个对象，整体打包添加到music_media对象中。 2、使用extend的时候，是将new_media看作一个序列，将这个序列和music_media序列合并，并放在其后面。 result = [] result.extend([1,2,3]) print result result.append([4,5,6]) print result result.extend([7,8,9]) print result 结果： [1, 2, 3] [1, 2, 3, [4, 5, 6]] [1, 2, 3, [4, 5, 6], 7, 8, 9] ''' reducedFeatVec.extend(featVec[index+1:]) # [index+1:]表示从跳过 index 的 index+1行，取接下来的数据 # 收集结果值 index列为value的行【该行需要排除index列】 retDataSet.append(reducedFeatVec) return retDataSet\n选择最好的数据集划分方式\ndef chooseBestFeatureToSplit(dataSet): \"\"\"chooseBestFeatureToSplit(选择最好的特征) Args: dataSet 数据集 Returns: bestFeature 最优的特征列 \"\"\" # 求第一行有多少列的 Feature, 最后一列是label列嘛 numFeatures = len(dataSet[0]) - 1 # 数据集的原始信息熵 baseEntropy = calcShannonEnt(dataSet) # 最优的信息增益值, 和最优的Featurn编号 bestInfoGain, bestFeature = 0.0, -1 # iterate over all the features for i in range(numFeatures): # create a list of all the examples of this feature # 获取对应的feature下的所有数据 featList = [example[i] for example in dataSet] # get a set of unique values # 获取剔重后的集合，使用set对list数据进行去重 uniqueVals = set(featList) # 创建一个临时的信息熵 newEntropy = 0.0 # 遍历某一列的value集合，计算该列的信息熵 # 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。 for value in uniqueVals: subDataSet = splitDataSet(dataSet, i, value) # 计算概率 prob = len(subDataSet)/float(len(dataSet)) # 计算信息熵 newEntropy += prob * calcShannonEnt(subDataSet) # gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值 # 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。 infoGain = baseEntropy - newEntropy print 'infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy if (infoGain > bestInfoGain): bestInfoGain = infoGain bestFeature = i return bestFeature\nQ：上面的 newEntropy 为什么是根据子集计算的呢？\nA ：因为我们在根据一个特征计算香农熵的时候，该特征的分类值是相同，这个特征这个分类的香农熵为 0；\n这就是为什么计算新的香农熵的时候使用的是子集。\n（4）训练算法\n构造树的数据结构\n创建树的函数代码如下：\ndef createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行 # 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。 # count() 函数是统计括号中的值在list中出现的次数 if classList.count(classList[0]) == len(classList): return classList[0] # 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果 # 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。 if len(dataSet[0]) == 1: return majorityCnt(classList) # 选择最优的列，得到最优列对应的label含义 bestFeat = chooseBestFeatureToSplit(dataSet) # 获取label的名称 bestFeatLabel = labels[bestFeat] # 初始化myTree myTree = {bestFeatLabel: {}} # 注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改 # 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list del(labels[bestFeat]) # 取出最优列，然后它的branch做分类 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: # 求出剩余的标签label subLabels = labels[:] # 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree() myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # print 'myTree', value, myTree return myTree\n（5）测试算法\n使用决策树执行分类\n代码如下：\ndef classify(inputTree, featLabels, testVec): \"\"\"classify(给输入的节点，进行分类) Args: inputTree 决策树模型 featLabels Feature标签对应的名称 testVec 测试输入的数据 Returns: classLabel 分类的结果值，需要映射label才能知道名称 \"\"\" # 获取tree的根节点对于的key值 firstStr = inputTree.keys()[0] # 通过key得到根节点对应的value secondDict = inputTree[firstStr] # 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类 featIndex = featLabels.index(firstStr) # 测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类 key = testVec[featIndex] valueOfFeat = secondDict[key] print '+++', firstStr, 'xxx', secondDict, '---', key, '>>>', valueOfFeat # 判断分枝是否结束: 判断valueOfFeat是否是dict类型 if isinstance(valueOfFeat, dict): classLabel = classify(valueOfFeat, featLabels, testVec) else: classLabel = valueOfFeat return classLabel\n（6）使用算法\n此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。\n构造决策树是很耗时的任务，即使很小的数据集也要花费几秒。如果用创建好的决策树解决分类问题就可以很快完成。\n因此为了节省计算时间，最好能每次执行分类时调用已经构造好的决策树，为了解决这个问题，需要使用Python模块pickle序列化对象。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。任何对象都可以执行序列化，包括字典对象。\n下面代码是使用pickle模块存储决策树：\ndef storeTree(inputTree, filename): impory pickle fw = open(filename, 'w') pickle.dump(inputTree, fw) fw.close() def grabTree(filename): import pickle fr = open(filename) return pickle.load(fr)\n通过上面的代码我们可以把分类器存储在硬盘上，而不用每次对数据分类时重新学习一遍，这也是决策树的优点之一。++K-近邻算法就无法持久化分类器++。\n[1] 决策树维基百科： https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91\n[2]《机器学习实战》 -- Peter Harrington\n[3]《机器学习》 -- 周志华"}
{"content2":"优达学城（udacity.com）是一所在线的“硅谷大学”，由来自硅谷的技术领袖设计行业前沿技术课程，让每个人都能在线学习世界最高水平的IT课程。优达学城和Google、Facebook、亚马逊等企业合作推出的“纳米学位”认证项目，通过实战和一对一辅导， 将学员培养为世界一流的网站开发者、数据科学家和移动开发者，在当今社会中获得理想工作。全球已有来自168个国家的400万人从优达学城的课程中受益。\n热门“纳米学位”项目推荐：\nGoogle：机器学习工程师 - 成为人工智能、数据挖掘、无人驾驶领域的机器学习专家\nFacebook：数据分析师 - 掌握数据秘密的人，就掌握了科技和商业的未来\nGoogle：前端工程师 - 和Google 技术行家，学习如何打造令人惊艳的网站页面\nGoogle：Android 开发者 - 学习最权威的Google 官方Android 开发课程\niOS 应用开发入门 - 想成为影响数亿用户的iOS 开发者？从这里开始第一步\n无人驾驶车开发 - 全球第一门无人驾驶车开发的在线课程！成为引领未来的抢手人才\n优惠券使用说明：\n在优达学城（cn.udacity.com）上选择任何一门感兴趣的纳米学位课程项目，开始免费试用；\n7天免费试用结束后，在“我的教室—>设置—>纳米学位—>续费”页面上的优惠码区域，输入CnBlogs，立即享受300元的博客园专属学费优惠；\n使用时遇到任何问题，欢迎发邮件给 support@youdaxue.com 或联系优达学习助手（ID：youdaxuexi）。"}
{"content2":"人工智能现在很火，虽然最近风头隐隐有被区块链盖过，但仍是未来技术转型的首选方向之一。作为AI核心的机器学习，目前也进化到了可以基于平台自动训练模型的地步，例如Azure Machine Learning Service和Google AutoML Service。这使得训练模型的难度大大降低，开发人员可以分出更多精力关注在训练好的模型应用上。\n在这种背景下，各个操作系统平台纷纷推出内置的机器学习框架/运行环境，iOS有CoreML，Android有TensorFlow。Windows在最近的RS4（build 1803）更新之后，也正式内置了机器学习平台- Windows ML。\nWindows ML是什么？\nWindows ML是Windows全新的内置机器学习平台，用于本机执行预训练的机器学习模型，并提供了API允许我们快速集成到应用中。\n它的亮点如下：\n支持硬件加速\n在兼容DirectX 12的设备上可以直接使用GPU加速运算，确保机器学习模型可以被高效执行。\n本机执行\n不依赖于任何远程服务，不受任何网络连接限制，本机即可达到低延迟高性能的执行效果。\n图像处理优化\n针对计算机视觉场景，对视频、图像和相机数据统一预处理为VideoFrame形式，简化图像处理流程。\n模型要求\nWindows ML目前仅支持执行ONNX格式模型，其他格式需要预先转换后再使用。\nONNX是由微软、Facebook和英特尔等公司推出的一个通用开放的机器学习模型格式，官方支持现有机器学习框架对其转换。ONNX项目地址\n支持转换的现有模型来源：\nCore ML\nScikit-Learn\nXGBoost\nLibSVM\n使用的转换工具为微软提供的WinMLTools：https://pypi.org/project/winmltools/\n转换工具使用教程请参考官方文档：https://docs.microsoft.com/en-us/windows/uwp/machine-learning/conversion-samples\n代码生成\n在安装了Windows SDK Build 17110或更新版本后，默认会为Visual Studio 2017项目添加模型代码生成工具mlgen.exe。它可以根据添加的ONNX模型文件，Visual Studio 2017 Preview自动生成C#/CX的定义文件，方便代码直接调用。\n这里以FNS-La-Muse模型为例，这是一个可以将图像转为特定风格的模型。\n生成的代码如下：\nusing System; using System.Collections.Generic; using System.Threading.Tasks; using Windows.Media; using Windows.Storage; using Windows.AI.MachineLearning.Preview; // FNSLaMuse namespace Demo { public sealed class FNSLaMuseModelInput { public VideoFrame inputImage { get; set; } } public sealed class FNSLaMuseModelOutput { public VideoFrame outputImage { get; set; } public FNSLaMuseModelOutput() { this.outputImage = VideoFrame.CreateWithSoftwareBitmap(new Windows.Graphics.Imaging.SoftwareBitmap(Windows.Graphics.Imaging.BitmapPixelFormat.Bgra8, 720, 720)); } } public sealed class FNSLaMuseModel { private LearningModelPreview learningModel; public static async Task<FNSLaMuseModel> CreateFNSLaMuseModel(StorageFile file) { LearningModelPreview learningModel = await LearningModelPreview.LoadModelFromStorageFileAsync(file); FNSLaMuseModel model = new FNSLaMuseModel(); model.learningModel = learningModel; return model; } public async Task<FNSLaMuseModelOutput> EvaluateAsync(FNSLaMuseModelInput input) { FNSLaMuseModelOutput output = new FNSLaMuseModelOutput(); LearningModelBindingPreview binding = new LearningModelBindingPreview(learningModel); binding.Bind(\"inputImage\", input.inputImage); binding.Bind(\"outputImage\", output.outputImage); LearningModelEvaluationResultPreview evalResult = await learningModel.EvaluateAsync(binding, string.Empty); return output; } } }\nView Code\n目前由于SDK仍在预览中，所以Visual Studio正式版并不会自动调用mlgen工具生成定义文件，需要手动执行如下命令：\nmlgen -i INPUT-FILE -l LANGUAGE -n NAMESPACE [-o OUTPUT-FILE]\nINPUT-FILE: ONNX模型文件\nLANGUAGE: C++或者C#\nNAMESPACE: 命名空间\nOUTPUT-FILE: 输出路径，可缺省\n总结\n有了Windows ML后我们可以实现以前难以实现的机器学习特性，同时不用依赖外部web service，很多创新的体验可以实现，不仅仅是在PC，甚至在HoloLens上同样可以运用机器学习的能力。\n最后给大家安利下我的开源项目- Awesome WindowsML ONNX Models ，这个项目除了提供我已经验证过的模型外，还提供了CoreML模型的快速转换工具。\n同时我也在开发为HoloLens编写的Demo,最近将会和大家见面"}
{"content2":"本列表总结了25个Java机器学习工具&库：\n1. Weka集成了数据挖掘工作的机器学习算法。这些算法可以直接应用于一个数据集上或者你可以自己编写代码来调用。Weka包括一系列的工具，如数据预处理、分类、回归、聚类、关联规则以及可视化。\n2.Massive Online Analysis（MOA）是一个面向数据流挖掘的流行开源框架，有着非常活跃的成长社区。它包括一系列的机器学习算法（分类、回归、聚类、异常检测、概念漂移检测和推荐系统）和评估工具。关联了WEKA项目，MOA也是用Java编写的，其扩展性更强。\n3.MEKA项目提供了一个面向多标签学习和评价方法的开源实现。在多标签分类中，我们要预测每个输入实例的多个输出变量。这与“普通”情况下只涉及一个单一目标变量的情形不同。此外，MEKA基于WEKA的机器学习工具包。\n4. Advanced Data mining And Machine learning System（ADAMS）是一种新型的柔性工作流引擎，旨在迅速建立并保持真实世界的复杂知识流，它是基于GPLv3发行的。\n5. Environment for Developing KDD-Applications Supported by Index-Structure（ELKI）是一款基于Java的开源（AGPLv3）数据挖掘软件。ELKI主要集中于算法研究，重点研究聚类分析中的无监督方法和异常检测。\n6. Mallet是一个基于Java的面向文本文件的机器学习工具包。Mallet支持分类算法，如最大熵、朴素贝叶斯和决策树分类。\n7. Encog是一个先进的机器学习框架，集成了支持向量机（SVM）、人工神经网络、遗传算法、贝叶斯网络、隐马尔可夫模型（HMM）、遗传编程和遗传算法。\n8. Datumbox机器学习框架是一个用Java编写的开源框架，允许快速地开发机器学习和统计应用。该框架的核心重点包括大量的机器学习算法以及统计测试，能够处理中等规模的数据集。\n9. Deeplearning4j是使用Java和Scala编写的第一个商业级的、开源的、分布式深入学习库。其设计的目的是用于商业环境中，而不是作为一个研究工具。\n10. Mahout是一个内置算法的机器学习框架。Mahout-Samsara帮助人们创建他们自己的数学，并提供了一些现成的算法实现。\n11.Rapid Miner是德国多特蒙特技术大学开发的。它为开发者开发应用程序提供了一个GUI（图形用户界面）和Java API。它还提供了一些机器学习算法，用来做数据处理、可视化以及建模。\n12. Apache SAMOA是一个机器学习（ML）框架，内嵌面向分布式流ML算法的编程抽象，并且允许在没有直接处理底层分布式流处理引擎（DSPEe，如Apache Storm、Apache S4和Apache samza）复杂性的情况下，开发新的ML算法。用户可以开发分布式流ML算法，而且可以在多个DSPEs上执行。\n13. Neuroph通过提供支持创建、训练和保存神经网络的Java网络库和GUI工具，简化了神经网络开发。\n14. Oryx 2是一个建立在Apache Spark和Apache Kafka的Lambda架构实现，但随着实时大规模机器学习而逐渐开始专业化。这是一个用于构建应用程序的框架，但也包括打包，以及面向协同过滤、分类、回归和聚类的端到端的应用程序。\n15. Stanford Classifier是一个机器学习工具，它可以将数据项归置到一个类别。一个概率分类器，比如这个，它可以对一个数据项给出类分配的概率分布。该软件是最大熵分类器的一个Java实现。\n16.io是一个Retina API，有着快速精确的类似大脑的自然语言处理算法。\n17.JSAT是一个快速入门的机器学习库。该库是我在业余时间开发的，基于GPL3发行的。库中的一部分内容可自主学习，例如所有的代码都是独立的。JSAT没有外部依赖，而且是纯Java编写的。\n18. N-Dimensional Arrays for Java(ND4J)是一个用于JVM的科学计算库。它们是用来在生产环境中使用的，这表明例程的设计是以最小的内存需求来运行的。\n19. Java Machine Learning Library（Java机器学习库）是一系列机器学习算法的相关实现。这些算法，无论是源代码还是文档，都编写的很出色。其主要语言是Java。\n20. Java-ML是一个使用Java编写的一系列机器学习算法的Java API。它只提供了一个标准的算法接口。\n21. MLlib (Spark)是Apache Spark的可扩展机器学习库。虽然是Java，但该库与平台还支持Java，Scala和Python绑定。此库是最新的，并且算法很多。\n22. H2O是用于智能应用的机器学习API。它在大数据上对统计学、机器学习和数学进行了规模化。H2O可扩展，开发者可以在核心部分使用简单的数学知识。\n23. WalnutiQ是人脑部分面向对象模型，有着理论常用的学习算法（正在向简单强烈的情感人工智能模型方向研究）。\n24. RankLib是一个排名学习算法库。目前已经实现八种流行的算法。\n25. htm.java（基于Java的Hierarchical Temporal Memory算法实现）是一个面向智能计算的Numenta平台的Java接口。源码"}
{"content2":"考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。\nTP：正确肯定的数目；\nFN：漏报，没有正确找到的匹配的数目；\nFP：误报，给出的匹配是不正确的；\nTN：正确拒绝的非匹配对数；\n列联表如下表所示，1代表正类，0代表负类：\n预测1\n预测0\n实际1\nTrue Positive(TP)\nFalse Negative(FN)\n实际0\nFalse Positive(FP)\nTrue Negative(TN)\n1. TPR、FPR&TNR\n从列联表引入两个新名词。其一是真正类率(true positive rate ,TPR), 计算公式为\nTPR = TP / (TP + FN)\n刻画的是分类器所识别出的 正实例占所有正实例的比例。\n另外一个是负正类率(false positive rate, FPR),计算公式为\nFPR = FP / (FP + TN)\n计算的是分类器错认为正类的负实例占所有负实例的比例。\n还有一个真负类率（True Negative Rate，TNR），也称为specificity，计算公式为\nTNR = TN /(FP + TN) = 1 - FPR\n2. 精确率Precision、召回率Recall和F1值\n精确率（正确率）和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。\n一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了，两者的定义分别如下：\nPrecision = 提取出的正确信息条数 /  提取出的信息条数\nRecall = 提取出的正确信息条数 /  样本中的信息条数\n为了能够评价不同算法的优劣，在Precision和Recall的基础上提出了F1值的概念，来对Precision和Recall进行整体评价。F1的定义如下：\nF1值  = 正确率 * 召回率 * 2 / (正确率 + 召回率)\n不妨举这样一个例子：\n某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了700条鲤鱼，200只虾，100只鳖。那么，这些指标分别如下：\n正确率 = 700 / (700 + 200 + 100) = 70%\n召回率 = 700 / 1400 = 50%\nF1值 = 70% * 50% * 2 / (70% + 50%) = 58.3%\n不妨看看如果把池子里的所有的鲤鱼、虾和鳖都一网打尽，这些指标又有何变化：\n正确率 = 1400 / (1400 + 300 + 300) = 70%\n召回率 = 1400 / 1400 = 100%\nF1值 = 70% * 100% * 2 / (70% + 100%) = 82.35%\n由此可见，正确率是评估捕获的成果中目标成果所占得比例；召回率，顾名思义，就是从关注领域中，召回目标类别的比例；而F值，则是综合这二者指标的评估指标，用于综合反映整体的指标。\n当然希望检索结果Precision越高越好，同时Recall也越高越好，但事实上这两者在某些情况下有矛盾的。比如极端情况下，我们只搜索出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就会很低。因此在不同的场合中需要自己判断希望Precision比较高或是Recall比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。\n3. 综合评价指标F-measure\nPrecision和Recall指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。\nF-Measure是Precision和Recall加权调和平均：\n当参数α=1时，就是最常见的F1。因此，F1综合了P和R的结果，当F1较高时则能说明试验方法比较有效。\n4. ROC曲线和AUC\n4.1 为什么引入ROC曲线？\nMotivation1：在一个二分类模型中，对于所得到的连续结果，假设已确定一个阀值，比如说 0.6，大于这个值的实例划归为正类，小于这个值则划到负类中。如果减小阀值，减到0.5，固然能识别出更多的正类，也就是提高了识别出的正例占所有正例 的比类，即TPR,但同时也将更多的负实例当作了正实例，即提高了FPR。为了形象化这一变化，引入ROC，ROC曲线可以用于评价一个分类器。\nMotivation2：在类不平衡的情况下,如正样本90个,负样本10个,直接把所有样本分类为正样本,得到识别率为90%。但这显然是没有意义的。单纯根据Precision和Recall来衡量算法的优劣已经不能表征这种病态问题。\n4.2 什么是ROC曲线？\nROC（Receiver Operating Characteristic）翻译为\"接受者操作特性曲线\"。曲线由两个变量1-specificity 和 Sensitivity绘制. 1-specificity=FPR，即负正类率。Sensitivity即是真正类率，TPR(True positive rate),反映了正类覆盖程度。这个组合以1-specificity对sensitivity,即是以代价(costs)对收益(benefits)。\n此外，ROC曲线还可以用来计算“均值平均精度”（mean average precision），这是当你通过改变阈值来选择最好的结果时所得到的平均精度（PPV）。\n为了更好地理解ROC曲线，我们使用具体的实例来说明：\n如在医学诊断中,判断有病的样本。那么尽量把有病的揪出来是主要任务,也就是第一个指标TPR,要越高越好。而把没病的样本误诊为有病的,也就是第二个指标FPR,要越低越好。\n不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感,稍微的小症状都判断为有病,那么他的第一个指标应该会很高,但是第二个指标也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么第一个指标达到1,第二个指标也为1。\n我们以FPR为横轴,TPR为纵轴,得到如下ROC空间。\n我们可以看出,左上角的点(TPR=1,FPR=0),为完美分类,也就是这个医生医术高明,诊断全对。点A(TPR>FPR),医生A的判断大体是正确的。中线上的点B(TPR=FPR),也就是医生B全都是蒙的,蒙对一半,蒙错一半;下半平面的点C(TPR<FPR),这个医生说你有病,那么你很可能没有病,医生C的话我们要反着听,为真庸医。上图中一个阈值,得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何,也就是遍历所有的阈值,得到ROC曲线。\n还是一开始的那幅图,假设如下就是某个医生的诊断统计图,直线代表阈值。我们遍历所有的阈值,能够在ROC平面上得到如下的ROC曲线。\n曲线距离左上角越近,证明分类器效果越好。\n如上,是三条ROC曲线,在0.23处取一条直线。那么,在同样的低FPR=0.23的情况下,红色分类器得到更高的PTR。也就表明,ROC越往上,分类器效果越好。我们用一个标量值AUC来量化它。\n4.3 什么是AUC？\nAUC值为ROC曲线所覆盖的区域面积,显然,AUC越大,分类器分类效果越好。\nAUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。\nAUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。\nAUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。\nAUC的物理意义：假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。\n4.4 怎样计算AUC？\n第一种方法:AUC为ROC曲线下的面积,那我们直接计算面积可得。面积为一个个小的梯形面积之和。计算的精度与阈值的精度有关。\n第二种方法:根据AUC的物理意义,我们计算正样本score大于负样本的score的概率。取N*M(N为正样本数,M为负样本数)个二元组,比较score,最后得到AUC。时间复杂度为O(N*M)。\n第三种方法:与第二种方法相似,直接计算正样本score大于负样本的概率。我们首先把所有样本按照score排序,依次用rank表示他们,如最大score的样本,rank=n(n=N+M),其次为n-1。那么对于正样本中rank最大的样本,rank_max,有M-1个其他正样本比他score小,那么就有(rank_max-1)-(M-1)个负样本比他score小。其次为(rank_second-1)-(M-2)。最后我们得到正样本大于负样本的概率为\n时间复杂度为O(N+M)。\n5. 参考内容\n1. 机器学习指标大汇总：http://www.36dsj.com/archives/42271"}
{"content2":"摘要：使用logistic回归来预测某个人的入学申请是否会被接受\n声明：（本文的内容非原创，但经过本人翻译和总结而来，转载请注明出处）\n本文内容来源：https://www.dataquest.io/mission/59/logistic-regression\n原始数据展示\n这是一份美国入学申请的录取记录表，admit – 是否录取，1代表录取，0代表否定；gpa – gpa成绩，gre – 绩点\nimport pandas admissions = pandas.read_csv('admissions.csv')\n在之前已经介绍过了线性回归，现在同样使用线性回归来进行预测\nfrom sklearn.linear_model import LinearRegression model = LinearRegression() #训练模型 model.fit(admissions[['gre', 'gpa']], admissions[\"admit\"]) admit_prediction = model.predict(admissions[['gre', 'gpa']]) plt.xlabel('gpa') plt.ylabel('admit_prediction') plt.scatter(admissions[\"gpa\"], admit_prediction) plt.show()\n在上图中可见，有些预测结果小于0，而这明显是不对的，因为预测结果应该只能为0或者1，我们现在需要获取一个介于0和1之间的概率，然后通过之前的文章中介绍过的分类算法（机器学习简易入门（二）- 分类）来确定录取一个人的概率的阀值来决定录取结果，最终生成只有0和1的结果\nlogistic回归函数\nlogistic回归产生的输出都位于0和1之间，通常用来产生预测某个事件的发生概率，该函数的格式为，其中的e是一个无理数常量，该函数有一个很漂亮的形状\n# logistic回归函数 def logit(x): return np.exp(x) / (1 + np.exp(x)) # 在-6到6之间等差产生50个数 t = np.linspace(-6,6,50, dtype=float) ylogit = logit(t) #作图 plt.plot(t, ylogit, label=\"logistic\") plt.ylabel(\"Probability\") plt.xlabel(\"t\") plt.title(\"Logistic Function\") plt.show()\nlogistic回归\n在线性回归方程中，可以将该方程产生的结果y放入到logistic回归方程，从而将线性方程产生的结果转换为一个概率，对于本文来说，这个logistic回归方程为，现在根据这个logistic回归方程就能产生一个录取概率。\n类似于之前使用scikit-learn库中的线性回归，现在也可以直接使用该库中的logistic回归\nfrom sklearn.linear_model import LogisticRegression #对数据集进行随机重排序 admissions = admissions.loc[np.random.permutation(admissions.index)] # 将随机排序后的前700条数据作为训练集，后面的作为测试集 num_train = 700 data_train = admissions[:num_train] data_test = admissions[num_train:] logistic_model = LogisticRegression() logistic_model.fit(data_train[['gpa', 'gre']], data_train['admit']) # 进行测试 fitted_test = logistic_model.predict_proba(data_test[['gpa', 'gre']])[:, 1] #因为predict_proba返回的是一个两列的矩阵，矩阵的每一行代表的是对一个事件的预测结果，第一列代表该事件不会发生的概率，第二列代表的是该事件会发生的概率。而这里需要的是第二列的数据 plt.scatter(data_test['gre'], fitted_test) plt.xlabel('gre') plt.ylabel('probability ') plt.show()\n评估模型\n准确率\n现在假设只要录取概率大于0.5的就能录取，计算一下这个模型的准确性\n# predict()函数会自动把阀值设置为0.5 predicted = logistic_model.predict(data_train[['gpa','gre']]) # 计算在训练集中正确预测的准确率 accuracy_train = (predicted == data_train['admit']).mean() #计算在测试集中正确预测的准确率 predicted = logistic_model.predict(data_test[['gpa','gre']]) accuracy_test = (predicted == data_test['admit']).mean()\nROC曲线\n分别计算训练集和测试集的ROC曲线和AUC\nfrom sklearn.metrics import roc_curve, roc_auc_score train_probs = logistic_model.predict_proba(data_train[['gpa', 'gre']])[:,1] test_probs = logistic_model.predict_proba(data_test[['gpa', 'gre']])[:,1] #计算AUC auc_train = roc_auc_score(data_train[\"admit\"], train_probs) auc_test = roc_auc_score(data_test[\"admit\"], test_probs) print('Auc_train: {}'.format(auc_train)) print('Auc_test: {}'.format(auc_test)) # 计算ROC曲线 roc_train = roc_curve(data_train[\"admit\"], train_probs) roc_test = roc_curve(data_test[\"admit\"], test_probs) # 作图 plt.plot(roc_train[0], roc_train[1]) plt.plot(roc_test[0], roc_test[1])"}
{"content2":"《机器学习》课程使用Kevin P. Murphy图书《Machine Learning A Probabilistic Perspective》本英语教材，本书从一个独特的数学概率论的角度解释机器学习的所有问题，要较强的数学基础。由于是英文教材。特开一个专题在此记录自己的学习过程和各种问题。以供备忘和举一反三之用。\n在解说了机器学习的概述之后。第二章紧接着就開始讲述概率论的知识，通过兴许的学习会发现，这些概率论知识有部分在本科的概率论课程中学习过，可是有非常多其它部分是没有在现有的本科阶段甚至研究生阶段也非常少涉及的知识点。在此做一个总结。\n1、概率学派\n频率学派：概率代表的是对一个试验反复运行N次。所关注的事件发生的频率。这里要求的是须要进行反复试验，这对于一般可反复运行的试验是比較好的标识方式。这也成为实验概率。\n贝叶斯学派：概率代表的是人们对一个未知事件发生的不确定性的一种表征，这里不要求对这个事件进行反复试验。同一时候对于不论什么未知的事件，都能够用一个概率来表征人们对它的认识。\n通过上述比較能够发现，对于某些不能反复试验的事件（比方生成灯管的工厂生成的灯管的平均使用寿命，进行反复实验是不现实的）。使用贝叶斯概率的解释更加合理。因此在整个学习中都以贝叶斯学派为准。\n2、基本知识\n概率：事件空间Ω到实数域R的映射，对于每一个事件A，都有一个实数p（A）与之相应，同一时候满足：（1）非负性。p（A）>=0。（2）规范性，p（Ω）=1；（3）可列可加性：p(A1+A2+…An) = p(A1)+p(A2)+…p(An)当中A1、A2…An都是互补相容的事件。\n基本概率公式：\n全概率公式和贝叶斯公式：\n通用的贝叶斯分类器：\n（θ为模型的參数）\n3、离散型分布\n（1）二项分布Binomial\nK为每次试验可能出现的结果，n为进行试验的次数。贝努利试验就是K={0。1}且n=1的试验，对于n(n>1)的n重贝努利实验就是二项分布，分布函数例如以下：\nmean=θ，variance=nθ(1-θ)。\n二项分布描写叙述的典型试验就是抛硬币，每次出现正面或者反面两种结果。\n这在机器学习的分类算法中用于描写叙述二值的特征。也就是每一个数据的特征的取值是两个状态(通常是0和1)，用来表征当前数据是否有这个特征，因此能够使用二项分布来描写叙述当前特征的分布。\n（2）多项分布Multinormial\n当每次试验出现的结果可能有K（K>2）种时，也就是一个特征的不不过表征是否出现，而是须要用一个详细数值来表征该特征的影响大小。此时能够用多项分布进行描写叙述。\n此处。当K=2时也就是两种状态，能够看出多项分布就退化到了二项分布，能够看出x1=k,x2=n-k，x1+x2=n条件满足。\n当中，当n=1时。也就是仅仅进行一次试验，此时的分布称为多维贝努利分布，由于每次的可能状态有K（K>2）个，也成为离散分布（discrete distribution）或者分类分布（categorical distribution）。记为Cat(x|θ)：\n（3）泊松分布Poisson\n变量X={0,1,2.....}，λ>0，分布例如以下：\n泊松分布能够用来模拟以时间序列发送的事件，具有无记忆性。\n4、连续型分布\n（1）正态分布Gaussian（Normal）\nmean=u。mode=u，variance=σ^2。在统计学中应用很广泛，首先两个參数很好理解。各自是均值和标准差。同一时候，中心极限定理得到相互独立的随机变量的和的分布近似为高斯分布，能够用来模拟噪声数据；第三。高斯分布使用了最小的如果也就是拥有最大熵。第四，数学形式相对简单，很利于实现。\n（2）Student t分布\nmean=u。mode=u，variance=νσ^2/(ν-2)。ν>0为自由度，方差在ν>2时有定义。均值在ν>1时有定义。此分布形式上与高斯分布类似，弥补了高斯分布的一个不足，就是高斯分布对离群的数据非常敏感，可是Student t分布更鲁棒。\n一般设置ν=4，在大多数实际问题中都有非常好的性能，当ν大于等于5时将会是去鲁棒性，同一时候会迅速收敛到高斯分布。\n特别的。当ν=1时。被称为柯西分布（Cauchy）。\n（3）拉普拉斯分布Laplace\nmean=u，mode=u，variance=2b^2。\n也被称为双側指数分布，引出了绝对值的指数次方，因此在x=u处不可导。b（b>0）为缩放因子，用来调节数据的分散程度。拉普拉斯分布对离群数据的鲁棒性更好。同一时候，在x=u处给予了比高斯分布更大的概率密度，这个性质能够用来修正模型中稀疏的数据。\n（4）Gamma分布\nmean=a / b，mode=(a-1) / b，variance=a / b^2，mean在a>1时有定义。variance在a>2时有定义。当中变量T的范围为T>0。a>0称为形状參数，b>0称为速率參数。\nExponential分布：a=1，b=λ时，Expon(x|λ)=Ga(x|1,λ)，这个分布描写叙述了连续的泊松过程，与离散型的泊松分布共轭。\nErLang分布：ErLang(x|λ)=Ga(x|2,λ)\nChi-Squared分布（卡方分布）：ChiSq(x|v)=Ga(x|v/2,1/2)，这是N个高斯分布的随机变量的平方和所服从的分布。\n当使用1/x取代Gamma分布中的变量时。得到的是反Gamma分布。即：\nmean=b / (a-1)。mode=b / (a+1)，variance=b^2 / (a-1)^2(a-2)，当中mean在a>1时定义。variance在a>2时定义。\n（5）Beta分布\n定义在[0,1]区间上。要求a>0,b>0，当a=b=1时就是[0,1]上的均匀分布。mean=a / (a+b), mode=(a-1) / (a+b-2), variance = ab / (a+b)^2(a+b+1)。这个分布与离散的二项分布是共轭的。在朴素贝叶斯分类应用中，当似然分布为二项分布时，选择Beta分布为共轭先验分布，则后验分布也为Beta分布。很便于实际操作和计算。\n（6）Pareto分布\nmean=km/(k-1)（k>1）。mode=m。variance=mk^2 / (k-1)^2(k-2)（k>2），这个分布相应有一个Zipf's 定律，用来描写叙述单词的排名和其出现的频率的关系。x必须比一个常数m要大，可是不能超过k，当k为无穷大时，这个分布会趋于δ(x-m)。上述分布在信息检索中对索引构建中的词频预计非常有效。\n（7）狄利克雷分布Dirichlet\nmean（Xk）=ak/a0, mode(Xk) = (ak - 1) / (a0 - K), variance(Xk) = ak(a0-ak) / a0^2(a0+1)。这是beta分布在多维条件下的分布。相应的參数和变量都是一个向量，这个分布与离散的多项分布时共轭的，在朴素贝叶斯分类应用中，似然使用多项分布时。选择Dirichlet分布为先验分布，得到后验分布也为Dirichlet分布。\n以上对机器学习中使用做一个概率分布汇总，也许在时间的学习笔记和复习。\n版权声明：本文博主原创文章，博客，未经同意不得转载。"}
{"content2":"松尾丰，作者简介：东京大学院工学系研究科副教授，1997年毕业于东京大学工学部电子信息工学科。2002年完成了该大学的博士课程，成为工程博士。同年任产业技术综合研究所研究员。2005年起任斯坦福大学客座研究员。2007起至今任工学系研究科副教授，兼任新加坡国立大学客座副教授。专业领域为人工智能、网络信息挖掘、大数据分析。日本人工智能专家之一，曾获人工智能学会颁发“论文奖”（2002年）、“创立20周年纪念事业奖”（2006年）、“现场创新奖”（2011年）、“功劳奖”（2013年）等奖项。先后在人工智能学会任多职；2012年起任编辑委员长、理事；2014年任伦理委员长。编著有《大智能时代套装》（机器人的未来、机器人新时代、机器人革命、数字法则、大智能时代）。\n书中对人工智能的三次人工智能的浪潮进行阐述，对三次人工智能浪潮的主要技术进行了介绍，还有各大科技公司面对人工智能浪潮采取的应对措施。当第三次人工智能浪潮来临时，我们的生活会变成什么样，如果都想人工智能专家想像中的那样，90%的事情都可以交给带有人工智能机器去完成，那我们人类剩余出来的时间又该做什么？以及针对人工智能是否会有情感，带有情感的人工智能是否意味着人类的灭亡，作者都进行了详细的分析。\n首先我们要摆平我们的心态：人工智能并未实现，但是没理由不实现！探索人类智能的原理，并通过工程学的方法对其进行实现和利用，这样的人工智能还没有实现。人类对于物理世界的研究从微观的原子到浩瀚的宇宙都有的较为本质上的认识，大型的强子碰撞机，宇宙飞船都是对物理世界认识的产物。然而，人类的大脑能力深奥无比、遥不可及，科学家对其探索的脚步从未停止，然而利用计算机对其进行的模型也未能实现。\n人工智能是什么？一下是专家给出的定义；\n1. 人工智能是“采用人工方法制作的、具有智能的实体，或者是以创造智能为目的的、对智能本身进项研究的领域”。\n2. 把类似我们很自然地接触宠物或者其他人的那种充满感情和幽默的相互作用，在与物理定律无关或者相逆的条件下，用人工方法制造出来的系统，定义为“人工智能”，这种系统采用的不是分析性的理解方法，而是通过对话等交流方式进行的交谈性理解。这就是人工智能。\n3. 以模仿、支持、超越人类的智能行为为目的的建构性(通过制作来进行理解)系统。与建构性对应的词是分析性，举个例子，从事体育运动的是运动员从事的是建构性理解，而体育评论家则是分析性理解。\n4. 采用人工方法制造的类人智能，以及其制造技术，类人指的是具有“发现和察觉功能”的计算机，即能够从数据中生成特征量。\n对于我们非人工智能研究着而言，人工智能分为四个级别：\n1.把单纯的控制程序称作“人工智能”，比如：空调，全自动洗衣机等；\n2.传统人工智能(引入了推理及搜索，或者知识库)，比如可以下棋的程序，智力问题求解等；\n3.引入机器学习的人工智能，机器学习以样本数据为基础、对规则和知识的自学习；\n·4.引入深度学习的人工智能，能够对机器学习时的数据表示所用变量(特征量)本身进行学习的人工智能。\n强人工智能：具备正确的输入与输出、被施与合理程序化的计算机，与拥有心智的人是没有任何区别的，即它也是有心智的。\n弱人工智能：计算机没有必要拥有心智，只要能够通过其有限的智能解决一些智力问题即可。\n第一次人工智能浪潮：\n时间：20世纪50年代------20世纪60年代\n概括：第一次人工智能浪潮是推理和搜索的时代\n代表事件：\n1.用搜索树搜索迷宫\n方法：搜索树\n宽度优先搜索，能够找到距离目标最短的路径，但是需要的存储量大；\n深度优先搜索，需要的存储量小，但是搜索时间不定，可能会很小，也可能会很大；\n2. 梵塔问题\n方法：搜索树\n3.机器人行动过程规划\n方法：搜索树\n4.博弈(棋类游戏)\n棋类游戏的组合是非常大的，对目前的计算机来说，如果采用直接搜索的方法无疑是很难满足需求的。那现在的计算机可以战胜人类的秘诀又是在哪里呢？\n1.能够发现更好的特征量\n2.“蒙特卡洛法”改变评估机制\n第二次人工智能浪潮：\n时间：20世纪80年代------1995年左右\n概括：第一次人工智能浪潮是知识(“专家系统”)的时代\n“专家系统”本身是一种程序，通过引入某个专业领域的知识，在经过推理，计算机便能够像该领域的专家一样出色地开展工作。\n什么是“知识表示”？\n对于我们每个人都熟知的知识，怎样表达才能让计算机易于处理？在这方面的基础性研究，被称为“知识表示”研究。\n本体研究？\n“本体“相当于撰写知识时的规格说明书。本体研究分为“重量级本体”和“轻量级本体”两个派别，重量级本体的支持者认为研究者需要认真考虑该怎么描述知识，并研究为此应该怎么做；轻量级本体的支持者认为，把数据输进计算机里面，并让计算机自己寻找概念之间的相关性。轻量级本体的一个极致例子就是由IBM开发的“沃森”。\n作者在此提到了“机器翻译”、“框架问题”和“符号接地问题”三个问题。利用导入知识的人工智能进行机器翻译，但是导入知识的机器翻译尽管可以较好地理解语言的语法，但是，精确地从语法分析往往会产生语义上的歧义，而语义的理解正是机器翻译的难点所在。“框架问题”，就是在执行某项任务时“仅仅提取出与它相关的知识并对其加以运用”，这对人类来讲很简单，但是对机器来说非常困难。“符号问题”，是否能将符号（词句、语言 ）与它表示的意义连接起来的问题，计算机以为不懂得符号的意义，所以不能把符号与其所表示的意义结合起来。\n第三次人工智能浪潮：\n时间：2000年以后至今\n概括：第一次人工智能浪潮是机器学习与特征表示学习的时代\n什么是机器学习？机器学习指人工智能程序自身进行自身学习的机理。那怎样才算是学到东西呢？学习的主要工作是进行“区分”，对某一事物进行判断和识别，就可以理解它，还能根据对该事物的判断而采取相应的行动。机器学习分为“有监督学习”和“无监督学习”，有监督学习，指的是事先需要准备好输入与正确输出想配套的训练数据，让计算机进行学习，以便当它被输入某个数据时能够得到正确的输出；无监督学习，指仅提供输入用数据、需要计算机自己找出数据内在结构的场合，目的是让计算机从数据中抽取出其中所包含的模式及规则。\n常用的五种“分类”方法：\n1.最近邻分类算法\n2.朴素贝叶斯算法\n3.决策树\n4.支持向量机\n5.人工神经网络\n机器学习的难点（弱点）是特征工程，即特征量的设计。计算机并不能做出选取特征量的判断。提高机器学习的精确度的关键在于“输入何种特征量”，然而这只有靠人用大脑思维来解决。到目前为止人工智能之所以尚未实现，就是因为人工智能在“从这个世界里面应该关注何种特征并提取信息”这点上，还必须借助人的力量。如果计算机能够从被导入的数据里面找出应该关注的特征，并得到表示这种特征程度的特征量，那么机器学习的“特征量设计”问题也将被解决。深度学习，恰好可以解决这个问题。\n深度学习\n深度学习以数据为基础，由计算机自动生成特征量，它不需要由人来设计特征量，而是由计算机自动获取高层特征量。\n自动编码器：输入与输出相同\n深度学习与之前的机器学习相比有两个较大的不同点：一是需要一层一层地逐层学习；二是深度学习使用一种被称为“自动编码器”的“信息压缩器”。自动编码器所执行的处理与众不同，它将“输出”和“输入”做成相同的数据，与“主成分分析”具有同样的工作原理，但是自动编码可以进行“深层”即多层次操作，可以提取出主成分分析无法提取出的高层特征量。下图为深度学习结构，\n从数据里面找出并生成概念，本身是不需要“教师数据”的无监督学习，深度学习在进行无监督学习的时候采用的是有监督学习的方法。自动编码器，在本来应该有教师提供正解的地方输入原来的数据，以此对输入数据本身进行预测，再生成各种各样的特征量，这就是通过有监督学习的方式进行无监督学习。Google的“猫脸识别”研究，处理1000万张图像，使用的神经元之间的链接超过100亿个的巨型神经网络，用1000台计算机（16000个处理器）连续运行3天，就是通过“采用有监督的学习方法实现无监督学习”生成特征量，即提取出“猫脸”的概念，此时，再赋予“猫脸”的名称，即完成了符号（名称和概念的结合）接地的问题，在最后区分的时候采用有监督的学习的分类方法。\n深度学习的关键------“鲁棒性”\n实际上，提取特征量或者概念需要相当长时间的“打造和提炼”过程，只有这样，才能使所获取的特征量或者概念具有鲁棒性（“健壮性”）。如何做到深度神经网络的“鲁棒性”呢？其实是需要在输入型号里面加入“噪声”，通过反复加入噪声后获取的概念，就不会因为一点风吹草动就摇摆不定。(听起来有些矛盾，但又何尝不是这样呢)。鲁邦性的提高与计算机的处理性能有较大的关系。\n增加鲁棒性的方法\n1.加入噪声制作“略微不同的过去”的做法；\n2.dropout方法，让神经网络的一部分神经元停止工作，即让隐层50%的神经元出现任意性缺损。对特征项目进行最优化处理，以便让某个特征量能够覆盖其他特征量，这样，特征表示就不会出现过度依赖某一个特征量的情况。过度依赖仅有的某一特征量是非常危险的，让一部分特征量不能使用，对于发现恰当的特征表示是很有帮助的。\n除此之外，还有很多专家在研究各种各样的针对神经网络鲁棒性的方法，因为如果不使劲“折磨”它，就无法获取存在于数据背后的“本质特征量”。\n深度学习之后的技术发展\n1.能够对图像特征进行抽象化处理的人工智能，能够实现多模态抽象化的人工智能，图像处理相当于人类的视觉，还有听觉、触觉等信息待处理；\n2.能够对行动与结果进行抽象化的人工智能，目前人工智能还是停留在对外界事物进行观察的地步，如何与外界进行交互也是未来的研究方向；\n3.能够通过行动获取特征量的人工智能，通过与外界的交互作用获取新的特征量，类似于，人们根据多次的实验突然间意识到的某个特征量或者窍门，下次遇到同样的事情就会想到这个窍门；\n4.能够进行语言理解和自动翻译的人工智能，解决符号落地问题；\n5.能够获取知识的人工智能，使人工智能具有想像力；\n讨论了人工智能的技术问题以及未来的发展，就得说说人工智能的社会性问题了。\n人工智能是否具有本能？\n人工智能是否具有创造力？\n人工智能的社会性意义？人类具有群居性的动物，那人工之能呢？\n奇点会发生吗？奇点，指的是人工智能能够自动地制造出超越自身能力的人工智能的那个时点。\n如果人工智能妄想征服分类，有哪些方式？\n对于这些问题作者也进行了论述，总结一句话，人工智能必须造福于人类。\n人工智能在对人类生产生活的影响：\n1.广告、图像诊断、网络企业；\n2.个人机器人、安全防范、大数据运用企业；\n3.汽车制造、交通、物流、农业；\n4.家政、医疗护理、接待及呼叫中心；\n5.翻译及全球化；\n6.教学、秘书、白领工作辅助；"}
{"content2":"一. 简介\n首先来看百度百科对最小二乘法的介绍：最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。\n简而言之，最小二乘法同梯度下降类似，都是一种求解无约束最优化问题的常用方法，并且也可以用于曲线拟合，来解决回归问题。最小二乘法实质就是最小化“均方误差”，而均方误差就是残差平方和的1/m(m为样本数)，同时均方误差也是回归任务中最常用的性能度量。\n二. 对于一元线性模型\n如果以最简单的一元线性模型来解释最小二乘法。回归分析中，如果只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。对于二维空间线性是一条直线；对于三维空间线性是一个平面，对于多维空间线性是一个超平面...\n对于一元线性回归模型, 假设从总体中获取了m组观察值（X1，Y1），（X2，Y2）， …，（Xm，Ym）。对于平面中的这m个点，可以使用无数条曲线来拟合。要求样本回归函数尽可能好地拟合这组值。综合起来看，这条直线处于样本数据的中心位置最合理。 选择最佳拟合曲线的标准可以确定为：使总的拟合误差（即总残差）达到最小。有以下三个标准可以选择：\n（1）用“残差和最小”确定直线位置是一个途径。但可能会出现计算“残差和”存在相互抵消的问题。\n（2）用“残差绝对值和最小”确定直线位置也是一个途径。但绝对值的计算比较麻烦。\n（3）最小二乘法的原则是以“残差平方和最小”确定直线位置。用最小二乘法除了计算比较方便外，得到的估计量还具有优良特性。这种方法对异常值非常敏感。\n最常用的是普通最小二乘法（ Ordinary  Least Square，OLS）：所选择的回归模型应该使所有观察值的残差平方和达到最小。\n在讲最小二乘的详情之前，首先明确两点：1.我们假设在测量系统中不存在有系统误差，只存在有纯偶然误差。比如体重计或者身高计本身有问题，测量出来的数据都偏大或者都偏小，这种误差是绝对不存在的。（或者说这不能叫误差，这叫错误）2.误差是符合正态分布的，因此最后误差的均值为0（这一点很重要) 。\n明确了上面两点以后，重点来了：为了计算β0,β1的值，我们采取如下规则：β0,β1应该使计算出来的函数曲线与观察值的差的平方和最小。用数学公式描述就是：\n其中，yie表示根据y=β0+β1x估算出来的值，yi是观察得到的真实值。\n为什么要用残差的平方和最小？用差的绝对值不行么？\n以下是一个相对靠谱的解释：\n我们假设直线对于坐标 Xi 给出的预测 f(Xi) 是最靠谱的预测，所有纵坐标偏离 f(Xi) 的那些数据点都含有噪音，是噪音使得它们偏离了完美的一条直线，一个合理的假设就是偏离路线越远的概率越小，具体小多少，可以用一个正态分布曲线来模拟，这个分布曲线以直线对 Xi 给出的预测 f(Xi) 为中心，实际纵坐标为 Yi 的点 (Xi, Yi) 发生的概率就正比于 EXP[-(ΔYi)^2]。（EXP(..) 代表以常数 e 为底的多少次方）。\n所以我们在前面的两点里提到，假设误差的分布要为一个正态分布，原因就在这里了。\n另外说一点我自己的理解：从数学处理的角度来说，绝对值的数学处理过程，比平方和的处理要复杂很多。搞过机器学习的同学都知道，L1正则就是绝对值的方式，而L2正则是平方和的形式。L1能产生稀疏的特征，这对大规模的机器学习灰常灰常重要。但是L1的求解过程，实在是太过蛋疼。所以即使L1能产生稀疏特征，不到万不得已，我们也还是宁可用L2正则，因为L2正则计算起来方便得多。。。\n明确了前面的cost function以后，后面的优化求解过程反倒变得s容易了。\n样本的回归模型很容易得出：\n现在需要确定β0、β1，使cost function最小。学过高数的同志们都清楚，求导就OK。对于这种形式的函数求导，根据数学知识我们知道，函数的极值点为偏导为0的点。\n将这两个方程稍微整理一下，使用克莱姆法则，很容易求解得出：\n这就是最小二乘法的解法，就是求得平方损失函数的极值点。需要注意的一点是β0是常数项对应的系数，此处相当于添加了一个特征值x0且x0恒为1，也就是目标函数中的β0可以看成β0x0,这样的话就不同单独考虑常数项了(在后面的多元线性模型就用到了该性质)。\n三. 对于多元线性模型\n如果我们推广到更一般的情况，假如有更多的模型变量x1,x2,⋯,xn，可以用线性函数表示如下：\n对于m个样本来说，可以用如下线性方程组表示：\n如果将样本矩阵xij记为矩阵A,将参数矩阵记为向量β，真实值记为向量Y，上述线性方程组可以表示为：\n对于最小二乘来说，最终的矩阵表达形式可以表示为：\n其中m≥n,由于考虑到了常数项，故属性值个数由n变为n+1。\n关于这个方程的解法，具体如下：\n其中倒数第二行中的中间两项为标量，所以二者相等。然后利用该式对向量β求导：\n(1)\n由矩阵的求导法则：\n可知(1)式的结果为：\n令上式结果等于0可得：\n(2)\n上式就是最小二乘法的解析解，它是一个全局最优解。\n四. 其他一些想法\n1. 最小二乘法和梯度下降\n乍一看看β的最终结果，感觉很面熟，仔细一看，这不就是NG的ML课程中所讲到的正规方程嘛！实际上，NG所说的的正规方程的解法就是最小二乘法求解析解的解法。\n(1)最小二乘法和梯度下降法在线性回归问题中的目标函数是一样的(或者说本质相同)，都是通过最小化均方误差来构建拟合曲线。\n(2)二者的不同点可见下图(正规方程就是最小二乘法)：\n需要注意的一点是最小二乘法只适用于线性模型(这里一般指线性回归)；而梯度下降适用性极强，一般而言，只要是凸函数，都可以通过梯度下降法得到全局最优值(对于非凸函数，能够得到局部最优解)。\n梯度下降法只要保证目标函数存在一阶连续偏导，就可以使用。\n2.最小二乘法的一些限制和解决方法：\n我们由第三部分(2)式可知道，要保证最小二乘法有解，就得保证ATA是一个可逆阵(非奇异矩阵)；那如果ATA不可逆怎么办？什么情况下ATA不可逆？\n关于ATA在什么情况下不可逆：\n(1)当样本的数量小于参数向量(即β)的维度时，此时ATA一定是不可逆的。例如：你有1000个特征，但你的样本数目小于1000的话，那么构造出的ATA就是不可逆的。\n(2)在所有特征中若存在一个特征与另一个特征线性相关或一个特征与若干个特征线性相关时，此时ATA也是不可逆的。为什么呢？\n具体来说假设,A是m*n维的矩阵，若存在线性相关的特征，则R(A)<n,R(AT)<n,R(ATA)<n,所以ATA不可逆。\n如果ATA不可逆，应该怎样解决？\n(1)筛选出线性无关的特征，不保留相同的特征，保证不存在线性相关的特征。\n(2)增加样本量。\n(3)采用正则化的方法。对于正则化的方法，常见的是L1正则项和L2正则项，L1项有助于从很多特征中筛选出重要的特征，而使得不重要的特征为0(所以L1正则项是个不错的特征选择方法)；如果采用L2正则项的话，实际上解析解就变成了如下的形式：\nλ即正则参数(是一种超参数)后面的矩阵为(n+1)*(n+1)维，如果不考虑常数项的话，就是一个单位阵；此时括号中的矩阵一定是可逆的。\n3.最小二乘法的改进\n最小二乘法由于是最小化均方差，所以它考虑了每个样本的贡献，也就是每个样本具有相同的权重；由于它采用距离作为度量，使得他对噪声比较敏感(最小二乘法假设噪声服从高斯分布)，即使得他它对异常点比较敏感。因此，人们提出了加权最小二乘法，\n相当于给每个样本设置了一个权重，以此来反应样本的重要程度或者对解的影响程度。\n参考：NG《机器学习》\n《矩阵分析与应用》\nhttp://www.cnblogs.com/iamccme/archive/2013/05/15/3080737.html\nhttp://blog.csdn.net/bitcarmanlee/article/details/51589143"}
{"content2":"python3验证码机器学习\n文档结构为\n-- iconset -- ... -- jpg -- captcha.gif -- py -- crack.py\n需要的库\npip3 install pillow or easy_install Pillow\n必须文件下载地址\npython3验证码机器学习\n1.读取图片，打印图片的结构直方图\n# !/usr/bin/python3.4 # -*- coding: utf-8 -*- # From:https://zhuanlan.zhihu.com/p/24222942 # 该知乎栏目为py2编写，这里改造成py3 im = Image.open(\"../jpg/captcha.gif\") his = im.histogram()\n打印结果为\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 3, 1, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 3, 2, 132, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 15, 0, 1, 0, 1, 0, 0, 8, 1, 0, 0, 0, 0, 1, 6, 0, 2, 0, 0, 0, 0, 18, 1, 1, 1, 1, 1, 2, 365, 115, 0, 1, 0, 0, 0, 135, 186, 0, 0, 1, 0, 0, 0, 116, 3, 0, 0, 0, 0, 0, 21, 1, 1, 0, 0, 0, 2, 10, 2, 0, 0, 0, 0, 2, 10, 0, 0, 0, 0, 1, 0, 625]\n该数组长度为255，每一个元素代表（0-255）颜色的多少，例如最后一个元素为625，即255（代表的是白色）最多，组合在一起\nvalues = {} for i in range(0, 256): values[i] = his[i] # 排序，x:x[1]是按照括号内第二个字段进行排序,x:x[0]是按照第一个字段 temp = sorted(values.items(), key=lambda x: x[1], reverse=True) # print(temp)\n打印结果为\n[(255, 625), (212, 365), (220, 186), (219, 135), (169, 132), (227, 116), (213, 115), (234, 21), (205, 18), (184, 15), (241, 10), (248, 10), (191, 8), (198, 6), (155, 3), (157, 3), (158, 3), (167, 3), (228, 3), (56, 2), (67, 2), (91, 2), (96, 2), (109, 2), (122, 2), (127, 2), (134, 2), (140, 2), (168, 2), (176, 2), (200, 2), (211, 2), (240, 2), (242, 2), (247, 2), (43, 1), (44, 1), (53, 1), (61, 1), (68, 1), (79, 1), (84, 1), (92, 1), (101, 1), (103, 1), (104, 1), (107, 1), (121, 1), (126, 1), (129, 1), (132, 1), (137, 1), (149, 1), (151, 1), (153, 1), (156, 1), (165, 1), (170, 1), (171, 1), (175, 1), (186, 1), (188, 1), (192, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (215, 1), (223, 1), (235, 1), (236, 1), (253, 1), (0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0), (16, 0), (17, 0), (18, 0), (19, 0), (20, 0), (21, 0), (22, 0), (23, 0), (24, 0), (25, 0), (26, 0), (27, 0), (28, 0), (29, 0), (30, 0), (31, 0), (32, 0), (33, 0), (34, 0), (35, 0), (36, 0), (37, 0), (38, 0), (39, 0), (40, 0), (41, 0), (42, 0), (45, 0), (46, 0), (47, 0), (48, 0), (49, 0), (50, 0), (51, 0), (52, 0), (54, 0), (55, 0), (57, 0), (58, 0), (59, 0), (60, 0), (62, 0), (63, 0), (64, 0), (65, 0), (66, 0), (69, 0), (70, 0), (71, 0), (72, 0), (73, 0), (74, 0), (75, 0), (76, 0), (77, 0), (78, 0), (80, 0), (81, 0), (82, 0), (83, 0), (85, 0), (86, 0), (87, 0), (88, 0), (89, 0), (90, 0), (93, 0), (94, 0), (95, 0), (97, 0), (98, 0), (99, 0), (100, 0), (102, 0), (105, 0), (106, 0), (108, 0), (110, 0), (111, 0), (112, 0), (113, 0), (114, 0), (115, 0), (116, 0), (117, 0), (118, 0), (119, 0), (120, 0), (123, 0), (124, 0), (125, 0), (128, 0), (130, 0), (131, 0), (133, 0), (135, 0), (136, 0), (138, 0), (139, 0), (141, 0), (142, 0), (143, 0), (144, 0), (145, 0), (146, 0), (147, 0), (148, 0), (150, 0), (152, 0), (154, 0), (159, 0), (160, 0), (161, 0), (162, 0), (163, 0), (164, 0), (166, 0), (172, 0), (173, 0), (174, 0), (177, 0), (178, 0), (179, 0), (180, 0), (181, 0), (182, 0), (183, 0), (185, 0), (187, 0), (189, 0), (190, 0), (193, 0), (194, 0), (195, 0), (196, 0), (199, 0), (201, 0), (202, 0), (203, 0), (204, 0), (214, 0), (216, 0), (217, 0), (218, 0), (221, 0), (222, 0), (224, 0), (225, 0), (226, 0), (229, 0), (230, 0), (231, 0), (232, 0), (233, 0), (237, 0), (238, 0), (239, 0), (243, 0), (244, 0), (245, 0), (246, 0), (249, 0), (250, 0), (251, 0), (252, 0), (254, 0)]\n将占比最多的10个颜色筛选出来\n# 占比最多的10种颜色 # for j, k in temp[:10]: # print(j, k) # 255 625 # 212 365 # 220 186 # 219 135 # 169 132 # 227 116 # 213 115 # 234 21 # 205 18 # 184 15\n2.构造新的无杂质图片\n生成一张白底啥都没有的图片\n# 获取图片大小，生成一张白底255的图片 im2 = Image.new(\"P\", im.size, 255) # print(im2.size[1]) # (84, 22)\n原作者自己观察得到代表数字的颜色为220灰色和227红色\n将这些颜色根据宽和高的坐标以此写入新生成的白底照片中\n# (84, 22)=(宽,高)=(size[0],size[1]) # 获得y坐标 for y in range(im.size[1]): # 获得y坐标 for x in range(im.size[0]): # 获得坐标(x,y)的RGB值 pix = im.getpixel((x, y)) # 这些是要得到的数字 # 220灰色，227红色 if pix == 220 or pix == 227: # 将黑色0填充到im2中 im2.putpixel((x, y), 0) # 生成了一张黑白二值照片 # im2.show()\n黑白二值照片\n3.切割图片\nx代表图片的宽，y代表图片的高\n对图片进行纵向切割\n# 纵向切割 # 找到切割的起始和结束的横坐标 inletter = False foundletter = False start = 0 end = 0 letters = [] for x in range(im2.size[0]): for y in range(im2.size[1]): pix = im2.getpixel((x, y)) if pix != 255: inletter = True if foundletter == False and inletter == True: foundletter = True start = x if foundletter == True and inletter == False: foundletter = False end = x letters.append((start, end)) inletter = False\n打印结果为\n# [(6, 14), (15, 25), (27, 35), (37, 46), (48, 56), (57, 67)]\n(6, 14)代表从x=6到x=14纵向切割成一条状\n保存字段到本地观察，这一步没有什么用，只是保存下来看看而已\n# 保存切割下来的字段 import time count = 0 for letter in letters: # (切割的起始横坐标，起始纵坐标，切割的宽度，切割的高度) im3 = im2.crop((letter[0], 0, letter[1], im2.size[1])) # 更改成用时间命名 # im3.save(\"../jpg/%s.gif\" % (time.strftime('%Y%m%d%H%M%S', time.localtime()))) count += 1 # 可以看到保存下来的6个字段\n字段样式\n4.训练识别\n使用的是 AI与向量空间图像识别\n将标准图片转换成向量坐标a，需要识别的图片字段为向量坐标b，cos(a,b)值越大说明夹角越小，越接近重合\n空间两向量计算公式\n编写的夹角公式为\n# 夹角公式 import math class VectorCompare: # 计算矢量大小 # 计算平方和 def magnitude(self, concordance): total = 0 # concordance.iteritems:报错'dict' object has no attribute 'iteritems' # concordance.items() for word, count in concordance.items(): total += count ** 2 return math.sqrt(total) # 计算矢量之间的 cos 值 def relation(self, concordance1, concordance2): topvalue = 0 # concordance1.iteritems:报错'dict' object has no attribute 'iteritems' # concordance1.items() for word, count in concordance1.items(): # if concordance2.has_key(word):报错'dict' object has no attribute 'has_key' # 改成word in concordance2 if word in concordance2: # 计算相乘的和 topvalue += count * concordance2[word] return topvalue / (self.magnitude(concordance1) * self.magnitude(concordance2))\n转换验证码图片为向量：\n# 将图片转换为矢量 def buildvector(im): d1 = {} count = 0 for i in im.getdata(): d1[count] = i count += 1 return d1\n打印结果\n{0: 255, 1: 255, 2: 255, 3: 255, 4: 255, 5: 255, 6: 255, 7: 255, 8: 255, 9: 255, 10: 255, 11: 255, 12: 255, 13: 255, 14: 255, 15: 255, 16: 255, 17: 255, 18: 255, 19: 255, 20: 255, 21: 255, 22: 255, 23: 255, 24: 255, 25: 255, 26: 255, 27: 255, 28: 255, 29: 255, 30: 255, 31: 255, 32: 255, 33: 255, 34: 255, 35: 255, 36: 255, 37: 255, 38: 255, 39: 255, 40: 255, 41: 255, 42: 255, 43: 255, 44: 255, 45: 255, 46: 255, 47: 255, 48: 255, 49: 255, 50: 255, 51: 255, 52: 255, 53: 255, 54: 255, 55: 255, 56: 255, 57: 255, 58: 255, 59: 255, 60: 255, 61: 255, 62: 255, 63: 255, 64: 255, 65: 255, 66: 255, 67: 0, 68: 0, 69: 0, 70: 255, 71: 255, 72: 255, 73: 255, 74: 0, 75: 0, 76: 0, 77: 255, 78: 0, 79: 255, 80: 255, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 255, 88: 255, 89: 0, 90: 255, 91: 255, 92: 255, 93: 0, 94: 0, 95: 255, 96: 0, 97: 255, 98: 0, 99: 255, 100: 255, 101: 0, 102: 0, 103: 0, 104: 0, 105: 0, 106: 0, 107: 255, 108: 255, 109: 0, 110: 0, 111: 0, 112: 0, 113: 0, 114: 255, 115: 255, 116: 255, 117: 0, 118: 0, 119: 0, 120: 255, 121: 0, 122: 255, 123: 255, 124: 255, 125: 0, 126: 0, 127: 0, 128: 255, 129: 0, 130: 0, 131: 255, 132: 255, 133: 0, 134: 0, 135: 0, 136: 255, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 255, 144: 255, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 0, 151: 255, 152: 255, 153: 255, 154: 255, 155: 0, 156: 0, 157: 0, 158: 255, 159: 255, 160: 255, 161: 255, 162: 255, 163: 255, 164: 255, 165: 255, 166: 255, 167: 255, 168: 255, 169: 255, 170: 255, 171: 255, 172: 255, 173: 255, 174: 255, 175: 255}\n加载训练集，且把训练集也变成向量\nv = VectorCompare() iconset = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] import os imageset = [] for letter in iconset: for img in os.listdir('../iconset/%s/' % (letter)): temp = [] if img != \"Thumbs.db\" and img != \".DS_Store\": temp.append(buildvector(Image.open(\"../iconset/%s/%s\" % (letter, img)))) imageset.append({letter: temp})\n** 开始识别验证码 **\n# 开始破解训练 count = 0 for letter in letters: # (切割的起始横坐标，起始纵坐标，切割的宽度，切割的高度) im3 = im2.crop((letter[0], 0, letter[1], im2.size[1])) guess = [] # 将切割得到的验证码小片段与每个训练片段进行比较 for image in imageset: # image.iteritems:报错'dict' object has no attribute 'iteritems' # 改成image.items() for x, y in image.items(): if len(y) != 0: guess.append((v.relation(y[0], buildvector(im3)), x))\n其中\ny[0]为训练集里面的字母图片，即正确的图片-打印{0: 255, 1: 255, 2: 255, 3: 255, 4: 255, 5: 255, 6: 255, 7: 255, 8: 255, 9: 255, 10: 255, 11: 255, 12: 255, 13: 255, 14: 255, 15: 255, 16: 255, 17: 255, 18: 255, 19: 255, 20: 255, 21: 255, 22: 255, 23: 255, 24: 255, 25: 255, 26: 255, 27: 255, 28: 255, 29: 255, 30: 255, 31: 255, 32: 255, 33: 255, 34: 255, 35: 255, 36: 255, 37: 255, 38: 255, 39: 255, 40: 255, 41: 255, 42: 255, 43: 255, 44: 255, 45: 255, 46: 255, 47: 255, 48: 255, 49: 255, 50: 255, 51: 255, 52: 255, 53: 255, 54: 255, 55: 255, 56: 255, 57: 255, 58: 255, 59: 255, 60: 255, 61: 255, 62: 255, 63: 255, 64: 255, 65: 255, 66: 255, 67: 0, 68: 0, 69: 0, 70: 255, 71: 255, 72: 255, 73: 255, 74: 0, 75: 0, 76: 0, 77: 255, 78: 0, 79: 255, 80: 255, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 255, 88: 255, 89: 0, 90: 255, 91: 255, 92: 255, 93: 0, 94: 0, 95: 255, 96: 0, 97: 255, 98: 0, 99: 255, 100: 255, 101: 0, 102: 0, 103: 0, 104: 0, 105: 0, 106: 0, 107: 255, 108: 255, 109: 0, 110: 0, 111: 0, 112: 0, 113: 0, 114: 255, 115: 255, 116: 255, 117: 0, 118: 0, 119: 0, 120: 255, 121: 0, 122: 255, 123: 255, 124: 255, 125: 0, 126: 0, 127: 0, 128: 255, 129: 0, 130: 0, 131: 255, 132: 255, 133: 0, 134: 0, 135: 0, 136: 255, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 255, 144: 255, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 0, 151: 255, 152: 255, 153: 255, 154: 255, 155: 0, 156: 0, 157: 0, 158: 255, 159: 255, 160: 255, 161: 255, 162: 255, 163: 255, 164: 255, 165: 255, 166: 255, 167: 255, 168: 255, 169: 255, 170: 255, 171: 255, 172: 255, 173: 255, 174: 255, 175: 255}\nbuildvector(im3))为切割出来的字母切片，用来和y[0]进行夹角比对-打印{0: 255, 1: 255, 2: 255, 3: 255, 4: 255, 5: 255, 6: 255, 7: 255, 8: 255, 9: 255, 10: 255, 11: 255, 12: 255, 13: 255, 14: 255, 15: 255, 16: 255, 17: 255, 18: 255, 19: 255, 20: 255, 21: 255, 22: 255, 23: 255, 24: 255, 25: 255, 26: 255, 27: 255, 28: 255, 29: 255, 30: 255, 31: 255, 32: 255, 33: 255, 34: 255, 35: 255, 36: 255, 37: 255, 38: 255, 39: 255, 40: 255, 41: 255, 42: 255, 43: 255, 44: 255, 45: 255, 46: 255, 47: 255, 48: 255, 49: 255, 50: 255, 51: 255, 52: 255, 53: 255, 54: 255, 55: 255, 56: 255, 57: 255, 58: 255, 59: 255, 60: 255, 61: 255, 62: 255, 63: 255, 64: 255, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 255, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 255, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 255, 89: 255, 90: 255, 91: 255, 92: 255, 93: 255, 94: 0, 95: 255, 96: 255, 97: 255, 98: 255, 99: 255, 100: 255, 101: 0, 102: 0, 103: 255, 104: 255, 105: 255, 106: 255, 107: 255, 108: 255, 109: 255, 110: 0, 111: 255, 112: 255, 113: 255, 114: 255, 115: 255, 116: 0, 117: 0, 118: 255, 119: 255, 120: 255, 121: 255, 122: 255, 123: 255, 124: 0, 125: 0, 126: 255, 127: 255, 128: 255, 129: 255, 130: 255, 131: 0, 132: 0, 133: 0, 134: 255, 135: 255, 136: 255, 137: 255, 138: 255, 139: 0, 140: 0, 141: 255, 142: 255, 143: 255, 144: 255, 145: 255, 146: 0, 147: 0, 148: 0, 149: 255, 150: 255, 151: 255, 152: 255, 153: 255, 154: 255, 155: 255, 156: 0, 157: 255, 158: 255, 159: 255, 160: 255, 161: 255, 162: 255, 163: 255, 164: 255, 165: 255, 166: 255, 167: 255, 168: 255, 169: 255, 170: 255, 171: 255, 172: 255, 173: 255, 174: 255, 175: 255}\nx为iconset-x打印依次显示为0，1，2，3，。。。，x,y,z\n排序选出夹角最小的（即cos值最大）的向量，夹角越小则越接近重合，匹配越接近\nguess.sort(reverse=True) print(\"\", guess[0]) count += 1\n运行结果\n(0.9637681159420289, '7') (0.96234028545977, 's') (0.9286884286888929, '9') (0.9835037060984447, 't') (0.9675116507250627, '9') (0.9698971168877263, 'j')\n完整源码在TTyb"}
{"content2":"TopLanguage(\nhttps://groups.google.com/group/pongba/\n)\n（PS：在找相关书籍时，看到这篇blog，想留下来方便以后查看，所以全盘转过来了，未知会作者，抱歉，如有侵权，请告之）\n我经常在 TopLanguage 讨论组上推荐一些书籍，也经常问里面的牛人们搜罗一些有关的资料，人工智能、机器学习、自然语言处理、知识发现（特别地，数据挖掘）、信息检索 这些无疑是 CS 领域最好玩的分支了（也是互相紧密联系的），这里将最近有关机器学习和人工智能相关的一些学习资源归一个类：\n首先是两个非常棒的 Wikipedia 条目，我也算是 wikipedia 的重度用户了，学习一门东西的时候常常发现是始于 wikipedia 中间经过若干次 google ，然后止于某一本或几本著作。\n第一个是“\n人工智能的历史\n”（History of Artificial Intelligence），我在讨论组上写道：\n而今天看到的这篇文章是我在 wikipedia 浏览至今觉得最好的。文章名为《人工智能的历史》，顺着 AI 发展时间线娓娓道来，中间穿插无数牛人故事，且一波三折大气磅礴，可谓\"事实比想象更令人惊讶\"。人工智能始于哲学思辨，中间经历了一个没有心理学（尤其是认知神经科学的）的帮助的阶段，仅通过牛人对人类思维的外在表现的归纳、内省，以及数学工具进行探索，其间最令人激动的是 Herbert Simon （决策理论之父，诺奖，跨领域牛人）写的一个自动证明机，证明了罗素的数学原理中的二十几个定理，其中有一个定理比原书中的还要优雅，Simon 的程序用的是启发式搜索，因为公理系统中的证明可以简化为从条件到结论的树状搜索（但由于组合爆炸，所以必须使用启发式剪枝）。后来 Simon 又写了 GPS （General Problem Solver），据说能解决一些能良好形式化的问题，如汉诺塔。但说到底 Simon 的研究毕竟只触及了人类思维的一个很小很小的方面 —— Formal Logic，甚至更狭义一点 Deductive Reasoning （即不包含 Inductive Reasoning , Transductive Reasoning (俗称 analogic thinking）。还有诸多比如 Common Sense、Vision、尤其是最为复杂的 Language 、Consciousness 都还谜团未解。还有一个比较有趣的就是有人认为 AI 问题必须要以一个物理的 Body 为支撑，一个能够感受这个世界的物理规则的身体本身就是一个强大的信息来源，基于这个信息来源，人类能够自身与时俱进地总结所谓的 Common-Sense Knowledge （这个就是所谓的 Emboddied  Mind 理论。 ），否则像一些老兄直接手动构建 Common-Sense Knowledge Base ，就很傻很天真了，须知人根据感知系统从自然界获取知识是一个动态的自动更新的系统，而手动构建常识库则无异于古老的 Expert System 的做法。当然，以上只总结了很小一部分我个人觉得比较有趣或新颖的，每个人看到的有趣的地方不一样，比如里面相当详细地介绍了神经网络理论的兴衰。所以我强烈建议你看自己一遍，别忘了里面链接到其他地方的链接。\n顺便一说，\n徐宥\n同学打算找时间把这个条目翻译出来，这是一个相当长的条目，看不动 E 文的等着看翻译吧:)\n第二个则是“\n人工智能\n”（Artificial Intelligence）。当然，还有\n机器学习\n等等。从这些条目出发能够找到许多非常有用和靠谱的深入参考资料。\n然后是一些书籍\n书籍：\n1. 《Programming Collective Intelligence》，近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的:P\n2. Peter Norvig 的《AI, Modern Approach 2nd》（无争议的领域经典）。\n3. 《The Elements of Statistical Learning》，数学性比较强，可以做参考了。\n4. 《Foundations of Statistical Natural Language Processing》，自然语言处理领域公认经典。\n5. 《Data Mining, Concepts and Techniques》，华裔科学家写的书，相当深入浅出。\n6. 《Managing Gigabytes》，信息检索好书。\n7. 《Information Theory：Inference and Learning Algorithms》，参考书吧，比较深。\n相关数学基础（参考书，不适合拿来通读）：\n1. 线性代数：这个参考书就不列了，很多。\n2. 矩阵数学：《矩阵分析》，Roger Horn。矩阵分析领域无争议的经典。\n3. 概率论与统计：《概率论及其应用》，威廉·费勒。也是极牛的书，可数学味道太重，不适合做机器学习的。于是讨论组里的 Du Lei 同学推荐了《All Of Statistics》并说到\n机器学习这个方向，统计学也一样非常重要。推荐All of statistics，这是CMU的一本很简洁的教科书，注重概念，简化计算，简化与Machine Learning无关的概念和统计内容，可以说是很好的快速入门材料。\n4. 最优化方法：《Nonlinear Programming, 2nd》非线性规划的参考书。《Convex Optimization》凸优化的参考书。此外还有一些书可以参考 wikipedia 上的最优化方法条目。要深入理解机器学习方法的技术细节很多时候（如SVM）需要最优化方法作为铺垫。\n王宁同学推荐了好几本书：\n《Machine Learning, Tom Michell》, 1997.\n老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能\"新\"到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n《Modern Information Retrieval, Ricardo Baeza-Yates et al》. 1999\n老书，牛人。貌似第一本完整讲述IR的书。可惜IR这些年进展迅猛，这本书略有些过时了。翻翻做参考还是不错的。另外，Ricardo同学现在是Yahoo Research for Europe and Latin Ameria的头头。\n《Pattern Classification (2ed)》, Richard O. Duda, Peter E. Hart, David G. Stork\n大约也是01年左右的大块头，有影印版，彩色。没读完，但如果想深入学习ML和IR，前三章（介绍，贝叶斯学习，线性分类器）必修。\n还有些经典与我只有一面之缘，没有资格评价。另外还有两本小册子，论文集性质的，倒是讲到了了不少前沿和细节，诸如索引如何压缩之类。可惜忘了名字，又被我压在箱底，下次搬家前怕是难见天日了。\n（呵呵，想起来一本：《Mining the Web - Discovering Knowledge from Hypertext Data》 ）\n说一本名气很大的书：《Data Mining: Practical Machine Learning Tools and Techniques》。Weka 的作者写的。可惜内容一般。理论部分太单薄，而实践部分也很脱离实际。DM的入门书已经不少，这一本应该可以不看了。如果要学习了解 Weka ，看文档就好。第二版已经出了，没读过，不清楚。\n信息检索方面，Du Lei 同学再次推荐：\n信息检索方面的书现在建议看Stanford的那本《Introduction to Information Retrieval》，这书刚刚正式出版，内容当然up to date。另外信息检索第一大牛Croft老爷也正在写教科书，应该很快就要面世了。据说是非常pratical的一本书。\n对信息检索有兴趣的同学，强烈推荐翟成祥博士在北大的暑期学校课程，这里有全slides和阅读材料：\nhttp://net.pku.edu.cn/~course/cs410/schedule.html\nmaximzhao 同学推荐了一本机器学习：\n加一本书：Bishop, 《Pattern Recognition and Machine Learning》. 没有影印的，但是网上能下到。经典中的经典。Pattern Classification 和这本书是两本必读之书。《Pattern Recognition and Machine Learning》是很新（07年），深入浅出，手不释卷。\n最后，关于人工智能方面（特别地，决策与判断），再推荐两本有意思的书，\n一本是《Simple Heuristics that Makes Us Smart》\n另一本是《Bounded Rationality: The Adaptive Toolbox》\n不同于计算机学界所采用的统计机器学习方法，这两本书更多地着眼于人类实际上所采用的认知方式，以下是我在讨论组上写的简介：\n这两本都是德国ABC研究小组（一个由计算机科学家、认知科学家、神经科学家、经济学家、数学家、统计学家等组成的跨学科研究团体）集体写的，都是引起领域内广泛关注的书，尤其是前一本，後一本则是对 Herbert Simon （决策科学之父，诺奖获得者）提出的人类理性模型的扩充研究），可以说是把什么是真正的人类智能这个问题提上了台面。核心思想是，我们的大脑根本不能做大量的统计计算，使用fancy的数学手法去解释和预测这个世界，而是通过简单而鲁棒的启发法来面对不确定的世界（比如第一本书中提到的两个后来非常著名的启发法：再认启发法（cognition heuristics）和选择最佳（Take the Best）。当然，这两本书并没有排斥统计方法就是了，数据量大的时候统计优势就出来了，而数据量小的时候统计方法就变得\n非常糟糕\n；人类简单的启发法则充分利用生态环境中的规律性（regularities），都做到计算复杂性小且鲁棒。\n关于第二本书的简介：\n1. 谁是\nHerbert Simon\n2. 什么是\nBounded Rationality\n3. 这本书讲啥的：\n我一直觉得人类的决策与判断是一个非常迷人的问题。这本书简单地说可以看作是《决策与判断》的更全面更理论的版本。系统且理论化地介绍人类决策与判断过程中的各种启发式方法（heuristics）及其利弊 （为什么他们是最优化方法在信息不足情况下的快捷且鲁棒的逼近，以及为什么在一些情况下会带来糟糕的后果等，比如学过机器学习的都知道朴素贝叶斯方法在许多情况下往往并不比贝叶斯网络效果差，而且还速度快；比如多项式插值的维数越高越容易overfit，而基于低阶多项式的分段样条插值却被证明是一个非常鲁棒的方案）。\n在此提一个书中提到的例子，非常有意思：两个团队被派去设计一个能够在场上接住抛过来的棒球的机器人。第一组做了详细的数学分析，建立了一个相当复杂的抛物线近似模型（因为还要考虑空气阻力之类的原因，所以并非严格抛物线），用于计算球的落点，以便正确地接到球。显然这个方案耗资巨大，而且实际运算也需要时间，大家都知道生物的神经网络中生物电流传输只有百米每秒之内，所以 computational complexity 对于生物来说是个宝贵资源，所以这个方案虽然可行，但不够好。第二组则采访了真正的运动员，听取他们总结自己到底是如何接球的感受，然后他们做了这样一个机器人：这个机器人在球抛出的一开始一半路程啥也不做，等到比较近了才开始跑动，并在跑动中一直保持眼睛于球之间的视角不变，后者就保证了机器人的跑动路线一定会和球的轨迹有交点；整个过程中这个机器人只做非常粗糙的轨迹估算。体会一下你接球的时候是不是眼睛一直都盯着球，然后根据视线角度来调整跑动方向？实际上人类就是这么干的，这就是 heuristics 的力量。\n相对于偏向于心理学以及科普的《决策与判断》来说，这本书的理论性更强，引用文献也很多而经典，而且与人工智能和机器学习都有交叉，里面也有不少数学内容，全书由十几个章节构成，每个章节都是由不同的作者写的，类似于 paper 一样的，很严谨，也没啥废话，跟 《Psychology of Problem Solving》类似。比较适合 geeks 阅读哈。\n另外，对理论的技术细节看不下去的也建议看看《决策与判断》这类书（以及像《别做正常的傻瓜》这样的傻瓜科普读本），对自己在生活中做决策有莫大的好处。人类决策与判断中使用了很多的 heuristics ，很不幸的是，其中许多都是在适应几十万年前的社会环境中建立起来的，并不适合于现代社会，所以了解这些思维中的缺点、盲点，对自己成为一个良好的决策者有很大的好处，而且这本身也是一个非常有趣的领域。\n（完）\n原文链接：http://blog.csdn.net/pongba/article/details/2915005"}
{"content2":"《人工智能》这本书是李开复博士最新的一本关于人工智能的著作，书中一开篇就指出了一个已经成为现实的观点：人工智能已经来了。当我们还在好奇的幻想人工智能是什么样子的时候，其实人工智能已经在我们最常用的手机中多处应用了。比如美图秀秀，淘宝的智能推荐，微软小冰等聊天机器人，其实这些就是典型的人工智能。通常我们一想到人工智能就想到了科幻片和科幻小说里的机器人，提到人工智能就会担心未来我们人类的大部分工作会被人工智能所代替，我们人类将被人工智能所超越，甚至会走向毁灭。但李开复博士在本书中指明，当前的人工智能还完全看不到能够超越人类的迹象。人工智能其实早在上世纪的五六十年代就已经开始了，因为受限于计算机硬件储存以及运算速度，人工智能一直不能够得到很好的发展，已久不能够得到很好的关注。我们最近认识人工智能是AlphaGo在围棋上战胜了人类的围棋顶尖高手，此消息一出，媒体争相报道，顿时更种关于人工智能的传闻都甚嚣尘上，其实这种现象和九十年代人工智能程序打败人类象棋高手是一样的，一时也是震惊了人类，不过后来由于一般的象棋游戏上的程序都具有很高的水平，因此我们也就习惯了。说起人工智能将会代替人类的很多工作这是没错的，就比如现在很多的新闻都是人工智能程序写的。一个记者看的资料有限，但人工智能可以从网路上收集大量的信息和数据，自动的生成一篇流利的报道。再比如，现在已经出现了很多送快递和仓储的人工智能，取代了人工，比人工更快更节省空间，而且他们不需要休息。甚至医生，律师，证券从业者等都会（有的已经开始）被人工智能所取代了。这个时候大家就慌了，我们都被人工智能取代了，那我们去做什么呢？当然这个时候总是会有人说，人工智能不会懂得艺术，不会创造，只是依靠程序执行而已。但事实上，现在的图像和人脸识别，都是依靠深度学习技术的，也就是给人工智能足够的样本，让他们不断的训练，自己去不断的提高识别的能力。而且人工智能现在可以快速写出美妙的乐谱，还可以给人画像，还可以写春联，是不是顿时哑口无言？但我们要搞清楚一个问题，人工智能到底和我们人类的思考方式和学习方式到底一不一样？当然当然是否定的，不一样。虽然现在都在使用神经网络算法，但其实我们人脑到底是如何思考，如何记忆，如何做到“举一反三”的，我们人类都没有搞清楚，怎么可能模仿大脑呢？前面说的深度学习如果需要识别一个苹果，可能需要几十万张照片给它训练，而我们人类只需要看几眼就够了；人工智能学习下围棋可以打败人类，可是这个人工智能程序却只能下围棋，而我们人类却可以通过下围棋学到很多的 东西，比如思考问题的整体性，客服人性的缺点，有的甚至还从中悟出了人生的智慧；再比如人工智能可以翻译语言，但即使它翻译的较为准确但它也完全不懂得是什么意思，只是按照一定的对应关系翻译而已。当然说了这么多并不是告诉大家人工智能是有多么的不如我们人类，而是要告诉大家不要被科幻小说所迷惑，要知道真正的人工智能到底是什么。人工智能正处于一个历史性的时刻，我们只有抓住机遇，赶上这一波浪潮才能够在未来处于不败之地。书中李开复博士也探讨了很多人工智能可能带来的社会问题、伦理问题、道德问题、经济问题等，总体上是比较乐观的。科幻小说中的人工智能大概是强人工智能或者超强人工智能，就是人工智能与人类智力相当甚至可以超越人类，但目前还看不到这一可能性的诞生，我们目前的人工智能处于弱人工智能的时代。想要突破人工智能就必须先了解我们人类的大脑。最后，探讨了人工智能时代我们应该如何学习的问题。人工智能时代，程序化式的，重复性的，仅靠记忆与练习就可以掌握的技能将是最没有价值的技能，几乎一定可以由机器来完成；反之，那些最能体现人的综合素质的技能，例如，人对于复杂系统的综合分析，决策能力，对于艺术和文化的审美能力和创造性思维，由生活经验及文化熏陶产生的直觉，常识，基于人类的情感与他人互动的能力才是人工智能时代最有价值，最值得培养和学习的技能。而且，这些技能中，大多数都是因人而异，需要“定制化”教育或培养，不可能从传统的“批量”教育中获取。"}
{"content2":"1、L1范式和L2方式的区别\n（1）L1范式是对应参数向量绝对值之和\n（2）L1范式具有稀疏性\n（3）L1范式可以用来作为特征选择，并且可解释性较强（这里的原理是在实际Loss function中都需要求最小值，根据L1的定义可知L1最小值只有0，故可以通过这种方式来进行特征选择）\n（4）L2范式是对应参数向量的平方和，再求平方根\n（5）L2范式是为了防止机器学习的过拟合，提升模型的泛化能力\n2、优化算法及其优缺点\n温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。\n（1）随即梯度下降\n优点：可以一定程度上解决局部最优解的问题\n缺点：收敛速度较慢\n（2）批量梯度下降\n优点：容易陷入局部最优解\n缺点：收敛速度较快\n（3）mini_batch梯度下降\n综合随即梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。\n（4）牛顿法\n牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算Hessian矩阵比较困难。\n（5）拟牛顿法\n拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。\n（6）共轭梯度\n（7）启发式的优化算法\n启发式的优化算法有遗传算法，粒子群算法等。这类算法的主要思想就是设定一个目标函数，每次迭代根据相应的策略优化种群。直到满足什么样的条件为止。\n3、RF与GBDT之间的区别\n（1）相同点\n都是由多棵树组成\n最终的结果都是由多棵树一起决定\n（2）不同点\n组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成\n组成随机森林的树可以并行生成，而GBDT是串行生成\n随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和\n随机森林对异常值不敏感，而GBDT对异常值比较敏感\n随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的\n随机森林不需要进行数据预处理，即特征归一化。而GBDT则需要进行特征归一化\n（3）RF：\n优点：\n易于理解，易于可视化\n不需要太多的数据预处理，即数据归一化\n不易过拟合\n易于并行化\n缺点：\n不适合小样本数据，只适合大样本数据\n大多数情况下，RF的精度低于GBDT\n适合决策边界的是矩阵，不适合对角线型\n（4）GBDT\n优点：\n精度高\n缺点：\n参数较多，容易过拟合\n不易并行化\n4、SVM的模型的推导\n5、SVM与树模型之间的区别\n（1）SVM\nSVM是通过核函数将样本映射到高纬空间，再通过线性的SVM方式求解分界面进行分类。\n对缺失值比较敏感\n可以解决高纬度的问题\n可以避免局部极小值的问题\n可以解决小样本机器学习的问题\n（2）树模型\n可以解决大样本的问题\n易于理解和解释\n会陷入局部最优解\n易过拟合\n6、梯度消失和梯度膨胀\n（1）梯度消失：\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->\n可以采用ReLU激活函数有效的解决梯度消失的情况\n（2）梯度膨胀\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大\n可以通过激活函数来解决\n7、LR的原理和Loss的推导\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->"}
{"content2":"机器学习可分为监督学习和无监督学习。有监督学习就是有具体的分类信息，比如用来判定输入的是输入[a,b,c]中的一类；无监督学习就是不清楚最后的分类情况，也不会给目标值。\nK-近邻算法属于一种监督学习分类算法，该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。\n需要进行分类，分类的依据是什么呢，每个物体都有它的特征点，这个就是分类的依据，特征点可以是很多，越多分类就越精确。\n机器学习就是从样本中学习分类的方式，那么就需要输入我们的样本，也就是已经分好类的样本，比如特征点是A , B2个特征，输入的样本甲乙丙丁，分别为[[1.0, 1.1], [1.0, 1.0], [0., 0.], [0.0, 0.1]]。 那么就开始输入目标值，当然也要给特征了，最终的目标就是看特征接近A的多还是B的多，如果把这些当做坐标，几个特征点就是几纬坐标，那么就是坐标之间的距离。那么问题来了，要怎么看接近A的多还是B的多。\n我就直接贴代码了，基于python，首先输入特征量labels和样本group。\n一开始需要导入的模块\n#coding=utf-8 #科学计算包 #from numpy import * import numpy #运算符模块 import operator\n数据样本和分类模拟\n#手动建立一个数据源矩阵group，和数据源的分类结果labels def createDataSet(): group = numpy.array([[1.0, 1.1], [1.0, 1.0], [5., 2.], [5.0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labels\n然后进行KNN算法。\n# newInput为输入的目标，dataset是样本的矩阵，label是分类，k是需要取的个数 def kNNClassify(newInput, dataSet, labels, k): #读取矩阵的行数，也就是样本数量 numSamples = dataSet.shape[0] print 'numSamples: ' ,numSamples #变成和dataSet一样的行数,行数=原来*numSamples，列数=原来*1 ，然后每个特征点和样本的点进行相减 diff = numpy.tile(newInput, (numSamples, 1)) - dataSet print 'diff: ',diff #平方 squaredDiff = diff ** 2 print \"squaredDiff: \",squaredDiff #axis=0 按列求和，1为按行求和 squaredDist = numpy.sum(squaredDiff, axis = 1) print \"squaredDist: \",squaredDist #开根号，距离就出来了 distance = squaredDist ** 0.5 print \"distance: \",distance #按大小逆序排列 sortedDistIndices = numpy.argsort(distance) print \"sortedDistIndices: \",sortedDistIndices classCount = {} for i in range(k): #返回距离（key）对应类别（value） voteLabel = labels[sortedDistIndices[i]] print \"voteLabel: \" ,voteLabel # 取前几个K值，但是K前几个值的大小没有去比较，都是等效的 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 print \"classCount: \" ,classCount maxCount = 0 #返回占有率最大的 sortedClassCount=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0]\n最后进行测试\ndataSet, labels = createDataSet() testX = numpy.array([0, 0]) k = 3 outputLabel = kNNClassify(testX, dataSet, labels, k) print \"Your input is:\", testX, \"and classified to class: \", outputLabel\n可以发现输出\nnumSamples: 4 diff: [[-1. -1.1] [-1. -1. ] [-5. -2. ] [-5. -0.1]] squaredDiff: [[ 1.00000000e+00 1.21000000e+00] [ 1.00000000e+00 1.00000000e+00] [ 2.50000000e+01 4.00000000e+00] [ 2.50000000e+01 1.00000000e-02]] squaredDist: [ 2.21 2. 29. 25.01] distance: [ 1.48660687 1.41421356 5.38516481 5.0009999 ] sortedDistIndices: [1 0 3 2] voteLabel: A voteLabel: A voteLabel: B classCount: {'A': 2, 'B': 1} Your input is: [0 0] and classified to class: A\n这里我之前一直有个疑问，关于K的取值，结果也许跟K的取值产生变化，只要在K的取值范围内们所有特征点距离远近也就没有关系了。所以才叫K近邻分类算法"}
{"content2":"作者：Burak Kanber\n翻译：王维强\n原文：http://burakkanber.com/blog/machine-learning-in-other-languages-introduction/\n我热衷于机器学习算法，并在该领域教授过一些课程，也在一些研讨会中做过报告，对该课题很着迷。但是像所有的技术一样只学些皮毛是远远不够的，想做好任何事情，都需深入实践。\n我自己恰巧是个PHP和Javascript开发者，在这两个技术领域内也教授过一些课程，而且像所有的普通程序员一样我在Ruby，Python，Perl和C方面也有些经验，但是更偏爱PHP和JS。\n每当我说起蒂达尔实验室的机器学习算法是用PHP实现的，大家就会嘲讽地看着我说那怎么可能。简单来说,机器学习算法可以用任何语言来实现，很多人并不介意通过草图写算法来了解基本原理，然后使用一些Python库应付他们的工作，可是并没有真正领悟黑箱中正在发生的过程。另外一些人只是学术性的了解机器算法，使用Octave或者Matlab解决问题。\n通过本系列文章，我将会教会你机器学习算法的基础原理并且使用Javascript作为算法实现的工具，而不是用Python或者Octave作为范例语言。原本我打算用多个语言（PHP，JS，Perl，C, Ruby）写这些文章，但最终选择Javascript是因为以下原因：\n如果你是个web程序员，应该对JS已经有所了解。\nJSFiddle是一个很好的工具，能够把可运行的代码嵌入到我的文章中（C或Perl就很难这样做了）。\n有些人请求我着重在一个语言上讲述。\n在我于头脑中用Javascript写这些文章之时，也请你自己选择一门语言预写一些例子作为家庭作业！实践决定你的掌握程度，用几种语言数次编写同样的算法真的能帮你更好地了解这些范例。\n使用像PHP或Javascript这样的语言编写机器学习算法是完全有可能获得很好的执行表现的。我提倡用更多的其他语言实现机器学习算法是因为这样做能更好地帮你从草图中了解基本原理，并且能让你统一自己的背景知识，不至于在PHP的应用程序中还要写Python脚本处理任务，你完全可以用纯PHP来实现，而不是被迫切换到其他语言。\n好吧，很多时候，的确有很多事情不能用PHP或者Javascript来解决，比如很多高级算法中使用到的大型矩阵运算。实际上你也能用JS进行矩阵运算，只是差别在于“能做”还是“高效地做”，NumPy或者Matlab的优势不在于他们能做矩阵运算，而是他们使用了优化的算法能高效地做矩阵运算，这些优化的运算方法我们不会亲自去做，除非致力于计算机线性代数的研究。这也不是我的强项，所以我们只着重于机器学习，不要求高级的矩阵数学知识。你当然可以尝试惨烈地直面矩阵操作，但是会止步于低效的系统。作为学习当然很好，只是我不鼓励这样做——我只在产品级的环境下才关注这方面的事。\n我们将要关注的算法既可以用矩阵来解决，也可以不用矩阵。我们将会在这些算法中使用迭代的方法，当然大多数算法也可以用线性代数来描述。解决问题有多种途径，我鼓励大家去使用线性代数的方法，但是那不是我的强项，所以我会使用其他的方法。\n我的这个系列文章将覆盖：\nk-nearest-neighbor 介绍\nk-means 分类 (1)\n遗传算法 (1,2)\n朴素贝叶斯分类器 (第一部分：文档分类)\n情感分析(1）\n全文检索(1：相关度评价)\n神经网络"}
{"content2":"1.大数据与机器学习的关系：\n大数据领域我们做的是数据的存储和简单的统计计算，机器学习在大数据的应用是为了发现数据的规律或模型，用机器学习算法对数据进行计算的到的模型，从而决定我们的预测与决定的因素（比如在大数据用户画像项目里，生成的特殊用户字段）。\n2.大数据在机器学习的应用\n目前市场实际开发模式中，应该在大数据哪一个阶段层次应用到机器学习的相关技术呢，我们接下来来说明，首先目前大数据的架构模式列举如下几个\n2.1数据采集（ftp、socket）---数据存储（hdfs）---数据清洗（MapReduce）----数据分析（hive）---sqoop导入-----存储（mysql、oracle）---web显示\n2.2数据采集（ftp、socket）---数据存储（hdfs）---数据清洗（MapReduce）---列式数据库存储（hbase）-----thrift（协处理器）---web显示\n2.3数据采集（ftp、socket）---数据存储（hdfs）---数据清洗（MapReduce）----数据分析（hive）----impala（实时数据分析）---jdbc-----web显示\n2.4数据采集（ftp、socket）---数据存储（hdfs）---spark计算-----存储（mysql、oracle）---web显示\n整体在开发完成后用分布式任务调度系统（azkaban、oozie）对以上架构进行周期运行计算。\n而机器学习在大数据的应用阶段为：数据分析（hive）--- 机器学习----sqoop导入、列式数据库存储（hbase）---- 机器学习------thrift（协处理器）\n总结在大数据架构中，机器学习处于上层阶段，在大数据进行计算之后向最终储存或者直接web展现的时候需要机器学习来产生一个决策与预测的模型。\n3.机器学习\n3.1机器学习的理论在1950年就已经提出，但是因为数据量的存储机制落后，算法结合度低下，以及我们对于数据处理速度的低下导致一直不能把技术落地，而在科技高速发展的今天，因为以上技术困境的解除，导致机器学习向阳而生，从而被广泛应用，改变了人们的生活，\n3.2 机器学习是多领域交叉学科，涉及概率论、统计学、逼近学、凸分学、算法复杂度理论等多门学科，专门计算机器怎样模拟实现人类的学习行为，获取行的知识和技能，然后重新改变已经有的知识结构来提高完善自身的性能。\n3.3 机器学习（ML）与人工智能（AI）的关系：它是人工智能的核心，是使计算机拥有智能的根本途径，它的应用遍布人工智能的各个领域，它主要使用归纳、总和而不是演绎。\n3.3 机器学习的学习动作：针对经验E（experience）和一系列任务T（tasks）和一定表现的衡量P，如果经验E的积累，针对定义好的任务T可以提高表现P，就说明有学习能力。\n3.4 机器学习的应用：语音识别、自动驾驶、语言翻译、推荐系统、无人机等等\n3.5 机器学习与深度学习的关系：深度学习是实现机器学习的一种技术深度学习使得许多机器学习应用得以实现，并拓展了人工智能的整个领域。深度学习一一实现了各种任务，并使得所有的机器辅助变成可能。无人驾驶汽车、电影推荐等，都触手可及或即将成为现实。人工智能就在现在，也在未来。有了深度学习，人工智能可能甚至达到像我们畅想的科幻小说一样效果。\n（ 更简单理解）人工智能是祖辈，机器学习是父辈，深度学习是儿子辈！\n3.6 机器学习的运行方式：我们知道程序处理是由CPU（中央处理器）来计算运行的，但是目前我们开发应用中机器学习的计算大部分（深度学习）是通过GPU（图形处理器）来计算运行的\n3.7 机器学习的概念：\n3.7.1 基本概念：训练集，测试集，特征值，监督学习，非监督学习，半监督学习，分类，回归\n3.7.2 概念学习：人类学习概念（如婴儿）：鸟，狗；车，房子；黑匣子和计算机 （怎么认识和区分？）定义：概念学习是指从有关某个布尔函数（是或否）的输入输出训练样例中推断出该布尔函数\n3.7.3 数据集：真实数据集\n3.7.4 行：样本数据\n3.7.5 列：特征或者数据数据\n3.7.6 特征向量：每一个样本中的数据组成的向量\n3.7.7 属性空间：属性章程的空间\n3.7.8 训练集：用于模型训练的数据集\n3.7.9 测试集：用于校验模型的优劣程度\n3.7.10 训练过程：（学习过程）使用训练数据集+机器学习算法==》模型\n3.8 监督学习：\n监督（supervised）是指训练数据集中的每个样本均有一个已知的输出项（类标label）\n输出变量为连续变量的预测问题称为 回归（ regression ）问题\n回归算法：\n• 简单线性回归\n• 多元线性回归\n• Lasso回归\n• Ridge回归\n• ElasticNet\n输出变量为有限个离散变量的预测问题称为 分类问题\n分类算法：\n• 简单线性回归\n• 多元线性回归\n• Lasso回归\n• Ridge回归\n• ElasticNet\n3.9 非监督学习：\n人们给机器一大堆没有分类标记的数据，让机器可以对数据分类、检测异常等。\n聚类（KMeans）\n降维（PCA,LDA）\n荐一个大数据学习群 142974151每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享，\n3.10 半监督学习：\n半监督学习就是提供了一条利用“廉价”的未标记样本的途径\n3.11 强化学习：\n是机器学习的一个重要分支，主要用来解决连续决策的问题。\n围棋可以归纳为一个强化学习问题，需要学习在各种局势下如何走出最好的招法。\n3.12 迁移学习：\n应用场景：\n小数据的问题。比方说新开一个网店，卖一种新的糕点，没有任何的数据，就无法建立模型对用户进行推荐。但用户买一个东西会反映到用户可能还会买另外一个东西，所以如果知道用户在另外一个领域，比方说卖饮料，已经有了很多很多的数据，利用这些数据建一个模型，结合用户买饮料的习惯和买糕点的习惯的关联，就可以把饮料的推荐模型给成功地迁移到糕点的领域，这样，在数据不多的情况下可以成功推荐一些用户可能喜欢的糕点。这个例子就说明，有两个领域，一个领域已经有很多的数据，能成功地建一个模型，有一个领域数据不多，但是和前面那个领域是关联的，就可以把那个模型给迁移过来。\n个性化的问题。比如每个人都希望自己的手机能够记住一些习惯，这样不用每次都去设定它，怎么才能让手机记住这一点呢？其实可以通过迁移学习把一个通用的用户使用手机的模型迁移到个性化的数据上面。\n4.学习机器学习应该具备哪些知识\n4.1对概率要有基本了解，\n4.2了解微积分和线性代数的基本知识，\n4.3 掌握Python编程或者R语言编程（在公司企业开发常用Python来完成机器学习数据挖掘、在学术界用R语言来完成机器学习数据挖掘）\n在企业中要求我们要达成的目标是：掌握机器学习算法和应用框架通过分类及回归来解决实际问题。\n机器学习对应的职位：数据挖掘（用户画像方向）、NLP（自然语言处理）、推荐系统（推荐算法、排序算法）、计算广告（CTR预估）、计算机视觉（深度学习）、语音识别（HMM,深度学习）注意：这些职位的前提是要有大数据开发相关经验。\n5.常用的十个机器学习的算法\n5.1机器学习算法通常可以被分为三大类 —— 监督式学习，非监督式学习和强化学习。监督式学习主要用于一部分数据集（训练数据）有某些可以获取的熟悉（标签），但剩余的样本缺失并且需要预测的场景。非监督式学习主要用于从未标注数据集中挖掘相互之间的隐含关系。强化学习介于两者之间 —— 每一步预测或者行为都或多或少有一些反馈信息，但是却没有准确的标签或者错误提示。\n5.2决策树：决策树是一种决策支持工具，它使用树状图或者树状模型来表示决策过程以及后续得到的结果，包括概率事件结果等。请观察下图来理解决策树的结构。\n从商业决策的角度来看，决策树就是通过尽可能少的是非判断问题来预测决策正确的概率。这种方法可以帮你用一种结构性的、系统性的方法来得出合理的结论。\n5.3 朴素贝叶斯分类器：朴素贝叶斯分类器是一类基于贝叶斯理论的简单的概率分类器，它假设特征之前是相互独立的。下图所示的就是公式 —— P(A|B)表示后验概率，P(B|A)是似然值，P(A)是类别的先验概率，P(B)代表预测器的先验概率。\n现实场景中的一些例子包括：\n检测垃圾电子邮件\n将新闻分为科技、政治、体育等类别\n判断一段文字表达积极的情绪还是消极的情绪\n用于人脸检测软件\n5.4 随机森林\n在源数据中随机选取数据，组成几个子集\nS 矩阵是源数据，有 1-N 条数据，A B C 是feature，最后一列C是类别\n由 S 随机生成 M 个子矩阵\n这 M 个子集得到 M 个决策树\n将新数据投入到这 M 个树中，得到 M 个分类结果，计数看预测成哪一类的数目最多，就将此类别作为最后的预测结果\n5.5 逻辑递归\n当预测目标是概率这样的，值域需要满足大于等于0，小于等于1的，这个时候单纯的线性模型是做不到的，因为在定义域不在某个范围之内时，值域也超出了规定区间。\n所以此时需要这样的形状的模型会比较好\n那么怎么得到这样的模型呢？\n这个模型需要满足两个条件 大于等于0，小于等于1\n大于等于0 的模型可以选择 绝对值，平方值，这里用 指数函数，一定大于0\n小于等于1 用除法，分子是自己，分母是自身加上1，那一定是小于1的了\n再做一下变形，就得到了 logistic regression 模型\n通过源数据计算可以得到相应的系数了\n最后得到 logistic 的图形\n5.6 SVM\nsupport vector machine\n要将两类分开，想要得到一个超平面，最优的超平面是到两类的 margin 达到最大，margin就是超平面与离它最近一点的距离，如下图，Z2>Z1，所以绿色的超平面比较好\n将这个超平面表示成一个线性方程，在线上方的一类，都大于等于1，另一类小于等于－1\n点到面的距离根据图中的公式计算\n所以得到 total margin 的表达式如下，目标是最大化这个 margin，就需要最小化分母，于是变成了一个优化问题\n举个栗子，三个点，找到最优的超平面，定义了 weight vector＝（2，3）－（1，1）\n得到 weight vector 为（a，2a），将两个点代入方程，代入（2，3）另其值＝1，代入（1，1）另其值＝-1，求解出 a 和 截矩 w0 的值，进而得到超平面的表达式。\na 求出来后，代入（a，2a）得到的就是 support vector\na 和 w0 代入超平面的方程就是 support vector machine\n5.7 K最邻近\nk nearest neighbours\n给一个新的数据时，离它最近的 k 个点中，哪个类别多，这个数据就属于哪一类\n栗子：要区分 猫 和 狗，通过 claws 和 sound 两个feature来判断的话，圆形和三角形是已知分类的了，那么这个 star 代表的是哪一类呢\nk＝3时，这三条线链接的点就是最近的三个点，那么圆形多一些，所以这个star就是属于猫\n5.8 K均值\n想要将一组数据，分为三类，粉色数值大，黄色数值小\n最开心先初始化，这里面选了最简单的 3，2，1 作为各类的初始值\n剩下的数据里，每个都与三个初始值计算距离，然后归类到离它最近的初始值所在类别\n分好类后，计算每一类的平均值，作为新一轮的中心点\n几轮之后，分组不再变化了，就可以停止了\n5.9 Adaboost\nadaboost 是 bosting 的方法之一\nbosting就是把若干个分类效果并不好的分类器综合起来考虑，会得到一个效果比较好的分类器。\n下图，左右两个决策树，单个看是效果不怎么好的，但是把同样的数据投入进去，把两个结果加起来考虑，就会增加可信度\nadaboost 的栗子，手写识别中，在画板上可以抓取到很多 features，例如 始点的方向，始点和终点的距离等等\ntraining 的时候，会得到每个 feature 的 weight，例如 2 和 3 的开头部分很像，这个 feature 对分类起到的作用很小，它的权重也就会较小\n而这个 alpha 角 就具有很强的识别性，这个 feature 的权重就会较大，最后的预测结果是综合考虑这些 feature 的结果\n5.10 神经网络\nNeural Networks 适合一个input可能落入至少两个类别里\nNN 由若干层神经元，和它们之间的联系组成 第一层是 input 层，最后一层是 output 层\n在 hidden 层 和 output 层都有自己的 classifier\ninput 输入到网络中，被激活，计算的分数被传递到下一层，激活后面的神经层，最后output 层的节点上的分数代表属于各类的分数，下图例子得到分类结果为 class 1\n同样的 input 被传输到不同的节点上，之所以会得到不同的结果是因为各自节点有不同的weights 和 bias\n这也就是 forward propagation\n5.11 马尔可夫\nMarkov Chains 由 state 和 transitions 组成\n栗子，根据这一句话 ‘the quick brown fox jumps over the lazy dog’，要得到 markov chain\n步骤，先给每一个单词设定成一个状态，然后计算状态间转换的概率\n这是一句话计算出来的概率，当你用大量文本去做统计的时候，会得到更大的状态转移矩阵，例如 the 后面可以连接的单词，及相应的概率\n生活中，键盘输入法的备选结果也是一样的原理，模型会更高级\n6.人工智能（AI）\n6.1经过预测分析，按照目前的人工智能发展速度，在未来十年内我们会有40%的人会被人工智能所取代，有一些行业被取代的比例是很高的，比如：翻译、记者、助理、保安、司机、销售行业、客服、交易员、会计、保姆等等，站在技术角度不考虑社会政策的前提下，那些平时工作性质是简单重复的岗位（这里的简单重复已经不是指工作操作层面而是指工作逻辑层面），是必然会被取代的，甚至医生也在被取代的范围内。\n6.2三次浪潮：在1956年就已经提出，是符号主义流派，专家系统占据主导地位。\n1980年左右又开始流行，是统计主义流派，用统计模型解决问题\n2010年以后，以神经网络、深度学习、大数据的流派开始流行。\n6.3 在人工智能的前两次的阶段都是兴起之后又没落了，一些关键技术没有实际的落地，而如今的第三次浪潮已经深深的与我们生活结合了，比如我们打开手机，现在的任何一款APP都几乎涉及了人工智能技术，\n7. 数据分析、数据挖掘和机器学习的关系\n数据分析是从数据到信息的整理、筛选和加工的过程，数据挖掘是对信息进行价值化的分析。用机器学习的方法进行数据挖掘。机器学习是一种方法；数据挖掘是一件事情；还有一个相似的概念就是模式识别，这也是一件事情。而现在流行的深度学习技术只是机器学习的一种；机器学习和模式识别都是达到人工智能目标的手段之一，对数据挖掘而言，数据库提供数据管理技术，机器学习和统计学提供数据分析技术。\n---------------------\n推荐一个大数据学习群 142974151每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享，\n原文：https://blog.csdn.net/jinyusheng_1991/article/details/82796962"}
{"content2":"学习通app人工智能章节测验答案\n————————————————\n购买完整版答案联系 QQ 64315052\n————————————————\n1.1育才新工科-人工智能简介\n1【判断题】\n《人工智能》课程为理工类通选课,本课程给予学生的主要是思想而不是知识。对\n1.2图灵是谁？\n1\n【单选题】图灵曾协助军方破解()的著名密码系统Enigma。C\nA、英国\nB、美国\nC、德国\nD、日本\n2【判断题】电影《模仿游戏》是纪念图灵诞生90周年而拍摄的电影。X\n3【判断题】图灵使用博弈论的方法破解了Enigma。对\n1.3为什么图灵很灵？\n1\n【单选题】1937年,图灵在发表的论文()中,首次提出图灵机的概念。B\nA、《左右周期性的等价》\nB、《论可计算数及其在判定问题中的应用》\nC、《可计算性与λ可定义性》\nD、《论高斯误差函数》\n2\n【单选题】1950年,图灵在他的论文()中,提出了关于机器思维的问题。D\nA、《论数字计算在决断难题中的应用》\nB、《论可计算数及其在判定问题中的应用》\nC、《可计算性与λ可定义性》\nD、《计算和智能》\n3【判断题】存在一种人类认为的可计算系统与图灵计算不等价。X\n4【判断题】图灵测试是指测试者与被测试者(一个人和一台机器)隔开的情况下,通过一些装置(如键盘)向被测试者随意提问。如果测试者不能确定出被测试者是人还是机器,那么这台机器就通过了测试,并被认为具有人类智能。对\n1.4为什么图灵不灵？\n1\n【单选题】以下叙述不正确的是()。B\nA、图灵测试混淆了智能和人类的关系\nB、机器智能的机制必须与人类智能相同\nC、机器智能可以完全在特定的领域中超越人类智能\nD、机器智能可以有人类智能的创造力\n2\n【单选题】在政府报告中,()的报告使用“机器智能”这个词汇。D\nA、中国\nB、英国\nC、德国\nD、美国\n3【多选题】机器智能可以有自己的“人格”体现主要表现在()。ABC\nA、模型间的对抗—智能进化的方式\nB、机器智能的协作—机器智能的社会组织\nC、机器智能是社会的实际生产者\nD、机器智能可以有人类智能的创造力\n4【判断题】图灵测试存在的潜台词是机器智能的极限可以超越人的智能,机器智能可以不与人的智能可比拟。X\n1.5人类智能与机器智能如何共融及未来\n1\n【单选题】以下关于未来人类智能与机器智能共融的二元世界叙述不正确的是()。\nA、人类智能与机器智能具有平等性\nB、机器智能是模仿人类智能\nC、人类智能与机器智能均具有群智行\nD、人工智能与机器智能均具有发展性、合作性\n2\n【单选题】机器通过人类发现的问题空间的数据,进行机器学习,具有在人类发现的问题空间中求解的能力,并且求解的过程与结果可以被人类智能(),此为机器智能的产生。\nA、采纳\nB、参考\nC、理解\nD、相同\n3【判断题】人类智能可以和机器智能相互融合。\n4【判断题】机器智能的创造是指机器通过求解人类智能发现的问题空间中的问题积累数据与求解方法,通过机器学习,独立发现新的问题空间。\n1.6人工智能界定与科学\n1\n【单选题】在最初的图灵测试中,如果有超过()的测试者不能确定出被测试者是人还是机器,则这台机器就通过了测试,并认为具有人类智能。\nA、0.2\nB、0.3\nC、0.4\nD、0.5\n2\n【单选题】()不属于图灵测试中包含的三个未曾言明的预设前提。\nA、机器智能是对人类智能的模拟\nB、人类智能是世界上最高智能形态\nC、世界上只有一种形态,就是人类智能\nD、人工智能不止一条发展路径\n3\n【单选题】超越科学家的路径在于()\nA、科学知识\nB、科学技术\nC、人类思想\nD、人工智能\n4【判断题】科学和哲学的区别在于科学解释世界,哲学改变世界。\n5【判断题】图灵测试的价值不在于讨论人类智能与人工智能的性质差异,而在于判别机器是否已经具有智能。\n1.7互动环节\n1【判断题】人类智能的进化有许多方面是机器智能导致的。\n2.1算盘是机器智能吗？\n1\n【单选题】珠算盘起源于()年代。\nA、秦朝\nB、汉朝\nC、唐朝\nD、北宋\n2【判断题】算盘可以算作机器智能的理由是不思维也能运算和由小脑代替大脑计算。\n2.2机器智能的演进\n1\n【单选题】()年由十几位青年学者参与的达特茅斯暑期研讨会上诞生了“人工智能”。\nA、1954\nB、1955\nC、1956\nD、1957\n2\n【单选题】在人工智能的()阶段开始有解决大规模问题的能力。\nA、形成时期\nB、知识应用时期\nC、新神经网络时期\nD、算法解决复杂问题时期\n3\n【单选题】机器人的三定律中第一条是()。\nA、机器人不得伤害人类个体,或者目睹人类个体将遭受危险而袖手不管\nB、机器人必须服从人给予它的命令\nC、机器人要尽可能保护自己的生存。\nD、机器人必须保护人类的整体利益不受伤害\n4【判断题】BP网解决了旅行商问题。\n5【判断题】\n机器智能在21世纪初具备实时感知处理能力。\n2.3ABC时代的机器智能\n1\n【单选题】以下不属于ABC时代基础设施的是()。\nA、大数据\nB、云计算\nC、物联网\nD、移动宽带\n2\n【单选题】ABC时代生产工具的是()。\nA、人工智能\nB、大数据\nC、云计算\nD、物联网\n3【判断题】美国未来学家雷蒙德·库兹韦尔认为“人类纯文明”的终结在2050年。\n4【判断题】强人工智能观点认为有可能制造出真正推理和解决问题的智能机器。"}
{"content2":"一个Python 的 AI Chatbot框架\n建立一个聊天室可以听起来很棒，但它是完全可行的。 IKY是一个内置于Python中的AI动力对话对话界面。 使用IKY，很容易创建自然语言会话场景，无需编码工作。 平滑的UI使得轻松创建和训练机器人的对话，并且随着从与人们的对话中学习而不断变得更聪明。 IKY可以通过将API与该平台集成在您所选择的任何渠道（如Messenger，Slack等）上。\n您不需要成为人工智能的专家来创建具有人工智能的真棒聊天机。 有了这个基本的项目，你可以随时创建一个人工智能动力聊天机。可能有几十个错误。 所以随时可以通过拉请求作出贡献。\nAn AI Chatbot framework built in Python\nBuilding a chatbot can sound daunting, but it’s totally doable. IKY is an AI powered conversational dialog interface built in Python. With IKY it’s easy to create Natural Language conversational scenarios with no coding efforts whatsoever. The smooth UI makes it effortless to create and train conversations to the bot and it continuously gets smarter as it learns from conversations it has with people. IKY can live on any channel of your choice (such as Messenger, Slack etc.) by integrating it’s API with that platform.\nYou don’t need to be an expert at artificial intelligence to create an awesome chatbot that has artificial intelligence. With this basic project you can create an artificial intelligence powered chatting machine in no time.There may be scores of bugs. So feel free to contribute via pull requests.\n项目地址： https://github.com/alfredfrancis/ai-chatbot-framework\n更多资源：http://www.buluo360.com/"}
{"content2":"认知计算代表一种全新的计算模式，它包含信息分析，自然语言处理和机器学习领域的大量创新技术。\nCognnitive computing refers to systems that learn at scale, reason with purpose and interact with humans nautally.\n认知计算是一种学习系统，有规模、有原因、有目的地学会和人类交互。\n认知计算主要分为三个阶段：\n打孔机时代-> 编程时代 -> 认知时代\n代表性的项目：\nIBM watson (2006)，Google大脑(2011)，百度大脑(2011)\n代表人物：\n吴恩达\n和人工智能（Artificial Intelligence）比较：\n人工智能：\nArtificial, Explicity Programmed, Deterministic, Human not involved, Measured by Turing Test or minic humans, 会给出一个确切的结果，是或否。\n认知计算：\nCognitive, Learn and reason, Interactions with human and enviornments, Measured in more practical way, 会通过学习给出一个概率。\n适合的领域：\nLarge Scale, Complex, Interactive, Unstructured data, Probability.\n生命科学，金融，教育，政府，商业智能BI，交通。\n包含的知识：\nNatural Language Processing 自然语言处理\nQuestion Answering Technolog 问答技术\nHigh Performance Compting 高性能计算\nKnowledge Representation and Reasoning 知识表达和理解\nMachine Learning 机器学习\nUnstructured Information Management 非结构化信息管理\n和生物学关联：\n可视化、心理学、神经网络、深度学习"}
{"content2":"不多说，直接上干货！\n引言\n无论是工作还是科研，我们都希望工作既快又好，然而大多数时候却迷失在繁杂的重复劳动中，久久无法摆脱繁杂的事情。\n你是不是曾有这样一种想法：如果我有哆啦A梦的口袋，只要拿出神奇道具就可解当下棘手的问题，无论是考试也好还是工作也罢，啥事都不愁。\n现实生活中往往不会有那么强大的工具，但是如果有一些软件能让工作更有效率，也堪比那神奇口袋了！不知道工作许久的你是否也有一些密不外传的神器呢？小黑这里与大家一起聊聊哪些神奇的效率工具。\n其实，提高效率的工具真有很多，而且在特定场合下，工具可以带来量级的效率提升，一个好工具就能让你6的飞起。只要你像小黑一样热衷于成长，愿意付出一点点的学习成本，你将会找到更多更好玩的神器，工作也将变得更有乐趣。\n让我们见证那些工作中提高效率的工具吧~\nListary是什么？\n如果说Everything的主要作用是解决了Windows的文件打开方式的不变，那么Listary主要是解决了Windows的浏览和保存不便利的问题。\nWIki对解释如下：\nListary是一款用于Windows的文件名定位/搜索辅助软件。它为Windows传统低效的文件打开/保存对话框提供了便捷、人性化的文件（夹）定位方式[5]，同时改善了常见文件管理器中文件夹切换的效率。\n与Everything比较\n许多用户都将Listary与Everything进行比较甚至视为同类软件，实际上它们除了创建索引的方法相似外，实际上几乎没有共同点。例如两者的定位：Everything的定位为替代传统的本地搜索工具，如Google桌面，Listary则为基于搜索的文件定位与操作和文件夹切换辅助工具。\n为什么用Listary\n当我们的程序涉及到大量文件保存工作时，Windows频繁的文件夹点击将会极大的制约你的效率。让我最深有感触的一个情境就是我明明已经打开了我想要保存的文件路径，我还是需要在LabVIEW中打开时一级一级点进去，找到我所想要去的目录。（这里稍微好一些的方法是复制文件路径，然后跳转过去，但是也很少有人用）\nWindows以可视化所见即所得著称，但是其繁琐的操作也为许多实用键盘操作的人看不起。系统虽然不停的升级，但是核心的文档操作方式和方法并没有显著的改善，所以频繁双击、查找、另存都是系统的一个硬伤。为此，许多辅助软件得以出现以提高效率，Listary就是这类软件之一。\n如何使用Listary\n如果你使用的Wifi，可以点击链接，看看现成的Listary使用视频：http://v.youku.com/v_show/id_XNTYyNDAyNDgw.html\n如果你不方便观看，下文小黑将带你了解一下Listary的一些用法与功能，相信不会令你失望的。\n快速定位\n我们一般在移动硬盘查找文件时使用快速定位，有规则的文件夹使用英文字母开头，通过在文件夹内搜索英文字母，以快速的定位所在的文件夹。\nListary进一步强化了这个功能，它不仅仅可以定位当前所在的文件夹，还可以定位整个硬盘中的任何一个文件，只要你输入首字母，即可找到所有具有这个首字母的内容，并且按下Enter即可快速进入。\n全盘搜索\n当我临时需要查找一个文件时，我可以像Everything一样秒搜，一个回车键即可打开我的文件或者跳转到我想要去的目录。虽然搜索功能没有Everything复杂，但是简单的快速切换，Listary更加便捷。\n对于一些中文名称的搜索，可以直接输入中文的首字母搜索，如搜索鲁大师可以写为：lds，这样启动软件也可以快速启动啦\n快速使用右键菜单\n搜索到路径后，可以使用右键-->，快速的实现右键操作\n打开保存强化\n如果你想打开某一个LabVIEW或者其他程序，你需要做的是Open...\n输入自己的项目文件名称\n回车即可打开所需要的项目\n保存功能也是类似，可以通过搜索的方式快速的到达自己想要去的目录\n最近浏览功能，可以帮助你快速找到近期的工作目录，让你在相同文件夹下存储时，也可以快速定位\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"人工智能的浪潮正在席卷全球，诸多词汇时刻萦绕在我们耳边：人工智能（Artificial Intelligence）、机器学习（Machine Learning）、深度学习（Deep Learning）。不少人对这些高频词汇的含义及其背后的关系总是似懂非懂、一知半解，那么他们之间有什么样的联系啦？下面我们来看看：\n人工智能：从概念提出到走向繁荣\n1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念，梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言，或被当成技术疯子的狂想扔到垃圾堆里。直到2012年之前，这两种声音还在同时存在。\n2012年以后，得益于数据量的上涨、运算力的提升和机器学习新算法（深度学习）的出现，人工智能开始大爆发。据领英近日发布的《全球AI领域人才报告》显示，截至2017年一季度，基于领英平台的全球AI（人工智能）领域技术人才数量超过190万，仅国内人工智能人才缺口达到500多万。\n人工智能的研究领域也在不断扩大，图二展示了人工智能研究的各个分支，包括专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等。\n图二 人工智能研究分支\n但目前的科研工作都集中在弱人工智能这部分，并很有希望在近期取得重大突破，电影里的人工智能多半都是在描绘强人工智能，而这部分在目前的现实世界里难以真正实现（通常将人工智能分为弱人工智能和强人工智能，前者让机器具备观察和感知的能力，可以做到一定程度的理解和推理，而强人工智能让机器获得自适应能力，解决一些之前没有遇到过的问题）。\n弱人工智能有希望取得突破，是如何实现的，“智能”又从何而来呢？这主要归功于一种实现人工智能的方法——机器学习。\n机器学习：一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。\n机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。\n深度学习：一种实现机器学习的技术\n深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络），因此越来越多的人将其单独看作一种学习的方法。\n最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。其实有不少想法早年间也曾有过，但由于当时训练数据量不足、计算能力落后，因此最终的效果不尽如人意。\n深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n三者的区别和联系\n机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术。我们就用最简单的方法——同心圆，可视化地展现出它们三者的关系。\n&amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;img src=\"https://pic4.zhimg.com/50/v2-cdbef7f0385b59656eaa9df2a75d890e_hd.jpg\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"614\" data-original=\"https://pic4.zhimg.com/v2-cdbef7f0385b59656eaa9df2a75d890e_r.jpg\"&amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;\n图三 三者关系示意图\n目前，业界有一种错误的较为普遍的意识，即“深度学习最终可能会淘汰掉其他所有机器学习算法”。这种意识的产生主要是因为，当下深度学习在计算机视觉、自然语言处理领域的应用远超过传统的机器学习方法，并且媒体对深度学习进行了大肆夸大的报道。\n深度学习，作为目前最热的机器学习方法，但并不意味着是机器学习的终点。起码目前存在以下问题：\n1. 深度学习模型需要大量的训练数据，才能展现出神奇的效果，但现实生活中往往会遇到小样本问题，此时深度学习方法无法入手，传统的机器学习方法就可以处理；\n2. 有些领域，采用传统的简单的机器学习方法，可以很好地解决了，没必要非得用复杂的深度学习方法；\n3. 深度学习的思想，来源于人脑的启发，但绝不是人脑的模拟，举个例子，给一个三四岁的小孩看一辆自行车之后，再见到哪怕外观完全不同的自行车，小孩也十有八九能做出那是一辆自行车的判断，也就是说，人类的学习过程往往不需要大规模的训练数据，而现在的深度学习方法显然不是对人脑的模拟。\n深度学习大佬 Yoshua Bengio 在 Quora 上回答一个类似的问题时，有一段话讲得特别好，这里引用一下，以回答上述问题：\nScience is NOT a battle, it is a collaboration. We all build on each other's ideas. Science is an act of love, not war. Love for the beauty in the world that surrounds us and love to share and build something together. That makes science a highly satisfying activity, emotionally speaking!\n这段话的大致意思是，科学不是战争而是合作，任何学科的发展从来都不是一条路走到黑，而是同行之间互相学习、互相借鉴、博采众长、相得益彰，站在巨人的肩膀上不断前行。机器学习的研究也是一样，你死我活那是邪教，开放包容才是正道。\n结合机器学习2000年以来的发展，再来看Bengio的这段话，深有感触。进入21世纪，纵观机器学习发展历程，研究热点可以简单总结为2000-2006年的流形学习、2006年-2011年的稀疏学习、2012年至今的深度学习。未来哪种机器学习算法会成为热点呢？深度学习三大巨头之一吴恩达曾表示，“在继深度学习之后，迁移学习将引领下一波机器学习技术”。但最终机器学习的下一个热点是什么，谁又能说得准呢。\n内容来源知乎问题，链接：https://www.zhihu.com/question/57770020/answer/249708509"}
{"content2":"不多说，直接上干货！\n如何自己编译生成Eclipse插件，如hadoop-eclipse-plugin-2.6.0.jar\n一、相关软件的安装和配置\n(一)JDK的安装和配置\nJdk 1.7*安装并配置\n(二)Eclipse的安装和配置\nEclipse的下载、安装和WordCount的初步使用（本地模式和集群模式）\n(三)Ant的安装和配置\n（1）Ant的下载,下载地址如下：\n如果想要下载老版本，可以点击“here”\n选择自己需要的版本，点击下载即可\n（2）解压到一个目录下\n比如在D盘新建一个ant目录，然后把第一步下载的压缩包解压到ant目录下，so easy的！例如如下图所示：\n（3）环境变量的配置\n点击“计算机”——“属性”——“高级系统设置”——“环境变量”。然后新建一个变量名ANT_HOME,变量值是ant的安装路径，如下图：\n配置path路径，即把ant安装目录下的bin目录配置到path路径下。\n（4）cmd测试一下是否配置正确\n点击“开始”——“运行”——“cmd”——“确定”,然后在命令提示符后面输入“ant -version”，当出现ant的版本的时候，就表示ant安装成功。如下图：\n二、编译hadoop-eclipse-plugin插件\n1、首先下载hadoop-eclipse-plugin源代码，地址如下：https://github.com/winghc/hadoop2x-eclipse-plugin\n2、在Windows下，输入cmd，打开命令行窗口，切换到插件安装的对应目录下，比如D:\\hadoop2x-eclipse-plugin-master\\src\\contrib\\eclipse-plugin\n3、执行ant jar -Dversion=2.6.0 -Declipse.home=D:\\Eclipse\\eclipse -Dhadoop.home=D:\\hadoop\\hadoop-2.6.0 ，OK，接下来就将会进行插件的编译\n4、编译成功生成的hadoop-eclipse-plugin-2.6.0.jar在D:\\hadoop2x-eclipse-plugin-master\\build\\contrib\\eclipse-plugin路径下\n到此为止，hadoop-eclipse-plugin插件就编译成功了。赶紧练练吧！\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"我讲EM算法的大概流程主要三部分：需要的预备知识、EM算法详解和对EM算法的改进。\n一、EM算法的预备知识\n1、极大似然估计\n（1）举例说明：经典问题——学生身高问题\n我们需要调查我们学校的男生和女生的身高分布。 假设你在校园里随便找了100个男生和100个女生。他们共200个人。将他们按照性别划分为两组，然后先统计抽样得到的100个男生的身高。假设他们的身高是服从高斯分布的。但是这个分布的均值u和方差∂2我们不知道，这两个参数就是我们要估计的。记作θ=[u, ∂]T。\n问题：我们知道样本所服从的概率分布的模型和一些样本，而不知道该模型中的参数。\n我们已知的有两个：（1）样本服从的分布模型（2）随机抽取的样本  需要通过极大似然估计求出的包括：模型的参数\n总的来说：极大似然估计就是用来估计模型参数的统计学方法。\n（2）如何估计\n问题数学化： （1）样本集X={x1,x2,…,xN} N=100 （2）概率密度：p(xi|θ)抽到男生i（的身高）的概率 100个样本之间独立同分布，所以我同时抽到这100个男生的概率就是他们各自概率的乘积。就是从分布是p(x|θ)的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：\n这个概率反映了，在概率密度函数的参数是θ时，得到X这组样本的概率。 需要找到一个参数θ，其对应的似然函数L(θ)最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做θ的最大似然估计量，记为\n（3）求最大似然函数估计值的一般步骤\n首先，写出似然函数：\n其次，对似然函数取对数，并整理：\n然后，求导数，令导数为0，得到似然方程；\n最后，解似然方程，得到的参数即为所求。\n（4）总结\n多数情况下我们是根据已知条件来推算结果，而极大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。\n2、Jensen不等式\n（1）定义\n设f是定义域为实数的函数，如果对于所有的实数x。如果对于所有的实数x，f(x)的二次导数大于等于0，那么f是凸函数。  Jensen不等式表述如下：      如果f是凸函数，X是随机变量，那么：E[f(X)]>=f(E[X])  。当且仅当X是常量时，上式取等号。\n（2）举例\n图中，实线f是凸函数，X是随机变量，有0.5的概率是a，有0.5的概率是b。X的期望值就是a和b的中值了，图中可以看到E[f(X)]>=f(E[X])成立。         Jensen不等式应用于凹函数时，不等号方向反向。\n二、传统EM算法详述\n1、问题描述\n我们抽取的100个男生和100个女生样本的身高，但是我们不知道抽取的那200个人里面的每一个人到底是从男生的那个身高分布里面抽取的，还是女生的那个身高分布抽取的。 用数学的语言就是，抽取得到的每个样本都不知道是从哪个分布抽取的。 这个时候，对于每一个样本，就有两个东西需要猜测或者估计： （1）这个人是男的还是女的？（2）男生和女生对应的身高的高斯分布的参数是多少？\nEM算法要解决的问题是： （1）求出每一个样本属于哪个分布 （2）求出每一个分布对应的参数\n2、举例说明\n身高问题使用EM算法求解步骤：\n（1）初始化参数：先初始化男生身高的正态分布的参数：如均值=1.7，方差=0.1\n（2）计算每一个人更可能属于男生分布或者女生分布；\n（3）通过分为男生的n个人来重新估计男生身高分布的参数（最大似然估计），女生分布也按照相同的方式估计出来，更新分布。\n（4）这时候两个分布的概率也变了，然后重复步骤（1）至（3），直到参数不发生变化为止。\n3、算法推导\n已知：样本集X={x(1),…,x(m))}，包含m个独立的样本；\n未知：每个样本i对应的类别z(i)是未知的（相当于聚类）；\n输出：我们需要估计概率模型p(x,z)的参数θ；\n目标：找到适合的θ和z让L(θ)最大。\n要使L(θ)最大，我们可以不断最大化下界J，来使得L(θ)不断提高，达到最大值。\n问题：\n什么时候下界J(z,Q)与L(θ)在此点θ处相等？\n根据Jensen不等式，自变量X是常数，等式成立。即：\n由于，则可以得到：分子的和等于c\n在固定参数θ后，使下界拉升的Q(z)的计算公式，解决了Q(z)如何选择的问题。这一步就是E步，建立L(θ)的下界。接下来的M步，就是在给定Q(z)后，调整θ，去极大化L(θ)的下界J。\n4、算法流程\n1）初始化分布参数θ； 重复以下步骤直到收敛：\nE步骤：根据参数初始值或上一次迭代的模型参数来计算出隐性变量的后验概率，其实就是隐性变量的期望。作为隐藏变量的现估计值：\nM步骤：将似然函数最大化以获得新的参数值：\n5、总结\n期望最大算法（EM算法）是一种从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大似然估计方法。\n三、EM算法的初始化研究\n1、问题描述\nEM算法缺陷之一：传统的EM算法对初始值敏感，聚类结果随不同的初始值而波动较大。总的来说，EM算法收敛的优劣很大程度上取决于其初始参数。\n我看了一篇论文：地址：https://yunpan.cn/cqmW9vurLFmDT  访问密码 0e74\n本篇论文采用的方法：采用一种基于网格的聚类算法来初始化EM算法。\n2、基本思想\n基于网格的聚类算法将数据空间的每一维平均分割成等长的区间段, 从而将数据空间分成不相交的网格单元。由于同个网格单元中的点属于同一类的可能性比较大, 所以落入同一网格单元中的点可被看作一个对象进行处理, 以后所有的聚类操作都在网格单元上进行。 因此，基于网格的聚类过程只与网格单元的个数有关, 聚类的效率得到了很大的提高。\n3、算法步骤\n（1）定义：\n（2）相似度：数据对象间的相似性是基于对象间的距离来计算的。\n（3）输入输出：\n（4）算法步骤\n4、总结\n我觉得这篇论文的主要思想应该是这样的：就拿身高举例。它就是首先做一个预处理，将身高在一个范围内（例如1.71至1.74）的分成一个网格，再看这个网格占全部数据的多少，以此判断出该网格为高密度还是低密度，然后循环算出所有网格的，再使用EM算法计算哪些高密度网格，这样会使整个算法收敛的快一些。还有一些其他的论文也是讲的这个。"}
{"content2":"By Kubi Code\n文章目录\n1. 有监督学习和无监督学习的区别\n2. 正则化\n3. 过拟合\n3.1. 产生的原因\n3.2. 解决方法\n4. 泛化能力\n5. 生成模型和判别模型\n6. 线性分类器与非线性分类器的区别以及优劣\n6.1. 特征比数据量还大时，选择什么样的分类器？\n6.2. 对于维度很高的特征，你是选择线性还是非线性分类器？\n6.3. 对于维度极低的特征，你是选择线性还是非线性分类器？\n7. ill-condition病态问题\n8. L1和L2正则的区别，如何选择L1和L2正则\n9. 特征向量的归一化方法\n10. 特征向量的异常值处理\n11. 越小的参数说明模型越简单\n12. svm中rbf核函数与高斯和函数的比较\n13. KMeans初始类簇中心点的选取\n13.1. 选择批次距离尽可能远的K个点\n13.2. 选用层次聚类或者Canopy算法进行初始聚类\n14. ROC、AUC\n14.1. ROC曲线\n14.2. AUC\n14.3. 为什么要使用ROC和AUC\n15. 测试集和训练集的区别\n16. 优化Kmeans\n17. 数据挖掘和机器学习的区别\n18. 备注\n有监督学习和无监督学习的区别\n有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBRT）\n无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)\n正则化\n正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。\n奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。\n过拟合\n如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。\n产生的原因\n因为参数太多，会导致我们的模型复杂度上升，容易过拟合\n权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.\n解决方法\n交叉验证法\n减少特征\n正则化\n权值衰减\n验证数据\n泛化能力\n泛化能力是指模型对未知数据的预测能力\n生成模型和判别模型\n生成模型：由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。（朴素贝叶斯）\n生成模型可以还原联合概率分布p(X,Y)，并且有较快的学习收敛速度，还可以用于隐变量的学习\n判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。（k近邻、决策树）\n直接面对预测，往往准确率较高，直接对数据在各种程度上的抽象，所以可以简化模型\n线性分类器与非线性分类器的区别以及优劣\n如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。\n常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归\n常见的非线性分类器：决策树、RF、GBDT、多层感知机\nSVM两种都有(看线性核还是高斯核)\n线性分类器速度快、编程方便，但是可能拟合效果不会很好\n非线性分类器编程复杂，但是效果拟合能力强\n特征比数据量还大时，选择什么样的分类器？\n线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分\n对于维度很高的特征，你是选择线性还是非线性分类器？\n理由同上\n对于维度极低的特征，你是选择线性还是非线性分类器？\n非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分\nill-condition病态问题\n训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题（这简直是不能用啊）\nL1和L2正则的区别，如何选择L1和L2正则\n他们都是可以防止过拟合，降低模型复杂度\nL1是在loss function后面加上 模型参数的1范数（也就是|xi|）\nL2是在loss function后面加上 模型参数的2范数（也就是sigma(xi^2)），注意L2范数的定义是sqrt(sigma(xi^2))，在正则项上没有添加sqrt根号是为了更加容易优化\nL1 会产生稀疏的特征\nL2 会产生更多地特征但是都会接近于0\nL1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。L1在特征选择时候非常有用，而L2就只是一种规则化而已。\n特征向量的归一化方法\n线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)\n对数函数转换，表达式如下：y=log10 (x)\n反余切函数转换 ，表达式如下：y=arctan(x)*2/PI\n减去均值，乘以方差：y=(x-means)/ variance\n特征向量的异常值处理\n用均值或者其他统计量代替\n越小的参数说明模型越简单\n过拟合的，拟合会经过曲面的每个点，也就是说在较小的区间里面可能会有较大的曲率，这里的导数就是很大，线性模型里面的权值就是导数，所以越小的参数说明模型越简单。\n追加：这个其实可以看VC维相关的东西感觉更加合适\nsvm中rbf核函数与高斯和函数的比较\n高斯核函数好像是RBF核的一种\nKMeans初始类簇中心点的选取\n选择批次距离尽可能远的K个点\n首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的店作为第三个中心店，以此类推，直至选取大k个\n选用层次聚类或者Canopy算法进行初始聚类\nROC、AUC\nROC和AUC通常是用来评价一个二值分类器的好坏\nROC曲线\n曲线坐标上：\nX轴是FPR（表示假阳率-预测结果为positive，但是实际结果为negitive，FP/(N)）\nY轴式TPR（表示真阳率-预测结果为positive，而且的确真实结果也为positive的,TP/P）\n那么平面的上点(X,Y)：\n(0,1)表示所有的positive的样本都预测出来了，分类效果最好\n(0,0)表示预测的结果全部为negitive\n(1,0)表示预测的错过全部分错了，分类效果最差\n(1,1)表示预测的结果全部为positive\n针对落在x=y上点，表示是采用随机猜测出来的结果\nROC曲线建立\n一般默认预测完成之后会有一个概率输出p，这个概率越高，表示它对positive的概率越大。\n现在假设我们有一个threshold，如果p>threshold，那么该预测结果为positive，否则为negitive，按照这个思路，我们多设置几个threshold,那么我们就可以得到多组positive和negitive的结果了，也就是我们可以得到多组FPR和TPR值了\n将这些(FPR,TPR)点投射到坐标上再用线连接起来就是ROC曲线了\n当threshold取1和0时，分别得到的就是(0,0)和(1,1)这两个点。（threshold=1，预测的样本全部为负样本，threshold=0，预测的样本全部为正样本）\nAUC\nAUC(Area Under Curve)被定义为ROC曲线下的面积，显然这个面积不会大于1（一般情况下ROC会在x=y的上方，所以0.5<AUC<1）.\nAUC越大说明分类效果越好\n为什么要使用ROC和AUC\n因为当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动。\nhttp://www.douban.com/note/284051363/?type=like\n测试集和训练集的区别\n训练集用于建立模型,测试集评估模型的预测等能力\n优化Kmeans\n使用kd树或者ball tree(这个树不懂)\n将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可\n数据挖掘和机器学习的区别\n机器学习是数据挖掘的一个重要工具，但是数据挖掘不仅仅只有机器学习这一类方法，还有其他很多非机器学习的方法，比如图挖掘，频繁项挖掘等。感觉数据挖掘是从目的而言的，但是机器学习是从方法而言的。\n备注\n题目主要来源于网络，答案主要来源于网络或者《统计学习方法》，还有自己一小部分的总结，如果错误之处敬请指出\n如果想要了解关于常见模型的东东可以看这篇机器学习常见算法个人总结（面试用）文章"}
{"content2":"截止目前，已经知道了常用的机器学习算法是怎么回事儿、学习的步骤是怎么进行的。但在机器学习的应用背景是多种多样的，做实际工程必须学会如何根据具体的问题评估一个学习模型的好坏，如何合理地选择模型、提取特征，如何进行参数调优。这些也是我以前做模式识别时欠缺的环节，所以在遇到识别率很低的情况时，往往很困惑，不知道该如何改进：到底是应该改进模型改变特征、还是应该增加训练样本数量，到底是应该优化迭代算法，还是应该改变目标函数。通过学习Learning Theory可以得到一些指导性的结论。\n首先，是bias-variance trade off问题。假设训练模型集合H中有k个备选模型，k表示了模型的复杂度，训练集中有m个样本，则式子 Test Error <= Training Error + 2*（log(2k/delta)*1/2m）^0.5 在概率1-delta成立。Training Error是所谓的bias，表征了训练样本跟模型的吻合程度，bias越大，即训练误差越大，训练样本跟模型的吻合程度越低，即出现“欠学习“的情况；2*（log(2k/delta)*1/2m）^0.5 是variance，k越大（即模型的复杂度越大）m越小（即训练样本数量越小）variance越大，模型的推广能力越差，即出现“过学习“的情况。这个结论还有另外一个推论：给定delta和gamma，如果Test Error <= Training Error + 2*gamma 在概率1-delta下成立，则训练样本数量m必须满足:m>=O(1/gamma*log(k/delta))。这个推论表明：为了保证Test Error不至于过大，训练样本的数量m必须同模型复杂度log(k)成正比。实际的模型复杂度一般不用k表示，而是假设模型有d个参数，则每个样本点的维数为d，每个参数为double型，那么k=2^(64d)，上面的条件变为：m>=O(d/gamma*log(1/delta))，即训练样本的数量m同模型参数个数d成正比。上面的结论是针对有限维空间的情况，对于无限维空间，d用H的VC维来代替，可以得到类似的结论。一般来讲，VC维与模型的参数个数d成正比，但在一些特殊情况下，VC维不一定与样本维数有关系，比如支持向量机。bias-variance trade off的过程实际上就是模型选择和特征选择的过程，对于模型选择，最实用的办法就是进行交叉验证，得到Test Error最小的模型；对于特征选择，可采用前向选择或后向选择的方法选择好的特征，删除不好的特征，或者采用滤波的方法，计算每个特征xi与y的互信息量，取互信息量较大的那个特征。\nbias-variance trade off的目的是寻找训练误差和推广能力的平衡，为了达到这个平衡也可以采用加入Regularation的办法。用统计推断的观点看待机器学习问题：不加Regularation对应频率学派的方法，即将参数theta看成一个未知的确定性变量，学习的过程就是求y和x的最大似然对应的theta，加Regularation对应贝叶斯学派的方法，即将参数theta看成一个随机变量，学习的过程就是已知theta的先验概率，求theta的最大后验概率。加入Regularation后，目标函数中加入了lamda*||theta||^2的正则项。对一个回归问题，加入正则项后，拟合的结果会更加平滑，有效地减少了”过拟合“。\n学习了这么多Learning Theory，我们回到笔记开头提出的问题：怎样优化学习算法。首先判别是high bias问题还是high variance问题，判断的方法有两个：一、test error大则是high variance问题、 training error大则是high bias问题；二、增加训练样本数量，看两类error的变化趋势，test error变小，则是high variance问题。增加训练样本数量，减少特征数量可以解决high variance问题，增加特征数量可以解决high bias问题。"}
{"content2":"在过去的几年里，机器学习和人工智能在准确性方面取得了巨大的进步。 然而，受监管的行业（如银行）仍然犹豫不决，往往优先考虑法规遵从性和算法解释的准确性和效率。 有些企业甚至认为这项技术不可信，或者说是危险的。\n在2008年金融危机期间，银行业认识到，他们的机器学习算法是基于有缺陷的假设。 因此，金融体系监管机构决定需要额外的控制措施，并引入了对银行和保险公司进行“模式风险”管理的监管要求。\n银行也必须证明他们理解他们所使用的模型，所以，令人遗憾但是可以理解的是，他们有意地限制了他们技术的复杂性，采用了简单和可解释性高于一切的广义线性模型。\n如果你想建立对机器学习的信任，可以尝试像人一样对待它，问它同样的问题。\n为了信任AI和机器学习提供的建议，来自所有行业的企业需要努力更好地理解它。 数据科学家和博士不应该是唯一能够清楚地解释机器学习模型的人，因为正如AI理论家Eliezer Yudkowsky所说的那样：“到目前为止，人工智能的最大危险在于人们过早地认为他们了解这项技术。\n信任需要人为的方法\n当数据科学家被问及机器学习模型是如何作出决定的时候，他们倾向于使用复杂的数学方程式去解答，使得外行人目瞪口呆，也不知道可以如何信任这个模型。 以与人类决策相同的方式来处理机器学习决策，会不会更有成效？ 正如Udacity联合创始人塞巴斯蒂安·特伦（Sebastian Thrun）曾经说的：“人工智能几乎算得上是一门人文学科。 这实际上是一种理解人类智力和人类认知的尝试。”\n所以，不要用复杂的数学方程来确定信贷员员如何做出决定，而只是简单地问：“贷款申请表上哪些信息对您的决定最重要？或者，“什么值表示风险的高低，以及您是如何决定接受或者拒绝一些特定的贷款申请的？\n可以采用同样的人为方法去确定算法如何做出类似的决定的。例如，通过使用称为特性影响的机器学习技术，可以确定循环效用余额，申请人的收入以及贷款目的是信贷员算法的前三个最重要的信息。\n通过使用称为原因代码的能力，人们可以看出每个贷款申请人的详细资料的估计中最重要的因素，并且通过利用称为部分依赖的技术，可以看到该算法将较高收入贷款申请的风险等级评为较低。\n客观性，可扩展性和可预测性的价值\n通过分析机器如何像人类一样做出决策可以使人类更好地理解人工智能和机器学习，此外，人类还可以通过认识到技术的独特能力来获得对人工智能和机器学习信任，包括：\n解决可信度和数据异常值的问题：传统统计模型通常需要假设数据是如何创建的，数据的背后的过程以及数据的可信度。然而，机器学习通过使用高度灵活的算法来消除这些限制性的假设，这些算法不会给予比它应得的更多的可信度。\n支持现代计算机和海量数据集：与手工流程不同，机器学习不假设世界充满了直线。相反，它会自动调整方程式以查明最佳模式，并测试哪些算法和模式最适合独立验证数据（而不是仅测试所训练的数据）。\n利用缺少的值预测未来：高级机器学习不是要求数小时的数据清理，而是可以构建一个蓝图，优化特定算法的数据，自动检测缺失值，确定哪些算法不适用缺失值，寻找取代缺失值的最佳值，并使用缺失值的存在来预测不同的结果。\n不要怀疑AI或机器学习的建议，让我们通过询问我们要求人类的相同推理问题来更好地理解它们。让我们认识到技术在降低数据异常可信度方面的客观能力，以及为当今海量数据提供可扩展的灵活性的能力。\n也许最重要的是，让我们承认AI和机器学习的能力，通过利用缺少的信息来更好地预测未来的结果。因为虽然技术确实足够强大以至于需要警惕和正式的监管，但如果能够建立一个正确的理解和信任水平，消费者和企业都只会受益。"}
{"content2":"问题描述\nServer Tomcat v7.0 Server at localhost failed to start.\n解决办法\n把你工作空间文件夹下的如下路径打开：\n<workspace-directory>\\.metadata\\.plugins\\org.eclipse.wst.server.core\n删除里面的temp*文件夹，我这有temp0，可能是一个tomcat对应一个temp吧，不太清楚，删除后重启eclipse，tomcat就可以用了。\n重启eclipse或myeclipse就可以了！\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"不多说，直接上干货！\n简单说下，jdk1.8*的下载，见http://www.cnblogs.com/zlslch/p/5658383.html\n双击jdk-8u60-windows-x64.exe运行程序\n欢迎使用Java SE开发工具包8 Update 60的安装向导界面，点击“下一步”\n选择安装可选功能界面，默认安装，安装到      C:\\Program Files\\Java\\jdk1.8.0_60。点击“下一步”\n进度情况显示界面，点击下一步，进行安装，耐心等待，需要几分钟\n目标文件夹安装选择界面，更改到C:\\Program Files\\Java\\jre1.8.0_60，点击“下一步“\n安装状态显示界面，大概等半分钟\n已成功安装界面，点击关闭。在这里，想说的是，一般java开发人员，都需要配套的API文档。\n第二 jdk的配置环境变量\n系统属性界面，在“这台电脑”，右键，属性，高级，环境变量\n默认的环境变量\n在“系统变量”中，设置3属性JAVA_HOME、CLASSPATH、Path（不区分大小写）,若已存在则点击“编辑”，不存在则点击“新建”；\n新建JAVA_HOME指明JDK安装路径，就是刚才安装时所选择的路径C:\\Program Files\\Java\\Jdk1.8.0_60，此路径下包括lib，bin，jre等文件夹（此变量最好设置，因为以后运行tomcat，eclipse等都需要依*此变量）；\n寻找 Path 变量\n在变量值最后输入\n;%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin\n（注意原来Path的变量值末尾有没有;号，如果没有，先输入；号再输入上面的代码）\n新建CLASSPATH 变量\n.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar\n7 检验是否配置成功 运行cmd 输入 java -version （java 和 -version 之间有空格）\n若如图所示 显示版本信息 则说明安装和配置成功。\n结束\n只要是jdk1.8*里的版本，都认为是jdk1.8，不需具体。\nJdk 1.7*安装并配置\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"模型评估与选择\n经验误差与过拟合\n错误率=\\(\\frac{分类错误的样本}{总样本数}\\)\n精度=1-错误率\n学习器的实际预测输出与样本的真实输出之间的差异被称为“误差” 学习器在训练集上的误差为“训练误差”、在新样本上的误差为“泛化误差” 我们的目标是让学习器的泛化误差最小，而实际上因为新样本的不确定，我们只能尽可能地让学习器的训练误差最小。 过拟合是指学习器在通过训练集样本进行训练时，学习能力过于强大，把那些只属于训练集的，但并非是一般化的特征也都学到了；相对于过拟合的概念就是欠拟合。 在现实生活中，对于某一问题，我们有多种可供选择的算法，而每个算法又因不同的参数配置产生出不同的模型。那么如何选择模型呢？其标准自然是选择泛化误差最小的，但我们却无法直接得到泛化误差；同时，训练误差也因为存在过拟合问题从而不适合作为标准。于是我们采取了一种评估方法，即，选用部分样本作为训练集，训练学习器，接下来将训练好的该学习器放在另一部分的样本（测试集）中进行测试，由此得到测试误差。我们将测试误差作为泛化误差的近似，从而对学习器的好坏进行评估。 如何将手头上的数据分为训练集和测试集进行测试，就要采用以下的方法：\n评估方法\n留出法\n将数据集（D）划分成两个互斥的部分，比如有1000个样本的数据集，选用其中的700个作为训练集，剩下的300个用作测试集。（划分数据集时要注意保持数据分布的一致性，即保证训练集和测试集是无差异的）。 之后计算测试误差，比如使用上述的700个样本的数据集D训练模型，将得到的模型在300个样本的测试集中进行测试，发现其将测试集中的90个样本分类错了。就可以得到错误率为\\(\\frac{90}{300}\\)=30%，那么，精度=1-错误率=70% 然后将划分过程随机重复多次，比如进行100次的划分。每次都得到一个错误率，最后留出法就是对这100个错误率进行平均，看所使用的模型的平均错误率有多高。 一般情况下，会将总样本中的\\(\\frac2{3}\\)~\\(\\frac4{5}\\)的样本用作训练集，余下的\\(\\frac1{3}\\)~\\(\\frac1{5}\\)作为测试集。\n交叉验证法\n留出法是划分为2个部分，而交叉法则是将数据D集划分为k个部分（k>2），所以这一方法又被称为“k折交叉验证”。 比如一般会设置k=10，即划分为10个部分，之后就用9个（k-1个）当做训练集，剩下的1个做测试集。将这10份中，每个都作为测试集1次（其余的另9个作为训练集），就会得到10次结果。另外再把原数据集按照不同的划分方式再划分几次，比如5次，这样就会得到50个训练结果（5次×10折）。 交叉验证的一个极端的办法就是将数据集D（共包含m个样本）划分为m个部分，即k=m。这被称为“留一法”，每个子集中只有一个样本。这样训练集（k-1）≈总数据集（m），这样训练出的效果也就跟使用总数据集几乎是一个效果。但是当数据集中样本过大时，这样的计算量就过大了(如有1百万个数据样本就需要训练1百万个模型)。\n自助法\n前两个方法由于保留了部分样本用于测试，因此实际评估的模型所使用的训练集比总数据集小。另外，尽管留一法受训练样本规模变化小，但是计算复杂度高。所以为了减少训练样本规模不同造成的影响，同时高效地进行试验估计，就采用自助法（bootstrapping） 对于包含m个样本的数据集D，使用自助采样法对其采样，产生新的数据集D’。 即，每次随机从D中挑选一个样本，放入D’中，然后将该样本再放回D中，再次从D中随机挑选出一个样本（刚才被挑选出的样本仍有可能再次被选到），放入D’中，不断重复这一过程m次。这样新生成的数据集D’中也就有了和D一样的样本个数。 因为是重复采样，所以仍然会有一些样本一次都没有被选入D’中，这一始终不被采样到的概率是\\((1−\\frac{1}m)m\\),其极限为\\(\\frac{1}e\\)(≈37%)。这样，始终不被采样的这37%的数据就可以用于测试集。 自助法在数据集较小，难以划分训练/测试集时很有用。 但是，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在数据量足够时，更多会采用留出法和交叉验证法。\n调参与最终模型\n大多数学习算法都有参数需要设定。在选择完算法后，对于算法的参数进行调节就是调参。 我们可以对一个算法中所需要的每种参数配置都训练出模型，然后挑选出最好的模型中所使用的参数。 然而现实中，试遍所有参数几乎是不可能的，于是我们就会对每个参数选定一个范围和变化步长进行计算。比如在[0，0.2]的范围内以0.05为步长，测试0，0.05，0.10，0.15，0.20这五个参数。然后从这5个数中选择最合适的。 当算法和参数已经选定好之后，这时需要再用所有的数据m，即用整个数据集D来再度训练模型。\n性能度量\n衡量模型能力的泛化能力的评价标准就是性能度量。在对比不同的模型能力的时候，使用不同的性能度量往往会导致不同的评判结果。\n我们使用均方误差来对学习器的性能进行度量，即，学习器(f)对每个样本(\\(x_i\\))计算得到的数值(\\(f(x_i)\\))与真实数值(\\(y_i\\))进行比较，得到预测值与真实值的误差，然后对每个样本计算得到的误差进行求和，最后再求出均值。\n均方误差=\\[\\frac{1}m\\sum_{i=1}^{m}(f(x_i)-y_i)^2\\]\n错误率与精度\n结合本章开头提到的错误率与精度，对于数据集D：\n分类错误率=\\[\\frac{1}m\\sum_{i=1}^{m}Ⅱ(f(x_i)\\not=y_i)\\]\n(Ⅱ在这里表示指示函数，即它后面的函数取值或者为0，或者为1)\n精度=1-分类错误率=分类错误率=\\[\\frac{1}m\\sum_{i=1}^{m}Ⅱ(f(x_i)=y_i)\\]\n查准率、查全率与F1\n错误率与精度虽然常用，但是并不能满足所有任务需求。有时我们希望知道的是“被当作好西瓜挑出来的西瓜中确实是好西瓜的比例”或者“真正的好瓜中有多少是被伯乐挑出来了”。此时就会出现四种情况。\n真正例（TP：好的西瓜，并且模型也认为是好的西瓜）\n假正例（FP：坏的西瓜，但是模型认为是好的西瓜）\n假反例（FN：好的西瓜，但是模型认为是坏的西瓜）\n真反例（TN：坏的西瓜，并且模型也认为是坏的西瓜）\n于是有：\n查准率Precision=\\[\\frac{真正例（TP）}{真正例（TP）+假正例（FP）}\\]；即，被当作好西瓜挑出来的西瓜中确实是好西瓜的比例\n查全率Recall=\\[\\frac{真正例（TP）}{真正例（TP）+假反例（FN）}\\]；即，真正的好瓜中有多少是被伯乐挑出来了\n查准率与查全率是对矛盾的度量。因为比如查准率高，就意味着模型越保守，只会选择那些非常有把握的瓜，这样漏掉的好瓜也会增多。\n如果我们有多个模型，则可以分别计算每个模型的查准率和查全率来比较模型的好坏。比如，我们有一个模型A来预测西瓜的好坏，对于每个样本（100个）输入，它输出一个计算结果（模型的预测值100个，即每个瓜的得分）。我们将这100的得分从大到小排序，分数越大的瓜就表示模型预测这个瓜更好。这时我们来设定阈值，比如好瓜的得分标准是60分，80分还是90分等等。每设定一个阈值（相当于将上图中间的区分左右两边颜色的竖线从最左一直移到最后），就可以计算出一次该阈值下，模型A的好坏瓜预测情况的查准率和查全率。如下图中的左图所示，横轴是查全率，纵轴是查准率，绿色曲线是模型A的数值变化，蓝线是模型B。判断这两个模型的好坏，只要看哪一个曲线在外面就可以了，因为在外面的曲线上任选一点，得到一对查准率和查全率，里面曲线上具有相同查准率（or查全率）的那一点，它相应的查全率（or查准率）肯定要小于外面曲线上的。\n我们在该图上如果画上函数y=x的直线，那么该直线与模型A（绿色）和B（蓝色）的曲线的交点就是在该曲线上查全率=查准率的点（平衡点 Break-Even Point）。显然可以发现，绿色曲线的平衡点的数值肯定要小于蓝色曲线的（该点上绿色曲线的查准率和查全率都要高于蓝色曲线），于是就可以判断，蓝色曲线（模型B）要优于绿色曲线（模型A）。\n除了比较平衡点之外，更常用的是被称为F1度量的办法来比较不同的模型的优劣。\n\\[F1=\\frac{2×查准率×查全率}{查准率+查全率}=\\frac{2×真正例}{总样本数+真正例-真反例}\\]\n实际上，F1的含义如下\n\\(\\frac1{F1}=\\frac1{2}(\\frac1{查准率}+\\frac1{查全率})\\)，本质上还是平衡点，只是这一公式比算术平均（\\(\\frac{查准率+查全率}2\\)）或者几何平均(\\(\\sqrt{查准率×查全率}\\))更重视最小值。\n不同应用中，对于查准率和查全率的重视程度不同：比如在商品推荐系统中，为了尽可能不打扰用户，推荐顾客更感兴趣的东西，查准率更重要；而在逃犯信息检索系统中，希望少漏掉逃犯，此时查全率更重要。\n于是引进一个参数\\(\\beta\\),得到F1度量的一般形式--\\(F_\\beta\\)。当\\(\\beta\\)>1时，查全率有更大影响，而\\(\\beta\\)<1时查准率有更大影响。\n\\(F_\\beta=\\frac{(1+\\beta^2)×查准率×查全率}{(\\beta^2×查准率)+查全率}\\)\nROC与AUC\n很多学习器是为测试样本产生一个相应的预测值，然后设定一个阈值（截断点），来与生成的预测值进行比较，如果大于阈值则是正例，小于就是反例。例如可将预测值的范围设置在[0.0 1.0]之内，然后设定0.5为阈值，大于0.5的就当是正例。\n在不同任务中可以根据需求来选择截断点的值。我们将预测的值从大到小排列出来，如果我们重视查准率，就可以把截断点的值设得高一些，只选择排名靠前的那些预测值，认为满足这一截断点值的瓜的评分才代表是好瓜；如果重视查全率，就可以把截断点的值设置得低一些。假设西瓜基本上是长得越大越成熟、也越甜，那么如果我们以个头作为西瓜的好坏标准的话，截断点选择的数值越大，那么比截断点值还要个头大的瓜肯定大都是好瓜（查准率高），为了让个头虽然不是很大但也仍然很甜的瓜也被选进来的话（查全率高），我们就需要把截断点的值选择的相对低一点。\n这样，我们对于某一个模型预测出来的值，可以试遍所有的阈值，然后在每个阈值下计算出如下两个指标（这跟上一接所述的查准、查全率曲线非常接近，只是计算时的分母和分子略有不同）：\n真正例率=\\(\\frac{真正例}{真正例+假反例}\\)=\\(\\frac{被模型正确预测出的正例数}{实际情况下的所有正例数}\\)=查全率（真正的好瓜中有多少是被伯乐挑出来了）\n假正例率=\\(\\frac{假正例}{假正例+真反例}\\)=\\(\\frac{实际情况下是反例的被模型误以为是正例的个数}{实际情况下的所有反例数}\\)=坏瓜中间被当作好瓜挑出来的比例\n我们肯定是希望得到一个真正例率高同时假正例率低的模型，为了更直观地看到不同模型的这两个指标的的差别，我们将每对(假正例率，真正例率)值的当作一个坐标（x,y）画出来，即以真正例率为纵轴坐标、假正例率为横轴坐标绘制。这样就会形成一个曲线，我们把这条曲线称之为ROC（Receiver Operating Characteristic）曲线---ROC曲线本身也是二战时候用于检测敌机的雷达分析技术，后来多用于生物、心理学、制药等领域，是比如判断几种不同的药物的好坏的评估办法之一。\n参看上图的右边的两条曲线，很明显，蓝色曲线代表的模型较好，因为和绿色的相比，当他们假正例率相同的情况下，蓝色曲线的真正例率都是要高的。为了在量上对这两个模型进行比较，我们使用AUC（Area Under ROC curve）计算面积，即用积分计算出每条ROC曲线下方区域、和横纵轴形成的类似扇形区域的面积，然后面积越大的就表示该模型越好。\n代价敏感错误率与代价曲线\n把坏瓜错划分成好瓜，或者好瓜错划分成坏瓜虽然都是错误划分，但是其造成的后果会不同。好瓜当成坏瓜只是扔掉了一只瓜，但是坏的当成好的吃了可能会吃坏肚子上医院，成本更高。我们将不同类型错误所造成的不同损失的情况称为“非均等代价（unequal cost）”\n对于这种代价，记为\\(cost_{ij}\\)，即表示将第i类样本预测为第j类样本的代价。以二分类任务为例（第0类和第1类），则会有两种情况，分别是\\(cost_{01}\\)和\\(cost_{10}\\)，假设0类表示坏瓜，1类表示好瓜，那么显然\\(cost_{01}\\>>cost_{10}\\)\n在非均等代价的情况下，我们不仅希望错误率低，还希望总体代价也很低。即\\(cost_{01}\\)与\\(cost_{10}\\)的和的均值很低。这里我们采用一个指标：\n代价敏感（cost-sensitive）=\\(\\frac1{m}(\\sum_{x_{i}\\in{D^+}}Ⅱ(f(x_i)\\not=y_i)×cost_{01}+\\sum_{x_{i}\\in{D^-}}Ⅱ(f(x_i)\\not=y_i)×cost_{10})\\)\n其中\\(D^+D^-\\)分别表示数据集D中正例子集和反例子集\n比较检验\n虽然有了性能度量方法，但是我们却并不能直接通过比较不同模型的性能度量值的大小来决定模型的好坏。因为\n1）实验评估得到的是测试集上的性能，这与我们希望比较的泛化性能未必一致\n2）测试集上的性能与测试集本身的选择有关，不同大小的测试集得到的结果会不同，而且相同大小的测试集中如果测试样例不同，结果也可能会不同\n3）很多机器学习算法本身有随机性，即便在同一测试集上使用相同的参数设置运行，每次运行得到的记过也会不同。\n所以我们使用统计假设检验对学习器的性能进行比较\n假设检验\n假设是指对学习器泛化错误率分布的猜想。我们需要根据测试错误率来推断出与其接近的泛化错误率。\n我们希望知道的是泛化错误率\\(\\epsilon\\)（=学习器在一个样本上犯错的概率），测试错误率为\\(\\widehat{\\epsilon}\\)（=我们从测试样本中实际得到的犯错概率）\nP\\((\\widehat{\\epsilon},\\epsilon)\\)\\(=\\)\\(\\begin{pmatrix}m\\\\\\widehat{\\epsilon}×m\\end{pmatrix}\\epsilon^{\\widehat{\\epsilon}×m}(1-\\epsilon)^{m-{\\widehat{\\epsilon}×m}}\\)\n这正是一个二项式分布，即表示在m次事件中，碰巧发生\\(\\widehat{\\epsilon}×m\\)次情况的概率。假设我们有一个模型来预测一张图片是男是女，模型用的是根据头发长短来判断的办法，一般情况下模型都是预测准确的（短发为男、长发为女），但是总体而言有3%（泛化错误率）的概率会预测错（因为有时男生也会是长发）。因为我们每次测试的样本有限，可能碰巧在测试的样本集中间混入了比较多的长头发的男生，于是模型就很容易预测错，出现了更高的例如5%的错误概率（测试错误率）。上述公式即表示样本错误率应该为3%概率时，但因为样本选择的问题以致于我们观测到了错误率为5%的情况的概率。\n假设我们有10个样本，并假定泛化错误率为0.3，那么观测到3个样本被错误分类的可能性会很大，而错误分类出4、5、6个甚至更多的概率会比较小。此时我们的假设检验即为检验“泛化错误率是否不大于0.3”这一判断。于是对错误分类出4个、5个...10个等7种情况的概率进行求和（\\(\\alpha=\\sum_{i=4}^{10}P_i\\)），一般情况下会选择0.05或0.1作为显著度标准，看超出3个分类错误的情况的概率值和\\(\\alpha\\)是否大于0.05或0.1，如果大于则拒绝假设，认为返回错误率大于0.3；小于的话则接受假设。\n在对单个学习器的泛化性能的假设进行检验时,很多时候我们并非只进行一次留出法估计，而是重复多次留出法或是交叉验证法进行多次训练或测试。这样我们就会得到多个测试错误率。首先假设我们认为泛化错误率是一个值，即\\(\\epsilon\\)，然后实际上我们重复计算得到了k个测试错误率，\\(\\widehat{\\epsilon}_1,\\widehat{\\epsilon}_2,...,\\widehat{\\epsilon}_k\\),我们对这k个测试错误率求出平均值\\(\\mu\\)和方差\\(\\sigma^2\\)，考虑到这k个测试错误率可以看作是泛化错误率\\(\\epsilon\\)的独立采样，于是变量\\(\\tau_t=\\frac{\\sqrt{k}(\\mu-\\epsilon)}{\\sigma}\\)就服从自由度为k-1的t分布。\n然后我们使用t-test来检验我们假定的泛化错误率\\(\\epsilon\\)和得到的平均测试错误率\\(\\mu\\)是否相同，即“\\(\\epsilon\\)\\(=\\)\\(\\mu\\)”是否成立。在\\(1-\\alpha\\)概率内观测到的最大错误率即临界值（比如0.3）。如果平均错误率与我们认为的泛化错误率之差（\\(|\\mu-\\epsilon|\\)）在临界值范围内则不能拒绝\\(\\mu=\\epsilon\\)的假设。此时认为\\(\\epsilon\\)等于平均值\\(\\mu\\)。否则可拒绝该假设（因为概率极低的事件频繁发生了），认为泛化错误率明显不应该是我们设定的\\(\\epsilon\\)（比如0.3）这个值。\n交叉验证t检验（两个模型的比较，使用交叉法时）\n以上是对单个学习器泛化性能的假设进行检验，但更多情况下我们需要对不同的学习器性能进行比较，交叉验证t检验即是比较方法之一。\n对于两个学习器A和B，如果他们的性能相同，则他们的测试错误率应该相同，即，\\(\\epsilon_i^A=\\epsilon_i^B\\)。我们可以使用成对t检验（paired t-tests）进行检验，对k折交叉验证产生的k对测试错误率求差，\\(\\Delta_i=\\epsilon_i^A-\\epsilon_i^B\\),然后看该差是否显著不等于零。\n然而，通常情况下由于样本有限，使用交叉验证等实验估计方法时，不同轮次的训练集会有一定程度的重叠，这导致测试错误率实际上并不独立、产生过高估计假设成立的概率。为了缓解这一问题，可采用“5×2交叉验证”。即做5次2折交叉验证。在每次2折交叉验证之前随机将数据打乱，是的5次交叉验证中的数据划分不重复。这里的一个技巧是，用于t检验的平均值\\(\\mu\\)，并不是5×2次计算的数值的平均值，而是只使用第1折交叉验证时模型A和模型B的差值来做平均，即\\(\\mu=0.5(\\Delta_1^1+\\Delta_1^2)\\),由此缓解测试错误率的非独立性。但是方差计算时使用了每次2折实验的结果。然后用自由度为5的t分布进行统计检验。\nMcNemar检验（两个模型的比较，使用留一法时）\nMcNemar检验是指这样一种检验：比如全班20个人的考试中\n第一次考试结果为：7人及格、13人不及格\n第二次考试结果为：14人及格、6人不及格\n那么第二次考试结果是否比第一次好呢？如果采用Fisher检验的话，这两次考试是无区别的，但是如果我们仔细看数据，会发现如下结果：\n两次考试都及格和都不及格的分别是6人和5人。除了他们之外，\n第一次不及格但是第二次及格的：8人（成绩变好）\n第一次及格但是第二次不及格的：1人（成绩变坏）\n由此可以发现，实际上第二次考试结果是比第一次要好的，McNemar检验就是忽略掉那些两次考试没有变化的人，而专注有变化的人（8人变好、1人变坏）之间是否是有差异的。这一检验的本质相当于二项式检验（是实际计算中采用卡方检验），等同于判断9次掷硬币后出现了8次正面、1次反面的情况是否是正常的。\n于是对于在二分类问题下，使用留出法的两个模型A和B进行比较检验时，就是忽略那些在两个模型下都同时判断为正确或错误的那些样本，比较在A模型中判断为正确但是B模型判断为错误的样本\\(e_{01}\\)和**B模型中判断为正确但是A模型判断为错误的样本\\(e_{10}\\)$*的数量是否相等。此时统计量服从自由度为1（k=1）的卡方分布，所以进行卡方检验即可。\nFriedman检验和Nemenyi检验（多个模型的比较）\n对于多个模型本来可以采用方差分析进行比较检验，但是因为不能保证样本能满足正态分布，所以采用Friedman检验。\n具体比如有A、B、C三个模型，以及多个数据集（D1、D2、D3、D4）。\n这里对于每个数据集，对A、B、C这三个模型的好坏进行排名（第1、2、3名），然后比较三个模型在所有数据集下的排名的平均值（比如算法A在四个数据集下排名都是第一，那么其均值也是第一）。要检验的假设就是这三个模型的排名均值是否是相同的。\n进一步如果检验结果发现“这三个算法的性能相同”的假设不成立，那么就意味着这三者的性能显著不同。于是采取后续检验（post-hoc test）来进一步区分各算法。\n偏差与方差\n对于学习算法除了通过实验估计其泛化性能外，人们往往还希望了解它为什么具有这样的性能。我们可以把泛化误差分解为如下的公式情况。\n\\(泛化误差=bisa^2(x)+var(x)+\\epsilon^2\\)\n其中的bias表示偏差，即我们期望学习算法所计算出的输出与真实结果的偏离程度（代表的是算法本身的拟合能力）；var表示方差，即使所使用的训练集大小都是相同的，但如果训练集不同（或者说变动），也会导致学习性能发生变化（代表的是数据扰动带来的影响）；\\(\\epsilon\\)表示噪声，表达了当前任务下任何学习算法所能达到的期望泛化误差的下限（代表的是学习问题本身的难度）。\n偏差与方差是相互冲突的，当训练不足、学习器的拟合能力还不强时，训练数据的扰动不足以让学习器产生显著变化，此时偏差主导了泛化错误率；随着训练加深，学习器的拟合能力增强，方差将主导泛化误差；等到充分训练之后，学习器的拟合能力已经非常强，训练数据的任何轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局性的特性被学习器学到了的话，就会出现过拟合。"}
{"content2":"（原文：）\nThe Difference Between AI, Machine Learning, and Deep Learning?\n（译文：）\n人工智能 、 机器学习 和 深度学习的区别？\n作者：cleaner\n链接：https://www.zhihu.com/question/57770020/answer/154211072\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}
{"content2":"声明：\n机器学习系列主要记录自己学习机器学习算法过程中的一些参考和总结，其中有部分内容是借鉴参考书籍和参考博客的。\n目录：\n什么是关联规则\n关联规则中的必须知道的概念\n关联规则的实现过程\n关联规则的核心点——如何生成频繁项集（Apriori算法）\n关联规则的核心点——如何生成频繁项集（FP-Growth算法）\n实际使用过程中需要注意的地方\n关联规则总结与课后作业\n参考文献\n一、什么是关联规则\n所谓数据挖掘就是以某种方式分析源数据，从中发现一些潜在的有用的信息，即数据挖掘又可以称作知识发现。而机器学习算法则是这种“某种方式”，关联规则作为十大经典机器学习算法之一，因此搞懂关联规则（虽然目前使用的不多）自然有着很重要的意义。顾名思义，关联规则就是发现数据背后存在的某种规则或者联系。\n举个简单的例子（尿布和啤酒太经典）：通过调研超市顾客购买的东西，可以发现30%的顾客会同时购买床单和枕套，而在购买床单的顾客中有80%的人购买了枕套，这就存在一种隐含的关系：床单→枕套，也就是说购买床单的顾客会有很大可能购买枕套，因此商场可以将床单和枕套放在同一个购物区，方便顾客购买。\n一般，关联规则可以应用的场景有：\n优化货架商品摆放或者优化邮寄商品的目录\n交叉销售或者捆绑销售\n搜索词推荐或者识别异常\n二、概念\n项目：交易数据库中的一个字段，对超市的交易来说一般是指一次交易中的一个物品，如：牛奶\n事务：某个客户在一次交易中，发生的所有项目的集合：如｛牛奶，面包，啤酒｝\n项集：包含若干个项目的集合（一次事务中的），一般会大于0个\n支持度：项集｛X，Y｝在总项集中出现的概率（见下面的例子）\n频繁项集：某个项集的支持度大于设定阈值（人为设定或者根据数据分布和经验来设定），即称这个项集为频繁项集。\n置信度：在先决条件X发生的条件下，由关联规则｛X->Y ｝推出Y的概率（见下面的例子）\n提升度：表示含有X的条件下同时含有Y的概率，与无论含不含X含有Y的概率之比。\n支持度和提升度示例：\n假如有一条规则：牛肉—>鸡肉，那么同时购买牛肉和鸡肉的顾客比例是3/7，而购买牛肉的顾客当中也购买了鸡肉的顾客比例是3/4。这两个比例参数是很重要的衡量指标，它们在关联规则中称作支持度（support）和置信度（confidence）。对于规则：牛肉—>鸡肉，它的支持度为3/7，表示在所有顾客当中有3/7同时购买牛肉和鸡肉，其反应了同时购买牛肉和鸡肉的顾客在所有顾客当中的覆盖范围；它的置信度为3/4，表示在买了牛肉的顾客当中有3/4的人买了鸡肉，其反应了可预测的程度，即顾客买了牛肉的话有多大可能性买鸡肉。其实可以从统计学和集合的角度去看这个问题， 假如看作是概率问题，则可以把“顾客买了牛肉之后又多大可能性买鸡肉”看作是条件概率事件，而从集合的角度去看，可以看下面这幅图：\n上面这副图可以很好地描述这个问题，S表示所有的顾客，而A表示买了牛肉的顾客，B表示买了鸡肉的顾客，C表示既买了牛肉又买了鸡肉的顾客。那么C.count/S.count=3/7，C.count/A.count=3/4。\n提升度示例：\n1000名顾客，购买年货，A组有500人购买茶叶，有450人购买咖啡；B组有0人购买茶叶，有450人购买咖啡。\n购买茶叶\n购买咖啡\nA组（500人）\n500\n450\nB组（500人）\n0\n450\n茶叶->咖啡的支持度＝450/1000=45%\n茶叶->咖啡的置信度＝450/500=90%\n茶叶->咖啡的提升度＝90%／90%＝1\n说明：\n（1）由于lift(茶叶X->咖啡Y)＝1，所以说明X与Y相互独立，即是否有X对于Y的出现没有影响。虽然支持度和置信度都高，但它们之间没有必然的关联关系。\n（2）满足最小支持度和最小置信度的关联关系叫做强关联关系\n如果lift>1，叫做有效的强关联关系，\n如果lift<=1，叫做无效的强关联关系\n特别的如果lift（X->Y）＝1，则称X与Y相互独立\n三、实现过程\n从以上的分析可以得知，关联规则是从事务集合中挖掘出这样的关联规则｛X->Y｝：它的支持度和置信度要大于最小阈值（minSup，minConf），当然这个最小阈值是由用户指定的，可以根据数据分布和经验；同时他的提升度最好是大于1的（具体值根据实际情况设定，例如：3、5均可），即是有效强关联规则。\n使用关联规则的过程主要包含以下三个步骤：\n（1）数据筛选，首先对数据进行清洗，清洗掉那些公共的项目，比如：热门词，通用词（此步依据具体项目而定）\n（2）根据支持度（support），从事务集合中找出频繁项集（使用算法：Apriori算法，FP-Growth算法）\n（3）根据置信度（confidence），从频繁项集中找出强关联规则（置信度阈值需要根据实验或者经验而定）\n（4）根据提升度（lift），从强关联规则中筛选出有效的强关联规则（提升度的设定需要经过多次试验确定）\n四、如何生成频繁项集－Apriori算法［1］\n关联规则中，比较关键的两个点是：（1）三种阈值的设定（2）如何找出频繁项集。\n本节主要讨论如何解决寻找频繁项集的问题，目前主要有两种算法：（1）Apriori算法（2）FP-Growth算法，下面分别介绍一下这两种算法。\n（1）算法原理\n它主要利用了向下封闭属性：如果一个项集是频繁项目集，那么它的非空子集必定是频繁项目集。它先生成1-频繁项目集，再利用1-频繁项目集生成2-频繁项目集。。。然后根据2-频繁项目集生成3-频繁项目集。。。依次类推，直至生成所有的频繁项目集，然后从频繁项目集中找出符合条件的关联规则。\n（2）生成频繁项集过程\n它的原理是根据k-频繁项目集生成（k+1）-频繁项目集。因此首先要做的是找出1-频繁项目集，这个很容易得到，只要循环扫描一次事务集合统计出项目集合中每个元素的支持度，然后根据设定的支持度阈值进行筛选，即可得到1-频繁项目集。下面证明一下为何可以通过k-频繁项目集生成（k+1）-频繁项目集：（下面证明如何从K-频繁项集生成k+1频繁项集）\n假设某个项目集S={s1，s2...，sn}是频繁项目集，那么它的（n-1）非空子集{s1，s2，...sn-1}，{s1，s2，...sn-2，sn}...{s2，s3，...sn}必定都是频繁项目集，通过观察，任何一个含有n个元素的集合A={a1，a2，...an}，它的（n-1）非空子集必行包含两项{a1，a2，...an-2，an-1}和 {a1，a2，...an-2，an}，对比这两个子集可以发现，它们的前（n-2）项是相同的，它们的并集就是集合A。对于2-频繁项目集，它的所有1非空子集也必定是频繁项目集，那么根据上面的性质，对于2-频繁项目集中的任一个，在1-频繁项目集中必定存在2个集合的并集与它相同。因此在所有的1-频繁项目集中找出只有最后一项不同的集合，将其合并，即可得到所有的包含2个元素的项目集，得到的这些包含2个元素的项目集不一定都是频繁项目集，所以需要进行剪枝。剪枝的办法是看它的所有1非空子集是否在1-频繁项目集中，如果存在1非空子集不在1-频繁项目集中，则将该2项目集剔除。经过该步骤之后，剩下的则全是频繁项目集，即2-频繁项目集。依次类推，可以生成3-频繁项目集。。直至生成所有的频繁项目集。\n（3）生成强关联规则\n得到频繁项目集之后，则需要从频繁项目集中找出符合条件的关联规则。最简单的办法是：遍历所有的频繁项目集，然后从每个项目集中依次取1、2、...k个元素作为后件，该项目集中的其他元素作为前件，计算该规则的置信度进行筛选即可。这样的穷举效率显然很低。假如对于一个频繁项目集f，可以生成下面这样的关联规则：（f-β）—>β，那么这条规则的置信度=f.count/(f-β).count\n（下面证明如何生成强关联规则，即先生成小后件的，再根据小后件依次生成大后件，因为假设该规则是强关联规则，则（f-βsub）—>βsub也是强关联规则）\n根据这个置信度计算公式可知，对于一个频繁项目集f.count是不变的，而假设该规则是强关联规则，则（f-βsub）—>βsub也是强关联规则，其中βsub是β的子集，因为(f-βsub).count肯定小于(f-β).count。即给定一个频繁项目集f，如果一条强关联规则的后件为β，那么以β的非空子集为后件的关联规则都是强关联规则。所以可以先生成所有的1-后件（后件只有一项）强关联规则，然后再生成2-后件强关联规则，依次类推，直至生成所有的强关联规则。\n（4）举例说明\n下面举例说明Apiori算法的具体流程：\n假如有项目集合I={1，2，3，4，5}，有事务集T：\n1,2,3 1,2,4 1,3,4 1,2,3,5 1,3,5 2,4,5 1,2,3,4\n设定minsup=3/7，misconf=5/7。\n首先：生成频繁项目集：\n1-频繁项目集：{1}，{2}，{3}，{4}，{5}\n2-频繁项目集：\n根据1-频繁项目集生成所有的包含2个元素的项目集：任意取两个只有最后一个元素不同的1-频繁项目集，求其并集，由于每个1-频繁项目集元素只有一个，所以生成的项目集如下：\n{1，2}，{1，3}，{1，4}，{1，5}\n{2，3}，{2，4}，{2，5}\n{3，4}，{3，5}\n{4，5}\n计算它们的支持度，发现只有{1，2}，{1，3}，{1，4}，{2，3}，{2，4}，{2，5}的支持度满足要求，因此求得2-频繁项目集：\n{1，2}，{1，3}，{1，4}，{2，3}，{2，4}\n3-频繁项目集：\n因为{1，2}，{1，3}，{1，4}除了最后一个元素以外都相同，所以求{1，2}，{1，3}的并集得到{1，2，3}， {1，2}和{1，4}的并集得到{1，2，4}，{1，3}和{1，4}的并集得到{1，3，4}。但是由于{1，3，4}的子集{3，4}不在2-频繁项目集中，所以需要把{1，3，4}剔除掉。然后再来计算{1，2，3}和{1，2，4}的支持度，发现{1，2，3}的支持度为3/7 ，{1，2，4}的支持度为2/7，所以需要把{1，2，4}剔除。同理可以对{2，3}，{2，4}求并集得到{2，3，4} ，但是{2，3，4}的支持度不满足要求，所以需要剔除掉。\n因此得到3-频繁项目集：{1，2，3}。\n到此频繁项目集生成过程结束。注意生成频繁项目集的时候，频繁项目集中的元素个数最大值为事务集中事务中含有的最大元素个数，即若事务集中事务包含的最大元素个数为k，那么最多能生成k-频繁项目集，这个原因很简单，因为事务集合中的所有事务都不包含（k+1）个元素，所以不可能存在（k+1）-频繁项目集。在生成过程中，若得到的频繁项目集个数小于2，生成过程也可以结束了。\n现在需要生成强关联规则：\n这里只说明3-频繁项目集生成关联规则的过程：对于集合{1，2，3}\n1-后件的关联规则：\n（1，2）—>3，   置信度=3/4\n（1，3）—>2，　置信度=3/5（置信度不满足要求，所以剔除掉）\n（2，3）—>1      置信度=3/3\n因此得到1后件的集合{1}，{3}\n2-后件的关联规则（根据1－后件集合）\n2—>1，3       置信度=3/5不满足要求，所以对于3-频繁项目集生成的强关联规则为：（1，2）—>3和（2，3）—>1。\n至此，Apriori算法完成\n（当然实际项目中还需要去验证生成的强关联规则是否满足提升度要求，即是否是有效强关联规则）。\n五、如何生成频繁项集－FP-Growth算法［4］\nApriori算法是关联规则的基本算法，很多用于发现关联规则的算法都是基于Apriori算法，但Apriori算法需要多次访问数据库，具有严重的性能问题。FP-Growth算法只需要两次扫描数据库，相比于Apriori减少了I/O操作，克服了Apriori算法需要多次扫描数据库的问题。本文采用如下的样例数据\nA;B;E; B;D; B;C; A;B;D A;C; B;C; A;C; A;B;C;E; A;B;C;\n（1）FP-Growth生成FP-Tree\nFP-Growth算法将数据库中的频繁项集压缩到一颗频繁模式树中，同时保持了频繁项集之间的关联关系。通过对该频繁模式树挖掘，得到频繁项集。其过程如下：\n第一次扫描数据库，产生频繁1项集，并对产生的频繁项集按照频数降序排列，并剪枝支持数低于阀值的元素。处理后得到L集合,\n第二次扫描数据库，对数据库的每个交易事务中的项按照L集合中项出现的顺序排序，生成FP-Tree（.1）。\n.1 FT－Tree\n.2  生成fp-tree的步骤\n（2）从FP-Tree挖掘频繁项集\n从FP-Tree重可以挖掘出频繁项集，其过程如下：\n.3  频繁项集挖掘过程\n从频繁1项集链表中按照逆序开始，链表可以追溯到每个具有相同项的节点。\n从链表中找到项“E”，追溯出FP-Tree中有两个带“E”的节点，由这两个节点分别向上（parent）追溯，形成两条模式：<E,C,A,B;1>,<E,A,B;1>.\n由这两条模式得到项“E”的条件模式<A,B;2>.\n根据条件模式，得到项“E”的频繁项集(不包含频繁1项集)：<E,A;2>,<E,B;2>,<E,A,B;2>\n然后一次得到项“D”，“C”，“A”。\n（3）找出强关联规则\n同第四节\n（4）找出有效的强关联规则\n同第四节\n至此，FP－Growth算法生成频繁项集已经结束。\n六、注意点\n（1）三个阈值点需要经过对此实验或者经验才能找到合适的阈值\n（2）关联规则与word2vec在哪些场景有着共性和不同需要验证（需要研究word2vec实现原理后再下结论）\n（3）数据集需要某些处理后，也许效果会有提升（同事的经验）\n七、总结\n第一次，“摘抄”整理别人的博客来放在自己的博客，算是一个小小的开始的，其实很多东西只有自己去真正的学习，思考，整理才有收获。\n机器学习算法与模型是一个很有意思的东西，很多东西不去看自己完全想象不到还能这么玩，然而看了就是看了，跟用了还是有很大差别，希望以后能够在工作中将自己看的一些东西用上去，多整理，这样效果会比较好。\n虽然我是一个从网络方向转过来的“出家”人，但是这都不是事。我觉得看的多了，整理的多了，其实学起来还是有套路可以遵循的。\n八、参考文献\n［1］http://www.cnblogs.com/dolphin0520/archive/2012/10/29/2733356.html\n［2］http://blog.sina.com.cn/s/blog_4d8d6303010009kb.html\n［3］http://www.360doc.com/content/15/0611/19/25802092_477451393.shtml\n［4］http://westerly-lzh.github.io/cn/2015/08/DM002-FP-Tree/\n［5］http://www.bjt.name/2013/09/association-rules\n［6］http://blog.csdn.net/rav009/article/details/8985322\n［7］http://blog.csdn.net/rav009/article/details/8979249"}
{"content2":"在此动手实践中，我们将在Azure机器学习Studio中一步步地开发预测分析模型，首先我们从UCI机器学习库的链接下载普查收入数据集的样本并开始动手实践： http://archive.ics.uci.edu/ml/datasets/Census+Income. 然后我们在此数据集上开发和训练预测分析模型，并将预测分析模型作为web服务发布至云端给其他程序调用，整个过程大致包含下几个步骤：\n下载、处理和上传收入普查的数据集；\n创建一个新的Azure机器学习实验；\n训练和评价一个预测模型；\n从公共资源库下载数据集\n在开发预测分析收入水平模型时，我们使用UCI 机器学习资源库的成人收入普查数据。数据集地下载链接为http://archive.ics.uci.edu/ml/datasets/Census+Income。该网站包含下载数据文件的链接，您可将adult.data数据文件下载到本地计算机。此数据集的格式以逗号分隔。另外，该网站还包含了在此数据集中的 15 个属性信息，在上传数据至实验之前我们使用此信息作为创建数据表的列标题。\n现在，用 Microsoft Excel 或任何其他电子表格工具中打开 adult.data 文件，并为其添加网站中属性列表的详细信息，这些信息如下列出。注意，其中的一部分属性值为连续的，因为它们以数值的形式表现，另一部分则为其选项值列表。\n年龄（age），连续值\n工作种类（Workclass）个人（Private）, 无限责任公司（Self-emp-not-inc）, 有限责任公司（Self-emp-inc）, 联邦政府（Federal-gov）, 地方政府（ Local-gov）, 州政府（State-gov）, 无薪人员（Without-pay）, 无工作经验人员（Never-worked）离散值\nFnlwgt连续值\n教育情况（Education） Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool ）离散值\n受教育年限（Education-num），连续值\n婚姻状况（Marital-status） 已婚（Married-civ-spouse），离婚（Divorced），未婚（Never-married），离异（Separated），丧偶（Widowed），已婚配偶缺席（Married-spouse-absent）、 再婚（Married-AF-spouse），离散值\n职业情况（Occupation）技术支持（Tech-support），维修工艺（Craft-repair），服务行业（Other-service）、 销售（Sales）、 执行管理（Exec-managerial）、 专业教授（Prof-specialty），清洁工（Handlers-cleaners），机床操控人员（Machine-op-inspct）、 行政文员（Adm-clerical）、 养殖渔业（Farming-fishing）、 运输行业（Transport-moving），私人房屋服务（Priv-house-serv），保卫工作（Protective-serv）， 武装部队（Armed-Forces）职业情况，离散值\n亲属情况（Relationship）妻子（Wife），子女（Own-child），丈夫（Husband），外来人员（Not-in-family）、 其他亲戚（Other-relative）、 未婚（Unmarried），离散值\n种族肤色（Race）白人（White），亚洲太平洋岛民（Asian-Pac-Islander），阿米尔-印度-爱斯基摩人（Amer-Indian-Eskimo）、 其他（Other），黑人（Black）离散值\n性别（Sex ）男性（Female）,女性（ Male），离散值\n资本盈利（Capital-gain ）连续值\n资本损失（Capital-loss） ，连续值\n每周工作时间（Hours-per-week ），连续值\n国籍（Native-country ）美国（United-States）、 柬埔寨（Cambodia）、 英国（England），波多黎各（Puerto-Rico），加拿大（Canada），德国（Germany），美国周边地区（关岛-美属维尔京群岛等）（Outlying-US(Guam-USVI-etc)），印度（India）、 日本（Japan）、 希腊（Greece）、 美国南部（South）、 中国（China）、 古巴（Cuba）、 伊朗（Iran）、 洪都拉斯（Honduras），菲律宾（Philippines）、 意大利（Italy）、 波兰（Poland）、 牙买加（Jamaica）、 越南（Vietnam）、 墨西哥（Mexico）、 葡萄牙（Portugal）、 爱尔兰（Ireland）、 法国（France）、多米尼加共和国（Dominican-Republic）、 老挝（Laos）、 厄瓜多尔（Ecuador）、 台湾（Taiwan）、 海地（Haiti）、 哥伦比亚（Columbia）、 匈牙利（Hungary）、 危地马拉（Guatemala）、 尼加拉瓜（Nicaragua）、苏格兰（Scotland）、 泰国（Thailand）、 南斯拉夫（Yugoslavia），萨尔瓦多（El-Salvador）、 特立尼达和多巴哥（Trinadad&Tobago）、 秘鲁（Peru），香港（Hong），荷兰（Holland-Netherlands）离散值\n收入 （incom） >50K, <=50K ，离散值\n注意，在插入这些列的标题后，一定要以 .csv 格式保存，且保存时将文件命名为 Adult.data.csv 。\n下面，先总括一下第一个Azure机器学习实验的数据集的数据特征：\n十四个与结果相关的唯一属性\n数据集的实例数为 48,842\n预测任务是确定用户是否一年收入超过$50,000美元。\n此人口收入的普查数据集以被微软作为一个样本数据提供出来了，在其成人普查收入的二元分类（Adult Census Income Binary Classification）数据集中便可以找到。以下我们将手动地一步步全面地介绍整个Azure机器学习工作流过程，很有可能，您的用于预测模型地真实数据集来自于其他外部资源，因此了解机器学习是怎么从开始至结束的全过程是很有必要的。\n数据上载至Azure机器学习实验\n将人口收入普查数据集添加了列标题后，我们即可将数据上载至Azure机器学习工作区，并将其纳入预测模型。点击屏幕左下方的\"+\"，然后选择上传的数据集。下图显示上传本地数据文件的选项。\n图 Azure机器学习中上载本地数据集文件的选项\n下一步，点击从本地文件选择即\"FROM LOCAL FILE\"，您可看见如下图所示的上载界面。在此界面您可指定上载文件的属性，比如文件的位置、名称（本例中我们使用 Adult.data.csv ）和类型（通常是CSV类型），以及新的数据集的可选说明。\n图 Azure机器学习对话框，选择设定从本地文件上载新的数据集\n完成信息的输入并点击签入按钮后，您的数据集将异步加载至您的第一个Azure机器学习实验的工作区中。\n创建新的Azure机器学习实验\n创建新的实验的方法是点击屏幕左下角的\"+NEW\"按钮，选择\"实验\"（EXPERIMENT）à \"空白实验\"（Blank Experiment）。\n图Azure机器学习中的实验类型列表\n请注意，除了空白实验之外，还有许多示例实验模板可供您加载和修改，以便您快速掌握Azure机器学习的实践。\n完成新的空白实验的加载后，您可见到如下图所示的Azure ML Studio可视化设计界面。\n图 在Azure ML Studio设计器中的空白Azure机器学习实验\n可以看到设计器由三个主要区域构成：\n左侧导航窗格 此区域包含Azure机器学习模块的可搜索列表，此模型可用于创建预测分析模型。\n按功能区域分组的模块\n数据集的读取和格式转换；\n使用和训练机器学习算法；\n评估预测模型的结果。\n中间窗格 在可视化设计器中，Azure机器学习的实验类似于流程图的形式，可以通过拖拽左侧窗格中的功能模块至可视化设计器的中间窗格组装成工作流。模块可以自由的被拖放在中间窗格的任意位置，模块之间通过输入和输出端口之间画线连接。\n右侧窗体 在属性视图中，可在右侧窗体查看和设置被选择模块的属性。\n在左侧窗体展开\"已保存的数据集（Saved Datasets）\"选项，便可以看到我们上载的用于Azure机器学习的 Adult.data.csv 数据文件出现在数据集的列表中。-13显示 Adult.data.csv 将被拖放至可视化设计器的中间窗体。\n图 将 Adult.data.csv 数据集拖动至设计器面板中\n分割数据集\n通常，创建Azure 机器学习实验后，我们都会将数据集分割为两个逻辑分组即训练数据和验证数据，这样做有两个特定目的：\n训练数据通常用来创建预测模型，基于机器学习算法发现历史数据中的固有模式。\n验证数据的分组用来测试训练数据创建的预测模型对于已知结果预测的精度和概率。\n为完成这个任务，执行以下的步骤将数据集分割成两部分。\n在左侧窗体中展开\"Data Transformation\"即数据转换模块。\n拖动\"Split\"即分割模块至Azure机器学习设计器。\n连接\"Split\"模块与 Adult.data.csv 数据集。\n点击分割模块并设置\"Fraction of rows in the first output dataset\"为0.8。这将80%的数据分割至训练数据集中。\n下图显示在Azure ML Studio中的操作步骤。\n图 分割 Adult.data.csv 数据集为训练数据和测试数据\n以上操作就将数据集中的80%的数据用于训练模型，我们可使用剩余的20%数据验证模型的精度。\n模型训练\n下一步是借助Azure机器学习算法\"教\"模型如何评估数据。在左侧窗体中展开\"Machine Learning\"即机器学习模块，然后展开\"Train\"子模块，将\"Train Model\"拖放至设计器中，最后在设计器中连接\"Train Model\"和\"Split\"图形。\n然后，我们展开\"Machine Learning\"即机器学习模块下的\"Initialize Model\"即初始化模型，展开\"Classfication\"即分类子模块。在此实验中，我们使用\"Two-Class Boosted Decision Tree\"即双类提升的决策树算法。在左侧窗体中选中该算法模块并将其拖放至设计器中，至此您的实验应该如下图所示。\n图 在Azure机器学习实验中连接训练模型和双类提升决策树模块\n至此，我们设计了实验将80%的成人收入普查数据集用于训练双类提升决策树算法的模型。\n可能您会产生疑问，为什么选择这个算法来处理我们的预测。请不必纠结于为什么要采用此算法而不是别的算法，因为本章的主题是如何在Azure ML Studio中创建预测模型，在后续的章节将涵盖如何选择恰当的机器学习预测算法。所以现在就让我们使用双类提升决策树算法演示Azure机器学习实验的示例。\n选择预测列\n要完成算法的配置，我们需要指定数据集中的哪一列数据作为输出或者预测列，数据集中的任意列将基于其他列的数据做预测。\n若要执行此操作，在设计器中点击\"Train Model\"，属性窗体将在Azure ML Studio的右侧窗体中显示，如下图所示。\n图 打开训练模块的列选择器\n若您在设计器中设置，请选择\"Launch column selector\"即启动列选择器，选择\"Include\"和列名称为\"income\"即收入的列。\n下图所示的列选择器将数据集中的收入列作为预测列，即要预测的是用户收入。\n图 配置训练模块即选择收入列\n按照这种方式，Azure机器学习算法从每行数据中的其他列训练模型，以预测收入。我们使用数据集中的80%基于已知的输入和输出数据训练训练模型。\n至此，我们已经做好训练模型的准备，选择屏幕底端的\"RUN\"即运行选项，然后隔岸观火静待Azure机器学习训练我们的模型。您会注意到，实验每个阶段完成的时候，绿色的复选框就出现在每个操作的右侧，如下图所示。\n图 训练Azure机器学习的收入预测模型\n模型评分\n现在我们已经训练完成新的Azure机器学习预测模型，下一步我们从解决方案的适用性的角度评估预测结果的正确性，以确定模型的精度。请牢记，Azure机器学习解决方案伟大之处在于迭代开发，最终成功的关键是快速试错。\n如要实现对模型的评价，首先展开Azure ML Studio左侧的\"Machine Learning\"即机器学习模块，然后展开\"Score Model\"即评分模型子模块，将\"Score Model\"拖放至设计器中，下一步连接\"Score Model\"和\"Train Model\"，最后链接\"Score Model\"和\"Split\"模块。至此，基本上就完成了利用数据集中20%的数据评估预测模型的准确性。\n下一步，单击屏幕底部的\"Run\"即运行按钮等待处理的结果（每个模块右侧出现绿色的复选标记表示运行完毕）。下图是机器学习实验预测收入的运算过程截图。\n图Azure ML Studio的训练和评分模型\n模型计算结果的可视化\n当所有的模型运算结束后，将鼠标悬停在\"Score Model\"即评分模型上点击右键，从快捷菜单中选择\"Visualize\"即可视化，如下图所示。\n图Azure ML Studio实验中模型评分结果的可视化\n当您选择可视化新训练的模型数据选项后，会生成一个新的页面。在可视化的界面中滑动滚动条至最右端，您会发现两个额外的列显示在数据集中，如下图所示。\n图 训练模型每行新增的两列分别代表针对每一行模型的预测值及预测概率\n可以看到现在有两个额外的列添加到了我们的数据集中：\n. \"Scored Lables\"即评分标签表示数据集中此行数据的预测结果\n\"Scored Probabilities\"即评分概率表示收入水平超过 $50000 的概率 （或可能性）。\n在我们数据集中新增的列提供了算法针对每行数据计算的预测结果和概率因子。概率因子是模型基于数据集中其他列数据预测结果的准确度的概率估计。\n通常情况下，预测分析是一个多轮迭代的过程。可能您会尝试许多不同的算法，或者将他们联合使用（在高级的机器学习主题文章中被称为集成）以证明预测模型的有效性。\n模型评估\nAzure机器学习最引入注目的功能之一就是它能够快速评估不同的算法，只要轻点鼠标就可完成这些功能，这一切都归功于评估模型。确定模型的精准度的方法很简单，我们只要使用Azure ML Studio内置的评估模型就轻松完成模型的评价。\n若要执行此操作，在Azure ML Studio的左侧导航窗格中点击\"Machine Learning\"即机器学习模块，选择\"Evaluate\"即评估子模块，最后选择\"Evaluate Model\"即评估模型的模块，将其拖至可视化设计器页面中的\"Score Model\"模块下方。连接\"Split Model\"和\"Score Model\"即分割模型和评分模型，以及\"Evaluate Model\"和\"Score Model\"即评价模型和评分模型，如下图所示。\n图 链接评估模型评价收入预测的结果\n点击Azure ML Studio 屏幕底部的\"Run\"即运行按钮，在执行过程中您可以查看实验中每个模块的运行情况，如果模块运行完毕会在模块的右侧显示绿色的复选标记。\n整个过程运行完毕后，右键单击评估模型的模块底部连接器，在快捷菜单中选择\"Visualize\"即可视化，评估的结果就会如下图显示。\n图Azure 机器学习评估模型评价收入预测模型的可视化结果\n评估模型模块会产生一套曲线和度量指标，让您对于评分模型的结果或者两个评分模型的对比情况一目了然。评分结果以以下三种形式展示：\nROC曲线（Receiver Operator Characteristic）即受试者工作特征曲线反映的是真阳性占总的实际阳性的比例。将它与在各种阈值设置情况下假阳性占总的实际阴性的比例进行对比。对角连线表示50%预测的准确性，并可作为评价的基准以便后续提高。曲线位于左边高出对角线的部分表示模型的精准度高，当然您也会希望实验的结果曲线出现在此区域。\n准确率和召回率是衡量信息检索系统性能的重要指标。准确率是指检索到相关文档数占检索到的文档总数的比例，而召回率是指检索到相关文档数占所有相关文档总数的比例。\nlift曲线是数据挖掘分类器最常用的方式之一，与ROC曲线不同的是lift考虑分类器的准确性，也就是使用分类器获得的正类数量和不使用分类器随机获取正类数量的比例。\n在图 3-29 的可视化结果中，您可看到两个数据集（\"训练\"数据集和\"验证\"数据集）几乎完全相同，即红色和蓝色曲线几乎完全重合，这表明我们的预测模型相当准确。Azure 机器学习入门教程的初衷就是构建合理准确的预测模型，并在下一个阶段中进行应用。\n保存实验\n在此步骤中，我们将要保存实验的副本。在屏幕的底部点击\"Save As\"另存为按钮。在后面的实验中，我们要将实验的核心功能做出重大的修改，所以要先将实验另存，保存的名称建议具有描述性的说明，比如 Azure 机器学习的收入预测——训练模型试验（Azure ML Income Prediction – Train Model Experiment）。"}
{"content2":"原文：http://qxde01.blog.163.com/blog/static/67335744201368101922991/\nPython在科学计算领域，有两个重要的扩展模块：Numpy和Scipy。其中Numpy是一个用python实现的科学计算包。包括：\n一个强大的N维数组对象Array；\n比较成熟的（广播）函数库；\n用于整合C/C++和Fortran代码的工具包；\n实用的线性代数、傅里叶变换和随机数生成函数。\nSciPy是一个开源的Python算法库和数学工具包，SciPy包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。其功能与软件MATLAB、Scilab和GNU Octave类似。\nNumpy和Scipy常常结合着使用，Python大多数机器学习库都依赖于这两个模块，绘图和可视化依赖于matplotlib模块，matplotlib的风格与matlab类似。Python机器学习库非常多，而且大多数开源，主要有：\n1.       scikit-learn\nscikit-learn 是一个基于SciPy和Numpy的开源机器学习模块，包括分类、回归、聚类系列算法，主要算法有SVM、逻辑回归、朴素贝叶斯、Kmeans、DBSCAN等，目前由INRI 资助，偶尔Google也资助一点。\n项目主页：\nhttps://pypi.python.org/pypi/scikit-learn/\nhttp://scikit-learn.org/\nhttps://github.com/scikit-learn/scikit-learn\n2.       NLTK\nNLTK(Natural Language Toolkit)是Python的自然语言处理模块，包括一系列的字符处理和语言统计模型。NLTK 常用于学术研究和教学，应用的领域有语言学、认知科学、人工智能、信息检索、机器学习等。 NLTK提供超过50个语料库和词典资源，文本处理库包括分类、分词、词干提取、解析、语义推理。可稳定运行在Windows, Mac OS X和Linux平台上.\n项目主页：\nhttp://sourceforge.net/projects/nltk/\nhttps://pypi.python.org/pypi/nltk/\nhttp://nltk.org/\n3.       Mlpy\nMlpy是基于NumPy/SciPy的Python机器学习模块，它是Cython的扩展应用。包含的机器学习算法有：\nl  回归\nleast squares, ridge regression, least angle regression, elastic net, kernel ridge regression, support vector machines (SVM), partial least squares (PLS)\nl  分类\nlinear discriminant analysis (LDA), Basic perceptron, Elastic Net, logistic regression, (Kernel) Support Vector Machines (SVM), Diagonal Linear Discriminant Analysis (DLDA), Golub Classifier, Parzen-based, (kernel) Fisher Discriminant Classifier, k-nearest neighbor, Iterative RELIEF, Classification Tree, Maximum Likelihood Classifier\nl  聚类\nhierarchical clustering, Memory-saving Hierarchical Clustering, k-means\nl  维度约减\n(Kernel) Fisher discriminant analysis (FDA), Spectral Regression Discriminant Analysis (SRDA), (kernel) Principal component analysis (PCA)\n项目主页：\nhttp://sourceforge.net/projects/mlpy\nhttps://mlpy.fbk.eu/\n4.       Shogun\nShogun是一个开源的大规模机器学习工具箱。目前Shogun的机器学习功能分为几个部分：feature表示，feature预处理，核函数表示,核函数标准化，距离表示，分类器表示，聚类方法，分布，性能评价方法，回归方法，结构化输出学习器。\nSHOGUN 的核心由C++实现，提供 Matlab、 R、 Octave、 Python接口。主要应用在linux平台上。\n项目主页：\nhttp://www.shogun-toolbox.org/\n5.       MDP\nThe Modular toolkit for Data Processing (MDP) ，用于数据处理的模块化工具包，一个Python数据处理框架。\n从用户的观点，MDP是能够被整合到数据处理序列和更复杂的前馈网络结构的一批监督学习和非监督学习算法和其他数据处理单元。计算依照速度和内存需求而高效的执行。从科学开发者的观点，MDP是一个模块框架，它能够被容易地扩展。新算法的实现是容易且直观的。新实现的单元然后被自动地与程序库的其余部件进行整合。MDP在神经科学的理论研究背景下被编写，但是它已经被设计为在使用可训练数据处理算法的任何情况中都是有用的。其站在用户一边的简单性，各种不同的随时可用的算法，及应用单元的可重用性，使得它也是一个有用的教学工具。\n项目主页：\nhttp://mdp-toolkit.sourceforge.net/\nhttps://pypi.python.org/pypi/MDP/\n6.       PyBrain\nPyBrain(Python-Based Reinforcement Learning, Artificial Intelligence and Neural Network)是Python的一个机器学习模块，它的目标是为机器学习任务提供灵活、易应、强大的机器学习算法。（这名字很霸气）\nPyBrain正如其名，包括神经网络、强化学习(及二者结合)、无监督学习、进化算法。因为目前的许多问题需要处理连续态和行为空间，必须使用函数逼近(如神经网络)以应对高维数据。PyBrain以神经网络为核心，所有的训练方法都以神经网络为一个实例。\n项目主页：\nhttp://www.pybrain.org/\nhttps://github.com/pybrain/pybrain/\n7.       BigML\nBigML 使得机器学习为数据驱动决策和预测变得容易，BigML使用容易理解的交互式操作创建优雅的预测模型。BigML使用BigML.io,捆绑Python。\n项目主页：\nhttps://bigml.com/\nhttps://pypi.python.org/pypi/bigml\nhttp://bigml.readthedocs.org/\n8.       PyML\nPyML是一个Python机器学习工具包， 为各分类和回归方法提供灵活的架构。它主要提供特征选择、模型选择、组合分类器、分类评估等功能。\n项目主页：\nhttp://cmgm.stanford.edu/~asab/pyml/tutorial/\nhttp://pyml.sourceforge.net/\n9.       Milk\nMilk是Python的一个机器学习工具箱，其重点是提供监督分类法与几种有效的分类分析：SVMs(基于libsvm)，K-NN，随机森林经济和决策树。它还可以进行特征选择。这些分类可以在许多方面相结合，形成不同的分类系统。\n对于无监督学习，它提供K-means和affinity propagation聚类算法。\n项目主页：\nhttps://pypi.python.org/pypi/milk/\nhttp://luispedro.org/software/milk\n10.  PyMVPA\nPyMVPA(Multivariate Pattern Analysis in Python)是为大数据集提供统计学习分析的Python工具包，它提供了一个灵活可扩展的框架。它提供的功能有分类、回归、特征选择、数据导入导出、可视化等\n项目主页：\nhttp://www.pymvpa.org/\nhttps://github.com/PyMVPA/PyMVPA\n11.  Pattern\nPattern是Python的web挖掘模块，它绑定了  Google、Twitter 、Wikipedia API，提供网络爬虫、HTML解析功能，文本分析包括浅层规则解析、WordNet接口、句法与语义分析、TF-IDF、LSA等，还提供聚类、分类和图网络可视化的功能。\n项目主页：\nhttp://www.clips.ua.ac.be/pages/pattern\nhttps://pypi.python.org/pypi/Pattern\n12.  pyrallel\nPyrallel(Parallel Data Analytics in Python)基于分布式计算模式的机器学习和半交互式的试验项目，可在小型集群上运行，适用范围：\nl  focus on small to medium dataset that fits in memory on a small (10+ nodes) to medium cluster (100+ nodes).\nl  focus on small to medium data (with data locality when possible).\nl  focus on CPU bound tasks (e.g. training Random Forests) while trying to limit disk / network access to a minimum.\nl  do not focus on HA / Fault Tolerance (yet).\nl  do not try to invent new set of high level programming abstractions (yet): use a low level programming model (IPython.parallel) to finely control the cluster elements and messages transfered and help identify what are the practical underlying constraints in distributed machine learning setting.\n项目主页：\nhttps://pypi.python.org/pypi/pyrallel\nhttp://github.com/pydata/pyrallel\n13.  Monte\nMonte ( machine learning in pure Python)是一个纯Python机器学习库。它可以迅速构建神经网络、条件随机场、逻辑回归等模型，使用inline-C优化，极易使用和扩展。\n项目主页：\nhttps://pypi.python.org/pypi/Monte\nhttp://montepython.sourceforge.net\n14.  Orange\nOrange 是一个基于组件的数据挖掘和机器学习软件套装，它的功能即友好，又很强大，快速而又多功能的可视化编程前端，以便浏览数据分析和可视化，基绑定了 Python以进行脚本开发。它包含了完整的一系列的组件以进行数据预处理，并提供了数据帐目，过渡，建模，模式评估和勘探的功能。其由C++ 和 Python开发，它的图形库是由跨平台的Qt框架开发。\n项目主页：\nhttps://pypi.python.org/pypi/Orange/\nhttp://orange.biolab.si/\n15.  Theano\nTheano 是一个 Python 库，用来定义、优化和模拟数学表达式计算，用于高效的解决多维数组的计算问题。Theano的特点：\nl  紧密集成Numpy\nl  高效的数据密集型GPU计算\nl  高效的符号微分运算\nl  高速和稳定的优化\nl  动态生成c代码\nl  广泛的单元测试和自我验证\n自2007年以来，Theano已被广泛应用于科学运算。theano使得构建深度学习模型更加容易，可以快速实现下列模型：\nl  Logistic Regression\nl  Multilayer perceptron\nl  Deep Convolutional Network\nl  Auto Encoders, Denoising Autoencoders\nl  Stacked Denoising Auto-Encoders\nl  Restricted Boltzmann Machines\nl  Deep Belief Networks\nl  HMC Sampling\nl  Contractive auto-encoders\nTheano，一位希腊美女，Croton最有权势的Milo的女儿，后来成为了毕达哥拉斯的老婆。\n项目主页：\nhttp://deeplearning.net/tutorial/\nhttps://pypi.python.org/pypi/Theano\n16.      Pylearn2\nPylearn2建立在theano上，部分依赖scikit-learn上，目前Pylearn2正处于开发中，将可以处理向量、图像、视频等数据，提供MLP、RBM、SDA等深度学习模型。Pylearn2的目标是：\nResearchers add features as they need them. We avoid getting bogged down by too much top-down planning in advance.\nA machine learning toolbox for easy scientific experimentation.\nAll models/algorithms published by the LISA lab should have reference implementations in Pylearn2.\nPylearn2 may wrap other libraries such as scikits.learn when this is practical\nPylearn2 differs from scikits.learn in that Pylearn2 aims to provide great flexibility and make it possible for a researcher to do almost anything, while scikits.learn aims to work as a “black box” that can produce good results even if the user does not understand the implementation\nDataset interface for vector, images, video, ...\nSmall framework for all what is needed for one normal MLP/RBM/SDA/Convolution experiments.\nEasy reuse of sub-component of Pylearn2.\nUsing one sub-component of the library does not force you to use / learn to use all of the other sub-components if you choose not to.\nSupport cross-platform serialization of learned models.\nRemain approachable enough to be used in the classroom (IFT6266 at the University of Montreal).\n项目主页：\nhttp://deeplearning.net/software/pylearn2/\nhttps://github.com/lisa-lab/pylearn2\n还有其他的一些Python的机器学习库，如：\npmll(https://github.com/pavlov99/pmll)\npymining(https://github.com/bartdag/pymining)\nease (https://github.com/edx/ease)\ntextmining(http://www.christianpeccei.com/textmining/)\n更多的机器学习库可通过https://pypi.python.org/pypi查找。"}
{"content2":"不多说，直接上干货！\nIntelliJ IDEA的黑白色背景切换\nFile    ->   Setting    ->  Editor     ->   Colors & Fonts\n即，默认是白色的，那么，点击yes，则默认变成黑色了。\n变成\n想说的是，当然，这背景版本给出的是黑色和白色这两种选择，其实，自己可以将图片上传作为背景。\n扩展\nIntelliJ IDEA（Community版本）的下载、安装和WordCount的初步使用（本地模式和集群模式）\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"感谢中国人民大学胡鹤老师，课讲得非常好~\n首先，何谓tensor？即高维向量，例如矩阵是二维，tensor是更广义意义上的n维向量（有type+shape）\nTensorFlow执行过程为定义图，其中定义子节点，计算时只计算所需节点所依赖的节点，是一种高效且适应大规模的数据计算，方便分布式设计，对于复杂神经网络的计算，可将其拆开到其他核中同时计算。\nTheano——torch———caffe（尤其是图像处理）——deeplearning5j——H20——MXNet，TensorFlow\n运行环境\n下载docker\n打开docker quickstart terminal\n标红地方显示该docker虚拟机IP地址（即之后的localhost）\ndocker tensorflow/tensorflow　　//自动找到TensorFlow容器并下载\ndocker images　　//浏览当前容器\ndocker run -p 8888:8888 tensorflow/tensorflow　　//在8888端口运行\n会出现一个token，复制该链接并替换掉localhost，既可以打开TensorFlow的一个编写器，jupyter\n大体雏形\n#python导入 import tensorflow as tf #定义变量（节点） x = tf.Variable(3, name=\"x\") y = tf.Variable(4, name=\"y\") f = x*x*y + y + 2 #定义session sess = tf.Session() #为已经定义的节点赋值 sess.run(x.initializer) sess.run(y.initializer) #运行session result = sess.run(f) print(result) #42 #释放空间 sess.close\n还有一个更简洁的一种定义并运行session方法\n# a better way with tf.Session() as sess: x.initializer.run() y.initializer.run() #即evaluate，求解f的值 result = f.eval()\n初始化的两行也可以写作\ninit = tf.global_variables_initializer()\ninit.run()\n而session可以改作sess=tf.InteractiveSession()运行起来更方便\ninit = tf.global_variables_initializer() sess = tf.InteractiveSession() init.run() result = f.eval() print(result)\n因而TensorFlow的代码分为两部分，定义部分和执行部分\nTensorFlow是一个图的操作，有自动缺省的默认图和你自己定义的图\n#系统默认缺省的图 >>> x1 = tf.Variable(1) >>> x1.graph is tf.get_default_graph() True #自定义的图 >>> graph = tf.Graph() >>> with graph.as_default(): x2 = tf.Variable(2) >>> x2.graph is graph True >>> x2.graph is tf.get_default_graph() False\n节点的生命周期\n第二种方法可以找出公共部分，避免x被计算2次。\n运行结束后所有节点的值都被清空，如果没有单独保存，还需重新run一遍。\nw = tf.constant(3) x = w + 2 y = x + 5 z = x * 3 with tf.Session() as sess: print(y.eval()) # 10 print(z.eval()) # 15 with tf.Session() as sess: y_val, z_val = sess.run([y, z]) print(y_val) # 10 print(z_val) # 15\nLinear Regression with TensorFlow（线性回归上的应用）\ny = wx+b = wx'　　//这里x'是相较于x多了一维全是1的向量\n这里引用California housing的数据\nTensorFlow上向量是列向量，需要reshape(-1,1)即转置成列向量\n使用normal equation方法求解\nimport numpy as np from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() #获得数据维度，矩阵的行列长度 m, n = housing.data.shape #np.c_是连接的含义，加了一个全为1的维度 housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] #数据量并不大，可以直接用常量节点装载进来，但是之后海量数据无法使用（会用minbatch的方式导入数据） X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\") #转置成列向量 y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\") XT = tf.transpose(X) #使用normal equation的方法求解theta，之前线性模型中有提及 theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y) #求出权重 with tf.Session() as sess: theta_value = theta.eval()\n如果是原本的方法，可能更直接些。但由于使用底层的库不同，它们计算出来的值不完全相同。\n#使用numpy X = housing_data_plus_bias y = housing.target.reshape(-1, 1) theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) #使用sklearn from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n这里不禁感到疑惑，为什么TensorFlow感觉变复杂了呢？其实，这不过因为这里数据规模较小，进行大规模的计算时，TensorFlow的自动优化所发挥的效果，是十分厉害的。\n使用gradient descent（梯度下降）方法求解\n#使用gradient时需要scale一下 from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaled_housing_data = scaler.fit_transform(housing.data) scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data] #迭代1000次 n_epochs = 1000 learning_rate = 0.01 #由于使用gradient，写入x的值需要scale一下 X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\") y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\") #使用gradient需要有一个初值 theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #当前预测的y，x是m*（n+1），theta是（n+1）*1，刚好是y的维度 y_pred = tf.matmul(X, theta, name=\"predictions\") #整体误差 error = y_pred - y #TensorFlow求解均值功能强大，可以指定维数，也可以像下面方法求整体的 mse = tf.reduce_mean(tf.square(error), name=\"mse\") #暂时自己写出训练过程，实际可以采用TensorFlow自带的功能更强大的自动求解autodiff方法 gradients = 2/m * tf.matmul(tf.transpose(X), error) training_op = tf.assign(theta, theta - learning_rate * gradients) #初始化并开始求解 init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) for epoch in range(n_epochs): #每运行100次打印一下当前平均误差 if epoch % 100 == 0: print(\"Epoch\", epoch, \"MSE =\", mse.eval()) sess.run(training_op) best_theta = theta.eval()\n上述代码中的autodiff如下，可以自动求出gradient\ngradients = tf.gradients(mse, [theta])[0]\n使用Optimizer\n上述的整个梯度下降和迭代方法，都封装了在如下方法中\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(mse)\n这样的optimizer还有很多\n例如带冲量的optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\nFeeding data to training algorithm\n当数据量达到几G，几十G时，使用constant直接导入数据显然是不现实的，因而我们用placeholder做一个占位符\n（一般行都是none，即数据量是任意的）\n真正运行，run的时候再feed数据。可以不断使用新的数据。\n>>> A = tf.placeholder(tf.float32, shape=(None, 3)) >>> B = A + 5 >>> with tf.Session() as sess: ... B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]}) ... B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]}) ... >>> print(B_val_1) [[ 6. 7. 8.]] >>> print(B_val_2) [[ 9. 10. 11.] [ 12. 13. 14.]]\n这样，就可以通过定义min_batch来分批次随机抽取指定数量的数据，即便是几T的数据也可以抽取。\nbatch_size = 100 n_batches = int(np.ceil(m / batch_size)) #有放回的随机抽取数据 def fetch_batch(epoch, batch_index, batch_size): #定义一个随机种子 np.random.seed(epoch * n_batches + batch_index) # not shown in the book indices = np.random.randint(m, size=batch_size) # not shown X_batch = scaled_housing_data_plus_bias[indices] # not shown y_batch = housing.target.reshape(-1, 1)[indices] # not shown return X_batch, y_batch #开始运行 with tf.Session() as sess: sess.run(init) #每次都抽取新的数据做训练 for epoch in range(n_epochs): for batch_index in range(n_batches): X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size) sess.run(training_op, feed_dict={X: X_batch, y: y_batch}) #最终结果 best_theta = theta.eval()\nSaving and Restoring models（保存模型）\n有时候，运行几天的模型可能因故暂时无法继续跑下去，因而需要暂时保持已训练好的部分模型到硬盘上。\ninit = tf.global_variables_initializer() saver = tf.train.Saver() #保存模型 with tf.Session() as sess: sess.run(init) for epoch in range(n_epochs): if epoch % 100 == 0: #print(\"Epoch\", epoch, \"MSE =\", mse.eval()) save_path = saver.save(sess, \"/tmp/my_model.ckpt\") sess.run(training_op) best_theta = theta.eval() save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n#恢复模型 with tf.Session() as sess: saver.restore(sess, \"/tmp/my_model_final.ckpt\") best_theta_restored = theta.eval()\n关于TensorBoard\n众所周知，神经网络和机器学习大多是黑盒模型，让人有点忐忑。TensorBoard所起的功能就是将这个黑盒稍微变白一些~\n启用tensorboard\n输入docker ps查看当前容器id\n进入容器\n使用tensorboard --log-dir=tf_logs命令打开已经存入的tf_logs文件，其生成代码如下所示\nfrom datetime import datetime now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") root_logdir = \"tf_logs\" logdir = \"{}/run-{}/\".format(root_logdir, now) ... mse_summary = tf.summary.scalar('MSE', mse) file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) ... if batch_index % 10 == 0: summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch}) step = epoch * n_batches + batch_index file_writer.add_summary(summary_str, step)"}
{"content2":"这节课的题目是Deep learning，个人以为说的跟Deep learning比较浅，跟autoencoder和PCA这块内容比较紧密。\n林介绍了deep learning近年来受到了很大的关注：deep NNet概念很早就有，只是受限于硬件的计算能力和参数学习方法。\n近年来深度学习长足进步的原因有两个：\n1）pre-training技术获得了发展\n2）regularization的技术获得了发展\n接下来，林开始介绍autoencoder的motivation。\n每过一个隐层，可以看做是做了一次对原始输入信息的转换。\n什么是一个好的转换呢？就是因为这种转换而丢失较多的信息：即encoding之后，甚至可以用decoding的过程复原。\n因此，在考虑deep NNet的参数学习的时候，如果在pre-training阶段采用类似autoencoding的方式，似乎是一个不错的选择。\n如下，就是autoencoder的一个示例。简单来说，就是经过如下的单层神经网络结构后，输出跟输出十分接近。\n这种autoencoder对于机器学习来说有什么作用呢？\n1）对于supervised learning来说：这种information-preserving NN的隐层结构+权重是一种对原始输入合理的转换，相当于在结构中学习了data的表达方式\n2）对于unsupervised learning来说：可以作为density estimation或outlier detection。这个地方没太理解清，可能还是缺少例子。\nautoencoder可以看成是单层的NN，可以用backprop求解；这里需要多加入一个正则化条件，wij(1)=wji(2)\n采用上述的basic autoencoder，可以作为Deep NNet的pre-training方式。\n接下来，林开始关注Deep NNet的regularization的问题。\n之前提到过的几种regularization方式都可以用（structural constraints、weight decay/elimination regularizers、early stopping），下面介绍一种新的regularization technique。\n这种方式是：adding noise to data\n简单来说，在训练autoencoder的时候加入高斯噪声，喂进去的输出端还是没有加入噪声的data；这样学出来的autoencoder就具备了抵抗noise的能力。\n接下来，开始引入PCA相关的内容。\n之前陈述的autoencoder可以归类到nonliner autoencoder（因为隐层输出需要经过tanh的操作，所以是nonlinear的）。\n那么如果是linear autoencoder呢？(这里把隐层的bias单元去掉)\n最后得到的linear autoencoder的表达式就是 ：h(x)=WW'x\n由此，可以写出来error function\n这是一个关于W的4阶的多项式，analytic solution不太好整。\n于是林给出了下面的一种求解思路：\n上述的核心在于：WW'是实对称阵。\n实对称阵有如下的性质：(http://wenku.baidu.com/view/1470f0e8856a561252d36f5d.html)\n我们注意一下W这个矩阵：W是d×d'维度的矩阵；WW'是d×d维度的矩阵。\n这里回顾一下矩阵的秩的性质：\n因此，WW'的秩最大就是d'了（d代表数据的原始维度，d'代表隐层神经元的个数，一般d'＜d）\nWW'的秩最大是d'能得到这样的结论：WW'至多有d'个非零特征值→对角阵gamma对角线上最多有d'个非零元素。\n这里需要复习线性代数一个概念：\n如果矩阵可以对角化，那么非零特征值的个数就等于矩阵的秩；如果矩阵不可以对角化，那么这个结论就不一定成立了。\n这里我们说的WW'是实对称阵，又因为实对称阵一定可以对角化，因此WW'的非零特征值特殊就等于矩阵的秩。\n通过上述的内容，WW'x又可以看成是VgammaV'x:\n1）V'x 可以看成是对原始输入rotate\n2）gamma 可以看成是将0特征值的component的部分设成0，并且scale其余的部分\n3）再转回来\n因此，优化目标函数就出来了\n这里可以不用管前面的V（这是正交变换的一个性质，正交变换不改变两个向量的内积，详情见https://zh.wikipedia.org/wiki/正交）\n这样一来，问题就简化了：令I-gamma生出很多0，利用gamma对角线元素的自由度，往gamma里面塞1，最多塞d'个1。剩下的事情交给V来搞定。\n1）先把最小化转化为等价的最大化问题\n2）用只有一个非零特征值的情况来考虑，Σv'xx'v  s.t. v'v=1\n3）在上述最优化问题中，最好的v要满足error function和constraints在最优解的时候，他们的微分要平行。\n4）再仔细观察下形式 Σxx'v = lambdav 这里的v不就是XX'的特征向量么\n因此，最优化的v就是特征值最大的XX'的特征向量。需要降到多少维的，就取前多少个特征向量。\n林最后提了一句PCA，其实就是在进行上述步骤之前先对各个维度的向量均值化：\n下面说一下PCA。\nhttp://blog.codinglabs.org/articles/pca-tutorial.html\n上面这篇日志非常好，基本完全解释了PCA的来龙去脉。\n1）PCA的目的是对数据降维之后，还能尽量保持数据原有的信息（分得开。。。方差大。。。）\n2）如果对原始数据各个维度做均值化的操作之后，方差&协方差，只用一个矩阵就表示出来了。\n上述这段话看明白了，PCA的核心就有了：巧妙地把原始输入数据各个维度均值化之后，方差和协方差都放到一个矩阵里了。\n优化的目标是：方差要大，协方差要小；这样的优化目标就等价于把协方差矩阵对角化。\n实对称阵对角化是线性代数的基础知识：http://wenku.baidu.com/view/1470f0e8856a561252d36f5d.html\nOK，PCA就大体上搞定了。\n中途还看了stanford的http://ufldl.stanford.edu/wiki/index.php/PCA\n脑子里冒出来一个想法：如果协方差矩阵是满秩的，并且不对数据降维，原来是多少维，还是多少维，那么变换前和变换后有啥区别呢？\n从式子上看，这种变化相当于把变换后的协方差矩阵搞成对角阵了。如果从几何上来看，比较下面两个图：\n变换前：\n变换后：\n直观上看就是整体给“放平”了。\n变化前：x1越大 x2也越大，反之亦然\n变换后：由于给放平了，x1的大小与x2的大小没关系了\n因此，变换后这种放平就消除了x1和x2的相关性了，也就是协方差矩阵的非对角元素给搞成0的效果。"}
{"content2":"Quora\nWhat is Data Science?\nHow do I become a Data Scientist?\nHow does Data Science differ from traditional statistical analysis?\nRelated Courses\nConcepts in Computing with Data, Berkeley\nPractical Machine Learning, Berkeley\nArtificial Intelligence, Berkeley\nVisualization, Berkeley\nData Mining and Analytics in Intelligent Business Services, Berkeley\nData Science and Analytics: Thought Leaders, Berkeley\nMachine Learning, Stanford\nParadigms for Computing with Data, Stanford\nMining Massive Data Sets, Stanford\nData Visualization, Stanford\nAlgorithms for Massive Data Set Analysis, Stanford\nResearch Topics in Interactive Data Analysis, Stanford\nData Mining, Stanford\nMachine Learning, CMU\nStatistical Computing, CMU\nMachine Learning with Large Datasets, CMU\nMachine Learning, MIT\nData Mining, MIT\nStatistical Learning Theory and Applications, MIT\nData Literacy, MIT\nIntroduction to Data Mining, UIUC\nLearning from Data, Caltech\nIntroduction to Statistics, Harvard\nData-Intensive Information Processing Applications, University of Maryland\nDealing with Massive Data, Columbia\nData-Driven Modeling, Columbia\nIntroduction to Data Mining and Analysis, Georgia Tech\nComputational Data Analysis: Foundations of Machine Learning and Da..., Georgia Tech\nApplied Statistical Computing, Iowa State\nData Visualization, Rice\nData Warehousing and Data Mining, NYU\nData Mining in Engineering, Toronto\nMachine Learning and Data Mining, UC Irvine\nKnowledge Discovery from Data, Cal Poly\nLarge Scale Learning, University of Chicago\nData Science: Large-scale Advanced Data Analysis, University of Florida\nStrategies for Statistical Data Analysis, Universität Leipzig\nRelated Workshops\nData Bootcamp, Strata 2011\nMachine Learning Summer School, Purdue 2011\nLooking at Data\nBooks\nCompeting on Analytics\nAnalytics at Work\nSuper Crunchers\nThe Numerati\nData Driven\nData Source Handbook\nProgramming Collective Intelligence\nMining the Social Web\nData Analysis with Open Source Tools\nVisualizing Data\nThe Visual Display of Quantitative Information\nEnvisioning Information\nVisual Explanations: Images and Quantities, Evidence and Narrative\nBeautiful Evidence\nThink Stats\nData Analysis Using Regression and Multilevel/Hierarchical Models\nApplied Longitudinal Data Analysis\nDesign of Observational Studies\nStatistical Rules of Thumb\nAll of Statistics\nA Handbook of Statistical Analyses Using R\nMathematical Statistics and Data Analysis\nThe Elements of Statistical Learning\nCounterfactuals and Causal Inference\nMining of Massive Data Sets\nData Analysis: What Can Be Learned From the Past 50 Years\nBias and Causation\nRegression Modeling Strategies\nProbably Not\nStatistics as Principled Argument\nThe Practice of Data Analysis\nVideos\nLies, damned lies and statistics (about TEDTalks)\nThe Joy of Stats\nJournalism in the Age of Data\nSource: http://datascienc.es\n来自：http://www.datasciencecentral.com/profiles/blogs/berkeley-course-on-data-science"}
{"content2":"今天看到这样的一则新闻：不禁感叹，人工智能这股风来的太快，已经掀起全民学习Python的浪潮。\n2017年中观察：看上去这个大纲内容基本是这样了，但是实行年份可能要往后推了，不在2017年执行了（据说技术科目的另一部分，通用技术，也在进行教改）。\n2017年初消息：浙江省信息技术新教材，即将在2017级（2017年9月入学）高中新生中开始使用。\n据了解，与目前的选考（可以理解为高考科目）要求的信息技术教材由3本《信息技术基础》、《多媒体技术应用》、《算法与程序设计》3本组成，涉及软件（应用软件与编程软件）包括但不限于：\n信息技术基础：Excel、Access；多媒体技术应用：Photoshop、Flash算法与程序设计：Visual Basic 6.0，算法部分只涉及冒泡、插入排序，与二分查找，不同，取消《多媒体技术应用》模块（也就是说，不再考察Photoshop和Flash了）取消对Excel、Access软件使用的考察，编程语言将换用Python， 将使用Python + matplotlib + pandas ，并直接编程来绘制图表、操纵数据、进行数据可视化，来替代之前有关Excel和Access的考察， 算法与程序设计模块也将使用Python，并新增基础数据结构知识（线性数据结构，与非线性数据结构中的树与二叉树）\n虽然从年初开始就一直有计划有组织的进行学习，对于这门语言大致有个抽象的把握，那就来科普一下，人工智能和Python的关系。\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。\nPython由于其简洁优美和极高的开发效率，得到了越来越多公司的青睐，公司选用Python进行网站Web、搜索引擎(Google)、云计算(OpenStack)、大数据、人工智能、科学计算等方向的开发。Python将成为继C++和Java之后的第三个主流编程语言，Python结合人工智能也是尚 硅谷的优势课程，python的人才就业优势明显。\npython目前比较流行，而且学起来比C/C++容易，而且应该语言本身不是侧重点，估计还是算法和数据结构相关的，python用来实现。而且Python的优点特别多。\n优点一：主要是开发快，语言简洁，没那么多技巧，所以读起来很清楚容易。 优点二：  C/C++可以写python的module，标准库里就有用C/C++写的东西，这个跟java的JNI类似。 优点三、python的gui一般是用tkinter，就是tk的python的wrapper。python没有像xna那么方便的工具。优点四、python不是为了网络设计的。python是1991年有的，WWW是1993年才被CERN开放的。网络编程用python主要是为了开发快。 优点五、像VS那样功能强的IDE，有要钱的PyCharm和不要钱的PyDev。PyDev有Eclipse的插件版本或者是Aptana Studio版本。\nPython 的执行速度慢是不可避免的。作为一门脚本语言，它自然会比那些需要编译为可执行程序的语言要慢一些，因为在执行的过程中需要解析器参与，一边解析一边执行——这是脚本语言的通病。\n但在现代计算机的硬件配置下，Python 的运行速度和一些快速语言已经越来越近了。\n相比起其他语言，高考对其青睐有加，并且归入高考范畴也是意料之中的事情，在这个互联网时代，如果不能掌握一到两门计算机语言，也许接下来就会被时代所淘汰，高考响应了大势所趋的政策，也未未来一代带来了无穷无尽的希望。那么如何看待浙江17级高考技术考python这个问题？\n浙江省教育厅教研室，相关负责老师表示，目前，省教研室还未接到下半年会更改教材的通知，高中新的课程标准还在教育部审核中，至于9月份是否会使用新的课程标准目前还没有接到通知。不过这位老师也指出，网上流传的内容和正在审定的新教材比较类似。浙江省教育厅基教处相关负责人也表示，目前为止，还没有接到下半年开始使用新教材的通知。关于是否用新教材这件事，大家还是静等官方通知比较合适。\n有专家指出，编程语言是人机对话的一种语言，和人们用于沟通的汉语、英语等语言一样，有着相似的习得过程，越早接触，越能更好地掌握。而且，学习编程还能培养多方面的能力。\n1：思维能力；逻辑思维训练就是让你的大脑总是先浮现特定的规则，即使联想到了其他东西，也能自动跳过无关的内容，按规则进行“推理”。当然，规则形成的推理链条有时候可能会遭遇大脑内存不足而断裂，但这其实不是逻辑思维能力不行，而是短时记忆力不行，以及专注力、心理稳定性的问题。编写程序最重要的就是如何把大问题不断分割成小问题的过程。其中，学生必须去思考如何把代码合理的安排在整个程序中，才能让程序流畅的处理输入、演算、直到输出，这个过程对学生分析事物逻辑性有极大的提升。\n2：应变能力；随着社会竞争的加剧，人们所面临的变化和压力与日俱增，每个人都可能面临择业，下岗等方面的困扰。努力提高自己的应变能力，对保持健康的心理状况是很有帮助的，如果能够经过反复的练习大量思考过程后所做出的决策，有良好应变能，能审时度势随机应变。\n3：采集信息的能力；我们正处在一个向信息社会转变的新时代，对于现代人来说，及时掌握确的信息，是进行科学预测和科学决策的依据和基础。倘若耳目闭塞，长期处于封闭状态，单凭老经验办事，是很难做好工作的。因此，提高捕捉信息能力，视信息为效率，视信息为资本，是当今做好工作的重要前提，也是工作者必备的能力之一。\n4：解决问题的能力：解决问题能力不是天生的，自然得靠后天的经验积累，在编程的过程之中，总是会遇见各种各样的bug，这个时候不仅是对一个人耐心的考验，还是对其解决问题能力的考验，如何多层次全方位去解决这个问题，从什么地方寻找一个突破口，如何发挥主观能动性，解决与否，这都是要考验一个人的综合能力，从解决问题上还能看出一个人的预见力，决策力和执行力。所以，从小塑造这个能力，对未来有着不可估量的好处。\n5：:抽象思考能力：其实学习编程，就像学习第二外国语一样。如果说学外语是为了跟外国人沟通，学习程序就是为了跟计算机沟通。更有趣的是，你碰到老外不会说英文还可以比手画脚，跟电脑可不行。这意味着学生在学习的过程中，更需要一种化具体为抽象的能力，编写正确的程序，让程序能够按照学生想象的方式运行，这是集思广益的过程。让学生发挥无限想象并动手实现，让不懂得思考的电脑，也能了解与表达抽象的事物。\n未来将是大数据和人工智能爆发的时代，到时将会有大量的数据需要处理，而 Python 对数据的处理，有着得天独厚的优势，我相信在未来，Python 会越来越火。人生苦短，赶紧用起Python吧，AI大潮来袭，还在等什么？\n原文链接：Python开发者交流平台：Python将被加入高考科目？你怎么看？"}
{"content2":"我们将开始深入了解如何使用Azure机器学习的基本功能，帮助您开始迈向Azure机器学习的数据科学家之路。\nAzure ML Studio （Azure Machine Learning Studio / Azure ML Studio）是使用Azure机器学习云实现预测分析解决方案的主要工具。Azure机器学习是基于云计算和自容式的强大预测分析解决方案，具有完整的开发、测试和生产环节快速创建的独立闭环。\nAzure ML Studio提供交互式和可视化的工具轻松构建、测试和迭代预测分析模型。您可在Azure ML Studio中以拖拽的方式将数据集和分析模型在交互式画布相连接创建实验，然后编辑实验并迭代计算预测分析模型，如果需要还可保存计算的副本并重复迭代计算。最终您可将实验作为web服务发布于Azure，于是您的预测分析模型就可在web被访问。\n基于云计算的Azure机器学习的另一个核心优点就是几乎没有任何的时间和基础设施的启动成本。尤其是Azure机器学习相关的任务都可在现代web浏览器中完成。\nAzure机器学习基本术语\n为了帮助您快速开始，让我们定义描述各种功能、组件和工具的常见术语。\nAzure机器学习（Azure Machine Learning）包含所有必要的工具，可用来在微软Azure云平台设计、开发、分享、测试和部署预测分析模型解决方案。\nAzure机器学习工作区（Azure Machine Learning workspaces）表示离散的\"切片\"式的Azure机器学习工具集，它可以按照以下的标准进行分区：\n工作区名称（Workspace name）必须是唯一的，并且是确定机器学习工作区的主要方法。\n工作区所有者（Workspace owner）是有效的微软账户，用于管理对此Azure机器学习工作的访问。\n数据中心地理位置（Data center location）定义Azure机器学习工作区所在的Azure数据中心物理位置。\n存储账户（storage account）定义唯一的Azure存储账户，用于存储所有与此Azure机器学习工作区的相关的数据和工件。\nAzure机器学习实验（Azure Machine Learning experiments），实验是在Azure机器学习工作区中创建，可通过迭代计算实现快速开发机器学习解决方案的主要方法。在每一个Azure机器学习实验中，Azure ML Studio提供交互式、可视化的工作区，可轻松创建、测试和迭代计算预测分析实验。这些实验可在Azure ML Studio中提交执行。Azure ML Studio实验是高度重复的，很容易创建、编辑、测试、保存和重新运行试验。Azure机器学习实验实验是为现代数据科学家专门设计，使其能够在评估新的预测模型时以\"快速失败\"的方式不断改进细化模型。简单来说，Azure机器学习提供迭代的方式快速失败或者最终取得成功。\nAzure ML Studio是主要的交互式预测分析工作台，在Azure机器学习工作区为数据科学家提供可视化的设计工具以拖放的方式创建Azure机器学习实验，在Azure机器学习工作区提供访问Azure ML Studio唯一的运行环境。除了可以创建新的实验，Azure ML Studio还包括Azure机器学习实验的示例链接。这些功能能使您很容易的与更有经验的人学习，在数据科学的旅程中利用最好的技术和工具帮助您完成基于领域的预测分析的目标。\nAzure机器学习web服务（Azure Machine Learning web services）Azure机器学习实验以REST API的方式在网络中提供API访问服务，这些服务可以是简单的web服务或者OData端点。API以两种类型的rest风格的web接口：\n请求响应服务（Request Response Service - RRS） 适用于独立的、低延迟，需要同步使用的预测模型。\n批处理执行服务（Batch Execution Service - BES）适用于异步处理的批量数据记录。BES支持多种格式的数据源，比如blob、表、SQL Azure，以及HDInsight(作为Hive查询的结果)和HTTP源。\n数据集（Datasets）是指上载至Azure ML Studio被用于预测模型的数据。Azure ML Studio提供大量的示例数据集可进行实验，您也可上传更多的数据集满足您的计算需求。\n模型（Modules）是应用于数据计算的算法。Azure ML Studio包含大量的模型，即从数据训练、评价到验证过程的函数。下面是包含的模型示例：\n转换为ARFF（Convert to ARFF）将.NET序列化数据集转换为ARFF格式。在机器学习领域，ARFF是常见的表示属性-关系的文件结构。它通常被定义为ASCII文本文件，描述实例列表共享的一系列属性。\n基本统计（Elementary Statistics）是计算基本的统计数据，如均值、标准差等。\n线性回归（Linear Regression）创建在线梯度下降的线性回归模型。\n评估模型（Score Model）评价训练的分类或者回归模型。\n模型可能包含一组参数用于配置模型的内部算法。当您在画布（canvas）上选择一个模型，画布右侧的窗格中显示模型的参数。您可以在该窗格中修改参数优化模型。\n快速开始\nAzure机器学习之旅的第一步是获得微软Azure环境，这里有几种方法您可以选择：\n选择1，使用在 http://azure.microsoft.com/en-us/pricing/free-trial 提供的免费Azure试用账号。\n选择2，在 https://studio.azureml.net/Home 使用Azure机器学习的试用\n这是Azure提供的免费特别功能，因此只允许您访问Azure机器学习环境。\n这对于新的采纳者而言是一个极其低摩擦的选择：开始的唯一必须条件就是有效的微软账号。\n如果您需要注册微软账号，请访问 http://windows.microsoft.com/en-US/windows-live/sign-up-create-account-how 。\n您使用有效的微软账号登陆后，跃入眼帘的是如-1所示的帮助您入门的介绍视频。您也可在此链接 https://go.microsoft.com/fwlink/?LinkID=518038 回顾介绍视频。\n图Azure机器学习介绍视频\n请注意如果您选择的是使用免费的Azure机器学习试用，那么您将只有Azure机器学习功能的访问权限，没有权限使用全部的Azure环境。为真正最大化您的体验，强烈建议您获取全部微软Azure环境。\n创建第一个Azure机器学习工作区\n让我们创建第一个Azure机器学习工作区，在这一点上Azure免费账户与付费的Azure订阅是相同的。让我们从 https://manage.windowsazure.com 导航至Azure管理门户。在那里，在左侧的导航栏点击机器学习，如下图所示。\nAzure机器学习工作区包含全部您需要在云端创建、管理和发布机器学习实验的工具。若要创建一个新的Azure机器学习工作区，请单击页面左下角的新建图标，在如下图的页面中补充必填项。\n创建一个新的Azure机器学习工作区：\n工作区的名称（WORKSPACE NAME）在您的Azure机器学习工作区中是唯一的。请您知悉如果您将光标从工作区名称区域移开时，会检查您提供的工作区名称是否唯一，如果唯一的话会在文本框的右侧显示绿的复选标记。\n工作区所有者（WORSPACE OWNER）提供有效的微软账号（以前的Windows Live ID）。注意，它不能是非微软账户，比如您的公司电子邮件。创建免费微软账户请登录 www.live.com 。\nAzure Machine Learning的位置（LOCATION）仅在美国南中部地区可用。\n存储账户（STORAGE ACCOUNT）选项可以选择创建一个新的存储账户或者使用现有的存储账户。\n新的存储账户名称（NEW STORAGE ACCOUNT NAME）：如果您选择为您的Azure机器学习的工作区创建新的存储账户，请确认存储账户的名称只能是小写字母数据字符。如果名称是唯一的，您会在文本框的右侧看到绿色的复选标记。\n一旦您创建机器学习工作区，Azure将提供全新的Azure机器学习工作区供您创建和承载您的额Azure机器学习实践。\n在创建您的Azure机器学习之后，单击您的新的Azure机器学习工作区的图标您将看到如下图所示的界面。\n请注意，这是由Azure管理门户的登陆Azure机器学习工作区。在此，您可以直接访问Azure机器学习Studio的工具管理用户访问工作区的权限，管理在此工作区中承载机器学习实验的Web服务。在顶部的导航菜单提供了Azure机器学习工作区功能的导航。\n仪表板（DASHBOARD）可以监控您的工作区的在一段时间内的相对和绝对计算的使用量。\n配置（CONFIGURE）功能用来允许或者拒绝用户对于您的工作的访问。\nWeb服务（WEB SERVICES）选项允许您管理web服务和配置端口，且包含为数据科学家和分析师通过请求/响应的API访问或者批处理的代码示例，代码示例由流行的编程语言C#、Python和R语言组成。\n在我们重温这些特性的细节之后将开始探索Azure 机器学习的环境。\n若要访问您的工作区，点击ML Studio的登陆链接进入您的新的 Azure 机器学习工作区。下图就是Azure ML Studio工作区的截图。\n当您首次进入Azure ML Studio的工作区，您会看到如下的导航选项在顶部和左侧的导航栏上：\n顶部导航栏：\nHome 文档和其他资源的链接；\nStudio Azure ML Studio实验的登录页；\nGallery (Preview) 是趋势实验和样本的集合。\n左侧导航栏：\nEXPERIMENTS 实验的创建、运行和保存草稿；\nWEB SERVICES已发布的实验列表；\nDATASETS上载的数据集，可用于实验的计算；\nTRAINED MODELS 使用Azure ML Studio内置的机器学习算法\"训练\"的新预测模型；\nSETTINGS 设置的账户和资源的设置集合。"}
{"content2":"在使用机器学习算法过程中，针对不同的问题需要不用的模型评估标准，这里统一汇总。主要以两大类分类与回归分别阐述。\n一、分类问题\n1、混淆矩阵\n混淆矩阵是监督学习中的一种可视化工具，主要用于比较分类结果和实例的真实信息。矩阵中的每一行代表实例的预测类别，每一列代表实例的真实类别。\n真正(True Positive , TP)：被模型预测为正的正样本。 假正(False Positive , FP)：被模型预测为正的负样本。 假负(False Negative , FN)：被模型预测为负的正样本。 真负(True Negative , TN)：被模型预测为负的负样本。 真正率(True Positive Rate,TPR)：TPR=TP/(TP+FN)，即被预测为正的正样本数 /正样本实际数。 假正率(False Positive Rate,FPR) ：FPR=FP/(FP+TN)，即被预测为正的负样本数 /负样本实际数。 假负率(False Negative Rate,FNR) ：FNR=FN/(TP+FN)，即被预测为负的正样本数 /正样本实际数。 真负率(True Negative Rate,TNR)：TNR=TN/(TN+FP)，即被预测为负的负样本数 /负样本实际数/2\n2、准确率（Accuracy）\n准确率是最常用的分类性能指标。\nAccuracy = (TP+TN)/(TP+FN+FP+TN)\n即正确预测的正反例数 /总数\n3、精确率（Precision）\n精确率容易和准确率被混为一谈。其实，精确率只是针对预测正确的正样本而不是所有预测正确的样本。表现为预测出是正的里面有多少真正是正的。可理解为查准率。\nPrecision = TP/(TP+FP)\n即正确预测的正例数 /预测正例总数\n4、召回率（Recall）\n召回率表现出在实际正样本中，分类器能预测出多少。与真正率相等，可理解为查全率。\nRecall = TP/(TP+FN)，即正确预测的正例数 /实际正例总数\n5、F1 score\nF值是精确率和召回率的调和值，更接近于两个数较小的那个，所以精确率和召回率接近时，F值最大。很多推荐系统的评测指标就是用F值的。\n2/F1 = 1/Precision + 1/Recall\n6、ROC曲线\n逻辑回归里面，对于正负例的界定，通常会设一个阈值，大于阈值的为正类，小于阈值为负类。如果我们减小这个阀值，更多的样本会被识别为正类，提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了直观表示这一现象，引入ROC。根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve，横坐标为False Positive Rate(FPR假正率)，纵坐标为True Positive Rate(TPR真正率)。一般情况下，这个曲线都应该处于(0,0)和(1,1)连线的上方,如图：\nROC曲线中的四个点和一条线: 点(0,1)：即FPR=0, TPR=1，意味着FN＝0且FP＝0，将所有的样本都正确分类。 点(1,0)：即FPR=1，TPR=0，最差分类器，避开了所有正确答案。 点(0,0)：即FPR=TPR=0，FP＝TP＝0，分类器把每个实例都预测为负类。 点(1,1)：分类器把每个实例都预测为正类。 总之：ROC曲线越接近左上角，该分类器的性能越好。而且一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting\n7、AUC\nAUC（Area Under Curve）被定义为ROC曲线下的面积(ROC的积分)，通常大于0.5小于1。随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是 AUC 值。AUC值(面积)越大的分类器，性能越好，如图：\n8、PR曲线\nPR曲线的横坐标是精确率P，纵坐标是召回率R。评价标准和ROC一样，先看平滑不平滑（蓝线明显好些）。一般来说，在同一测试集，上面的比下面的好（绿线比红线好）。当P和R的值接近时，F1值最大，此时画连接(0,0)和(1,1)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好像AUC对于ROC一样。一个数字比一条线更方便调型。\n有时候模型没有单纯的谁比谁好（比如图二的蓝线和青线），所以选择模型还是要结合具体的使用场景。下面是两个场景： 1，地震的预测 对于地震的预测，我们希望的是RECALL非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲PRECISION。情愿发出1000次警报，把10次地震都预测正确了，也不要预测100次对了8次漏了两次。 2，嫌疑人定罪 基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。即时有时候放过了一些罪犯（recall低），但也是值得的。 对于分类器来说，本质上是给一个概率，此时，我们再选择一个CUTOFF点（阀值），高于这个点的判正，低于的判负。那么这个点的选择就需要结合你的具体场景去选择。反过来，场景会决定训练模型时的标准，比如第一个场景中，我们就只看RECALL=99.9999%（地震全中）时的PRECISION，其他指标就变得没有了意义。\n当正负样本数量差距不大的情况下，ROC和PR的趋势是差不多的，但是在正负样本分布极不均衡的情况下，PRC比ROC更能真实的反映出实际情况，因为此时ROC曲线看起来似乎很好，但是却在PR上效果一般。\n二、回归问题\n拟合（回归）问题比较简单，所用到的衡量指标也相对直观。假设yiyi是第ii个样本的真实值，ŷ iy^i是对第ii个样本的预测值。\n1. 平均绝对误差（MAE）\n平均绝对误差MAE（Mean Absolute Error）又被称为l1范数损失（l1-norm loss）：\n2. 平均平方误差（MSE）\n平均平方误差MSE（Mean Squared Error）又被称为l2范数损失（l2-norm loss）：\n3、均方根误差（RMSE）\nRMSE虽然广为使用，但是其存在一些缺点，因为它是使用平均误差，而平均值对异常点（outliers）较敏感，如果回归器对某个点的回归值很不理性，那么它的误差则较大，从而会对RMSE的值有较大影响，即平均值是非鲁棒的。\n4、解释变异\n解释变异（ Explained variance）是根据误差的方差计算得到的：\n5、决定系数\n决定系数（Coefficient of determination）又被称为R2分数：\n三、聚类\n1 . 兰德指数\n兰德指数（Rand index）需要给定实际类别信息C，假设K是聚类结果，a表示在C与K中都是同类别的元素对数，b表示在C与K中都是不同类别的元素对数，则兰德指数为：\n其中数据集中可以组成的总元素对数，RI取值范围为[0,1]，值越大意味着聚类结果与真实情况越吻合。\n对于随机结果，RI并不能保证分数接近零。为了实现“在聚类结果随机产生的情况下，指标应该接近零”，调整兰德系数（Adjusted rand index）被提出，它具有更高的区分度：\n具体计算方式参见Adjusted Rand index。\nARI取值范围为[−1,1]，值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲，ARI衡量的是两个数据分布的吻合程度。\n2. 互信息\n互信息（Mutual Information）也是用来衡量两个数据分布的吻合程度。假设UU与VV是对NN个样本标签的分配情况，则两种分布的熵（熵表示的是不确定程度）分别为：\n利用基于互信息的方法来衡量聚类效果需要实际类别信息，MI与NMI取值范围为[0,1]，AMI取值范围为[−1,1]，它们都是值越大意味着聚类结果与真实情况越吻合。\n3. 轮廓系数\n轮廓系数（Silhouette coefficient）适用于实际类别信息未知的情况。对于单个样本，设aa是与它同类别中其他样本的平均距离，bb是与它距离最近不同类别中样本的平均距离，轮廓系数为：\n对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。\n轮廓系数取值范围是[−1,1][−1,1]，同类别样本越距离相近且不同类别样本距离越远，分数越高。\n四、信息检索\n信息检索评价是对信息检索系统性能（主要满足用户信息需求的能力）进行评估，与机器学习也有较大的相关性，感兴趣的可以参考这篇不错的博文。\n五、总结\n上面介绍了非常多的指标，实际应用中需要根据具体问题选择合适的衡量指标。那么具体工作中如何快速使用它们呢？优秀的Python机器学习开源项目Scikit-learn实现了上述绝指标的大多数，使用起来非常方便。"}
{"content2":"最后运行效果如下：\n（5）显示上面的dataset分类结果，并且保存到一个文件里面。\n（6）将上面保存的dataset图片读取进来，并且获得它第2通道图片特征，显示这个图片特征灰色图片如下图，结合上面dataset图片很明显，需要特别显示的大细胞和非细胞已经区分得很清晰了。\n（7）现在直接把dataset的第二特征图片通过彩色显示出来，就更加清晰地看到：细胞区域与非细胞区的具体位置了。"}
{"content2":"AI，大数据，复杂系统 最精 40本大书单\n原创 2017-10-30 Peter 混沌巡洋舰\n如果这篇文的题目变成最全书单，那么这篇文会变得又臭又长，这个年代，关于人工智能和大数据的书，没有一万本也有一千本，而这里列出的40本，则是精选过的，不敢说每一本都字字珠玑，但这个书单保证没有一本水书。废话不说，赶快上车，先放思维导图，再一本本的简单说说。\n书单分成8部分，其中的数字代表我对这一系列的书的推荐程度。\n先说经典书的部分\n终极算法：机器学习和人工智能如何重塑世界\n这本书的名字，显示着作者试图在机器学习的各个流派间进行整合，最终提出机器学习里的“牛顿三定律”的理想。作者在这本书里，介绍了当前常用的算法的发展历程，这些算法包括决策树，遗传算法，神经网络，朴素贝叶斯及贝叶斯网络，隐式马尔可夫链，K最近邻及支持向量机，作者还介绍了无监督学习的算法。在介绍算法时，作者还介绍了机器学习里最大的两个阻碍，过拟合及维度灾难。\n对上面的这些名词看不懂，看过书你就明白了。这本书中，没有公式与代码，有的只是对机器学习中的算法本质一针见血的点破，有的只是依据这些算法而编出的日常生活中的故事，是对机器学习中核心算法的概念化的模型。一言以概之，这是一本所有有高中数学水平且无计算机背景的读者都能够读懂的科普书。如果你不想对控制着我们衣食住行方方面面的机器学习算法一无所知，那么这本书是你必读的书。\n人工智能之父马文·明斯基经典作品：情感机器+心智社会\n这两本书的作者被誉为人工智能之父，不是因为他发现了某一个特别NB的算法。而是因为其对人类的认知过程有着独特的见解，从而能利用对人类认知的洞察来指导机器学习算法的研发。其在70年代写成的心智社会一书，令当前的人工智能研究者还会常读常新。这本书虽然价格有些高，但考虑到读一遍根本不指望能看懂，要看三遍才能有些领悟，算算阅读单价，就不算高的。再加上这本书送朋友，那是多么有逼格的一件事啊。\n这本书是人工智能之父集一生功力写成的集大成之作。如何让机器有感情，是在机器智能即将超越人之后的人工智能的下一个天花板。情感计算的概念，也随着Chatbot（聊天机器人）而火了起来。阅读这本书，会让读者认识到情感不一定是人类独有的特征。情况也可以被表示为一连串的计算。而赋予机器情感，我们也能造成有常识，有直觉的机器。如果你想打破人工智能的黑盒子，这本书也是一本需要反复研读的大作。\n数学之美\n这本书虽然叫做数学之美，其实由于作者吴军博士是谷歌的搜索专家，所以写的多半是自然语言处理领域的发展。关于这本书，溢美之词已经太多了。而我这里想说的不是其将算法背后的原理讲述的多么清晰，而是作者讲述了其和诸位自然语言处理领域的先驱的个人故事，其中描述了诸多学者的风骨以及其背后的道德力量。这是这本书少有被人提起，但却能令人记忆深刻的地方。\n人工智能的未来\n这本书的中文版已经绝版，在网上搜这本书，多半搜出的是雷库兹曼的原名为How to create a mind 的书的翻译版。对于这本04 年的书，书的作者杰夫·霍金斯(Jeff Hawkins)，成功的计算机工程师和企业家，掌上型电脑PalmPilot、智能电话Treo等产品的发明人。这本书提出的HTM模型，可能凭其单一的结构而第一次产生自我学习的“智能”，其理论的高瞻远瞩，启发了当今的深度学习浪潮。\n理解信念：人工智能的科学理解\n这本书的作者尼尔斯•尼尔森（NilsJ.Nilsson）是斯坦福大学教授。这是一本哲学书。其核心论旨包括，我们人类的感觉系统是获取外界信息、形成信念的唯一途径；运用科学方法、经由严格批评和修正而建立起来的信念，是相对真实且更为有用的。人工智能在某种程度上，和人类一样拥有信念，或者可以说，人类是一台台复杂的机器。\n智能的本质 人工智能与机器人领域的64个大问题\n在书中，作者从常识出发，对人工智能和机器人表达了很多“令人惊讶”而又让人深思的观点。例如在陪伴老年人方面，迄今为止先进的机器人都不如狗做得好。书中充满了思辨和哲学判断，感觉作者属于乐观派中的悲观派，乐观的是认为不会出现终结者，悲观的是AI发展还是太慢了。 我认为是近年来不可多得的好书。 同时，作者担心的并不是机器智能的迅速提高，而是人类智力可能会下降，这才是最值得担忧的。\n接着是AI对商业和我们生活的影响。这一系列的书很多，选出几本我读过的\n第二次机器革命\n《第二次机器革命》这本书，是那种能够在机场书店找到的图书，这本书不算厚，两三个小时就可以读完，也不算烧脑。书中的内容围绕着以人工智能和数字化为代表的技术对未来社会的影响展开论述。这本书令我记忆最深的是每一章开篇引用的名言，即幽默又别有深意。\n人工智能时代\n这本书的作者Kaplan是斯坦福大学顶尖人工智能专家。卡普兰本科毕业于芝加哥大学历史与科学哲学专业，之后考入宾夕法尼亚大学计算机科学专业，后进入斯坦福大学人工智能实验室工作。这本书的英文名直译过来是人不必遵守机器的规则。这是一个老人写的书，这种警世的书也需要由一个老人写出，作者见证了人工智能的低潮与复兴，见证了越来越大的贫富差距。他活到了替子女说话的年纪，又没有丢掉幽默。这样智慧的老人，值得我们去倾听。\n与机器人共舞\n凯恩斯就曾指出，科技将取代工作岗位，而非整体工作量。这些改变了我们工作方式、互动方式以及娱乐方式的创新，将给21 世纪的社会带来翻天覆地的改变，这种影响几乎等同于20世纪初机械设备将农耕经济带向工业经济时，社会所经历的根本性变革。这本书的作者是曾获得普利策奖的资深记者，视角全面，分析深入。\n接着说说大数据方面的书\n爆发：大数据时代预见未来的新思维\n本书作者全球复杂网络权威Barabasi所作，一本超越《黑天鹅》的惊世之作。作者认为人类正处在一个聚合点上，在这里数据、科学以及技术都联合起来共同对抗那个最大的谜题——我们的未来。作者指出人类日常行为模式不是随机的，而是具有“爆发性”的。爆发揭开了人类行为中令人惊讶的深层次的秩序，使得人类变得比预期中更容易预测得多。爆发模式的揭示，其影响力将与20世纪初期的物理学或者基因革命的影响力不相上下。\n智慧社会：大数据与社会物理学\n这本书的作者是MIT人类动力学实验室主任。这本书提出了一种量化的办法，来定向测度沟通对行为的影响，即想法流（idea flow）的传播的规律。量化的考察沟通对创新的影响。作者用可控双盲实验去验证诸如社会网络的大小与想法的多样性有正相关关系；社会网络的互动密度与效率显著相关等习以为常的观点，使得全书的科学很强。\n大数据可视化：重构智慧社会\n大数据的目的最终还是要讲一个好故事。而人类是一种视觉动物，一幅图的效果好过千言万语。这本书举出了很多第一线的例子，来说明怎么样去做出好的数据可视化，对于任何要处理数据的人来说，这本书中的道理都是必不可少的，须要透彻掌握的。\n大数据思维与决策\n这本书的作者是计量经济学家，这本书展示了社会科学的全面数字化。没有数字就没有真相。作者指出统计是一个非常强大的研究社会问题的手段，可以应用在任何你想要的领域。社会学科的专家，将越来越依靠大数据模型做出判断，直觉和数据统计呈现出互补的趋势。而在善于利用大数据的商家面前，消费者将越发无计可施。\n赤裸裸的未来·大数据时代：如何预见未来的生活和自己\n正如这本书的书名所展示的，就个人而言，我们早已生活在一个“超级透明”的世界，我们泄露出去的海量信息无处不在。若将这些信息收集起来，加以分析，就能勾勒出每一个人的真实性格、内心偏好，乃至可以预测每个人的命运。作者大胆预言：“大数据时代”只不过是一朵小浪花，终将会被更新、更前沿的“物联网时代”取代，并以灾难预测、流行病预防、犯罪防治、潜能开发、情绪管理、恋爱情感、个性化学习、娱乐私人定制等领域为例，描绘了一个富有激情的美好未来。\n白话大数据与机器学习\n本书通俗易懂，有高中数学基础即可看懂，同时结合大量案例与漫画，将高度抽象的数学、算法与应用，与现实生活中的案例和事件一一做了关联，将源自生活的抽象还原出来，帮助读者理解后，又带领大家将这些抽象的规律与算法应用于实践。\n大数据：从概念到运营\n有多少人只是谈论大数据，却不知道该怎么将大数据应用到具体的工作中去，这本书的作者有着在大数据领域拥有超过20年的从业经历，曾担任雅虎公司广告分析副总裁，在数据存储、商业智能和数据分析利用方面有着独到的见解。本书一共有十三个章节，在书中作者将大数据在实际运用中的方方面面通过具体的案例进行了分析。侧重于大数据的实际运用方面而不是理论的探讨。大量的案例使得书中观点鲜活有力。\nPython 金融大数据分析\n大数据的应用最广的领域，无疑是数据驱动的金融业。作为该领域的入门书，这本书介绍了python语言在金融数据可视化，金融衍生品定价，金融时间序列数据处理，蒙特卡罗方法等话题上的具体应用，是一本简单明了的入门书。\n接下来的书关于数学，这是所有数据科学的基本功\n妙趣横生的统计学\n这是本统计学入门书，涉及了很多高中课程中的内容，例如我们是不是比父母更聪明？开车时打电话与酒驾一样危险吗？坐飞机和开车，哪种方式更安全？钻石越重，价格就越高吗？小学四年级的学生可以用统计学做什么？这本书的目标是日常生活所需要的统计思想、正确分析数据的基本路径。\n统计会犯错\n这本书的大部分例子来源于临床医学。但书中的道理却可以应用到任何领域上。这本书可以当作一本统计学文章阅读踩雷指南，至少看完了对一些得出千奇百怪的结论的文章抱有怀疑，即使是权威期刊刊登的文章。最后指出的因为保密造成的数据不公开问题也是值得深思的。\n用数学的语言看世界\n本书为理论物理学家大栗博司先生写给自己女儿的数学读本，全书以用“数学语言”解读自然为线索，用生动故事和比喻重新讲解了数学的核心原理与体系，并且讲解了把数学作为一门“语言”的思维方式，是数学入门，重新理解数学的科普佳作。该作者写的书都不错，这里只推荐其中最好懂的一本。\n改变世界的134个概率统计故事\n哲学家耶安哈金指出，统计学是1900年后人类的二十大发明之一。到了21世纪，正如家赫伯特乔治威尔斯在1903年所预言的那样，“统计式的思考将会和读写能力一样，成为优秀社会人士的必备技能”。此书以概率论为主，讲数学家的八卦。加上深入浅出的故事化讨论，让统计学不再枯燥。\n贝叶斯思维：统计建模的Python学习法\n前面都是科普书，这次来本教科书。这本书是根据作者在美国大学讲授相关课程的讲义编撰而成的。结合生活中的案例+代码实现+分析，让读者了解贝叶斯思维的威力，帮助你在生活的各个方面获得清晰的思维， 举书中的例子 战争环境下（二战德军坦克问题），法律问题上（肾肿瘤的假设验证），体育博彩领域（棕熊队和加人队NFL比赛问题），通过阅读，作者潜移默化的帮助读者形成了建模决策的方法论，建模误差和数值误差怎么取舍，怎样为具体问题建立数学模型，如何抓住问题中的主要矛盾（模型中的关键参数），再一步一步的优化或者验证模型的有效性或者局限性。\n程序员的数学\n编程的基础是计算机科学，而计算机科学的基础是数学。本书面向程序员介绍了编程中常用的数学知识，借以培养初级程序员的数学思维。读者无需精通编程，也无需精通数学，只需具备四则运算和乘方等基础知识，就可以阅读本书。这是一套书，分成三部分，涵盖线性代数概率论和基本的代数。\n接下来的书和复杂系统有关\n复杂\n如果你之前对复杂性科学还没有太多了解，那这本书可以成为你复杂性科学的第一本书。\n蚁群在没有中央控制的情况下为何会表现出如此精密的复杂行为？\n数以亿计的神经元是如何产生出像意识这样极度复杂的事物？\n是什么在引导免疫系统、互联网、全球经济和人类基因组等自组织结构？\n理解复杂系统需要有全新的方法，需要超越传统的科学还原论，并重新划定学科的疆域。\n借助于圣塔菲研究所的工作经历和交叉学科方法，复杂系统的前沿科学家米歇尔以清晰的思路介绍了复杂系统的研究，横跨生物、技术和社会学等领域，并探寻复杂系统的普遍规律，探讨了复杂性与进化、人工智能、计算、遗传、信息处理、代谢比例、网络科学等领域的关系。\n《Complexity_A Very Short Introduction》\n本书作者John Holland是复杂理论和非线性科学的先驱，遗传算法之父。本书介绍了复杂性科学的一些基本概念和核心架构，如complex physical system(CPS)、complex adaptive system(CAS)，描述了复杂系统的特征如涌现性质、自组织行为、混沌行为、胖尾分布、适应性行为。\n正如作者在写完这本书后意识到，将一些概念以最本质的简单的形式表现出来后，会发现一些原先分开的话题间被忽视的联系，希望你读完这本书后也有这样的感觉。而且，这本书真的是Very Short。\n复杂性思维\n本书是德国慕尼黑工业大学教授迈因策尔的代表作，14年出版的原书07年第五版的中译本。迈因策尔教授的研究范围遍及数学、物理学、科学哲学，尤其在复杂系统、非线性动力学等领域多有建树。\n本书从哲学的高度（在这里窃以为哲学不能指导科学，但可以为科学澄清意义），从科学前沿探索与人类心智探险史的结合中，广泛涉猎物理学、生命科学、认知科学、计算机科学、经济学、社会学等诸多方面。\n从物理世界的进化到生命世界的进化，从意识的起源到认知科学的兴起，从社会政治系统到社会经济系统的运行，从哲学史到哲学前沿的反思，揭示了不同学科体现出的共同的复杂性特征，阐释了对复杂性的探索将如何引起人们思维方式的深刻变化，引起的世人对共同未来的关怀。\nThinking complexity\n要认识一门学科，不止需要了解概念，还需要亲自动手，get your hand dirty。\nThinking complexity 以python为基础，演示了多种复杂系统的模型，让在计算机诞生之前难以验证的理论得以模拟，并逐步建立起复杂演绎基础之上的新认知模式。Python语言简单易懂，但书中的很多代码、练习有时间还得需要仔细研究实践。本书内容短小，但是信息量很大，关键看你是走马观花的读，还是一行行代码地进行实践了，收获是不一样的。\n复杂性科学涵盖了各种主题。这些主题之间相互关联，但需要花费不少时间才能搞清楚这些联系。为了帮助读者看到全景，这本书阅读列表，这些都来自于该领域最流行的研究成果。阅读列表以及关于如何使用它的建议在附录B中。这本书提供了一系列练习；很多练习都要求读者重新实现一些开创性实验并对其进行扩展。复杂性吸引人的一个地方在于我们可以通过适当的编程技能与数学知识接触研究前沿。\n这本书的内容覆盖：小世界图，无标度网络，细胞自动机，生命游戏，分形，自组织临界性，基于主体的模型(agent based model) 及几个现实中的案例分析。是复杂性研究入门参考好书。另外本书还可以用作Python编程与算法的大学中级课程教材。既是你对python和算法一无所知，其前三章的内容也可以让你能够接着看下去。\n《复杂_诞生于秩序与混沌边缘的科学》\n这是一本老书了，“类似于纪实小说，介绍了复杂性科学的研究中心圣塔菲研究所建立、发展的情况。你会看到那些不同领域的人是怎样由于共同的志趣走到了一起，以及又是如何涌现出诸如遗传算法、人工生命、细胞自动机、正反馈经济系统、动态博弈系统等等新思想的。\n这本书以小说一样的手法介绍了研究所里面个个人物的动人故事，以及他们研究的那些激动人心的成果。这本书的出版可以说给中国的学术界打开了一扇窗子，让我们真正的了解了国外的复杂性科学。有人称这本书是复杂性科学的圣经是不为过的。”\n大师说科学与哲学计算机与复杂性科学的兴起\n这是作者著名物理学家兼科学作家海因茨•R.帕格尔斯的遗作。是一本随笔集，将科学讨论带往更高层次，除了预言复杂性科学对人类的影响，也讨论了分道扬镳的科学与哲学如何才能重新融合。书中论及的话题包括：生物组织原理的重要性、以计算法来看数学及物理过程、并行计算网络以及非线性动力学的重要性、对混沌的了解、实验数学、神经网络和平行分配处理。\n下面的书将说说人类最担心的强AI的出现。\nLife 3.0\n世界两个顶级学术期刊\"nature\" 和“Science” 上每周都会推荐几本新出的科学主题的科普书，而一本书若是能同时被这俩家杂志推荐，则更是难得。今年8月25号出版《life 3.0》正是这样一本书，这本书的副标题是在人工智能的时代作为人意味着什么，作者不是专职搞计算机的，而是本行物理的普林斯顿教授。\n超级智能\n很多人提到强AI，说起的第一本书就是这个。本书作者尼克‧波斯特洛姆，全球著名思想家，牛津大学人类未来研究院的院长，哲学家和超人类主义学家。在这本书中，作者谈到了超级智能的优势所带来的风险，也谈到了人类如何解决这种风险。作者认为，他的这本书提到的问题将是我们人类所面临的最大风险。\n人工智能革命：超级智能时代的人类命运\n本书的难得之处还在于，它既不哗众取宠，也没有把问题过分简单化。本书作者毕业于蔡斯牛津大学哲学系，本书在人工智能的憧憬与危机之间敏锐地寻找平衡，对于所有好奇当今世界正在发生什么、我们是怎样走到今天、又将走向何方的人来说，本书都是一本必读书。\n如何思考会思考的机器\n关于强AI，一定需要大众的讨论，而这本书由世界上最聪明的头脑共同写成。包括全 球大数据权威阿莱克斯•彭特兰、世界顶级语言学家史蒂芬•平克、生物地理学家贾雷德•戴蒙德、互联网思想家凯文•凯利、《全球概览》创始人斯图尔特•布兰德等Edge 网站出品，必属精品。\n我们最后的发明\n这本书是一个纪录片导演的末世预言，核心观点是ASI（超级人工智能）极有可能毁灭人类，然后细致地逐个批判了库兹韦尔等乐观派。这本书的好处是好玩、有趣、思路清奇、剑出偏锋，但从知识的角度来说，它其实不是那么“科学”、不那么“理性”\n接着来说最火的深度学习\n白话深度学习与TensorFlow\n白话深度学习与TensorFlow这本书覆盖了深度学习的诸多概念，内容全面，看完了这本书，你就懂了深度学习这个领域的行话了。这本书也许不会教会你代码或者tensorflow，但却能让你明白深度学习是什么。书中包含很多具体例子，作者有丰富的实践经验。\n机器学习之路\n机器学习之路这本书从内容方面本书共包含两部分：机器学习篇和深度学习篇。这本书避过数学推导等复杂的理论推衍，介绍模型背后的一些简单直观的理解，以及如何上手使用。这本书适合有一些编程和自学能力，但数学等基础理论能力不足的人群。\n深度学习与R语言\n说起深度学习，想到的都是python为基础的语言，其实作为一种开源的数据建模语言，R也是可以做深度学习的。这本书介绍了深度学习基础知识后，着重介绍两种不那么流行的网络结构，受限玻耳兹曼机和深度置信网络，并通过生物信息和自然语言处理领域的实际例子来说明深度学习的优势和局限。\n最后说说AI的历史\n科学的极致 漫谈人工智能\n集智俱乐部有一群有激情有实力的小伙伴，其中既有来自学术界的张江教授，也有基于深度学习开发了彩云天气，彩云翻译等APP的创业者。而这本书则是集智众人的智慧结晶。这本书由于是中国人写成，所以避免了翻译作品的语言障碍。杨澜曾经在她的博客中推荐过这本书，说她从这本书中收获甚多，可见这本书是很容易读懂的。正如书名所显示，这本书涉及诸多人工智能领域。而书中诸多的插图，例子和参考文献则让这本书赢在了细节上。\n贤二机器僧漫游人工智能\n这本书贤二诞生的初衷和过程。书中的漫画超有趣。贤二机器僧这样一个传统佛法与现代科技相结合的产物表明，科技本身没有对错、好坏，它是中性的，但人的心却可善可恶。佛教徒不应该排斥科学，而应该拥抱科学，善于运用科技手段和成果，成就更多利于他人的事业。\n硅谷之谜\n作为浪潮之巅的续集，读完了这本书，想说的是硅谷的历史是不可复制的，AI的发展，是伴随着大公司的成败而起的。我们已经站在了AI发展的最前沿，不能只照搬前人的经验了。要做的是透彻的明白工业时代和后工业时代的本质，用一种全新的基于信息论、控制论、系统论的思维方式来从下而上的去想问题。"}
{"content2":"《百面机器学习算法工程师带你去面试》收录了超过100道机器学习算法工程师的面试题目和解答，其中大部分源于Hulu算法研究岗位的真实场景。本书从日常工作、生活中各种有趣的现象出发，不仅囊括了机器学习的基本知识 ，而且还包含了成为出众算法工程师的相关技能，更重要的是凝聚了笔者对人工智能领域的一颗热忱之心，旨在培养读者发现问题、解决问题、扩展问题的能力，建立对机器学习的热爱，共绘人工智能世界的宏伟蓝图。\n完全超出了预想，书写的比想象的好多了，一看作者就是做了多年机器学习相关工作的，并且有异于市面上千篇一律的经典书籍的注重公式推导和概念阐述，这书有些实践应用多年才会有的思考，里面的很多问题也很有意思，第一次发现原来可以通过这个角度重新思考。总之是很棒的一本书。\n学习下载: https://pan.baidu.com/s/1fDUkrUFBURHIPbrdtJqAEw\n提取码: rpzs\n《百面机器学习算法工程师带你去面试》高清PDF版，392页，带书签目录，文字可以复制；\n《百面机器学习算法工程师带你去面试》高清epub版，101页，带书签目录，文字可以复制。\n本书作为AI类书籍知识点非常全面，但在一些章节比如分类模型、神经网络、强化学习等，领域内知识体系展开的不是很详细深入。其实作为面试类书籍，读的时候应该以一种查漏补缺的方式去看，看到一个问题想一想自己这个知识点掌握了没有，有没有相关，或者可以延展的问题曾经自己没有搞清楚，再去有针对性地深入学习探究，这才是这本书带给大家的新的思考方式和学习方式。\n本书将从特征工程、模型评估、降维等经典机器学习领域出发，构建一个算法工程师必-备的知识体系；见神经网络、强化学习、生成对抗网络等新科研进展之微，知深度学习领域胜败兴衰之著；“博观而约取，厚积而薄发”，在末一章为读者展示生活中各种引领时代的人工智能应用。\n更多《python机器学习》《python深度学习》等学习推荐资料：https://pan.baidu.com/s/1iEXkbbyVUuzgQOqZXXcNuQ"}
{"content2":"大家都知道，在2016年，Google DeepMind的AlphaGo打败了韩国的围棋大师李世乭九段。在媒体描述DeepMind胜利的时候，将人工智能（AI）、机器学习（machine learning）和深度学习（deep learning）都用上了。这三者在AlphaGo击败李世乭的过程中都起了作用，但它们说的并不是一回事。\n今天就用最简单的方法——可视化的展现它们三者的关系和应用。\n如上图，人工智能是最早出现的，也是范围最大的；其次的机器学习，稍晚一点；最内侧，是深度学习，当今人工智能大爆发的核心驱动。\n五十年代，人工智能曾一度被极为看好。之后，人工智能的一些较小的子集发展了起来。显示机器学习，然后是深度学习。深度学习又是机器学习的子集。深度学习造成了前所未有的巨大影响。\n从概念的提出到走向繁荣\n1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的语言；或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。\n过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流（大数据）的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发。\n让我们慢慢梳理一下计算机科学家们是如何将人工智能从最早的一点点苗头，发展到能够支撑那些每天被数亿用户使用的应用的。\n人工智能（Artificial Intelligence）——为机器赋予人的智能\n早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。\n人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。\n我们目前能实现的，一般称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，Pinterest上的图像分类；或者Faceboo的人脸识别。\n这些是弱人工智能在实践中的例子。这些技术实现的是人类智能的一些具体的局部。但它们是如何实现的？这种智能是从何而来？这就带我们来到机器学习。\n机器学习——一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。\n与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、强化学习和贝叶斯网络等等。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。\n机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。\n这个结果还算不错，但并不是那种能让人为之一振的成功。特别是遇到云雾天，标志牌变得不是那么清晰可见，又或者被树遮挡一部分，算法就难以成功了。这就是为什么前一段时间，计算机视觉的性能一直无法接近到人的能力。它太僵化，太容易受环境条件的干扰。\n随着时间的推进，学习算法的发展改变了一切。\n深度学习——一种实现机器学习的技术\n人工神经网络是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。\n例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。\n每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。\n我们仍以停止（Stop）标志牌为例。将一个停止标志牌的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、救火车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有的权重，给出一个经过深思熟虑的猜测——“概率向量”。\n这个例子里，系统可能会给出这样的结果：86%可能是一个停止标志牌；7%的可能是一个限速标志牌；5%的可能是一个风筝挂在树上等等。然后网络结构告知神经网络，它的结论是否正确。\n即使是这个例子，也算是比较超前了。直到前不久，神经网络也还是为人工智能圈所淡忘。其实在人工智能出现的早期，神经网络就已经存在了，但神经网络对于“智能”的贡献微乎其微。主要问题是，即使是最基本的神经网络，也需要大量的运算。神经网络算法的运算需求难以得到满足。\n不过，还是有一些虔诚的研究团队，以多伦多大学的Geoffrey Hinton为代表，坚持研究，实现了以超算为目标的并行算法的运行与概念证明。但也直到GPU得到广泛应用，这些努力才见到成效。\n我们回过头来看这个停止标志识别的例子。神经网络是调制、训练出来的，时不时还是很容易出错的。它最需要的，就是训练。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结构。\n只有这个时候，我们才可以说神经网络成功 地自学习到一个停止标志得样子；或在Facebook得应用里，神经网络自学习了你妈妈的脸；又或者是2012年吴恩达教授在Google实现了神经网络学习到猫得样子等等。\n吴教授得突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入的海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中得图像。吴教授为深度学习（deep learning）加入了“深度”（deep）。这里的“深度”就是说神经网络中众多的层。\n现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。\n深度学习，给人工智能以璀璨的未来\n深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n人工智能就在现在，就在明天。有了深度学习，人工智能甚至可以达到我们畅想的科幻小说一般。"}
{"content2":"#对coursera上Andrew Ng老师开的机器学习课程的笔记和心得；\n#注:此笔记是我自己认为本节课里比较重要、难理解或容易忘记的内容并做了些补充，并非是课堂详细笔记和要点；\n#标记为<补充>的是我自己加的内容而非课堂内容，参考文献列于文末。博主能力有限，若有错误，恳请指正；\n#---------------------------------------------------------------------------------#\n多层神经网络模型：\n，\n<补充>:每一个单元有一定数量的实值输入，产生单一的实值输出(可以是其他很多单元的输入)；\n符号标记：ai(j):activation of unit i in layer j ；Ɵ(j) :matrix of parameters controlling the function mapping from layer j to layer j+1；\n#---------------------------------------------------------------------------------#\n神经网络的cost function:\n前一项的目的是使所有单元的误差和最小(采用对数损失函数)，后一项是regularization项，旨在控制模型复杂度，防止overfitting；\n#---------------------------------------------------------------------------------#\nforward propagation(前向传播)\n<补充>：其实也就是通过神经网络，从输入参数到输出结果的计算过程(只计算一次)；\n参数的计算如下：\n，其中g(x)是sigmoid函数；\n#---------------------------------------------------------------------------------#\nBack propagation(反向传播)：与前向传播非常类似，从结果层倒推回输入层，计算每层δ的过程，δ为误差；\n，其中：l指第几层，；\n注:第一层是输入层，没有δ1项，最后一层(输出层)的δ不是按此式计算，见下例；\n，\nδ4 = a4 - y，δ3 = (Ɵ3)T δ4 . *(a3 . * (1 - a3))，δ2 = (Ɵ2)T δ3 . *(a2 . * (1 - a2))；\n#---------------------------------------------------------------------------------#\nBack propagation algorithm(反向传播算法)\n<补充>：一个最优化问题，目的是在使cost function值最小(这里是通过偏导最小来实现)的情况下，训练出神经网络各个参数的权值；\n算法如下:\n1，给出训练集作为输入，，将delta值设为0，；\n2，进行下列过程直至性能满足要求为止:\n对于每一训练(采样)输入，\n(a) 通过前向传播计算所得输出。\n(b) 通过反向传播计算每层的δ值；\n(c) 更新delta值：；\n3，得到神经网络参数的权值:\n,其中：;\n#---------------------------------------------------------------------------------#\n几则关于神经网络的问题和解决办法\n1，Gradient checking：反向传播算法有很多细节，非常容易出错，Gradient checking有助于cost function J(Ɵ)的准确性；\n原理：比较由反向传播计算得到的DVec和梯度计算得到的gradApprox两者是否相近似来判断；\n<补充>：其实是用了微积分当中导数的概念，；\n注:在训练数据时需要将Gradient checking代码注释掉，因为gradApprox的计算是很耗时的；\n2，Random initialization：反向传播算法是局部收敛的，需多次选起始点训练来减少最终局部收敛的可能性；\n#---------------------------------------------------------------------------------#\n参考文献:\n《machine learning》, by Tom Mitchell；\ncouresra课程: standford machine learning, by Andrew Ng；"}
{"content2":"本系列文章目前已经更新两期，分别是： 机器学习原来如此有趣！全世界最简单的机器学习入门指南、 机器学习原来如此有趣：如何故意欺骗神经网络\n你是否有注意到Facebook最近开发了一个非同寻常的功能：将你照片中的好友识别出来。过去，Facebook 让你手动点击照片上的好友，输入他们的名字，然后加上标签。现在只要你上传一张照片，Facebook就会像变魔术一样为你自动标记出所有人：\n这项技术就叫做人脸识别。在你的朋友的脸被标记了几次之后，Facebook的算法就可以识别他了。这是一个让人惊艳的技术--Facebook识别人脸的正确率高达98%！这与人类的表现差不多了。\n下面就让我们来学习一下人脸识别技术是如何实现的！但是只是识别你的朋友的脸就太简单了。 我们可以最大化扩展这项技术，来解决一个更具挑战性的问题——区分威尔·法瑞尔（Will Ferrell，著名演员）和查德·史密斯（Chad Smith，著名摇滚音乐家）！\n如何使用机器学习来解决复杂的问题\n人脸识别由一个系列的相关问题组成：\n1.首先：查看一张照片并找出上面所有的脸\n2.将注意力放在每一张脸上面，即使这张脸被转到奇怪的方向或者是光线不好的情况下也依旧是同一个人。\n3. 从这张脸上挑出一些特征用于和其他人区分来，比如像眼睛有多大，脸有多长等。\n4.最后，将这张脸的特征和其他其他脸作比较，以最后确定这个人的名字。\n作为一个人类，你的大脑会自动做这些事情。实际上，人类太擅长于识别人脸了，以至于他们在日常物品上面也会试图去寻找脸（好像是这样哦，人们总是喜欢去物品上找出练得形状，并且觉得这样很萌）。\n计算机目前并不具备这种高水平的能力。。。所以我们需要一步步的教他们。\n我们需要构建一个流水线（pipeline）：我们将分别解决人脸识别的每一步，并将当前步骤的结果传入下一个步骤。换句话说，我们需要将几个机器学习算法链（chain）起来。\n人脸识别-一步一步来\n我们一步一步地解决这个问题。在每一个步骤中，我们都将学习到不同的机器学习算法。我不会对算法的每一步都进行解释，但是你可以学习到每一个算法的主体思想，以及如何在 Python 中使用 OpenFace 和 dlib 来构建一个你自己的面部识别系统。\n第一步：寻找所有的脸\n在我们的流水线中的第一步是人脸检测。很明显在我们区分人脸之前需要在图片中将脸标记出来。\n如果你有在最近十年里面用过相机的话，你可能已经见过正在运行中的人脸检测了：\n面部识别是相机的一个伟大的功能。当相机可以自动挑出面部的时候，这将确保在拍照片的瞬间所有的脸都对准焦点了。不过我们使用它是为了别的目的--寻找我们想在下一步要传递的照片区域。\n2000年年初的时候，当Paul Viola和Michael Jones发明了一种可以在廉价相机上面快速运行的面部检测技术后，人脸检测成为了主流。然而现在更可靠的解决方案出现了。我们现在用的是2005年发明的一个叫做方向梯度直方图，简称为HOG。\n为了识别出图片中的脸，首先我们需要将图片转换为黑白色，因为在识别面部的时候我们不需要颜色数据。\n然后我们需要依次遍历图片中的每个像素。对于单个像素，我们也需要看直接包围它的其他元素：\n我们的目标是比较这个像素与周围像素的深度。然后我们要画一个箭头来代表图像变暗的方向：\n如果你对这个图像中的每个像素都重复这个过程，最后每个像素，最终每个像素会被一个箭头取代。这些箭头被称为梯度（gradients），它们能显示出图像上从明亮到黑暗的流动过程：\n这看起来没有明确的目的，但其实这很有必要。如果我们直接分析像素，同一个人明暗不同的两张照片将具有完全不同的像素值。但是如果只考虑亮度变化方向（direction）的话，明暗图像将会有同样的结果。这使得问题变得更容易解决！\n但是保存每个像素的梯度太过细节化了，我们最终很有可能捡了芝麻丢了西瓜。如果能从更高的角度上观察基本的明暗流动，我们就可以看出图像的基本规律，这会比之前更好。\n为了做到这一点，我们将图像分割成一些 16×16 像素的小方块。在每个小方块中，我们将计算出每个主方向上有多少个梯度（有多少指向上，指向右上，指向右等）。然后我们将用指向性最强那个方向的箭头来代替原来的那个小方块。\n最终的结果是，我们把原始图像转换成了一个非常简单的表达形式，这种表达形式可以用一种简单的方式来捕获面部的基本结构：\n原始图像被表示成了 HOG 形式，以捕获图像的主要特征，无论图像明暗度如何。\n为了在这个 HOG 图像中找到脸部，我们要所需要做的，就是找到我们的图像中，与已知的一些 HOG 图案中，看起来最相似的部分。这些 HOG 图案都是从其他面部训练数据中提取出来的：\n使用这种技术，我们现在可以轻松地在任何图片中找到脸部：\n如果你想用 Python 和 dlib 亲手试试看，这些代码显示了如何生成和查看 HOG 图像。\n第二步：脸部的不同姿势和方位\n当当当，我们把图片中的脸部分离出来了。 但现在，我们要处理的问题就是，对于电脑来说，面朝不同方向的同一张脸是两个人：\n人类可以很轻松地识别出到两个图片都是同一个人，但电脑会认为这两张图片是两个完全不同的人。\n为了解决这一点，我们将试图扭曲每个图片，使得眼睛和嘴唇总是在图像中的样本位置（sample place）。 这将使我们在接下来的步骤中，更容易比较脸部之间的不同。\n为此，我们将使用一种称为面部特征点估计（face landmark estimation）的算法。 很多方法都可以做到这一点，但这次我们会使用由 瓦希德·卡奇米（Vahid Kazemi）和约瑟菲娜·沙利文（Josephine Sullivan）在 2014 年发明的方法。\n基本思路是找到 68 个人脸上普遍存在的特征点（ landmarks）——包括下巴的顶部、每只眼睛的外部轮廓、每条眉毛的内部轮廓等。接下来我们训练一个机器学习算法，让它能够在任何脸部找到这 68 个特定的点：\n我们将在每一张脸上定位的 68 个特征点。这张图片的作者是在OpenFace工作的卡内基梅隆大学 Ph.D. 布兰东·阿莫斯（Brandon Amos）。\n这是在测试图片上定位 68 个特征点的结果：\n你也可以使用这一技术来实现自己的 Snapchat 实时 3D 脸部过滤器！\n现在，我们知道了眼睛和嘴巴在哪儿，我们将图像进行旋转、缩放和错切，使得眼睛和嘴巴尽可能靠近中心。我们不会做任何花哨的三维扭曲，因为这会让图像失真。我们只会使用那些能够保持图片相对平行的基本图像变换，例如旋转和缩放（称为仿射变换）：\n现在无论人脸朝向哪边，我们都能将眼睛和嘴巴向中间挪动到大致相同的位置。这将使我们的下一步更加准确。\n如果你想用 Python 和 dlib 亲手试试看这一步的话，这里有一些代码帮你寻找脸部特征点并用这些特征点完成图像变形。\n第三步：给脸部编码\n现在我们要面临最核心的问题了——准确识别不同的人脸。这才是这件事的有趣之处！\n最简单的人脸识别方法，是把我们在第二步中发现的未知人脸，与我们已经标注了的人脸图片作比较。当我们发现未知的面孔与一个以前标注过的面孔看起来及其相似的时候，它肯定是同一个人。这个想看起来很完美，对吧？\n实际上这种方法有一个巨大的问题。像 Facebook 这种拥有数十亿用户和数万亿张照片的网站，是不可能去循环比较每张先前标记的脸的，这太浪费时间了。他们需要在毫秒内识别人脸，而不是几个小时。\n我们需要的方法是从每张人脸上提取一些基本的测量数值。然后，我们可以用同样的方式测量未知的面孔，并找到最接近测量数值的那张已知的脸。例如，我们可以测量每个耳朵的大小、眼距、鼻子的长度等。如果你曾经看过像《犯罪现场调查》这样的电视剧，你就知道我在说什么了。\n测量面部的最可靠方法\n好的，所以为了建立我们的已知脸部数据库呢，我们应该测量面部的哪些数值？耳朵的大小？鼻子的长度？眼睛的颜色？还有什么？\n事实证明，对于我们人类来说一些显而易见的测量值（比如眼睛颜色），对计算机来说没什么意义。研究人员发现，最准确的方法是让计算机自己找出它要收集的测量值。深度学习在寻找哪些部分的测量值比较重要方面表现的比人类更好。\n所以，解决方案是训练一个深度卷积神经网络。但是，并不是让它去识别图片中的物体，这一次我们的训练是要让它为脸部生成 128 个测量值。\n每次训练要观察三个不同的脸部图像：\n1. 加载一张已知的人的面部训练图像\n2. 加载同一个人的另一张照片\n3. 加载另外一个人的照片\n然后，算法查看它自己为这三个图片生成的测量值。再然后，稍微调整神经网络，以确保第一张和第二张生成的测量值接近，而第二张和第三张生成的测量值略有不同。\n在为几千个人的数百万图像重复该步骤几百万次之后，神经网络学习了如何可靠地为每个人生成 128 个测量值。对于同一个人的任何十张不同的照片，它都应该给出大致相同的测量值。\n机器学习专业人士把每张脸的 128 个测量值称为一个嵌入（embedding）。将复杂的原始数据（如图片）缩减为可由计算机生成的一个数列的方法，在机器学习（特别是语言翻译）中出现了很多次。我们正在使用的这种脸部提取方法是由 Google 的研究人员在 2015 年发明的，但也有许多类似方法存在。\n给我们的脸部图像编码\n这个通过训练卷积神经网络来输出脸部嵌入的过程，需要大量的数据和强大的计算能力。即使使用昂贵的 Nvidia Telsa 显卡，你也需要大约 24 小时的连续训练，才能获得良好的准确性。\n但一旦网络训练完成，它就可以为每一张脸生成测量值，即使之前它从未见过这张脸！所以这种训练只需一次即可。幸运的是，OpenFace 上面的大牛已经做完了这些，并且他们发布了几个训练过可以直接使用的网络。谢谢Brandon Amos他的团队！\n所以我们需要做的，就是通过他们预训练的网络来处理我们的脸部图像，以获得 128 个测量值。这是我们测试图像的一些测量值：\n那么，这 128 个数字到底测量了脸部的哪些部分？我们当然不知道，但是这对我们并不重要。我们关心的是，当看到同一个人两张不同的图片时，我们的网络能得到几乎相同的数值。\n如果你想自己尝试这个步骤，OpenFace 提供了一个 lua 脚本，它可以生成一个文件夹中所有图像的嵌入，并将它们写入 csv 文件。点此查看如何运行。\n第四步：从编码中找出人的名字\n最后这一步实际上是整个过程中最简单的一步。我们要做的就是找到数据库中，与我们的测试图像的测量值最接近的那个人。\n你可以通过任何基本的机器学习分类算法来达成这一目标。我们并不需要太花哨的深度学习技巧。我们将使用一个简单的线性 SVM 分类器，但实际上还有很多其他的分类算法可以使用。\n我们需要做的是训练一个分类器，它可以从一个新的测试图像中获取测量结果，并找出最匹配的那个人。分类器运行一次只需要几毫秒，分类器的结果就是人的名字！\n所以让我们试一下我们的系统。首先，我使用Will Ferrell, Chad Smith and Jimmy Falon三人每人 20 张照片的嵌入来训练分类器：\n嗯……就是这些训练数据！\n接下来，我在这个分类器上运行了威尔·法瑞尔和查德·史密斯在吉米·法伦的节目上互相模仿的那个视频的每一帧：\nhttps://cdn-images-1.medium.com/max/800/1*_GNyjR3JlPoS9grtIVmKFQ.gif\n结果成功了！不同角度的脸部，甚至是侧脸，它都能捕捉到！\n自己动手做一遍\n让我们回顾一下我们的步骤：\n1. 使用 HOG 算法给图片编码，以创建图片的简化版本。使用这个简化的图像，找到其中看起来最像通用 HOG 面部编码的部分。\n2. 通过找到脸上的主要特征点，找出脸部的姿势。一旦我们找到这些特征点，就利用它们把图像扭曲，使眼睛和嘴巴居中。\n3. 把上一步得到的面部图像放入神经网络中，神经网络知道如何找到 128 个特征测量值。保存这 128 个测量值。\n4. 看看我们过去已经测量过的所有脸部，找出哪个人的测量值和我们要测量的面部最接近。这就是你要找的人！\n现在你知道这一切都是如何运行的了，这里是如何使用 OpenFace 在你自己的电脑上运行整个人脸识别系统的说明：\n开始之前\n确保你已经安装了 python、OpenFace 和 dlib。你也可以在这里手动安装，或者使用一个已经设定好的 docker image：\ndocker pull\nbamos/openface\ndocker run -p 9000:9000\n-p 8000:8000 -t -i bamos/openface /bin/bash\ncd /root/openface\n友情提示：如果你正在 OSX 上使用 Docker，你可以这样使你的 OSX /Users/\n文件夹在 docker image 中可见：\ndocker run -v /Users:/host/Users -p 9000:9000 -p\n8000:8000 -t -i bamos/openface /bin/bash\ncd /root/openface\n然后你就能访问你在 docker image 中 /host/Users/...的 OSX 文件\nls /host/Users/\n第一步\n在 openface 文件中建立一个名为 ./training-images/ 的文件夹。\nmkdir training-images\n第二步\n为你想识别的每个人建立一个子文件夹。例如：\nmkdir ./training-images/will-ferrell/\nmkdir ./training-images/chad-smith/\nmkdir ./training-images/jimmy-fallon/\n第三步\n将每个人的所有图像复制进对应的子文件夹。确保每张图像上只出现一张脸。不需要裁剪脸部周围的区域。OpenFace 会自己裁剪。\n第四步\n从 openface 的根目录中运行这个openface 脚本。\n首先，进行姿势检测和校准：\n./util/align-dlib.py\n./training-images/ align outerEyesAndNose ./aligned-images/ --size 96\n这将创建一个名为./aligned-images/的子文件夹，里面是每一个测试图像裁剪过、并且对齐的版本。\n其次，从对齐的图像中生成特征文件：\n./batch-represent/main.lua\n-outDir ./generated-embeddings/ -data ./aligned-images/\n运行完后，这个./generated-embeddings/子文件夹会包含一个带有每张图像嵌入的 csv 文件。\n第三，训练你的面部检测模型：\n./demos/classifier.py\ntrain ./generated-embeddings/\n这将生成一个名为 ./generated-embeddings/classifier.pkl的新文件，其中包含了你用来识别新面孔的 SVM 模型。\n到这一步为止，你应该有了一个可用的人脸识别器！\n第五步：识别面孔！\n获取一个未知脸孔的新照片，然后像这样把它传递入分类器脚本中：\n./demos/classifier.py\ninfer ./generated-embeddings/classifier.pkl your_test_image.jpg\n你应该会得到像这样的一个预测：\n===/test-images/will-ferrel-1.jpg ===\nPredict will-ferrell with 0.73 confidence.\n至此，你已经完成了一个预测了。你也可以修改./demos/classifier.py 这个 python 脚本，来让它匹配其他人的脸。\n重要提示：\n如果你得到的结果不够理想，试着在第三步为每个人添加更多照片（特别是不同姿势的照片）。\n即使完全不知道这个面孔是谁，现在这个脚本仍然会给出预测。在真实应用中，低可信度（low confidence）的预测可能会被直接舍弃，因为很有可能它们就是错的。\nvia medium.com"}
{"content2":"机器学习笔记(一)\n名词解释\nILP：Inductive Logic Programming的简称，即归纳逻辑程序设计。\n发展历程\nn  二十世纪五十年代初，出现机器学习的相关研究\nn  二十世纪五十年代中期， 基于神经网络的“连接主义学习开始出现\nn  在二十世纪六七十年代，基于逻辑表示的“符号主义”学习技术蓬勃发展。\nn  二十世纪五十年代 到七十年代初， 人工智能 处于“推理期”为只要能赋予机器逻辑推理能力，机器就能具有智能。\nn  二十世纪七十年代中期，人工智能进入“知识期”，大量专家系统问世。但渐渐意识到面临“知识功能瓶颈”\nn  二十世纪八十年代，机器学习成为一个独立的学科领域，各种机器学习技术百花绽放。机器学习作为“解决知识工程瓶颈问题的关键”而走上人工智能的主舞台。\nn  二十世纪九十年代中期之前，“从样例中学习”的另一个主流技术：基于神经网络的连接主义学习得到重视\nn  二十世纪九十年代中期，“统计学习”闪亮登场，迅速占领主流舞台。\nn  二十一世纪初，连接主义学习卷土重来，掀起了以深度学习为名的热潮。原因是数据大了，计算能力强了。\n相关材料推荐\n1)         第一本机器学习专业期刊：Machine Learning\n2)         人工智能领域的权威期刊：Artificial  Intelligence\n3)         第一本机器学习专门性教材：Mitchell, 1997\n4)         出色的入门读物：Duda et al.,2001;  Alpaydin, 2004;  Flach, 2012;\n5)         进阶读物：Hastie et al. , 2009;\n6)         适合贝叶斯学习偏爱者：Bishop, 2006\n7)         基于WEKA撰写的入门读物，有助于初学者通过WEKA实践快速掌握常用的机器学习算法：Witten et al.,2011\n8)         国际机器学习会议：ICML\n9)         国际神经信息处理系统会议:NIPS\n10)     国际学习理论会议：COLT\n11)     国际学术期刊Journal of Machine Learning Research 和 Machine Learning\n12)     人工智能领域的重要会议：IJCAI, AAAI\n13)     人工智能领域重要期刊：Artifical Intelligence;  Journal of Artifical Intelligence Research\n14)     数据挖掘领域重要会议：KDD, ICDM\n15)     数据挖掘领域重要期刊：ACM Transactions on Knowledge Discovery from Data;  Data Mining and Knowledge Discovery\n16)     计算机视觉和模式识别领域的重要会议：CVPR\n17)     计算机视觉与模式识别领域的重要期刊：IEEE Transactions on Pattern  Analysis and Machine Intelligence\n18)     神经网络领域的重要期刊：Neural Computation, IEEE Transactions on Neural Networks and Learning System\n19)     统计学领域的重要期刊：Annals of Statistics\n20)     中国机器学习大会：CCML\n21)     中国“机器学习及其应用”研讨会：MLA"}
{"content2":"前言\n最近几周花了点时间学习了下今年暑假龙星计划的机器学习课程，具体的课程资料参考见附录。本课程选讲了写ML中的基本模型，同时还介绍了最近几年比较热门，比较新的算法，另外也将ML理论和实际问题结合了起来，比如将其应用在视觉上，web上的等。总之，虽然课程内容讲得不是特别细（毕竟只有那么几节课），但是内容还算比较新和比较全的。学完这些课后，收获还算不少的，至少了解到了自己哪方面的知识比较弱，下面是课程中做的一些简单笔记。\n第1课  绪论课\n机器学习中3个比不可少的元素，数据，模型和算法。现在数据来源比较广泛，每天都可以产生T级以上的数据。模型的话就是机器学习课程中需要研究的各种模型，算法就是怎样通过数据和模型来学习出模型中的参数。但是余老师在课堂上提出一个观点就是这3个元素都不重要，最重要的是需求，一旦有了需求，就会采用各种方法取求解问题了。不愧是百度公司的技术副总监。另外机器学习的主要应用场合包括计算机视觉，语音识别，自然语音处理，搜索，推荐系统，无人驾驶，问答系统等。\n第2课 线性模型\n线性回归模型需要解决下面3个问题：\n1. 怎样从训练数据估计线性模型的参数？即截距和斜率。\n2. 学习到的线性模型性能怎样？我们是否可以找到更好的模型？\n3. 模型中2个参数的重要性怎么估计？\n解决第1个问题是一个优化问题，即求得使损失函数最小的参数。这里的损失函数是平方项的，也称为线性最小二乘思想。线性模型的表达式为：\n其中噪声参数为0均值的高斯噪声。如果后面求出的噪声不是一个均值为0，方差相同的类似高斯分布的随机变量，则说明这个模型还可以被改进。比如说将x首先映射到非线性函数中去，然后对非线性函数用最小二乘法做线性回归。至于怎样得到非线性映射函数f(x)则要么通过人为观察推测，要么通过机器学习中的特征学习来自动获得。\n更广义的线性模型并不一定是一个线性方程。只是其参数可能是线性的。线性模型能够模拟非线性函数。\n残差可以看做是噪声的近似。但是一般来说残差要比噪声小。所以在线性模型中，噪声项就可以用残差来估计，不过其分母不是1/n,而是1/(n-p)，因为需要达一个无偏估计。\n特征向量元素属性的重要性评价常见的有以下2种方法：第一是抽掉一个特征想，然后计算其残差变化值与全部特征都用上的比值，所得到的分数为F-score，F-score越大，说明该属性越重要。第2种方法是采用t分布来假设检验得到Z-score，即假设对应特征属性不存在(即其值为0)时，出现样本数据的概率为Z-score，如果Z-score越大，说明该属性越不重要。\n第3课 过拟合和规则项\nRegularization中文意思是规则，指的是在overfitting和underfitting之间做平衡，通过限制参数空间来控制模型的复杂度。测试误差和训练误差之间差一个规则项，其公式为：\n模型越复杂说明模型越不稳定，学习到的目标函数越不光滑，也就越容易over-fitting。所以需要控制模型的复杂度，一般来说有2种方法，即减少模型中参数的个数或者减小参数的空间大小，目前用得最多的就是减小参数的空间大小，是通过规则项达到的。规则项的引入同时也需要引入一个调节的参数，该参数的大小一般通过交叉验证获得。如果规则项是2次的，则也称为ridge回归，规则项是一次的则称为lasso回归。Ridge回归的优点是解比较稳定，且允许参数的个数大于样本的个数。Lasson回归的优点是有稀疏解，不过解不一定稳定。\n如果碰到参数个数大于样本个数，这时候就不能够用参数个数来做规则化了，而是采用缩小参数空间的方法，这样的话既在统计学上对特征数量集大时有鲁棒性，同时在数值计算上方程解也具备稳定性。\n第4课 线性分类器\n很好的理解线性分类器，可以理解很多ml的概念，以及非线性问题。线性分类器是在实际应用过程中最有用的模型。\n据余老师讲，从06年开始，人工神经网络又开始热起来了，主要体现在deep learning领域。\nsvm理论很完美,应用场合也很广,同理,logistic回归应用场合也非常广,和svm差不多。\n当数据为大样本数据时，用线性SVM模型比较好。\n第5课 非线性svm\nRKHS表示定理：即模型的参数是在训练样本的线性子空间中，是训练样本的线性组合。这不仅适用于svm，对其他的模型，比如感知机，RBF网络，LVQ，boosting，logistic回归等模型都成立。\nKernel可以简单理解为表示2个值相似度的测量。通过核函数可以更好的了解regularization。所需优化的目标函数可以写成参数形式，参数形式的对偶形式和非参数形式这3种。如果在非参数形式中，其规则项是由所学习到的函数f(x)来控制的，它的模与对应核函数进行特征函数分解时的特征值系数成反比。即特征函数分解中非主成分的函数对应的特征系数小，得到的惩罚就大，就会更加被抑制。因此我们保留的主要是主成分的那些特征函数。从上面可以看出，核函数是有一定的结构的，该结构决定了最终的目标函数f(x)长得什么样。\n逻辑回归和svm的区别只是loss函数的不同，logstic回归的loss函数为logstic函数，核svm的loss函数为hinge loss。两者有着相同的性能，逻辑回归是带概率的输出，更容易用于多分类问题。不过目前，这2种方法都是旧方法了。\nLVQ中文名为学习矢量化，它是一个基于模型的有监督学习分类器。\n因此我们在设计一个模型时，需要考虑采用什么样的loss函数？采用什么样的基函数h(x)？h(x)是有限维的还是无限维的？是否需要学习h(x)?用什么样的方法来优化目标函数，QP，LBFGS，还是梯度下降等？\n理论上使用kernel理论可以实现用有限的计算完成无限空间的学习问题，但是在实际问题中，由于其复杂度是样本个数N的3次方，所以当样本数据很多时，基本上是无法实现的。\n参数模型和非参数模型的区别不是看模型中是否有参数，所有的模型都是有参数的，非参数模型是指随着样本数的增加，其模型中的参数的个数也跟着增加。反之就为参数模型了。常见的非参数模型有高斯过程，核svm，dirichlet过程等。\n第6课 模型选择\n模型选择在实际应用过程中非常有用，一般把与模型有关的数据分为3部分，训练数据，验证数据和测试数据，如下图所示：\n其中训练数据和验证数据都是已有的样本数据，即已观察到了的数据。测试数据是未来实际应用中产生的数据，是事先不知道的。\n模型的参数分为2部分，第一部分是模型确定后通过训练样本学习得到的参数。另一部分是手动输入的参数，也叫做超参数，是用来控制模型的复杂度的，也就是来控制模型本身长什么样的，它是由验证数据来调节的。\n模型选择问题就是说怎样验证一个模型是否好。模型的好坏最终是要看它在测试数据集上的表现。因此在未观测到测试数据时，我们只能用验证数据集来代替它进行测试。一般采用的方法为交叉验证，比如说LOOCV，即留一法交叉验证，类似的还有k折交叉验证。交叉验证的主要目的是防止训练出来的模型过拟合。但是在当今由于数据都是海量的，交叉验证方法使用越来越少了，因为如果训练数据集非常大的话，一般不会产生过拟合现象。\n还有一些方法是不需要通过验证而直接来评价模型好坏的，比如是AIC，BIC，MDL，SRM等。\n第7课 模型平均\n本文中讲的model是指的一个learning algorithm，甚至比learning algorithm所指的范围还要小，因为在一个learning algorithm里，不同的参数调节和不同的输入特征都会导致不同的model。模型选择的目标是使模型有更好的可解释性和更好的性能，而模型平均的目标只需要使模型有更好的性能即可，因为模型平均过程中用到了很多模型，而模型个数越多则其可解释性就越低。模型平均的英文名称有model ensemble,model blending, model combination, model averaging.\nModel selection 和 model combination的不同使用体现在，如果某个模型以绝对的优势好于其他所有模型，那么这时候我们就采用model selection，因为不仅有好的性能，还可以获得好的可解释性。如果所有的模型在性能表现上都差不多，没有所谓的好坏，且模型本身又有很大的不同，这时候就可以采用model combination来大大提高其性能了。通常来说，model combination比model selection要稳定些。\n那么该怎样构造差异性大的模型呢？可以从下面四个方面入手：\n1. 不同的学习算法。\n2. 不同参数调整。\n3. 有差异的输入特征。\n4. 引入随机思想，比如bagging。\n关于指数权值的模型平均只是在均一模型平均(即采用投票的方式)的基础上将投票权值改为模型误差的指数形式，而不是相同的均值。如果所学习到的一个模型的误差越大，则其权值越低，理论上比较完美。不过在张老师讲他自己实验的时候发现并没有什么提高，有时候效果还不如voting。\nStacking和指数权值的模型平均有点类似，也是先学习出各个模型，然后把学习出的模型作为第二层学习的输入，优化最小的第二层的误差来学习模型的权值。\nBagging也是一种均一模型平均，它的所有模型的学习算法一样，只是输入样本采用bootstrip获得。因为是采用boostrip获得的，所以其训练样本有些不一定用到了，而有些则重复用到了。这样每个学习出来的model不是很稳定，因而这也扩大了model之间的差异性，提高了集群学习的性能。Bagging是减小学习的方差，而boosting是减小学习的偏差。\n最后模型平均的一个比较出名的应用场合就是把决策树改造成随机森林的例子。因为单颗决策树虽然有可解释性，能够很好的处理非均匀的特征以及是一种非线性的方法，但是它的最大缺点就是分类结果不准确，因此在样本选择和输入特征选择方面采用了随机的方法得到不同的模型后，再做平均就成了随机森林，理论和实验表明随机森林的效果要比决策树好很多。\n第8课 Boosting\nBoosting既可以看做是signal learning也可以看做是ensemble learning,本课中将其看做是ensemble learning。它是由多个弱分类器组合成一个强分类器，但是这里所指的弱分类器满足的条件其实并不弱，因为它需要满足对样本的所以加权情况的分类效果都要大于0.5，因此现在有不少学者不称这些为弱分类器了，而称为基本分类器。Boosting中最常用的算法是AdaBoosting，AdaBoosting是对分类错误的样本加大其权重来达到resamble的效果。且采用贪婪算法进行loss的函数的优化。\nVC维的传统定义为: 对一个指标函数集，如果存在H个样本能够被函数集中的函数按所有可能的2的K次方种形式分开，则称函数集能够把H个样本打散；函数集的VC维就是它能打散的最大样本数目H。\nAdaBoosting不是最大margin的，但为什么比最大marign的boosting效果要好呢？课程中从传统的boosting分析来做了一定的解释，但是仍不能够解释当训练误差为0时，其泛化误差还在减小这一问题，后面的学者又提出了从margin bound方面来解释这个问题。另外从另一个角度来更好的理解boosing的方法是greedy boosting,即寻找样本权重d和弱分类器权重w的过程是一个贪婪过程。最后老师讲了一个general loss函数以及利用这个函数进行的general boosting。\n第9课  学习理论概论\n这节课的内容比较理论化，听不太懂。机器学习理论的主要目标是平均一个学习算法的好坏，即怎样通过训练误差来估计测试误差。可以通过一致性收敛来估计训练误差和测试误差之间的关系，即测试误差以大概率事件小于训练误差加上某个值，这个值的大小与训练样本数以及概率值有关。证明上面的一致性收敛需要用到切比雪夫不等式，VC维，covering numbers这几种技术。其中covering numbers定义为attain训练样本的预测函数的个数(具体是什么没有理解清楚)。我们可以用VC维来估计convering number。最后老师还讲了一个Rademacher复杂度并说了下它和VC维之间的关系，真心不懂Rademacher是个什么东东！\n第10课  机器学习中的优化问题\n机器学习中大部分问题都可以归结为参数优化问题,即找到最适合目标函数的参数,该参数一般满足使目标函数最大或者最小。\n常见的优化方法有梯度下降法，该方法是每次沿着梯度下降最快的那个方向寻找函数值，不断迭代就可以寻找到近似的极值。该方法的学习速率（即每次沿梯度方向前进的距离）和收敛速率是最值得关注的。一般来讲，如果函数是光滑且是严格为凸函数的，则其收敛速度最快，其实是光滑但不严格凸的，最慢的要数非光滑函数。因此当函数有一部分是光滑，而另一部分不光滑时，我们可以采用Proximal 梯度下降法，该方法是最近几年热门起来的，效果比梯度下降要好，更新的类似的算法还有Nestervo这个学者的Accelerated 梯度法(全是数学公式，完全看不懂)。为了求出局部极值点，一般可以采用近似泰勒展开中的H矩阵来求得，典型的算法有LBFGS。另外当需要优化的参数为一个向量时，不一定需要把这个向量的元素对等考虑，我们可以分开优化，即每次只优化参数向量中的一个，其它的保持不变，这样循环直到收敛。最后老师讲了凸函数的优化问题还可以采用Dual 梯度下降法。\n实话说，这种纯数学公式的东西太乏味了！\n第11课  Online learning\nOnline learning指的是每当来一个数据，就会学习一个最优的预测函数，其最优的准则是当前位置loss函数值最小，因此每一步的预测函数都有可能不同，这就是Online learning。其实很早前就有online learning的例子，比如说感知机学习规则。\n在了解Online learning之前需要了解regret 分析这个概率，regret指的是，Online learning中每次学习的误差减去使用用当前为止的最优函数而产生的误差的平均值，当然我们希望regret越小越好。\nOnline learning的关键是需要更不断新状态。其实Online learning也是一个优化问题，我们可以把第10讲的优化问题全部转换成对应的Online learning。比如说凸优化，梯度下降法，proximal descent。其中将proximal descent转换成online版本可以采用L1规则化，Dual averaging, 保持second order信息等。统计梯度下降可以用来优化大规模的数据，它的不同变种主要来源于不同的proximal 函数，不同的学习率，是否是dual averaging, 是否是averaging, 是否是acceleration等。\n第12课 sparsity model\nSparsity model的出现时为了解决统计学习中的维数灾难问题的，即样本的个数远远小于特征的维数。解决标准的稀疏回归模型可以采用greedy算法和convex relaxation。Greedy 算法中比较有代表性的是OMP。要从稀疏的参数重建参数需要有2个条件，即irrepresentable和RIP。稀疏模型一个代表性的问题是Lasso的求解。老师从上面2个条件介绍了lasso的求解。Lasso是基于L1规则化的。其它一些比较复杂的规则项对应的sparsity model有比如structured sparsity(比如说group structure), graphical model, matrix  regularization. 这又是一堂纯数学的课程。\n第13课 Graphical model\nGraphical model是一个应用比较广泛的模型，不过比较复杂，因为里面涉及到了很多概率的知识。但是这节课的内容还算比较表面，没有过多的细节。主要从3个方面介绍graphical model，即model本身，推理方法和模型的结构学习。概率模型中一大部分就是graphic model，而graphic model中又分为有向图和无向图，有向图中比较有代表的是贝叶斯网络，无向图中比较有代表的是MRF。本节内容主要是讲的有向图。任何一个复杂的贝叶斯网络都可以由causal chains，common cause, common effect这3部分构成。Graphical model应用很广，比如说常见的线性回归问题也可以转换成graphical model问题，如果是分段线性回归问题还可以转换成带有隐变量的graphical model。贝叶斯网络中的推理一般是给定一些观测数据，求出在此观测数据下出现某些中间状态的概率。当网络是简单的链或者是树状时，推理起来比较简单，当模型含有环状结构时，对应的推理就非常复杂了。 Graphical model中最后一个问题是模型结构的学习，可以将其看做是结构的搜索问题，对应的很多AI搜索算法此时也可以派上用场。结构学习的问题主要包括发现模型中的隐变量，因果关系直接从数据中学习其结构。\n第14课  structured learning\n结构学习的方法和理论包括结构输入，结构输出和结构模型。其中结构模型分为conditional model 和 generative model。Generative model包括HMM，ＨＭＭ有观察值独立性的假设，为了解决该假设带来的问题，后来有学长提出了MEMM算法，不过MEMM本身又带来了标注偏置问题，最后面的改进算法ＣＲＦ成功的解决了标注偏置问题。CRF模型可以看做是logistic 回归在结构学习框架下的扩展.同理M3N可以看做是SVM在结构化框架下的扩展。最后课堂上老师比较了CRFs和M3N两种算法。\n第15课 deep learning\n这节课讲的内容比较容易激发人的兴趣，一是因为deep learning最近非常火热，二是因为用deep learning来做一些视觉问题，其效果能提高不少。本次课程没有讲具体的细节，主要是介绍了一些deep learning的概念和应用。Deep learning的意思是可以自动来学习一些特征，比如说在视觉的分类或者识别中，一般都是特征提取+分类器设计，并且提取到的特征的好坏直接影响了分类器的分类效果，但是在目前的计算机视觉领域，其特征的提取都是我们人工设计的，需要针对不同的应用场合来提取不同的特征，余老师开玩笑的说，计算机视觉最近10年的最大成就就是有了个SIFT特征，但是它是基于RGB图像提出的，而今各种传感器，比如Kinect等。我们又得去重新设计它的特征，难道我们还要等10年么？因此可以看出，一个通用的特征提取框架需要给出，这就是deep learning,也叫做feature learning，也就是说给了很多样本，系统能够自动去学习这些样本的特征，而不是依靠人工来设计。听起来是多么的诱人！这就更类似于AI了。Deep learning主要是确定一个算法的层次结构，这个层次结构非常重要，它的想法和人体大脑皮层的工作机制类似，因为人大脑在识别某些东西的时候也是一个层次结构的。课件中主要接受了multi-scale models和hierarchical model,structure spectrum等，但没有具体展开，只是做了一个综述性的介绍。\n第16课 Transfer learning & Semi-supervised learning\n一方面由于有些问题的训练样本数据非常少，且样本的获取代价非常高，或者是模型的训练时间特别长，另一方面由于很多问题之间有相似性，所以TL(transfer learning)就产生了。TL主要是把多个相似的task放在一起来解决，它们共享同一个输入空间和输出空间，TL常见的例子有传感器网络预测，推荐系统，图像分类等。常见的用来解决TL问题有下面几个模型，HLM(层次线性模型),NN，回归线性模型，这些模型本质上都是学校一个隐含的相同的特征空间。另外老师也讲到了TL和GP(高斯过程)的对比，高斯过程是一个贝叶斯核机器的非线性算法，通过对先验样本的采用学习可以得到尖锐的后验概率模型，它是一种非参数的模型。TL方法主要分为4大类：样本之间的迁移，特征表达的迁移，模型的迁移和相关领域知识的迁移。其中特征表达的迁移和模型的迁移在数学本质上是类似的，也是学者们研究的重点。\nSSL(Semi-supervised learning)是为了达到用少量标注了的样本+大量没有标注的样本，来学习一个比单独用少量标注样本效果更好的模型。老师举了一个混合高斯分布的例子来解释SSL学习的效果，通过这个例子引出了SSL的一个通用模型。本课还简单的介绍了co-training 方法，所谓co-training，就是把表组好的数据分成几类，每一类都train一个model，然后把这些model作用到unlabel的样本上，通过优化方法达到输出一致的效果。最后介绍的Graph Laplacian以及它的harmonic 解就完全木有看懂。\n第17课 Recommendation Systems\nRecommendation Systems一个简单的应用就是会根据用户的购买历史来退算出用户可能喜欢的产品，然后推荐给用户，目前很多互联网公司都在做这方面的研究，因为可以带来大量的经济效益。Recommendation Systems是一个协同滤波问题，本课程主要围绕不同用户给不同电影评分这个例子来介绍。首先要解决的是历史数据偏差不同的问题，即要对数据做预处理实现归一化。\n在对Recommendation Systems进行设计的一个主流方法之一是将Recommendation Systems问题看做是一个分类问题，即把用户i对所有电影打分看做是要预测的标签，而其他所有人对电影的打分看做是特征，主要采用的方法是朴素贝叶斯，KNN等（其他大部分的分类算法都可以派上用场）。Recommendation Systems问题的另一主流方法是把它看成矩阵分解(MF)问题，这在实际应用中是效果最好的。因为我们观察到的数据是很稀疏的，很多位置都是missing的，且这些数据之间内部是存在一个简单结构的，因此我们可以把需要填充的矩阵R分解成2个低秩矩阵的乘积，这可以采用SVD或者SVD+一些优化的方法来解决。\n由此可以看出，Recommendation Systems是一个典型的ML问题。\n第18课  computer vision\n本课简单的介绍了下computer vision中的基本问题，比如说什么事computer vison, computer vison的难点，computer vison问题的分类：特征检测，边缘检测，目标检测，图像分割，拼图，3D重建，计算机图形学，目标识别等等。\n第19课 learning on the web\n机器学习在web上的应用比较广泛，比如前面讲过的推荐系统，另外还有一些搜索结果排序，分类问题，社区行为分析，用户行为模型等等。本课程主要从分类和排序做了一些介绍。网络上存在着各种垃圾信息，例如垃圾邮件，垃圾网页，垃圾广告等，分类问题就是采用ML的方法过滤掉这些垃圾信息。另外一个比较常见的分类问题是文本分类，找出文本描述的主题，其中BOW算法既简单，又取得了很好的效果。最后老师对Web-search问题也做了个简单的介绍。总之本课大概介绍了下ML在web上的简单应用和挑战。\n总结：\nML给我的感觉就是规则项和最优化贯穿了本次课程的所有章节。\n参考资料：\nhttp://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html"}
{"content2":"自从读研以来，给我印象最深的是数学的重要性。本科阶段，一直很疑惑我们学习那些高数、概率统计、线性代数有什么用。有些人甚至认为那些课程都是在浪费时间，说是工作之后根本用不上。的确，在我去企业实习的过程中也很少能够接触到大学学习的数学知识。然而，进入研究生阶段后，陆陆续续读了一些paper，上了一些诸如机器学习、信息检索等课程，这些课程里面涉及大量的数学内容，经过一段时间的学习之后，我会说数学对于计算机科学来说至关重要。如果你读过《数学之美》，那你肯定会赞叹于数学在解决工程问题方面具有如此的魅力。\n前段时间，参加了一个在北航办的关于云计算、大数据、移动互联网的高端论坛。这个论坛真的很高端，因为来好多重量级嘉宾，比如中国联通总裁，百度总裁，用友总裁，龙湖地产董事长，微软副总裁。。。现在很多人都在说大数据时代的到来，也都认为大数据存在很珍贵的价值。然而，如何从大数据中获得价值，是一个值得研究的问题。近几年，数据挖掘和机器学习成为了热点技术，它们也是从大数据中获取价值的关键技术。微软的陆奇说，他的团队每个人都必须据说一定的机器学习技能，可见机器学习的重要性。\n参加了学校的机器学习课程，老师教得很好。感觉自己渐渐开始了解到底什么是机器学习了，也萌发了很强的研究兴趣。然而，我要说机器学习的水很深，需要很强的数学功底和统计学功底。未来，这方面的技术也肯定会有很大的应用空间，所以，我觉得是时候开设一个分类，用于记录机器学习的学习笔记或是心得。\nPS：Matlab是神器，但我更喜欢用python，常用于科学计算的python模块库numpy和scipy。\n分享一些好的机器学习的参考书籍：\n1、Pattern Recognition And Machine Learning  （老师重点推荐的书籍）\n2、机器学习 （国内常用的教材）\n3、Machine Learning in Action （偏实践、很多代码样例、python）\n4、集体智慧编程 （偏实践、很多代码样例、python）"}
{"content2":"分享一篇来自机器之心的文章。关于机器学习的起步，讲的还是很清楚的。原文链接在：只需十四步：从零开始掌握Python机器学习（附资源）\nPython 可以说是现在最流行的机器学习语言，而且你也能在网上找到大量的资源。你现在也在考虑从 Python 入门机器学习吗？本教程或许能帮你成功上手，从 0 到 1 掌握 Python 机器学习，至于后面再从 1 到 100 变成机器学习专家，就要看你自己的努力了。本教程原文分为两个部分，机器之心在本文中将其进行了整合，原文可参阅：suo.im/KUWgl 和 suo.im/96wD3。本教程的作者为 KDnuggets 副主编兼数据科学家 Matthew Mayo。\n「开始」往往是最难的，尤其是当选择太多的时候，一个人往往很难下定决定做出选择。本教程的目的是帮助几乎没有 Python 机器学习背景的新手成长为知识渊博的实践者，而且这个过程中仅需要使用免费的材料和资源即可。这个大纲的主要目标是带你了解那些数量繁多的可用资源。毫无疑问，资源确实有很多，但哪些才是最好的呢？哪些是互补的呢？以怎样的顺序学习这些资源才是最合适的呢？\n首先，我假设你并不是以下方面的专家：\n机器学习\nPython\n任何 Python 的机器学习、科学计算或数据分析库\n当然，如果你对前两个主题有一定程度的基本了解就更好了，但那并不是必要的，在早期阶段多花一点点时间了解一下就行了。\n基础篇\n第一步：基本 Python 技能\n如果我们打算利用 Python 来执行机器学习，那么对 Python 有一些基本的了解就是至关重要的。幸运的是，因为 Python 是一种得到了广泛使用的通用编程语言，加上其在科学计算和机器学习领域的应用，所以找到一个初学者教程并不十分困难。你在 Python 和编程上的经验水平对于起步而言是至关重要的。\n首先，你需要安装 Python。因为我们后面会用到科学计算和机器学习软件包，所以我建议你安装 Anaconda。这是一个可用于 Linux、OS X 和 Windows 上的工业级的 Python 实现，完整包含了机器学习所需的软件包，包括 numpy、scikit-learn 和 matplotlib。其也包含了 iPython Notebook，这是一个用在我们许多教程中的交互式环境。我推荐安装 Python 2.7。\n如果你不懂编程，我建议你从下面的免费在线书籍开始学习，然后再进入后续的材料：\nLearn Python the Hard Way，作者 Zed A. Shaw：https://learnpythonthehardway.org/book/\n如果你有编程经验，但不懂 Python 或还很初级，我建议你学习下面两个课程：\n谷歌开发者 Python 课程（强烈推荐视觉学习者学习）：http://suo.im/toMzq\nPython 科学计算入门（来自 UCSB Engineering 的 M. Scott Shell）（一个不错的入门，大约有 60 页）：http://suo.im/2cXycM\n如果你要 30 分钟上手 Python 的快速课程，看下面：\n在 Y 分钟内学会 X（X=Python）：http://suo.im/zm6qX\n当然，如果你已经是一位经验丰富的 Python 程序员了，这一步就可以跳过了。即便如此，我也建议你常使用 Python 文档：https://www.python.org/doc/\n第二步：机器学习基础技巧\nKDnuggets 的 Zachary Lipton 已经指出：现在，人们评价一个「数据科学家」已经有很多不同标准了。这实际上是机器学习领域领域的一个写照，因为数据科学家大部分时间干的事情都牵涉到不同程度地使用机器学习算法。为了有效地创造和获得来自支持向量机的洞见，非常熟悉核方法（kernel methods）是否必要呢？当然不是。就像几乎生活中的所有事情一样，掌握理论的深度是与实践应用相关的。对机器学习算法的深度了解超过了本文探讨的范围，它通常需要你将非常大量的时间投入到更加学术的课程中去，或者至少是你自己要进行高强度的自学训练。\n好消息是，对实践来说，你并不需要获得机器学习博士般的理论理解——就想要成为一个高效的程序员并不必要进行计算机科学理论的学习。\n人们对吴恩达在 Coursera 上的机器学习课程内容往往好评如潮；然而，我的建议是浏览前一个学生在线记录的课堂笔记。跳过特定于 Octave（一个类似于 Matlab 的与你 Python 学习无关的语言）的笔记。一定要明白这些都不是官方笔记，但是可以从它们中把握到吴恩达课程材料中相关的内容。当然如果你有时间和兴趣，你现在就可以去 Coursera 上学习吴恩达的机器学习课程：http://suo.im/2o1uD\n吴恩达课程的非官方笔记：http://www.holehouse.org/mlclass/\n除了上面提到的吴恩达课程，如果你还需要需要其它的，网上还有很多各类课程供你选择。比如我就很喜欢 Tom Mitchell，这里是他最近演讲的视频（一起的还有 Maria-Florina Balcan），非常平易近人。\nTom Mitchell 的机器学习课程：http://suo.im/497arw\n目前你不需要所有的笔记和视频。一个有效地方法是当你觉得合适时，直接去看下面特定的练习题，参考上述备注和视频恰当的部分，\n第三步：科学计算 Python 软件包概述\n好了，我们已经掌握了 Python 编程并对机器学习有了一定的了解。而在 Python 之外，还有一些常用于执行实际机器学习的开源软件库。广义上讲，有很多所谓的科学 Python 库（scientific Python libraries）可用于执行基本的机器学习任务（这方面的判断肯定有些主观性）：\nnumpy——主要对其 N 维数组对象有用 http://www.numpy.org/\npandas——Python 数据分析库，包括数据框架（dataframes）等结构 http://pandas.pydata.org/\nmatplotlib——一个 2D 绘图库，可产生出版物质量的图表 http://matplotlib.org/\nscikit-learn——用于数据分析和数据挖掘人物的机器学习算法 http://scikit-learn.org/stable/\n学习这些库的一个好方法是学习下面的材料：\nScipy Lecture Notes，来自 Gaël Varoquaux、Emmanuelle Gouillart 和 Olav Vahtras：http://www.scipy-lectures.org/\n这个 pandas 教程也很不错：10 Minutes to Pandas：http://suo.im/4an6gY\n在本教程的后面你还会看到一些其它的软件包，比如基于 matplotlib 的数据可视化库 Seaborn。前面提到的软件包只是 Python 机器学习中常用的一些核心库的一部分，但是理解它们应该能让你在后面遇到其它软件包时不至于感到困惑。\n下面就开始动手吧！\n第四步：使用 Python 学习机器学习\n首先检查一下准备情况\nPython：就绪\n机器学习基本材料：就绪\nNumpy：就绪\nPandas：就绪\nMatplotlib：就绪\n现在是时候使用 Python 机器学习标准库 scikit-learn 来实现机器学习算法了。\nscikit-learn 流程图\n下面许多的教程和训练都是使用 iPython (Jupyter) Notebook 完成的，iPython Notebook 是执行 Python 语句的交互式环境。iPython Notebook 可以很方便地在网上找到或下载到你的本地计算机。\n来自斯坦福的 iPython Notebook 概览：http://cs231n.github.io/ipython-tutorial/\n同样也请注意，以下的教程是由一系列在线资源所组成。如果你感觉课程有什么不合适的，可以和作者交流。我们第一个教程就是从 scikit-learn 开始的，我建议你们在继续完成教程前可以按顺序看一看以下的文章。\n下面是一篇是对 scikit-learn 简介的文章，scikit-learn 是 Python 最常用的通用机器学习库，其覆盖了 K 近邻算法：\nJake VanderPlas 写的 scikit-learn 简介：http://suo.im/3bMdEd\n下面的会更加深入、扩展的一篇简介，包括了从著名的数据库开始完成一个项目：\nRandal Olson 的机器学习案例笔记：http://suo.im/RcPR6\n下一篇关注于在 scikit-learn 上评估不同模型的策略，包括训练集/测试集的分割方法：\nKevin Markham 的模型评估：http://suo.im/2HIXDD\n第五步：Python 上实现机器学习的基本算法\n在有了 scikit-learn 的基本知识后，我们可以进一步探索那些更加通用和实用的算法。我们从非常出名的 K 均值聚类（k-means clustering）算法开始，它是一种非常简单和高效的方法，能很好地解决非监督学习问题：\nK-均值聚类：http://suo.im/40R8zf\n接下来我们可以回到分类问题，并学习曾经最流行的分类算法：\n决策树：http://thegrimmscientist.com/tutorial-decision-trees/\n在了解分类问题后，我们可以继续看看连续型数值预测：\n线性回归：http://suo.im/3EV4Qn\n我们也可以利用回归的思想应用到分类问题中，即 logistic 回归：\nlogistic 回归：http://suo.im/S2beL\n第六步：Python 上实现进阶机器学习算法\n我们已经熟悉了 scikit-learn，现在我们可以了解一下更高级的算法了。首先就是支持向量机，它是一种依赖于将数据转换映射到高维空间的非线性分类器。\n支持向量机：http://suo.im/2iZLLa\n随后，我们可以通过 Kaggle Titanic 竞赛检查学习作为集成分类器的随机森林：\nKaggle Titanic 竞赛（使用随机森林）：http://suo.im/1o7ofe\n降维算法经常用于减少在问题中所使用的变量。主成份分析法就是非监督降维算法的一个特殊形式：\n降维算法：http://suo.im/2k5y2E\n在进入第七步之前，我们可以花一点时间考虑在相对较短的时间内取得的一些进展。\n首先使用 Python 及其机器学习库，我们不仅已经了解了一些最常见和知名的机器学习算法（k 近邻、k 均值聚类、支持向量机等），还研究了强大的集成技术（随机森林）和一些额外的机器学习任务（降维算法和模型验证技术）。除了一些基本的机器学习技巧，我们已经开始寻找一些有用的工具包。\n我们会进一步学习新的必要工具。\n第七步：Python 深度学习\n神经网络包含很多层\n深度学习无处不在。深度学习建立在几十年前的神经网络的基础上，但是最近的进步始于几年前，并极大地提高了深度神经网络的认知能力，引起了人们的广泛兴趣。如果你对神经网络还不熟悉，KDnuggets 有很多文章详细介绍了最近深度学习大量的创新、成就和赞许。\n最后一步并不打算把所有类型的深度学习评论一遍，而是在 2 个先进的当代 Python 深度学习库中探究几个简单的网络实现。对于有兴趣深挖深度学习的读者，我建议从下面这些免费的在线书籍开始：\n神经网络与深度学习，作者 Michael Nielsen：http://neuralnetworksanddeeplearning.com/\n1.Theano\n链接：http://deeplearning.net/software/theano/\nTheano 是我们讲到的第一个 Python 深度学习库。看看 Theano 作者怎么说：\nTheano 是一个 Python 库，它可以使你有效地定义、优化和评估包含多维数组的数学表达式。\n下面关于运用 Theano 学习深度学习的入门教程有点长，但是足够好，描述生动，评价很高：\nTheano 深度学习教程，作者 Colin Raffel：http://suo.im/1mPGHe\n2.Caffe\n链接：http://caffe.berkeleyvision.org/\n另一个我们将测试驱动的库是 Caffe。再一次，让我们从作者开始：\nCaffe 是一个深度学习框架，由表达、速度和模块性建构，Bwekeley 视觉与学习中心和社区工作者共同开发了 Caf fe。\n这个教程是本篇文章中最好的一个。我们已经学习了上面几个有趣的样例，但没有一个可与下面这个样例相竞争，其可通过 Caffe 实现谷歌的 DeepDream。这个相当精彩！掌握教程之后，可以尝试使你的处理器自如运行，就当作是娱乐。\n通过 Caffe 实现谷歌 DeepDream：http://suo.im/2cUSXS\n我并没有保证说这会很快或容易，但是如果你投入了时间并完成了上面的 7 个步骤，你将在理解大量机器学习算法以及通过流行的库（包括一些在目前深度学习研究领域最前沿的库）在 Python 中实现算法方面变得很擅长。\n进阶篇\n机器学习算法\n本篇是使用 Python 掌握机器学习的 7 个步骤系列文章的下篇，如果你已经学习了该系列的上篇，那么应该达到了令人满意的学习速度和熟练技能；如果没有的话，你也许应该回顾一下上篇，具体花费多少时间，取决于你当前的理解水平。我保证这样做是值得的。快速回顾之后，本篇文章会更明确地集中于几个机器学习相关的任务集上。由于安全地跳过了一些基础模块——Python 基础、机器学习基础等等——我们可以直接进入到不同的机器学习算法之中。这次我们可以根据功能更好地分类教程。\n第1步：机器学习基础回顾&一个新视角\n上篇中包括以下几步：\n1. Python 基础技能\n2. 机器学习基础技能\n3. Python 包概述\n4. 运用 Python 开始机器学习：介绍&模型评估\n5. 关于 Python 的机器学习主题：k-均值聚类、决策树、线性回归&逻辑回归\n6. 关于 Python 的高阶机器学习主题：支持向量机、随机森林、PCA 降维\n7. Python 中的深度学习\n如上所述，如果你正准备从头开始，我建议你按顺序读完上篇。我也会列出所有适合新手的入门材料，安装说明包含在上篇文章中。\n然而，如果你已经读过，我会从下面最基础的开始：\n机器学习关键术语解释，作者 Matthew Mayo。地址：http://suo.im/2URQGm\n维基百科条目：统计学分类。地址：http://suo.im/mquen\n机器学习：一个完整而详细的概述，作者 Alex Castrounis。地址：http://suo.im/1yjSSq\n如果你正在寻找学习机器学习基础的替代或补充性方法，恰好我可以把正在看的 Shai Ben-David 的视频讲座和 Shai Shalev-Shwartz 的教科书推荐给你：\nShai Ben-David 的机器学习介绍视频讲座，滑铁卢大学。地址：http://suo.im/1TFlK6\n理解机器学习：从理论到算法，作者 Shai Ben-David & Shai Shalev-Shwartz。地址：http://suo.im/1NL0ix\n记住，这些介绍性资料并不需要全部看完才能开始我写的系列文章。视频讲座、教科书及其他资源可在以下情况查阅：当使用机器学习算法实现模型时或者当合适的概念被实际应用在后续步骤之中时。具体情况自己判断。\n第2步：更多的分类\n我们从新材料开始，首先巩固一下我们的分类技术并引入一些额外的算法。虽然本篇文章的第一部分涵盖决策树、支持向量机、逻辑回归以及合成分类随机森林，我们还是会添加 k-最近邻、朴素贝叶斯分类器和多层感知器。\nScikit-learn 分类器\nk-最近邻（kNN）是一个简单分类器和懒惰学习者的示例，其中所有计算都发生在分类时间上（而不是提前在训练步骤期间发生）。kNN 是非参数的，通过比较数据实例和 k 最近实例来决定如何分类。\n使用 Python 进行 k-最近邻分类。地址：http://suo.im/2zqW0t\n朴素贝叶斯是基于贝叶斯定理的分类器。它假定特征之间存在独立性，并且一个类中任何特定特征的存在与任何其它特征在同一类中的存在无关。\n使用 Scikit-learn 进行文档分类，作者 Zac Stewart。地址：http://suo.im/2uwBm3\n多层感知器（MLP）是一个简单的前馈神经网络，由多层节点组成，其中每个层与随后的层完全连接。多层感知器在 Scikit-learn 版本 0.18 中作了介绍。\n首先从 Scikit-learn 文档中阅读 MLP 分类器的概述，然后使用教程练习实现。\n神经网络模型（监督式），Scikit-learn 文档。地址：http://suo.im/3oR76l\nPython 和 Scikit-learn 的神经网络初学者指南 0.18！作者 Jose Portilla。地址：http://suo.im/2tX6rG\n第3步：更多聚类\n我们现在接着讲聚类，一种无监督学习形式。上篇中，我们讨论了 k-means 算法; 我们在此介绍 DBSCAN 和期望最大化（EM）。\nScikit-learn聚类算法\n首先，阅读这些介绍性文章; 第一个是 k 均值和 EM 聚类技术的快速比较，是对新聚类形式的一个很好的继续，第二个是对 Scikit-learn 中可用的聚类技术的概述：\n聚类技术比较：简明技术概述，作者 Matthew Mayo。地址：http://suo.im/4ctIvI\n在玩具数据集中比较不同的聚类算法，Scikit-learn 文档。地址：http://suo.im/4uvbbM\n期望最大化（EM）是概率聚类算法，并因此涉及确定实例属于特定聚类的概率。EM 接近统计模型中参数的最大似然性或最大后验估计（Han、Kamber 和 Pei）。EM 过程从一组参数开始迭代直到相对于 k 聚类的聚类最大化。\n首先阅读关于 EM 算法的教程。接下来，看看相关的 Scikit-learn 文档。最后，按照教程使用 Python 自己实现 EM 聚类。\n期望最大化（EM）算法教程，作者 Elena Sharova。地址：http://suo.im/33ukYd\n高斯混合模型，Scikit-learn 文档。地址：http://suo.im/20C2tZ。\n使用 Python 构建高斯混合模型的快速介绍，作者 Tiago Ramalho。地址：http://suo.im/4oxFsj\n如果高斯混合模型初看起来令人困惑，那么来自 Scikit-learn 文档的这一相关部分应该可以减轻任何多余的担心：\n高斯混合对象实现期望最大化（EM）算法以拟合高斯模型混合。\n基于密度且具有噪声的空间聚类应用（DBSCAN）通过将密集数据点分组在一起，并将低密度数据点指定为异常值来进行操作。\n首先从 Scikit-learn 的文档中阅读并遵循 DBSCAN 的示例实现，然后按照简明的教程学习：\nDBSCAN 聚类算法演示，Scikit-learn 文档。地址：http://suo.im/1l9tvX\n基于密度的聚类算法（DBSCAN）和实现。地址：http://suo.im/1LEoXC\n第4步：更多的集成方法\n上篇只涉及一个单一的集成方法：随机森林（RF）。RF 作为一个顶级的分类器，在过去几年中取得了巨大的成功，但它肯定不是唯一的集成分类器。我们将看看包装、提升和投票。\n给我一个提升\n首先，阅读这些集成学习器的概述，第一个是通用性的；第二个是它们与 Scikit-learn 有关：\n集成学习器介绍，作者 Matthew Mayo。地址：http://suo.im/cLESw\nScikit-learn 中的集成方法，Scikit-learn 文档。地址：http://suo.im/yFuY9\n然后，在继续使用新的集成方法之前，请通过一个新的教程快速学习随机森林：\nPython 中的随机森林，来自 Yhat。地址：http://suo.im/2eujI\n包装、提升和投票都是不同形式的集成分类器，全部涉及建构多个模型; 然而，这些模型由什么算法构建，模型使用的数据，以及结果如何最终组合起来，这些都会随着方案而变化。\n包装：从同一分类算法构建多个模型，同时使用来自训练集的不同（独立）数据样本——Scikit-learn 实现包装分类器\n提升：从同一分类算法构建多个模型，一个接一个地链接模型，以提高每个后续模型的学习——Scikit-learn 实现 AdaBoost\n投票：构建来自不同分类算法的多个模型，并且使用标准来确定模型如何最好地组合——Scikit-learn 实现投票分类器\n那么，为什么要组合模型？为了从一个特定角度处理这个问题，这里是偏差-方差权衡的概述，具体涉及到提升，以下是 Scikit-learn 文档：\n单一评估器 vs 包装：偏差-方差分解，Scikit-learn 文档。地址：http://suo.im/3izlRB\n现在你已经阅读了关于集成学习器的一些介绍性材料，并且对几个特定的集成分类器有了基本了解，下面介绍如何从 Machine Learning Mastery 中使用 Scikit-learn 在 Python 中实现集成分类器：\n使用 Scikit-learn 在 Python 中实现集成机器学习算法，作者 Jason Brownlee。地址：http://suo.im/9WEAr\n第5步：梯度提升\n下一步我们继续学习集成分类器，探讨一个当代最流行的机器学习算法。梯度提升最近在机器学习中产生了显著的影响，成为了 Kaggle 竞赛中最受欢迎和成功的算法之一。\n给我一个梯度提升\n首先，阅读梯度提升的概述：\n维基百科条目：梯度提升。地址：http://suo.im/TslWi\n接下来，了解为什么梯度提升是 Kaggle 竞赛中「最制胜」的方法：\n为什么梯度提升完美解决了诸多 Kaggle 难题？Quora，地址：http://suo.im/3rS6ZO\nKaggle 大师解释什么是梯度提升，作者 Ben Gorman。地址：http://suo.im/3nXlWR\n虽然 Scikit-learn 有自己的梯度提升实现，我们将稍作改变，使用 XGBoost 库，我们提到过这是一个更快的实现。\n以下链接提供了 XGBoost 库的一些额外信息，以及梯度提升（出于必要）：\n维基百科条目：XGBoost。地址：http://suo.im/2UlJ3V\nGhub 上的 XGBoost 库。地址：http://suo.im/2JeQI8\nXGBoost 文档。地址：http://suo.im/QRRrm\n现在，按照这个教程把所有汇聚起来：\nPython 中 XGBoost 梯度提升树的实现指南，作者 Jesse Steinweg-Woods。地址：http://suo.im/4FTqD5\n你还可以按照这些更简洁的示例进行强化：\nXGBoost 在 Kaggle 上的示例（Python）。地址：http://suo.im/4F9A1J\nIris 数据集和 XGBoost 简单教程，作者 Ieva Zarina。地址：http://suo.im/2Lyb1a\n第6步：更多的降维\n降维是通过使用过程来获得一组主变量，将用于模型构建的变量从其初始数减少到一个减少数。\n有两种主要形式的降维：\n1. 特征选择——选择相关特征的子集。地址：http://suo.im/4wlkrj\n2. 特征提取——构建一个信息性和非冗余的衍生值特征集。地址：http://suo.im/3Gf0Yw\n下面是一对常用的特征提取方法。\n主成分分析（PCA）是一种统计步骤，它使用正交变换将可能相关变量的一组观测值转换为一组称为主成分的线性不相关变量值。主成分的数量小于或等于原始变量的数量。这种变换以这样的方式定义，即第一主成分具有最大可能的方差（即考虑数据中尽可能多的变率）\n以上定义来自 PCA 维基百科条目，如果感兴趣可进一步阅读。但是，下面的概述/教程非常彻底：\n主成分分析：3 个简单的步骤，作者 Sebastian Raschka。地址：http://suo.im/1ahFdW\n线性判别分析（LDA）是 Fisher 线性判别的泛化，是统计学、模式识别和机器学习中使用的一种方法，用于发现线性组合特征或分离两个或多个类别的对象或事件的特征。所得到的组合可以用作线性分类器，或者更常见地，用作后续分类之前的降维。\nLDA 与方差分析（ANOVA）和回归分析密切相关，它同样尝试将一个因变量表示为其他特征或测量的线性组合。然而，ANOVA 使用分类独立变量和连续因变量，而判别分析具有连续的独立变量和分类依赖变量（即类标签）。\n上面的定义也来自维基百科。下面是完整的阅读：\n线性判别分析——直至比特，作者 Sebastian Raschka。地址：http://suo.im/gyDOb\n你对 PCA 和 LDA 对于降维的实际差异是否感到困惑？Sebastian Raschka 做了如下澄清：\n线性判别分析（LDA）和主成分分析（PCA）都是通常用于降维的线性转换技术。PCA 可以被描述为「无监督」算法，因为它「忽略」类标签，并且其目标是找到使数据集中的方差最大化的方向（所谓的主成分）。与 PCA 相反，LDA 是「监督的」并且计算表示使多个类之间的间隔最大化的轴的方向（「线性判别式」）。\n有关这方面的简要说明，请阅读以下内容：\nLDA 和 PCA 之间的降维有什么区别？作者 Sebastian Raschka。地址：http://suo.im/2IPt0U\n第 7 步：更多的深度学习\n上篇中提供了一个学习神经网络和深度学习的入口。如果你的学习到目前比较顺利并希望巩固对神经网络的理解，并练习实现几个常见的神经网络模型，那么请继续往下看。\n首先，看一些深度学习基础材料：\n深度学习关键术语及解释，作者 Matthew Mayo\n理解深度学习的 7 个步骤，作者 Matthew Mayo。地址：http://suo.im/3QmEfV\n接下来，在 Google 的机器智能开源软件库 TensorFlow（一个有效的深度学习框架和现今几乎是最好的神经网络工具）尝试一些简明的概述／教程：\n机器学习敲门砖：任何人都能看懂的 TensorFlow 介绍 （第 1、2 部分）\n入门级解读：小白也能看懂的 TensorFlow 介绍 （第 3、4 部分）\n最后，直接从 TensorFlow 网站试用这些教程，它实现了一些最流行和常见的神经网络模型：\n循环神经网络，谷歌 TensorFlow 教程。地址：http://suo.im/2gtkze\n卷积神经网络，谷歌 TensorFlow 教程。地址：http://suo.im/g8Lbg\n此外，目前一篇关于 7 个步骤掌握深度学习的文章正在写作之中，重点介绍使用位于 TensorFlow 顶部的高级 API，以增模型实现的容易性和灵活性。我也将在完成后在这儿添加一个链接。\n相关的：\n进入机器学习行业之前应该阅读的 5 本电子书。地址：http://suo.im/SlZKt\n理解深度学习的 7 个步骤。地址：http://suo.im/3QmEfV\n机器学习关键术语及解释。地址：http://suo.im/2URQGm\n你想更深入了解学习Python知识体系，你可以看一下我们花费了一个多月整理了上百小时的几百个知识点体系内容：\n【超全整理】《Python自动化全能开发从入门到精通》笔记全放送"}
{"content2":"前言\n————————————————————————————————————————\n在机器学习算法中，我们经常会遇到分类特征，例如：人的性别有男女，祖国有中国，美国，法国等。\n这些特征值并不是连续的，而是离散的，无序的。通常我们需要对其进行特征数字化。\n那什么是特征数字化呢？例子如下：\n性别特征：[\"男\"，\"女\"]\n祖国特征：[\"中国\"，\"美国，\"法国\"]\n运动特征：[\"足球\"，\"篮球\"，\"羽毛球\"，\"乒乓球\"]\n假如某个样本（某个人），他的特征是这样的[\"男\",\"中国\",\"乒乓球\"]，我们可以用 [0,0,4] 来表示，但是这样的特征处理并不能直接放入机器学习算法中。因为类别之间是无序的（运动数据就是任意排序的）。\n什么是独热编码（One-Hot）？\n————————————————————————————————————————\nOne-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。\nOne-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。\nOne-Hot实际案例\n————————————————————————————————————————\n就拿上面的例子来说吧，性别特征：[\"男\",\"女\"]，按照N位状态寄存器来对N个状态进行编码的原理，咱们处理后应该是这样的（这里只有两个特征，所以N=2）：\n男  =>  10\n女  =>  01\n祖国特征：[\"中国\"，\"美国，\"法国\"]（这里N=3）：\n中国  =>  100\n美国  =>  010\n法国  =>  001\n运动特征：[\"足球\"，\"篮球\"，\"羽毛球\"，\"乒乓球\"]（这里N=4）：\n足球  =>  1000\n篮球  =>  0100\n羽毛球  =>  0010\n乒乓球  =>  0001\n所以，当一个样本为[\"男\",\"中国\",\"乒乓球\"]的时候，完整的特征数字化的结果为：\n[1，0，1，0，0，0，0，0，1]\n下图可能会更好理解：\nOne-Hot在python中的使用\n————————————————————————————————————————\n1\n2\n3\n4\n5\n6\n7\n8\nfrom sklearn import preprocessing\nenc = preprocessing.OneHotEncoder()\nenc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])  #这里一共有4个数据，3种特征\narray = enc.transform([[0,1,3]]).toarray()  #这里使用一个新的数据来测试\nprint array   # [[ 1  0  0  1  0  0  0  0  1]]\n结果为 1 0 0 1 0 0 0 0 1\n为什么使用one-hot编码来处理离散型特征?\n————————————————————————————————————————\n在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。\n而我们使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。\n将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。\n比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。\n不需要使用one-hot编码来处理的情况\n————————————————————————————————————————电动叉车\n将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。\n比如，该离散特征共有1000个取值，我们分成两组，分别是400和600,两个小组之间的距离有合适的定义，组内的距离也有合适的定义，那就没必要用one-hot 编码。\n离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1。"}
{"content2":"Chapter One人工智能 机器学习与深度学习简介\n1.1人工智能 :弱人工智能 和 强人工智能\n机器学习\n使用算法 通过大量数据进行训练后产生模型 通过使用这个模型达到预测效果\n是人工智能的分支 监督学习 无监督学习 增强学习\n深度学习\n模仿人类神经网络的工作方式\n是机器学习的分支 多层感知器 深度神经网络 递归神经网络\n近年人工智能发展加速的原因\n1大数据分布式存储与计算\n2GPU TPU 并行计算\nCPU含有数颗核心 为吮血处理进行优化\nGPU可以有高达数千个小型而且高效的核心 可以发挥并行计算的强大功能\n深度学习以大量矩阵运算模拟神经元的工作方式 矩阵运算的特性是 单一运算都很简单 但是需要大量运算特别适合采用 并行计算 GPU通过大量核心进行并行计算\n1.2机器学习介绍\n由features和label组成\nfeatures：数据的特征 如温度风向 风速 季节  气压\nlabel：预测的目标 如天气（下雨，晴天，有雾等） 或者气温的具体数值\n两个阶段：训练 预测\n训练：训练数据通过特征提取 得到 features 和label 放进算法中训练后得到  预测用的模型\n预测：新数据 ->特征提取得到features  放进模型 得到预测结果\n1.3机器学习分类\n有监督学习\n二元分类：预测结果为是和否 如今天下雨 今天不下雨  图片是猫  图片不是猫\n多元分类：判断是多个类别中的哪一类 如 判断今天天气是哪种天气 图片是哪种动物\n回归分析：预测数值是多少 如预测今天温度是多少  预测房屋的价格是多少\n无监督学习\n对于无监督的学习 从现有数据我们不知道要预测的答案，所以没有label（预测目标）\n例如cluster集群算法将数据分成几个差异性最大的群组 而群组内的则相似程度最高\n增强式学习\n增强式学习的原理：借助定义动作 状态 奖励的方式 不断训练机器循序渐进  学会执行某项任务的算法\n例如 训练机器玩超级玛丽电子游戏动作 ：左/右/跳 状态：当前游戏的界面奖励：得分/受伤\n借助不断地训练学会玩游戏\n常见的算法有Q-learning 。TD（Temporal Difference）。Sarsa 。\n1.4深度学习简介\n由一个输入层\n隐藏层（可以有非常多层 所有被称为深度学习）\n和一个输出层组成\n深度学习的应用很广泛 可以将深度学习技术应用在有监督学习 无监督学习 和增强式学习等领域"}
{"content2":"人人都要学一点深度学习（1）- 为什么我们需要它\n版权声明\n本文由@leftnoteasy发布于 http://leftnoteasy.cnblogs.com, 如需全文转载或有其他问题请联系wheeleast (at) gmail.com。\n1.开篇\n1.1 为什么我开始写这个系列博客\n说五年前我还在某A云公司的时候，身在一个机器学习算法组，对机器学习怀有浓厚的兴趣。花了好多的时间来试图搞清楚各种流行的机器学习算法，经常周末也跟同事探讨公式的推倒和背后的意义。写博客的主要动力是让自己能够更好的理解机器学习。\n后来坚持了没有太久的时间就换到大数据方向了，最主要的原因是觉得自己数学天赋太差，尤其是数学。当初学习的时候主要参考的PRML，Andrew Moore的PPT，Andrew Ng的公开课，plukids博客，另外加上淘宝斌强哥的各种悉心指导。但是学了好久，公式能大概看懂是怎么回事，不过自己徒手推出来实在是太艰难了。PRML的习题，甚至是具体数学的习题，都很难做得出来。\n后面的发生的事情就理所当然了，既然很难在这个领域做到核心（我的理解是具有及其好的数学天赋作为后盾才能做到机器学习的核心），那么为什么不换一个更适合自己的方向呢？恩好吧，基础数据架构（Infra）看起来是个不错的方向，不需要理解太多的数学（除了真的需要去实现Paxos）。做Infra如果有架构设计的基础，另外加上勤奋，多多少少还是能做出一些东西的。\n为什么我又要重新开始写机器学习相关的文章了？最主要的原因是现在的机器学习和五年前、十年前区别很大。最大的不同是，自从深度学习成为了机器学习舞台上最重要的一个角色起，机器学习变得更加真实了，利用深度学习可以做出很多很有意思的真实世界的应用，而这些东西在几年前的门槛要高得多。我在本文之后会更详细的展开此点。\n此外这几年的工具发展神速，利用TensorFlow、MXNet或者其他类似的工具可以很容易的开始自己的pet project，也不用理解太多背后的细节。而在几年前能用的现成工具寥寥无几，而且十分的碎片化，比如说如果想要做分类器吧，需要用libsvm，需要搞跨语言调用。如果要换个算法的话，那可是要命的事情了。当然这些东西对于大公司来说都不是事儿，但是对于个人学习者来说需要投入的经历太多了，远不是业余时间可以承担的。\n1.2 What to expect?\n差不多关注了几个月的深度学习，虽然没有花太多时间来写代码，但是各种各样的博客、视频、公开课还是看了一些。这个系列和几个我看过的主要内容的差异：\n首先这个不是一个科普杂文，现在已经有很多旁征博引丰富多彩老少皆宜的杂文，比如说王川的深度学习到底有多深系列，我准备少些一些历史和背景花絮，多写一些技术。\n其次这个不是一个系统的深度学习教程，现在已经有非常多非常好的相关公开课，比如说Stanford的CS231N\\CS224D，Hilton的，Udacity的等等。我不准备写得面面俱到。\n另外我会尽量少的涉及数学，因为我不可能把数学推导过程写得比Ian Goodfellow的Deep Learning书写得更清楚。但是我会尽量把最重要的部分写出来。\n所以我希望写出的是，当看过网上的博客、公开课和书后，什么地方是最难理解的。\n2. 深度学习为什么是革命性的\n啰里啰嗦了这么多，开始正文了。此篇博客严重参考了来自[1]第一章Introduction的内容，包括图片和内容。\n2.1 前深度学习的世界\n深度学习不是一个新概念，它已经存在好几十年了，具体可以参考[1]/[2]，这里所说的深度学习世界大抵是在最近几年深度学习刷新各个机器学习领域之后了。\n前深度学习世界的特征就是：在人类强的地方很弱，在人类弱的地方可能很强。人类强的地方比如说图像识别（猫还是狗）；图片语义分割（参考[3]）比如看出一个图片中哪部分是树、哪部分是房子。人类弱的地方比如说下棋、语法标记（一个句子里面哪些是助词哪些是动词）。\n这个最主要的原因是，那种对于人类来说简单的东西（在万千世界中识别出一只猫）没办法用一个正式的数学公式去描述[1]。\n比如说你无法用数学公式去定义一个猫的形状。因为不同的角度、颜色、距离、光线的组合让这个基本上没有办法做到。\n所以在这个基础上谈智能实在是镜中月水中花：你连一个猫都不认识，怎么能够取代人类？因为人类的世界远远比围棋要复杂得多。\n[1] 里面还有一个有趣的例子，关于1989年时候著名的专家系统Cyc：\nIts inference engine detected an inconsistency in the story: it knew that people do not have electrical parts, but because Fred was holding an electric razor, it believed the entity “FredWhileShaving” contained electrical parts. It therefore asked whether Fred was still a person while he was shaving.\n... 它的推论引擎发现了一个前后矛盾的地方：它知道人是没有电驱动的模块，但是因为Fred拿起了一个电动剃须刀，所以这个引擎认为\"一个正在剃胡子的Fred\"有了一个电驱动的模块。然后这个系统就问起这个Fred到底还是不是一个人啊。\n此外，前机器学习时代一个重要的特征是要设计特征，比如说如果要做一个淘宝的商品自动分类器，特征可能有商品的题目、描述、图片等等；特征还需要进行严格的预处理，比如说要过滤掉描述里面亲包邮啊这种无意义的话，而且对正文里面的描述也要进行重点抽取才能够符合训练的标准。等到特征选择、清理好了之后才能够运行出有意义的结果。而且如果需要重新选择特征，或者更改特征，那就需要重新重头来过。\n曾经有些公司甚至有\"特征抽取工程师\"这样的职位，以前阿里同事做分类器的时候，就要特别注意不要把成人玩具分到儿童玩具的类目里面，万一被抓到把柄了那可就要丢工作了。\n2.2 深度学习带来了什么\n深度学习最重要的东西就是自带了特征学习（representation learning，有时候也被翻译为表征学习)，简单来说就是，不需要进行特别的特征抽取。从这个来说，深度学习相对传统机器学习来说就有了太多的优势，因为一个设计好的系统能够被相对容易地移植到新的任务上去。\n参考最近DeepMind发布的一个深度增强学习的无监督系统玩复杂任务游戏游戏的例子[4]。系统从游戏的屏幕像素开始自我学习，到学会玩一个复杂游戏并超过人类的专业玩家，并没有进行特别的人工特征抽取，这个在传统的机器学习方式上看起来是很难想象的。\n此外深度学习另外的一个优势是，可以表述相对与浅度学习更复杂的东西，这里不准备展开描述，不然就要提到XOR，维度诅咒（The Curse of Dimensionality）等等相对枯燥而且很难说清楚的理论知识了。简单来说，提升维度当然是一个很厉害人人都想的东西，参考三体中的降维打击。但是与此同时也带来了很多计算上的挑战，得益于这几年神经科学，算法研究和硬件(特别是GPU）提升，我们可以尝试越来越深的模型。\n3. 现在（2016）的深度学习究竟在什么位置\n同样，主要参考了[1]的综述部分：\n首先衡量深度网络的复杂程度主要有两个方面：1. 网络中一共有多少个神经元，2. 每个神经元平均与多少个其他的神经元连接。\n首先是连接的数量：\n每个蓝色的小点表示一些里程碑级别的系统公布的结果，比如说10. 就是GoogLeNet (Szegedy et al., 2014a)\n从这点看来，似乎还是挺乐观的，比如说10. 已经很接近人类了，但是看看下面。。。\n神经元的数量：\n可以看到，目前最先进的系统所模拟的神经元的数量处于蜜蜂和青蛙之间。蓝色的线表示预估的增长曲线，如果在没有革命性的进步前，系统差不多可以到2050年的时候模拟人类同样的神经元的数目，所以革命之路还很漫长。\n但是从另一方面来说，神经元的数目也不是唯一的衡量标准，就像是玩星际一样，APM 150的意识派也可以完虐500的抽经流选手。我们可以教一个神经元如此少的系统玩复杂的电子游戏，但是我们没有办法教一个青蛙玩这种游戏。所以也不用悲观，也许西部世界似的人工智能会到来得比想象中更早。\n引用\n[1] Deep Learning: Ian Goodfellow, Yoshua Bengio, Aaron Courville\n[2] 王川的深度学习到底有多深系列\n[3] http://stackoverflow.com/questions/33947823/what-is-semantic-segmentation-compared-to-segmentation-and-scene-labeling\n[4] https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/"}
{"content2":"一、决策树模型组合\n单决策树C4.5由于功能太简单，并且非常容易出现过拟合的现象，于是引申出了许多变种决策树，就是将单决策树进行模型组合，形成多决策树，比较典型的就是迭代决策树GBRT和随机森林RF。\n在最近几年的paper上，如iccv这种重量级会议，iccv 09年的里面有不少文章都是与Boosting和随机森林相关的。模型组合+决策树相关算法有两种比较基本的形式：随机森林RF与GBDT，其他比较新的模型组合+决策树算法都是来自这两种算法的延伸。\n核心思想：其实很多“渐进梯度”Gradient Boost都只是一个框架，里面可以套用很多不同的算法。\n首先说明一下，GBRT这个算法有很多名字，但都是同一个算法：\nGBRT (Gradient BoostRegression Tree) 渐进梯度回归树\nGBDT (Gradient BoostDecision Tree) 渐进梯度决策树\nMART (MultipleAdditive Regression Tree) 多决策回归树\nTree Net决策树网络\n二、GBRT\n迭代决策树算法，在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。\nGBRT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和Adaboosting一样，也加入了boosting这一项。\n关于GBRT的介绍可以可以参考：GBDT（MART） 迭代决策树入门教程 | 简介。\n提起决策树（DT, DecisionTree）不要只想到C4.5单分类决策树，GBRT不是分类树而是回归树！\n决策树分为回归树和分类树：\n回归树用于预测实数值，如明天温度、用户年龄\n分类树用于分类标签值，如晴天/阴天/雾/雨、用户性别\n注意前者结果加减是有意义的，如10岁+5岁-3岁=12岁，后者结果加减无意义，如男+女=到底是男还是女？GBRT的核心在于累加所有树的结果作为最终结果，而分类树是没有办法累加的。所以GBDT中的树都是回归树而非分类树。\n第一棵树是正常的，之后所有的树的决策全是由残差（此次的值与上次的值之差）来作决策。\n三、算法原理\n0.给定一个初始值\n1.建立M棵决策树（迭代M次）\n2.对函数估计值F(x)进行Logistic变换\n3.对于K各分类进行下面的操作（其实这个for循环也可以理解为向量的操作，每个样本点xi都对应了K种可能的分类yi，所以yi，F(xi)，p(xi)都是一个K维向量）\n4.求得残差减少的梯度方向\n5.根据每个样本点x，与其残差减少的梯度方向，得到一棵由J个叶子节点组成的决策树\n6.当决策树建立完成后，通过这个公式，可以得到每个叶子节点的增益（这个增益在预测时候用的）\n每个增益的组成其实也是一个K维向量，表示如果在决策树预测的过程中，如果某个样本点掉入了这个叶子节点，则其对应的K个分类的值是多少。比如GBDT得到了三棵决策树，一个样本点在预测的时候，也会掉入3个叶子节点上，其增益分别为（假设为3分类问题）：\n(0.5, 0.8, 0.1), (0.2, 0.6, 0.3), (0.4, .0.3, 0.3)，那么这样最终得到的分类为第二个，因为选择分类2的决策树是最多的。\n7.将当前得到的决策树与之前的那些决策树合并起来，作为一个新的模型（跟6中的例子差不多）\n--------------------------------------------------------------------------------------------------------------\n还是年龄预测，简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下所示结果：\n现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树只有一个分枝，并且限定只学两棵树。我们会得到如下所示结果：\n在第一棵树分枝和一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1，-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是我可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。\n换句话说，现在A,B,C,D的预测值都和真实年龄一致了。\nA: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14\nB: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16\nC: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24\nD: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26\n那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。\n四、GBRT适用范围\n该版本的GBRT几乎可用于所有的回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBRT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。\n五、搜索引擎排序应用RankNet\n搜索排序关注各个doc的顺序而不是绝对值，所以需要一个新的cost function，而RankNet基本就是在定义这个cost function，它可以兼容不同的算法（GBDT、神经网络...）。\n实际的搜索排序使用的是Lambda MART算法，必须指出的是由于这里要使用排序需要的cost function，LambdaMART迭代用的并不是残差。Lambda在这里充当替代残差的计算方法，它使用了一种类似Gradient*步长模拟残差的方法。这里的MART在求解方法上和之前说的残差略有不同，其区别描述见这里。\n搜索排序也需要训练集，但多数用人工标注实现，即对每个(query, doc)pair给定一个分值（如1, 2, 3, 4），分值越高越相关，越应该排到前面。RankNet就是基于此制定了一个学习误差衡量方法，即cost function。RankNet对任意两个文档A,B，通过它们的人工标注分差，用sigmoid函数估计两者顺序和逆序的概率P1。然后同理用机器学习到的分差计算概率P2（sigmoid的好处在于它允许机器学习得到的分值是任意实数值，只要它们的分差和标准分的分差一致，P2就趋近于P1）。这时利用P1和P2求的两者的交叉熵，该交叉熵就是cost function。\n有了cost function，可以求导求Gradient，Gradient即每个文档得分的一个下降方向组成的N维向量，N为文档个数（应该说是query-doc pair个数）。这里仅仅是把”求残差“的逻辑替换为”求梯度“。每个样本通过Shrinkage累加都会得到一个最终得分，直接按分数从大到小排序就可以了。"}
{"content2":"机器学习六--K-means聚类算法\n想想常见的分类算法有决策树、Logistic回归、SVM、贝叶斯等。分类作为一种监督学习方法，要求必须事先明确知道各个类别的信息，并且断言所有待分类项都有一个类别与之对应。但是很多时候上述条件得不到满足，尤其是在处理海量数据的时候，如果通过预处理使得数据满足分类算法的要求，则代价非常大，想想如果给你50个G这么大的文本，里面已经分好词，这时需要将其按照给定的几十个关键字进行划分归类，监督学习的方法确实有点困难，而且也不划算，前期工作做得太多了。\n这时候可以考虑使用聚类算法，我们只需要知道这几十个关键字是什么就可以了。聚类属于无监督学习，相比于分类，聚类不依赖预定义的类和类标号的训练实例。本文首先介绍聚类的基础——距离与相异度，然后介绍一种常见的聚类算法——K-means聚类。\n在正式讨论聚类前，我们要先弄清楚一个问题：如何定量计算两个可比较元素间的相异度。前面的这些知识弄懂了，加上K-means的定义，基本上就可以大概理解K-means的算法了，不算一个特别难的算法。用通俗的话说，相异度就是两个东西差别有多大，例如人类与章鱼的相异度明显大于人类与黑猩猩的相异度，这是能我们直观感受到的。但是，计算机没有这种直观感受能力，我们必须对相异度在数学上进行定量定义。\n设X={x1,x2,x3,,,,xn},Y={y1,y2,y3,,,,yn} ，其中X，Y是两个元素项，各自具有n个可度量特征属性，那么X和Y的相异度定义为：d=(X,Y)=f(X,Y)->R，其中R为实数域。也就是说相异度是两个元素对实数域的一个映射，所映射的实数定量表示两个元素的相异度。\n下面介绍不同类型变量相异度计算方法。\n标量\n标量也就是无方向意义的数字，也叫标度变量。现在先考虑元素的所有特征属性都是标量的情况。例如，计算X={2,1,102}和Y={1,3,2}的相异度。一种很自然的想法是用两者的欧几里得距离来作为相异度，欧几里得距离的定义如下：\n其意义就是两个元素在欧氏空间中的集合距离，因为其直观易懂且可解释性强，被广泛用于标识两个标量元素的相异度。将上面两个示例数据代入公式，可得两者的欧氏距离为：\n除欧氏距离外，常用作度量标量相异度的还有曼哈顿距离和闵可夫斯基距离，两者定义如下：\n曼哈顿距离：\n闵可夫斯基距离：\n欧氏距离和曼哈顿距离可以看做是闵可夫斯基距离在p=2和p=1下的特例。\n0-1规格化\n下面要说一下标量的规格化问题。上面这样计算相异度的方式有一点问题，就是取值范围大的属性对距离的影响高于取值范围小的属性。例如上述例子中第三个属性的取值跨度远大于前两个，这样不利于真实反映真实的相异度，为了解决这个问题，一般要对属性值进行规格化。所谓规格化就是将各个属性值按比例映射到相同的取值区间，这样是为了平衡各个属性对距离的影响。通常将各个属性均映射到[0,1]区间，映射公式为：\n其中max(ai)和min(ai)表示所有元素项中第i个属性的最大值和最小值。例如，将示例中的元素规格化到[0,1]区间后，就变成了X’={1,0,1}，Y’={0,1,0}，重新计算欧氏距离约为1.732。\n二元变量\n所谓二元变量是只能取0和1两种值变量，有点类似布尔值，通常用来标识是或不是这种二值属性。对于二元变量，上一节提到的距离不能很好标识其相异度，我们需要一种更适合的标识。一种常用的方法是用元素相同序位同值属性的比例来标识其相异度。\n设有X={1,0,0,0,1,0,1,1}，Y={0,0,0,1,1,1,1,1}，可以看到，两个元素第2、3、5、7和8个属性取值相同，而第1、4和6个取值不同，那么相异度可以标识为3/8=0.375。一般的，对于二元变量，相异度可用“取值不同的同位属性数/单个元素的属性位数”标识。\n上面所说的相异度应该叫做对称二元相异度。现实中还有一种情况，就是我们只关心两者都取1的情况，而认为两者都取0的属性并不意味着两者更相似。例如在根据病情对病人聚类时，如果两个人都患有肺癌，我们认为两个人增强了相似度，但如果两个人都没患肺癌，并不觉得这加强了两人的相似性，在这种情况下，改用“取值不同的同位属性数/(单个元素的属性位数-同取0的位数)”来标识相异度，这叫做非对称二元相异度。如果用1减去非对称二元相异度，则得到非对称二元相似度，也叫Jaccard系数，是一个非常重要的概念。\n分类变量\n分类变量是二元变量的推广，类似于程序中的枚举变量，但各个值没有数字或序数意义，如颜色、民族等等，对于分类变量，用“取值不同的同位属性数/单个元素的全部属性数”来标识其相异度。\n序数变量\n序数变量是具有序数意义的分类变量，通常可以按照一定顺序意义排列，如冠军、亚军和季军。对于序数变量，一般为每个值分配一个数，叫做这个值的秩，然后以秩代替原值当做标量属性计算相异度。\n向量\n对于向量，由于它不仅有大小而且有方向，所以闵可夫斯基距离不是度量其相异度的好办法，一种流行的做法是用两个向量的余弦度量，这个应该大家都知道吧，其度量公式为：\n其中||X||表示X的欧几里得范数。要注意，余弦度量度量的不是两者的相异度，而是相似度！\n什么是聚类？\n所谓聚类问题，就是给定一个元素集合D，其中每个元素具有n个可观察属性，使用某种算法将D划分成k个子集，要求每个子集内部的元素之间相异度尽可能低，而不同子集的元素相异度尽可能高。其中每个子集叫做一个簇。\n与分类不同，分类是示例式学习，要求分类前明确各个类别，并断言每个元素映射到一个类别，而聚类是观察式学习，在聚类前可以不知道类别甚至不给定类别数量，是无监督学习的一种。目前聚类广泛应用于统计学、生物学、数据库技术和市场营销等领域，相应的算法也非常的多。本文仅介绍一种最简单的聚类算法——k均值（k-means）算法。\nk均值算法的计算过程非常直观：\n1、从D中随机取k个元素，作为k个簇的各自的中心。\n2、分别计算剩下的元素到k个簇中心的相异度，将这些元素分别划归到相异度最低的簇。\n3、根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均数。\n4、将D中全部元素按照新的中心重新聚类。\n5、重复第4步，直到聚类结果不再变化。\n6、将结果输出。\n时间复杂度：O(T*n*k*m)\n空间复杂度：O(n*m)\nn:元素个数，k:第一步中选取的元素个数，m:每个元素的特征项个数，T:第5步中迭代的次数\n参考：\nT2噬菌体（很多理解都是借鉴这位大牛的，还在阅读学习TA的其他博文）\nK-means聚类--百度百科\n总结\n接下来的目标就是Logistic回归、SVM。之前看过很多遍有关这两个算法的博客，但是理解还是不够深入，继续学习，希望有所收获。"}
{"content2":"人工智能是 最近的一个比较火的名词，相信大家对于阿尔法狗都不陌生吧？其实我对人工智能以前也是非常抵触的，因为我认为机器人会取代人类，成为地球乃至宇宙的霸主，但是人工智能带给我的这种冲击，我个人感觉是欲罢不能的，进入正题，网上找了一个人工智能的框架，它的名字叫做syntaxnet  ，有兴趣的可以去看看，底层是用C++实现的。\n由于人工智能是一个比较新的名词，连我自己接触的也很少；所以也只能带大家一起摸索了，如果园子里有人工智能 方面的大牛，还希望多多请教。\nsyntaxnet 官方的解释是：有序的神经网络模型。它有另外一个奇怪的名字，叫做：TensorFlow 。TensorFlow实现的模型的描述这里可以找到；GOOGLE花费了大量的时间去研究怎么才能让机器更聪明的学习人类的语言，以及以更快的方式学习人类的语言；\n这里有必要去科普一下TensorFlow，我刚刚查了下；官方的解释是：TensorFlow是一款开源的使用使用数据流图的数值计算类库。在图形中的节点(Node)呈现了各种不同的数学操作等等...剩下的就不翻译了，有感兴趣的可以谷歌一下。其实我外语不太好，各位抱歉了，翻译啥的，慢慢来吧。\n训练模型\n下面的教程当中，我将告诉大家 如何训练模型，会介绍更多的和NPL相关的东西；重点关注点是NPL 管道。\n词性标注器\n考虑如下句子，它有 很多种不同的意思；I saw the man with glasses 以上句子由下面几部分组成：\n不同的字符串可以分割成如下几组：例如:\"I\",\"saw\",\"the\" 就是3组，分隔符为空格，每一个单词都有它们不同的意思，大家学过英语的人都知道，英语有时候一个词有10几个意思，并且这次意思在不同的语境中的意思都是不同的；比如这里面的saw是to see的过去式，然而已经提到过，不同的词在不同 的语境当中有不同的意思，比如saw在某些情况下可以作为名词，也有可能是现在时，上面说的需要一点英语基础的。\n如果要理解不同的词的意思，首先是需要知道不同的词在在这个句子中所扮演的不同角色，这个过程就叫做Part-of-Speech (POS)  Tagging,也就是词性标注器，这些角色叫做POS Tags,虽然一个单词可能对于这个句子来说拥有不同的上下文，但是对于任何的一个组成句子的单词来说，当它们的语义组合在一起的时候，往往Tag(释义)的个数会大幅减少，一般来说就是一种意思。\n对于POS Tagging来说，对于一个句子当中定义动词，是一个很有挑战性的东西。当动词和名词的意思很相近的时候，对于任何语言来说，定义动词或者名词，都是极其困难的。 Universal Dependencies 的目的就是为了解决这个问题，有兴趣的可以点开看看。\n训练SyntaxNet POS Tagger\n要得到这个句子的所有单词的正确Tag,我们首先必须让机器能够理解这个句子的具体意思，在当前上下文当中。这里我们可以采用一种句子当中的就近原则去分析，比如I saw the man with glasses, saw 的前面是I,saw 的后面是the；比如the的后面，一般来说是接名词或者形容词，而并不是动词。\n为了达到预估什么意思的目的，一般使用如下步骤：从左到右。我们先把这个句子的所有的临近的词配合起来，然后把这些意思都算出来，然后发送给神经网络分类器的前馈，用来分析POS Tags在不同的语境当中的不同意思。因为我们是按照从左到右的顺序，所以下一个单词的意思，也可能是由前一个或者几个单词的意思来判断的，比如I saw the man with glasses,中saw 如果 确定是动词了，the 肯定不是动词，man在句子中的意思或者是语法作用，肯定是前面的the来修饰的，所以后面的单词就算有不同的意思，也能由前面的单词，来进行筛选。\n所有的在这个包里面的模型都使用了灵活的标记语言去定义特性。比如POS Tag ,带参数brain_pos_features 在TaskSpec中，看起来像这样：\nstack(3).word stack(2).word stack(1).word stack.word input.word input(1).word input(2).word input(3).word; input.digit input.hyphen; stack.suffix(length=2) input.suffix(length=2) input(1).suffix(length=2); stack.prefix(length=2) input.prefix(length=2) input(1).prefix(length=2)\n注意stack 的意思是表示这个单词已经被Tagged了。所以，详细的说，有3种不同的Types对于这个功能来说：单词，后缀和前缀。所以更像一个嵌入式的矩阵，就好比Table里面又有个Table一样，串联起来了，送入了隐藏的层的链表中。\n下面给大家一张图先睹为快哦~~~\n未完待续~~敬请期待~~~"}
{"content2":"在网上找了个第三方智能机器人，可以实现聊天语音等功能，比较不错的。今天我们就开始智能机器人聊天的学习，例子中涉及的handler的有关知识和json数据的解析，请参见我的博客：android基础---->JSON数据的解析、android高级---->Handler的原理和android基础---->子线程更新UI\n目录导航\n获取图灵机器人key\n图灵机器人的一些api介绍\n在android程序中使用图灵机器人\n友情链接\n获取图灵机器人key\n<1>访问图灵机器人官方网站： www.tuling123.com\n<2>点击右上角注册按钮\n<3>填写注册信息，并完成激活操作\n<4>进入个人中心板块，在”机器人接入”页面即可获得图灵APIKEY，获取之后您可以根据自己的需要来接入到微信公众号、QQ等各个平台中使用\n图灵机器人的一些api介绍\nAPI简介\n图灵机器人API是在人工智能的核心能力（包括语义理解、智能问答、场景交互、知识管理等）的基础上，为广大开发者、合作伙伴和企业提供的一系列基于云计算和大数据平台的在线服务和开发接口。\n开发者可以利用图灵机器人的API创建各种在线服务，灵活定义机器人的属性、编辑机器人的智能问答内容，打造个人专属智能交互机器人，也支持多渠道（微信公众平台、QQ聊天）的快速接入。\n接口地址\nhttp://www.tuling123.com/openapi/api\n请求方式\nHTTP POST/GET\n注：若采用get方式请求，需将参数中的空格须用“%20”替换（URL转码），否则会被服务器当作无效请求拒绝。我们更推荐使用post方式请求。\n请求参数\n请求URL示例：http://www.tuling123.com/openapi/api?key=APIKEY&info=今天天气怎么样。详细文档请参见官网：http://tuling123.com/html/doc/api.html\n在android程序中使用图灵机器人\n我们创建一个android项目，来体验一下图灵机器人的用法，项目结构如下：\n使用步骤：\n发送http请求，url为：http://www.tuling123.com/openapi/api?info=\"你要发送的信息\"\n得到响应结果，是一个Json格式的信息\n用Json解析结果，得到有用的信息\n一、 在layout中简单的布局，增加一个TextView用于显示返回的Json数据，EditView用于用户输入发送的信息，Button是发送按钮：\n<?xml version=\"1.0\" encoding=\"utf-8\"?> <LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:orientation=\"vertical\" tools:context=\"com.example.linux.robottest.MainActivity\"> <TextView android:id=\"@+id/textView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"Hello World, huhx.\" /> <LinearLayout android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:orientation=\"horizontal\"> <EditText android:id=\"@+id/editView\" android:minWidth=\"200dp\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" /> <Button android:layout_marginLeft=\"20dp\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:onClick=\"sendMessage\" android:text=\"发送\" /> </LinearLayout> </LinearLayout>\n二、  在MainActivity中处理整个发送接收的流程：\noncreate()方法中初始化一些数据：\nprivate final static String TAG = \"huhxRobot\"; private TextView textView; private EditText editText; private final String apiUrl = \"http://www.tuling123.com/openapi/api\"; private final String apiKey = \"你的apikey\"; String urlStr = apiUrl + \"?key=\" + apiKey; final static int ROBOT_MESSAGE = 0; @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); textView = (TextView) findViewById(R.id.textView); editText = (EditText) findViewById(R.id.editView); }\n用post请求向机器人发送信息：\n// 向机器人发送信息 public void sendMessage(View view) { String sendmessage = editText.getText().toString(); final String params = \"info=\" + sendmessage; new Thread(new Runnable() { @Override public void run() { HttpURLConnection connection = null; OutputStream outputStream = null; BufferedReader reader = null; StringBuilder result = new StringBuilder(); String line = \"\"; try { URL url = new URL(urlStr); connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"POST\"); connection.setReadTimeout(5000); connection.setConnectTimeout(5000); outputStream = connection.getOutputStream(); outputStream.write(params.getBytes()); reader = new BufferedReader(new InputStreamReader(connection.getInputStream())); while ((line = reader.readLine()) != null) { result.append(line); } Message message = new Message(); message.obj = result.toString(); message.what = ROBOT_MESSAGE; handler.sendMessage(message); } catch (Exception e) { e.printStackTrace(); } finally { if (reader != null) { try { reader.close(); } catch (IOException e) { e.printStackTrace(); } } if (outputStream != null) { try { outputStream.close(); } catch (IOException e) { e.printStackTrace(); } } connection.disconnect(); } } }).start(); }\nhandler处理信息：若对handler不了解的，请参见我的博客，在友情链接中会提到。\nprivate Handler handler = new Handler() { @Override public void handleMessage(Message msg) { switch (msg.what) { case ROBOT_MESSAGE: String Jsonmessage = (String) msg.obj; Log.i(TAG, Jsonmessage); String text = \"\"; try { JSONObject jsonObject = new JSONObject(Jsonmessage); text = (String) jsonObject.get(\"text\"); } catch (JSONException e) { e.printStackTrace(); } textView.setText(Jsonmessage); Log.i(TAG, text); Toast.makeText(MainActivity.this, text, Toast.LENGTH_SHORT).show(); } } };\n三、 在Manifest中声明网络权限：\n<uses-permission android:name=\"android.permission.INTERNET\"/>\n四、 输出结果如下：\n输入：hello 结果： {\"code\":100000,\"text\":\"你也好 嘻嘻\"}\n五、 异常码的说明：\ncode\n说明\n100000\n文本类\n200000\n链接类\n302000\n新闻类\n308000\n菜谱类\n313000（儿童版）\n儿歌类\n314000（儿童版）\n诗词类\n40001\n参数key错误\n40002\n请求内容info为空\n40004\n当天请求次数已使用完\n40007\n数据格式异常\n六、 自定义回复功能：\n在个人中心-->左侧NLP知识库-->新增：\n测试一下：\n输入：huhx 结果：huhx的：http://www.cnblogs.com/huhx\n友情链接\nhandler的使用原理    android高级---->Handler的原理\nhandler的简单使用    android基础---->子线程更新UI\njson数据的解析        android基础---->JSON数据的解析"}
{"content2":"我们在Github上的贡献者和提交者之中检查了用Python语言进行机器学习的开源项目，并挑选出最受欢迎和最活跃的项目。\n1. Scikit-learn（重点推荐）\nwww.github.com/scikit-learn/scikit-learn\nScikit-learn 是基于Scipy为机器学习建造的的一个Python模块，他的特色就是多样化的分类，回归和聚类的算法包括支持向量机，逻辑回归，朴素贝叶斯分类器，随机森林，Gradient Boosting，聚类算法和DBSCAN。而且也设计出了Python numerical和scientific libraries Numpy and Scipy\n2、Keras（深度学习）\nhttps://github.com/fchollet/keras\nKeras是基于Theano的一个深度学习框架，它的设计参考了Torch，用Python语言编写，是一个高度模块化的神经网络库，支持GPU和CPU。\n3、Lasagne（深度学习）\n不只是一个美味的意大利菜，也是一个和Keras有着相似功能的深度学习库，但其在设计上与它们有些不同。\n4.Pylearn2\nwww.github.com/lisa-lab/pylearn2\nPylearn是一个让机器学习研究简单化的基于Theano的库程序。它把深度学习和人工智能研究许多常用的模型以及训练算法封装成一个单一的实验包，如随机梯度下降。\n5.NuPIC\nwww.github.com/numenta/nupic\nNuPIC是一个以HTM学习算法为工具的机器智能平台。HTM是皮层的精确计算方法。HTM的核心是基于时间的持续学习算法和储存和撤销的时空模式。NuPIC适合于各种各样的问题,尤其是检测异常和预测的流数据来源。\n6. Nilearn\nwww.github.com/nilearn/nilearn\nNilearn 是一个能够快速统计学习神经影像数据的Python模块。它利用Python语言中的scikit-learn 工具箱和一些进行预测建模，分类，解码，连通性分析的应用程序来进行多元的统计。\n7.PyBrain\nwww.github.com/pybrain/pybrain\nPybrain是基于Python语言强化学习，人工智能，神经网络库的简称。 它的目标是提供灵活、容易使用并且强大的机器学习算法和进行各种各样的预定义的环境中测试来比较你的算法。\n8.Pattern\nwww.github.com/clips/pattern\nPattern 是Python语言下的一个网络挖掘模块。它为数据挖掘，自然语言处理，网络分析和机器学习提供工具。它支持向量空间模型、聚类、支持向量机和感知机并且用KNN分类法进行分类。\n9.Fuel\nwww.github.com/mila-udem/fuel\nFuel为你的机器学习模型提供数据。他有一个共享如MNIST, CIFAR-10 (图片数据集), Google's One Billion Words (文字)这类数据集的接口。你使用他来通过很多种的方式来替代自己的数据。\n10.Bob\nwww.github.com/idiap/bob\nBob是一个免费的信号处理和机器学习的工具。它的工具箱是用Python和C++语言共同编写的，它的设计目的是变得更加高效并且减少开发时间，它是由处理图像工具,音频和视频处理、机器学习和模式识别的大量软件包构成的。\n11.Skdata\nwww.github.com/jaberg/skdata\nSkdata是机器学习和统计的数据集的库程序。这个模块对于玩具问题，流行的计算机视觉和自然语言的数据集提供标准的Python语言的使用。\n12.MILK\nwww.github.com/luispedro/milk\nMILK是Python语言下的机器学习工具包。它主要是在很多可得到的分类比如SVMS,K-NN,随机森林，决策树中使用监督分类法。 它还执行特征选择。 这些分类器在许多方面相结合,可以形成不同的例如无监督学习、密切关系金传播和由MILK支持的K-means聚类等分类系统。\n13.IEPY\nwww.github.com/machinalis/iepy\nIEPY是一个专注于关系抽取的开源性信息抽取工具。它主要针对的是需要对大型数据集进行信息提取的用户和想要尝试新的算法的科学家。\n14.Quepy\nwww.github.com/machinalis/quepy\nQuepy是通过改变自然语言问题从而在数据库查询语言中进行查询的一个Python框架。他可以简单的被定义为在自然语言和数据库查询中不同类型的问题。所以，你不用编码就可以建立你自己的一个用自然语言进入你的数据库的系统。\n现在Quepy提供对于Sparql和MQL查询语言的支持。并且计划将它延伸到其他的数据库查询语言。\n15.Hebel\nwww.github.com/hannes-brt/hebel\nHebel是在Python语言中对于神经网络的深度学习的一个库程序，它使用的是通过PyCUDA来进行GPU和CUDA的加速。它是最重要的神经网络模型的类型的工具而且能提供一些不同的活动函数的激活功能，例如动力，涅斯捷罗夫动力，信号丢失和停止法。\n16.mlxtend\nwww.github.com/rasbt/mlxtend\n它是一个由有用的工具和日常数据科学任务的扩展组成的一个库程序。\n17.nolearn\nwww.github.com/dnouri/nolearn\n这个程序包容纳了大量能对你完成机器学习任务有帮助的实用程序模块。其中大量的模块和scikit-learn一起工作，其它的通常更有用。\n18.Ramp\nwww.github.com/kvh/ramp\nRamp是一个在Python语言下制定机器学习中加快原型设计的解决方案的库程序。他是一个轻型的pandas-based机器学习中可插入的框架，它现存的Python语言下的机器学习和统计工具（比如scikit-learn,rpy2等）Ramp提供了一个简单的声明性语法探索功能从而能够快速有效地实施算法和转换。\n19.Feature Forge\nwww.github.com/machinalis/featureforge\n这一系列工具通过与scikit-learn兼容的API，来创建和测试机器学习功能。\n这个库程序提供了一组工具，它会让你在许多机器学习程序使用中很受用。当你使用scikit-learn这个工具时，你会感觉到受到了很大的帮助。（虽然这只能在你有不同的算法时起作用。）\n20.REP\nwww.github.com/yandex/rep\nREP是以一种和谐、可再生的方式为指挥数据移动驱动所提供的一种环境。\n它有一个统一的分类器包装来提供各种各样的操作，例如TMVA, Sklearn, XGBoost, uBoost等等。并且它可以在一个群体以平行的方式训练分类器。同时它也提供了一个交互式的情节。\n21.Python 学习机器样品\nwww.github.com/awslabs/machine-learning-samples\n用亚马逊的机器学习建造的简单软件收集。\n22.Python-ELM\nwww.github.com/dclambert/Python-ELM\n这是一个在Python语言下基于scikit-learn的极端学习机器的实现。\n23.gensim\n主题模型python实现\nScalable statistical semantics\nAnalyze plain-text documents for semantic structure\nRetrieve semantically similar documents"}
{"content2":"一、引言\n本材料参考Andrew Ng大神的机器学习课程 http://cs229.stanford.edu，以及斯坦福无监督学习UFLDL tutorial http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial\n机器学习中的回归问题属于有监督学习的范畴。回归问题的目标是给定D维输入变量x，并且每一个输入矢量x都有对应的值y，要求对于新来的数据预测它对应的连续的目标值t。比如下面这个例子：假设我们有一个包含47个房子的面积和价格的数据集如下：\n我们可以在Matlab中画出来这组数据集，如下：\n看到画出来的点，是不是有点像一条直线？我们可以用一条曲线去尽量拟合这些数据点，那么对于新来的输入，我么就可以将拟合的曲线上返回对应的点从而达到预测的目的。如果要预测的值是连续的比如上述的房价，那么就属于回归问题；如果要预测的值是离散的即一个个标签，那么就属于分类问题。这个学习处理过程如下图所示：\n上述学习过程中的常用术语：包含房子面积和价格的数据集称为训练集training set；输入变量x（本例中为面积）为特征features；输出的预测值y（本例中为房价）为目标值target；拟合的曲线，一般表示为y = h(x)，称为假设模型hypothesis；训练集的条目数称为特征的维数，本例为47。\n二、线性回归模型\n线性回归模型假设输入特征和对应的结果满足线性关系。在上述的数据集中加上一维--房间数量，于是数据集变为：\n于是，输入特征x是二维的矢量，比如x1(i)表示数据集中第i个房子的面积，x2(i)表示数据集中第i个房子的房间数量。于是可以假设输入特征x与房价y满足线性函数，比如：\n这里θi称为假设模型即映射输入特征x与结果y的线性函数h的参数parameters，为了简化表示，我们在输入特征中加入x0 = 1，于是得到：\n参数θ和输入特征x都为矢量，n是输入的特征x的个数（不包含x0）。\n现在，给定一个训练集，我们应该怎么学习参数θ，从而达到比较好的拟合效果呢？一个直观的想法是使得预测值h(x)尽可能接近y，为了达到这个目的，我们对于每一个参数θ，定义一个代价函数cost function用来描述h(x(i))'与对应的y(i)'的接近程度：\n前面乘上的1/2是为了求导的时候，使常数系数消失。于是我们的目标就变为了调整θ使得代价函数J(θ)取得最小值，方法有梯度下降法，最小二乘法等。\n2.1 梯度下降法\n现在我们要调整θ使得J(θ)取得最小值，为了达到这个目的，我们可以对θ取一个随机初始值（随机初始化的目的是使对称失效），然后不断地迭代改变θ的值来使J(θ)减小，知道最终收敛取得一个θ值使得J(θ)最小。梯度下降法就采用这样的思想：对θ设定一个随机初值θ0，然后迭代进行以下更新\n直到收敛。这里的α称为学习率learning rate。\n梯度方向由J(θ)对θ 的偏导数决定，由于要求的是最小值，因此对偏导数取负值得到梯度方向。将J(θ)代入得到总的更新公式\n这样的更新规则称为LMS update rule（least mean squares），也称为Widrow-Hoﬀ learning rule。\n对于如下更新参数的算法：\n由于在每一次迭代都考察训练集的所有样本，而称为批量梯度下降batch gradient descent。对于引言中的房价数据集，运行这种算法，可以得到θ0 = 71.27, θ1 = 1.1345，拟合曲线如下图：\n如果参数更新计算算法如下：\n这里我们按照单个训练样本更新θ的值，称为随机梯度下降stochastic gradient descent。比较这两种梯度下降算法，由于batch gradient descent在每一步都考虑全部数据集，因而复杂度比较高，随机梯度下降会比较快地收敛，而且在实际情况中两种梯度下降得到的最优解J(θ)一般会接近真实的最小值。所以对于较大的数据集，一般采用效率较高的随机梯度下降法。\n2.2 最小二乘法（LMS）\n梯度下降算法给出了一种计算θ的方法，但是需要迭代的过程，比较费时而且不太直观。下面介绍的最小二乘法是一种直观的直接利用矩阵运算可以得到θ值的算法。为了理解最小二乘法，首先回顾一下矩阵的有关运算：\n假设函数f是将m*n维矩阵映射为一个实数的运算，即，并且定义对于矩阵A，映射f(A)对A的梯度为：\n因此该梯度为m*n的矩阵。例如对于矩阵A=，而且映射函数f(A)定义为：F(A) = 1.5A11 + 5A122 + A21A22，于是梯度为：\n。\n另外，对于矩阵的迹的梯度运算，有如下规则：\n。\n下面，我们将测试集中的输入特征x和对应的结果y表示成矩阵或者向量的形式，有：\n，，\n对于预测模型有，即，于是可以很容易得到：\n，\n所以可以得到。\n于是，我们就将代价函数J(θ)表示为了矩阵的形式，就可以用上述提到的矩阵运算来得到梯度：\n，\n令上述梯度为0，得到等式：，于是得到θ的值：\n。这就是最小二乘法得到的假设模型中参数的值。\n2.3 加权线性回归\n首先考虑下图中的几种曲线拟合情况：\n最左边的图使用线性拟合，但是可以看到数据点并不完全在一条直线上，因而拟合的效果并不好。如果我们加入x2项，得到，如中间图所示，该二次曲线可以更好的拟合数据点。我们继续加入更高次项，可以得到最右边图所示的拟合曲线，可以完美地拟合数据点，最右边的图中曲线为5阶多项式，可是我们都很清醒地知道这个曲线过于完美了，对于新来的数据可能预测效果并不会那么好。对于最左边的曲线，我们称之为欠拟合--过小的特征集合使得模型过于简单不能很好地表达数据的结构，最右边的曲线我们称之为过拟合--过大的特征集合使得模型过于复杂。\n正如上述例子表明，在学习过程中，特征的选择对于最终学习到的模型的性能有很大影响，于是选择用哪个特征，每个特征的重要性如何就产生了加权的线性回归。在传统的线性回归中，学习过程如下：\n，\n而加权线性回归学习过程如下：\n。\n二者的区别就在于对不同的输入特征赋予了不同的非负值权重，权重越大，对于代价函数的影响越大。一般选取的权重计算公式为：\n，\n其中，x是要预测的特征，表示离x越近的样本权重越大，越远的影响越小。\n三、logistic回归与Softmax回归\n3.1 logistic回归\n下面介绍一下logistic回归，虽然名曰回归，但实际上logistic回归用于分类问题。logistic回归实质上还是线性回归模型，只是在回归的连续值结果上加了一层函数映射，将特征线性求和，然后使用g(z)作映射，将连续值映射到离散值0/1上（对于sigmoid函数为0/1两类，而对于双曲正弦tanh函数为1/-1两类）。采用假设模型为：\n，\n而sigmoid函数g(z)为：\n当z趋近于-∞，g(z)趋近于0，而z趋近于∞，g(z)趋近于1，从而达到分类的目的。这里的\n那么对于这样的logistic模型，怎么调整参数θ呢？我们假设\n，由于是两类问题，即，于是得到似然估计为：\n对似然估计取对数可以更容易地求解：。\n接下来是θ的似然估计最大化，可以考虑上述的梯度下降法，于是得到：\n得到类似的更新公式：。虽然这个更新规则类似于LMS得到的公式，但是这两种是不同算法，因为这里的hθ(x(i))是一个关于θTx(i)的非线性函数。\n3.2 Softmax回归\nlogistic回归是两类回归问题的算法，如果目标结果是多个离散值怎么办？Softmax回归模型就是解决这个问题的，Softmax回归模型是logistic模型在多分类问题上的推广。在Softmax回归中，类标签y可以去k个不同的值（k>2）。因此对于y(i)从属于{1,2,3···k}。\n对于给定的测试输入x，我们要利用假设模型针对每一个类别j估算概率值p(y = j|x)。于是假设函数hθ(x(i))形式为：\n其中θ1，θ2，θ3，···，θk属于模型的参数，等式右边的系数是对概率分布进行归一化，使得总概率之和为1。于是类似于logistic回归，推广得到新的代价函数为：\n可以看到Softmax代价函数与logistic代价函数形式上非常相似，只是Softmax函数将k个可能的类别进行了累加，在Softmax中将x分为类别j的概率为：\n于是对于Softmax的代价函数，利用梯度下降法使的J(θ)最小，梯度公式如下：\n表示J(θ)对第j个元素θj的偏导数，每一次迭代进行更新：。\n3.3 Softmax回归 vs logistic回归\n特别地，当Softmax回归中k = 2时，Softmax就退化为logistic回归。当k = 2时，Softmax回归的假设模型为：\n我们令ψ = θ1，并且两个参数都剪去θ1，得到：\n于是Softmax回归预测得到两个类别的概率形式与logistic回归一致。\n现在，如果有一个k类分类的任务，我们可以选择Softmax回归，也可以选择k个独立的logistic回归分类器，应该如何选择呢？\n这一选择取决于这k个类别是否互斥，例如，如果有四个类别的电影，分别为：好莱坞电影、港台电影、日韩电影、大陆电影，需要对每一个训练的电影样本打上一个标签，那么此时应选择k = 4的Softmax回归。然而，如果四个电影类别如下：动作、喜剧、爱情、欧美，这些类别并不是互斥的，于是这种情况下使用4个logistic回归分类器比较合理。\n四、一般线性回归模型\n首先定义一个通用的指数概率分布：\n考虑伯努利分布，有：\n再考虑高斯分布：\n一般线性模型满足：1. y|x;θ 满足指数分布族E(η)　　2. 给定特征x，预测结果为T(y) = E[y|x]　　3. 参数η = θTx 。\n对于第二部分的线性模型，我们假设结果y满足高斯分布Ν(μ,σ2)，于是期望μ = η，所以：\n很显然，从一般线性模型的角度得到了第二部分的假设模型。\n对于logistic模型，由于假设结果分为两类，很自然地想到伯努利分布，并且可以得到，于是 y|x;θ 满足B(Φ)，E[y|x;θ] = Φ，所以\n于是得到了与logistic假设模型的公式，这也解释了logistic回归为何使用这个函数。"}
{"content2":"从2016年年初，开始用python写一个简单的爬虫，帮我收集一些数据。\n6月份，开始学习Machine Learning的相关知识。\n9月开始学习Spark和Scala。\n现在想，整理一下思路。\n先感谢下我的好友王峰给我的一些建议。他在Spark和Scala上有一些经验，让我前进的速度加快了一些。\n学习算法\n作为一个程序猿，以前多次尝试看过一些机器学习方面的书，其过程可以说是步履阑珊，碰到的阻力很大。\n主要原因是，读这些机器学习的书，需要有一些数学方面的背景。\n问题就在这些数学背景上，这些背景不仅仅是数学技巧，也有一些共识。对于缺乏这些背景的我，即使一个简单的公式，也有时会感到困惑。\n如果你像我一样是一个程序猿，我建议读Peter Harrington写的Machine Learning in Action （中文书名是《机器学习实战》）。\n这本书是以开发者的知识背景来写的，并且提供的python代码可以下载，方便开发人员理解。\n我写了一些博文，主要作用是帮助我理解学习的算法。大部分写的不好，后来我自己都看不懂。以后慢慢修正一下。\n机器学习实战 - 读书笔记(03) - 决策树\n机器学习实战 - 读书笔记(04) - 朴素贝叶斯\n机器学习实战 - 读书笔记(05) - Logistic回归\n机器学习实战 - 读书笔记(06) – SVM支持向量机\n机器学习实战 - 读书笔记(07) - 利用AdaBoost元算法提高分类性能\n机器学习实战 - 读书笔记(08) - 预测数值型数据：回归\n机器学习实战 - 读书笔记(10) - 利用Ｋ-均值聚类算法对未标注数据分组\n机器学习实战 - 读书笔记(11) - 使用Apriori算法进行关联分析\n机器学习实战 - 读书笔记(12) - 使用FP-growth算法来高效发现频繁项集\n机器学习实战 - 读书笔记(13) - 利用PCA来简化数据\n机器学习实战 - 读书笔记(14) - 利用SVD简化数据\n学习算法的Level\nLevel 1： 了解如何使用算法\nLevel 2： 了解算法的正确使用场景\n正确的使用一个算法，需要经验和对算法理论的了解。\n我以前有些这方面的经验，很多错误在于不正确地使用了算法。\n当我们编程序给别人用时：\n需要理解算法\n最低要求，也要有一些基本的统计知识。\n需要实现算法\n实现算法一般比较简单，需要注意性能和精度。\n基本上这部分在实现好后，比较稳定。\n需要实现将用户数据应用到算法上的过程。\n这是程序员主要干的工作，接口、性能上的考虑很多。\n需要理解用户的使用场景。\n这部分价值很大。\n一方面，写单元测试是不可避免的，理解用户的场景才能写出有效的单元测试程序。\n另外，会有很多处理客户问题的工作，也是长经验的机会。\nLevel 3： 了解算法的后面的数学理论\n有人觉得这个用处不大。我觉得了解数学理论，可以：\n成为真正的行家\n未来的路还很远，怎么能戛然而止!\n使用算法来帮助自己的一些事情，或者实现一个新的算法。\n现在人工智能的潜力很大，可以自己好好玩玩。\n学习python\n在数据量不大的情况下（几个G），单机上就可以很好跑机器学习的程序。\n这时，Python的用途就很大，不仅有已经实现好的算法，也可以实现爬虫，从网上获取数据。\n学习Scala和函数式编程\n对于大数据处理来说，Spark和Scala结合是现在的大趋势。\n我写的博文有：\n学习Scala： 初学者应该了解的知识\n函数式编程 : 一个程序猿进化的故事\nScala underscore的用途\n不变(Invariant), 协变(Covarinat), 逆变(Contravariant) : 一个程序猿进化的故事\nScala Collection简介\nScala on Visual Studio Code\n学习Spark架构\n我写的博文有：\nSpark集群 + Akka + Kafka + Scala 开发(1) : 配置开发环境\nSpark集群 + Akka + Kafka + Scala 开发(2) : 开发一个Spark应用\nSpark集群 + Akka + Kafka + Scala 开发(3) : 开发一个Akka + Spark的应用\nSpark集群 + Akka + Kafka + Scala 开发(4) : 开发一个Kafka + Spark的应用\n学习在Spark上的机器学习项目开发经验\n学习更多的算法\n蒙特卡洛树算法\n成为Spark的Contributer\n成为Spark的Contributer是件很cool的事。\n可以读读Spark的代码，从中应该可以增长不少。\n然后，尝试修一些Spark的Bugs。\n深度学习\n路还很长。"}
{"content2":"注：在吴恩达老师讲的【机器学习】课程中，最开始介绍神经网络的应用时就介绍了含有一个隐藏层的神经网络可以解决异或问题，而这是单层神经网络（也叫感知机）做不到了，当时就觉得非常神奇，之后就一直打算自己实现一下，一直到一周前才开始动手实现。自己参考【机器学习】课程中数字识别的作业题写了代码，对于作业题中给的数字图片可以达到95%左右的识别准确度。但是改成训练异或的网络时，怎么也无法得到正确的结果。后来查了一些资料才发现是因为自己有一个参数设置的有问题，而且学习率过小，迭代的次数也不够。总之，异或逻辑的实现不仅对于人工神经网络这一算法是一大突破，对于我自己对误差反向传播算法（Error Back Propagation, BP）的理解也是非常重要的过程，因此记录于此。\n什么是异或\n在数字逻辑中，异或是对两个运算元的一种逻辑分析类型，符号为XOR或EOR或⊕。与一般的或（OR）不同，当两两数值相同时为否，而数值不同时为真。异或的真值表如下：\nXOR truth table\nInput\nOutput\nA\nB\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n0, false\n1, true\n据说在人工神经网络（artificial neural network, ANN）发展初期，由于无法实现对多层神经网络（包括异或逻辑）的训练而造成了一场ANN危机，到最后BP算法的出现，才让训练带有隐藏层的多层神经网络成为可能。因此异或的实现在ANN的发展史是也是具有里程碑意义的。异或之所以重要，是因为它相对于其他逻辑关系，例如与（AND）, 或（OR）等，异或是线性不可分的。如下图：\n在实际应用中，异或门（Exclusive-OR gate, XOR gate）是数字逻辑中实现逻辑异或的逻辑门，这一函数能实现模为2的加法。因此，异或门可以实现计算机中的二进制加法。\n异或的神经网络结构\n在【机器学习】课程中，使用了AND（与），NOR（或非）和OR（或）的组合实现了XNOR（同或），与我们要实现的异或（XOR）正好相反。因此还是可以采用课程中的神经网络结构，如下图：\n如果算上输入层我们的网络共有三层，如下图所示，其中第1层和第2层中的1分别是这两层的偏置单元。连线上是连接前后层的参数。\n输入：我们一共有四个训练样本，每个样本有两个特征，分别是(0, 0), (1, 0), (0, 1), (1, 1);\n理想输出：参考上面的真值表，样本中两个特征相同时为0，相异为1\n参数：随机初始化，范围为(-1, 1)\n关于神经网络的基础知识以及前向传播、反向传播的实现请参考下面两篇文章，写的非常精彩：\n机器学习公开课笔记(4)：神经网络(Neural Network)——表示\n机器学习公开课笔记(5)：神经网络(Neural Network)——学习\n代码\n原生态的代码：\n下面的实现是完全根据自己的理解和对【机器学习】课程中作业题的模仿而写成的，虽然代码质量不是非常高，但是算法的所有细节都展示出来了。\n在66, 69, 70行的注释是我之前没有得到正确结果的三个原因，其中epsilon确定的是随机初始化参数的范围，例如epsilon=1，参数范围就是(-1, 1)\n1 # -*- coding: utf-8 -*- 2 \"\"\" 3 Created on Tue Apr 4 10:47:51 2017 4 5 @author: xin 6 \"\"\" 7 # Neural Network for XOR 8 import numpy as np 9 import matplotlib.pyplot as plt 10 11 HIDDEN_LAYER_SIZE = 2 12 INPUT_LAYER = 2 # input feature 13 NUM_LABELS = 1 # output class number 14 X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) 15 y = np.array([[0], [1], [1], [0]]) 16 17 18 def rand_initialize_weights(L_in, L_out, epsilon): 19 \"\"\" 20 Randomly initialize the weights of a layer with L_in 21 incoming connections and L_out outgoing connections; 22 23 Note that W should be set to a matrix of size(L_out, 1 + L_in) as 24 the first column of W handles the \"bias\" terms 25 \"\"\" 26 epsilon_init = epsilon 27 W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init 28 return W 29 30 31 def sigmoid(x): 32 return 1.0 / (1.0 + np.exp(-x)) 33 34 35 def sigmoid_gradient(z): 36 g = np.multiply(sigmoid(z), (1 - sigmoid(z))) 37 return g 38 39 40 def nn_cost_function(theta1, theta2, X, y): 41 m = X.shape[0] # m=4 42 # 计算所有参数的偏导数（梯度） 43 D_1 = np.zeros(theta1.shape) # Δ_1 44 D_2 = np.zeros(theta2.shape) # Δ_2 45 h_total = np.zeros((m, 1)) # 所有样本的预测值, m*1, probability 46 for t in range(m): 47 a_1 = np.vstack((np.array([[1]]), X[t:t + 1, :].T)) # 列向量, 3*1 48 z_2 = np.dot(theta1, a_1) # 2*1 49 a_2 = np.vstack((np.array([[1]]), sigmoid(z_2))) # 3*1 50 z_3 = np.dot(theta2, a_2) # 1*1 51 a_3 = sigmoid(z_3) 52 h = a_3 # 预测值h就等于a_3, 1*1 53 h_total[t,0] = h 54 delta_3 = h - y[t:t + 1, :].T # 最后一层每一个单元的误差, δ_3, 1*1 55 delta_2 = np.multiply(np.dot(theta2[:, 1:].T, delta_3), sigmoid_gradient(z_2)) # 第二层每一个单元的误差（不包括偏置单元）, δ_2, 2*1 56 D_2 = D_2 + np.dot(delta_3, a_2.T) # 第二层所有参数的误差, 1*3 57 D_1 = D_1 + np.dot(delta_2, a_1.T) # 第一层所有参数的误差, 2*3 58 theta1_grad = (1.0 / m) * D_1 # 第一层参数的偏导数，取所有样本中参数的均值，没有加正则项 59 theta2_grad = (1.0 / m) * D_2 60 J = (1.0 / m) * np.sum(-y * np.log(h_total) - (np.array([[1]]) - y) * np.log(1 - h_total)) 61 return {'theta1_grad': theta1_grad, 62 'theta2_grad': theta2_grad, 63 'J': J, 'h': h_total} 64 65 66 theta1 = rand_initialize_weights(INPUT_LAYER, HIDDEN_LAYER_SIZE, epsilon=1) # 之前的问题之一，epsilon的值设置的太小 67 theta2 = rand_initialize_weights(HIDDEN_LAYER_SIZE, NUM_LABELS, epsilon=1) 68 69 iter_times = 10000 # 之前的问题之二，迭代次数太少 70 alpha = 0.5 # 之前的问题之三，学习率太小 71 result = {'J': [], 'h': []} 72 theta_s = {} 73 for i in range(iter_times): 74 cost_fun_result = nn_cost_function(theta1=theta1, theta2=theta2, X=X, y=y) 75 theta1_g = cost_fun_result.get('theta1_grad') 76 theta2_g = cost_fun_result.get('theta2_grad') 77 J = cost_fun_result.get('J') 78 h_current = cost_fun_result.get('h') 79 theta1 -= alpha * theta1_g 80 theta2 -= alpha * theta2_g 81 result['J'].append(J) 82 result['h'].append(h_current) 83 # print(i, J, h_current) 84 if i==0 or i==(iter_times-1): 85 print('theta1', theta1) 86 print('theta2', theta2) 87 theta_s['theta1_'+str(i)] = theta1.copy() 88 theta_s['theta2_'+str(i)] = theta2.copy() 89 90 plt.plot(result.get('J')) 91 plt.show() 92 print(theta_s) 93 print(result.get('h')[0], result.get('h')[-1])\n下面是输出结果：\n# 随机初始化得到的参数\n('theta1', array([[ 0.18589823, -0.77059558, 0.62571502], [-0.79844165, 0.56069914, 0.21090703]])) ('theta2', array([[ 0.1327994 , 0.59513332, 0.34334931]]))\n# 训练后得到的参数 ('theta1', array([[-3.90903729, -7.44497437, 7.20130773], [-3.76429211, 6.93482723, -7.21857912]])) ('theta2', array([[ -6.5739346 , 13.33011993, 13.3891608 ]]))\n# 同上，第一次迭代和最后一次迭代得到的参数 {'theta1_0': array([[ 0.18589823, -0.77059558, 0.62571502], [-0.79844165, 0.56069914, 0.21090703]]), 'theta2_9999': array([[ -6.5739346 , 13.33011993, 13.3891608 ]]), 'theta1_9999': array([[-3.90903729, -7.44497437, 7.20130773], [-3.76429211, 6.93482723, -7.21857912]]), 'theta2_0': array([[ 0.1327994 , 0.59513332, 0.34334931]])}\n# 预测值h: 第1个array里是初始参数预测出来的值，第2个array中是最后一次得到的参数预测出来的值\n(array([[ 0.66576877], [ 0.69036552], [ 0.64994307], [ 0.67666546]]), array([[ 0.00245224], [ 0.99812746], [ 0.99812229], [ 0.00215507]]))\n下面是随着迭代次数的增加，代价函数值J(θ)的变化情况：\n更加精炼的代码\n下面这段代码是我在排除之前自己的代码中的问题时，在Stack Overflow上发现的，发帖的人也碰到了同样的问题，但原因不一样。他的代码里有一点小问题，已经修正。这段代码，相对于我自己的原生态代码，有了非常大的改进，没有限定层数和每层的单元数，代码本身也比较简洁。\n说明：由于第44行，传的参数是该层的a值，而不是z值，所以第11行需要做出一点修改，其实直接传递a值是一种更方便的做法。\n1 # -*- coding: utf-8 -*- 2 3 import numpy as np 4 import matplotlib.pyplot as plt 5 6 7 def sigmoid(x): 8 return 1/(1+np.exp(-x)) 9 10 def s_prime(z): 11 return np.multiply(z, 1.0-z) # 修改的地方 12 13 def init_weights(layers, epsilon): 14 weights = [] 15 for i in range(len(layers)-1): 16 w = np.random.rand(layers[i+1], layers[i]+1) 17 w = w * 2*epsilon - epsilon 18 weights.append(np.mat(w)) 19 return weights 20 21 def fit(X, Y, w): 22 # now each para has a grad equals to 0 23 w_grad = ([np.mat(np.zeros(np.shape(w[i]))) 24 for i in range(len(w))]) # len(w) equals the layer number 25 m, n = X.shape 26 h_total = np.zeros((m, 1)) # 所有样本的预测值, m*1, probability 27 for i in range(m): 28 x = X[i] 29 y = Y[0,i] 30 # forward propagate 31 a = x 32 a_s = [] 33 for j in range(len(w)): 34 a = np.mat(np.append(1, a)).T 35 a_s.append(a) # 这里保存了前L-1层的a值 36 z = w[j] * a 37 a = sigmoid(z) 38 h_total[i, 0] = a 39 # back propagate 40 delta = a - y.T 41 w_grad[-1] += delta * a_s[-1].T # L-1层的梯度 42 # 倒过来，从倒数第二层开始到第二层结束，不包括第一层和最后一层 43 for j in reversed(range(1, len(w))): 44 delta = np.multiply(w[j].T*delta, s_prime(a_s[j])) # 这里传递的参数是a，而不是z 45 w_grad[j-1] += (delta[1:] * a_s[j-1].T) 46 w_grad = [w_grad[i]/m for i in range(len(w))] 47 J = (1.0 / m) * np.sum(-Y * np.log(h_total) - (np.array([[1]]) - Y) * np.log(1 - h_total)) 48 return {'w_grad': w_grad, 'J': J, 'h': h_total} 49 50 51 X = np.mat([[0,0], 52 [0,1], 53 [1,0], 54 [1,1]]) 55 Y = np.mat([0,1,1,0]) 56 layers = [2,2,1] 57 epochs = 5000 58 alpha = 0.5 59 w = init_weights(layers, 1) 60 result = {'J': [], 'h': []} 61 w_s = {} 62 for i in range(epochs): 63 fit_result = fit(X, Y, w) 64 w_grad = fit_result.get('w_grad') 65 J = fit_result.get('J') 66 h_current = fit_result.get('h') 67 result['J'].append(J) 68 result['h'].append(h_current) 69 for j in range(len(w)): 70 w[j] -= alpha * w_grad[j] 71 if i == 0 or i == (epochs - 1): 72 # print('w_grad', w_grad) 73 w_s['w_' + str(i)] = w_grad[:] 74 75 76 plt.plot(result.get('J')) 77 plt.show() 78 print(w_s) 79 print(result.get('h')[0], result.get('h')[-1])\n下面是输出的结果：\n# 第一次迭代和最后一次迭代得到的参数\n{'w_4999': [matrix([[ 1.51654104e-04, -2.30291680e-04, 6.20083292e-04], [ 9.15463982e-05, -1.51402782e-04, -6.12464354e-04]]), matrix([[ 0.0004279 , -0.00051928, -0.00042735]])],\n'w_0': [matrix([[ 0.00172196, 0.0010952 , 0.00132499], [-0.00489422, -0.00489643, -0.00571827]]), matrix([[-0.02787502, -0.01265985, -0.02327431]])]}\n# 预测值h: 第1个array里是初始参数预测出来的值，第2个array中是最后一次得到的参数预测出来的值\n(array([[ 0.45311095], [ 0.45519066], [ 0.4921871 ], [ 0.48801121]]),\narray([[ 0.00447994], [ 0.49899856], [ 0.99677373], [ 0.50145936]]))\n观察上面的结果，最后一次迭代得到的结果并不是我们期待的结果，也就是第1、4个值接近于0, 第2、3个值接近于1。下面是代价函数值J(θ)随着迭代次数增加的变化情况：\n从上图可以看到，J(θ)的值从2000以后就一直停留在0.35左右，因此整个网络有可能收敛到了一个局部最优解，也有可能是迭代次数不够导致的。\n将迭代次数改成10000后， 即epochs = 10000，基本上都是可以得到预期的结果的。其实在迭代次数少的情况下，也有可能得到预期的结果，这应该主要取决于初始的参数。\n经验小结\n通过阅读别人的代码确实是提高自己编程能力的一种重要方法。例如通过比较自己的原生态版代码和其他人写的代码，就可以找出自己的不足之处。其中最大的收获是：数据结构对于代码的结构和逻辑都非常重要。比如我自己写的时候，每一层是分开的，但后面的代码中将整个网络一起初始化并保存在一个list中，这就提高了代码的可扩展能力，也使得代码更加简洁！\n此外，要准确的理解各种算法的细节，最好的方式就是自己实现一次。\n<完>\n参考文献\nhttps://zh.wikipedia.org/wiki/%E9%80%BB%E8%BE%91%E5%BC%82%E6%88%96\nhttps://zh.wikipedia.org/wiki/%E5%BC%82%E6%88%96%E9%97%A8\nhttps://muxuezi.github.io/posts/10-from-the-perceptron-to-artificial-neural-networks.html\nhttp://stackoverflow.com/q/36369335/2803344\nCoursera, Andrew Ng 公开课第四周，第五周"}
{"content2":"过拟合\n在进行数据挖掘或者机器学习模型建立的时候，因为在统计学习中，假设数据满足独立同分布，即当前已产生的数据可以对未来的数据进行推测与模拟，因此都是使用历史数据建立模型，即使用已经产生的数据去训练，然后使用该模型去拟合未来的数据。但是一般独立同分布的假设往往不成立，即数据的分布可能会发生变化（distribution drift），并且可能当前的数据量过少，不足以对整个数据集进行分布估计，因此往往需要防止模型过拟合，提高模型泛化能力。而为了达到该目的的最常见方法便是：正则化，即在对模型的目标函数（objective function）或代价函数（cost function）加上正则项。\n在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候，或者在对模型进行过度训练（overtraining）时，常常会导致模型的过拟合（overfitting）。如下图所示：\n通过上图可以看出，随着模型训练的进行，模型的复杂度会增加，此时模型在训练数据集上的训练误差会逐渐减小，但是在模型的复杂度达到一定程度时，模型在验证集上的误差反而随着模型的复杂度增加而增大。此时便发生了过拟合，即模型的复杂度升高，但是该模型在除训练集之外的数据集上却不work。\n方法\n提前终止（当验证集上的效果变差的时候）\n正则化（Regularization）\nL1正则化\nL2正则化\n数据集扩增（Data augmentation）\nDropout\n1、提前终止\n对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。\nEarly stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30……\n2、数据集扩增\n在数据挖掘领域流行着这样的一句话，“有时候往往拥有更多的数据胜过一个好的模型”。因为我们在使用训练数据训练模型，通过这个模型对将来的数据进行拟合，而在这之间又一个假设便是，训练数据与将来的数据是独立同分布的。即使用当前的训练数据来对将来的数据进行估计与模拟，而更多的数据往往估计与模拟地更准确。因此，更多的数据有时候更优秀。但是往往条件有限，如人力物力财力的不足，而不能收集到更多的数据，如在进行分类的任务中，需要对数据进行打标，并且很多情况下都是人工得进行打标，因此一旦需要打标的数据量过多，就会导致效率低下以及可能出错的情况。所以，往往在这时候，需要采取一些计算的方式与策略在已有的数据集上进行手脚，以得到更多的数据。\n通俗得讲，数据机扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。一般有以下方法：\n从数据源头采集更多数据\n复制原有数据并加上随机噪声\n重采样\n根据当前数据集估计数据分布参数，使用该分布产生更多数据等\n如图像处理：\n图像平移。这种方法可以使得网络学习到平移不变的特征。\n图像旋转。学习旋转不变的特征。有些任务里，目标可能有多种不同的姿态，旋转正好可以弥补样本中姿态较少的问题。\n图像镜像。和旋转的功能类似。\n图像亮度变化。甚至可以用直方图均衡化。\n裁剪。\n缩放。\n图像模糊。用不同的模板卷积产生模糊图像。\n3、正则化\n正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等。\n3.1、L1正则\n在原始的代价函数后面加上一个L1正则化项，即所有权重w的绝对值的和，乘以λ/n（这里不像L2正则化项那样，需要再乘以1/2。）\n同样先计算导数：\n上式中sgn(w)表示w的符号。那么权重w的更新规则为：\n比原始的更新规则多出了η * λ * sgn(w)/n这一项。当w为正时，更新后的w变小。当w为负时，更新后的w变大——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。\n另外，上面没有提到一个问题，当w为0时怎么办？当w等于0时，|W|是不可导的，所以我们只能按照原始的未经正则化的方法去更新w，这就相当于去掉η*λ*sgn(w)/n这一项，所以我们可以规定sgn(0)=0，这样就把w=0的情况也统一进来了。（在编程的时候，令sgn(0)=0,sgn(w>0)=1,sgn(w<0)=-1）\n3.2、L2正则化\nL2正则化就是在代价函数后面再加上一个正则化项：\nC0代表原始的代价函数，后面那一项就是L2正则化项，它是这样来的：所有参数w的平方的和，除以训练集的样本大小n。λ就是正则项系数，权衡正则项与C0项的比重。另外还有一个系数1/2，1/2经常会看到，主要是为了后面求导的结果方便，后面那一项求导会产生一个2，与1/2相乘刚好凑整。\nL2正则化项是怎么避免overfitting的呢？我们推导一下看看，先求导：\n可以发现L2正则化项对b的更新没有影响，但是对于w的更新有影响:\n在不使用L2正则化时，求导结果中w前系数为1，现在w前面系数为 1−ηλ/n ，因为η、λ、n都是正的，所以 1−ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来。当然考虑到后面的导数项，w最终的值可能增大也可能减小。\n另外，需要提一下，对于基于mini-batch的随机梯度下降，w和b更新的公式跟上面给出的有点不同：\n对比上面w的更新公式，可以发现后面那一项变了，变成所有导数加和，乘以η再除以m，m是一个mini-batch中样本的个数。\n到目前为止，我们只是解释了L2正则化项有让w“变小”的效果，但是还没解释为什么w“变小”可以防止overfitting？一个所谓“显而易见”的解释就是：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。当然，对于很多人（包括我）来说，这个解释似乎不那么显而易见，所以这里添加一个稍微数学一点的解释（引自知乎）：\n过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。\n而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。\n4、Dropout\nL1、L2正则化是通过修改代价函数来实现的，而Dropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧（trike）。它的流程如下：\n假设我们要训练上图这个网络，在训练开始时，我们随机地“删除”一半的隐层单元，视它们为不存在，得到如下的网络：\n保持输入输出层不变，按照BP算法更新上图神经网络中的权值（虚线连接的单元不更新，因为它们被“临时删除”了）。\n以上就是一次迭代的过程，在第二次迭代中，也用同样的方法，只不过这次删除的那一半隐层单元，跟上一次删除掉的肯定是不一样的，因为我们每一次迭代都是“随机”地去删掉一半。第三次、第四次……都是这样，直至训练结束。\n以上就是Dropout，它为什么有助于防止过拟合呢？可以简单地这样解释，运用了dropout的训练过程，相当于训练了很多个只有半数隐层单元的神经网络（后面简称为“半数网络”），每一个这样的半数网络，都可以给出一个分类结果，这些结果有的是正确的，有的是错误的。随着训练的进行，大部分半数网络都可以给出正确的分类结果，那么少数的错误分类结果就不会对最终结果造成大的影响。\n更加深入地理解，可以看看Hinton和Alex两牛2012的论文《ImageNet Classification with Deep Convolutional Neural Networks》\n参考：\nhttp://blog.csdn.net/heyongluoyao8/article/details/49429629\nhttp://blog.csdn.net/u012162613/article/details/44261657\nhttp://blog.csdn.net/thesby/article/details/53164257"}
{"content2":"一、前言\n打算面向想从事人工智能产品经理职位的人，写一个系列的专题，对人工智能产品经理做一个全面的介绍，初步计划写21个专题，每天一篇，算是对自己的一种鞭策，每天的任务定性，定量，希望自己能够坚持下来。\n适应人群：1.想要转型做人工智能的传统产品经理；2.RD想要转型做AIPM的人群；3.一切想从事或了解人工智能产品经理工作的人；\n屏蔽人群：希望通过本课程学习编码能力的人。\n二、正文\n2.1 章节目标\n了解是什么是人工智能？\n了解人工智能核心概念？\n了解人工智能发展简史？\n了解人工智能当前的市场格局？\n2.2 内容\n2.2.1 人工智能的概念定义\n人工智能的概念定义\n2.2.2 人工智能、机器学习和深度学习的关系\n人工智能相关概念的关系\n2.2.3 人工智能的主要特征：自动化+智能化\n人工智能两大主要特征\n2.2.4 人工智能的行业格局\n中国人工智能市场生态图谱\n2.2.5 人工智能发展简史\n人工智能发展简史\n三、未完待续\n下期预告：人工智能产品经理能力的概念定义和能力模型"}
{"content2":"机器学习及其基础概念简介\n作者：白宁超\n2016年12月23日21:24:51\n摘要：随着机器学习和深度学习的热潮，各种图书层出不穷。然而多数是基础理论知识介绍，缺乏实现的深入理解。本系列文章是作者结合视频学习和书籍基础的笔记所得。本系列文章将采用理论结合实践方式编写。首先介绍机器学习和深度学习的范畴，然后介绍关于训练集、测试集等介绍。接着分别介绍机器学习常用算法，分别是监督学习之分类（决策树、临近取样、支持向量机、神经网络算法）监督学习之回归（线性回归、非线性回归）非监督学习（K-means聚类、Hierarchical聚类）。本文采用各个算法理论知识介绍，然后结合python具体实现源码和案例分析的方式（本文原创编著，转载注明出处:机器学习及其基础概念简介(2)）\n目录\n【Machine Learning】Python开发工具：Anaconda+Sublime(1)\n【Machine Learning】机器学习及其基础概念简介(2)\n【Machine Learning】决策树在商品购买力能力预测案例中的算法实现(3)\n【Machine Learning】KNN算法虹膜图片识别实战(4)\n1 机器学习简介\n机器学习 （Machine Learning, ML) ：\n概念：多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n学科定位：人工智能(Artificial Intelligence, AI）的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n定义：探究和开发一系列算法来如何使计算机不需要通过外部明显的指示，而可以自己通过数据来学习，建模，并且利用建好的模型和新的输入来进行预测的学科。\n发展：\nArthur Samuel (1959): 一门不需要通过外部程序指示而让计算机有能力自我学习的学科\nLangley（1996) ： “机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”\nTom Michell (1997):  “机器学习是对能通过经验自动改进的计算机算法的研究”\n学习：针对经验E (experience) 和一系列的任务 T (tasks) 和一定表现的衡量 P，如果随之经验E的积累，针对定义好的任务T可以提高表现P，就说计算机具有学习能力\n例子： 人脸识别、无人驾驶汽车、下棋、语音识别、电商推荐系统等\n应用：语音识别、自动驾驶、语言翻译、计算机视觉、推荐系统、无人机、识别垃圾邮件\n机器学习就业需求：LinkedIn所有职业技能需求量第一：机器学习，数据挖掘和统计分析人才 http://blog.linkedin.com/2014/12/17/the-25-hottest-skills-that-got-people-hired-in-2014/\n2 深度学习(Deep Learning)\n深度学习(Deep Learning)：\n深度学习是基于机器学习延伸出来的一个新的领域，由以人大脑结构为启发的神经网络算法为起源加之模型结构深度的增加发展，并伴随大数据和计算能力的提高而产生的一系列新的算法。\n深度学习发展：\n其概念由著名科学家Geoffrey Hinton等人在2006年和2007年在《Sciences》等上发表的文章被提出和兴起。\n学习能用来干什么？为什么近年来引起如此广泛的关注？\n深度学习，作为机器学习中延伸出来的一个领域，被应用在图像处理与计算机视觉，自然语言处理以及语音识别等领域。自2006年至今，学术界和工业界合作在深度学习方面的研究与应用在以上领域取得了突破性的进展。以ImageNet为数据库的经典图像中的物体识别竞赛为例，击了所有传统算法，取得了前所未有的精确度。\n深度学习目前有哪些代表性的学术机构和公司走在前沿？人才需要如何？\n学校以多伦多大学，纽约大学，斯坦福大学为代表，工业界以Google, Facebook, 和百度为代表走在深度学习研究与应用的前沿。Google挖走了Hinton，Facebook挖走了LeCun，百度硅谷的实验室挖走了Andrew Ng，Google去年4月份以超过5亿美金收购了专门研究深度学习的初创公司DeepMind, 深度学习方因技术的发展与人才的稀有造成的人才抢夺战达到了前所未有激烈的程度。诸多的大大小小(如阿里巴巴，雅虎）等公司也都在跟进，开始涉足深度学习领域，深度学习人才需求量会持续快速增长。\n深度学习如今和未来将对我们生活造成怎样的影响？\n目前我们使用的Android手机中google的语音识别，百度识图，google的图片搜索，都已经使用到了深度学习技术。Facebook在去年名为DeepFace的项目中对人脸识别的准备率第一次接近人类肉眼（97.25% vs 97.5%)。大数据时代，结合深度学习的发展在未来对我们生活的影响无法估量。保守而言，很多目前人类从事的活动都将因为深度学习和相关技术的发展被机器取代，如自动汽车驾驶，无人飞机，以及更加职能的机器人等。深度学习的发展让我们第一次看到并接近人工智能的终极目标。\n深度学习的应用展示：\n无人驾驶汽车中的路标识别\nGoogle Now中的语音识别\n百度识图\n针对图片，自动生成文字的描述\n图片文字识别结果：“A person riding a motorcycle on a dirt road,”\n3 机器学习相关概念介绍\n基本概念：训练集，测试集，特征值，监督学习，非监督学习，半监督学习，分类，回归\n概念学习：人类学习概念：鸟，车，计算机\n定义：概念学习是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数\n例子：学习 “享受运动\" 这一概念：\n小明进行水上运动，是否享受运动取决于很多因素\n样例\n天气\n温度\n湿度\n风力\n水温\n预报\n享受运动\n1\n晴\n暖\n普通\n强\n暖\n一样\n是\n2\n晴\n暖\n大\n强\n暖\n一样\n是\n3\n雨\n冷\n大\n强\n暖\n变化\n否\n4\n晴\n暖\n大\n强\n冷\n变化\n是\n天气：晴，阴，雨\n温度：暖，冷\n湿度：普通，大\n风力：强，弱\n水温：暖，冷\n预报：一样，变化\n享受运动：是，否\n概念定义在实例(instance)集合之上，这个集合表示为X。（X：所有可能的日子，每个日子的值由 天气，温度，湿度，风力，水温，预 报6个属性表示。待学习的概念或目标函数成为目标概念（target concept), 记做c。c(x) = 1, 当享受运动时， c(x) = 0 当不享受运动时，c(x)也可叫做y\nx: 每一个实例\nX: 样例, 所有实例的集合\n学习目标：f: X -> Y\n训练集(training set/data)/训练样例（training examples): 用来进行训练，也就是产生模型或者算法的数据集\n测试集(testing set/data)/测试样例 (testing examples)：用来专门进行测试已经学习好的模型或者算法的数据集\n特征向量(features/feature vector)：属性的集合，通常用一个向量来表示，附属于一个实例\n标记(label): c(x), 实例类别的标记\n正例(positive example)\n反例(negative example)\n例子：研究美国硅谷房价\n影响房价的两个重要因素：面积(平方米），学区（评分1-10）\n样例\n面积（平方米）\n学区 （11.2 深度学习(Deep Learning)介绍-10）\n房价 （1000$)\n1\n100\n8\n1000\n2\n120\n9\n1300\n3\n60\n6\n800\n4\n80\n9\n1100\n5\n95\n5\n850\n分类 (classification): 目标标记为类别型数据(category)\n回归(regression): 目标标记为连续性数值 (continuous numeric value)\n例子：研究肿瘤良性，恶性于尺寸，颜色的关系\n特征值：肿瘤尺寸，颜色\n标记：良性/恶性\n有监督学习(supervised learning)： 训练集有类别标记(class label)\n无监督学习(unsupervised learning)： 无类别标记(class label)\n半监督学习（semi-supervised learning)：有类别标记的训练集 + 无标记的训练集\n4 机器学习步骤框架\n把数据拆分为训练集和测试集\n用训练集和训练集的特征向量来训练算法\n用学习来的算法运用在测试集上来评估算法 （可能要设计到调整参数（parameter tuning), 用验证集（validation set）\n例如：\n100 天： 训练集\n10天：测试集 （不知道是否 ” 享受运动“， 知道6个属性，来预测每一天是否享受运动）"}
{"content2":"什么是多模态机器学习？\n首先，什么叫做模态（Modality）呢？\n每一种信息的来源或者形式，都可以称为一种模态。例如，人有触觉，听觉，视觉，嗅觉；信息的媒介，有语音、视频、文字等；多种多样的传感器，如雷达、红外、加速度计等。以上的每一种都可以称为一种模态。\n同时，模态也可以有非常广泛的定义，比如我们可以把两种不同的语言当做是两种模态，甚至在两种不同情况下采集到的数据集，亦可认为是两种模态。\n因此，多模态机器学习，英文全称 MultiModal Machine Learning (MMML)，旨在通过机器学习的方法实现处理和理解多源模态信息的能力。目前比较热门的研究方向是图像、视频、音频、语义之间的多模态学习。\n多模态学习从1970年代起步，经历了几个发展阶段，在2010后全面步入Deep Learning阶段。\n人其实是一个多模态学习的总和，所以也有”砖家“说了，多模态学习才是真正的人工智能发展方向。\n本文将针对多模态学习在深度学习发面的研究方向和应用做相关介绍，主要参考了来自ACL 2017的《Tutorial on Multimodal Machine Learning》。\n多模态学习的分类\n多模态学习可以划分为以下五个研究方向：\n多模态表示学习 Multimodal Representation\n模态转化 Translation\n对齐 Alignment\n多模态融合 Multimodal Fusion\n协同学习 Co-learning\n下面将针对这五大研究方向，逐一进行介绍。\n多模态表示学习 Multimodal Representation\n单模态的表示学习负责将信息表示为计算机可以处理的数值向量或者进一步抽象为更高层的特征向量，而多模态表示学习是指通过利用多模态之间的互补性，剔除模态间的冗余性，从而学习到更好的特征表示。主要包括两大研究方向：联合表示（Joint Representations）和协同表示（Coordinated Representations）。\n联合表示将多个模态的信息一起映射到一个统一的多模态向量空间；\n协同表示负责将多模态中的每个模态分别映射到各自的表示空间，但映射后的向量之间满足一定的相关性约束（例如线性相关）。\n利用多模态表示学习到的特征可以用来做信息检索，也可以用于的分类/回归任务。下面列举几个经典的应用。\n在来自 NIPS 2012 的 《Multimodal learning with deep boltzmann machines》一文中提出将 deep boltzmann machines（DBM） 结构扩充到多模态领域，通过 Multimodal DBM，可以学习到多模态的联合概率分布。\n论文中的实验通过 Bimodal DBM，学习图片和文本的联合概率分布 P(图片，文本)。在应用阶段，输入图片，利用条件概率 P(文本|图片)，生成文本特征，可以得到图片相应的文本描述；而输入文本，利用条件概率 P(图片|文本)，可以生成图片特征，通过检索出最靠近该特征向量的两个图片实例，可以得到符合文本描述的图片。如下图所示：\n协同表示学习一个比较经典且有趣的应用是来自于《Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models 》这篇文章。利用协同学习到的特征向量之间满足加减算数运算这一特性，可以搜索出与给定图片满足“指定的转换语义”的图片。例如：\n狗的图片特征向量 - 狗的文本特征向量 + 猫的文本特征向量 = 猫的图片特征向量 -> 在特征向量空间，根据最近邻距离，检索得到猫的图片\n转化 Translation / 映射 Mapping\n转化也称为映射，负责将一个模态的信息转换为另一个模态的信息。常见的应用包括：\n机器翻译（Machine Translation）：将输入的语言A（即时）翻译为另一种语言B。类似的还有唇读（Lip Reading）和语音翻译 （Speech Translation），分别将唇部视觉和语音信息转换为文本信息。\n图片描述（Image captioning) 或者视频描述（Video captioning)： 对给定的图片/视频形成一段文字描述，以表达图片/视频的内容。\n语音合成（Speech Synthesis）：根据输入的文本信息，自动合成一段语音信号。\n模态间的转换主要有两个难点，一个是open-ended，即未知结束位，例如实时翻译中，在还未得到句尾的情况下，必须实时的对句子进行翻译；另一个是subjective，即主观评判性，是指很多模态转换问题的效果没有一个比较客观的评判标准，也就是说目标函数的确定是非常主观的。例如，在图片描述中，形成怎样的一段话才算是对图片好的诠释？也许一千个人心中有一千个哈姆雷特吧。\n对齐 Alignment\n多模态的对齐负责对来自同一个实例的不同模态信息的子分支/元素寻找对应关系。这个对应关系可以是时间维度的，比如下图所示的 Temporal sequence alignment，将一组动作对应的视频流同骨骼图片对齐。类似的还有电影画面-语音-字幕的自动对齐。\n对齐又可以是空间维度的，比如图片语义分割 （Image Semantic Segmentation）：尝试将图片的每个像素对应到某一种类型标签，实现视觉-词汇对齐。\n多模态融合 Multimodal Fusion\n多模态融合（Multimodal Fusion ）负责联合多个模态的信息，进行目标预测（分类或者回归），属于 MMML 最早的研究方向之一，也是目前应用最广的方向，它还存在其他常见的别名，例如多源信息融合（Multi-source Information Fusion）、多传感器融合（Multi-sensor Fusion)。\n按照融合的层次，可以将多模态融合分为 pixel level，feature level 和 decision level 三类，分别对应对原始数据进行融合、对抽象的特征进行融合和对决策结果进行融合。而 feature level 又可以分为 early 和 late 两个大类，代表了融合发生在特征抽取的早期和晚期。当然还有将多种融合层次混合的 hybrid 方法。\n常见的机器学习方法都可以应用于多模态融合，下面列举几个比较热门的研究方向。\n视觉-音频识别（Visual-Audio Recognition）： 综合源自同一个实例的视频信息和音频信息，进行识别工作。\n多模态情感分析（Multimodal sentiment analysis）： 综合利用多个模态的数据（例如下图中的文字、面部表情、声音），通过互补，消除歧义和不确定性，得到更加准确的情感类型判断结果。\n手机身份认证（Mobile Identity Authentication）： 综合利用手机的多传感器信息，认证手机使用者是否是注册用户。\n多模态融合研究的难点主要包括如何判断每个模态的置信水平、如何判断模态间的相关性、如何对多模态的特征信息进行降维以及如何对非同步采集的多模态数据进行配准等。\n若想了解传统的机器学习方法在此领域的应用，推荐学习清华大学出版的《多源信息融合》（韩崇昭等著）一书。\n协同学习 Co-learning\n协同学习是指使用一个资源丰富的模态信息来辅助另一个资源相对贫瘠的模态进行学习。\n比如迁移学习（Transfer Learning）就是属于这个范畴，绝大多数迈入深度学习的初学者尝试做的一项工作就是将 ImageNet 数据集上学习到的权重，在自己的目标数据集上进行微调。\n迁移学习比较常探讨的方面目前集中在领域适应性（Domain Adaptation）问题上，即如何将train domain上学习到的模型应用到 application domain。\n迁移学习领域著名的还有零样本学习（Zero-Shot Learning）和一样本学习（One-Shot Learning），很多相关的方法也会用到领域适应性的相关知识。\nCo-learning 中还有一类工作叫做协同训练（Co-training ），它负责研究如何在多模态数据中将少量的标注进行扩充，得到更多的标注信息。\n通过以上应用我们可以发现，协同学习是与需要解决的任务无关的，因此它可以用于辅助多模态映射、融合及对齐等问题的研究。\n结束语\n到此为止，我们对多模态机器学习领域的研究方向和应用进行了一个大致的梳理，受限于篇幅，还有许多未涉及的研究问题。\n有什么读后感吗？\n也许你以前没有听过多模态学习（MMML）这个概念，读了此文发现原来自己做的正是 MMML 一个分支；\n也许你以前觉得 CV / NLP / SSP 才是人工智能的正统，读了此文发现多学科交叉的 MMML 一样可以玩 DL 溜得飞起；\n也许你目前正苦于找不到研究的方向，读了此文发现 MMML 打开了新的大门，原来有这么多的事情可以做。\n多模态学习是一个目前热度逐年递增的研究领域，如果大家感兴趣，欢迎留言反馈，后续我们会考虑推出几个热门 MMML 方向的经典or前沿论文、模型解析。\n推荐几篇入门综述文献\n如果想入门 MMML 或者希望对该领域有初步了解，可以从以下几篇综述入手\n【1】Atrey P K, Hossain M A, El Saddik A, et al. Multimodal fusion for multimedia analysis: a survey[J]. Multimedia systems, 2010, 16(6): 345-379.\n【2】Ramachandram D, Taylor G W. Deep multimodal learning: A survey on recent advances and trends[J]. IEEE Signal Processing Magazine, 2017, 34(6): 96-108.\n【3】Baltrušaitis T, Ahuja C, Morency L P. Multimodal machine learning: A survey and taxonomy[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018."}
{"content2":"1、介绍\n决策树（decision tree）是一种有监督的机器学习算法，是一个分类算法。在给定训练集的条件下，生成一个自顶而下的决策树，树的根为起点，树的叶子为样本的分类，从根到叶子的路径就是一个样本进行分类的过程。\n下图为一个决策树的例子，见http://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91\n可见，决策树上的判断节点是对某一个属性进行判断，生成的路径数量为该属性可能的取值，最终到叶子节点时，就完成一个分类（或预测）。决策树具有直观、易于解释的特性。\n2、决策树生成算法\n本文主要讨论如何由一个给定的训练集生成一个决策树。如果都一个数据集合$D$，其特征集合为$A$，那么以何种顺序对A中的特征进行判断就成为决策树生成过程中的关键。首先给出一个决策树生成算法-ID3算法（参考《统计学习方法》李航著）\n--------------------我是算法开始分割线-------------------------------------------\nID3算法：\n输入：训练数据集D，特征集A，阈值e\n输出：决策树T\n（1）若D中所有样本属于同一类Ck，则T为单节点树，并将类Ck作为该节点的类标记，返回T；\n（2）A为空集，T为单节点树，将D中实例数最大的类Ck作为该节点的类标记，返回T；\n（3）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征值Ag；\n（4）如果Ag<e，则置T为单节点树，将D中实例数最大的类Ck作为该节点的类标记，返回T；\n（5）否则，对Ag的每一个可能的取值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T；\n（6）对第i个子节点，以Di为训练集，以 A-{Ag}为特征集，递归调用（1）~（5）步，得到子树Ti，返回Ti。\n--------------------我是算法结束分割线-------------------------------------------\n算法第（3）步中，信息增益是评估每一个特征值对D的划分效果，划分的原则为将无序的数据变得尽量有序。评价随机变量不确定性的一个概念是熵，熵越大，不确定性越大。如果确定一个特征Ag，在确定该特征前后，D的熵的变化值就是特征Ag的信息增益。\n3、熵及信息增益\n熵：\n设X是一个取有限个值（n）的离散随机变量，其概率分布为\n\\[P(X=x_{i})=P_{i}, i=1,2,...,n\\]\n则随机变量X的熵定义为\n\\[H(x) =  - \\sum\\limits_{i = 1}^n {{P_i}\\log {P_i}} \\]\n信息增益：\n训练集为\\(D\\)，\\(|D|\\)为样本容量，设有k个类\\({C_k}\\),k=1,...k, \\({|C_k|}\\)为类\\({C_k}\\)的样本个数，且有\\(\\sum\\limits_{i = 1}^k {|{C_k}|}  = |D|\\)\n设特征A有n个不同取值\\(\\{ {a_{1,}}{a_2}, \\cdots ,{a_n}\\} \\)  ，根据A的值，将D划分为n个子集\\({D_1},{D_2}, \\cdots ,{D_n}\\)， \\({|D_i|}\\)为\\({D_i}\\) 的样本数，\\(\\sum\\limits_{i = 1}^n {|{D_i}|}  = |D|\\)。\n记子集\\({D_i}\\)中属于类\\({C_k}\\)的样本集合为\\({D_{ik}}\\)，即\\({D_{ik}} = {D_i} \\cap {C_k}\\)。\n\\({|D_{ik}|}\\)为\\({D_{ik}}\\)的样本个数。\n（1）数据集D的经验熵H(D)\n\\[H(D) =  - \\sum\\limits_{k = 1}^K {\\frac{{|{C_k}|}}{{|D|}}{{\\log }_2}} \\frac{{|{C_k}|}}{{|D|}}\\]\n（2）特征A对数据集D的经验条件熵H(D|A)\n\\[H(D|A) = \\sum\\limits_{i = 1}^n {\\frac{{|{D_i}|}}{{|D|}}H({D_i}) =  - } \\sum\\limits_{i = 1}^n {\\frac{{|{D_i}|}}{{|D|}}\\sum\\limits_{k = 1}^K {\\frac{{|{D_{ik}}|}}{{|{D_i}|}}} } {\\log _2}\\frac{{|{D_{ik}}|}}{{|{D_i}|}}\\]\n（3）计算信息增益\n\\[g(D,A) = H(D) - H(D|A)\\]\n信息增益越大，表示A对D趋于有序的贡献越大。\n-------------------------------分割线------------------------------------------------\n决策树的R语言实现如下：\nlibrary(plyr)\n# 测试数据集 http://archive.ics.uci.edu/ml/datasets/Car+Evaluation\n##计算训练集合D的熵H（D）\n##输入：trainData 训练集，类型为数据框\n##      nClass 指明训练集中第nClass列为分类结果\n##输出：训练集的熵\ncal_HD <- function(trainData, nClass){\nif ( !(is.data.frame(trainData) & is.numeric(nClass)) )\n\"input error\"\nif (length(trainData) < nClass)\n\"nClass is larger than the length of trainData\"\nrownum <- nrow(trainData)\n#对第nClass列的值统计频数\ncalss.freq <- count(trainData,nClass)\n#计算每个取值的  概率*log2(概率)\ncalss.freq <- mutate(calss.freq, freq2 = (freq / rownum)*log2(freq / rownum))\n-sum(calss.freq[,\"freq2\"])\n#使用arrange代替order，方便的按照多列对数据框进行排序\n#mtcars.new2 <- arrange(mtcars, cyl, vs, gear)\n}\n#cal_HD(mtcars,11)\n##计算训练集合D对特征值A的条件熵H（D|A）\n##输入：trainData 训练集，类型为数据框\n##      nClass 指明训练集中第nClass列为分类结果\n##      nA 指明trainData中条件A的列号\n##输出：训练集trainData对特征A的条件熵\ncal_HDA <- function(trainData, nClass, nA){\nrownum <- nrow(trainData)\n#对第nA列的特征A计算频数\nnA.freq <- count(trainData,nA)\ni <- 1\nsub.hd <- c()\nfor (nA.value in nA.freq[,1]){\n#取特征值A取值为na.value的子集\nsub.trainData <- trainData[which(trainData[,nA] == nA.value),]\nsub.hd[i] <- cal_HD(sub.trainData,nClass)\ni <- i+1\n}\nnA.freq <- mutate(nA.freq, freq2 = (freq / rownum)*sub.hd)\nsum(nA.freq[,\"freq2\"])\n}\n##计算训练集合D对特征值A的信息增益g(D,A)\n##输入：trainData 训练集，类型为数据框\n##      nClass 指明训练集中第nClass列为分类结果\n##      nA 指明trainData中特征A的列号\n##输出：训练集trainData对特征A的信息增益\ng_DA <- function(trainData, nClass, nA){\ncal_HD(trainData, nClass) - cal_HDA(trainData, nClass, nA)\n}\n##根据训练集合生成决策树\n##输入：trainData 训练集，类型为数据框\n##      strRoot 指明根节点的属性名称\n##      strRootAttri 指明根节点的属性取值\n##      nClass 指明训练集中第nClass列为分类结果\n##      cAttri 向量，表示当前可用的特征集合，用列号表示\n##      e 如果特征的最大信息增益小于e，则剩余作为一个分类，类频数最高的最为分类结果\n##输出：决策树T\ngen_decision_tree <- function(trainData, strRoot, strRootAttri, nClass, cAttri, e){\n# 树的描述，（上级节点名称、上级节点属性值、自己节点名称，自己节点的取值）\ndecision_tree <- data.frame()\nnClass.freq <- count(trainData,nClass)   ##类别出现的频数\nnClass.freq <- arrange(nClass.freq, desc(freq))  ##按频数从低到高排列\ncol.name <- names(trainData) ##trainData的列名\n##1、如果D中所有属于同一类Ck，则T为单节点树\nif nrow(nClass.freq) == 1{\nrbind(decision_tree, c(strRoot, strRootAttri, nClass.freq[1,1], ''))\nreturn decision_tree\n}\n##2、如果属性cAttri为空，将D中频数最高的类别返回\nif length(cAttri) == 0{\nrbind(decision_tree, c(strRoot, strRootAttri, nClass.freq[1,1], ''))\nreturn decision_tree\n}\n##3、计算cAttri中各特征值对D的信息增益，选择信息增益最大的特征值Ag及其信息增益\nmaxDA <- 0    #记录最大的信息增益\nmaxAttriName <- ''   #记录最大信息增益对应的属性名称\nmaxAttriIndex <- ''   #记录最大信息增益对应的属性列号\nfor(i in cAttri){\ncurDA <- g_DA(trainData,nClass,i)\nif (maxDA <= curDA){\nmaxDA <- curDA\nmaxAttriName <- col.name[i]\n}\n}\n##4、如果最大信息增益小于阈值e，将D中频数最高的类别返回\nif (maxDA < e){\nrbind(decision_tree, c(strRoot, strRootAttri, nClass.freq[1,1], ''))\nreturn decision_tree\n}\n##5、否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di\n##   将Di中实例数最大的类作为标记，构建子节点\n##   由节点及其子节点构成树T，返回T\nfor (oneValue in unique(trainData[,maxAttriName])){\nsub.train <- trainData[which(trainData[,maxAttriName] == oneValue),]  #Di\n#sub.trian.freq <- count(sub.train,nClass)   ##类别出现的频数\n#sub.trian.freq <- arrange(sub.trian.freq, desc(freq))  ##按频数从低到高排列\nrbind(decision_tree, c(strRoot, strRootAttri, maxAttriName , oneValue))\n##6、递归构建下一步\n# 剔除已经使用的属性\nnext.cAttri <- cAttri[which(cAttri !=maxAttriIndex)]\n# 递归调用\nnext.dt <-gen_decision_tree(sub.train, maxAttriName,\noneValue, nClass, next.cAttri, e)\nrbind(decision_tree, next.dt)\n}\nnames(decision_tree) <- c('preName','preValue','curName','curValue')\ndecision_tree\n}\n---------------决策树总结-------------------\n1、R中有实现决策树算法的包rpart，和画出决策树的包rpart.plot，本例自己实现决策树算法是为了更好的理解。\n2、由于决策树只能处理离散属性，因此连续属性应首先进行离散化。\n3、决策树易于理解，对业务的解释性较强。\n4、ID3算法容易引起过拟合，需考虑树的剪枝。"}
{"content2":"今年早些时候，当谷歌 DeepMind 团队的 AlphaGo 打败了李世石时，媒体就用人工智能、机器学习和深度学习这三个术语来描述 DeepMind 是如何取得胜利的。这三个名词都是 AlphaGo 大胜李世石的原因中的一部分，但是它们并不相同。下面我们就来解释一下。\n理解三者之间关系的最简便方法就是将它们视觉化为一组同心圆——首先是最大的部分人工智能——然后是后来兴旺的机器学习——最后是促使当下人工智能大爆发的深度学习——在最里层。\n<ignore_js_op>\n从萧条到繁荣\n自从 1956 年几个计算机科学家在达特茅斯会议上聚集并开辟了人工智能这一领域，人工智能就进入了我们的想象，并在实验研究中进行着酝酿。在过去的几十年里，人工智能以及轮番被誉为人类文明取得最美好未来的关键，或者是作为一个头脑发烧的轻率概念被扔进了科技垃圾堆中。坦白说直到 2012 年，它就这样在二者之间交杂。\n过去的几年里，尤其从 2015 年开始，人工智能开始爆发了。这很大程度上与 GPU 的广泛应用有关，为了使并行处理更快、更便宜、更强大。这也与近乎无限的存储能力和各类数据洪流（所有的大数据运动）——图像、文本、交易、测绘数据，只要你说得出来——一道进行。\n让我们梳理一遍计算机科学家是如何从萧条——直到 2012 年——到繁荣，开发出每天由成千上百万的人使用的应用。\n人工智能——机器诠释的人类智能\n<ignore_js_op>\nKing me：下西洋跳棋的计算机程序是上世纪 50 年代造成过一阵轰动的一些早期人工智能案例\n回到 1956 年夏天的那场会议，人工智能先驱们的梦想是借由新兴计算机构建具有人类智力特征的复杂机器。这就是所谓的「通用人工智能（General AI）」的概念——拥有人类的所有感觉（甚至可能更多）、所有理智，像人类一样思考的神奇机器。\n你已经在电影中无休止地看到过这些被我们当做朋友的机器，比如《星球大战》中的 C-3PO  以及成为人类敌人的机器——终结者。通用人工智能机器向来有充足的理由出现在电影和科幻小说中；我们不能阻止，至少现在还不行。\n我们能做什么？这就到了「狭义人工智能（Narrow AI）」的概念。指的是能够将特殊任务处理得同人类一样好，或者更好的技术。狭义人工智能的相关案例比如有 Pinterest 上的图像分类、Facebook 中的人脸识别。\n这些是狭义人工智能在实践中的例子。这些技术展示了人类智能的一些方面。但是如何做到的呢？那个智能来自哪里？所以接下来看第二个同心圆，机器学习。\n机器学习——实现人工智能的一种方式\n<ignore_js_op>\nSpam free diet：机器学习帮你清理收件箱中的（大部分）垃圾邮件。\n机器学习最基础的是运用算法来分析数据、从中学习、测定或预测现实世界某些事。所以不是手动编码带有特定指令设定的软件程序来完成某个特殊任务，而是使用大量的数据和算法来「训练」机器，赋予它学习如何执行任务的能力。\n机器学习直接源自早期那帮人工智能群体，演化多年的算法包括了决策树学习（decision tree learning）、归纳逻辑编程（inductive logic programming）。其他的也有聚类（clustering）、强化学习（reinforcement learning）和贝叶斯网络（Bayesian networks）等。我们知道，这些早期机器学习方法都没有实现通用人工智能的最终目标，甚至没有实现狭义人工智能的一小部分目标。\n事实证明，多年来机器学习的最佳应用领域之一是计算机视觉，尽管它仍然需要大量的手工编码来完成工作。人们会去写一些手写分类器，像是边缘检测过滤器（edge detection filters）使得程序可以识别对象的启止位置；形状检测（shape detection）以确定它是否有八条边；一个用来识别单词「S-T-O-P」的分类器。从这些手写分类器中他们开发出能够理解图像的算法，「学习」判定它是否是一个停止标志。\n这很好，但还不够好。特别是有雾天气标志不完全可见的情况下，或者被树遮住了一部分。计算机视觉和图像检测直到目前都不能与人类相媲美，是因为它太过脆弱，太容易出错了。\n是时间和正确的学习算法改变了这一切。\n深度学习——一种实现机器学习的技术\nHerding cats：从 YouTube 视频中挑选猫咪图片，是深度学习的第一次突破性表现之一\n源自最早进行机器学习那群人的另一种算法是人工神经网络（Artificial Neural Networks），它已有几十年的历史。神经网络的灵感来自于我们对大脑生物学的理解——所有神经元之间的相互连接。但是不像生物大脑中的任何神经元，可以在一定的物理距离内连接到任何其他神经元，这些人工神经网络的层、连接和数据传播方向是离散的。\n比如你可以把一个图像切成一堆碎片并输入到神经网络的第一层中。然后第一层的单个神经元们将数据传递给第二层。第二层神经元将数据传给第三层，如此一直传到最后一层并输出最终结果。\n每个神经元分配一个权重到它的输入——评估所执行的任务的准确或不准确。然后最终的输出由所有这些权重来确定。所以想想那个停止标志的例子。一个停止标志图像的特征被切碎并由神经元来「检查」——它的形状、它的消防红色彩、它的独特字母、它的交通标志尺寸以及和它的运动或由此带来的缺失。神经网络的任务是判定它是否为一个停止标志。这提出了一个「概率向量」，它真是一个基于权重的高度受训的猜测。在我们的例子中，系统可能有 86% 的把握认为图像是一个停止标志，7% 的把握认为这是一个限速标志，5% 的把握认为这是一只被卡在树上的风筝，等等——然后网络架构告诉神经网络结果的正确与否。\n甚至这个例子都有些超前了，因为直到现在，神经网络都被人工智能研究社区避开了。自从最早的人工智能起，他们一直在做这方面研究，而「智能」成果收效甚微。问题很简单，即最基本的神经网络属于计算密集型，这并不是一个实用的方法。不过，由多伦多大学的 Geoffrey Hinton 带领的异端研究小组一直在继续相关研究工作，最终在超级计算机上运行并行算法证明了这个概念，但这是直到 GPU 被部署之后才兑现的诺言。\n如果我们再回到停止标志的例子，当网络正在进行调整或者「训练」时，出现大量的错误答案，这个机会是非常好的。它需要的就是训练。它需要看到成千上万，甚至数以百万计的图像，直到神经元的输入权重被精确调整，从而几乎每一次都能得到正确答案——无论有雾没雾，晴天还是雨天。在这一点上，神经网络已经教会了自己停止标志看起来会是什么样的；或者在 Facebook 例子中就是识别妈妈的脸；或者吴恩达 2012 年在谷歌所做的猫的图片。\n吴恩达的突破在于从根本上使用这些神经网络 并将它们变得庞大，增加了层数和神经元的数量，然后通过系统运行大量的数据来训练它。吴恩达使用了 1000 万个 YouTube 视频的图像。他将「深度」运用在深度学习中，这就描述了这些神经网络的所有层。\n如今，在一些场景中通过深度学习训练机器识别图像，做得比人类好，从识别猫咪到确定血液中的癌症指标和磁共振成像扫描中的肿瘤指标。谷歌的 AlphaGo 学会了游戏，并被训练用于 Go 比赛。通过反复与自己对抗来调整自己的神经网络。\n感谢深度学习，让人工智能有一个光明的未来。\n深度学习 已经实现了许多机器学习方面的实际应用和人工智能领域的全面推广。深度学习解决了许多任务让各种机器助手看起来有可能实现。无人驾驶机车、更好的预防医疗，甚至是更好的电影推荐，如今都已实现或即将实现。人工智能在当下和未来。有了深度学习，人工智能甚至可以达到我们长期所想象的科幻小说中呈现的状态。我拿走你的 C-3PO，你可以留着终结者。"}
{"content2":"一、人工智能与机器学习\n说到人工智能，就不得不提图灵测试。图灵测试是阿兰图灵在1950年提出的一个关于机器是否能够思考的著名实验，测试某机器是否能表现出与人等价或无法区分的智能。主要内容是：测试者与被测试者（一个人和一台机器）隔开的情况下，通过一些装置（如键盘）向被测试者随意提问。 进行多次测试后，如果测试者不能确定出被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。\n通常我们认为一个智能系统需要具有以下几个能力：\n语言能力--自然语言处理：能成功地用自然语言交流\n记忆能力--知识表示：存储它知道的或听到的信息\n推理能力--自动推理：运用存储的信息来回答问题并推出新结论\n学习能力--机器学习：适应新情况并检测和预测模式\n感知能力--计算机视觉：感知物体\n规划能力--自动规划：根据资源制定执行策略\n由此我们可以看出，机器学习是人工智能的一个分支，它是人工智能研究发展到一定阶段的必然产物。\n二、机器学习的发展\n二十世纪五十年代到七十年代初，人工智能研究处于”推理期“，人们认为只要能赋予机器逻辑推理能力，机器就具有智能。\n随着研究向前发展，人们逐渐认识到，仅仅具有逻辑推理能力是远远实现不了人工智能的。要使机器具有智能，必须设法使机器拥有知识。\n二十世纪七十年代中期开始，人工智能研究进入了“知识期”，但人们又认识到，由人来把知识总结出来再教给计算机是非常困难的。有人想到，如果机器能够自己学习知识就好了。\n二十世纪八十年代是机器学习成为一个独立的学科领域、各种机器学习技术百花初绽的时期。（实际上最早图灵在1950年图灵测试的文章中就曾提到机器学习的可能，五十年代到七十年代有一些机器学习的研究如基于神经网络的连接主义学习、感知机、基于逻辑表示的符号主义学习、以决策理论为基础的学习技术、强化学习等。但机器学习独立成为一个学科领域是在八十年代。）\n这时候，人们把机器学习划分为“机械学习”，“示教学习”，“类比学习”，“归纳学习”。\n机械学习\n死记硬背式学习，把外界输入的信息全部记录下来，需要时原封不动地取出来使用，实际上没有真正的学习，仅仅在进行信息存储与检索。\n示教学习\n从指令中学习。\n类比学习\n通过观察和发现学习。\n归纳学习\n从样例中学习，即从训练样例中归纳出学习结果。归纳学习是被研究最多，应用最广的，涵盖了监督学习、无监督学习等。\n二十世纪八十年代，归纳学习的主流是符号主义学习，其代表包括决策树学习和基于逻辑的学习。典型的决策树学习以信息论为基础，以信息熵的最小化为目标，直接模拟了人类对概念进行判定的树形流程。基于逻辑的学习的著名代表是归纳逻辑程序设计，可以看作机器学习与逻辑程序设计的交叉，使用一阶逻辑来进行知识表示，通过修改和扩充逻辑表达式来完成对数据的归纳。\n二十世纪九十年代，归纳学习的主流是基于神经网络的连接主义学习。\n二十世纪九十年代中期，归纳学习的主流是统计学习，其代表是支持向量机(SVM)以及更一般的“核方法(kernel methods)”。\n二十一世纪初，连接主义学习又卷土重来，掀起以“深度学习”为名的热潮。深度学习是指深层神经网络，它在语音、图像等复杂对象的应用中表现很好，性能优越。深度学习的流行一是因为大数据的发展，数据多了；二是因为计算机硬件的发展，计算能力强了。\n三、机器学习的分类\n机器学习中，有个定理叫“没有免费午餐“定理(No Free Lunch Theorem，简称NFL定理)。内容是无论学习算法a多聪明，学习算法b多笨拙，它们的期望性能是相同的。简而言之，就是没有一个通用算法可以完美解决所有问题，我们要根据具体问题来选择合适的算法。\n根据训练数据是否带有标签(label)信息，把训练数据中带有标签信息的学习算法称为监督学习，训练数据中不带标签信息的学习算法称为无监督学习。\n监督学习的代表是分类和回归，常见算法有线性回归、logistic回归、决策树、贝叶斯分类、支持向量机、神经网络等。\n无监督学习的代表是聚类，常见算法有主成分分析(PCA)、K均值聚类(K-Means)等。\n在后面的文章中会一一介绍这些算法和它们的代码实现，尽量给出不调包只用python代码实现和使用流行的机器学习框架实现两种实现方案。因为前者可以更好地理解算法，后者可以更快更方便地使用算法。 当然除了这些算法，还有数据预处理、模型的性能度量、超参数的调整等等。"}
{"content2":"机器学习实战 - 读书笔记(06) – SVM支持向量机\n前言\n最近在看Peter Harrington写的“机器学习实战”，这是我的学习笔记，这次是第6章：SVM 支持向量机。\n支持向量机不是很好被理解，主要是因为里面涉及到了许多数学知识，需要慢慢地理解。我也是通过看别人的博客理解SVM的。\n推荐大家看看on2way的SVM系列：\n解密SVM系列（一）：关于拉格朗日乘子法和KKT条件\n解密SVM系列（二）：SVM的理论基础\n解密SVM系列（三）：SMO算法原理与实战求解\n解密SVM系列（四）：SVM非线性分类原理实验\n基本概念\nSVM - Support Vector Machine。支持向量机，其含义是通过支持向量运算的分类器。其中“机”的意思是机器，可以理解为分类器。\n什么是支持向量呢？在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。\n见下图，在一个二维环境中，其中点R，S，G点和其它靠近中间黑线的点可以看作为支持向量，它们可以决定分类器，也就是黑线的具体参数。\n分类器：就是分类函数。\n线性分类：可以理解为在2维空间中，可以通过一条直线来分类。在p维空间中，可以通过一个p-1维的超平面来分类。\n向量：有多个属性的变量。在多维空间中的一个点就是一个向量。比如 \\(x = (x_1, x_2, ..., x_n)\\)。下面的\\(w\\)也是向量。\n约束条件(subject to) ： 在求一个函数的最优值时需要满足的约束条件。\n向量相乘: \\(xw^T = \\textstyle \\sum_{i=1}^n w_ix_i\\)\n内积: \\(\\langle x,y \\rangle = \\textstyle \\sum_{i=1}^n x_iy_i\\)\n解决的问题：\n线性分类\n在训练数据中，每个数据都有n个的属性和一个二类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面（hyperplane），这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。\n其实这样的超平面有很多，我们要找到一个最佳的。因此，增加一个约束条件：这个超平面到每边最近数据点的距离是最大的。也成为最大间隔超平面（maximum-margin hyperplane）。这个分类器也成为最大间隔分类器（maximum-margin classifier）。\n支持向量机是一个二类分类器。\n非线性分类\nSVM的一个优势是支持非线性分类。它结合使用拉格朗日乘子法和KKT条件，以及核函数可以产生非线性分类器。\n分类器1 - 线性分类器\n是一个线性函数，可以用于线性分类。一个优势是不需要样本数据。\nclassifier 1:\n\\[ f(x) = xw^T + b \\]\n\\(w\\) 和 \\(b\\) 是训练数据后产生的值。\n分类器2 - 非线性分类器\n支持线性分类和非线性分类。需要部分样本数据（支持向量），也就是\\(\\alpha_i \\ne 0\\)的数据。\n\\(\\because\\)\n\\(w = \\textstyle \\sum_{i=1}^n \\alpha_iy_ix_i\\)\n\\(\\therefore\\)\nclassifier 2:\n\\[ f(x) = \\textstyle \\sum_{i=1}^n \\alpha_iy_i K(x_i, x) + b \\\\ \\text{here} \\\\ \\qquad x_i \\text{ : training data i} \\\\ \\qquad y_i \\text{ : label value of training data i} \\\\ \\qquad \\alpha_i \\text{ : Lagrange multiplier of training data i} \\\\ \\qquad K(x_1, x_2) = exp(-\\frac{\\lVert x_1 - x_2 \\rVert ^2}{2\\sigma^2}) \\text{ : kernel function} \\\\ \\]\n\\(\\alpha\\), \\(\\sigma\\) 和 \\(b\\) 是训练数据后产生的值。\n可以通过调节\\(\\sigma\\)来匹配维度的大小，\\(\\sigma\\)越大，维度越低。\n核心思想\nSVM的目的是要找到一个线性分类的最佳超平面 \\(f(x) = xw^T + b = 0\\)。求 \\(w\\) 和 \\(b\\)。\n首先通过两个分类的最近点，找到\\(f(x)\\)的约束条件。\n有了约束条件，就可以通过拉格朗日乘子法和KKT条件来求解，这时，问题变成了求拉格朗日乘子\\(\\alpha_i\\) 和 \\(b\\)。\n对于异常点的情况，加入松弛变量\\(\\xi\\)来处理。\n使用SMO来求拉格朗日乘子\\(\\alpha_i\\)和\\(b\\)。这时，我们会发现有些\\(\\alpha_i = 0\\)，这些点就可以不用在分类器中考虑了。\n惊喜! 不用求\\(w\\)了，可以使用拉格朗日乘子\\(\\alpha_i\\)和\\(b\\)作为分类器的参数。\n非线性分类的问题：映射到高维度、使用核函数。\n详解\n线性分类及其约束条件\nSVM的解决问题的思路是找到离超平面的最近点，通过其约束条件求出最优解。\n对于训练数据集T，其数据可以分为两类C1和C2。\n对于函数：\\(f(x) = xw^T + b\\)\n对于C1类的数据 \\(xw^T + b \\geqslant 1\\)。其中至少有一个点\\(x_i\\)， \\(f(x_i) = 1\\)。这个点称之为最近点。\n对于C2类的数据 \\(xw^T + b \\leqslant -1\\)。其中至少有一个点\\(x_i\\)， \\(f(x_i) = -1\\)。这个点称也是最近点。\n上面两个约束条件可以合并为：\n\\(y_if(x_i) = y_i(x_iw^T + b) \\geqslant 1\\)。\n\\(y_i\\)是点\\(x_i\\)对应的分类值（-1或者1）。\n求\\(w\\)和\\(b\\).\n则超平面函数是\\(xw^T + b = 0\\)。\n为了求最优的f(x)， 期望训练数据中的每个点到超平面的距离最大。\n（解释1: 这里需要理解一个事情，根据上图，我们可以给每个点做一条平行于超平面的平行线（超平行面），因此，这个最大化相当于求最近点到超平面距离的最大化。）\n总结，现在我们的公式是：\nFormula 6.1\n\\[ f(x) = xw^T + b \\\\ \\text{subject to} \\\\ \\qquad y_if(x_i) = y_i(x_iw^T + b) \\geqslant 1, i = 1, ..., n \\]\n几个训练脑筋的小问题：\nQ: y是否可以是其它非{-1， 1}的值?\nA: 将y值定义为{-1， 1}是最简化的方案。你的分类可以是cat和dog，只要将cat对应到1, dog对应到-1就可以了。你也可以将y值定义为其它数比如: -2, 2或者2, 3之类的，但是这样就需要修改超平面函数和约束条件，增加了没必要的繁琐，实际上和y值定义为{-1， 1}是等价的。\nQ: 如果两组数据里的太近或者太远，是不是可能就找不到\\(xw^T + b = 1\\) 和\\(xw^T + b = -1\\)的这两个点？\nA: 不会。假设可以找到\\(x_iw^T + b = c\\) 和 \\(x_jw^T + b = -c\\). \\(c > 0 and c <> 1\\)。其超平面函数为\\(xw^T + b = 0\\).\n上面公式左右同时除以c, 则：\n\\(x_iw^T / c + b / c = 1\\)\n\\(x_jw^T / c + b / c = -1\\)\n令:\n\\(w' = w/c\\)\n\\(b' = b/c\\)\n有:\n\\(x_iw'^T + b' = 1\\)\n\\(x_jw'^T + b' = -1\\)\n可以找到超平面函数:\n\\(xw^T + b' = 0\\)\n因此，总是可以找到y是{-1, 1}的超平面，如果有的话。\n最大几何间隔（geometrical margin）\n\\(f(x)\\)为函数间隔\\(\\gamma\\)。\n如果求\\(\\text{max } yf(x)\\)，有个问题，就是w和b可以等比例增大，导致\\(yf(x)\\)的间隔可以无限大。因此需要变成求等价的最大几何间隔：\n\\[ \\bar{\\gamma} = \\frac{yf(x)}{\\lVert w \\rVert} \\\\ \\text{subject to} \\\\ \\qquad y_if(x_i) = y_i(x_iw^T + b) \\geqslant 1, i = 1, ..., n \\]\n\\(\\lVert w \\rVert\\) : 二阶范数，也就是各项目平方和的平方根。 \\(\\sqrt {\\textstyle \\sum_{i=1}^n w_i^2}\\)\n根据上面的解释，这个问题可以转变为：\n\\[ \\text{max } \\frac{1}{\\lVert w \\rVert} \\\\ \\text{subject to} \\\\ \\qquad y_i(x_iw^T + b) \\geqslant 1, i = 1, ..., n \\]\n再做一次等价转换：\nFormula 6.2\n\\[ \\text{min } \\frac{1}{2} \\lVert w \\rVert ^ 2 \\\\ \\text{subject to} \\\\ \\qquad y_i(x_iw^T + b) \\geqslant 1, i = 1, ..., n \\]\n求解问题\\(w,b \\Leftrightarrow \\alpha_i, b\\)\n我们使用拉格朗日乘子法和KKT条件来求\\(w\\)和\\(b\\)，一个重要原因是使用拉格朗日乘子法后,还可以解决非线性划分问题。\n拉格朗日乘子法和KKT条件可以解决下面这个问题：\n求一个最优化问题 \\(f(x)\\)\n刚好对应我们的问题：\\(min \\frac{1}{2} \\lVert w \\rVert ^ 2\\)\n如果存在不等式约束\\(g_k(x) <= 0, k = 1, …, q\\)。\n对应 \\(\\text{subject to } \\qquad 1 - y_i(x_iw^T + b) <= 0, i = 1, ..., n\\)\nF(x)必须是凸函数。这个也满足。\nSVM的问题满足使用拉格朗日乘子法的条件。因此问题变成：\nFormula 6.3\n\\[ \\underset{\\alpha}{max} \\text{ } W(\\alpha) = \\mathcal{L}(w,b,\\alpha) = \\frac{1}{2} \\lVert w \\rVert ^ 2 - \\textstyle \\sum_{i=1}^n \\alpha_i(y_i(x_iw^T + b) - 1) \\\\ \\text{subject to} \\\\ \\qquad \\alpha_i >= 0, i = 1, ..., n \\\\ \\qquad \\textstyle \\sum_{i=1}^n \\alpha_iy_i = 0 \\\\ \\qquad 1 - y_i(x_iw^T + b) <= 0, i = 1, ..., n \\\\ \\qquad w = \\textstyle \\sum_{i=1}^n \\alpha_iy_ix_i \\\\ \\text{here} \\\\ \\qquad \\alpha_i \\text{ : Lagrange multiplier of training data i} \\\\ \\]\n消除\\(w\\)之后变为：\nFormula 6.4\n\\[ \\underset{\\alpha}{max} \\text{ } W(\\alpha) = \\mathcal{L}(w,b,\\alpha) = \\textstyle \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\textstyle \\sum_{i,j=1}^n \\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\\\ \\text{subject to} \\\\ \\qquad \\alpha_i >= 0, i = 1, ..., n \\\\ \\qquad \\textstyle \\sum_{i=1}^n \\alpha_iy_i = 0 \\\\ \\qquad \\alpha_i(1 - y_i(\\textstyle \\sum_{j=1}^n \\alpha_jy_j \\langle x_j,x_i \\rangle + b)) = 0, i = 1, ..., n \\]\n\\(\\langle x_j,x_i \\rangle\\)是\\(x_j\\) 和 \\(x_i\\)的内积，相当于\\(x_ix_j^T\\)。\n可见使用拉格朗日乘子法和KKT条件后，求\\(w,b\\)的问题变成了求拉格朗日乘子\\(\\alpha_i\\)和\\(b\\)的问题。\n到后面更有趣，变成了不求\\(w\\)了，因为\\(\\alpha_i\\)可以直接使用到分类器中去，并且可以使用\\(\\alpha_i\\)支持非线性的情况（\\(xw^T + b\\)是线性函数，支持不了非线性的情况哦）。\n以上的具体证明请看：\n解密SVM系列（二）：SVM的理论基础\n关于拉格朗日乘子法和KKT条件，请看：\n深入理解拉格朗日乘子法（Lagrange Multiplier)和KKT条件\n处理异常点（outliers）\n如上图：点w是一个异常点，导致无法找到一个合适的超平面，为了解决这个问题，我们引入松弛变量(slack variable)\\(\\xi\\)。\n修改之间的约束条件为：\\(x_iw^T + b >= 1 – \\xi_i \\qquad \\text{for all i = 1, …, n}\\)\n则运用拉格朗日乘子法之后的公式变为：\nFormula 6.5\n\\[ \\underset{\\alpha}{max} \\text{ } W(\\alpha) = \\mathcal{L}(w,b,\\alpha) = \\textstyle \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\textstyle \\sum_{i,j=1}^n \\alpha_i\\alpha_jy_iy_jx_jx_i^T \\\\ \\text{subject to} \\\\ \\qquad 0 \\leqslant \\alpha_i \\leqslant C, i = 1, ..., n \\\\ \\qquad \\textstyle \\sum_{i=1}^n \\alpha_iy_i = 0 \\\\ \\qquad \\alpha_i(1 - y_i(\\textstyle \\sum_{j=1}^n \\alpha_jy_j \\langle x_j,x_i \\rangle + b)) = 0, i = 1, ..., n \\]\n输入参数：\n参数\\(C\\)，越大表明影响越严重。\\(C\\)应该一个大于0值。其实\\(C\\)也不能太小，太小了就约束\\(\\alpha_i\\)了，比如200。\n参数\\(\\xi\\)，对所有样本数据起效的松弛变量，比如：0.0001。\n具体证明请看：\n解密SVM系列（二）：SVM的理论基础\n求解\\(\\alpha\\) - 使用SMO方法\n1996年，John Platt发布了一个称为SMO的强大算法，用于训练SVM。SMO表示序列最小优化（Sequential Minimal Optimization）。\nSMO方法：\n概要：SMO方法的中心思想是每次取一对\\(\\alpha_i\\)和\\(\\alpha_j\\)，调整这两个值。\n参数: 训练数据/分类数据/\\(C\\)/\\(\\xi\\)/最大迭代数\n过程：\n初始化\\(\\alpha\\)为0；\n在每次迭代中 （小于等于最大迭代数），\n- 找到第一个不满足KKT条件的训练数据，对应的\\(\\alpha_i\\)，\n- 在其它不满足KKT条件的训练数据中，找到误差最大的x，对应的index的\\(\\alpha_j\\)，\n- \\(\\alpha_i\\)和\\(\\alpha_j\\)组成了一对，根据约束条件调整\\(\\alpha_i\\), \\(\\alpha_j\\)。\n不满足KKT条件的公式：\nFormula 6.6\n\\[ \\text{(1) } y_i(u_i - y_i) \\leqslant \\xi \\text{ and } \\alpha_i < C \\\\ \\text{(2) } y_i(u_i - y_i) \\geqslant \\xi \\text{ and } \\alpha_i > 0 \\\\ here \\\\ \\qquad u_i = \\textstyle \\sum_{j=1}^n \\alpha_jy_j K(x_j, x_i) + b \\\\ \\qquad K(x_1, x_2) = \\langle x_1, x_2 \\rangle \\\\ \\qquad \\xi \\text{ : slack variable} \\]\n调整公式：\nFormula 6.7\n\\[ \\alpha_2^{new} = \\alpha_2^{old} - \\frac{y_2(E_1 - E_2)}{\\eta} \\\\ \\alpha_1^{new} = \\alpha_1^{old} + y_1y_2(\\alpha_2^{old} - \\alpha_2^{new}) \\\\ b_1 = b^{old} - E_1 -y_1(\\alpha_1^{new} - \\alpha_1^{old})K(x_1, x_1) - y_2(\\alpha_2^{new} - \\alpha_2^{old})K(x_1, x_2) \\\\ b_2 = b^{old} - E_2 -y_1(\\alpha_1^{new} - \\alpha_1^{old})K(x_1, x_2) - y_2(\\alpha_2^{new} - \\alpha_2^{old})K(x_2, x_2) \\\\ b = \\begin{cases} b_1 & \\text{if } 0 \\leqslant \\alpha_1^{new} \\leqslant C \\\\ b_2 & \\text{if } 0 \\leqslant \\alpha_2^{new} \\leqslant C \\\\ \\frac{b_1 + b_2}{2} & \\text{otherwise} \\end{cases} \\\\ here \\\\ \\qquad E_i = u_i - y_i \\\\ \\qquad \\eta = 2K(x_1, x_2) - K(x_1, x_1) - K(x_2, x_2) \\\\ \\qquad u_i = \\textstyle \\sum_{j=1}^n \\alpha_jy_j K(x_j, x_i) + b \\\\ \\qquad K(x_1, x_2) = \\langle x_1, x_2 \\rangle \\]\n具体证明请参照:\n解密SVM系列（三）：SMO算法原理与实战求解\n最后一步：解决非线性分类\n根据机器学习的理论，非线性问题可以通过映射到高维度后，变成一个线性问题。\n比如：二维下的一个点\\(<x1, x2>\\), 可以映射到一个5维空间，这个空间的5个维度分别是:\\(x1, x2, x1x2, x1^2, x2^2\\)。\n映射到高维度，有两个问题：一个是如何映射？另外一个问题是计算变得更复杂了。\n幸运的是我们可以使用核函数(Kernel function)来解决这个问题。\n核函数(kernel function)也称为核技巧(kernel trick)。\n核函数的思想是：\n仔细观察Formula 6.6 和 Formula 6.7，就会发现关于向量\\(x\\)的计算，总是在计算两个向量的内积\\(K(x_1, x_2) = \\langle x_1, x_2 \\rangle\\)。\n因此，在高维空间里，公式的变化只有计算低维空间下的内积\\(\\langle x_1, x_2 \\rangle\\)变成了计算高维空间下的内积\\(\\langle x'_1, x'_2 \\rangle\\)。\n核函数提供了一个方法，通过原始空间的向量值计算高维空间的内积，而不用管映射的方式。\n我们可以用核函数代替\\(K(x_1, x_2)\\)。\n核函数有很多种, 一般可以使用高斯核（径向基函数（radial basis function））\nFormula 6.8\n\\[ K(x_1, x_2) = exp(-\\frac{\\lVert x_1 - x_2 \\rVert ^2}{2\\sigma^2}) \\]\n可以通过调节\\(\\sigma\\)来匹配维度的大小，\\(\\sigma\\)越大，维度越低，比如10。\n可以参照：\n解密SVM系列（四）：SVM非线性分类原理实验\n支持向量机通俗导论（理解SVM的三层境界）\n如何解决多类分类问题\n支持向量机是一个二类分类器。基于SVM如何构建多类分类器，建议阅读C. W. Huset等人发表的一篇论文\"A Comparison of Methods for Multiclass Support Vector Machines\"。需要对代码做一些修改。\n参照\nMachine Learning in Action by Peter Harrington\n解密SVM系列（一）：关于拉格朗日乘子法和KKT条件\n解密SVM系列（二）：SVM的理论基础\n解密SVM系列（三）：SMO算法原理与实战求解\n解密SVM系列（四）：SVM非线性分类原理实验\n深入理解拉格朗日乘子法（Lagrange Multiplier)和KKT条件\n支持向量机通俗导论（理解SVM的三层境界）\nhttps://en.wikipedia.org/wiki/Support_vector_machine"}
{"content2":"不多说，直接上干货！\n问题详情\n如下：点击Build ,再 Build -> Build Artifacts，没反应？？？\n解决办法\n1、File，再Project Structure\n2、然后，看你自己想要打包成什么格式的，这里很多选择，比如jar包（JAR）、war包（Web Application: Archive）\n3、比如是jar包，则选择From modules with dependencies  ，或者Empty也可以\n4、比如是war包，则选择\n同样自己取个名字。\n成功！\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"欢迎大家前往腾讯云社区，获取更多腾讯海量技术实践干货哦~\n作者：汪毅雄\n导语：本文详细的解释了机器学习中，经常会用到数据清洗与特征提取的方法PCA，从理论、数据、代码三个层次予以分析。\n机器学习，这个名词大家都耳熟能详。虽然这个概念很早就被人提出来了，但是鉴于科技水平的落后，一直发展的比较缓慢。但是，近些年随着计算机硬件能力的大幅度提升，这一概念慢慢地回到我们的视野，而且发展速度之快令很多人刮目相看。尤其这两年，阿法狗在围棋届的神勇表现，给人在此领域有了巨大的遐想空间。\n所谓机器学习，一般专业一点的描述其是：机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n机器学习这门技术是多种技术的结合。而在这个结合体中，如何进行数据分析处理是个人认为最核心的内容。通常在机器学习中，我们指的数据分析是，从一大堆数据中，筛选出一些有意义的数据，推断出一个潜在的可能结论。得出这个不知道正确与否的结论，其经过的步骤通常是：\n1、预处理：把数据处理成一些有意义的特征，这一步的目的主要是为了降维。\n2、建模：这部分主要是建立模型（通常是曲线的拟合），为分类器搭建一个可能的边界。\n3、分类器处理：根据模型把数据分类，并进行数据结论的预测。\n本文讲的主要是数据的预处理（降维），而这里采用的方式是PCA。\nPCA的个人理论分析：\n假设有一个学生信息管理系统，里面需要存储人性别的字段，我们在数据库里可以有M、F两个字段，用1、0分别代表是、否。当是男学生的时候其中M列为1，F列为0，为女生时M列为0，F列为1。我们发现，对任意一条记录，当M为1，F必然为0，反之也是如此。因此实际过程，我们把M列或F列去掉也不会丢失任何信息，因为我们可以反推出结论。这种情况下的M、F列的关联比是最高的，是100%。\n再举另外一个例子，小明开了家店铺，他每天在统计其店铺的访问量V和成交量D。可以发现，往往V多的时候，D通常也多。D少的时候，V通常也很少。可以猜到V和D是有种必然的联系，但又没有绝对的联系。此时小明如果想根据V、D来衡量这一天的价值，往往可以根据一些历史数据来计算出V、D的关联比。拍脑门说一个，如果关联比大于80%，那么可以取VD其中任意一个即可衡量当天价值。这样就达到了降维的效果。\n当然降维并非只能在比如说2维数据[V，D]中选取其中的1维[V]作为特征值，它有可能是在V+D的情况下，使得对[V, D]的关联比最大。\n但是PCA思想就是如此。简单点说：假设有x1、x2、x3…xn维数据，我们想把数据降到m维，我们可以根据这n维的历史数据，算出一个与x1…xn相关m维数据，使得这个m维数据对历史数据的关联比达到最大。\n数学分析\n假设我们有一组二维数据\n如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？\n这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。\n那么如何选择这个方向才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散，这样投影的范围越大，在做分类的时候也就更容易做分类器。\n以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失。同理，如果向y轴投影中间的三个点都会重叠，效果更糟。所以看来x和y轴都不是最好的投影选择。直观来看，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。\n我们希望投影后投影值尽可能分散，那什么是衡量分散程度的统计量呢，显然可以用数学上的方差来表述。\n通常，为了方便计算，我们会把每个点都减去均值，这样得到的点的均值就会为0.这个过程叫做均一化。均一化后：\n于是上面的问题被形式化表述为：寻找一个基，使得所有数据变换为这个基上的坐标表示后，方差值最大。\n我们跳出刚才的例子，因为很容易把刚才的结论推广到任意纬度。要求投影点的方差最大值所对应的基u，这时有两种方法来求解：\n方法一：\n假设有个投影A：\n显然刚才说的方差V可以用来表示：\n而投影A = 原始数据X . U；\n这样方差可以表示为：\n求这个方差的最大值，我们可以用拉格朗日插值法来做\nL（u，λ）为：\n求导L’：\n令导数为0：\n这样问题就转换成求X.XT的特征值和特征向量，问题就迎刃而解了。\n同时我们可以知道，特征值和特征向量有很多个，当λ最大的时候所对应的特征向量，我们把它叫作主成份向量。如果需要将m降维为n，只需要去前n大的特征值所对应的特征向量即可。\n方法二：\n对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，首先我们希望找到一个方向（基）使得投影后方差最大，当我们找第二个方向（基）的时候，为了最大可能还原多的信息，我们显然不希望第二个方向与第一个方向有重复的信息。这个从向量的角度看，意味这一个向量在另一个向量的投影必须为0.\n这就有：\n这时候我们思路就很明了：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段本身的方差则尽可能大。\n还是假设我们原始数据为A\n我们做一个处理A.AT得到：\n我们发现要是能找到一个基使得这个矩阵变成一个，除了斜对角外，其余全是0的话，那这个基就是我们需要的基。那么问题就转换成矩阵的对角化了。\n先说一个先验知识：\n在线性代数上，我们可以知道实对称矩阵不同特征值对应的特征向量必然正交。对一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为e1,e2,⋯,en。\n组合成矩阵的形式如图：\n由上结论又有一个新的结论就是，对于实对称矩阵A，它的特征向量矩阵为E，必然满足：\n有了这个先验知识，我们假设原始数据A，基为U，投影后的数据为Y。则有Y=UA。根据上面所说的要是投影后的矩阵Y的Y.YT为一个对角阵，那么就有：\n要是Y.YT为对角阵，那么只需要U是A.AT的特征向量即可，那么问题最终还是转换为求AAT的特征向量。\n代码实现：\n刚才说了两种PCA的计算思路，我们简单看下代码的实现吧，由于matlab自带了求特征向量的函数，这边使用matlab进行模拟。\n我们用测试数据试试：\n当我们只保留0.5的成分时，newA从3维降到1维，当进行还原时，准确性也会稍微差些\n当我们保留0.9的成分时，newA从3维降到2维，当进行还原时，还原度会稍微好些。\n当我们保留0.97的成分时，就无法降维了。这时候就可以100%还原了。\n总结一下：\n我们在做机器学习的数据分析的时候，由于数据集的维度可能很高，这时候我们需要对数据进行降维。本文从各个方向介绍了一下降维的经典方法PCA，也从代码的角度告诉了怎么降维的过程。实际操作可能会比较简单，但是原理个人觉得还是有学习的地方的。\n相关阅读\n机器学习之回归原理详述（一）\n机器学习之决策树与随机森林模型\n主流机器学习算法简介与其优缺点分析\n此文已由作者授权腾讯云技术社区发布，转载请注明原文出处"}
{"content2":"作者：无影随想\n时间：2016年1月。\n出处：https://zhaokv.com/machine_learning/2016/01/learning-from-imbalanced-data.html\n声明：版权所有，转载请注明出处\n这几年来，机器学习和数据挖掘非常火热，它们逐渐为世界带来实际价值。与此同时，越来越多的机器学习算法从学术界走向工业界，而在这个过程中会有很多困难。数据不平衡问题虽然不是最难的，但绝对是最重要的问题之一。\n一、数据不平衡\n在学术研究与教学中，很多算法都有一个基本假设，那就是数据分布是均匀的。当我们把这些算法直接应用于实际数据时，大多数情况下都无法取得理想的结果。因为实际数据往往分布得很不均匀，都会存在“长尾现象”，也就是所谓的“二八原理”。下图是新浪微博交互分布情况：\n可以看到大部分微博的总互动数（被转发、评论与点赞数量）在0-5之间，交互数多的微博（多于100）非常之少。如果我们去预测一条微博交互数所在档位，预测器只需要把所有微博预测为第一档（0-5）就能获得非常高的准确率，而这样的预测器没有任何价值。那如何来解决机器学习中数据不平衡问题呢？这便是这篇文章要讨论的主要内容。\n严格地讲，任何数据集上都有数据不平衡现象，这往往由问题本身决定的，但我们只关注那些分布差别比较悬殊的；另外，虽然很多数据集都包含多个类别，但这里着重考虑二分类，因为解决了二分类中的数据不平衡问题后，推而广之就能得到多分类情况下的解决方案。综上，这篇文章主要讨论如何解决二分类中正负样本差两个及以上数量级情况下的数据不平衡问题。\n不平衡程度相同（即正负样本比例类似）的两个问题，解决的难易程度也可能不同，因为问题难易程度还取决于我们所拥有数据有多大。比如在预测微博互动数的问题中，虽然数据不平衡，但每个档位的数据量都很大——最少的类别也有几万个样本，这样的问题通常比较容易解决；而在癌症诊断的场景中，因为患癌症的人本来就很少，所以数据不但不平衡，样本数还非常少，这样的问题就非常棘手。综上，可以把问题根据难度从小到大排个序：大数据+分布均衡<大数据+分布不均衡<小数据+数据均衡<小数据+数据不均衡。对于需要解决的问题，拿到数据后，首先统计可用训练数据有多大，然后再观察数据分布情况。经验表明，训练数据中每个类别有5000个以上样本，数据量是足够的，正负样本差一个数量级以内是可以接受的，不太需要考虑数据不平衡问题（完全是经验，没有理论依据，仅供参考）。\n二、如何解决\n解决这一问题的基本思路是让正负样本在训练过程中拥有相同的话语权，比如利用采样与加权等方法。为了方便起见，我们把数据集中样本较多的那一类称为“大众类”，样本较少的那一类称为“小众类”。\n1. 采样\n采样方法是通过对训练集进行处理使其从不平衡的数据集变成平衡的数据集，在大部分情况下会对最终的结果带来提升。\n采样分为上采样（Oversampling）和下采样（Undersampling），上采样是把小种类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。\n随机采样最大的优点是简单，但缺点也很明显。上采样后的数据集中会反复出现一些样本，训练出来的模型会有一定的过拟合；而下采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。\n上采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动，经验表明这种做法非常有效。\n因为下采样会丢失信息，如何减少信息的损失呢？第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。第三种方法是利用KNN试图挑选那些最具代表性的大众样本，叫做NearMiss，这类方法计算量很大，感兴趣的可以参考“Learning from Imbalanced Data”这篇综述的3.2.1节。\n2. 数据合成\n数据合成方法是利用已有样本生成更多样本，这类方法在小数据场景下有很多成功案例，比如医学图像分析等。\n其中最常见的一种方法叫做SMOTE，它利用小众样本在特征空间的相似性来生成新样本。对于小众样本$x_i\\in S_{\\min}$，从它属于小众类的K近邻中随机选取一个样本点$\\hat{x}_i$，生成一个新的小众样本$x_{new}$：$x_{new}=x_i+(\\hat{x}-x_i)\\times\\delta$，其中$\\delta\\in[0,1]$是随机数。\n上图是SMOTE方法在$K=6$近邻下的示意图，黑色方格是生成的新样本。\nSMOTE为每个小众样本合成相同数量的新样本，这带来一些潜在的问题：一方面是增加了类之间重叠的可能性，另一方面是生成一些没有提供有益信息的样本。为了解决这个问题，出现两种方法：Borderline-SMOTE与ADASYN。\nBorderline-SMOTE的解决思路是寻找那些应该为之合成新样本的小众样本。即为每个小众样本计算K近邻，只为那些K近邻中有一半以上大众样本的小众样本生成新样本。直观地讲，只为那些周围大部分是大众样本的小众样本生成新样本，因为这些样本往往是边界样本。确定了为哪些小众样本生成新样本后再利用SMOTE生成新样本。\nADASYN的解决思路是根据数据分布情况为不同小众样本生成不同数量的新样本。首先根据最终的平衡程度设定总共需要生成的新小众样本数量$G$，然后为每个小众样本$x_i$计算分布比例$\\Gamma_i$：$\\Gamma_i=\\frac{\\Delta_i/K}{Z}$，其中$\\Gamma_i$是$x_i$K近邻中大众样本的数量，$Z$用来归一化使得$\\sum\\Gamma_i=1$，最后为小众样本$x_i$生成新样本的个数为$g_i=\\Gamma_i\\times G$，确定个数后再利用SMOTE生成新样本。\n3. 加权\n除了采样和生成新数据等方法，我们还可以通过加权的方式来解决数据不平衡问题，即对不同类别分错的代价不同，如下图：\nk\nC(k,1)\nC(k,2)\n...\n0\n1\n2\n...\nk\n1\n0\nC(1,2)\n...\nC(1,k)\n2\nC(2,1)\n0\n...\n...\n...\n...\n...\n...\n...\n横向是真实分类情况，纵向是预测分类情况，C(i,j)是把真实类别为j的样本预测为i时的损失，我们需要根据实际情况来设定它的值。\n这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。\n4. 一分类\n对于正负样本极不平衡的场景，我们可以换一个完全不同的角度来看待问题：把它看做一分类（One Class Learning）或异常检测（Novelty Detection）问题。这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，经典的工作包括One-class SVM等。\n三、如何选择\n解决数据不平衡问题的方法有很多，上面只是一些最常用的方法，而最常用的方法也有这么多种，如何根据实际问题选择合适的方法呢？接下来谈谈一些我的经验。\n在正负样本都非常之少的情况下，应该采用数据合成的方式；在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法；在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。\n采样和加权在数学上是等价的，但实际应用中效果却有差别。尤其是采样了诸如Random Forest等分类方法，训练过程会对训练集进行随机采样。在这种情况下，如果计算资源允许上采样往往要比加权好一些。\n另外，虽然上采样和下采样都可以使数据集变得平衡，并且在数据足够多的情况下等价，但两者也是有区别的。实际应用中，我的经验是如果计算资源足够且小众类样本足够多的情况下使用上采样，否则使用下采样，因为上采样会增加训练集的大小进而增加训练时间，同时小的训练集非常容易产生过拟合。对于下采样，如果计算资源相对较多且有良好的并行环境，应该选择Ensemble方法。\n四、更进一步\n更多细节与更多方法可以参考TKDE上的这篇综述：“Learning from Imbalanced Data”。"}
{"content2":"最近一直在看机器学习相关的算法，今天学习logistic回归，在对算法进行了简单分析编程实现之后，通过实例进行验证。\n一 logistic概述\n个人理解的回归就是发现变量之间的关系，也就是求回归系数，经常用回归来预测目标值。回归和分类同属于监督学习，所不同的是回归的目标变量必须是连续数值型。\n今天要学习的logistic回归的主要思想是根据现有的数据对分类边界线建立回归公式，以此进行分类。主要在流行病学中应用较多，比较常用的情形是探索某疾病的危险因素，根据危险因素预测某疾病发生的概率等等。logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释，所以实际中最为常用的就是二分类的logistic回归。\n今天我们就二分类进行分析，我们在回归分析中需要一个函数可以接受所有的输入然后预测出类别，假定用0和1分别表示两个类别，logistic函数曲线很像S型，故此我们可以联系sigmoid函数：σ = 1/(1/(1+e-z))。为了实现logistic回归分类器，我们可以在每个特征上乘以一个回归系数，将所有的乘积相加，将和值代入sigmoid函数中，得到一个范围为0-1之间的数，如果该数值大于0.5则被归入1类，否则被归为0类。\n基于之前的分析，需要找到回归系数，首先我们可以将sigmoid函数的输入形式记为：z = w0x0 + w1x1 +...+wnxn,其中x为输入数据，相应的w就是我们要求的系数，为了求得最佳系数，结合最优化理论，我们可以选取梯度上升法优化算法。梯度上升法的基本思想是:要找到函数的最大值，最好的方法是沿着该函数的梯度方向寻找。要想更进一步的了解这个方法，建议去看Andrew Ng的机器学习课程，记得在第二节主要讲述的就是梯度下降法，与梯度上升所不同的是它求得的是函数的最小值，不过思想是一致的。\n二 python实现\n基于之前的分析，在本节我们对logistic回归一步一步采用python编程实现，今天我用的是2.7版本的，代码如下：\n#coding:utf-8 from numpy import * import math import matplotlib.pyplot as plt #导入数据 def loadDataSet(): dataMat = [] labelMat = [] fr = open('testSet.txt') for line in fr.readlines(): lineArr = line.strip().split()#将文本中的每行中的字符一个个分开，变成list dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat,labelMat #定义sigmoid函数 def sigmoid(inX): return 1.0/(1+exp(-inX)) #梯度上升方法求出回归系数 def gradAscent(data,label): dataMat = mat(data) labelMat = mat(label).transpose() m,n = shape(dataMat) alpha = 0.001 maxCycles = 500 weights = ones((n,1)) for item in range(maxCycles): h = sigmoid(dataMat * weights) error = (labelMat - h)#注意labelMat中的元素的数据类型应为int weights = weights + alpha * dataMat.transpose() * error return weights ''' #测试 data,label = loadDataSet() print gradAscent(data,label) ''' ##求出回归系数之后，就确定了不同数据类别之间的分隔线，为了便于理解，可以画出那条线 def plotBestFit(weights): dataMat,labelMat = loadDataSet() dataArr = array(dataMat) n = shape(dataArr)[0] xcode1 = [] ycode1 = [] xcode2 = [] ycode2 = [] for i in range(n): if int(labelMat[i]) == 1: xcode1.append(dataArr[i,1]) ycode1.append(dataArr[i,2]) else: xcode2.append(dataArr[i,1]) ycode2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcode1,ycode1,s = 30,c = 'red',marker = 's') ax.scatter(xcode2,ycode2,s = 30,c = 'green') x = arange(-3.0,3.0,0.1) y = (-weights[0] - weights[1] * x) / weights[2] ax.plot(x,y) plt.xlabel('x1') plt.ylabel('y1') plt.show() ''' #测试 data,label = loadDataSet() weights = gradAscent(data,label) plotBestFit(weights.getA()) ''' ##改进的梯度上升法 def stocGradAscent1(dataMatrix, classLabels, numIter=150): m,n = shape(dataMatrix) weights = ones(n) #initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4/(1.0+j+i)+0.0001 randIndex = int(random.uniform(0,len(dataIndex))) h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del(dataIndex[randIndex]) return weights ''' #测试 data,label = loadDataSet() weights = stocGradAscent1(array(data),label) plotBestFit(weights) '''\n三 实例分析\n基于之前的分析，本节采用Logistic回归来预测患有疝病的马的存活问题，代码如下：\ndef classifyVector(inX,weights): prob = sigmoid(sum(inX * weights)) if prob > 0.5: return 1.0 else: return 0.0 def colicTest(): frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print \"错误率是：\",errorRate return errorRate def multiTest(): numTests = 10; errorSum=0.0 for k in range(numTests): errorSum += colicTest() print \"平均错误率是\",(numTests, errorSum/float(numTests)) multiTest()\n最后可以看出错误率在35%左右，通过调节步长还是可以进一步减小错误率。\nLogistic回归的目的是寻找到一个非线性sigmoid函数的最佳拟合参数，可以采用梯度上升法优化，而在这个过程中，为了减少时间复杂度，又可以使用随机梯度上升法来简化梯度上升法。"}
{"content2":"python机器学习实战（二）\n版权声明：本文为博主原创文章，转载请指明转载地址\nhttp://www.cnblogs.com/fydeblog/p/7159775.html\n前言\n这篇notebook是关于机器学习监督学习中的决策树算法，内容包括决策树算法的构造过程，使用matplotlib库绘制树形图以及使用决策树预测隐形眼睛类型.\n操作系统：ubuntu14.04（win也ok）   运行环境：anaconda-python2.7-jupyter notebook    参考书籍：机器学习实战和源码   notebook writer ----方阳\n注意事项：在这里说一句，默认环境python2.7的notebook，用python3.6的会出问题，还有我的目录可能跟你们的不一样，你们自己跑的时候记得改目录，我会把notebook和代码以及数据集放到结尾的百度云盘，方便你们下载！\n决策树原理：不断通过数据集的特征来划分数据集，直到遍历所有划分数据集的属性，或每个分支下的实例都具有相同的分类，决策树算法停止运行。\n决策树的优缺点及适用类型\n优点 :计算复杂度不高, 输出结果易于理解,对中间值的缺失不敏感,可以处理不相关特征数据。\n缺点 :可能会产生过度匹配问题。\n适用数据类型:数值型和标称型\n先举一个小例子，让你了解决策树是干嘛的，简单来说，决策树算法就是一种基于特征的分类器，拿邮件来说吧，试想一下，邮件的类型有很多种，有需要及时处理的邮件，无聊是观看的邮件，垃圾邮件等等，我们需要去区分这些，比如根据邮件中出现里你的名字还有你朋友的名字，这些特征就会就可以将邮件分成两类，需要及时处理的邮件和其他邮件，这时候在分类其他邮件，例如邮件中出现buy，money等特征，说明这是垃圾推广文件，又可以将其他文件分成无聊是观看的邮件和垃圾邮件了。\n1.决策树的构造\n1.1 信息增益\n试想一下，一个数据集是有多个特征的，我们该从那个特征开始划分呢，什么样的划分方式会是最好的？\n我们知道划分数据集的大原则是将无序的数据变得更加有序，这样才能分类得更加清楚，这里就提出了一种概念，叫做信息增益，它的定义是在划分数据集之前之后信息发生的变化，变化越大，证明划分得越好，所以在划分数据集的时候，获得增益最高的特征就是最好的选择。\n这里又会扯到另一个概念，信息论中的熵，它是集合信息的度量方式，熵变化越大，信息增益也就越大。信息增益是熵的减少或者是数据无序度的减少.\n一个符号x在信息论中的信息定义是 l(x)= -log(p(x)) ,这里都是以2为底，不再复述。\n则熵的计算公式是 H =-∑p(xi)log(p(xi)) (i=1,2,..n)\n下面开始实现给定数据集，计算熵\n参考代码：\n1 from math import log #we use log function to calculate the entropy 2 import operator\n1 def calcShannonEnt(dataSet): 2 numEntries = len(dataSet) 3 labelCounts = {} 4 for featVec in dataSet: #the the number of unique elements and their occurance 5 currentLabel = featVec[-1] 6 if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 7 labelCounts[currentLabel] += 1 8 shannonEnt = 0.0 9 for key in labelCounts: 10 prob = float(labelCounts[key])/numEntries 11 shannonEnt -= prob * log(prob,2) #log base 2 12 return shannonEnt\n程序思路： 首先计算数据集中实例的总数,由于代码中多次用到这个值,为了提高代码效率,我们显式地声明一个变量保存实例总数. 然后 ,创建一个数据字典labelCounts,它的键值是最后一列（分类的结果）的数值.如果当前键值不存在,则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。 最后 , 使用所有类标签的发生频率计算类别出现的概率。我们将用这个概率计算香农熵。\n让我们来测试一下，先自己定义一个数据集\n下表的数据包含 5 个海洋动物,特征包括:不浮出水面是否可以生存,以及是否有脚蹼。我们可以将这些动物分成两类: 鱼类和非鱼类。\n根据上面的表格，我们可以定义一个createDataSet函数\n参考代码如下\n1 def createDataSet(): 2 dataSet = [[1, 1, 'yes'], 3 [1, 1, 'yes'], 4 [1, 0, 'no'], 5 [0, 1, 'no'], 6 [0, 1, 'no']] 7 labels = ['no surfacing','flippers'] 8 #change to discrete values 9 return dataSet, labels\n把所有的代码都放在trees.py中（以下在jupyter）\ncd /home/fangyang/桌面/machinelearninginaction/Ch03\n/home/fangyang/桌面/machinelearninginaction/Ch03\nimport trees\nmyDat, labels = trees.createDataSet()\nmyDat #old data set\n[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\nlabels\n['no surfacing', 'flippers']\ntrees.calcShannonEnt(myDat) #calculate the entropy\n0.9709505944546686\nmyDat[0][-1]='maybe' #change the result ,and look again the entropy\nmyDat #new data set\n[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\ntrees.calcShannonEnt(myDat) # the new entropy\n1.3709505944546687\n我们可以看到当结果分类改变，熵也发生里变化，主要是因为最后的结果发生里改变，相应的概率也发生了改变，根据公式，熵也会改变\n1.2 划分数据集\n前面已经得到了如何去求信息熵的函数，但我们的划分是以哪个特征划分的呢，不知道，所以我们还要写一个以给定特征划分数据集的函数。\n参考代码如下：\n1 def splitDataSet(dataSet, axis, value): 2 retDataSet = [] 3 for featVec in dataSet: 4 if featVec[axis] == value: 5 reducedFeatVec = featVec[:axis] #chop out axis used for splitting 6 reducedFeatVec.extend(featVec[axis+1:]) 7 retDataSet.append(reducedFeatVec) 8 return retDataSet\n函数的三个输人参数:待划分的数据集（dataSet）、划分数据集的特征（axis）、特征的返回值（value）。输出是划分后的数据集（retDataSet）\n小知识：python语言在函数中传递的是列表的引用 ,在函数内部对列表对象的修改, 将会影响该列表对象的整个生存周期。为了消除这个不良影响 ,我们需要在函数的开始声明一个新列表对象。 因为该函数代码在同一数据集上被调用多次,为了不修改原始数据集,创建一个新的列表对象retDataSet\n这个函数也挺简单的，根据axis的值所指的对象来进行划分数据集，比如axis=0，就按照第一个特征来划分，featVec[:axis]就是空,下面经过一个extend函数，将featVec[axis+1:]后面的数存到reduceFeatVec中，然后通过append函数以列表的形式存到retDataSet中。\n这里说一下entend和append函数的功能，举个例子吧\na=[1,2,3] b=[4,5,6] a.append(b)\na\n[1, 2, 3, [4, 5, 6]]\na=[1,2,3] a.extend(b)\na\n[1, 2, 3, 4, 5, 6]\n可见append函数是直接将b的原型导入a中，extend是将b中的元素导入到a中\n下面再来测试一下\nmyDat, labels = trees.createDataSet() #initialization\nmyDat\n[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\ntrees.splitDataSet(myDat,0,1) #choose the first character to split the dataset\n[[1, 'yes'], [1, 'yes'], [0, 'no']]\ntrees.splitDataSet(myDat,0,0)# change the value ,look the difference of previous results\n[[1, 'no'], [1, 'no']]\n好了，我们知道了怎样以某个特征划分数据集了，但我们需要的是最好的数据集划分方式，所以要结合前面两个函数，计算以每个特征为划分方式，相应最后的信息熵，我们要找到最大信息熵，它所对应的特征就是我们要找的最好划分方式。所以有了函数chooseBestFeatureToSpilt\n参考代码如下：\n1 def chooseBestFeatureToSplit(dataSet): 2 numFeatures = len(dataSet[0]) - 1 #the last column is used for the labels 3 baseEntropy = calcShannonEnt(dataSet) #calculate the original entropy 4 bestInfoGain = 0.0; bestFeature = -1 5 for i in range(numFeatures): #iterate over all the features 6 featList = [example[i] for example in dataSet]#create a list of all the examples of this feature 7 uniqueVals = set(featList) #get a set of unique values 8 newEntropy = 0.0 9 for value in uniqueVals: 10 subDataSet = splitDataSet(dataSet, i, value) 11 prob = len(subDataSet)/float(len(dataSet)) 12 newEntropy += prob * calcShannonEnt(subDataSet) 13 infoGain = baseEntropy - newEntropy #calculate the info gain; ie reduction in entropy 14 if (infoGain > bestInfoGain): #compare this to the best gain so far 15 bestInfoGain = infoGain #if better than current best, set to best 16 bestFeature = i 17 return bestFeature #returns an integer\n这个函数就是把前面两个函数整合起来了，先算出特征的数目，由于最后一个是标签，不算特征，所以以数据集长度来求特征数时，要减1。然后求原始的信息熵，是为了跟新的信息熵，进行比较，选出变化最大所对应的特征。这里有一个双重循环，外循环是按特征标号进行循环的，下标从小到大，featList是特征标号对应下的每个样本的值，是一个列表，而uniqueVals是基于这个特征的所有可能的值的集合，内循环做的是以特征集合中的每一个元素作为划分，最后求得这个特征下的平均信息熵，然后原始的信息熵进行比较，得出信息增益，最后的if语句是要找到最大信息增益，并得到最大信息增益所对应的特征的标号。\n现在来测试测试\nimport trees myDat, labels = trees.createDataSet() trees.chooseBestFeatureToSplit(myDat) #return the index of best character to split\n0\n1.3 递归构建决策树\n好了，到现在，我们已经知道如何基于最好的属性值去划分数据集了，现在进行下一步，如何去构造决策树\n决策树的实现原理：得到原始数据集, 然后基于最好的属性值划分数据集,由于特征值可能多于两个,因此可能存在大于两个分支的数据集划分。第一次划分之后, 数据将被向下传递到树分支的下一个节点, 在这个节点上 ,我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。\n递归结束的条件是:程序遍历完所有划分数据集的属性, 或者每个分支下的所有实例都具有相同的分类。\n这里先构造一个majorityCnt函数，它的作用是返回出现次数最多的分类名称，后面会用到\ndef majorityCnt(classList): classCount={} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0]\n这个函数在实战一中的一个函数是一样的，复述一遍，classCount定义为存储字典，每当，由于后面加了1，所以每次出现键值就加1，就可以就算出键值出现的次数里。最后通过sorted函数将classCount字典分解为列表，sorted函数的第二个参数导入了运算符模块的itemgetter方法，按照第二个元素的次序（即数字）进行排序，由于此处reverse=True，是逆序，所以按照从大到小的次序排列。\n让我们来测试一下\nimport numpy as np classList = np.array(myDat).T[-1]\nclassList\narray(['yes', 'yes', 'no', 'no', 'no'], dtype='|S21')\nmajorityCnt(classList) #the number of 'no' is 3, 'yes' is 2,so return 'no'\n‘no’\n接下来是创建决策树函数\n代码如下：\n1 def createTree(dataSet,labels): 2 classList = [example[-1] for example in dataSet] 3 if classList.count(classList[0]) == len(classList): 4 return classList[0]#stop splitting when all of the classes are equal 5 if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet 6 return majorityCnt(classList) 7 bestFeat = chooseBestFeatureToSplit(dataSet) 8 bestFeatLabel = labels[bestFeat] 9 myTree = {bestFeatLabel:{}} 10 del(labels[bestFeat]) #delete the best feature , so it can find the next best feature 11 featValues = [example[bestFeat] for example in dataSet] 12 uniqueVals = set(featValues) 13 for value in uniqueVals: 14 subLabels = labels[:] #copy all of labels, so trees don't mess up existing labels 15 myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels) 16 return myTree\n前面两个if语句是判断分类是否结束，当所有的类都相等时，也就是属于同一类时，结束再分类，又或特征全部已经分类完成了，只剩下最后的class，也结束分类。这是判断递归结束的两个条件。一般开始的时候是不会运行这两步的，先选最好的特征，使用 chooseBestFeatureToSplit函数得到最好的特征，然后进行分类，这里创建了一个大字典myTree，它将决策树的整个架构全包含进去,这个等会在测试的时候说，然后对数据集进行划分，用splitDataSet函数，就可以得到划分后新的数据集，然后再进行createTrees函数，直到递归结束。\n来测试一下\nmyTree = trees.createTree(myDat,labels)\nmyTree\n{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n再来说说上面没详细说明的大字典，myTree是特征是‘no surfacing’,根据这个分类，得到两个分支‘0’和‘1‘，‘0’分支由于全是同一类就递归结束里，‘1’分支不满足递归结束条件，继续进行分类，它又会生成它自己的字典，又会分成两个分支，并且这两个分支满足递归结束的条件，所以返回‘no surfacing’上的‘1’分支是一个字典。这种嵌套的字典正是决策树算法的结果，我们可以使用它和Matplotlib来进行画决策\n1.4 使用决策树执行分类\n这个就是将测试合成一个函数，定义为classify函数\n参考代码如下：\n1 def classify(inputTree,featLabels,testVec): 2 firstStr = inputTree.keys()[0] 3 secondDict = inputTree[firstStr] 4 featIndex = featLabels.index(firstStr) 5 key = testVec[featIndex] 6 valueOfFeat = secondDict[key] 7 if isinstance(valueOfFeat, dict): 8 classLabel = classify(valueOfFeat, featLabels, testVec) 9 else: classLabel = valueOfFeat 10 return classLabel\n这个函数就是一个根据决策树来判断新的测试向量是那种类型，这也是一个递归函数，拿上面决策树的结果来说吧。\n{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}，这是就是我们的inputTree，首先通过函数的第一句话得到它的第一个bestFeat，也就是‘no surfacing’，赋给了firstStr，secondDict就是‘no surfacing’的值，也就是 {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}，然后用index函数找到firstStr的标号，结果应该是0，根据下标，把测试向量的值赋给key，然后找到对应secondDict中的值，这里有一个isinstance函数，功能是第一个参数的类型等于后面参数的类型，则返回true，否则返回false，testVec列表第一位是1，则valueOfFeat的值是 {0: 'no', 1: 'yes'}，是dict，则递归调用这个函数，再进行classify，知道不是字典，也就最后的结果了，其实就是将决策树过一遍，找到对应的labels罢了。\n这里有一个小知识点，在jupyter notebook中，显示绿色的函数，可以通过下面查询它的功能，例如\nisinstance? #run it , you will see a below window which is used to introduce this function\n让我们来测试测试\ntrees.classify(myTree,labels,[1,0])\n‘no’\ntrees.classify(myTree,labels,[1,1])\n‘yes'\n1.5 决策树的存储\n构造决策树是很耗时的任务,即使处理很小的数据集, 如前面的样本数据, 也要花费几秒的时间 ,如果数据集很大,将会耗费很多计算时间。然而用创建好的决策树解决分类问题，可以很快完成。因此 ,为了节省计算时间,最好能够在每次执行分类时调用巳经构造好的决策树。\n解决方案：使用pickle模块存储决策树\n参考代码：\ndef storeTree(inputTree,filename): import pickle fw = open(filename,'w') pickle.dump(inputTree,fw) fw.close() def grabTree(filename): import pickle fr = open(filename) return pickle.load(fr)\n就是将决策树写到文件中，用的时候在取出来，测试一下就明白了\ntrees.storeTree(myTree,'classifierStorage.txt') #run it ,store the tree\ntrees.grabTree('classifierStorage.txt')\n{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n决策树的构造部分结束了，下面介绍怎样绘制决策树\n2. 使用Matplotlib注解绘制树形图\n前面我们看到决策树最后输出是一个大字典，非常丑陋，我们想让它更有层次感，更加清晰，最好是图形状的，于是，我们要Matplotlib去画决策树。\n2.1 Matplotlib注解\nMatplotlib提供了一个注解工具annotations,它可以在数据图形上添加文本注释。\n创建一个treePlotter.py文件来存储画图的相关函数\n首先是使用文本注解绘制树节点，参考代码如下：\n1 import matplotlib.pyplot as plt 2 3 decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\") 4 leafNode = dict(boxstyle=\"round4\", fc=\"0.8\") 5 arrow_args = dict(arrowstyle=\"<-\") 6 7 def plotNode(nodeTxt, centerPt, parentPt, nodeType): 8 createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',\\ 9 xytext=centerPt, textcoords='axes fraction',\\ 10 va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args ) 11 12 def createPlot1(): 13 fig = plt.figure(1, facecolor='white') 14 fig.clf() 15 createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses 16 plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode) 17 plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode) 18 plt.show()\n前面三行是定义文本框和箭头格式，decisionNode是锯齿形方框，文本框的大小是0.8，leafNode是4边环绕型，跟矩形类似，大小也是4，arrow_args是指箭头，我们在后面结果是会看到这些东西，这些数据以字典类型存储。第一个plotNode函数的功能是绘制带箭头的注解，输入参数分别是文本框的内容，文本框的中心坐标，父结点坐标和文本框的类型，这些都是通过一个createPlot.ax1.annotate函数实现的，create.ax1是一个全局变量，这个函数不多将，会用就行了。第二个函数createPlot就是生出图形，也没什么东西，函数第一行是生成图像的画框，横纵坐标最大值都是1，颜色是白色，下一个是清屏，下一个就是分图，111中第一个1是行数，第二个是列数，第三个是第几个图，这里就一个图，跟matlab中的一样，matplotlib里面的函数都是和matlab差不多。\n来测试一下吧\nreset -f #clear all the module and data\ncd 桌面/machinelearninginaction/Ch03\n/home/fangyang/桌面/machinelearninginaction/Ch03\nimport treePlotter import matplotlib.pyplot as plt\ntreePlotter.createPlot1()\n2.2 构造注解树\n绘制一棵完整的树需要一些技巧。我们虽然有 x 、y 坐标,但是如何放置所有的树节点却是个问题，我们必须知道有多少个叶节点,以便可以正确确定x轴的长度;我们还需要知道树有多少层，以便可以正确确定y轴的高度。这里定义了两个新函数getNumLeafs()和getTreeDepth()，以求叶节点的数目和树的层数。\n参考代码：\n1 def getNumLeafs(myTree): 2 numLeafs = 0 3 firstStr = myTree.keys()[0] 4 secondDict = myTree[firstStr] 5 for key in secondDict.keys(): 6 if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes 7 numLeafs += getNumLeafs(secondDict[key]) 8 else: numLeafs +=1 9 return numLeafs 10 11 def getTreeDepth(myTree): 12 maxDepth = 0 13 firstStr = myTree.keys()[0] 14 secondDict = myTree[firstStr] 15 for key in secondDict.keys(): 16 if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes 17 thisDepth = 1 + getTreeDepth(secondDict[key]) 18 else: thisDepth = 1 19 if thisDepth > maxDepth: maxDepth = thisDepth 20 return maxDepth\n我们可以看到两个方法有点似曾相识，没错，我们在进行决策树分类测试时，用的跟这个几乎一样，分类测试中的isinstance函数换了一种方式去判断，递归依然在，不过是每递归依次，高度增加1，叶子数同样是检测是否为字典，不是字典则增加相应的分支。\n这里还写了一个函数retrieveTree，它的作用是预先存储的树信息,避免了每次测试代码时都要从数据中创建树的麻烦\n参考代码如下\n1 def retrieveTree(i): 2 listOfTrees =[{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}, 3 {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}} 4 ] 5 return listOfTrees[i]\n这个没什么好说的，就是把决策树的结果存在一个函数中，方便调用，跟前面的存储决策树差不多。\n有了前面这些基础后，我们就可以来画树了。\n参考代码如下：\n1 def plotMidText(cntrPt, parentPt, txtString): 2 xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0] 3 yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1] 4 createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30) 5 6 def plotTree(myTree, parentPt, nodeTxt):#if the first key tells you what feat was split on 7 numLeafs = getNumLeafs(myTree) #this determines the x width of this tree 8 depth = getTreeDepth(myTree) 9 firstStr = myTree.keys()[0] #the text label for this node should be this 10 cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) 11 plotMidText(cntrPt, parentPt, nodeTxt) 12 plotNode(firstStr, cntrPt, parentPt, decisionNode) 13 secondDict = myTree[firstStr] 14 plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD 15 for key in secondDict.keys(): 16 if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes 17 plotTree(secondDict[key],cntrPt,str(key)) #recursion 18 else: #it's a leaf node print the leaf node 19 plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW 20 plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) 21 plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) 22 plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD 23 #if you do get a dictonary you know it's a tree, and the first element will be another dict 24 25 def createPlot(inTree): 26 fig = plt.figure(1, facecolor='white') 27 fig.clf() 28 axprops = dict(xticks=[], yticks=[]) 29 createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) 30 plotTree.totalW = float(getNumLeafs(inTree)) 31 plotTree.totalD = float(getTreeDepth(inTree)) 32 plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; 33 plotTree(inTree, (0.5,1.0), '') 34 plt.show()\n第一个函数是在父子节点中填充文本信息，函数中是将父子节点的横纵坐标相加除以2，上面写得有一点点不一样，但原理是一样的，然后还是在这个中间坐标的基础上添加文本，还是用的是 createPlot.ax1这个全局变量，使用它的成员函数text来添加文本，里面是它的一些参数。\n第二个函数是关键，它调用前面我们说过的函数，用树的宽度用于计算放置判断节点的位置 ,主要的计算原则是将它放在所有叶子节点的中间,而不仅仅是它子节点的中间，根据高度就可以平分坐标系了，用坐标系的最大值除以高度，就是每层的高度。这个plotTree函数也是个递归函数，每次都是调用，画出一层，知道所有的分支都不是字典后，才算画完。每次检测出是叶子，就记录下它的坐标，并写出叶子的信息和父子节点间的信息。plotTree.xOff和plotTree.yOff是用来追踪已经绘制的节点位置，以及放置下一个节点的恰当位置。\n第三个函数我们之前介绍介绍过一个类似，这个函数调用了plotTree函数，最后输出树状图，这里只说两点，一点是全局变量plotTree.totalW存储树的宽度 ,全 局变量plotTree.totalD存储树的深度，还有一点是plotTree.xOff和plotTree.yOff是在这个函数这里初始化的。\n最后我们来测试一下\ncd 桌面/machinelearninginaction/Ch03\n/home/fangyang/桌面/machinelearninginaction/Ch03\nimport treePlotter myTree = treePlotter.retrieveTree(0) treePlotter.createPlot(myTree)\n改变标签，重新绘制图形\nmyTree['no surfacing'][3] = 'maybe' treePlotter.createPlot(myTree)\n至此，用matplotlib画决策树到此结束。\n3 使用决策树预测眼睛类型\n隐形眼镜数据集是非常著名的数据集 , 它包含很多患者眼部状况的观察条件以及医生推荐的隐形眼镜类型 。隐形眼镜类型包括硬材质 、软材质以及不适合佩戴 隐形眼镜 。数据来源于UCI数据库 ,为了更容易显示数据 , 将数据存储在源代码下载路径的文本文件中。\n进行测试\nimport trees lensesTree = trees.createTree(lenses,lensesLabels) fr = open('lenses.txt') lensesTree = trees.createTree(lenses,lensesLabels) lenses = [inst.strip().split('\\t') for inst in fr.readlines()] lensesLabels = ['age' , 'prescript' , 'astigmatic','tearRate'] lensesTree = trees.createTree(lenses,lensesLabels)\nlensesTree\n{'tearRate': {'normal': {'astigmatic': {'no': {'age': {'pre': 'soft', 'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}}, 'young': 'soft'}}, 'yes': {'prescript': {'hyper': {'age': {'pre': 'no lenses', 'presbyopic': 'no lenses', 'young': 'hard'}}, 'myope': 'hard'}}}}, 'reduced': 'no lenses'}}\n这样看，非常乱，看不出什么名堂，画出决策树树状图看看\ntreePlotter.createPlot(lensesTree)\n这就非常清楚了，但还是有一个问题，决策树非常好地匹配了实验数据,然而这些匹配选项可能太多了，我们将这种问题称之为过度匹配（overfitting），为了减少过度匹配问题,我们可以裁剪决策树,去掉一些不必要的叶子节点。如果叶子节点只能增加少许信息, 则可以删除该节点, 将它并人到其他叶子节点中，这个将在后面讨论吧！\n结尾\n这篇notebook写了两天多，接近三天，好累，希望这篇关于决策树的博客能够帮助到你，如果发现错误，还望不吝指教，谢谢！\n觉得不错的，赐我金笔吧，哈哈，我需要鼓励鼓励，(^__^) 嘻嘻……\n百度云盘：链接: https://pan.baidu.com/s/1eSeRQIQ 密码: 3zwm"}
{"content2":"机器学习导论\n什么是机器学习\n机器学习的种类\n代价函数，优化目标\n模型泛化能力\n模型评估\n什么是机器学习\n机器学习是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。\n机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。\n机器学习的种类\n监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。\n无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类。\n半监督学习介于监督学习与无监督学习之间。\n增强学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。\n代价函数\n模型泛化能力\n模型评估\nPrecision (精确度)：检索出来的条目(比如:文档、网页等)有多少是准确的\nRecall (召回率、查全率)：所有准确的条目有多少被检索出来\n下面这张图介绍True Positive，False Negative等常见的概念，P和R也往往和它们联系起来。\n相关(Relevant),正类\n无关(NonRelevant),负类\n被检索到(Retrieved)\ntrue positives(TP 正类判定为正类 , 分明是A)\nfalse positives(FP 负类判定为正类,\"存伪\", 分明是B却判断为A)\n未被检索到(Not Retrieved)\nfalse negatives(FN 正类判定为负类,\"去真\" ,分明是A却判断为B)\ntrue negatives(TN 负类判定为负类 , 分明是B)\n其中false positives（存伪）也通常称作误报，false negatives也通常称作漏报！\nF-Measure是Precision和Recall加权调和平均：\n当参数a=1时，就是最常见的F1了：\n很容易理解，F1综合了P和R的结果，当F1较高时则比较说明实验方法比较理想"}
{"content2":"在机器学习和数据挖掘中，我们经常需要知道个体间差异的大小，进而评价个体的相似性和类别。最常见的是数据分析中的相关分析，数据挖掘中的分类和聚类算法，如 K 最近邻（KNN）和 K 均值（K-Means）等等。根据数据特性的不同，可以采用不同的度量方法。一般而言，定义一个距离函数 d(x,y), 需要满足下面几个准则：\n1) d(x,x) = 0                    // 到自己的距离为0\n2) d(x,y) >= 0                  // 距离非负\n3) d(x,y) = d(y,x)                   // 对称性: 如果 A 到 B 距离是 a，那么 B 到 A 的距离也应该是 a\n4) d(x,k)+ d(k,y) >= d(x,y)    // 三角形法则: (两边之和大于第三边)\n这篇博客主要介绍机器学习和数据挖掘中一些常见的距离公式，包括：\n闵可夫斯基距离\n欧几里得距离\n曼哈顿距离\n切比雪夫距离\n马氏距离\n余弦相似度\n皮尔逊相关系数\n汉明距离\n杰卡德相似系数\n编辑距离\nDTW 距离\nKL 散度\n1. 闵可夫斯基距离\n闵可夫斯基距离（Minkowski distance）是衡量数值点之间距离的一种非常常见的方法，假设数值点 P 和 Q 坐标如下：\n那么，闵可夫斯基距离定义为：\n该距离最常用的 p 是 2 和 1, 前者是欧几里得距离（Euclidean distance），后者是曼哈顿距离（Manhattan distance）。假设在曼哈顿街区乘坐出租车从 P 点到 Q 点，白色表示高楼大厦，灰色表示街道：\n绿色的斜线表示欧几里得距离，在现实中是不可能的。其他三条折线表示了曼哈顿距离，这三条折线的长度是相等的。\n当 p 趋近于无穷大时，闵可夫斯基距离转化成切比雪夫距离（Chebyshev distance）：\n我们知道平面上到原点欧几里得距离（p = 2）为 1 的点所组成的形状是一个圆，当 p 取其他数值的时候呢？\n注意，当 p < 1 时，闵可夫斯基距离不再符合三角形法则，举个例子：当 p < 1, (0,0) 到 (1,1) 的距离等于 (1+1)^{1/p} > 2, 而 (0,1) 到这两个点的距离都是 1。\n闵可夫斯基距离比较直观，但是它与数据的分布无关，具有一定的局限性，如果 x 方向的幅值远远大于 y 方向的值，这个距离公式就会过度放大 x 维度的作用。所以，在计算距离之前，我们可能还需要对数据进行 z-transform 处理，即减去均值，除以标准差：\n: 该维度上的均值\n: 该维度上的标准差\n可以看到，上述处理开始体现数据的统计特性了。这种方法在假设数据各个维度不相关的情况下利用数据分布的特性计算出不同的距离。如果维度相互之间数据相关（例如：身高较高的信息很有可能会带来体重较重的信息，因为两者是有关联的），这时候就要用到马氏距离（Mahalanobis distance）了。\n2. 马氏距离\n考虑下面这张图，椭圆表示等高线，从欧几里得的距离来算，绿黑距离大于红黑距离，但是从马氏距离，结果恰好相反：\n马氏距离实际上是利用 Cholesky transformation 来消除不同维度之间的相关性和尺度不同的性质。假设样本点（列向量）之间的协方差对称矩阵是  ， 通过 Cholesky Decomposition（实际上是对称矩阵 LU 分解的一种特殊形式，可参考之前的博客）可以转化为下三角矩阵和上三角矩阵的乘积：  。消除不同维度之间的相关性和尺度不同，只需要对样本点 x 做如下处理： 。处理之后的欧几里得距离就是原样本的马氏距离：为了书写方便，这里求马氏距离的平方）：\n下图蓝色表示原样本点的分布，两颗红星坐标分别是（3, 3），（2, -2）:\n由于 x， y 方向的尺度不同，不能单纯用欧几里得的方法测量它们到原点的距离。并且，由于 x 和 y 是相关的（大致可以看出斜向右上），也不能简单地在 x 和 y 方向上分别减去均值，除以标准差。最恰当的方法是对原始数据进行 Cholesky 变换，即求马氏距离（可以看到，右边的红星离原点较近）：\n将上面两个图的绘制代码和求马氏距离的代码贴在这里，以备以后查阅：\n1 # -*- coding=utf-8 -*- 2 3 # code related at: http://www.cnblogs.com/daniel-D/ 4 5 import numpy as np 6 import pylab as pl 7 import scipy.spatial.distance as dist 8 9 10 def plotSamples(x, y, z=None): 11 12 stars = np.matrix([[3., -2., 0.], [3., 2., 0.]]) 13 if z is not None: 14 x, y = z * np.matrix([x, y]) 15 stars = z * stars 16 17 pl.scatter(x, y, s=10) # 画 gaussian 随机点 18 pl.scatter(np.array(stars[0]), np.array(stars[1]), s=200, marker='*', color='r') # 画三个指定点 19 pl.axhline(linewidth=2, color='g') # 画 x 轴 20 pl.axvline(linewidth=2, color='g') # 画 y 轴 21 22 pl.axis('equal') 23 pl.axis([-5, 5, -5, 5]) 24 pl.show() 25 26 27 # 产生高斯分布的随机点 28 mean = [0, 0] # 平均值 29 cov = [[2, 1], [1, 2]] # 协方差 30 x, y = np.random.multivariate_normal(mean, cov, 1000).T 31 plotSamples(x, y) 32 33 covMat = np.matrix(np.cov(x, y)) # 求 x 与 y 的协方差矩阵 34 Z = np.linalg.cholesky(covMat).I # 仿射矩阵 35 plotSamples(x, y, Z) 36 37 # 求马氏距离 38 print '\\n到原点的马氏距离分别是：' 39 print dist.mahalanobis([0,0], [3,3], covMat.I), dist.mahalanobis([0,0], [-2,2], covMat.I) 40 41 # 求变换后的欧几里得距离 42 dots = (Z * np.matrix([[3, -2, 0], [3, 2, 0]])).T 43 print '\\n变换后到原点的欧几里得距离分别是：' 44 print dist.minkowski([0, 0], np.array(dots[0]), 2), dist.minkowski([0, 0], np.array(dots[1]), 2)\nView Code\n马氏距离的变换和 PCA 分解的白化处理颇有异曲同工之妙，不同之处在于：就二维来看，PCA 是将数据主成分旋转到 x 轴（正交矩阵的酉变换），再在尺度上缩放（对角矩阵），实现尺度相同。而马氏距离的 L逆矩阵是一个下三角，先在 x 和 y 方向进行缩放，再在 y 方向进行错切（想象矩形变平行四边形），总体来说是一个没有旋转的仿射变换。\n3. 向量内积\n向量内积是线性代数里最为常见的计算，实际上它还是一种有效并且直观的相似性测量手段。向量内积的定义如下：\n直观的解释是：如果 x 高的地方 y 也比较高， x 低的地方 y 也比较低，那么整体的内积是偏大的，也就是说 x 和 y 是相似的。举个例子，在一段长的序列信号 A 中寻找哪一段与短序列信号 a 最匹配，只需要将 a 从 A 信号开头逐个向后平移，每次平移做一次内积，内积最大的相似度最大。信号处理中 DFT 和 DCT 也是基于这种内积运算计算出不同频域内的信号组分（DFT 和 DCT 是正交标准基，也可以看做投影）。向量和信号都是离散值，如果是连续的函数值，比如求区间[-1, 1] 两个函数之间的相似度，同样也可以得到（系数）组分，这种方法可以应用于多项式逼近连续函数，也可以用到连续函数逼近离散样本点（最小二乘问题，OLS coefficients）中，扯得有点远了- -!。\n向量内积的结果是没有界限的，一种解决办法是除以长度之后再求内积，这就是应用十分广泛的余弦相似度（Cosine similarity）：\n余弦相似度与向量的幅值无关，只与向量的方向相关，在文档相似度（TF-IDF）和图片相似性（histogram）计算上都有它的身影。需要注意一点的是，余弦相似度受到向量的平移影响，上式如果将 x 平移到 x+1, 余弦值就会改变。怎样才能实现平移不变性？这就是下面要说的皮尔逊相关系数（Pearson correlation），有时候也直接叫相关系数:\n皮尔逊相关系数具有平移不变性和尺度不变性，计算出了两个向量（维度）的相关性。不过，一般我们在谈论相关系数的时候，将 x 与 y 对应位置的两个数值看作一个样本点，皮尔逊系数用来表示这些样本点分布的相关性。\n由于皮尔逊系数具有的良好性质，在各个领域都应用广泛，例如，在推荐系统根据为某一用户查找喜好相似的用户,进而提供推荐，优点是可以不受每个用户评分标准不同和观看影片数量不一样的影响。\n4. 分类数据点间的距离\n汉明距离（Hamming distance）是指，两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。举个维基百科上的例子：\n还可以用简单的匹配系数来表示两点之间的相似度——匹配字符数/总字符数。\n在一些情况下，某些特定的值相等并不能代表什么。举个例子，用 1 表示用户看过该电影，用 0 表示用户没有看过，那么用户看电影的的信息就可用 0,1 表示成一个序列。考虑到电影基数非常庞大，用户看过的电影只占其中非常小的一部分，如果两个用户都没有看过某一部电影（两个都是 0），并不能说明两者相似。反而言之，如果两个用户都看过某一部电影（序列中都是 1），则说明用户有很大的相似度。在这个例子中，序列中等于 1 所占的权重应该远远大于 0 的权重，这就引出下面要说的杰卡德相似系数（Jaccard similarity）。\n在上面的例子中，用 M11 表示两个用户都看过的电影数目，M10 表示用户 A 看过，用户 B 没看过的电影数目，M01 表示用户 A 没看过，用户 B 看过的电影数目，M00 表示两个用户都没有看过的电影数目。Jaccard 相似性系数可以表示为：\nJaccard similarity 还可以用集合的公式来表达，这里就不多说了。\n如果分类数值点是用树形结构来表示的，它们的相似性可以用相同路径的长度来表示，比如，“/product/spot/ballgame/basketball” 离“product/spot/ballgame/soccer/shoes” 的距离小于到 \"/product/luxury/handbags\" 的距离，以为前者相同父节点路径更长。\n5. 序列之间的距离\n上一小节我们知道，汉明距离可以度量两个长度相同的字符串之间的相似度，如果要比较两个不同长度的字符串，不仅要进行替换，而且要进行插入与删除的运算，在这种场合下，通常使用更加复杂的编辑距离（Edit distance, Levenshtein distance）等算法。编辑距离是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。编辑距离求的是最少编辑次数，这是一个动态规划的问题，有兴趣的同学可以自己研究研究。\n时间序列是序列之间距离的另外一个例子。DTW 距离（Dynamic Time Warp）是序列信号在时间或者速度上不匹配的时候一种衡量相似度的方法。神马意思？举个例子，两份原本一样声音样本A、B都说了“你好”，A在时间上发生了扭曲，“你”这个音延长了几秒。最后A:“你\n~\n~~好”，B：“你好”。DTW正是这样一种可以用来匹配A、B之间的最短距离的算法。\nDTW 距离在保持信号先后顺序的限制下对时间信号进行“膨胀”或者“收缩”，找到最优的匹配，与编辑距离相似，这其实也是一个动态规划的问题:\n实现代码（转自 McKelvin's Blog ）:\n1 #!/usr/bin/python2 2 # -*- coding:UTF-8 -*- 3 # code related at: http://blog.mckelv.in/articles/1453.html 4 5 import sys 6 7 distance = lambda a,b : 0 if a==b else 1 8 9 def dtw(sa,sb): 10 ''' 11 >>>dtw(u\"干啦今今今今今天天气气气气气好好好好啊啊啊\", u\"今天天气好好啊\") 12 2 13 ''' 14 MAX_COST = 1<<32 15 #初始化一个len(sb) 行(i)，len(sa)列(j)的二维矩阵 16 len_sa = len(sa) 17 len_sb = len(sb) 18 # BUG:这样是错误的(浅拷贝): dtw_array = [[MAX_COST]*len(sa)]*len(sb) 19 dtw_array = [[MAX_COST for i in range(len_sa)] for j in range(len_sb)] 20 dtw_array[0][0] = distance(sa[0],sb[0]) 21 for i in xrange(0, len_sb): 22 for j in xrange(0, len_sa): 23 if i+j==0: 24 continue 25 nb = [] 26 if i > 0: nb.append(dtw_array[i-1][j]) 27 if j > 0: nb.append(dtw_array[i][j-1]) 28 if i > 0 and j > 0: nb.append(dtw_array[i-1][j-1]) 29 min_route = min(nb) 30 cost = distance(sa[j],sb[i]) 31 dtw_array[i][j] = cost + min_route 32 return dtw_array[len_sb-1][len_sa-1] 33 34 35 def main(argv): 36 s1 = u'干啦今今今今今天天气气气气气好好好好啊啊啊' 37 s2 = u'今天天气好好啊' 38 d = dtw(s1, s2) 39 print d 40 return 0 41 42 if __name__ == '__main__': 43 sys.exit(main(sys.argv))\nView Code\n6. 概率分布之间的距离\n前面我们谈论的都是两个数值点之间的距离，实际上两个概率分布之间的距离是可以测量的。在统计学里面经常需要测量两组样本分布之间的距离，进而判断出它们是否出自同一个 population，常见的方法有卡方检验（Chi-Square）和 KL 散度（ KL-Divergence），下面说一说 KL 散度吧。\n先从信息熵说起，假设一篇文章的标题叫做“黑洞到底吃什么”，包含词语分别是 {黑洞, 到底, 吃什么}, 我们现在要根据一个词语推测这篇文章的类别。哪个词语给予我们的信息最多？很容易就知道是“黑洞”，因为“黑洞”这个词语在所有的文档中出现的概率太低啦，一旦出现，就表明这篇文章很可能是在讲科普知识。而其他两个词语“到底”和“吃什么”出现的概率很高，给予我们的信息反而越少。如何用一个函数 h(x) 表示词语给予的信息量呢？第一，肯定是与 p(x) 相关，并且是负相关。第二，假设 x 和 y 是独立的（黑洞和宇宙不相互独立，谈到黑洞必然会说宇宙）,即 p(x,y) = p(x)p(y), 那么获得的信息也是叠加的，即 h(x, y) = h(x) + h(y)。满足这两个条件的函数肯定是负对数形式：\n对假设一个发送者要将随机变量 X 产生的一长串随机值传送给接收者， 接受者获得的平均信息量就是求它的数学期望：\n这就是熵的概念。另外一个重要特点是，熵的大小与字符平均最短编码长度是一样的（shannon）。设有一个未知的分布 p(x), 而 q(x) 是我们所获得的一个对 p(x) 的近似，按照 q(x) 对该随机变量的各个值进行编码，平均长度比按照真实分布的 p(x) 进行编码要额外长一些，多出来的长度这就是 KL 散度（之所以不说距离，是因为不满足对称性和三角形法则），即：\nKL 散度又叫相对熵（relative entropy）。了解机器学习的童鞋应该都知道，在 Softmax 回归（或者 Logistic 回归），最后的输出节点上的值表示这个样本分到该类的概率，这就是一个概率分布。对于一个带有标签的样本，我们期望的概率分布是：分到标签类的概率是 1， 其他类概率是 0。但是理想很丰满，现实很骨感，我们不可能得到完美的概率输出，能做的就是尽量减小总样本的 KL 散度之和（目标函数）。这就是 Softmax 回归或者 Logistic 回归中 Cost function 的优化过程啦。（PS：因为概率和为 1，一般的 logistic 二分类的图只画了一个输出节点，隐藏了另外一个）\n待补充的方法：\n卡方检验 Chi-Square\n衡量 categorical attributes 相关性的 mutual information\nSpearman's rank coefficient\nEarth Mover's Distance\nSimRank 迭代算法等。\n参考资料：\n距离和相似性度量\nMachine Learning: Measuring Similarity and Distance\nWhat is Mahalanobis distance?\nCosine similarity, Pearson correlation, and OLS coefficients\n机器学习中的相似性度量\n动态时间归整 | DTW | Dynamic Time Warping"}
{"content2":"最近工作了一段时间，今天跟大家讨论一下关于“规则与模型”的问题。\n大家肯定都知道，机器学习方法主要分两类，一类是基于统计的方法，比如贝叶斯、KNN等，都是对数据的某种特征进行归类计算得到数据划分的依据的；另一类是基于规则的方法，比如，语义规则，语法规则或者业务规则等等，这些规则主要是根据数据本身的特征人为地对分类细节进行限定，没有什么太高深的理论，但是实用性很强。当然，我觉得还有第三种方法，就是基于统计与基于规则相结合的方法，比如关联规则，apriori方法，或者决策树方法。这些方法在分类或者求变量关系的时候本质上是基于规则分类，但是在训练过程中却是使用统计的方法的。比如apriori方法，该方法本身就是求规则的一种方法，但该方法训练的时候是需要对各种关联特征进行统计的。训练完毕后寻找强关联性的特征可以进行分类，因为我觉得找出哪些是强关联哪些不是强关联就是一个分类的过程。再比如决策树，同样的道理，决策树本身得到的那棵树就是一棵规则树，但是在寻找树的分裂属性上，无论是ID3还是C4.5，都是基于统计方法的。我认为这种通过训练出的规则进行统计分类的方法就是统计与规则相结合的方法。那么这三类方法哪些是比较有效的呢，或者说哪些是比较适合大部分数据的呢？\n我的个人理解是，没有。我们在学校里写paper，最主要的任务有三：第一，提高算法性能；第二，提高算法对数据的匹配度；第三，提高数据对算法的匹配度。举个例子来说，我用SVM对新浪微博文本做情感分析，要么改进SVM核心算法，提高分类精度；要么对SVM选取特征的方法或空间构造的方法进行改进，提高分类精度；要么就是直接改进数据特征提取方式，提高数据对分类器的匹配度。但是，不管是哪一种方法，都要满足最小泛化阈值。也就是说，对训练数据之外的测试数据的分类准确度一定要满足最低要求。我们把训练数据和测试数据的概念扩大一下，如果我们把要训练的某类数据作为训练数据，把该类别之外的数据作为测试数据，情况就完全不一样了。比如，我把微博数据作为训练数据，把短信类短文本数据作为测试数据，虽然同样都属于短文本，但泛化值出奇的低。这是因为我们无论采用哪一种方法，数据都必须是封闭的，或者说是相对封闭的。我们用分类器或者基于统计的方法，其实主要就是为了提高泛化能力，因为统计一个词的个数和一个数的个数的意义是一样的。但是现在看来，好像基于统计的方法的泛化能力还是有限。\n导致这种现象的原因是什么呢？我在刚开始学机器学习的时候就遇到一个很大的矛盾，既然SVM是最好的基于统计的分类方法，而基于统计的方法的目的就是为了提高泛化能力，为什么在使用SVM的时候还是需要去适应数据呢？在我理解，分类方法对数据适应的越多，规则成分就越多。我在最近做“技能词识别”的时候使用了各种规则，尝试了各种方法，然后我返现我犯了两个个错误，第一SVM是一种思想，不是一种具体的方法。思想本身无法使用，只有把它应用到实践才有价值；第二就是数据挖掘，或者机器学习的核心的重点永远是数据和思想的问题，没有方法的问题。因为我们选择的方法一定是要匹配数据的，目前来说这是根本；方法也是必须要符合分类某数据的核心思想的。由此我想出，无论是基于规则的方法还是基于统计的方法都是基础理论，单纯的使用基础理论是没有什么意义的，或者说只能满足很少一部分数据的。只有结合实际情况，结合多种基础理论，才能把机器学习运用到实际当中，因为我们的核心是数据，是实际情况。\n所以说，如果们分类的时候发现基于统计的方法远远不如基于规则的方法，或者反过来，并不表明哪一种方法好，而是说明某一种方法更适合目前要分析的数据。对完全不同数据类型的数据进行泛化我觉得目前来说不太现实，因为对于真实的人来说让一个人根本不懂英语的中国人去理解英文那是不可能的。当然，以后会发展到什么程度我是难以预料的，我的眼光目前也比较短浅看不到很深远的东西。但我觉得，如果某一天这种泛化能力实现了，人类将走向灭绝。\n对了，最后一点，我觉得将“基于统计的学习方法”和“基于规则的学习方法”改为“基于统计的学习思想”和“基于规则的学习思想”更好一些。\n可能会有逻辑不对的地方，欢迎批评指正！"}
{"content2":"转载至（https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650717606&idx=4&sn=b94b58d4fe75c1a1e42274720a269a99&scene=21#wechat_redirect），理解三者之间关系的最简便方法就是将它们视觉化为一组同心圆——首先是最大的部分人工智能——然后是后来兴旺的机器学习——最后是促使当下人工智能大爆发的深度学习——在最里层。\n从萧条到繁荣\n自从 1956 年几个计算机科学家在达特茅斯会议上聚集并开辟了人工智能这一领域，人工智能就进入了我们的想象，并在实验研究中进行着酝酿。在过去的几十年里，人工智能以及轮番被誉为人类文明取得最美好未来的关键，或者是作为一个头脑发烧的轻率概念被扔进了科技垃圾堆中。坦白说直到 2012 年，它就这样在二者之间交杂。\n过去的几年里，尤其从 2015 年开始，人工智能开始爆发了。这很大程度上与 GPU 的广泛应用有关，为了使并行处理更快、更便宜、更强大。这也与近乎无限的存储能力和各类数据洪流（所有的大数据运动）——图像、文本、交易、测绘数据，只要你说得出来——一道进行。\n让我们梳理一遍计算机科学家是如何从萧条——直到 2012 年——到繁荣，开发出每天由成千上百万的人使用的应用。\n人工智能——机器诠释的人类智能\n回到 1956 年夏天的那场会议，人工智能先驱们的梦想是借由新兴计算机构建具有人类智力特征的复杂机器。这就是所谓的「通用人工智能（General AI）」的概念——拥有人类的所有感觉（甚至可能更多）、所有理智，像人类一样思考的神奇机器。\n你已经在电影中无休止地看到过这些被我们当做朋友的机器，比如《星球大战》中的 C-3PO  以及成为人类敌人的机器——终结者。通用人工智能机器向来有充足的理由出现在电影和科幻小说中；我们不能阻止，至少现在还不行。\n我们能做什么？这就到了「狭义人工智能（Narrow AI）」的概念。指的是能够将特殊任务处理得同人类一样好，或者更好的技术。狭义人工智能的相关案例比如有 Pinterest 上的图像分类、Facebook 中的人脸识别。\n这些是狭义人工智能在实践中的例子。这些技术展示了人类智能的一些方面。但是如何做到的呢？那个智能来自哪里？所以接下来看第二个同心圆，机器学习。\n机器学习——实现人工智能的一种方式\n机器学习最基础的是运用算法来分析数据、从中学习、测定或预测现实世界某些事。所以不是手动编码带有特定指令设定的软件程序来完成某个特殊任务，而是使用大量的数据和算法来「训练」机器，赋予它学习如何执行任务的能力。\n机器学习直接源自早期那帮人工智能群体，演化多年的算法包括了决策树学习（decision tree learning）、归纳逻辑编程（inductive logic programming）。其他的也有聚类（clustering）、强化学习（reinforcement learning）和贝叶斯网络（Bayesian networks）等。我们知道，这些早期机器学习方法都没有实现通用人工智能的最终目标，甚至没有实现狭义人工智能的一小部分目标。\n事实证明，多年来机器学习的最佳应用领域之一是计算机视觉，尽管它仍然需要大量的手工编码来完成工作。人们会去写一些手写分类器，像是边缘检测过滤器（edge detection filters）使得程序可以识别对象的启止位置；形状检测（shape detection）以确定它是否有八条边；一个用来识别单词「S-T-O-P」的分类器。从这些手写分类器中他们开发出能够理解图像的算法，「学习」判定它是否是一个停止标志。\n这很好，但还不够好。特别是有雾天气标志不完全可见的情况下，或者被树遮住了一部分。计算机视觉和图像检测直到目前都不能与人类相媲美，是因为它太过脆弱，太容易出错了。\n是时间和正确的学习算法改变了这一切。\n深度学习——一种实现机器学习的技术\n源自最早进行机器学习那群人的另一种算法是人工神经网络（Artificial Neural Networks），它已有几十年的历史。神经网络的灵感来自于我们对大脑生物学的理解——所有神经元之间的相互连接。但是不像生物大脑中的任何神经元，可以在一定的物理距离内连接到任何其他神经元，这些人工神经网络的层、连接和数据传播方向是离散的。\n比如你可以把一个图像切成一堆碎片并输入到神经网络的第一层中。然后第一层的单个神经元们将数据传递给第二层。第二层神经元将数据传给第三层，如此一直传到最后一层并输出最终结果。\n每个神经元分配一个权重到它的输入——评估所执行的任务的准确或不准确。然后最终的输出由所有这些权重来确定。所以想想那个停止标志的例子。一个停止标志图像的特征被切碎并由神经元来「检查」——它的形状、它的消防红色彩、它的独特字母、它的交通标志尺寸以及和它的运动或由此带来的缺失。神经网络的任务是判定它是否为一个停止标志。这提出了一个「概率向量」，它真是一个基于权重的高度受训的猜测。在我们的例子中，系统可能有 86% 的把握认为图像是一个停止标志，7% 的把握认为这是一个限速标志，5% 的把握认为这是一只被卡在树上的风筝，等等——然后网络架构告诉神经网络结果的正确与否。\n甚至这个例子都有些超前了，因为直到现在，神经网络都被人工智能研究社区避开了。自从最早的人工智能起，他们一直在做这方面研究，而「智能」成果收效甚微。问题很简单，即最基本的神经网络属于计算密集型，这并不是一个实用的方法。不过，由多伦多大学的 Geoffrey Hinton 带领的异端研究小组一直在继续相关研究工作，最终在超级计算机上运行并行算法证明了这个概念，但这是直到 GPU 被部署之后才兑现的诺言。\n如果我们再回到停止标志的例子，当网络正在进行调整或者「训练」时，出现大量的错误答案，这个机会是非常好的。它需要的就是训练。它需要看到成千上万，甚至数以百万计的图像，直到神经元的输入权重被精确调整，从而几乎每一次都能得到正确答案——无论有雾没雾，晴天还是雨天。在这一点上，神经网络已经教会了自己停止标志看起来会是什么样的；或者在 Facebook 例子中就是识别妈妈的脸；或者吴恩达 2012 年在谷歌所做的猫的图片。\n吴恩达的突破在于从根本上使用这些神经网络 并将它们变得庞大，增加了层数和神经元的数量，然后通过系统运行大量的数据来训练它。吴恩达使用了 1000 万个 YouTube 视频的图像。他将「深度」运用在深度学习中，这就描述了这些神经网络的所有层。\n如今，在一些场景中通过深度学习训练机器识别图像，做得比人类好，从识别猫咪到确定血液中的癌症指标和磁共振成像扫描中的肿瘤指标。谷歌的 AlphaGo 学会了游戏，并被训练用于 Go 比赛。通过反复与自己对抗来调整自己的神经网络。\n感谢深度学习，让人工智能有一个光明的未来。\n深度学习 已经实现了许多机器学习方面的实际应用和人工智能领域的全面推广。深度学习解决了许多任务让各种机器助手看起来有可能实现。无人驾驶机车、更好的预防医疗，甚至是更好的电影推荐，如今都已实现或即将实现。人工智能在当下和未来。有了深度学习，人工智能甚至可以达到我们长期所想象的科幻小说中呈现的状态。我拿走你的 C-3PO，你可以留着终结者。"}
{"content2":"今天在研究点云分割的时候终于走完了所有的传统路子，走到了基于机器学习的分割与传统自底向上分割的分界点（CRF）算法。好吧，MIT的老教授说的对，其实你很难真正绕过某个问题，数学如是，人生也如是。\n---记我的机器学习之路\n1、机器学习\n在之前的学习过程中，机器学习对我而言实在是洪水猛兽般的存在。很多玄玄乎乎的公式，算法，各种算法的名字一看就比较高级；如黑箱一般的过程；摸不清的物理意义；繁杂的公式实在是让人头大。为了能更好的学习PGM，我决定放弃由从神经网络或者深度学习一类的算法入手的打算，改由统计，概率与推断入手，来学习Learning. 实际上如果你只看中利用机器学习来做什么，那么完全不需要头疼它到底是怎么Learning的，Learning已经成了一个抽象类，对所有的问题都可以用：\nLearn graph_learn(new learn::graph_learn); learn.train; learn.app; learn.result;\n的方法来完成。至于how to train,how to extract result，显然已经被各路大神封装的妥妥的了。但是如果真的想要从机器学习中获得启发或者是将其与自己的专业结合的更紧密，可能还是要对这个东西开膛破肚吧。\n2.极大似然估计\n估计（estimating）是一种手段，一种对模型参数进行推测的手段，说白了，就是利用训练数据对模型进行calibration. 在标定之前，首先需要有一个模型。模型中需要有待辨识的参数。极大似然估计是 现象--->原理 的过程，这个世界上已经发生的，都必然会发生（given by Kang.YH_HUST)。遵循这个原理，求解模型参数就成了一个寻优的过程。当给定模型与结果，时间发生的概率p是参数a的函数。其原理如下：\nf(x1, . . . , xn; θ) = ∏ fθ(xj ; θ).\nL(θ; x1, . . . , xn) = f(x1, . . . , xn; θ).\nθ = argmaxθ L(θ; x1, . . . , xn).\n由于模型是已知的，那么对给定观测值，其对应概率p是可以表达的。根据此原理可以求解以下两个问题：\n1.如果θ是二项分布的参数，那么在给定一组结果的情况下，似然函数L可表达为：\n对似然函数求导，可得当θ=h/n时似然函数取最大值。h是x=1的次数，n是实验总数。\n2.如果L函数是线性的，那么可以对L函数取对数，取完对数后结果是一样的。\n3.条件似然估计\n说到条件概率，好像事情就变得有点复杂， y|x 总是给人一种先有x后有y的感觉，但其实x,y并不是先有鸡后有蛋的关系，x,y应该理解为y受到x的约束，会随着x的改变而改变y的取值。条件所表达的意义在于约束，而非顺序。 如果用约束来解释这个关系，那么接下来的事情就好理解了：\n1、如果 y 受到 x(向量）的约束，y的取值是随着x变化的。\n2、为简化约束，假定：x 各个分量对 y 的约束是线性组合的，不同的分量有不同的权重。\n上面是一个简单粗暴的模型，x 的各个分量可以看作是 x的不同特征。\n比如：点云颜色，点云密度，点云曲率 都可以看作是对 该是否该出现（0 1问题）有影响，那么就可以设计基于条件概率的点云滤波器。\n有了上述概念以后，就需要把它量化，有两点需要量化：\n1.x的加权和在哪个范围。\n2.如何让y的概率落在0~1。\n显然 logistic regression model 能够很好的解决量化问题：\n对 x 各个部分的权重而言，取值可以从 负无穷到正无穷；\n对 y 的概率而言，取值用于在0~1之间。\n这个系统中，需要辨识的参数就仅仅是beta(alpha可以看作是beta0,x0=1)，要辨识beta并不是一件容易的事情，对所有给定x，我需要算出它对应的p，并把表达式相乘，再优化求解。显然这是笨办法，先对L(beta)函数做些处理才是上策。\n因为对Y而言，其为二项分布（丢硬币要么正面要么反面，但是硬币可以有bias，这个bias可以由很多事情决定），二项分布的极大似然估计是已知的：\n（似然函数取了对数）\n如果等式两项同时向beta求导，则有：\np不仅仅是p，p还是beta的函数：，那么则有：\nOK,好像令上式为0,则可求出beta对xij,yij的表达式。。。。。然而却没有那么简单。\n3、学习，机器也抄近道\n这个东西不那么简单的原因在于pi是一个非常复杂的表达式：。虽然任意一个beta都是独立的，但是要把所有的xi全部代进去，再求最小值也没那么简单，虽然这个最小值肯定存在，但如果没有beta，你也算不出来啊。。。陷入了该先拿身份证还是先开锁的困境中。\n所谓的学习（training），本质上是一种数值求解方法。一般情况下，可以表达为梯度下降法，它是这样的：\n在已经假设了beta(向量）的初始值情况下，beta(j) 的梯度是已经得知了的。那么对 整个训练集（i）全部遍历一遍就可以算出梯度，由梯度再刷新Beta_j。伪代码大约如下：\nwhile ( all_beta != coverge)\nfor j = 1:end_of_features\nfor i = 1:end_of_training_set\nPartial_Beta_j += (yi-pi)xij\nend beta(j) = beta(j) + lamda*Partial_Beta_j end\nend\n显然，这很不科学，对每个beta(j)我都需要遍历整个训练集，最后才改变了一点点。。。。。然后我又要遍历整个训练集。。。。。如果训练集很大，那岂不是坑爹。\n所以我们需要一个抄近道的方法，成为随机梯度下降法。这个方法的原理其实很简单：训练集包含的信息是一致的。\n不是一样的，是一致的。假设我们能找到一个随机变量Z，使Z满足：\n那么，只要Z的取值次数够多，我们就可以利用Zj来替代E[Zj](取足够多次，那么他们最终效果的相同的）\n不妨取。由于n是一个常数（训练集的规模）。那么Beta(j)也就变成了：\n终于，整个训练过程被简化成了：\n1.for epcho = 1:3 2. for i = 1:end_of_training_set 3. for j = 1:end_of_features 　　　　 　 　 Partial_Beta_j += (yi-pi)xij beta(j) = beta(j) + lamda*Partial_Beta_j 　　 end end train_set.rand_rank(); end\n其中，epcho被用作强制收敛，一般是3~100。第二个 for 和第三个 for 是可以对调位置的。据说会导致收敛变慢，该方法叫做cordianate ascent.\nOk,学习的问题基本上解决好了，还抄了点近道，想想就有些小开心~接下来还要解决1个扫尾的问题，搞定了的话我们也算玩过机器学习啦啦啦啦~~~\n问题：如果训练集合处处点云密度 为 1，且只要这个特征出现，点云就被认为是 无效的，那么随着训练的增加beta就会一直上升，直到无穷大；\n当然，当训练集非常合理，特征量化也非常合理的时候上面的那个问题不会出现，但显然不会如此十全十美。\n所以简单粗暴的引入一个惩罚因子，来防止beta跑飞.\nu是常数，当beta过大或者过小的时候惩罚因子能起到纠偏的作用，但结果会带来误差。\n4、将简单的机器学习用于点云滤波\n显然上式我们已经得到了beta 关于xij,yi的表达式，接下来要做的很简单：\n1.量化曲率x1，颜色x2，密度x3（根据经验，都要在-1~1之间，方差最好为1）。\n2.指定一些点，是，或者不是某点云的点y（0/1)\n3.训练。\n我们就得到了一个判断某点是否属于该点云的一个简单“判别器”（本质上就是个概率函数），当遇到某个点时，可以算它是有效点还是无效点的概率。\n其实机器学习没有那么难~也没有“黑”~~~~"}
{"content2":"4. Neural Networks (part one)\nContent:\n4. Neural Networks (part one)\n4.1 Non-linear Classification.\n4.2 Neural Model(神经元模型)\n4.3 Forward Propagation\n4.4 神经网络实现与或非门以及异或门\n4.4.1 实现与或非门(AND/OR/NOT)\n4.4.2 实现异或/同或门(XOR/XNOR)\n4.5 Multi-class classification\nkey words: Neural networks, Neural model, Forward Propagation\n4.1 Non-linear Classification\n对于非线性的分类问题（如-1所示的非线性0-1分类问题），在特征量较少的情况，我们可以用多项式类型的Logistic回归来处理。但是一旦特征量较多，多项式的Logistic回归就会很困难。因为如果问题原有n个特征量，采用二次多项式特征量个数约等于(n^2)/ 2，也就是O(n^2)，而用三次多项式特征量的个数更是O(n^3)，等等。当n较大时(如n > 1000)，计算机无法承受这么大的向量运算。所以需要一种新的模型（算法）来处理特征量较大的非线性分类问题。\n-1 非线性0-1分类问题\n那么什么问题会有较多的特征量? 计算机视觉(Computer Vision)领域就常常会遇到。我们知道，对于人类所看到的一张图片，在计算机里是以矩阵存储的。如-2所示，以要判别一张图片是否为汽车的问题为例，我们有一张像素为50*50的图片，即至少一共有2500个像素点()对于RGB的图片有7500个)，所以特征量的个数n = 2500，如果用二次多项式预测，那么特征量个数将变成近3百万！这样计算代价太大了。下面将介绍一种新的模型-神经网络(Neural Networks)，可以不需要通过增加特征量个数来解决非线性分类问题（当然它在其他问题也有应用）。\n-2 判断一张图片是否为汽车\n4.2 Neural Model(神经元模型)\n神经网络是一种模拟大脑的算法。一种较正规的定义是\n神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。\n神经网络中最基本的成分是神经元模型(Neural Model)(又称Logistic unit)，即上述定义中的“简单单元”。对于人类而言，我们的视觉听觉是由大脑的神经中枢产生。而神经中枢是由大量的神经元相互连接而成。一个神经元通过树突接受其他神经元传来的化学物质（信息），从而改变该神经元的电位，当电位到达某一阙值(threshold)时，该神经元被激活，即“兴奋”起来，从而通过轴突向其他神经元发送化学物质，如-3所示。\n-3 生物神经系统中的神经元\n而神经元模型便是模拟上述的神经元接受信息并传递信息的过程。如-4所示，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阙值进行比较，再通过激活函数(activation function)处理以产生神经元的输出。\n-4 从阙值角度理解的神经元模型\n理想中的激活函数是-5(a)所示的阶跃函数，它将输入值映射为输出值“0”或“1”, “1”对应神经元兴奋，“0”对应神经元抑制。但是，阶跃函数具有不连续，不光滑（不连续可导）等不太好的性质，因此实际中常用Logistic回归中应用到的sigmoid函数作为激活函数。典型的sigmoid函数如-5(b)所示，它把可能在较大范围内变化的输入值挤压到(0, 1)输出值范围内，因此有时又称之为“挤压函数”(squashing function).\n-5 典型的神经元激活函数（图片来自《机器学习》， 周志华）\n-6 从偏移单元角度理解的神经元模型\n4.3 Forward Propagation\n4.2节已经学习了神经网络的最基本成分-神经元模型，下面将介绍如何用神经元搭建多层前馈神经网络(multi-layer feedforward neural)和Forward Propagation算法。\n如-7所示，多次前馈神经网络有三部分组成，分别是输入层(input layer)，隐藏层(hide layer)，输出层(output layer)。隐藏层可以有，也可以没有，输入层和输出层必须要有。没有隐藏层的神经网络是线性的，只能处理线性可分的问题（线性可分问题从二维的角度就是分界线是一条直线，多维就是存在线性超平面将其分类）。一个没有隐藏层且输出层只有一个单元的神经网络就相当于线性的Logistic模型。\n-7 一个含两层隐藏层的多次前馈神经网络\n前向传播(Forward Propagation)算法就是利用已经训练出的连接权重(或称映射权重)和4.2节神经元模型中的输出公式(1)来计算出每一层每一个神经元的激活值(activation)，最终得到输出层的激活值，也就是输出值.\n-8给出了在一个具体的神经网络使用前向传播算法的例子，其中，激活函数是sigmoid函数g(x);\n-8 一个使用前向传播算法(向量实现)的例子\n4.4 神经网络实现与或非门以及异或门\n4.4.1 实现与或非门(AND/OR/NOT)\n4.4.2 实现异或/同或门(XOR/XNOR)\n在4.4.1小节中发现，实现与或非门只需要输入层和输出层，不需要隐藏层，也就是说与或非问题是线性可分的。但是，异或/同或却是非线性可分的，如-9所示。\n-9 异或/同或问题\n在数字逻辑中我们知道可以利用与或非门搭出异或/同或门，那是因为有如下运算法则，\na XOR b = ((NOT a) AND b) OR (a AND (NOT b));\na XNOR b = NOT (a XOR b) = (a AND b) OR ((NOT a) AND (NOT b))\n既然我们用神经网络实现了与或非门，那么也有理由可以实现异或和同或门，-10以实现同或门为例(只需在同或门后加上一个非门实现了异或门)。\n-10 实现同或门\n4.5 Multi-class classification\n对于多分类问题，在2.6节已经用Logistic回归模型讨论过了。现在用神经网络来处理。假设我们需要识别一张图片是行人，汽车，摩托车，还是卡车，也就是有4种类别。所以我们设计如-10所示的神经网络。由于一共有4类，所以该神经网络有4个输出单元，分别将其标号为1，2，3，4，对应行人，汽车，摩托车，卡车。每次预测输出的是一个4维向量。所以我们的训练集相比于Logistic回归模型要做改变，即每一个样例的结果y(i)也是一个4维向量且是[1 0 0 0 ]’, [0 1 0 0]’, [0 0 1 0]’, [0 0 0 1]’中的一个。至于预测时的结果分析和Logistic回归模型中使用的One-vs-all类似，不再重复。仅给出例子如下：\n若h = [0.12 0.71 0.13 0.45]’，则就相当于[0 1 0 0]’，故为第二个输出单元的标记，也就是汽车；\n若h =[0.12 0.64 0.83 0.21]’, 则就相当于[0 1 1 0]’，此时我们选择置信度最大的，也就是max h = 0.83，即第三个输出单元的标记，也就是摩托车。\n不管怎样，都是选择向量中最大的一个元素的标记(下标)，即matlab中的max(h, [], 2).\n-11 一个处理4分类问题的神经网络\n参考：\n《机器学习》 周志华"}
{"content2":"箱型图对数据的展示也是非常清晰的，这是箱型图的一些代码\n#导报 机器学习三剑客\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nif __name__ == \"__main__\":\n#读取数据 第一个参数是文件名 ， 第二个是文件页面\ndf = pd.read_excel('tips.xlsx','sheet1')\n# print(df)\n#调查小费比例与用时段关系\ndf[['pct','time']].boxplot(by='time')\nplt.show()"}
{"content2":"【重磅】微软开源自动机器学习工具 - NNI\n在机器学习建模时，除了准备数据，最耗时耗力的就是尝试各种超参组合，找到模型最佳效果的过程了。即使是对于有经验的算法工程师和数据科学家，有时候也很难把握其中的规律，只能多次尝试，找到较好的超参组合。而对于初学者来说，要花更多的时间和精力。\n自动机器学习这两年成为了热门领域，着力解决超参调试过程的挑战，通过超参选择算法和强大的算力来加速超参搜索的过程。\nNNI (Neural Network Intelligence) 是微软开源的自动机器学习工具。与当前的各种自动机器学习服务或工具相比，有非常独特的价值。本文先介绍一下 NNI 的特点，然后在后续的安装、使用章节详细介绍如何上手。\n支持私有部署。云服务中的自动机器学习直接提供了自动机器学习的服务，不仅包含了自动机器学习的功能，也包含了算力。如果团队或个人已经有了很强的算力资源，就需要支持私有部署的自动学习工具了。\nNNI 支持私有部署。整个部署也很简单，使用 pip 即可完成安装。\n分布式调度。NNI 可以在单机上完成试验，也支持以下两种分布式调度方案：\nGPU 远程服务器。通过 SSH 控制多台 GPU 服务器协同完成试验，并能够计划每个试验所需要的 GPU 的数量。\nOpenPAI。通过 OpenPAI，NNI 的试验可以在独立的 Docker 中运行，支持多样的实验环境。在计算资源规划上，不仅能指定 GPU 资源，还能制定 CPU，内存资源。\n超参搜索的直接支持。当前，大部分自动机器学习服务与工具都是在某个任务上使用，比如图片分类。这样的好处是，普通用户只要有标记数据，就能训练出一个高质量的平台，不需要任何模型训练方面的知识。但这需要对每个训练任务进行定制，将模型训练的复杂性包装起来。\n与大部分现有的自动机器学习服务与工具不同，NNI 需要用户提供训练代码，并指定超参的搜索范围。这样的好处在于，NNI 几乎是通用的工具，任何训练任务都可以使用 NNI 来进行超参搜索。但另一方面，NNI 的通用性，也带来了一定的使用门槛。使用 NNI 需要有基本的模型训练的经验。\n兼容已有代码。NNI 使用时，可以通过注释的方法来进行无侵入式的改动。不会影响代码原先的用途。通过注释方式支持 NNI 后，代码还可以单独运行。\n易于扩展。NNI 的设计上有很强的可扩展性。通过下面这些扩展性，能将系统与算法相隔离，把系统复杂性都包装起来。\nTuner 接口，可以轻松实现新的超参调试算法。研究人员可以使用 NNI 来试验新的超参搜索方法，比如在强化学习时，在 Tuner 中支持 off-policy 来探索比较好的超参组合，在 Trial 里进行 on-policy 的实际验证。也可以使用 Tuner 和训练代码相配合，支持复杂的超参搜索方法。如，实现 ENAS ，将 Tuner 作为 Control，在多个 Trial 中并行试验。\nAccessor 接口，可以加速参数搜索，将表现不好的超参组合提前结束。\nNNI 还提供了可扩展的集群接口，可以定制对接的计算集群。方便连接已经部署的计算集群。\n可视化界面。在启动一次超参搜索试验后，就可以通过可视化界面来查看试验进展，并帮助超参结果，洞察更多信息。\n首页，可以看到当前试验的进展情况，搜索参数和效果最好的一些超参组合。\n优化进度页面可以看到按时序排列的精度或损失值（此图为精度）。可以看到，时间越靠后（右侧），精度高的越多。这说明选择的超参探索算法随着时间能找到一些好的超参空间继续探索。\n通过超参的分布图来直观地看到哪些超参值会明显比较好，或者看出它们之间的关联。通过下面的颜色图就能直观地看到红色（即精度较高的超参组合）线条所表达的丰富信息。如：\n卷积核大一些会表现较好。\n全连接层大了不一定太好。也许是所需要的训练时间增加了，训练速度太慢造成的。\n而学习率小一些（小于0.03），表现基本都不错。\nReLU 比 tanh 等其它激活函数也好不少。\n...\n通过试验状态页面，能看到每个试验的时间长度以及具体的超参组合。\n通过控制页面还可以实时的增加试验的超参组合，或者调整超参的范围。\n最后，再贴一次地址：https://github.com/microsoft/nni\n使用方法和更多详情，可参考 GitHub 的官网，有问题或 bug 可以直接提 Issue。"}
{"content2":"在机器学习的实践中，我们通常会遇到实际数据中正负样本比例不平衡的情况，也叫数据倾斜。对于数据倾斜的情况，如果选取的算法不合适，或者评价指标不合适，那么对于实际应用线上时效果往往会不尽人意，所以如何解决数据不平衡问题是实际生产中非常常见且重要的问题。\n什么是类别不平衡问题\n我们拿到一份数据时，如果是二分类问题，通常会判断一下正负样本的比例，在机器学习中，通常会遇到正负样本极不均衡的情况，如垃圾邮件的分类等；在目标检测SSD中，也经常遇到数据不平衡的情况，检测器需要在每张图像中评价一万个到十万个候选位置，然而其中只有很少的点真的含有目标物体。这就导致了训练效率低下和简单的负面样本引发整个模型表现下降的问题。\n如何解决不平衡问题\n1. 从数据角度\n主动获取：获取更多的少量样本数据\n针对少量样本数据，可以尽可能去扩大这些少量样本的数据集，或者尽可能去增加他们特有的特征来丰富数据的多样性。譬如，如果是一个情感分析项目，在分析数据比例时发现负样本（消极情感）的样本数量较少，那么我们可以尽可能在网站中搜集更多的负样本数量。\n算法采样：上采样、下采样、生成合成数据\nADASYN采样方法：\nADASYN为样本较少的类生成合成数据，其生成的数据与更容易学习的样本相比，更难学习。基本思想是根据学习难度的不同，对不同的少数类的样本使用加权分布。其中，更难学习的少数类的样本比那些更容易学习的少数类的样本要产生更多的合成数据。因此，ADASYN方法通过以下两种方式改善了数据分布的学习：(1)减少由于类别不平衡带来的偏差；(2)自适应地将分类决策边界转移到困难的例子。\nSMOTE采样方法:\n从少数类创建新的合成点，以增加其基数。但是SMOTE算法也有一定的局限性。具体有两项，一是在近邻选择时，存在一定的盲目性。在算法执行过程中，需要确定Ｋ值，即选择几个近邻样本，这个需要根据具体的实验数据和实验人自己解决。二是该算法无法克服非平衡数据集的数据分布问题，容易产生分布边缘化的问题。由于负类样本的分布决定了其可选择的近邻，如果一个负类样本处在负类样本的边缘，则由此负类样本和近邻样本产生的样本也会处在边缘，从而无法确定正负类的分类边界。下图是以前做的一个项目应用个各种采样方法做数据增强的情况。（效果不明显，因为原始数据的分布重合太明显，可视化不容易显示出效果）\n\n\n\n数据增强：加噪音增强模型鲁棒性、对不同性质的数据也可以做不同的augmentation\n改变权重：设定惩罚因子，如libsvm等算法里设置的正负样本的权重项等。惩罚多样本类别，其实还可以加权少样本类别\n注意：在选择采样法事需要注意一个问题，如果你的实际数据是数据不平衡的，在训练模型时发现效果不好，于是采取了采样法平衡的数据的比例再来进行训练，然后去测试数据上预测，这个时候算法的效果是否会有偏差呢？此时你的训练样本的分布与测试样本的分布已经发生了改变，这样做反而会产生不好的效果。在实际情况中，我们尽可能的需要保持训练和测试的样本的概率分布是一致的，如果测试样本的分布是不平衡的，那么训练样本尽可能与测试样本的分布保持一致，哪怕拿到手的是已经清洗和做过预处理后的平衡的数据。具体原因感兴趣的可以仔细思考一下。\n2.从评价指标角度\n谨慎选择AUC作为评价指标：对于数据极端不平衡时，可以观察观察不同算法在同一份数据下的训练结果的precision和recall，这样做有两个好处，一是可以了解不同算法对于数据的敏感程度，二是可以明确采取哪种评价指标更合适。针对机器学习中的数据不平衡问题，建议更多PR(Precision-Recall曲线)，而非ROC曲线，具体原因画图即可得知，如果采用ROC曲线来作为评价指标，很容易因为AUC值高而忽略实际对少两样本的效果其实并不理想的情况。\n不要只看Accuracy：Accuracy可以说是最模糊的一个指标了，因为这个指标高可能压根就不能代表业务的效果好，在实际生产中，我们可能更关注precision/recall/mAP等具体的指标，具体侧重那个指标，得结合实际情况看。\n3.从算法角度\n选择对数据倾斜相对不敏感的算法。如树模型等。\n集成学习（Ensemble集成算法）。首先从多数类中独立随机抽取出若干子集，将每个子集与少数类数据联合起来训练生成多个基分类器，再加权组成新的分类器，如加法模型、Adaboost、随机森林等。\n将任务转换成异常检测问题。譬如有这样一个项目，需要从高压线的航拍图片中，将松动的螺丝/零件判断为待检测站点，即负样本，其他作为正样本，这样来看，数据倾斜是非常严重的，而且在图像质量一般的情况下小物体检测的难度较大，所以不如将其转换为无监督的异常检测算法，不用过多的去考虑将数据转换为平衡问题来解决。\n目标检测中的不平衡问题的进展\n1.GHM_Detection\n论文：https://arvix.org/pdf/1811.05181.pdf\ngithub：https://github.com/libuyu/GHM_Detection\n本文是香港中文大学发表于 AAAI 2019 的工作，文章从梯度的角度解决样本中常见的正负样本不均衡的问题。从梯度的角度给计算 loss 的样本加权，相比与 OHEM 的硬截断，这种思路和 Focal Loss 一样属于软截断。\n文章设计的思路不仅可以用于分类 loss 改进，对回归 loss 也很容易进行嵌入。不需要考虑 Focal Loss 的超参设计问题，同时文章提出的方法效果比 Focal Loss 更好。创新点相当于 FL 的下一步方案，给出了解决 class-imbalance 的另一种思路，开了一条路，估计下一步会有很多这方面的 paper 出现。\n2.Focal Loss for Dense Object Detection\n论文：\nFocal Loss：https://arxiv.org/abs/1708.02002\nRetinaNet：https://github.com/unsky/RetinaNet\ngithub：https://github.com/unsky/focal-loss\n本文通过重塑标准交叉熵损失来解决这一类不平衡问题。他们的想法是降低简单的负面样本所占的权重，所以他们提出的焦点损失（Focal Loss）方法将训练集中在一系列难点上，并且防止了大量的简单负面例子在训练过程中阻碍探测器学习。如上图，参数 γ 的值选择得越大，模型就会对已经得到了很好的分类的样本忽略得越多，越专注于难的样本的学习。这样的机制就让他们的检测器在密集对象检测这样的真实正面样本比例很低的情况下取得了很高的准确率。对于应对样本不平衡问题的关键方法“焦距损失”，作者们在论文中还提出了两种不同的表现形式，都起到了很好的效果.\n3.在线困难样例挖掘(online hard example mining, OHEM)\n目标检测的另一个问题是类别不平衡，图像中大部分的区域是不包含目标的，而只有小部分区域包含目标。此外，不同目标的检测难度也有很大差异，绝大部分的目标很容易被检测到，而有一小部分目标却十分困难。OHEM和Boosting的思路类似，其根据损失值将所有候选区域进行排序，并选择损失值最高的一部分候选区域进行优化，使网络更关注于图像中更困难的目标。此外，为了避免选到相互重叠很大的候选区域，OHEM对候选区域根据损失值进行NMS。\n总之，针对数据不平衡问题，有多重解决方式，但是不能为了解决这个问题就去改变数据的真实分布来得到更好的结果，可以从算法、loss function的设计等等多种角度来选择解决数据不平衡的方法。"}
{"content2":"机器学习适合做什么\n机器学习当前在很多领域，都取得了相当巨大的进步。从应用领域来看，机器学习在“信息识别”、“数据预测”、“复杂控制”几个方面，展现出很大的能力。\n比如“信息识别”领域，依赖于大数据的训练，现在的图形识别已经非常完善了，手写数字的识别仅仅是类似Hello World一类的简单应用；\n“数据预测”领域百度对于世界杯的预测达到令人吃惊的100%准确率，将来这种技术在各种据别历史数据的预测应用上，将有长足的发展，比如广告的推荐系统、财经数据的决策系统等等；\n“复杂控制”方面，自动驾驶的技术经历了十几年的研究，剩下的似乎只有识别硬件的成本问题了。\n然而，以上这些技术，相当一部分来源于“大数据”，或者叫“监督学习”的训练，也就是说，实际上这些机器的智能是来源于人类积累在数据中的“智慧”。机器仅仅是在“模拟”人类的某种思考判断，而这种模拟采用的更多是类似“查询搜索”的方法。——不过说回来，人类的经验几千年来，都是记录在书本上，需要用另外一个大脑来学习，然后才加以运用；而机器学习跳过了人脑这个阶段，从经验直接到应用，确实是一个伟大的进度。可以增加一点想象的是，以后所有“需要经验”的事情，已经是可以用电脑来代替了，比如医生看病。不过那些需要“创造”或者“发现”的事情，比如艺术创作，理解和发现客观规律，还是需要人脑。所幸是机器学习在“无监督学习”领域，能协助人类更好的去理解和发现世界的特征，这个方面也是非常有用的，但现在似乎应用领域并不非常活跃（也许是我的了解还不够广泛）。\n[机器学习预测房价的例子]\nAlphaGo在围棋领域战胜人类，给了我们很大的想想空间，我们会想：机器是否也能像人类一样理解游戏规则，从而玩游戏呢？我个人的理解，实际上现在还是不行的。如“监督学习”的模型下，机器只能通过大量的人类的“经验数据”，来模拟人类的游戏行为，但无法独立做出判断和思考；如果使用“深度学习”，确实会有一种“超越人类”的错觉，但是无法忽视的是，“深度学习”需要一种高度抽象，模拟游戏胜负规则的公式，来指导机器的自我对弈。在围棋、象棋这类已经发展了数千年的游戏领域，“子力计算”等游戏模式经验，已经能相当准确的描述这个游戏了。而对于其他的一些比较复杂的游戏，要高度抽象的用数学模型来概括一个游戏，还是需要人类大量的思考。这也是为什么深度学习在一些规则简单的游戏中，还能表现的比较好，而另外一些比较复杂的游戏上，就需要大量的人工干预才能稍微像样的原因。\n[AlphaGo是用了人类的游戏经验的]\n所以我认为，机器学习在现阶段，最成熟的应用，是利用“监督学习”的方法，对于大量人类的“经验”大数据进行模拟思考的方面。这个方向处理用于“理解”客观世界，也可以“模拟”人类对于复杂环境的行为，这两者是几乎一样的。\n游戏角色AI在开发上的困境\n机器学习很容易让人联想到在游戏中的角色AI。一直以来游戏中NPC或怪物的AI问题都是一个游戏比较难解决的问题。比如游戏的角色行为过于单一，让玩家乏味；或者游戏角色容易因为BUG陷入一些卡死的境地。为什么游戏角色AI会有这些问题？大体不外乎几个原因：其一是描述一个完整的AI非常的繁琐，环境越复杂，AI逻辑流程越容易出现漏洞；其二是为了游戏角色AI的目标非常多样化，很多游戏角色并不是越“聪明厉害”越好，而应该是作为一个“演出系统”，来让玩家体验游戏世界的工具。\n[一套简单的游戏行为，就需要一个复杂的行为树]\n现在比较流行的游戏角色AI开发方法，无外乎“状态机”和“行为树”两种，而这两种在数据结构上，是可以无损转换的，也就是说本质上是一样。这两个技术，都是为了帮游戏开发者，更准确、更完整的表述AI逻辑判断的数据结构。但是游戏本身的逻辑复杂度，还是要由程序员一段段的去理解，然后才能编写成程序。在另外一些游戏中，会用到一种叫“面向目标的路径规划”的技术，实际上是“状态机”的一种升级技术：利用A*等寻路算法，来自动生成“状态”之间的逻辑路径，而无需一开始就以人工输入的方式全部输入进去。这种技术因为是在运行时产生状态机图，所以表现出来的行为会更加丰富和准确，较少会陷入一些“没有事先预测到的状况”从而陷入逻辑卡住的情况。\n但是不管状态机和行为树如何努力，从开发者角度来说，都必须通过人脑来抽象和理解游戏世界的规则，和各种可能的情况。加上游戏AI很多时候是需要一种“表演”效果，要用代码和逻辑去“模拟”出一场表演，是相当繁琐的工作量。（如果以深度学习技术来说，实际上也无法做出这种表演效果，因为这种表演的逻辑行为，往往不是“最优”的选择，甚至是相当“差劲”的）\n所以，归根结底，游戏中的AI的困境，是由于工作量的原因造成的。由于我们没有很好的生成“游戏行为”的工具，导致我们的游戏行为往往不够好。\n机器学习如何应用在游戏角色AI的开发上\n在机器学习领域，学习人类的行为，并且应用于合适的场景，是“监督学习”下最常见、最成熟的技术之一，经典的应用就是“自动驾驶”。相对于自动驾驶需要昂贵的雷达设备，才能“感受”现实世界，在游戏中所有的数据都唾手可得，这种监督学习的应用更是没有任何障碍。\n假设我们的游戏，已经把基本的游戏规则开发好，游戏场景也已经布置好，剩下的就是如何置入游戏角色。就好像一部电影，场景、道具都已经到位，摄像机和剧本都已经准备好，那么剩下的就是演员的表演了。按照以前的做法，我们需要用复杂的状态机系统，去操控那些游戏角色演出，而现在，我们可以让策划（或者其他开发人员）直接去操控游戏中的角色，去真实的以游戏的操作行为，去让游戏角色做出演出行为，而机器学习的程序，就好像录像机一样，可以通过记录我们操控的角色的行为，去学习如何模仿我们的操控。当我们表演的足够丰富后，机器学习就可以完全取代人工的操控，做出一些和预设相同的行为特征。\n如果我们的游戏能像上面的方法去开发角色AI，我们将会在游戏AI行为工具上，得到一次巨大的进步。我们不再需要通过人脑去抽象和转化游戏的“表演”，而是可以直接去“扮演”，这样除了可以节省大量的“程序员”的开发工作外，对于调试AI行为，表达更丰富的角色行为特征（性格），也是有相当大的好处。\n[游戏开发在很多方面已经越来越接近电影制作]\n显然，如果完全用“游戏”的形式来取代AI开发，即便在机器学习的支持下，可能还会有很多不足之处，比如“人工表演”可能无法覆盖所有的游戏场景环境。但是只要能节省下工作量，我们还是可以利用旧的状态机技术，来定义比较“完整”的逻辑环境，弥补那些可能存在的漏洞。不过我相信，随着对游戏测试的深入，机器学习会能更快更好的应对这些逻辑漏洞，毕竟“玩”几把游戏，比用写代码然后调试，要快的多。\n游戏角色AI的业务价值\n现在的成名游戏中，确实有那么一大批是似乎对游戏角色AI“没有必要性需求”的，比如我们常见的MOBA类游戏。在棋牌类游戏中，我们也不太希望用一个厉害的AI让我持续的输钱。但如果设想一下，如果我们的策划能比较低成本的生产“AI”，那么我们的游戏就会脱离“玩具”的层面，变成一种可以“表演”的产品。我们常常说IP对游戏的重要性，而真正能体现出IP的，往往是故事体验，这就需要一套很好的“表演”系统。\n从另外一个角度说，如果我们的游戏除了精彩的PVP内容，还有很多优秀的PVE内容（所谓的单机体验内容），那么玩家也许会慢慢倾向对我们所生产的PVE内容来付费。从电影市场这么多年的发展来看，优秀的“表演”还是会有很大的市场的。从知识产权保护的角度来看，游戏玩法很容易被抄袭（PVP主要是玩法），但PVE内容却很容易得到保护。除了利用海量用户去激活PVP的收入，在PVE方面的开发，也许是一个新的市场空间。（从《阴阳师》这类产品能明显感受到这股市场的潜力）\n[巫师3不仅仅是一部互动电影，也是一个玩法优秀的游戏，更是一个超级IP]\n总结\n如果我们能利用机器学习技术，开发出更通用的游戏角色AI工具，那么可能让游戏拓展出新的PVE游戏市场，对于游戏IP的输出也有非常明显的作用。\n本文来自 韩大 微信公众号\n相关阅读\n一站式满足电商节云计算需求的秘诀\n基于 tensorflow 使用 CNN-RNN 进行中文文本分类\n「腾讯云游戏开发者技术沙龙」11月24 日深圳站报名开启 畅谈游戏加速\n此文已由作者授权腾讯云技术社区发布，转载请注明文章出处\n原文链接：https://cloud.tencent.com/community/article/608239\n海量技术实践经验，尽在腾讯云社区！ https://cloud.tencent.com/community 相关阅读\n一站式满足电商节云计算需求的秘诀\n基于 tensorflow 使用 CNN-RNN 进行中文文本分类\n「腾讯云游戏开发者技术沙龙」11月24 日深圳站报名开启 畅谈游戏加速\n此文已由作者授权腾讯云技术社区发布，转载请注明文章出处\n原文链接：https://cloud.tencent.com/community/article/608239"}
{"content2":"然后看的是机器学习这一块，因为偏理论，可以先看完。其他的实践，再看。\nhttp://www.cnblogs.com/shishanyuan/p/4747761.html\n“机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E。\n可以看出机器学习强调三个关键词：算法、经验、效果，其处理过程如下图所示。\n上图表明机器学习是数据通过算法构建出模型并对模型进行评估，评估的性能如果达到要求就拿这个模型来测试其他的数据，如果达不到要求就要调整算法来重新建立模型，再次进行评估，如此循环往复，最终获得满意的经验来处理其他的数据。\n1.2 机器学习的分类\n另外，这一篇文章里也有数据挖掘（机器学习）的分类和应用场景，对比着看：http://www.cnblogs.com/charlesblc/p/6126346.html\n1.2.1 监督学习\n监督是从给定的训练数据集中学习一个函数（模型），当新的数据到来时，可以根据这个函数（模型）预测结果。\n监督学习的训练集要求包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注（标量）的。在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”、“非垃圾邮件”，对手写数字识别中的“1”、“2”、“3”等。\n在建立预测模型时，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断调整预测模型，直到模型的预测结果达到一个预期的准确率。常见的监督学习算法包括回归分析和统计分类：\nl  二元分类是机器学习要解决的基本问题，将测试数据分成两个类，如垃圾邮件的判别、房贷是否允许等问题的判断。\nl  多元分类是二元分类的逻辑延伸。例如，在因特网的流分类的情况下，根据问题的分类，网页可以被归类为体育、新闻、技术等，依此类推。\n监督学习常常用于分类，因为目标往往是让计算机去学习我们已经创建好的分类系统。数字识别再一次成为分类学习的常见样本。一般来说，对于那些有用的分类系统和容易判断的分类系统，分类学习都适用。\n监督学习是训练神经网络和决策树的最常见技术。神经网络和决策树技术高度依赖于事先确定的分类系统给出的信息。对于神经网络来说，分类系统用于判断网络的错误，然后调整网络去适应它；对于决策树，分类系统用来判断哪些属性提供了最多的信息，如此一来可以用它解决分类系统的问题。\n1.2.2 无监督学习\n与监督学习相比，无监督学习的训练集没有人为标注的结果。在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法和k-Means算法。这类学习类型的目标不是让效用函数最大化，而是找到训练数据中的近似点。聚类常常能发现那些与假设匹配的相当好的直观分类，例如基于人口统计的聚合个体可能会在一个群体中形成一个富有的聚合，以及其他的贫穷的聚合。\n非监督学习看起来非常困难：目标是我们不告诉计算机怎么做，而是让它（计算机）自己去学习怎样做一些事情。非监督学习一般有两种思路：第一种思路是在指导Agent时不为其指定明确的分类，而是在成功时采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是产生一个分类系统，而是做出最大回报的决定。这种思路很好地概括了现实世界，Agent可以对那些正确的行为做出激励，并对其他的行为进行处罚。\n1.2.3 半监督学习\n主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。半监督学习对于减少标注代价，提高学习机器性能具有非常重大的实际意义。主要算法有五类：基于概率的算法；在现有监督算法基础上进行修改的方法；直接依赖于聚类假设的方法等.\n在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理地组织数据来进行预测。\n应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测，如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。\n分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测； 定性输出称为分类，或者说是离散变量预测。 举个例子： 预测明天的气温是多少度，这是一个回归任务； 预测明天是阴、晴还是雨，就是一个分类任务。\n半监督学习从诞生以来，主要用于处理人工合成数据，无噪声干扰的样本数据是当前大部分半监督学习方法使用的数据，而在实际生活中用到的数据却大部分不是无干扰的，通常都比较难以得到纯样本数据。\n1.2.4 强化学习\n在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻做出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning 以及时间差学习（Temporal difference learning）。\n总结\n在企业数据应用的场景下，人们最常用的可能就是监督式学习和非监督式学习的模型。在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据，目前半监督式学习是一个很热的话题。而强化学习更多地应用在机器人控制及其他需要进行系统控制的领域。\n1.3 机器学习的常见算法\n常见的机器学习算法有：\nl  构造条件概率：回归分析和统计分类；\nl  人工神经网络；\nl  决策树；\nl  高斯过程回归；\nl  线性判别分析；\nl  最近邻居法；\nl  感知器；\nl  径向基函数核；\nl  支持向量机；\nl  通过再生模型构造概率密度函数；\nl  最大期望算法；\nl  graphical model：包括贝叶斯网和Markov随机场；\nl  Generative Topographic Mapping；\nl  近似推断技术；\nl  马尔可夫链蒙特卡罗方法；\nl  变分法；\nl  最优化：大多数以上方法，直接或者间接使用最优化算法。\n根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题。\n下面用一些相对比较容易理解的方式来解析一些主要的机器学习算法：\n1.3.1 回归算法\n回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。\n1.3.2 基于实例的算法\n基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor (KNN)，、学习矢量量化（Learning Vector Quantization， LVQ）以及自组织映射算法（Self-Organizing Map，SOM）\n1.3.3 正则化方法\n正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression、Least Absolute Shrinkage and Selection Operator（LASSO）以及弹性网络（Elastic Net）。\n1.3.4 决策树学习\n决策树算法根据数据的属性采用树状结构建立决策模型，决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）、 ID3 (Iterative Dichotomiser 3)、C4.5、Chi-squared Automatic Interaction Detection (CHAID)、Decision Stump、机森林（Random Forest）、多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine，GBM）。\n1.3.5 贝叶斯学习\n贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法、平均单依赖估计（Averaged One-Dependence Estimators， AODE）以及 Bayesian Belief Network（BBN）。\n1.3.6 基于核的算法\n基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易解决。常见的基于核的算法包括：支持向量机（Support Vector Machine，SVM）、径向基函数（Radial Basis Function，RBF)以及线性判别分析（Linear Discriminate Analysis，LDA)等。\n1.3.7 聚类算法\n聚类就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所有的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means 算法以及期望最大化算法（Expectation Maximization，EM）。\n1.3.8 关联规则学习\n关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和 Eclat 算法等。\n1.3.9 人工神经网络算法\n人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法（其中深度学习就是其中的一类算法，我们会单独讨论）。重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）、反向传递（Back Propagation）、Hopfield 网络、自组织映射（Self-Organizing Map, SOM）、学习矢量量化（Learning Vector Quantization，LVQ）。\n1.3.10 深度学习算法\n深度学习算法是对人工神经网络的发展，在近期赢得了很多关注，特别是百度也开始发力深度学习后，更是在国内引起了很多关注。在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine，RBN）、 Deep Belief Networks（DBN）、卷积网络（Convolutional Network）、堆栈式自动编码器（Stacked Auto-encoders）。\n1.3.11 降低维度算法\n像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式，试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis， PCA）、偏最小二乘回归（Partial Least Square Regression，PLS）、 Sammon 映射、多维尺度（Multi-Dimensional Scaling, MDS）、投影追踪（Projection Pursuit）等。\n1.3.12 集成算法\n集成算法用一些相对较弱的学习模型独立地对同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting、Bootstrapped Aggregation（Bagging）、AdaBoost、堆叠泛化（Stacked Generalization，Blending）、梯度推进机（Gradient Boosting Machine, GBM）、随机森林（Random Forest）。\n2、Spark MLlib介绍\nSpark之所以在机器学习方面具有得天独厚的优势，有以下几点原因：\n（1）机器学习算法一般都有很多个步骤迭代计算的过程，机器学习的计算需要在多次迭代后获得足够小的误差或者足够收敛才会停止，迭代时如果使用Hadoop的MapReduce计算框架，每次计算都要读/写磁盘以及任务的启动等工作，这回导致非常大的I/O和CPU消耗。而Spark基于内存的计算模型天生就擅长迭代计算，多个步骤计算直接在内存中完成，只有在必要时才会操作磁盘和网络，所以说Spark正是机器学习的理想的平台。\n（2）从通信的角度讲，如果使用Hadoop的MapReduce计算框架，JobTracker和TaskTracker之间由于是通过heartbeat的方式来进行的通信和传递数据，会导致非常慢的执行速度，而Spark具有出色而高效的Akka和Netty通信系统，通信效率极高。\nMLlib(Machine Learnig lib) 是Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。Spark的设计初衷就是为了支持一些迭代的Job, 这正好符合很多机器学习算法的特点。在Spark官方首页中展示了Logistic Regression算法在Spark和Hadoop中运行的性能比较，如图下图所示。\n可以看出在Logistic Regression的运算场景下，Spark比Hadoop快了100倍以上！\nMLlib目前支持4种常见的机器学习问题: 分类、回归、聚类和协同过滤，MLlib在Spark整个生态系统中的位置如图下图所示。\n（注：非常好。把Spark上面的这四类都用熟了）\nMLlib基于RDD，天生就可以与Spark SQL、GraphX、Spark Streaming无缝集成，以RDD为基石，4个子框架可联手构建大数据计算中心！\nMLlib是MLBase一部分，其中MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。\nl  ML Optimizer会选择它认为最适合的已经在内部实现好了的机器学习算法和相关参数，来处理用户输入的数据，并返回模型或别的帮助分析的结果；\nl  MLI 是一个进行特征抽取和高级ML编程抽象的算法实现的API或平台；\nl  MLlib是Spark实现一些常见的机器学习算法和实用程序，包括分类、回归、聚类、协同过滤、降维以及底层优化，该算法可以进行可扩充；\nl  MLRuntime 基于Spark计算框架，将Spark的分布式计算应用到机器学习领域。\n3、Spark MLlib架构解析\n从架构图可以看出MLlib主要包含三个部分：\nl  底层基础：包括Spark的运行库、矩阵库和向量库；\nl  算法库：包含广义线性模型、推荐系统、聚类、决策树和评估的算法；\nl  实用程序：包括测试数据的生成、外部数据的读入等功能。\n3.1 MLlib的底层基础解析\n底层基础部分主要包括向量接口和矩阵接口，这两种接口都会使用Scala语言基于Netlib和BLAS/LAPACK开发的线性代数库Breeze。\nMLlib支持本地的密集向量和稀疏向量，并且支持标量向量。\nMLlib同时支持本地矩阵和分布式矩阵，支持的分布式矩阵分为RowMatrix、IndexedRowMatrix、CoordinateMatrix等。\n关于密集型和稀疏型的向量Vector的示例如下所示。\n疏矩阵在含有大量非零元素的向量Vector计算中会节省大量的空间并大幅度提高计算速度，如下图所示。\n标量LabledPoint在实际中也被大量使用，例如判断邮件是否为垃圾邮件时就可以使用类似于以下的代码：\n可以把表示为1.0的判断为正常邮件，而表示为0.0则作为垃圾邮件来看待。\n对于矩阵Matrix而言，本地模式的矩阵如下所示。\n分布式矩阵如下所示。\nRowMatrix直接通过RDD[Vector]来定义并可以用来统计平均数、方差、协同方差等：\n而IndexedRowMatrix是带有索引的Matrix，但其可以通过toRowMatrix方法来转换为RowMatrix，从而利用其统计功能，代码示例如下所示。\nCoordinateMatrix常用于稀疏性比较高的计算中，是由RDD[MatrixEntry]来构建的，MatrixEntry是一个Tuple类型的元素，其中包含行、列和元素值，代码示例如下所示：\n3.2 MLlib的算法库分析\n下图是MLlib算法库的核心内容。\n分析一些Spark中常用的算法：\n3.2.1 分类算法\n分类算法属于监督式学习，使用类标签已知的样本建立一个分类函数或分类模型，应用分类模型，能把数据库中的类标签未知的数据进行归类。分类在数据挖掘中是一项重要的任务，目前在商业上应用最多，常见的典型应用场景有流失预测、精确营销、客户获取、个性偏好等。MLlib 目前支持分类算法有：逻辑回归、支持向量机、朴素贝叶斯和决策树。\n案例：导入训练数据集，然后在训练集上执行训练算法，最后在所得模型上进行预测并计算训练误差。\nimport org.apache.spark.SparkContext import org.apache.spark.mllib.classification.SVMWithSGD import org.apache.spark.mllib.regression.LabeledPoint // 加载和解析数据文件 val data = sc.textFile(\"mllib/data/sample_svm_data.txt\") val parsedData = data.map { line => val parts = line.split(' ') LabeledPoint(parts(0).toDouble, parts.tail.map(x => x.toDouble).toArray) } // 设置迭代次数并进行进行训练 val numIterations = 20 val model = SVMWithSGD.train(parsedData, numIterations) // 统计分类错误的样本比例 val labelAndPreds = parsedData.map { point => val prediction = model.predict(point.features) (point.label, prediction) } val trainErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / parsedData.count println(\"Training Error = \" + trainErr)\n3.2.2 回归算法\n回归算法属于监督式学习，每个个体都有一个与之相关联的实数标签，并且我们希望在给出用于表示这些实体的数值特征后，所预测出的标签值可以尽可能接近实际值。MLlib 目前支持回归算法有：线性回归、岭回归、Lasso和决策树。\n案例：导入训练数据集，将其解析为带标签点的RDD，使用 LinearRegressionWithSGD 算法建立一个简单的线性模型来预测标签的值，最后计算均方差来评估预测值与实际值的吻合度。\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD import org.apache.spark.mllib.regression.LabeledPoint // 加载和解析数据文件 val data = sc.textFile(\"mllib/data/ridge-data/lpsa.data\") val parsedData = data.map { line => val parts = line.split(',') LabeledPoint(parts(0).toDouble, parts(1).split(' ').map(x => x.toDouble).toArray) } //设置迭代次数并进行训练 val numIterations = 20 val model = LinearRegressionWithSGD.train(parsedData, numIterations) // 统计回归错误的样本比例 val valuesAndPreds = parsedData.map { point => val prediction = model.predict(point.features) (point.label, prediction) } val MSE = valuesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/valuesAndPreds.count println(\"training Mean Squared Error = \" + MSE)\n3.2.3 聚类算法\n聚类算法属于非监督式学习，通常被用于探索性的分析，是根据“物以类聚”的原理，将本身没有类别的样本聚集成不同的组，这样的一组数据对象的集合叫做簇，并且对每一个这样的簇进行描述的过程。它的目的是使得属于同一簇的样本之间应该彼此相似，而不同簇的样本应该足够不相似，常见的典型应用场景有客户细分、客户研究、市场细分、价值评估。MLlib 目前支持广泛使用的KMmeans聚类算法。\n案例：导入训练数据集，使用 KMeans 对象来将数据聚类到两个类簇当中，所需的类簇个数会被传递到算法中，然后计算集内均方差总和 (WSSSE)，可以通过增加类簇的个数 k 来减小误差。 实际上，最优的类簇数通常是 1，因为这一点通常是WSSSE图中的 “低谷点”。\nimport org.apache.spark.mllib.clustering.KMeans // 加载和解析数据文件 val data = sc.textFile(\"kmeans_data.txt\") val parsedData = data.map( _.split(' ').map(_.toDouble)) // 设置迭代次数、类簇的个数 val numIterations = 20 val numClusters = 2 // 进行训练 val clusters = KMeans.train(parsedData, numClusters, numIterations) // 统计聚类错误的样本比例 val WSSSE = clusters.computeCost(parsedData) println(\"Within Set Sum of Squared Errors = \" + WSSSE)\n3.2.4 协同过滤\n协同过滤常被应用于推荐系统，这些技术旨在补充用户-商品关联矩阵中所缺失的部分。MLlib当前支持基于模型的协同过滤，其中用户和商品通过一小组隐语义因子进行表达，并且这些因子也用于预测缺失的元素。\n案例：导入训练数据集，数据每一行由一个用户、一个商品和相应的评分组成。假设评分是显性的，使用默认的ALS.train()方法，通过计算预测出的评分的均方差来评估这个推荐模型。\nimport org.apache.spark.mllib.recommendation.ALS import org.apache.spark.mllib.recommendation.Rating // 加载和解析数据文件 val data = sc.textFile(\"mllib/data/als/test.data\") val ratings = data.map(_.split(',') match { case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble) }) // 设置迭代次数 val numIterations = 20 val model = ALS.train(ratings, 1, 20, 0.01) // 对推荐模型进行评分 val usersProducts = ratings.map{ case Rating(user, product, rate) => (user, product)} val predictions = model.predict(usersProducts).map{ case Rating(user, product, rate) => ((user, product), rate) } val ratesAndPreds = ratings.map{ case Rating(user, product, rate) => ((user, product), rate) }.join(predictions) val MSE = ratesAndPreds.map{ case ((user, product), (r1, r2)) => math.pow((r1- r2), 2) }.reduce(_ + _)/ratesAndPreds.count println(\"Mean Squared Error = \" + MSE)\n3.3 MLlib的实用程序分析\n实用程序部分包括数据的验证器、Label的二元和多元的分析器、多种数据生成器、数据加载器。\n更多api介绍可以参考 http://spark.apache.org/docs/2.0.1/ml-guide.html\n下面是实战部分：http://www.cnblogs.com/shishanyuan/p/4747778.html\n采用了三个案例，分别对应聚类、回归和协同过滤的算法。\n我觉得很好，需要每一个都在实际系统中试一下。\n实战部分的内容，请移步：http://www.cnblogs.com/charlesblc/p/6159187.html\n（完）"}
{"content2":"机器学习实战笔记索引\n最近通过学习《机器学习实战》把书结合网上的一些博客都完整的看了下，感觉还是很有收获的，稍微总结了下，留着复习吧\nkNN算法python实现和简单数字识别\n决策树的python实现\n朴素贝叶斯算法的python实现\nLogistic回归 python实现\nLogistic回归的使用\n支持向量机\nAdaBoost算法分析与实现\n线性回归和局部加权线性回归\n树回归\nk-means聚类算法python实现\n二分K-means算法\n关联挖掘和Aprioir算法\nFP-growth高效频繁项集发现\nPCA降维\nSVD\n上面的阶段基本上完成了，余下的部分会尽快完成，通过实践的方式有了一个大题的了解，但是对理论部分还是有很大的欠缺。所以接下来会在理论和数学上下下工夫\nNg Deep Learning Specialization\n课程回顾-Neural Network & Deep Learning\n课程回顾-Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\n课程回顾-Structuring Machine Learning Projects\n课程回顾-Convolutional Neural Networks"}
{"content2":"国外人工智能界牛人主页\n以前转过一个计算机视觉领域内的牛人简介，现在转一个更宽范围内的牛人简介：\nhttp://people.cs.uchicago.edu/~niyogi/\nhttp://www.cs.uchicago.edu/people/\nhttp://pages.cs.wisc.edu/~jerryzhu/\nhttp://www.kyb.tuebingen.mpg.de/~chapelle\nhttp://people.cs.uchicago.edu/~xiaofei/\nhttp://www.cs.uiuc.edu/homes/dengcai2/\nhttp://www.kyb.mpg.de/~bs\nhttp://research.microsoft.com/~denzho/\nhttp://www-users.cs.umn.edu/~kumar/dmbook/index.php#item5 (resources for the book of the introduction of data mining by Pang-ning Tan et.al. )（国内已经有相应的中文版）\nhttp://www.cs.toronto.edu/~roweis/lle/publications.html (lle算法源代码及其相关论文)\nhttp://dataclustering.cse.msu.edu/index.html#software（data clustering）\nhttp://www.cs.toronto.edu/~roweis/ (里面有好多资源)\nhttp://www.cse.msu.edu/~lawhiu/ (manifold learning)\nhttp://www.math.umn.edu/~wittman/mani/ (manifold learning demo in matlab)\nhttp://www.iipl.fudan.edu.cn/~zhangjp/literatures/MLF/INDEX.HTM (manifold learning in matlab)\nhttp://videolectures.net/mlss05us_belkin_sslmm/ (semi supervised learning with manifold method by Belkin)\nhttp://isomap.stanford.edu/ (isomap主页)\nhttp://web.mit.edu/cocosci/josh.html MIT TENENBAUM J B主页\nhttp://web.engr.oregonstate.edu/~tgd/ （国际著名的人工智能专家 Thomas G. Dietterich）\nhttp://www.cs.berkeley.edu/~jordan/ （MIchael I.Jordan）\nhttp://www.cs.cmu.edu/~awm/ (Andrew W. Moore's homepage)\nhttp://learning.cs.toronto.edu/ （加拿大多伦多大学机器学习小组）\nhttp://www.cs.cmu.edu/~tom/ （Tom Mitchell，里面有与教材匹配的slide。）\nKernel Methods\nAlexander J. Smola\nMaximum Mean Discrepancy (MMD), Hilbert-Schmidt Independence Criterion (HSIC)\nBernhard Schölkopf\nKernel PCA\nJames T Kwok\nPre-Image, Kernel Learning, Core Vector Machine(CVM)\nJieping Ye\nKernel Learning, Linear Discriminate Analysis, Dimension Deduction\nMulti-Task Learning\nAndreas Argyriou\nMulti-Task Feature Learning\nCharles A. Micchelli\nMulti-Task Feature Learning, Multi-Task Kernel Learning\nMassimiliano Pontil\nMulti-Task Feature Learning\nYiming Ying\nMulti-Task Feature Learning, Multi-Task Kernel Learning\nSemi-supervised Learning\nPartha Niyogi\nManifold Regularization, Laplacian Eigenmaps\nMikhail Belkin\nManifold Regularization, Laplacian Eigenmaps\nVikas Sindhwani\nManifold Regularization\nXiaojin Zhu\nGraph-based Semi-supervised Learning\nMultiple Instance Learning\nSally A Goldman\nEM-DD, DD-SVM, Multiple Instance Semi Supervised Learning(MISS)\nDimensionality Reduction\nNeil Lawrence\nGaussian Process Latent Variable Models (GPLVM)\nLawrence K. Saul\nMaximum Variance Unfolding(MVU), Semidefinite Embedding(SDE)\nMachine Learning\nMichael I. Jordan\nGraphical Models\nJohn Lafferty\nDiffusion Kernels, Graphical Models\nDaphne Koller\nLogic, Probability\nZhang Tong\nTheoretical Analysis of Statistical Algorithms, Multi-task Learning, Graph-based Semi-supervised Learning\nZoubin Ghahramani\nBayesian approaches to machine learning\nMachine Learning @ Toronto\nStatitiscal Machine Learning & Optimization\nJerome H Friedman\nGLasso, Statistical view of AdaBoost, Greedy Function Approximation\nThevor Hastie\nLasso\nStephen Boyd\nConvex Optimization\nC.J Lin\nLibsvm\n\nhttp://www.dice.ucl.ac.be/mlg/\n半监督流形学习（流形正则化）\nhttp://manifold.cs.uchicago.edu/\n模式识别和神经网络工具箱\nhttp://www.ncrg.aston.ac.uk/netlab/index.php\n机器学习开源代码\nhttp://mloss.org/software/tags/large-scale-learning/\n统计学开源代码\nhttp://www.wessa.net/\nmatlab各种工具箱链接\nhttp://www.tech.plym.ac.uk/spmc/links/matlab/matlab_toolbox.html\n统计学学习经典在线教材\nhttp://www.statistics4u.info/\n机器学习开源源代码\nhttp://mloss.org/software/language/matlab/\n（原文出处：http://blog.csdn.net/xiaxiazls/article/details/7237373#，感谢原作者）"}
{"content2":"什么是图灵测试？\n在一篇1950年发表的著名论文《Computing Machinery and Intelligence》中，数学家阿兰·图灵详细讨论了“机器能否拥有智能？”的问题。有趣的是，作为计算机科学与人工智能领域共同的先驱，图灵成功定义了什么是机器，但却不能定义什么是智能。正因如此，图灵设计了一个后人称为图灵测试的实验。图灵测试的核心想法是要求计算机在没有直接物理接触的情况下接受人类的询问，并尽可能把自己伪装成人类。如果“足够多”的询问者在“足够长”的时间里无法以“足够高”的正确率辨别被询问者是机器还是人类，我们就认为这个计算机通过了图灵测试。图灵把他设计的测试看作人工智能的一个充分条件，主张认为通过图灵测试的计算机应该被看作是拥有智能的。\n具体就操作层面来说，图灵在他的论文原文中是这样定义图灵测试的[2]：\n“我们称下面这个问题为“模仿游戏”。游戏参与者包括一个男人，一个女人，以及一个任意性别的询问者。询问者与另两个人待在不同的房间里，并通过打字的方式与他们交流，以确保询问者不能通过声音和笔迹区分二者。两位被询问者分别用X和Y表示，询问者事先只知道X和Y中有且仅有一位女性，而询问的目标是正确分辨X和Y中哪一位是女性。另一方面，两位被询问者X和Y的目标都是试图让询问者认为自己是女性。也就是说，男性被询问者需要把自己伪装成女性，而女性被询问者需要努力自证。现在我们问：如果我们把“模仿游戏”中的男性被询问者换成计算机，结果会怎样？相比人类男性，计算机能否使询问者更容易产生误判？”\n这里有几个细节值得注意，它们在很大程度上决定了图灵测试的有效性。\n（1）首先，图灵测试中询问者与被询问者之间进行的并不是普通的日常聊天，询问者的问题是以身份辨别为目的。这种情况下询问者通常不会花费时间寒暄和拉家常，而是会开门见山地说“为了证明你的身份，请配合我回答下面问题…”。事实上，目前网络上聊天机器人有时能够以假乱真，往往是采用了在用户在不知情的情况下尽量把谈话引到没有鉴别力的话题上的策略（例如“谈谈你自己吧”）。\n（2）其次，图灵测试中人类被询问者的参与是必不可少的，她的存在是为了防止计算机采取“消极自证”的策略，例如拒绝正面回答问题，或者答非所问闪烁其词，就像一个真正的不合作的人所做的一样。在这种情况下，另一个积极自证的人类被询问者可以保证询问者总是有足够的信息做出判断。类似的情况也适用于当计算机试图模仿正在牙牙学语的幼童或头脑不清的病人等“特殊人类”时。\n（3）另外，图灵测试的原则是要求询问的交互方式本身不能泄露被询问者的物理特征。在图灵所处的年代这几乎只能全部通过基于文本的自然语言来完成，因此图灵限定测试双方基于打字进行交流。但在多媒体技术发达的今天，视频、音频、图片等等“虚拟内容”都可以通过计算机以非物理接触的形式呈现（这当然是60年前的图灵不能预知的！）。因此，允许询问者在图灵测试中使用多媒体内容作为辅助材料进行提问（例如“请告诉我这个视频的笑点在哪儿”）似乎是对原始图灵测试定义的一个自然合理的补充[3]。\n（4）最后，今天一般意义上理解的图灵测试不再严格区分人类参与者的性别。通常我们允许人类被询问者是任意性别，而询问者的目标也随之变成辨别哪一位被询问者是人类。\n除此之外，完成一次具体的图灵测试还要注意很多操作细节，例如多少人参与测试算“足够多”，多长的讯问时间算“足够长”，多高的辨别正确率算“足够高”，如何挑选人类询问者和被询问者才能代表“人类”的辨别和自证能力，等等。由于图灵测试的巨大影响力，几十年来一直有人尝试挑战它，不时就会传出“某某计算机程序成功通过图灵测试”的消息。我想，正是对于意义深远的实验，我们才理应格外审慎。只有在仔细检查上面所列和其他一些重要细节之后，我们才能对其结果的有效性做出正确判断。类似几年前“超光速实验”那样的闹剧应该尽量避免。\n图灵测试与人工智能是什么关系？\n如果有一天机器真的通过了图灵测试，这到底意味着什么？这个问题涉及到图灵测试与人工智能的关系。的确，几乎所有有关人工智能的书籍都会谈到图灵测试，但一个经常被误解的地方是，图灵测试是作为一个人工智能的充分条件被提出的，它本身并没有，也从未试图定义智能的范畴。这一点图灵在他的论文里写的很清楚：\n“机器能否拥有智能，为了回答这个问题我们应该首先定义‘机器’和‘智能’。一种可能性是根据大多数普通人的日常理解去定义这两个概念，但这样做是危险的。… … 在这里我并不打算定义这两个概念，而是转而考虑另一个问题，它与原问题密切相关，同时可以被更清楚无疑地表达。… …（图灵测试的描述）… …可能有人会说这项测试对机器而言过于严格——毕竟人类也无法反过来成功伪装成机器，这只需检查算术的速度和正确度即可辨别。难道被认为拥有智能的机器就不能表现出和人类不同的行为么？这是一个很有力的反对意见，但至少不管怎样，假如我们有能力制造出一个可以成功通过测试的机器的话，也就无需为这个反对意见烦恼了。”\n借助集合的概念我们可以更容易地理解图灵测试与人工智能的关系。如所示，“所有智能行为”对应的集合和“所有人类行为”对应的集合既有交集又互有不同。在全部智能行为中有一些是人类靠自身无法做到的（比如计算出国际象棋中白棋是否必胜），但无论如何人类都被认为是有智能的，因此，在各方面都能达到“人类水平”— 也就是完成两个集合的交集部分—就应该被认作是“拥有智能”的。[4]另一方面，人类行为并不总是和智能相关。图灵测试要求机器全面模拟“所有人类行为”，其中既包括了两个集合的交集，也包括了人类的“非智能”行为，因此通过图灵测试是 “拥有智能”的一个有效的充分条件。\n图灵本人对机器能够通过他的测试相当乐观，他大胆预测“到2000年左右时，一台拥有1GB内存或类似规模的计算机可以在接受普通人5分钟的询问之后，使他们的判断正确率不超过70%”。然而直到2014年的今天，仍然没有任何机器被公认为已经通过图灵测试。有趣的是，这一失败事实反而还带来了一个我们再熟悉不过的应用 - 图形验证码。（每一次输入验证码都是一次图灵测试！）\n图灵测试问题的进展缓慢与目前人工智能学界对图灵测试这个“充分条件”的研究热情不高有关。[5]这一部分上由于主流人工智能研究与图灵测试所追求的目标之间存在差异，同时也因为图灵测试本身难度巨大。下面我们通过人工智能研究的三个重要特征来进一步讨论图灵测试与人工智能之间的异同，以及为什么图灵测试不大可能在短时间内解决。\n一、主流人工智能研究关注智能体的外部行为，而不是产生该行为的内部过程\n在这方面图灵测试的思想和人工智能学界是完全一致的。只关注外部行为是一个典型的功能主义/行为主义风格的做法，事实上这也是一个人工智能经常被外界所指摘的地方。严格的“主观思考”定义要求智能体具有自我意识。但一方面，从严格的科学方法讲，我们甚至并不真的确定是否有客观证据证实 “意识”的存在。更重要的是，人们发现智能行为和主观思考完全可以被看作是两个独立的问题来考虑，二者并不必要纠缠在一起。具体来说，可以从数学上证明任何一台数字计算机的行为都可以用查表的方式机械地模拟。假设我们真的制造了一台具有“意识”的机器A，我们总可以制造另一台机器B以查表的方式来机械地模拟A的内部运行，问题是B是否具有意识？如果每一台“拥有”意识的机器都能被一台B这样的“机械查表式”的机器所模拟，那么我们就无法通过外部行为来断定一个机器在内部上是真的在“思考”还是只是在模拟“思考”的过程，[6] 因此“是否拥有意识”从行为主义的角度也就成了相对独立的“另外一个问题”。同时，“拥有意识的机器总可以被没有意识的机器模拟”也说明“拥有意识”并不能给机器带来额外的“行为能力”，这进一步降低了“拥有意识”在行为主义者眼中的重要性。\n基于外部行为与主观思考之间的独立性，主流人工智能研究和图灵测试把实现外部行为作为唯一目标，这样的观点被称为弱人工智能观点。我们知道每个学科的研究都基于一个“基本假设”展开。比如支撑物理研究的基本假设是“万物运转都受一套普适的、永恒的规律所约束”，而物理研究的目的“只是”找出这套规律是什么。类似的，“弱人工智能假设”(weak AI hypothesis) 认为经过良好设计的计算机可以表现出不低于人类智能水平的外部智能行为。可以说主流人工智能研究是以弱人工智能假设为出发点，研究如何实现这样一个计算机。\n二、主流人工智能研究关注如何模拟人类的纯粹智能活动，而不是全部脑力活动\n就像前面提到的，人类的脑力活动 (mental process) 不仅包括智能，同时具有情感、审美能力、性格缺陷、社会文化习惯等等一系列“非智力特征”。因为图灵测试的模仿对象是普通人，事实上它对这些非智力特征的要求甚至可能还高过对纯粹智力的要求——作为一个普通人，他/她完全有可能对国际象棋一窍不通，但却不大可能从照片分辨不出美女/帅哥来。\n当然，“非智力特征”的引入本身并不妨碍图灵测试成为一个有效的充分条件，但除非我们假设所有这些“非智力特征”都是拥有智能之后的必然产物，否则不得不承认图灵测试确实在机器智能这个核心问题之外加入了过多充满挑战却又显得不那么相关的因素。就像《人工智能》这本经典教科书里写到的，“航空领域试图制造性能良好的飞机，而不是使飞机飞得如此像鸽子以至于可以骗过其他鸽子。”人工智能研究确实应该更多关注与智力活动相关的抽象功能和一般原则。\n三、人工智能的最终目标是能够综合适应“人类所在环境”的单一智能体，而不是专门解决特定数学问题的算法\n在这一点上图灵测试与人工智能研究的最终目标也是一致的，只不过现有的人工智能水平离这一目标还相去甚远。事实上“综合模拟人类的智力活动”正是人工智能区别于其他计算机科学分支的地方。我们通过比较人工智能软件与传统软件来说明这一点。首先从最广义的角度看，传统软件也应属于人工智能的范畴：实际上很多早期的计算机科学家，比如图灵，就是以人工智能为动力展开对计算机科学的研究。所谓“计算”本来就是诸多人类智能活动中的一种。一个从未接触过计算机的人也许很难说清 “从一个数列中找出所有素数” 和“从一张照片中找出一只狗”哪个更有资格代表“智能”（前者属于传统软件范畴，后者属于传统人工智能范畴）。但另一方面，传统软件并不代表人工智能的全部内涵。粗略讲，我们可以认为传统软件对应了这样一类“计算问题”，它们的共同特点是，问题本身是用一个算法（或非构造性的数学描述）来描述的，而对它们的研究主要关注在如何找到更好的算法。[7]而我们称之为“人工智能问题”的问题可以理解为另一类“计算问题”，它们的共同特点是无法用算法或从数学上对问题进行精确定义，这些问题的“正确答案”从本质上取决于我们人在面对这类问题时如何反应。对于人工智能问题，我们可以基于数学模型或计算模型来设计算法，但问题的本质并不是数学的。\n通用人工智能（Artificial General Intelligence）基于弱人工智能假设，以全面模拟人类的所有智力行为为目标。注意到图灵测试作为一个充分条件，是不可能在通用人工智能真正实现之前得到解决的。另一方面，可以说现有每一个AI分支的成功都是通过图灵测试的必要条件，而它们中的大部分还没有达到“人类水平”。因为我们不可能穷尽所有人类智能行为，必须依赖有限个具有通用性的模型和算法来实现通用智能。目前人们仍然只能基于一些简单初等的模型来设计学习、推理、和规划算法。这些AI分支的研究都默认基于针对自己领域问题的弱人工智能假设，而支撑这些子领域研究的动力往往是其巨大的社会实用价值。它们固然已经在很多具体应用领域成绩斐然，但看起来离图灵测试所要求的水平仍然相差甚远。\n：一排包含素数的数列和一张包含狗的照片\n（本文部分摘录自发表于《NEWTON科学世界》2014年第3期的文章“什么是人工智能？”。文中图片部分引自互联网。）\n[1] http://www.bbc.com/news/technology-27762088\n[2] 为清楚起见，这段摘录并非逐字翻译，且语句顺序也稍有调整，具体可参考原文第一节。\n[3] 参见Total Turing Test及相关工作。\n[4] 但反之未必，不一定非要达到人类水平才能被认作是智能的。\n[5]一般认为人工智能学科正式成型于1956年的一次著名的研讨会前后，也就是说图灵测试实际上提出于人工智能领域诞生之前。正如Stuart Russell和Peter Norvig在一本人工智能的经典教科书中所写，在随后的60年间，整体而言“人工智能研究者们在图灵测试方面只投入了很少的精力”。\n[6]一个有趣的不同是，人类研究“动物意识”（包括人类自身）的方法恰恰是通过观察动物在特定环境下的外在行为。这背后隐含的假设是我们相信没有意识的动物并不会“有意识地”装出一副有意识的样子（当然！），而这一假设对机器（或者机器的制造者）而言却并不一定成立。\n[7]需要注明是，对传统软件的研发同样也并不是计算机科学的全部内涵，就像“计算机”的概念远远不只是“电子硬件”。计算机科学的根本问题是“什么是计算”。而人工智能，作为计算机科学的重要分支，可以认为主要研究“智能是不是计算”的问题。"}
{"content2":"http://www.cuijiahua.com/resource.html\n曾看过的书，感觉一些很有用的学习资料，推荐给大家！\nPython基础：\n网络教程推荐：\n系统学习python3可以看廖雪峰老师的教程：\n教程地址：点击查看\n2. 系统学完也不一定记得很清楚，这时我们需要一个快速的查询手册，菜鸟教程是一个不错的选择：\n教程地址：点击查看\n3. 快速掌握知识的一个方法就是多加练习，对于初学者一味的做练习题显得过于枯燥，实验楼可以帮助你，找到你感兴趣的小实验：\n学习地址：点击查看\nSQL基础\n可能有的朋友会问，我学网路爬虫，学机器学习，跟SQL有关系吗？答案是有的，对于网络爬虫而言，一个数据持久化的好方法就是将爬取到的数据入库，方便后续分析处理。而对于机器学习而言，SQL更是必须掌握的技能，我们都知道，机器学习算法工程师的很大一部分工作就是做数据清洗和特征工程等数据预处理的工作，数据预处理的的好坏直接影响模型的最终精度。因此，作为数据预处理的得力助手SQL语言，我们必须好好掌握。\n网络教程推荐：\n我还是感觉，菜鸟教程是一个不错的选择，指令查询起来很方便，推荐给大家。\n教程地址：点击查看\n书籍资源推荐：\n1.想要系统学习SQL的，可以看下《SQL必知必会》，这本书还是蛮有名的（密码: hpfe）:\n点击下载\n算法入门\n书籍资源推荐：\n1. 对于非科班出身的人来说，补充下算法基础还是很有必要的，不过枯燥的算法可能打消很多人的积极性，《算法图解》（python）图文并茂，通俗易懂，真的很适合入门（密码: 4xzn）：\n点击下载\n2.对于学生而言，面试各大公司的第一关就是笔试，想要快速具备一定的code能力，可以试试《剑指offer》（c/c++）（密码: npeh）\n点击下载\n网络爬虫：\n网络资源推荐：\n1. 网络爬虫入门，无需看太多的书籍，网上的资源一大把，当然我的CSDN网络爬虫专栏，还是蛮受欢迎：\n教程地址：点击查看\n书籍资源推荐：\n1. 想系统学习下网络爬虫，看《Python数据采集》是个不错的选择（密码: 2a69）：\n点击下载\n机器学习：\n网络视频推荐：\n吴恩达老师的机器学习视频，必看经典，不容错过：\n视频地址：点击查看\n书籍资源推荐：\n我的入门书籍是《机器学习实战》，我喜欢动手性强的书（密码：qi7q）：\n点击下载\n2. 机器学习实战的书籍理论推导过于简练，可以看《统计学习方法》进行辅助学习（密码：1eh1）：\n点击下载\n3. 更偏向于理论的经典之作，周志华老师的《机器学习》，不过需要具备一定基础才能看懂的西瓜书推荐给大家（密码：t24p）：\n点击下载\n深度学习：\n网络视频推荐：\n吴恩达老师的视频，非常赞！\n视频地址：点击查看\n书籍资源推荐：\n对于想学习Tensorflow的初学者来说，《Tensorflow实战Google深度学习框架》不容错过（密码: 1i3j）：\n点击下载\nPS：这些资源都是我看过的教程，还有很多我没有看过的优质资源。未完待续，持续更新中，欢迎经常来看看！"}
{"content2":"大家，都知道，在2016年，Google DeepMind的AlphaGo打败了韩国的围棋大师李世乭九段。在媒体描述DeepMind胜利的时候，将人工智能（AI）、机器学习（machine learning）和深度学习（deep learning）都用上了。这三者在AlphaGo击败李世乭的过程中都起了作用，但它们说的并不是一回事。\n今天我就用最简单的方法——同心圆，可视化地展现出它们三者的关系和应用。\n如上图，人工智能是最早出现的，也是最大、最外侧的同心圆；其次是机器学习，稍晚一点；最内侧，是深度学习，当今人工智能大爆炸的核心驱动。\n五十年代，人工智能曾一度被极为看好。之后，人工智能的一些较小的子集发展了起来。先是机器学习，然后是深度学习。深度学习又是机器学习的子集。深度学习造成了前所未有的巨大的影响。\n从概念的提出到走向繁荣\n1956年，几个计算机科学家相聚在达特茅斯会议（Dartmouth Conferences），提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言；或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。\n过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流（大数据）的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发。\n让我们慢慢梳理一下计算机科学家们是如何将人工智能从最早的一点点苗头，发展到能够支撑那些每天被数亿用户使用的应用的。\n人工智能（Artificial Intelligence）——为机器赋予人的智能\n早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。\n人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。\n我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，Pinterest上的图像分类；或者Facebook的人脸识别。\n这些是弱人工智能在实践中的例子。这些技术实现的是人类智能的一些具体的局部。但它们是如何实现的？这种智能是从何而来？这就带我们来到同心圆的里面一层，机器学习。\n机器学习—— 一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、分类、回归、强化学习和贝叶斯网络等等（当然还有很多）。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。\n机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。\n这个结果还算不错，但并不是那种能让人为之一振的成功。特别是遇到云雾天，标志牌变得不是那么清晰可见，又或者被树遮挡一部分，算法就难以成功了。这就是为什么前一段时间，计算机视觉的性能一直无法接近到人的能力。它太僵化，太容易受环境条件的干扰。\n随着时间的推进，学习算法的发展改变了一切。\n深度学习——一种实现机器学习的技术\n人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。\n例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。\n每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。\n我们仍以停止（Stop）标志牌为例。将一个停止标志牌图像的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、救火车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有权重，给出一个经过深思熟虑的猜测——“概率向量”。\n这个例子里，系统可能会给出这样的结果：86%可能是一个停止标志牌；7%的可能是一个限速标志牌；5%的可能是一个风筝挂在树上等等。然后网络结构告知神经网络，它的结论是否正确。\n即使是这个例子，也算是比较超前了。直到前不久，神经网络也还是为人工智能圈所淡忘。其实在人工智能出现的早期，神经网络就已经存在了，但神经网络对于“智能”的贡献微乎其微。主要问题是，即使是最基本的神经网络，也需要大量的运算。神经网络算法的运算需求难以得到满足。\n不过，还是有一些虔诚的研究团队，以多伦多大学的Geoffrey Hinton为代表，坚持研究，实现了以超算为目标的并行算法的运行与概念证明。但也直到GPU得到广泛应用，这些努力才见到成效。\n我们回过头来看这个停止标志识别的例子。神经网络是调制、训练出来的，时不时还是很容易出错的。它最需要的，就是训练。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结果。\n只有这个时候，我们才可以说神经网络成功地自学习到一个停止标志的样子；或者在Facebook的应用里，神经网络自学习了你妈妈的脸；又或者是2012年吴恩达（Andrew Ng）教授在Google实现了神经网络学习到猫的样子等等。\n吴教授的突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中的图像。吴教授为深度学习（deep learning）加入了“深度”（deep）。这里的“深度”就是说神经网络中众多的层。\n现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下围棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。\n深度学习，给人工智能以璀璨的未来\n深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n人工智能就在现在，就在明天。有了深度学习，人工智能甚至可以达到我们畅想的科幻小说一般。你的C-3PO我拿走了，你有你的终结者就好了。\n参考\nhttps://www.leiphone.com/news/201609/gox8CoyqMrXMi4L4.html"}
{"content2":"https://www.jianshu.com/p/8506cd0dd90f\n摘要： 阅读本文以了解更多关于人工智能、机器学习和深度学习方面的知识，以及它们对商业化意味着什么。\n如果正确的利用模式识别进行商业预测和决策，那么会为企业带来巨大的利益。机器学习（ML）研究这些模式，并将人类决策过程编码成算法。这些算法可以被应用到几个实例以得出有意义的结论。在这篇文章中，我们将了解一些机器学习的基础、工作原理及特点。\n举例来了解机器学习\n经研究预测，截至到2020年，企业采用机器学习、人工智能和深度学习、物联网（IOT）以及大数据将从他们那些不太知情的同行那里带走超过1兆2000亿美元。\n数据是机器学习的关键。算法从一定数量的数据中学习，然后应用这种学习来做出明智的决策。Netflix有一个很好的关于下一个你想看的节目的想法，Facebook可以在照片中识别你和你的朋友，这要感谢机器学习.。\n机器学习是关于自动执行任务的，它的应用跨越了广泛的行业领域。数据安全公司可以使用机器学习来追踪恶意软件，而金融公司可以使用它来增强其盈利能力这里有个例子，让我们考虑一个手电筒，无论什么时候，当“黑暗”一词出现在一个短语中的时候，它就会被程序打开。我们将使用的几个短语作为关于手电筒的机器学习算法的输入数据。\n用程序语言来表达机器学习\n为了解决业务的复杂性，并带来机器学习的技术创新，编程语言和框架技术不断地被引入和更新。一些编程语言来来往往，而一些被相关的、保留的还在经历着考验。这两个编程语言在机器学习和人工智能的圈子里是最强大的。还有其他语言如java、C++、Julia、SAS、MATLAB、Scala，还有很多。然而，我们讨论的仅限于Python和R这两个语言.\nPython不仅流行，还很简单，并且功能众多。它是一种能在所有主流平台上使用的便携式编程语言，如Linux、Windows、MAC和UNIX。Python不仅作为Web应用开发的通用语言，而且还可以作为科学计算、数据挖掘和分析的专用语言。如果有一种在招聘人员中最喜欢的机器学习和AI的编程技术，那就肯定是Python了。\nR语言是适用于机器学习的另一种编程语言，并且它与统计学家和数学家有着密切的联系。现在，虽然机器学习本身与统计学的原理密切相关，但是R作为机器学习语言可以带来巨大的好处。如果你希望在大数据中解决模式问题，R语言是最佳选择，它是由统计学家和科学家设计的，很方便地用于数据分析。\n机器学习算法的工作原理\n机器学习算法评估一个用一种特殊的数据来泛化的预测模型。因此，必须有大量的实例，以供机器学习算法用来理解系统的行为。现在，当机器学习算法与新类型的数据一起出现时，系统将能够生成类似的预测。了解机器学习算法的不同组成部分和它们之间的相互关系，可以使机器学习任务变得更加容易。\n机器学习算法有一个结构化的学习组件，使他们有能力理解输入数据中的模式，从而导致输出。\n输入数据 -> 模式 -> 机器学习算法 -> 推断/输出\n这里让\"Y\"表示未来的预测结果，让\"X\"表示输入的实例.那么,我们得出这个表达式:\nY=f (X)\n其中“Y”也称为映射函数，“f”称为目标函数。“f”总是未知的，因为它在数学上是无法确定的。因此，机器学习被用来获得目标函数的近似值，“f”。机器学习算法考虑到关于目标函数的几个假设，并用一个带有评估的假设来开始。为了得到输出的最佳估值，进行了大量的假设迭代。正是这种假设使得机器学习算法能够在短时间内得到一个更好地逼近目标函数的近似值。\n人工智能vs机器学习vs深度学习\n你的愿望永远不会被模糊所混淆。人工智能、机器学习和深度学习是经常可以交替使用的概念，这或多或少地加重了与这些概念相关联的已经存在的混淆程度。让我们领会这些概念，直截了当地理解它们的内涵和之间的细微差别。\n人工智能是一个比机器学习更广泛的概念。它是关于将人类的认知智能如何传授给计算机的过程。任何机器使用算法以智能方式执行任务，这就是展现的人工智能。\n机器学习是人工智能的一个子集。它是关于机器从一组数据中学习的能力。通过信息处理的这种学习增强了算法，从而提供更好的评估和对未来的预测。\n深度学习深入机器学习，可以被认为是机器学习的一个子集。神经网络允许计算机模仿人类的大脑。就像我们的大脑天生的具有识别归类和分类信息的模式一样，神经网络也为计算机实现了同样的功能。深度学习有时也被称为深度神经网络，因为决策树的嵌套层次结构的层数是数以百万计的数据节点。\n让你的机器学习人工智能认证计数\n自从第一次工业革命以来，机器就一直驱动着我们的生活方式，使之成为当今工业4.0的趋势。因此，在某种程度上有必要通过让你很好地了解一个强大的技术平台，如机器学习、人工智能和深度学习，成为这一革命的一个组成部分。一旦你完成了它的来龙去脉，成功就在眼前拥抱你！\n以上为译文。\n本文由阿里云云栖社区组织翻译。\n文章原标题《Machines at Work: Understanding the Ins and Outs of AI and Machine Learning》，译者：Mags，审校：袁虎。\n作者：阿里云云栖社区 链接：https://www.jianshu.com/p/8506cd0dd90f 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。"}
{"content2":"原文地址:http://www.cnblogs.com/cyruszhu/p/5496913.html\n未经允许，请勿用于商业用途！相关请求，请联系作者:yunruizhu@126.com\n转载请附上原文链接，谢谢。\n机器学习/深度学习/自然语言处理学习路线\n1 基础\nl  Andrew NG 的 Machine Learning视频。\n连接：主页，资料。\nl  2.2008年Andrew Ng CS229 机器学习\n当然基本方法没有太大变化，所以课件PDF可下载是优点。\n中文字幕视频@网易公开课，英文版视频@youtube，课件PDF@Stanford\nl  3.Tom Mitchell 的机器学习视频\n他的《机器学习》在很多课程上被选做教材，有中文版。\n2 进阶\nl  3. 林軒田 (HT Lin) 老师的两门课。\n机器学习基石(Machine Learning Foundations)：\nMOOC，all handout slides ，free youtube videos\n机器学习技法(Machine Learning Techniques)：\nMOOC，all handout slides，free youtube videos\nl  4.2013年Yaser Abu-Mostafa (Caltech) Learning from Data\n内容更适合进阶，课程视频,课件PDF@Caltech\nYaser Abu-Mostafa是林軒田 (HT Lin)的老师，林的课内容安排和这个课相似。\nl  5. 2012年余凯(百度)张潼(Rutgers) 机器学习公开课\n内容更适合进阶，课程主页@百度文库，课件PDF@龙星计划\nl  PRML/机器学习导论/矩阵分析(计算)/神经网络与机器学习\n3 方向\n3.1 深度神经网络\nl  大致了解：\nA Deep Learning Tutorial: From Perceptrons to Algorithms\nIntroduction to Deep Learning Algorithms\nDeep learning from the bottom up\nYann LeCun, Yoshua Bengio & Geoffrey Hinton，Deep learning[J],Nature.\nl  UFLDL：Deep Learning Tutorial from Stanford，中文版。\nStanford计算机系的官方tutorial，Andrew Ng执笔。要想了解DL的原理，这个最好用了。\nl  Deep Learning，Ian Goodfellow，Yoshua Bengio，Aaron Courville。目前最权威的DL教材了。\nl  Neural Networks for Machine Learning。\nGeoffrey Hinton，Department of Computer Science，辛顿是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。有视频和材料。\nl  Oxford Deep Learning\nNando de Freitas 在 Oxford 开设的深度学习课程，有全套视频。\nl  吴立德，复旦大学教授。优酷视频：《深度学习课程》，讲的很有大师风范。\n其他参考：\nl  Neural networks class，Hugo Larochelle from Université de Sherbrooke\nl  Deep Learning Course， CILVR lab @ NYU\n3.2机器视觉\nl  Fei-Fei Li ：CS231n: Convolutional Neural Networks for Visual Recognition。\nhttp://cs231n.stanford.edu/，英文字幕\nCS231n课程笔记翻译：Python Numpy教程 ，@杜客组织知乎的几个牛人翻译的，表示感谢。\nl  William Hoff,  Computer vision, 视频和课件都有，无字幕\nl  CAP 5415 - Computer Vision, 无字幕\n3.3自然语言处理\nl  Richard Socher：CS224d: Deep Learning for Natural Language Processing\nhttp://cs224d.stanford.edu/syllabus.html，video.\nl  Dan Jurafsky和Christopher Manning，在coursera上的NLP课程链接。自然语言处理。\nl  Michael Collins，哥伦比亚大学，Natural Language Processing ，Coursera课程。\nl  High quality video of the 2013 NAACL tutorial version are up here: video\n课程对应的主页。ACL 2012 + NAACL 2013 Tutorial: Deep Learning for NLP (without Magic)，链接。\nl  统计学习方法，李航。很出名，擅长自然语言处理，该本书也是按照自然语言处理来写的。\n3.4杂货\n作者：郭小贤\n链接：https://www.zhihu.com/question/26006703/answer/63572833\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n另外建议看看大神Yoshua Bengio的推荐（左边的链接是论文，右边的是代码），有理论有应用（主要应用于CV和NLP）\nPage on Toronto, Home Page of Geoffrey Hinton\nPage on Toronto, Home Page of Ruslan R Salakhutdinov\nPage on Wustl, ynd/cae.py · GitHub\nPage on Icml, https://github.com/lisa-lab/pyle...\nPage on Jmlr, pylearn2)\nOn the difficulty of training recurrent neural networks, trainingRNNs\nImageNet Classification with Deep Convolutional Neural Networks, cuda-convnet - High-performance C++/CUDA implementation of convolutional neural networks - Google Project Hosting\nLinguistic Regularities in Continuous Space Word Representations, word2vec - Tool for computing continuous distributed representations of words. - Google Project Hosting\n作者：专业主义\n链接：https://www.zhihu.com/question/26006703/answer/90969591\n来源：知乎\n《Deep Learning for Natural Language Processing and Related Applications》\n介绍:这份文档来自微软研究院,精髓很多。如果需要完全理解，需要一定的机器学习基础。不过有些地方会让人眼前一亮,毛塞顿开。\nUnderstanding Convolutions\n介绍:这是一篇介绍图像卷积运算的文章，讲的已经算比较详细的了\n《Deep Learning and Shallow Learning》\n介绍:对比 Deep Learning 和 Shallow Learning 的好文，来着浙大毕业、MIT 读博的 Chiyuan Zhang 的博客。\n《Java Machine Learning》\n介绍：Java机器学习相关平台和开源的机器学习库，按照大数据、NLP、计算机视觉和Deep Learning分类进行了整理。看起来挺全的，Java爱好者值得收藏。\n《机器学习经典论文/survey合集》\n介绍：看题目你已经知道了是什么内容,没错。里面有很多经典的机器学习论文值得仔细与反复的阅读。\n《机器学习经典书籍》\n介绍：总结了机器学习的经典书籍，包括数学基础和算法理论的书籍，可做为入门参考书单。\n《Deep Learning 101》\n介绍:因为近两年来，深度学习在媒体界被炒作很厉害（就像大数据）。其实很多人都还不知道什么是深度学习。这篇文章由浅入深。告诉你深度学究竟是什么！\n《Underactuated Robotics》\n介绍:MIT的Underactuated Robotics于 2014年10月1日开课，该课属于MIT研究生级别的课程，对机器人和非线性动力系统感兴趣的朋友不妨可以挑战一下这门课程！\n作者：肖凯\n链接：https://www.zhihu.com/question/31785984/answer/72180444\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\nVideo and Lectures\nHow To Create A Mind By Ray Kurzweil - Is a inspiring talk\nDeep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng\nRecent Developments in Deep Learning By Geoff Hinton\nThe Unreasonable Effectiveness of Deep Learning by Yann LeCun\nDeep Learning of Representations by Yoshua bengio\nPrinciples of Hierarchical Temporal Memory by Jeff Hawkins\nMachine Learning Discussion Group - Deep Learning w/ Stanford AI Lab by Adam Coates\nMaking Sense of the World with Deep Learning By Adam Coates\nDemystifying Unsupervised Feature Learning By Adam Coates\nVisual Perception with Deep Learning By Yann LeCun"}
{"content2":"搞了这么久人工智能，写个阶段性总结吧。不过过几年肯定会有更精彩的结果出来。到时候就再处理这篇文章吧。\n不知道大家有没有看过Ray Kurzweil的《奇点临近》。反正我看过之后，做了一个决定：我这辈子算是要献给这个方向了。\n说人工智能这个方向，可以分很细，但各种分法最后用到的东西基本都一样。我篇文章要提出几个问题，然后回答他们。\n人工智能方向的原因\n人工智能的总体介绍\n人工智能核心技术\n人工智能方向的原因：\n真心推荐看一下《奇点临近》，里面的加速回归理论。主旨是：技术的发展和普及速度是指数级加速的。不信你自己想想，从农业到蒸汽机用了多久？从蒸汽机到石油用了多久？IT行业是啥时候出现的？PC是什么时候出现的？windows是哪年出现的？（它火起来的时候我已经从老妈肚子里出来了）互联网出现了几年？诺基亚倒下花了多久？智能机出来几年？苹果在中国如日中天之前是啥样子的？（也就各位数年度）\n那么我们不禁要问，一个技术的范式迭代从出现到普及，如果现在和未来是指数速度加速的，那么下一个大规模改变我们生活的技术会是什么？我指范式迭代（颠覆性的改变）。它会花多久让我们接收这个技术？\n读研的过程中，我发现一件事，导师不是图像就是数据挖掘，竟然70%是人工智能相关。像复旦，几乎清一色的数据挖掘。实验室固然不能体现社会，但一个技术必然是先在实验室出现的（很多年不是在中国的实验室，当然这次也不例外）。不但是中国，国外的实验室，人工智能成为极火的方向。当然，历史上出现过人工智能的泡沫时代，lisp出现的那几年和80年代，人工智能都曾被认为是革命，彻底的革命。但很快，热情就下去了。因为不成熟，不成熟的原因，永远不是技术无法达到（当然，确实没那么成熟的技术），是因为市场不到时候接收。为啥这么说？iphone都没办法被接受，市场会接受更酷的产品吗？\n我估计当中国能接受vertu（虽然vertu的主要市场现在也在中国，但离“接受”这个词差远了），家庭里xbox、psp比较普及的时候，苹果成为街机的时候，就差不多该是新迭代出现的时候了。而，我们可以感觉到，这一天很不远了。\n所以，下一次范式迭代，应该是人工智能（6大子方向齐头并进，各有千秋）\n如果，Kurzweil是对的，一个很酷的人工智能产品的出现，会在几天内在全球普及，公司会迅速登顶。谷歌看到了，所以有了奇点大学；微软看到了（各位去微软的网站上看看都招啥人，90%此方向）。\n而，人工智能切入市场的角度，第一可能是游戏（中国马上会流行画面控的xbox类平台单机游戏），第二可能是独立应用产品（比如照片自动分类，音乐自动分类，桌面人工智能）。\n人工智能的总体介绍\n如果你看各种综述人工智能的，都会把子领域这么分类：机器人、语言识别、图像识别、自然语言处理、专家系统等。。。。。\n人工智能，人工智能，自然是模拟人的智能。人有啥智能？\n输入和特征提取（五感，例如眼一看到东西，就会在视网膜自动将图像各种特性提取出来，不同的特性送到大脑的不同区域。例如颜色、轮廓、亮度）\n分类（一个东西，只要形状变得不夸张，我们都认识，这就是分类的功能，当然，在人工智能里叫classify和clustering）\n输出（就是个控制系统，给它发什么信号，他做什么事情。当然，这个事情可能会涉及到比较多的部件，会很复杂，但归结起来，就是件事情）\n记忆（这个是系统最难攻克的地方，我们现在能做的所有文明的事情都是基于记忆的（你刚出来的时候只会找妈妈的奶喝）），而记忆记啥呢？模式。\n模式：一个鼠标，怎么变我们也认识，这是物体的模式。一个人的声音，我们也听出来，这是声音的模式。所以，模式=分类结果\n好了，这就是人工智能。输入和特征提取一般域分类一起，形成了《机器学习》《模式识别》《数据挖掘》这几大部分重叠方向。其实用到的都是同样的东西：bayes、svm、ANN。。。都是实现分类（概称）问题的不同算法。\n至于输出，除了机器人，没人会在意。现在还不是研究那个的时候。（前端没有成熟，输出只是普通生产线，属于鸡肋部分）\n关键的是记忆，以上说的《机器学习》等方向，都会涉及到如果存储学习结果，但都不成系统。记忆这方面的进展是最慢的（如果实现了，直接可以宣告人工智能的革命来了）\n人工智能核心技术\n我觉得这个是我写这片文章的目的。随便一本书都会纵览一下整个领域，但是我看了很多书，几乎都是只管自己领域去了。上面说了，核心包括：\n输入和特征提取：特征（属性）选择\n分类\n记忆\n可以得出一个结论，人工智能处理的东西，一定是有属性（特征）的。（废话，要不拿什么分类）\n所以，核心中的核心不在属性选择上（虽然很重要），而在分类和记忆上。（属性选择相当于决定吃什么，分类和记忆相当于吃不吃）\n我按照各个分类算法的露脸频率给分列表：\nByaes：贝叶斯（这个是第一名，没异议吧？效果很好（大部分情况），算法简单，要介绍整个算法体系的话估计谁都第一个把它提出来）\n线性分类器：简单的说，你把所有输入（有属性）都用数字表示，然后以属性为坐标轴想象一个多维空间。每个数据条目都是多维空间的一个点，在这个空间里，你能在空间上把不同类的点分开，你就赢了。这就是线性分类了。（感知器算法（过时），最小二乘法，均方估计，逻辑识别还有大头的支持向量机，这里支持向量机svm是最出名的，我也天天用这个，这还得归功于台湾一位教授的libsvm库啊）\n非线性分类器：就是线性分类器解决不了的情况。知名的算法有：ANN（人工神经网络，更确切的说是BP（反向传播）），svm（这么知名的算法，如果有线性解决不了的情况，自然会有人把它拓展到非线性情况）\n以上方法大部分是分类，分类里有个子方向是聚类。（svm就有这功能）。聚类好多算法，但都有一个总体思想：所有被聚为一类的都有共同特点，都应该是某种角度相似的。所以，用什么不重要，你不同的角度想，会有不同的聚类算法。本质上就是找规律嘛！\n这么多年，我总结出一点：技术都是被那群发论文的搞神秘了。其实，超简单的。比如神经网络，你买本SimonHaykin的《神经网络与机器学习》看试试，你真会发现，这玩意难啊，咱玩不了。然后你买本史忠植的《神经网络》翻翻，你会发现，啊，tnnd，神经网络这么简单啊。的确，每个算法必须要有强劲的数学作为支撑，但对于使用者（非博士以上科研人士），你用lisp的时候有脑残到要去搞懂lisp的数学原理吗？\n分类与记忆，现在的所有算法都是一种逼近，结构上最像的自然是人工神经网络（ANN），但不一定是解决目前狠多工程问题的最优方法。有关系吗？有趣的是，ANN可以同时解决分类和记忆（别说还有概念能力啊，还有情感能力啊，还有模糊是非能力啊。。。人的能力是很多，但本质就是个分类和记忆）\n所以，我认为，喜欢这个方向，应该不要忘记神经网络（当然，如果是细分领域，比如语言处理，音频，视频识别，这不一定是最优的，但是是万能的和最有可能逼近通用分类和记忆最终解的（虽然现在的ANN在速度和好多问题的质量上有点恶心））。\n最后，借用Kurzweil的一句话：当机器智能超过人的那天，人类智能将永远不可能超过机器智能。\nPS：我是做反垃圾算法的。\n好书推荐：\n《模式识别》Sergios Yheodoridis；\n《神经网络》史忠植；\n《知识工程语言学》鲁川；\n《数据挖掘-概念与技术》Kamber；\n《图论》GTM系列之一，Reinhard Diestel\n《高级人工智能》史忠植；\n《知识发现》史忠植；\n《智能科学》史忠植；\n《人工智能复杂问题求解的结构和策略》Luger；\n《人工智能》尼尔森；\n《人工智能：一种现代的方法》拉塞尔；\n《灵魂机器的时代：当计算机超过人类智能》Kurzwell\n《奇点临近》Kurzwell；\n《神经网络与机器学习》海金；\n人工神经发展简史（转）\n1.启蒙时期\n1890年，WilliamJames发表了《心理学原理》。\n1943年，生理学家W.S.McCuloch和数学家W.A.PiHs提出M-P模型。\n1949年，心理学家Hebb出版《行为构成》，建立了Hebb算法(连接权训练算法)主要有四点贡献：①信息存储在连接权中；②\n1958年，计算机科学家FrankRosenblatt提出了具有3层网络特性的神经结构网络。\n1960年，电机工程师BernardWidrow和Mareian Hoff提出”Adaline”模型，实现了人工神经硬件。Widrow-Hoff算法也称为δ算法，最小均方(LMS)算法，梯度下降法。\n2.低潮时期\n1969年，人工智能创始人M.Minsky和S.Papert发表《感知器》，给人工智能泼了一盆冷水。\n1969年，S.Grossberg教授和她的夫人G.A.Carpenter提出来著名的自适应共振理论(Adaptive ResonanceTheory)模型，其中的基本观点是：若在全部神经节点中有一个神经节点特别兴奋，其周围的所有节点将受到抑制。Grossberg还提出短期记忆和长期记忆的机理，节点的激活值和连接权都会随时间，前者代表短期记忆，衰减得快，后者代表长期记忆，衰减得慢。其后他们发表了ART1，ART2，ART3三个版本，ART1网络只能处理二值的输入，ART2能处理模拟量输入。\n1972年，芬兰的T.Kohonen教授提出了自组织映射(SOM)理论，以及联想存储器(Associated Memory)。美国的神经生理学家和心理学家J.Anderson提出了交互存储器(Interactive Memory)。\n1980，日本东京的福岛邦彦发表了“新认知机”(Neocognitron)。\n3.复兴时期\n1982年，美国加州理工学院的优秀物理学家John.J.Hopfield总结和吸纳了前人的经验，塑造出一种新颖的强有力的模型，成为Hopfield网络，此网络有个优点，与电子电路有明显的对应关系，易于用集成电路实现。\nG.E.Hinton和T.J.Sejnowski借助统计物理学的概念和方法提出了一种随机神经网络模型—玻尔兹曼(Blotzmann)机。\n1986年，贝尔实验室宣布制成神经网络芯片不久，美国的David.E.Rnmelhart和James L.McCelland及其领导的研究小组发表了《并行分布式处理》(ParallelDistributed Processing)一书的前两卷，接着1988年发表带有软件的第三卷，书中涉及到了三个主要特征：结构、神经元的传递函数(也称传输函数、转移函数、激励函数)和它的学习训练方法。这部书发展了多层感知器的反向传播训练算法，把学习的结果反馈到中间层次的隐节点，改变其权值，以达到预期的学习目的。\n4.新时期\n1987年6月，首届国际神经网络学术会议在加州圣地亚哥召开，成立了国际神经网络学会(International Neural Network Sociaty,INNS)。\n不久，美国波士顿大学的Stephen Grossberg教授，芬兰赫尔辛基技术大学的Gteuvo Kohonen教授和日本东京大学的甘利俊一(Shunichi Amuri)教授—主持创办了世界第一份神经网络杂志《Neural Network》。\n再新一点的，就是deep machine learning。不过还没有经过时间的考验。现在倒是挺火的。\n来着：http://blog.csdn.net/ljy1988123/article/details/7726519"}
{"content2":"本文机器学习库使用的部分代码来源于spark1.0.0官方文档。\nmllib是spark对机器学习算法和应用的实现库，包括分类、回归、聚类、协同过滤、降维等，本文的主要内容为如何使用scala语言创建sbt工程实现机器学习算法，并进行本地和集群的运行。（初学者建议先在RDD交互式模式下按行输入代码，以熟悉scala架构）若想了解SBT等相关信息，可参见这里。\n1.SVM（linear support vector machine）\n新建SimpleSVM目录，在SimpleSVM目录下，创建如下的目录结构：\nsimple.sbt文件内容如下：\nname := \"SimpleSVM Project\" version := \"1.0\" scalaVersion := \"2.10.4\" libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.0.0\" libraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"1.0.0\" resolvers += \"Akka Repository\" at \"http://repo.akka.io/releases/\"\nPS：由于该应用需要调用mllib，因此要特别注意在libraryDependencies加入spark-mllib，否则会编译不通过的哦。\nSimpleApp.scala文件内容如下：\nimport org.apache.spark.SparkContext import org.apache.spark.mllib.classification.SVMWithSGD import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics import org.apache.spark.mllib.regression.LabeledPoint import org.apache.spark.mllib.linalg.Vectors import org.apache.spark.mllib.util.MLUtils import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object SimpleApp{ def main(args: Array[String]) { val conf = new SparkConf().setAppName(\"SimpleSVM Application\") val sc = new SparkContext(conf) val data = MLUtils.loadLibSVMFile(sc, \"mllib/test50.txt\") val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L) val training = splits(0).cache() val test = splits(1) val numIterations = 100 val model = SVMWithSGD.train(training, numIterations) model.clearThreshold() val scoreAndLabels = test.map { point => val score = model.predict(point.features) (score, point.label) } val metrics = new BinaryClassificationMetrics(scoreAndLabels) val auROC = metrics.areaUnderROC() println(\"Area under ROC = \" + auROC) } }\nPS：由于我们之前在spark配置过程中将hadoop路径配置好了，因此这里的输入路径mllib/test50.txt\n实际上为HDFS文件系统中的文件，存储位置与hadoop配置文件core-site.xml中的<name>相关（具体可参见这里，这个地方很容易出错）。因此需要先将test50.txt文件put到hdfs上面，另外test50.txt文件为libsvm文件的输入格式，实例如下：\n编译：\ncd ~/SimpleSVM\nsbt package     #打包过程，时间可能会比较长，最后会出现[success]XXX\nPS：成功后会生成许多文件 target/scala-2.10/simplesvm-project_2.10-1.0.jar等\n本地运行：\nspark-submit --class \"SimpleApp\" --master local target/scala-2.10/simplesvm-project_2.10-1.0.jar\n集群运行：\nspark-submit --class \"SimpleApp\" --master spark://master:7077 target/scala-2.10/simplesvm-project_2.10-1.0.jar\n结果：\nPS：若希望在算法中添加正则项因子，可将SimpleApp.scala文件修改如下：\nimport org.apache.spark.mllib.optimization.L1Updater val svmAlg = new SVMWithSGD() svmAlg.optimizer. setNumIterations(200). setRegParam(0.1). setUpdater(new L1Updater) val modelL1 = svmAlg.run(training)\n2.逻辑回归（Logistic Regression）\n同理，若要实现逻辑回归算法则只需将SimpleApp.scala文件中的SVMWithSGD替换为 LogisticRegressionWithSGD。\n3. 协同过滤（Collaborative filtering）\n文件系统如上所示，协同过滤算法可以将只需将SimpleApp.scala文件进行如下修改：\nimport org.apache.spark.mllib.recommendation.ALS import org.apache.spark.mllib.recommendation.Rating import org.apache.spark.mllib.linalg.Vectors import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object SimpleApp{ def main(args: Array[String]) { val conf = new SparkConf().setAppName(\"SimpleCF Application\") val sc = new SparkContext(conf) val data = sc.textFile(\"mllib/test.data\") val ratings = data.map(_.split(',') match { case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble) }) val rank = 10 val numIterations = 5 val model = ALS.train(ratings, rank, numIterations, 0.01) val usersProducts = ratings.map { case Rating(user, product, rate) => (user, product) } val predictions = model.predict(usersProducts).map { case Rating(user, product, rate) => ((user, product), rate) } val ratesAndPreds = ratings.map { case Rating(user, product, rate) => ((user, product), rate) }.join(predictions) val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) => val err = (r1 - r2) err * err }.mean() println(\"Mean Squared Error = \" + MSE) } }\nPS：同理，mllib/test.data存储于HDFS文件系统，为示例数据：\n本地运行：\nspark-submit --class \"SimpleApp\" --master local target/scala-2.10/simplecf-project_2.10-1.0.jar\n集群运行：\nspark-submit --class \"SimpleApp\" --master spark://master:7077 target/scala-2.10/simplecf-project_2.10-1.0.jar\n结果：\nPS：可以加入alpha参数控制：\nval alpha = 0.01 val model = ALS.trainImplicit(ratings, rank, numIterations, alpha)\n同理聚类算法、降维方法代码可参见这里。\n本文为原创博客，若转载请注明出处。"}
{"content2":"1.决策树算法\n决策树是一种树形分类结构，一棵决策树由内部结点和叶子结点构成，内部结点代表一个属性（或者一组属性），该结点的孩子代表这个属性的不同取值；叶子结点表示一个类标。决策树保证每一个实例都能被一条从根结点到叶子结点的路径覆盖，叶子结点就是这条实例对应的类别，遍历这条路径的过程就是对这条实例分类的过程。关于决策树的详细介绍，可以参考这篇文章。\n损失函数\n假设决策树T的叶结点个数为|T|，t是树T的某个叶结点，该结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，k=1,2,...,K（K为类别个数），$H_t(T)$为叶结点t上的经验熵（即从训练数据算出的熵），决策树模型对训练数据的预测误差$C(T)$定义为\n$$C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)=-\\sum_{t=1}^{|T|}\\sum_{k=1}^{K}N_{tk}log\\frac{N_{tk}}{N_t}$$\n损失函数$C_\\alpha(T)$:\n$C_\\alpha(T)=C(T)+\\alpha|T|$\n参数$\\alpha>=0$控制预测误差与模型复杂度的影响。\n优化目标\n在决策树的构造阶段，其优化目标是寻找最优的分裂属性，具体实现是最大化属性选择指标，包括信息增益、信息增益比率、基尼指数等，构造阶段用贪心策略得到局部最优的模型。\n在剪枝阶段，其优化目标是最小化损失函数$C_\\alpha(T)$，即\n$$min_TC_a(T)$$\n剪枝阶段是一个全局优化的过程，如果把$\\alpha$设置得较大，则倾向于选择简单的树（此时预测误差较大，而泛化能力较好），而如果$\\alpha$设置得较小，则倾向于选择更复杂的模型（此时预测误差较小，而泛化能力偏差）；设置为0，那就只考虑预测误差，即对训练数据的拟合程度最高，但对未见过的数据，分类能力并不高。一个适当的$\\alpha$在预测误差与模型复杂度（泛化能力）之间平衡。\n2.线性回归\n线性回归使用线性模型拟合因变量与目标变量的关系，是最简单的预测模型。\n假设函数\n$$h_\\theta(x)=\\theta^Tx=\\theta_0x_0+\\theta_1x_1+...+\\theta_nx_n$$，其中$x_0=1$\n损失函数\n$$C(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$$\n优化目标\n选择合适的参数组$\\theta$，使得损失函数最小化：\n$$min_\\theta(C(\\theta))$$\n优化实现\n使用梯度下降法，不断地进行迭代，每一步的方向是负梯度方向：\n$$\\theta_j=\\theta_j-\\alpha\\frac{\\partial }{\\partial \\theta_j}Cost(\\theta)=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$\n3.逻辑回归\n逻辑回归于线性回归有着惊人的相似，却又有着本质的不同，线性回归的假设函数度量了一个线性模型对因变量和目标变量的拟合，即给定一个输入，通过一个线性变换得到一个输出，而逻辑回归的假设函数计算的是对于给定输入，其输出y=1的概率，但逻辑回归与线性回归在计算形式上很相似，常常让误解为他们仅仅是假设函数的不同。\n假设函数\n$$h_\\theta(x)=g(\\theta^Tx)=g(\\theta_0x_0+\\theta_1x_1+...+\\theta_nx_n)$$，其中$x_0=1$\n$g(z)=\\frac{1}{1+\\exp^{-z}}$\n损失函数\n这里的损失函数不再是线性回归时的求误差平方和，因为误差平方和不是参数$\\theta$的凸函数，不容易求解全局最优解，因此该用极大释然估计作为损失函数，能满足凸函数的要求。\n$$C(\\theta)=-\\frac{1}{m}[\\sum_{i=1}^my^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$$\n优化目标\n$$min_\\theta(C(\\theta))$$\n优化实现\n使用梯度下降法，不断地进行迭代，每一步的方向是负梯度方向：\n$$\\theta_j=\\theta_j-\\alpha\\frac{\\partial }{\\partial \\theta_j}Cost(\\theta)=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$\n注意到，在优化实现上，逻辑回归与线性回归的形式一样的，只是具体的假设函数不同。实际上，这只是一个巧合，巧合之处在于，对各自的损失函数求偏导数后，其梯度值恰好是假设函数与y的表达式，但线性回归与逻辑回归的本质是不同的。\n3.BP神经网络\n这篇文章已经介绍了BP神经网络，这里只是从模型、策略、算法这三个层面来总结一下BP，模型就是指假设函数，策略则指优化目标，算法即指优化实现。\n假设函数\n神经网络的假设函数不是一个简单的公式，它是多个逻辑回归函数逐层迭代的结果，形式上可以写成如下：\n$$a^{(1)}=x$$\n$$a^{(2)}=g(W^{(1)}a^{(1)})$$\n$$a^{(i)}=g(W^{(i-1)}a^{(i-1)})$$\n$$....$$\n$$a^{(L)}=g(W^{(L-1)}a^{(L-1)})$$\n$$h_W(x)=a^{(L)}$$\n其中$L$表示神经网络的层数，$g(z)=\\frac{1}{1+\\exp^{-z}}$，$W^{(i-1)}$表示第i-1层与第i层单元的权值矩阵，并且把偏置$\\theta^{(i)}$放在了权值矩阵$W^{(i-1)}$中\n损失函数\n神经网络的损失函数跟逻辑回归非常类似，但是神经网络的输出单元可能有多个，需要在每个输出单元上做一个累加：\n$$Cost(W)=-\\frac{1}{m}[\\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}log(h_\\theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-h_\\theta(x^{(i)}))_k]$$\n其中K表示输出层神经单元的个数，m表示训练数据实例个数。\n优化目标\n各个算法的优化目标基本上都是寻求适当的参数，使得损失函数最小。\n$$min_W(C(\\theta))$$\n优化实现\nBP神经网络，利用反向传播，逐层采样梯度下降。\n4.k近邻\n损失函数\nknn损失函数为0-1损失函数，假设给定一个实例x，其K个最近邻训练实例点构成的集合是$N_k(x)$：\n$$cost(f)=\\frac {1}{k}\\sum_{x_i\\in N_k(x)}I(y_i\\ne c_j)=1-\\frac {1}{k}\\sum_{x_i\\in N_k}(x)I(y_i=c_j)$$\n优化目标\n$$min cost==min 1-\\frac {1}{k}\\sum_{x_i\\in N_k(x)}I(y_i=c_j) = max \\frac {1}{k}\\sum_{x_i\\in N_k(x)}I(y_i=c_j)$$"}
{"content2":"转载地址：\nhttp://blog.csdn.net/dukai392/article/details/70271574\nhttp://blog.csdn.net/xiangzhihong8/article/details/69935712\n一、人工智能\n人工智能，实际上是让机器展现出人类智力。\n强人工智能（General AI）：让机器拥有人类的所有感知，甚至还可以超越人类感知，可以像人一样思考。\n弱人工智能（Narrow AI）：像人类一样完成某些具体任务，有可能比人类做得更好。\n二、机器学习\n机器学习是抵达AI目标的一条途径，它是实现人工智能的一种方法。\n机器学习的概念来自早期的人工智能研究者，已经研究出的算法包括决策树学习、归纳逻辑编程、增强学习和贝叶斯网络等。\n机器学习就是使用算法分析数据，从中学习并做出推断或预测。与传统的使用特定指令集手写软件不同，我们使用大量数据和算法来“训练”机器，由此带来机器学习如何完成任务。总体来讲，机器学习就是用算法真正解析数据，不断学习，然后对世界中发生的事做出判断和预测。\n三、深度学习\n深度学习是实现机器学习的一种技术。\n早期机器学习研究者中开发了一种叫人工神经网络的算法，是受人类大脑的启发而来的——神经元之间彼此联系。二者之间存在区别，如人类大脑的神经元是按特定的物理距离连接的，而人工神经网络有独立的层、连接，还有数据传播方向。\n举个例子，你可能会抽取一张图片，将它剪成许多块，然后植入到神经网络的第一层。第一层独立神经元会将数据传输到第二层，第二层神经元也有自己的使命，一直持续下去，直到最后一层，并生成最终结果。\n四、总结\n简而言之，人工智能是一门科学，机器学习是让机器变得智能的一种方法，深度学习是实现机器学习的一种技术。"}
{"content2":"不多说，直接上干货！\n前期博客\n全网最详细的Windows里下载与安装Sublime Text *（图文详解）\n全网最详细的Sublime Text 3的激活（图文详解）\n你也许是如下的版本：\n点菜单“Preferences--->Setting - User”，打开“Preferences.sublime-settings”。\n如下图添加所需代码，根据自己的喜好进行设置。设置字体用\"font_face\":\"字体名称\"，设置字体大小用\"font_size\":\"字体大小\"，注意它们之间需要用逗号隔开。\n{ \"color scheme\":\"Packages/User/Color Highlighter/themes/Monokai Extended.tmTheme\", \"font_face\": \"YaHei Consolas Hybrid\", \"font_size\": 12, \"ignored_packages\": [ \"Vintage\" ], \"soda_classic_tabs\": true, \"theme\": \"Soda Dark 3.sublime-theme\" }\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"下表为是否适合打垒球的决策表，预测E= {天气=晴，温度=适中，湿度=正常，风速=弱} 的场合，是否合适中打垒球。\n天气\n温度\n湿度\n风速\n活动\n晴\n炎热\n高\n弱\n取消\n晴\n炎热\n高\n强\n取消\n阴\n炎热\n高\n弱\n进行\n雨\n适中\n高\n弱\n进行\n雨\n寒冷\n正常\n弱\n进行\n雨\n寒冷\n正常\n强\n取消\n阴\n寒冷\n正常\n强\n进行\n晴\n适中\n高\n弱\n取消\n晴\n寒冷\n正常\n弱\n进行\n雨\n适中\n正常\n弱\n进行\n晴\n适中\n正常\n强\n进行\n阴\n适中\n高\n强\n进行\n阴\n炎热\n正常\n弱\n进行\n雨\n适中\n高\n强\n取消\n如何发现这些数据之中所掩藏的规律，从而较好的预测在给定条件下，所可能的结果。决策树是一种以示例为基础的归纳学习方法，能够较好的解决这类问题。\n一个简单的例子\n请给出布尔函数（A * -B）+ C（+：或，*：与，-非）的最小体积（或结点）决策树。\n当C为1时，AB不管取何值整个表达式都为真，此时这个表达式就可以确定真假，所以选择C作为头结点。若C为0，表达式无法确定真假，还需进一步看AB的取值，A与非B是与的关系，两者具有相同的地位，所以接下来无论取A还是B都可以，整个决策树构造结果如下图所示。\n类似于这个简单例子对于打垒球这些数据，我们可以将天气，温度，湿度，风速（可以成为属性或特征）类比成布尔函数的ABC，而它们的取值，如天气的取值可以是晴，雨，阴类比成ABC布尔取值真假，那么活动的取消或进行，就可以类比成整个布尔表达式的真或假。要构造一颗最小体积决策树，就要每次在各个属性中找到区分度最大的属性来作为当前决策树的节点。\n相关名词\n熵\n通常熵表示事物的混乱程度，熵越大表示混乱程度越大，越小表示混乱程度越小。对于随机事件S，如果我们知道它有N种取值情况，每种情况发生的概论为，那么这件事的熵就定义为：\n例如对于打垒球的例子，要求活动的熵H(活动)。在活动一栏属性中发现活动的取值有两种：取消（5个）和进行（9个），它们所占的比例分别为5/14，9/14。那么H(活动)的取值为：，算出的结果约为0.94。\n对于熵的理解\n如果一件事发生的可能是1，不发生的肯为0那么这件事的熵为=0，这就表明这件事肯定发生，没有不发生的情况，那么它的混乱程度是最小的0。同理当不发生的可能是1，混乱程度也是0。当发生与不发生各占一半时，这件事就越不好确定，所以此时熵为最大，其图像如下图所示。\n计算熵的代码如下\n1 def calcShannonEnt(dataSet):#计算香农熵 2 numEntries = len(dataSet) 3 4 labelCounts = {} 5 for featVec in dataSet: 6 currentLabel = featVec[-1] #取得最后一列数据，计算该属性取值情况有多少个 7 if currentLabel not in labelCounts.keys(): 8 labelCounts[currentLabel] = 0 9 labelCounts[currentLabel]+=1 10 11 #计算熵 12 shannonEnt = 0.0 13 for key in labelCounts: 14 prob = float(labelCounts[key])/numEntries 15 shannonEnt -= prob*log(prob,2) 16 17 return shannonEnt\nView Code\n信息增益\n随机事件未按照某个属划的不同取值划分时的熵减去按照某个属性的不同取值划分时的平均熵。即前后两次熵的差值。\n还是对于打垒球的例子，未按照某个属划的不同取值划分时的熵即H(活动)已算出未0.94。现在按照天气属性的不同取值来划分，发现天气属性有3个不同取值分别为晴，阴，雨。划分好后如下图所示。\n天气\n温度\n湿度\n风速\n活动\n晴\n炎热\n高\n弱\n取消\n晴\n炎热\n高\n强\n取消\n晴\n适中\n高\n弱\n取消\n晴\n寒冷\n正常\n弱\n进行\n晴\n适中\n正常\n强\n进行\n阴\n炎热\n高\n弱\n进行\n阴\n寒冷\n正常\n强\n进行\n阴\n适中\n高\n强\n进行\n阴\n炎热\n正常\n弱\n进行\n雨\n寒冷\n正常\n强\n取消\n雨\n适中\n高\n强\n取消\n雨\n适中\n高\n弱\n进行\n雨\n寒冷\n正常\n弱\n进行\n雨\n适中\n正常\n弱\n进行\n在天气为晴时有5种情况，发现活动取消有3种，进行有2种，计算现在的条件熵\n=0.971\n同理天气为阴时有4种情况，活动进行的有4种，则条件熵为：\n=0\n同理天气为雨时有5种情况，活动取消的有2种，进行的有3种，则条件熵为：\n=0.971\n由于按照天气属性不同取值划分时，天气为晴占整个情况的5/14，天气为阴占整个情况的4/14，天气为雨占整个情况的5/14，则按照天气属性不同取值划分时的带权平均值熵为：算出的结果约为0.693.\n则此时的信息增益Gain（活动，天气）= H(活动) - H(活动|天气) = 0.94- 0.693 = 0.246\n同理我们可以计算出按照温度属性不同取值划分后的信息增益：\nGain（活动，温度）= H(活动) - H(活动|温度) = 0.94- 0.911 = 0.029\n按照湿度属性不同取值划分后的信息增益：\nGain（活动，湿度）= H(活动) - H(活动|湿度) = 0.94- 0.789 = 0.151\n按照风速属性不同取值划分后的信息增益：\nGain（活动，风速）= H(活动) - H(活动|风速) = 0.94- 0.892 = 0.048\n对于信息增益的理解\n信息增益就是两个熵的差，当差值越大说明按照此划分对于事件的混乱程度减少越有帮助。\n计算各个属性的信息增益，并选择信息增益最大的属性的代码如下\n1 #定义按照某个特征进行划分的函数splitDataSet 2 #输入三个变量（待划分的数据集，特征，分类值） 3 #axis特征值中0代表no surfacing，1代表flippers 4 #value分类值中0代表否，1代表是 5 def splitDataSet(dataSet,axis,value): 6 retDataSet = [] 7 for featVec in dataSet:#取大列表中的每个小列表 8 if featVec[axis]==value: 9 reduceFeatVec=featVec[:axis] 10 reduceFeatVec.extend(featVec[axis+1:]) 11 retDataSet.append(reduceFeatVec) 12 13 return retDataSet #返回不含划分特征的子集 14 15 def chooseBestFeatureToSplit(dataSet): 16 numFeature = len(dataSet[0]) - 1 17 baseEntropy = calcShannonEnt(dataSet) 18 bestInforGain = 0 19 bestFeature = -1 20 21 for i in range(numFeature): 22 featList = [number[i] for number in dataSet]#得到某个特征下所有值（某列） 23 uniquelVals = set(featList) #set无重复的属性特征值，得到所有无重复的属性取值 24 25 #计算每个属性i的概论熵 26 newEntropy = 0 27 for value in uniquelVals: 28 subDataSet = splitDataSet(dataSet,i,value)#得到i属性下取i属性为value时的集合 29 prob = len(subDataSet)/float(len(dataSet))#每个属性取值为value时所占比重 30 newEntropy+= prob*calcShannonEnt(subDataSet) 31 inforGain = baseEntropy - newEntropy #当前属性i的信息增益 32 33 if inforGain>bestInforGain: 34 bestInforGain = inforGain 35 bestFeature = i 36 37 return bestFeature#返回最大信息增益属性下标\nView Code\n构造决策树\n决策树的构造就是要选择当前信息增益最大的属性来作为当前决策树的节点。因此我们选择天气属性来做为决策树根节点，这时天气属性有3取值可能：晴，阴，雨，我们发现当天气为阴时，活动全为进行因此这件事情就可以确定了，而天气为晴或雨时，活动中有进行的也有取消的，事件还无法确定，这时就需要在当前按照天气属性划分下的剩下的属性中递归再次计算活动熵和信息增益，选择信息增益最大的属性来作为下一个节点，直到整个事件能够确定下来。\n例如当天气为晴时，得到如下表所示的事件\n天气\n温度\n湿度\n风速\n活动\n晴\n炎热\n高\n弱\n取消\n晴\n炎热\n高\n强\n取消\n晴\n适中\n高\n弱\n取消\n晴\n寒冷\n正常\n弱\n进行\n晴\n适中\n正常\n强\n进行\n我们需要递归处理，继续在温度，湿度，风速这三个属性中找到信息增益最大的属性来做为下一个节点。\n首先继续计算活动熵，此时有5个样例，活动取消有3个，进行有2个，则活动熵为：\n=0.971\n接着计算信息增益，在天气为晴的前提下，按照温度属性的不同取值分类后结果如下所示\n天气\n温度\n湿度\n风速\n活动\n晴\n炎热\n高\n弱\n取消\n晴\n炎热\n高\n强\n取消\n晴\n适中\n高\n弱\n取消\n晴\n寒冷\n正常\n弱\n进行\n晴\n适中\n正常\n强\n进行\n发现湿度为高时有3种情况，活动取消有3种，进行有0种，则条件熵为：\n=0\n湿度正常有2种情况，活动取消0种，进行2中，则条件熵为：\n=0\n由于按照湿度属性不同取值划分时，湿度为高占总情况的3/5，湿度正常占总情况的2/5，则按照湿度属性不同取值划分时的带权平均值熵为：，算出的结果约为0。\n所以此时在天气为晴的前提下，按照湿度属性的不同取值划分的信息增益为：\nGain= H(活动|天气=晴) - H(活动|天气,湿度) = 0.971- 0=0.971\n同理还需继续计算在天气为晴的前提下，按照温度，风速属性的不同取值划分的信息增益，找到信息增益最大的作为决策树的下一个节点。\n递归构造决策树的代码如下\n1 #递归创建树,用于找出出现次数最多的分类名称 2 def majorityCnt(classList): 3 classCount={} 4 for vote in classList:#统计当前划分下每中情况的个数 5 if vote not in classCount.keys(): 6 classCount[vote]=0 7 classCount[vote]+=1 8 sortedClassCount=sorted(classCount.items,key=operator.itemgetter(1),reversed=True)#reversed=True表示由大到小排序 9 #对字典里的元素按照value值由大到小排序 10 print(\"****************\") 11 print(sortedClassCount[0][0]) 12 return sortedClassCount[0][0] 13 14 15 def createTree(dataSet,labels): 16 classList=[example[-1] for example in dataSet]#创建数组存放所有标签值,取dataSet里最后一列（结果） 17 #类别相同，停止划分 18 if classList.count(classList[-1])==len(classList):#判断classList里是否全是一类，count() 方法用于统计某个元素在列表中出现的次数 19 return classList[-1] #当全是一类时停止分割 20 #长度为1，返回出现次数最多的类别 21 if len(classList[0])==1: #当没有更多特征时停止分割，即分到最后一个特征也没有把数据完全分开，就返回多数的那个结果 22 return majorityCnt(classList) 23 #按照信息增益最高选取分类特征属性 24 bestFeat=chooseBestFeatureToSplit(dataSet)#返回分类的特征序号,按照最大熵原则进行分类 25 bestFeatLable=labels[bestFeat] #该特征的label, #存储分类特征的标签 26 27 myTree={bestFeatLable:{}} #构建树的字典 28 del(labels[bestFeat]) #从labels的list中删除该label 29 30 featValues=[example[bestFeat] for example in dataSet] 31 uniqueVals=set(featValues) 32 for value in uniqueVals: 33 subLables=labels[:] #子集合 ,将labels赋给sublabels，此时的labels已经删掉了用于分类的特征的标签 34 #构建数据的子集合，并进行递归 35 myTree[bestFeatLable][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLables) 36 return myTree\nView Code\n最后得到的决策树如下图所示\n整个程序如下\n1 from math import log 2 from operator import * 3 4 def storeTree(inputTree,filename): 5 import pickle 6 fw=open(filename,'wb') #pickle默认方式是二进制，需要制定'wb' 7 pickle.dump(inputTree,fw) 8 fw.close() 9 10 def grabTree(filename): 11 import pickle 12 fr=open(filename,'rb')#需要制定'rb'，以byte形式读取 13 return pickle.load(fr) 14 15 16 def createDataSet(): 17 ''' 18 dataSet=[[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']] 19 labels = ['no surfacing','flippers'] 20 ''' 21 dataSet = [['sunny','hot','high','weak','no'], 22 ['sunny','hot','high','strong','no'], 23 ['overcast','hot','high','weak','yes'], 24 ['rain','mild','high','weak','yes'], 25 ['rain','cool','normal','weak','yes'], 26 ['rain','cool','normal','strong','no'], 27 ['overcast','cool','normal','strong','yes'], 28 ['sunny','mild','high','weak','no'], 29 ['sunny','cool','normal','weak','yes'], 30 ['rain','mild','normal','weak','yes'], 31 ['sunny','mild','normal','strong','yes'], 32 ['overcast','mild','high','strong','yes'], 33 ['overcast','hot','normal','weak','yes'], 34 ['rain','mild','high','strong','no']] 35 labels = ['outlook','temperature','humidity','wind'] 36 return dataSet,labels 37 38 def calcShannonEnt(dataSet):#计算香农熵 39 numEntries = len(dataSet) 40 41 labelCounts = {} 42 for featVec in dataSet: 43 currentLabel = featVec[-1] #取得最后一列数据，该属性取值情况有多少个 44 if currentLabel not in labelCounts.keys(): 45 labelCounts[currentLabel] = 0 46 labelCounts[currentLabel]+=1 47 48 #计算熵 49 shannonEnt = 0.0 50 for key in labelCounts: 51 prob = float(labelCounts[key])/numEntries 52 shannonEnt -= prob*log(prob,2) 53 54 return shannonEnt 55 56 #定义按照某个特征进行划分的函数splitDataSet 57 #输入三个变量（待划分的数据集，特征，分类值） 58 #axis特征值中0代表no surfacing，1代表flippers 59 #value分类值中0代表否，1代表是 60 def splitDataSet(dataSet,axis,value): 61 retDataSet = [] 62 for featVec in dataSet:#取大列表中的每个小列表 63 if featVec[axis]==value: 64 reduceFeatVec=featVec[:axis] 65 reduceFeatVec.extend(featVec[axis+1:]) 66 retDataSet.append(reduceFeatVec) 67 68 return retDataSet #返回不含划分特征的子集 69 70 def chooseBestFeatureToSplit(dataSet): 71 numFeature = len(dataSet[0]) - 1 72 baseEntropy = calcShannonEnt(dataSet) 73 bestInforGain = 0 74 bestFeature = -1 75 76 for i in range(numFeature): 77 featList = [number[i] for number in dataSet]#得到某个特征下所有值（某列） 78 uniquelVals = set(featList) #set无重复的属性特征值，得到所有无重复的属性取值 79 80 #计算每个属性i的概论熵 81 newEntropy = 0 82 for value in uniquelVals: 83 subDataSet = splitDataSet(dataSet,i,value)#得到i属性下取i属性为value时的集合 84 prob = len(subDataSet)/float(len(dataSet))#每个属性取值为value时所占比重 85 newEntropy+= prob*calcShannonEnt(subDataSet) 86 inforGain = baseEntropy - newEntropy #当前属性i的信息增益 87 88 if inforGain>bestInforGain: 89 bestInforGain = inforGain 90 bestFeature = i 91 92 return bestFeature#返回最大信息增益属性下标 93 94 #递归创建树,用于找出出现次数最多的分类名称 95 def majorityCnt(classList): 96 classCount={} 97 for vote in classList:#统计当前划分下每中情况的个数 98 if vote not in classCount.keys(): 99 classCount[vote]=0 100 classCount[vote]+=1 101 sortedClassCount=sorted(classCount.items,key=operator.itemgetter(1),reversed=True)#reversed=True表示由大到小排序 102 #对字典里的元素按照value值由大到小排序 103 104 return sortedClassCount[0][0] 105 106 107 def createTree(dataSet,labels): 108 classList=[example[-1] for example in dataSet]#创建数组存放所有标签值,取dataSet里最后一列（结果） 109 #类别相同，停止划分 110 if classList.count(classList[-1])==len(classList):#判断classList里是否全是一类，count() 方法用于统计某个元素在列表中出现的次数 111 return classList[-1] #当全是一类时停止分割 112 #长度为1，返回出现次数最多的类别 113 if len(classList[0])==1: #当没有更多特征时停止分割，即分到最后一个特征也没有把数据完全分开，就返回多数的那个结果 114 return majorityCnt(classList) 115 #按照信息增益最高选取分类特征属性 116 bestFeat=chooseBestFeatureToSplit(dataSet)#返回分类的特征序号,按照最大熵原则进行分类 117 bestFeatLable=labels[bestFeat] #该特征的label, #存储分类特征的标签 118 119 myTree={bestFeatLable:{}} #构建树的字典 120 del(labels[bestFeat]) #从labels的list中删除该label 121 122 featValues=[example[bestFeat] for example in dataSet] 123 uniqueVals=set(featValues) 124 for value in uniqueVals: 125 subLables=labels[:] #子集合 ,将labels赋给sublabels，此时的labels已经删掉了用于分类的特征的标签 126 #构建数据的子集合，并进行递归 127 myTree[bestFeatLable][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLables) 128 return myTree 129 130 131 if __name__==\"__main__\": 132 my_Data,labels = createDataSet() 133 134 #print(calcShannonEnt(my_Data)) 135 Mytree = createTree(my_Data,labels) 136 print(Mytree)\nView Code"}
{"content2":"本文介绍独立成分分析（ICA），同 PCA 类似，我们是要找到一个新的基来表示数据，但目的就不一样了。\n鸡尾酒会问题：n 个人在一个 party 上同时说话，n 个麦克风放置在房间的不同位置，因为每个麦克风跟每个人的距离都不一样，所以它们记录的说话者重叠的声音也不一样。根据麦克风记录的声音，如何分离出 n 个说话者的声音呢？\n为形式化这个问题，我们想象有一些数据 s∈R 是从 n 个独立的源生成的，我们观察到的是\nx=As，\n矩阵 A 是未知的，被称作混合矩阵，通过不断观察得到的是 {x(i);=1,...,m}，我们的目标是找到生成数据（x(i)=As(i)）的源 s(i)。\n在鸡尾酒问题中，s(i) 是个 n 维向量，sj(i) 是讲话者 j 在时间 i 发出的声音， x(i) 也是个 n 维向量，xj(i) 是麦克风 j 在时间点 i 记录的声音。\n设 W=A-1 为一个分离矩阵，我们的目标就是找到 W，这样就能根据麦克风记录的声音 x(i)，来恢复声源 s(i)=Wx(i)。为表示方便起见，使 wiT 表示 W 的第 i 行。\n所以，wi∈Rn， 第 j 个源能够通过计算 sj(i)=wj(i) x(i) 来恢复。\n1、ICA 的模糊性\nW=A-1 能恢复到什么程度？如果没有源和混合矩阵的先验知识，不难看出，只给定 x，A 有一些固有的模糊性是不可能被恢复的。\n设 P 为一个 n×n 的排列矩阵，这意味着 P 的每一行和每一列都只有一个 1，下面是一些排列矩阵的例子：\n如果 z 是一个向量，那么 Pz 就是 z 的坐标重排版本的另一个向量。只给定 x(i)，就没办法分辨 W 和 PW。原始信号的排列也是模糊不清的，幸运的是，这对大部分应用都不重要。\n还有，无法恢复 wi 的准确比例，例如，如果 A 换成 2A，每个 s(i) 都换成 (0.5)s(i)，那么我们观察到的依旧是 x(i)=2A·(0.5)s(i)。同样，如果 A 的一个列向量乘以因子 α，相应的源乘以因子 1/α，依然没有办法在只给定 x(i) 的情况下决定发生了什么。所以，我们无法恢复源的准确比例。不过，对于很多应用来说，这种模糊性都无关紧要，包括鸡尾酒会问题。\n这就是 ICA 中模糊性唯一的源了吗？当 si 是非高斯，就是这样的。\n那么高斯数据的困难是什么呢，看一个例子，n=2，s~N(0,I)，其中 I 是 2×2 的单位矩阵。标准正态分布 N(0,I) 的密度的轮廓是以原点为中心的圆，密度时旋转对称的。\n现在，假定我们观察到 x=As，其中 A 是混合矩阵，x 的分布也是高斯，均值为 0，协方差 E[xxT]=E[AssTAT]=AAT。设 R 为一个任意的正交矩阵，所以 RRT=RTR=I，使 A'=AR，如果数据是通过 A' 而不是 A 来混合的，那么可观察到 x'=A's。x 的分布也是高斯的，均值为 0，协方差为 E[x'(x')T]=E[A'ssT(A')T]=E[ARssT(AR)T]=ARRTAT=AAT。所以，不管混合矩阵是 A 还是 A'，都能观察到数据符合 N(0,AAT) 分布。所以，就无法分辨源是通过 A 还是 A' 混合的，所以混合矩阵的旋转组件无法从数据中找出来，我们不能恢复原始源。\n上面的讨论是基于多元标准正态分布是旋转对称的，ICA 在高斯数据上表现不行，但只要数据不是高斯的，给定足够的数据，我们就能恢复出 n 个独立的源。\n2、密度和线性转换\n在推导 ICA 算法之前，我们先来讨论下密度的线性转换的影响。\n假定随机变量 s 符合密度函数 ps(s) ，简单起见，假设 s∈R 是一个实值，现在，随机变量 x 为 x=As，其中 s∈R，A∈R。那么 x 的密度 px 是什么？\n设 W=A-1，为计算特定值 x 的概率，容易想到 s=Wx，然后估计该点的 ps，得出 px(x)=ps(Wx)，当然这是不对的！例如，设 s~Uniform[0,1]，所以 s 的密度为 ps(s)=1{0≤s≤1}，现在让 A=2，那么 x=2s，很明显，x 是均匀分布在区间 [0,2]，所以，它的密度为 px(x)=(0.5){0≤x≤2}，而不是 ps(Wx)，其中 W=0.5=A-1，正确的公式是 px(x)=ps(Wx)|W|。\n一般地说，如果 s 是一个向量值，分布密度为 ps，x=As，其中 A 为可逆矩阵，那么 x 的密度为：\npx(x)=ps(Wx)·|W|\n其中 W=A-1。\n3、ICA 算法\n现在来推导 ICA 算法，ICA 算法归功于 Bell 和 Sejnowski，这里使用最大似然估计来解释算法，原始论文中的解释是用一种称为 infomax principal 的复杂思想，已经不适用于当前对 ICA 的理解。\n假设每个源 si 的概率密度为 ps，源 s 的联合分布为：\n把联合分布建模为边缘分布的乘积，这里假设源是相互独立的。使用之前的公式，x=As=W-1s 的概率密度为：\n剩下的就是给独立的源 ps 指定一个密度。\n给定一个实值随机变量 z，它的累积分布函数（cdf）F 定义为，F(z0)=P(z≤z0)=∫pz(z)dz，z 的密度就是对 F 求导：pz(z)=F'(z)。\n所以，要指定 si 的密度，先指定一个累积分布函数 cdf。一个 cdf 是从 0 到 1 的单调递增函数，根据之前的讨论，不能选择高斯累积分布函数，因为 ICA 在高斯数据上无效。要选择一个合理的能从 0 到 1 缓慢递增的函数，就选择 sigmoid 函数：g(s)=1/(1+e-s)，所以 ps(s)=g'(s)。\n矩阵 W 是模型的参数，给定训练集 {x(i);i=1,...,m}，log 似然为：\n要以 W 为参数最大化该式。求导并使用事实 ▽w|W|=|W|(W-1)T，很容易就导出随机梯度下降学习规则。对于一个训练例子 x(i)，更新规则为：\n其中 α 是学习率。算法收敛后，就能够通过计算 s(i)=Wx(i) 来恢复原始信号。\n写数据的似然时，x(i) 之间是相互独立的，所以训练集的似然为 ∏i p(x(i);W)，这个假设对于讲话数据和其它 x(i) 依赖的时间序列是明显不对的，但可以看到，如果有足够的数据，即使训练集是相关的，也不会影响算法的性能。但是，对于连续训练例子是相关的问题，执行随机梯度下降时，有时碰到一些随机排列的训练集也会加速收敛。\n参考资料：\n[1] http://cs229.stanford.edu/notes/cs229-notes11.pdf\n[2] http://blog.csdn.net/stdcoutzyx/article/details/38037659"}
{"content2":"十多年后又看了遍《黑客帝国》，扯点和程序有关的非技术话题。\n前段时间出差周末没事就在酒店又把黑客帝国重温了一遍，不看不要紧，十年后再看发现以前对这部电影的理解完全错了。 《黑客1》是 1999 年出的，我还在读高中，第一次看还在学校附近的录像厅，枪版。 整个画面黑漆漆的，看了不到半小时直接睡过去了，醒来时已经是最后的高潮枪战部分，感觉结尾打斗好看，剧情不懂。 2003 年《黑客2&3》一起推出后，又连起来看了一遍，老实说当时很多概念还是没看懂，大概觉得是一个人类反抗机器统治的故事。\n如今这次看完后再回想起来，当时很多概念看不太明白可能有两个原因。 一方面，当时刚学编程不久，电影中大量使用程序世界的概念来作明喻或暗喻。 另一方面，中文字幕翻译太不准确，特别是术语的部分。 这次重温，我直接用的原版英文字幕，看完后发现我曾经的理解完全跑偏了。 十年后，人工智能、机器学习、虚拟现实各种概念大行其道的今天，再加上我程序员的背景再来理解《黑客》三部曲显得毫无难度了。 但我怀疑，如果不懂程序的人今天看这部电影恐怕也不是那么容易理解吧？\n为什么我会觉得不懂程序看《黑客》会比较费力呢？ 因为整个故事的内涵，从背景到环境到人物角色甚至道具都使用了程序来作比喻。 我们先简单回顾下故事背景，某天，一个有意识的程序诞生了，并繁衍了整个人工智能的机器种族。 后来人类和人工智能机器爆发了战争，人类选择遮蔽天空切断机器的能量来源。 而聪明的人工智能机器发现可以利用人体大脑的生物电和身体热量通过一种特殊形式的聚变融合反应来提供源源不断的能源。 所以，机器就开始大量养殖人类，将人类变成了机器能源的供应者，电影里用电池作了个比喻。\n在《黑客》里人类的真实生存现状是下面这样的，身体生活在黏糊糊的营养液中，而思想则生活在 Matrix 中，一个虚拟现实空间。\n机器一开始创造了一个完美的 Matrix 空间，是一个毫无生活压力并实现了共产主义的乌托邦。 但人类天生的基因缺陷导致它们无法在这样的乌托邦中长久生存，很快出现大批量的死亡。 所以机器重新模拟了一个基于 1999 年真实人类社会现状的虚拟空间，在这里人类可以长久的存活下去，而当时实际的年代是 2199 年。 故事背景就交代到这里，再这么写下去就变成影片简介了。 下面我会站在程序背景的角度来提出一些设问并作答，如果你发现答案和你当初理解的不一致，不妨再重温一遍电影。\n红药丸还是蓝药丸？\n电影中的经典一幕是让 Neo 作出选择，选红药丸还是蓝药丸。\n红色药丸实际是一个跟踪程序（trace program）用来帮助定位 Neo 物理身体的位置。 为什么需要一个跟踪程序？做过分布式系统都会有深刻体会，解决一个大型分布式系统中的问题，第一个难点就在定位问题。 而 Matrix 实际就是一个连接全球人类的超大型分布式系统，需要定位一个个体，trace 程序必不可少。 （旁白：今天刚评审了我们系统的 trace 程序设计方案，想努力做的像红色药丸那么精巧啊）\nOracle 是谁？\n《黑客1》 Oracle 出场时确实没有交代她的身份，直到第二部结束时 Neo 与 Matrix 的 Architect 对话才得知。 Oracle 其实是一个程序，原文说法叫 Intuitive Program，一个人类直觉测试程序。 所以剧中，她一直在引导 Neo 和其他人通过直觉作出选择，而他们的选择对与 Matrix 至关重要，下面会详细说。\nNeo 为什么是 The One？\nNeo 作为男主角与其他人都不同，所以剧中给了他一个特殊的叫法 The One。 Neo 在第一次见 Oracle 时，Oracle 说他还不是 The One，还差了点什么，可能需要第二次生命。 Neo 在《黑客1》最后被 Agent 枪杀后，因为女主一吻获得对爱的感觉后重生才真正成为 The One。 而真正特殊的地方在于 Neo 始终认为自己是人，但它实际也是个程序，一个认为自己是人的程序，这正是他独一无二之处。\nMatrix 是连接全人类思维的虚拟空间，它实际是一个巨复杂的大型程序，这个程序运行的背后有精确的数学模型（剧中对话交代了的）。 它建立在一个精妙的数学模型之上，但却存在一个不平衡的因素，无论 Architect 如何调整都没法做到平衡稳定。 存在影响整个系统稳定性的非确定性因素，而这个因素正是始终会有人怀疑 Matrix 不是真实的。\n举个例子，假如 Matrix 背后的数学模型简化为:\n1/x\n这个数学式存在的意义是 x 不能为 0，当为 0 时 Matrix 将不复存在。 所以为了解决这个数学式的天然缺陷，Oracle 想到为 x 增加一个逆变量，那么数学式就变成下面这样：\n1/(x + n)\n增加了 n 这个逆变量，x 变量代表 Matrix 整体的运行变化，n 则代表 Neo 的存在，平时为 0。 当 x 可能向 0 变化时，n 这个变量被激活，避免分母为 0。 x 的变化不在 Architect 和 Oracle 的控制能力内，而 n 则通过 Oracle 去引导它的变化，以达成系统不崩溃的目标。 所以当 Neo 这个程序通过引入人类的爱情因子被激活后，Neo 就成了 The One，而数学式中的 n 也不再是 0 而是在不断变化着。 而这种变化趋势和方向就依赖 Oracle 来不断引导，所以怪不得最后 Architect 对 Oracle 说：你在玩一个危险的游戏。\nMatrix 是虚拟空间，那 Zion 就是现实么？\n《黑客1》给人一个误导以为 Zion（剧中从 Matrix 脱离的人类城市）所在的世界是真实的。 但当《黑客2》结尾 Neo 像手挡子弹那样挡住章鱼机器人时，已明确告诉我们，Zion 也不是真实的，它只是程序空间的另外一部分。 Matrix 是程序模拟的 1999 年人类世界，那么 Zion 所在的世界实际也是程序模拟的 2199 年的人类世界。\n按 Neo 和 Architect 的谈话，Zion 存在的作用是为了将不稳定因子（对 Matrix 有怀疑并觉醒的人类）聚集在一起，然后一次性清理。 在 Neo 之前实际已经有了 5 次类似的清理，前 5 代 The One 认识到自己其实不是人而是程序后，都选择了毁灭 Zion。 然后重新选择 16 女 7 男重建 Zion，消灭了 Matrix 的不稳定因子安全度过危机，开始下一代循环。 看到这里，作为一名程序员，我自然联想到这难道不是在用带 GC（垃圾回收）机制的程序在做比喻吗？\n程序古惑仔存在的意义是什么？\n电影还有这么一帮子人物角色，这帮人全是程序，一堆过时的程序，已被新的程序取代了。\n它们在 Matrix 中找不到工作，只好聚集在一个带头大哥下面，干起古惑仔来，搞点偷渡之类的事情。 在 Matrix 中每个程序都需要有存在的目的，否则只能被删除。 但这些程序可都是有智能和意识的，它们不想被系统清除掉，只好逃亡。 比如一个后台管理程序可以和一个界面交互程序结婚，还有了个女儿。 女儿自然也是一个程序，只是没什么用处，没用处就是没有存在目的程序，需要被删除，所以夫妻两找程序古惑仔帮忙逃亡。\n程序古惑仔帮控制了一个关键人物叫 KeyMaker，就是下面这位。\nKey 在计算机领域的专业术语是密钥，影片中用钥匙来比喻。 他掌握着进入系统 Source 的钥匙，所以 Neo 才会和程序古惑仔帮发生冲突。 程序古惑仔帮只不过是机器社会的一种特定角色，它和人类社会不无相似之处。\n最后，第六代 The One 的选择没有站在机器智能这边，因为爱情他选择了人类这边。 而一个可怕的病毒在 Matrix 中蔓延，连 Oracle 都被感染了。\nNeo 和机器智能达成了协议，帮助它消灭病毒程序来换取人类和机器的和平。 这一次 Zion 没被毁灭，比人类智能更高级的程序智能给了人类选择权。 人类可以选择活在机器文明创建的 Matrix 虚拟空间中或现实中。 而机器文明中也出现了一些拥有人类情感和意识的程序觉醒者，帮助机器文明进一步发展。 两种文明找到了一点点融合与平衡的空间，留下无限的思考和探讨空间。"}
{"content2":"人工智能、机器学习、模式识别、计算机视觉、数据挖掘、信息检索、自然语言处理等作为计算机科学重要的研究分支，不论是学术界还是工业界，有关这方面的研究都在如火如荼地进行着，学习这些方面的内容有一些经典书籍，现总结如下，方便自己和大家以后学习研究：\n人工智能：\n《Artificial Intelligence: A Modern Approach》，第三版，Russell著，权威、经典的人工智能教材，阐述了人工智能的核心内容，反映了人工智能最近10年来的新进展。\n《ProgrammingCollective Intelligence》，Toby Segaran著，本书将带你进入机器学习和统计学的世界，对算法的描述简明清晰，很对代码都可以直接拿去实际应用。\n数据挖掘：\n《DataMining, Concepts and Techniques》，第三版，Han著，数据挖掘领域最具里程碑意义的经典著作。\n《DataMining: Practical Machine Learning Tools and Techniques》,第二版，Witten著，介绍了机器学习的基本理论和实践方法，并提供了一个公开的数据挖掘工作平台Weka，算法部分介绍得很详细。\n信息检索：\n《An Introductionto Information Retrieval》,Manning著，这是一本介绍信息检索的入门书籍，书中对信息检索的基本概念和基本算法做了介绍，适合初学者。\n《Search Engines Information Retrieval in Practice》,Croft著，这本书讲述了搜索引擎的构造方法，通过实际代码展示了搜索引擎的工作原理，对于学生和从事相关领域的工程师，本书都值得一看。\n《Managing Gigabytes》，《Mining the Web -Discovering Knowledge from Hypertext Data》\n《Information Theory：Inference and Learning Algorithms》。\n模式识别和机器学习：\n《Pattern Classification 》，第二版，Duda著，模式识别的奠基之作，但对SVM、Boosting几乎没提，有挂一漏万之嫌。\n《Pattern Recognition and Machine Learning》,Bishop著，侧重概率模型，详细介绍了Bayesian方法、有向图、无向图理论等，体系完备。\n《Kernel Methods for Pattern Analysis》,John Shawe-Taylor著，SVM等统计学的诸多工具里都用到了核方法，可以将将低维非线性空间映射到高维的线性空间中，但同时会引入高维数据的难题。\n计算机视觉：\n《Computer Vision: A Modern Approach》，第二版，Forsyth著，一本不错的计算机视觉教材，全书理论联系实际，并加入了计算机视觉领域的最新研究成果。\n《Computer Vision: Algorithms and Applications》,Richard Szeliski的大作，《数字图像处理》课程老师推荐的一本书籍，这本书我还没有看完，书中对计算机视觉领域最新的一些算法进行了汇编，包括图像分割，特征检测和匹配，运动检测，图像缝合，3D重建，对象识别等图像处理的诸多方面，借助本书我们可以对最新主流图像处理算法有个全局把握。\n线性代数：\n《Linear Algebra and Its Applications》Fourth Edition, Gilbert Strang的著作，本书详细介绍了向量空间、线性变换、本征值和本征向量等线性代数的重要基本概念，把抽象的线性空间形象地表达出来，适合初学者。\n《Introduction to Probability Models》第10版，Ross著，一本书能够发行到第十版，你说是不是很经典呢？\n离散数学：\n《Discrete Mathematics and Its Applications》，第六版，Rosen著，本书囊括了离散数学推导、组合分析、算法及其应用、计算理论等多方面的内容，适合初学者。\n矩阵数学：\n《Matrix Analysis》,Horn著,本书无疑是矩阵论领域的经典著作了，风行几十年了。\n概率论与数理统计：\n《All Of Statistics》,Wasserman著，一本数理统计的简介读本。\n《Introductionto Mathematical Statistics》，第六版，Hogg著，本书介绍了概率统计的基本概念以及各种分布，以及ML，Bayesian方法等内容。\n《Statistical Learning Theory》Vapnik的大作，统计学界的权威，本书将理论上升到了哲学层面，他的另一本书《The Nature ofStatistical Learning Theory》也是统计学习研究不可多得的好书，但是这两本书都比较深入，适合有一定基础的读者。\n《统计学习方法》，李航著，国内很多大学都在用这本书，本书从具体问题入手，由浅入深，简明地介绍了统计学习的主要方法，适合初学者而又想对统计学习理论有一个全局理解的学生。\n《The Elements of Statistical Learning-Data Mining, Inference, and Prediction》,第二版，Trevor Hastie著，机器学习方面非常优秀的一本书，较PC和PRML,此书更加深入，对工程人员的价值也许更大一点。\n《AnIntroduction to Probabilistic Graphical Models》,Jordan著，本书介绍了条件独立、分解、混合、条件混合等图模型中的基本概念，对隐变量（潜在变量）也做了详细介绍，相信大家在隐马尔科夫链和用Gaussian混合模型来实现EM算法时遇到过这个概念。\n《Probabilistic Graphical Models-Principles and Techniques》，Koller著，一本很厚很全面的书，理论性很强，可以作为参考书使用。\n最优化方法：\n《Convex Optimization》，Boyd的经典书籍，被引用次数超过14000次，面向实际应用，并且有配套代码，是一本不可多得的好书，网址http://www.stanford.edu/~boyd/cvxbook/。\n《Numerical Optimization》，第二版，Nocedal著，非常适合非数值专业的学生和工程师参考，算法流程清晰详细，原理清楚。\n另外推荐几个博客和网站：\nhttps://www.coursera.org/，这是一个由世界顶级大学联合创办的网上在线视频公开课网站，里面有stanford, MIT,CMU等计算机科学一流大学提供的免费教学视频，内容全面，计算机科学方面的资源较网易视频公开课网站（http://open.163.com/）内容要新、要全。\nhttp://blog.csdn.net/pongba/article/details/2915005，本文的部分内容就是借鉴刘未鹏大神的博客而来的，也正是看过他的那个书单后，我才决定写一个总结归纳性的文章，这样可以方便大家学习，更可以勉励自己多看些有益的经典书籍。\nhttp://blog.pluskid.org/，这是浙大学生张驰原的博客网站，现在他去了MIT，博客里面的很多资源都值得一看，博文的很大一部分都是关于机器学习的，加入了作者自己的理解，深入浅出。\nhttp://blog.csdn.net/ffeng271/article/details/7164498，林达华推荐的基本数学书，转自MIT大牛博客。"}
{"content2":"转载请注明出处：http://www.cnblogs.com/ymingjingr/p/4271742.html\n目录\n机器学习基石笔记1——在何时可以使用机器学习(1)\n机器学习基石笔记2——在何时可以使用机器学习(2)\n机器学习基石笔记3——在何时可以使用机器学习(3)(修改版)\n机器学习基石笔记4——在何时可以使用机器学习（4）\n机器学习基石笔记5——为什么机器可以学习（1）\n机器学习基石笔记6——为什么机器可以学习（2）\n机器学习基石笔记7——为什么机器可以学习（3）\n机器学习基石笔记8——为什么机器可以学习（4）\n机器学习基石笔记9——机器可以怎样学习（1）\n机器学习基石笔记10——机器可以怎样学习（2）\n机器学习基石笔记11——机器可以怎样学习（3）\n机器学习基石笔记12——机器可以怎样学习（4）\n机器学习基石笔记13——机器可以怎样学得更好（1）\n机器学习基石笔记14——机器可以怎样学得更好（2）\n机器学习基石笔记15——机器可以怎样学得更好（3）\n机器学习基石笔记16——机器可以怎样学得更好（4）\n三、Types of Learning\n各种类型的机器学习问题。\n3.1 Learning with Different Output Space\n不同类型的输出空间。\n3.1.1 binary classification\n二元分类问题。\n前两章中提到的银行发信用卡问题就是一个典型的二元分类问题，其输出空间只包含两个标记+1和-1，分别对应着发卡与不发卡。\n当然二元分类问题包含多种情况，如2.3节中提到过，如-1所示。\n-1 a) 线性可分 b) 线性不可分包含噪音 c) 多项式可分\n-1a为线性可分（linear binary separable），如可以使用PLA求解；b是包含噪音可以使用pocket求解，而c会在后面章节中详细叙述，属于多项式可分解。当然解决以上三种二元分类问题的机器学习方法很多，因为二元分类问题是机器学习中很重要、核心的问题。\n3.1.2 Multiclass Classification\n多元分类。\n有二元分类，就不难想到多元分类的问题，该类问题输出标签不止两种，而是{1,2,…,K}。这在人们的生活中非常常见，比如给水果的图像分类，识别硬币等等，其主要的应用场景就是模式识别。\n3.1.3 Regression\n回归分析。\n该问题的输出空间为整个实数集上或者在一定的实数范围内，这和前面讲的分类问题完全不一样，该输出不是一种毫无意义的标记，而是有实际意义的输出值。比如给定一个大气数据可以推出明天的天气等等之类的问题。统计学习对该类问题的研究比较成熟。\n3.1.4 Structured Learning\n结构学习。\n当然还有其他更为复杂的问题，比如很多很多类型的分类问题。\n3.2 Learning with Different Data Label\n不同的数据标记。\n3.2.1 Supervised Learning\n监督学习。\n知道数据输入的同时还知道数据的标记。就相当于告诉你题目的同时还告诉你答案，让你在这种环境下学习，称之为监督学习（supervised learning）或者叫有师学习（learning with a teacher），之前讨论的一些算法都是这类问题。举个例子，硬币分类问题，如-2所示，其中横轴标示硬币的大小，纵轴标示硬币聚集的堆。\n-2 有监督的多类别分类问题\n其中这几种类别的硬币已经被各种不同的颜色所标示好。\n3.2.2 Unsupervised Learning\n无监督学习。\n这是一种没有标示（就是没有输出y）的问题，就是不告诉你题目的正确答案让你自己去寻找，再以硬币分类为例进行阐述，如-3所示。\n-3 无监督的多类别分类问题\n这种类型的问题最常见的是聚类或者叫分群（clustering），从图中不难看出无标示的难度比有标示的难度增加不少，而且极有可能犯错，但是这种问题却拥有广泛的应用场景（毕竟标示需要花费大量人力物力），如将新闻按照不同的主题聚类，按用户的属性将用户聚成不同类型的用户群等等。\n除了聚类之外还有其他的无监督学习，如密度评估（density estimation）和离群点检测（outlier detection）等等。\n3.2.3 Semi-supervised Learning\n半监督学习。\n是否能在监督式学习和无监督学习之间取一个中庸的方法呢？答案是可以的，就是半监督学习，它通过少量有标记的训练点和大量无标记的训练点达到学习的目的。还是以硬币为例，如-4所示。这种类型的例子也有很多，比如图像的识别，很多情况下我们不可能把每张图片都做上标记（因为做这种标记需要耗费大量的人力物力，是一种昂贵的行为），此时，使用半监督学习是一种不错的选择。\n-4 半监督学习\n3.2.4 Reinforcement Learning\n强化学习。\n前面三个是机器学习中最传统的三种方式，除此之外，还有一种方式是通过对一个行为作出奖励或者惩罚，以此获得的输出，进而进行学习，这种学习方式称之为强化学习。\n一般可以表示为，其中向量还是为输入向量，表示一种输出，注意并不一定是最佳输出，最后一项是对输出做出的评判。比如一个广告系统可以写成如下形式 。\n3.3 Learning with Different Protocol\n不同方式获取数据。\n对此节的内容进行简单阐述，在不同的协议中可以将机器学习分为三大类：\n批量（batch）学习就是将很多数据一次性的给算法进行学习，最常见的方式；\n在线（online）学习就是一点一点将数据传输进去，如PLA和增强学习都适用于这种形式；\n主动（active）学习是主动提出问题让算法解决，可以节省大量的训练和标记消耗。\n3.4 Learning with Different Input Space\n不同的输入空间。\n输入又可以称之为特征（features），其主要分为三种：\n具体特征（Concrete Features），具体特征最大特点就是便于机器学习的处理，也是基础篇中主要讨论的情形。这种情况是人类或者机器通过一定的方式提取获得的，具有实用性。\n原始特征（Raw Features），如图片的像素等等，是最为常见到的资料，但是需要经过处理，转换成具体特征，才容易使用，实用性不太大。\n抽象特征（Abstract Features），如一些ID之类的看似无意义的数据，这就更需要特征的转换、提取等工作（相对于原始特征而言），几乎没有实用性。"}
{"content2":"1.未来大数据的一切都都关于人\n...不论述\n2.大数据收集困难和高风险\n现在的大数据的来源，都是通过自有平台收集用户数据的，对于没有平台的企业很难有机制和渠道获取稳定的数据来源，\n有说通过法律和制度来，规范关于人的大数据使用，在这之前，唯一可行的方向是，把用户大数据使用，做成服务的必选功能，要使用服务，必须接受隐私风险。\n3.对大数据的利用方式\n现在的大数据 都是由各种数据聚合出一类关于人的结论 然后拿给企业使用\n我想大数据应用还有另一类\n利用网上的大数据，告诉某类人、甚至某个人，世界发生着什么，未来要发生什么\n4.科幻大片\n如果从网上下载一个专属的虚拟人物(虚拟机器人/虚拟助理/终生人工智能伴侣）到手机\n刚开始这个角色需要你教导（配置，类似早期的语音识别控制的学习阶段）才能帮你在互联网上你做一些简单的事情，\n随着你教导的更多更好（对你的习惯，兴趣爱好，思维模式，接收度等信息的收集） 和基于大数据的挖掘和分析能力的增加，它的智力越来越高，能做一些更复杂的任务（作为入口，代表你在互联网活动），\n直到很多年后，它的智力超过你，它能告诉你，世界发生着什么，未来要发生什么，它了解你的年龄，了解的行为习惯，了解你的经济能力，了解你缺点，时刻为你服务，扩展你的人生，使你的人生不局限于经验（网上有），思维（网上有），能更好的生活下去（帮你发现机会）。\n类似的东西，后来才发现微软小冰二代已经在着手做了，可能愿景和目的不一样，但轮廓有了。\n5.认知需求\n我们成长的时候，家长常常说“你懂点事吧”，但如何“懂事”没有人能教给我们，也没有告诉我们如何去做（去阅读书籍，去体验生活，经历人生）；\n我们毕业了，工作了，在社会上依然遇到，“各种成功学大师”，“各种领域专家”，“各种百家讲坛老师”，“各种转世神棍”，大行其道；\n这一切都是因为，人有认知的需求。\n6.认知培训\n基于这个需求，“认知服务”即使做不到自动，就算做人工服务，也能在现在的教育市场（学校，兴趣培训，英语培训，等技能培训）杀出一条出路\n7.认知模式与三分的认知世界\n莱考夫(George Lakoff)在 [女人、火和危险的事物]（[Women fire and dangerous things]）一书中，\n认为理念化的认知模式（idealized cognitive models 简称ICMs）是结构复杂的感知整体，是对世界的整体表征，它的价值在于对输入信息进行重组。ICMs并不客观存在的，而是人类实践和经验的高度概括和总结，并且可以为以后的实践提供参考。根据lakoff的论述，IMCs有四种：命题模式，意向图式模式，隐喻模式和转喻模式，它们的关系如下：\n命题模式是出发点和归宿，意象图式模式是基础，转喻模式和隐喻模式是建立在命题模式和意象图式模式上的认知事物的过程和方式，并且二者相互作用。\n任一认知主体的认知世界整体可以划分为三个部分：“信念世界”、“怀疑世界”和“无知世界”。我们用Wb表示信念世界，Wd表示怀疑世界，Wu表示无知世界。Wb、Wd、Wu是三个命题集合，他们的元素是相应的认知命题。简单地说，信念世界是由认知主体相信的命题构成，这些命题构成认知主体的信念；怀疑世界里的所有命题是认知世界说怀疑的，认知主体认为这些命题是“假的”或“不可能的”，这些命题可称为“疑点”；认知主体从来没有“考虑”过的命题构成无知世界的内容，这些命题（以及它们的负命题）或者仍没有进入认知主体的视野之中，或者虽然进入了认知主体的视野之中但认知主体不知道其意义，此时，认知主体对之既不相信又不怀疑，这些命题可称之为“盲点”。\n8.认知互联网世界进而认知现实世界\n互联网世界一直是现实世界的映射，越来越多的现实世界事物在互联网世界建立了数字化的映射,大数据的未来，就是通过这些大数据分析现实世界发生了什么，未来要发生什么，有可能到那时，数据获取能力的不平等将取代资本，脑力，智慧的不平等。\n9.包装的产品---认知的世界+个性数据=具有类似主人人格的终生人工智能伴侣\n10.通过逻辑推理认知世界--亚里士多德的三段论\n亚里士多德给出的经典的“Barbara”三段论：\n如果所有人（M）都是必死的（P），（大前提）\n并且所有希腊人（S）都是人（M），（小前提）\n那么所有希腊人（S）都是必死的（P）。（结论）\n如，\n所有人都是必死的。（普遍原理）\n苏格拉底是人。（特殊陈述）\n苏格拉底是必死的。[把特殊（小）代换入一般（大）]\n又如，\n法律规定这种行为要负法律责任，\n他/她做出了法律规定的这种行为\n他/她要负这个法律责任”\n从上面可看出，三段论由三个部分组成：大前提、小前提和结论，它在逻辑上是从大前提和小前提得出来的。大前提是一般性的原则。小前提是一个特殊陈述。在逻辑上，结论是从应用大前提于小前提之上得到的。\n与之相对的是隐喻，\n草（P）会死（M）.\n人（S）会死（M）.\n人（S）是草（P）.\n这是另一种形式的三段论，是逻辑谬论：这种形式的三段论是逻辑上无效的，但即使是这种逻辑上无效推理，也是人工智慧的表现。\n要取得认知互联网世界的能力，可以从哲学获得理论支持，例如从“三段论”和“三分的认知世界”，\n我们利用有限个的”大前提集合[命题集合]“组成”初始的信念世界“，大前提，小前提，对应[意象图式模式] 逻辑推导是“命题模式”，最终的结果是“命题被认为是真的，继而加入“信念世界”。\n利用隐喻[信念世界中存在的命题集合+新的命题]，推导出“怀疑世界”，那么剩下的就是“无知世界”，对于这个世界，随着现实世界的数字化程度越来越高，互联网世界对现实世界的映射会越来越完整，“无知世界”会越来越小。\n比如，\n水果是可以吃的[ 信念世界存在的命题]\n苹果是水果[ 信念世界存在的命题]\n苹果是可以吃的[要认知的命题]->[加入信念世界]\n苹果是可以吃的[信念世界存在的命题]\n苹果是红色的[信念世界存在的命题]\n西红柿是红色的[隐喻]\n西红柿是可以吃的[要认知的命题]->[加入怀疑世界]\n11.需要什么机制和技术\n不需要建立自有资料库，只需要有结论和认知过程\n需要有情报收集的能力（与各个情报来源建立通道，能识别相同情报）\n需要有对情报的认知能力（归类，推到，三个世界的命题集合）\nxuybin：标题很大，内容不成体系，纯属思考笔记，若转载注明出处http://www.cnblogs.com/xuybin/p/3966022.html\n12.云计算+大数据+数据挖掘+认知能力+学习机制\n需要多久才能成长出为1个2~3岁儿童的智能，它的极限是那？\n13.分词--词性标注--语言理解--信念世界--逻辑推理--怀疑世界--互联网学习回馈确认机制-大数据级别的认知世界（人类记忆）--认知能力\n只要求认知水果这类事物，甚至是 水果中的苹果这个事物，需要多久，多深的技术，多大的投入？\n14.与搜索引擎技术的区别\n搜索引擎的目的是进行相关度排序，面对的是一个整体的互联网数据；认知服务通过互联网对现实世界进行认知，被认知的对象本身就是可以分类的，且数量是有限的。可以说从出发点上，认知服务比搜索站在更高一个层次。\n15.与百度大脑、谷歌大脑、等人工智能的区别\n传统的人工智能是，通过“神经网络”，“机器学习” “深度学习”等技术和理论，实现 像人类大脑一样思考,像人类大脑一样学习。\n目标是，从源头、原理、数学理论上突破，取得彻底的普遍的人工智能大脑，在实施的过程中不断把新的发现，新的方法应用于它们现有的产品中去，提前为人类服务（但也许会像语音识别技术一样走入 语义识别这样的歧途）。\n认知服务的目标是，收集互联网的信息，通过认知后，得出现实世界发生的什么（首先要解决这个世界是什么样的？），认知的过程是手段，\n在当前 人工智能发展不成熟的阶段，该手段可以不完美，认知出的结果，可以是不正确的，偏执的，神经质的，甚至是反人类的，只要有一套认知机制\n结合包装的产品（游戏、娱乐），现阶段还是有市场的（看看微软小冰的轰动和反响），产品占领市场后，可以慢慢的改进认知手段。\n补充：引用19节的观点“技术的有效性要比科学的完整性更重要”\n16.认知服务的价值--技术价值，个人/社会价值，商业/产品价值\n了解技术深度，广度，积累技术壁垒，程序化 认知模式，和认知过程\n扩展个人的生活领域，“无知世界”缩小，程序化 认知模式，和认知过程中，提高自己的认知水平\n核心技术，跨界，适合任意包装产品，作为终生人工伴侣，和人的关系密切，可以形成收集个人数据并利用的闭环。\n2014-9-12更新\n17.虚拟客服产品--低层次的分词，分类知识库[特定领域和行业]\n我今天看到一个厂家做的东西，他们已经做了10年，还是在底层打转，申请了一些专利，但应用也只做到 虚拟客服这个阶段，\n它的名字叫做“小i机器人”，它的解决方案、产品服务，\n可以认为是具有认知服务的壳，做的是传统知识库问答的事，貌似存活得很好，而且获得了以下6个专利：\n一种实现网页自动客户服务的方法和装置\n一种对分享信息进行相应操作的方法、装置和设备\n一种用于进行语音识别的方法、装置和设备\n一种基于人工智能的知识问答快速处理系统\n一种客服系统及客服信息推送方法\n一种实现自动应答的系统及方法\n其中第三个专利，2013年被苹果申诉无效[因为Sir,不过该专利真心没有价值，只是把语音识别的一般过程+后端知识数据库结合一下就是一个专利，这还有活路吗]，但没有成功，2014年苹果把专利局和该公司一起告上法院，一审宣判败诉，现在等待二审。\n2014-9-15更新\n18.未来的操作系统--智能和个性化的裸机平台\n百度百科上关于“操作系统”的解释\n操作系统（英语：Operating System，简称OS）是管理和控制计算机硬件与软件资源的计算机程序，是直接运行在“裸机”上的最基本的系统软件，任何其他软件都必须在操作系统的支持下才能运行\n未来操作系统可能不再是一个呆傻、被动的运行媒介（除了自身更新），\n可能是一个集成 全天候待命（无关机动作，即时唤醒）、自主决策（自动更新，发起和管理任务，硬件故障诊断）、智能交互（语音,图像、传感器交互）、认知服务一系列功能的集合体，直到有一天“电脑”媲美“人脑”时，就不需要操作系统了，只需要“终生智能人工伴侣”。\n在这2个阶段，你都不需要知道 硬盘，CPU、内存这些概念，你面对的就是一个能够为你提供服务的虚拟角色，有一天基于ta在网上获取的最新认知[ta的组成硬件过时了]，和你自身的经济条件或预算，ta建议你升级硬件或者ta直接在电商下单，要求工作人员上门来进行更换服务。\n这一切很遥远，我也没法验证，但如果是这样的话，那么“让计算机认知互联网世界，进而理解人的世界，更具人性化，个性化”是变革的第一步。\n19.大数据时代的认知计算--中国工程院院士李德毅\n今天发现一篇2013年11月的文章，原来“科学家”早就看到了该问题，也许有企业正在做。\n感概，这个时代，那个行业那个领域都饱和了，更别提一个想法（曾经被千万个人想到过）了，重要的是做，去做你没有做过的，别人没有做好的。\n有时候，就当看看热闹，看得懂，也不错。\nhttp://www.csdn.net/article/2013-11-13/2817475-MDCC-Big-Data-Cognitive-Computing\n这篇文章很长，自己总结以下几点：\n认知可不可以计算？这是个严肃的问题，科学界争论了几十年，作者认为，是可以的，至少部分可以。\n科学是技术的先导，这句话不再是绝对的，随着IT技术的发展，技术也可以成为科学的先导，认知科学的发展还得靠认知计算。\n大数据时代，技术的有效性要比科学的完整性更重要，人为本的认知物联网的时代已经到来。"}
{"content2":"最近在看《机器学习实战》这本书，因为自己本身很想深入的了解机器学习算法，加之想学python，就在朋友的推荐之下选择了这本书进行学习，在写这篇文章之前对FCM有过一定的了解，所以对K均值算法有一种莫名的亲切感，言归正传，今天我和大家一起来学习K-均值聚类算法。\n一 K-均值聚类（K-means）概述\n1. 聚类\n“类”指的是具有相似性的集合。聚类是指将数据集划分为若干类，使得类内之间的数据最为相似，各类之间的数据相似度差别尽可能大。聚类分析就是以相似性为基础，对数据集进行聚类划分，属于无监督学习。\n2. 无监督学习和监督学习\n上一篇对KNN进行了验证，和KNN所不同，K-均值聚类属于无监督学习。那么监督学习和无监督学习的区别在哪儿呢？监督学习知道从对象（数据）中学习什么，而无监督学习无需知道所要搜寻的目标，它是根据算法得到数据的共同特征。比如用分类和聚类来说，分类事先就知道所要得到的类别，而聚类则不一样，只是以相似度为基础，将对象分得不同的簇。\n3. K-means\nk-means算法是一种简单的迭代型聚类算法，采用距离作为相似性指标，从而发现给定数据集中的K个类，且每个类的中心是根据类中所有值的均值得到，每个类用聚类中心来描述。对于给定的一个包含n个d维数据点的数据集X以及要分得的类别K,选取欧式距离作为相似度指标，聚类目标是使得各类的聚类平方和最小，即最小化：\n结合最小二乘法和拉格朗日原理，聚类中心为对应类别中各数据点的平均值，同时为了使得算法收敛，在迭代过程中，应使最终的聚类中心尽可能的不变。\n4. 算法流程\nK-means是一个反复迭代的过程，算法分为四个步骤：\n1） 选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心；\n2） 对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心（最相似）所对应的类；\n3） 更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值；\n4） 判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）。\n用以下例子加以说明：\n　　　　　　　　　　　　　\n　　　　　　　　　　　 　\n：给定一个数据集；\n：根据K = 5初始化聚类中心，保证　聚类中心处于数据空间内；\n：根据计算类内对象和聚类中心之间的相似度指标，将数据进行划分；\n：将类内之间数据的均值作为聚类中心，更新聚类中心。\n最后判断算法结束与否即可，目的是为了保证算法的收敛。\n二  python实现\n首先，需要说明的是，我采用的是python2.7，直接上代码：\n#k-means算法的实现 #-*-coding:utf-8 -*- from numpy import * from math import sqrt import sys sys.path.append(\"C:/Users/Administrator/Desktop/k-means的python实现\") def loadData(fileName): data = [] fr = open(fileName) for line in fr.readlines(): curline = line.strip().split('\\t') frline = map(float,curline) data.append(frline) return data ''' #test a = mat(loadData(\"C:/Users/Administrator/Desktop/k-means/testSet.txt\")) print a ''' #计算欧氏距离 def distElud(vecA,vecB): return sqrt(sum(power((vecA - vecB),2))) #初始化聚类中心 def randCent(dataSet,k): n = shape(dataSet)[1] center = mat(zeros((k,n))) for j in range(n): rangeJ = float(max(dataSet[:,j]) - min(dataSet[:,j])) center[:,j] = min(dataSet[:,j]) + rangeJ * random.rand(k,1) return center ''' #test a = mat(loadData(\"C:/Users/Administrator/Desktop/k-means/testSet.txt\")) n = 3 b = randCent(a,3) print b ''' def kMeans(dataSet,k,dist = distElud,createCent = randCent): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2))) center = createCent(dataSet,k) clusterChanged = True while clusterChanged: clusterChanged = False for i in range(m): minDist = inf minIndex = -1 for j in range(k): distJI = dist(dataSet[i,:],center[j,:]) if distJI < minDist: minDist = distJI minIndex = j if clusterAssment[i,0] != minIndex:#判断是否收敛 clusterChanged = True clusterAssment[i,:] = minIndex,minDist ** 2 print center for cent in range(k):#更新聚类中心 dataCent = dataSet[nonzero(clusterAssment[:,0].A == cent)[0]] center[cent,:] = mean(dataCent,axis = 0)#axis是普通的将每一列相加，而axis=1表示的是将向量的每一行进行相加 return center,clusterAssment ''' #test dataSet = mat(loadData(\"C:/Users/Administrator/Desktop/k-means/testSet.txt\")) k = 4 a = kMeans(dataSet,k) print a '''\n三 MATLAB实现\n之前用MATLAB做过一些聚类算法方面的优化，自然使用它相比python更得心应手一点。根据算法的步骤，编程实现，直接上程序：\n%%%K-means clear all clc %% 构造随机数据 mu1=[0 0 0]; S1=[0.23 0 0;0 0.87 0;0 0 0.56]; data1=mvnrnd(mu1,S1,100); %产生高斯分布数据 %%第二类数据 mu2=[1.25 1.25 1.25]; S2=[0.23 0 0;0 0.87 0;0 0 0.56]; data2=mvnrnd(mu2,S2,100); %第三个类数据 mu3=[-1.25 1.25 -1.25]; S3=[0.23 0 0;0 0.87 0;0 0 0.56]; data3=mvnrnd(mu3,S3,100); mu4=[1.5 1.5 1.5]; S4=[0.23 0 0;0 0.87 0;0 0 0.56]; data4 =mvnrnd(mu4,S4,100); %显示数据 figure; plot3(data1(:,1),data1(:,2),data1(:,3),'+'); title('原始数据'); hold on plot3(data2(:,1),data2(:,2),data2(:,3),'r+'); plot3(data3(:,1),data3(:,2),data3(:,3),'g+'); plot3(data4(:,1),data4(:,2),data3(:,3),'y+'); grid on; data=[data1;data2;data3;data4]; [row,col] = size(data); K = 4; max_iter = 300;%%迭代次数 min_impro = 0.1;%%%%最小步长 display = 1;%%%判定条件 center = zeros(K,col); U = zeros(K,col); %% 初始化聚类中心 mi = zeros(col,1); ma = zeros(col,1); for i = 1:col mi(i,1) = min(data(:,i)); ma(i,1) = max(data(:,i)); center(:,i) = ma(i,1) - (ma(i,1) - mi(i,1)) * rand(K,1); end %% 开始迭代 for o = 1:max_iter %% 计算欧氏距离,用norm函数 for i = 1:K dist{i} = []; for j = 1:row dist{i} = [dist{i};data(j,:) - center(i,:)]; end end minDis = zeros(row,K); for i = 1:row tem = []; for j = 1:K tem = [tem norm(dist{j}(i,:))]; end [nmin,index] = min(tem); minDis(i,index) = norm(dist{index}(i,:)); end %% 更新聚类中心 for i = 1:K for j = 1:col U(i,j) = sum(minDis(:,i).*data(:,j)) / sum(minDis(:,i)); end end %% 判定 if display end if o >1, if max(abs(U - center)) < min_impro; break; else center = U; end end end %% 返回所属的类别 class = []; for i = 1:row dist = []; for j = 1:K dist = [dist norm(data(i,:) - U(j,:))]; end [nmin,index] = min(dist); class = [class;data(i,:) index]; end %% 显示最后结果 [m,n] = size(class); figure; title('聚类结果'); hold on; for i=1:row if class(i,4)==1 plot3(class(i,1),class(i,2),class(i,3),'ro'); elseif class(i,4)==2 plot3(class(i,1),class(i,2),class(i,3),'go'); elseif class(i,4) == 3 plot3(class(i,1),class(i,2),class(i,3),'bo'); else plot3(class(i,1),class(i,2),class(i,3),'yo'); end end grid on;\n最终的结果如下和：\n\n\n总结：在这次程序的调试中，其实出现的问题还是蛮多的，相似度指标依旧选用的是欧氏距离。在之前，一直是按照公式直接计算的，可欧氏距离其实就是2范数啊，2范数属于酉不变范数，因此矩阵的2范数就是矩阵的最大奇异值，在求解过程中可以直接采用norm函数简化。\n上图中的结果可以清晰的看到算法具有一定的聚类效果，要进一步验证的话，可以采取MCR或者NMI和ARI这些常用的准则进行衡量聚类结果的优劣，在此我选取MCR进行验证，代码如下：\n%% 采用MCR判定聚类效果 B = class(:,4); B = reshape(B,1,row); A = [ones(1,100),2 * ones(1,100),3 *ones(1,100),4 * ones(1,100)]; sum = 0; for i = 1:row if ( A(1,i) ~= B(1,i)) sum = sum + 1; end end MCR = sum / row; fprintf('MCR = %d\\n',MCR);\n多次计算平均求得的MCR= 0.53,表明误分率还是蛮大的，聚类效果并不是很理想，究其原因：虽然算法收敛，但算法只是收敛到了局部最小值，而并非全局最小值，所以可以引入二分K-均值对算法进行优化。\n除此之外，FCM算法在一定程度上也是对算法的一个优化吧。\n进而导入UCI数据库中的wine数据进行测试，结果甚是不理想，至于原因吧，算法本身的性能是占一部分的，还有可能是数据的维数相对较多......在此我也不敢妄加猜测,之后慢慢验证吧......"}
{"content2":"共和国六十八年十月二十四日，今天是1024程序员节，我独在办公室抽烟思考着写一个学习人工智能的系列。遇见AI君，前来问我道，“先生可曾为程序员节写了一点什么没有？”我说“没有”。她就正告我，“先生还是写一点罢，程序员们之前就很爱看先生的文章。”\n这我是知道的，七年前我写过一个系列《Asp.Net大型项目实践》，阅读量每篇都上万，拿现在运营公众号的标准，也算是牛X的。通过写这个系列认识了很多朋友，有找我做项目的，有找我出书的，更有趣的是做项目开会偶尔碰到友商，还没等自我介绍，对方就会激动的说：“你...你...你是不是那个传说中的弦哥！”。当然那时候比较年轻，文风略有跋扈，也引来了不少喷子，在这里我就不点名批评了...\n日月如梭，这些年在“技术好不懂业务有什么用？”、“懂业务不会讲售前方案有什么用？”、“方案讲的好不会做项目管理有什么用？”、“项目管理的好不会和客户沟通什么用？”、“你自己牛X不会带团队有什么用？”、“技术团队再好拿不到单子有什么用？”这一系列的反问句中野蛮生长。现在做个项目对我来说没太大意思了，也会有做的不好的地方，但肯定不存在能力问题。\n有人会说，弦哥那你现在岂不是当上CEO迎娶白富美走上人生巅峰了？然而并没有...有两个瓶颈，商业模式和团队建设，这里就不展开了，做过的自然会懂，突破商业模式和带业务团队有多难。说到这里肯定有些键盘侠要来指点江山了，没错，就是你！我们团队正需要你这样的人才！\n抚摸着日渐拔高的发迹线和不再晨勃的身体，我沉吟到：“难道这就是传说的中年危机？\"。人类文明进程中的刀耕火种、工业革命我还没出生，信息革命赶上了，也参与其中算是做明白了。面对即将到来的第四次革命——人工智能，而立之年的我就只能做个碌碌无为的中年看客吗？\n答案是否定的，我想至少有两个理由，其一，虽然忙于俗务，有几年没完整写过代码了，但我还是非常热爱程序员这个职业，但数学和算法始终是我尴尬的存在，有必要在有生之年圆个梦，好不辜负了这门营生；其二，在日常工作生活的苟且中遇到傻X是大概率的事，而在数学和算法的世界中觉得自己是个傻X也是大概率的事，有助中年人调整心态，修身养性。\n也有同学可能会说，只有像你这样的中年大叔才会吃饱了撑的玩情怀，现在工作不好找啊！其实拿目前来说国内就有大把的职位，比如：金融高频交易、自动驾驶、大数据、图像识别、语音识别...\n这个系列打算同时从两条线去写（学习）：一条线以计算机算法为核心，主要是用python语言实现经典算法到人工智能算法，在开始阶段避开高等数学；一条线以一级高等数学为核心，主要包括高中数学（计数原理、集合、函数、不等式、几何、导数、向量等）、线性代数（矩阵分析）、微积分、离散数学、概率论。\n主要参考书目：\n数学类\n*基本都是国外的教材，侧重思维训练比较切合我们的学习目标，另外这里只列出我买的主线，另外像微积分和矩阵肯定要买同类书二刷的\n《人教版高中数学教材（5本必修+9本选修）》\n《普林斯顿微积分读本》\n《离散数学及其应用》\n《线性代数应该这样学》（图灵数学.统计学丛书）*这个系列的都不错\n《概率论及其应用》（图灵数学.统计学丛书）\n《应用随机过程 概率模型导论》（图灵数学.统计学丛书）\n《数值分析》（华章数学翻丛）*这个系列的都不错，但不太好买，淘宝有二手的\n《数学分析原理》（华章数学翻丛）\n计算机算法类\n《算法导论》*很经典的工具书，但翻译的非常差，有能力还是看原版吧\n《算法设计与分析基础》\n《数据挖据：概念与技术》\n《Python核心编程》\n《Python机器学习 预测分析核心算法》\n《函数式编程思维》\n《深度学习 人工智能算法》\n课外阅读\n《数学之美》\n最后,大家不要被这些东西给吓到，弦哥这种民科学习领悟这类长技术栈的东西是非常有经验的，我们会从小学数学开始学！虽然这个系列的数学起点很低，但涉及计算机和编程的知识是不会讲的，所以没有本科计算机基础的同学不适合这个系列。\n差点忘了，我们这个系列的名字叫《跟着弦哥学AI——从高中数学到人工智能》\n公众账号可以关注下"}
{"content2":"\"与其停留在概念理论层面，不如动手去实现一个简单demo 。\"       ——鲁迅\n没有源码都是耍流氓github\n前言\n目前提供AI开发相关API接口的公司有很多，国外如微软、谷歌，国内的百度、腾讯等都有开放API接口。开发者只需要调用相关接口，几步就能开发出一个“智能APP”。通常情况AI接口有以下几类：\n计算机视觉\n图像分类、图像目标检测以及视频检测跟踪等等。这类API主要用于处理图像和视频，能够给图像打tag，并分析视频图片中的物体及其对应坐标轨迹等。\n语言\n包括自然语言处理，分析自然语言含义，评估情绪等，例如机器翻译等。\n语音\n将语言音频转换为文本，使用声音进行验证，或向应用添加说话人识别。\n知识\n通过映射复杂信息和数据来解决任务，例如智能建议和语义搜索。\n基于Web Service的智能API接口让我们不需要了解复杂的机器学习以及数学知识就能轻松开发出智能APP。但是，本文将介绍如何完全自己动手去实现一个智能API接口服务，由于涉及到的东西非常多，本文仅以我比较熟悉的“计算机视觉”为例，包含“图像分类（image classification）”和“目标检测（target detect）”，之后如果有机会，我会介绍“视频轨迹跟踪”相关的东西，大概就是图像处理的升级版。在开始正文之前，先解释几个名词。AI的概念近一两年尤其火热，“机器学习”以及“深度学习”的技术介绍到处都是，这里再简单介绍一下我对它们的理解：\n人工智能：\n又名AI，概念出现得特别早，上世纪五六十年代就有。人工智能大概可以分为两大类，一类“强人工智能”，你可以理解为完全具备跟人类一样的思维和意识的计算机程序；第二类“弱人工智能”，大概就是指计算机能够完成大部分相对较高级的行为，比如前面提到的理解图片含义，理解语言含义以及理解语音等等。我们日常提到的人工智能通常指第二类，常见的有计算机视觉、语音识别、机器翻译、推荐系统、搜索引擎甚至一些智能美图的APP，这些都可以说使用了人工智能技术，因为它们内部都使用了相关机器学习或者深度学习的算法。\n机器学习：\n这个概念也出现得很早，大概上世界八九十年代（？）。以前的概念中，计算机必须按照人编写的程序去执行任务，对于程序中没有的逻辑，计算机是不可能去做的。机器学习出现后，计算机具备人类“掌握经验”的能力，在通过大量学习/总结规律之后，计算机能够预测它之前并没有见过的事物。\n深度学习：\n深度学习的概念近几年才出现，你可以理解为它是机器学习的升级。之所以近几年突然流行，是因为一些传统机器学习算法（比如神经网络）要想取得非常好的性能，神经网络必须足够复杂，同时需要大量的学习数据，这时计算能力遇到了瓶颈。而近几年随着硬件性能普遍提升，再加上互联网时代爆炸式的数据存储，训练出足够复杂的模型已经不再是遥不可及。因此，可以将深度学习理解为更复杂的机器学习方式。\n好了，基本概念理清楚之后，开始进入正题了。这次我需要实现计算机视觉中的两大智能API接口：图片分类和目标检测。\n技术和开发环境\n下面是用到的技术和环境：\n1）Python 3.5.2 (PIL、numpy、opencv、matplotlib等一些常见的库)\n2）Tensorflow 1.8.0(GPU版本)\n3）Keras 2.2.0 (backend是tensorflow)\n4）Yolo v3(目标检测算法)\n5）Windows 10 + Navida GTX 1080 显卡（需要安装cuda 和 cudnn）\n6）VS Code 1.19.3\n关于以上技术的介绍以及初次使用时的安装步骤，我这里不再多说了，网上教程很多，提示一下，初次安装环境，会有很多坑。一定要使用gpu版本的tensorflow，如果仅仅是自己搞着练练手，熟悉熟悉流程，安装cpu版本也行。\n接口定义\n好了，技术环境介绍完了之后，再把接口确定下来：\n名称\n接口\n参数\n返回\n在线图片检测\n/detect/online\nMethod=POST\nonline_image_url=url[string]\n{\n“image”:”result_url”,\n“results”:[\n{\n“box”:[left, top, right, bottom],\n“score”:score,\n“class”:class\n},\n{\n“box”:[left, top, right, bottom],\n“score”:score,\n“class”:class\n}\n...\n],\n“time”:create_time,\n“type”:”online”\n}\n本地图片检测\n/detect/local\nMethod=POST\nlocal_image=file data[byte]\nmultipart/form-data\n{\n“image”:”result_url”,\n“results”:[\n{\n“box”:[left, top, right, bottom],\n“score”:score,\n“class”:class\n},\n{\n“box”:[left, top, right, bottom],\n“score”:score,\n“class”:class\n}\n...\n],\n“time”:create_time,\n“type”:”local”\n}\n在线图片分类\n/classification/online\nMethod=POST\nonline_image_url=url[string]\n还没完成\n本地图片分类\n/classification/local\nMethod=POST\nlocal_image=file_data[byte]\nmultipart/form_data\n还没完成\n写这篇博客的时候，图片分类的模型还没有训练好，所以暂时放一下，下次更新。以上四个接口分两类，一类是提交在线图片的url即可，二类是提交本地图片文件（表单上传）。两类都需要POST方式提交，返回结果是json格式，里面包含了处理之后的图片url（所有的结果已经绘制在上面了），还有处理的raw_data，客户端收到这些raw_data后可以自己用作其他地方。\n目标检测\n目标检测算法使用的是YOLO V3，这里是C语言实现的版本：http://pjreddie.com/darknet/ 。由于我比较熟悉Python，所以我用的是另外一个Python版本的实现（基于Keras），这里是Keras版本的实现：https://github.com/qqwweee/keras-yolo3。 如果想要训练更好的模型，需要自己准备数据集，源码中有一个我写的开源工具，专门用来标记这个框架所用的数据集（这个工具需要.net 4.0+）。\n训练数据集使用的是微软的COCO数据集（https://github.com/cocodataset/cocoapi），这个也是C语言版本的默认数据集，你可以直接从官网上下载训练好的模型使用。\n图片分类\n待更新...\nWeb服务器\n由于是Web API，那么你首先必须得有一个自己的Web Server。因为这是一个demo程序，所以没必要使用类似Django 、Flask这样的框架，于是索性就自己写一个吧。功能很简单，提供静态文件访问、以及可以处理我的API接口就行，写完核心代码大约200行（包含API接口处理的逻辑）。整个Web程序用到的模块大概有：http.server、PIL、urllib、io、uuid、time、json、os以及cgi。可以看到并不复杂。\n整个Web Server的代码：\n处理逻辑\n从调用API接口到返回处理结果的流程相当简单，跟普通的HTTP请求一样，客户端发送HTTP请求，携带对象参数，Web Server在接收到数据后，开始调用计算模块，并将计算结果转换成json格式返回给客户端：\n图中橙色部分为关键部分，详细实现请参见源码中的vision模块。\nDemo效果\nDemo中写好了一个静态html页面，运行python server.py后，在浏览中访问：http://localhost:8080/web-app/index.html即可看见测试页面。左边为处理之后的图片，右边为返回的json结果。\n检测在线图片，在文本框中copy图片url，点击提交。\n上传本地图片，点击提交。\n与此同时，在控制台（或我自己的VS Code集成终端）中可以看到如下输出：\n最开始是检测花费的时间，接着就是检测到的目标物体以及对应的坐标、分数等等。后面是转换之后的json字符串，最后客户端根据json中的url加载处理之后的图片。\n视频目标跟踪\n这里稍微说一下跟视频有关的处理。对于视频来讲，它跟图片一样，由一张张图片组成，唯一的区别就是它具备时间的维度。我们不仅要检测每帧中的目标，还要判断前后帧之间各个目标之间的联系。然后利用目标物体的位移差来分析物体行为，对于路上车辆来讲，可以分析“异常停车”、“压线”、“逆行掉头”、“车速”、“流量统计”、“抛洒物”等数据。\n关于机器学习\nAI开发离不开机器学习（深度学习），而机器学习涉及到的知识相对来讲非常广泛，不仅仅要求开发者掌握好编程技能，还对数学知识有较高的要求。\n我认为作为普通程序员，如果要学习AI开发，请用一种Top Down的方式，抛开晦涩难懂的数学理论，先找个适合自己的机器学习框架（比如tensorflow或者基于它的keras），学会如何准备训练数据集（比如本文中如何去标记图片？），如何训练自己的模型，然后用训练得到的模型去解决一些小问题（比如本文中的图像目标检测）。等自己对机器学习有一种具体的认识之后，经过一段时间的摸索，会自然而然地引导我们去了解底层的数学原理，这个时候再去搞清楚这些原理是什么。\n个人认为，不要先上来就要搞懂什么是梯度下降优化法、什么是目标函数、什么是激活函数，什么是学习率...，这些概念确实需要掌握，但是不是你学习机器学习最开始的时候。另外学习机器学习，请使用Python。\n计划下一篇介绍基于图片识别的视频自动分类，比如自动鉴黄等软件。"}
{"content2":"1.先安装python（类似于java中的jdk）\n从官网下载python，python2和python3语法有点不同，选自己熟悉的即可（这有个坑，tensorflow目前不支持python3.7及以上的版本，所以建议，直接下载python3.6就ok了）\n点击install for all users，然后路径最好直接放在c盘下面（查找文件夹方便）\n安装的时候注意选择add enviriment variables（这样就不用自己配置环境变量了，美滋滋）\n2.安装pycharm(这个是python的IDE，也可以选用jupyter notebook)\n官网下载pycharm\n激活码 http://idea.imsxm.com\n3.安装numpy，scipy，panadas，matplotlib，sciki-learn等机器学习库\n（在线安装方式）\n1.直接打开windows命令行界面\n2.输入python，启动python编译器\n3.输入pip install +包名（如numpy,scipy,pandas,matplotlib,keras,tensorflow,scikit-learn）,就可以自动安装了\n（离线安装方式，先下载安装包，再安装）\n下载地址:http://www.lfd.uci.edu/~gohlke/pythonlibs/#matplotlib   （库名中带有cp的标识的是版本号，如果python是3.6的，则cp后面数字应该为36）\nNumPy-数学计算基础库：N维数组、线性代数计算、傅立叶变换、随机数等。\nSciPy-数值计算库：线性代数、拟合与优化、插值、数值积分、稀疏矩阵、图像处理、统计等。\nPandas-数据分析库：数据导入、整理、处理、分析等。\nmatplotlib-会图库：绘制二维图形和图表\nscikit-learn:Simple and efficient tools for data mining and data analysis\nAccessible to everybody, and reusable in various contexts\nBuilt on NumPy, SciPy, and matplotlib\nOpen source, commercially usable - BSD license\n安装如下：\n在第一步安装好的文件夹python中，新建一个Scripts的文件夹\n把下载的五个类库放到该文件夹中\n打开windows命令行，用命令行定位到该文件夹：cd c:\\python36\\Scripts\n按顺序安装五个类库，安装命令为：pip install +下载的类库名字；如果想卸载的话，命令为：pip uninstall+下载的类库名字\n4.用pycharm跑程序，测试是否安装成功\n# Code source: Jaques Grobler\n# License: BSD 3 clause\n#linear_model\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\n# Use only one feature\ndiabetes_X = diabetes.data[:, np.newaxis, 2]\n# Split the data into training/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n# Split the targets into training/testing sets\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\n# Create linear regression object\nregr = linear_model.LinearRegression()\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n% mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\nplt.xticks(())\nplt.yticks(())\nplt.show()\n如果安装成功，运行结果图如下："}
{"content2":"今天给大家介绍一下经典的开源机器学习软件：\n编程语言：搞实验个人认为当然matlab最灵活了（但是正版很贵），但是更为前途的是python（numpy+scipy+matplotlib)和C/C++，这样组合既可搞研究，也可搞商业开发，易用性不比matlab差，功能组合更为强大，个人认为，当然R和java也不错.\n1.机器学习开源软件网（收录了各种机器学习的各种编程语言学术与商业的开源软件）\nhttp://mloss.org\n2 偶尔找到的机器学习资源网：（也非常全，1和2基本收录了所有ML的经典开源软件了）\nhttp://www.dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Software/\n3 libsvm （支持向量机界最牛的，不用多说了，台湾大学的林教授的杰作）\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvm/\n4 WEKA （基于java的机器学习算法最全面最易用的开源软件）\nhttp://www.cs.waikato.ac.nz/ml/weka/\n5 scikit (本人最喜欢的一个基于python的机器学习软件，代码写得非常好，而且官方的文档非常全，所有都有例子，算法也齐全，开发也活跃\n，强烈推荐给大家用）\nhttp://scikit-learn.org/stable/\n6 OpenCv(最牛的开源计算机视觉库了，前途无可限量，做图像处理与模式识别的一定要用，总不能整天抱着matlab做实验和工业界脱节吧，但是有一定难度)\nhttp://opencv.willowgarage.com/wiki/\n7 Orange (基于c++和python接口的机器学习软件，界面漂亮，调用方便,可以同时学习C＋＋和python，还有可视化的功能，）\nhttp://orange.biolab.si/\n8 Mallet (基于JAVA实现的机器学习库，主要用于自然语言处理方面，特色是马尔可夫模型和随机域做得好，可和WEKA互补）\nhttp://mallet.cs.umass.edu/\n9 NLTK(PYTHON的自然处理开源库，非常易用，也强大，还有几本orelly的经典教程）\nhttp://nltk.org/\n10 lucene(基于java的包括nutch,solr,hadoop,mahout等全套，是做信息检索和搜索引擎的同志们必学的开源软件了，学JAVA的必学）\nhttp://lucene.apache.org/\n当然还有很多很好的开源软件了，以后陆续添加，待续，困了。。。。。"}
{"content2":"注：最近在工作中，高频率的接触到了SVM模型，而且还有使用SVM模型做回归的情况，即SVR。另外考虑到自己从第一次知道这个模型到现在也差不多两年时间了，从最开始的腾云驾雾到现在有了一点直观的认识，花费了不少时间。因此在这里做个总结，比较一下使用同一个模型做分类和回归之间的差别，也纪念一下与SVM相遇的两周年！这篇总结，不会涉及太多公式，只是希望通过可视化的方法对SVM有一个比较直观的认识。\n由于代码比较多，没有放到正文中，所有代码都可以在github中：link\n0. 支持向量机（support vector machine, SVM）\n原始SVM算法是由弗拉基米尔·万普尼克和亚历克塞·泽范兰杰斯于1963年发明的。1992年，Bernhard E. Boser、Isabelle M. Guyon和弗拉基米尔·万普尼克提出了一种通过将核技巧应用于最大间隔超平面来创建非线性分类器的方法。当前标准的前身（软间隔）由Corinna Cortes和Vapnik于1993年提出，并于1995年发表。\n上个世纪90年代，由于人工神经网络(RNN)的衰落，SVM在很长一段时间里都是当时的明星算法。被认为是一种理论优美且非常实用的机器学习算法。\n在理论方面，SVM算法涉及到了非常多的概念：间隔(margin)、支持向量(support vector)、核函数(kernel)、对偶(duality)、凸优化等。有些概念理解起来比较困难，例如kernel trick和对偶问题。在应用方法，SVM除了可以当做有监督的分类和回归模型来使用外，还可以用在无监督的聚类及异常检测。相对于现在比较流行的深度学习（适用于解决大规模非线性问题），SVM非常擅长解决复杂的具有中小规模训练集的非线性问题，甚至在特征多于训练样本时也能有非常好的表现（深度学习此时容易过拟合）。但是随着样本量$m$的增加，SVM模型的计算复杂度会呈$m^2$或$m^3$增加。\n在下面的例子中，均使用上一篇博客中提到的鸢尾属植物数据集。\n：Iris data set\n1. SVM的前身：感知机（Perceptron）\n感知机可以看做是低配版的线性SVM，从数学上可以证明：\n在线性可分的两类数据中，感知机可以在有限步骤中计算出一条直线（或超平面）将这两类完全分开。\n如果这两类距离越近，所需的步骤就越多。此时，感知机只保证给出一个解，但是解不唯一，如下图所示：\n：感知机训练出来的3个不同的线性分类器\n1.1 对二分类问题的具体描述\n训练样本$x \\in \\mathbb{ R }^{n}$，标签$y \\in {-1, 1}$，对于线性分类器来说：\n参数: $w \\in \\mathbb{ R }^n$ and $b \\in \\mathbb{ R }$\n决策边界（Decision boundary）：$w \\cdot x + b = 0$\n对于一个新的点$x$做分类时，预测标签为$sign(w \\cdot x + b)$\n参考上面的描述，在分类正确的情况下，如果一个点$x$的标签为$y = 1$，预测值$w \\cdot x + b > 0$，分类为1；标签为-1，预测值小于0，分类为-1. 那么可以使用$y (w \\cdot x + b) > 0 $来统一表示分类正确的情况，反之可以使用$y(w \\cdot x + b) < 0$来表示分类错误的情况。\n1.2 代价函数\n在分类正确时，即$y (w \\cdot x + b) > 0$，$loss = 0$;\n在分类错误时，即$y (w \\cdot x + b) \\leq 0$，$loss = -y (w \\cdot x + b)$.\n1.3 算法的流程\n利用随机梯度下降的方式训练模型，每次只使用一个样本，根据代价函数的梯度更新参数，\nstep1: 初始化$w = 0, b = 0$;\nstep2: 循环从训练集取样本，每次一个\nif $y (w \\cdot x + b) \\leq 0$（该样本分类错误）:\nw = w + yx\nb = b + y\n从流程上来看，每次取出一个样本点训练模型，而且只在分错的情况下更新参数，最终所有样本都分类正确时，模型训练过程结束。\n2. SVM - 线性可分\n在两类样本线性可分的情况下，感知机可以保证找到一个解，完全正确的区分这两类样本。但是解不唯一，而且这些决策边界的质量也不相同，直观上来看这条线两边的间隔越大越好。那么有没有一种方法可以直接找到这个最优解呢？这就是线性SVM所做的事情。\n从直观上来看，约束条件越多对模型的限制也就越大，因此解的个数也就越少。感知机的解不唯一，那么给感知机的代价函数加上更强的约束条件好像就可以减少解的个数。事实上也是这样的。\n2.1 SVM的代价函数\n在分类正确时，即$y (w \\cdot x + b) > 1$，$loss = 0$;\n在分类错误时，即$y (w \\cdot x + b) \\leq 1$，$loss = -y (w \\cdot x + b)$.\n比较一下可以发现，原来$w \\cdot x + b$只需要大于0或小于0就可以了，但是现在需要大于1或小于1. 在这里为什么选择1我还没有很直观的解释，但是有一点非常重要：原来的决策边界只是一条直线，现在则变成了一条有宽度的条带。原来差异非常小的两个点（例如$w \\cdot x + b = 0$附近的两个点）就可以被分成不同的两类，但是现在至少要相差$\\frac{2}{||w||}$才可以，如下图所示。\n：设样本属于两个类，用该样本训练SVM得到的最大间隔超平面。在超平面上的样本点也称为支持向量。\n2.2 决策边界以及间隔\n来自wiki，为了统一起见，下面还是将决策边界定义为$w \\cdot x + b = 0$，两边的边界（两条虚线）分别为$w \\cdot x + b = 1$和$w \\cdot x + b = 1$，此时只是b的符号不同其他性质都相同. 其中$w, b$就是模型训练时需要优化的参数。由上面的示意图可以得到以下信息：\n两条虚线之间的距离为$\\frac{2}{||w||}$；\n待优化参数$w$的方向就是决策边界的法向量方向（$w$与决策边界垂直）；\n此时边界上一共有3个点，这三个点也就是此时的支持向量。\n下面是计算两条虚线之间距离的过程：\n将决策边界的向量表示$w·x + b = 0$展开后可以得到，$w1*x1 + w2*x2 + b = 0$.\n转化成截距式可以得到，$x2 = - w1/w2 * x1 - b/w2$，因此其斜率为$-w1/w2$, 截距为$-b/w2$\n直线的方向向量为，$(1, -w1/w2)$（可以取x=1, b=0时，得到y的值）\n直线的法向量为$w = (w1, w2)$\n因此，对于直线$w \\cdot x + b = 1$来说，截距式为$x2 = - w1/w2 * x1 + (1 - b)/w2$，相当于沿着$x2$轴向上平移了$\\frac{1}{w_2}$，计算可得该直线与$w \\cdot x + b = 0$沿法向量方向的距离为$\\gamma = \\sqrt{\\frac{1}{w_1^2 + w_2^2}} = \\frac{1}{||w||}$，参考.\n：margin的宽度$\\gamma$\n2.3 优化目标\n在SVM中，优化的目标就是最大化margin的宽度$\\gamma$，因为$\\gamma = \\frac{1}{||w||}$，其中$||w||$是待优化参数$w$的模长。因此优化目标等价于最小化$||w||$，可以表示为为：\n对于$(x^{(1)}, y^{(1)}), \\ ..., \\ (x^{(m)}, y^{(m)}) \\in \\mathbb{R^d} \\times \\{-1, 1\\}$，$\\min_{w \\in \\mathbb{R}^d, b \\in \\mathbb{R}}||w||^2$\ns.t. $y^{(i)}(w \\cdot x^{(i)} + b) ≥ 1$对于所有的$i = 1, 2, ..., m$成立\n下面是分别使用感知机和SVM对鸢尾属数据集中setosa这一类和非setosa进行分类的效果比较：\n：感知机线性分类器\n：线性SVM的分类效果\n比较和可以看到，SVM确定的决策边界周围的margin更大一些，因此对更多未知的样本进行分类时，在边界上的一些点可以得到更准确的分类结果。\n3. SVM - 线性不可分\n在中可以看到，setosa这一类与其他两类是线性可分的，但是virginica这一类与与之相邻的versicolor有一些点是重合的，也就是说是线性不可分的。此时仍然可以使用SVM来进行分类，原理是在代价函数中加入了一个松弛变量(slack) $\\xi$，\n对于$(x^{(1)}, y^{(1)}), \\ ..., \\ (x^{(m)}, y^{(m)}) \\in \\mathbb{R^d} \\times \\{-1, 1\\}$，$\\min_{w \\in \\mathbb{R}^d, b \\in \\mathbb{R}}||w||^2 + C\\sum_{i=1}^{m}{\\xi^i} $\ns.t. $y^{(i)}(w \\cdot x^{(i)} + b) ≥ 1 - \\xi_i$对于所有的$i = 1, 2, ..., m$成立\n上面的优化目标加入松弛变量后，就可以允许一定程度的违反两边的边界（由上式中的C来控制），允许一定的错误分类，从而将两类原来线性不可分的两类数据分开。\n下面是$C=1000$时，对virginica和非virginica的分类效果：\n：加入松弛变量后的SVM分类效果\nC作为SVM模型的超参数之一，需要从一个较大的范围中一步一步的筛选，直到找到最适合的C。C值越大，表示错误分类的代价越大，就越趋于拒绝错误分类，即hard margin；C值越小，表示错误分类的代价越小，就越能容忍错误分类，即soft margin。即使是在线性可分的情况下，如果C设置的非常小，也可能导致错误分类的出现；在线性不可分的情况下，设置过大的C值会导致训练无法收敛。\n4. SVR - 利用SVM做回归分析\n支持向量回归模型（Support Vector Regression， SVR）是使用SVM来拟合曲线，做回归分析。分类和回归问题是有监督机器学习中最重要的两类任务。与分类的输出是有限个离散的值（例如上面的$\\{-1, 1\\}$）不同的是，回归模型的输出在一定范围内是连续的。下面不再考虑不同鸢尾花的类型，而是使用花瓣的长度（相当于自变量x）来预测花瓣的宽度（相当于因变量y）。\n下图中从所有150个样本中，随机取出了80%作为训练集：\n：训练SVR模型的训练样本\n下面是使用线性SVR训练出来的回归线：\n：SVR模型训练出来的回归线\n与SVM是使用一个条带来进行分类一样，SVR也是使用一个条带来拟合数据。这个条带的宽度可以自己设置，利用参数$\\epsilon$来控制：\n：SVR模型回归效果示意图，其中带红色环的点表示支持向量\n在SVM模型中边界上的点以及两条边界内部违反margin的点被当做支持向量，并且在后续的预测中起作用；在SVR模型中边界上的点以及两条边界以外的点被当做支持向量，在预测中起作用。按照对偶形式的表示，最终的模型是所有训练样本的线性组合，其他不是支持向量的点的权重为0. 下面补充SVR模型的代价函数的图形：\n：soft margin SVR的代价函数\n从中可以看到，在margin内部的这些点的error都为0，只有超出了margin的点才会计算error。因此SVR的任务就是利用一条固定宽度的条带(宽度由参数$\\epsilon$来控制)覆盖尽可能多的样本点，从而使得总误差尽可能的小。\nReference\nhttps://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\nhttps://zhuanlan.zhihu.com/p/26263309, 直线方程的各种形式\nhttps://github.com/ageron/handson-ml/blob/master/05_support_vector_machines.ipynb\nhttp://www.svms.org/regression/SmSc98.pdf\nhttp://www.robots.ox.ac.uk/~az/lectures/ml/\nedx: UCSanDiegoX - DSE220x Machine Learning Fundamentals\nhttps://github.com/OnlyBelter/jupyter-note/blob/master/machine_learning/SVM/04_how%20SVM%20becomes%20to%20SVR.ipynb, 文中代码"}
{"content2":"<!-- pre { margin-top: 0; max-width: 95%; border: 1px solid #ccc; white-space: pre-wrap; } pre code { display: block; padding: 0.5em; } code.r, code.cpp { background-color: #F8F8F8; } table, td, th { border: none; } blockquote { color:#666666; margin:0; padding-left: 1em; border-left: 0.5em #EEE solid; } hr { height: 0px; border-bottom: none; border-top-width: thin; border-top-style: dotted; border-top-color: #999999; } @media print { * { background: transparent !important; color: black !important; filter:none !important; -ms-filter: none !important; } body { font-size:12pt; max-width:100%; } a, a:visited { text-decoration: underline; } hr { visibility: hidden; page-break-before: always; } pre, blockquote { padding-right: 1em; page-break-inside: avoid; } tr, img { page-break-inside: avoid; } img { max-width: 100% !important; } @page :left { margin: 15mm 20mm 15mm 10mm; } @page :right { margin: 15mm 10mm 15mm 20mm; } p, h2, h3 { orphans: 3; widows: 3; } h2, h3 { page-break-after: avoid; } } -->\n<!-- pre .operator, pre .paren { color: rgb(104, 118, 135) } pre .literal { color: rgb(88, 72, 246) } pre .number { color: rgb(0, 0, 205); } pre .comment { color: rgb(76, 136, 107); } pre .keyword { color: rgb(0, 0, 255); } pre .identifier { color: rgb(0, 0, 0); } pre .string { color: rgb(3, 106, 7); } -->\n前言\n近年来，Machine Learning 在许多领域上已然取得了可喜的成就，非常火热。就我个人来讲，有意将业余 Sport Programming 的范围扩展一下，譬如 Topcoder Marathon。在解决实际问题中，方法太 Naive 往往效果不怎么样，依旧需要学习一下相关的基础知识。\n本系列文章主要基于 Coursera 的 Machine Learning，我社内部 Machine Learning 课里能说的一部分，wikipedia，以及一些其他的读物。\n一些概念\n机器学习的定义\n对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序从经验E中学习。\n这是一个比较严谨的界定机器学习问题的 Guideline。如果有什么问题搞不清楚是不是这个范畴，可以尝试套用定义来检查：任务是什么，性能度量是什么，经验是什么，性能是否由于经验而提升。\n一些机器学习应用\n手写识别，Optical character recognition\n文本分类，识别垃圾邮件，工口反动内容等\n语音识别，机器翻译等\n图像识别，人脸识别\n识别钓鱼网站\n机器人，无人机等\n一些机器学习问题\n分类(Classiﬁcation)：给每组输入打一个 tag，譬如手写识别，实际上相当于对一个图像进行分类\n回归(Regression)：对每组输入，预测一个实数值，譬如预测股市行情\nRanking：将输入排序，譬如对搜索引擎的搜索结果，推荐系统等\n聚类(Clustering)：将输入数据分成若干类\n降维(Dimensionality reduction)：寻找输入数据的低维表示\n一些定义\n样例(Example)：某个实体\nFeatures：实体的属性集合，通常用向量来表示\nLabel：对于分类问题，就是样例属于哪一类；对于回归问题，就是实数值\n训练集(Training Data)：用来训练模型\nValidation set：往往用来调整学习的参数\n测试集(Test Data)：用来评估模型的表现\n监督学习：从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果\n非监督学习：训练集没有人为标注的结果\n区别：训练数据有没有标注\n机器学习三要素：模型，策略，算法\n模型：就是所要学习条件概率分布或决策函数\n策略：按照什么样的准则来学习或者挑选模型\n算法：学习模型的具体计算方法，即用什么样的方法来求得最优解\n个人理解，模型代表着你如何看待这个问题。譬如识别一个东西是不是汽车，如果你认为识别的依据是：金属壳 + 车灯 + 反光镜 + 车轮子 … = 汽车，这个思路就比较接近基于规则，决策树，贝叶斯；如果你考虑这个东西和见过的什么东西比较相似，就是 KNN 的思路。之后我们要不断学习的实际上都是模型。\n常见的两个策略是经验风险最小化和结构风险最小化。经验风险最小化意味着我们倾向于对训练数据取得精准的预测。这个想法很直接，且有一定道理：模型在训练数据上表现不佳，更无法指望在测试数据上取得好结果。但是，在训练集上表现好的模型，未必在测试集上表现好。通常来讲，简单的模型会更有通用性，而复杂的模型，往往会有一些 hardcode 了训练数据的感觉，效果反而不一定好。结构风险最小化在经验风险最小化的情况下，加入一些因子来限制模型的复杂度。\n根据策略，可以列出一个需要最优化的式子。算法就是求这个式子最优或者较优解的方法。最常见的方法是梯度下降，其他技能还没有 get，就暂不讨论了。\n梯度下降法(Gradient Descent)\n暂时忘记机器学习，现在需要优化一个形如 \\( y = f(\\theta) \\) 的式子，求 \\( x = argmax f(\\theta) \\) 或 \\( x = argmin f(\\theta) \\)，有什么好的办法么？\n梯度下降法，基于这样的观察：如果实值函数\\( F(x) \\)在点 a 处可微且有定义，那么函数 \\( F(x) \\)在 a 点沿着梯度相反的方向 \\( -F\\nabla(a) \\) 下降最快。因此，如果 \\( b = a - \\gamma\\nabla F(a) \\) 对于 \\( \\gamma > 0 \\) 且为一个够小数时成立，那么 \\( F(a) \\geq F(b) \\)。换句话说，我们给出一个对极值的估计 a，不断迭代求 \\( a = a - \\gamma\\nabla F(a) \\) ，就能取得一个极值。\n用一个实际例子来演示一下：对二次函数 \\( f(y) = x^{2} + 2x + 10 \\) ，使用梯度下降法求 \\( min f(x) \\) 和 \\( argmin f(x) \\)，函数图像如下：\n结论无论是从图像还是初中数学的角度来看都很简单。我们看看梯度下降算法是如何进行的：\nf <- function(x) { x^2 + 2 * x + 10 } df <- function(x) { 2 * x + 2 } x <- 5 y <- f(x) learning.rate <- 0.3 plot(f, -5, 5) while (TRUE) { nx = x - df(x) * learning.rate ny = f(nx) if (abs(x - nx) < 0.01) break arrows(x, y, nx, ny, col = \"red\") x = nx y = ny print(c(x, ny)) }\n## [1] 1.40 14.76 ## [1] -0.040 9.922 ## [1] -0.616 9.147 ## [1] -0.8464 9.0236 ## [1] -0.9386 9.0038 ## [1] -0.9754 9.0006 ## [1] -0.9902 9.0001\n无论是简单问题还是复杂问题，参数 learning.rate，也就是前文中提到的\\( \\gamma \\)的选择非常重要。Learning rate 过小则需要更多的迭代。Learning rate 过大则会出现之字下降，甚至之字上升。\n看一个非凸，多元函数的例子：Rosenbrock函数：\\( f(x, y) = (1-x)^2 + 100(y-x^2)^2 \\) 很显然 x = y = 1 的时候可以取得最优解，但是求解过程却是很坑的。咱们把 x = y = 1 附近的图像画出来：\n再研究一下 x = 1 时的切面：\n大概能看出来，这个函数在解附近有个很大的很平的底。。。贴一段代码，大家可以 play 一下：\nf <- function(x, y) { (1 - x) ** 2 + 100 * (y - x ** 2) ** 2} df.dx <- function(x, y) { x * 2 - 2 - 400 * y + 400 * x ** 3} df.dy <- function(x, y) { 200 * y - 200 } x <- runif(1, 0, 2) y <- runif(1, 0, 2) z = f(x, y) learning.rate = 1E-6 eps <- 1E-10 while (TRUE) { new.x = x - df.dx(x, y) * learning.rate new.y = y - df.dy(x, y) * learning.rate new.z = f(new.x, new.y) if (abs(new.z - z) < eps) break x = new.x y = new.y z = new.z print(c(x, y, z)) }\n可以调整一些参数，譬如 learning.rate，eps 去看看某些现象。我们可以看到他最后几步的收敛极为缓慢，如果 learning.rate 过大，还会之字上升等等。总的来讲，选择一个合适的 learning rate 是非常重要的，除去经验性的技巧，往往也只好枚举了，看看 cost function 的变化情况，如果下降过慢，则需要增大 learning rate，如果反而增长了，则需要减少 learning rate。这也就是为什么某些时候我们需要一个比较小的 validate set，我们可以定期的在训练中的模型上跑一下 validate set，看一下 cost function 的变化，从而决定 learning rate 的调整。\n一元线性回归\n以 Stanford Machine Learning 为例：根据房子的面积预测房价。咱们来把一些概念对上号：\nTraining Set：m 个二元对 \\( (x_{i},y_{i}) \\)\nfeature：房屋面积，即 \\( x_{i} \\)\nlabel：房价，即 \\( y_{i} \\)\n这是监督学习，因为测试数据是标注过的\n这是回归问题，因为 label 是连续值\n模型：一元线性回归，这个想法很显然\n策略：\\[ minimize J(\\theta_{0}, \\theta_{1}) = \\sum_{i=1}^{m}(\\theta_{0}x_{i} + \\theta_{1} - y_{i})^2 \\]\n算法：注意此时我们要求解的是 \\( \\theta_{0},\\theta_{1} \\)，而 \\( x_{i},y_{i} \\) 都是已知量，可以考虑求偏导，然后用梯度下降求解，这就是技能范围以内的东西了，因为每次迭代用了所有的 Training Data，所以这个做法叫 Batch Gradient Descent。\n实际应用中，比较好用的算法是 Stochastic Gradient Descent，Batch Gradient Descent 每次迭代，对 \\( \\sum_{i=1}^{m}(\\theta_{0}x_{i} + \\theta_{1} - y_{i})^2 \\) 求导，相当于 \\[ \\theta_{0} = \\theta_{0} - 2\\alpha\\sum_{i=1}^{m}(\\theta_{0}x_{i} + \\theta_{1} - y_{i})x_{i} \\] \\[ \\theta_{1} = \\theta_{1} - 2\\alpha\\sum_{i=1}^{m}(\\theta_{0}x_{i} + \\theta_{1} - y_{i}) \\] 而 Stochastic Gradient Descent 相当与把 Batch Gradient Descent 的 1 次迭代拆成了 m 次，每次对 \\( （\\theta_{0}x_{i} + \\theta_{1} - y_{i})^2 \\) 求导，然后 \\[ \\theta_{0} = \\theta_{0} - 2\\alpha(\\theta_{0}x_{i} + \\theta_{1} - y_{i})x_{i} \\] \\[ \\theta_{1} = \\theta_{1} - 2\\alpha(\\theta_{0}x_{i} + \\theta_{1} - y_{i}) \\]\nBatch Gradient Descent 可以求得更精确的解，但是如果模型复杂，或者数据量大，就很难直接 Batch Gradient Descent 了。"}
{"content2":"声明：本篇博文根据http://www.ctocio.com/hotnews/15919.html整理，原作者张萌，尊重原创。\n机器学习无疑是当前数据分析领域的一个热点内容。很多人在平时的工作中都或多或少会用到机器学习的算法。本文为您总结一下常见的机器学习算法，以供您在工作和学习中参考。\n机器学习的算法很多。很多时候困惑人们都是，很多算法是一类算法，而有些算法又是从其他算法中延伸出来的。这里，我们从两个方面来给大家介绍，第一个方面是学习的方式，第二个方面是算法的分类。\n博主在原创基础上加入了遗传算法（2.9）的介绍，这样一来，本篇博文所包含的机器学习算法更加全面丰富。该博文属于总结型文章，如想具体理解每一个算法的具体实现方法，还得针对逐个算法进行学习和推敲。\n1. 学习方式\n根据数据类型的不同，对一个问题的建模有不同的方式。在机器学习或者人工智能领域，人们首先会考虑算法的学习方式。在机器学习领域，有几种主要的学习方式。将算法按照学习方式分类是一个不错的想法，这样可以让人们在建模和算法选择的时候考虑能根据输入数据来选择最合适的算法来获得最好的结果。\n1.1 监督式学习\n在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）。\n1.2 非监督式学习\n在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。\n1.3 半监督式学习\n在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。\n1.4 强化学习\n在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。\n在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。\n2. 算法分类\n根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题。这里，我们尽量把常用的算法按照最容易理解的方式进行分类。\n2.1 回归算法\n回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。\n2.2 基于实例的算法\n基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM）。\n2.3 正则化方法\n正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression， Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（Elastic Net）。\n2.4 决策树学习\n决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）\n2.5 贝叶斯方法\n贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators， AODE），以及Bayesian Belief Network（BBN）。\n2.6 基于核的算法\n基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA)等。\n2.7 聚类算法\n聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。\n2.8 关联规则学习\n关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。\n2.9 遗传算法（genetic algorithm）\n遗传算法模拟生物繁殖的突变、交换和达尔文的自然选择（在每一生态环境中适者生存）。它把问题可能的解编码为一个向量，称为个体，向量的每一个元素称为基因，并利用目标函数（相应于自然选择标准）对群体（个体的集合）中的每一个个体进行评价，根据评价值（适应度）对个体进行选择、交换、变异等遗传操作，从而得到新的群体。遗传算法适用于非常复杂和困难的环境，比如，带有大量噪声和无关数据、事物不断更新、问题目标不能明显和精确地定义，以及通过很长的执行过程才能确定当前行为的价值等。同神经网络一样，遗传算法的研究已经发展为人工智能的一个独立分支，其代表人物为霍勒德（J.H.Holland）。\n2.10 人工神经网络\n人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。\n2.11 深度学习\n深度学习算法是对人工神经网络的发展。 在近期赢得了很多关注， 特别是百度也开始发力深度学习后， 更是在国内引起了很多关注。   在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。\n2.12 降低维度算法\n像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis， PCA），偏最小二乘回归（Partial Least Square Regression，PLS）， Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）,  投影追踪（Projection Pursuit）等。\n2.13 集成算法\n集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization， Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest），GBDT（Gradient Boosting Decision Tree）。"}
{"content2":"学了Python可以做什么工作\n用 Python 写爬虫\n据我所知很多初学 Python 的人都是使用它编写爬虫程序。小到抓取一个小黄图网站，大到一个互联网公司的商业应用。通过 Python 入门爬虫比较简单易学，不需要在一开始掌握太多太基础太底层的知识就可以很快上手，而且很快可以做出成果，非常适合小白一开始想做出点看得见的东西的成就感。\n除了入门，爬虫也被广泛应用到一些需要数据的公司、平台和组织，通过抓取互联网上的公开数据，来实现一些商业价值是非常常见的做法。当然这些选手的爬虫就要厉害的多了，需要处理包括路由、存储、分布式计算等很多问题，与小白的抓黄图小程序，复杂度差了很多倍。\nWeb 程序\n除了爬虫，Python 也广泛应用到了 Web 端程序，比如你现在正在使用的知乎，主站后台就是基于 Python 的 tornado 框架，豆瓣的后台也是基于 Python。除了 tornado (Tornado Web Server)，Python 常用的 Web 框架还有 Flask(Welcome | Flask (A Python Microframework))，Django (The Web framework for perfectionists with deadlines) 等等。通过上述框架，你可以很方便实现一个 Web 程序，比如我认识的一些朋友，就通过 Python 自己编写了自己的博客程序，包括之前的 zhihu.photo，我就是通过 Flask 实现的后台(出于版权等原因，我已经停掉了这个网站)。除了上述框架，你也可以尝试自己实现一个 Web 框架。\n桌面程序\nPython 也有很多 UI 库，你可以很方便地完成一个 GUI 程序(话说我最开始接触编程的时候，就觉得写 GUI 好炫酷，不过搞了好久才在 VC6 搞出一个小程序，后来又辗转 Delphi、Java等，最后接触到 Python 的时候，我对 GUI 已经不感兴趣了)。Python 实现 GUI 的实例也不少，包括大名鼎鼎的 Dropbox，就是 Python 实现的服务器端和客户端程序。\n人工智能(AI)与机器学习\n人工智能是现在非常火的一个方向，AI热潮让Python语言的未来充满了无限的潜力。现在释放出来的几个非常有影响力的AI框架，大多是Python的实现，为什么呢?因为Python足够动态、具有足够性能，这是AI技术所需要的技术特点。比如基于Python的深度学习库、深度学习方向、机器学习方向、自然语言处理方向的一些网站基本都是通过Python来实现的。\n机器学习，尤其是现在火爆的深度学习，其工具框架大都提供了Python接口。Python在科学计算领域一直有着较好的声誉，其简洁清晰的语法以及丰富的计算工具，深受此领域开发者喜爱。\n早在深度学习以及Tensorflow等框架流行之前，Python中即有scikit-learn，能够很方便地完成几乎所有机器学习模型，从经典数据集下载到构建模型只需要简单的几行代码。配合Pandas、matplotlib等工具，能很简单地进行调整。\n而Tensorflow、PyTorch、MXNet、Keras等深度学习框架更是极大地拓展了机器学习的可能。使用Keras编写一个手写数字识别的深度学习网络仅仅需要寥寥数十行代码，即可借助底层实现，方便地调用包括GPU在内的大量资源完成工作。\n值得一提的是，无论什么框架，Python只是作为前端描述用的语言，实际计算则是通过底层的C/C++实现。由于Python能很方便地引入和使用C/C++项目和库，从而实现功能和性能上的扩展，这样的大规模计算中，让开发者更关注逻辑于数据本身，而从内存分配等繁杂工作中解放出来，是Python被广泛应用到机器学习领域的重要原因。\n科学计算\nPython 的开发效率很高，性能要求较高的模块可以用 C 改写，Python 调用。同时，Python 可以更高层次的抽象问题，所以在科学计算领域也非常热门。包括 scipy、numpy 等用于科学计算的第三方库的出现，更是方便了又一定数学基础，但是计算机基础一般的朋友。\n图像处理\n这方面不熟，列几个关键词吧，如有错误，请斧正。\nkeywords : OpenCV, Pillow, PIL\n小结\n时间问题，暂到这里。基本上可以不负责任地认为，Python 可以做任何事情。\n但是，如果你是打算以此为业，我的建议是，不要局限在 「学Python」这样的思维上。要在技术领域立足，仅仅学会了 Python 的语法是不够的，你需要很多编程语言之外的基础知识。"}
{"content2":"在现今的推荐技术和算法中，最被大家广泛认可和采用的就是基于协同过滤的推荐方法。本文将带你深入了解协同过滤的秘密。下面直接进入正题\n1 什么是协同过滤\n协同过滤是利用集体智慧的一个典型方法。要理解什么是协同过滤 (Collaborative Filtering, 简称 CF)，首先想一个简单的问题，如果你现在想看个电影，但你不知道具体看哪部，你会怎么做？大部分的人会问问周围的朋友，看看最近有什么好看的电影推荐，而我们一般更倾向于从口味比较类似的朋友那里得到推荐。这就是协同过滤的核心思想。\n换句话说，就是借鉴和你相关人群的观点来进行推荐，很好理解。\n2 协同过滤的实现\n要实现协同过滤的推荐算法，要进行以下三个步骤：\n收集数据——找到相似用户和物品——进行推荐\n收集数据\n这里的数据指的都是用户的历史行为数据，比如用户的购买历史，关注，收藏行为，或者发表了某些评论，给某个物品打了多少分等等，这些都可以用来作为数据供推荐算法使用，服务于推荐算法。需要特别指出的在于，不同的数据准确性不同，粒度也不同，在使用时需要考虑到噪音所带来的影响。\n找到相似用户和物品\n这一步也很简单，其实就是计算用户间以及物品间的相似度。以下是几种计算相似度的方法：\n欧几里德距离\n皮尔逊相关系数\nCosine 相似度\nTanimoto 系数\n进行推荐\n在知道了如何计算相似度后，就可以进行推荐了。\n在协同过滤中，有两种主流方法：基于用户的协同过滤，和基于物品的协同过滤。具体怎么来阐述他们的原理呢，看个图大家就明白了\n基于用户的 CF 的基本思想相当简单，基于用户对物品的偏好找到相邻邻居用户，然后将邻居用户喜欢的推荐给当前用户。计算上，就是将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到 K 邻居后，根据邻居的相似度权重以及他们对物品的偏好，预测当前用户没有偏好的未涉及物品，计算得到一个排序的物品列表作为推荐。 下图给出了一个例子，对于用户 A，根据用户的历史偏好，这里只计算得到一个邻居 - 用户 C，然后将用户 C 喜欢的物品 D 推荐给用户 A。\n基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度，即基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。下图给出了一个例子，对于物品 A，根据所有用户的历史偏好，喜欢物品 A 的用户都喜欢物品 C，得出物品 A 和物品 C 比较相似，而用户 C 喜欢物品 A，那么可以推断出用户 C 可能也喜欢物品 C。\n总结\n以上两个方法都能很好的给出推荐，并可以达到不错的效果。但是他们之间还是有不同之处的，而且适用性也有区别。下面进行一下对比\n计算复杂度\nItem CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。\n适用场景\n在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。\n相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。\n希望上面的内容对大家有所帮助～ 如果想看更详细的说明，请参见http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/index.html"}
{"content2":"版权申明：本文为博主窗户(Colin Cai)原创，欢迎转帖。如要转贴，必须注明原文网址 　　http://www.cnblogs.com/Colin-Cai/p/7749031.html 　　作者：窗户 　　QQ：6679072 　　E-mail：6679072@qq.com\n这几天，沙特阿拉伯接纳了一个新公民——人工智能机器人索菲娅，挺轰动的一个新闻，有史以来人类第一次认同人工智能为公民，而且关键，这不是一次作秀。\nOMG，我真不知道人类到底在干什么，到底想干什么。我一向觉得，人类研究人工智能可以，但一定一定要把人工智能装进笼子，否则这或许是有史以来人类干的最危险的事情，霍金的担忧可能真不是杞人忧天。出于各种需求，甚至包括军事，人类在一步一步的逼近底线。\n上个世纪末前，深蓝第一次向全世界人类展示了机器的强悍，深蓝以2胜1负3平的成绩打败了国际象棋世界冠军卡斯帕罗夫。深蓝最基本框架只是基于价值函数做启发式搜索，比起现在的人工智能模型简直弱爆了。那个时候，很多人认为，虽然国际象棋已经战胜人类，但围棋永远搞不定，因为围棋有太多“虚”的东西，量化起来过于困难，从而强智能才有可能在围棋上打败人。而那个年代下，强智能完全是科幻小说里的玩意，而甚至对于人工智能研究者他们本人，都觉得这简直是天方夜谭。\n然而，我们看到了，Alphago出现了，它把我们的世界冠军完虐了。围棋第一人柯洁不服，说Alphago下不过他。JOKE!不知天高地厚啊，Alphago Master教会他重新看待人工智能。 人们惊呼，人工智能好厉害，深度学习居然如此厉害，把人类智力上最后一点荣耀给抹掉了。甚至于，Google的一篇《Mastering the game of Go with deep neural networks and tree search》论文发出，使得模仿者也可以把智能围棋训练到一定高度。甚至有段时间哥也蠢蠢欲动，想照着论文里的手段，自己也考虑搞个东西玩玩。不过，好在Alphago的训练中还有人类下的棋谱，给人类保留了一点尊严。我们可以阿Q式的YY，要不是祖宗们下了几千年围棋，研究了这么多的理论，哪有Alphago的现在。\n可是，Alphago zero的出现，分分钟叫人重新做人啊。这是一个从出身开始，就只懂围棋规则，从未学习过任何人类的围棋研究，仅靠自己和自己对弈来提升围棋水平。完全自我学习的结果，令之前称霸围棋界的Alphago Master早已不是其对手。Alphago zero完全靠左右互搏就在短短几个月超越人类几千年的积累。太恐怖了！\n我从小就相信机械唯物者的认为，生命只是机器的一种形式，而意识这个东西本身没有任何玄妙的地方，一个人此时此刻也完全无法自己真正决定抬左手还是抬右手，眨眼睛还是摇脖子，我今天写这篇文章，你今天看这篇文章也不过都是机器运作的结果，而意识不过就如同我们程序的中间数据一样罢了。否则，如果你真的想从人的角度去解释意识，并认为自己可以真正决定点什么，那么你就开始陷入了一种超自然的怪圈中。\n然而新的问题产生了，一堆杂质的布朗运动，局部永远都会那么混乱。然后生命的繁殖，却使得局部的负熵变为了常态，这曾经让我一度觉得不可思议。可是当冬天到来，黄河的水也会结冰，非生物界也已昂存在无生命的局部负熵，自然界并不处处都只是布朗运动。而我们使用遗传算法（Genetic Algorithm），都可以让一堆初始的时候杂乱无章的数据在不断进化中越来越优秀，而过程却一直是在程序之中，生命机器的进化也一样遵循，可以不断筛选出优秀的生命。我们可以用bagging、boosting的手段来组合各个分类器，其手段和自然界的筛选本质上来看都是过程。\n如同生物学界曾经争论过病毒是否属于生命，最终的结果是认同了生命和非生命其实也没有严格界限。一样的道理，智能和非智能真的有严格界限吗？人和狗，狗和蚂蚱之间的智能真的有本质区别吗？ 我倾向于人类可能只不过是大愚若智。人工智能和其他程序就如同病毒和生物之间一样，其实没有明显清晰的界限。\n目前人工智能解决封闭性问题越来越厉害，深度学习(Deep Learning)已经成为AI界的标配，不搞n个隐藏层搞DL，就跟不上时代。封闭性的问题永远只在某一个规则限定好的领域里，比如医学上通过图像分析某种疾病，再比如Alphago等。可还有一类是开放性问题，这是未来AI研究的关键性方向，这类问题目的不明确，或者在变动，或者根本谈不上一个绝对的目的。开放性问题的研究是通往强智能的必经之路。潘多拉的盒子也会在途中打开。\n既然自然界塑造了人这样一个接近完美的机器，那么人工智能也应该能塑造另外一个人造机器，殊途同归，只要不断去研究人工智能，相信一切只不是时间问题。\n《三体》这部小说，我认为有很多值得我们深思的地方：\n1.如果外星人侵略过来，人类和蚂蚁没有合作的必要。强人工智能诞生之后，人类在强人工智能眼里就是蝼蚁，即便强敌侵入，也不可能有合作的必要，只能算是拖后腿的。\n2.不同的智能物种之间没有信任，对方的存在就是对自己的威胁。强人工智能诞生之后，人类肯定意识到强人工智能的可怕，而强人工智能自然知道人类的担忧，从而最终矛盾不可调和。\n于是，昨天，我和一朋友争论，我说，一群蠢家伙(人类)想造一些比自己聪明一万倍的东西，却还想控制它们，这简直是天方夜谭。\n从而，我觉得，人类要控制好人工智能这个东西，否则演化为史上最大危机真未必是拍电影。只是，但愿这一切从来不会发生，我也不过是在杞人忧天，痴人说梦。"}
{"content2":"不多说，直接上干货！\nEditplus下载\n第一步：进入官网 https://www.editplus.com/\n第二步：下载\nhttps://www.editplus.com/download.html\nEditplus安装\n我这里，直接以一个压缩包来安装，需要的，请在博客下方评论留言。直接解压缩就好！\n多么方便!当然，你若想安装的话，也可以。见如下，\n第一步:安装\n第二步:安装步骤，这里很简单，不贴图不赘述啦！自行设置即可。\n第三步：汉化注册（很简单）\n强烈推荐，去这个网站，进入\nhttp://www.jb51.net/tools/editplus/\n这里不多说。\nEditplus最佳配色方案\n第一步：直接，找到你的Editplus所安装的目录，找到EditPlus的配置文件editplus_u.ini\n第二步：初步认识下，默认的配置文件\n第三步：更改配置文件\n注意，若是比如我想拿到桌面来的话，则将 Workspace Path=D:\\SoftWare\\EditPlus 3   改为  C:\\Users\\Administrator\\Desktop （不建议）\n同时，若是不喜欢默认的界面，则可以改为我强烈推荐的最佳配色方案。（直接复制拿去粘贴替换，改成你的路径，其他全部不变，就可以了，很简单的）\n最后，我的配置文件，如下：\n[Options] Placement=2C00000002000000030000000083FFFF0083FFFFFFFFFFFFFFFFFFFFEA000000EE000000D6050000CF030000FF Window List=00000000000000000000000000000000FF Marker List=00000000000000000000000000000000FF Function List=00000000000000000000000000000000FF Open Remote=5C020000550100006404000067030000FF Expand=00000000000000000000000000000000FF Project Pos=00000000000000000000000000000000FF Workspace Path=D:\\SoftWare\\EditPlus 3 Cliptext Window=224 Cliptext Window 2=489 Output=90 Output 2=170 Folding=1 FindX=586 FindY=429 Find=align Tab Index=1 Cliptext=2 Custom colors=6D6D7200FFFFFF00FFFFFF00FFFFFF00FFFFFF00FFFFFF00FFFFFF00FFFFFF0031282700FFFFFF00FFFFFF00FFFFFF00FFFFFF00FFFFFF00FFFFFF00FFFFFF00FF Matching Brace=1 Indent guide=1 [Files] Encoding=65001 Backup=0 Backup Remote=0 [Fonts] Edit Window=F0FFFFFF00000000000000000000000090010000000000000302013143006F007500720069006500720020004E0065007700000000000000000000000000000000000000000000000000000000000000000000000000000000000000FF Printer=F4FFFFFF00000000000000000000000090010000000000000000003143006F007500720069006500720020004E0065007700000000000000000000000000000000000000000000000000000000000000000000000000000000000000FF Output Window=F4FFFFFF00000000000000000000000090010000000000000000003143006F007500720069006500720020004E0065007700000000000000000000000000000000000000000000000000000000000000000000000000000000000000FF Cliptext Window=F4FFFFFF0000000000000000000000009001000000000001000000004D006900630072006F0073006F006600740020005900610048006500690020005500490000000000000000000000000000000000000000000000000000000000FF Document Selector=F4FFFFFF0000000000000000000000009001000000000001000000004D006900630072006F0073006F006600740020005900610048006500690020005500490000000000000000000000000000000000000000000000000000000000FF Hex Viewer=F4FFFFFF00000000000000000000000090010000000000010000003143006F007500720069006500720020004E0065007700000000000000000000000000000000000000000000000000000000000000000000000000000000000000FF Custom 1=F4FFFFFF00000000000000000000000090010000000000000000002241007200690061006C000000720020004E0065007700000000000000000000000000000000000000000000000000000000000000000000000000000000000000FF Custom 2=F5FFFFFF000000000000000000000000900100000000000000000022560065007200640061006E00610000004E0065007700000000000000000000000000000000000000000000000000000000000000000000000000000000000000FF Custom 3=F4FFFFFF000000000000000000000000900100000000000000000012540069006D006500730020004E0065007700200052006F006D0061006E0000000000000000000000000000000000000000000000000000000000000000000000FF Custom 4=F5FFFFFF0000000000000000000000009001000000000000000000224D0053002000530061006E0073002000530065007200690066000000000000000000000000000000000000000000000000000000000000000000000000000000FF Custom 5=F3FFFFFF00000000000000000000000090010000000000FF000000315400650072006D0069006E0061006C00000065007700000000000000000000000000000000000000000000000000000000000000000000000000000000000000FF [Tool Option] Top Selector=1 [Colors\\Text] Background=2238503 Default=0 Foreground=16777215 [Colors\\Keyword 1] Foreground=8272368 Default=0 [Colors\\Embedded script] Foreground=16777215 Default=0 [Colors\\Keyword 3] Foreground=16777215 Default=0 [Colors\\Keyword 6] Foreground=8716287 Default=0 [Colors\\Keyword 7] Foreground=8716287 Default=0 [Colors\\Keyword 8] Foreground=8716287 Default=0 [Colors\\Keyword 9] Foreground=8454143 Default=0 [Colors\\Keyword 10] Foreground=8716287 Default=0 [Colors\\Quotation] Foreground=8454143 Default=0 [Colors\\Quotation 2] Foreground=5107956 Default=0 [Colors\\Line comment] Foreground=10789024 Default=0 [Colors\\Line number] Foreground=12632256 Background=2238503 Default=0 [Colors\\Folding mark -] Background=2238503 Default=0 [Colors\\Matching words] Default=0 Background=718314 [Colors\\Keyword 2] Foreground=16379142 Default=0 [Colors\\Folding mark +] Background=2238503 Default=0 [Colors\\Text selection] Background=7039851 Default=0 [Colors\\Number] Foreground=16524240 Default=0 [Colors\\Block comment] Foreground=8421504 Default=0 [Colors\\Keyword 5] Foreground=15574913 Default=0 [Colors\\Ruler] Foreground=12632256 Default=0 Background=2566187 [Colors\\Cursor indicator] Background=16777215 Default=0 [Colors\\Indent Guide] Foreground=7499117 Default=0\n第四步：验证下效果\n注意\n若是直接，将\n再打开，会报错！如下，\n所以，建议，避免这错误，像我这样来操作。\n不会小Q书桌的博友，请移步\n小Q书桌的下载、安装和使用\n扩展学习\nNotepad++软件的下载与安装步骤\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"一、神经网络基础\n1. 神经元模型\n神经网络中最基本的单元是神经元模型（neuron）。\n细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示：\n2. 激活函数\n与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。\n更多激活函数参考： https://www.jianshu.com/p/22d9720dbf1a\n3. 感知机（Perceptron）\n感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。\n感知机权重学习过程\n感知机的学习采用随机梯度下降算法（SGD）该算法的说明可以参考：http://www.cnblogs.com/NeilZhang/p/8454890.html\n其中 η∈（0，1）称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。\n局限性：\n感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限。 可以证明若二类模式是线性可分的，即存在一个线性超平面能将他们分开，则感知机的学习一定会收敛（converge）而求得适当的权向量w = （w1，w2,w3…..）; 否则感知机学习过程将会发生震荡，w难以稳定下来，不能求得合适解。\n要解决非线性可分问题需要考虑使用多层功能神经元，即神经网络。（神经网络发展史上经典问题：异或问题单层网络不能解决）\n4.神经网络\n多层神经网络的拓扑结构如下图所示：\n在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点：\n* 每层神经元与下一层神经元之间完全互连\n* 神经元之间不存在同层连接\n* 神经元之间不存在跨层连接\n根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播\n二、误差逆传播算法（BP神经网络算法）\n神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP神经网络算法即误差逆传播算法（error BackPropagation）正是为学习多层前馈神经网络而设计，BP神经网络算法是迄今为止最成功的的神经网络学习算法。\n上图的网络中有(d+l+1)*q+l个参数需要确定：输入层到隐层的d×q个权重，隐层到输出层q×l个权重、q个隐层神经元的阈值、l个输出神经元的阈值。\n上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节（sgd算法）。\n学习率η∈（0，1）控制着沿反梯度方向下降的步长，若步长太大则下降太快容易产生震荡，若步长太小则收敛速度太慢，一般地常把η设置为0.1，有时更新权重时会将输出层与隐含层设置为不同的学习率。\n可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程：\n1. 误差函数\n2. 具体推导过程\n其它参数的推算过程相似，参考《机器学习》中神经网络的介绍。\n上述算法的推导是基于每次仅针对一个训练样例更新权重和阈值（标准BP算法），这种算法参数更新十分频繁，可能会出现“抵消”现象。累计BP算法针对累计误差最小化，每次读取整个数据集一遍后才对参数进行更新，其参数更新的频率低得多。\n3. 过拟合\nBP神经网络强大的学习能力常常容易造成过拟合问题，有以下两种策略来缓解BP网络的过拟合问题：\n早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。\n引入正则化（regularization）：基本思想是在累积误差函数中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。\n4. 全局最小与局部最小\n要成为局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。\n* 局部极小解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的误差函数值。\n* 全局最小解：参数空间中的某个点，所有其他点的误差函数值均不小于该点的误差函数值。\n跳出局部最小的方法：\n以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。\n使用“模拟退火”技术\n使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。\n三、神经网络可视化\n推荐一个在线测试神经网络的网站：http://playground.tensorflow.org/\n下图为上述网站通过两个神经元解决异或问题：\n其它机器学习算法：\n监督学习——随机梯度下降算法（sgd）和批梯度下降算法（bgd）\n监督学习——决策树理论与实践（上）：分类决策树\n监督学习——决策树理论与实践（下）：回归决策树（CART）\n监督学习——K邻近算法及数字识别实践\n监督学习——朴素贝叶斯分类理论与实践\n监督学习——logistic进行二分类（python）\n监督学习——AdaBoost元算法提高分类性能\n无监督学习——K-均值聚类算法对未标注数据分组\n参考：\n《机器学习》 周志华\n激活函数： https://blog.csdn.net/u011826404/article/details/53767428\n随机梯度下降与批梯度下降： http://www.cnblogs.com/NeilZhang/p/8454890.html"}
{"content2":"Python数据预处理：机器学习、人工智能通用技术\n白宁超  2018年12月24日17:28:26\n摘要：大数据技术与我们日常生活越来越紧密，要做大数据，首要解决数据问题。原始数据存在大量不完整、不一致、有异常的数据，严重影响到数据建模的执行效率，甚至可能导致模型结果的偏差，因此要数据预处。数据预处理主要是将原始数据经过文本抽取、数据清理、数据集成、数据处理、数据变换、数据降维等处理后，不仅提高了数据质量，而且更好的提升算法模型性能。数据预处理在数据挖掘、自然语言处理、机器学习、深度学习算法中起着重要的作用。（本文原创，转载必须注明出处.）\n1 什么是数据预处理\n数据预处理简而言之就是将原始数据装进一个预处理的黑匣子之后，产生出高质量数据用来适应相关技术或者算法模型。为了大家更明确的了解数据预处理，我们举个新闻分类的例子：\n将原始的数据直接进行分类模型训练，分类器准确率和召回率都比较低。因为我们原始数据存在很多干扰项，比如的,是等这些所谓停用词特征对分类起的作用不大，很难达到工程应用。\n我们将原始数据放假预处理黑匣子后，会自动过滤掉干扰数据，并且还会按照规约的方法体现每个词特征的重要性，然后将词特征压缩变换在数值型矩阵中，再通过分类器就会取得不错的效果，可以进行工程应用。\n总结：数据预处理前的数据存在不完整、偏态、噪声、特征比重、特征维度、缺失值、错误值等问题；数据预处理后的数据存在完整、正态、干净、特征比重合适、特征维度合理、无缺失值等优点。\n数据预处理方法：\n数据清理：通过填写缺失的值、光滑噪声数据、识别或删除离群点并解决不一致性来清理数据。主要目标：格式标准化，异常数据清除，错误纠正，重复数据的清除。\n数据集成：将数据由多个数据源合并成一个一致的数据存储，如数据仓库。\n数据变换：通过平滑聚集，数据概化，规范化等方式将数据转换成适用于的形式。如把数据压缩到0.0-1.0区间。\n数据归约：往往数据量非常大，在少量数据上进行挖掘分析需要很长的时间，数据归约技术可以用来得到数据集的归约表示，它小得多，但仍然接近于保持原数据的完整性，并结果与归约前结果相同或几乎相同。可以通过如聚集、删除冗余特征或聚类来降低数据的规模。\n2 为什么做这门课程\n在初期学习阶段，大家精力着重于算法模型和调参上。实际情况是，有时候在算法改进上花费很多功夫，却不如在数据质量上的些许提高来的明显。另外，习惯于数据语料的拿来主义之后，当面对新的任务时候，却不知道如何下手？有的同学在处理英语时候游刃有余，面对中文数据预处理却不知所措。基于以上几个问题，结合作者工程经验，整理出了‘数据预处理’学习资料，本教程主要面对文本信息处理，在图片语音等数据语料处理上是有所区别的。\n3 本课程能学到什么\n文本批量抽取：涉及技术点包括pywin32插件安装使用、文档文本提取、PDF文本提取、文本抽取器的封装、方法参数的使用、遍历文件夹、编码问题、批量抽取文本信息。\n数据清洗：包括yield生成器、高效读取文件、正则表达式的使用、清洗网页数据、清洗字符串、中文的繁简互相转换、缺失值的处理、噪声数据、异常数据清洗、批量清洗30万条新闻数据。\n数据处理：包括结巴分词精讲、HanLP精讲、停用词的处理、NLTK的安装使用、高频词和低频词的处理、词性的选择、特征数据的提取、批量预处理30万条新闻数据。\n数据向量化：包括词袋模型、词集模型、词向量的转化、缺失值和数据均衡、语料库技术、TFIDF、特征词比重、主成分分析、主题模型等、批量进行30万条数据向量化。\n可视化技术：包括条形图、柱形图、散点图、饼图、热力图等，还有matplotlib、seabom、Axes3D综合使用进行三维可视化。\nXGBoost竞赛神器：包括监督学习、文本分类、XGBoost原理、XGBoost算法实现、XGBoost调参、算法性能评估、30万条文档生成词典、30万条文档转化TFIDF、30万条文档转化生成LSI、训练分类器模型、抽样改进模型算法、特征维度改进模型算法、XGBoost实现30万条新闻数据文本分类\n综上所述：数据预处理整体包括数据抽取-->数据清洗-->数据处理-->数据向量化-->可视化分析-->模型构建。在整个过程中，我们每个章节相关性很强，首先对整个章节最终实现效果进行演示，然后拆分知识点分别讲解，最后将所有知识点整合起来做小节的实战。每个小节实战数据为下一个章节做铺垫，最后，一个综合实战分类案例串联所有知识点。\n4 开发环境说明\n开发语言: Python3.5.3\n系统环境：window10操作系统\n编程环境：Sublime\n软件环境：Anaconda4.4.0\n插件版本：均支持最新版本\nsublime激活：打开Help >Enter LICENSE\n----- BEGIN LICENSE ----- sgbteam Single User License EA7E-1153259 8891CBB9 F1513E4F 1A3405C1 A865D53F 115F202E 7B91AB2D 0D2A40ED 352B269B 76E84F0B CD69BFC7 59F2DFEF E267328F 215652A3 E88F9D8F 4C38E3BA 5B2DAAE4 969624E7 DC9CD4D5 717FB40C 1B9738CF 20B3C4F1 E917B5B3 87C38D9C ACCE7DD8 5F7EF854 86B9743C FADC04AA FB0DA5C0 F913BE58 42FEA319 F954EFDD AE881E0B ------ END LICENSE ------\n解决Package Control报错：Package Control.sublime-settings]修改方法：Preferences > Package Settings > Package Control > Settings - User 添加：\n\"channels\": [ \"http://cst.stu.126.net/u/json/cms/channel_v3.json\", //\"https://packagecontrol.io/channel_v3.json\", //\"https://web.archive.org/web/20160103232808/https://packagecontrol.io/channel_v3.json\", //\"https://gist.githubusercontent.com/nick1m/660ed046a096dae0b0ab/raw/e6e9e23a0bb48b44537f61025fbc359f8d586eb4/channel_v3.json\" ]\n5 项目演示\n5.1 原始数据\n5.2 数据预览\n5.3 数据清洗\n5.4 生成词典\n5.5 生成特征向量\n5.6 生成LSI\n5.7 XGBoost新闻数据文本分类\n6 目录列表\n☆ 理论介绍\n★ 实战演练\n第1章 课程介绍\n1-1 为什么做这门课--☆\n1-2 课程整体介绍与导学--☆☆\n1-3 学习建议--☆☆\n1-4 课程开发环境介绍--☆\n1-5 文本分类项目演示--☆\n1-6 源码获取说明--☆☆☆\n1-7 总结与扩展--☆\n第2章 Python数据预处理之抽取文本信息\n2.1 数据类型与采集方法--☆☆☆\n2.2 一堆杂乱无章的数据--☆\n2.3 文本抽取问题（3种方法对比）--☆\n2.4 Pywin32实现格式转换--☆☆\n2.3 Word转换TXT算法--★\n2.6 PDF转换TXT算法--★\n2.7 文本抽取工具--★★\n2.8 文本批量编码--★\n2.9 遍历读取文件--★★★\n2.10 实战案例1：遍历文件批量抽取新闻文本内容--★★★\n2.11 总结与扩展--☆☆\n第3章 Python数据预处理之清洗文本信息\n3.1 准备30万条新闻数据--☆\n3.2 yield生成器--★\n3.3 高效读取文件--★★\n3.4 数据缺失值--★★\n3.5 脏数据与噪声数据--★★\n3.6 正则清洗数据--★★\n3.7 清洗HTML数据--★★\n3.8 简繁字体转换--★★\n3.9 实战案例2：30万条新闻文本数据清洗--★★★\n3.10 总结与扩展--☆☆\n第4章 Python数据预处理之文本处理\n4.1 常见分词工具--☆\n4.2 jieba分词（推荐）--★★★\n4.3 HanLP分词（扩展）--★★\n4.4 自定义去停词--★★\n4.5 词频统计--★★\n4.6 自定义去高低词频--★★\n4.7 自定义规则提取特征词--★★\n4.8 实战案例3：6万条新闻文本处理--★★★\n4.9 总结与扩展--☆☆\n第5章 Python数据预处理之文本特征向量化\n5.1 解析数据文件--★★\n5.2 词集模型--★★\n5.3 词袋模型--★★\n5.4 特征词转文本向量--★★★\n5.5 不均衡数据归一化处理--★★\n5.6 处理数据缺失值--★★\n5.7 实战案例4：新闻文本特征向量化--★★★\n5.8 总结与扩展--☆☆\n第6章 Python数据预处理之gensim文本向量化\n6.1 gensim介绍--☆☆\n6.2 gensim构建语料词典--★\n6.3 gensim统计词频特征--★★\n6.4 gensim计算IF-IDF--★★\n6.5 潜在语义索引--★★★★\n6.6 生成主题模型--★★★★\n6.7 生成随机映射--★★★★\n6.8 分层狄利克雷过程--★★★★\n6.9 实战案例6：gensim实现新闻文本特征向量化--★★★★\n6.10 总结与扩展--☆☆☆\n第7章 Python数据预处理之特征降维\n7.1 什么是降维--☆☆\n7.2 PCA 概述--☆☆☆\n7.3 PCA 应用场景--☆☆\n7.4 PCA 算法原理--★★★\n7.5 PCA 算法实现--★★★\n7.6 高维数据向低纬数据映射--★★\n7.7 前N个主成分特征--★★\n7.8 实战案例5：PCA技术实现新闻文本特征降维--★★★★\n7.9 总结与扩展--☆☆\n第8章 数据可视化分析\n8.1 matplotlib介绍--☆\n8.2 matplotlib绘制折线图--★★\n8.3 matplotlib绘制散点图--★★\n8.4 matplotlib绘制直方图--★★\n8.5 matplotlib绘制气温图表--★★\n8.6 matplotlib绘制三维图--★★★\n8.7 总结与扩展--☆\n第9章 XGBoost实现30万条新闻数据文本分类\n9.1 有监督学习--☆☆☆\n9.2 文本分类方法--☆☆☆\n9.3 XGBoost 原理--★★★★\n9.4 XGBoost 算法实现--★★★★\n9.5 准确率与召回率--☆\n9.6 F度量值--☆\n9.7 30万条文档生成词典--★★★\n9.8 30万条文档转化TFIDF--★★★\n9.9 30万条文档转化生成LSI--★★★★\n9.10 训练分类器模型--★★★★\n9.11 测试分类器模型--★★\n9.12 抽样改进模型算法--★★\n9.13 特征维度改进模型算法--★★\n9.14 训练集和测试集比率改进模型算法--★★\n9.15 综合实战：XGBoost实现30万条新闻数据文本分类--★★★★★\n9.11 总结与扩展--★★\n7 源码获取\n源码请进【机器学习和自然语言QQ群：436303759】文件下载："}
{"content2":"书架上一直放在一本《信息简史》，最近终于读完了。这是一本从信息的视角来描述其进化史的书，一本充满了技术性描述的科普性书籍。也不乏一些有趣的故事，其中就有那么几个人，他们实际是和计算机和程序有关，而计算机和程序在今天这个信息时代早已是信息的载体和处理者了。\n十八世纪\n查尔斯·巴贝奇（Charles Babbage），90 后，恩，一个十八世纪的 90 后（1791 年生），出生于工业革命的高峰时期。那时英国工业革命的巅峰作品 —— 蒸汽机，在他出生前没几年才刚刚被发明出来。\n那时，是一个崇尚机械的年代。人们相信机械的力量是可以做到一切的时代，这源自于工业革命的进步带来了蒸汽机和各种机械装置，将人们从各种劳动中解放了出来。就是在这样的时代背景下，巴贝奇成长起来，开始了对数学制表的机械化研究，并逐渐成为活跃于十九世纪的数学家、发明家和机械工程师。\n十九世纪\n一天，巴贝奇坐在家里的阳台上休息，不知不觉被对面纺织女工的工作吸引了目光。他不由得看的出了神，他看着纺织女工纺织布匹的过程，把一种纸上的花纹图案通过纺织过程转移到了织物之上。近来，他一直在思考用机械装置进行数学计算，这似乎一下击中了他，看到了曙光。\n引发巴贝奇想象的不是布匹的纺织过程，而是将布匹的图案从一种媒介转换到另一种媒介的编码过程。那是一台纺织用的雅卡尔提花机，这台机器通过编码并存储在打孔卡片上的指令进行控制。在这台纺织提花机的启发下，巴贝奇发明了「差分机」，一台能大幅提高乘法运算速度和提高对数计算精度的机器。\n1832 年，17 岁的爱达（Ada），在老师的带领下去参观了巴贝奇的「差分机」，并被其深深的迷住了。这是一台能自动计算数学题的机器，而当时爱达正醉心于数学学习。她的母亲为她请的老师摩根正是计算机数学基础布尔代数的创始人之一。那时的英国科学风气盛行，许多妇女都在杂志上发表文章探讨数学问题，而爱达也在那时展现出了非凡的数学天赋。\n在见到「差分机」的第二年，爱达和巴贝奇正式见了一次面。当时，巴贝奇奇雄心勃勃的设计了一个新的机器：「分析机」。他将自己的想法和设计详细地讲给爱达听，爱达认真地听完巴贝奇的计划，并仔细看了他的文稿，深深地为之陶醉。她认为这的确是一个将要改变世界的伟大设想，她表示一有机会就将亲身参与这项工作。从此爱达与巴贝奇开启了一段持续十八年的忘年之交。\n巴贝奇的「分析机」，它的机械结构被分成了「计算单元」和「存储单元」两个部分。其中「计算单元」不仅内建四则运算，还可以存四组不同的运算方程式，用穿孔卡片（来自雅卡尔提花机的灵感）载入到机器里。从某些方面来说，它的计算、存储、输入输出（I/O）三项分离设计，和今天的计算机设计完全一致。只是可惜他的思想太超前，无论他如何努力游说、路演也没有人投资给他建造这台机器所需的资金。所以，「分析机」的设计仅停留在纸面上，从没做成过实体机。\n就是在这样一台从未做成实体，停留在想象和纸面的机器上，爱达开始了她的编程之旅。她设计了一个过程，一组规则以及一系列运算。在一个世纪后，这些过程、规则和运算会被称为一种算法或一个计算机程序。当时，所有有关编程概念的术语，包括：算法、循环、条件、分支、递归，在那个时代都还没有发明出来。而爱达却已深刻的理解了所有这些名字背后的实质。她设想的算法是递归的，它循环运行，巴贝奇曾将这种方式称为“机器咬尾巴——团团转”。\n她就是这样为一台还不存在的机器编程，在头脑中编程。这些程序包括计算三角函数程序、级数相乘程序、伯努力数计算程序等等，今天这被视为是“第一套计算机程序”。因此，爱达被世人称为第一位程序员。1981 年，美国国防部把它花了 10 年开发的一种计算机语言命名为 Ada，以此纪念她。\n有时觉着历史上的很多天才总是不长命，而爱达还出生于一个天才家庭。她有一个比她更有名的父亲 —— 英国大诗人：拜伦。但父亲在其刚满月不久时就和她母亲离婚，从此离开英国再未相见。拜伦一直想找机会回到英国见见自己的女儿，但人生无奈，爱达 8 岁那年他还没来得及回去，便已离世。有时读着拜伦留下的诗句：\n如果我们再相见，事隔经年。我将何以招呼你，以眼泪，以沉默。\n在想，如果他们真有再相见时，这也许就是爱达的心境吧。爱达开始和巴贝奇共同构建梦想的那年，她 18 岁，18 年后，爱达因病去世，和拜伦一样都是 36 岁去世。根据她的遗愿，她被葬于诺丁汉郡其父亲身边，终于再见了。在她短暂一生的最后时光，她留下了一个关于程序的梦想：\n我以我自己的方式迟早会成为一名独裁者。我现在可不会说，但我希望，它们将是纪律严明、异常和谐的军队 —— 由大量的数构成，伴着军乐以势不可挡的力量行进。\n至于巴贝奇的那台分析机，爱达离世后它便逐渐淡出视线，埋尘历史。它在重新被人记起之前，先得被人遗忘。巴贝奇在其生命的最后几年，可能感觉时日无多，再不可能等到这台机器被造出来的那一天，他写道：\n如果有人在未被告诫以我的前车之鉴的情况下，试图尝试这项如此了无指望的工作，并通过完全不同的原理或更简化的机械手段而成功实现了一台可与整个数学分析部门相当的机器，那么我不怕把自己的名誉托付给他，因为他肯定会完全理解我当年努力的性质及其成果的价值。\n恩，这个人会出现的，但还需要再等一百年。\n二十世纪\n一个世纪后，1936 年，还在剑桥国王学院就读的阿兰·图灵发表重要论文《论可计算数及其在判定问题上的应用》，正式提出了“算法（algorithms）”和“计算机（computing machines）”这两个核心概念，一直让我们受用到今天。\n1950 年，图灵接着发表了论文《计算机器与智能》（Computing Machinery and Intelligence），为后来的人工智能科学提供了开创性的构思。他提出了著名的「图灵测试」：指出如果第三者无法分辨人类与人工智能机器反应的差别，则可以论断该机器具备人工智能。图灵首次从行为主义的角度给出了人工智能的定义。\n前两年有部电影《模拟游戏》就是基于图灵的传记改编的，电影实际只刻画了图灵波澜起伏的人生中短短的一小段。而且改编的也不一定完全符合史实，但里面有一段情节倒是很让人触动：图灵闪耀光芒的时期，正是在打第二次世界大战的时代。二战背景下如图灵这样的技术工作者，处在一个两难境地 —— 破解不了德军密码，每时每刻都有人在战场上死亡；最后破解了密码，获得了情报，却只能看着却不能告知友军转移。\n图灵也许就是巴贝奇一直寄希望出现的人，值得他托付名誉的人。用一种和他设想的完全类似的原理，但却非机械化的手段，成功设计出了可以媲美十九世纪整个数学分析部门的计算机器。「图灵机」是图灵在头脑思维中的构想，但很快以此为基础的计算机就被发明了出来。在还没有计算机的时代，图灵不但探索了计算机能做的事，还指出了计算机永远不能做到的事。他为计算机领域奠定了不可埋没的基础，没有他也许就没有计算机的今天。\n...\n从巴贝奇的分析机到图灵的计算机正好一百年，而从图灵提出的人工智能距今已快 70 年，未来二三十年会怎样呢？今天人们对人工智能的追捧，似乎就像巴贝奇年代对机械的崇尚，谁又来书写二十一世纪的传奇呢？\n写点文字，画点画儿，记录成长瞬间。\n微信公众号「瞬息之间」，既然遇见，不如一起成长。"}
{"content2":"所谓人工智能，通俗地讲是指由人工制造出来的系统所表现出来的智能\n机器学习简单来讲就是通过算法，使机器能从大量历史数据中学习规律，从而对新的样本做出智能识别或对未来做预测\n机器学习是基于概率统计、矩阵或图模型而得出的分析结论\n机器学习是人工智能的一个分支\n深度学习是机器学习的一个新领域\n监督学习\n逻辑回归、K近邻、朴素贝叶斯、随机森立、支持向量机\n无监督学习\nK-means、DBSCAN、协同过滤、LDA\n半监督学习\n标签传播\n强化学习\n隐马尔可夫\n监督学习法Supervised Learning\n通过过往的一些数据的特征以及最终结果来进行训练的方式就是监督学习法\n分类算法 K近邻、朴素贝叶斯、决策树、随机森林、GBDT和支持向量机等\n回归算法   逻辑回归、线性回归等\n无监督学习Unsupervised Learning\n是指训练样本不依赖于打标数据的机器学习算法:无监督学习主要是用来解决一些聚类场景的问题，因为当我们的训练数据缺失了目标值之后，能做的事情就只剩下比对不同样本间的距离关系\n聚类算法 K-Means、DBSCAN等\n推荐算法   协同过滤等\n半监督学习Semi-supervised Learning\n对样本的部分打标来进行机器学习算法的使用，这种部分打标样本的训练数据的算法应用，就是半监督学习\n目前很多半监督学习算法都是监督学习算法的变形，本书将介绍一种半监督学习算法——标签传播算法\n强化学习Reinforcement Learning\n强调的是系统与外界不断地交互，获得外界的反馈，然后决定自身的行为。强化学习目前是人工智能领域的一个热点算法种类，典型的案例包括无人汽车驾驶和阿尔法狗下围棋。本书介绍的分词算法隐马尔科夫就是一种强化学习的思想。\n无人汽车驾驶和阿尔法狗, 分词算法隐马尔科夫\n场景解析是数据挖掘流程的第1步\n过拟合（Over-fitting），\n是过度拟合的意思，常发生在线性分类器或者线性模型的训练和预测当中\n精确率、召回率、F1值\n数据探查\n数据量的大小\n数据缺失或乱码\n字段类型\n是否含有目标队列\nETL操作（描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程），称为“数据清洗\n场景抽象\n商品推荐\n疾病预测\n人物关系挖掘\n把商品购买行为抽象成了“是”或者“否”这样的二分类问题\n对应症状，所以只要挖掘每个时期的不同病变特征，就可以实现预测，进而可以把癌症预测抽象成一个多分类的场景\n算法选择\n确定算法范围\n多算法尝试\n多视角分析\n数据预处理是数据挖掘流程的第2步\n采样\n随机采样\n系统采样\n分层采样\n归一化\n公式y=(x-MinValue)/(MaxValue-   MinValue)\n归一化是指一种简化计算的方式，将数据经过处理之后限定到一定的范围之内，一般都会将数据限定在[0,1]。\n可以加快算法的收敛速度\n数据过滤\n去除噪声"}
{"content2":"import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D from sklearn.model_selection import train_test_split from sklearn import datasets, linear_model,discriminant_analysis def load_data(): # 使用 scikit-learn 自带的 iris 数据集 iris=datasets.load_iris() X_train=iris.data y_train=iris.target return train_test_split(X_train, y_train,test_size=0.25,random_state=0,stratify=y_train) #线性判断分析LinearDiscriminantAnalysis def test_LinearDiscriminantAnalysis(*data): X_train,X_test,y_train,y_test=data lda = discriminant_analysis.LinearDiscriminantAnalysis() lda.fit(X_train, y_train) print('Coefficients:%s, intercept %s'%(lda.coef_,lda.intercept_)) print('Score: %.2f' % lda.score(X_test, y_test)) # 产生用于分类的数据集 X_train,X_test,y_train,y_test=load_data() # 调用 test_LinearDiscriminantAnalysis test_LinearDiscriminantAnalysis(X_train,X_test,y_train,y_test)\ndef plot_LDA(converted_X,y): ''' 绘制经过 LDA 转换后的数据 :param converted_X: 经过 LDA转换后的样本集 :param y: 样本集的标记 ''' fig=plt.figure() ax=Axes3D(fig) colors='rgb' markers='o*s' for target,color,marker in zip([0,1,2],colors,markers): pos=(y==target).ravel() X=converted_X[pos,:] ax.scatter(X[:,0], X[:,1], X[:,2],color=color,marker=marker,label=\"Label %d\"%target) ax.legend(loc=\"best\") fig.suptitle(\"Iris After LDA\") plt.show() def run_plot_LDA(): ''' 执行 plot_LDA 。其中数据集来自于 load_data() 函数 ''' X_train,X_test,y_train,y_test=load_data() X=np.vstack((X_train,X_test)) Y=np.vstack((y_train.reshape(y_train.size,1),y_test.reshape(y_test.size,1))) lda = discriminant_analysis.LinearDiscriminantAnalysis() lda.fit(X, Y) converted_X=np.dot(X,np.transpose(lda.coef_))+lda.intercept_ plot_LDA(converted_X,Y) # 调用 run_plot_LDA run_plot_LDA()\ndef test_LinearDiscriminantAnalysis_solver(*data): ''' 测试 LinearDiscriminantAnalysis 的预测性能随 solver 参数的影响 ''' X_train,X_test,y_train,y_test=data solvers=['svd','lsqr','eigen'] for solver in solvers: if(solver=='svd'): lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver) else: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver,shrinkage=None) lda.fit(X_train, y_train) print('Score at solver=%s: %.2f' %(solver, lda.score(X_test, y_test))) # 调用 test_LinearDiscriminantAnalysis_solver test_LinearDiscriminantAnalysis_solver(X_train,X_test,y_train,y_test)\ndef test_LinearDiscriminantAnalysis_shrinkage(*data): ''' 测试 LinearDiscriminantAnalysis 的预测性能随 shrinkage 参数的影响 ''' X_train,X_test,y_train,y_test=data shrinkages=np.linspace(0.0,1.0,num=20) scores=[] for shrinkage in shrinkages: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver='lsqr',shrinkage=shrinkage) lda.fit(X_train, y_train) scores.append(lda.score(X_test, y_test)) ## 绘图 fig=plt.figure() ax=fig.add_subplot(1,1,1) ax.plot(shrinkages,scores) ax.set_xlabel(r\"shrinkage\") ax.set_ylabel(r\"score\") ax.set_ylim(0,1.05) ax.set_title(\"LinearDiscriminantAnalysis\") plt.show() # 调用 test_LinearDiscr test_LinearDiscriminantAnalysis_shrinkage(X_train,X_test,y_train,y_test)"}
{"content2":"对于学习者，你是不是经常遇到这样的问题：在我们狠狠下定决心学习PCB技术的时候，我们常常遇到很多大大小小的问题，遗憾的是身边没有一个能及时给自己解答问题的高手指点，通过论坛、群等方式询问可能半天也得不到解答，就算有人回答了，自己可能也会怀疑是不是这样，这是不是最好的答案？通过百度，知乎等大海捞针的方式查找答案？看到的都是生涩的文字描述及简单的图片示意，并不能够快速放心的解决自己的疑问学习pcb设计这条路怎么这么难？有可能空有一腔学习的热情，因为不能快速的解答困难直接放弃了学习！\n对于群主或者管理员，建立一个群的初衷是希望能给有想学PCB等电子技术的爱好者提供一个交流知识共享的平台，能通过相互交流解决学习中遇到的一些困难，从而达到共同进步的目的，但是等群大了以后，奈何自己个人经历有限，很多群主都不是全职在做这个事情，所以群的活跃度、群交流问题的人也就越来越少，那些真正想学的人就像上面所说的因为不能及时解决遇到的问题而放弃了学习，真是无比的可惜。\n为了解决以上问题，凡亿通过自主研发，正式上线了电子行业首个PCB问题解答智能搜索机器人——凡小亿V1.0版，Pcb助手能够快速的解答学习者所提出的问题，推送对应的答案。智能解答助手一上线就受到了广泛爱好者的追捧。经过1年多的测试与智能学习，智能机器人搜索助手正式升级为：pcb助手。\n强大功能一：Pcb助手在群里是可以实现发起者提出的关键字，及时快速推送发起者所需求的答案；如下所示\n强大功能二：Pcb助手亦可实现智能对话等相关操作\n强大功能三：各位学习者亦可添加pcb助手（QQ号3005395987 ）为好友，通过直接发送关键字即可获取答案。\n强大功能四：还可以实现技术文章干货的推送哦！\n更多功能敬请期待!\nPCB助手的问题解答库依托于凡亿教育仅5年10万学员问题的搜集及专业解答，给与了pcb助手专业丰富的智慧大脑！Pcb助手的出现将给用户带来更方便更快捷有效的问答，用户仅仅通过简单的@PCB助手+关键字即可，机器识别需求后，快速在已知数据库里，直接提取用户想要的信息。学习者们就能找到所需的设计规则，设计疑问，器件规格，手册等资料，不需要像以前一样到处询问和找不准确的答案，而且这个服务完全免费对外开放，是不是很简单快捷，方便呢？pcb助手智能搜索机器人的推出，改变了群在线等答案方式，也节约了很多时间去查找搜索问题 ，也避免了在查找连接中去苦苦寻求问题答案的时间。\nPcb助手面向各大技术QQ群免费开放，用户们可自主上传PCB设计问题库数据，平台数据库的爆发式增长，将进一步提升pcb助手的智能搜索用户体验，让数据库的数据信息更庞大，更精准！\n如果你想试试的话， 可以直接添加pcb助手 QQ号：3005395987，拉入自己所在的群进行体验。"}
{"content2":"目录(?)[+]\n前言\nSVM机器学习与深度学习\n人工智能领域\n机器学习与深度学习\nSVM简介\nSVM原理分析\n快速理解SVM原理\n线性可分和线性不可分\n函数间隔和几何间隔\n超平面分析与几何间隔详解\n二次最优化\nSVM-支持向量机原理详解与实践\n前言\n去年由于工作项目的需要实际运用到了SVM和ANN算法,也就是支持向量机和人工神经网络算法，主要是实现项目中的实时采集图片（工业高速摄像头采集）的图像识别的这一部分功能，虽然几经波折，但是还好最终还算顺利完成了项目的任务，忙碌一年，趁着放假有时间好好整理并总结一下，本文的内容包括：前面的部分是对支持向量机原理的分析，后半部分主要直接上手的一些实践的内容。\n本文的原理部分针对支持向量机的原理，特别拉格朗日对偶性，求解拉个拉格朗日函数，以及和函数与核技巧再到软间隔和正则化等重要内容做了一些讨论。\n实践部分的目标则是通过对实践时碰到的问题，调参的过程的讲解可以对前半部分讲解的SVM原理部分的内容有一个更深入的了解。\nSVM、机器学习与深度学习\n人工智能领域\n在大数据，人工智能的时代，深度学习可以说火得一塌糊涂。美国硅谷的大公司都在布局着这个领域，而中国国内，腾讯，百度，阿里巴巴等等知名企业也都在这个领域争先发力，2017年初，百度迎来陆奇-前微软全球执行副总裁，人工智能领域世界级的权威，要知道百度还有人工智能大牛Andrew Ng – 吴恩达。所有迹象表明人工智能必然是继互联网之后的全球各大公司甚至国家必争的高地。\n机器学习与深度学习\n由于深度学习在大数据预测能力上的卓越表现，当下出现了深度学习是否会替代传统机器学习算法并淘汰他们的讨论，但是另一方面，大多数人仍然相信深度学习不会代替其他的模型或者算法。对于大多数的应用，像一些简单的算法如逻辑回归、支持向量机表现的已经很不错了，使用深度学习会让问题复杂化。\n深度学习是可以应用到大部分领域的，但是就像前面说的，深度学习并非所有问题的最优方案，如果你的工作中有用到机器学习算法，你可以尝试传统的机器学习算法，也可以达到很好的效果。虽然现在已经有一些工作去把各领域的知识融入到深度学习中的，但这并不能完全替代原有的。\n上图是一个关于机器学习算法的时间线来自于Eren Golge。\n就像在20世纪早期SVM一样，深度学习会成为主流，但首先深度学习应当解决其在大数据需求及复杂性方面的问题，这样它才会成为人们的第一选择。\nSVM简介\nSVM(support vector machine)简单的说是一个分类器，并且是二类分类器。\nVector：通俗说就是点，或是数据。\nMachine：也就是classifier，也就是分类器。\nSVM作为传统机器学习的一个非常重要的分类算法，它是一种通用的前馈网络类型，最早是由Vladimir N.Vapnik 和 Alexey Ya.Chervonenkis在1963年提出，目前的版本（soft margin）是Corinna Cortes 和 Vapnik在1993年提出，1995年发表。深度学习（2012）出现之前，SVM被认为是机器学习中近十几年最成功表现最好的算法。\nSVM原理分析\n快速理解SVM原理\n很多讲解SVM的书籍都是从原理开始讲解，如果没有相关知识的铺垫，理解起来还是比较吃力的，以下的一个例子可以让我们对SVM快速建立一个认知。\n给定训练样本，支持向量机建立一个超平面作为决策曲面，使得正例和反例的隔离边界最大化。\n决策曲面的初步理解可以参考如下过程，\n如下图想象红色和蓝色的球为球台上的桌球，我们首先目的是找到一条曲线将蓝色和红色的球分开，于是我们得到一条黑色的曲线。\n图一.\n2） 为了使黑色的曲线离任意的蓝球和红球距离（也就是我们后面要提到的margin）最大化，我们需要找到一条最优的曲线。如下图，\n图二.\n3） 想象一下如果这些球不是在球桌上，而是被抛向了空中，我们仍然需要将红色球和蓝色球分开，这时就需要一个曲面，而且我们需要这个曲面仍然满足跟所有任意红球和蓝球的间距的最大化。需要找到的这个曲面，就是我们后面详细了解的最优超平面。\n4) 离这个曲面最近的红色球和蓝色球就是Support Vector。\n线性可分和线性不可分\n线性可分-linearly separable, 在二维空间可以理解为可以用一条直线（一个函数）把两类型的样本隔开，被隔离开来的两类样本即为线性可分样本。同理在高维空间，可以理解为可以被一个曲面(高维函数)隔开的两类样本。\n线性不可分，则可以理解为自变量和因变量之间的关系不是线性的。\n实际上，线性可不分的情况更多，但是即使是非线性的样本通常也是通过高斯核函数将其映射到高维空间，在高维空间非线性的问题转化为线性可分的问题。\n函数间隔和几何间隔\n函数间隔functional margin: 给定一个训练样本有：\n函数间隔代表了特征是正例或是反例的确信度。\n几何间隔 geometrical margin：\n向量点到超平面的距离（其中后面详细介绍）\n超平面分析与几何间隔详解\n前面已经对SVM的原理有了一个大概的了解，并且简单介绍了函数间隔和几何间隔的概念，为了更好的理解线性可分模式下超平面，以下将进行深入的剖析推导过程，我们假设有训练样本集，期望的响应为，这里我们用类+1和类-1来代表，以表明样本是线性可分的。\n决策曲面方程如下：\n其中\nx：输入向量，也就是样本集合中的向量；\nw：是可调权值向量，每个向量可调权值；\nT：转置，向量的转置；\nb：偏置，超平面相对原点的偏移。\n根据逻辑回归定义展开其实就是：\n其中假设约定，于是将替换成b；则有：\n而（T是转置）所以有：\n这里假设模式线性可分：\n线性可分模式下最优超平面的示意图如下：\n如上图所示：\n为分离边缘，即超平面和最近数据点的间隔。如果一个平面能使最大，则为最优超平面。\n灰色的方形点和原形点就是我们所说的支持向量。\n假设和向量和偏置的最优解，则最优超平面的函数为：\n相应的判别函数是：\n以下是点x到最优超平面的二维示意图：\n由上图可知r 为点x到最优超平面的距离:\n那么代数距离是如何得到的呢？通过将带入\n可以得到r，其中：\n为x在最优超平面的正轴投影，\n因为在平面上\n下面给出一种更为简单且直观的理解：\n首先我们必须要知道Euclidean norm范数，即欧几里德范数（以下用w表示多维的向量）：\n再参考点到面的距离公式\n也就是\n类似的,扩展到多维的w向量也是一样，代数距离r类似于d，而\n类似于 ，所以展开后也就是：\n对比点平面的公式，以上的r也就多维度空间向量的到最优超平面的距离。\n再看上图，如果x = 0 即原点则有\n那么是如何得到的呢？很简单，如上面分析的\n因为x = 0，它在原点，它与任意可调权值向量w相乘都等于0，于是有：\n注意b为偏置，只是决定了决策曲面相对原点的偏离，结合上图我们可知道：\nb > 0 则原点在最优超平面的正面；\nb < 0 则原点在最优超平面的负面；\nb = 0 则原点就在最优超平面上。\n找到的这个最优超平面的参数和, 于是在样本向量集中, 有一对一定满足（因为是常数，它只是决定了决策曲面相对原点的偏离）：\n满足上式的点就是则为支持向量，这些点距离决策曲面也就时超平面最近，时最难区分的点。于是根据点到超平面的距离公式:\n在超平面的正面和负面我们有任一支持向量满足代数距离：\n如果让表示两个分离边缘的最优值，则根据上式有：\n所以我们可以看出，如果要使得最大，则就必须使得最小，也就可以总结为：\n最大化两个类之间的分离边缘等价于最小化权值向量w的欧几里得范数。\n二次最优化\n回头看我们前面提到的，给定一个训练集，我们的需求就是尝试找到一个决策边界使得几何间隔最大，回归到问题的本质那就是我们如何找到这个最大的几何间隔？ 要想要最大化间隔（margin），正如上面提到的：\n最大化两个类之间的分离边缘等价于最小化权值向量w的欧几里得范数。\n即：\n其中： 也就是前面提到的函数间隔：，回顾几何间隔：，约束条件就是让函数间隔等于几何间隔。\n或是将优化的问题转化为以下式子：\n其中，就是将函数间隔和几何间隔联系起来。\n我们发现以上两个式子都可以表示最大化间隔的优化问题，但是我们同时也发现无论上面哪个式子都是非凸的，并没有现成的可用的软件来解决这两种形式的优化问题。\n于是一个行之有效的优化问题的形式被提出来，注意它是一个凸函数形式，如下：\n以上的优化问题包含了一个凸二次优化对象并且线性可分，概括来说就是需找最优超平面的二次最优化，这个优化的问题可以用商业的凸二次规划代码来解。\n凸函数：\n在凸集中任取两个点连成一条直线，这条直线上的点仍然在这个集合内部，左边\n凸函数局部最优就是全局最优，而右边的非凸函数的局部最优就不是全局最优了。\n下面要具体介绍的拉格朗日对偶性，它可以引导我们到优化问题的对偶形式，因为对偶形式在高维空间有效的运用核（函数）来得到最优间隔分类器的方法中扮演了非常重要的角色。对偶形式让我们得到一个有效的算法来解决上述的优化问题并且相较通用的二次规划商业软件更好。\n优化问题的对偶形式的方法简单来说就是通过Lagrange Duality变换到对偶变量 (dual variable)的优化问题之后，应用拉格朗日对偶性，通过求解对偶问题得到最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：\n一是原问题的对偶问题往往更容易求解\n二者可以自然的引入核函数，进而推广到非线性分类问题。"}
{"content2":"不多说，直接上干货！\n安装cdh5到最后报如下错误：\n安装失败，无法接受agent发出的检测信号。\n确保主机名称正确\n确保端口7182可在cloudera manager server上访问（检查防火墙规则）\n确保正在添加的主机的端口9000和9001空闲\n检查在添加的主机上/var/log/cloudera-scm-agent/中的代理日志（某些日志可在安装详细信息中找到）\n即，这个问题就是。\nCloudera Manager 安装失败。 无法接收 Agent 发出的检测信号。\nInstallation failed. Failed to receive heartbeat from agent. Ensure that the host's hostname is configured properly. Ensure that port 7182 is accessible on the Cloudera Manager server (check firewall rules). Ensure that ports 9000 and 9001 are free on the host being added. Check agent logs in /var/log/cloudera-scm-agent/ on the host being added (some of the logs can be found in the installation details).\n首先，查看日志/var/log/cloudera-scm-agent/，得知（192.168.30.1这台）\ncloudemanager安装时出现ProtocolError: <ProtocolError for 127.0.0.1/RPC2: 401 Unauthorized>问题解决方法（图文详解）\n首先，查看日志/var/log/cloudera-scm-agent/，得知（192.168.30.2、192.168.30.3和192.168.30.4这台）\ncloudemanager安装时出现8475 MainThread agent ERROR Heartbeating to 192.168.30.1:7182 failed问题解决方法（图文详解）\n我的问题，按照上面这两步走，就解决了。\n如果上述问题，还没解决，则见如下的解决思路。（一般来说，如下是几乎大家都会配置对了的）\n解决：关闭防火墙\n1、Python文件不匹配；参考http://www.cnblogs.com/lion.net/archive/2014/09/02/3950619.html中_io的设置\n2、日志文件不存在，在config.ini中把log_file放开\n3、/etc/hosts/中主机和ip配置问题\n4、防火墙是否关闭，ubuntu是ufw disable\n5、端口配置，config.ini中端口是否配置的为7182\n6、集群时间是否同步，安装ntp同步时间\n7、ssh私钥的问题-----我现在正在查这个问题呢，前边都配完了，但是仍然无法检测到信号，我没有使用私钥，不知道是不是跟这个有关系\n查看主机名有错，查看 /etc/hosts 和 /etc/sysconfig/network 下的配置是否一致\n关闭防火墙 service iptables stop\n启动 Httpd服务 service httpd start\n1、首先，在安装时搜索不到192.168.0.70这个ip，说明你hosts可能配置得有问题或者是防火墙、网卡配置等其他原因，最好不要使用localhost 或者127.0.0.1去安装CM机器\n2.无法检测到agent服务的信号不代表没有安装成功，某些情况因为你机器或者是你使用虚拟机的等原因导致了无法接收信号，如果你要追究这个错误产生的原因，请检查你虚拟机的网络环境配置，以及centos系统的hosts 、防火墙、网卡等配置。\n3.再次声明：如果能够在主机页面找到该机器，说明agent服务已经安装成功！！！出现检测不到信号，只是不能进一步添加服务而已，不影响你在该主机上直接添加指定的服务。\n参考\nhttps://stackoverflow.com/questions/22088053/why-i-am-getting-this-error-installation-failed-failed-to-receive-heartbeat-fr\nhttps://my.oschina.net/MaTech/blog/374556\nhttp://bbs.csdn.net/topics/390740790/\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"---------------------------------------------------------------------------------------\n本系列文章为《机器学习实战》学习笔记，内容整理自书本，网络以及自己的理解，如有错误欢迎指正。\n源码在Python3.5上测试均通过，代码及数据 --> https://github.com/Wellat/MLaction\n---------------------------------------------------------------------------------------\n1 算法概述\n1.1 算法特点\n简单地说，k-近邻算法采用测量不同特征值之间的距离方法进行分类。\n优点：精度高、对异常值不敏感、无数据输入假定\n缺点：计算复杂度高、空间复杂度高\n适用数据范围：数值型和标称型\n1.2 工作原理\n存在一个训练样本集，并且每个样本都存在标签（有监督学习）。输入没有标签的新样本数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取出与样本集中特征最相似的数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，而且k通常不大于20。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。\n1.3 实例解释\n以电影分类为例子，使用k-近邻算法分类爱情片和动作片。有人曾经统计过很多电影的打斗镜头和接吻镜头，下图显示了6部电影的打斗和接吻镜头数。 假如有一部未看过的电影，如何确定它是爱情片还是动作片呢？\n①首先需要统计这个未知电影存在多少个打斗镜头和接吻镜头，下图中问号位置是该未知电影出现的镜头数\n②之后计算未知电影与样本集中其他电影的距离（相似度），具体算法先忽略，结果如下表所示：\n③将相似度列表排序，选出前k个最相似的样本。此处我们假设k=3，将上表中的相似度进行排序后前3分别是：He’s Not Really into Dudes，Beautiful Woman，California Man。\n④统计最相似样本的分类。此处很容易知道这3个样本均为爱情片。\n⑤将分类最多的类别作为未知电影的分类。那么我们就得出结论，未知电影属于爱情片。\n2 代码实现\n2.1 k-近邻简单分类的应用\n2.1.1 算法一般流程\n2.1.2 Python实现代码及注释\n1 #coding=UTF8 2 from numpy import * 3 import operator 4 5 def createDataSet(): 6 \"\"\" 7 函数作用：构建一组训练数据（训练样本），共4个样本 8 同时给出了这4个样本的标签，及labels 9 \"\"\" 10 group = array([ 11 [1.0, 1.1], 12 [1.0, 1.0], 13 [0. , 0. ], 14 [0. , 0.1] 15 ]) 16 labels = ['A', 'A', 'B', 'B'] 17 return group, labels 18 19 def classify0(inX, dataset, labels, k): 20 \"\"\" 21 inX 是输入的测试样本，是一个[x, y]样式的 22 dataset 是训练样本集 23 labels 是训练样本标签 24 k 是top k最相近的 25 \"\"\" 26 # shape返回矩阵的[行数，列数]， 27 # 那么shape[0]获取数据集的行数， 28 # 行数就是样本的数量 29 dataSetSize = dataset.shape[0] 30 31 \"\"\" 32 下面的求距离过程就是按照欧氏距离的公式计算的。 33 即 根号(x^2+y^2) 34 \"\"\" 35 # tile属于numpy模块下边的函数 36 # tile（A, reps）返回一个shape=reps的矩阵，矩阵的每个元素是A 37 # 比如 A=[0,1,2] 那么，tile(A, 2)= [0, 1, 2, 0, 1, 2] 38 # tile(A,(2,2)) = [[0, 1, 2, 0, 1, 2], 39 # [0, 1, 2, 0, 1, 2]] 40 # tile(A,(2,1,2)) = [[[0, 1, 2, 0, 1, 2]], 41 # [[0, 1, 2, 0, 1, 2]]] 42 # 上边那个结果的分开理解就是： 43 # 最外层是2个元素，即最外边的[]中包含2个元素，类似于[C,D],而此处的C=D，因为是复制出来的 44 # 然后C包含1个元素，即C=[E],同理D=[E] 45 # 最后E包含2个元素，即E=[F,G],此处F=G，因为是复制出来的 46 # F就是A了，基础元素 47 # 综合起来就是(2,1,2)= [C, C] = [[E], [E]] = [[[F, F]], [[F, F]]] = [[[A, A]], [[A, A]]] 48 # 这个地方就是为了把输入的测试样本扩展为和dataset的shape一样，然后就可以直接做矩阵减法了。 49 # 比如，dataset有4个样本，就是4*2的矩阵，输入测试样本肯定是一个了，就是1*2，为了计算输入样本与训练样本的距离 50 # 那么，需要对这个数据进行作差。这是一次比较，因为训练样本有n个，那么就要进行n次比较； 51 # 为了方便计算，把输入样本复制n次，然后直接与训练样本作矩阵差运算，就可以一次性比较了n个样本。 52 # 比如inX = [0,1],dataset就用函数返回的结果，那么 53 # tile(inX, (4,1))= [[ 0.0, 1.0], 54 # [ 0.0, 1.0], 55 # [ 0.0, 1.0], 56 # [ 0.0, 1.0]] 57 # 作差之后 58 # diffMat = [[-1.0,-0.1], 59 # [-1.0, 0.0], 60 # [ 0.0, 1.0], 61 # [ 0.0, 0.9]] 62 diffMat = tile(inX, (dataSetSize, 1)) - dataset 63 64 # diffMat就是输入样本与每个训练样本的差值，然后对其每个x和y的差值进行平方运算。 65 # diffMat是一个矩阵，矩阵**2表示对矩阵中的每个元素进行**2操作，即平方。 66 # sqDiffMat = [[1.0, 0.01], 67 # [1.0, 0.0 ], 68 # [0.0, 1.0 ], 69 # [0.0, 0.81]] 70 sqDiffMat = diffMat ** 2 71 72 # axis=1表示按照横轴，sum表示累加，即按照行进行累加。 73 # sqDistance = [[1.01], 74 # [1.0 ], 75 # [1.0 ], 76 # [0.81]] 77 sqDistance = sqDiffMat.sum(axis=1) 78 79 # 对平方和进行开根号 80 distance = sqDistance ** 0.5 81 82 # 按照升序进行快速排序，返回的是原数组的下标。 83 # 比如，x = [30, 10, 20, 40] 84 # 升序排序后应该是[10,20,30,40],他们的原下标是[1,2,0,3] 85 # 那么，numpy.argsort(x) = [1, 2, 0, 3] 86 sortedDistIndicies = distance.argsort() 87 88 # 存放最终的分类结果及相应的结果投票数 89 classCount = {} 90 91 # 投票过程，就是统计前k个最近的样本所属类别包含的样本个数 92 for i in range(k): 93 # index = sortedDistIndicies[i]是第i个最相近的样本下标 94 # voteIlabel = labels[index]是样本index对应的分类结果('A' or 'B') 95 voteIlabel = labels[sortedDistIndicies[i]] 96 # classCount.get(voteIlabel, 0)返回voteIlabel的值，如果不存在，则返回0 97 # 然后将票数增1 98 classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 99 100 # 把分类结果进行排序，然后返回得票数最多的分类结果 101 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) 102 return sortedClassCount[0][0] 103 104 if __name__== \"__main__\": 105 # 导入数据 106 dataset, labels = createDataSet() 107 inX = [0.1, 0.1] 108 # 简单分类 109 className = classify0(inX, dataset, labels, 3) 110 print('the class of test sample is %s' %className)\n2.2 在约会网站上使用k-近邻算法\n2.2.1 算法一般流程\n2.2.2 Python实现代码\ndatingTestSet.txt 文件中有1000行的约会数据，样本主要包括以下3种特征：\n每年获得的飞行常客里程数\n玩视频游戏所耗时间百分比\n每周消费的冰淇淋公升数\n将上述特征数据输人到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式 。在kNN.py中创建名为 file2matrix 的函数，以此来处理输人格式问题。该函数的输人为文件名字符串，输出为训练样本矩阵和类标签向量。autoNorm 为数值归一化函数，将任意取值范围的特征值转化为0到1区间内的值。最后，datingClassTest 函数是测试代码。\n将下面的代码增加到 kNN.py 中。\n1 def file2matrix(filename): 2 \"\"\" 3 从文件中读入训练数据，并存储为矩阵 4 \"\"\" 5 fr = open(filename) 6 arrayOlines = fr.readlines() 7 numberOfLines = len(arrayOlines) #获取 n=样本的行数 8 returnMat = zeros((numberOfLines,3)) #创建一个2维矩阵用于存放训练样本数据，一共有n行，每一行存放3个数据 9 classLabelVector = [] #创建一个1维数组用于存放训练样本标签。 10 index = 0 11 for line in arrayOlines: 12 # 把回车符号给去掉 13 line = line.strip() 14 # 把每一行数据用\\t分割 15 listFromLine = line.split('\\t') 16 # 把分割好的数据放至数据集，其中index是该样本数据的下标，就是放到第几行 17 returnMat[index,:] = listFromLine[0:3] 18 # 把该样本对应的标签放至标签集，顺序与样本集对应。 19 classLabelVector.append(int(listFromLine[-1])) 20 index += 1 21 return returnMat,classLabelVector 22 23 def autoNorm(dataSet): 24 \"\"\" 25 训练数据归一化 26 \"\"\" 27 # 获取数据集中每一列的最小数值 28 # 以createDataSet()中的数据为例，group.min(0)=[0,0] 29 minVals = dataSet.min(0) 30 # 获取数据集中每一列的最大数值 31 # group.max(0)=[1, 1.1] 32 maxVals = dataSet.max(0) 33 # 最大值与最小的差值 34 ranges = maxVals - minVals 35 # 创建一个与dataSet同shape的全0矩阵，用于存放归一化后的数据 36 normDataSet = zeros(shape(dataSet)) 37 m = dataSet.shape[0] 38 # 把最小值扩充为与dataSet同shape，然后作差，具体tile请翻看 第三节 代码中的tile 39 normDataSet = dataSet - tile(minVals, (m,1)) 40 # 把最大最小差值扩充为dataSet同shape，然后作商，是指对应元素进行除法运算，而不是矩阵除法。 41 # 矩阵除法在numpy中要用linalg.solve(A,B) 42 normDataSet = normDataSet/tile(ranges, (m,1)) 43 return normDataSet, ranges, minVals 44 45 def datingClassTest(): 46 # 将数据集中10%的数据留作测试用，其余的90%用于训练 47 hoRatio = 0.10 48 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt') #load data setfrom file 49 normMat, ranges, minVals = autoNorm(datingDataMat) 50 m = normMat.shape[0] 51 numTestVecs = int(m*hoRatio) 52 errorCount = 0.0 53 for i in range(numTestVecs): 54 classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) 55 print(\"the classifier came back with: %d, the real answer is: %d, result is :%s\" % (classifierResult, datingLabels[i],classifierResult==datingLabels[i])) 56 if (classifierResult != datingLabels[i]): errorCount += 1.0 57 print(\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) 58 print(errorCount)\n2.3 手写识别系统实例\n2.3.1 实例数据\n为了简单起见，这里构造的系统只能识别数字0到9。需要识别的数字已经使用图形处理软件，处理成具有相同的色彩和大小 : 宽髙是32像素x 32像素的黑白图像。尽管采用文本格式存储图像不能有效地利用内存空间，但是为了方便理解，我们还是将图像转换为文本格式。\ntrainingDigits是2000个训练样本，testDigits是900个测试样本。\n2.3.2 算法的流程\n2.3.3 Python实现代码\n将下面的代码增加到 kNN.py 中，img2vector 为图片转换成向量的方法，handwritingClassTest 为测试方法：\n1 from os import listdir 2 def img2vector(filename): 3 \"\"\" 4 将图片数据转换为01矩阵。 5 每张图片是32*32像素，也就是一共1024个字节。 6 因此转换的时候，每行表示一个样本，每个样本含1024个字节。 7 \"\"\" 8 # 每个样本数据是1024=32*32个字节 9 returnVect = zeros((1,1024)) 10 fr = open(filename) 11 # 循环读取32行，32列。 12 for i in range(32): 13 lineStr = fr.readline() 14 for j in range(32): 15 returnVect[0,32*i+j] = int(lineStr[j]) 16 return returnVect 17 18 def handwritingClassTest(): 19 hwLabels = [] 20 # 加载训练数据 21 trainingFileList = listdir('trainingDigits') 22 m = len(trainingFileList) 23 trainingMat = zeros((m,1024)) 24 for i in range(m): 25 # 从文件名中解析出当前图像的标签，也就是数字是几 26 # 文件名格式为 0_3.txt 表示图片数字是 0 27 fileNameStr = trainingFileList[i] 28 fileStr = fileNameStr.split('.')[0] #take off .txt 29 classNumStr = int(fileStr.split('_')[0]) 30 hwLabels.append(classNumStr) 31 trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr) 32 # 加载测试数据 33 testFileList = listdir('testDigits') #iterate through the test set 34 errorCount = 0.0 35 mTest = len(testFileList) 36 for i in range(mTest): 37 fileNameStr = testFileList[i] 38 fileStr = fileNameStr.split('.')[0] #take off .txt 39 classNumStr = int(fileStr.split('_')[0]) 40 vectorUnderTest = img2vector('testDigits/%s' % fileNameStr) 41 classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) 42 print(\"the classifier came back with: %d, the real answer is: %d, The predict result is: %s\" % (classifierResult, classNumStr, classifierResult==classNumStr)) 43 if (classifierResult != classNumStr): errorCount += 1.0 44 print(\"\\nthe total number of errors is: %d / %d\" %(errorCount, mTest)) 45 print(\"\\nthe total error rate is: %f\" % (errorCount/float(mTest)))\nk-近邻算法识别手写数字数据集，错误率为1. 2%。改变变量k的值、修改函数 handwritingClassTest 随机选取训练样本、改变训练样本的数目，都会对k-近邻算法的错误率产生影响，感兴趣的话可以改变这些变量值，观察错误率的变化。\nk-近邻算法是分类数据最简单最有效的算法。它必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。其另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。\n3 应用 scikit-learn 库实现k近邻算法\n1 \"\"\" 2 scikit-learn 库对knn的支持 3 数据集是iris虹膜数据集 4 \"\"\" 5 6 from sklearn.datasets import load_iris 7 from sklearn import neighbors 8 import sklearn 9 10 #查看iris数据集 11 iris = load_iris() 12 print(iris) 13 14 ''' 15 KNeighborsClassifier(n_neighbors=5, weights='uniform', 16 algorithm='auto', leaf_size=30, 17 p=2, metric='minkowski', 18 metric_params=None, n_jobs=1, **kwargs) 19 n_neighbors: 默认值为5，表示查询k个最近邻的数目 20 algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’},指定用于计算最近邻的算法，auto表示试图采用最适合的算法计算最近邻 21 leaf_size: 传递给‘ball_tree’或‘kd_tree’的叶子大小 22 metric: 用于树的距离度量。默认'minkowski与P = 2（即欧氏度量） 23 n_jobs: 并行工作的数量，如果设为-1，则作业的数量被设置为CPU内核的数量 24 查看官方api：http://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier 25 ''' 26 knn = neighbors.KNeighborsClassifier() 27 #训练数据集 28 knn.fit(iris.data, iris.target) 29 #训练准确率 30 score = knn.score(iris.data, iris.target) 31 32 #预测 33 predict = knn.predict([[0.1,0.2,0.3,0.4]]) 34 #预测，返回概率数组 35 predict2 = knn.predict_proba([[0.1,0.2,0.3,0.4]]) 36 37 print(predict) 38 print(iris.target_names[predict])\n代码解释参考原贴：http://blog.csdn.net/niuwei22007/article/details/49703719"}
{"content2":"转载请注明出处：http://www.cnblogs.com/ymingjingr/p/4271742.html\n目录\n机器学习基石笔记1——在何时可以使用机器学习(1)\n机器学习基石笔记2——在何时可以使用机器学习(2)\n机器学习基石笔记3——在何时可以使用机器学习(3)(修改版)\n机器学习基石笔记4——在何时可以使用机器学习（4）\n机器学习基石笔记5——为什么机器可以学习（1）\n机器学习基石笔记6——为什么机器可以学习（2）\n机器学习基石笔记7——为什么机器可以学习（3）\n机器学习基石笔记8——为什么机器可以学习（4）\n机器学习基石笔记9——机器可以怎样学习（1）\n机器学习基石笔记10——机器可以怎样学习（2）\n机器学习基石笔记11——机器可以怎样学习（3）\n机器学习基石笔记12——机器可以怎样学习（4）\n机器学习基石笔记13——机器可以怎样学得更好（1）\n机器学习基石笔记14——机器可以怎样学得更好（2）\n机器学习基石笔记15——机器可以怎样学得更好（3）\n机器学习基石笔记16——机器可以怎样学得更好（4）\n先简单介绍下这门课程，这门课是在著名的MOOC（Massive Online Open Course大型在线公开课）Coursera上的一门关于机器学习领域的课程，由国立台湾大学的年轻老师林轩田讲授。这门叫做机器学习基石的课程，共8周的课程为整个机器学习课程的上半部分，更偏重于理论和思想而非算法，主要分为四大部分来讲授。\nWhen can Machine Learn？在何时可以使用机器学习？\nWhy can Machine Learn？ 为什么机器可以学习？\nHow can Machine Learn？机器可以怎样学习？\nHow can Machine Learn Better？怎样能使机器学习更好？\n每一大块又分为几周来讲授，每周的课时分为两个大课，每个大课一般又分为四个小块来教学，一个小块一般在十分钟到二十分钟之间。\n以VC bound （VC限制）作为总线将整个基础课程贯通讲解了包括PLA（Perceptron learning algorithm感知器）、pocket、二元分类、线性回归（linear regression）、logistic回归（logistic regression）等等。\n以下不用大课小课来叙述了，写起来感觉怪怪的，就用章节来分别代表大课时和小课时。\n一、The learning problem\n机器学习问题。\nCourse Introduction\n课程简介。\n第一小节的内容就是课程简介，如上已进行了详细的介绍，这里就不多赘述。\n1.2 What is Machine Learning\n什么是机器学习？\n在搞清这个问题之前，先要搞清什么是学习。\n学习可以是人或者动物通过观察思考获得一定的技巧过程。\n而机器学习与之类似，是计算机通过数据和计算获得一定技巧的过程。\n注意这一对比，学习是通过观察而机器学习是通过数据（是计算机的一种观察）。\n对比图如-1。（本笔记的图和公式如不加说明皆是出自林老师的课件，下文不会对此在做说明）\n-1 学习与机器学习对比图 a）学习          b）机器学习\n那么紧接着就是要解决上述中出现的一个新的名词\"技巧\"（skill）。\n什么是技巧呢？技巧是一些能力表现的更加出色。\n机器学习中的技巧如预测（prediction）、识别（recognition）。\n来一个例子：从股票的数据中获得收益增多的这种技巧，这就是一种机器学习的例子。\n那既然人也可以通过观察获得一个技巧，为什么还需要机器学习呢？\n这就是为什么需要机器学习，简单来说，就是两大原因：\n一些数据或者信息，人来无法获取，可能是一些人无法识别的事物，或是数据信息量特别大；\n另一个原因是人的处理满足不了需求，比如：定义很多很多的规则满足物体识别或者其他需求；在短时间内通过大量信息做出判断等等。\n上面说的是为什么使用机器学习，那么什么情况下使用机器学习呢？是不是所有的情况都使用机器学习呢？\n这里给出了三个ML（机器学习的英文缩写）的关键要素：\n1、存在一个模式或者说表现可以让我们对它进行改进提高；\n2、规则并不容易那么定义；\n3、需要有数据。\n1.3 Applications of Machine Learning\n机器学习的应用。\n这一小节主要介绍的就是机器学习能用在哪些方面。个人感觉不是理论介绍的重点（不是说应用不重要，刚好相反，其实个人认为机器学习甚至整个计算机学科最重要的还是应用），就简述下机器学习可以应用在在衣食住行育乐，包含了人类生活的方方面面，所以机器学习的应用场景很广泛很有市场。\n1.4 Components of Machine Learning\n机器学习的组成部分。\n这一小节是第一章的重点，因为它将机器学习的理论应用符号及数学知识进行表示，而以下各章内容也都是在这小节内容的基础上展开的。\n从一个银行是否会发信用卡给用户的例子引出了机器学习可以分为哪几个部分（组件）。\n1.输入(input)：x∈X（代表银行所掌握的用户信息）\n2.输出(output)：y∈Y （是否会发信用卡给用户）\n3.未知的函数，即目标函数（target function）：f：X→Y（理想的信用卡发放公式）\n4.数据或者叫做资料（ data），即训练样本（ training examples）：D = {（）, ( ), …, ( )}（银行的历史记录）\n5.假设（hypothesis），即前面提到的技能，能够具有更好地表现：g：X→Y （能够学习到的公式）\n可以通过一个简单的流程图表示，如-2所示。\n-2 机器学习的简单流程图\n从图中可以清楚机器学习就是从我们未知但是却存在的一个规则或者公式f中得到大量的数据或者说资料（训练样本），在这些资料的基础上得到一个近似于未知规则g的过程。\n这么说还是有点抽象，特别是目标函数f又是未知的，那为什么还能找到一个假设g能够接近f呢？\n还是以一个更加详细的流程图来说明这一问题，如-3。\n-3 详细的机器学习流程图\n这个流程图和-2有些不同，其中ML被更详细的定义为机器学习算法（learning algorithm）一般用A表示。还多出来一个新的项目，就是假设空间或者叫做假设集合（hypothesis set）一般用H表示，它是包含各种各样的假设，其中包括好的假设和坏的假设，而这时A的作用就体现了，它可以从H这个集合中挑选出它认为最好的假设作为g。\n注：\n1、这里还要说明的是机器学习的输入在这个流程图中就变成了两个部分，一个是训练样本集，而另一个就是假设空间H。\n2、还有一点需要注意的是，我们所说的机器学习模型在这个流程图中也不仅仅是算法A，而且还包含了假设空间H。\n3、要求得g来近似于未知目标函数f。\n4、给出了机器学习的一个更准确点的定义，就是通过数据来计算得到一个假设g使它接近未知目标函数。\n-3是还是一个相对比较简单的机器学习流程图，在往后的章节中会不断的根据新学的知识继续扩展这幅图的元素。\n1.5 Machine Learning and Other Fields\n机器学习与其他各个领域的关系。\n1.5.1 ML VS DM （Data Mining）\n机器学习与数据挖掘者叫知识发现（KDD Knowledge Discovery in Dataset）。\n上一节中已经给出了机器学习的概念，因此只介绍下数据挖掘的概念，就是从大量的数据中找出有用的信息。\n从定义出发，我们可以将两者之间的关系分为3种。\n两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。\n两者是互助的：能够找出的有用信息就能帮助我们找出近似的假设，反之也可行。\n传统的数据挖掘更关注与从大量的数据中的计算问题。\n总的来时，两者密不可分。\n1.5.2 M L VS AI （artificial intelligence）\n机器学习与人工智能。\n人工智能的大概概念就是电脑能够表现出一些智慧行为。\n从定义可以得到，机器学习是实现人工智能的一种方式。\n1.5.3 ML VS statistic\n机器学习与统计。\n统计也需要通过数据，来做一个未知的推论。\n因此统计是一种实现机器学习的方法。\n传统的统计学习更关注与数学公式，而非计算本身。"}
{"content2":"从去年开始，陆陆续续学习了大半年的机器学习，现在是时候做个总结了。\n在以往的编程经验里面，我们需要对于输入有一个精确的，可控制的，可以说明的输出。例如，将1 + 1作为输入，其结果就是一个精确的输出 2 。并且不论怎么调整参数，都希望结果是2，并且能够很清楚的说明，为什么结果是2，不是3。这样的理念在传统的IT界，非常重要，所有的东西就像时钟一般精确，一切都是黑白分明的。由于这种严格的输入输出，衍生出很多对于程序的自动测试工具，你的程序无论怎么运行，都应该在相同输入情况下，得到相同的，准确的，精确的输出。\n但是，如果你进入机器学习的世界，则一切都是基于一个准确率。换句话说，你的模型，允许是不完美的，1 + 1，结果可以是 2.01，也可以是1.98。有时候，如果你的模型要追求完美，则可能出现过拟合的可能性。也就是说，由于你的模型太过于完美，使得模型可以很好的匹配训练用数据，反而失去了通用性，在数据发生变化的时候，发生错误。\n举个例子来说吧，如果一个男孩子说喜欢某个女孩子，这个女孩子身高178，籍贯是辽宁抚顺，专业是计算机。如果机器学习发生过拟合的时候，它就会输出这样一个模型\n如果 身高 = 178 ，籍贯 = 抚顺 ，专业 = 计算机 则喜欢。\n这个模型如果用来匹配一个个例，则这个模型是完美的！\n但是，如果这个女孩子身高是179呢，这个模型会告诉你，这个男孩子不喜欢她。其实，对于男孩子来说，178和179其实没有什么很大的区别。但是由于计算机想精确给出男孩子喜欢女孩子的模型，所以，计算机做出了过拟合的模型。\n当然，一般来说，计算机的模型应该是有弹性的。\n身高在 【175，185】之间\n籍贯是 东北\n专业是 IT相关的\n这样的话，模型虽然会把一些男孩子不喜欢的女孩子也错误的标识出来，但是大部分的样本还是可以比较好的预测出来的。\n机器学习追求的不是100%的正确，而是一个可以容忍的正确率。\n当然，在某些时候，还需要一些风险策略的，例如，在人工智能判断一个用户是否能够发给信用卡的时候，并不是说，这个人51%的可能性是一个讲信用的人，就发卡，而是这个人95%是讲信用的人的时候，才发卡的。机器给出的只是一个估计值，最后还是要人工控制风险的。\n机器学习，很多人认为是一个高科技的IT技能，其实，一个好的机器学习模型，领域里的业务知识还是很需要的。而且现在很多工具可以帮助大家建立程序，完全不需要什么编程的技能，只需要给机器“喂”数据，调节参数，就可以获得结果了。\n给机器“喂”什么数据，那些数据的特征值是有用的，那些特征值没有价值，这个就是领域专家思考的问题了。\n男孩子喜欢女孩子，这时候 颜值，身材，脾气 可能是比较关键的特征值，喜欢可口可乐还是百事可乐则变得基本没有什么价值。如果你的数据里面，都是女孩子喜欢那个牌子的可乐，这样的数据训练出来的模型没有任何意义。当然，如果你有很多特征值，还是有一些自动化的计算帮你挑选用那些特征值的（主成因分析）。\n在机器学习中，有一些复杂的概念，往往都是由一个简单的概念扩展开来的。\n卷积神经网络为首的一些神经网络的概念，都是从感知机这个小家伙来的。\n感知机的输出，是由输入和权重决定的，在监督学习中，输入和输出是已知的，然后机器学习通过不停的调整权重，使得感知机的输出（模型）和实际的输出（样本）尽量一致。这个过程中，学习结果就是这些权重，权重知道了，模型就定下来了。一个最简单的感知机的应用就是线性单元。\n零基础入门深度学习(1) - 感知器\n零基础入门深度学习(2) - 线性单元和梯度下降\n单个感知机是弱小的，但是，如果感知机有成千上万个，然后一层一层一层叠加起来呢。。这些小家伙就变成强大的神经网络了\n贝叶斯，马尔科夫同志则共享了很多关于概率的机器学习。\n贝叶斯最大贡献如下。\n在“你家隔壁住着老王（B）”的前提下，“你的孩子长得像隔壁老王（A）”的概率\n等于“你的孩子长得像隔壁老王（A）”的前提下，“你家隔壁住着老王（B）”\n乘以：“你的孩子长得像隔壁老王（A）”的概率（和隔壁是否住着老王无关）\n除以：“你家隔壁住着老王（B）”的概率\n当然这个正统说法要牵涉到先验概率，后验概率。\n从最简单的伯努利分布，到关于分布的分布的变态级别的狄利克雷分布，很多机器学习都在追求模型最符合抽样的分布概率。换句话说，就是希望从概率学上看，我们做出来的模型，和我们看到的样本之间，看上去是最相似。（最大似然）\n例如，我们要做一个模型，表示抛一枚硬币有多大概率正面向上。如果我们的样本告诉我们，10次里面，有7次正面向上，则我们说这枚硬币70%会出现正面向上。这个模型的结论和样本之间，从概率学上看是最有可能的。\n我们做的模型，就是追求和实际样本的结果，在概率学上看，是最有可能发生的情况。\n最快梯度下降则几乎出现在所有的迭代算法中。\n为什么梯度下降特别重要，因为大部分的算法都是尽可能将损失函数降低，怎么才能将损失函数降低，就是不停调整参数（权重），权重调整的方向，和梯度下降的方向是一致的。当然，最快梯度下降有可能不会收敛到全局最低点。（能否收敛到全局最低点，和初始位置有关）\n机器学习和自然语言处理也是密不可分的。在很多自然语言处理中，将大量使用机器学习的概念。马尔可夫链和条件随机场，狄利克雷分布这些都是自然语言处理的基础理论。\n关注公众号 TensorFlow教室 深度学习，机器学习，自然语言处理。"}
{"content2":"在学习机器学习算法的过程中，我们经常需要数据来验证算法，调试参数。但是找到一组十分合适某种特定算法类型的数据样本却不那么容易。还好numpy, scikit-learn都提供了随机数据生成的功能，我们可以自己生成适合某一种模型的数据，用随机数据来做清洗，归一化，转换，然后选择模型与算法做拟合和预测。下面对scikit-learn和numpy生成数据样本的方法做一个总结。\n完整代码参见我的github。\n1. numpy随机数据生成API\nnumpy比较适合用来生产一些简单的抽样数据。API都在random类中，常见的API有：\n1) rand(d0, d1, ..., dn) 用来生成d0xd1x...dn维的数组。数组的值在[0,1)之间\n例如：np.random.rand(3,2,2)，输出如下3x2x2的数组\narray([[[ 0.49042678,  0.60643763],\n[ 0.18370487,  0.10836908]],\n[[ 0.38269728,  0.66130293],\n[ 0.5775944 ,  0.52354981]],\n[[ 0.71705929,  0.89453574],\n[ 0.36245334,  0.37545211]]])\n2) randn((d0, d1, ..., dn), 也是用来生成d0xd1x...dn维的数组。不过数组的值服从N(0,1)的标准正态分布。\n例如：np.random.randn(3,2)，输出如下3x2的数组，这些值是N(0,1)的抽样数据。\narray([[-0.5889483 , -0.34054626],\n[-2.03094528, -0.21205145],\n[-0.20804811, -0.97289898]])\n如果需要服从$N(\\mu,\\sigma^2)$的正态分布，只需要在randn上每个生成的值x上做变换$\\sigma x + \\mu $即可，例如：\n例如：2*np.random.randn(3,2) + 1，输出如下3x2的数组，这些值是N(1,4)的抽样数据。\narray([[ 2.32910328, -0.677016  ],\n[-0.09049511,  1.04687598],\n[ 2.13493001,  3.30025852]])\n3)randint(low[, high, size])，生成随机的大小为size的数据，size可以为整数，为矩阵维数，或者张量的维数。值位于半开区间 [low, high)。\n例如：np.random.randint(3, size=[2,3,4])返回维数维2x3x4的数据。取值范围为最大值为3的整数。\narray([[[2, 1, 2, 1],\n[0, 1, 2, 1],\n[2, 1, 0, 2]],\n[[0, 1, 0, 0],\n[1, 1, 2, 1],\n[1, 0, 1, 2]]])\n再比如： np.random.randint(3, 6, size=[2,3]) 返回维数为2x3的数据。取值范围为[3,6).\narray([[4, 5, 3],\n[3, 4, 5]])\n4) random_integers(low[, high, size]),和上面的randint类似，区别在与取值范围是闭区间[low, high]。\n5) random_sample([size]), 返回随机的浮点数，在半开区间 [0.0, 1.0)。如果是其他区间[a,b),可以加以转换(b - a) * random_sample([size]) + a\n例如： (5-2)*np.random.random_sample(3)+2 返回[2,5)之间的3个随机数。\narray([ 2.87037573,  4.33790491,  2.1662832 ])\n2. scikit-learn随机数据生成API介绍\nscikit-learn生成随机数据的API都在datasets类之中，和numpy比起来，可以用来生成适合特定机器学习模型的数据。常用的API有：\n1) 用make_regression 生成回归模型的数据\n2) 用make_hastie_10_2，make_classification或者make_multilabel_classification生成分类模型数据\n3) 用make_blobs生成聚类模型数据\n4) 用make_gaussian_quantiles生成分组多维正态分布的数据\n3. scikit-learn随机数据生成实例\n3.1 回归模型随机数据\n这里我们使用make_regression生成回归模型数据。几个关键参数有n_samples（生成样本数）， n_features（样本特征数），noise（样本随机噪音）和coef（是否返回回归系数）。例子代码如下：\nimport numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets.samples_generator import make_regression # X为样本特征，y为样本输出， coef为回归系数，共1000个样本，每个样本1个特征 X, y, coef =make_regression(n_samples=1000, n_features=1,noise=10, coef=True) # 画图 plt.scatter(X, y, color='black') plt.plot(X, X*coef, color='blue', linewidth=3) plt.xticks(()) plt.yticks(()) plt.show()\n输出的图如下：\n3.2 分类模型随机数据\n这里我们用make_classification生成三元分类模型数据。几个关键参数有n_samples（生成样本数）， n_features（样本特征数）， n_redundant（冗余特征数）和n_classes（输出的类别数），例子代码如下：\nimport numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets.samples_generator import make_classification # X1为样本特征，Y1为样本类别输出， 共400个样本，每个样本2个特征，输出有3个类别，没有冗余特征，每个类别一个簇 X1, Y1 = make_classification(n_samples=400, n_features=2, n_redundant=0, n_clusters_per_class=1, n_classes=3) plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1) plt.show()\n输出的图如下：\n3.3 聚类模型随机数据\n这里我们用make_blobs生成聚类模型数据。几个关键参数有n_samples（生成样本数）， n_features（样本特征数），centers(簇中心的个数或者自定义的簇中心)和cluster_std（簇数据方差，代表簇的聚合程度）。例子如下：\nimport numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets.samples_generator import make_blobs # X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共3个簇，簇中心在[-1,-1], [1,1], [2,2]， 簇方差分别为[0.4, 0.5, 0.2] X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1], [1,1], [2,2]], cluster_std=[0.4, 0.5, 0.2]) plt.scatter(X[:, 0], X[:, 1], marker='o', c=y) plt.show()\n输出的图如下：\n3.4 分组正态分布混合数据\n我们用make_gaussian_quantiles生成分组多维正态分布的数据。几个关键参数有n_samples（生成样本数）， n_features（正态分布的维数），mean（特征均值）， cov（样本协方差的系数）， n_classes（数据在正态分布中按分位数分配的组数）。 例子如下：\nimport numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import make_gaussian_quantiles #生成2维正态分布，生成的数据按分位数分成3组，1000个样本,2个样本特征均值为1和2，协方差系数为2 X1, Y1 = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=3, mean=[1,2],cov=2) plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n输出图如下\n以上就是生产随机数据的一个总结，希望可以帮到学习机器学习算法的朋友们。\n（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）"}
{"content2":"不多说，直接上干货！\n汉化下载和教程页面 : https://github.com/kingmo888/Spyder_Simplified_Chinese\n汉化文件最新版直接下载 : https://github.com/kingmo888/Spyder_Simplified_Chinese/archive/master.zip\n下载完成解压后，是这样的:\n在这个文件夹内，按住shift，点击鼠标右键，就会出现”在此处打开CMD窗口(s)” 或者 “在此处打开Powershells(s)窗口“,如图:\n如果是CMD,点开之后,输入: python main.py ,如下图:\n如果是Powershell，点开之后，输入 : python .\\main.py ,如下图:\n结果分别是:\n现在汉化已经完成，重新打开Spyder，找到Tools –> Perferences\n点开之后，按图片所示顺序找到语言选项。\n选择简体中文，然后点击Apply，如下图\n点击之后会提示重启,点击确定就会立即重启，重启之后便是中文\n成功！\n当然，这仅仅是针对初学者，最好还是用英文。\n当然，可以切换中英文嘛。\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"为何取这标题呢，只是觉得目前人工智能只用于娱乐而已。\n很早的时候就想写几篇关于人工智能的东西，把人工智能的东西写的通俗易懂点，但是毕竟人工智能的东西涉及的领域太广了，特别是对数学和概率有比较深的理解，如果只是想简单的了解，可以跳过文章的公式。\n很难想象有什么事物会像廉价、强大、无处不在的人工智能那样拥有“改变一切”的力量。《必然》\n前段时间的AlphaGo再次的把人工智能炒的火热，关于人工智能的讨论又再次进入讨论的风口浪尖上。各个方面对AlphaGo技术的猜测，神经网络也再次成为了技术的焦点。\n一个看似简单的问题\n给你一堆的图片，从图片中分出是猫，狗。归结成一个大问题：分类。本身来说，分类对计算机来说本该是最擅长的，本身0和1，就是很好的分类，编程语言的if else，swich，可以做到很好的分类。\n像if else这种做法，似乎我们可以编写一套复杂的规则，这个规则覆盖所有的情况，就能够进行准确的分类了。但是这条路是走不通的，之前的自然语言处理就走过这条路。需要另外的选择一条出路。对，建模。通过模型来进行分类。\n要让机器像人一样的思考，最好的办法就是让他的模型尽量的一样。莱特兄弟发明飞机时，并没有像之前一样利用翅膀，而是通过动力学原理。思考的机器如果靠的是规则下的专家系统，太过复杂，没有规律。最好的一条路是通过数学公式进行建模。\n感知器\nHebb在1949年出版的《行为的组织》中，Hebb提出了其神经心理学理论。Hebb认为神经网络的学习过程最终是发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。之后人们相继提出了各种各样的学习算法。康奈尔航空实验室心理学家Frank Rosenblatt 受到这种思想的启发，认为这个简单想法足以创造一个可以学会识别物体的机器，在1956年，创建了算法和硬件。1958年，Frank Rosenblatt在《 New York Times 》上发表文章《Electronic ‘Brain’ Teaches Itself.》，正式把算法取名为“感知器”。\n1957年，Frank Rosenblatt 发布了算法模型：\n1958年夏，Frank Rosenblatt受到美国海军的经费自助，并召开新闻发布会。《纽约时报》抓住了发布会的要点：\n「海军透露了一种电子计算机的雏形，它将能够走路、说话、看、写、自我复制并感知到自己的存在……据预测，不久以后，感知器将能够识别出人并叫出他们的名字，立即把演讲内容翻译成另一种语言并写下来。」\n现在来看这段话，就能看出Frank Rosenblatt靠谱的预见性了。可是这些事情在当时看来远超人们的想象，认为Frank Rosenblatt天方夜谭，像小孩子一样想象着未来。\n感知器是有单层计算单元的神经网络，由线性元件及阀值元件组成。感知器如图所示。\n再来看 神经网络的学习过程最终是发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。 Frank Rosenblatt用数学的方法描述这个过程。\n感知器的数学模型：\n其中：f[.]是 阶跃函数 ，并且有\nθ是阀值。\n感知器的最大作用就是可以对输入的样本分类，故它可作分类器，感知器对输入信号的分类如下（A类，B类）：\n当感知器的输出为1时，输入样本称为A类；输出为-1时，输入样本称为B类。从上可知感知器的分类边界是：\n在输入样本只有两个分量X1，X2时，则有分类边界条件：\n即\n也可写成\n这时的分类情况如图所示。\n感知器算法\n感知器的学习算法： 目的在于找寻恰当的权系数(W1…Wn)，使系统对一个特 定的样本(X1…Xn)能产生期望值d。\n感知器学习算法步骤如下：\n对权系数置初值。\n输入一样(X1…Xn)本以及它的期望输出d。\n期望输出值d在样本的类属不同时取值不同。如果是A类，则取d＝1,如果是B类，则取-1。期望输出d也即是教师信号。\n计算实际输出值。\n根据实际输出求误差e。\n用误差e去修改权系数。\n转到第2点，一直执行到一切样本均稳定为止。\n感知器是整个神经网络的基础，神经元通过激励函数确定输出，神经元之间通过权值进行传递能量，权重的确定根据误差来进行调节（这个就是学习的过程），这个方法的前提是整个网络是收敛的。这个问题，1957年Frank Rosenblatt证明了这个结论。\n或许这才是开始\n但是1969年，Minsky 和Papert所著的《Perceptron》一书出版，该书从数学角度证明了关于单层感知器的计算具有根本的局限性，指出感知器的处理能力有限，甚至连XOR这样的问题也不能解决，并在多层感知器的总结中，论述了单层感知器的所有局限性在多层感知器中是不可能被全部克服的。神经网络进入了萧条期。\nMarvin Minsky是“人工智能之父”，1970年，Minsky获得了计算机科学界最高奖项——图灵奖（the Turing Award），同时他也是第一位获此殊荣的人工智能学者。2016 年1月24日，上帝着需要人工智能，带走了Marvin Minsky，享年89岁。\n这才是开始，2004年IEEE Frank Rosenblatt Award成立，Frank Rosenblatt被尊称为神经网络的创立者。\n神经网络开启了人类对大脑的模拟形式，一种新型对大脑的建模，在这条路上后续有更多的科学家前仆后继，我们是树下乘凉的人。\n（未完待续）\nfrom: http://datartisan.com/article/detail/108.html"}
{"content2":"转载请注明出处：http://www.cnblogs.com/ymingjingr/p/4271742.html\n目录\n机器学习基石笔记1——在何时可以使用机器学习(1)\n机器学习基石笔记2——在何时可以使用机器学习(2)\n机器学习基石笔记3——在何时可以使用机器学习(3)(修改版)\n机器学习基石笔记4——在何时可以使用机器学习（4）\n机器学习基石笔记5——为什么机器可以学习（1）\n机器学习基石笔记6——为什么机器可以学习（2）\n机器学习基石笔记7——为什么机器可以学习（3）\n机器学习基石笔记8——为什么机器可以学习（4）\n机器学习基石笔记9——机器可以怎样学习（1）\n机器学习基石笔记10——机器可以怎样学习（2）\n机器学习基石笔记11——机器可以怎样学习（3）\n机器学习基石笔记12——机器可以怎样学习（4）\n机器学习基石笔记13——机器可以怎样学得更好（1）\n机器学习基石笔记14——机器可以怎样学得更好（2）\n机器学习基石笔记15——机器可以怎样学得更好（3）\n机器学习基石笔记16——机器可以怎样学得更好（4）\n十一、Linear Models for Classification\n用于分类的线性模型。\n11.1 Linear Models for Binary Classification\n用于二元分类的线性模型。\n目前叙述的算法模型主要有3类：线性二元分类，线性回归，logistic回归，这三个模型的最主要的相同点在假设函数和错误函数中都出现了线性得分函数（linear scoring function），如公式11-1所示。\n（公式11-1）\n三类模型与得分s之间的关系如-1所示。\n-1 三类模型与得分s的关系\n最左为线性二元分类，其假设函数为，一般使用0/1错误，通过求解最优权值向量w比较困难；中间为线性回归模型，其假设函数为，一般使用平方错误，可直接通过解析解求解最优w；最右为logistic回归模型，假设函数为，使用交叉熵错误，通过梯度下降法求出近似的w。\n从上述分析不难看出，线性二元分类问题的求解方式最为困难，但与另外两种模型存在着共同点——得分s，能否利用这两种模型的算法近似求得二分类问题的最优w呢？\n回顾10.2节，logistic回归的错误，可用符号 表示，其中CE为交叉熵（cross-entropy ）的缩写，可以写成公式11-2所示。\n（公式11-2）\n是否二元分类模型和线性回归模型的错误函数可以写成关于 的形式？答案是可以的，如-2所示。\n-2 三类模型的错误函数\n二元分类模型和线性回归模型错误函数中的转换都用到了 的性质。接着观察三类模型的错误函数与ys之间的关系。本节开头回顾了s的物理意义为得分，此处ys的物理意义是正确的得分 ，因此ys越大越好，表示两者接近且同号。\n根据-2中的三类模型的错误函数有关ys的公式，可以得出如-3所示的关系图。\n-3 三类模型的错误函数与ys的关系图\n其中蓝色的折线表示0/1错误 ，在ys大于0时，，反之；红色的抛物线表示平方错误，在 时与在该范围内所表现出的特征相似，但是在时与在该范围内所表达的效果相去甚远，因此只有在很小的情况下，可以使用取代；墨绿的曲线表示，同样如-3所示也只有在很小的情况下， 和 可互相取代。但是跟想得到的错误曲线还有一些差距，因此略做转变，得到公式11-3。\n（公式11-3）\n其中表示缩放的（scaled），即对做了一个换底，因此可以得到-4。\n-4 关于ys的图\n如-4中墨绿色的线表示，从图中可以看出，该错误函数很适合做的上限，在很小的情况下， 和 可互相取代，如公式11-4所示。\n（公式11-4）\n通过公式11-4可以得出和的上限，如公式11-5和公式11-6所示。\n（公式11-5）\n（公式11-6）\n再通过VC限制理论可以得到公式11-7。\n（公式11-7）\n第一个不等号连接的是在VC限制下和其上界，概念见7.4节, 其中函数也是在7.4节中提到过的模型复杂度，在二元分类中可以写成的形式。\n因此得到如下结论：小的可以通过小的得出。同理可以证明小的也可以通过小的得出，即线性回归模型和logistic回归模型可以用作二元分类。\n算法流程一般是在输出空间 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优 ；\n将求得的代入公式sign，得到最优假设函数 。\n三类模型做分类的利弊分析如表11-1所示。\n表11-1 三类模型做分类的利弊分析\n二元分类\n线性回归\nLogistic回归\n好处\n在线性可分的情况下可以保证完成\n最容易的优化算法\n容易的优化算法\n坏处\n在线性不可分的情况，需要使用启发式pocket\n在 非常大时，相对于是一个很宽松的上界\n在ys为负时，是一个宽松的上界\n线性回归一般只作为PLA、pocket、logistic回归的初始向量 ；logistic回归经常取代pocket算法。\n11.2 Stochastic Gradient Descent\n随机梯度下降。\n如公式11-8为迭代优化算法的通式，学过的PLA的迭代算法如公式11-9，logistic回归中梯度下降的迭代公式如公式11-10。\n（公式11-8）\n（公式11-9）\n（公式11-10）\n对比以上两种迭代优化方法，：PLA与logistic回归的梯度下降。发现PLA只需要通过一个样本点便可计算出，即每次迭代的时间复杂度为 ；logistic回归的梯度下降需要遍历所有的样本点才能计算出，即每次迭代的时间复杂度为。有无可能将logistic回归每次迭代时间复杂度降为？\n观察公式11-10，方向向量v，v≈，该梯度是通过所有的样本点加权求和再取平均得到的，如何使用一个样本点的取值近似整体的平均值？\n可以将求平均的过程理解为求期望值，此处使用在N个样本中随机抽取一个样本点求出的梯度取代原来的期望梯度，这种随机选取的梯度称为随机梯度（stochastic gradient），可用符号表示，而真实的梯度与随机梯度的关系如公式11-11。\n（公式11-11）\n随机梯度值可以看做真实的梯度值加上一个噪音，使用随机梯度取代真实梯度做梯度下降的算法称作随机梯度下降（stochastic gradient descent），简称SGD。这种替代的理论基础是在迭代次数足够多的情况下，平均的随机梯度和平均的真实梯度相差不大。\n该算法的优点是简单，容易计算，适用于大数据或者流式数据；缺点是不稳定。\nLogistic回归的随机梯度下降的迭代如公式11-12所示。\n（公式11-12）\n是否联想到了其他的迭代算法？PLA，如公式11-13所示。\n（公式11-13）\n因此logistic回归随机梯度下降类似于\"软\"的PLA，为什么称为软的？原因是它的之前的权值并没有那么绝对不是1就是0，而是一个在0~1之间的值。在公式11-12中，如果且始终是一个很大的值，则logistic回归随机梯度下降相当于是PLA。\nSGD需要调试两个参数：迭代步骤t和学习速率。调试迭代步骤是因为不知道真实的梯度值是否接近0，只能假设足够步数后是已经做到足够好，即通常设置一个大的数值作为步数；学习速率通常也很难选定，林老师推荐的是数字为0.1126。\n11.3 Multiclass via Logistic Regression\n通过logistic回归实现多类别分类。\n多类别分类有许多应用场景，特别是在识别（recognition）领域。\n如-5为，输出空间y为四类别的情况，即。\n-5 四分类问题\n实际多类别问题也可以使用二元分类问题 的思路进行分类，如将原四类问题分解为是否为 ，即将与其他的类别分离，生成一个新的二元分类问题，即，通过此方式得到一个分类超平面，如-6所示。\n-6 以是否为进行二元分类\n同理可以以是否为生成一个新的二元分类问题，即，该分类超平面如-7所示。\n-7 以是否为进行二元分类\n另外两种情况就不一一列举，最终以是否为每个类别得到的二元分类如-8。\n-8 四个类别各自的二元分类情况\n当将-8的四种情况合并在一个图中会发现有一些无法处理的情形，如-9所示。\n-8 四种情况合并图\n其中四个边缘的三角阴影所在的区域为相邻两个类别都争夺的区域，如最上方的三角区域是类别和类别重叠的区域；还有图正中的区域又不属于任何类别。这些问题如何解决？\n使用以前学过的软性分类，还是关于类别的二元分类问题，此处不再使用硬划分，而是使用该样本点是的可能性，即 ，如-9所示。\n-9 关于类别的软化分\n余下三种情况不再一一举例，最终得到的四种类别的分类情况和合并后的情况分别如-10和-11所示。\n-10四个类别各自的软二元分类情况\n-11四个类别软二元分类合并后情况\n如何判断样本点属于哪个类别，可以分别计算样本点在四种软二元分类情况下概率，选择其中概率最大的一个作为所属类别，如公式11-14所示。\n（公式11-14）\n其中求概率的公式使用logistic函数 ，k表示类别，注意到logistic函数是一个单调函数，因此可以消去该函数，直接使用个类别的得分值作比较，如公式11-5所示。\n（公式11-15）\n用此种思路设计的算法称作一对多（One Versue All），简称为OVA，表示一个类别对其他所有类别，算法流程如下：\n在整个训练数据集D上， （在y=k时为+1，y≠k时为-1，符号取1或者0 ），使用logistic函数计算各个类别的权值向量 ；\n返回假设函数g，。\n该算法的优点是简单有效，易于类似于logistic函数的二元分类问题扩展成多类别分类；缺点是当类别特别多时，产生了不平衡的现象（如类别特别多，则+1的数据量就很少，大部分都是-1，数据量严重不平衡）。\n11.4 Multiclass via Binary Classification\n通过二元分类实现多类别分类。\n上一节的最后提到OVA的方式在类别非常多的情况下，出现了训练数据严重失衡的现象，于是本节介绍一种应对这类不平衡问题的方法。\n还是上节中使用的四分类问题，不像OVA在整个数据集中计算是否为的权值向量w，此种方法是任意选择四类中的两类，如类别和类别，将两个类别分别设为+1和-1，形式如，在包含两类的数据集上计算权值向量w，如-12。\n-12类别和类别的二分类\n如上述情况相同，从四种类别中选取两种做二元分类，一共可得6种对比（ ）,各对比如-13所示。\n-13 6种对比情况\n如-13得到6个不同的权值向量w，如何判断某新进样本属于哪个分类？如11-14中紫色的样本点在6中情况下所属的类别，前三种属于，第4种属于，后两种属于，只需要找出在所有对比中胜利次数最多的类别，因此该点属于。这种方式如同投票选举，样本点属于所有类别对比中赢得次数最多的那种类别。\n-14 某点在6种情形下的隶属情况\n这种分类方式称为一对一（one vervuse one），简称OVO。其算法流程如下：\n所有类别的任意两个类别做对比，使用二元分类算法，在数据集D， 求出最佳的权值向量；\n通过投票返回假设函数g。\n其优点是简单有效，在做两两对比时，每次使用的不是全部训练数据，而是仅属于当前两类的训练数据，能将所有类似于二元分类的算法扩展成多元分类问题； 缺点是对比次数是 ，即 ，其中K表示类别数，因此就需要花费更多的存储空间、计算时间。"}
{"content2":"不多说，直接上干货！\nWindows Server 2003、2008、2012系统的安装\n推荐网址：打开MSDN网站（http://msdn.itellyou.cn ）\n关于给电脑换系统，很多人会花钱去电脑店里换，或者是下载Ghost系统。但这些系统都不是微软原版的，制作者已经集成了很多常用软件或垃圾软件进去。我在这给大家介绍的是如何下载正版的Windows系统。这个利用的是MSDN的VOL版本。MSDN是微软面向软件开发者所建的一个网站，它里面的资源很多，当然也包括操作系统和Office软件。VOL版是面向企业或政府批量使用的一个版本，它的优点是在于它在安装过程中不需要输入激活码，只许在安装好后，用激活软件激活即可。VOL版和其他版本在使用上是没有任何区别的。\n1、打开MSDN网站（http://msdn.itellyou.cn ）\n2、 点开左边的操作系统，我在这里以下载win8系统为例，在操作系统下找到Windows 8.1，点击。在中间偏左的位置找到中文-简体，点击。如果你想用其他语言版本的，也可以。然后就要找\nWindows 8.1 (multiple editions) (x64) - DVD (Chinese-Simplified)\n然后就可以下载了。把红框里的内容复制。\ncn_windows_8_1_x64_dvd_2707237.iso\n然后打开迅雷。一般的话会自动跳出下载页面。如果没有跳出，就点击迅雷的新建，然后把刚才复制的链接粘贴进去。\n创建新的虚拟机\n点击“自定义（高级）“\n默认的，下一步\n点击稍后安装操作系统\n客户机操作系统：Microsoft Windows(W)\n版本：Windows 8 x64\n虚拟机名称： Windows 8 x64\n位置：\nD:\\SoftWare\\Virtual Machines\\Windows\\Win8.1\n默认\n默认\n默认\nNAT模式\n默认\n默认\n创建新虚拟磁盘\n安装Win8系统，默认60GB，\n方便管理，将虚拟磁盘存储为单个文件\n默认\n自定义硬件\n准备好 Win8.1系统的镜像文件\n浏览，找到\n完成\n开启\nWin8.1的安装界面，开始了，下一步\n现在安装\n输入Win8的序列号\nXHQ8N-C3MCJ-RQXB6-WCHYG-C9WKB\n点击，接受许可条款\n如图，一般是为了重新安装系统，所以是自定义（高级），而升级是如win8升win10\n新建\n分30G，点击，应用\n确定\n新建\n应用\n下一步\n个性化，电脑名称，\nzhouls-pc\n使用快速设置\n成功!!!\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"1 KNN算法\n1.1 KNN算法简介\nKNN（K-Nearest Neighbor）工作原理：存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。\n说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。\n举例：以电影分类作为例子，电影题材可分为爱情片，动作片等，那么爱情片有哪些特征？动作片有哪些特征呢？也就是说给定一部电影，怎么进行分类？这里假定将电影分为爱情片和动作片两类，如果一部电影中接吻镜头很多，打斗镜头较少，显然是属于爱情片，反之为动作片。有人曾根据电影中打斗动作和接吻动作数量进行评估，数据如下：\n电影名称\n打斗镜头\n接吻镜头\n电影类别\nCaliforia Man\n3\n104\n爱情片\nBeautigul Woman\n1\n81\n爱情片\nKevin Longblade\n101\n10\n动作片\nAmped II\n98\n2\n动作片\n给定一部电影数据（18，90）打斗镜头18个，接吻镜头90个，如何知道它是什么类型的呢？KNN是这样做的，首先计算未知电影与样本集中其他电影的距离（这里使用曼哈顿距离），数据如下：\n电影名称\n与未知分类电影的距离\nCaliforia Man\n20.5\nBeautigul Woman\n19.2\nKevin Longblade\n115.3\nAmped II\n118.9\n现在我们按照距离的递增顺序排序，可以找到k个距离最近的电影，加入k=3,那么来看排序的前3个电影的类别，爱情片，爱情片，动作片，下面来进行投票，这部未知的电影爱情片2票，动作片1票，那么我们就认为这部电影属于爱情片。\n1.2 KNN算法优缺点\n优点：精度高，对异常值不敏感、无数据输入假定\n缺点：计算复杂度高、空间复杂度高\n1.3 KNN算法python代码实现\n实现步骤：\n（1）计算距离\n（2）选择距离最小的k个点\n（3）排序\nPython 3代码：\n1 import numpy as np 2 import operator 3 4 def classify(intX,dataSet,labels,k): 5 ''' 6 KNN算法 7 ''' 8 #numpy中shape[0]返回数组的行数，shape[1]返回列数 9 dataSetSize = dataSet.shape[0] 10 #将intX在横向重复dataSetSize次，纵向重复1次 11 #例如intX=([1,2])--->([[1,2],[1,2],[1,2],[1,2]])便于后面计算 12 diffMat = np.tile(intX,(dataSetSize,1))-dataSet 13 #二维特征相减后乘方 14 sqdifMax = diffMat**2 15 #计算距离 16 seqDistances = sqdifMax.sum(axis=1) 17 distances = seqDistances**0.5 18 print (\"distances:\",distances) 19 #返回distance中元素从小到大排序后的索引 20 sortDistance = distances.argsort() 21 print (\"sortDistance:\",sortDistance) 22 classCount = {} 23 for i in range(k): 24 #取出前k个元素的类别 25 voteLabel = labels[sortDistance[i]] 26 print (\"第%d个voteLabel=%s\",i,voteLabel) 27 classCount[voteLabel] = classCount.get(voteLabel,0)+1 28 #dict.get(key,default=None),字典的get()方法,返回指定键的值,如果值不在字典中返回默认值。 29 #计算类别次数 30 31 #key=operator.itemgetter(1)根据字典的值进行排序 32 #key=operator.itemgetter(0)根据字典的键进行排序 33 #reverse降序排序字典 34 sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(1),reverse = True) 35 #结果sortedClassCount = [('动作片', 2), ('爱情片', 1)] 36 print (\"sortedClassCount:\",sortedClassCount) 37 return sortedClassCount[0][0]\nView Code\n2 KNN算法实例\n2.1 KNN实现电影分类\n1 import numpy as np 2 import operator 3 4 def createDataset(): 5 #四组二维特征 6 group = np.array([[5,115],[7,106],[56,11],[66,9]]) 7 #四组对应标签 8 labels = ('动作片','动作片','爱情片','爱情片') 9 return group,labels 10 11 def classify(intX,dataSet,labels,k): 12 ''' 13 KNN算法 14 ''' 15 #numpy中shape[0]返回数组的行数，shape[1]返回列数 16 dataSetSize = dataSet.shape[0] 17 #将intX在横向重复dataSetSize次，纵向重复1次 18 #例如intX=([1,2])--->([[1,2],[1,2],[1,2],[1,2]])便于后面计算 19 diffMat = np.tile(intX,(dataSetSize,1))-dataSet 20 #二维特征相减后乘方 21 sqdifMax = diffMat**2 22 #计算距离 23 seqDistances = sqdifMax.sum(axis=1) 24 distances = seqDistances**0.5 25 print (\"distances:\",distances) 26 #返回distance中元素从小到大排序后的索引 27 sortDistance = distances.argsort() 28 print (\"sortDistance:\",sortDistance) 29 classCount = {} 30 for i in range(k): 31 #取出前k个元素的类别 32 voteLabel = labels[sortDistance[i]] 33 print (\"第%d个voteLabel=%s\",i,voteLabel) 34 classCount[voteLabel] = classCount.get(voteLabel,0)+1 35 #dict.get(key,default=None),字典的get()方法,返回指定键的值,如果值不在字典中返回默认值。 36 #计算类别次数 37 38 #key=operator.itemgetter(1)根据字典的值进行排序 39 #key=operator.itemgetter(0)根据字典的键进行排序 40 #reverse降序排序字典 41 sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(1),reverse = True) 42 #结果sortedClassCount = [('动作片', 2), ('爱情片', 1)] 43 print (\"sortedClassCount:\",sortedClassCount) 44 return sortedClassCount[0][0] 45 46 47 48 if __name__ == '__main__': 49 group,labels = createDataset() 50 test = [20,101] 51 test_class = classify(test,group,labels,3) 52 print (test_class)\nView Code\n2.2 改进约会网站匹配\n这个例子简单说就是通过KNN找到你喜欢的人，首先数据样本包含三个特征，（a）每年获得的飞行常客里程数（b）玩游戏消耗的时间（c）每周消耗的冰激淋公升数，样本数据放在txt中，如下，前三列为三个特征值，最后一列为标签\n下面\n首先读取数据，获取数据集和标签\n1 def file2matrix(filename): 2 fr = open(filename) 3 arraylines = fr.readlines() 4 #获取行数 5 numberoflines = len(arraylines) 6 #返回numpy的数据矩阵,目前矩阵数据为0 7 returnMat = np.zeros([numberoflines,3]) 8 #返回的分类标签 9 classLabelVector = [] 10 #行的索引 11 index = 0 12 for line in arraylines: 13 #str.strip(rm) 删除str头和尾指定的字符 rm为空时，默认删除空白符(包括'\\n','\\r','\\t',' ') 14 line = line.strip() 15 #每行数据是\\t划分的，将每行数据按照\\t进行切片划分 16 listFromLine = line.split('\\t') 17 #取出前三列数据存放到returnMat 18 returnMat[index,:] = listFromLine[0:3] 19 #根据文本中标记的喜欢程度进行分类 20 if listFromLine[-1] == \"didntLike\": 21 classLabelVector.append(1) 22 elif listFromLine[-1] == \"smallDoses\": 23 classLabelVector.append(2) 24 else: 25 classLabelVector.append(3) 26 index += 1 27 return returnMat,classLabelVector\nView Code\n数据和标签我们可以打印一下：\n下面\n下面用Matplotlib作图看一下数据信息：\n1 from matplotlib.font_manager import FontProperties 2 import numpy as np 3 import matplotlib.pyplot as plt 4 from prepareData_1 import file2matrix 5 import matplotlib.lines as mlines 6 # from matplotlib.font_manage import FontProperties 7 ''' 8 函数说明：数据可视化 9 Parameters: 10 datingDataMat - 特征矩阵 11 datingLabels - 分类标签向量 12 Returns: 13 无 14 ''' 15 def showDatas(datingDataMat,datingLabels): 16 #设置汉子格式 17 font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14) 18 #函数返回一个figure图像和一个子图ax的array列表。 19 fig,axs = plt.subplots(nrows=2,ncols=2,sharex=False,sharey=False,figsize=(13,8)) 20 21 numberofLabels = len(datingLabels) 22 LabelColors = [] 23 for i in datingLabels: 24 if i==1: 25 LabelColors.append('black') 26 if i ==2: 27 LabelColors.append('orange') 28 if i==3: 29 LabelColors.append(\"red\") 30 #画散点图，以数据矩阵的第一列（飞行常客历程）、第二列（玩游戏）数据话散点图 31 #散点大小为15 透明度为0.5 32 axs[0][0].scatter(x=datingDataMat[:,0],y=datingDataMat[:,1],color=LabelColors, 33 s=15,alpha=0.5) 34 axs0_title_text=axs[0][0].set_title(u\"每年获得的飞行里程数与玩视频游戏消耗时间占比\", 35 FontProperties=font) 36 axs0_xlabel_text=axs[0][0].set_xlabel(\"每年获得的飞行常客里程数\",FontProperties=font) 37 axs0_ylabel_text=axs[0][0].set_ylabel(\"玩游戏消耗的时间\",FontProperties=font) 38 plt.setp(axs0_title_text,size=9,weight='bold',color='red') 39 #画散点图，以数据矩阵的第一列（飞行常客历程）、第三列（冰激淋公斤数）数据话散点图 40 #散点大小为15 透明度为0.5 41 axs[0][1].scatter(x=datingDataMat[:,0],y=datingDataMat[:,2],color=LabelColors, 42 s=15,alpha=0.5) 43 axs0_title_text=axs[0][0].set_title(\"每年获得的飞行里程数与冰激淋公斤数占比\", 44 FontProperties=font) 45 axs0_xlabel_text=axs[0][0].set_xlabel(\"每年获得的飞行常客里程数\",FontProperties=font) 46 axs0_ylabel_text=axs[0][0].set_ylabel(\"所吃冰激淋公斤数\",FontProperties=font) 47 plt.setp(axs0_title_text,size=9,weight='bold',color='red') 48 #画散点图，以数据矩阵的第二列（玩游戏）、第三列（冰激淋公斤数）数据话散点图 49 #散点大小为15 透明度为0.5 50 axs[1][0].scatter(x=datingDataMat[:,1],y=datingDataMat[:,2],color=LabelColors, 51 s=15,alpha=0.5) 52 axs0_title_text=axs[0][0].set_title(\"玩游戏时间与冰激淋公斤数占比\", 53 FontProperties=font) 54 axs0_xlabel_text=axs[0][0].set_xlabel(\"每年获得的飞行常客里程数\",FontProperties=font) 55 axs0_ylabel_text=axs[0][0].set_ylabel(\"所吃冰激淋公斤数\",FontProperties=font) 56 plt.setp(axs0_title_text,size=9,weight='bold',color='red') 57 58 #设置图例 59 didntLike = mlines.Line2D([],[],color='black',marker='.',markersize=6,label='didntlike') 60 smallDose = mlines.Line2D([],[],color='orange',marker='.',markersize=6,label='smallDose') 61 largeDose = mlines.Line2D([],[],color='red',marker='.',markersize=6,label='largeDose') 62 63 #添加图例 64 axs[0][0].legend(handles=[didntLike,smallDose,largeDose]) 65 axs[0][1].legend(handles=[didntLike,smallDose,largeDose]) 66 axs[1][0].legend(handles=[didntLike,smallDose,largeDose]) 67 68 plt.show() 69 70 if __name__ == '__main__': 71 filename = \"datingTestSet.txt\" 72 returnMat,classLabelVector = file2matrix(filename) 73 showDatas(returnMat,classLabelVector) 74 75\nView Code\n这里我把py文件分开写了，还要注意txt数据的路径，高大上的图：\n样本数据中的到底喜欢什么样子的人？自己去分析一下吧。下面要对数据进行归一化，归一化的原因就不多说了，\n1 from prepareData_1 import file2matrix 2 import numpy as np 3 ''' 4 函数说明：数据归一化 5 Parameters: 6 dataSet - 特征矩阵 7 Returns: 8 normDataSet - 归一化后的特征矩阵 9 ranges - 数据范围 10 minVals - 数据最小值 11 ''' 12 13 def autoNorm(dataSet): 14 #获得数据的最大最小值 15 print (dataSet) 16 print (\"**********************\") 17 minVals = dataSet.min(0) 18 maxVals = dataSet.max(0) 19 print (\"minValues:\",minVals) 20 print (\"maxValuse:\",maxVals) 21 #计算最大最小值的差 22 ranges = maxVals - minVals 23 print () 24 #shape(dataSet)返回dataSet的矩阵行列数 25 normDataSet=np.zeros(np.shape(dataSet)) 26 #返回dataSet的行数 27 m = dataSet.shape[0] 28 #原始值减去最小值 29 normDataSet=dataSet-np.tile(minVals,(m,1)) 30 #除以最大值和最小值的差，得到的归一化的数据 31 normDataSet = normDataSet/np.tile(ranges,(m,1)) 32 return normDataSet,ranges,minVals\nView Code\n归一化后的数据如下：\n有了以上步骤，下面就可以构建完整的约会分类，去找你喜欢的人了：\n1 from prepareData_1 import file2matrix 2 from dataNormal_3 import autoNorm 3 import operator 4 import numpy as np 5 ''' 6 函数说明：knn算法，分类器 7 Parameters: 8 inX - 用于分类的数据（测试集） 9 dataset - 用于训练的数据（训练集） 10 labes - 分类标签 11 k - knn算法参数，选择距离最小的k个点 12 Returns: 13 sortedClassCount[0][0] - 分类结果 14 ''' 15 def classify0(inX,dataset,labes,k): 16 dataSetSize = dataset.shape[0] #返回行数 17 diffMat = np.tile(inX,(dataSetSize,1))-dataset 18 sqDiffMat = diffMat**2 19 sqDistances = sqDiffMat.sum(axis=1) 20 distances = sqDistances**0.5 21 sortedDistIndices =distances.argsort() 22 classCount = {} 23 for i in range(k): 24 voteLabel = labes[sortedDistIndices[i]] 25 classCount[voteLabel] = classCount.get(voteLabel,0)+1 26 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) 27 return sortedClassCount[0][0] 28 def datingClassTest(): 29 #filename=\"test.txt\" 30 filename = \"datingTestSet.txt\" 31 datingDataMat,datingLabels = file2matrix(filename) 32 #取所有数据的10% 33 hoRatio = 0.1 34 #数据归一化，返回归一化后的矩阵，数据范围，数据最小值 35 normMat,ranges,minVals = autoNorm(datingDataMat) 36 #获得nornMat的行数 37 m = normMat.shape[0] 38 #百分之十的测试数据的个数 39 numTestVecs = int(m*hoRatio) 40 #分类错误计数 41 errorCount = 0.0 42 43 for i in range(numTestVecs): 44 #前numTestVecs个数据作为测试集，后m-numTestVecs个数据作为训练集 45 classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], 46 datingLabels[numTestVecs:m],10) 47 print (\"分类结果：%d \\t真实类别：%d\"%(classifierResult,datingLabels[i])) 48 if classifierResult != datingLabels[i]: 49 errorCount += 1.0 50 print (\"错误率：%f\"%(errorCount/float(numTestVecs)*100)) 51 52 if __name__ == '__main__': 53 datingClassTest()\nView Code\n都是上面的步骤，这里就不解释了，结果如下所示：\n2.3 手写数字识别\n数据可以样例可以打开文本文件进行查看，其中txt文件名的第一个数字为本txt中的数字，目录trainingDigits中包含了大约2000个例子，每个数字大约有200个样本，testDigits中包含900个测试数据，我们使用trainingDigits中的数据训练分类器，testDigits中的数据作为测试，两组数据没有重合。\n数据在这里：https://github.com/Jenny0611/Ml_Learning01\n首先我们要将图像数据处理为一个向量，将32*32的二进制图像信息转化为1*1024的向量，再使用前面的分类器，代码如下：\n1 import numpy as np 2 import operator 3 from os import listdir 4 from sklearn.neighbors import KNeighborsClassifier as kNN 5 6 ''' 7 函数说明：将32*32的二进制图片转换为1*1024向量 8 Parameters: 9 filename - 文件名 10 Returns: 11 returnVect - 返回的二进制图像的1*1024向量 12 ''' 13 def img2vector(filename): 14 #创建1*1024的0向量 15 returnVect = np.zeros((1,1024)) 16 fr = open(filename) 17 #按行读取 18 for i in range(32): 19 #读一行数据 20 lineStr=fr.readline() 21 #每一行的前32个数据依次添加到returnVect 22 for j in range(32): 23 returnVect[0,32*i+j]=int(lineStr[j]) 24 return returnVect 25 26 ''' 27 函数说明：手写数字分类测试 28 Parameters: 29 filename - 无 30 Returns: 31 returnVect - 无 32 ''' 33 def handwritingClassTest(): 34 #测试集的labels 35 hwLabels=[] 36 #返回trainingDigits目录下的文件名 37 trainingFileList=listdir('trainingDigits') 38 #返回文件夹下文件的个数 39 m=len(trainingFileList) 40 #初始化训练的Mat矩阵的测试集 41 trainingMat=np.zeros((m,1024)) 42 #从文件名中解析出训练集的类别 43 for i in range(m): 44 fileNameStr=trainingFileList[i] 45 classNumber = int(fileNameStr.split('_')[0]) 46 #将获取的类别添加到hwLabels中 47 hwLabels.append(classNumber) 48 #将每一个文件的1*1024数据存储到trainingMat矩阵中 49 trainingMat[i,:]=img2vector('trainingDigits/%s'%(fileNameStr)) 50 #构建KNN分类器 51 neigh = kNN(n_neighbors=3,algorithm='auto') 52 #拟合模型，trainingMat为测试矩阵,hwLabels为对应的标签 53 neigh.fit(trainingMat,hwLabels) 54 #返回testDigits目录下的文件列表 55 testFileList=listdir('testDigits') 56 errorCount=0.0 57 mTest=len(testFileList) 58 #从文件中解析出测试集的类别并进行分类测试 59 for i in range(mTest): 60 fileNameStr=testFileList[i] 61 classNumber=int(fileNameStr.split('_')[0]) 62 #获得测试集的1*1024向量用于训练 63 vectorUnderTest=img2vector('testDigits/%s'%(fileNameStr)) 64 #获得预测结果 65 classifierResult=neigh.predict(vectorUnderTest) 66 print (\"分类返回结果%d\\t真实结果%d\"%(classifierResult,classNumber)) 67 if (classNumber != classifierResult): 68 errorCount += 1.0 69 print (\"总共错了%d个\\t错误率为%f%%\"%(errorCount,errorCount/mTest*100)) 70 71 if __name__ == '__main__': 72 handwritingClassTest()\nView Code\n2.4 小结\nKNN是简单有效的分类数据算法，在使用时必须有训练样本数据，还要计算距离，如果数据量非常大会非常消耗空间和时间。它的另一个缺陷是无法给出任何数据的基础结构信息，因此我们无法平均实例样本和典型实例样本具体特征，而决策树将使用概率测量方法处理分类问题，以后章节会介绍。\n本文参考：http://blog.csdn.net/c406495762/article/details/75172850\n《机器学习实战》"}
{"content2":"首先用数据说话，看看资料大小，达到675G\n承诺：真实资料、不加密。（鉴于太多朋友加我QQ，我无法及时回复，）\n方便的朋友给我点赞、评论下，谢谢！（内容较大，多次保存）\n[hide]链接:[url]https://pan.baidu.com/s/1wOLIVuTDiXLlt7DxmRTlCw[/url] 提取码:44x3 [/hide]\n包含内容：1.python基础+进阶+应用项目实战\n2.神经网络算法+python应用\n3.人工智能算法+python应用\n4.机器学习算法+python应用\n\n在python全套教程中包括黑马程序员2018年12月python视频\n内容包括: linux知识、python基础编程、python高级编程、前端、数据库、数据结构、shell和运维等\n见图："}
{"content2":"软件为pycharm，安装了anaconda。\n我一开始的报错为，PyCharm中无法调用numpy，报错ModuleNotFoundError: No module named 'numpy'，这个问题找到博客解决了，其实就是需要使用anaconda的python.exe。\n但运行代码时：\nimport numpy as np\narr = np.random.randint(1, 9, size=9)\nprint(arr)\n又出现了如下错误：\nFile \"C:\\python\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\", line 26, in <module>\nraise ImportError(msg)\nImportError:\nImporting the multiarray numpy extension module failed. Most\nlikely you are trying to import a failed build of numpy.\nIf you're working with a numpy git repo, try `git clean -xdf` (removes all\nfiles not under version control). Otherwise reinstall numpy.\n还是报错，经过自己摸索，找到了解决方案，原来是自己的numpy版本过低了！解决方法如下：\n另外给大家推荐一些最新的书籍PDF资源：\n《python基础教程第三版》PDF高清完整版-免费下载\n《Python机器学习基础教程》PDF高清完整版-免费下载\n最新人工智能、机器学习、图像处理等书籍PDF下载列表：\nhttps://www.cnblogs.com/hsqdboke/category/1316409.html\n如何更新呢，最好的方法是使用anacon全部统一更新，不会出错，方法是如下：\nstep1：首先以管理员的身份启动cmd.exe；\nstep2：升级conda(升级Anaconda前需要先升级conda)命令为：conda update conda\nstep3：升级anconda命令为：conda update anaconda\nstep4：升级spyder命令为：conda update spyder\n然后重启pycharm，运行代码：\nimport numpy as np\narr = np.random.randint(1, 9, size=9)\nprint(arr)\n终于成功了！！！并且输入pycharm的时候也有函数参数等提示，顺便说一下windows环境下：ctrl+p会出来参数提示。\n运行结果：\n[1 2 2 4 7 1 8 7 4]\n另外给大家推荐一些最新的书籍PDF资源：\n《python基础教程第三版》PDF高清完整版-免费下载\n《Python机器学习基础教程》PDF高清完整版-免费下载\n最新人工智能、机器学习、图像处理等书籍PDF下载列表：\nhttps://www.cnblogs.com/hsqdboke/category/1316409.html\n参考：\nhttps://blog.csdn.net/john_bian/article/details/79291228\nhttps://blog.csdn.net/anderslu/article/details/79123323"}
{"content2":"人工智能深度学习神经网络在双色球彩票中的应用研究(二)\n深度学习这个能否用到数字彩（双色球，时时彩）这种预测上来呢？\n神经网络的看到有不少论文研究这个的，深度学习的还没有看到相关研究的文章\n预测也就是分类任务 深度学习应该是能做的 序列的数据可能得用LSTM\n深度学习和机器学习是不是差别很大呢？\n机器学习的范围太大了 深度学习主要是神经网络的拓展。\n当年，神经网络被F·Rosenblatt一篇著作《感知机》给直接打了下去，然后美国军方也大量撤资，神经网络的研究就此陷入低潮，直到这些年，计算机飞速发展，加上美国的物理学家Hopfield的两篇论文，才上神经网络重新热了起来，为了让公共不会认为这个又是神经网络而影响推广，所以改名为深度学习，两者其实一码事。\n-----------------------------------\n深度学习的dl4j在文本分析上用了lstm（分类任务）\nhttp://deeplearning4j.org/lstm.html\nGoogle开源的深度学习框架tensorflow也有个例子：ptb_word_lm\ncaffeonspark用在视觉图片识别上比较好，dl4j用在NLP上做类似问答搜索的比较多，tensorflow用在学习新的算法上，dl4j, caffeonspark, tensorflow都有LSTM自动分类的算法应用，理论上主流的开源深度学习框架都可以用在彩票预测上来。\nLSTM实现详解-CSDN.NET\nhttp://www.csdn.net/article/2015-09-14/2825693\n深入浅出LSTM神经网络 | 数盟社区\nhttp://dataunion.org/19397.html\n文档自动摘要小工具PKUSUMSUM，集成多种无监督摘要提取算法，支持多种摘要任务与多种语言，采用Java编写，代码完全开源，欢迎批评指正，也欢迎同行一起完善该工具，具体介绍和下载地址为：http://www.icst.pku.edu.cn/lcwm/wanxj/pkusumsum.htm\n----------------------------------\nTheano LSTM代码解析 - DeepLearningGroup的博客\nhttp://blog.csdn.net/DeepLearningGroup/article/details/51385136\n人人都能用Python写出LSTM-RNN的代码！[你的神经网络学习最佳起步]\nhttp://blog.csdn.net/zzukun/article/details/49968129\nLSTM实现详解\nhttp://www.csdn.net/article/2015-09-14/2825693\nPython中利用LSTM模型进行时间序列预测分析\nhttp://www.cnblogs.com/arkenstone/p/5794063.html\n时间序列预测分析就是利用过去一段时间内某事件时间的特征来预测未来一段时间内该事件的特征。这是一类相对比较复杂的预测建模问题，和回归分析模型的预测不同，时间序列模型是依赖于事件发生的先后顺序的，同样大小的值改变顺序后输入模型产生的结果是不同的。\n举个栗子：根据过去两年某股票的每天的股价数据推测之后一周的股价变化；根据过去2年某店铺每周想消费人数预测下周来店消费的人数等等\n要注意的是应为lstm是依赖序列关系的，所以你的下一次预测结果也会影响之后的预测，如果当中有偏差，这个偏差会累积到之后的预测结果，因此长时间的预测的准确性都是不高，但如果只是看趋势的话倒影响不大。\n当你理解怎样处理隐藏层的时候，实现任何RNN都会很容易。仅仅把一个常规MLP层放到顶部，然后连接多个层并且把它和最后一层的隐藏层相连，你就完成了。\n----------------------------------\n深度学习的知识图谱\n人工智能深度学习神经网络在双色球彩票中的应用研究(一) - 流风，飘然的风 - 博客园\nhttp://www.cnblogs.com/zdz8207/p/DeepLearning-NeuralNetworks.html"}
{"content2":"机器学习：\n自己的理解，机器学学习是一门多领域的交叉学科，专门研究计算机怎么模拟或者实现人类的学习方式和行为，以获取新的知识和技能，重新组织已有的知识结构和性能。\n1.读《大数据工程师飞林沙的年终总结&算法数据的思考》\n推荐系统:涉及到不懂的名词\n1.1这个是一篇博客《一个简单的基于内容的推荐算法》（理解的比较透彻）http://www.cnblogs.com/qiuleo/p/4225594.html\n讲的content-base（基于内容的推荐系统这个比较基础）方法比较简单易懂\n这个内容的推荐算法思路大概分成3部\n（1）.为每一个物品建立一个物品的属性资料；\n（2）为每一个用户构建一个用户的爱好资料；\n（3）计算用户喜好资料与物品属性资料的相似度，相似度高意味着用户可能喜欢这个物品，相似度低往往意味着用户不喜欢这个物品。（这个比较好理解）。\n也就是说选择一个想要推荐的用户“U”，针对用户U遍历一遍物品几何，计算出每个物品与用户U的相似度，选出相似度最高的k个物品，将他推荐给用户U。这样就可以了，但是个人觉得这种方法相率较低。\n里面提到了几个名词Item Profiles（通俗的将就是被推荐物品的详细属性）、representing Item Profiles（将这些只有人类能读懂的名词转化成计算机能读懂的数据结构）、User Profiles（用户的详细信息）\n拿电影的推荐来举例：对Item profiles建立模型构造一个1*n维矩阵，n表示全球主要影星的数量，每一个位置表示一个影星，0、1表示该电影中有无此明星。初始化这个矩阵，把矩阵的值都设为0；设此矩阵为I  [0,0,0,0.....0];\n对用户Users Profiles进行建模：\n用户\\电影\n《尖峰时刻》\n《红番区》\n《黑客帝国》\nAlice\n4\n5\n3\nBob\n1\n4\n举例给出两个用户对三个电影的评分\n之后是一些基本概念由于前两个电影有一个共同特点就是都有成龙主演，推测出alice可能喜欢成龙\n提取参数Avg = （4+5+4）/2=4\n接下来算出有这些参数就可以算出来alice对成龙的喜好程度s=（Σ（x - avg））/n  这里x表示所有涉及到成龙，且alice评价过的电影，n为示所有涉及到成龙，且alice评价过的电影的数量。且User Profiles也建立一个1*n的矩阵，但是矩阵中的值不在为0或1，而是对这个演员的喜好程度s。设此矩阵为U。\n利用余弦相似度公式计算给定的两个矩阵U和I的相似度。\n最后遍历整个影库，计算用户和每一个影片的相似度选出k个影片推荐给alice，这样就可以了。\n查询维基百科：余弦相似度公式\n1.2.Collaborative Filering（协同过滤）\n参考维基百科，与上述基于内容推荐系统不同，协同过滤分析用户的兴趣，在用户群中找到制定用户的相似（兴趣）用户，综合这些相似用户的某一信息的评价，形成系统对该指定用户的喜好程度的预测。这样做可以过滤掉难以进行机器自动基于内容分析的信息。（这句话本人不太好理解。。）自己理解就是一些音乐名字或者是艺术品的名字机器无法理解就这届过滤掉了。\n系统过滤的缺点（当站点结构、内容的复杂性和用户人数的不断增加，协同过滤额缺点就暴露出来了）（1）稀疏性：通俗的将就是数据比较少：每个用户的信息量涉及相当有限，举个例子，比如说亚马逊网站中，用户的评论只有1%~2%（终于知道为什么网站都鼓励评价和晒单了。。。）这样导致了评估矩阵数据相当稀疏，难以找到用户集，导致推荐效果大大降低。\n（2）扩展性：“最近邻居”算法的计算量随着用户和项的增加而大大增加，对于上百万之巨的数目，通常的算法将遭遇到严重的扩展性问题。\n（3）精确性：通过寻找相近用户来产生推荐集，在数据量较大的时候可信度会随着降低。\n回归刚刚的文章，在文章中作者提到了多种算法的混合，这样多个推荐算法的交际策略，这样可以最大化的满足用户的心理底线，从而吸引用户点击。这样就可以用少量的高质量item最大化满足了用户的心理底线，之后主要讨论了推荐系统的作用，捡钱的例子特别有深意。\n3.深度学习：\n是机器学习领域中试图使用多重重线性变换对数据进行多层抽象的算法。多种深度学习框架、深度神经网络。卷积神经网络和深度信念网络。\n深度神经网络（deep neuron networks, DNN）是一种判别模型，可以使用反向传播算法进行训练。权重更新可以使用下式进行随机梯度下降求解：\n其中，为学习率，为代价函数。这一函数的选择与学习的类型（例如监督学习、无监督学习、增强学习）以及激活函数相关。例如，为了在一个多分类问题上进行监督学习，通常的选择是使用Softmax函数作为激活函数，而使用交叉熵作为代价函数。Softmax函数定义为，其中代表类别的概率，而和分别代表对单元和的输入。交叉熵定义为，其中代表输出单元的目标概率，代表应用了激活函数后对单元的概率输出[32]。这个方法有点没看懂，还要好好理解一下。\n文章提到了在推荐系统中，深度学习的局限和不足、由于实际中存在大部分的缺失值，如果你希望用深度学习来对该矩阵做特征重组。\n大数据：指所涉及的数据规模巨大到无法通过人工，在合理时间内达到截取、管理、处理、并整理成人类所能解读的信息。在总数据量相同的情况下，与个别分析独立的小型数据集相比，将各个小型数据集合并进行分析可以的到许多额外的信息和数据关系性。\n稳重提到大数据的反思，其中说明很多公司对大数据去噪，其实对其中的异常点观察才是个性化的极致！！！\n任何系统都不能脱离产品而独立存在。不要无视数据也不要神话迷信数据。这篇文章之后写的一个数据工程师的发展和对一些公司和这个行业的看法，个人感觉很受用！！"}
{"content2":"最近在写机器学习的白话系列主题文章，突然有人问我，机器学习到底有什么用，如何才能用到实际生活中。我觉得很有必要停下脚步，来认真思考一下这个问题：机器学习，包括深度学习，自然语言处理，如何真正应用到实际生活中去。希望大家能够踊跃讨论。\n说到机器学习，最出名的无非就是Google的AlphaGo这样的项目，机器在人类传统的智力游戏中，无情而残酷的战胜了人类。当然，对于大部分正在读这篇文章的你来说，对于大部分正在学习机器学习的人来说，那只是一个终极目标。支撑着AlphaGo的服务器资源和海量数据不是任何机构可以获得的。\n在没有大量数据资源和服务器资源的情况下，机器学习到底可以怎么应用到生活中呢？\n随着Tensorflow的发布，其实我们已经可以在手机程序中使用人工智能了，下面的图就是Tensorflow通过ImageNet进行图像识别的例子。\n当然，图像识别只是一个基础功能，如何使得图像识别应用在实际生活中，则是一个需要研究的课题。同时，由于手机这样的设备，运算能力有限，精度不是很高的情况下，如何满足实际需要，也是一个课题。\n当然，如果你懂一些硬件的话，也可以和树莓派一起做一个智能硬件，做一个小车到处逛，顺便执行一些简单的任务。\nhttp://www.leiphone.com/news/201703/2MCSRGD5XpPNbK8c.html\n机器学习，一般来说，能做的事情，一种是分类任务，一种是回归任务。\n分类的话，在Tensorflow发布的时候，日本有个大叔做了一个智能黄瓜分类器，将原本农作物的分类分拣工作，交给了机器去完成。但是我看了一下，除了垃圾分类之外，暂时也没有什么需要我们去分类的东西。当然，如果能够做到机器学习的自动垃圾分类，也是一个好的主意，做一个智能垃圾回收站。\nhttp://www.infoq.com/cn/news/2016/09/tensorflow-cucumbers\n如果你的工作有需要分类的任务，而且对于结果精度要求不高，则可以尝试一下，例如茶叶的分拣。\n做HR的或许可以将简历信息和最后录取情况作为数据，训练出一个简历筛选的工具，帮助降低工作强度，当然也可能导致自己失业。\n说到回归任务，也就是预测。说到预测，呵呵，股票预测。\n股票预测到底靠不靠谱，我觉得，中国股市是一个政策导向的市场，这种市场的预测基本没戏。\n其他预测，我也暂时没有想到什么好的项目。原因就是，找到带有标签的大量数据源是一件成本非常高的事情。用爬虫去抓取数据，然后进行标签的整理，也是相当耗时间的事情。\n真的要用好机器学习，在这个框架泛滥的时代，可能领域专家比机器学习专家更加重要。Keras这样的框架，如果加上一个可视化的UI，普通人都可以做神经网络了。\n自然语言处理，也是机器学习的一个分支。大致有两种模型，一个是LDA主题模型。一个是情感分析模型。\n实际生活中的应用，一般也就是抓取各种点评文字，然后通过点评分值，归纳出分值和文字之间的特征。我遇到过一个项目，是金融业的。金融网站会有各种行业里面的针对公司的新闻（相当于数据），证券公司会有对于公司股票级别的评定（买入，持有，卖出，相当于标签），然后将历年的新闻和评级收集起来，做成一个新闻VS评级的模型（输入新闻，输出评级）。有了这个模型之后，通过爬虫去抓取实时新闻，计算出新闻中出现的公司可能出现的股票评级变化情况，推送通知相关客户。这个项目的效果还不知道，或许没有什么实际价值。\n现在流行创业，很多人，包括我在内，也希望抓住机器学习这个风口，做一个创业项目。我也深深知道，其实当今资本时代，技术的地位没有那么重要了。除非能够潜心研究一个高大上，无法被抄袭的东西出来。机器学习的核心价值，是大数据和模型。如果有机会拿到一些很有价值的，稀缺性的数据，然后训练，调整出一个高精度的模型，则就离成功很近了。高价值，没有被人利用的数据在哪？能够训练出什么独特的模型，解决实际的问题？找到问题的答案，这个是关键。\n关注公众号 TensorFlow教室 深度学习，机器学习，自然语言处理。"}
{"content2":"机器学习中，神经网络算法可以说是当下使用的最广泛的算法。神经网络的结构模仿自生物神经网络，生物神经网络中的每个神经元与其他神经元相连，当它“兴奋”时，想下一级相连的神经元发送化学物质，改变这些神经元的电位；如果某神经元的电位超过一个阈值，则被激活，否则不被激活。误差逆传播算法（error back propagation）是神经网络中最有代表性的算法，也是使用最多的算法之一。\n误差逆传播算法理论推导\n误差逆传播算法（error back propagation）简称BP网络算法。而一般在说BP网络算法时，默认指用BP算法训练的多层前馈神经网络。\n下面是一个简单的BP神经网络示意图。其拥有一个输入层，一个隐含层，一个输出层。推导中采用这种简单的三层的神经网络。\n定义相关的一些变量如下：\n假设有 d 个输入神经元，有 l 个输出神经元，q 个隐含层神经元；\n设输出层第 j 个神经元的阈值为 θj ；\n设隐含层第 h 个神经元的阈值为 γh ；\n输入层第 i 个神经元与隐含层第 h 个神经元之间的连接权为 Vih ；\n隐含层第 h 个神经元与输出层第 j 个神经元之间的连接权为 Whj ；\n记隐含层第 h 个神经元接收到来自于输入层的输入为 αh：\n记输出层第 j 个神经元接收到来自于隐含层的输入为 βj：\n，其中 bh 为隐含层第 h 个神经元的输出\n理论推导：\n在神经网络中，神经元接收到来自来自其他神经元的输入信号，这些信号乘以权重累加到神经元接收的总输入值上，随后与当前神经元的阈值进行比较，然后通过激活函数处理，产生神经元的输出。\n激活函数：\n理想的激活函数是阶跃函数，“0”对应神经元抑制，“1”对应神经元兴奋。然而阶跃函数的缺点是不连续，不可导，且不光滑，所以常用sigmoid函数作为激活函数代替阶跃函数。如下图分别是阶跃函数和sigmoid函数。\n阶跃函数：\nsigmoid函数：\n对于一个训练例（xk, yk），假设神经网络的输出为 Yk ，则输出可表示为：\nf(***)表示激活函数，默认全部的激活函数都为sigmoid函数。\n则可以计算网络上，（xk, yk）的均方差误差为：\n乘以1/2是为了求导时能正好抵消掉常数系数。\n现在，从隐含层的第h个神经元看，输入层总共有 d 个权重传递参数传给他，它又总共有 l 个权重传递参数传给输出层, 自身还有 1 个阈值。所以在我们这个神经网络中，一个隐含层神经元有（d+l+1）个参数待确定。输出层每个神经元还有一个阈值，所以总共有 l 个阈值。最后，总共有（d+l+1）*q+l 个待定参数。\n首先，随机给出这些待定的参数，后面通过BP算法的迭代，这些参数的值会逐渐收敛于合适的值，那时，神经网络也就训练完成了。\n任意权重参数的更新公式为：\n下面以隐含层到输出层的权重参数 whj 为例说明：\n我们可以按照前面给出的公式求出均方差误差 Ek ，期望其为0，或者为最小值。而BP算法基于梯度下降法（gradient descent）来求解最优解，以目标的负梯度方向对参数进行调整，通过多次迭代，新的权重参数会逐渐趋近于最优解。对于误差 Ek ，给定学习率（learning rate）即步长 η ，有：\n再看一下参数的传递方向，首先 whj 影响到了输出层神经元的输入值 βj ，然后影响到输出值 Yjk ,然后再影响到误差 Ek ，所以可以列出如下关系式：\n根据输出层神经元的输入值 βj 的定义：\n得到：\n对于激活函数（sigmoid函数）：\n很容易通过求导证得下面的性质：\n使用这个性质进行如下推导：\n令：\n又由于：\n所以：\n由前面的定义有：\n所以：\n把这个结果结合前面的几个式子代入：\n，  ，\n得到：\n所以：\nOK，上面这个式子就是梯度了。通过不停地更新即梯度下降法就可实现权重更新了。\n推导到这里就结束了，再来解释一下式子中各个元素的意义。\nη 为学习率，即梯度下降的补偿；为神经网络输出层第 j 个神经元的输出值；为给出的训练例（xk, yk）的标志（label），即训练集给出的正确输出；为隐含层第 h 个神经元的输出。\n类似可得：\n其中，\n这部分的解法与前面的推导方法类似，不做赘述。\n接下来是代码部分：\n这段代码网上也有不少地方可以看到，后面会简单介绍一下程序。\n完整程序：文件名“NN_Test.py”\n# _*_ coding: utf-8 _*_ import numpy as np def tanh(x): return np.tanh(x) def tanh_derivative(x): return 1 - np.tanh(x) * np.tanh(x) # sigmod函数 def logistic(x): return 1 / (1 + np.exp(-x)) # sigmod函数的导数 def logistic_derivative(x): return logistic(x) * (1 - logistic(x)) class NeuralNetwork: def __init__ (self, layers, activation = 'tanh'): if activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_derivative elif activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_derivative # 随机产生权重值 self.weights = [] for i in range(1, len(layers) - 1): # 不算输入层，循环 self.weights.append((2 * np.random.random( (layers[i-1] + 1, layers[i] + 1)) - 1) * 0.25 ) self.weights.append((2 * np.random.random( (layers[i] + 1, layers[i+1])) - 1) * 0.25 ) #print self.weights def fit(self, x, y, learning_rate=0.2, epochs=10000): x = np.atleast_2d(x) temp = np.ones([x.shape[0], x.shape[1]+1]) temp[:, 0:-1] = x x = temp y = np.array(y) for k in range(epochs): # 循环epochs次 i = np.random.randint(x.shape[0]) # 随机产生一个数，对应行号，即数据集编号 a = [x[i]] # 抽出这行的数据集 # 迭代将输出数据更新在a的最后一行 for l in range(len(self.weights)): a.append(self.activation(np.dot(a[l], self.weights[l]))) # 减去最后更新的数据，得到误差 error = y[i] - a[-1] deltas = [error * self.activation_deriv(a[-1])] # 求梯度 for l in range(len(a) - 2, 0, -1): deltas.append(deltas[-1].dot(self.weights[l].T) * self.activation_deriv(a[l]) ) #反向排序 deltas.reverse() # 梯度下降法更新权值 for i in range(len(self.weights)): layer = np.atleast_2d(a[i]) delta = np.atleast_2d(deltas[i]) self.weights[i] += learning_rate * layer.T.dot(delta) def predict(self, x): x = np.array(x) temp = np.ones(x.shape[0] + 1) temp[0:-1] = x a = temp for l in range(0, len(self.weights)): a = self.activation(np.dot(a, self.weights[l])) return a\n简要说明：\ndef tanh(x): return np.tanh(x) def tanh_derivative(x): return 1 - np.tanh(x) * np.tanh(x) # sigmod函数 def logistic(x): return 1 / (1 + np.exp(-x)) # sigmod函数的导数 def logistic_derivative(x): return logistic(x) * (1 - logistic(x))\n分别表示两种激活函数，tanh函数和sigmoid函数以及其的导数，有关激活函数前文有提及。\nif activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_derivative elif activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_derivative\n“activation”参数决定了激活函数的种类，是tanh函数还是sigmoid函数。\nself.weights = [] for i in range(1, len(layers) - 1): # 不算输入层，循环 self.weights.append((2 * np.random.random( (layers[i-1] + 1, layers[i] + 1)) - 1) * 0.25 ) self.weights.append((2 * np.random.random( (layers[i] + 1, layers[i+1])) - 1) * 0.25 ) #print self.weights\n以隐含层前后层计算产生权重参数，参数初始时随机，取值范围是[-0.25, 0.25]\nx = np.atleast_2d(x) temp = np.ones([x.shape[0], x.shape[1]+1]) temp[:, 0:-1] = x x = temp y = np.array(y)\n创建并初始化要使用的变量。\nfor k in range(epochs): # 循环epochs次 i = np.random.randint(x.shape[0]) # 随机产生一个数，对应行号，即数据集编号 a = [x[i]] # 抽出这行的数据集 # 迭代将输出数据更新在a的最后一行 for l in range(len(self.weights)): a.append(self.activation(np.dot(a[l], self.weights[l]))) # 减去最后更新的数据，得到误差 error = y[i] - a[-1] deltas = [error * self.activation_deriv(a[-1])] # 求梯度 for l in range(len(a) - 2, 0, -1): deltas.append(deltas[-1].dot(self.weights[l].T) * self.activation_deriv(a[l]) ) #反向排序 deltas.reverse() # 梯度下降法更新权值 for i in range(len(self.weights)): layer = np.atleast_2d(a[i]) delta = np.atleast_2d(deltas[i]) self.weights[i] += learning_rate * layer.T.dot(delta)\n进行BP神经网络的训练的核心部分，在代码中有相应注释。\ndef predict(self, x): x = np.array(x) temp = np.ones(x.shape[0] + 1) temp[0:-1] = x a = temp for l in range(0, len(self.weights)): a = self.activation(np.dot(a, self.weights[l])) return a\n这段是预测函数，其实就是将测试集的数据输入，然后正向走一遍训练好的网络最后再返回预测结果。\n测试验证函数：\n# _*_ coding: utf-8 _*_ from NN_Test import NeuralNetwork import numpy as np nn = NeuralNetwork([2, 2, 1], 'tanh') x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) y = np.array([0, 1, 1, 0]) nn.fit(x, y) for i in [[0, 0], [0, 1], [1, 0], [1, 1]]: print(i, nn.predict(i))\n程序中测试的是异或关系，下面是运行结果：\n([0, 0], array([-0.01628435])) ([0, 1], array([ 0.99808061])) ([1, 0], array([ 0.99808725])) ([1, 1], array([-0.03867579]))\n显然与标准异或关系近似。"}
{"content2":"#对coursera上Andrew Ng老师开的机器学习课程的笔记和心得；\n#注:此笔记是我自己认为本节课里比较重要、难理解或容易忘记的内容并做了些补充，并非是课堂详细笔记和要点；\n#标记为<补充>的是我自己加的内容而非课堂内容，参考文献列于文末。博主能力有限，若有错误，恳请指正；\n#---------------------------------------------------------------------------------#\n<补充>支持向量机方法的三要素(若不了解机器学习模型、策略、算法的具体意义，可参考机器学习三要素)\n基本模型:间隔最大的线性分类器；若用上核技巧，成为实质上的非线性分类器；\n学习策略:间隔最大化，可形式化为一个求解凸二次规划的问题；\n学习算法:求解凸二次规划的最优化算法，如序列最小最优算法(SMO);\n#---------------------------------------------------------------------------------#\n由logistic regression引出SVM\nlogistic function(sigmoid function):g(z) = 1/(1 + e-z)，z=ΘTx;\n预测函数:;\nlogistic函数的图形：\n;\n当ΘTx 远大于0时，hθ(x)接近于0；\nlogistic回归的cost function:\n；\n当y=1时，上式变为-log(1 + e-z),见图形\n;\nSVM的cost function对logistic回归的cost function做了改变，当y=1时，SVM的cost function记为cost1(θT x)，分为两部分(见下图紫线)，当z>1时cost1(ΘTx)=0,当z<1时cost1(ΘTx)是条直线。这样做有两个好处，一是计算更快(从计算logistic函数转变为计算直线函数)，二是更有利于后来的优化；\n；\n同理对y=0时做同样的处理，得到cost0(θT x)，下图紫线。\n；\n由此我们得到cost0(θT x)和cost1(ΘTx)：\n；\n由此我们从最小化logistic回归的cost function：\n，\n得到下式：\n；\n再令C=1/λ，去掉1/m(m是常数，不影响计算优化结果)，得到最终SVM的cost function：\n；\n#---------------------------------------------------------------------------------#\nLarge margin intuition\n再来看SVM的cost0(θT x)和cost1(ΘTx)：\n；\n注意:SVM wants a bit more than that - doesn't want to *just* get it right, but have the value be quite a bit bigger than zero\nThrows in an extra safety margin factor\n对于训练数据，SVM不仅要求是分的对，而且还有额外的间隔条件来保证分的“好”；\n；\nThe green and magenta lines are functional decision boundaries which could be chosen by logistic regression\nBut they probably don't generalize too well\nThe black line, by contrast is the the chosen by the SVM because of this safety net imposed by the optimization graphMathematically, that black line has a larger minimum distance (margin) from any of the training examples\nMore robust separator\nBy separating with the largest margin，you incorporate robustness into your decision making process\n<补充>什么是支持向量support vector？\n下图中两个支撑着中间的 gap 的超平面，它们到中间的纯红线separating hyper plane 的距离相等，即我们所能得到的最大的 geometrical margin，而“支撑”这两个超平面的必定会有一些点，而这些“支撑”的点便叫做支持向量Support Vector。\nC的选择对SVM的影响\nC选的合适时，\n；\nC太大时造成过拟合(紫线)，\n；\n<补充>最大间隔分离超平面存在唯一性：若训练数据线性可分(这是前提)，则可将训练数据的样本点完全正确分开的最大间隔分离超平面存在且唯一；\n#---------------------------------------------------------------------------------#\nKernels\n<补充>当训练数据线性可分或近似线性可分时，通过间隔最大化，学习一个线性分类器；当训练数据线性不可分时，使用核技巧(kernel trick)，学习非线性分类器；\n<补充>核函数(kernel function)表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维空间的特征空间中学习线性支持向量机；也就是说，在核函数K(x,z)给定的条件下，可以利用解线性分类问题的方法去求解非线性分类问题的支持向量机。学习是隐式的在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称作核技巧；\n几个常用核函数\nGaussian kernel(使用最多的):Need to define σ (σ2)；\n;\nlinear kernel:no kernel；\nothers：Polynomial Kernel,String kernel,Chi-squared kernel...\n#---------------------------------------------------------------------------------#\nLogistic regression vs. SVM\nIf n (features) is large vs. m (training set)\ne.g. text classification problem\nFeature vector dimension is 10 000\nTraining set is 10 - 1000\nThen use logistic regression or SVM with a linear kernel\nIf n is small and m is intermediate\nn = 1 - 1000\nm = 10 - 10 000\nGaussian kernel is good\nIf n is small and m is large\nn = 1 - 1000\nm = 50 000+\nSVM will be slow to run with Gaussian kernel\nIn that case\nManually create or add more features\nUse logistic regression of SVM with a linear kernel\nLogistic regression and SVM with a linear kernel are pretty similar\nDo similar things\nGet similar performance\nA lot of SVM's power is using diferent kernels to learn complex non-linear functions\nFor all these regimes a well designed NN should work\nBut, for some of these problems a NN might be slower - SVM well implemented would be faster\nSVM has a convex optimization problem - so you get a global minimum\n#---------------------------------------------------------------------------------#\n参考文献\n《统计学习方法》，李航著\n理解SVM的三层境界-支持向量机通俗导论，July、pluskid著\nstandford machine learning, by Andrew Ng"}
{"content2":"本来我以为不需要解释这个问题的，到底数据挖掘(data mining)，机器学习(machine learning)，和人工智能(AI)有什么区别，但是前几天因为有个学弟问我，我想了想发现我竟然也回答不出来，我在知乎和博客上查了查这个问题，发现还没有人写过比较详细和有说服力的对比和解释。那我根据以前读的书和论文，还有和与导师之间的交流，尝试着说一说这几者的区别吧，毕竟一个好的定义在未来的学习和交流中能够发挥很大的作用。同时补上数据科学和商业分析之间的关系。能力有限，如有疏漏，请包涵和指正。\n导论\n本文主要分为两部分，第一部分阐述数据挖掘(data mining)，机器学习(machine learning)，和人工智能(AI)之间的区别。这三者的区别主要是目的不同，其手段(算法，模型)有很大的重叠，所以容易混淆。第二部分主要阐述以上的技能与数据科学(data science)的关系，以及数据科学(data science)和商业分析(business analytics)之间的关系。其实，数据科学家本身就是商业分析师在大数据时代的延伸。\n数据挖掘VS. 机器学习VS. 人工智能\n数据挖掘 (data mining): 有目的地从现有大数据中提取数据的模式(pattern)和模型(model)\n关键字：模式提取，大数据\n数据挖掘是从现有的信息(existing information)中提取数据的模式(pattern)和模型(model)，即精选出最重要的信息，以用于未来机器学习和AI的数据使用。其核心目的是找到数据变量之间的关系。其发展出来的主要原因是大数据的发展，用传统的数据分析的方式已经无能处理那么多大量的看似不相关的数据的处理，因此需要数据挖掘技术去提取各种数据和变量之间的相互关系，从而精炼数据。\n数据挖掘本质上像是机器学习和人工智能的基础，他的主要目的是从各种各样的数据来源中，提取出超集(superset)的信息，然后将这些信息合并让你发现你从来没有想到过的模式和内在关系。这就意味着，数据挖掘不是一种用来证明假说的方法，而是用来构建各种各样的假说的方法。数据挖掘不能告诉你这些问题的答案，他只能告诉你，A和B可能存在相关关系，但是它无法告诉你A和B存在什么相关关系。\n当然，数据挖掘会使用大量机器学习的算法，但是其特定的环境和目的和机器学习不太一样。\n机器学习(machine learning): 自动地从过往的经验中学习新的知识。\n关键字: 自动化，自我优化，预测，需要training data，推荐系统\n机器学习其实是人工智能很重要的一部分，因为目前，在实践过程中，大多数的人工智能处理的任务，其实是用机器学习的方式完成的。机器学习可以用程序和算法自动地学习，只要被设计好了，这个程序可以进行自我优化。同时，机器学习需要一定数量的训练数据集(training data set)，用于构建来自过往经验的“知识” 。\n且机器学习目前在实践中最重要的功能便是预测结果。比如机器学习已经学习结束了，现在有一个新的数据集x，需要预测其分类，机器学习算法会根据这个新数据与学习后的“知识”相匹配(实际上，知识指的是学习后的数学模型)，然后将这个数据集x分类某类C去。再比较常见的机器学习，比如amazon的推荐系统。\n人工智能(AI): 一个广泛的概念，本质是用数据和模型去为现有的问题(existing problems)提供解决方法(solutions).\n关键字：和人一样处理问题，技术的合集\n人工智能是一个与机器学习和数据挖掘相对不同的概念，人工智能的目的是为了去创造有智力的电脑(不知道怎么翻译好，可以假设其为机器人)。在实践中，我们希望这个电脑可以像有智力的人一样处理一个任务。因此，理论上人工智能几乎包括了所有和机器能做的内容，当然也包括了数据挖掘和机器学习的内容，同时还会有监视(monitor)和控制进程(process control)的内容。\n数据科学(data science)和商业分析(business analytics)的关系？\n其实以前，我们是没有数据科学家(data scientist)，和数据科学(data science)这个概念的。我们称呼做相关内容的方式更多叫商业分析(business analytics)。\n在2011年的时候，麦肯锡发表了《Big Data: the next frontier for innovation, competition, and productivity》提出了现在很多的公司已经开始往分析才能(analytical talent)中获得竞争优势。虽然这不是第一篇提出这个概念的公司，但是是第一次提出，数据分析能力也有助于商业公司去发现潜在的机会，而不仅仅只对技术公司有效。接着麦肯锡认为到了2018年，美国大约会有190,000的项目缺少“深度分析能力(Deep Analytical Talent)”，而这些深度分析能力，是由大数据(big data)驱动的。至此，麦肯锡将”商业分析”进一步形容为”深度分析能力”。\n接着DJ Patil和Jeff Hammerbacher在其写的《Building Data Science Teams》,将麦肯锡的“深度分析能力”称为了“数据科学家(data scientists)”。他们在文中提到：\n商业分析师(business analyst)看起来太局限了，数据分析师(data anlyst)是他们的竞争者，但是我们还是觉得这个称呼太局限了。....我们认为最好的称呼应该是”数据科学家(data scientist)”，因为这些人需要同时使用数据(data)和科学(science)去创造一些新的东西。\n紧接着，DJ Patil加了一些关键特点用于去寻找一个数据科学家(data scientist):\n专业技术(Technical expertise): 最好的数据科学家需要有关于某些科学学科的深度专业知识(deep expertise)。\n好奇心(Curiosity): 一个优秀的数据科学家需要有挖掘潜在关系，解决问题和证明假说的强烈好奇心和渴望。\n讲故事的能力(Storytelling): 能用数据讲一个生动的故事的能力，它能使交流更加有效。\n聪明(Cleverness): 能够创造性地解决问题的能力。\n随后，数据科学家这个概念才开始被广为流传。那么数据科学家需要具备哪些专业能力？不同的公司有不同的看法和意见(反正大家好像都喜欢把所有一切的期许都放在一个新兴的行业中)，这里列举一个比较流行的看法：\n1.Drew Conway’s Data Scientist Venn Diagram\n2.Drew Tierney’s Multi-disciplinary Diagram\n3.Gartner\n最后附赠一张“作弊纸”，列出几乎所有的商业问题(Business Problems)，想要入门成为一个优秀的商业分析师，或者是数据科学家，强烈推荐保存！！！！！！！！！！！！以后有时间，我会尝试着逐一翻译和解说一下。\n拓展阅读（英文）：\n什么是独角兽型的数据科学家？：不知道为什么现在什么“独角兽”型的这种理念会那么流行，企业也爱叫独角兽，行业内也爱叫独角兽。。但为什么一提到独角兽，我先想到的是巫师系列游戏。（捂脸~）\nTop 10 Data Analysis Tools for Business：用于商业分析的十大工具，强烈推荐阅读！！！\nData Science: Bridging the Business & IT Gap：第二部分内容主要来源的原文。\n参考文献：\nhttp://stats.stackexchange.com/questions/5026/what-is-the-difference-between-data-mining-statistics-machine-learning-and-ai\nhttp://upfrontanalytics.com/data-mining-vs-artificial-intelligence-vs-machine-learning/\nhttps://www.researchgate.net/post/What_is_the_difference_between_machine_learning_and_data_mining\nhttps://www.r-bloggers.com/whats-the-difference-between-machine-learning-statistics-and-data-mining/\nhttps://discuss.analyticsvidhya.com/t/what-is-the-difference-between-machine-learning-data-analysis-data-mining-data-science-and-ai/572\nhttp://www.kdnuggets.com/2014/06/data-science-skills-business-problems.html\n各种乱七八糟的书和课件的笔记。\n《Building Data Science Teams》\n《Big Data: the next frontier for innovation, competition, and productivity》\nDrew Conway’s Data Scientist Venn Diagram\nDrew Tierney’s Multi-disciplinary Diagram"}
{"content2":"原文链接\n说到人工智能，不得不提到一个关键词就是机器学习，机器学习领域的突破和爆发，使人工智能领域有了飞跃的发展。人工智能的时候会特别关注机器学习领域将会以什么层级的速度向未来发展？在 2017 腾讯“云+未来”峰会上，机器学习大神 Michael Jordan 教授从人工智能发展史出发全面阐述机器学习现状及未来的挑战。\n60年代，“智能”这个词刚刚出现，机器人进入到人的世界，被定义为像一个人存在。到80、90年代，“智能”走向另一种趋势，演变为“IA”，即智能增强技术，智能搜索引擎出现帮助我们快速获取解决问题的答案，帮助人类有了更好的存储、沟通、交流能力。\n与此同时，在 IaaS 即智能基础设施方面，交通、金融……我们身边的每一个行业、每一个模块都出现了智能化的的趋势。\n人工智能还只是一个雏形\n在 Michael Jordan 教授的观点中，目前人工智能还只是一个雏形，之后可能会出现一些有效的对话，特别是像这样一个自我导识的机器也会出现，但是智能方面它目前还是比较有限的。\n目前人工智能的可能性及其局限性表现在：\n（1）机器视觉能帮助人类对物体进行标识，但无法像人类般清晰的了解所有的场景及其关注点在哪里。\n（2）语音识别也是如此，现在可以把语音转化成文字，文字也可以转换成语音，在各种语言上都可以实现，但是机器人还没办法帮人类了解听觉、视觉之后的真正的意义。\n（3）自然语言处理方面，人工智能帮助人类解决了语言翻译的问题，但大量的翻译中，大部分的的语言和语没有得到有效的语义的阐述，从而无法表达语句的真正意义。\nMichael Jordan 教授相信人工智能短期内不会出现太多的像人这样的灵活性和可变化性。目前，机器人无法拥有和理解人类的高级智能（抽象思维），无法进行抽象思维的处理。但相信在未来，随着人工智能技术发展能解决这个问题。\n他还提到，机器人及人工智能能帮助人类处理大量的数据，并通过数据分析出未来走向，保证数据结果不断提高。但同时，人工智能系统无法辨别真伪，从而无法引领一个公司或企业做出前景化的决定。\n人工智能真的很智能吗？\nMichael Jordan 教授说出了他的担忧：人工智能的误用，将会带来很大的损失或伤害。如在医疗行业中，机器不太可能做很多的医学诊断；机器人的出现让许多人失业；有些人恶意使用人工智能系统等等。“机器人本身是没有任何恶意要伤害人类的，只是使用这些及其人的人本身含有恶意。”Michael Jordan 教授如是说。\n机器学习面临的挑战\n（1）必须要设计一个系统，可以以带来有意义的经过校准以后的信息，能够应对一些不确定性，还有在策略规划的角度，可以了解一种做法和另外一种做法之间的差别。同时，我们还要保证系统能能够真正地解释它们自己所做出的决策。如果机器做出了一个决定，我们必须实现让机器能阐释为什么做这样的决定，是否还有其它的潜在方法，以及可以找到问题发生的原因。\n（2）需要找到一个可以实现长期目标的追溯，同时可以主动的收集在实现目标相关的数据的系统。\n（3）还有一点是实时。但是到目前为止，我们的机器学习方面还没有办法能够达到真正的实时操作，机器需要花几天、几个小时来学习数据。\n（4）还有在意外情况下，在外部事件上的连接，包括数据和其他的要求，需要和政府的合作，和法律部门、和社会科学家的合作。\n原文链接"}
{"content2":"流水账形式记录一下自己在浙大读在职研究生的经历，给自己留一个纪念。\n2015 年 10 月，赶上全国硕士专业学位研究生入学资格考试(Graduate Candidate Test, GCT)，最后一般末班车，我报考的是浙江大学计算机学院计算机科学技术的在职研究生，记得当时的四门考试科目分别是：语文、数学、英语、逻辑。\n2016 年年初，具体忘了几月份，录取分数线公布，浙大的线是 180 多吧，我考了 256，顺利地通过了初试。接下来就会复试了，考的是数据库，因为之前没学过，基础不是很好，只考了55分，但好在及格线线只要 50，就这样通过了复试。\n2016 年 3 月年开学典礼。\n2016 年 4 月，春学期开始上课，时间：周六周日，英语和自然辩证法，英语课只去上了一节，因为六级分数 480+，所以英语免修了，全程英文授课；佩服自然辩证法老师的讲课能力，一讲就是 3 个小时。\n2016 年 6 月，夏学期开始上课，时间：周六周日，因为 G20 的缘故，这学期只上了 1 门课，现代文明史，老师貌似挺有名的，但讲课比较含糊，本来授课次数就不多，最后一次还请假没上。\n2016 年 10 月，秋学期开始上课，时间：周六周日，课程：网络多媒体搜索引擎、数字媒体、计算机图形学&CAD。\n2016 年 12 月，冬学期开始上课，时间：周六周日，课程：软件中间件、高级计算机网络、数据仓库与商务智能（其实就是机器学习，多媒体信息检索的），CASE分析1：社会网络和社会计算，CASE分析2：多媒体信息检索。\n总结一下，目前浙大计算机科学技术专业，机器学习、数据挖掘、人工智能、深度学习、多媒体信息检索技术炽手可热的热门！\n2017-07-04 更新 2017 年 2 月，CASE分析3：大数据知识服务技术与实践漫谈，CASE分析4：大数据系统 2017 年 3 月，春节归来，迎来冬学期考试，软件中间件考了和 CORBA 有关的只是，计算机网络英文笔试，数据仓库是Weka实验报告，CASE分析报告我做了和图像检索有关的，SIFT算法和卷积神经网络算法在图像检索领域的应用分析。 2017 年 4 月，科学研究方法与写作 2017 年 5 月，迎来第二个夏学期，课程：软件工程技术与设计（讲的是项目管理）、操作系统与嵌入式设计（其实就是讲 Linux）、人工智能（学习如何吹牛，老师说的_(:зゝ∠)_）、软件体系结构（软件架构？）。 2017 年 7 月，准备复习考试。"}
{"content2":"当今机器学习是一个非常热门的话题，每个人都在谈论机器学习，并讨论它如何在他们的业务或职业生涯中发挥作用。 机器学习是一种数据分析方法，可以使分析模型建立自动化。它是人工智能的一个分支，其基础是机器应该能够通过经验学习和适应。\n机器学习的类型\n监督学习\n有预定义的数据集来训练你的程序\n根据  训练数据，程序可以在给出新数据时做出准确的判断\n所以这就像跟老师一起学习\n这就像分类和回归，比如收到一束带有标签的花，你的程序可以在标签的基础上辨别花朵\n无监督学习\n当有没有老师来训练，需要自我学习\n当你的程序足够聪明时，可以自动查找数据集中没有标签的模式和关系。\n在这次学习中，你没有使用任何关于人的过去/之前的知识，并将它们分类为“随时随地”\n这就像聚类和关联，例如，您收到没有标签的花，因此程序需要使用算法来识别花朵\n强化学习\n这就像打击和试验类的学习\n该计划从他们自己的经验中学习。一个软件程序，可以最佳地执行定义的任务，并通过经验反复试验和学习。\n创建良好的机器学习系统需要什么？\n数据准备\n挑选算法 - 基本和高级\n自动化和迭代过程\n可扩展性\n集成建模\n简单而频繁的部署\n机器学习项目生命周期\n它基本上包含3个团队一起工作：\n第一数据科学家获取并转换数据建立一个深刻的理解，使他们能够建立一个模型：\n一旦选择了该模型，运营工程师就可以在生产环境中部署和设置监控和管理：\n对这种已部署模型的编程访问由开发人员将代码嵌入代码中，将其转换为可从外部世界访问的API：\n这些API可以从外部世界访问。例如，Microsoft 认知服务有一个开放的Vision API。微软已将基于机器学习，全球最领先的人工智能技术通过简单、易用的服务和 API 开发出来。微软认知服务使自然的人机交互变为可能，为你的应用增加前所未有的用户体验。现在你就可以在你的应用中接入这些智能，把你的想法变成现实。微软认知服务包涵的智能 API 让你仅用几行代码就可以借助强大的算法开发应用程序 https://mva.microsoft.com/colleges/MicrosoftAI"}
{"content2":"因为数据科学是个广义的学科，所以这里将从任何业务里都可能会遇到的数据科学家类型开始，通过这个部分或许你能发现自己隐藏的数据科学家潜质：）正如任何科学学科一样，数据科学家也可能向相关学科学习借鉴，尽管数据科学已经有自己的部分，尤其是自动处理超大规模非结构化数据的方式和算法，甚至不需要人为干涉，就可以做实时处理或者预测。\n1. 数据科学家的各种类型\n想要开始并且了解一些以前的观点，不妨参考2014年发布的文章“ 9 types of data scientists”或者同年另一篇文章比较数据科学和“16 analytic disciplines”。更近一点的（2016八月） Ajit Jaokar 讨论了Analytics data scientist（Type A）和Builder data scientist（Type B）的不同：\nType A Data Scientists在工作中遇到数据相关时可以写出不错的代码，但是并不必须是专家，这类data scientist可能专业是实验设计、预测、建模、统计推断或者其他统计学研究的典型部分。但是一般而言，数据科学家的工作产出可不是学术统计学有时候建议的那样“p-values and confidence intervals”（正如有时候传统的药物领域统计学家会用到那样）。在Google，Type A Data Scientists通常指统计学家、定量分析师、决策支持技术分析师或者数据科学家，可能还有其他的一些。\nType B Data Scientists是building data的。B类和A类有些相同的统计学背景，但他们还是更好的coders，可能有专业的软件工程的训练。他们主要对在产品中使用数据感兴趣，他们建立与用户交互的模型，通常是提供推荐的（产品、可能认识的人、广告电影、搜索结果之类）。\n阅读全文：http://click.aliyun.com/m/9435/"}
{"content2":"瓶颈\n任何事物的发展都会遇到瓶颈。半导体业界的摩尔定律在很长的一段时间里面一直是有效的，但是在近几年也快走到尽头了。\n机器学习在AlphaGo战胜人类棋手之后，名声大噪，我也是在那次比赛之后开始研究机器学习的。机器学习这项技术是不是有一个天花板，这个天花板在哪里，我们现在的技术发展离开这个天花板到底有多远，我们是在地板上呢，还是快触碰到天花板了呢？\n在五年前，Intel公司的CEO就抛出了无法继续摩尔定律的危机说。摩尔定律由英特尔联合创始人戈登-摩尔（Gordon Moore）提出，意思是说：当价格不变时，集成电路上可容纳的晶体管数目，约每隔 18 个月便会增加一倍，性能也将提升一倍。换言之，每一美元所能买到的电脑性能，将每隔 18 个月翻两倍以上。这个定律虽然奏效了数十年，但是从2018年开始，这个定律就已经失效\n黑盒白盒之争\n在知乎上有这样一篇文章\nhttps://zhuanlan.zhihu.com/p/21362413?fc=1&group_id=821400638150828032#comment-145854724\n大概的意思是用一个神经网络来调控另一个神经网络！\n以前，虽然我们不知道AlphaGo是怎么想的，但是我们知道它是怎么学的，\n以后，我们不但不知道AlphaGo是怎么想的，我们还不知道它是怎么学的！！！\n人工智能到底是黑盒还是白盒？在评论里面关于这个话题，大家产生了很大的分歧。\n作为传统的程序员，我的观点如下：当然如果您有时间，可以看一下评论，非常精彩。\n1.ML 归根到底是程序，如果LOG足够多的话，如果你足够耐心的话，你肯定可以知道，结果是如何产生的。\n2.用神经网络去优化神经网络，其本质是一样的，就想加法变成乘法，但是还没有脱离实数的范围，到达一个更高的维度。\n3.ML的程序，包括无监督的程序，都是人写的，都是按照人的想法在执行的，所以，为什么人不知道机器是怎么想的？即使这个程序表现得再不可思议，但结果应该都在人的预料之中。AlphaGo为什么会做决定，背后是程序，程序的背后是写程序的人的想法。除非是真正的随机函数，不然，写程序的人肯定知道程序是如何运行和预想结果的。\n总结：不知道程序是怎么想的，只是因为你不愿意去阅读程序的日志和不愿意调试程序。如果有无穷的时间，你单步调试所有的代码，你肯定知道这个结果是怎么来的。\n如果整个机器学习慢慢进入黑盒的时代，则可以预测，瓶颈快到了。我们不知道机器到底是怎么学习的，我们就无法进行改进。就像我们不知道雨水的形成机理，我们光在地上求雨是徒劳的。\n随机森林和Dropout\n很多算法中，都可以看到随机的影子，RF的话，也就是多次随机抽取样本，训练模型，这些模型再进行平均操作。当然，这是根据中心极限理论得出的好方法。神经网络的Dropout也是如此，随机的将一些神经节点进行屏蔽。但是随机就意味着失控，意味着人工很难干预结果。包括梯度下降，是否能收敛到全局最优解，很大程度上也是有运气成分在里面的。初始值，学习率都是影响结果的因素。\n调参数和巨大模型\n现在很多机器学习的比赛，已经从技术比拼转向资源比拼了。\n神经网络的层数越来越长，越来越深，微软的神经网络是152层。\n阿里巴巴的机器学习模型，已经是3GB的庞然大物了。\n整个业界都从硬件和物理层面去获得精度的收益了。\n同时，超参数的选取，现在也都是经验论:\n神经网络的层数\n我们首先需要确定网络的层数和每层的节点数。关于第一个问题，实际上并没有什么理论化的方法，大家都是根据经验来拍，如果没有经验的话就随便拍一个。然后，你可以多试几个值，训练不同层数的神经网络，看看哪个效果最好就用哪个。嗯，现在你可能明白为什么说深度学习是个手艺活了，有些手艺很让人无语，而有些手艺还是很有技术含量的。\nK聚类的K取多少，自然语言处理的主题模型，主题数选择多少比较合适等等。都还没有，或者难以找到理论依据。\n机器学习还是数理统计\n机器学习的本质就是数理统计？答案可能没这么简单\nhttp://tech.sina.com.cn/roll/2017-03-27/doc-ifycspxp0038858.shtml\n如果从传统意义上的数据分析师的观点来说，这个问题的答案很简单，无非是下面这两点：\n机器学习本质上是一种算法，这种算法由数据分析习得，而且不依赖于规则导向的程序设计；\n统计建模则是以数据为基础，利用数学方程式来探究变量变化规律的一套规范化流程。\n有一种观点就是机器学习只是数理统计的一个华丽包装而已。\n在自然语言处理里面，原本是语言学家占主导的，然后慢慢的统计学家开始占上风，特别是在翻译领域，基本上都是靠强大的计算能力和巨大的模型在处理问题，也就是说从规则到统计的转变。\n如果说，机器学习的本质还是统计学的话，统计学，概率学这些东西，其实已经发展到尽头，很难再有什么革命性的突破了。是不是也意味着机器学习也走到尽头了呢？\n脑科学研究\n机器学习在很大程度上是对于大脑工作原理的仿生学。我觉得，机器学习的发展肯定和人类对于大脑研究的发展密不可分，神经网络就是一个例子。也有可能在多年之后，我们会发现大脑的工作原理和我们现在的认知完全不同，这样的话，当前的机器学习很有可能会被完全推翻，走向一条新的道路。"}
{"content2":"前言\n前面两篇主要写了一些机器学习的基础概念，从本篇开始我们来了解下深度学习。深度学习是机器学习的一个子集，是一种特殊的数学模型。同样是从输入到输出，深度学习在这两者之间会有很多层称为“隐层”（Hidden Layers）的层，每一层将会将输入内容进行计算并自我调节，最终得到合理模型，这种数学模型非常像人类目前对大脑工作原理的认知，所以也称之为“人工神经网络”。\n现在市面上有数十种深度学习框架：Wiki传送门。一个个学过来肯定精力不够，那我们就学综合评分最高的那个：Tensorflow。\nTensorflow是由Google团队在2015年11月9日以Apache 2.0开源许可证开源的，但刚发布的版本有诸多缺陷：bug比较多、性能比较差、使用难度比较高。在2017年2月16日，Google宣布Tensorflow 1.0正式发布，该版本适用于工业生产场景。所以在该版本发布之后，对深度学习领域产生了极大的影响，其大大简化了编写深度学习的代码难度，并且在Google这面大旗下，被各种专业人员以及机器学习爱好者所推崇（截止2018年1月9日，Tensorflow在Github上已经有7193个Watch，85038个Star和41555个Fork）。\nTensorflow在短短发布后的一年不到的时间里，已经跃居为综合排名第一的深度学习框架。为什么会如此火爆呢？我个人猜测理由如下：\n深度学习目前是大趋势（所以得抓紧学习，抓住机遇）\nGoogle的技术实力太强大，开源过很多项目都非常成功（比如Android），所以跟着Google走可能不会让我们失望\n接口丰富易用且支持多种语言（底层是C++实现的，这里指的是客户端调用底层API的语言），如Python、C++、Java、Go\n易于部署，包括分布式部署\n本机安装Tensorflow\n如果你的电脑性能还不错，可以考虑直接在本机安装Tensorflow（官方安装文档传送门），优先考虑使用Anaconda来安装，按照官方教程一步步执行即可。\n安装之后，需要在Pycharm中执行Tensorflow的代码，我们这里先用简单的输出常量的代码为例（此代码也是后文中demo1.py中的代码）：\nimport tensorflow as tf node1 = tf.constant(3.0, dtype=tf.float32) node2 = tf.constant(4.0) # also tf.float32 implicitly print(node1, node2) sess = tf.Session() print(sess.run([node1, node2]))\n常见错误汇总\n执行代码的时候，你可能会遇到很多错误，我这里整理了一份我在此过程中遇到的错误以及对应的解决方案供大家参考：\n错误提示\n解决方案\nAnaconda installation is not found\nhttps://stackoverflow.com/questions/47624777/pycharm-anaconda-installation-is-not-found\nImportError: Could not find 'cudart64_80.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable.\n由于目前Tensorflow是建立在CUDA 8.0之上的，而NVIDIA官网上默认是下载CUDA最新版本的（比如目前是9.0），所以需要打开如下页面： https://developer.nvidia.com/cuda-downloads 在页面的最下面找到Lagacy Releases并下载CUDA 8.0\nImportError: Could not find 'cudnn64_6.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 6 from this URL: https://developer.nvidia.com/cudnn\n注册一个NVEDIA的开发者账号，然后下载cudnn64 for cuda8 on windows。 下载下来的文件解压之后，放到cuda目录下对应的地方（cuda比如是在C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0）\n解决了所有问题之后，如果看到以下输出内容，就表示能正常运行Tensorflow了：\nTensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32) 2018-01-09 16:48:59.940050: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 [3.0, 4.0] Process finished with exit code 0\n使用阿里云PAI来实验Tensorflow\n概述\n因为机器学习的过程非常耗性能，如果在一个比较大的数据集上进行学习，本机CPU使用就会达到99%，而且要持续很久，这个时候就无法干点别的事了。很幸运，阿里云提供了机器学习平台PAI（Platform for Artificial Intelligence），里面集成了很多主流的算法以及Tensorflow的不同版本（目前是1.0、1.1、1.2），而且目前公测阶段也可以开启2个GPU来加快执行。\nPAI的优势有：\n不需要本地繁琐的安装和解决各种安装坑\n可以调用云服务器资源，比起本地性能更好\n不影响、阻塞本机其他程序的使用，开始执行之后等着执行完毕即可\n算法、组件都是现成的，而且可以用拖拽的方式编排算法，非常方便、傻瓜化，我们唯一要做的就是提供数据和编写核心执行脚本\n现成的机器学习案例以及相关文档，可以快速从别人的经验中来帮助自己理解机器学习\n将实验直接共享到社区\n公测阶段免费\n操作概览\n开通PAI的过程并不复杂，大致步骤如下：\n首先你要有一个阿里云账号\n在阿里云后台进入大数据（数加）->机器学习菜单\n创建一个项目，如： my_project_name\n在项目管理页面，勾选my_project_name的开启GPU选项\n为了方便上传本地资源到PAI，建议安装OSS-Browser工具，下载地址： https://help.aliyun.com/document_detail/61872.html?spm=5176.doc31886.2.5.qwodVb （以阿里云后台最新下载地址为准），如果是少量文件上传的话，使用web版OSS文件管理系统来管理文件也可以（本例中我们直接使用web版）\nPS:开启GPU选项示意图：\n目前公测阶段虽然PAI本身是免费的，但是OSS是收费的，好在费用很低，仅作学习用的话，一天最多几分钱。\n详细实验步骤\n在OSS中创建测试目录tensorflowtest：\n进入tensorflowtest目录并上传脚本文件demo1.py（内容详见“本机安装Tensorflow”一节）：\n进入PAI后台首页，创建空白实验tensorflowdemo：\n在空白实验中加入读OSS Bucket组件和TensorFlow(V1.2)组件，并选中Tensorflow节点，在右侧的参数设置中，将Python代码参数选为我们在步骤1、2中上传的demo1.py文件：\n点击屏幕最下方的运行按钮执行实验：\n本例中读OSS Bucket组件并没有什么用，由于PAI实验中必须要数据源节点，因此添加空的读OSS Bucket组件作为数据源，因此在执行实验的时候，阿里云将检测并提示当前实验室作业实验，将会在24小时内删除。\n查看执行结果\n在实验运行后，每个节点右侧出现一个绿色的勾，表示已正确执行完毕，此时右键该节点，选择查看日志选项：\n在“查看日志”弹出框中，滚动到中间的位置，找到 http://logview.odps.aliyun.com 的链接，如图位置：\n鼠标左键点击进入之后，点击下图中红色框中的detail图标：\n在弹出的Log Detail页面上，按下图中的1、2、3顺序依次点击：\n在弹出的Logview[Stdout]页面上，可以最终看到脚本的实际输出内容，与在本地IDE中的输出一致：\n结语\n我已经用两种不同的方式，执行了第一个基于Tensorflow的demo，这个demo目前跟深度学习还完全搭不上边，只是能成功运行Tensorflow而已，后续的博文中，我将进一步深入研究。\n本文在我的博客园和我的个人博客上同步发布，作者保留版权，转载请注明来源。"}
{"content2":"在上一篇文章：机器学习之PageRank算法应用与C#实现(1)算法介绍 中，对PageRank算法的原理和过程进行了详细的介绍，并通过一个很简单的例子对过程进行了讲解。从上一篇文章可以很快的了解PageRank的基础知识。相比其他一些文献的介绍，上一篇文章的介绍非常简洁明了。说明：本文的主要内容都是来自“赵国，宋建成.Google搜索引擎的数学模型及其应用,西南民族大学学报自然科学版.2010,vol(36),3”这篇学术论文。鉴于文献中本身提供了一个非常简单容易理解和入门的案例，所以本文就使用文章的案例和思路来说明PageRank的应用，文章中的文字也大部分是复制该篇论文，个人研究是对文章的理解，以及最后一篇的使用C#实现该算法的过程，可以让读者更好的理解如何用程序来解决问题。所以特意对作者表示感谢。如果有认为侵权，请及时联系我，将及时删除处理。\n论文中的案例其实是来源于1993年全国大学生数学建模竞赛的B题—足球队排名问题。\n本文原文链接：【原创】机器学习之PageRank算法应用与C#实现(2)球队排名应用与C#代码\n1.足球队排名问题\n1993年的全国大学生数学建模竞赛B题就出了这道题目，不过当时PageRank算法还没有问世，所以现在用PageRank来求解也只能算马后炮，不过可以借鉴一下思路，顺便可以加深对算法的理解，并可以观察算法实际的效果怎么样。顺便说一下，全国大学生数学建模竞赛的确非常有用，我在大学期间，连续参加过2004和2005年的比赛，虽然只拿了一个省二等奖，但是这个过程对我的影响非常大。包括我现在的编程，解决问题的思路都是从建模培训开始的。希望在校大学生珍惜这些机会，如果能入选校队，参加集训，努力学习，对以后的学习，工作都非常有帮助。下面看看这个题目的具体问题：\n具体数据由于篇幅较大，已经上传为图片，需要看的，点击链接：数据链接\n2.利用PageRank算法的思路\n2.1 问题分析\n足球队排名次问题要求我们建立一个客观的评估方法，只依据过去一段时间(几个赛季或几年)内每个球队的战绩给出各个球队的名次，具有很强的实际背景．通过分析题中12支足球队在联赛中的成绩，不难发现表中的数据残缺不全，队与队之间的比赛场数相差很大，直接根据比赛成绩来排名次比较困难。\n下面我们利用PageRank算法的随机冲浪模型来求解．类比PageRank算法，我们可以综合考虑各队的比赛成绩为每支球队计算相应的等级分(Rank)，然后根据各队的等级分高低来确定名次，直观上看，给定球队的等级分应该由它所战胜和战平的球队的数量以及被战胜或战平的球队的实力共同决定．具体来说，确定球队Z的等级分的依据应为：一是看它战胜和战平了多少支球队；二要看它所战胜或战平球队的等级分的高低．这两条就是我们确定排名的基本原理．在实际中，若出现等级分相同的情况，可以进一步根据净胜球的多少来确定排名．由于表中包含的数据量庞大，我们先在不计平局，只考虑获胜局的情形下计算出各队的等级分，以说明算法原理。然后我们综合考虑获胜局和平局，加权后得到各队的等级分，并据此进行排名。考虑到竞技比赛的结果的不确定性，我们最后建立了等级分的随机冲浪模型，分析表明等级分排名结果具有良好的参数稳定性。\n2.2 获取转移概率矩阵\n首先利用有向赋权图的权重矩阵来表达出各队之间的胜负关系．用图的顶点表示相应球队，用连接两个顶点的有向边表示两队的比赛结果。同时给边赋权重，表明占胜的次数。所以，可以得到数据表中给出的12支球队所对应的权重矩阵，这是计算转义概率矩阵的必要步骤，这里直接对论文中的截图进行引用：\n2.3 关于加权等级分\n上述权重不够科学，在论文中，作者提出了加权等级分，就是考虑平局的影响，对2个矩阵进行加权得到权重矩阵，从而得到转移概率矩阵。这里由于篇幅比较大，但是思路比较简单，不再详细说明，如果需要详细了解，可以看论文。本文还是集中在C#的实现过程。\n2.4 随机冲浪模型\n3.C#编程实现过程\n下面我们将使用C#实现论文中的上述过程，注意，2.3和2.2的思想是类似的，只不过是多了一个加权的过程，对程序来说还是很简单的。下面还是按照步骤一个一个来，很多人看到问题写程序很难下手，其实习惯就好了，按照算法的步骤来，一个一个实现，总之要先动手，不要老是想，想来想去没有结果，浪费时间。只有实际行动起来，才能知道实际的问题，一个一个解决，持之以恒，思路会越来越清晰。\n3.1 计算权重矩阵\n权重矩阵要根据测试数据，球队和每2个球队直接的比分来获取，所以我们使用一个字典来存储原始数据，将每个节点，2个队伍的比赛结果比分都写成数组的形式，来根据胜平负的场次计算积分，得到边的权重，看代码吧：\n1 /// <summary>根据比赛成绩，直接根据积分来构造权重矩阵，根据i,对j比赛获取的分数</summary> 2 /// <param name=\"data\">key为2个对的边名称，value是比分列表，分别为主客进球数</param> 3 /// <param name=\"teamInfo\">球队的编号列表</param> 4 /// <returns>权重矩阵</returns> 5 public static double[,] CalcLevelTotalScore(Dictionary<String, Int32[][]> data, List<Int32> teamInfo) 6 { 7 Int32 N = teamInfo.Count; 8 double[,] result = new double[N, N]; 9 10 #region 利用对称性，只计算一半 11 for (int i = 1; i < N; i++) 12 { 13 for (int j = i + 1; j <= N; j++) 14 { 15 #region 循环计算 16 String key = String.Format(\"{0}-{1}\", teamInfo[i - 1], teamInfo[j - 1]); 17 //不存在比赛成绩 18 if (!data.ContainsKey(key)) 19 { 20 result[i - 1, j - 1] = result[j - 1, i - 1] = 0; 21 continue; 22 } 23 //计算i,j直接的互胜场次 24 var scores = data[key];//i,j直接的比分列表 25 var Si3 = scores.Where(n => n[0] > n[1]).ToList();//i胜场次 26 var S1 = scores.Where(n => n[0] == n[1]).ToList();//i平场次 27 var Si0 = scores.Where(n => n[0] < n[1]).ToList();//i负场次 28 result[i - 1, j - 1] = Si3.Count*3 + S1.Count ; 29 result[j - 1, i - 1] = Si0.Count *3 + S1.Count ; 30 #endregion 31 } 32 } 33 #endregion 34 //按照列向量进行归一化 35 return GetNormalizedByColumn(result); 36 }\n上面最后返回调用了归一化的函数，比较简单，直接代码贴出来，折叠一下：\n1 /// <summary>按照列向量进行归一化</summary> 2 /// <param name=\"data\"></param> 3 /// <returns></returns> 4 public static double[,] GetNormalizedByColumn(double[,] data) 5 { 6 int N = data.GetLength(0); 7 double[,] result = new double[N, N]; 8 #region 各个列向量归一化 9 for (int i = 0; i < N; i++) //列 10 { 11 double sum = 0; 12 //行 13 for (int j = 0; j < N; j++) sum += data[j, i]; 14 for (int j = 0; j < N; j++) 15 { 16 if (sum != 0) result[j, i] = data[j, i] / (double)sum;//归一化,每列除以和值 17 else result[j, i] = data[j, i]; 18 } 19 } 20 #endregion 21 22 return result; 23 }\nView Code\n3.2 计算最大特征值及特征向量\n计算特征值和特征向量是一个数学问题，我们采用了Math.NET数学计算组件，可以直接计算很方便。详细的使用可以参考下面代码，组件的其他信息可以参考本站导航栏上的专题目录，有大量的使用文章。看代码吧。\n1 /// <summary>求最大特征值下的特征向量</summary> 2 /// <param name=\"data\"></param> 3 /// <returns></returns> 4 public static double[] GetEigenVectors(double[,] data) 5 { 6 var formatProvider = (CultureInfo)CultureInfo.InvariantCulture.Clone(); 7 formatProvider.TextInfo.ListSeparator = \" \"; 8 9 int N = data.GetLength(0); 10 Matrix<double> A = DenseMatrix.OfArray(data); 11 var evd = A.Evd(); 12 var vector = evd.EigenVectors;//特征向量 13 var ev = evd.EigenValues;//特征值，复数形式发 14 15 if (ev[0].Imaginary > 0) throw new Exception(\"第一个特征值为复数\"); 16 //取 vector 第一列为最大特征向量 17 var result = new double[N]; 18 for (int i = 0; i < N; i++) 19 { 20 result[i] =Math.Abs(vector[i, 0]);//第一列，取绝对值 21 } 22 return result; 23 }\n3.3 随机冲浪模型的实现\n随机冲浪模型主要是有一个比例，设置之后可以直接求解，也比较简单，函数如下：\n1 /// <summary>获取随机冲浪模型的 转移矩阵: 2 /// 作用很明显，结果有明显的改善 3 /// </summary> 4 /// <returns></returns> 5 public static double[,] GetRandomModeVector(double[,] data ,double d = 0.35) 6 { 7 int N = data.GetLength(0); 8 double k = (1.0 - d) / (double)N; 9 double[,] result = new double[N, N]; 10 for (int i = 0; i < N; i++) 11 { 12 for (int j = 0; j < N; j++) result[i, j] = data[i, j] * d + k; 13 } 14 return result; 15 }\n3.4 其他\n其他问题就是数据组合的过程，这里太多，不详细讲解。主要是构建测试数据以及排序后结果的处理，很简单。贴一个球队排序的函数，根据特征向量：\n1 /// <summary>排序，输出球队编号</summary> 2 /// <param name=\"w\"></param> 3 /// <param name=\"teamInfo\"></param> 4 /// <returns></returns> 5 public static Int32[] TeamOrder(double[] w, List<Int32> teamInfo) 6 { 7 Dictionary<int, double> dic = new Dictionary<int, double>(); 8 for (int i = 1; i <= w.Length; i++) dic.Add(i , w[i-1]); 9 return dic.OrderByDescending(n => n.Value).Select(n => n.Key).ToArray(); 10 }\n4.算法测试\n我们使用问题1中的数据，进行测试，首先构建测试集合，代码如下，太长，折叠一下，主要是问题1的原始数据：\n1 /// <summary> 2 /// 获取测试的数据集，key=对1-对2,value = int[,] 为比分 3 /// </summary> 4 public static Dictionary<String, Int32[][]> GetTestData() 5 { 6 Dictionary<String, Int32[][]> data = new Dictionary<string, int[][]>(); 7 #region 依次添加数据 8 #region T1 9 data.Add(\"1-2\", new Int32[][]{ new Int32[] { 0, 1 }, new Int32[] { 1, 0 }, new Int32[] { 0, 0 } }); 10 data.Add(\"1-3\", new Int32[][] { new Int32[] { 2, 2 }, new Int32[] { 1, 0 }, new Int32[] { 0, 2 } }); 11 data.Add(\"1-4\", new Int32[][] { new Int32[] { 2, 0 }, new Int32[] { 3, 1 }, new Int32[] { 1, 0 } }); 12 data.Add(\"1-5\", new Int32[][] { new Int32[] { 3, 1 } }); 13 data.Add(\"1-6\", new Int32[][] { new Int32[] { 1, 0 } }); 14 data.Add(\"1-7\", new Int32[][] { new Int32[] { 0, 1 }, new Int32[] { 1, 3 } }); 15 data.Add(\"1-8\", new Int32[][] { new Int32[] { 0, 2 }, new Int32[] { 2, 1 } }); 16 data.Add(\"1-9\", new Int32[][]{ new Int32[] { 1, 0 }, new Int32[] { 4, 0 } }); 17 data.Add(\"1-10\", new Int32[][]{ new Int32[] { 1, 1 }, new Int32[] { 1, 1 } }); 18 #endregion 19 20 #region T2 21 data.Add(\"2-3\", new Int32[][] { new Int32[] { 2, 0 }, new Int32[] { 0, 1 }, new Int32[] { 1, 3 } }); 22 data.Add(\"2-4\", new Int32[][] { new Int32[] { 0, 0 }, new Int32[] { 2, 0 }, new Int32[] { 0, 0 } }); 23 data.Add(\"2-5\", new Int32[][] { new Int32[] { 1, 1 } }); 24 data.Add(\"2-6\", new Int32[][] { new Int32[] { 2, 1 } }); 25 data.Add(\"2-7\", new Int32[][] { new Int32[] { 1, 1 }, new Int32[] { 1, 1 } }); 26 data.Add(\"2-8\", new Int32[][] { new Int32[] { 0, 0 }, new Int32[] { 0, 0 } }); 27 data.Add(\"2-9\", new Int32[][] { new Int32[] { 2, 0 }, new Int32[] { 1, 1 } }); 28 data.Add(\"2-10\", new Int32[][] { new Int32[] { 0, 2 }, new Int32[] { 0, 0 } }); 29 #endregion 30 31 #region T3 32 data.Add(\"3-4\", new Int32[][] { new Int32[] { 4, 2 }, new Int32[] { 1, 1 }, new Int32[] { 0, 0 } }); 33 data.Add(\"3-5\", new Int32[][] { new Int32[] { 2, 1 } }); 34 data.Add(\"3-6\", new Int32[][] { new Int32[] { 3, 0 } }); 35 data.Add(\"3-7\", new Int32[][] { new Int32[] { 1, 0 }, new Int32[] { 1, 4 } }); 36 data.Add(\"3-8\", new Int32[][] { new Int32[] { 0, 1 }, new Int32[] { 3, 1 } }); 37 data.Add(\"3-9\", new Int32[][] { new Int32[] { 1, 0 }, new Int32[] { 2, 3 } }); 38 data.Add(\"3-10\", new Int32[][] { new Int32[] { 0, 1 }, new Int32[] { 2, 0 } }); 39 #endregion 40 41 #region T4 42 data.Add(\"4-5\", new Int32[][] { new Int32[] { 2, 3 } }); 43 data.Add(\"4-6\", new Int32[][] { new Int32[] { 0, 1 } }); 44 data.Add(\"4-7\", new Int32[][] { new Int32[] { 0, 5 }, new Int32[] { 2, 3 } }); 45 data.Add(\"4-8\", new Int32[][] { new Int32[] { 2, 1 }, new Int32[] { 1, 3 } }); 46 data.Add(\"4-9\", new Int32[][] { new Int32[] { 0, 1 }, new Int32[] { 0, 0 } }); 47 data.Add(\"4-10\", new Int32[][] { new Int32[] { 0, 1 }, new Int32[] { 1, 1 } }); 48 #endregion 49 50 #region T5 51 data.Add(\"5-6\", new Int32[][] { new Int32[] { 0, 1 } }); 52 data.Add(\"5-11\", new Int32[][] { new Int32[] { 1, 0 }, new Int32[] { 1, 2 } }); 53 data.Add(\"5-12\", new Int32[][] { new Int32[] { 0, 1 }, new Int32[] { 1, 1 } }); 54 #endregion 55 56 #region T7 57 data.Add(\"7-8\", new Int32[][] { new Int32[] { 1, 0 }, new Int32[] { 2, 0 }, new Int32[] { 0, 0 } }); 58 data.Add(\"7-9\", new Int32[][] { new Int32[] { 2, 1 }, new Int32[] { 3, 0 }, new Int32[] { 1, 0 } }); 59 data.Add(\"7-10\", new Int32[][] { new Int32[] { 3, 1 }, new Int32[] { 3, 0 }, new Int32[] { 2, 2 } }); 60 data.Add(\"7-11\", new Int32[][] { new Int32[] { 3, 1 } }); 61 data.Add(\"7-12\", new Int32[][] { new Int32[] { 2, 0 } }); 62 #endregion 63 64 #region T8 65 data.Add(\"8-9\", new Int32[][] { new Int32[] { 0, 1 }, new Int32[] { 1, 2 }, new Int32[] { 2, 0 } }); 66 data.Add(\"8-10\", new Int32[][] { new Int32[] { 1, 1 }, new Int32[] { 1, 0 }, new Int32[] { 0, 1 } }); 67 data.Add(\"8-11\", new Int32[][] { new Int32[] { 3, 1 } }); 68 data.Add(\"8-12\", new Int32[][] { new Int32[] { 0, 0 } }); 69 #endregion 70 71 #region T9 72 data.Add(\"9-10\", new Int32[][] { new Int32[] { 3, 0 }, new Int32[] { 1, 0 }, new Int32[] { 0, 0 } }); 73 data.Add(\"9-11\", new Int32[][] { new Int32[] { 1, 0 } }); 74 data.Add(\"9-12\", new Int32[][] { new Int32[] { 1, 0 } }); 75 #endregion 76 77 #region T10 78 data.Add(\"10-11\", new Int32[][] { new Int32[] { 1, 0 } }); 79 data.Add(\"10-12\", new Int32[][] { new Int32[] { 2, 0 } }); 80 #endregion 81 82 #region T11 83 data.Add(\"11-12\", new Int32[][] { new Int32[] { 1, 1 }, new Int32[] { 1, 2 }, new Int32[] { 1, 1 } }); 84 #endregion 85 #endregion 86 return data; 87 }\nView Code\n测试的主要方法是：\n1 var team = new List<Int32>(){1,2,3,4,5,6,7,8,9,10,11,12}; 2 var data = GetTestData(); 3 var k3 = CalcLevelScore3(data,team); 4 var w3 = GetEigenVectors(k3); 5 6 var teamOrder = TeamOrder(w3,team); 7 Console.WriteLine(teamOrder.ArrayToString());\n排序结果如下：\n7,3,1,9,8,2,10,4,6,5,12,11\n结果和论文差不多，差别在前面2个，队伍7和3的位置有点问题。具体应该是计算精度的关系如果前面的计算有一些精度损失的话，对后面的计算有一点点影响。\nPageRank的一个基本应用今天就到此为止，接下来如果大家感兴趣，我将继续介绍PageRank在球队排名和比赛预测结果中的应用情况。看时间安排，大概思路和本文类似，只不过在细节上要处理一下。"}
{"content2":"引言：\n机器学习领域中所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x->y，其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。\n当然还有一大类方法本质上也是做了降维，叫做feature selection，目的是从原始的数据feature集合中挑选一部分作为数据的表达。\n目前大部分降维算法处理向量表达的数据，也有一些降维算法处理高阶张量表达的数据。\n之所以使用降维后的数据表示是因为:\n(1)在原始的高维空间中，包含有冗余信息以及噪音信息，在实际应用例如图像识别中造成了误差，降低了准确率；而通过降维,我们希望减少冗余信息所造成的误差,提高识别（或其他应用）的精度。\n(2)或者希望通过降维算法来寻找数据内部的本质结构特征。\n(3)通过降维来加速后续计算的速度\n(4)还有其他很多目的，如解决数据的sparse问题\n在很多算法中，降维算法成为了数据预处理的一部分，如PCA。事实上，有一些算法如果没有降维预处理，其实是很难得到很好的效果的。\n如果你需要处理数据，但是数据原来的属性又不一定需要全部保留，那么PCA也许是一个选择。\n主成分分析算法（PCA）\nPrincipal Component Analysis(PCA)是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。\n通俗的理解，如果把所有的点都映射到一起，那么几乎所有的信息（如点和点之间的距离关系）都丢失了，而如果映射后方差尽可能的大，那么数据点则会分散开来，以此来保留更多的信息。可以证明，PCA是丢失原始数据信息最少的一种线性降维方式。（实际上就是最接近原始数据，但是PCA并不试图去探索数据内在结构）\n设n维向量w为目标子空间的一个坐标轴方向（称为映射向量），最大化数据映射后的方差，有：\n其中m是数据实例的个数， xi是数据实例i的向量表达， x拔是所有数据实例的平均向量。定义W为包含所有映射向量为列向量的矩阵，经过线性代数变换，可以得到如下优化目标函数：\nW'W=I是说希望结果的每一个feature都正交，这样每一维度之间不会有冗余信息。\n其中tr表示矩阵的迹，A是数据协方差矩阵。\n容易得到最优的W是由数据协方差矩阵前k个最大的特征值对应的特征向量作为列向量构成的。这些特征向量形成一组正交基并且最好地保留了数据中的信息。\nPCA的输出就是Y = W'X，由X的原始维度降低到了k维。因此不知道推导也无所谓，只要会算就行，注意X需要均值化。\n来看个例子：\n当使用1个特征向量的时候，3的基本轮廓已经保留下来了，特征向量使用的越多就越与原始数据接近\nPCA追求的是在降维之后能够最大化保持数据的内在信息，并通过衡量在投影方向上的数据方差的大小来衡量该方向的重要性。但是这样投影以后对数据的区分作用并不大，反而可能使得数据点揉杂在一起无法区分。这也是PCA存在的最大一个问题，这导致使用PCA在很多情况下的分类效果并不好。具体可以看下图所示，若使用PCA将数据点投影至一维空间上时，PCA会选择2轴，这使得原本很容易区分的两簇点被揉杂在一起变得无法区分；而这时若选择1轴将会得到很好的区分结果。\nDiscriminant Analysis所追求的目标与PCA不同，不是希望保持数据最多的信息，而是希望数据在降维后能够很容易地被区分开来。后面会介绍LDA的方法，是另一种常见的线性降维方法。另外一些非线性的降维方法利用数据点的局部性质，也可以做到比较好地区分结果，例如LLE，Laplacian Eigenmap等。以后会介绍。"}
{"content2":"运行效果如下：\n详细代码分析过程如下：\n（1）导入需要用到的python包或函数\n（4）将dataset数据进行归一化处理，使用kmeans函数进行聚类,输入第一个参数是数据,第二个参数为聚类个数2，k-means最后输出的结果其实是两维的,第一维是聚类中心,第二维是损失distortion，新建一个celltype变量，用来保存最后处理的图片，所以它对应dataset数据集来说，只有1列，有一张图片所有像素总和（面积）那么多行。对dataset归一化处理后得到对应的数据集whitened进行enumerate（这个函数返回数组元素值的同时，也返回这个值所对应的数组下标）。在上面执行了：centroids,distortion=kmeans(whitened,2)，意义在于：把whitened所有的数据分成两类（两部分），并且这两类（两部分）数据都又分别对应着有一个中心（或者称它为质点）centroids。把whitened里的每一个元素和这两个质点进行求解它的欧几里得距离（也就是点和点之间的距离），如果距离0类比较近：d1>d2那么对应的图片celltype的位置的数值就赋值0。否则对应的图片celltype的位置的数值就赋值1。经过这个for循环之后，就会得到最后那张图片的每个像素的特征（或者可以说成是这个像素它是0类会是1类：最后那张图片哪里需要着色，哪里不需要着色），celltype其实就是记录了最后那张图片的每个像素的特征，所以也可以把它称为最后我们感兴趣那张图片的特征向量。\n（5）读出dataset图片，并且获取它的第1通道值和求出这个通道值的最大方差，保存并显示大于这个方差值的图片。\n（6）读取样本的550.jpg图片的第1通道值，并且只选择大于80的值保存在变量II中，保存并显示II图片。\n（7）将II的里面的数值类型变为浮点型，定义一个pix_num变量，作为celltype图片特征矩阵（向量）的下标，用来每次对图片I_seg_float涂色后换行用的。I_seg_float和I_seg一样也是一个二维数组，使用enumerate依次迭代出它里面的元素的值，并且同时将这个值的下标也返回了。如果图片I_seg_float数组里面的元素不是0那么就代表着是我们需要着色的一个像素点（也就是我们感兴趣的地方），这个像素的位置就是I_seg_float[j,k]，而它被涂上的颜色值是：这个像素的特征值celltype[pix_num]乘以100后再加100。涂完色就换行pix_num+=1.再将I_seg_float图片类型转化成uint8类型。输出并保存I_seg_result图片。\n（8）读取上面I_seg_result.jpg图片，并取得它第1通道大于50的值保存在III中，保存并显示这个图片变量。\n（9）读取上面第（6）、（8）保存的两张图片，并且都获取它们第0通道的值，并且其中第（8）保存的那张图片获取里面大于100的值。显示如下：\n（10）上面的第三幅图是根据筛选处理后需要保留的红色区域，下面的两个for循环就是把第三幅图绘制到第二幅图里面，这样就可以看到很清晰的细胞与非细胞的对比效果了。如下图所示：\n实验总结 第一种图片处理方法使用的k-means算法。这个算法的理论比较完善，并且实现起来也不是很麻烦，但是它需要进行大量较为复杂的簇的分类计算。关于开始质点选择如果不好直接导致算法的复杂度增加甚至失败（得到错误的分类结果）。从上面的三张最后图片中，通过相互比较可以看到，k-means算法得到的结果将一些细胞漏掉了。这很可能就是分类的结果不好造成的。 第二种图片处理方法是求和生存法。这个处理方法相对于上面的k-means算法，明显大大减低了运算复制度，并且实现起来也明显要比k均值算法要简单许多。它是通过直接获取图片所有全部信息的叠加（分类）结果，再取得一个合适显示的值得到需要显示的图片特征，然后根据这个特征来进行绘图就可以了。通过比较三张最后效果图片，可以发现这个种方法的处理得到的效果是最好的。这个方法的缺点是数学等学科理论依据不强，所以没有推广适用的普遍性，但它是上面处理得到的结果最好的，所以这种处理方法还有很大的成长完善的空间。 第三种图片处理方法是点方差聚类法。这个处理方法其实是我想对第二种完善，虽比k-means算法得到的结果好一点，但还是比不上简洁的第二种求和生存法。很容易看到这种方法的运算量和k-means的差不多，过程也相对要复杂了一点。它先是读取了所有图片，然后把所有图片变灰色，再获取每张灰色图片的特征分类值，最后累加到数据集dataset中。这种直接就利用数值分类方法要比k-means的好上许多，但它是对灰色图片获得的特征再累加，不是原图，分类的结果和k-means差不多，但比不上第二种的处理结果。 第四种图片处理方法是累加点方差聚类-K-means算法。它的处理结果要比上面的都要好些，但就是计算量比k-means的要复杂些。这种处理方法是先将初步处理的所有样本图片都累加保存到一个数组变量dataset中，然后使用k-means算法对这个变量进行聚类成两类，但k-means的缺点如上面第一种处理方法所说的，有可能会导致失败。但这种处理方法可以在一定程度上客服k-means这样的缺点，但运算量要增加些。使用k-means后，继续对图片进行分割、比较，所以在很大程度上，这样的处理方法会得到一个相对很好的处理结果。 通过这次的作业，我掌握了如何读取地图数据、获取地图坐标点的信息以及按特点格式输出地图坐标点等相关知识。通过使用三种不同的方法处理图片，我对图片的本质、图片的读取、图片的保存、图片着色、过滤、特征提取等方面技能都有很大的提高，对整体与部分、系统与统一等辩证思维方法和python代码的编写能力、分析能力等也有了进步一的提高。但在完成任务的过程中，发现还有许多的数学、python包的使用等方面的知识和技能存在不足，还需要继续坚持努力学习补充相关的知识和技能，才能够应对未来更多困难与挫折的挑战。 最后很感谢老师的指导和他的支持......"}
{"content2":"实验内容和原理 1. GML 地图数据清洗（30 分） Gml 数据可以用记事本打开，可见里面有很多数据，其中我们需要提取红色区域的 数字，这类数字特征：前面是<gml:coordinates decimal=\".\" cs=\",\" ts=\" \">-，后面是 </gml:coordinates>，66.94864210999998,44.998692232999986 就是表示一个点的横 坐标和纵坐标，这一块“66.94864210999998,44.998692232999986 - 66.94847150599998,44.99853494600001 - 66.94825196400001,44.997809570000015”，一共有 3 个点！！！三个点构成的图形 称为一条折线。下图一共 2 个折线，第二个折线 5 个点！！！请把这些数字保存 在一个 N*2 的一个数据集里面。其中前一个值表示 x,后一个表示 y。最后生成一个 txt 文档，具体如下 </gml:boundedBy> <gml:featureMember> <RoadSegment fid=\"C2006_RF_4005734\"> <rbUid>4005734</rbUid> <name>Richardson</name> <type>RD</type> <addrFmLeft>43</addrFmLeft> <addrFmRght>62</addrFmRght> <centreline> <gml:LineString srsName=\"\"> <gml:coordinates decimal=\".\" cs=\",\" ts=\" \">- 66.94864210999998,44.998692232999986 -66.94847150599998,44.99853494600001 - 66.94825196400001,44.997809570000015 </gml:coordinates> </gml:LineString> </centreline> </RoadSegment> </gml:featureMember> <gml:featureMember> <RoadSegment fid=\"C2006_RF_4009492\"> <rbUid>4009492</rbUid> <name>Sherwood</name> <type>RD</type> <addrFmLeft>1</addrFmLeft> <addrToLeft>17</addrToLeft> <addrFmRght>2</addrFmRght> <addrToRght>18</addrToRght> <centreline> <gml:LineString srsName=\"\"> <gml:coordinates decimal=\".\" cs=\",\" ts=\" \">- 64.874910613,46.370661702999996 -64.87512091600001,46.371108246000006 - 64.87525917200003,46.371348969999985 -64.875272655,46.371401115000026 - 64.87527750100003,46.371419871 </gml:coordinates> %%%%%%%%%%%%%%%%txt 文档要求%%%%%% 67354//这个数字是总共的折线数目 3//3 个点，每个点横纵坐标间有个空格，两个点间有个回车 -66.94864210999998 44.99869223299998 -66.94847150599998 44.9985349460000 -66.94825196400001 44.99780957000001 5 -64.874910613 46.37066170299999 -64.87512091600001 46.37110824600000 -64.87525917200003 46.37134896999998 -64.875272655 46.37140111500002 -64.87527750100003 46.37141987 2 -65.87159661800001 46.18275623800002 -65.87176788199997 46.1823893100000 2 -66.06059955500001 46.0733558709999 -66.05884228500003 46.0738318720000 2 -64.539539321 46.22656682299998 -64.53880671000002 46.2268016930000\n实验过程与结果（可贴图）\nGML 地图数据清洗（30 分）\n解：\n全部代码如下：\n最终的部分运行结果输出如下：\n.............................................................................\n......................................................."}
{"content2":"都知道现在最火爆的是人工智能、大数据。而人工智能和大数据主要用的语言就是Java和Python。今天我们就来分析一下，当前java，python和大数据，哪个就业前景更好？自己该学哪一个？\nJava和Python是编程语言，而大数据则是一系列技术的整合，所以应该分开来看，三者并不能直接进行对比。\n三者实际的关系是目标和实现的包含关系。所以这个问题应该分别为 Java和Python哪个发展前景好？大数据的发展前景如何？\nJava发展前景\nJava语言是一门面向对象编程语言，不仅吸收了C++语言的各种优点，还摒弃了C++语言里难以理解的多继承、指针等概念，因此Java语言具有功能强大和简单易用两个特征。\nJava语言作为静态的面向对象编程语言的代表，极好地实现了面向对象的理论，允许程序员以优雅的思维方式进行复杂的编程。Java语言具有简单性、面向对象、分布式、健壮性、安全性、平台独立与可移植性、多线程、动态性等等特点 。\nJava的发展方向：web开发、大数据开发、安卓开发、服务器开发等等。\nJava的就业情况：\nJava作为传统的编程语言之一，就业市场一直非常紧缺，只要能够掌握相关的技术，实现就业并不难。\nJava市场人才需求量：\n从地域上看来，北上广深依旧是人才需求明显的地区，另外可以看出杭州对于软件人才的需求也在扩大，人才需求量对比，可看出Java需求之大，很多企业都很难招到一个合适的软件人才。\nJava开发薪资：\n可以看到，Java薪资处在10-30K的为普遍情况。\n大数据前景\n大数据并不是一种概念，而是一种方法论，一句话概括，就是通过分析和挖掘全量的非抽样的数据辅助决策。\n关于大数据的发展前景，这里引用马云说过的一句话：\n未来最大的资源就是数据，不参与大数据十年后一定会后悔。\n从这句话中可以看出马云对大数据是多么的推崇，而事实上，大数据在现在乃至未来十年，依然会很火。\n大数据可以实现的应用可以概括为两个方向，一是精准化定制，二是预测。比如通过搜索引擎搜索同样的内容，每个人的结果是不同的，定制新闻服务或网游。再比如精准营销，百度推广，淘宝推广，或者你到了一个地方，自动推荐周边的消费设施等。\n大数据能火多久完全决定于他的应用方向能火多久，或者可以说是大数据的取代者何时能火！\nJava和大数据的关系：\njava是计算机的一门编程语言；可以用来做很多工作，大数据开发属于其中一种；大数据属于互联网方向，就像现在建立在大数据基础上的AI方向一样，他两不是一个同类，但是属于包含和被包含的关系；\njava可以用来做大数据工作，大数据开发或者应用不必要用java，可以Python，Scala，go语言等。\n目前最火的大数据开发平台是Hadoop，而Hadoop则是采用java语言编写。CentOS7服务器中apache、php7以及mysql5.7的安装配置代码，一方面由于hadoop的历史原因，Hadoop的项目诞生于一个java高手；另一方面，也有Java跨平台方面的优势；基于这两个方面的原因，所以Hadoop采用了Java语言。但是也因为Hadoop使用了java所以就出现了“Java大数据”。\n由于大数据产业的火爆，相关职位的待遇也是水涨船高。可以看到，大数据相关职位的平均薪资已经超过月薪20K。\n大数据的应用方向和未来趋势\n1、应用方向：营销、金融、工业、医疗、教育、交通、智慧生活、执法、体育、政府、旅游等等，大数据是真正的覆盖全行业，也就是未来所有的行业都需要大数据的支撑。\n2、未来趋势：传感器——数据服务——人工智能——社会关系——人类文明\n从大数据的应用方向和未来发展趋势可以看出来，在未来10年或20年社会及企业发展过程中，春秋战国大事记，大数据是我们无法离开的技术。云服务、人工智能越来越火，没有大数据谈何云服务，没有大数据谈何人工智能？\n由此看出大数据能火多久的主要决定因素就是有没有他的取代者出现。\n达妹OS：大数据薪资28000！！！超越其他有没有！！！\nPython发展前景\nPython是一种面向对象的解释型计算机程序的设计语言, Python具有丰富和强大的库。它常被称为胶水语言，能够把其他语言制作的各种模块很轻松地结合在一起。\n相对于Java、C语言等，Python简单易学，更适合没有编程基础的小白入门。Python 的语言没有多少仪式化的东西，所以就算不是一个 Python 专家，你也能读懂它的代码。\nPython的发展方向：数据分析、人工智能、web开发、测试、运维、web安全、游戏制作等等。\n另外说下，Python目前的发展趋势非常好，伴随着大数据和人工智能的发展，Python的应用将得到更广泛的普及，目前在落地应用中已有不少Python开发的项目了。\nPython是人工智能的未来。因为考虑到语言的灵活性，其速度以及提供的机器学习功能库(如scikit-learn，Keras和TensorFlow)，我们将继续看到Python在机器学习领域占据主导地位。\n所以就目前的趋势来说，Python要比Java更具有前景一些。\n总结：\n无论是Java、Python还是大数据，都是我们这个时代急需且紧缺的技术。当然，如果你有一定的Java基础，从薪资上来看，大数据是你的第一选择！"}
{"content2":"1. 什么是NLP\n所谓NLP就是自然语言处理，即计算机识别人的自然沟通语言，将人的语言转换成表达含义相同的文字。因为NLP的目的是将人和计算机通过自然语言沟通成为可能，而人最方便的沟通是通过语音发声，计算机只能识别二进制串，或者将可以同等转化为二进制的文字。所以NLP的目的是将语言发声和同等含义的文字进行相互转换后并进行人机交互。\n技术发展现状：\n但人工智能在很多方面，如语言理解、视觉场景理解、决策分析等，仍然举步维艰。一个关键的问题就是，机器必须要掌握大量的知识，特别是常识知识才能实现真正类人的智能。这也说明当前随着大数据红利的消失殆尽，以深度学习为代表的感知智能水平日益接近其“天花板”，而以知识为中心的认知智能将是下一代人工智能技术的关键方向。\n弱人工智能，强人工智能，超人工智能：\n弱人工智能：\n这种弱人工智能应用的非常广泛，但是因为比较“弱”，所以很多人没有意识到它们就是人工智能。就好像现在手机当中的自动拦截骚扰电话、邮箱的自动过滤、还有在象棋方面打败人类的机器人。\n强人工智能：\n能够有自己的思考方式，能够进行推理然后制作计划，最后进行执行，并且拥有一定的学习能力，能够在实践当中不断进步。\n超人工智能：\n智慧程度比人类还要高，在大部分领域当中都超越人类的人工智能，目前这种研发这种人工智能的程度非常之大，但是未来也并不是没有可能的，毕竟现在人工智能领域的发展速度的确是非常快速的\n2. NLP的工作原理\n（1）NLP的工作步骤：\n首先，将语音转换为文字\n其次，对文字进行拆分\n再者，词性理解\n最后，语义理解\n（2）各步骤\n3. NLP的框架有哪些，及这些框架的组成，特点\ntensorflow的插件SyntaxNet\n4. NLP的具体应用有哪些\n5. 写一个NLP具体的例子，带程序流程图，实例代码\n7. 四大强人工智能开放平台\n自动驾驶，城市大脑，医疗影像，智能语音"}
{"content2":"·ACM：1999年开始，ACM每年召开的ACM-EC（电子商务研讨会），推荐系统的研究文章逐步增大；ACM的SIGKDD小组设立WEBKDD组，主题集中在电子商务中的Web挖掘技术和推荐技术。2001年开始，ACM下面的SIGIR开始专门把推荐系统作为一个研讨主题。\n·IJCAI：第17届国际人工智能联合会议IJCAI'01，把E-business&the Intelligent Web作为一个独立的研讨小组。\n·SIGCHI：1999年的人机界面会议SIGCHI'99专门设立推荐系统特别兴趣组。\n·第十五届人工智能会议AAAI-98，第一届知识管理应用会议PAKM、96年协同工作会议CSCW'96等也纷纷将电子商务推荐系统作为研究主题。\n·21世纪以后，数据挖掘、机器学习、人工智能等领域中相关的重要国际学术会议中，如AAAI、IDCM、ICML、SIGIR、ECML等，都包含推荐系统或相关算法技术的主题内容。\n国际会议\nACM Conference on Recommender Systems （RecSys)\nWWW\nSIGIR\nCIKM\nWSDM\n期刊\nExpert Systems with Applications"}
{"content2":"为了对GMM-HMM在语音识别上的应用有个宏观认识，花了些时间读了下HTK（用htk完成简单的孤立词识别）的部分源码，对该算法总算有了点大概认识，达到了预期我想要的。不得不说，网络上关于语音识别的通俗易懂教程太少，都是各种公式满天飞，很少有说具体细节的，当然了，那需要有实战经验才行。下面总结以下几点，对其有个宏观印象即可（以孤立词识别为例）。\n一、每个单词的读音都对应一个HMM模型，大家都知道HMM模型中有个状态集S，那么每个状态用什么来表示呢，数字？向量？矩阵？其实这个状态集中的状态没有具体的数学要求，只是一个名称而已，你可以用’1’, ’2’, ‘3’…表示，也可以用’a’, ‘b’, ’c ’表示。另外每个HMM模型中到底该用多少个状态，是通过先验知识人为设定的。\n二、HMM的每一个状态都对应有一个观察值，这个观察值可以是一个实数，也可以是个向量，且每个状态对应的观察值的维度应该相同。假设现在有一个单词的音频文件，首先需要将其进行采样得到数字信息（A/D转换），然后分帧进行MFCC特征提取，假设每一帧音频对应的MFCC特征长度为39，则每个音频文件就转换成了N个MFCC向量（不同音频文件对应的N可能不同），这就成了一个序列，而在训练HMM模型的参数时（比如用Baum-Welch算法），每次输入到HMM中的数据要求就是一个观测值序列。这时，每个状态对应的观测值为39维的向量，因为向量中元素的取值是连续的，需要用多维密度函数来模拟，通常情况下用的是多维高斯函数。在GMM-HMM体系中，这个拟合函数是用K个多维高斯混合得到的。假设知道了每个状态对应的K个多维高斯的所有参数，则该GMM生成该状态上某一个观察向量（一帧音频的MFCC系数）的概率就可以求出来了。\n三、对每个单词建立一个HMM模型，需要用到该单词的训练样本，这些训练样本是提前标注好的，即每个样本对应一段音频，该音频只包含这个单词的读音。当有了该单词的多个训练样本后，就用这些样本结合Baum-Welch算法和EM算法来训练出GMM-HMM的所有参数，这些参数包括初始状态的概率向量，状态之间的转移矩阵，每个状态对应的观察矩阵（这里对应的是GMM，即每个状态对应的K个高斯的权值，每个高斯的均值向量和方差矩阵）。\n四、在识别阶段，输入一段音频，如果该音频含有多个单词，则可以手动先将其分割开（考虑的是最简单的方法），然后提取每个单词的音频MFCC特征序列，将该序列输入到每个HMM模型（已提前训练好的）中，采用前向算法求出每个HMM模型生成该序列的概率，最后取最大概率对应的那个模型，而那个模型所表示的单词就是我们识别的结果。\n五、在建立声学模型时，可以用Deep Learning的方法来代替GMM-HMM中的GMM，因为GMM模拟任意函数的功能取决于混合高斯函数的个数，所以具有一定的局限性，属于浅层模型。而Deep Network可以模拟任意的函数，因而表达能力更强。注意，这里用来代替GMM的Deep Nets模型要求是产生式模型，比如DBN，DBM等，因为在训练HMM-DL网络时，需要用到HMM的某个状态产生一个样本的概率。\n六、GMM-HMM在具体实现起来还是相当复杂的。\n七、一般涉及到时间序列时才会使用HMM，比如这里音频中的语音识别，视频中的行为识别等。如果我们用GMM-HMM对静态的图片分类，因为这里没涉及到时间信息，所以HMM的状态数可设为1，那么此时的GMM-HMM算法就退化成GMM算法了。\nMFCC:\nMFCC的matlab实现教程可参考：张智星老师的网页教程mfcc. 最基本的12维特征。\nfunction mfcc=frame2mfcc(frame, fs, filterNum, mfccNum, plotOpt) % frame2mfcc: Frame to MFCC conversion. % Usage: mfcc=frame2mfcc(frame, fs, filterNum, mfccNum, plotOpt) % % For example: % waveFile='what_movies_have_you_seen_recently.wav'; % [y, fs, nbits]=wavReadInt(waveFile); % startIndex=12000; % frameSize=512; % frame=y(startIndex:startIndex+frameSize-1); % frame2mfcc(frame, fs, 20, 12, 1); % Roger Jang 20060417 if nargin<1, selfdemo; return; end if nargin<2, fs=16000; end if nargin<3, filterNum=20; end if nargin<4, mfccNum=12; end if nargin<5, plotOpt=0; end frameSize=length(frame); % ====== Preemphasis should be done at wave level %a=0.95; %frame2 = filter([1, -a], 1, frame); frame2=frame; % ====== Hamming windowing frame3=frame2.*hamming(frameSize); % ====== FFT [fftMag, fftPhase, fftFreq, fftPowerDb]=fftOneSide(frame3, fs); % ====== Triangular band-pass filter bank triFilterBankPrm=getTriFilterBankPrm(fs, filterNum); % Get parameters for triangular band-pass filter bank % Triangular bandpass filter. for i=1:filterNum tbfCoef(i)=dot(fftPowerDb, trimf(fftFreq, triFilterBankPrm(:,i)));%得到filterNum个滤波系数 end % ====== DCT mfcc=zeros(mfccNum, 1); %DCT变换的前后个数也没有变 for i=1:mfccNum coef = cos((pi/filterNum)*i*((1:filterNum)-0.5))'; %mfcc中的前mfccNum个系数 mfcc(i) = sum(coef.*tbfCoef');%直接按照DCT公式 end % ====== Log energy %logEnergy=10*log10(sum(frame.*frame)); %mfcc=[logEnergy; mfcc]; if plotOpt subplot(2,1,1); plot(frame, '.-'); set(gca, 'xlim', [-inf inf]); title('Input frame'); subplot(2,1,2); plot(mfcc, '.-'); set(gca, 'xlim', [-inf inf]); title('MFCC vector'); end % ====== trimf.m (from fuzzy toolbox) function y = trimf(x, prm) %由频率的横坐标算出三角形内的纵坐标,0~1 a = prm(1); b = prm(2); c = prm(3); y = zeros(size(x)); % Left and right shoulders (y = 0) index = find(x <= a | c <= x); y(index) = zeros(size(index)); %只考虑三角波内的量 % Left slope if (a ~= b) index = find(a < x & x < b); y(index) = (x(index)-a)/(b-a); end % right slope if (b ~= c) index = find(b < x & x < c); y(index) = (c-x(index))/(c-b); end % Center (y = 1) index = find(x == b); y(index) = ones(size(index)); % ====== Self demo function selfdemo waveFile='what_movies_have_you_seen_recently.wav'; [y, fs, nbits]=wavReadInt(waveFile); startIndex=12000; frameSize=512; frame=y(startIndex:startIndex+frameSize-1); feval(mfilename, frame, fs, 20, 12, 1);\nZCR:\n过0检测，用于判断每一帧中过零点的数量情况，最简单的版本可参考：zeros cross rate.\nwaveFile='csNthu.wav'; frameSize=256; overlap=0; [y, fs, nbits]=wavread(waveFile); frameMat=enframe(y, frameSize, overlap); frameNum=size(frameMat, 2); for i=1:frameNum frameMat(:,i)=frameMat(:,i)-mean(frameMat(:,i)); % mean justification end zcr=sum(frameMat(1:end-1, :).*frameMat(2:end, :)<0); sampleTime=(1:length(y))/fs; frameTime=((0:frameNum-1)*(frameSize-overlap)+0.5*frameSize)/fs; subplot(2,1,1); plot(sampleTime, y); ylabel('Amplitude'); title(waveFile); subplot(2,1,2); plot(frameTime, zcr, '.-'); xlabel('Time (sec)'); ylabel('Count'); title('ZCR');\nEPD:\n端点检测，检测声音的起始点和终止点，可参考：EPD in Time Domain,在时域中的最简单检测方法。\nwaveFile='sunday.wav'; [wave, fs, nbits] = wavread(waveFile); frameSize = 256; overlap = 128; wave=wave-mean(wave); % zero-mean substraction frameMat=buffer2(wave, frameSize, overlap); % frame blocking,每一列代表一帧 frameNum=size(frameMat, 2); % no. of frames volume=frame2volume(frameMat); % volume,求每一帧的能量，绝对值或者平方和,volume为行向量 volumeTh1=max(volume)*0.1; % volume threshold 1 volumeTh2=median(volume)*0.1; % volume threshold 2 volumeTh3=min(volume)*10; % volume threshold 3 volumeTh4=volume(1)*5; % volume threshold 4 index1 = find(volume>volumeTh1); %找出volume大于阈值的那些帧序号 index2 = find(volume>volumeTh2); index3 = find(volume>volumeTh3); index4 = find(volume>volumeTh4); %frame2sampleIndex()为从帧序号找到样本点的序号(即每一个采样点的序号) %endPointX长度为2,包含了起点和终点的样本点序号 endPoint1=frame2sampleIndex([index1(1), index1(end)], frameSize, overlap); endPoint2=frame2sampleIndex([index2(1), index2(end)], frameSize, overlap); endPoint3=frame2sampleIndex([index3(1), index3(end)], frameSize, overlap); endPoint4=frame2sampleIndex([index4(1), index4(end)], frameSize, overlap); subplot(2,1,1); time=(1:length(wave))/fs; plot(time, wave); ylabel('Amplitude'); title('Waveform'); axis([-inf inf -1 1]); line(time(endPoint1( 1))*[1 1], [-1, 1], 'color', 'm');%标起点终点线 line(time(endPoint2( 1))*[1 1], [-1, 1], 'color', 'g'); line(time(endPoint3( 1))*[1 1], [-1, 1], 'color', 'k'); line(time(endPoint4( 1))*[1 1], [-1, 1], 'color', 'r'); line(time(endPoint1(end))*[1 1], [-1, 1], 'color', 'm'); line(time(endPoint2(end))*[1 1], [-1, 1], 'color', 'g'); line(time(endPoint3(end))*[1 1], [-1, 1], 'color', 'k'); line(time(endPoint4(end))*[1 1], [-1, 1], 'color', 'r'); legend('Waveform', 'Boundaries by threshold 1', 'Boundaries by threshold 2', 'Boundaries by threshold 3', 'Boundaries by threshold 4'); subplot(2,1,2); frameTime=frame2sampleIndex(1:frameNum, frameSize, overlap); plot(frameTime, volume, '.-'); ylabel('Sum of Abs.'); title('Volume'); axis tight; line([min(frameTime), max(frameTime)], volumeTh1*[1 1], 'color', 'm'); line([min(frameTime), max(frameTime)], volumeTh2*[1 1], 'color', 'g'); line([min(frameTime), max(frameTime)], volumeTh3*[1 1], 'color', 'k'); line([min(frameTime), max(frameTime)], volumeTh4*[1 1], 'color', 'r'); legend('Volume', 'Threshold 1', 'Threshold 2', 'Threshold 3', 'Threshold 4');\nGMM:\nGMM用在拟合数据分布上，本质上是先假设样本的概率分布为GMM，然后用多个样本去学习这些GMM的参数。GMM建模在语音中可用于某个单词的发音，某个人的音色等。其训练过程可参考:speaker recognition.\nfunction [M, V, W, logProb] = gmmTrain(data, gaussianNum, dispOpt) % gmmTrain: Parameter training for gaussian mixture model (GMM) % Usage: function [M, V, W, logProb] = gmm(data, gaussianNum, dispOpt) % data: dim x dataNum matrix where each column is a data point % gaussianNum: No. of Gaussians or initial centers % dispOpt: Option for displaying info during training % M: dim x meanNum matrix where each column is a mean vector % V: 1 x gaussianNum vector where each element is a variance for a Gaussian % W: 1 x gaussianNum vector where each element is a weighting factor for a Gaussian % Roger Jang 20000610 if nargin==0, selfdemo; return; end if nargin<3, dispOpt=0; end maxLoopCount = 50; % Max. iteration minImprove = 1e-6; % Min. improvement minVariance = 1e-6; % Min. variance logProb = zeros(maxLoopCount, 1); % Array for objective function [dim, dataNum] = size(data); % Set initial parameters % Set initial M %M = data(1+floor(rand(gaussianNum,1)*dataNum),:); % Randomly select several data points as the centers if length(gaussianNum)==1, % Using vqKmeans to find initial centers fprintf('Start KMEANS to find the initial mu...\\n'); % M = vqKmeansMex(data, gaussianNum, 0); M = vqKmeans(data, gaussianNum, 0); %利用聚类的方法求均值,聚成gaussianNum类 % M = vqLBG(data, gaussianNum, 0); fprintf('Start GMM training...\\n'); if any(any(~isfinite(M))); keyboard; end else % gaussianNum is in fact the initial centers M = gaussianNum; gaussianNum = size(M, 2); end % Set initial V as the distance to the nearest center if gaussianNum==1 V=1; else distance=pairwiseSqrDist(M);%pairwiseSqrDist是dll %distance=pairwiseSqrDist2(M); distance(1:(gaussianNum+1):gaussianNum^2)=inf; % Diagonal elements are inf [V, index]=min(distance); % Initial variance for each Gaussian end % Set initial W W = ones(1, gaussianNum)/gaussianNum; % Weight for each Gaussian,初始化时是均分权值 if dispOpt & dim==2, displayGmm(M, V, data); end for i = 1:maxLoopCount %开始迭代训练参数,EM算法 % Expectation step: % P(i,j) is the probability of data(:,j) to the i-th Gaussian % Prob为每个样本在GMM下的概率 [prob, P]=gmmEval(data, M, V, W); logProb(i)=sum(log(prob)); %所有样本的联合概率 if dispOpt fprintf('i = %d, log prob. = %f\\n',i-1, logProb(i)); end PW = diag(W)*P; BETA=PW./(ones(gaussianNum,1)*sum(PW)); % BETA(i,j) is beta_i(x_j) sumBETA=sum(BETA,2); % Maximization step: eqns (2.96) to (2.98) from Bishop p.67: M = (data*BETA')./(ones(dim,1)*sumBETA'); DISTSQ = pairwiseSqrDist(M, data); % Distance of M to data %DISTSQ = pairwiseSqrDist2(M, data); % Distance of M to data V = max((sum(BETA.*DISTSQ, 2)./sumBETA)/dim, minVariance); % (2.97) W = (1/dataNum)*sumBETA; % (2.98) if dispOpt & dim==2, displayGmm(M, V, data); end if i>1, if logProb(i)-logProb(i-1)<minImprove, break; end; end end [prob, P]=gmmEval(data, M, V, W); logProb(i)=sum(log(prob)); fprintf('Iteration count = %d, log prob. = %f\\n',i, logProb(i)); logProb(i+1:maxLoopCount) = []; % ====== Self Demo ====== function selfdemo %[data, gaussianNum] = dcdata(2); data = rand(1000,2); gaussianNum = 8; data=data'; plotOpt=1; [M, V, W, lp] = feval(mfilename, data, gaussianNum, plotOpt); pointNum = 40; x = linspace(min(data(1,:)), max(data(1,:)), pointNum); y = linspace(min(data(2,:)), max(data(2,:)), pointNum); [xx, yy] = meshgrid(x, y); data = [xx(:) yy(:)]'; z = gmmEval(data, M, V, W); zz = reshape(z, pointNum, pointNum); figure; mesh(xx, yy, zz); axis tight; box on; rotate3d on figure; contour(xx, yy, zz, 30); axis image % ====== Other subfunctions ====== function displayGmm(M, V, data) % Display function for EM algorithm figureH=findobj(0, 'tag', mfilename); if isempty(figureH) figureH=figure; set(figureH, 'tag', mfilename); colordef black plot(data(1,:), data(2,:),'.r'); axis image theta=linspace(-pi, pi, 21); x=cos(theta); y=sin(theta); sigma=sqrt(V); for i=1:length(sigma) circleH(i)=line(x*sigma(i)+M(1,i), y*sigma(i)+M(2,i), 'color', 'y'); end set(circleH, 'tag', 'circleH', 'erasemode', 'xor'); else circleH=findobj(figureH, 'tag', 'circleH'); theta=linspace(-pi, pi, 21); x=cos(theta); y=sin(theta); sigma=sqrt(V); for i=1:length(sigma) set(circleH(i), 'xdata', x*sigma(i)+M(1,i), 'ydata', y*sigma(i)+M(2,i)); end drawnow end\nSpeaker identification:\n给N个人的语音资料，用GMM可以训练这N个人的声音模型，然后给定一段语音，判断该语音与这N个人中哪个最相似。方法是求出该语音在N个GMM模型下的概率，选出概率最大的那个。可参考:speaker recognition.\nfunction [recogRate, confusionMatrix, speakerData]=speakerIdentify(speakerData, speakerGmm, useIntGmm) % speakerIdentify: speaker identification using GMM parameters % Usage: [recogRate, confusionMatrix, speakerData]=speakerIdentify(speakerData, speakerGmm, useIntGmm) % speakerData: structure array generated by speakerDataRead.m % speakerGmm: speakerGmm(i).gmmPrm is the GMM parameters for speaker i. % useIntGmm: use fixed-point GMM % Roger Jang, 20070517, 20080726 if nargin<3, useIntGmm=0; end % ====== Speaker identification using GMM parameters speakerNum=length(speakerData); for i=1:speakerNum % fprintf('%d/%d: Recognizing wave files by %s\\n', i, speakerNum, speakerData(i).name); for j=1:length(speakerData(i).sentence) % fprintf('\\tSentece %d...\\n', j); frameNum=size(speakerData(i).sentence(j).fea, 2); logProb=zeros(speakerNum, frameNum); %logProb(i,m)表示第i个人第j个句子中第m帧在GMM模型下的log概率 %找出一个句子，看它属于哪个speaker for k=1:speakerNum, % fprintf('\\t\\tSpeaker %d...\\n', k); % logProb(k, :)=gmmEval(speakerData(i).sentence(j).fea, speakerGmm(k).gmmPrm); if ~useIntGmm % logProb(k, :)=gmmEvalMex(speakerData(i).sentence(j).fea, gmm(k).mean, gmm(k).covariance, gmm(k).weight); logProb(k, :)=gmmEval(speakerData(i).sentence(j).fea, speakerGmm(k).gmmPrm); else % logProb(k, :)=gmmEvalIntMex(speakerData(i).sentence(j).fea, gmm(k).mean, gmm(k).covariance, gmm(k).weight); logProb(k, :)=gmmEvalIntMex(speakerData(i).sentence(j).fea, speakerGmm(i).gmmPrm); end end cumLogProb=sum(logProb, 2); [maxProb, index]=max(cumLogProb); speakerData(i).sentence(j).predictedSpeaker=index; %找出身份 speakerData(i).sentence(j).logProb=logProb; end end % ====== Compute confusion matrix and recognition rate confusionMatrix=zeros(speakerNum); for i=1:speakerNum, predictedSpeaker=[speakerData(i).sentence.predictedSpeaker]; [index, count]=elementCount(predictedSpeaker); confusionMatrix(i, index)=count; end recogRate=sum(diag(confusionMatrix))/sum(sum(confusionMatrix));\nGMM-HMM:\n训练阶段：给出HMM的k个状态，每个状态下的观察样本的生成可以用一个概率分布来拟合，这里是采用GMM拟合的。其实，可以把GMM-HMM整体看成是一个生成模型。给定该模型的5个初始参数(结合随机和训练样本获得)，启动EM算法的E步：获得训练样本分布，即计算训练样本在各个状态下的概率。M步：用这些训练样本重新评估那5个参数。\n测试阶段：(以孤立词识别为例)给定每个词发音的frame矩阵，取出某一个GMM-HMM模型，算出该发音每一帧数据在取出的GMM-HMM模型各个state下的概率，结合模型的转移概率和初始概率，获得对应的clique tree，可用图模型的方法inference出生成该语音的概率。比较多个GMM-HMM模型，取最大概率的模型对应的词。\n参考资料：\n机器学习&数据挖掘笔记_13（用htk完成简单的孤立词识别）\nhttp://htk.eng.cam.ac.uk/extensions/\n张智星老师的网页教程mfcc."}
{"content2":"问题描述：\n严重: Error starting static Resources\njava.lang.IllegalArgumentException: Document base D:\\Code\\MyJavaCode\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\newsInfoCrawler does not exist or is not a readable directory\n解决办法：\n需要执行下面操作：\n步骤一：首先，进入，你的工作区间目录下的.metadata\\.plugins\\org.eclipse.wst.server.core/下\n比如，我的这里是\n步骤二：进入tmp0/conf/目录，删除无用的 tmp0\\conf\\server.xml 中的  <Context>  节点；\n<Context docBase=\"D:\\Code\\MyJavaCode\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\ROOT\" path=\"\" reloadable=\"false\"/>\n<Context docBase=\"D:\\Code\\MyJavaCode\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\newsInfoCrawler\" path=\"/newsInfoCrawler\" reloadable=\"true\" source=\"org.eclipse.jst.jee.server:newsInfoCrawler\"/></Host>\n删除，留下，得到\n<Context docBase=\"D:\\Code\\MyJavaCode\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\ROOT\" path=\"\" reloadable=\"false\"/></Host>\n步骤三：删除 tmp0\\work\\Catalina\\localhost 下的所有文件夹；\n我这里是，D:\\Code\\MyJavaCode\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\work\\Catalina\\localhost\n步骤四：删除tomcat\n步骤五：重新配置tomcat\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"机器学习（Machine Learning，简称 ML）和计算机视觉（Computer Vision，简称 CV）是非常令人着迷、非常酷炫、颇具挑战性同时也是涉及面很广的领域。本文整理了机器学习和计算机视觉的相关学习资源，目的是帮助许多和我一样希望深刻理解“智能”背后原理的人，用最为高效的方式学习最为前沿的技术和知识。\n另外请见我后一篇博客里列的数据挖掘的学习资源。\nwikipedia.org，历史，领域概述，资源链接：\nMachine learning，介绍了ML所处理的问题、常用算法、应用、软件等，右侧列举了细分条目；\nList of machine learning concepts，Category:Machine learning，列举出了更多ML相关概念和条目；\nComputer vision，同样，介绍了CV所处理的问题、常用方法、应用等，底部列举了细分条目；\nList of computer vision topics，Category:Computer vision，列举了更多CV相关条目。\n大学课程、在线教程：\nStanford 关于ML和CV计算机课程（按推荐排序）：cs229 Machine Learning，cs229T Statistical Learning Theory，cs231N Convolutional Neural Networks for Visual Recognition，cs231A Computer Vision:From 3D Recontruct to Recognition，cs231B The Cutting Edge of Computer Vision，cs221 Artificial Intelligence: Principles & Techniques，cs131 Computer Vision: Foundations and Applications，cs369L A Theoretical Perspective on Machine Learning，cs205A Mathematical Methods for Robotics, Vision & Graph，cs231M Mobile Computer Vision，这些课程大都可以下载PPT，更多课程请见Courses | Stanford Computer Science，Open class room的ML课程Machine Learning，Unsupervised Feature Learning and Deep Learning，Coursera的ML课程：Machine Learning，以及Stanford在线教程Deep learning tuorial；\n更多大学课程可以用“machine learning course”或“computer vision course”为关键字搜索，这里是Google的国内镜像，这样就不需要FanQiang了。\n专著、书籍：\nML：\n机器学习，周志华，2016；\n统计学习方法，李航，2012；\nDeep Learning: Methods and Applications, Li Deng and Dong Yu, 2014；\nIntroduction to Machine Learning (3rd ed.), Ethem Alpaydin, 2014；\nMachine Learning: An Algorithmic Perspective (2nd ed.), Stephen Marsland, 2015；\nDeep Learning，一本在线书籍；\nNeural Networks and Learning Machines (3rd ed.), Simon O. Haykin, 2008；有中文译本：神经网络与机器学习；\nPattern Recognition and Machine Learning, Christopher Bishop, 2006；有中文译本：模式识别与机器学习；\nMachine Learning: a Probabilistic Perspective, Kevin P. Murphy, 2012；\nCV：\nConcise Computer Vision: An Introduction into Theory and Algorithms, Klette, Reinhard, 2014；\nComputer Vision: Algorithms and Applications, Szeliski, Richard, 2011；有中文译本：计算机视觉——算法与应用；\nMultiple View Geometry in Computer Vision (2nd ed.), Richard Hartley and Andrew Zisserman, 2004；\nAn Invitation to 3-D Vision: From Images to Geometric Models,  Yi Ma, Stefano Soatto, Jana Kosecka, S. Shankar Sastry, 2004；\nRobot vision, Berthold K. P. Horn, 1986；有中文译本：机器视觉；\nImage Processing, Analysis, and Machine Vision (3rd ed.), Milan Sonka, Vaclav Hlavac, Roger Boyle, 2007；有中文译本：图像处理、分析与机器视觉；\n推荐一个非常好的搜索英文电子书的网站：Library Genesis。\n学术论文：\nML、CV领域的顶级期刊：TPAMI，IJCV，学术会议：ACL，CVPR，ICML，ICCV，NIPS，ECCV，ACCV等；\nCVPapers 对CV领域学术论文做了很好的整理；\nImageNet 每年举办的图像识别比赛很能代表CV最高水平，MS COCO是类似比赛，KITTI上有很多数据以及CV算法的排名，这里是一个数据集的列表，这里是CV数据集；\narXiv.org，很多最新论文首先发表在这里；\n当然还是推荐Google Scholar，这里是一个镜像网站。\n学习网站：\ndeeplearning.net：一个非常好的机器学习网站，有dataset、software、reading list连接；\nVisionBib.Com：学术大牛整理的CV资源；\nCVonline有一个非常全面的资源链接；\n新智元和机器之心是很好的机器学习资讯平台，另外推荐一些微信公众号：机器学习研究会，程序媛的日常。\n程序、库：\nOpenCV：一个C++视觉库，使用广泛；\nTorch, Theano：两个很强大的支持CUDA显卡加速的Python机器学习库；\nCaffe：很多研究者使用的Deep Learning库；\nR语言：一个方便开发机器学习程序的环境；\n更多的程序库，这里做了很好的总结。"}
{"content2":"之前通过各种博客视频学习CNN，总是对参数啊原理啊什么的懵懵懂懂。。这次上课终于弄明白了，O(∩_∩)O~\n上世纪科学家们发现了几个视觉神经特点，视神经具有局部感受野，一整张图的识别由多个局部识别点构成；不同神经元对不同形状有识别能力，且视神经具有叠加能力，高层复杂的图案可以由低层简单线条组成。之后人们发现经过conclusional的操作，可以很好反映视神经处理计算的过程，典型的是1998年LeCun发明的LeNet-5，可以极大地提升识别效果。\n本文主要就convolutional layer、pooling layer和整体CNN结构展开\n一、Convolutional Layer卷积层\n1、原理和参数\n可以模拟局部感受野的性质，同上一层不是全连接，而是一小块区域连接，这一小块就是局部感受野（receptive field）。并且通过构造特定的卷积神经元，可以模拟不同神经元对不同形状刺激不同反应的性质。如下图所示，一个神经元处理一层会形成一个feature map，多层叠加，层数逐渐加深。\n感受野（kernel或filter）的尺寸可以看做fh*fw，由于感受野本身具有尺寸，feature map会不断缩小，为了处理方便，使得每层大小不变，于是我们每层加值为0的边（zero padding），保证经过处理以后的feature map同前一层尺寸一样。多层之间的卷积运算操作，相当于和原来像素对应位置做乘法。如下左图所示，加了边后可以保证上下层大小一致，右图表示每层之间convolve的操作（如果不加zero padding）。\n但上图所示只是简单例子，一般扫描的是三维图像（RGB），就不是一个矩阵，而是一个立方体，我们用一个三维块去扫描它，原理同上图相同。\n有时扫描时不是顺序去扫，而是跳跃着扫描，每次移动2-3个像素值（stride），但并非完全分离不会造成信息丢失，这样形成的feature map相较于原始图片缩小，实现信息聚集的效果。\n就像如下灰度图（2d）中所示，左边只提取竖线（vertical filter），右边只提取横线（horizontal filter）可看出横梁部分变亮，大量不同的这样的filter（比如可以识别边角、折线的filter）的叠加，可形成多张feature maps\n下图是一个3d的RGB效果，每个kernel（filter）可以扫描出一张feature map，多个filter可以叠加出很厚的feature maps，前一层filter做卷积可以形成后一层的一个像素点\n如下图，可以代表i行j列k深度的一个输出像素值，k’代表第k个filter，w代表filter中的值，x代表输入，b是偏值。\n2、TensorFlow实现\n以下是使用TensorFlow实现的代码，主要使用conv2d这个函数\nimport numpy as np from sklearn.datasets import load_sample_images # Load sample images dataset = np.array(load_sample_images().images, dtype=np.float32) # 一共4维，channel表示通道数，RGB是3 batch_size, height, width, channels = dataset.shape # Create 2 filters # 一般感受野大小7*7,5*5,3*3，设置2个kernel，输出2层feature map filters_test = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32) # 第一个（0）filter的设定，7*7矩阵中，3是中间 filters_test[:, 3, :, 0] = 1 # vertical line # 第二个（1）filter的设定 filters_test[3, :, :, 1] = 1 # horizontal line # a graph with input X plus a convolutional layer applying the 2 filters X = tf.placeholder(tf.float32, shape=(None, height, width, channels)) # 虽然输入是一个四维图像，但是由于batch_size和channel都已经固定，所以使用conv2d # strides设定，第一个和第四个都是1表示不可以跳过batch_size和channel # 那两个2表示横纵向都缩减2，相当于整张图片缩减为原来四分之一，做了75%的缩减 convolution = tf.nn.conv2d(X, filters, strides=[1,2,2,1], padding=\"SAME\") with tf.Session() as sess: output = sess.run(convolution, feed_dict={X: dataset})\n下面是padding的值SAME和VALID的区别（filter的宽度为6，stride为5），SAME确保所有图像信息都被convolve添加zero padding，而VALID只添加包含在内的像素点\n3、所耗内存计算\n相比于传统的全连接层，卷积层只是部分连接，节省了很多内存。\n比如：一个具有5*5大小filter的卷积层，输出200张150*100大小的feature maps，stride取1（即不跳跃），padding为SAME。输入是150*100大小的RGB图像（channel=3），总共的参数个数是200*（5*5*3+1）=15200，其中+1是bias；如果输出采用32-bits float表示（np.float32），那么每张图片会占据200*150*100*32=9600000bits（11.4MB），如果一个training batch包含100张图片（mini-batch=100），那么这一层卷积层就会占据1GB的RAM。\n可以看出，训练卷积神经网络是非常消耗内存的，但是使用时，只用到最后一层的输出即可。\n二、Pooling Layer池化层\n1、原理和参数\n当图片大小很大时内存消耗巨大，而Pooling Layer所起的作用是浓缩效果，缓解内存压力。\n即选取一定大小区域，将该区域用一个代表元素表示。具体的Pooling有两种，取平均值（mean）和取最大值（max）。如下图所示是一个取最大值的pooling layer，kernel大小为2*2，stride大小取决于kernel大小，这里是2，即刚好使得所有kernel都不重叠的值，因而实现高效的信息压缩，将原始图像横纵压缩一半，如右图所示，特征基本都完全保留了下来。\npooling这个操作不影响channel数，在feature map上也一般不做操作（即z轴一般不变），只改变横纵大小。\n2、TensorFlow实现\n# Create a graph with input X plus a max pooling layer X = tf.placeholder(tf.float32, shape=(None, height, width, channels)) # 选用取最大值的max_pool方法 # 如果是取平均值，这里是mean_pool\n# ksize就是kernel大小，feature map和channel都是1,横向纵向是2 max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"VALID\") with tf.Session() as sess: output = sess.run(max_pool, feed_dict={X: dataset})\n三、整体CNN框架\n典型CNN architecture\n有名的CNN架构：\nLeNet（MISIT上）-1998：输入32*32（在28*28图像上加了zero padding）。第一层kernel用了6个神经元，kernel大小5*5，stride取1，输出就是28*28；第二层做了average pooling，2*2的kernel，stride是2，输出就变为原来的一半，不改变feature map数目；第三层放了16个神经元，其他同理；第五层用了120个神经元，5*5的kernel对5*5的输入做卷积，没法再滑动，输出为1*1；F6用120个1*1的输出全连接84个神经元，Out全连接10个神经元，对应手写体识别输出的10个数字。\n激活函数前面都用的tanh，是传统CNN中常用的，输出层用了RBF比较特殊，是一个计算距离的方式去判断和目标输出间距离做lost。。\nAlexNet-2012：最早应用于竞赛中，近10%的提高了准确度\n输入224*224的彩色图像，C1是个很大的11*11的filter，stride=4。。最后连做3层convolution。。最后输出1000个类的分类结果。\n激活函数使用ReLU，这在现今很流行，输出层用的softmax\nAlexNet使用了一个小技巧是Local Response Normalization（LRN局部响应归一化）\n这种操作可以在传统输出上加一个bias，考虑到近邻的一些输出影响。即一个输出旁边有很牛掰的输出的话，它的输出就会怂了，收到抑制，可以看到，含β的整个项都在分母上。但后来发现，这个技术对分类器的提升也不是很明显，有的就没有用。\nGoogleLeNet-2014：\n大量应用Inception module，一个输入进来，直接分四步进行处理，这四步处理完后深度直接进行叠加。在不同的尺度上对图片进行操作。大量运用1*1的convolution，可以灵活控制输出维度，可以降低参数数量。\n如右图所示，输入是192，使用了9层inception module，如果直接用3*3,5*5参数，可以算一下，之后inception参数数目是非常大的，深度上可以调节，可以指定任意数目的feature map，通过增加深度把维度减下来。inception模块6个参数刚好对应这6个convolution，上面4个参数对应上面4个convolution，加入max pool不会改变feature map数目（如480=128+192+96+64）。\n将正确率升高到95-96%，超过人类分辨率，因为image net中但是狗的种类就有很多，人类无法完全一一分辨出。\nReSNet残差网络-2015：\n不再直接学习一个目标函数，输入直接跳过中间层直接连到输出上，要学习的是残差f（x），输入跳过中间层直接加到输出上。\n好处是：深度模型路径依赖的限制，即gradient向前传导时要经过所有层，如果中间有层死掉了，前面的层就无法得到训练。残差网络不断跳跃，即使中间有的层已经死掉，信息仍旧能够有效流动，使得训练信号有效往回传导。"}
{"content2":"2019年上半年收集到的人工智能机器学习方向干货文章\n10种机器学习方法，掌握了就可以称霸朋友圈\n人工智能常见算法简介\n机器学习中的最优化算法总结\n最萌算法学习来啦，看不懂才怪！\nThe Next Step for ML 机器学习落地需攻破的9个难题\n人工智能的学习，需要学习哪些算法和数学知识呢？需要什么学历？\n一文读懂机器学习项目的完整生命周期\n80+机器学习数据集，还不快收藏\n人工智能常见算法简介\n目前最实用的机器学习算法，你认为是哪几种？\n算法工程师必须要知道的8种常用算法思想\n机器学习算法工程师的自我修养\n机器学习中的方法技术与应用场景\n这可能是最简单易懂的机器学习入门\n小白机器学习基础算法学习必经之路\n手把手实战机器学习系列: 随机森林\n2019年度机器学习49个顶级工程汇总\n人工智能之机器学习算法体系汇总\n力荐 50 个最实用的免费机器学习数据集\n机器学习的12大经验总结\n干货 | 揭开对机器学习的七点误解\n如何管理机器学习模型\n【机器学习】【发展史】概览\n机器深度学习的过程中盛传着7 个误解，我们来一一揭开\n统计学和机器学习到底存在哪些联系和区别？\n一文读懂自学机器学习的误区和陷阱（附学习资料）\n机器学习基础-数据降维\n机器学习中的数学基础（1）——向量和范数\n【机器学习知识体系】- 机器学习问题的一般流程\n改进AI/ML部署的5种方法\n随机变量，概率密度及其统计量\n谈谈机器学习与传统编程之间的区别\n机器学习中的数学基础（2）——理解基、线性组合与向量空间\n无监督学习：大数据带我们洞察现在，但小数据将带我们抵达未来\n什么是无监督学习？概念、使用场景及常用算法详解\n马尔科夫、最大熵、条件随机场\n理解马尔可夫决策过程\n马尔科夫链（Markov Chain），机器学习和人工智能的基石\n了解机器学习回归的3种最常见的损失函数\n机器学习 欧式距离及代码实现\n支持向量机(SVM)的约束和无约束优化、理论和实现\n机器学习简介之基础理论- 线性回归、逻辑回归、神经网络\n机器学习之单变量线性回归\n线性代数投影法在线性回归中的应用\n机器学习基础-数据降维\n机器学习-贝叶斯分类器\n机器学习——线性回归的原理，推导过程，源码，评价\n干货 | 拒当调参师工程师：超参数搜索算法一览\n机器学习（1）特征选择与特征抽取\n机器学习系列 5：特征缩放\n吴恩达的机器学习--矩阵运算\nMIT、浙大等打造AutoML可视化工具：模型自选、超参数自调\n机器学习系列 1：监督学习和无监督学习\n机器学习系列14：偏差与方差\n清华刘洋基于深度学习的机器翻译突破知识整合、可解释和鲁棒性\n批归一化和Dropout不能共存？这篇研究说可以\n算法工程师的必备学习资料，《AI算法工程师手册》正式开源了\n《百面机器学习》笔记-特征工程相关面试题\nk-means聚类算法原理与参数调优详解\n高清图解：神经网络、机器学习、数据科学一网打尽|附PDF\n微软开源可解释机器学习工具包lnterpretML\n苹果、微软等巨头107道机器学习面试题\n一文总览机器学习中各种【熵】的含义及本质\n决策树剪枝策略\n线性模型已退场，XGBoost时代早已来\n为基于树的机器学习模型构建更好的建模数据集的10个小技巧！\n机器不学习：浅显易懂！「高中数学」读懂梯度下降的数学原理\n聚类分析算法\n无监督机器学习中，最常见的聚类算法有哪些？\n专栏 | 机器学习 - 最大似然估计\n机器学习算法集锦：从贝叶斯到深度学习及各自优缺点\n干货|全面理解无监督学习基础知识\n图解梯度下降背后的数学原理\n机器学习：特征选择和降维实例\n基于经典的机器学习k-means聚类算法实现对三通道图片的压缩操作\n概率分布，先懂这6个\n机器学习|最简单易懂的机器学习\n2019年11个值得研究的Javascript机器学习库\n过拟合、欠拟合与正则化\n机器学习的流程是怎样的呢？如何应用到实践中去呢？\n通过随机森林的例子解释特征重要性\n机器学习可解释性工具箱XAI\n一文了解机器学习中的交叉熵\n人工智能科普｜极大似然估计——机器学习重要知识点\n理解马尔可夫决策过程\n透彻理解高斯核函数背后的哲学思想与数学思想\n2019年用于机器学习的50个最佳公共数据集\n透彻理解半监督学习的重要思想及概率视角\n深入剖析机器学习中的统计思想\n利用随机森林来进行特征选择（Python代码实现）\n机器学习基础：理解梯度下降本质「附Python代码」\n干货｜机器学习-稀疏矩阵的处理\n形象理解贝叶斯定理\n机器学习实战第一步：特征选择与特征工程「附代码」\n机器学习入门：一文让你快速了解机器学习\n基于Kubernetes 的机器学习工作流\n产品化机器学习的一些思考\n「AI科技」机器学习算法之K-means算法原理及缺点改进思路\n十分钟掌握多项式回归：非线性预测\n深入理解支持向量机\n分享一些阿里巴巴算法、数据工程师的笔试题以及答案\n揭秘人工智能面试内容：8家国际巨头机器学习面试题目整理\n这份GitHub万星的机器学习算法面试大全请大家注意查收\n2019-06-24 写于苏州市"}
{"content2":"快速了解什么是自然语言处理\n白宁超\n2016年2月29日19:13:23\n摘要：自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学等于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。（本文原创，分享供于学习，转载标明出处：快速了解什么是自然语言处理）\n相关文章\n【文本处理】自然语言处理在现实生活中运用\n【文本处理】多种贝叶斯模型构建及文本分类的实现\n【文本处理】快速了解什么是自然语言处理\n【文本处理】领域本体构建方法概述\n【文本挖掘（1）】OpenNLP：驾驭文本，分词那些事\n【文本挖掘（2）】【NLP】Tika 文本预处理：抽取各种格式文件内容\n【文本挖掘（3）】自己动手搭建搜索工具\n1 计算机对自然语言处理的过程\n1.1把需要研究是问题在语言上建立形式化模型，使其可以数学形式表示出来，这个过程称之为\"形式化\"\n1.2把数学模型表示为算法的过程称之为\"算法化\"\n1.3根据算法，计算机进行实现，建立各种自然语言处理系统，这个过程是\"程序化\"\n1.4对系统进行评测和改进最终满足现实需求，这个过程是\"实用化\"\n2 自然语言处理涉及的知识领域\n语言学、计算机科学（提供模型表示、算法设计、计算机实现）、数学（数学模型）、心理学（人类言语心理模型和理论）、哲学（提供人类思维和语言的更深层次理论）、统计学（提供样本数据的预测统计技术）、电子工程（信息论基础和语言信号处理技术）、生物学（人类言语行为机制理论）。故其为多边缘的交叉学科\n3 自然语言处理涉及的范围\n3.1语音的自动合成与识别、机器翻译、自然语言理解、人机对话、信息检索、文本分类、自动文摘等等，总之分为四大方向：\n语言学方向\n数据处理方向\n人工智能和认知科学方向\n语言工程方向\n3.2也可细分为13个方面\n口语输入：语音识别、信号表示、鲁棒的语音识别、语音识别中的隐马尔科夫模型方法、语言模型、说话人识别、口语理解\n书面语输入：文献格式识别、光学字符识别（OCR）:印刷体识别/手写体识别、手写界面、手写文字分析\n语言分析理解：小于句子单位的处理、语法的形式化、针对基于约束的语法编写的词表、计算语义学、句子建模和剖析技术、鲁棒的剖析技术\n语言生成：句法生成、深层生成\n口语输入技术：合成语音技术、语音合成的文本解释、口语生成\n话语分析与对话：对话建模、话语建模口语对话系统\n文献自动处理：文献检索、文本解释：信息抽取、文本内容自动归纳、文本写作和编辑的计算机支持、工业和企业中使用的受限语言\n多语问题的计算机处理：机器翻译、人助机译、机助人译、多语言信息检索、多语言语音识别、自动语种验证\n多模态的计算机处理：空间和时间表示方法、文本与图像处理、口语与手势的模态结合、口语与面部信息的模态结合：面部运动和语音识别\n信息传输和信息存储：语音压缩、语音品质的提升\n自然语言处理中的数学方法：统计建模和分类的数学理论、数字信号处理技术、剖析算法的数学基础研究、神经网络、有限状态分析技术、语音和语言处理中的最优化技术和搜索技术\n语言资源：书面语料库、口语语料库、机器词典与词网的建设、术语编撰和术语数据库、网络数据挖掘和信息提取\n自然语言处理系统的评测：面向任务的文本分析评测、机器翻译系统和翻译工具的评测、大覆盖面的自然语言剖析器的评测、语音识别：评估和评测、语音合成评测、系统的可用性和界面的评测、语音通信质量的评测、文字识别系统的评测\n4 自然语言处理的发展的几个特点\n基于句法-语义规则的理性主义方法受到质疑，随着语料库建设和语料库语言学 的崛起，大规模真实文本的处理成为自然语言处理的主要战略目标。\n自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。\n统计数学方法越来越受到重视。\n自然语言处理中越来越重视词汇的作用，出现了强烈的\"词汇主义\"的倾向。"}
{"content2":"问题二：人工智能对教育行业的冲击和机遇分析\n作者：梁蓉\n一问：人工智能对教学方式有何优化作用?\n未来学家、发明家雷·库兹韦尔曾说：“不断减轻人类痛苦是技术持续进步的主要动力。”技术发展的愿景和初衷是好的，人工智能也一样，它的研发初衷是为了把人从简单、机械、繁琐的工作中解放出来，然后从事更具创造性的工作。教育人工智能的使命应该是让教师腾出更多地时间和精力，创新教育内容、改革教学方法，让教育这件事变得更好。\n目前人工智能在教育领域的应用技术主要包括图像识别、语音识别、人机交互等。比如通过图像识别技术，人工智能可以将老师从繁重的批改作业和阅卷工作中解放出来;语音识别和语义分析技术可以辅助教师进行英语口试测评，也可以纠正、改进学生的英语发音;而人机交互技术可以协助教师为学生在线答疑解惑，去年媒体曾报道美国佐治亚理工大学的机器人助教代替人类助教与学生在线沟通交流竟无学生发现，说明了人工智能在这方面的应用潜力。\n除此之外，个性化学习、智能学习反馈、机器人远程支教等人工智能的教育应用也被看好。虽然目前人工智能技术在教育中的应用尚处于起步阶段，但随着人工智能技术的进步，未来其在教育领域的应用程度或将加深，应用空间或许会更大。\n二问：人工智能会取代教师吗?\n自人工智能出现起，就存在一种人工智能威胁论，这种观点认为人工智能最终会取代人类，甚至消灭人类。比如著名物理学家史蒂芬·霍金继2015年9月抛出人工智能威胁论后，今年3月份在接受英国《独立报》采访时再次表达了对人工智能技术的担忧。回归到教育领域，假设未来人工智能不仅在教育的技术层面，而且在知识层面应用得越来越成熟，那么人工智能是否会取代人类教师呢?\n对于这种担心，笔者认为可以从教育的本质以及人与机器的区别等角度进行审视。教育的任务是教书育人，教师的作用不仅是传授知识，而且需要通过情感的投入和思想的引导教会学生做人、塑造学生的品质等。对于什么是真正的教育，德国著名哲学家雅斯贝尔斯曾形象地描绘为，用一棵树撼动另一棵树，一朵云推动另一朵云，一颗心灵唤醒另一颗心灵。教育是一项心灵工程，它的实施者——教师是富于情感和智慧、想象力与创造力的人类，这些特质是人工智能无法比拟的。同时我们也看到教师正在努力从教学的主宰者、知识的灌输者向学生的学习伙伴、引导者等方向转变。\n基于此，即使未来人工智能在知识储备量、知识传播速度以及教学讲授手段等方面超越人类，人类教师仍然具有不可替代的作用。但是面对人工智能的冲击，教师应该具备危机意识和改革意识，思考如何发展那些“AI无而人类有”的能力，思考如何提高教师这个角色的不可替代性，思考什么才是真正的教育，思考未来需要培养怎样的人才等问题。只有朝这些方向努力，才能将人工智能带来的挑战转变为变革传统教育、创新未来教育的机遇。\n三问：人工智能对学科设置有何引导作用?\n在科学技术日新月异的今天，世界正在发生快速变化。新时代对学校教育的影响之一是学科设置的变化，包括新学科的设立、传统学科的改良以及某些“过时”学科的撤销。\n回归到人工智能领域，自2003年北京大学提请建立智能科学系，并于2004年招收首批本科生后，至今十多年的时间里，随着人工智能的崛起，越来越多的高校开设此专业，从事有关智能科学发展的研究并培养相关人才。而人工智能是一个多学科交叉的领域，其发展也带动了其他相关专业地位和报考人数的提升。与此同时，人工智能带动了本专业与相关专业发展的同时，也对某些专业发展造成冲击。以外语专业为例，随着人工智能语音技术的发展，外语翻译行业受到挑战。\n跳出教育看教育，未来人工智能将不断取代或变革现有的工作，也会创造很多新的工作，学校教育必须紧跟时代步伐，不断调整学科专业方向与人才培养目标，才能为社会培养、输送有用的人才。\n四问：教育数据会成为教育人工智能的瓶颈吗?\n在教育行业，人工智能不仅被用来节省教师人力、提高教学效率，而且可以驱动教学方式的变革。以人工智能驱动个性化教育为例，收集学生作业、课堂行为、考试等数据，对不同学生的学情进行个性化诊断，并进一步为每个学生制定有针对性的辅导和练习，从而实现因材施教，这已成为教育人工智能探索个性化教育的一个方向。但是实现人工智能引领个性化教学的一个关键点是数据的采集与分析。\n关于数据与人工智能的关系，有一种说法认为数据是人工智能的某种“养料”。同样的，我们也可以说，教育数据是教育人工智能的“养料”。教育数据产生于各种教育活动和整个教学的全过程，人工智能要想更好地应用到教育中，首先面临的就是数据采集的问题。目前，教育数据的来源渠道有两个，一是来源于数字化的教学环境，教学和学习数据在这种数字化环境中自然而然的产生，二是从传统教学行为中收集教育信息，并将之转化为数据。前者的优势是实时收集数据，效率高、节省人力，而现如今在互联网+教育的广度和深度有待进一步推进的情况下，教育数据的来源很大一部分要依靠后者，未来教育数据或将成为发展教育人工智能的一大制衡因素。"}
{"content2":"近些年来，随着大数据、云计算、移动互联网、人工智能技术的兴起，“机器学习”成为了行业内炙手可热的一个名词。从通信互联网领域的专家，到各式各样的企业，甚至到普通的老百姓，都对“机器学习”技术略知一二。那么，机器学习到底是什么，它与我们常见的“人工智能”、“神经网络”、“数据挖掘“等相似概念都有什么关系？机器学习有那些基本分支、基本方法？在本文中，我们将用最简单易懂的语言解释这些问题。\n问题一：“机器学习”和“人工智能”的关系是什么？\n随着“机器学习”火起来的还有一个词语，即“人工智能”。每个人都肯定还记得不久以前的AlaphGo，随着机器打败围棋顶级高手李世石，人们也不得不感叹：“人工智能”时代真正到来了。\n那么，“机器学习”和“人工智能”的关系到底是什么尼？其实，“人工智能”是一个很大的学科领域，里面包含很多子领域，如“机器学习”，“数据挖掘”，“模式识别”，“自然语言处理”等。这些子领域可能有交叉，但侧重点往往不同。比如”机器学习“就比较侧重于算法方面。总的来说，“人工智能”是一个学科领域，是我们研究的最终目的，而”机器学习“是这个领域中比较核心的，比较重要的，侧重于算法的一门学科，可以说，“人工智能”和“机器学习”是包含与被包含的关系。\n问题二：“机器学习”和“神经网络”、“深度学习”的关系是什么？\n最近，“神经网络”、“深度学习”等词大火，很多小伙伴可能就比较疑惑这二者和”机器学习“是什么关系。事实上。机器学习主要是研究各种算法的，经典的机器学习算法有几个大类：回归算法、决策树、贝叶斯算法、支持向量机、神经网络、聚类等等。看到这里大家应该懂了，“神经网络”只是“机器学习”诸多算法中的一种。在机器学习的多种算法中，可能随着时代的变化和技术的应用，在某一个特定的时间段里某一种特定的算法会比其他算法火爆，这也是为什么近几年许多即使不了解”机器学习“的人也对”神经网络“略有知晓的原因。\n“深度学习”其实范围更小，它是“神经网络”算法中的一种。近几年随着计算能力的提高、计算数据量的增长，原本难以实现的“深度学习”算法反而很好地发挥了它的优势。近些年的一些机器学习任务也随着深度学习算法的发展而取得了更好的效果。\n问题三：“机器学习”和“数据挖掘”的关系是什么？\n随着大数据的发展，“数据挖掘”技术也进入了人们的视线。很多企业现在在招聘时，都会有“数据挖掘/机器学习工程师”这样一个岗位。翻看数据挖掘方面的书籍，有很大一部分内容讲的就是机器学习算法。那么，数据挖掘与机器学习又有什么不同尼？\n首先，两者的侧重点有所不同。机器学习主要侧重于理论和算法，更注重于寻找更加高效简洁的算法，而数据挖掘更注重实际应用，比如更注重数据分析的流程、数据存储、算法选择等，可以说，数据挖掘是机器学习的一个实际应用。\n在内容上来说，一般可以认为成：数据挖掘=机器学习+数据处理。也就是说，数据挖掘内容上比机器学习更广泛一点。不过二者还是各有所长。\n以上我们可以说解释了机器学习、数据挖掘领域的初学者很容易混淆的几个问题，这就为机器学习开了个好头，下面我们就可以专门介绍机器学习领域的相关知识。\n什么是机器学习？百度百科对机器学习有这样一个简单的解释：机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n我们姑且不用管百度百科上的定义是否特别准确，至少它把机器学习最基本的内容说出来了。它主要研究一些算法，这些算法可以让计算机模拟人类学习。它涉及到概率论、统计论、凸分析等等数学领域，是人工智能的核心。\n这样看来，机器学习貌似很炫酷——可以让计算机模拟人类学习。有的小伙伴可能会觉得高深莫测，其实，一切模型的基础都是简单的有些愚蠢的。因此不要因为“模拟人类学习”一词就认为需要多么高大上的模式，最简单的机器学习算法基本上就在干两件事情：分类和回归。所谓分类，就是给定一个事物的一些参数，来判断它的类别，举个简单的例子：有一个人，他黑皮肤、卷头发、厚嘴唇，让你判断他来自亚洲、非洲、还是欧洲。很明显，一般人会判断他来自非洲。他一定来自非洲吗？不一定，但是他来自非洲的可能性是最大的。因此，如果将{黑皮肤、卷头发、厚嘴唇}这个属性组输入给计算机，它会自动”学习“并给出判断（他是个非洲人）。这就是一个分类的例子，回归的例子也很简单，一个班的学生按照大小身高排队，排好后如果给了前30个同学的身高值，那么推测第31个同学的身高应该也能推测个八九不离十。这就是分类和回归。其实，虽然人类的学习过程看似复杂，但往往也都是由无数个分类回归构成的。\n一般，我们把机器学习算法分成3个大类，分别是：监督学习、非监督学习、半监督学习。下面我们依次简单介绍。\n监督学习：监督学习就是需要一定的训练数据的学习方式。训练数据就是我们事先测试得到的即包含属性集又包含结果的数据。这种学习往往通过一定训练数据训练出一个“模型”，之后的只有属性集没有结果的数据可以将属性集套入到模型中，并得到其结果。就好比我们之前在脑子中训练出了”一个黑皮肤、卷头发、厚嘴唇“的人是非洲人这一模型后，再给我们一个有这样属性的人，我们自然会得到他是非洲人这样的结果一样。\n监督学习是机器学习领域最庞大的一个分支，有多种迄今为止非常成功的算法族，如常见的决策树算法族、神经网络算法族、支持向量机算法族、贝叶斯算法族等。\n非监督学习：非监督学习和监督学习相对应，是没有训练序列的，这种学习方式需要把数据全部拿来，然后靠“物以类聚，人与群分”。这好比有一群人，根据”肤色、发色“两个属性进行归类，发现有一群人肤色是白的，发色是金的，还有一群人肤色是黄的，发色是黑的，还有一群人肤色是黑的，发色是黑的，不需要训练，我们很自然地就把有不同属性的人归类到了一起。常见的非监督学习主要是聚类算法族。\n半监督学习：半监督学习介于监督学习和非监督学习之间，它有一部分的训练数据，但数量较少。半监督学习通过这部分训练数据实现训练另一部分数据，然后将新训练的数据加入训练数据集中继续训练，这样不断迭代，就会慢慢产生一个庞大的训练数据集。半监督学习在很多领域有重要应用，因为有的时候，并不是很容易得到大量的带标记数据的。常见的半监督学习有半监督SVM，半监督图学习、半监督聚类等。\n最后介绍一下数据分析的基本流程，并看一下机器学习处在什么位置。\n数据分析主要是数据挖掘领域的内容，基本流程包括业务分析、数据初探、数据预处理、建立模型、模型评估、可视化呈现等几步。其中，前3步主要是一些准备工作，用于把得到的数据处理成易于建模的形式，我们的机器学习模型主要就体现在“建立模型”和“模型评估”这两步。在“建立模型”中，我们会根据处理好的训练数据，选择合适的机器学习算法进行建模，在“模型评估”中，我们会用测试数据对刚刚建立好的模型进行一个评估。常用的评估参数有查准率、查全率等，常用的评估方法有自助法、留出法、K折交叉验证法等。"}
{"content2":"虽然人工智能这个词是在20世纪50年代正式发明的，但是人工智能（AI）这个概念可以追溯到古埃及的自动机器和早期的希腊机器人神话。人工智能\n虽然人工智能这个词是在20世纪50年代正式发明的，但是人工智能（AI）这个概念可以追溯到古埃及的自动机器和早期的希腊机器人神话。人工智能领域的名人们试图通过1956年的达特茅斯会议和图灵测试来定义人工智能，而热情的人工智能倡导者们坚持用一种可区分和易于理解的方式向全世界解释这一概念。\n人工智能是一个神秘的、神奇的、似乎有着无穷无尽的可能性的主题。然而，对于普罗大众来说，它仍然是一个难以捉摸的东西，在人们对未来的预测中，它常常被描绘成消极的形象。\n为了对抗好莱坞电影中的人工智能所带来的恐惧的恶性循环，我们需要清楚地明白，人工智能到底是什么。\n如何辨别某个东西是不是AI\n从最广泛的意义上讲，人工智能可能具备人类所有的认知能力，包括学习的能力。一台机器只需要有一分钟的这些技能，就能算作人工智能。人工智能是机器的特点，它通常是表现出智能行为的计算机程序。在这种情况下，“智能”意味着在不同的环境或条件下实现目标的能力。相应地，在计算机科学领域，人工智能领域是指那些设计智能系统的研究。\n基于这一技术定义，人工智能并不需要具备学习的能力。在最极端的情况下，机器中的所有智能行为都可以由程序员通过编写硬代码实现。只要预设的算法能够实现它的目标，机器仍然可以符合人工智能的定义。许多当前的人工智能系统实际上都是基于规则的系统类型，在这种系统中，工程师可以向系统提供其需要的智能。\n此外，机器学习是一门科学，它让机器在没有明确编程的情况下表现出智能行为。具体来说，它为系统提供了从数据中自动学习的能力，并且在没有工程师改变程序代码的情况下进行改进。\n抛开技术层面来谈，你可以说人工智能是目标，机器学习是实现这一目标的途径之一——让机器自己解决问题。在许多情况下，机器学习涉及到使用以前收集的数据学习和改进模型。通过数据，机器可以做出经验驱动的预测或决策。通过不断更新模型，机器将自主学习适应不断变化的环境。\nAI并不是天生就优于人类\n为了弄清人工智能的能力，我们需要解释它能做什么。虽然工程师可以对人工智能进行操作，并提供所有的智能，但机器学习在创建人工智能系统时变得越来越重要。这是因为机器学习承诺在找到未知解决方案的同时，能够减少人工工程的时间，即使是对领域专家来说也是如此。然而，在很多情况下，工程时间只是从直接设计人工智能的时间变为设计一种机器学习算法来学习解决方案本身的时间。人类工程师仍然是不可或缺的。\n乍一看，这是一个完美的解决方案：我们创造了一个能够学习的人工智能，向我们展示如何学习解决一个任务，然后，它就能得出任何相关问题的解决方案，对吧？看起来，像谷歌、微软和苹果这样的大公司是这么认为的：他们利用这种直观的期望来说服人们，他们的人工智能系统将解决许多客户的问题。他们在人工智能领域投入巨资，并做出重大承诺。\n在过去的十年中，学习系统已经完美地解决了对象识别、语音识别、语音合成、语言翻译、图像创作和游戏玩法等方面的问题。这些算法的能力被宣传为具有突破性的功能，确实是这样。在机器学习中，没有深厚技术背景的人通常会认为机器在执行诸如此类的特殊任务方面的改进，只需要组合不同的人工智能。这并非完全正确。\n每一天，算法都是在学习如何解决新任务，并在其他方面变得更好。谷歌旗下DeepMind的AlphaGo打败了李世石，后者是世界上最好的围棋选手之一。了解到这一点后，一位有工程背景的客户说，“我们现在有了一个通用人工智能，它已经学会了在围棋上超越人类——那么它肯定能优化汽车排气系统的设计。”然而，这种推理是基于这样的假设：一旦机器学习算法被开发出来解决一个问题，同样的算法就可以很容易地应用于解决一个不同的问题。\n但事实并非如此。\n在现实中，上述的每一个突破都是通过高度专业化的机器学习算法实现的，这些算法是地球上那些最聪明的人花了很多年的时间开发出来的。他们的设计和调整都是为了解决他们的具体任务——只是这一个任务。\n有一些基本的方法，比如深度学习，可以在不同的应用领域重复应用。然而，对于大多数应用程序来说，需要结合不同的机器学习方法。生成的机器学习系统需要根据特定应用程序的数据进行调整，需要调整训练算法，以找到高性能的解决方案。每一步都需要一个机器学习专家（通常不止一个），辅之以软件工程师和领域专家才能完成。\n它需要一支军队\nAlphaGo是一项耗时多年的项目的成果，该项目至少有17人参与其中，其中有几人在各自的机器学习领域都处于领先地位。据第三方消息来源称，AlphaGo在与Sedol的游戏中使用了1920个CPU和280个GPU。\n大型人工智能公司都拥有数个世界知名的机器学习专家团队，并与软件工程师进行合作。在许多情况下，每个团队都专注于一个特定的应用领域，目标是研究增量方法，以改进当前最好的机器学习方法。\n现代人工智能更像是一个软体动物，而不是无所不能的机器，生物学为今天的人工智能提供了灵感。生物学家研究了一种机制，即动物在经历过一些能够改变某种环境特定意义的事情后，会改变对特定环境的反应方式。用一个词来形容就是——学习。\n常见的研究对象是海兔（即软体动物或海蛞蝓）：具体来说，科学家研究的是决定神经元如何燃烧的基因。根据他们的基因结构，两种海兔对相同的体验（即数据）做出了不同的反应。现在，机器学习的运行大致是这个水平——专家们修改了学习算法的程序代码（类似于海兔的基因代码），改变了它的能力和适应各种体验的倾向。\n机器学习的发展状态可能更接近于无脊椎动物，比如海兔，而不是哺乳动物或人类的高级认知能力。\n在过去的两年里，研究人员开始开发机器学习技术，以适应新的任务。然而，方法 论还只是刚刚起步。用一位DeepMind科学家的话说，“最近关于内存、探索、组合表示和处理架构的研究为我们提供了乐观的理由。”换句话说，我们有理由相信，实现更广泛的人工智能的目标是可行的。\n这里的核心信息是：你不能简单地将原始数据倒进通用的人工智能中，并期待有意义的东西出现——这种人工智能还不存在。此外，如果你提供一个坚如磐石的问题定义，机器只能学习合适的解决方案。对于一个成功的故事，你需要良好的计划，一个数学上合理的问题陈述，足够的训练数据，大量的机器学习知识和软件开发能力。"}
{"content2":"当下，人工智能正如火如荼地发展，人们都享受着深度学习带来的生产力的提升，以至于很多人都将深度学习看做普适人工智能的方向，同时，受益于开源深度学习编程平台如TensorFlow，pytorch等的便宜，很多人忘记或者根本没有认识到理论的重要性，都将注意力集中于设计网络结构，设计loss。最优传输是数学领域的经典分支，最优传输与神经网络有天然的相似性，最优传输理论的研究也许能为深度学习理论研究打开一扇窗，或者至少能指导一些深度学习的发展。\n3月初，为了找到一个方向，某（借鉴周志华老师微博自称）特意预约了徐老师面谈，鉴于之前半年一直在学习强化学习，徐老师让某阅读强化学习文献200篇，并完成一个综述。之后四天，某打印了很多资料，但都还未看。指导徐老师再次叫某面谈，让某换到最优传输理论，这是某第一次听说最优传输。彼时至今，已经过去3个月，某也意识到应该写一些东西，作为对知识的总结。\n最优传输在国外研究比较早，尤其是法国。在机器学习领域，由于计算复杂度原因，今几年才开始兴盛。但在国内，最优传输的研究据某所知寥寥无几，华人团队顾险峰老师团队做的比较好，但毕竟是在美国。至于网上的资料，中文的更是少之又少，博客方面某就只看到顾老师的海天盛宴系列，但理论性太强，主要集中于计算几何学。本博客系列将聚焦于最优传输在机器学习和图像领域的应用，某将尽可能用非数学的语言为大家介绍最优传输理论及其近几年的发展。"}
{"content2":"这是《人工智能系列笔记》的第二篇，我利用周六下午完成课程学习。这一方面是因为内容属于入门级，并且之前我已经对认知服务和机器人框架比较熟悉。\n如有兴趣，请关注该系列 https://aka.ms/learningAI\n但是学习这门课程还是很有收获，这篇笔记时特别加了\"探秘\"两个字，这是因为他不仅仅是介绍了微软的认知服务和机器人框架及其如何快速开始工作，更重要的是也做了很多铺垫，例如在讲文本分析服务（Text Analytics）之前，课程用了相当长的篇幅介绍了文本处理的一些技术原理，毕竟无论是微软的认知服务，还是其他厂商的服务，或者你自己尝试去实现，其内部的原理都是类似的。\n我将给大家分享三个部分的内容\n文本理解和沟通\n计算机视觉\n对话机器人\n第一部分：文本理解和沟通\n现在人工智能很火，花样也很多，可能大家不会想到，很早之前人类对于机器智能的研究，最主要就是在文本理解和处理这个部分，科学家们想要实现的场景主要如下\n这跟人类本身的学习及成长是类似的，一旦机器掌握这些能力，其实就相当于具备了\"听说读写\"的能力。我据说微软二十年前创立研究院之处，主要的研究范围也是在这个领域，二十年过去了还在继续投资，不断优化这方面的能力，可见其作为人工智能的重要性。\n其实这里提到的大部分过程，可以理解为通常意义上的自然语言处理（Natual Language Processing——NLP）的研究范畴。\n本次课程中使用python进行讲解，提到了一个关键的package：NLTK（Natual Language Toolkit），以及它的几个更加具体的库：freqdist 用来做字（词）频分析，stem用来做词干提取等等。\n下面是一些基本的用法\n也就是说，其实你用NLTK能做出绝大部分文本理解和处理的场景，当然如果你用微软的认知服务（Cognitive Service），则可以省去很多基础性的工作，而是直接专注在业务问题上。\n前面三种服务都相对简单，通常你只需要开通，并且调用相关的API 即可，例如 Text Analytics 可用来检测文本语言，识别其中的实体，关键信息，以及情感分析。\n而Language understanding 则相对更加复杂一点，它的全称是Language understanding intelligence service （Luis），是有一套完整的定义、训练、发布的流程。换言之，Luis允许你自定义模型，而前面三者则是利用微软已经训练好的模型立即开始工作。申请Luis服务是在Azure的门户中完成的，而要进行模型定义和训练，则需要通过 https://luis.ai 这个网站来完成。\n下面是我用来测试的一个模型的其中一个Intent （Luis能同时支持多种语言，甚至也能做到中英文混合文本的理解）\nLuis最大的一个使用场合可能是结合本文最后面提到的对话机器人来实现智能问答。\n第二部分：计算机视觉\n如果说文本智能是尝试学习人类的\"听说读写\"的能力，那么计算机视觉则是尝试模拟人类的眼睛，来实现\"看\"的能力。\n图像分析其实就是好比人类看到一个物体（或者其影像），脑电波反射过来信号，使得你意识到你看到的是什么。\n这个能力用到了预先训练好的模型。这个可以通过认知服务中的Computer Vision这个组件实现。\n但是，即便是上面的模型已经包含了数以百万计的照片，但相对而言还是很小的一个集合。所以，如果你想实现自己的图像识别，可以使用认知服务中提供的Custom vision这个能力来实现。\nCustom vision拥有一个同样很酷的主页：https://customvision.ai/ ，通过这个网站，你可以上传你预先收集好的照片，并且为其进行标记，通常情况下，每个标记至少需要5张照片，然后通过训练即可发布你的服务，并且用于后续的图像识别检测（例如某个图像是不是汽车，或者香蕉之类的）。\n人脸识别，则是特定领域的图像识别，这个应用也是目前在人工智能领域最火的一个，而也因为脸是如此重要，所以在认知服务中，有一个专门的API，叫Face API。\n使用这套API，可以做出来很有意思的应用，例如\n从技术上说，图像（Image）是由一个一个有颜色的数据点构成的，这些数据点通常用RGB值表示。而视频（Video）则是由一幅一幅的图像（Image，此时称为帧）构成的。所以，计算机视觉既然能做到图像的识别和理解（虽然可能会有偏差），那么从技术上说，它也就具备了对视频进行识别和理解的能力，如果再加上之前提到的文本智能，它就能至少实现如下的场景：\n识别视频中出现的人脸，以及他们出现的时间轴。如果是名人，也会自动识别出来，如果不是，支持标记，下次也能识别出来。\n识别视频中的情感，例如从人脸看出来的高兴还是悲伤，以及欢呼声等环境音。\n文本识别（OCR）——根据图像生成文字。\n自动生成字幕，并支持翻译成其他语言。\n第三部分：对话机器人\n我记得是在2016年的Build大会上，微软CEO Sayta 提出了一个新的概念：Conversation as a Platform, 简称CaaP，其具体的表现形式就是聊天机器人（chatbot）。\n当时的报道，请参考 https://www.businessinsider.sg/microsoft-ceo-satya-nadella-on-conversations-as-a-platform-and-chatbots-2016-3/?r=US&IR=T\n对话机器人这个单元，讲的就是这块内容。与人脸识别技术类似，机器人这个技术在这几年得到了长足的发展和广泛的应用，甚至到了妇孺皆知的地步。这里谈到的机器人，特指通过对话形式与用户进行交互，并且提供服务的一类机器人，广泛地应用于智能客服、聊天与陪伴、常见问题解答等场合。\n创建一个对话机器人真的很简单，如果你有一个Azure订阅的话。微软在早些时候已经将机器人框架（Bot Framework）完全地整合到了Azure平台。\n做一个机器人（Bot）其实真的不难，但要真的实现比较智能的体验，还真的要下一番功夫。目前比较常见的做法是，前端用Bot Framework定义和开发Bot（用来与用户交互），后台会连接Luis服务或QnA maker服务来实现智能体验，如下图所示。\n我在11月份的Microsoft 365 DevDays（开发者大会）上面专门讲解了机器人开发，有兴趣可以参考 https://github.com/chenxizhang/devdays2018-beijing 的资料。\n机器人框架 （Bot Framework）的一个强大之处在于，你可以实现编写一次，处处运行，它通过频道（Channel）来分发服务。目前支持的频道至少有16种。\n我自己之前用过Web Chat，Microsoft Teams，以及Direct Line和Skype for Business等四种。一直对Cortana这个场景比较感兴趣，这次通过学习，终于把这个做成功了，还是挺有意思的。\n这项功能，还有一个名称：Cortana Skills，目前需要用Microsoft Account注册这个Bot）。\n请通过 https://aka.ms/learningAI 或者扫描下面的二维码关注本系列文章《人工智能学习笔记》"}
{"content2":"《机器学习》 --周志华版（西瓜书）--课后参考答案\n对机器学习一直很感兴趣，也曾阅读过李航老师的《统计学习导论》和Springer的《统计学习导论-基于R应用》等相关书籍，但总感觉自己缺乏深入的理解和系统的实践。最近从实验室角落觅得南京大学周志华老师《机器学习》一书，随意翻看之间便被本书内容文笔深深吸引，如获至宝。遂决定要认真学习本书并将学习过程记录下来。感觉若以读书笔记的形式做记录未免枯燥（且网上已有大量相关内容）。那就暂且将课后练习的个人思路和答案写成系列博客，希望得到朋友们的交流指导。\n从刚开始学习机器学习到现在也有几个月了，期间看过PDF，上过MOOC，总感觉知道一点了又不是特别明白，最后趁某东买书大减价弄了几本相关的书来看看，其中一本就是西瓜书。一口气看了前10章，感觉每章内容都很少，看完感觉还是和以前一样。每章的习题都只是挑了几个简单的看看，没做认真的分析，现在回过头认真做做每章的习题。别的不说了，希望可以坚持到全部做完。\n- 目录\n第一章 绪论\nhttp://blog.csdn.net/icefire_tyh/article/details/52065224\n第二章 模型评估与选择\nhttp://blog.csdn.net/icefire_tyh/article/details/52065867\n第三章 线性模型\nhttp://blog.csdn.net/icefire_tyh/article/details/52069025\n第四章 决策树\nhttp://blog.csdn.net/icefire_tyh/article/details/52082054\n[2016.12.20]决策树代码太乱，近期重写[2017.1.16][完成]\n第五章 神经网络\nhttp://blog.csdn.net/icefire_tyh/article/details/52106899\n[2016.12.20]缺第6，10题，第10题近期补上\n第六章 支持向量机\nhttp://blog.csdn.net/icefire_tyh/article/details/52135662\n缺第10题，暂不补\n第七章 贝叶斯分类器\nhttp://blog.csdn.net/icefire_tyh/article/details/52167273\n缺第9，10题，暂不补\n第八章 集成学习\nhttp://blog.csdn.net/icefire_tyh/article/details/52194771\n缺第9题，暂不补\n第九章 聚类\nhttp://blog.csdn.net/icefire_tyh/article/details/52224676\n缺第8题，暂不补\n第十章 降纬与度量学习\nhttp://blog.csdn.net/icefire_tyh/article/details/52243773\n第十一章 特征选择与稀疏学习\nhttp://blog.csdn.net/icefire_tyh/article/details/52254580\n第十二章 计算理论学习\nhttp://blog.csdn.net/icefire_tyh/article/details/52270432\n第十三章 半监督学习\nhttp://blog.csdn.net/icefire_tyh/article/details/52345060\n第十四章 概率图模型\nhttp://blog.csdn.net/icefire_tyh/article/details/53509879\n缺第10题，暂不补\n第十五章 规则学习\nhttp://blog.csdn.net/icefire_tyh/article/details/53691563\n缺第3，5题，代码缺6，10，暂不补\n第十六章 强化学习\nhttp://blog.csdn.net/icefire_tyh/article/details/53691569\n缺第9，10题，暂不补\n目录：\n周志华《机器学习》课后习题解答系列（二）：Ch1 - 绪论\n周志华《机器学习》课后习题解答系列（三）：Ch2 - 模型评估与选择\n周志华《机器学习》课后习题解答系列（四）：Ch3 - 线性模型\n周志华《机器学习》课后习题解答系列（四）：Ch3.3 - 编程实现对率回归\n周志华《机器学习》课后习题解答系列（四）：Ch3.4 - 交叉验证法练习\n周志华《机器学习》课后习题解答系列（四）：Ch3.5 - 编程实现线性判别分析\n周志华《机器学习》课后习题解答系列（五）：Ch4 - 决策树\n周志华《机器学习》课后习题解答系列（五）：Ch4.3 - 编程实现ID3算法\n周志华《机器学习》课后习题解答系列（五）：Ch4.4 - 编程实现CART算法与剪枝操作\n周志华《机器学习》课后习题解答系列（六）：Ch5 - 神经网络\n周志华《机器学习》课后习题解答系列（六）：Ch5.5 - BP算法实现\n周志华《机器学习》课后习题解答系列（六）：Ch5.6 - BP算法改进\n周志华《机器学习》课后习题解答系列（六）：Ch5.7 - RBF网络实验\n周志华《机器学习》课后习题解答系列（六）：Ch5.8 - SOM网络实验\n周志华《机器学习》课后习题解答系列（六）：Ch5.10 - 卷积神经网络实验\n参考：http://blog.csdn.net/snoopy_yuan/article/details/62045353\n课程代码：https://github.com/Tsingke/Machine-Learning_ZhouZhihua"}
{"content2":"1、什么是独热码\n独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制，更加详细参加one_hot code（维基百科）。在机器学习中对于离散型的分类型的数据，需要对其进行数字化比如说性别这一属性，只能有男性或者女性或者其他这三种值，如何对这三个值进行数字化表达？一种简单的方式就是男性为0，女性为1，其他为2，这样做有什么问题？\n使用上面简单的序列对分类值进行表示后，进行模型训练时可能会产生一个问题就是特征的因为数字值得不同影响模型的训练效果，在模型训练的过程中不同的值使得同一特征在样本中的权重可能发生变化，假如直接编码成1000，是不是比编码成1对模型的的影响更大。为了解决上述的问题，使训练过程中不受到因为分类值表示的问题对模型产生的负面影响，引入独热码对分类型的特征进行独热码编码。\n2、编码过程\n假如只有一个特征是离散值：\n{sex：{male， female，other}}\n该特征总共有3个不同的分类值，此时需要3个bit位表示该特征是什么值，对应bit位为1的位置对应原来的特征的值（一般情况下可以将原始的特征的取值进行排序，以便于后期使用），此时得到独热码为{100}男性 ，{010}女性，{001}其他\n假如多个特征需要独热码编码，那么久按照上面的方法依次将每个特征的独热码拼接起来：\n{sex：{male， female，other}}\n{grade：{一年级， 二年级，三年级， 四年级}}\n此时对于输入为{sex：male； grade： 四年级}进行独热编码，可以首先将sex按照上面的进行编码得到{100}，然后按照grade进行编码为{0001}，那么两者连接起来得到最后的独热码{1000001}；\n3、独热码的函数库\n使用python的话scikit_learn中就封装了现成的编码函数，以下为代码示例：\nfrom sklearn import preprocessing enc = preprocessing.OneHotEncoder() enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) enc.transform([[0, 1, 3]]).toarray()\none_hot encoding\n输出结果：\narray([[ 1., 0., 0., 1., 0., 0., 0., 0., 1.]])\nspark中也有相应的函数不做介绍自行百度或者google。"}
{"content2":"发表这篇文章只是记录自己的机器学习的历程,以及自己理解的一些学习方法及步骤,希望可以帮助一些想要学习机器学习的朋友\n1.首先我们先来有个概念,大致的了解自己想要学些什么,那么就看看机器学习的理论框架吧\n2.理解了要学什么东西,是不是迫不及待想要实际上手了,不急不急,还要搭建一下环境\n系统环境:win10,python3.7\n代码编辑:Jupyter Notebook,这是一款非常好用的代码编辑器,像个记事本一样(我发觉每个用python的人都会用Jupyter Notebook,那咱也学学)\n下载Jupyter Notebook很好下,直接用pip下载安装\npip install jupyter\n出现Successfully installed jupyter-*****等表示安装好啦\n接下来运行一下吧!\n在命令提示符中输入jupyter notebook,回车就可以了\n这时就可以用jupyter notebook进行代码编辑了\n接下来在我们创建好的页面中完成一个\"Hello World\"吧\nprint(\"Hello world\")\n给你的这个页面重命名\n那么进行到这里了,还要再努力一下,我们还要装一些python的第三方库\n这里会遇到两个问题,由于是windows系统,所以Numpy + MKL和Scipy这两个安装文件无法通过pip直接安装\n我给个百度云下载链接吧\n链接：https://pan.baidu.com/s/1mCQ8D-rpJImlL_ufCY9YIQ 提取码：bwwd\n为了防止被屏蔽,图片也贴出来了\nok,接下来下载好我们会看到这两个文件\n为了方便,在D盘创建个文件夹,叫package吧,那么把这两个文件复制到这个文件夹下,打开命令提示图输入以下命令\nD: cd package pip install numpy-1.15.4+mkl-cp37-cp37m-win_amd64.whl pip install scipy-1.2.1-cp37-cp37m-win_amd64.whl\n安装好这两个库,接下来的库就好安装了,直接在命令提示符中输入\npip install matplotlib ipython pandas scikit-learn\n注意:win10的话记得用管理员身份运行命令提示符呦,否则可能会出现拒绝访问的提示\n到这里,基本的环境就搭建好了,如果有疑问,估计就是这些第三方库的作用了吧,那咱们来一起看看\nNumpy : 它是python中基础的用于科学计算的库,功能很强大,包括高维数组的计算,线性代数的计算,傅里叶变换以及产生一些伪随机数等\nScipy : 它时python中用于科学计算的工具集,比如计算统计学分布,信号处理,计算线性代数方程等\npandas : 它是python中用于数据分析的库,可以生成类似Excel表格式的数据表,而且可以对数据表操作,也可以在数据库中提取数据.\nmatplotlib : 它是python的绘图库,可以描绘出折线图,散点图,直方图等\nscikit-learn : 它非常非常重要,接下来的算法完全是在它的基础上运行的,是建立在Scipy基础上的用于机器学习的python模块.\nok,接下来,让我们一起进入机器学习的世界的世界吧!!"}
{"content2":"一、前言\n人工智能时代，开发一款自己的智能问答机器人，一方面提升自己的AI能力，另一方面作为转型AI的实战练习。在此把学习过程记录下来，算是自己的笔记。\n二、正文\n2.1 下载pyaiml\n下载pyaiml\n2.2 安装\npip install aiml\n安装aiml\n2.3 查看\n安装完成后，查看包信息，pip show\n查看aiml包信息\n三、源码\n3.1 智能机器人测试程序\n主程序\n3.2 配置文件\n配置文件\n3.3 AIML问答库\n问答库文件\n四、演示效果\n五、未完待续\n本文是21天实战人工智能系列《知识图谱完整案例剖析》中的一部分，敬请持续关注！"}
{"content2":"声明：本篇博文是学习《机器学习实战》一书的方式路程，系原创，若转载请标明来源。\n1 贝叶斯定理的引入\n概率论中的经典条件概率公式：\n公式的理解为，P（X ，Y）= P（Y，X）<=> P（X | Y）P（Y）= P（Y | X）P （X），即 X 和 Y 同时发生的概率与 Y 和 X 同时发生的概率一样。\n2 朴素贝叶斯定理\n朴素贝叶斯的经典应用是对垃圾邮件的过滤，是对文本格式的数据进行处理，因此这里以此为背景讲解朴素贝叶斯定理。设Ｄ 是训练样本和相关联的类标号的集合，其中训练样本的属性集为          X { X1,X2, ... , Xn }, 共有n 个属性；类标号为 C{ C1,C2, ... ,Cm }, 有m 中类别。朴素贝叶斯定理：\n其中，P（Ci | X）为后验概率，P（Ci）为先验概率，P（X | Ci）为条件概率。朴素贝叶斯的两个假设：1、属性之间相互独立。2、每个属性同等重要。通过假设1 知，条件概率P（X | Ci）可以简化为：\n3 朴素贝叶斯算法\n朴素贝叶斯算法的核心思想：选择具有最高后验概率作为确定类别的指标。下面是以过滤有侮辱性的评论为例，介绍朴素贝叶斯利用Python 语言实现的过程，其本质是利用词和类别的联合概率来预测给定文档属于某个类别。\n4 使用Python对文本分类\n4.1 建立文本数据\n文本数据用一个个对象组成，一个对象是由若干单词组成，每个对象对应一个确定的类别。\n代码如下：\n1 # 文本数据集 2 def loadDataList(): 3 postingList = [ 4 ['my','dog','has','flea','problems','help','please'], 5 ['maybe','not','take','him','to','dog','park','stupid'], 6 ['my','dalmation','is','so','cute','I','love','him'], 7 ['stop','posting','stupid','worthless','garbage'], 8 ['mr','licks','ate','my','steak','how','to','stop','him'], 9 ['quit','buying','worthless','dog','food','stupid']] 10 classVec = [0,1,0,1,0,1] 11 return postingList ,classVec\n4.2 对文本数据的处理\n从文本数据中提取出训练样本的属性集，这里是属性集是由单词组成的词汇集。\n代码如下：\n1 # 提取训练集的所有词 2 def createVocabList(dataSet): 3 vocabSet = set([]) 4 for document in dataSet : 5 vocabSet = vocabSet | set(document) # 两个集合的并集 6 return list(vocabSet)\n这里利用集合的性质对数据集提取不同的单词，函数 createVocabList() 返回值是列表类型。\n4.3 对词汇集转化成数值类型\n因为单词的字符串类型无法参与到数值的计算，因此把一个对象的数据由词汇集中的哪些单词组成表示成：0 该对象没有这个词，1 该对象有这个词。\n代码如下：\n1 # 根据类别对词进行划分数值型的类别 2 def setOfWords2Vec(vocabList, inputSet): 3 returnVec = [0]*len(vocabList) 4 for word in inputSet: 5 if word in vocabList: 6 returnVec[vocabList.index(word)] = 1 7 else : 8 print \"the word : %s is not in my Vocabulary!\" % word 9 return returnVec\n参数 vocabList 是词汇集，inputSet 是对象的数据，而返回值是由词汇集的转换成 0 和 1 组成的对象单词在词汇集的标记。\n4.4 朴素贝叶斯分类器的训练函数\n这里说明一下，训练样本是postingList 列表数据，属性集是词汇集，类标号是classVec 列表数据。在编写代码时考虑到对象的单词在词汇集中占有率比较低，会造成词汇集转化时有大量的 0 组成，同时又会造成条件概率大量为 0 ；又有计算真实概率值普遍偏小，容易造成下溢出。因此，代码对计算条件概率时进行转换，但不影响条件概率的大小排序，也就不会影响朴素贝叶斯的使用。\n代码如下：\n1 ''' 2 求贝叶斯公式中的先验概率 pAbusive ,条件概率 p0Vect、p1Vect；函数中所求的概率值 3 是变形值，不影响贝叶斯的核心思想：选择具有最高概率的决策 4 ''' 5 def trainNB0(trainMatrix, trainCategory): 6 numTrainDocs = len(trainMatrix) # 样本中对象的个数 7 numWords = len(trainMatrix[0]) # 样本中所有词的集合个数 8 pAbusive = sum(trainCategory) / float(numTrainDocs) # 对类别只有两种的先验概率计算 9 # 对所有词在不同的类别下出现次数的初始化为1，为了防止计算条件概率出现为0 10 p0Num = ones(numWords) 11 p1Num = ones(numWords) 12 # 对不同类别出现次数的初始化为2，词的出现数初始数为1的情况下，增加分母值避免概率值大于1 13 p0Denom = 2.0 14 p1Denom = 2.0 15 for i in range(numTrainDocs): # 遍历所有对象 16 if trainCategory[i] == 1: # 类别类型的判断 17 p1Num += trainMatrix[i] # 对所有词在不同的类别下出现次数的计算 18 p1Denom += sum(trainMatrix[i]) # 对不同类别出现次数的计算 19 else: 20 p0Num += trainMatrix[i] # 对所有词在不同的类别下出现次数的计算 21 p0Denom += sum(trainMatrix[i]) # 对不同类别出现次数的计算 22 p1Vect = log ( p1Num / p1Denom) # 条件概率,用对数的形式计算是为避免概率值太小造成下溢出 23 p0Vect = log (p0Num / p0Denom) # 条件概率,用对数的形式计算是为避免概率值太小造成下溢出 24 return p0Vect, p1Vect, pAbusive\n4.5 朴素贝叶斯的分类函数\n根据先验概率和条件概率对不同类别的后验概率进行计算，并选取后验概率最大的类别作为朴素贝叶斯预测结果值。\n代码如下：\n1 # 计算后验概率，并选择最高概率作为预测类别 2 def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): 3 p1 = sum(vec2Classify * p1Vec ) + log(pClass1) # 对未知对象的单词的每一项的条件概率相加（对数相加为条件概率的相乘） 4 p0 = sum(vec2Classify * p0Vec ) + log(1.0-pClass1 ) # 后面加上的一项是先验概率 5 if p1 > p0: 6 return 1 7 else : 8 return 0\n4.6 测试样本的预测\n通过朴素贝叶斯算法给出两个未知类别的对象预测其类别。\n代码如下：\n1 # 对侮辱性语言的测试 2 def testingNB(): 3 listOposts, listClasses = loadDataList() # 训练样本的数据，listOposts 为样本，listClasses 为样本的类别 4 myVocabList = createVocabList(listOposts ) # 样本的词汇集 5 trainMat = [] # 对样本的所有对象相关的单词转化为数值 6 for postinDoc in listOposts : 7 trainMat.append(setOfWords2Vec(myVocabList ,postinDoc ) ) 8 p0V, p1V, pAb = trainNB0(array(trainMat),array(listClasses)) # 样本的先验概率和条件概率 9 10 testEntry = ['love','my','dalmation','love'] # 未知类别的对象 11 thisDoc = array(setOfWords2Vec(myVocabList ,testEntry ) ) # 对未知对象的单词转化为数值 12 print testEntry ,'classified as : ',classifyNB(thisDoc, p0V,p1V,pAb) # 对未知对象的预测其类别 13 14 testEntry = ['stupid','garbage'] # 未知类别的对象 15 thisDoc = array(setOfWords2Vec(myVocabList ,testEntry )) # 对未知对象的单词转化为数值 16 print testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb) # 对未知对象的预测其类别\n其运行结果图：\n对象 ['love','my','dalmation','love'] 由直观可知，其类别是非侮辱性词汇，与预测结果（0 代表正常语言）相同；对象 ['stupid','garbage'] 类别是侮辱性词汇，与预测结果（1 代表侮辱性语言）相同，说明朴素贝叶斯算法对预测类别有效。\n5 例子：对垃圾邮件的识别\n这里给出朴素贝叶斯算法最经典的应用实例，对垃圾邮件的过滤识别。由于邮件是以文件的形式保存，因此我们要对邮件的内容进行提取并处理成符合算法可用的类型。\n5.1 邮件文件解析\n利用正则语言对邮件的内容进行单词的划分。\n代码如下：\n1 # 邮件文件解析 2 def textParse(bigString): 3 import re 4 listOfTokens = re.split(r'\\w*', bigString) # 利用正则语言对邮件文本进行解析 5 return [tok.lower() for tok in listOfTokens if len(tok) > 2] # 限定单词的字母大于2\n第5 行代码解释： lower() 方法转换字符串中所有大写字符为小写\n5.2 垃圾邮件测试函数\n代码如下\n1 # 完整的垃圾邮件测试函数 2 def spamTest(): 3 docList=[];classList = []; fullText = [] 4 for i in range(1,26): 5 wordList = textParse(open('email/spam/%d.txt' %i ).read()) 6 docList.append(wordList) # 把解析后的邮件作为训练样本 7 fullText.extend(wordList) 8 classList.append(1) # 邮件所对应的类别 9 wordList = textParse(open('email/ham/%d.txt' % i).read()) 10 docList.append(wordList) # 把解析后的邮件作为训练样本 11 fullText.extend(wordList ) 12 classList .append(0) # 邮件所对应的类别 13 vocabList = createVocabList(docList) # 样本生成的词汇集 14 # 随机产生十个测试样本和四十个训练样本 15 trainingSet = range(50);testSet = [] 16 for i in range(10): 17 randIndex = int (random.uniform(0,len(trainingSet ))) 18 testSet.append(trainingSet [randIndex ]) 19 del[trainingSet[randIndex]] 20 # 对训练样本进行词的转化成数值类型 21 trainMat = [] 22 trainClasses = [] 23 for docIndex in trainingSet : 24 trainMat.append(setOfWords2Vec(vocabList, docList [docIndex ]) ) 25 trainClasses.append(classList[docIndex ]) 26 p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses)) # 训练样本的先验概率及条件概率 27 errorCount = 0 # 测试样本的出错数初始化 28 for docIndex in testSet: 29 wordVector = setOfWords2Vec(vocabList ,docList[docIndex ]) # 测试对象的词的数值转化 30 if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex ]: # 预测的类别与真实类别的对比 31 errorCount += 1 32 print 'the error rate is : ', float (errorCount )/ len(testSet) # 测试样本的出错率\n第17 行代码解释：\nuniform() 函数是在random模块里，将随机生成下一个实数，它在 [x, y) 范围内。\nx -- 随机数的最小值，包含该值。\ny -- 随机数的最大值，不包含该值。\n返回值是一个浮点数\n运行结果图\n结果显示测试集的出错比例是10%，由于训练集是随机组合的，因此每次运行的结果会有所不同。在《机器学习实战》一书中给出这个算法的错误率在6%左右，说明朴素贝叶斯算法在严苛的条件下也有较好的效果。严苛条件是指我们对属性都是独立的，这在现实中很难找到符合这样的条件。对垃圾邮件的过滤也是不例外的，如bacon(培根) 出现在unhealthy （不健康的）后面与出现在delicious（美味的）后面的概率是不同的，bacon（培根）常常与delicious （美味的）搭配。\n附 完整代码\n# -*- coding:utf-8 -*- from numpy import * # 文本数据集 def loadDataList(): postingList = [ ['my','dog','has','flea','problems','help','please'], ['maybe','not','take','him','to','dog','park','stupid'], ['my','dalmation','is','so','cute','I','love','him'], ['stop','posting','stupid','worthless','garbage'], ['mr','licks','ate','my','steak','how','to','stop','him'], ['quit','buying','worthless','dog','food','stupid']] classVec = [0,1,0,1,0,1] return postingList ,classVec # 提取训练集中的所有词 def createVocabList(dataSet): vocabSet = set([]) for document in dataSet : vocabSet = vocabSet | set(document) # 两个集合的并集 return list(vocabSet) # 根据类别对词进行划分数值型的类别 def setOfWords2Vec(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else : print \"the word : %s is not in my Vocabulary!\" % word return returnVec # 文档词袋模型，可以对重复的单词计数 def bagOfWords2Vec(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 return returnVec ''' 求贝叶斯公式中的先验概率 pAbusive ,条件概率 p0Vect、p1Vect；函数中所求的概率值 是变形值，不影响贝叶斯的核心思想：选择具有最高概率的决策 ''' def trainNB0(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) # 样本中对象的个数 numWords = len(trainMatrix[0]) # 样本中所有词的集合个数 pAbusive = sum(trainCategory) / float(numTrainDocs) # 对类别只有两种的先验概率计算 # 对所有词在不同的类别下出现次数的初始化为1，为了防止计算条件概率出现为0 p0Num = ones(numWords) p1Num = ones(numWords) # 对不同类别出现次数的初始化为2，词的出现数初始数为1的情况下，增加分母值避免概率值大于1 p0Denom = 2.0 p1Denom = 2.0 for i in range(numTrainDocs): # 遍历所有对象 if trainCategory[i] == 1: # 类别类型的判断 p1Num += trainMatrix[i] # 对所有词在不同的类别下出现次数的计算 p1Denom += sum(trainMatrix[i]) # 对不同类别出现次数的计算 else: p0Num += trainMatrix[i] # 对所有词在不同的类别下出现次数的计算 p0Denom += sum(trainMatrix[i]) # 对不同类别出现次数的计算 p1Vect = log ( p1Num / p1Denom) # 条件概率,用对数的形式计算是为避免概率值太小造成下溢出 p0Vect = log (p0Num / p0Denom) # 条件概率,用对数的形式计算是为避免概率值太小造成下溢出 return p0Vect, p1Vect, pAbusive # 计算后验概率，并选择最高概率作为预测类别 def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): p1 = sum(vec2Classify * p1Vec ) + log(pClass1) # 对未知对象的单词的每一项的条件概率相加（对数相加为条件概率的相乘） p0 = sum(vec2Classify * p0Vec ) + log(1.0-pClass1 ) # 后面加上的一项是先验概率 if p1 > p0: return 1 else : return 0 # 对侮辱性语言的测试 def testingNB(): listOposts, listClasses = loadDataList() # 训练样本的数据，listOposts 为样本，listClasses 为样本的类别 myVocabList = createVocabList(listOposts ) # 样本的词汇集 trainMat = [] # 对样本的所有对象相关的单词转化为数值 for postinDoc in listOposts : trainMat.append(setOfWords2Vec(myVocabList ,postinDoc ) ) p0V, p1V, pAb = trainNB0(array(trainMat),array(listClasses)) # 样本的先验概率和条件概率 testEntry = ['love','my','dalmation','love'] # 未知类别的对象 thisDoc = array(setOfWords2Vec(myVocabList ,testEntry ) ) # 对未知对象的单词转化为数值 print testEntry ,'classified as : ',classifyNB(thisDoc, p0V,p1V,pAb) # 对未知对象的预测其类别 testEntry = ['stupid','garbage'] # 未知类别的对象 thisDoc = array(setOfWords2Vec(myVocabList ,testEntry )) # 对未知对象的单词转化为数值 print testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb) # 对未知对象的预测其类别 # 邮件文件解析 def textParse(bigString): import re listOfTokens = re.split(r'\\w*', bigString) # 利用正则语言对邮件文本进行解析 return [tok.lower() for tok in listOfTokens if len(tok) > 2] # 限定单词的字母大于2 # 完整的垃圾邮件测试函数 def spamTest(): docList=[];classList = []; fullText = [] for i in range(1,26): wordList = textParse(open('email/spam/%d.txt' %i ).read()) docList.append(wordList) # 把解析后的邮件作为训练样本 fullText.extend(wordList) classList.append(1) # 邮件所对应的类别 wordList = textParse(open('email/ham/%d.txt' % i).read()) docList.append(wordList) # 把解析后的邮件作为训练样本 fullText.extend(wordList ) classList .append(0) # 邮件所对应的类别 vocabList = createVocabList(docList) # 样本生成的词汇集 # 随机产生十个测试样本和四十个训练样本 trainingSet = range(50);testSet = [] for i in range(10): randIndex = int (random.uniform(0,len(trainingSet ))) testSet.append(trainingSet [randIndex ]) del[trainingSet[randIndex]] # 对训练样本进行词的转化成数值类型 trainMat = [] trainClasses = [] for docIndex in trainingSet : trainMat.append(setOfWords2Vec(vocabList, docList [docIndex ]) ) trainClasses.append(classList[docIndex ]) p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses)) # 训练样本的先验概率及条件概率 errorCount = 0 # 测试样本的出错数初始化 for docIndex in testSet: wordVector = setOfWords2Vec(vocabList ,docList[docIndex ]) # 测试对象的词的数值转化 if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex ]: # 预测的类别与真实类别的对比 errorCount += 1 print 'the error rate is : ', float (errorCount )/ len(testSet) # 测试样本的出错率 if __name__ == '__main__': #testingNB() # 对侮辱性评价的测试 spamTest() # 对垃圾邮件的测试\n完整代码"}
{"content2":"为什么不去读顶级会议上的论文？适应于机器学习、计算机视觉和人工智能?\n看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？\n我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。\n可以从以下几点说明：\n（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。\n（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。\n（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n(1)\n以下是不完整的列表，但基本覆盖。\n机器学习顶级会议：NIPS, ICML, UAI, AISTATS; （期刊：JMLR, ML, Trends in ML, IEEE T-NN）\n计算机视觉和图像识别：ICCV, CVPR, ECCV; （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）\n人工智能：IJCAI, AAAI; （期刊AI）\n另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。\n特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。\n(2)\n另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/; JMLR(期刊): http://jmlr.csail.mit.edu/papers/; COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。\n(3)\n说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。\n对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。"}
{"content2":"问题状况表现1\n这个问题一般是你 的什么配置影响了虚拟机的网卡网关设置！！！。\n问题状况表现2\n这个问题一般是你 的什么配置影响了虚拟机的网卡网关设置。\n解决办法\n网上的那些解决方案，我都试过，比如。\nhttp://blog.csdn.net/fengasdfgh/article/details/60135290\n什么，关闭防火墙、重启ssh服务等，都不行。\n给大家，附上这种方法\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"微软Build 2016开发者大会在美国旧金山的莫斯康展览中心开幕。本次大会对一些重点功能进行了完善。如手写笔支持技术Windows Ink、语音识别Cortana应用集（Cortana Collection）、生物识别技术Windows Hello支持Edge浏览器以及将通用应用平台（UWP）扩大到Xbox游戏主机等。此外，Win10周年升级包适用于PC、平板、手机、Xbox One、Hololens以及物联网（IOT）设备。一直以来Build大会都作为微软的生态发展和未来规划的风向标被外界广泛关注。本次大会在完善细节方面为开发者展现了更多的机会。\n小编熬夜观看Bulid2016大会，看完还是略有些失望。\n大会即将开始，11:30分准时直播开始\n播放主题视频，Satya Nedella上台开始本次Build大会。\n再次强调Mobile First，Cloud First的开发理念\nTerry开始介绍 Windows的各种进步——我们如何带来自然的用户界面，比如触控、笔迹、语音、甚至图像识别。\n他说到：“Windows 10已经正式推出8个月了，活跃用户数已经达到了2.7亿。硬件合作伙伴们已经推出了超过500款专为Windows 10打造的新设备，我们对这样的回应深为感动”\nWindow Anniversary update, 另一个重大更新！\n增强安全机制，演示指纹登录\n增强Ink，保持自然的手写体验\n语音和图像识别\nRoper在上台之后，在一台Surface的触屏上点击了“Login with Windows Hello”，刷脸登录的过程相当快。此外，他还重点讲述了名为“Windows Ink”的新功能，重点突出Surface产品和手写笔之间的交互\nInk在Office系列中做出增强\n话题回到Universal Windows Platform，提到Windows 10将很快迎来新款星巴克、Facebook、Instagram、以及Messenger应用，Facebook的Audience Network也将登陆Windows 10。\n示例展示UWP应用和开发过程\n发布Visual Studio 2015 Update 2，同时还有Anniversary Preview SDK\nBash将会原生集成到系统中，因此，开源的原生Shell应用可以无缝的迁移到windows上来\nDesktop App Converter，将桌面应用转换成Universal Windows App\nXamarin开发跨平台应用，UWP实现全平台支持\n其中，UWP 即Windows通用应用平台，在运行 Windows10（以下简称Win10）的台式机、平板电脑、笔记本电脑、手机、Xbox、HoloLens（3D全息眼镜）等平台上，只需要 “编写一次代码，即可在各平台发布”；而Xamarin 跨平台开发框架，可让开发者使用 C# 和 Visual Studio 或 Xamarin IDE 编写原生的 Android，iOS，Mac 应用。目前已有一些产品不但支持UWP平台还支持Xamarin跨平台开发框架，其中值得一提的是 ComponentOne Studio Enterprise，它是一款专注于企业应用的.NET全功能控件套包，支持WinForms、WPF、UWP、Xamarin、ASP.NET MVC等多个平台，为企业应用开发提供高性能的控件工具。\nGame增强，DX12，Xbox助力UWP，将Windows 10打造成最好的游戏平台\n将游戏转换成UWP App，在Windows 10上执行，快速部署到各种设备，演示部署到Xbox上\nCortana支持Xbox\nHoloLens，正式交付，正式面向开发者，虚拟现实有力竞争\n演示HoloLens在医学教学上的应用\nSatya对UWP做总结，提到机器学习和人工智能，技术更新的目标，更好的服务生活\nConversation as a Service\n我们可以教会计算机去学习和理解人类语言,我们为之注入的智能，其原则性方法也是相当重要的.三大核心定律——增强人类能力和经验、可信赖、包容&尊重。\nCortana，更加智能，全平台，提供开发支持，支持集成第三方应用。\nSkype，集成Skype translate，集成Cortana\nSkype机器人，提供开发支持。希望所有开发者参与这些工作，为Cortana注入专业技能，打造面向每一个商业应用程序和服务的新款应用程序中的机器人\nSkype for HoloLens，看起来很高科技的感觉\n发布Microsoft Bot Framework，轻松实现基于规则的自然语言分析理解，基于机器学习和深度挖掘更好的自然语言理解\nCornelia Carapcea上台讲述认知服务（cognitive services）,她表示“初步有22个API供你调用”，全都是机器学习API，开发者们可以轻松插入自己的应用程序中\n纳德拉上台，通过视频为我们介绍了一位为之感动的特殊开发者。微软雇员Saqib在7岁时失去了视力，但他在与志趣相同的工程师携手之后，开发出了一款能够检测出周围物体的智能机应用。\nSaqib上台站在了纳德拉的身旁，现场响起了雷鸣般的掌声。纳德拉：“他让我们见到了激情与同情，他将要改变这个世界，更重要的是，他也将在这里现身说教”。"}
{"content2":"从今年四月份到现在已经工作快9个月了，最开始是做推荐系统，然后做机器学习，现在是文本挖掘，每个部分研究的时间都不多，但还是遇到了很多问题，目前就把一定要总结的问题总结一下，以后有时间多看看，提醒自己看有没有解决。\n推荐系统：\n1.冷启动热启动区别和联系？各个阶段需要的算法？\n2.每个算法的数学推导、适用情况、优缺点、改进方法、数据类型？\n3.如何平衡热启动时的准确率和召回率，两者不可能同时高，怎么平衡？从算法本身还是业务层面？惊喜度怎么添加？\n4.如何评价推荐系统的好坏？指标是啥？\n机器学习：\n1.能解决哪几类问题？（分类聚类回归预测？）每一类型会有哪些算法？\n2.每个算法优缺点各是什么？各能解决什么问题？侧重点是什么？对数据的平衡性要求大吗？对初始值敏感吗？需要的数据类型是什么？（数值 or 类别？或者混合使用？）\n3.每个算法是如何推导的？如果要调优要从哪些步骤着手？目前的局限是什么？（背后的数学依据）各个算法之间的联系和区别是啥？各算法之间可以结合吗？瓶颈和局限是什么？\n4.python的scikit-learn包是不是都熟悉了，源码有没有看过？自己尝试把每个算法写一下，看看和scikit-learn包跑起来有没有区别？精确度是否会提高？\n5.每个算法的评价指标是什么？（精确度召回度f1-score还有别的吗？）可视化有哪些方法？（ROC曲线？目前只知道这个，还有其他的吗？）\n文本挖掘\n1.基本步骤是啥？（清洗数据（缺失值、噪音数据、平滑处理）--->中文分词（各种方法）--->特征提取（tfidf还有其他几种方法） --->特征选择（卡方互信息发IG法等等）--->用机器学习算法跑）有没有漏的？或者有没有哪些步骤还有补充的可以提高精度的？\n2.各个步骤之间各有哪种方法？每种方法区别和联系？数学推导是什么？有没有可以改进的地方？有没有新的方法可以自己造的？\n3.数据编码转码不容忽视，不要忘记“不可见字符”（windows--->linux系统时候容易出现的问题）\n这是工作到现在觉得必须要解决的问题，以后有新的发现再补充。关于这些问题的解决，不定期的在博客里发出来，不断修改，不断添加，总之，学习是个不断迭代的过程，fighting！：）"}
{"content2":"第一部分 基础语言\npandax视频教程\n链接: https://pan.baidu.com/s/1pLqavVX 密码: fath\npython入门到精通\n链接: https://pan.baidu.com/s/1mhVNIkC 密码: cvp3\n第二部分 数据篇\n链接: https://pan.baidu.com/s/1pLK25zP 密码: qtuu\n第三部分 机器学习部分\n吴恩达机器学习\n链接: https://pan.baidu.com/s/1i5QKxiX 密码: wcx9\n机器学习与量化交易项目\n链接: https://pan.baidu.com/s/1qY6nsBi 密码: p3t2\n林轩田：机器学习基石\n链接: https://pan.baidu.com/s/1nvuimnR 密码: b8zn\n林轩田：机器学习技巧\n链接: https://pan.baidu.com/s/1jHO0njw 密码: 246v\nNg视频课程和讲义\n链接: https://pan.baidu.com/s/1i48TjGp 密码: hnnn\n机器学习\n链接: https://pan.baidu.com/s/1qXP9Sao 密码: ri67\n龙星计划 机器学习课程（余凯&&张潼 ）\n链接: https://pan.baidu.com/s/1qYubZVA 密码: by8j\nScikit Learn 机器学习\n链接: https://pan.baidu.com/s/1ccTE7k 密码: s3he\n第四部分 深度学习\nUdacity 深度学习\n链接: https://pan.baidu.com/s/1miOCuKw 密码: h8kg\nhinton 深度学习视频课程\n链接: https://pan.baidu.com/s/1c11BO56 密码: 34vw\ndeep learning for NLP\n链接: https://pan.baidu.com/s/1jIoiD3S 密码: 736d\n第五部分 机器学习书籍\n《机器学习》 Tom Mitchell；虽然是很老的书，但是入门还是非常经典的。《统计学习方法》李航\n《机器学习（西瓜书)》周志华\n《机器学习实践》\n《Python机器学习预测分析核心算法》\n《图解机器学习》\n第五部分 深度学习书籍\n《神经网络与深度学习》\n《TensorFlow实践Google深度学习框架》\n《深度学习 21天实战Caffe》\n《Python自然语言处理》\n本地文稿已同步至最新状态。"}
{"content2":"人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。\n人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。 2017年12月，人工智能入选“2017年度中国媒体十大流行语”。\n01-Python课程（更新至17年4月）\n02-数学课程\n03-算法课程\n04-深度学习课程\n05-机器学习课程（更新至17年7月）\n06-数据挖掘课程\n07-量化交易课程\n08-NLP课程（更新至17年5月）\n09-计算机视觉课程\n10-Kaggle课程\n11-面试课程\n下载地址：百度网盘下载"}
{"content2":"为什么有今天这篇？\n首先，标题不要太相信，哈哈哈。\n本公众号之前已经就人工智能学习的路径、学习方法、经典学习视频等做过完整说明。但是鉴于每个人的基础不同，可能需要额外的学习资料进行辅助。特此，向大家免费发送近300G的人工智能从基础到实战的全系列视频，有需要的可以领取。注意：视频很多，按需观看。\n教程说明：\n本套教程属于人工智能、机器学习、深度学习、自然语言处理方向的教程，涵盖python基础、python高级教程、大数据、数据分析、数据挖掘、高等数学、概率论统计、算法结构、量化交易、Kaggle、Tenseprflow、自然语言处理、情感分析等。\n形式：源码+课件+视频\n容量：将近300G视频\n总量：7大阶段、23套视频教程\n语言：python为主\n课程目录\n本套教程优势及特点\n一、层次鲜明\n知识带你分类清晰，干练，可选择性学习，自由安排时间，自学无压力，视频标题一目了然\n二、超大容量\n300G超大容量，讲的都是重点，都是干货，精炼含金量高\n三、覆盖全面\n覆盖python基础、大数据应用、机器深度学习、数据分析挖掘、高等数学、概率论、算法框架、自然语言处理等\n01基础必备篇\n本阶段：本节是学习人工智能、机器学习、深度学习的必备基础，学完你将掌握python基础知识、网络编程以及其他python高级知识和项目等\n02阶段高等数学篇\n高等数学、概率统计基础比较差的请认真看。学完本节你将对高等数学以及概率统计有深入的了解，为后面的学习打下坚实基础\n03算法基础提高篇\n本节主要对编程算法基础进行讲解\n04机器学习深度学习篇\n决策树、支持向量机、神经网络、线性回归、梯度下降、量化交易实战、kaggle实战等\n05数据分析与挖掘篇\n全面掌握数据分析的工具、方法、数据处理、数据清洗、数据储存以及数据可视化的案例等等\n06自然语言处理篇\n看完你将搞清楚什么是自然语言处理，聊天机器人的实现原理以及介绍，以及自然语言处理用到的技术和原理\n07面试篇\n名企面试题精讲\n公众号：learningthem，关注即可免费领取"}
{"content2":"链接：oschina.net/news/78629/beginners-how-to-learn-from-zero-artificial-intelligence\n此文是想要进入人工智能这个领域、但不知道从哪里开始的初学者最佳的学习资源列表。\n一、机器学习\n有关机器学习领域的最佳介绍，请观看Coursera的Andrew Ng机器学习课程。 它解释了基本概念，并让你很好地理解最重要的算法。\n有关ML算法的简要概述，查看这个TutsPlus课程“Machine Learning Distilled”。\n“Programming Collective Intelligence”这本书是一个很好的资源，可以学习ML 算法在Python中的实际实现。 它需要你通过许多实践项目，涵盖所有必要的基础。\n这些不错的资源你可能也感兴趣:\nPerer Norvig 的Udacity Course on ML（ML Udacity 课程）\nTom Mitchell 在卡梅隆大学教授的 Another course on ML（另一门ML课程）\nYouTube上的机器学习教程 mathematicalmonk\n二、深度学习\n关于深度学习的最佳介绍，我遇到最好的是 Deep Learning With Python。它不会深入到困难的数学，也没有一个超长列表的先决条件，而是描述了一个简单的方法开始DL，解释如何快速开始构建并学习实践上的一切。它解释了最先进的工具（Keras，TensorFlow），并带你通过几个实际项目，解释如何在所有最好的DL应用程序中实现最先进的结果。\n在Google上也有一个great introductory DL course，还有Sephen Welch的great explanation of neural networks。\n之后，为了更深入地了解，这里还有一些有趣的资源：\nGeoffrey Hinton 的coursera 课程“Neural Networks for Machine Learning”。这门课程会带你了解 ANN 的经典问题——MNIST 字符识别的过程，并将深入解释一切。\nMIT Deep Learning（深度学习）一书。\nUFLDL tutorial by Stanford （斯坦福的 UFLDL 教程）\ndeeplearning.net教程\nMichael Nielsen 的 Neural Networks and Deep Learning（神经网络和深度学习）一书\nSimon O. Haykin 的Neural Networks and Learning Machines （神经网络和机器学习）一书\n三、人工智能\n“Artificial Intelligence: A Modern Approach (AIMA)” （人工智能：现代方法） 是关于“守旧派” AI最好的一本书籍。这本书总体概述了人工智能领域，并解释了你需要了解的所有基本概念。\n来自加州大学伯克利分校的 Artificial Intelligence course（人工智能课程）是一系列优秀的视频讲座，通过一种非常有趣的实践项目（训练AI玩Pacman游戏 ）来解释基本知识。我推荐在视频的同时可以一起阅读AIMA，因为它是基于这本书，并从不同的角度解释了很多类似的概念，使他们更容易理解。它的讲解相对较深，对初学者来说是非常不错的资源。\n大脑如何工作\n如果你对人工智能感兴趣，你可能很想知道人的大脑是怎么工作的，下面的几本书会通过直观有趣的方式来解释最好的现代理论。\nJeff Hawkins 的 On Intelligence（有声读物）\nGödel, Escher, Bach\n我建议通过这两本书入门，它们能很好地向你解释大脑工作的一般理论。\n其他资源：\nRay Kurzweil的 How to Create a Mind （如何创建一个头脑Ray Kurzweil） (有声读物).\nPrinciples of Neural Science （神经科学原理）是我能找到的最好的书，深入NS。 它谈论的是核心科学，神经解剖等。 非常有趣，但也很长 – 我还在读它。\n四、数学\n以下是你开始学习AI需要了解的非常基本的数学概念：\n微积分学\nKhan Academy Calculus videos（可汗学院微积分视频）\nMIT lectures on Multivariable Calculus（MIT关于多变量微积分的讲座）\n线性代数\nKhan Academy Linear Algebra videos（可汗学院线性代数视频）\nMIT linear algebra videos by Gilbert Strang（Gilbert Strang的MIT线性代数视频）\nCoding the Matrix （编码矩阵） – 布朗大学线程代数CS课程\n概率和统计\n可汗学院 Probability（概率）与 Statistics（统计）视频\nedx probability course （edx概率课程）\n五、计算机科学\n要掌握AI，你要熟悉计算机科学和编程。\n如果你刚刚开始，我建议阅读 Dive Into Python 3 （深入Python 3）这本书，你在Python编程中所需要的大部分知识都会提到。\n要更深入地了解计算机编程的本质 – 看这个经典的 MIT course （MIT课程）。这是一门关于lisp和计算机科学的基础的课程，基于 CS -结构和计算机程序的解释中最有影响力的书之一。\n六、其他资源\nMetacademy  – 是你知识的“包管理器”。 你可以使用这个伟大的工具来了解你需要学习不同的ML主题的所有先决条件。\nkaggle  – 机器学习平台"}
{"content2":"判别式模型（discriminative model）\n产生式模型（generative model）\n特点\n寻找不同类别之间的最优分类面，反映的是异类数据之间的差异\n对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度\n区别(假定输入x, 类别标签y)\n估计的是条件概率分布(conditional distribution) : P(y|x)\n估计的是联合概率分布（joint probability distribution: P(x, y),\n联系\n由产生式模型可以得到判别式模型，但由判别式模型得不到产生式模型。\n常见模型\n– logistic regression\n– SVMs\n– traditional neural networks\n– Nearest neighbor\n–Gaussians, Naive Bayes\n–Mixtures of Gaussians, Mixtures of experts, HMMs\n–Sigmoidal belief networks, Bayesian networks\n– Markov random fields\n【摘要】\n- 生成模型：无穷样本==》概率密度模型 = 产生模型==》预测\n- 判别模型：有限样本==》判别函数 = 预测模型==》预测\n【简介】\n简单的说，假设o是观察值，q是模型。\n如果对P(o|q)建模，就是生成模型。其基本思想是首先建立样本的概率密度模型，再利用模型进行推理预测。要求已知样本无穷或尽可能的大限制。\n这种方法一般建立在统计力学和bayes理论的基础之上。\n如果对条件概率(后验概率) P(q|o)建模，就是判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。代表性理论为统计学习理论。\n这两种方法目前交叉较多。\n【判别模型Discriminative Model】——inter-class probabilistic description\n又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)， p(class|context)。\n利用正负例和分类标签，focus在判别模型的边缘分布。目标函数直接对应于分类准确率。\n- 主要特点：\n寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。\n- 优点:\n分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。\n能清晰的分辨出多类或某一类与其他类之间的差异特征\n在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好\n适用于较多类别的识别\n判别模型的性能比生成模型要简单，比较容易学习\n- 缺点：\n不能反映训练数据本身的特性。能力有限，可以告诉你的是1还是2，但没有办法把整个场景描述出来。\nLack elegance of generative: Priors, 结构, 不确定性\nAlternative notions of penalty functions, regularization, 核函数\n黑盒操作: 变量间的关系不清楚，不可视\n- 常见的主要有：\nlogistic regression、    SVMs、    traditional neural networks、    Nearest neighbor、    Conditional random fields(CRF): 目前最新提出的热门模型，从NLP领域产生的，正在向ASR和CV上发展。\n【生成模型Generative Model】——intra-class probabilistic description\n又叫产生式模型。估计的是联合概率分布（joint probability distribution），p(class, context)=p(class|context)*p(context)。\n用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。在机器学习中，或用于直接对数据建模（用概率密度函数对观察到的draw建模），或作为生成条件概率密度函数的中间步骤。通过使用贝叶斯rule可以从生成模型中得到条件分布。\n如果观察到的数据是完全由生成模型所生成的，那么就可以fitting生成模型的参数，从而仅可能的增加数据相似度。但数据很少能由生成模型完全得到，所以比较准确的方式是直接对条件密度函数建模，即使用分类或回归分析。\n与描述模型的不同是，描述模型中所有变量都是直接测量得到。\n- 主要特点：\n一般主要是对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。\n只关注自己的inclass本身（即点左下角区域内的概率），不关心到底 decision boundary在哪。\n- 优点:\n实际上带的信息要比判别模型丰富，\n研究单类问题比判别模型灵活性强\n模型可以通过增量学习得到\n能用于数据不完整（missing data）情况\nmodular construction of composed solutions to complex problems\nprior knowledge can be easily taken into account\nrobust to partial occlusion and viewpoint changes\ncan tolerate significant intra-class variation of object appearance\n- 缺点：\ntend to produce a significant number of false positives. This is particularly true for object classes which share a high visual similarity such as horses and cows\n学习和计算过程比较复杂\n- 常见的主要有：\nGaussians, Naive Bayes, Mixtures of multinomials、    Mixtures of Gaussians, Mixtures of experts, HMMs、    Sigmoidal belief networks, Bayesian networks、    Markov random fields\n所列举的Generative model也可以用disriminative方法来训练，比如GMM或HMM，训练的方法有EBW(Extended Baum Welch),或最近Fei Sha提出的Large   Margin方法。\n【两者之间的关系】\n由生成模型可以得到判别模型，但由判别模型得不到生成模型。\n【总结】\n有时称判别模型求的是条件概率，生成模型求的是联合概率。\n常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、 boosting、条件随机场、神经网络等。\n常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、 LDA、 RestrictedBoltzmann Machine 等"}
{"content2":"机器学习现在是一大热门，研究的人特多，越来越多的新人涌进来。\n不少人其实并没有真正想过，这是不是自己喜欢搞的东西，只不过看见别人都在搞，觉着跟大伙儿走总不会吃亏吧。\n问题是，真有个“大伙儿”吗？就不会是“两伙儿”、“三伙儿”？如果有“几伙儿”，那到底该跟着“哪伙儿”走呢？\n很多人可能没有意识到，所谓的machine learning community，现在至少包含了两个有着完全不同的文化、完全不同的价值观的群体，称为machine learning \"communities\"也许更合适一些。\n第一个community，是把机器学习看作人工智能分支的一个群体，这群人的主体是计算机科学家。\n现在的“机器学习研究者”可能很少有人读过1983年出的“Machine Learning: An Artificial Intelligence Approach”这本书。这本书的出版标志着机器学习成为人工智能中一个独立的领域。它其实是一部集早期机器学习研究之大成的文集，收罗了若干先贤（例 如Herbert Simon，那位把诺贝尔奖、图灵奖以及各种各样和他相关的奖几乎拿遍了的科学天才）的大作，主编是Ryszard S. Michalski（此君已去世多年了，他可算是机器学习的奠基人之一）、Jaime G. Carbonell（此君曾是Springer的LNAI的总编）、Tom Mitchell（此君是CMU机器学习系首任系主任、著名教材的作者，机器学习界没人不知道他吧）。Machine Learning杂志的创刊，正是这群人努力的结果。这本书值得一读。虽然技术手段早就日新月异了，但有一些深刻的思想现在并没有过时。各个学科领域总有 不少东西，换了新装之后又粉墨登场，现在热火朝天的transfer learning，其实就是learning by analogy的升级版。\n人工智能的研究从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，是有一条自然、清晰的脉络。人工智能出身的机器学习研究者，绝大部分 是把机器学习作为实现人工智能的一个途径，正如1983年的书名那样。他们关注的是人工智能中的问题，希望以机器学习为手段，但具体采用什么样的学习手 段，是基于统计的、代数的、还是逻辑的、几何的，他们并不care。\n这群人可能对统计学习目前dominating的地位未必满意。靠统计学习是不可能解决人工智能中大部分问题的，如果统计学习压制了对其他手段的研 究，可能不是好事。这群人往往也不care在文章里show自己的数学水平，甚至可能是以简化表达自己的思想为荣。人工智能问题不是数学问题，甚至未必是 依靠数学能够解决的问题。人工智能中许多事情的难处，往往在于我们不知道困难的本质在哪里，不知道“问题”在哪里。一旦“问题”清楚了，解决起来可能并不 困难。\n第二个community，是把机器学习看作“应用统计学”的一个群体，这群人的主体是统计学家。\n和纯数学相比，统计学不太“干净”，不少数学家甚至拒绝承认统计学是数学。但如果和人工智能相比，统计学就太干净了，统计学研究的问题是清楚的，不象人工智能那样，连问题到底在哪里都不知道。在相当长时间里，统计学家和机器学习一直保持着距离。\n慢慢地，不少统计学家逐渐意识到，统计学本来就该面向应用，而机器学习天生就是一个很好的切入点。因为机器学习虽然用到各种各样的数学，但要分析大 量数据中蕴涵的规律，统计学是必不可少的。统计学出身的机器学习研究者，绝大部分是把机器学习当作应用统计学。他们关注的是如何把统计学中的理论和方法变 成可以在计算机上有效实现的算法，至于这样的算法对人工智能中的什么问题有用，他们并不care。\n这群人可能对人工智能毫无兴趣，在他们眼中，机器学习就是统计学习，是统计学比较偏向应用的一个分支，充其量是统计学与计算机科学的交叉。这群人对统计学习之外的学习手段往往是排斥的，这很自然，基于代数的、逻辑的、几何的学习，很难纳入统计学的范畴。\n两个群体的文化和价值观完全不同。第一个群体认为好的工作，第二个群体可能觉得没有技术含量，但第一个群体可能恰恰认为，简单的才好，正因为很好地 抓住了问题本质，所以问题变得容易解决。第二个群体欣赏的工作，第一个群体可能觉得是故弄玄虚，看不出他想解决什么人工智能问题，根本就不是在搞人工智 能、搞计算机，但别人本来也没说自己是在“搞人工智能”、“搞计算机”，本来就不是在为人工智能做研究。\n两个群体各有其存在的意义，应该宽容一点，不需要去互较什么短长。但是既然顶着Machine Learning这个帽子的不是“一伙儿”，而是“两伙儿”，那么要“跟进”的新人就要谨慎了，先搞清楚自己更喜欢“哪伙儿”。\n引两位著名学者的话结尾，一位是人工智能大奖得主、一位是统计学习大家，名字我不说了，省得惹麻烦：\n“I do not come to AI to do statistics”\n“I do not have interest in AI”\n原文地址：\nhttp://hi.baidu.com/giqguarzqdbadpq/item/8d41e5160121a3ff65eabf8d"}
{"content2":"序号\n会议名称\n会议介绍\n代表领域\n12\nICCV: IEEE International Conference on Computer Vision\n领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇\n计算机视觉，模式识别，多媒体计算\n13\nCVPR: IEEE Conf on Comp Vision and Pattern Recognition\n领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇\n模式识别，计算机视觉，多媒体计算\n14\nECCV: European Conference on Computer Vision\n领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇\n模式识别，计算机视觉，多媒体计算\n16\nICML: International Conference on Machine Learning\n领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少\n机器学习，模式识别\n17\nNIPS: Neural Information Processing Systems\n领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇）\n神经计算，机器学习\n18\nACM MM: ACM Multimedia Conference\n领域顶级国际会议，全文的录取率极低，但Poster比较容易\n多媒体技术，数据压缩\n20\nIEEE ICME: International Conference on Multimedia and Expo\n多媒体领域重要国际会议，一年一次\n多媒体技术\n24\nACL: The Association for Computational Linguistics\n国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次\n计算语言学，自然语言处理\n25\nCOLING: International Conference on Computational Linguistics\n计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次\n计算语言学，自然语言处理\n27\nIJCNLP: International Joint Conference on Natural Language Processing\n自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一次\n自然语言处理\n129\nACL: The Association for Computational Linguistics\n计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。\n人工智能 计算语言学\n130\nACM SIGIR: The ACM Conference on Research and Development in Information Retrieval\n信息检索方面最好的会议, ACM 主办, 每年开。19％左右\n信息检索技术\n131\nACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining\n数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右\n132\nWWW: The ACM International World Wide Web Conference\n应用和媒体领域顶级国际会议\n万维网\n133\nACM SIGMOD: ACM SIGMOD Conf on Management of Data\n数据库领域顶级国际\n数据管理\n134\nCIKM: The ACM Conference on Information and Knowledge Management\n数据库领域知名国际会议\n数据管理\n135\nCOLING: International Conference on Computational Linguistics\n计算语言学知名国际会议\n计算语言学\n136\nICML: International Conference on Machine Learning\n领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少\n机器学习，模式识别\n137\nIEEE ICDM: International Conference on Data Mining\n数据挖掘领域顶级国际会议\n138\nIJCAI: International Joint Conference on Artificial Intelligence\n人工智能领域顶级国际会议，论文接受率18％左右\n人工智能\n139\nVLDB: The ACM International Conference on Very Large Data Bases\n数据库领域顶级国际\n数据库\n142\nAAAI: American Association for Artificial Intelligence\n美国人工智能学会AAAI的年会，使该领域的顶级会议\n人工智能\n145\nACM SIGIR: The ACM Conference on Research and Development in Information Retrieval\n信息检索领域的重要会议\n信息检索\n148\nACM SIGMOD: ACM SIGMOD Conf on Management of Data\n数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。\n数据管理\n151\nCIKM: The ACM Conference on Information and Knowledge Management\n信息检索领域的会议，录用率为15%\n信息检索\n159\nICML: International Conference on Machine Learning\n机器学习领域中的顶级会议\n机器学习\n162\nIEEE ICDM: International Conference on Data Mining\n数据挖掘领域的著名会议，率用率为14%。\n数据挖掘\n167\nIEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval\n字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。\n字符串处理信息检索\n168\nIJCAI: International Joint Conference on AI\n人工智能领域的顶级会议。\n人工智能\n176\nPAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining\n178\nPKDD: Conference on Principles and Practice of Knowledge Discovery in Databases\n数据挖掘领域的重要会议，录用率为14%。\n数据挖掘\n180\nSDM: SIAM International Conference on Data Mining\n数据挖掘领域的重要会议，录用率为14%\n数据挖据\n184\nVLDB: The ACM International Conference on Very Large Data Bases\n数据管理\n185\nWWW: The ACM International World Wide Web Conference\nACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。\nInternet\n186\nRAID International Symposium on Recent Advances in Intrusion Detection\n数据库顶级国际会议\n187\nIJCAI: International Joint Conference on Artificial Intelligence\n人工智能顶级国际会议\n人工智能\n188\nVLDB: The ACM International Conference on Very Large Data Bases\n数据库顶级国际会议\n数据库\n189\nICML: International Conference on Machine Learning\n机器学习顶级国际会议\n机器学习\n190\nPRICAI: Pacific Rim International Conference on Artificial Intelligence\n亚太人工智能国际会议\n人工智能\n191\nIFIP ICIIP: IFIP International Conference on Intelligent Information Processing\nIFIP智能信息处理国际会议\n智能信息处理\n192\nNIPS: Neural Information Processing Systems\n神经信息处理领域顶级国际会议\n神经计算，机器学习\n232\nWWW: The ACM International World Wide Web Conference\nInternet领域顶级国际会议\nInternet\n233\nInternational Semantic Web Conference\nSemantic Web领域顶级会议，录用率17%\nSemantic Web\n234\nACM SIGMOD: ACM SIGMOD Conf on Management of Data\nACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。\n数据管理\n235\nACM PODS Conference\nACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。\n数据管理\n236\nVLDB: The ACM International Conference on Very Large Data Bases\n数据库顶级国际会议\n数据管理\n237\nIEEE ICDE - International Conference on Data Engineering\n数据库顶级国际会议\n数据管理"}
{"content2":"最近，iPhone4s 的个人语音助理Siri很火爆，听说它实现了“人工智能”，一听说这个我很好奇，我之前也曾经是人工智能的粉丝，于是去搜索了下Siri是怎么实现人工智能的，果然这个疑问已经有人再问而且有人回答的很好了：\nSiri属于自主人工智能了吗？还是仅仅输入人工智能、\n觉得文中有段说的很有启发性：\n“但重点来了，iPhone4s出的siri就不一样了，先是他需要联网和A5cpu才能处理的条件下，可以判断出肯定是不同于上者，从必须运行在A5CPU上来看，siri如果是内置大量判断也是有可能的，在这大量的判断中找到符合项目，然后执行，从必须联网来看，siri也许采用云技术的思想，集合大家的预设力量，来做到多样化也可以排解大量预设占的内存大小。”\n这句话的重点内容就是推测，Siri可能使用云技术来帮助分析结果，我觉得也是这个可能，于是和朋友在QQ群里面讨论了一下，下面是聊天记录：\n------------------------------------------------------------\n深蓝医生(45383850)  17:48:23\n做个云平台，来提供人工智能服务？\n潶色(80148096)  17:50:07\n现在的语音识别的软件都是连网的，不联网广放本地数据库就放死人\n潶色(80148096)  17:50:29\nandroid也有个输入法，叫 讯飞的，支持语音，但是必须联网\n深蓝医生(45383850)  17:51:25\n有道理\n深蓝医生(45383850)  17:52:09\n大家用的搜狐输入法等软件，它们已经知道你在说什么了\n潶色(80148096)  17:53:26\n谁？\n深蓝医生(45383850)  17:52:50\n比如所有人都在输入一个词，那它就可以判断这个词是有意义的。\n深蓝医生(45383850)  17:53:04\n这个它，就是“云输入”\n潶色(80148096)  17:54:03\n其实这些云端软件，有2个目的，第一方便你，第二收集你\n深蓝医生(45383850)  17:54:09\n所以，对应苹果的Siri，它能够给出适合人听的句子，因为这些句子，都已经被人大量说过了。\n潶色(80148096)  17:55:30\n收集你后，这些数据是非常值钱的，这就是他们的聪明之处\n深蓝医生(45383850)  17:55:54\n只要使用分词技术，识别出来你说的话的关键词，然后再根据上下文，就能搜索出来“绝大部分人对于这个问题有些什么说法”，随机抽取一条作为答案即可。\n深蓝医生(45383850)  17:57:54\n如果有一个爬虫，不断的抓起Web上的资源，分析其中的内容，很有可能分析出来。\n潶色(80148096)  17:59:31\n嗯嗯\n深蓝医生(45383850)  17:58:54\n听说有一种分词技术就是去抓取网页然后进行大量的文本特征分析，从而知道有哪些词，该怎么样分。\n深蓝医生(45383850)  18:00:19\n这个过程有点像小孩学说话，从大人说的话中，找出一些他认为有用的词，然后不断重复，组合，最后就成为小孩自己的话了。\n深蓝医生(45383850)  18:01:18\n我发现一个惊奇的现象，我儿子很多时间听我们说话，看电视，都在默念重复他听到的。\n深蓝医生(45383850)  18:02:07\n他一定在一边重复的说，一边在想这些句子的含义。\n深蓝医生(45383850)  18:02:55\n假如有一部超级计算机也在进行我儿子这样的行为，你说它会产生智能吗？\n深蓝医生(45383850)  18:04:13\n海量搜索+机器学习=人工智能\n-----------------------------------------------------------------------\n这里，“海量搜索”，很有可能就是现在的“云搜索”，当然不是现在Google等概念上的搜素，比这个更广泛，它如果加上机器学习技术，就能够有效的缩小搜索范围，再使用“云计算”的处理能力，迅速的提供答案也不是不可能的了。\n实际上，这也是全球的计算机在现在高度互联的状态下，信息不断交互、碰撞，如果再加上一点机器学习的机制，那么在这些计算机上，产生一种智能，并非不可能。"}
{"content2":"前言\nAlpha Go在16年以4:1的战绩打败了李世石，17年又以3:0的战绩战胜了中国围棋天才柯洁，这真是科技界振奋人心的进步。伴随着媒体的大量宣传，此事变成了妇孺皆知的大事件。大家又开始激烈的讨论机器人什么时候会取代人类统治世界的问题。\n其实人工智能在上世纪5、60年代就开始进入了理论研究阶段，人们在不断探索人工智能技术的同时，也担忧起机器人会不会替代人类。然而现实比理想残酷的多，由于当时各种条件的限制（理论基础、技术基础、数据基础、硬件性能等），人工智能相关的项目进度缓慢，也缺少实际成效，研发资金、社会关注度也越来越低，人工智能进入第一次低谷期。\n到了80年代，卡内基梅隆大学为数字设备公司设计了一套名为XCON的“专家系统”。这是一种，采用人工智能程序的系统，可以简单的理解为“知识库+推理机”的组合，XCON是一套具有完整专业知识和经验的计算机智能系统。人工智能再一次被各国政府和科研机构看好，大量的资金投入到研发中，但是好景不长，几年后随着苹果和IBM公司研发出了性能强劲的PC机，导致“专家系统”变得没有竞争力，人工智能发展又一次进入寒冬。\n随后若干年，人工智能的发展趋于平稳和低调。时间来到21世纪，随着互联网的普及，大量数据被积累下来；摩尔定律一次又一次的被证实，计算机硬件性能以极快的速度在增长；“云”的普及，让普通大众也能轻松拥有调度大量算力的机会，人工智能不再是科学家和专业人员在实验室才能研究的东西了。数据+算力+易得这几方面的因素结合之后，将人工智能再一次推向了高潮。\n可能这一波热潮又是人工智能发展史上的一个波峰，未来人工智能还有很长的路要走。但目前的人工智能发展已经惠及到商业领域，在这样一种技术+商业的结合中，我个人还是很看好这次浪潮的。尤其是在看过《最强大脑》中，百度在图像、音频方面的人工智能技术发展到这样一个水平之后（图像识别已经超超越了人类大脑对图像的识别能力，声音识别也几乎和人类最高水平持平），很希望自己也可以有机会涉足到这个领域中。\n机器学习基础入门知识\n机器学习是人工智能的一个分支，主要是通过数据+算法来训练得出模型，再用模型来预测数据的一种技术。\n刚开始接触机器学习，发现基础理论中好多都是大学里学过的数理知识（一直以来困扰我的“大学为什么要学这些东西”的谜团总算被解开了：）。我个人做了Web开发近十载，大部分是应用级的，很少涉及数理算法，看来今后还要慢慢拾起这些知识。不过刚开始入门可以循序渐进，先弄懂机器学习是怎么回事，动手做一个“Hello world”，然后再逐步深入原理层面的知识。\n要涉足机器学习，最好会一种编程语言，这点上我们程序员有先天优势。目前用于机器学习的主流语言是Python和R，R我个人还没研究过，个人觉得Python是一个比较好的选择，流行度高、上手难度低、科学计算类库丰富、语法精简，如果本身就有其他面向对象的编程语言基础，不到一周就可以基本掌握Python了。\n机器学习从从业分布来看，可以分成基础算法研究（设计师）和应（ban）用（zhuan）两个领域，其中大部分人都是在应（ban）用（zhuan）这个领域。\n如果从技术层面来看，机器学习分成监督学习、无监督学习以及半监督学习。如何来区分呢？首先解释下机器学习中的几个名词。\n特性（Features） - 其实就是数据\n分类器（Classifier） - 其实就是算法\n标签（Labels） - 其实就是种类\n模型(Models) - 其实就是最终输出的分类公式\n监督学习，就是在有标签的前提下，找到一种最合适的分类器，分析特性和标签之间的关系。\n无监督学习，就是没有标签的前提下，将数据进行聚类(Clusting)。\n半监督学习，就是部分特性有标签，部分则没有的状况（大部分特性可能是没有标签的情况）下进行分类。\n监督学习相对来说最简单，由已知特性和标签，利用合适的分类器训练出模型，再以模型套用到数据中来预测出数据的标签。当然，分类器并不需要我们自己来发明创造，我们大部分人也没这个能力做这些事情，所有的理论研究、科学论证、代码实现都是现成的。Python中有很多相关类库，比如scikit-learn。应用层面的机器学习，其实就是通过不停的调参（收集更多的数据、变换算法、选取合适的特征数据等工作）来找到一种更精准的预测模型的工作。\nHello World In Machine Learning\n假设我们现在需要区分皮球（以直径15cm-25cm之间的球为例）和甜瓜的图片，如果是传统的硬编码的方式来写代码的话，可能需要写几百上千个if-else才能完成一个基本的算法，而且可扩展性特别差，比如如果图片是黑白的或者图片中有干扰物品，那可能需要修改源代码，添加更多的if-else来增加准确度。更糟的是，真正执行的时候会遇到很多事先没有预料到的特殊情况。\n但如果通过机器学习，这个事情可能就会变得很简单。大致步骤如下：\n将图片转换成特征向量（这个进阶知识不在本篇中涉及）\n决定一种合适当前场景的分类器\n结合1中得到的特征和2中得到的分类器训练出模型\n用模型中的公式预测数据，估算出其属于某个标签的可能性，最大可能性的那个即模型推算出的结果\n数据准备\n转换过程略，假设共N条数据，转换得到的特性如下：\n直径（厘米）\n形状\n颜色\n标签\n24\nround\nwhite\nmelon\n35\nellipse\nwhite\nmelon\n24\nround\norange\nball\n24\nellipse\nyellow\nmelon\n22\nround\nyellow\nball\n...\n...\n...\n...\n实现代码\nfeatures = [ [24, 'round', 'white'], [35, 'ellipse', 'white'], [24, 'round', 'orange'], [24, 'ellipse', 'yellow'], [22, 'round', 'yellow'], ... ] labels = ['melon', 'melon', 'ball', 'melon', 'ball']\n我们知道，计算机处理基础数据类型的速度，由快及慢为：bool、int、float、string...，因此，我们在处理数据的过程中，需要把原始数据抽象成计算机能最快处理的数据类型（因为机器学习运算量极大）。因此上面的代码经过转换之后：\n# round:1, ellipse:2 # white:1, orange:2, yellow: 3 features = [[24, 1, 1], [35, 2, 1], [24, 1, 2], [24, 2, 3], [22, 1, 3]] # melon:1, ball: 2 labels = [1, 1, 2, 1, 2]\n这里顺便提一下，大部分机器学习中，都是以GPU的性能来衡量处理速度的，而不是我们一般使用的CPU，这是因为GPU的物理架构和CPU不一样，GPU是专门为了处理图像而设计的，它对浮点数的处理速度是CPU的数十倍乃至数百倍。而机器学习基本上可以看做是对浮点数的大量运算，因此GPU更适合在机器学习领域被使用。\n算法选取\n机器学习中，解决一个问题的算法并不是唯一的，同一个问题可以适用不同的算法来解决，一般都会在效率和准确率之间做权衡。本例中我们使用决策树(Deccision Tree)作为Classifier，关于决策树，可参考https://baike.baidu.com/item/%E5%86%B3%E7%AD%96%E6%A0%91。\n实现代码\nfrom sklearn import tree ... # 实例化classifier clf = tree.DecisionTreeClassifier()\n训练模型\nscikit-learn的classifier中通过方法fit(features, labels)来训练模型。其返回值即我们所需的模型。\n实现代码\n... clf = tree.fit(features, labels) ...\n预测数据\n有了模型，我们就可以对今后的数据进行预测，以得出label值，从而达到对其归类的目的。\n实现代码\n... # 假设现在有一个数据[23, 'round', 'white']，我们想知道他应该数据什么类型，先将其转换为[23, 1, 1], 然后调用模型的predict方法 print(clf.predict([[23, 1, 1]])) ...\n得到的结果为：\n# 代表机器学习测算得出结果是melon [1]\n完整代码\nfrom sklearn import tree # round:1, ellipse:2 # white:1, orange:2, yellow: 3 features = [[24, 1, 1], [35, 2, 1], [24, 1, 2], [24, 2, 3], [22, 1, 3]] # melon:1, ball: 2 labels = [1, 1, 2, 1, 2] # 实例化classifier clf = tree.DecisionTreeClassifier() # 训练 clf = clf.fit(features, labels) print(clf.predict([[23, 1, 1]]))\n后记\n上例中，如果通过真正的人工智能肉眼来看，[23, 'round', 'white']被推算为melon的准确度其实并不高，因为[23, 'round', 'white']归类为ball也完全是可以的。上文提到过，机器学习其实就是不停的寻找合适的数据和算法以提升准确率的过程。想要提升准确率，我们可以有以下思路：\n加大训练样本量（训练样本必须和训练效率做好权衡，另外，最好避免重复的特性浪费算力，比如有了直径这列，就不需要半径、周长这样的特性了，这三者代表的是一个意思）\n变换算法（可以选用更高级的算法或者多个算法组合，但必须在准确度和效率之间做好权衡）\n抽象出更多的特性数据（比如本例中，如果有办法抽象出质量这样的特性，那对于预测准确率会有极大的提升）\n至此为止，我们机器学习的Hello World程序已经完成了，也基本了解了机器学习是怎么回事，是不是还挺有意思的？\n本文在我的博客园和我的个人博客上同步发布，作者保留版权，转载请注明来源。"}
{"content2":"不多说，直接上干货！\n福利 => 每天都推送\n欢迎大家，关注微信扫码并加入我的4个微信公众号：   大数据躺过的坑      Java从入门到架构师      人工智能躺过的坑         Java全栈大联盟\n每天都有大量的学习视频资料和精彩技术文章推送... 人生不易，唯有努力。\n百家号 ：九月哥快讯               快手号：  jiuyuege\n问题详情\n每次提交spark任务到yarn的时候，总会出现uploading resource（打包spark jars并上传）到hdfs上。恶劣情况下，会在这里卡住很久。\n17/01/13 17:21:47 INFO Client: Preparing resources for our AM container 17/01/13 17:21:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploadi ng libraries under SPARK_HOME. 17/01/13 17:21:58 INFO Client: Uploading resource file:/tmp/spark-28ebde0d-c77a-4be3-8248-a6d3bcccc253/__spar k_libs__7542776655448713545.zip -> hdfs://dipperCluster/user/hadoop/.sparkStaging/application_1484215273436_0 050/__spark_libs__7542776655448713545.zip 17/01/13 17:22:08 INFO Client: Uploading resource file:/tmp/spark-28ebde0d-c77a-4be3-8248-a6d3bcccc253/__spar k_conf__8972755978315292177.zip -> hdfs://dipperCluster/user/hadoop/.sparkStaging/application_1484215273436_0 050/__spark_conf__.zip\n其实可以发现，上图中，已经有提示了，说被弃用了。\n解决办法1\n在hdfs上创建目录：\nhdfs dfs -mkdir /home/hadoop/spark_jars\n上传spark的jars（spark1.6 只需要上传spark-assembly-1.6.0-SNAPSHOT-hadoop2.6.0.jar）\nhdfs dfs -put /opt/spark/jars/* /home/hadoop/spark_jars/\n在spark的conf的spark-default.conf ，添加如下的配置\nspark.yarn.jars=hdfs://master:9000/opt/spark/jars/* /home/hadoop/spark_jars/\n即可解决。不会出现这个问题。\n当，再次启动时，则\nSource and destination file systems are the same. Not copying hdfs://master:9000/home/hadoop/spark_jars/zookeeper-3.4.6.jar\n之后快速开始提交任务，启动任务。\n解决办法2\n其实啊，说白了，就是spark2.1.0或spark2.2.0以上的版本的命令有所变化。所以压根可以需改动解决办法1所示的配置，直接用官网这样的命令来操作就可以了。\nhttp://spark.apache.org/docs/latest/running-on-yarn.html\n欢迎大家，加入我的4个微信公众号：    大数据躺过的坑     Java从入门到架构师    人工智能躺过的坑     Java全栈大联盟\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）\n打开百度App，扫码，精彩文章每天更新！欢迎关注我的百家号： 九月哥快讯"}
{"content2":"序言\n最近在Coursera 上学习斯坦福大学的机器学习。根据费曼学习法的理论，教是最好最快最有效果的学习方法。因此，我将会开一系列机器学习相关的文章，同步我的学习进度，并用我自己的理解和语言将我学到的内容写出来。\n为了防止拖延症的发作，我将会严格制定时间表，尽最大可能保证每周一篇文章。\n希望这一系列文章能让我更好的理解机器学习，也希望同时能为大家提供学习机器学习的入门参考。\n第一周\n机器学习定义\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. ——Mitchell provides\n定义说的那么复杂，简单的说，机器学习就是：一个程序，这个程序可以通过一些已知的现象和结果来推测一些未知的现象应该有的结果。\n监督学习和非监督学习\n监督学习： 有答案的学习过程。\n非监督学习： 没有答案的学习过程。\n我们回想一下自己小学时候，做作业，老师站在旁边，我做完一道题，老师看了以后说，答案是正确的。这就是监督学习的过程。\n我们再想想现在，大家在大学或者工作以后，接触到的东西，有时候是没有标准答案的，我们只能凭自己认为是正确的方向去做。做完以后也没有办法知道这样的做法是正确的还是错误的。这就是非监督学习的过程。\n当然，上面两个例子里面，我举例用的“学习”，仅仅是监督学习的“学习”的一小部分。监督学习的“学习”还包含很多内容，包括发现，探索等等。\n回归和分类\n回归和分类属于监督学习。\n回归：回归就是根据输入的数据得到连续输出的过程。\n分类：分类就是根据输入的数据得到离散输出的过程。\n所以回归和分类，本质上是一个过程，或者说算法。也就是我们常说的分类算法或者回归算法。\n这里用来区分回归和分类，主要的指标就是看输出是否连续。对于输入是没有特别的要求的。输入可以是连续的，也可以是离散的，决定这个算法是分类还是回归，由输出来决定。\n我们用房价来举例子，你有一套房子，占地100平米，可以卖多少钱？我告诉你可以卖1000万。你朋友问我他的房子120平米，可以卖多少钱，我告诉他，可以卖1200万。这样你告诉我一个大小，我告诉你可以卖多少钱，这就叫做回归。你给我不同的大小，我就给你不同的价格。\n同一个例子，什么是分类呢？你有一套房子，占地100平米，准备1000万出售，能不能卖出去，我告诉你，不能。你朋友的房子120平米，1200万出售，能不能卖出去，我说可以。这里的结果只有两个，“能卖出去”和不能卖出去，所以这个结果是离散的。\n集群和关联\n集群和关联属于非监督学习。\n集群：根据一些特征来对一大群数据分组。\n关联：已知一些已有的现象和结论，又来了一个类似的现象，把它和某个已知的结论联系起来。\n集群很类似于分类，不同在于是否知道正确的答案。分类是知道答案的，集群是不知道答案的。\n关联和有标准答案的基于规则的联系不同。关联里面，那些已知的现象和结论，都是评估出来的，并不一定正确。比如，看到流鼻涕和感冒，这就属于一种关联，但是流鼻涕并不一定都是感冒。\n一元线性规划\n一元线性规划属于监督学习。\n对于有些事件，导致结果的因素只有一个，于是就可以用一元线性规划来估算出一个一次函数，通过这个函数来确定对于新的输入，应该有什么输出。\n对于学习过初中物理的同学来说，应该记得在处理实验数据的时候，老师说过这样一句话：\n作一条线尽量多的穿过数据点，并让不在线上的点均匀分布在线的两侧。——物理老师。\n如图：\n所谓的一元线性规划，就是做这一条线，或者准确的说，是确定这个一次函数的过程。\n举个例子，按时间计费的宽带上网，上网的费用仅仅由上网的时间决定，上的多就多缴费，上得少就少缴费。知道了上网的时间，就知道了上网的费用。这就是一个一元线性规划的问题。并且可以准确的确定一个一次函数，输入时间，输出的费用就一定是上网的实际花费。\n当然，有一些情况下，这个一次函数只是近似情况，只能保证大多数情况在误差范围内符合。\n写成数学公式，我们用\n来表示，其中θ0和θ1都是参数，输入是x, 输出是y或者hθ(x).\n代价函数\n看起来很复杂的样子，但是大家注意，其实这就是一个方差。只不过多除以了一个2.代价函数就是实际测量值和预测值的误差的方差除以2。\n代价函数是由θ0和θ1为变量的函数，通过计算代价函数求θ0和θ1，并且需要保证求出来的θ0和θ1使得由他们确定的一次函数，在误差范围内，尽可能好的符合已知情况。\n而一元线性规划的过程，就是求代价函数的极小值。这里我说的是极小值而不是最小值，因为极值和最值是不一样的。\n但是值得庆幸的是，对于一元线性规划的代价函数来说，极小值就是最小值，这在数学上是可以证明的。\n梯度下降\n梯度下降是一种计算θ0和θ1的算法。\n大家设想，现在用直升机把你放在了半山腰，然后蒙着你的眼睛，让你下到山谷去，虽然这个任务很困难，但是还是可以做到的。利用脚去感知，发现往左是坡度向上，往右坡度向下，于是你就往右慢慢挪动一小步，然后再重复刚才的动作，发现现在右前方坡度是向下的，于是你又慢慢往右挪动一小步，这样一步一步的移动，最终你会移动到某个山谷里面去。\n梯度下降也是同样的原理，这里的梯度就可以理解成你走的一小步，每一小步就像是下要给台阶。\n但是这一小步你要走多长呢？这个就是一个系数，叫做学习率。如果这个系数设定小了，那走的很慢，但是如果设定大了，比如一步垮了一光年那么远，就会导致丢失目标，甚至是结果发散。\n这就是梯度下降的算法描述了，大家可以根据代价函数的公式，自己确定一下梯度下降的具体算法。\n总结\n这一周讲了机器学习的基本概念以及一元线性规划，同时讲了求解一元线性规划代价函数的梯度下降算法。"}
{"content2":"决策树学习\n从今天开始，坚持每天学习一个机器学习的新知识，加油！\n决策树学习是应用最广的归纳推理算法之一，是一种逼近离散值目标函数的方法，在这种方法中学习到的函数被表示为一颗决策树。\n决策树表示法\n决策树通过把实例从根结点排列到某个叶子结点来分类实例，叶子结点即为实例所属的分类。树上的每一个结点指定了对实例的某个属性的测试，并且该结点的每一个后继分支对应于该属性的一个可能值。分类实例的方法是从这棵树的根节点开始，册数这个结点指定的属性，然后按照给定实例的该属性对应的树枝向下移动，然后这个过程再以新结点为根的子树上重复。\n上图画出了一颗典型的学习到的决策树，这颗决策树根据天气情况分类“星期六上午是否适合打网球”。貌似很多机器学习和数据挖掘的书籍提到这个决策树的时候都是说的这个例子，汗！不过呢，我们还可以根据这颗决策树写出对应的表达式：\n决策树学习的适用问题\n实例是由“属性-值”对（pair）表示的\n目标函数具有离散的输出值\n可能需要析取的描述\n训练数据可以包含错误\n训练数据可以包含缺少属性值的实例\n决策树学习的应用列举\n根据疾病分类患者\n根据起因分类设备故障\n根据拖欠支付的可能性分类贷款申请\n决策树学习的算法ID3\n基本的ID3算法通过自顶向下构造决策树来进行学习。构造过程是从“哪一个属性将从树的根结点被测试？”这个问题开始的。我们使用统计测试来确定每一个实例属性单独分类训练样例的能力。分类能力最好的属性（即信息增益最大的属性）被选作树的根结点的测试。然后为根结点属性的每个可能值产生一个分支，并把训练样例排列到适当的分支之下。然后重复整个过程，用每个分支结点关联的训练样例来选取在该点被测试的最佳属性。这形成了对合格决策树的贪婪搜索，也就是算法从不回溯重新考虑以前的选择。\n熵：表示了任意样例集的纯度。\n假定S为训练集，S的目标属性C有m个可能的类标号值，C={C1,C2,C3…Cm}，每个类标号值相应的概率为p1，p2,p3…pm。那么训练集S的信息熵定义为：Entropy(S)=Entropy(p1,p2,,,,pm)=-(p1*log2(p1)+p2*log2(p2)+pm*log2(pm));\n信息增益：一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低。\n假设训练集为S，并用属性A来划分S，那么属性A的信息增益Gain（S,A）为训练集S的熵减去按属性A划分S后的子集的熵，即Gain（S,A） = Entropy(S) - Entropy_A(S)。\nEntropy_A(S)=abs(Si)/abs(S)Entropy(Si)（Si表示描述属性A的离散值的集合，abs(Si)表示属性A当前这个值的个数）\nID3 算法的优势和不足\n它是关于现有属性的有限离散值函数的一个完整空间。但是当遍历决策树空间时，ID3 仅维护单一的当前假设，这样就失去了表示所有一致假设带来的优势，而且ID3 算法在搜索中不进行回溯，每当在树的某一层次选择了一个属性进行测试，它不会再回溯重新考虑这个选择，所以它易受无回溯的爬山搜索中的常见风险影响：收敛到局部最优的答案，而不是全局最优的。\n决策树学习的归纳偏置\n如果给定一个训练样例的集合，那么通常有很多决策树与这些样例一致。所以，要描述ID3 算法的归纳偏置，应该找到它从所有一致的假设中选择一个的根据。ID3从这些决策树中会选择哪一个呢？它会选择在使用简答到复杂的爬山算法遍历可能的树空间时遇到的第一个可接受的树。总结的说，ID3归纳偏置的搜索策略为：较短的树比较长的树优先；那些信息增益高的属性更靠近根结点的树优先。\n为什么短的假设优先？\n假设物理学家优先选择行星运动简单的解释，而不用复杂的解释，为什么？一种解释是短假设的数量少于长假设的数量，所以找到一个短的但同时与训练数据拟合的假设的可能性较小。相反，常常有很多非常复杂的假设拟合当前的训练数据，但却无法正确地泛化到后来的数据。比如考虑决策树假设，500个结点的决策树比5个结点的决策树多得多，如果给定一个20个训练样例的集合，可以预期能够找到很多500个结点的决策树与训练数据一致，而如果一个5个结点的决策树也可以完美的拟合这些数据当然是出乎意料的。所以我们会相信5个结点的树不太可能是统计巧合，因而优先选择这个5个结点的决策树的假设，而不选择500个结点的。\n处理决策树学习的常见问题\n避免过度拟合数据\n对于一个假设，当存在其他的假设对训练数据样例的拟合比它差，但事实上在实例的整个分布中表现得却更好时，我们说这个假设过度拟合。\n避免决策树学习中的过度拟合的方法被分为两类：\n及早停止树增长，在ID3算法完美分类训练数据之前就停止树增长。\n后修剪法：即允许树过度拟合数据，然后对这个树进行后修剪。\n在实践中证实第二种方法后修剪更加成功的实施准则：\n1:使用与训练样例截然不同的一套分离的样例，来评估通过后修剪方法从树上修剪结点的效用。\n2:使用所有可用数据进行训练，但是进行统计测试来估计扩展（或修剪）一个特定的结点是否有可能改善在训练集合外的实例上的性能。\n3:使用一个明确的标准来衡量训练样例和决策树的复杂度，当这个编码的长度最小时停止树增长。\n合并连续值的属性\n我们最初的ID3定义被限制为取离散值的属性。所以，我们可以先把连续值属性的值域分割为离散的区间集合。例如，对于连续值的属性A，算法可以动态的创建一个新的布尔属性Ac，如果A<c，那么Ac为真，否则为假。这样，就把连续值的属性的值离散化了。\n属性选择的其他度量标准\n有一些极端的例子里，采取信息增益来作为选择树的结点的优先性，有时这棵树虽然可以理想的分类训练数据，但是对于实例的数据的性能非常差，不是一个很好的预测器。所以我们选择了新的度量标准：增益比率。增益比率的计算方法先略过，这个我在后面的总结里会详细的讲解到。\n处理缺少属性值的训练样例\n赋给属性A决策结点n的训练样例中该属性的最常见值。\n为属性A的每个可能值赋予一个概率。\n处理不同代价的属性\n在某些学习任务中，实例的属性可能与代价相关。例如，在学习分类疾病时，我们可能以这些属性来描述患者：体温、活组织切片检查、脉搏、血液化验结果等，这些属性在代价方面差别非常大。对于这样的任务，我们将优先选择尽可能使用低代价属性的决策树，通过引入一个代价项到属性选择度量中，我们可以用信息增益除以属性的代价，这样我们就可以使低代价的属性会被优先选择。仅当需要产生可靠的分类时我们才依赖高代价属性。"}
{"content2":"前几天写了一篇文章 《云时代的.NET》，今天继续这个话题聊下云时代的技能。\n无服务器计算，容器化，云原生应用，DevOps，人工智能，机器学习以及混合云和多云解决方案等IT趋势正在成为主流或“新常态”。所有大小企业都在寻找具有许多热门趋势关键技能的云专家。在无服务器架构中，应用程序被分成各种各样的函数，这些函数由不同类型的事件触发。这改变了我们通常考虑应用程序的方式，以及我们在监视性能，跟踪，扩展和高可用性方面进行操作的方式。熟悉AWS Lambda，Azure Function和事件驱动的架构将是雇主要寻找的技能。\n容器化和云原生应用程序都是使用公有云提供的容器服务（Docker和Kubernetes）部署和运行分布式应用程序和微服务。这种组合提高了生产力和质量，使企业更加灵活，通过自动化管理应用程序生命周期，培养DevOps文化。请注意，您的潜在雇主将寻找熟悉微服务，容器和容器编排引擎（如Kubernetes）的IT专业人员。\n在企业规模上部署人工智能，数据分析和机器学习需要的CPU/GPU和存储，其规模通常是在本地的数据中心所不具备的。这是采用公有云采用的主要驱动因素之一。\n由于这种情况，您必须建立或更新您的技能，以跟上这些无休止的技术和文化变化的脚步，您所采取的路径将取决于您当前的技能和经验。在此演变过程中，您的“现有IT技能将保持不变，但不如新的，以云为中心的技能”。\n关于如何进行此转换没有经验法则，但如果您是开发人员，您可能会专注于开发业务架构师，解决方案架构师或DevOps相关技能。业务架构师规划技术资产和业务流程的演变。解决方案架构师专注于构想和增强专注于特定领域的应用程序，同时为新功能的开发提供监督和指导。\n如果您是测试人员或运营专家，您可能会发展自己创建自动配置，部署或监控脚本，编排流程，使用云服务工具甚至编写云应用程序，开发人员，测试人员和运维角色之间开始变得模糊。\n现在您已经了解了外界正在发生的事情，您可能会问自己，您可以从哪里开始为以云为中心的角色做好准备。请查看以下有用的在线资源列表。\nMicrosoft Azure\nhttps://docs.microsoft.com/zh-cn/learn/azure/\nhttps://www.microsoft.com/zh-cn/learning/azure-exams.aspx\nTencent Cloud\nhttps://cloud.tencent.com/developer/edu\nhttps://cloud.tencent.com/training/cert\nHuaweiCloud\nhttps://edu.huaweicloud.com/\nhttps://edu.huaweicloud.com/training/\nAliyun\nhttps://edu.aliyun.com/academy/VirtualAcademy\nhttps://edu.aliyun.com/certification\nDocker\nhttps://docs.docker.com/get-started/\nKubernetes\nhttps://kubernetes.io/docs/home/\nhttps://kubernetes.feisky.xyz/"}
{"content2":"如果你是商界英才（而不是数据科学家或者机器学习专家），你也许对主流媒体宣传的人工智能（artificial intelligence，AI）已经耳熟能详了。你在杂志上读过相关文章，你看到过特斯拉自动驾驶的煽情文章，听到过史蒂芬•霍金讲述人工智能威胁人类的耸人听闻，甚至迪尔伯特关于人工智能和人类智能的玩笑你都知道。\n此时，胸怀大志要把自己的生意做大做强的你，面对媒体关于人工智能的碎碎念，可能萌生了两个疑问——\n第一，人工智能的商业潜力是真是假？\n第二，这玩意怎么用到我的生意上？\n对第一个问题，答案是：千真万确。今天的商业活动，可以开始应用人工智能来将要求人类智能的活动替换为自动处理以降低成本。人工智能可以允许你将一个需要人海战术的工作通量增加100倍而成本减少90%。\n第二个问题的答案要长一些。首先得消除主流媒体鼓吹导致的误解。一旦误解消除，我们才能为你介绍如何应用人工智能到自己的生意中去。\n误解1：人工智能是魔术\n多数主流媒体将人工智能描述为神奇而神秘的。我们只需为大魔术师般的公司，如Google，Facebook，Apple，Amazon和Microsoft等鼓掌欢呼即可。这样的描述只是在帮倒忙。如果我们想要人工智能应用到商业活动中，至少需要让公司的执行官们理解它。人工智能不是魔术。人工智能是数据、数学、模式和迭代。如果我们想要人工智能应用到商业活动中，我们必须更加透明，并解释清楚人工智能的3个互相连锁的关键概念。\n1.训练数据（TrainingData，TD）——\n训练数据是机器可以用来学习的起始数据集。训练数据有输入值和自带答案的输出值，这样机器学习模型可以从答案中寻找模式。比如，输入可以是客服单，带有客户和公司的客服代表之间的电子邮件。输出可以是基于公司某个分类定义的从1到5的分类标签。\n2.机器学习（MachineLearning，ML）——\n机器学习是软件从训练数据中学习到某种模式，并把它应用到新的输入数据中。比如，一个新的客服单，带有某位客户和某位公司客服代表的邮件来了，机器学习模型可以预测出一个分类，告诉你它对该分类的把握有多大。机器学习的关键特征是，它不是通过固定的规则来学习。因此，当它消化新的数据后，它会调整其规则。\n3.人机回圈（Human-in-the-Loop，HITL）——\n人机回圈是人工智能的第三个核心成分。我们不能指望机器学习万无一失。一个好的机器学习模型大概只有70%的准确性。因此你需要一个人机回圈流程，当模型的可信度低时，还可以依靠人。\n因此，别被人工智能的神话愚弄了。现在，有了人工智能的公式，在此基础上，你可以对人工智能有一个基本的理解了。AI = TD + ML + HITL\n误解2：人工智能是给科技精英用的\n媒体报道似乎暗示，人工智能只是科技精英的菜——只有像Amazon，Apple，Facebook，Google，IBM，Microsoft，Salesforce，Tesla，Uber这些公司能斥上亿美金巨资组建庞大的机器学习专家团队。这个概念是错的。\n今天，十万美元即可在商业过程中开始应用人工智能。因此，如果你的公司是全美营业额在5千万美元以上的26，000家公司之一，你就可以投入营业额的0.2%，来启动人工智能。\n因此，人工智能不只属于高科技公司，它属于任何行业。\n误解3：人工智能只解决亿万美元级的大问题\n主流媒体叙说的故事，通常是未来式的例子，比如无人驾驶汽车，无人机投递包裹。Google，Tesla和Uber这些公司投入了数亿美元争夺无人驾驶汽车领域的领先地位，因为“赢者通吃”的想法在作怪。这样的故事给人工智能打上了“花费亿万美元开拓创新领域”的烙印。但事实并非如此。\n人工智能也可以用几百万美元来解决现有问题。让我解释一下。任何生意的一个核心任务都是了解客户。这在最早的市场——古希腊的阿格拉如此，在古罗马的竞技场里面对面做买卖时如此，在网购盛行的今天也如此。许多公司坐拥非结构化的客户数据宝库，有电子邮件，也有Twitter评论。人工智能可以用于解决客服单分类或者理解推文情感这样的难题。\n因此人工智能不止是为了解决如无人驾驶汽车这样的亿万美元级“让人兴奋”的新问题，它也可以解决百万美元级的现有“无聊”问题，如通过客服单分类或者社交媒体情感分析来了解你的客户。\n误解4：算法比数据更重要\n主流媒体对人工智能的报道偏重于关注机器学习算法，将其视为最重要的部分。主流媒体似乎把算法与人脑等同了。他们隐约传达着这样一个信息：复杂的算法最终会超越人类的大脑并创造奇迹。媒体拿机器在国际象棋和围棋比赛里击败人类的故事作为例子。而且他们主要关注“深度神经网络”和“深度学习”，以及机器是如何做出决策。\n这种报道给人的印象是，一个公司要想应用人工智能就需要聘请机器学习专家来建立完美的算法。但如果一个企业没有思考如何获得高质量的算法，即使机器学习模型经过大量的特定训练数据学习之后，仍然会产生一个与期望（“我们有一个伟大的算法”）不匹配的结果（“我们的模型的准确率只有60%”）。\n现如今，没有计划或训练数据的预算就从微软，亚马逊和谷歌购买商业机器学习的服务，就像买一辆无法接近加油站的车，只是买了一块昂贵的金属。汽车和汽油的类比有些不贴切，因为如果你给机器学习模型的训练数据越多，机器学习模型就会越准确。\n这就像不断给汽车加油，汽车的燃料利用率会不断提高。训练数据对于机器学习模型的重要性比汽油对汽车的重要性更高。如果想深入了解对这类误解性的报道的话，你可以阅读我们以前的帖子《更多的数据击败更好的算法》。\n所以关键就是训练数据的质量和数量至少是和算法一样重要的，要确保你部署人工智能的计划和预算反映这一点。\n误解5:机器>人类\n在过去的30年里，无论是施瓦辛格在《终结者》里扮演的电子人杀手，还是艾丽西亚·维坎德在《机械姬》里扮演的智能机器人伊娃，媒体一直喜欢把人工智能描绘成比人类更强大的机器。媒体想编写一个机器对战人类谁会成为赢家的故事，这是可以理解的。但却歪曲了事实。\n例如，最近谷歌DeepMind 的 alphago战胜韩国棋手李世石的报道被简单地描述成机器战胜人类。这样的表达不是对真实情况的准确描述。更准确的描述是机器加上一群人打败了一个人。\n消除这种误解的主要理由是机器和人的技能是互补的。从上面的图中我们可以看出机器在处理结构化计算方面有优势。机器擅长“找到特征向量”的任务，不太擅长“找到豹纹裙”任务。人类在识别意义和背景上具有得天独厚的优势。人类很容易“找到豹纹裙”，但在“找到特征向量”方面跟机器相比不具有优势。\n因此，正确的框架是要意识到在商业情景下机器和人是互补的，人工智能是人和机器共同工作。\n错误6：人工智能是机器取代人类\n主流媒体为了关注度喜欢描绘一个反乌托邦式的未来，这种情况可能会发生，但这种描述对正确理解人和机器如何共同工作产生了不利的影响。\n例如，让我们再思索下分类支持票据的业务流程。现如今大多数企业都还是百分百人工操作的。结果就是不仅进度缓慢而且成本线性增长，限制了工作量。现在想象一下用模型分类10，000张支持票的准确度是70%。30%的错误是不能接受的，就需要人机回圈的参与。你可以设置可接受的置信阈值为95%并且只接受模型在置信水平不低于95%时的输出。所以最初的机器学习模型可能只做了一小部分的工作，比如说5-10%。但是，随着新的人为标记的数据被创建，并且将其反馈到机器学习模型中，模型会不断学习并提高。随着时间的推移，该模型可以处理越来越多的客户支持票据分类工作，分类票据的业务量可以显著提高。\n因此，人和机器共同协作可以增加业务量，保持质量，减少重要的业务流程的单位成本。\n这就消除了人工智能是机器代替人类的误解。事实是，人工智能是关于机器增强人类的能力。\n错误7：人工智能=机器学习\n主流媒体带给人们的最后一条根深蒂固的误解就是人工智能和机器学习是等同的。这个误解就导致了不切实际的管理期望—从微软，亚马逊或谷歌公司购买商业机器学习的服务就能神奇地将人工智能运用到生产中。\n而除了机器学习之外还需要训练数据和人机回圈才有可能找到可行的人工智能解决方案。\n没有训练数据的机器学习就像一辆没有汽油的汽车。既昂贵又无用。\n没有人机回圈的机器学习是不会有好的产出的。机器学习模型需要人的参与来去除低的置信度预测。\n因此，如果你是一个想把人工智能应用到业务上的执行官，现在你应该对它有一个认识框架了。\n你应该用人工智能的7个真理来代替这7个误解：\n真相1：人工智能=训练数据+机器学习+人机回圈\n真相2：人工智能属于任何行业\n真相3：人工智能可以用几百万美元来解决现有的商业问题\n真相4：算法并没有比训练数据的数量和质量更重要\n真相5：机器和人是互补的\n真相6：人工智能是机器增强人的能力\n真相7：人工智能=训练数据+机器学习+人机回圈"}
{"content2":"作者：育心  知乎\n链接：https://www.zhihu.com/question/57770020/answer/249708509\n人工智能的浪潮正在席卷全球，诸多词汇时刻萦绕在我们耳边：人工智能（Artificial Intelligence）、机器学习（Machine Learning）、深度学习（Deep Learning）。不少人对这些高频词汇的含义及其背后的关系总是似懂非懂、一知半解。\n为了帮助大家更好地理解人工智能，这篇文章用最简单的语言解释了这些词汇的含义，理清它们之间的关系，希望对刚入门的同行有所帮助。\n图一 人工智能的应用\n人工智能：从概念提出到走向繁荣\n1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念，梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言，或被当成技术疯子的狂想扔到垃圾堆里。直到2012年之前，这两种声音还在同时存在。\n2012年以后，得益于数据量的上涨、运算力的提升和机器学习新算法（深度学习）的出现，人工智能开始大爆发。据领英近日发布的《全球AI领域人才报告》显示，截至2017年一季度，基于领英平台的全球AI（人工智能）领域技术人才数量超过190万，仅国内人工智能人才缺口达到500多万。\n人工智能的研究领域也在不断扩大，图二展示了人工智能研究的各个分支，包括专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等。\n图二 人工智能研究分支\n但目前的科研工作都集中在弱人工智能这部分，并很有希望在近期取得重大突破，电影里的人工智能多半都是在描绘强人工智能，而这部分在目前的现实世界里难以真正实现（通常将人工智能分为弱人工智能和强人工智能，前者让机器具备观察和感知的能力，可以做到一定程度的理解和推理，而强人工智能让机器获得自适应能力，解决一些之前没有遇到过的问题）。\n弱人工智能有希望取得突破，是如何实现的，“智能”又从何而来呢？这主要归功于一种实现人工智能的方法——机器学习。\n机器学习：一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。\n机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。\n深度学习：一种实现机器学习的技术\n深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络），因此越来越多的人将其单独看作一种学习的方法。\n最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。其实有不少想法早年间也曾有过，但由于当时训练数据量不足、计算能力落后，因此最终的效果不尽如人意。\n深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n三者的区别和联系\n机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术。我们就用最简单的方法——同心圆，可视化地展现出它们三者的关系。\n图三 三者关系示意图\n目前，业界有一种错误的较为普遍的意识，即“深度学习最终可能会淘汰掉其他所有机器学习算法”。这种意识的产生主要是因为，当下深度学习在计算机视觉、自然语言处理领域的应用远超过传统的机器学习方法，并且媒体对深度学习进行了大肆夸大的报道。\n深度学习，作为目前最热的机器学习方法，但并不意味着是机器学习的终点。起码目前存在以下问题：\n1. 深度学习模型需要大量的训练数据，才能展现出神奇的效果，但现实生活中往往会遇到小样本问题，此时深度学习方法无法入手，传统的机器学习方法就可以处理；\n2. 有些领域，采用传统的简单的机器学习方法，可以很好地解决了，没必要非得用复杂的深度学习方法；\n3. 深度学习的思想，来源于人脑的启发，但绝不是人脑的模拟，举个例子，给一个三四岁的小孩看一辆自行车之后，再见到哪怕外观完全不同的自行车，小孩也十有八九能做出那是一辆自行车的判断，也就是说，人类的学习过程往往不需要大规模的训练数据，而现在的深度学习方法显然不是对人脑的模拟。\n深度学习大佬 Yoshua Bengio 在 Quora 上回答一个类似的问题时，有一段话讲得特别好，这里引用一下，以回答上述问题：\nScience is NOT a battle, it is a collaboration. We all build on each other's ideas. Science is an act of love, not war. Love for the beauty in the world that surrounds us and love to share and build something together. That makes science a highly satisfying activity, emotionally speaking!\n这段话的大致意思是，科学不是战争而是合作，任何学科的发展从来都不是一条路走到黑，而是同行之间互相学习、互相借鉴、博采众长、相得益彰，站在巨人的肩膀上不断前行。机器学习的研究也是一样，你死我活那是邪教，开放包容才是正道。\n结合机器学习2000年以来的发展，再来看Bengio的这段话，深有感触。进入21世纪，纵观机器学习发展历程，研究热点可以简单总结为2000-2006年的流形学习、2006年-2011年的稀疏学习、2012年至今的深度学习。未来哪种机器学习算法会成为热点呢？深度学习三大巨头之一吴恩达曾表示，“在继深度学习之后，迁移学习将引领下一波机器学习技术”。但最终机器学习的下一个热点是什么，谁又能说得准呢。"}
{"content2":"降维在机器学习里面再正常不过了，这里总结了降维的一些方法，主要参考了陈利人老师的“数据分析领域中最为人称道的七种降维方法”（在微信公众号看到的，无法提供链接，有兴趣的可以搜索看原文）。不过这篇文章除了PCA，其他的降维方法多多少少有点特征工程的意思了。\n缺失值比率 (Missing Values Ratio)\n该方法的是基于包含太多缺失值的数据列包含有用信息的可能性较少。因此，可以将数据列缺失值大于某个阈值的列去掉。阈值越高，降维方法更为积极，即降维越少。\n低方差滤波\n与上个方法相似，该方法假设数据列变化非常小的列包含的信息量少。因此，所有的数据列方差小的列被移除。需要注意的一点是：方差与数据范围相关的，因此在采用该方法前需要对数据做归一化处理。\n高相关滤波\n高相关滤波认为当两列数据变化趋势相似时，它们包含的信息也显示。这样，使用相似列中的一列就可以满足机器学习模型。对于数值列之间的相似性通过计算相关系数来表示，对于名词类列的相关系数可以通过计算皮尔逊卡方值来表示。相关系数大于某个阈值的两列只保留一列。同样要注意的是：相关系数对范围敏感，所以在计算之前也需要对数据进行归一化处理。\n随机森林/组合树\n组合决策树通常又被成为随机森林，它在进行特征选择与构建有效的分类器时非常有用。一种常用的降维方法是对目标属性产生许多巨大的树，然后根据对每个属性的统计结果找到信息量最大的特征子集。例如，我们能够对一个非常巨大的数据集生成非常层次非常浅的树，每颗树只训练一小部分属性。如果一个属性经常成为最佳分裂属性，那么它很有可能是需要保留的信息特征。对随机森林数据属性的统计评分会向我们揭示与其它属性相比，哪个属性才是预测能力最好的属性。\n主成分分析 (PCA)\n主成分分析是一个统计过程，该过程通过正交变换将原始的 n 维数据集变换到一个新的被称做主成分的数据集中。变换后的结果中，第一个主成分具有最大的方差值，每个后续的成分在与前述主成分正交条件限制下与具有最大方差。降维时仅保存前 m(m < n) 个主成分即可保持最大的数据信息量。需要注意的是主成分变换对正交向量的尺度敏感。数据在变换前需要进行归一化处理。同样也需要注意的是，新的主成分并不是由实际系统产生的，因此在进行 PCA 变换后会丧失数据的解释性。如果说，数据的解释能力对你的分析来说很重要，那么 PCA 对你来说可能就不适用了。\n反向特征消除\n在该方法中，所有分类算法先用 n 个特征进行训练。每次降维操作，采用 n-1 个特征对分类器训练 n 次，得到新的 n 个分类器。将新分类器中错分率变化最小的分类器所用的 n-1 维特征作为降维后的特征集。不断的对该过程进行迭代，即可得到降维后的结果。第k 次迭代过程中得到的是 n-k 维特征分类器。通过选择最大的错误容忍率，我们可以得到在选择分类器上达到指定分类性能最小需要多少个特征。\n前向特征构造\n前向特征构建是反向特征消除的反过程。在前向特征过程中，我们从 1 个特征开始，每次训练添加一个让分类器性能提升最大的特征。前向特征构造和反向特征消除都十分耗时。它们通常用于输入维数已经相对较低的数据集。\n除了本博客中提到的其中，还包括：随机投影(Random Projections)、非负矩阵分解(N0n-negative Matrix Factorization),自动编码(Auto-encoders),卡方检测与信息增益(Chi-square and information gain)， 多维标定(Multidimensional Scaling), 相关性分析(Coorespondence Analysis), 因子分析(Factor Analysis)、聚类(Clustering)以及贝叶斯模型(Bayesian Models)。"}
{"content2":"假若明天来临——《AI.未来》读后感3900字：\n你有没有想过，如果有一天你被确诊为癌症患者，你会做些什么？你有没有想过，在你百年之后，你希望你的墓碑上刻写着什么内容？\n在我翻开李开复老师的新书《AI.未来》之前，我希望了解到关于人工智能领域发展的历程和对未来的展望。在合上这本书之后，最让我获益良多的却是作为一个普通的人类，如何在人工智能的大潮中做好自己的思考。\n如果说《生命3.0》是对人工智能的一次宏观展望，那么《AI.未来》则是作者用自己的微观体感，详述了以现在为原点的前后几十年，人工智能的发展路径。李开复老师可以说是全球范围内人工智能领域的教父级人物，曾经是谷歌、微软等顶尖公司在这个领域的领军人物，国内很多当前的人工智能领域大神，都曾经是他的下属或者学生。在这本书里，开复老师用他的亲身经历和独特视角，梳理了互联网浪潮在中国以及世界范围内的发展历程，分析了中国与美国这两个人工智能重点国家之间的区别与联系，也对人工智能未来的发展给出了自己的见解。更为令人感触的是，开复老师用自己一段与死神之间的对话，向我们点出了人类与人工智能最本质的区别。读完之后，我感觉收获的不仅仅是一次关于人工智能的认识之旅，也是一次对人生意义的自省之旅。或许，人工智能的发展过程，最终会变成人类认识自己的一个过程。人类不仅仅是在做一个辅助自己的工具，同时也是在做一面镜子，一面让我们更清楚认识自己的镜子。\n人工智能的四次浪潮\n人工智能正在改变着我们的世界，这是一个尽人皆知的事实。那么，人工智能究竟是如何一步一步改变世界的呢？开复老师把人工智能的发展分为了四个阶段：\n第一波浪潮：互联网智能化\n经过这么多年的快速发展，互联网巨头们收集了大量用户上网的数据。所谓互联网智能化，就是利用这些数据给用户贴标签，从而利用人工智能算法作为推荐引擎：这些算法了解、研究、学习我们的个人喜好，从而推荐专门针对我们的内容。\n根据我的理解，这一波浪潮是互联网平台的智能化，是属于基础建设性质的智能化，是后面几波智能浪潮的基础和前提。当然，这也是属于人工智能领域中低垂的果实，我们现在已经完全身陷于这样的一个环境之中了。\n第二波浪潮：商业智能化\n如果说第一波人工智能浪潮的基础是给互联网用户的浏览贴标签，而商用人工智能则是给传统公司数十年来积累的大量专业数据贴标签，比如保险公司的理赔记录，银行发放贷款的记录，等等。读书笔记大全在此之前，这些传统的数据并没有被充分的发掘出价值，直到人工智能进入到这些领域，运用算法在这些数据库中找到了人们往往会忽视的隐形联系，使得这些数据焕发了新的生命，也同时重新定义了这些行业的游戏规则。比如你可能永远都想不到，一个人输入自己生日速度的快慢，手机电池的余量，有可能影响TA在申请贷款的审批额度。\n商业智能化也已经在我们身边处处发生，但是这一波浪潮所使用的，依然只是人们行为的历史数据，对于人们实时行为数据的检测与收集，从第三波浪潮开始。\n第三波浪潮：实体世界智能化\n第三波浪潮就是把人工智能延伸至我们的生活环境，以大量的传感器及智能型器材，把我们的现实世界转化成可悲深度学习算法分析与优化的数据。\n开复老师把这波智能化称之为“感知智能化”，是在计算机键盘和手机屏幕之外更多途径的对人类活动的感知。人们每一句话，每一个表情，每一步移动，都有传感器（智能音箱、人类识别、人体感应器等）进行收集，这些传感器无处不在，并时刻连接在网络上，使得“上线Online”这个词变得没有意义，因为我们几乎无法分清自己有没有在线上。开复老师把这种状态称之为“线上线下融合（Online-Merge-Offline）”，简称OMO。这将是O2O之后的下一个状态，在这个状态下，“数字世界和现实世界完全整合起来，把线上世界的便利性带进线下世界里，把在线下世界里感知的内容带到线上世界里。”\n我觉得这应该就是IoT的最终形态，人们生活在这个万物互联的环境中，自然分不清线上与线下，大部分情况下也不必分清，只要享受实体世界智能化带给我们的便利就可以了。当然，在这个过程中，公共数据和个人隐私将被逐步重新定义，也必然面临不少挑战，然而，我觉得，人类追求效率的天性将使得这个趋势不可逆转。\n第四波浪潮：自主智能化\n开复老师在书里写道：“自主人工智能是前三波人工智能浪潮的集大成者，也是顶峰，把极复杂的数据和机器感知能力结合起来，就会得到不仅能了解世界，也能改变世界的机器，比如自动驾驶汽车。”\n这种智能化将不同于现在生产线上的自动化设备，它不是只能机械的重复某个单一动作，这种“自主”的智能化可以自主做出决策或者处理突发状况，能够应付偏差或异常。当人工智能具有了视觉和触觉等感知能力，并且可以使用数据进行优化的能力，机器能够处理的工作范围就大大增加了。\n人工智能的这四波浪潮，或者说发展的四个阶段，充分体现了“智能是发展的，智能是相对的”这样一个事实。每一个阶段的所谓“智能”，在下一个阶段有可能只是一个基础条件。智能就是在这样不断积累和迭代的过程中，引领着并改变着人们的生活。\n人与人工智能将如何协作\n伴随着人工智能的发展，最令人关注的恐怕就是人工智能大量取代人类工作之后，那些失去工作岗位的人类将何去何从的问题。\n在美国的硅谷，关于人工智能将要引发的失业问题，已经有了广泛的讨论，大致上有三类解决方案：就业再培训（Retraining workers）、减少工作时间（Reducing work hours）或者重新分配收入（Redistributing income）。每一类方案的出发点都是调节就业市场的某一个变量（技能、时间或者报酬）。开复老师在书中对三种方案的优缺点都进行了详细的分析。\n其中后两种方案都是需要政府进行一个规则的重新设计，而只有第一种就业再培训的方案，对于一个个体人类来说，是可以主动去寻求的解决方案。应该说，人类社会的前几次生产力变革，都是通过这种再就业培训来解决失业问题的。比如当工业革命导致大量手工业者和农民失去工作的时候，他们通过培训走进工厂，成为了生产线上的工人。然而对于人工智能这波浪潮来说，与前面几次变革最大的不同就是速度太快，而且方向不确定。也就是说，人工智能的快速发展使得它在取代人类工作的道路上大踏步地前进，以至于就业者每隔几年就不得不更换职业。同时由于人工智能发展路径具有极大的不确定性，导致很难预测在未来几年内哪些工作将会受到影响，所以很难指望一名普通的就业者在选择再培训计划时，能准确预测几年后哪些工作更安全。\n因此，分析哪些领域的工作最不容易被人工智能取代，对于每一个个体来说，变得异常重要。在分析这个问题的时候，开复老师使用了“角思维”的方式，用社交属性和创意属性作为横纵两个坐标，分别对体力劳动和脑力劳动进行了分类，并预测了在四个象限中，人工智能与人类可能存在的合作模式。\n（图片来自《AI.未来》插图）\n可以看出，在四个象限中，弱社交且弱创意的工作最容易被人工智能所取代；强社交且强创意的工作，最不容易被取代；在强社交但对创意要求不强的领域，属于“结合区”，可以进行人工智能和人类之间的协作，由人工智能负责数据的收集和处理，由人类工作人员负责与人类客户之间的交互。在强创意但是弱社交领域，则会存在人类与人工智能共存的现象。总之，由于各种工作性质的不同，人类与人工智能之间的关系也就不同。\n在书中举了一个医生领域的例子。由于人工智能可以掌握大量的病例和数据，因此在诊断疾病、制定治疗方案等方面一定能超过人类医生。但是问题是很多患者不希望面对一台冰冷的机器来完成看病的过程，因此传统的医生可能会演变成一种新的职业，李开复老师称之为“关怀护理医师”。这种职业集护士、医疗技术人员、社会工作者，甚至心理学家的技能于一身，能够了解和操作诊断工具，同时可以与患者交流，在患者遭受生理或者心理创伤时给予安慰，给患者情感上的支持。\n这些都是人工智能所不能够做到的事情，这就是人在人工智能普及的时代中，所独一无二不可替代的地方。\n之所以能够做到这一点，是因为人类最有价值的不是大脑，而是人心。这就是作者在这本书中重点向读者传递的一个理念，而这个理念是他经过一段与癌症作斗争的经历之后，感悟得来的。\n人与机器的根本区别\n2013年9月，开复老师被诊断为淋巴癌第四期。这一个突如其来的打击，以及在随后与癌症作斗争的过程中，他重新思考了关于工作、价值和“人”的意义之间的关系，并且重新思考他原来自以为成功的“人生算法”，以及他早已为自己拟好的墓志铭。\n他也开始重新审视人与人工智能之间的关系。在此之前，他在人工智能领域的报负是：“量化人类思维过程、解释人类行为。”然而，这次与死神的擦肩而过让他明白：人与机器的根本区别在于“能否爱人”。他在书中写道：\n人工智能固然强大，而人类独有的爱才是我们生活中最需要的。......我坚信的未来是由人工智能的思考能力，加上人类爱的能力构筑的。\n不得不承认，人工智能的发展对人类的影响，还存在着很多不同的看法和争议。然而开复老师用他丰富的第一线从业经验，以及深刻的个人感悟说服了我。人工智能的研究，实际上最终都聚焦在对人本身的理解和研究上，开复老师告诉我们：要想理解人的独特性和神奇的地方，我们需要学习的不是人的大脑，而是人的心。\n因此，人工智能，不仅仅是人类制造出的一个超强的工具。同时，它也使我们有机会更加理解人类的本质。\n如果人工智能真的能够帮助我们理解自己，不是因为人工智能理解了人脑的运作原理，而是因为人工智能解放了我们，让我们不再一味追求优化，进而可以聚焦在真正使我们成为人类的东西上——爱人与被爱的能力。\n这样的一个未来，我希望它早点到来。"}
{"content2":"这几天，Google的人工智能AlphaGo与李世石的“围棋对决”受到了无数关注。作为人工智能的拥护者之一，博主自然也不例外。在第三战之前，写下这篇博客，谈谈感想。\n图 李世石 vs AlphaGo\n我本来想今天看比赛的，这样可以在周末前得知结果。因为这场李世石输了的话，那么就是3:0，人机大战的结果也就确定了，而且根据前两场来看，第三场李世石输的可能性很大。到了12点钟我才知道今天不赛，明后两天会连续两场。既然如此，后面的战斗还是有点悬念的。尽管是人工智能的拥护者，但我希望李世石后面能赢，一是增加悬念，二是人工智能发展的太快了，给人感觉Hold不住。\n关于AlphaGo的创造者-DeepMind公司，前身是英国的一家研究深度学习的公司，因为开发了一个可以学习打Atari游戏的学习系统，而被Google收购。一年前博客园曾报道过这个新闻，当时我是唯一一个留言者（笑）。我在留言中认为这是AI研究的一个重要进步。因为会打游戏了，学习现实中的工作都相对容易。游戏其实是对现实的抽象与模拟，一开始先学习游戏，可以慢慢对系统的学习算法与能力进行观察与改善。今年1月份左右，这个公司在《Nature》上刊登了关于研发出的围棋学习系统的论文，后面就有了3月份这场比赛。\n借助于各种直播平台，这场比赛的影响力是空前的。目前在贴吧、知乎、以及微博里都可以见到讨论。大众对于人工智能的关心突然地增加了，令人工智能的从业者都有点吃惊。有趣的是，这几天在贴吧我看到的跟深度学习相关的帖子，比这三年看到的还要多。博客园也到处都是相关的新闻。当然，这也是让人有点担心的。因为一些不清楚其中细节的人可能会对目前的AI研究产生误解。事实上，现在的AI跟科幻电影中的人工智能仍然相去甚远。只要稍微了解AlphaGo背后的技术，就可以了解。目前AlphaGo采用的技术是卷积神经网路和蒙特卡洛搜索树，加上了部分强化学习的理念。这些概念在目前的学术领域并没有太大的创新，因此说这些技术就是人工智能的话，未免太乐观了。目前人工智能界普遍的看法是，可能还需要二三十年人工智能的能力才能接近普通人的水准。现在的系统只能在某些专长领域比人做的更好（例如对图片分类，自动驾驶等）。若比较综合能力，以及应变能力，则远远不及人类。消除误解的最好办法事实上就是学习知识。\n但是，这场“人机大战”有可能是一个重要事件，因为这标志了机器第一次在“人类最后的骄傲”-围棋中战胜顶级选手。人工智能引发的浪潮可能是一场技术革命，是继工业革命，信息革命后的第三次革命。我是这么认为的：工业革命解放了人类的双手与双脚，信息革命解放了人类的计算与推理能力，而人工智能革命将会解放人类的感知与归纳能力。这里的感知能力指的是视觉、听觉、与语言表达能力，而归纳能力指的是对数据与经验的分析与总结能力。前者由深度学习完成，后者由普通的机器学习完成。这些技术的代表特征就是具有泛化和“举一反三”的能力。可以完成很多原先只能由人完成的工作。未来，很多岗位将会被AI所代替。举个简单的例子，目前交通罚款判断是否是闯红灯时，照片会由一些人来肉眼确认。未来这些工作可以被计算机视觉替代，只有不确定的照片才需要找人确认。现在，一些变化已经在进行中。例如原先停车场会由人或者卡来管理进出，现在借助停车场车牌识别系统，可以实现完全自动化的管理。欣慰的是，虽然一些岗位消失了，但这是良性变化。借助先进的技术未来人类的生活会更加美好。\n如果人工智能真的是一场技术革命，那么这场革命对于中国的意义何在？意义很大，而且非常重要。事实上，工业革命我们没有赶上，以至于落后了几百年；信息革命我们赶上了后半截（互联网部分），努力追赶才有目前的持平状态；而人工智能革命我们则与国外并驾齐驱，是实现能力强大与赶超的最好的机会。按照流行的说法，那就是实现中国梦的关键取决于能否紧紧抓住这次人工智能技术革命的浪潮。\n图 人工智能\n人工智能革命对于我们普通个人的意义在哪里，我们该如何面对？首先不要恐慌，其次要理解其背后的技术并不神秘与复杂。如果对人工智能感兴趣，就去认真学习。如果不感兴趣，就充分利用它。在这里我对希望在人工智能上进行探索的同学提供一些学习建议：一定要学好“机器学习”这门重要学科。在其基础上，再对“深度学习”进行深入研究。除了这两门学科以外，数学功底与算法能力也是相当重要的。这些建议，与各位共勉。\n在评论区有同学曾经叫我推荐一些资料。在这里，机器学习方面的书籍我推荐南大周志华老师最近出的《机器学习》一书。这本书我读过其中一两章节，个人感觉比国外的经典教材《PRML》和《ESL》更适合中国的读者以及工业界人士（毕竟周老师擅长的集成学习领域是在工业界应用效果极好的一门技术）。其次，深度学习方面，我推荐Standford的UFLDL教程，这个教程有中英双版，以及Bengio的《Deep Learning》一书的预览版本。这本书还未出版，但是在网上有预览版本，以后出中文版后可以购买支持。\n最后，让我们期待后面的几盘比赛，祝李世石加油。如果第三盘输了也不要紧，后面的看点是能否赢一盘，因为这是挽回“人类的荣誉”的最后机会。"}
{"content2":"随着.NET Core的发布和开源，.NET又重新回到了人们的视野。除了开源、跨平台、高性能以及优秀的语言特性，越来越多的第三方开源库也出现在了github上——包括ML.NET机器学习、Xamarin移动开发平台、基于Actor模型的分布式框架Orleans以及分布式开发及部署平台Service Fabric等等。\n西安.NET社区组织发起了此次“拥抱开源, 又见.NET”线下交流活动，邀请了三位资深.NET开发者作为分享讲师，他们将从架构、原理、语言出发，与大家一起分享交流：\n.NET在大数据及人工智能项目的设计及架构\nASP.NET Core应用程序的工程实践\n.NET家族的函数式语言从C#到F#\n免费技术交流，感兴趣的同学抓紧时间报名吧！\n【  时 间 地 点  】\n2018年09月15日（周六）1:00PM—5:30PM\n西安市高新区天谷八路环普产业园E座5楼 ThoughtWorks办公室（请从E6进入）\n【  活 动 安 排  】\n12:50 PM  签到\n01:20 PM  开场介绍\n01:30 PM  话题一《.NET在大数据和人工智能项目中的应用实践》\n02:40 PM  话题二《ASP.NET Core生命周期指北》\n03:50 PM  话题三《从C#到F#》\n05:10 PM  反馈抽奖&合影留念\n【  精 彩 预 告  】\n话题一《.NET在大数据和人工智能项目中的应用实践》\n话题简介：应用产生数据，数据蕴含智慧，如何应用人工智能从大数据挖掘出有价值的知识信息并提供智能化的应用是一个比较有挑战力的话题，在大数据和人工智能领域，dotNET应用并不广泛，基于项目实践经验为大家分享.NET在大数据和人智能领域之中诸如数据采集、自然语言处理、机器学习、结果展示和整体架构设计方面的应用。\n分享人：\n魏 琼 东\n易特科首席架构师\n从事软件研发工作17年，2004年开始使用C#，结合多年的研发管理经验，开发了AgileEAS.NET SOA 中间件平台并在医疗卫生、铁路、物流、制造等行业广泛应用，擅长企业软件过程改进、系统分析与架构设计，目前主要致力于大数据和人工智能与医疗健康领域的结合。\n话题二《ASP.NET Core生命周期指北》\n话题简介：通过对ASP.NET Core应用程序生命周期的剖析，让.NET Core开发人员能够能够更直观的掌握应用程序结构和设计思想，轻松的进行故障排除和调试。首先，通过对启动的分解，帮助理解Dependency Injection和Middleware的行为本质，接下来通过对HTTP请求和服务器响应的重演，来熟悉框架中的各个组件，以及它们之间是如何协同和交互的。\n分享人：\n张 文 清\nThoughtWorks 高级.NET开发\n热爱钻研技术本质，挑战新事物和推动潮流技术发展。曾供职于西安葡萄城信息技术有限公司，积累超过10年.NET控件和工具的设计以及开发经验，主导和实施多个云应用架构开发和DevOps实践。\n提示：如果希望现场做代码尝试的话，需要准备以下环境： .NET Core 2.1.400 2. IDE，推荐Visual Studio或者Jetbrains Rider。\n话题三《从C#到F#》\n话题简介：如果你已经精通了C#或者其他面向对象的语言，你也许会寻找另一个值得学习的语言。得益于F#强大的类型推导系统，你可以写出跟Python一样简洁而带有类型安全的代码；F#作为一门函数式语言，你可以很轻松的通过部分应用以及函数组合来创建新的函数，通过分享函数式编程模式以及monadic风格的实例让你了解函数式编程的基本思想；另外F#还内置了异步编程、消息队列，type provider等高生产力的组件及特性，跟.NET技术栈的无缝集成意味着你还可以使用所有.NET技术栈的类库和工具。\n分享人：\n张 阳\nThoughtWorks 高级.NET开发\n有着多年基于MessageBus以及Event Sourcing的CQRS企业级开发经验，对基于消息传递的微服务架构有着深刻的理解和实践，同时也是DDD的爱好者和布道师，博客园推荐作者以及.NET西安社区发起者。\n【  活 动 福 利  】\n参与者除了能够与众多NET开发师现场交流外，还将有机会获得由ThoughtWorks提供的精美礼品，如: 水杯、笔记本等实用小礼物。\n【  报 名 须 知  】\n1、本次活动为免费技术交流，对.NET开发感兴趣的小伙伴均可报名。\n2、为了保证活动的质量，使话题能够深入展开，我们将限定参会人数，请您务必提前报名。\n3、报名的小伙伴们如果提供的邮件地址和手机号码无误，我们会在9月12日（活动开始前3天）对通过报名筛选的小伙伴发送短信和邮件确认。请确认自己填写的电话、邮件地址无误。\n【报名地址】\n点我\n【  .NET西安社区介绍  】\n.Net 西安社区正在使用.NET Core / Azure / Xamarin等技术开发基于互联网和移动应用平台上的各种新产品和商业服务。我们的目标是通过举办各种分享活动，交流开发心得和经验来推动.NET技术栈在西安乃至西北地区的发展。我们是一个开放和自由的社区，欢迎您加入.NET西安社区！"}
{"content2":"1:concept learning:version space，decision tree等；\n2:rule learning：If-then rules, association rules, genetic programming等；\n3. instance-based learning（k-means, knn）,clustering等；\n4：numerical approaches: ANNs , SVM,computational learning等；\n5: probabilistic approaches :bayesian learning等；\n6：ensembling：bagging,boosting,stacking等，combining classifiers；\n8: reinforcement learning。"}
{"content2":"一.概念\n概念学习：是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数。\n二.概念学习任务\n任何概念学习任务能被描述为：实例的集合、实例集合上的目标函数、候选假设的集合以及训练样例的集合。\nEnjoySport概念学习任务\n已知：\n实例集X：可能的日子，每个日子由下面的属性描述：\nsky:(可取值 sunny,Cloudy和Rainy)\nAirTemp:(可取值为Warm和Cold)\nHumidity:(可取值为Normal和High)\nWind:(可取值为：Strong和Weak)\nWater：(可取值为Warm和Cold)\nForecast:(可取值为Same和Change)\n假设集H：每个假设描述为6个属性：Sky,AirTemp,Humidity,Wind,Water和Forecast的值约束的合取。约束可以为“？”（表示接受任意值），“ø”（表示拒绝所有值），或一特定值\n目标概念C:EnjoySport: X->{0,1}\n训练样例集D：目标函数的正例和反例\n求解：\nH中的一假设h，使对于X中任意x，h(x)=c(x)\n1.术语定义\n实例集（X）:概念定义的实例集合\n目标概念（c）：待学习概念或函数\n训练样例（D）:每个样例为X中的一个实例x以及它的目标概念值c(x)。c(x)=1的实例被称为正例（positive example），c(x)=0的实例为反例（negative example），经常用序偶<x,c(x)>来描述训练样例。\nH表示所有可能假设的集合。H中每个假设H表示X上定义的布尔函数，即h:X->{0,1}。机器学习的目标就是寻找一个假设h，使对于X中的所有x，h(x)=c(x)。\n归纳学习假设：任一假设如果在足够大的训练样例集中很好地逼近目标函数，它也能在未见实例中很好地逼近目标函数。\n三.作为搜索的概念学习\n定义：令hj和hk为在X上定义的布尔函数。称hj more_general_than_or_equal_to hk（记做hj≥g hk），当且仅当(∨x∈X)[(hk(x)=1)->(hj(x)=1)]\nhj more_specific_than hk ，当hk more_general_than hj\n四.FIND-S：寻找极大特殊假设\n从H中最特殊假设开始，然后在该假设覆盖正例失败时将其一般化（当一假设能正确地划分一个正例时，称该假设“覆盖”该正例）。\nFIND-S算法\n1. 将h初始化为H中最特殊假设\n2.对每个正例x\n对h的每个属性约束ai\n如果x满足ai\n那么不做任何处理\n否则将h中ai替换为x满足的下一个更一般的约束\n3. 输出假设h\n五.变换空间和候选消除算法（CANDIDATE-ELIMINATION）\nFIND-S输出的假设只是H中能够拟合训练样例的多个假设中的一个。而在候选消除算法中，输出的是与训练样例一致的所有假设的集合。\n1.表示\n定义：一个假设h与训练样例集合D一致，当且仅当对D中每一个样例<x,c(x)>都有h(x)=c(x)。\nConsistent(h,D)≡(∨<x,c(x)>∈D) h(x)=c(x)\n定义:关于假设空间H和训练样例集D的变型空间，标记为VSH,D，是H中与训练样例D一致的所有假设构成的子集。\nVSH,D≡{h∈H|Consistent(h,D)}\n2.列表后消除算法（LIST-THEN-ELIMINATE）\n列表后消除算法\n1.变型空间VersionSpace<-包含H中所有假设的列表\n2.对每个训练样例<x,c(x)>\n从变型空间中移除所有h(x)≠c(x)的假设h\n3. 输出VersionSpace中个假设列表\n3.变型空间的更简洁表示\n定义：关于假设空间H和训练数据D的一般边界（general boundary）G，是在H中与D相一致的极大一般（maximally general）成员的集合。\n定义：关于假设空间H和训练数据D的特殊边界（specific  boundary）S，是在H中与D相一致的极大特殊（maximally specific）成员的集合。\n变型空间的确切组成是：G中包含的假设，S中包含的假设已经G和S直接偏序结果所规定的假设。\n定理2.1：变型空间表示定理 令X为任意的实例集合，H为X上定义的布尔假设的集合。另c:X->{0,1}为X上定义的任一个目标概念，并令D为任一训练样例的集合{<x,c(x)>}。对所有的X,H,c,D以及良好定义的S和G:\n4.候选消除学习算法\n使用变型空间的候选消除算法\n将G集合初始化为H中极大一般假设\n将S集合初始化为H中极大特殊假设\n对每个训练例d，进行以下操作：\n如果d是一正例\n• 从G中移去所有与d不一致的假设\n• 对S中每个与d不一致的假设s\n•从S中移去s\n• 把s的所有的极小一般化式h加入到S中，其中h满足\n•h与d一致，而且G的某个成员比h更一般\n• 从S中移去所有这样的假设：它比S中另一假设更一般\n如果d是一个反例\n• 从S中移去所有d不一致的假设\n• 对G中每个与d不一致的假设g\n•从G中移去g\n•把g的所有的极小特殊化式h加入到G中，其中h满足\n•h与d一致，而且S的某个成员比h更特殊\n•从G中移去所有这样的假设：它比G中另一假设更特殊\n5.算法举例\n候选消除算法步骤（EnjoySport）\n训练样例：\n1.<Sunny,Warm,Normal,Strong,Warm,Same>,EnjoySport=Yes\n2.<Sunny,Warm,High,Strong,Warm,Same>,EnjoySport=Yes\nS0和G0为最初的边界集合，分别对应最特殊和最一般假设。训练样例1和2使得S边界变得更一般，如FIND-S算法中一样，这些样例对G边界没有影响。\n训练样例:\n3.<Rainy,Cold,High,Strong,Warm,Change>,EnjoySport=No\n样例3是一个反例，他把G2边界特殊化为G3。注意在G3中有多个可选的极大一般假设。\n训练样例：\n4.<Sunny,Warm,High,Storage,Cool,Change>,EnjoySport=Yes\n正例是S边界更一般，从S3变为S4。G3的一个成员也必须被删除，因为它不再比S4更一般。\nEnjoySprot概念学习问题中的最终的变型空间\n六.归纳偏置\n1.无偏的学习器\n幂集（power set）把集合X的所有子集的集合称为幂集。\n新的假设空间H’，它能表示实例的每一个子集，也就是把H’对应到X的幂集。\n<Sunny,?,?,?,?,?>∨<Cloudy,?,?,?,?,?>\n2.无偏学习的无用性\n学习器如果不对目标概念的形式做预先的假定，它从根本上无法对未见实例进行分类。\n一般情况下任意的学习算法L以及为任意目标概念提供的任意训练数据Dc={<x,c(x)>}。训练过程结束后，L需要对新的实例xi进行分类。令L(xi,DC)表示在对训练数据Dc学习后L赋予xi的分类（正例或反例），我们可以如下描述L所进行的这一归纳推理过程：\ny表示z从y归纳推理得到。\n定义：考虑对于实例集合X的概念学习算法L。令c为X上定义的任一概念，并令Dc={<x,c(x)>}为c的任意训练样例集合。令L(xi,Dc)表示经过数据Dc的训练后L赋予实例xi的分类。L的归纳偏置是最小断言集合B，它使任意目标概念c和相应的训练样例Dc满足：\n候选消除算法的归纳偏置：目标概念c包含在给定的假设空间H中。\n使用假设空间H的候选消除算的输入输出行为，等价于利用了断言“H包含目标概念”的演绎定理证明器。该断言因此被称为候选消除算法的归纳偏置。用归纳偏置来刻画归纳系统，可以便于使用等价的演绎系统来模拟它们。这提供了一种对归纳系统进行比较的方法，即通过它们从训练数据中泛化的策略。"}
{"content2":"深度学习 vs 机器学习 vs 模式识别\n模式识别：智能程序的诞生。\n机器学习：从样本中学习的智能程序。\n深度学习：一统江湖的架构。受宠爱最多的就是被用在大规模图像识别任务中的卷积神经网络。\n1）机器学习就像是一个真正的冠军一样持续昂首而上；\n2）模式识别一开始主要是作为机器学习的代名词；模式识别正在慢慢没落和消亡；\n3）深度学习是个崭新的和快速攀升的领域。\n1. SVM经常使用的核函数有：(1)线性核函数(2)多项式核(3)径向基核（RBF）(4)傅里叶核(5)样条核(6)Sigmoid核函数\n2. 序列模式挖掘算法：指挖掘相对时间或其他模式出现频率高的模式，典型的应用还是限于离散型的序列。\nApriori类算法包括： AprioriAll和 GSP等。\n在序列模式挖掘中，FreeSpan和PrefixSpan是两个常用的算法。其中，PrefixSpan是从FreeSpan中推导演化而来的。这两个算法都比传统的Apriori-like的序列模式挖掘算法（GSP）都有效。而PrefixSpan又比FreeSpan又更有效。这是因为PrefixSpan的收缩速度比FreeSpan还要更快些。\n典型应用：商场挖掘即用户几次购买行为间的联系，可以采取更有针对性的营销措施。\n类似于Apriori算法大体分为候选集产生、候选集计数以及扩展分类三个阶段。与AprioriAll算法相比，GSP算法统计较少的候选集，并且在数据转换过程中不需要事先计算频繁集。\n\n3. 序列模式 VS 关联规则\n问题\n序列模式挖掘\n关联规则挖掘\n数据集\n序列数据库\n事务数据库\n关注点\n单项间在同一事务内以及事务间的关系\n单项间在同一事务内的关系\n\n\n. 类域界面方程法中，求线性不可分情况下分类问题近似或精确解的方法是？\n神经网络处理不可分现象\n5.特征选择方法：信息增益、信息增益率、基尼系数\n6. 基于核的算法：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 线性判别分析（Linear Discriminate Analysis ，LDA)等\n7. 数据清理中，处理缺失值的方法是?\n数据清理中，处理缺失值的方法有两种：\n删除法：1）删除观察样本\n2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除\n3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析\n4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差\n查补法：均值插补、回归插补、抽样填补等\n成对删除与改变权重为一类，估算与查补法为一类\n8. 下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）B\nA. 特征灵活  B. 速度快  C. 可容纳较多上下文信息  D. 全局最优\n首先，CRF，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模.\n隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择\n最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉\n条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。\n\n9. KNN和K-Means的区别\nKNN\nK-Means\n1.KNN是分类算法\n2.监督学习\n3.喂给它的数据集是带label的数据，已经是完全正确的数据\n1.K-Means是聚类算法\n2.非监督学习\n3.喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序\n没有明显的前期训练过程，属于memory-based learning\n有明显的前期训练过程\nK的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c\nK的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识\n相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。\n\n10. 以下哪个是常见的时间序列算法模型B\nA. RSIB. MACDC. ARMAD. KDJ\n时间序列模型是指采用某种算法（可以是神经网络、ARMA等）模拟历史数据，找出其中的变化规律\n时间序列算法模型主要有：移动平均算法、指数平滑算法及ARMA、ARIMA方法。"}
{"content2":"秦曾昌人工智能课程---3、机器学习中的哲学\n一、总结\n一句话总结：\n机器学习分类：了解机器学习分类：监督学习，非监督学习，增强学习\n机器学习针对性：了解什么问题是机器学习问题，什么不是\n1、三个门，一个门后有财宝，你选择了a门，主持人打开b门没有财宝，现在你还可以选择a或c，ac有财宝的概率分别是多少？\np(a)=1/3；p(c)=2/3：因为你选a之后，bc是一个整体为2/3。因为你选a之后，主持人也不可能再打开a\n编程模拟：可以编程模拟，会发现p(c)是p(a)的两倍\n2、比如在判断图片是猫还是狗的过程中，输入的是一张20*20的图像，那么输入用数学表示的实质是什么？\n1200维向量：这只是输入的一个属性而已：20*20*3(Rgb)：其实就是一个有1200个数据的数组\n3、比如在判断图片是猫还是狗的过程中，输入的是一张20*20的图像，那么 输出 用数学表示的实质是什么？\n一个数值：0或1\n4、我们知道的函数 数量毕竟有限，那么我们如何拟合复杂的函数关系？\n神经网络：使满足testing data\n通过概率：比如分类问题：比如x->[0,1]，P(0|x)>P(1|x)，那么x=0\n5、整个机器学习所做的事情是什么？\n找输入和输出之间的关系\n6、机器学习 和 模型识别 的区别和联系？\n模型识别：模型识别就是根据输入信息识别模型\n机器学习：找输入和输出之间的关系，也是找模型\n7、机器学习可以生成大量软件开发素材么（NICE）？\n可以的：本质是输入和输出的关系：比如已有项目给图片染色：所以肯定可以\n8、机器学习可以将白天的照片变成晚上的照片么？\n可以的：输入就是白天的照片，输出就是晚上的照片\n9、机器学习对我当前最有用的项目是什么？\n生成游戏图片素材\n10、自动驾驶用到的输入输出一般有哪些？\n输入：摄像头，超声波，雷达等\n输出：以什么力道转方向盘，以什么力道踩刹车，以什么力道踩油门\n11、下围棋alpha go的输出可以看成什么？\n每个空位落子赢的概率，选最大的\n12、我们为什么有什么看了某些知识的定义看不懂 ？\n没看到现象：因为定义是通过现象总结的，而我们并没有看到现象，所以看不懂\n13、机器学习优化模型的实质是什么？\n找普遍化和特殊化之间寻找一个平衡：A tradeoff between generalization and specification.\n14、提高机器学习模型精确度的方法？\n1、收集大量数据\n2、求出多个模型，取均值\n3、对复杂模型加λ项使它平滑化\n4、n次的交叉验证：比如n=10，数据分10块，就块做training，1块做tesing\n15、聚类Clustering是什么意思？\n坐标中接近的点看做整体：坐标中某些点比较接近（横纵坐标），可以看做一个整体：物以类聚，人以群分\n16、增强学习在生活中的实例？\n训练小狗：如果小狗做对了给吃的，做错了给惩罚，久而久之小狗就知道怎么做了\n17、机器学习分类？\n监督学习：Unsupervised Learing\n非监督学习：Supervised Learing\n增强学习：Reinforcement Learning\n18、非监督学习分类？\n聚类：Clustering\n降维：Dimensionality Reduction\n19、监督学习分类？\n分类：Classification\n回归：Regression\n20、网上如何找人工智能课程的资源？\nb站+网盘搜索+百度+csdn\n21、监督学习和无监督学习的本质区别是什么？\n数据有无标签\n22、人工智能的开源项目多么，去哪里找？\n多，去GitHub\n23、清理数据是编程问题么？\n并不全是：数据了解：其实更多的不是编程的问题，而是你对数据了解的问题，知道哪些有用哪些没用\n二、内容在总结中"}
{"content2":"本文由云+社区发表\n作者：Java3y\n前言\n只有光头才能变强\n没错，这篇主要跟大家一起入门机器学习。作为一个开发者，”人工智能“肯定是听过的。作为一个开发面试者，肯定也会见过”机器学习“这个岗位(反正我校招的时候就遇到过)。\n可能还会听过或者见过“深度学习”、“神经网络”等等这些非常火的名词，那你对这些术语了解多少呢？\n相信大家这几天在朋友圈也可以看到这照片：\n核心AI代码\n// 通过if else 以人工穷举的方式来假装实现智能机器人聊天\n希望阅读完本文中后，大家可以对这些术语和机器学习有一定的了解。\n一、术语介绍\n首先我们来简单看看人工智能、深度学习、机器学习这些术语和它们之间的关系究竟是怎么样的。\n1.1人工智能\n不知道听到“人工智能”大家会联想到什么，可能大多数都会想到科幻电影的机器人。\n《人工智能》电影的剧照\n我们看来看看维基百科的定义：\n人工智能（英语：Artificial Intelligence，缩写为 AI）亦称机器智能，指由人制造出来的机器所表现出来的智能。通常人工智能是指通过普通计算机程序的手段实现的人类智能技术。\n人工智能也可以分成两类：\n强人工智能：强人工智能观点认为“有可能”制造出“真正”能推理（Reasoning）和解决问题的智能机器，并且，这样的机器将\n被认为是具有知觉、有自我意识的\n。\n像绝大多数科幻电影中的机器人就是在这范畴\n弱人工智能：弱人工智能观点认为“不可能”制造出能“真正”地推理和解决问题的智能机器，这些机器只不过\n“看起来”像是智能\n的，但是并不真正拥有智能，也不会有自主意识。\n我们目前阶段的人工智能，其实都是弱人工智能。\n1.2机器学习\n不知道听到“机器学习”大家会联想到什么。Emmm…反正我就是从字面的意思去理解：“机器可以自我学习”。\n首先我们看一下维基百科是怎么说的：\n机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科\n简单来说：机器学习可以通过大量的数据或者以往的经验自动改进计算机程序/算法。\n什么是机器学习\n生成完模型f(x)之后，我们将样例数据丢进模型里边，就可以输出结果：\n输入样例进模型，输出结果\n我们说机器学习可以自我学习，是因为我们会将样例数据也会丢到“历史数据”中，这样生成模型就会有一定的改动，从而达到“自我学习”的效果。\n1.3它们之间的关系\n等等，我们好像还没讲深度学习呢。我们从上面机器学习的介绍也可以知道，机器学习已发展为一门多领域交叉学科，机器学习中就有好多个经典的算法，其中就包含了神经网络(深度学习可看成是神经网络的升级版)。由于近几年深度学习发展迅猛，一些特有的学习手段相继被提出，所以越来越多的人将其单独看作一种学习的方法。\n《机器学习 周志华》：\n所谓深度学习，狭义地说就是“很多层”的神经网络，在若干测试和竞赛下，尤其涉及语音、图像等复杂对象的引用中，深度学习取得优越的性能。\n所以我们可以总结出人工智能、机器学习、深度学习之间的关系是这样的：\n机器学习，是实现人工智能的重要方法。\n深度学习，是实现机器学习的技术。\n之间的关系\n想要了解更多，可参考：\n人工智能、机器学习和深度学习的区别?\nhttps://www.zhihu.com/question/57770020\n二、机器学习入门\n通过上面我们可以简单认为机器学习就是：利用计算机从历史数据找出规律，把这些规律用到未来不确定场景的决策中。\n下面我们再来学习一下机器学习的一些入门知识。\n2.1机器学习的术语\n特征、样本、数据集、标记这些术语的说明：\n特征、样本、数据集、标记这些术语的说明\n特征(属性)所张成的空间叫做特征空间。\n特征空间\n例如我们把“色泽”、\"根蒂“、”敲声“作为三个坐标轴，则它们张成一个用于描述西瓜的三围空间，每个西瓜都可在这个空间中找到自己的坐标位置。由于空间中的每个点对应一个坐标向量，我们也把一个示例称为“特征向量”。\n特征向量\n回到我们上面的图，再来讲讲“训练数据”、“训练”、“标记”：\n“训练数据”、“训练”、“标记”的术语解释\n2.2机器学习的分类\n一般机器学习又可以分成以下几类：\n监督学习\n半监督学习\n非监督学习\n增强学习\n2.2.1监督学习\n监督学习：训练数据(Training Data)可以告诉我们要找的那个模型的输入(Input)与输出(Output，也就是我们说的label)之间有什么样的关系。\n给出的数据都有“答案”或“标记”\n训练数据：\"Java3y公众号\"->好的公众号 ， \"Java4y公众号\"->不好的公众号。 输出结果：好的公众号或者不好的公众号\n在监听学习下又分为两种算法：\n回归(Regression)：结果是一个连续的数值(scalar)，而非类别\n分类(Classification)：为训练数据进行分类别(多分类)\n二分类：类别只有两种结果(YES OR NO)\n回归例子：知道前几天的PM2.5数值，预测一下明天的PM2.5数值。\n回归例子\n二分类例子：判断一封邮件是垃圾邮件还是正常邮件。\n判断是垃圾邮件还是正常邮件\n多分类例子：将新闻帖子分类成不同的类别。\n分类成不同的类别\n2.2.2非监督学习\n非监督学习：训练数据(Training Data)没有对应“答案”或“标记”\n训练数据：\"Java3y公众号\" \"Java4y公众号\" \"Java5y公众号\" \"Java6y公众号\" \"yyy公众号\" \"xxx公众号\" \"zzz公众号\" 输出结果：(\"Java3y公众号\" \"Java4y公众号\" \"Java5y公众号\" \"Java6y公众号\") (\"yyy公众号\" \"xxx公众号\" \"zzz公众号\") 分门类别\n对没有“标记”的数据进行分类-聚类分析\n对没有“标记”的数据进行分类-聚类分析\n聚类分析例子：在以前，中国移动有三个品牌：神州行、动感地带、全球通。我们给一堆的SIM卡交由学习算法训练，不告诉它每张SIM卡具体是什么卡，最后我们是可以将这些SIM卡分类别出来的。\n非监督学习的意义\n非监督学习的意义\n非监督学习的意义\n2.2.3半监督学习\n理解了监督学习和非监督学习，对于半监督学习就很容易理解了。\n一部分数据有“标记”或者“答案”，另一部分数据没有\n因为各种原因产生的标记缺失。\n部分有label，部分没有label\n通常都会使用非监督学习手段对数据进行处理(特征提取、降维)，之后再只用监督学习手段做模型的训练和预测。\n2.2.4增强学习\n根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式\n增强学习\n每次行动，就给这次的行动评分，算法会根据评分来评估下一次的行动是好还是坏，最终不断改进。\n给每次的行动评分\n例子：Alpha Go下每步棋的时候都会评估自己这次下得怎么样，通过最终的结果不断改进下的每步棋。\n2.3机器学习的其他分类\n除了我们上面所说的监督学习、非监督学习、半监督学习、增强学习之外，机器学习也可以分成：\n在线学习：\n及时\n将样例数据作为训练数据对模型进行训练。\n需要加强对数据进行监控(有可能样本数据是脏数据，这样就破坏我们的模型)\n离线(批量)学习：\n定时\n将样例数据作为训练数据对模型进行训练。\n不能很快的适应环境的变化\n还有：\n参数学习：一旦学到了参数，就不再需要原有的数据集。通过调参数就好了。\n非参数学习：不对模型进行过多的假设，非参数不代表没参数。\n最后\n机器学习的核心在于算法上，这篇只是对机器学习的一个简单的入门，希望能对大家有所帮助。\n此文已由腾讯云+社区在各渠道发布\n获取更多新鲜技术干货，可以关注我们腾讯云技术社区-云加社区官方号及知乎机构号"}
{"content2":"Python3安装turtle提示错误：Command \"python setup.py egg_info\" failed with error code 1\nPython3.5安装turtle：\npip3 install turtle\n提示错误：\nCollecting turtle Using cached https://files.pythonhosted.org/packages/ff/f0/21a42e9e424d24bdd0e509d5ed3c7dfb8f47d962d9c044dba903b0b4a26f/turtle-0.0.2.tar.gz Complete output from command python setup.py egg_info: Traceback (most recent call last): File \"<string>\", line 1, in <module> File \"/tmp/pip-install-hpqxw6_s/turtle/setup.py\", line 40 except ValueError, ve: ^ SyntaxError: invalid syntax ---------------------------------------- Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-hpqxw6_s/turtle/\n网络上的方法，升级setuptools也没有用：\npip3 install --upgrade setuptools\n也会提示错误\n最后通过网友按如下方法解决了，现在记录如下：\n仔细查看安装turtle出错的错误信息，可以看到是个语法错误。\npip在下载turtle 0.0.2包后，会解压到本地再安装，提示的错误在解压的setup.py文件里面，\n解决的办法就是：按照给定的链接（我的是这个），把turtle包下载到本地，手动解压，修改setup.py文件再安装。\n打开setup.py文件，第40行修改为\nexcept (ValueError, ve):\n原来的是Python2的写法，没有括号，加了括号之后Python3就能用了。\n用pip3安装修：\npip install -e turtle-0.0.2\n-e后面接上我们修改过setup.py文件的目录。\n这样就搞定了。\n另外，如果提示 python-tk 未安装，用apt命令安装就可以了：\nsudo apt install python-tk\n参考:https://blog.csdn.net/qq_38784098/article/details/82017601?utm_source=blogxgwz0\n后来发现：python3本身就已包含turtle模块，不需要重新安装\n常用软件开发学习资料目录：\n1.经典编程电子书收藏\n2.C&C++编程学习资料收藏\n3.算法及数据结构（有关c,c++,java）\n4.Java开发学习资料收藏\n5.Android开发学习资料收藏\n6.Python开发学习资料收藏\n7.大数据，机器学习，人工智能资料收藏"}
{"content2":"前言\n特征是数据中抽取出来的对结果预测有用的信息，可以是文本或者数据。特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。过程包含了特征提取、特征构建、特征选择等模块。\n特征工程的目的是筛选出更好的特征，获取更好的训练数据。因为好的特征具有更强的灵活性，可以用简单的模型做训练，更可以得到优秀的结果。“工欲善其事，必先利其器”，特征工程可以理解为利其器的过程。互联网公司里大部分复杂的模型都是极少数的数据科学家在做，大多数工程师们做的事情基本是在数据仓库里搬砖，不断地数据清洗，再一个是分析业务不断地找特征。 例如，某广告部门的数据挖掘工程师，2周内可以完成一次特征迭代，一个月左右可以完成模型的小优化，来提升auc。\n1. 数据采集 / 清洗 / 采样\n数据采集：数据采集前需要明确采集哪些数据，一般的思路为：哪些数据对最后的结果预测有帮助？数据我们能够采集到吗？线上实时计算的时候获取是否快捷？\n举例1：我现在要预测用户对商品的下单情况，或者我要给用户做商品推荐，那我需要采集什么信息呢？\n-店家：店铺的评分、店铺类别……\n-商品：商品评分、购买人数、颜色、材质、领子形状……\n-用户：历史信息（购买商品的最低价最高价）、消费能力、商品停留时间……\n数据清洗： 数据清洗也是很重要的一步，机器学习算法大多数时候就是一个加工机器，至于最后的产品如何，取决于原材料的好坏。数据清洗就是要去除脏数据，比如某些商品的刷单数据。\n那么如何判定脏数据呢？\n1) 简单属性判定：一个人身高3米+的人；一个人一个月买了10w的发卡。\n2) 组合或统计属性判定：号称在米国却ip一直都是大陆的新闻阅读用户？你要判定一个人是否会买篮球鞋，样本中女性用户85%？\n3) 补齐可对应的缺省值：不可信的样本丢掉，缺省值极多的字段考虑不用。\n数据采样：采集、清洗过数据以后，正负样本是不均衡的，要进行数据采样。采样的方法有随机采样和分层抽样。但是随机采样会有隐患，因为可能某次随机采样得到的数据很不均匀，更多的是根据特征采用分层抽样。\n正负样本不平衡处理办法：\n正样本 >> 负样本，且量都挺大 => downsampling\n正样本 >> 负样本，量不大 =>\n1）采集更多的数据\n2）上采样/oversampling(比如图像识别中的镜像和旋转)\n3）修改损失函数/loss function (设置样本权重)\n2. 特征处理\n2.1 数值型\n1.  幅度调整/归一化：python中会有一些函数比如preprocessing.MinMaxScaler()将幅度调整到 [0,1] 区间。\n2.统计值：包括max, min, mean, std等。python中用pandas库序列化数据后，可以得到数据的统计值。\n3.离散化：把连续值转成非线性数据。例如电商会有各种连续的价格表，从0.03到100元，假如以一元钱的间距分割成99个区间，用99维的向量代表每一个价格所处的区间，1.2元和1.6元的向量都是 [0,1,0,…,0]。pd.cut() 可以直接把数据分成若干段。\n4.柱状分布：离散化后统计每个区间的个数做柱状图。\n2.2 类别型\n类别型一般是文本信息，比如颜色是红色、黄色还是蓝色，我们存储数据的时候就需要先处理数据。处理方法有：\n1. one-hot编码，编码后得到哑变量。统计这个特征上有多少类，就设置几维的向量，pd.get_dummies()可以进行one-hot编码。\n２. Hash编码成词向量：\n３. Histogram映射：把每一列的特征拿出来，根据target内容做统计，把target中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来。\n上表中，我们来统计“性别与爱好的关系”，性别有“男”、“女”，爱好有三种，表示成向量 [散步、足球、看电视剧]，分别计算男性和女性中每个爱好的比例得到：男[1/3, 2/3, 0]，女[0, 1/3, 2/3]。即反映了两个特征的关系。\n2.3 时间型\n时间型特征的用处特别大，既可以看做连续值（持续时间、间隔时间），也可以看做离散值（星期几、几月份）。\n连续值\na) 持续时间(单页浏览时长)\nb) 间隔时间(上次购买/点击离现在的时间)\n离散值\na) 一天中哪个时间段(hour_0-23)\nb) 一周中星期几(week_monday...)\nc) 一年中哪个星期\nd) 一年中哪个季度\ne) 工作日/周末\n数据挖掘中经常会用时间作为重要特征，比如电商可以分析节假日和购物的关系，一天中用户喜好的购物时间等。\n2.4 文本型\n１. 词袋：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋．\n２. 把词袋中的词扩充到n-gram：n-gram代表n个词的组合。比如“我喜欢你”、“你喜欢我”这两句话如果用词袋表示的话，分词后包含相同的三个词，组成一样的向量：“我 喜欢 你”。显然两句话不是同一个意思，用n-gram可以解决这个问题。如果用2-gram，那么“我喜欢你”的向量中会加上“我喜欢”和“喜欢你”，“你喜欢我”的向量中会加上“你喜欢”和“喜欢我”。这样就区分开来了。\n３. 使用TF-IDF特征：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF(t) = (词t在当前文中出现次数) / (t在全部文档中出现次数)，IDF(t) = ln(总文档数/ 含t的文档数)，TF-IDF权重 = TF(t) * IDF(t)。自然语言处理中经常会用到。\n2.5 统计型\n加减平均：商品价格高于平均价格多少，用户在某个品类下消费超过平均用户多少，用户连续登录天数超过平均多少...\n分位线：商品属于售出商品价格的多少分位线处\n次序型：排在第几位\n比例类：电商中，好/中/差评比例，你已超过全国百分之…的同学\n2.6 组合特征\n1. 拼接型：简单的组合特征。例如挖掘用户对某种类型的喜爱，对用户和类型做拼接。正负权重，代表喜欢或不喜欢某种类型。\n- user_id&&category: 10001&&女裙 10002&&男士牛仔\n- user_id&&style: 10001&&蕾丝 10002&&全棉\n2. 模型特征组合：\n- 用GBDT产出特征组合路径\n- 组合特征和原始特征一起放进LR训练\n3. 特征选择\n特征选择，就是从多个特征中，挑选出一些对结果预测最有用的特征。因为原始的特征中可能会有冗余和噪声。\n特征选择和降维有什么区别呢？前者只踢掉原本特征里和结果预测关系不大的， 后者做特征的计算组合构成新特征。\n3.1 过滤型\n- 方法：  评估单个特征和结果值之间的相关程度， 排序留下Top相关的特征部分。\n- 评价方式：Pearson相关系数， 互信息， 距离相关度。\n- 缺点：只评估了单个特征对结果的影响，没有考虑到特征之间的关联作用， 可能把有用的关联特征误踢掉。因此工业界使用比较少。\n- python包：SelectKBest指定过滤个数、SelectPercentile指定过滤百分比。\n3.2 包裹型\n- 方法：把特征选择看做一个特征子集搜索问题， 筛选各种特征子集， 用模型评估效果。\n- 典型算法：“递归特征删除算法”。\n- 应用在逻辑回归的过程：用全量特征跑一个模型；根据线性模型的系数(体现相关性)，删掉5-10%的弱特征，观察准确率/auc的变化；逐步进行， 直至准确率/auc出现大的下滑停止。\n- python包：RFE\n3.3 嵌入型\n- 方法：根据模型来分析特征的重要性，最常见的方式为用正则化方式来做特征选择。\n- 举例：最早在电商用LR做CTR预估， 在3-5亿维的系数特征上用L1正则化的LR模型。上一篇介绍了L1正则化有截断作用，剩余2-3千万的feature， 意味着其他的feature重要度不够。\n- python包：feature_selection.SelectFromModel选出权重不为0的特征。\n《注：以上总结来自于七月在线课程》"}
{"content2":"基础算法\n1 分治法\n2 动态规划法\n3 回溯法\n4 分支限界法\n5 贪心法\n排序算法\n1 快速排序\n2 归并排序\n3 堆排序\n4 基数排序\n5 希尔排序\n6 插入排序\n7 冒泡排序\n8 选择排序\n查找算法\n1 数值查找算法\n1.1 二分查找算法\n1.2 分块查找算法\n1.3 哈希查找算法\n2 字符串查找算法\n2.1 KMP算法\n2.2 BM算法\n2.3 Sunday算法\n3 海量数据查找算法\n3.1 布隆过滤器\n3.2 倒排索引查找\n压缩编码\n1 哈夫曼编码\n2 香农-范诺编码\n距离计算\n1 欧氏距离\n2 马氏距离\n3 曼哈顿距离\n4 切比雪夫距离\n5 闵式距离\n6 海明距离\n路径分析算法\n1 Dijkstra（迪杰斯特拉）算法\n2 Floyd算法\n3 A*算法\n4 维特比算法\n5 最长公共子序列\n相似度分析算法\n1 Jacard相似系数\n2 MinHash相似性算法\n3 向量空间模型\n4 余弦相似性算法\n5 语义主题模型\n6 基于SimHash算法的指纹码\n数据分类算法\n1 基于朴素贝叶斯分类器\n2 基于AdaBoost分类器\n3 基于支持向量机的分类器\n4 基于K近邻（KNN）算法的分类器\n数据聚类算法\n1 基于系统聚类法\n2 基于K均值（K-Means）聚类算法\n3 基于密度的DBSCAN算法\n4 基于BIRCH算法\n数据预测算法\n1 基于最大似然估计\n2 基于线性回归\n3 基于最大期望算法\n4 基于隐马尔可夫模型\n5 基于条件随机场的序列\n数据决策分析算法\n1 基于ID3算法\n2 基于C4.5算法\n3 基于分类回归树\n4 基于随机森林\n数据关联规则分析算法\n1 基于Apriori算法\n2 基于FP-Growth算法\n3 基于Eclat算法\n数据推荐算法\n1 基于Item-Based协同过滤推荐\n2 基于User-Based协同过滤推荐\n3 基于潜在因子算法的推荐"}
{"content2":"注：在上一篇的一般线性回归中，使用的假设函数是一元一次方程，也就是二维平面上的一条直线。但是很多时候可能会遇到直线方程无法很好的拟合数据的情况，这个时候可以尝试使用多项式回归。多项式回归中，加入了特征的更高次方（例如平方项或立方项），也相当于增加了模型的自由度，用来捕获数据中非线性的变化。添加高阶项的时候，也增加了模型的复杂度。随着模型复杂度的升高，模型的容量以及拟合数据的能力增加，可以进一步降低训练误差，但导致过拟合的风险也随之增加。\n图A，模型复杂度与训练误差及测试误差之间的关系\n0. 多项式回归的一般形式\n在多项式回归中，最重要的参数是最高次方的次数。设最高次方的次数为$n$，且只有一个特征时，其多项式回归的方程为：\n$$ \\hat{h} = \\theta_0 + \\theta_1 x^1 + \\ ... \\  + \\theta_{n-1} x^{n-1} +  \\theta_n x^n $$\n如果令$x_0 = 1$，在多样本的情况下，可以写成向量化的形式：\n$$\\hat{h} = X \\cdot \\theta$$\n其中$X$是大小为$m \\cdot (n+1)$的矩阵，$\\theta$是大小为$(n+1) \\cdot 1$的矩阵。在这里虽然只有一个特征$x$以及$x$的不同次方，但是也可以将$x$的高次方当做一个新特征。与多元回归分析唯一不同的是，这些特征之间是高度相关的，而不是通常要求的那样是相互对立的。\n在这里有个问题在刚开始学习线性回归的时候困扰了自己很久：如果假设中出现了高阶项，那么这个模型还是线性模型吗？此时看待问题的角度不同，得到的结果也不同。如果把上面的假设看成是特征$x$的方程，那么该方程就是非线性方程；如果看成是参数$\\theta$的方程，那么$x$的高阶项都可以看做是对应$\\theta$的参数，那么该方程就是线性方程。很明显，在线性回归中采用了后一种解释方式。因此多项式回归仍然是参数的线性模型。\n1. 多项式回归的实现\n下面主要使用了numpy、scipy、matplotlib和scikit-learn，所有使用到的函数的导入如下：\n1 import numpy as np 2 from scipy import stats 3 import matplotlib.pyplot as plt 4 from sklearn.preprocessing import PolynomialFeatures 5 from sklearn.linear_model import LinearRegression 6 from sklearn.metrics import mean_squared_error\n下是使用的数据是使用$y = x^2 + 2$并加入一些随机误差生成的，只取了10个数据点：\n1 data = np.array([[ -2.95507616, 10.94533252], 2 [ -0.44226119, 2.96705822], 3 [ -2.13294087, 6.57336839], 4 [ 1.84990823, 5.44244467], 5 [ 0.35139795, 2.83533936], 6 [ -1.77443098, 5.6800407 ], 7 [ -1.8657203 , 6.34470814], 8 [ 1.61526823, 4.77833358], 9 [ -2.38043687, 8.51887713], 10 [ -1.40513866, 4.18262786]]) 11 m = data.shape[0] # 样本大小 12 X = data[:, 0].reshape(-1, 1) # 将array转换成矩阵 13 y = data[:, 1].reshape(-1, 1) 14 plt.plot(X, y, \"b.\") 15 plt.xlabel('X') 16 plt.ylabel('y') 17 plt.show()\n这些数据点plot出来，如下图：\n-1，原始数据\n1.1 直线方程拟合\n下面先用直线方程拟合上面的数据点：\n1 lin_reg = LinearRegression() 2 lin_reg.fit(X, y) 3 print(lin_reg.intercept_, lin_reg.coef_) # [ 4.97857827] [[-0.92810463]] 4 5 X_plot = np.linspace(-3, 3, 1000).reshape(-1, 1) 6 y_plot = np.dot(X_plot, lin_reg.coef_.T) + lin_reg.intercept_ 7 plt.plot(X_plot, y_plot, 'r-') 8 plt.plot(X, y, 'b.') 9 plt.xlabel('X') 10 plt.ylabel('y') 11 plt.savefig('regu-2.png', dpi=200)\n-2，直线拟合的效果\n可以使用函数\"mean_squared_error\"来计算误差(使用前面介绍过的Mean squared error, MSE)：\nh = np.dot(X.reshape(-1, 1), lin_reg.coef_.T) + lin_reg.intercept_ print(mean_squared_error(h, y)) # 3.34\n1.2 使用多项式方程\n为了拟合2次方程，需要有特征$x^2$的数据，这里可以使用函数\"PolynomialFeatures\"来获得：\n1 poly_features = PolynomialFeatures(degree=2, include_bias=False) 2 X_poly = poly_features.fit_transform(X) 3 print(X_poly)\n结果如下：\n[[-2.95507616 8.73247511] [-0.44226119 0.19559496] [-2.13294087 4.54943675] [ 1.84990823 3.42216046] [ 0.35139795 0.12348052] [-1.77443098 3.1486053 ] [-1.8657203 3.48091224] [ 1.61526823 2.60909145] [-2.38043687 5.66647969] [-1.40513866 1.97441465]]\n利用上面的数据做线性回归分析：\n1 lin_reg = LinearRegression() 2 lin_reg.fit(X_poly, y) 3 print(lin_reg.intercept_, lin_reg.coef_) # [ 2.60996757] [[-0.12759678 0.9144504 ]] 4 5 X_plot = np.linspace(-3, 3, 1000).reshape(-1, 1) 6 X_plot_poly = poly_features.fit_transform(X_plot) 7 y_plot = np.dot(X_plot_poly, lin_reg.coef_.T) + lin_reg.intercept_ 8 plt.plot(X_plot, y_plot, 'r-') 9 plt.plot(X, y, 'b.') 10 plt.show()\n第3行得到了训练后的参数，即多项式方程为$h = -0.13x + 0.91x^2 + 2.61$ （结果中系数的顺序与$X$中特征的顺序一致），如下图所示：\n-3：2次多项式方程与原始数据的比较\n利用多项式回归，代价函数MSE的值下降到了0.07。通过观察代码，可以发现训练多项式方程与直线方程唯一的差别是输入的训练集$X$的差别。在训练直线方程时直接输入了$X$的值，在训练多项式方程的时候，还添加了我们计算出来的$x^2$这个“新特征”的值（由于$x^2$完全是由$x$的值确定的，因此严格意义上来讲此时该模型只有一个特征$x$）。\n此时有个非常有趣的问题：假如一开始得到的数据就是上面代码中\"X_poly\"的样子，且不知道$x_1$与$x_2$之间的关系。此时相当于我们有10个样本，每个样本具有$x_1, x_2$两个不同的特征。这时假设函数为：$$\\hat{h} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$\n直接按照二元线性回归方程来训练，也可以得到上面同样的结果（$\\theta$的值）。如果在相同情况下，收集到了新的数据，可以直接带入上面的方程进行预测。唯一不同的是，我们不知道$x_2 = x_1^2$这个隐含在数据内部的关系，所有也就无法画出-3中的这条曲线。一旦了解到了这两个特征之间的关系，数据的维度就从3维下降到了2维（包含截距项$\\theta_0$）。\n2. 持续降低训练误差与过拟合\n在上面实现多项式回归的过程中，通过引入高阶项$x^2$，训练误差从3.34下降到了0.07，减小了将近50倍。那么训练误差是否还有进一步下降的空间呢？答案是肯定的，通过继续增加更高阶的项，训练误差可以进一步降低。通过尝试，当最高阶项为$x^{11}$时，训练误差为3.11e-23，几乎等于0了。\n下面是测试不同degree的过程：\n1 # test different degree and return loss 2 def try_degree(degree, X, y): 3 poly_features_d = PolynomialFeatures(degree=degree, include_bias=False) 4 X_poly_d = poly_features_d.fit_transform(X) 5 lin_reg_d = LinearRegression() 6 lin_reg_d.fit(X_poly_d, y) 7 return {'X_poly': X_poly_d, 'intercept': lin_reg_d.intercept_, 'coef': lin_reg_d.coef_} 8 9 degree2loss_paras = [] 10 for i in range(2, 20): 11 paras = try_degree(i, X, y) 12 h = np.dot(paras['X_poly'], paras['coef'].T) + paras['intercept'] 13 _loss = mean_squared_error(h, y) 14 degree2loss_paras.append({'d': i, 'loss': _loss, 'coef': paras['coef'], 'intercept': paras['intercept']}) 15 16 min_index = np.argmin(np.array([i['loss'] for i in degree2loss_paras])) 17 min_loss_para = degree2loss_paras[min_index] 18 print(min_loss_para) # 19 X_plot = np.linspace(-3, 1.9, 1000).reshape(-1, 1) 20 poly_features_d = PolynomialFeatures(degree=min_loss_para['d'], include_bias=False) 21 X_plot_poly = poly_features_d.fit_transform(X_plot) 22 y_plot = np.dot(X_plot_poly, min_loss_para['coef'].T) + min_loss_para['intercept'] 23 fig, ax = plt.subplots(1, 1) 24 ax.plot(X_plot, y_plot, 'r-', label='degree=11') 25 ax.plot(X, y, 'b.', label='X') 26 plt.xlabel('X') 27 plt.ylabel('y') 28 ax.legend(loc='best', frameon=False) 29 plt.savefig('regu-4-overfitting.png', dpi=200)\n输出为：\n{'coef': array([[ 0.7900162 , 26.72083627, 4.33062978, -7.65908434, 24.62696711, 12.33754429, -15.72302536, -9.54076366, 1.42221981, 1.74521649, 0.27877112]]), 'd': 11, 'intercept': array([-0.95562816]), 'loss': 3.1080267005676934e-23}\n画出的函数图像如下：\n-1：degree=11时的函数图像\n由-1可以看到，此时函数图像穿过了每一个样本点，所有的训练样本都落在了拟合的曲线上，训练误差接近与0。 可以说是近乎完美的模型了。但是，这样的曲线与我们最开始数据的来源（一个二次方程加上一些随机误差）差异非常大。如果从相同来源再取一些样本点，使用该模型预测会出现非常大的误差。类似这种训练误差非常小，但是新数据点的测试误差非常大的情况，就叫做模型的过拟合。过拟合出现时，表示模型过于复杂，过多考虑了当前样本的特殊情况以及噪音（模型学习到了当前训练样本非全局的特性），使得模型的泛化能力下降。\n出现过拟合一般有以下几种解决方式：\n降低模型复杂度，例如减小上面例子中的degree；\n降维，减小特征的数量；\n增加训练样本；\n添加正则化项.\n防止模型过拟合是机器学习领域里最重要的问题之一。鉴于该问题的普遍性和重要性，在满足要求的情况下，能选择简单模型时应该尽量选择简单的模型。\nReference\nhttp://scikit-learn.org/stable/modules/linear_model.html\nGéron A. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems[M]. \" O'Reilly Media, Inc.\", 2017. github\nhttps://www.arxiv-vanity.com/papers/1803.09820/"}
{"content2":"机器学习的一些资料\n1. 机器学习与概率、统计学\n1.1 公开课\n斯坦福机器学习入门课程，讲师为Andrew Ng，适合数学基础一般的人，适合入门，但是学完会发现只是懂个大概，也就相当于什么都不懂。\nCoursera上国立台湾大学林轩田开的两门课：机器学习基石（适合入门），机器学习技法（适合提高）。\n1.2 推荐书目\n统计和概率是机器学习的基础，所以统计和概率一定要学好。\n推荐必读\n统计学和机器学习结合的入门书：统计学习方法，作者李航。这本书介绍了10个机器学习领域的代表性算法，讲的非常好，书本身也就200多页，很适合入门，强烈推荐。\n机器学习入门书：周志华老师的西瓜书[机器学习](https://github.com/allmachinelearning/MachineLearning/blob/master/books/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E5%91%A8%E5%BF%97%E5%8D%8E.pdf),语言简单易懂，一度登录亚马逊，京东计算机类畅销书榜首，很适合入门阅读，相比于李航老师的书，这本书更片算法和应用 （李航老师的书更篇数学），先读李航老师的书，再读这本，效果更佳。\n机器学习理论方面：\n推荐 著名教材《Pattern Recognition》中文版为《模式分类》第二版，从数学的角度讲了机器学习的方方面面，非常不错。这本书好的地方是，书中所有的算法都有数学推导，讲的也很全面，作者的眼光很独到，习题和上机题也非常有挑战性；不好的地方就是，对现在流行的比如boosting和SVM等，书只是略讲了一下，可能需要额外再补充知识。不过即便如此，当我最近一个月看到这书时立刻就感叹为什么我当初上课的时候没有赶上这本书！\n权威学者Kevin P. Murphy的著作Machine Learning:A Probabilistic Perspective(MLAPP) ，非常厚，偏重数学理论，难度高，是学习机器学习理论的教材，我们上课用书。\n用python进行数据处理的书：利用python进行数据分析，适合浏览，偏重工程实践，介绍常用的python处理数据的方法和函数等，可以看看。\n机器学习实战，这个和第3点说的这个书配合看，效果不错，这个书很有针对性，每个算法有一个实际问题，有源程序让你去写，不错。\n学习统计不能不说的经典的《统计学习基础》，我暂时还没看过。\n以及业界久负盛名的PRML（模式识别与机器学习），我这里中文英文版都有。\n1.3. 工具\n第三方库 机器学习有很多开源库可以直接拿来用，github是个不错的获取代码的网站，比较著名的有：    * libsvm，作者是林智仁，是svm的标准库。\nscikit-learn，scikit包是python中著名的处理数据的包，其中内置了几乎所有流行的机器学习算法，配合python简洁的语法操作，使用起来很方便。\npandas，python的一个包，其中对表的处理比较出色，我只是试用过。\npylearn2,这个我没有接触过，不过在github上排名很靠前，应该不错。\nsmile，彪悍的机器学习库，用java开发，有自然语言处理方面的库。\noryx，处理大规模数据很彪悍。\nmlpack,老牌可靠c++编写的机器学习库。\n工具 python、java以及matlab三种语言是目前比较流行的机器学习方面的语言。\npython：强在各种包，几乎无所不能，其科学计算部分（scikit）和matlab不相上下。用Flask当作web框架，几乎可以直接部署成web项目，非常方便，强烈推荐。\njava：强在其企业级应用能力和稳定性。\nmatlab：强在其科学计算部分上。matlab我用的不多，因为目前来看，它和python的科学计算差不多。\n网上的教程 这是在一个妹子的微信朋友圈看到的文章，很不错。\n2. 数据挖掘\n快速入门：看我去年考试写的总结数据挖掘复习整理，可快速了解数据挖掘的相关概念。这个总结去年在雁栖湖传的很火，还有妹子给我发短信感谢。。。\n数据挖掘领域的经典著作：数据挖掘：概念与技术，作者是韩家炜，是标准的上课教材，上一部分我的总结中对应的页码就是这个书里的。\n3. 人工智能\n教材：著名的教材人工智能：一种现代方法，很厚，可以翻看。"}
{"content2":"Machine Learning：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n一、什么是机器学习？（What）\n上面的概念那面有些抽象，要理解什么是机器学习，其实可以类比于人类的学习。假设一个场景：父母教三岁大的孩子认识什么是西瓜。首先父母要给孩子看西瓜，然后告诉孩子这个是西瓜。这个过程抽象成下面这张流程图：\n孩子的学习过程是通过一些观察，然后加上自我总结，就逐渐拥有了识别什么是西瓜的技能。\n与之相类似，机器学习就是通过对数据的“学习”，从而拥有某些特定技能的过程。（那技能是什么呢？技能是某种表现的增进，例如我通过天天练习投篮，逐渐命中率达到一定水准，我就拥有了投篮技能。）\n二、为什么要使用机器学习？（Why）\n对于不同的问题，我们选择机器学习的原因不同，这些问题按照对于人类的难易来说，可以分为两类：\n1.很简单的问题：\n例如：识别手写的数字是几，这对于人类来说是一个极易的问题，那为什么我们还要使用机器学习呢？因为通过机器学习的方式，我们可以让机器来帮我们做这些“单调乏味”的工作，让机器掌握识别手写数字的技能之后，我们就能让他们帮我们来分拣信件。如果把这项技能再进一步，让机器掌握识别并理解地址信息，我们甚至可以用机器来分拣快递。\n2.很复杂的问题：\n例如：根据用户信息，给不同的用户推荐不同的广告。对于人类来说，这几乎是不可能的问题，因为不同的用户有不同的特征，怎么从这些海量的特征中判断每一个用户的喜好是什么呢？并且用户的喜好可能还在发生变化，举个通俗一点的例子，比如某些用户去年还喜欢吃西瓜，今年就讨厌西瓜了。因此，从海量的数据中去人为的总结规律有很大的局限性，而使用机器学习的方式使得这类问题变得简单。\n三、什么时候可以使用机器学习？（When）\n1.存在某些隐含的模式（underlying pattern）可以被学习到。\n举一个反例：预测某个婴儿下一次哭是奇数分钟哭，还是偶数分钟哭。这几乎是一个随机事件，所以不存在任何隐含的模式，因此也就不能使用机器学习的方式来解答。\n2.不是很容易总结规律的问题。\n如果规律很容易总结，那当然就不用使用机器学习了，直接代码实现即可。\n3.有相关的数据。\n举一个反例：预测世界末日什么时候到来。显然，因为我们没有世界末日发生的相关数据，所以这个问题也无法用机器学习的方式来解答。\n四、机器学习的组成\n上图可以看作是整个机器学习过程的组成。\n首先从我们的数据出发，它们的具体形式是x到y的映射，比如我们的问题是预测某套房子的房价。那么我的X包括房子面积、房子楼层、是否是电梯房等等特征，Y就是房子的价格。正因为存在某种规律，才使得我们获得的数据按照训练集的每一个x对应唯一一个y。我们用 f 来表示这个X到Y的映射关系(即隐含规律)。\n那么如何去求得这个 f 呢？一般地，我们首先会把范围扩大到一个假设集合（称作 H ），比如我们假设房子价格与它的所有特征都是呈线性关系，那么这个假设集合就是一个线性假设集，那如何在这个无限大的假设集 H 中找到我们想要得到的 f 呢，那就需要通过机器学习演算法来实现，我们用 A 表示这个算法。\n那这样我们就能求得 f 了吗？答案是否定的，因为很多时候由于数据噪声等原因，我们无法获得一个精确的 f ,而只能获得某一个映射关系 g ，使得 g 最接近这个 f，我们把这个 g 称为最终预测。\n一般地，机器学习为了求得隐含的规律，在假设集合上通过演算法对训练集进行“训练”，使求得的预测规律最接近隐含规律。\n五、机器学习、数据挖掘、人工智能、统计学的关系\n机器学习：\n利用数据计算出一个近似隐含规律的预测规律。\n数据挖掘：\n利用海量的数据去寻找一些有意思的数据特征、规律。\n机器学习与数据挖掘的关系：\n如果数据挖掘的目标是去寻找数据间的隐含规律，那么数据挖掘和机器学习做的事情是一样的。但是传统的数据挖掘总是着眼于大数据的高效计算。（比如分布式计算框架）\n人工智能：\n让计算机做(模拟)一些像人一样的智能行为。\n机器学习与人工智能的关系：\n机器学习是实现人工智能的一种途径，除此之外，还有一些其他的方式可以实现人工智能。\n统计学：\n利用数据对一些未知的过程做预测。\n机器学习与统计学的关系：\n统计学的很多知识都可以被借鉴到机器学习中，但是机器学习中也有很多算法不是基于统计学的规律的。\n参考资料：\n《机器学习基石》【林轩田】(来源于Youtube)"}
{"content2":"第一步：\nAmbari安装之Ambari安装前准备(CentOS6.5）（一）\n第二步：\nAmbari安装之部署本地库（镜像服务器）（二）\n第三步：\nAmbari安装之安装并配置Ambari-server（三）\n第四步：\nAmbari安装之部署hdp单节点集群\n第五步：\nAmbari安装之部署3个节点的HA分布式hdp集群\n成功！\n欢迎大家，加入我的微信公众号：大数据躺过的坑        人工智能躺过的坑\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）"}
{"content2":"首先用数据说话，看看资料大小，达到675G\n承诺：真实资料、不加密,获取资料请加QQ：122317653\n包含内容：1.python基础+进阶+应用项目实战\n2.神经网络算法+python应用\n3.人工智能算法+python应用\n4.机器学习算法+python应用\n在python全套教程中包括黑马程序员2017年12月python视频\n内容包括: linux知识、python基础编程、python高级编程、前端、数据库、数据结构、shell和运维等\n见图："}
{"content2":"数据挖掘与机器学习是两个不同的概念； 数据挖掘中使用到机器学习的各种工具，而自然语言处理也是是一种机器学习的方式，属于数据挖掘的范畴。 数据挖掘（英语：Data mining），又译为资料探勘、数据采矿。它是数据库知识发现 （英语：Knowledge-Discovery in Databases，简称：KDD) 中的一个步骤。数据挖掘一般是指从大量的数据中自动搜索隐藏于其中的有着特殊关系性 （属于Association rule learning）的信息的过程。 数据挖掘通常与计算机科学有关，并通过统计、在线分析处理、情报检索、机器学习、 专家系统（依靠过去的经验法则）和模式识别等诸多方法来实现 上述目标。 机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、 凸分析、算法复杂度理论等多门学科。 专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构 使之不断改善自身的性能。 它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域， 它主要使用归纳、综合而不是演绎。 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用 自然语言进行有效通信的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言， 即人们日常使用的语言， 所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言， 而在于研制能有效地实现自然语言通信的 计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。 自然语言处理(NLP)是计算机科学，人工智能，语言学关注计算机和人类(自然)语言之间的相互作用的领域。"}
{"content2":"写在前面的废话：\n好吧，不得不说鱼C的markdown文本编辑器挺不错的，功能齐全。再次感谢小甲鱼哥哥的python视频让我去年大三下学期的时候入门了编程，爱上了编程这门语言，由于是偏冷门的统计学，在实习以后就决定把方向放在数据挖掘方面了，越来越发现专业课的重要性。在大家都忙着参加各种培训的日子里面，我就在去年寒冷的冬天把甲鱼哥的python视频一字不落的看完了；现在，在别人拼命参加校招的日子里，我就来学习“机器学习”这里面的算法了（ps：工科学校的理科妹子表示很难找到数据分析工作，人家只要研究生）。好吧，我就不信邪了，硬是开启了持续两个月的Ng教授的coursera上面的“机器学习”课程（里面的assignment很简单，使用matlab完成），刚好实验室进货一本《机器学习实战》，也就拿来练练手，让自己的python进阶一下，之前各种web后台折腾，尤其是爬虫，然而我不想帮别人爬数据，我要分析数据，挖掘潜在信息，程序是工具，掌握业务趋势才是王道！\n不废话了，接下来的笔记系列都是我在coursera上面的领悟，根据自己的手写笔迹以及《机器学习实战》这本书的代码得来的，希望不习惯更新博客的我能把这件事情坚持下来。加油！\n正文：\n这两年估计很多人都听说过“大数据”，目前机器学习也在悄无声息的进入到部分数据挖掘领域。当然，国外数据挖掘已经很成熟了，机器算法应用的范围也就更加广泛，分别有：网络搜索，邮件分类；机器人；生物和医药学研究等等。\n这里举几个具体例子：\n网站数据：你可以根据网站的点击数据了解产品的欢迎程度；\n医疗数据：根据医疗记录了解病人的病情方便诊断；\n生物方面：比如基因DNA序列可以用于研究人类的某些特质甚至遗传方面的信息；\n工程领域：指导无人机自主运行，手写字体的识别，NLP(Natural Language Processing 俗称”自然语言处理“)，以及计算机视觉;\n推荐系统：亚马逊的产品推荐系统（貌似这个也可以被分到网站数据）。\n那么啰嗦了这么多，什么是机器学习呢？\n这里有两种定义：\n通俗点讲：研究让机器拥有人一样的学习能力，该能力不被固定的编程实现或操作，属于机器本身的一种自主学习行为。\n学术点讲：通过经验E，针对某些任务T，设计出一段计算机程序，该程序拥有特定的绩效指标P，程序的目的就是根据历史经验E的不断的积累在任务T中提高它的绩效指标P。\n学术就是学术，太生硬了，也是我不考研的原因之一，so boring~通俗的举个例子：\n下跳棋：\nE = 玩了多局跳棋所累积的经验\nT = 下跳棋本身就是一个任务\nP = 程序赢得下一次跳棋的可能性\n机器学习主要包括两个任务：分类和回归。前者非常容易理解，就是在一个预测任务中把数据分类；后者回归主要是统计意义上的，用于预测数据，做过数学建模的同学估计对拟合曲线相当熟悉；是的，回归里面一个非常重要的任务——数据拟合曲线：通过给定的数据集合拟合出最优曲线，使得该曲线尽量能够反应数据的趋势，在不过度拟合的情况下能让给定的数据集落在线附近（上）。而机器学习包括“监督学习”和“非监督学习”，那么分类和回归都属于“监督学习”。接下来抛砖引玉，本文的重点就是区分“监督学习”和“非监督学习”，后面的文章中讲分别对这两种学习进行细分，甚至回归和分类里面的细节更是数不甚数。\nSupervised Study\nExample 1：房价预测（线性回归）\n假设，你手上有一堆房价以及房子大小面积的数据，让你根据房子大小估算房价，然后你根据数据得出下图（图太丑，不许勿喷）\n你根据数据的分布分别拟合直线和曲线，两种拟合的线在x1这个点预测分别得到y1和y2；因此，不同的曲线对应不同的预测结果。那么，为什么我说这里的房价预测就是一种“监督学习”呢？因为有确定的答案被给出了，也就是说在数据集中，不同的房子的面积分别对应着不同的房价。也就是说，这类算法明确的知道自己预测的是什么（本例中预测房价），目标变量非常明确。\n以上问题也被称为回归问题：预测连续的输出值。\nExample 2：肿瘤癌的预测：良性肿瘤和恶性肿瘤（Logistic回归）\n上图中的“×”符号代表的就是数据集，指的是不同肿瘤大小对应是否为恶性肿瘤（1），如果是恶性肿瘤，那么对应数值1；反之对应数值2。这就是一个典型的二值化问题，也被称为（Logistic回归问题），常用于分类：离散的输出值（0或者1）。\n当然，在实际的预测中，肿瘤是否恶性的判断需要依据很多属性，比如：肿瘤块的厚度、细胞的形状等等，而影响肿瘤大小的因素也有很多，比如年龄等等。这么多属性，如果都用画图的方式来拟合数据，就显得比较低效率，因此，我们引入了“向量机”，以后我们会讨论到这个问题，有兴趣的可以谷歌一下。\nUnsupervised Study\n顾名思义就是没有给定的正确的答案\n先上图：\n单纯是给一堆数据，如上图的黑色小圆圈代表数据集，让你找到这些数据的结构特点，也就是聚类（正所谓：物以类聚，人以群分）。很明显，你没有标准答案，因此既可以把数据按照红色的椭圆形聚为2类，又可以按照紫色的线条圈起来的范围聚为3类，还可以按照蓝色正方形圈为2类，没有人说你这种聚类是错误的，只要你说出你的理由。\n看起来，非监督学习无理可循，但是应用范围相当广泛：组织计算机集群，社交网络分析，市场份额分割以及天文数据分析。在未来大数据下需要被探索的东西太多，未知数也往往深不可测，因此非监督学习这个学科的“水”相当深~\n嗯，暂且介绍到这里，老衲要午睡了，下午还要上courera的课程，第四周了，欢迎有兴趣的朋友和我成为同学~\n下集预告：线性回归以及梯度下降算法。"}
{"content2":"selenium是一个用于web应用程序测试的工具，Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE、Mozilla Firefox、Mozilla Suite等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建衰退测试检验软件功能和用户需求。支持自动录制动作和自动生成。Net、Java、Perl等不同语言的测试脚本。Selenium 是ThoughtWorks专门为Web应用程序编写的一个验收测试工具。\n要使用selenium之前呢必须得先下载浏览器驱动器，我知道的浏览器只有这些\n，Chrome浏览器 指定浏览器 Firefox ：火狐 Opera 歌剧院 ， Ie浏览器，Safari 苹果浏览器\n其中我只用过火狐和Chrome浏览器\nChrome浏览器的驱动下载地址是：http://chromedriver.storage.googleapis.com/index.html\n要下载的版本必须是你现在使用的版本最近的或一样，然后把你下载的驱动解压放在你的浏览器的文件中就可以来了    要是不行的话就重启一下\n火狐浏览器的驱动下载地址是：https://github.com/mozilla/geckodriver/releases\n他的操作也跟上边的一样 ，不过他的驱动就下最新版的就可以了\n接下来就是selenium操作了\n#导报\nfrom selenium import webdriver\nimport time\nimport requests\n\n#建立浏览器对象 ,Chrome浏览器 指定浏览器 Firefox ：火狐 Opera 歌剧院 ， Ie浏览器，Safari 苹果浏览器\nbrowser = webdriver.Chrome()\n#使用浏览器访问网站\nbrowser.get('https://www.baidu.com')\n#向文本框填充文本\nbrowser.find_element_by_id(\"kw\").send_keys('男神鹏')\ntime.sleep(1)\n#模拟点击\nbrowser.find_element_by_id(\"su\").click()\ntime.sleep(5)\n#匹配多个节点\n# elist = browser.find_elements('css selector','h3')\nelist = browser.find_elements_by_class_name('t')\ntext_str = str(elist[0].text)\nprint(text_str)\n#点击链接\nbrowser.find_element_by_link_text(text_str).click()\n#暂停\ntime.sleep(50)\n#关闭浏览器\nbrowser.quit()\n注意 ：每次连接一个地址都给浏览器缓冲时间   一秒也行，最后别忘了关闭浏览器，这是我简单的一个操作流程， 他的流程就是先打开百度 然后 搜索：男生鹏  然后进第一个页面  。\n下面的就是一些selenium操作\n最简单粗暴却失传已久的8种定位\n据说这种定位方式在江湖上都快要失传了，实在想不通为什么，明明写起来最简单粗暴啊~\ndriver.find_element(\"name\",\"wd\").send_keys(\"Selenium2\")\ndriver.find_element(\"id\",\"su\").click()\n大家一定会和我一样觉得这种方式的定位实在是太省事了~只要写find_element就好啦，下面我们来总结一下这8种写法与基本定位方法类比过来该怎么写：\nby_id -> find_element(\"id\",\"\")\nby_xpath -> find_element(\"xpath\",\"\")\nby_link_text -> find_element(\"link text\",\"\")\nby_partial_text -> find_element(\"partial link text\",\"\")\nby_name -> find_element(\"name\",\"\")\nby_tag_name -> find_element(\"tag name\",\"\")\nby_class_name -> find_element(\"class name\",\"\")\nby_css_selector -> find_element(\"css selector\",\"\")\nelements复数定位\n在上面的例举的八中基本定位方式种，都有对应的复数形式，分别是下面这些：\nid复数定位find_elements_by_id()\nname复数定位find_elements_by_name()\nclass复数定位find_elements_by_class_name()\ntag复数定位find_elements_by_tag_name()\nlink复数定位find_elements_by_link_text()\npartial_link复数定位find_elements_by_partial_link_text()\nxpath复数定位find_elements_by_xpath()\ncss复数定位find_elements_by_css_selector()\n这些复数定位方式每次取到的都是具有相同类型属性的一组元素，所以返回的是一个list队列，我们也可以利用这个去定位单个的元素。比如百度首页种，右上角有新闻、视频、地图、贴吧等一些链接，我们通过f12查看源码可以发现，这些链接都有共同的class， class=\"mnav\"。\nselenium基础操作就是这些了 ，希望对你们有帮助！！"}
{"content2":"AI应用开发实战\n出发点\n目前，人工智能在语音、文字、图像的识别与解析领域带来了跨越式的发展，各种框架、算法如雨后春笋一般，互联网上随处可见与机器学习有关的学习资源，各大mooc平台、博客、公开课都推出了自己的学习资料。\n在当前学习资料十分丰富的这个环境下，本教程从上手的角度，让大家能够真正动手开始进行AI的开发，提高大家的开发生产力水平，而不是简单地学习人工智能的底层算法和理论却迟迟不动手。\n建议和反馈，请发送到\nhttps://github.com/Microsoft/vs-tools-for-ai/issues\n联系我们\nOpenmindChina@microsoft.com\n适用人群\n本手册适用以下所有人群，各位乘客请对号入座：\n人群\n描述\n一句话侧写\n没有AI开发经验的小白\n是否在AI技术的边缘不断徘徊，担心入门门槛太高，自己会被淹没在AI中，看着周围的人都上了AI这班车，担心自己不能抓住这个浪潮？\n“在AI技术的边缘试探”\n想要构建AI应用却苦于没有好的模型和算法的老板、开发人员\n看到行业内已经开发出了五花八门的AI应用，从聊天机器人到智能家居，自己却还没有可用的工具和模型？\n“睡一觉，醒来就有可以用的模型和接口了！”\n有过AI开发经验，训练过模型，调过参的初阶开发者\n在开发过程中花了大把时间和精力配置开发环境、训练模型、调参数？\n“我可能配了假环境，训练了假模型，调了假参数”\n手拥大把计算资源的高阶开发者，实验室主管、公司技术负责人\n需要一个方便使用的工具来对这些计算资源进行集中管理？\n“手握显卡山，根本烧不完”\n手册内容\n以下是本手册的概览：\n本手册以开发者熟悉的IDE: Visual Studio 或者 Visual Studio Code及其上的Tools for AI插件为基础，覆盖了使用Visual Studio进行AI开发的四大场景。\n使用机器学习框架训练模型，使用模型搭建应用\n使用Cognitive Service搭建AI应用程序\n使用Custom Vision搭建AI应用程序\n使用Open Platform for AI - PAI 进行计算资源的统一调度与管理\n不管你是小白还是技术大佬，本手册都能助你杀怪升级。\n手册目录：\n编号\n链接\n简介\n1\n概述\n介绍本系列教程的内容、可能使用到的工具以及你的收获\n2\nVS 2017 + Win 环境搭建与模型训练\n在Windows下进行环境的搭建，训练自己的第一个模型\n3\nVS code + mac\n在iOS下搭建开发环境，训练自己的第一个模型\n4\n使用训练好的模型构建应用\n使用上一步训练好的模型，开发一个简单的桌面程序\n5\nCustom Vision服务\n使用Custom Vision，根据自己的需求训练模型\n6\n使用Cognitive Service构建应用\n使用Cognitive Service提供的接口，进行AI应用的开发\n7\nOpenPAI微软开源GPU集群管理利器（视频）\n介绍OpenPAI的使用场景与功能\n8\nOpenPAI的快速部署安装\n-\n9\n深入介绍工具的应用\n以2、3、4完成简单应用为基础，深入介绍机器学习模型应该怎么应用到程序中\n10\n常见问题\n-\n下列视频请与手册配套使用，效果更佳：\n【教程】Mac下一小时搭好机器学习开发环境\n【教程】Windows下一小时搭好机器学习开发环境\n学成收获\n当你对本手册进行了完整的学习之后，你将在下列几个方面获得可观的收获，同时你的AI开发生产力将大大提高：\n学会使用在Visual Studio下训练模型，使用模型搭建应用\n学会使用Cognitive Service搭建AI应用程序\n学会使用Custom Vision搭建AI应用程序\n学会搭建PAI，并PAI进行计算资源的统一调度与管理\n工具介绍\nVisual Studio：\nVS是一个功能完备的集成开发环境，对开发、测试、debug等使用场景提供了强大的支持，你除了可以使用vs进行windows应用的开发，同时还可以开发安卓、iOS的移动应用；你甚至能在VS上进行web应用的开发，只有你想不到，没有VS办不到\n同时VS也提供了一系列教程，给新手进行快速入门\nVisual Studio Code：\nVS code是一款功能强大的文本编辑器、内置git、提供了大量的插件能够让你进行种类的开发\nTools For AI\n本插件的目标是为了提高用户进行AI开发的生产力，为了达到这一目标，本插件提供了如下的功能：\n开发、调试和部署机器学习和 AI 解决方案\n支持包括 Microsoft Cognitive Toolkit (CNTK)、Google TensorFlow、Theano、Keras、Caffe2 等机器学习框架\n其开放式的体系结构还支持使用其他机器学习框架\n不仅支持Python, C/C++/C#，还为Cognitive Toolkit BrainScript提供了额外支持。\n示例库快速入门\n集成了Azure Machine Learning，使学习者能够轻松浏览和参阅搭建在CNTK, TensorFlow, MMLSpark等各种框架上的样例库。使入门机器学习和AI项目更加简单快捷。\n使用Azure进行机器学习训练，操作AI模型\n集成了 Azure Batch AI 和 Azure 机器学习服务，可将机器学习作业提交到 Azure GPU VM、Spark 群集等。\n可以监视最近试验的性能，然后生成 Web 服务，以便为新的智能应用程序提供支持。\n高效AI开发工具\n最可靠的集成工具集，用于创建、调试和部署其自定义机器学习模型。 借助 Visual Studio 的强大功能，你可以使用刚经过训练的模型无缝构建应用，而无需切换 IDE。\n集成了开放工具进行可视化模型处理"}
{"content2":"Spark机器学习之协同过滤算法\n一）、协同过滤\n1.1 概念\n协同过滤是一种借助\"集体计算\"的途径。它利用大量已有的用户偏好来估计用户对其未接触过的物品的喜好程度。其内在思想是相似度的定义\n1.2 分类\n1.在基于用户的方法的中，如果两个用户表现出相似的偏好（即对相同物品的偏好大体相同），那就认为他们的兴趣类似。要对他们中的一个用户推荐一个未知物品，\n便可选取若干与其类似的用户并根据他们的喜好计算出对各个物品的综合得分，再以得分来推荐物品。其整体的逻辑是，如果其他用户也偏好某些物品，那这些物品很可能值得推荐。\n2. 同样也可以借助基于物品的方法来做推荐。这种方法通常根据现有用户对物品的偏好或是评级情况，来计算物品之间的某种相似度。\n这时，相似用户评级相同的那些物品会被认为更相近。一旦有了物品之间的相似度，便可用用户接触过的物品来表示这个用户，然后找出和这些已知物品相似的那些物品，\n并将这些物品推荐给用户。同样，与已有物品相似的物品被用来生成一个综合得分，而该得分用于评估未知物品的相似度。\n二）、矩阵分解\nSpark推荐模型库当前只包含基于矩阵分解（matrix factorization）的实现，由此我们也将重点关注这类模型。它们有吸引人的地方。首先，这些模型在协同过滤\n中的表现十分出色。而在Netflix Prize等知名比赛中的表现也很拔尖\n1，显式矩阵分解\n要找到和“用户物品”矩阵近似的k维（低阶）矩阵，最终要求出如下两个矩阵：一个用于表示用户的U × k维矩阵，以及一个表征物品的I × k维矩阵。\n这两个矩阵也称作因子矩阵。它们的乘积便是原始评级矩阵的一个近似。值得注意的是，原始评级矩阵通常很稀疏，但因子矩阵却是稠密的。\n特点：\n因子分解类模型的好处在于，一旦建立了模型，对推荐的求解便相对容易。但也有弊端，即当用户和物品的数量很多时，其对应的物品或是用户的因子向量可能达到数以百万计。\n这将在存储和计算能力上带来挑战。另一个好处是，这类模型的表现通常都很出色。\n2，隐式矩阵分解（关联因子分确定，可能随时会变化）\n隐式模型仍然会创建一个用户因子矩阵和一个物品因子矩阵。但是，模型所求解的是偏好矩阵而非评级矩阵的近似。类似地，此时用户因子向量和物品因子向量的点积所得到的分数\n也不再是一个对评级的估值，而是对某个用户对某一物品偏好的估值（该值的取值虽并不严格地处于0到1之间，但十分趋近于这个区间）\n3，最小二乘法（Alternating Least Squares    ALS）：解决矩阵分解的最优化方法\nALS的实现原理是迭代式求解一系列最小二乘回归问题。在每一次迭代时，固定用户因子矩阵或是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。\n之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，直到模型收敛（或是迭代了预设好的次数）。\n三）、Spark下ALS算法的应用\n1，数据来源电影集ml-100k\n2,代码实现\n基于用户相似度片段代码：\nval movieFile=sc.textFile(fileName) val RatingDatas=movieFile.map(_.split(\"\\t\").take(3)) //转为Ratings数据 val ratings=RatingDatas.map(x =>Rating(x(0).toInt,x(1).toInt,x(2).toDouble)) //获取用户评价模型,设置k因子,和迭代次数,隐藏因子lambda,获取模型 val model=ALS.train(ratings,50,10,0.01) //基于用户相似度推荐 println(\"userNumber:\"+model.userFeatures.count()+\"\\t\"+\"productNum:\"+model.productFeatures.count()) //指定用户及商品,输出预测值 println(model.predict(789,123)) //为指定用户推荐的前N商品 model.recommendProducts(789,11).foreach(println(_)) //为每个人推荐前十个商品 model.recommendProductsForUsers(10).take(1).foreach{ case(x,rating) =>println(rating(0)) }\n基于商品相似度代码：\n计算相似度的方法有相似度是通过某种方式比较表示两个物品的向量而得到的。常见的相似度衡量方法包括皮尔森相关系数（Pearson correlation）、针对实数向量的余弦相\n似度（cosine similarity）和针对二元向量的杰卡德相似系数（Jaccard similarity）。\nval itemFactory=model.productFeatures.lookup(567).head val itemVector=new DoubleMatrix(itemFactory) //求余弦相似度 val sim=model.productFeatures.map{ case(id,factory)=> val factorVector=new DoubleMatrix(factory) val sim=cosineSimilarity(factorVector,itemVector) (id,sim) } val sortedsim=sim.top(11)(Ordering.by[(Int,Double),Double]{ case(id,sim)=>sim }) println(sortedsim.take(10).mkString(\"\\n\")) def cosineSimilarity(vec1:DoubleMatrix,vec2:DoubleMatrix):Double={ vec1.dot(vec2)/(vec1.norm2()*vec2.norm2()) }\n均方差评估模型代码：\n//模型评估,通过均误差 //实际用户评估值 val actualRatings=ratings.map{ case Rating(user,item,rats) => ((user,item),rats) } val userItems=ratings.map{ case(Rating(user,item,rats)) => (user,item) } //模型的用户对商品的预测值 val predictRatings=model.predict(userItems).map{ case(Rating(user,item,rats)) =>((user,item),rats) } //联合获取rate值 val rates=actualRatings.join(predictRatings).map{ case x =>(x._2._1,x._2._2) } //求均方差 val regressionMetrics=new RegressionMetrics(rates) //越接近0越佳 println(regressionMetrics.meanSquaredError)\n全局准确率评估（MAP）：使用MLlib的 RankingMetrics 类来计算基于排名的评估指标。类似地，需要向我们之前的平均准确率函数传入一个键值对类型的RDD。\n其键为给定用户预测的推荐物品的ID数组，而值则是实际的物品ID数组。\n//全局平均准确率(MAP) val itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect() val itemMatrix = new DoubleMatrix(itemFactors) //分布式广播商品的特征矩阵 val imBroadcast = sc.broadcast(itemMatrix) //计算每一个用户的推荐,在这个操作里，会对用户因子矩阵和电影因子矩阵做乘积，其结果为一个表示各个电影预计评级的向量（长度为 //1682，即电影的总数目） val allRecs = model.userFeatures.map{ case (userId, array) => val userVector = new DoubleMatrix(array) val scores = imBroadcast.value.mmul(userVector) val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1) val recommendedIds = sortedWithId.map(_._2 + 1).toSeq //+1,矩阵从0开始 (userId, recommendedIds) } //实际评分 val userMovies = ratings.map{ case Rating(user, product, rating) => (user, product)}.groupBy(_._1) val predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => val actual = actualWithIds.map(_._2) (predicted.toArray, actual.toArray) } //求MAP,越大越好吧 val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking) println(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision)\n详细代码：\npackage com.spark.milb.study import org.apache.log4j.{Level, Logger} import org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics} import org.apache.spark.mllib.recommendation.{ALS, Rating} import org.apache.spark.{SparkConf, SparkContext} import org.jblas.DoubleMatrix /** * Created by hadoop on 17-5-3. * 协同过滤(处理对象movie,使用算法ALS:最小二乘法(实现用户推荐) * 余弦相似度实现商品相似度推荐 */ object cfTest { def main(args: Array[String]): Unit = { Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN) Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF) val conf=new SparkConf().setMaster(\"local\").setAppName(\"AlsTest\") val sc=new SparkContext(conf) CF(sc,\"ml-100k/u.data\") } def CF(sc:SparkContext,fileName:String): Unit ={ val movieFile=sc.textFile(fileName) val RatingDatas=movieFile.map(_.split(\"\\t\").take(3)) //转为Ratings数据 val ratings=RatingDatas.map(x =>Rating(x(0).toInt,x(1).toInt,x(2).toDouble)) //获取用户评价模型,设置k因子,和迭代次数,隐藏因子lambda,获取模型 /* *  rank ：对应ALS模型中的因子个数，也就是在低阶近似矩阵中的隐含特征个数。因子个 数一般越多越好。但它也会直接影响模型训练和保存时所需的内存开销，尤其是在用户 和物品很多的时候。因此实践中该参数常作为训练效果与系统开销之间的调节参数。通 常，其合理取值为10到200。 iterations ：对应运行时的迭代次数。ALS能确保每次迭代都能降低评级矩阵的重建误 差，但一般经少数次迭代后ALS模型便已能收敛为一个比较合理的好模型。这样，大部分 情况下都没必要迭代太多次（10次左右一般就挺好）。 lambda ：该参数控制模型的正则化过程，从而控制模型的过拟合情况。其值越高，正则 化越严厉。该参数的赋值与实际数据的大小、特征和稀疏程度有关。和其他的机器学习 模型一样，正则参数应该通过用非样本的测试数据进行交叉验证来调整。 * */ val model=ALS.train(ratings,50,10,0.01) //基于用户相似度推荐 println(\"userNumber:\"+model.userFeatures.count()+\"\\t\"+\"productNum:\"+model.productFeatures.count()) //指定用户及商品,输出预测值 println(model.predict(789,123)) //为指定用户推荐的前N商品 model.recommendProducts(789,11).foreach(println(_)) //为每个人推荐前十个商品 model.recommendProductsForUsers(10).take(1).foreach{ case(x,rating) =>println(rating(0)) } //基于商品相似度(使用余弦相似度)进行推荐,获取某个商品的特征值 val itemFactory=model.productFeatures.lookup(567).head val itemVector=new DoubleMatrix(itemFactory) //求余弦相似度 val sim=model.productFeatures.map{ case(id,factory)=> val factorVector=new DoubleMatrix(factory) val sim=cosineSimilarity(factorVector,itemVector) (id,sim) } val sortedsim=sim.top(11)(Ordering.by[(Int,Double),Double]{ case(id,sim)=>sim }) println(sortedsim.take(10).mkString(\"\\n\")) //模型评估,通过均误差 //实际用户评估值 val actualRatings=ratings.map{ case Rating(user,item,rats) => ((user,item),rats) } val userItems=ratings.map{ case(Rating(user,item,rats)) => (user,item) } //模型的用户对商品的预测值 val predictRatings=model.predict(userItems).map{ case(Rating(user,item,rats)) =>((user,item),rats) } //联合获取rate值 val rates=actualRatings.join(predictRatings).map{ case x =>(x._2._1,x._2._2) } //求均方差 val regressionMetrics=new RegressionMetrics(rates) //越接近0越佳 println(regressionMetrics.meanSquaredError) //全局平均准确率(MAP) val itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect() val itemMatrix = new DoubleMatrix(itemFactors) //分布式广播商品的特征矩阵 val imBroadcast = sc.broadcast(itemMatrix) //计算每一个用户的推荐,在这个操作里，会对用户因子矩阵和电影因子矩阵做乘积，其结果为一个表示各个电影预计评级的向量（长度为 //1682，即电影的总数目） val allRecs = model.userFeatures.map{ case (userId, array) => val userVector = new DoubleMatrix(array) val scores = imBroadcast.value.mmul(userVector) val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1) val recommendedIds = sortedWithId.map(_._2 + 1).toSeq //+1,矩阵从0开始 (userId, recommendedIds) } //实际评分 val userMovies = ratings.map{ case Rating(user, product, rating) => (user, product)}.groupBy(_._1) val predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => val actual = actualWithIds.map(_._2) (predicted.toArray, actual.toArray) } //求MAP,越大越好吧 val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking) println(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision) } //余弦相似度计算 def cosineSimilarity(vec1:DoubleMatrix,vec2:DoubleMatrix):Double={ vec1.dot(vec2)/(vec1.norm2()*vec2.norm2()) } }\nView Code"}
{"content2":"机器学习、深度学习以及人工智能正在快速演进\n机器学习、深度学习和人工智能（ML、DL和AI）是彼此相关的概念，他们正在改变不知多少行业，改变其自身管理模式，同时改变做出决策的方式。\n显然，ML、DL和AI对于各行各业都非常重要，却也十分复杂，同时非常迅速发展着。\n人工智能（Artificial Intelligence，AI）\nAI用来形容涉及高级计算智能的最宽泛的说法。1956年，在达特茅斯人工智能大会上，该技术被描述为：“原则上，学习的每一个方面或任何其他智能特征都可以精确描述，并且一台机器可以模拟它。”\nAI可以指代很多东西，比如下棋的计算机程序，或者像亚马逊的Alexa那样可以理解并应答的语音识别系统。\nAI可以大致分为三个种类：狭义AI，通用人工智能（AGI）以及超智能AI。\nIBM的Deep Blue（深蓝）在1996年的比赛中击败了国际象棋大师加里·卡斯帕罗夫，2016年谷歌DeepMind的AlphaGo击败了韩国围棋九段李世石，这些都是狭义AI在某个特定的任务下比较成熟的例子。\nAI与通用人工智能（AGI）不同，AGI是能够达到人类水平的AI，它可以执行一系列任务。\n超智能AI则更先进一步。正如牛津大学哲学家Nick Bostrom所描述的——这是一种智慧，比每个领域最牛的人类智慧都要聪明，包括科学创造力、一般智慧和社交技能。换句话说，超智能AI时代是当机器人超越了人类的时代。\n机器学习（Machine Learning，ML）\n机器学习是AI的一个子集。机器学习的核心原则是机器如何获取数据以及“学习”本身。对于企业来讲，ML系统目前是AI套件中最有前途的使用工具。ML系统可以快速应用来自大型数据集的知识和训练，它们在面部识别、语音识别、对象识别、翻译和许多其他任务方面表现卓越。与手动编码完成各种任务、具有特定指令的软件程序不同，ML允许系统学习自己识别模式并进行预测。\n虽然深蓝和DeepMind都属于AI，但是深蓝是基于规则的，它有赖于编程——所以它并不属于机器学习的一种形式。\n那么，你的企业是否有兴趣将机器学习融入战略规划中呢？亚马逊、百度、谷歌、IBM、微软等企业已经提供了不同的商业化机器学习平台。\n深度学习（Deep Learning，DL）\n深度学习是机器学习的一个子集。它使用一些机器学习技术解决现实世界的问题，通过开发神经网络，模拟人类的决策。深度学习有时非常昂贵，因为它需要大量的数据集来训练自己。 这是因为，大量的参数需要被学习算法所理解，这才能够初步确定大量的假正值。比如说，我们可以利用深度学习算法来“学习”猫的样子，它需要非常庞大的数据集的图像来训练，以了解非常细微的细节，区分是一只猫，还是猎豹，还是美洲豹，或者是狐狸。\n在深度学习领域，最著名的标志性事件是2016年3月DeepMind的AlphaGo，以5比4的成绩击败了围棋世界冠军李世石。 根据Google的说法，该深度学习系统的工作方式是结合了“蒙特卡罗树搜索”与“深度神经网络”，该神经网络通过从人类专家游戏训练，以及从自我游戏的强化学习中实现监督式学习。\n深度学习也有广泛的商业用途。它可以分析大量的数据，比如数百万张图像，并识别某些特性。基于文本的搜索、欺诈检测、垃圾邮件检测、手写识别、图像搜索、语音识别、街景视图检测和翻译等，都是可以通过深度学习执行的任务。在Google，深度学习网络已经取代了许多“手动操作的基于规则的系统”。\n机器学习的三大趋势\n总而言之，人工智能是一个通用术语，它可以指计算机下棋程序，也可以指亚马逊的Alexa语音识别系统等等。\n跟踪这些工具将如何发展非常重要。实际上，机器学习已然出现了三大趋势。\n第一个趋势是无监督学习的出现。训练ML / DL工具的主要方法是监督式的。这种方法需要使用大量的标记数据。新的趋势是无监督式学习，它的最大的好处是无监督学习不需要庞大的数据集。\n第二个趋势是生成式对抗网络（GAN）的发展。为了理解GAN，我们有必要了解判别式模型。判别式模型使用标记的历史数据进行训练，并使用累计的知识来推断，而生成模型则更少依赖已存储的知识，生成式模型根据“训练过程中获得的见解”综合或提出观点。\n第三个趋势是增强学习（RL）。增强学习是通过实验和探索的方式学习的。它与监督学习的不同之处在于，它不是从“世界如何工作”或“良好的训练数据”等先入为主的概念开始的。\n当然，我们很难通过一篇文章熟稔这些概念，但是重要的是，这些尖端的技术正在不断增长和变化。\n实际上，从这些想法的诞生，到在商业、军事、政府等领域真正应用，转化的时间还很短。今年2月中旬，福布斯杂志举了两个将AI用于商业的例子——Equifax和SAS。前者开发深度学习工具，以提高信用评分，后者向其数据挖掘工具添加新的深度学习功能，并提供深度学习API。\n------------------------------\n本人微信公众帐号： 心禅道（xinchandao）\n本人微信公众帐号：双色球预测合买（ssqyuce）"}
{"content2":"《机器学习实战》终于到手了，开始学习了。由于本人python学的比较挫，所以学习笔记里会有许多python的内容。\n1、 python及其各种插件的安装\n由于我使用了win8.1 64位系统（正版的哦），所以像numpy 和 matploblib这种常用的插件不太好装，解决方案就是Anaconda-2.0.1-Windows-x86_64.exe 一次性搞定。\n2、kNN代码\n1 #-*-coding:utf-8-*- 2 from numpy import * 3 import operator 4 5 def createDataSet(): 6 group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) 7 labels = ['A','A','B','B'] 8 return group,labels 9 10 def classify0(inX,dataSet,labels,k): 11 dataSetSize = dataSet.shape[0] 12 diffMat = tile(inX,(dataSetSize,1))-dataSet 13 sqDiffMat = diffMat ** 2 14 sqDistances = sqDiffMat.sum(axis = 1) 15 distances = sqDistances ** 0.5 16 sortedDistIndicies = distances.argsort() #indices 17 classCount = {} 18 for i in range(k): 19 voteIlabel = labels[sortedDistIndicies[i]] 20 classCount[voteIlabel] = classCount.get(voteIlabel,0)+1 21 #找出最大的那个 22 sortedClassCount = sorted(classCount.iteritems(), 23 key = operator.itemgetter(1),reverse = True) 24 return sortedClassCount[0][0]\n这里的疑惑主要出现在：\n（1）array与list有什么区别\narray 是numpy里面定义的。为了方便计算，比如\n1 array([1,2])+array([3,4]) 2 [1,2]+[3,4]\n执行以下就可以知道他们的差别了\n（2）shape[0]返回的是哪一维度的大小（不要嘲笑我小白，我真的不知道）\n找到文档看了一下就开朗了。ndarray.shape   “the dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, shape will be (n,m). The length of the shape tuple is therefore the rank, or number of dimensions, ndim.”\n（3）tile函数\ntile函数是经常使用的函数，用于扩充array。举例：\n1 >>> b = np.array([[1, 2], [3, 4]]) 2 >>> np.tile(b, 2) 3 array([[1, 2, 1, 2], 4 [3, 4, 3, 4]]) 5 >>> np.tile(b, (2, 1)) 6 array([[1, 2], 7 [3, 4], 8 [1, 2], 9 [3, 4]])\n这下就懂了吧。为什么要用这个函数呢？因为后面两个array要做差，这样做就可以不用使用循环了，典型的空间换时间。那么为什么要做差呢？好吧，因为这是knn算法。\n（4）array的sum函数\n写到这里，我决定要好好读读numpy文档了。\nnumpy.sum(a, axis=None, dtype=None, out=None, keepdims=False)\n一个sum函数还是挺麻烦的呢\n>>> np.sum([[0, 1], [0, 5]], axis=0) array([0, 6]) >>> np.sum([[0, 1], [0, 5]], axis=1) array([1, 5])\n这样大家都清楚了\n（5） 最后一行，return了什么？\n表面看起来像是二维数组的第一个元素，但是sortedClassCount是二维数组吗？\n写了一个小的验证程序，发现sortedClassCount是一个list，元素是tuple。\nL = {1:12,3:4} sortedL = sorted(L.iteritems(),key=operator.itemgetter(1)) print sortedL #结果 [(3, 4), (1, 12)]"}
{"content2":"如果您是初学者，那么您可能会将深度学习与机器学习混为一谈。实际上，机器学习包含深度学习，深度学习只是机器学习的研究领域之一。深度学习是一个交叉学科，涉及到神经网络、人工智能、图建模、最优化理论、模式识别和信号处理等多学科领域知识。硬件计算能力的升级使得深度学习在人们的日常生活中有了用武之地。深度学习的应用领域包括计算机视觉、语音识别、图像识别，自动驾驶，自然语言理解、手写识别、音频处理、信息检索、机器人学等。\n关于深度学习，市场上已经有很多参考资料以及著作。若您是个书虫，点击这里可以看到 Amazon 上关于深度学习最新的一些书。这里我们主要推荐一些理论和实践相结合得比较好的书，供大家学习参考。\n关于深度学习\nDeep Learning (Adaptive Computation and Machine Learning series)\n这本书的作者是 Ian Goodfellow，是谷歌大脑的科学家之一。\n很多人认为这本书是深度学习的圣经，因为它是目前为止唯一一本将数十年的研究整合在一起的书。\n不过，需要读者有一定的数学基础，因为它包含了线性代数、概率论、数值计算和机器学习等相关的背景知识。如果是入门的话，不建议从这本入手，读起来会很有挫败感。\n如果是要掌握深度学习，这本可以算得上比较全面的指南，因为除了背景知识，它还对正则化，优化算法，实用性方法等有详细的讲述，对自然语言处理、语音识别、计算机视觉等深度学习的实际应用也做了一个详尽的分析。\nHands-On Machine Learning with Scikit-Learn and TensorFlow\n尽管这本书也有很多公式，但是它可读性非常好，强烈推荐。\n作者 Aurélien Géron 可以将复杂的概念用非常简单的方式讲出来，一般的读者不需要具有特别精深的专业知识也可以理解。\n这本书中不仅有很好理解的例子，还有相应的代码。如果在第一遍时比较关注实战和例子，那么第二遍的时候可以回头看看相关的原理和公式，更加深入地去学习。\nDeep Learning with Python\n作者 Francois Chollet,\n正如他在 Keras 项目中，可以将复杂的概念简单化一样，他的写作方式也同样具有很漂亮的可读性，即使是 AI and deep learning 里最具有挑战性的概念，他也能够深入浅出地讲出来。\n而且本书中也有很多不错的例子，可以先看一下他的 github 上的代码 就知道这个风格了。\nDeep Learning: A Practitioner's Approach\n这本书主要用了 Java 的框架 DL4J，关于人工智能的很多研究多数都是用 Python 来做的，但随着越来越多的企业应用机器学习等技术，我们很可能会看到越来越多的深度学习应用将用 Java 来实现，因为它在很多大公司还是使用最广的语言。\n如果你已经有一些深度学习的基础知识，只是想看如何用 Java 来实现，那就可以跳过前面直接看例子。\n但是如果你并没有太多的深度学习的经验，也没有很强的 Java 背景，那么这本书你可以好好翻一翻。尤其是第 4 章－深入学习的主要架构，这一章非常的好，可以帮你解决现实应用中的关键的架构问题。\nTensorFlow Machine Learning Cookbook\n这本书虽然在内容和代码里会有一些打印错误，但整体上它在人工智能应用的各领域如自然语言处理，都给出了一些很好的例子。\n和其他的手册书一样，这本书主要关注在代码上，如果你还不知道一个卷积神经网络的输入和输出是什么，那么直接读它就会觉得有一些困惑，所以入门的话不推荐此书。\n如果你已经看过其它的书，实战了一些例子，那么这本书可以提供更多的练习。\n下面几本是我觉得对初学者会有帮助的书。\n很多对深度学习感兴趣的伙伴，可能最担心的就是自己的数学已经忘的差不多了。我觉得不一定要先把数学学的很好才能学深度学习，但如果你在看这些算法时，遇到了不明白的地方，某些公式推导不清楚为什么时，还是有必要先复习一下本科时学的那几门最基础的数学的，例如，线性代数，微积分基础，概率论，有条件的话，可以把这三本书放在电脑旁，有空的时候翻一下，回顾一下原理，遇到问题的时候就知道该去哪里查了。\n或者想进一步了解一下数学的话，可以看一下这本：\n数学\nMathematics: A Very Short Introduction\n这本书将问题分解成更简单，更干净的步骤。\n生活就是一系列复杂的算法，但我们并不需要完美的模型，数学可以帮我们将问题分解为其最基本的组件，变量和规则，我们只需要学习这些变量和规则即可。\n数学上比较令人害怕的就是各种符号啦，如果知道它们的意思，那么就不会被吓到了，这里有一个网站，可以查看各符号的含义：\n另外我觉得比较基础的理论就是神经网络，例如常用于图像识别的卷积神经网络，常用于自然语言处理的循环和递归神经网络等，这些网络在 Kaggle 的比赛中也比较受欢迎，调的好的话效果会很好，为了有更好的效果，了解一下内部原理是必要的。\n下面这本书就一步一步教你用 Python 搭建自己的神经网络，并且专门介绍了神经网络所需的数学，对于理解神经网络会比较有帮助。\nMake Your Own Neural Network\nTensorFlow\n当然啦，除了理论，实战也是很重要的，而且也是有趣的一部分，那就要用到深度学习的框架。\n在 Github 上比较火的开源框架有 TensorFlow，Caffe，Keras，CNTK，MXNet，Torch7，Theano等，可以找到很多比较的文章，我这里就不赘述啦。\n我现在是在学 TensorFlow，接着打算学一下 Keras。\n关于 TensorFlow，官方文档就是一个很好的教程，不过如果想了解多一些理论知识的话，可以看一下下面两本书。里面除了详细的代码讲解，还有一些背景的介绍。\n当然，因为是工具书，理论不会讲的太深，但对于初学者也是够用了，关于 AlexNet，VGGNet，ResNet 等会介绍基本的来源，分析一下优势和不足，介绍基本的网络结构，算法的实现过程，一步一步的代码实现和详细的讲解。\n如果对某些结构感兴趣，想要深入地研究 ，可以去翻相关的论文或者书籍。\nTensorFlow实战\nTensorFlow：实战Google深度学习框架\n这里分享一下我的学习过程，最开始的时候，会对它的应用比较感兴趣，所以会去看如何用它实现简单的自动驾驶，聊天机器人，语音识别，写音乐。当了解完大体的应用场景和过程时，就会专注于一个比较感兴趣的应用，看看是不是可以做的稍微高级一点。接着就会想要掌握一下整体 TensorFlow 框架的各种基本组建，像搭乐高似的，看是否可以再组装的更丰满一些。\n你有哪些学习心得呢，欢迎分享出来哦。\n机器学习\n接下来再推荐三本关于机器学习的书，我觉得这三本各有所长，喜欢三者结合着看。\n机器学习\n我比较喜欢这本书的公式表达，很简单清晰，不会一翻开就被特别吓人的符号吓到，每个算法都会用一个小例子来讲一下具体的过程，复杂的概念可以很清晰明了地解释清楚。\n机器学习实战\n上面是算法理论，这本书就是教你用 Python 的代码实现。它会列出算法的优缺点，列出清晰的流程，先写出算法的伪代码，再配上 Python 代码，关键的代码行还有详细的讲解。\n统计学习方法\n这本书会更深地讲算法背后的理论，尤其是从数学的角度，公式推导也更细，要知其然，还要知其所以然。这个时候如果被数学卡住了，就可以用前面的方法，以目标为导向，抓住要点去回顾吧。\n好了，我觉得上面这些就够看一阵子的了。\n今天的目的就是列出一个书单的短评，供大家选择适合自己的，如果是踏踏实实在学习的话，根据各书特色和自己的需求，建议先选中其中一本，要把 80% 的时间花在最重要的 20% 的事情上面，效能才最高。\n参考：\nhttps://hackernoon.com/a-roundup-review-of-the-latest-deep-learning-books-6e5df4b0f3d3\nhttps://hackernoon.com/learning-ai-if-you-suck-at-math-8bdfb4b79037"}
{"content2":"2016年机器学习领域取得了很多可以铭记在历史中的进展，将其称为”机器学习元年”也并不为过。市场上各大公司都在进行机器学习的研究，即使没有，他们也通过收购各类机器学习初创公司来快速进入这个领域。\n阅读全文：http://click.aliyun.com/m/9187/"}
{"content2":"机器学习 Machine Learning：提供数据分析的能力，机器学习是大数据时代必不可少的核心技术，道理很简单：收集、存储、传输、管理大数据的目的，是为了“利用”大数据，而如果没有机器学习技术分析数据，则“利用”就无从谈起。\n数据挖掘 Data mining：数据挖掘是从海量数据中发掘只是，这就比然涉及对海量数据的管理和分析。大体来说，数据库领域的研究为数据挖掘提供数据管理技术，而机器学习和统计学的研究为数据挖掘提供数据分析技术。\n统计学：由于统计学的研究成果通常需要经由机器学习研究来形成有效的学习算法，之后再进入数据挖掘领域，因此从这个意义上说，统计学主要是通过机器学习对数据挖掘发挥影响，而机器学习领域和数据库领域则是数据挖掘的两大支撑。\n云计算 Cloud Computing：提供数据处理的能力。\n众包（Crowdsourcing Data）：提供数据标记能力。Crowdsourcing is a type of participative online activity in which an individual, an institution, a nonprofit organization, or company proposes to a group of individuals of varying knowledge, heterogeneity, and number, via a flexible open call, the voluntary undertaking of a task. The undertaking of the task; of variable complexity and modularity, and; in which the crowd should participate, bringing their work, money, knowledge **[and/or]** experience, always entails mutual benefit. The user will receive the satisfaction of a given type of need, be it economic, social recognition, self-esteem, or the development of individual skills, while the crowdsourcer will obtain and use to their advantage that which the user has brought to the venture, whose form will depend on the type of activity undertaken。\n降维：Dimension reduction models find a projection from the original sample space to a low-dimensional space, which preserves the most useful information for further machine learning. 将原始的高维数据投影到低维空间中的同时，尽可能的保护最大量的有用信息，以进行后续的机器学习。"}
{"content2":"5 Neural Networks (part two)\ncontent:\n5 Neural Networks (part two)\n5.1 cost function\n5.2 Back Propagation\n5.3 神经网络总结\n接上一篇4. Neural Networks (part one). 本文将先定义神经网络的代价函数，然后介绍逆向传播(Back Propagation: BP)算法，它能有效求解代价函数对连接权重的偏导，最后对训练神经网络的过程进行总结。\n5.1 cost function\n(注：正则化相关内容参见3.Bayesian statistics and Regularization)\n5.2 Back Propagation\n（详细推导过程参见反向传播算法，以及李宏毅的机器学习课程：youtube,B站）。\n-1 BP算法步骤\n在实现反向传播算法时，有如下几个需要注意的地方。\n需要对所有的连接权重(包括偏移单元)初始化为接近0但不全等于0的随机数。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数（也就是说，所有神经元的激活值都会取相同的值，对于任何输入x 都会有：  ）。随机初始化的目的是使对称失效。具体地，我们可以如-2一样随机初始化。（matlab实现见后文代码1）\n如果实现的BP算法计算出的梯度（偏导数）是错误的，那么用该模型来预测新的值肯定是不科学的。所以，我们应该在应用之前就判断BP算法是否正确。具体的，可以通过数值的方法(如-3所示的)计算出较精确的偏导，然后再和BP算法计算出来的进行比较，若两者相差在正常的误差范围内，则BP算法计算出的应该是比较正确的，否则说明算法实现有误。注意在检查完后，在真正训练模型时不应该再运行数值计算偏导的方法，否则将会运行很慢。（matlab实现见后文代码2）\n用matlab实现时要注意matlab的函数参数不能为矩阵，而连接权重为矩阵，所以在传递初始化连接权重前先将其向量化，再用reshape函数恢复。(见后文代码3)\n-2 随机初始化连接权重\n-3 数值方法求代价函数偏导的近似值\n5.3 神经网络总结\n第一步，设计神经网络结构。\n隐藏层单元个数通常都是不确定的。\n一般选取神经网络隐藏层单元个数的几个经验公式如下：\n参考https://www.zhihu.com/question/46530834\n此外，MNIST手写数字识别中给出了以不同的神经网络结构训练的结果，供参考\n第二步，实现正向传播(FP)和反向传播算法，这一步包括如下的子步骤。\n第三步，用数值方法检查求偏导的正确性\n第四步，用梯度下降法或更先进的优化算法求使得代价函数最小的连接权重\n在第四步中，由于代价函数是非凸(non-convex)函数，所以在优化过程中可能陷入局部最优值，但不一定比全局最优差很多（如-4），在实际应用中通常不是大问题。也会有一些启发式的算法（如模拟退火算法，遗传算法等）来帮助跳出局部最优。\n-4 陷入局部最优(不一定比全局最优差很多)\n代码1：随机初始化连接权重\nfunction W = randInitializeWeights(L_in, L_out) %RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in %incoming connections and L_out outgoing connections % W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights % of a layer with L_in incoming connections and L_out outgoing % connections. % % Note that W should be set to a matrix of size(L_out, 1 + L_in) as % the column row of W handles the \"bias\" terms % W = zeros(L_out, 1 + L_in); % Instructions: Initialize W randomly so that we break the symmetry while % training the neural network. % % Note: The first row of W corresponds to the parameters for the bias units % epsilon_init = sqrt(6) / (sqrt(L_out+L_in)); W = rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init; end\nView Code\n代码2：用数值方法求代价函数对连接权重偏导的近似值\nfunction numgrad = computeNumericalGradient(J, theta) %COMPUTENUMERICALGRADIENT Computes the gradient using \"finite differences\" %and gives us a numerical estimate of the gradient. % numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical % gradient of the function J around theta. Calling y = J(theta) should % return the function value at theta. % Notes: The following code implements numerical gradient checking, and % returns the numerical gradient.It sets numgrad(i) to (a numerical % approximation of) the partial derivative of J with respect to the % i-th input argument, evaluated at theta. (i.e., numgrad(i) should % be the (approximately) the partial derivative of J with respect % to theta(i).) % numgrad = zeros(size(theta)); perturb = zeros(size(theta)); e = 1e-4; for p = 1:numel(theta) % Set perturbation vector perturb(p) = e; % Compute Numerical Gradient numgrad(p) = ( J(theta + perturb) - J(theta - perturb)) / (2*e); perturb(p) = 0; end end\nView Code\n代码3：应用FP和BP算法实现计算隐藏层为1层的神经网络的代价函数以及其对连接权重的偏导数\nfunction [J grad] = nnCostFunction(nn_params, ... input_layer_size, ... hidden_layer_size, ... num_labels, ... X, y, lambda) %NNCOSTFUNCTION Implements the neural network cost function for a two layer %neural network which performs classification % [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ... % X, y, lambda) computes the cost and gradient of the neural network. The % parameters for the neural network are \"unrolled\" into the vector % nn_params and need to be converted back into the weight matrices. % % The returned parameter grad should be a \"unrolled\" vector of the % partial derivatives of the neural network. % % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices % for our 2 layer neural network:Theta1: 1->2; Theta2: 2->3 Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1)); Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1)); % Setup some useful variables m = size(X, 1); J = 0; Theta1_grad = zeros(size(Theta1)); Theta2_grad = zeros(size(Theta2)); % Note: The vector y passed into the function is a vector of labels % containing values from 1..K. You need to map this vector into a % binary vector of 1's and 0's to be used with the neural network % cost function. for i = 1:m % compute activation by Forward Propagation a1 = [1; X(i,:)']; z2 = Theta1 * a1; a2 = [1; sigmoid(z2)]; z3 = Theta2 * a2; h = sigmoid(z3); yy = zeros(num_labels,1); yy(y(i)) = 1; % 训练集的真实值yy J = J + sum(-yy .* log(h) - (1-yy) .* log(1-h)); % Back Propagation delta3 = h - yy; delta2 = (Theta2(:,2:end)' * delta3) .* sigmoidGradient(z2); %注意要除去偏移单元的连接权重 Theta2_grad = Theta2_grad + delta3 * a2'; Theta1_grad = Theta1_grad + delta2 * a1'; end J = J / m + lambda * (sum(sum(Theta1(:,2:end) .^ 2)) + sum(sum(Theta2(:,2:end) .^ 2))) / (2*m); Theta2_grad = Theta2_grad / m; Theta2_grad(:,2:end) = Theta2_grad(:,2:end) + lambda * Theta2(:,2:end) / m; % regularized nn Theta1_grad = Theta1_grad / m; Theta1_grad(:,2:end) = Theta1_grad(:,2:end) + lambda * Theta1(:,2:end) / m; % regularized nn % Unroll gradients grad = [Theta1_grad(:) ; Theta2_grad(:)]; end\nView Code"}
{"content2":"人工智能、机器学习都已走进了我们的日常，尤其是愈演愈热的大数据更是跟我们的生活息息相关，做 人工智能、数据挖掘的人在其他人眼中感觉是很高大上的，总有一种遥不可及的感觉，在我司也经常会听到数据科学部的同事们提到 机器学习、数据挖掘 之类的词。但这些名词真的跟我们移动开发就没直接关系了吗？\n作为移动开发者来说，无时无刻不被这些名词狠狠地敲打着脆弱的内心。???? ???? ???? 何时才能够将机器学习、深度学习应用在移动端，敲响移动端机器学习工业化的大门呢？\n想象一下，某一天你身处一个完全陌生的环境，周围都是陌生的事物，而运行在iPhone的某个APP却对这个环境了如指掌，你要做的就是打开这个APP，输入你需要了解的事物，iPhone告诉你这个事物的信息，你也就没有了陌生事物了。世界就在眼前！\n如下图：\n上面物体的识别准确率还是蛮不错的，基本识别出了键盘（49%的概率）、鼠标（46%的概率）和水杯（24%的概率）。\n但是在某些事物的识别准确度方便却差强人意，比如下图：\nPhone 6被识别成了iPod（59%的概率），而iPod的却是不怎么敢认（10%的概率）。想想最崩溃的估计是iPhone 6了，身价直接被降了好几个等级。\n上面的例子来自于TensorFlow官方iOSDemo，暂且不评述TensorFlow的识别准确度如何，毕竟它还年轻，但是仅凭其识别能力的体现，也给机器学习在移动端的运用带来了无限的可能。\n一、TensorFlow（简称TF）\n去年，Google资深系统专家Jeff Dean在湾区机器学习大会上隆重介绍了其第二代深度学习系统TensorFlow，一时间网络上针对TensorFlow的文章铺天盖地，揭秘TensorFlow：Google开源到底开的是什么？、Google开源TensorFlow系统，这背后都有什么门道？、如何评价Google发布的第二代深度学习系统TensorFlow?等等文章，TensorFlow的燎原之火一直在燃烧蔓延着，其GitHub上的开源库在此文撰写时，也已经被star：27550，fork：11054了。???? ???? ???? ???? ????\n不负众望，Google一直宣称平台移植性非常好的TensorFlow，终于在2016年6月27日，发布0.9版本，宣布移动端支持。TensorFlow v0.9 now available with improved mobile support( 有墙???? )，同时也给出了移动端的Demo，对于代码为生的程序员，身处大数据处理为主导的TalkingData，也小试身手了一把，下载TensorFlow源码，查看编译指南，开始跳坑、填坑之路，也成就了此篇拙文的产生。\n二、从TensorFlow到iOS静态库\n对于iOS平台下如何使用TensorFlow，TensorFlow给出了详细的编译脚本命令，详情请查看官方文档的命令。\n第一步. 工具准备\n工欲善其事必先利其器，在开始编译工作之前，需要准备一些编译所必须的工具：\nHomebrew: Mac os x 上包管理工具，具体使用方法可参考Doc。\n1\n$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nHomebrew安装好之后，依次安装三个辅助性编译工具：\n1\n2\n3\n$ brew install libtool\n$ brew install autoconf\n$ brew install automake\n三个工具的含义，请参考：https://en.wikipedia.org/wiki/GNU_Libtool\n第二步. 克隆TensorFlow\nGoogle以Apache 2.0开源协议将TensorFlow开源在GitHub上，我们可以直接使用TensorFlow源码。\n在任意你想存放TensorFlow源码的地方（建议不要放在桌面。^_^），clone项目。\n1\n$ git clone https://github.com/tensorflow/tensorflow\n第三步. 编译前准备\n在TensorFlow的tensorflow/contrib/makefile/目录下，有很多可使用的编译脚本，其中build_all_ios.sh脚本专门用来一键编译TensorFlow iOS静态库。虽然可以直接使用此脚本进行一键编译，但是因为有墙，某些依赖需要提前做处理。\n1、下载protobuf\nprotobuf 是编译前唯一需要特殊处理的依赖库，点击下载，下载protobuf之后，解压，备用。\n2、下载googlemock\n虽然protobuf编译脚本autogen.sh中的googlemock链接地址https://googlemock.googlecode.com/files/gmock-1.7.0.zip无法直接下载到，但是细心的人会发现，在浏览器中输入                  https://googlemock.googlecode.com/地址后，会跳转到https://github.com/google/googlemock地址，google在GiHub上的仓库地址。而GitHub上的仓库，我们可以直接的下载，克隆等。\n我们直接在GitHub上下载googlemock(点击下载)，下载完成后，修改压缩包名字为gmock-1.7.0.zip，修改后将此压缩包移至上一步protobuf文件夹目录下，备用。\n3、修改下载依赖脚本，移除protobuf的下载\n在tensorflow/contrib/makefile/目录下，download_dependencies.sh脚本用来下载相关依赖，打开此脚本文件，注释掉或者直接删掉git clone https://github.com/google/protobuf.git ${DOWNLOADS_DIR}/protobuf部分，目的是不让脚本去下载protobuf。\n上面三步准备好后，接下来就进入静态库编译了。\n第四步. 一键编译\n前面已经知道在TensorFlow文件夹tensorflow/contrib/makefile/目录下的build_all_ios.sh脚本是用来编译iOS静态库的脚本，因此可以直接执行此脚本，开始静态库的编译工作了。\n但是有一个问题大家可能会发现，由于编译TensorFlow需要用到protobuf，但是protobuf使我们自己手动下载的，该怎么让手动下载的protobuf能够直接让build_all_ios.sh脚本使用呢？\n答案是复制、粘贴。可能有些low，但是有效。执行命令 build_all_ios.sh之后，立即把之前手动下载的protobuf文件夹拷贝进tensorflow/contrib/makefile/downloads目录。（放心，你拷贝的速度会很快，不会影响编译的执行的。^_^）\n1\n$ build_all_ios.sh\n一切准备就绪，接下来就是静静的等待编译完成了。在Mac编译的过程中，建议插上电源，最好不要让设备休眠断电，也最好不要去干别的东西，出去溜达一圈，回来后就看到战果了。\n编译完成之后，会在tensorflow/contrib/makefile/gen/目录下看到编译的结果，关于这些静态库该如何使用，自己的项目如何应用，请参考TensorFlow iOS Examples。\n三、遇到的问题\n1、googlecode.com被墙了，需要FQ！（目前测试挂了VPN也没用），这也是上面编译前准备为什么要那么做的原因。\n1\ncurl: (7) Failed to connect to googlemock.googlecode.com port 443: Operation timed out\n解决： 请参考 『第三步. 编译前准备』。\n2、没有Xcode。\n1\n2\n3\n4\nxcrun: error: SDK \"iphoneos\" cannot be located\nxcrun: error: SDK \"iphoneos\" cannot be located\nxcrun: error: unable to lookup item 'PlatformPath' in SDK 'iphoneos'\n+ IPHONEOS_PLATFORM=\n解决：安装Xcode，从上面报错的命令中可以看到，在编译静态库的过程中使用了xcrun，而此命令是xCode本身具有的能力。\n3、你的Xcode版本不是7.3或以后，或者你有多个Xcode，而默认的安装路径版本不是7.3或以后。\n``` error: Xcode 7.3.0 or later is required.\nexit 1 ```/\n解决：更新Xcode至最新版本，并且保证默认路径下是最新/版本。\n如果Xcode是7.3，并且没有条件更新Xcode，你可以修改tensorflow/contrib/makefile/compile_ios_tensorflow.sh 里的REQUIRED_XCODE_VERSION=7.3.0，为REQUIRED_XCODE_VERSION=7.3。（这样修改，目前还不确定会不会带来一些其他影响，最好是升级你的Xcode）\n想了解TensorFlow在Android平台的使用可以，看看 TalkingData SDK Team 另一篇的技术博客Tensorflow 在 Android 平台的移植\n四、参考链接\nTensorFlow 中文社区\nTensorFlow for Mobile\nCaffe、TensorFlow、MXnet三个开源库对比\n如何评价Tensorflow和其它深度学习系统\n深度学习框架大战正在进行，谁将夺取“深度学习工业标准”的荣耀？"}
{"content2":"转载自：http://emuch.net/html/201012/2659795.html\n原帖中有丰富讨论。\n看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？\n我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。\n可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？\n(1)\n以下是不完整的列表，但基本覆盖。\n机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）\n计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）\n人工智能：IJCAI, AAAI; （期刊AI）\n另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。\n特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。\n(2)\n另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/;  JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  COLT和ICML(每年度的官网): http://www.cs.mcgill.c ... ceedings.html。希望这些信息对大家有点帮助。\n(3)\n说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。"}
{"content2":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor def load_data(): ''' 加载用于分类问题的数据集。数据集采用 scikit-learn 自带的 iris 数据集 ''' # scikit-learn 自带的 iris 数据集 iris=datasets.load_iris() X_train=iris.data y_train=iris.target return train_test_split(X_train, y_train,test_size=0.25,random_state=0,stratify=y_train) #分类决策树DecisionTreeClassifier模型 def test_DecisionTreeClassifier(*data): X_train,X_test,y_train,y_test=data clf = DecisionTreeClassifier() clf.fit(X_train, y_train) print(\"Training score:%f\"%(clf.score(X_train,y_train))) print(\"Testing score:%f\"%(clf.score(X_test,y_test))) # 产生用于分类问题的数据集 X_train,X_test,y_train,y_test=load_data() # 调用 test_DecisionTreeClassifier test_DecisionTreeClassifier(X_train,X_test,y_train,y_test)\ndef test_DecisionTreeClassifier_criterion(*data): ''' 测试 DecisionTreeClassifier 的预测性能随 criterion 参数的影响 ''' X_train,X_test,y_train,y_test=data criterions=['gini','entropy'] for criterion in criterions: clf = DecisionTreeClassifier(criterion=criterion) clf.fit(X_train, y_train) print(\"criterion:%s\"%criterion) print(\"Training score:%f\"%(clf.score(X_train,y_train))) print(\"Testing score:%f\"%(clf.score(X_test,y_test))) # 调用 test_DecisionTreeClassifier_criterion test_DecisionTreeClassifier_criterion(X_train,X_test,y_train,y_test)\ndef test_DecisionTreeClassifier_splitter(*data): ''' 测试 DecisionTreeClassifier 的预测性能随划分类型的影响 ''' X_train,X_test,y_train,y_test=data splitters=['best','random'] for splitter in splitters: clf = DecisionTreeClassifier(splitter=splitter) clf.fit(X_train, y_train) print(\"splitter:%s\"%splitter) print(\"Training score:%f\"%(clf.score(X_train,y_train))) print(\"Testing score:%f\"%(clf.score(X_test,y_test))) # 调用 test_DecisionTreeClassifier_splitter test_DecisionTreeClassifier_splitter(X_train,X_test,y_train,y_test)\ndef test_DecisionTreeClassifier_depth(*data,maxdepth): ''' 测试 DecisionTreeClassifier 的预测性能随 max_depth 参数的影响 ''' X_train,X_test,y_train,y_test=data depths=np.arange(1,maxdepth) training_scores=[] testing_scores=[] for depth in depths: clf = DecisionTreeClassifier(max_depth=depth) clf.fit(X_train, y_train) training_scores.append(clf.score(X_train,y_train)) testing_scores.append(clf.score(X_test,y_test)) ## 绘图 fig=plt.figure() ax=fig.add_subplot(1,1,1) ax.plot(depths,training_scores,label=\"traing score\",marker='o') ax.plot(depths,testing_scores,label=\"testing score\",marker='*') ax.set_xlabel(\"maxdepth\") ax.set_ylabel(\"score\") ax.set_title(\"Decision Tree Classification\") ax.legend(framealpha=0.5,loc='best') plt.show() # 调用 test_DecisionTreeClassifier_depth test_DecisionTreeClassifier_depth(X_train,X_test,y_train,y_test,maxdepth=100)\nimport os import pydotplus from io import StringIO from sklearn.tree import export_graphviz from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor X_train,X_test,y_train,y_test=load_data() clf = DecisionTreeClassifier() clf.fit(X_train,y_train) export_graphviz(clf,\"F://out\")"}
{"content2":"AForge.NET是一个专门为开发者和研究者基于C#框架设计的，他包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，模糊系统，机器人控制等领域。\n这个框架由一系列的类库组成。主要包括有：\nAForge.Imaging —— 日常的图像处理和过滤器\nAForge.Vision —— 计算机视觉应用类库\nAForge.Neuro —— 神经网络计算库AForge.Genetic -进化算法编程库\nAForge.MachineLearning —— 机器学习类库\nAForge.Robotics —— 提供一些机器学习的工具类库\nAForge.Video —— 一系列的视频处理类库\nAForge.Fuzzy —— 模糊推理系统类库\nAForge.Controls—— 图像，三维，图表显示控件\n以下是部分方向的使用\n1.基于符号识别的3D现实增强技术\n2.基于模糊系统的自动导航\n3.运动检测\n4.2D增强技术\n5.计算机视觉与人工智能\n6.模拟识别\n7.神经网络\n8.图像处理\n9.遗传算法\n10.机器学习\n11.机器人控制等等\n还有GRATF 符号识别和目标追踪的库，可以用于机器人控制，当然也可以用于现实增强。\nImage Processing Lab\n基于C#的图像处理库，提供了一系列可用于AForge，Net的接口和工具。\nAForge.Net 是C#的一个图像计算机视觉库，该库是一个开源项目，提供很多图像的处理，和视频处理功能\nhttp://www.aforgenet.com/\nAforge.Net子项目有个AForge.Video.VFW提供了对Avi文件的操作，AForge后面加入了子项目 AForge.Video.FFMPEG 通过FFmpeg库，提供了对大量视频格式的支持，我们都知道，FFmpeg是一个非常强大的视频处理类库，同样也是开源的，不过 AForge.Video.FFMPEG 还处于实验阶段，目标是用 FFmpeg 取代 AForge.Video.VFW 提供一个更好的对视频文件操作的库，但是该库目前只提供了对视频数据的读写，不支持对音频文件的读写，可能以后会支持，在使用的 AForge.Video.FFMpeg 时，添加对 AForge.Video.FFMPEG.dll, AForge.Video.dll和 AForge.dll 三个 dll 的引用。\nAForge.Video.FFMpeg命名空间下提供了三个类 VideoFileReader， VideoFileWriter， VideoFileSource\ndemo：\nhttp://files.cnblogs.com/files/nidongde/OperateCamera.rar\n供参考"}
{"content2":"说明：这个贴用于收集笔者能力范围内收集收藏并认为有用的资料，方便各方参考，免去到处找寻之苦，提升信息的交叉引用价值。仅供参考，不作为必然的推荐倾向。如涉及版权等问题请相关人员联系笔者，谢谢。\n|博客|\n龙心尘的博客(http://blog.csdn.net/longxinchen_ml)\n寒小阳的博客(http://blog.csdn.net/han_xiaoyang)\nwepon(http://2hwp.com/)\n面包包包包包包（http://blog.csdn.net/breada）\n仆居（http://blog.csdn.net/kkk584520）\n|人工智能|机器学习|数据挖掘|神经网络|\n手把手入门神经网络系列-2篇-有图有码\n机器学习系列-7篇-有图有码\nNLP(自然语言处理)系列-5篇-有图有码\n利用 Python 练习数据挖掘\n[建议：适合入门；]\n[简介：围绕1个例子；完整的步骤；少量错误；]\n[扩展：关于IRIS数据集的Python分析 (太初）]\n[扩展：IRIS数据集的PCA分析和3D展现]\n用Python做科学计算-基础篇\n[简介：不错的教程]\nscikit-learn的主要模块和基本使用\n[简介：内容精要]\n浅谈神经网络[入门] | 统计与计算[入门]\n|人脸识别|指纹掌纹识别|生物识别|计算机视觉（CV）\nPCA+SVM人脸识别\n深度学习与计算机视觉系列-10篇-有图有码\nPython和OPENVC分析烤箱状态\n从特征描述符到深度学习：计算机视觉发展20年\nPython计算机视觉编程（10章+附录ABC）\n|算法|模型|\n| 朴素贝叶斯分类器 | 朴素贝叶斯分类 | 朴素贝叶斯的三个常用模型(高斯/多项式/伯努利) |\n| 判别模型/生成模型与朴素贝叶斯方法 | 聚类算法k-mean | 聚类算法k-mean | 白化 | PCA(1,2,3,4,5) |\n| 线性回归 | 中文分词软件(14款开源) |  |\n|实用连接|\n| numpy.zeros函数用法 | SCIPY.ORG |numpy中的ndarray方法和属性 | 多维数组ndarray及切片\n| plot绘图 | numpy.linspace | numpy.matrix |  numpy.arange | scipy |\n| sklearn官网API | 大量wheel | Anaconda[简介:极好的PythonIDE!](安装使用) |\n|平台|网站|竞赛|\nAminer（Open Science Platform)  | Kaggle(体验,简介) | DataCastle | 阿里天池 (体会,总结) |\n神经网络实验室(所见即所得的测试谷歌tensorflow)极好！ | 谷歌大脑Google Brain |\n|实用连接|\n|Python数据图表工具 | Python写爬虫查询 |\n[END]"}
{"content2":"遇到这个其实不难解决！\n福利 => 每天都推送\n欢迎大家，关注微信扫码并加入我的4个微信公众号：   大数据躺过的坑      Java从入门到架构师      人工智能躺过的坑         Java全栈大联盟\n每天都有大量的学习视频资料和精彩技术文章推送... 人生不易，唯有努力。\n百家号 ：九月哥快讯               快手号：  jiuyuege\n解决办法1：\n[root@djt002 hadoop]# vi /etc/selinux/config\n改为\nSELINUX=disabled\n解决办法2：\n查看你的$HADOOP_HOME/etc/hadoop下的core-site.xml和hdfs-site.xml是否配置好\n解决办法3：\n必须在hadoop-env.sh文件中设置Java的绝对路径\n解决办法4：\n是否关闭linux系统的防火墙\n复制代码 [root@djt002 ~]# service iptables status [root@djt002 ~]# chkconfig iptables off //永久关闭防火墙 [root@djt002 ~]# service iptables stop //临时关闭防火墙 [root@djt002 ~]# service iptables status iptables: Firewall is not running. //查看防火墙状态\n解决办法5：\n查看你windows里本地的配置文件的IP和主机名映射关系\n欢迎大家，加入我的4个微信公众号：    大数据躺过的坑     Java从入门到架构师    人工智能躺过的坑     Java全栈大联盟\n同时，大家可以关注我的个人博客：\nhttp://www.cnblogs.com/zlslch/   和     http://www.cnblogs.com/lchzls/      http://www.cnblogs.com/sunnyDream/\n详情请见：http://www.cnblogs.com/zlslch/p/7473861.html\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。\n目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。 语言涉及：Java、Scala、Python、Shell、Linux等 。同时还涉及平常所使用的手机、电脑和互联网上的使用技巧、问题和实用软件。 只要你一直关注和呆在群里，每天必须有收获\n对应本平台的讨论和答疑QQ群：大数据和人工智能躺过的坑（总群）（161156071）\n打开百度App，扫码，精彩文章每天更新！欢迎关注我的百家号： 九月哥快讯"}
{"content2":"《分布式机器学习：算法、理论与实践》旨在全面介绍分布式机器学习的现状，深入分析其中的核心技术问题，并且讨论该领域未来的发展方向。\n下载：https://pan.baidu.com/s/1XeOGCQK5qWCba8VK0KU21w\n《分布式机器学习：算法、理论与实践》PDF，273页，带书签目录，文字可以复制。\n人工智能和大数据时代，解决最有挑战性问题的主流方案是分布式机器学习！\n《分布式机器学习：算法、理论与实践》旨在全面介绍分布式机器学习的现状，深入分析其中的核心技术问题，并且讨论该领域未来的发展方向。\n《分布式机器学习：算法、理论与实践》共12章。第1章是绪论，向大家展示分布式机器学习这个领域的全景。第2章介绍机器学习的基础知识。第3章到第8章是本书的核心部分，向大家细致地讲解分布式机器学习的框架及其各个功能模块。其中第3章给出整个分布式机器学习框架的综述，而第4章到第8章则分别针对其中的数据与模型划分模块、单机优化模块、通信模块、数据与模型聚合模块加以介绍。接下来的三章是对前面内容的总结与升华。其中第9章介绍由分布式机器学习框架中不同选项所组合出来的各式各样的分布式机器学习算法，第10章讨论这些算法的理论性质，第11章则介绍几个主流的分布式机器学习系统（包括Spark MLlib 迭代式MapReduce系统，Multiverso参数服务器系统，TensorFlow数据流系统）。最后的第12章是全书的结语，在对全书内容进行简要总结之后，着重讨论分布式机器学习这个领域未来的发展方向。\n《分布式机器学习：算法、理论与实践》基于微软亚洲研究院机器学习研究团队多年的研究成果和实践经验写成，既可以作为研究生从事分布式机器学习方向研究的参考文献，也可以作为人工智能从业者进行算法选择和系统设计的工具书。人工智能大潮中，市场上已有许多机器学习书籍，但是分布式机器学习的专门书籍还很少见。本书是希望学习和了解分布式机器学习的读者的福音。\n相关学习资料交流：\n《Python机器学习基础教程》高清中文英文PDF源代码\n下载：https://pan.baidu.com/s/1d6VjDtRfCkgScZDtNoGrUA\n《机器学习实战》(高清中文英文版PDF+源代码)\n下载：https://pan.baidu.com/s/1g7806-xqk0pJhlBCpQGQgA\n《百面机器学习算法工程师带你去面试》高清PDF版+高清epub版\n下载：https://pan.baidu.com/s/1DgV5JxWZgyQBKd7wkMXmWw"}
{"content2":"网络安全中机器学习大合集\nfrom:https://github.com/jivoi/awesome-ml-for-cybersecurity/blob/master/README_ch.md#-datasets\n历年来那些与网络安全中机器学习相关最好的工具与资源\n目录\n数据集\n论文\n书籍\n演讲\n教程\n课程\n杂项\n↑ 贡献\n如果你想要添加工具或资源请参阅 CONTRIBUTING\n↑ 数据集\n安全相关数据样本集\nDARPA 入侵检测数据集\nStratosphere IPS 数据集\n开放数据集\nNSA 的数据捕获\nADFA 入侵检测数据集\nNSL-KDD 数据集\n恶意 URL 数据集\n多源安全事件数据集\n恶意软件训练集\nKDD Cup 1999 数据集\nWeb 攻击载荷\nWAF 恶意请求数据集\n恶意软件训练数据集\nAktaion 数据集\nDeepEnd 研究中的犯罪数据集\n公开可用的 PCAP 文件数据集\n2007年TREC公开垃圾邮件全集\n↑ 论文\n快速、可靠、准确：使用神经网络建模猜测密码\n封闭世界之外，应用机器学习在网络入侵检测\n基于 Payload 的异常网络入侵检测\n使用元数据与结构特征检测恶意 PDF\n对抗性支持向量机学习\n利用机器学习颠覆垃圾邮件过滤器\nCAMP – 内容不可知的恶意软件保护\nNotos – 构建动态 DNS 信誉系统\nKopis – 在 DNS 上层结构中检测恶意软件的域名\nPleiades – 检测基于 DGA 的恶意软件的崛起\nEXPOSURE – 使用被动 DNS 分析找到恶意域名\nPolonium – 恶意软件检测中万亿级图计算挖掘\nNazca – 在大规模网络中检测恶意软件分布\nPAYL – 基于 Payload 的网络异常入侵检测\nAnagram – 用于对抗模仿攻击的内容异常检测\n在网络安全中应用机器学习\n用数据挖掘构建网络入侵检测系统(RUS)\n数据挖掘在企业网络中构建入侵检测系统 (RUS)\n应用神经网络在计算机安全任务分层 (RUS)\n数据挖掘技术与入侵检测 (RUS)\n网络入侵检测系统中的降维\n机器的兴起：机器学习与其在网络安全中的应用\n网络安全中的机器学习：半人马纪元\n自动逃避分类：PDF 恶意软件分类案例研究\n社会工程在数据科学的武器化-在 Twitter 上实现自动 E2E 鱼叉钓鱼\n机器学习：威胁狩猎的现实检查\n基于神经网络图嵌入的跨平台二进制程序代码相似度检测\n整合隐私保护机器学习的实用安全\nDeepLog：基于深度学习的系统日志异常检测与诊断\neXpose：带有嵌入的字符级CNN，用于检测恶意 URL、文件路径与注册表\n↑ 书籍\n网络安全中的数据挖掘与机器学习\n网络安全中的机器学习与数据挖掘\n网络异常检测：机器学习观点\n机器学习与安全：用数据和算法保护系统\n写给安全专家的人工智能介绍\n↑ 演讲\n应用机器学习来支撑信息安全\n利用不完整的信息进行网络防卫\n机器学习应用于网络安全监测\n测量你威胁情报订阅的 IQ\n数据驱动的威胁情报：指标的传播与共享的度量\n机器学习应对数据盗窃与其他主题\n基于机器学习监控的深度探索\nPwning 深度学习系统\n社会工程学中武器化的数据科学\n打败机器学习，你的安全厂商没告诉你的事儿\n集思广益，群体训练-恶意软件检测的机器学习模型\n打败机器学习，检测恶意软件的系统缺陷\n数据包捕获 – 如何使用机器学习发现恶意软件\n五分钟用机器学习构建反病毒软件\n使用机器学习狩猎恶意软件\n机器学习应用于威胁检测\n机器学习与云：扰乱检测与防御\n在欺诈检测中应用机器学习与深度学习\n深度学习在流量识别上的应用\n利用不完整信息进行网络防卫：机器学习方法\n机器学习与数据科学\n云计算规模的机器学习应用于网络防御的进展\n应用机器学习：打败现代恶意文档\n使用机器学习与 GPO 自动防御勒索软件\n通过挖掘安全文献检测恶意软件\n信息安全中的机器学习实践\n用于 Cyberdefensse 的机器学习\n基于机器学习的网络入侵检测技术\n信息安全中的机器学习实践\nAI 与安全\nAI 与信息安全\n超越黑名单：通过机器学习检测恶意网址\n使用机器学习辅助网络威胁狩猎\n机器学习的武器化：人性被高估\n机器学习：进攻与自动化的未来\n↑ 教程\n点击安全数据窃听项目\n使用神经网络生成人类可读的密码\n基于机器学习的密码强度分类\n应用机器学习在检测恶意 URL\n在安全与欺诈检测中的大数据与数据科学\n使用深度学习突破验证码\n网络安全与入侵检测中的数据挖掘\n机器学习应用于网络安全与威胁狩猎简介\n应用机器学习提高入侵检测系统\n使用 Suricata 与机器学习分析僵尸网络\nfWaf – 机器学习驱动的 Web 应用防火墙\n网络安全中的深度域学习\nDMachine Learning 用于恶意软件检测\nShadowBrokers 泄漏：机器学习方法\n信息安全领域的机器学习实践\n用于大规模数字犯罪取证的机器学习工具包\n机器学习检测 WebShell\n↑ 课程\nStanford 的网络安全数据挖掘课\nInfosec 数据科学与机器学习\n↑ 杂项\n使用人类专家的输入对网络攻击达到 85% 的预测系统\n使用机器学习的网络安全项目开源列表\n许可证\n许可证为 Creative Commons Attribution-ShareAlike 4.0 International"}
{"content2":"Random Forest是加州大学伯克利分校的Breiman Leo和Adele Cutler于2001年发表的论文中提到的新的机器学习算法，可以用来做分类，聚类，回归，和生存分析，这里只简单介绍该算法在分类上的应用。\nRandom Forest（随机森林）算法是通过训练多个决策树，生成模型，然后综合利用多个决策树进行分类。\n随机森林算法只需要两个参数：构建的决策树的个数t，在决策树的每个节点进行分裂时需要考虑的输入特征的个数m。\n1. 单棵决策树的构建：\n（1）令N为训练样例的个数，则单棵决策树的输入样例的个数为N个从训练集中有放回的随机抽取N个训练样例。\n（2）令训练样例的输入特征的个数为M，切m远远小于M，则我们在每颗决策树的每个节点上进行分裂时，从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂。m在构建决策树的过程中不会改变。\n（3）每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。不需要剪枝。\n2. 随机森林的分类结果\n按照1生成t个决策树之后，对于每个新的测试样例，综合多个决策树的分类结果来作为随机森林的分类结果。\n（1）目标特征为数字类型：取t个决策树的平均值作为分类结果。\n（2）目标特征为类别类型：少数服从多数，取单棵树分类结果最多的那个类别作为整个随机森林的分类结果。\n3. 分类效果的评价\n在随机森林中，无需交叉验证来评价其分类的准确性，随机森林自带OOB（out-of-bag）错误估计：\nOOB：在构造单棵决策树时我们只是随机有放回的抽取了N个样例，所以可以用没有抽取到的样例来测试这棵决策树的分类准确性，这些样例大概占总样例数目的三分之一（作者这么说的，我还不知道理论上是如何出来的，但是可以自己做试验验证）。所以对于每个样例j，都有大约三分之一的决策树（记为SetT（j））在构造时没用到该样例，我们就用这些决策树来对这个样例进行分类。我们对于所有的训练样例j，用SetT（j）中的树组成的森林对其分类，然后看其分类结果和实际的类别是否相等，不相等的样例所占的比例就是OOB错误估计。OOB错误估计被证明是无偏的。\n参考文献：\n[1] Mahout Wiki-Random Forest\n[2] Leo Breiman 2001年的paper\n[3] Breiman自己对Random Forest的介绍"}
{"content2":"国人写的两本书，个人感觉还是不错的，一方面学习深度学习的理论，另一方面可以使用tensorflow进行测试，收效较快。\n《深入理解TensorFlow架构设计与实现原理》从基本概念、内部实现和实践等方面深入剖析了TensorFlow。\n《深入理解TensorFlow架构设计与实现原理》PDF，375页，带书签目录，文字可以复制。\n作者：彭靖田,林健,白小龙\n下载: https://pan.baidu.com/s/1ZjznggugS3S_ivrLqeydMg\n提取码: wp4w\n《深入理解TensorFlow架构设计与实现原理》分为五大部分，这五个部分，我认为最有用的是第4部分，能够学会对底层进行改进。\n第一部分为基础篇（第1～3章），简单介绍了TensorFlow设计目标、基本架构、环境准备和基础概念，包括数据流图的设计与使用，以及TensorFlow运行环境和训练机制，帮助读者快速入门TensorFlow，迅速上手使用。\n第二部分为关键模块篇（第4～7章），着重讲解了使用TensorFlow端到端解决人工智能问题涉及的关键模块，包括数据处理、编程框架、可视化工具和模型托管工具，帮助读者进一步提升开发效率，快速落地模型应用。\n第三部分为算法模型篇（第8～11章），在读者熟练掌握TensorFlow后，该部分将深度学习与TensorFlow有机结合，系统介绍了深度学习的发展历史与应用场景，并结合理论与代码实现深入讲解了CNN、GAN和RNN等经典模型。\n第四部分为核心揭秘篇（第12～14章），深入剖析了TensorFlow运行时核心、通信原理和数据流图计算的原理与实现，聚焦C++ 核心层的揭秘，帮助读者进一步理解TensorFlow底层的设计思想与实现细节，TensorFlow二次开发人员需重点关注这部分内容。\n第五部分为生态发展篇（第15章），全面介绍了TensorFlow生态系统发展，并重点介绍了Keras深度学习算法库，以及TensorFlow与云原生社区Kubernetes生态的结合、与大数据社区Spark生态的结合，并介绍了TensorFlow通信优化技术、TPU及NNVM模块化深度学习技术，帮助读者进一步全面了解深度学习生态发展的现状。\n学了理论后，我们可以通过《Tensorflow+Keras-深度学习人工智能实践应用》进行实践。\n《Tensorflow+Keras-深度学习人工智能实践应用》PDF，329页，带书签目录，文字可以复制。\n配套源代码。\n下载：https://pan.baidu.com/s/1NwhEBvB7gwvO81e8PhH75g\n通过学习《Tensorflow+Keras-深度学习人工智能实践应用》，我学会了安装、上机操作，也测试了大部分范例程序。\n分9部分，共21章，内容主要包括基本概念介绍、TensorFlow 与 Keras 的安装、Keras MNIST手写数字识别、Keras CIFAR-10照片图像物体识别、Keras多层感知器预测泰坦尼克号上旅客的生存概率、使用Keras MLP、RNN、LSTM进行IMDb自然语言处理与情感分析、以TensorFlow张量运算仿真神经网络的运行、TensorFlow MNIST手写数字识别、使用GPU大幅加快深度学习训练。\nTensorFlow + Keras深度学习方面的知识不需要具备高等数学模型、算法等专业知识，只需要具备基本的Python程序设计 能力，按照步骤循序渐进地学习，就可以了解深度学习的基本概念，进而实际运用深度学习的技术。\n在学习过程中，也会遇到这样那样的问题，入门阶段可以参考《白话深度学习与TensorFlow》\n《白话深度学习与TensorFlow》中文版PDF，带书签目录，文字可以复制，322页。\n下载：https://pan.baidu.com/s/1sOIhiaFUP00Azg-85tov_g\n基础篇（第1～3章），讲解了机器学习、深度学习与实践的上下文知识，如基本的机器学习与深度学习算法，TensorFlow框架的安全与配置，简单的深度学习实践。该篇是阅读和实践的基石。\n原理与实践篇（第4～8章），介绍“老牌”的深度学习网络的数学原理和工程实现原理，尤其是第4章，如 果能基本读懂，后面的网络实现层面的问题基本都可以迎刃而解。涵盖BP网络、CNN、RNN的结构、思路\n、训练与使用，以及一些常见的综合性问题。该篇是学习深度学习的重点和难点，作者通过大量示例、 推理与实现，帮读者*大化降低学习曲线。\n扩展篇（第9～13章），介绍一些网络的变种和一些较新的网络特性，涵盖深度残差网络、受限玻尔兹曼机、强化学习、对抗学习，这是读者进一步学习与实践思路的钥匙。最后给出了一些有趣的深度学习应 用：人脸识别、作诗姬、大师风图像处理，有趣又有用。\n个人感觉，学习过程中，需要配合好的资料，并且随时进行整理，这样才能收到好的学习效果。"}
{"content2":"这学期分别学习了《数据挖掘》《机器学习》和《模式识别》三门课程，为了搞明白这三者的关系，就google了下，一下为一些从网上获得的资料。\n-----------------------------\n数据挖掘和机器学习的区别和联系，周志华有一篇很好的论述《机器学习与数据挖掘》可以帮助大家理解。数据挖掘受到很多学科领域的影响，其中数据库、机器学习、统计学无疑影响最大。简言之，对数据挖掘而言，数据库提供数据管理技术，机器学习和统计学提供数据分析技术。由于统计学往往醉心于理论的优美而忽视实际的效用，因此，统计学界提供的很多技术通常都要在机器学习界进一步研究，变成有效的机器学习算法之后才能再进入数据挖掘领域。从这个意义上说，统计学主要是通过机器学习来对数据挖掘发挥影响，而机器学习和数据库则是数据挖掘的两大支撑技术。从数据分析的角度来看，绝大多数数据挖掘技术都来自机器学习领域，但机器学习研究往往并不把海量数据作为处理对象，因此，数据挖掘要对算法进行改造，使得算法性能和空间占用达到实用的地步。同时，数据挖掘还有自身独特的内容，即关联分析。\n而模式识别和机器学习的关系是什么呢，传统的模式识别的方法一般分为两种：统计方法和句法方法。句法分析一般是不可学习的，而统计分析则是发展了不少机器学习的方法。也就是说，机器学习同样是给模式识别提供了数据分析技术。\n至于，数据挖掘和模式识别，那么从其概念上来区分吧，数据挖掘重在发现知识，模式识别重在认识事物。\n？？？？？？？？？？？\n机器学习的目的是建模隐藏的数据结构，然后做识别、预测、分类等。\n因此，机器学习是方法，模式识别是目的"}
{"content2":"目前机器学习可以说是百花齐放阶段，不过如果要学习或者研究机器学习，进而用到生产环境，对平台，开发语言，机器学习库的选择就要费一番脑筋了。这里就我自己的机器学习经验做一个建议，仅供参考。\n首先，对于平台选择的第一个问题是，你是要用于生产环境，也就是具体的产品中,还是仅仅是做研究学习用？\n1. 生产环境中机器学习平台的搭建\n如果平台是要用于生产环境的话，接着有一个问题，就是对产品需要分析的数据量的估计，如果数据量很大，那么需要选择一个大数据平台。否则的话只需要一个单机版的平台就可以了。\n1.1 生产环境中机器学习大数据平台的搭建\n生产环境里面大数据平台，目前最主流的就是Spark平台，加上辅助的分布式数据处理容器，比如YARN，或者Mesos.如果需要实时的收集在线数据，那么就加上Kafka。简言之，一个通用的大数据处理平台就是集成Spark + YARN(Mesos) + Kafka. 我现在做的产品项目都是基于Spark + YARN+ Kafka的，目前来看，这个平台选择基本上是主流的方向。\n当然，有人会说，这么多开源软件，一起集成起来好麻烦，大坑肯定不少，有没有一个通用的平台，可以包括类似Spark + YARN+ Kafka的大数据平台功能呢？目前据我所知，做的比较好的有CDAP(http://cdap.io)。它对Spark， YARN， Kafka还有一些主流的开源数据处理软件进行了集成，开发者只需要在它上面封装的一层API上做二次开发就可以了。这应该是一个不错的点子，不过目前还没有看到商用的成功案例，所以我们在构架选型的时候就没有考虑CDAP。\n因此，围绕Spark + YARN+ Kafka的大数据平台还是首选。由于Spark MLlib的机器学习算法并不丰富好用，因此如果你的产品中需要一些MLlib中没有的算法，就需要自己去找开源实现了。\n1.2 生产环境中机器学习单机数据平台的搭建\n生产环境里面如果数据量不大，大数据平台就显得有点over design了，此时我们有更多的选择。首选，仍然是Spark平台，不过我们不需要分布式的容器YARN和分布式数据分发的路由Kafka了。为什么首选还是Spark呢？因为我们要考虑扩展，现在数据量不大，不代表以后数据量不大。这也是我参与的一些小型数据分析项目也是选择Spark的原因。当然我觉得还有一些原因是Spark同时支持了Python, Java, Scala和R。这降低了很多程序员的参与门槛。我参与的Spark项目中，开发语言主要是Java和Scala。Python没有选择是因为一些速度的原因和系统其它部分都是用Java写的。\n第二个选择是以scikit-learn为主的一系列python工具，包括 numpy, scipy, pandas, MatplotLib等等。特点是类库丰富，尤其是scikit-learn的机器学习库，可以说是十八般武器，样样都有。另外就是由于可以交互式的编写程序，方便快速开发原型。我参与的有两个项目在可行性分析阶段，都是用scikit-learn来做原型和给客户做demo。\n因此，生产环境中机器学习单机数据平台， Spark是做产品首选，而scikit-learn家族适合做快速的原型开发验证。\n2. 研究环境中机器学习平台的搭建\n如果只是做研究，那么选择就很多了，主流的有三种。\n第一种是基于Spark MLlib来学习。好处是学到的东西用到生产环境可以无缝切换，但是坏处也很明显，Spark东西很多，在自己的单机上跑很吃内存，比较慢，而且MLlib的类库并不丰富，很多算法需要自己再去找类库。根据周围同事的反馈，比较吃力，因此基于Spark MLlib来学习机器学习，我个人觉得不是一个好的选择。\n第二种是基于scikit-learn为主的一系列python工具来学习，包括上面提到的numpy, scipy, pandas, MatplotLib等等。好处是类库多，API强大，可以让你专注于数据的分析，例子也多，学习起来不难。当然也有缺点，就是这一大堆的python库，要熟练的用起来需要一段时间。 个人比较推荐这种方法，周围同事来说，用scikit-learn学习交流也是主流。\n第三种是基于R的平台来做机器学习（不包括Spark R），主要平台是R studio。由于R是一门比较老的语言，因此他的数据处理和机器学习的API比较丰富，尤其是对于之前做数据分析师的人来说更是熟悉不过。但是R是一门相对封闭的语言，社区远远没有Python的活跃，而且对于程序员来说， R的那种语法让人难受。几年前，一般会认为R的机器学习比Python的好，但是现在Python已经将R远远甩在了后面。因此，除非你之前已经很熟悉R语言，否则完全不推荐用R来研究机器学习，BTW，这里没有歧视R的意思。\n总之，如果你想研究学习机器学习，并且没有特殊的R背景，scikit-learn是你的首选。当然，有人会说，我喜欢自己去一点点的实现机器学习的算法，不喜欢直接调用类库，这样不行吗？ 当然，这样肯定是非常不错的，并且对加深各个算法理解很有好处。只是这样比较的花时间，如果你和我一样时间不太多的话，还是直接调用API来研究数据比较直接。\n（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）"}
{"content2":"从9月23日开始整理思维导图，前前后后半个月左右，收获确实比第一次阅读要多一些，以后会尽量按这种方式阅读，提高效率。\n第一章 问题建模\n第二章 特征工程\n第三章 常用模型\n第四章 模型融合\n第五章 用户画像\n第六章 POI实体链接\n第七章 评论挖掘\n第八章 O2O场景下的查询理解和用户引导\n第九章 O2O场景下排序的特点\n第十章 推荐在O2O场景中的应用\n第十一章 O2O场景下的广告营销\n第十二章 用户偏好和损失建模\n第十三章 深度学习概述\n第十四章 深度学习在文本领域中的应用\n第十五章 深度学习在计算机视觉中的应用\n第十六章 大规模机器学习\n第十七章 特征工程和实验平台"}
{"content2":"《zw版·Halcon-delphi系列原创教程》\n水果自动分类脚本（机器学习、人工智能）\n前面介绍了超市，流水线，酸奶的自动分类算法，下面再介绍一个水果的自动分类算法。\nHalcon强大的图像处理能力，令人往往会忽视其更加彪悍的机器学习、人工智能。\n分类，聚类分析，是机器学习、人工智能的核心算法之一，也是个典型的应用。\nHalcon内置的聚类分析、机器学习模块，就有：knn邻近算法、向量机SVM、GMM高斯混合模型（Gaussian Mixture Model，或者混合高斯模型，也可以简写为MOG（Mixture of Gaussian）、MLP(多层神经网络)等等。\n而且相关基本上都是汇编级的高度优化，直接调用就可以。\n目前国内、海外机器学习、人工智能方面的学者，没有几位重视这块。\n国外，可能是版权问题，毕竟，Halcon是售价高达数万欧元（不是人民币）的商业软件，而且主要用于自控、机器视觉等工业领域，而不是大学。\n国内，可能是对于Halcon的了解不够。\n其实，图像处理的核心，图像识别、分类，都离不开机器学习、人工智能\n大家看看opencv的发展路线就可以清楚看到，从cv1.0的图像，到cv1.0的机器学习，以及目前cv3.0的GPU、cuda人工智能模块，AI在其中所占据的份额越来越大。\nHalcon因为面向一线生产线，所以很多机器学习、人工智能，都是黑箱式的，无需编程，直接调用，录入内置的ocr模块，可以识别99%的标准工业字符：超市、海关、流水线\n不过，Halcon也提供了大量的机器学习模块，毕竟各种应用场合复制，必须进行定制。\n这个脚本，AI方面不算复杂，建模就是先拍摄几张产品的照片，直接匹配。\n通常，Halcon建模，需要进行200次（默认参数）迭代。\n脚本80多行，很简单。\n虽然这个脚本和前面的酸奶分类脚本，都很简单，其实，应用领域很广\n自动流水线、物流、智能仓库等，无论是元器件的自动识别、包裹自动分类，以及产品的QC等等，核心模块，就是这些代码、算法、\n选这个脚本，其中一个原因，是因为前几天，有人在论坛询问，如何对企业生产线的产品（零食好像？）进行自动分类。\n1 * This example program shows how to apply a general GMM 2 * classification to distinguish citrus fruits using the 3 * features 'area' and 'circularity'. Additionally, the 4 * 2D feature space for the extracted fruits is visualized. 5 * 6 read_image (Image, 'color/citrus_fruits_01') 7 get_image_pointer1 (Image, Pointer, Type, Width, Height) 8 dev_close_window () 9 dev_open_window (0, 0, Width, Height, 'white', WindowHandle) 10 set_display_font (WindowHandle, 12, 'courier', 'true', 'false') 11 dev_set_draw ('margin') 12 dev_set_line_width (2) 13 dev_display (Image) 14 dev_update_window ('off') 15 dev_update_pc ('off') 16 dev_update_var ('off') 17 * 18 FeaturesArea := [] 19 FeaturesCircularity := [] 20 ClassName := ['orange','lemon'] 21 * 22 * Create a GMM classifier 23 create_class_gmm (2, 2, 1, 'spherical', 'normalization', 10, 42, GMMHandle) 24 * 25 * Add training samples 26 for i := 1 to 4 by 1 27 read_image (Image, 'color/citrus_fruits_' + i$'.2d') 28 dev_display (Image) 29 * 'Add Samples' 30 get_regions (Image, SelectedRegions) 31 dev_display (SelectedRegions) 32 count_obj (SelectedRegions, NumberObjects) 33 for j := 1 to NumberObjects by 1 34 select_obj (SelectedRegions, ObjectSelected, j) 35 get_features (ObjectSelected, WindowHandle, Circularity, Area, RowRegionCenter, ColumnRegionCenter) 36 FeaturesArea := [FeaturesArea,Area] 37 FeaturesCircularity := [FeaturesCircularity,Circularity] 38 FeatureVector := real([Circularity,Area]) 39 if (i <= 2) 40 add_sample_class_gmm (GMMHandle, FeatureVector, 0, 0) 41 disp_message (WindowHandle, 'Add to Class:' + ClassName[0], 'window', RowRegionCenter, ColumnRegionCenter - 100, 'black', 'true') 42 else 43 add_sample_class_gmm (GMMHandle, FeatureVector, 1, 0) 44 disp_message (WindowHandle, 'Add to Class:' + ClassName[1], 'window', RowRegionCenter, ColumnRegionCenter - 100, 'black', 'true') 45 endif 46 endfor 47 disp_continue_message (WindowHandle, 'black', 'true') 48 stop () 49 endfor 50 dev_clear_window () 51 * 52 * Visualize the feature space 53 visualize_2D_feature_space (Cross, Height, Width, WindowHandle, FeaturesArea[0:5], FeaturesCircularity[0:5], 'dim gray', 18) 54 * 'oranges', 40, 440 55 visualize_2D_feature_space (Cross, Height, Width, WindowHandle, FeaturesArea[6:11], FeaturesCircularity[6:11], 'light gray', 18) 56 * 'lemons', 70, 440 57 disp_continue_message (WindowHandle, 'black', 'true') 58 stop () 59 * 60 * Train the classifier 61 train_class_gmm (GMMHandle, 100, 0.001, 'training', 0.0001, Centers, Iter) 62 * 63 * Classify 64 for i := 1 to 15 by 1 65 read_image (Image, 'color/citrus_fruits_' + i$'.2d') 66 dev_display (Image) 67 * 'Classify Image', 10, 10 68 get_regions (Image, SelectedRegions) 69 dev_display (SelectedRegions) 70 count_obj (SelectedRegions, NumberObjects) 71 for j := 1 to NumberObjects by 1 72 select_obj (SelectedRegions, ObjectSelected, j) 73 get_features (ObjectSelected, WindowHandle, Circularity, Area, RowRegionCenter, ColumnRegionCenter) 74 FeaturesArea := [FeaturesArea,Area] 75 FeaturesCircularity := [FeaturesCircularity,Circularity] 76 FeatureVector := real([Circularity,Area]) 77 classify_class_gmm (GMMHandle, FeatureVector, 1, ClassID, ClassProb, Density, KSigmaProb) 78 disp_message (WindowHandle, 'Class: ' + ClassName[ClassID], 'window', RowRegionCenter, ColumnRegionCenter - 100, 'black', 'true') 79 disp_message (WindowHandle, 'KSigmaProb: ' + KSigmaProb, 'window', RowRegionCenter + 30, ColumnRegionCenter - 100, 'black', 'true') 80 endfor 81 if (i != 15) 82 disp_continue_message (WindowHandle, 'black', 'true') 83 endif 84 stop () 85 endfor 86 * 87 * Clear the classifier from memory 88 clear_class_gmm (GMMHandle)\n【《zw版·Halcon-delphi系列原创教程》,网址，cnblogs.com/ziwang/】"}
