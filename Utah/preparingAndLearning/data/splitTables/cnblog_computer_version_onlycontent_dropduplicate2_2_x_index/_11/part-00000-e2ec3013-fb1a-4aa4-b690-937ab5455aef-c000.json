{"content2":"1. ===功能===人工智能现在已经能实现很多功能了，比如语音识别——李开复博士当年做的工作奠定了很多当今识别系统的基础。这里忍不住说一下，Siri本身的技术并没有特别大的亮点，真正nb的是它的模式（语音识别直接与搜索引擎结合在一起，产品体验做得好。而且关键是这样的模式能采集到更多数据，使得系统的精度越来越高）自然语言理解——目前看到的最强的结果应该是IBM Watson。但其实我们现在用的搜索引擎、中文输入法、机器翻译（虽然其实还不怎么work）都和自然语言理解相关。这块儿不是我的专业，请 @段维斯 同学补充。数据挖掘——随着近年数据量的疯狂增长，数据挖掘也有了长足进步。最具有代表性的是前几年著名的Netflix challenge（Netflix公司公开了自己的用户评分数据，让研究者根据这些数据对用户没看过的电影预测评分，谁先比现有系统好10%，谁就能赢100万美元）最后这一比赛成绩较好的队伍，并非是单一的某个特别nb的算法能给出精确的结果，而是把大量刻画了不同方面的模型混合在一起，进行最终的预测。计算机视觉——目前越来越多的领域跟视觉有关。大家可能一开始想到的都是自动驾驶。虽然大家都在说googleX的无人车， 但实际上现在无论是商业上，还是技术整合上最成功的算法是Mobile Eye的辅助驾驶系统。这个公司也是目前computer vision领域最挣钱的公司。从实现新功能方面说，视觉的发展的趋势主要有两方面，A) 集成更多的模块，从问题的各种不同方面，解决同一个问题（比如Mobile Eye，就同时使用了数十种方法，放到一起最终作出决策） B) 使用新的信息，解决一个原来很难的问题。这方面最好的例子是M$的Kinect，这个产品最让人拍案叫绝的就是那个红外pattern投影仪。2. ===理论基础===这里说的是数学理论，是为实现功能解决问题而存在的。与人类的智能的联系在下一节说。从这个角度，我们已经有了很多强有力的数学工具，从高斯时代的最小二乘法，到现在比较火的凸优化，其实我们解决绝大多数智能问题的套路，都可以从某种意义上转换成一个优化问题。真正限制我们解这个优化问题的困难有以下三个：计算复杂度——能保证完美解的算法大都是NP-hard的。如何能让一个系统在当前的硬件下“跑起来”，就需要在很多细节取巧，这是很多learning paper的核心冲突。模型假设——所有模型都要基于一些假设，比如说，无人车会假设周围的汽车加速度有一个上限（至少不会瞬间移动吧，否则怎么闪避）绝大多数假设都不能保证绝对正确，我们只是制定那些在大多数时候合理的假设，然后基于这些假设建模（比如，在语音识别里，我们是否要假设存在背景噪声呢？如果有背景噪声，这个噪声应该符合什么特点呢？这时候无论你怎么定标准，总能找出“反例”）数据基础——任何学习过程都需要数据的支持，无论是人类学说话学写字，还是计算机学习汽车驾驶。但是就数据采集本身来说，成功的案例并不多。大概这个世界上最强的数据采集就是google了吧。每次你搜索一个关键词，然后点进去，google就自动记录了你的行为，然后以此数据来训练自己的算法。随着深度学习技术的成熟，AI人工智能正在逐步从尖端技术慢慢变得普及。AlphaGo和人类的对弈，并不是我们以往所理解的电子游戏，电子游戏的水平永远不会提升，而AlphaGo则具备了人工智能最关键的“深度学习”功能。AlphaGo中有两个深度神经网络，Value Networks（价值网络）和 Policy Networks（策略网络）。其中Value Networks评估棋盘选点位置，Policy Networks选择落子。这些神经网络模型通过一种新的方法训练，结合人类专家比赛中学到的棋谱，以及在自己和自己下棋（Self-Play）中进行强化学习。也就是说，人工智能的存在，能够让AlphaGo的围棋水平在学习中不断上升。人工智能的技术应用主要是在以下几个方面：自然语言处理（包括语音和语义识别、自动翻译）、计算机视觉（图像识别）、知识表示、自动推理（包括规划和决策）、机器学习和机器人学。按照技术类别来分，可以分成感知输入和学习与训练两种。计算机通过语音识别、图像识别、读取知识库、人机交互、物理传感等方式，获得音视频的感知输入，然后从大数据中进行学习，得到一个有决策和创造能力的大脑。从上世纪八九十年代的PC时代，进入到互联网时代后，给我们带来的是信息的爆炸和信息载体的去中心化。而网络信息获取渠道从PC转移到移动端后，万物互联成为趋势，但技术的限制导致移动互联网难以催生出更多的新应用和商业模式。而如今，人工智能已经成为这个时代最激动人心、最值得期待的技术，将成为未来10年乃至更长时间内IT产业发展的焦点。人工智能概念其实在上世纪80年代就已经炒得火热，但是软硬件两方面的技术局限使其沉迷了很长一段时间。而现在，大规模并行计算、大数据、深度学习算法和人脑芯片这四大催化剂的发展，以及计算成本的降低，使得人工智能技术突飞猛进。一、驱动人工智能发展的先决条件物联网——物联网提供了计算机感知和控制物理世界的接口和手段，它们负责采集数据、记忆、分析、传送数据、交互、控制等等。摄像头和相机记录了关于世界的大量的图像和视频，麦克风记录语音和声音，各种传感器将它们感受到的世界数字化等等。这些传感器，就如同人类的五官，是智能系统的数据输入，感知世界的方式。而大量智能设备的出现则进一步加速了传感器领域的繁荣，这些延伸向真实世界各个领域的触角是机器感知世界的基础，而感知则是智能实现的前提之一。大规模并行计算——人脑中有数百至上千亿个神经元，每个神经元都通过成千上万个突触与其他神经元相连，形成了非常复杂和庞大的神经网络，以分布和并发的方式传递信号。这种超大规模的并行计算结构使得人脑远超计算机，成为世界上最强大的信息处理系统。近年来，基于GPU（图形处理器）的大规模并行计算异军突起，拥有远超CPU的并行计算能力。从处理器的计算方式来看，CPU计算使用基于x86指令集的串行架构，适合尽可能快的完成一个计算任务。而GPU从诞生之初是为了处理3D图像中的上百万个像素图像，拥有更多的内核去处理更多的计算任务。因此GPU天然具备了执行大规模并行计算的能力。云计算的出现、GPU的大规模应用使得集中化的数据计算处理能力变得前所未有的强大。大数据——根据统计，2015年全球产生的数据总量达到了十年前的20多倍，海量的数据为人工智能的学习和发展提供了非常好的基础。机器学习是人工智能的基础，而数据和以往的经验，就是人工智能学习的书本，以此优化计算机的处理性能。深度学习算法——最后，这是人工智能进步最重要的条件，也是当前人工智能最先进、应用最广泛的核心技术，深度神经网络（深度学习算法）。2006年，Geoffrey Hinton教授发表的论文《A fast learning algorithm for deep belief nets》。他在此文中提出的深层神经网络逐层训练的高效算法，让当时计算条件下的神经网络模型训练成为了可能，同时通过深度神经网络模型得到的优异的实验结果让人们开始重新关注人工智能。之后，深度神经网络模型成为了人工智能领域的重要前沿阵地，深度学习算法模型也经历了一个快速迭代的周期，Deep Belief Network、Sparse Coding、Recursive Neural Network, Convolutional Neural Network等各种新的算法模型被不断提出，而其中卷积神经网络（Convolutional Neural Network，CNN）更是成为图像识别最炙手可热的算法模型。二、IT巨头在人工智能上的投入技术的进步使得人工智能的发展在近几年显著加速，IT巨头在人工智能上的投入明显增大，一方面网罗顶尖人工智能的人才，另一方面加大投资力度频频并购，昭示着人工智能的春天已经到来。电商一班邢利栋"}
