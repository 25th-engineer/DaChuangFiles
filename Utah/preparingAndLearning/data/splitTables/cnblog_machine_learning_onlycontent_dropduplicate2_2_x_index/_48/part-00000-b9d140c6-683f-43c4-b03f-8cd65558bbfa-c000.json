{"content2":"1、L1范式和L2方式的区别\n（1）L1范式是对应参数向量绝对值之和\n（2）L1范式具有稀疏性\n（3）L1范式可以用来作为特征选择，并且可解释性较强（这里的原理是在实际Loss function中都需要求最小值，根据L1的定义可知L1最小值只有0，故可以通过这种方式来进行特征选择）\n（4）L2范式是对应参数向量的平方和，再求平方根\n（5）L2范式是为了防止机器学习的过拟合，提升模型的泛化能力\n2、优化算法及其优缺点\n温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。\n（1）随即梯度下降\n优点：可以一定程度上解决局部最优解的问题\n缺点：收敛速度较慢\n（2）批量梯度下降\n优点：容易陷入局部最优解\n缺点：收敛速度较快\n（3）mini_batch梯度下降\n综合随即梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。\n（4）牛顿法\n牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算Hessian矩阵比较困难。\n（5）拟牛顿法\n拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。\n（6）共轭梯度\n（7）启发式的优化算法\n启发式的优化算法有遗传算法，粒子群算法等。这类算法的主要思想就是设定一个目标函数，每次迭代根据相应的策略优化种群。直到满足什么样的条件为止。\n3、RF与GBDT之间的区别\n（1）相同点\n都是由多棵树组成\n最终的结果都是由多棵树一起决定\n（2）不同点\n组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成\n组成随机森林的树可以并行生成，而GBDT是串行生成\n随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和\n随机森林对异常值不敏感，而GBDT对异常值比较敏感\n随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的\n随机森林不需要进行数据预处理，即特征归一化。而GBDT则需要进行特征归一化\n（3）RF：\n优点：\n易于理解，易于可视化\n不需要太多的数据预处理，即数据归一化\n不易过拟合\n易于并行化\n缺点：\n不适合小样本数据，只适合大样本数据\n大多数情况下，RF的精度低于GBDT\n适合决策边界的是矩阵，不适合对角线型\n（4）GBDT\n优点：\n精度高\n缺点：\n参数较多，容易过拟合\n不易并行化\n4、SVM的模型的推导\n5、SVM与树模型之间的区别\n（1）SVM\nSVM是通过核函数将样本映射到高纬空间，再通过线性的SVM方式求解分界面进行分类。\n对缺失值比较敏感\n可以解决高纬度的问题\n可以避免局部极小值的问题\n可以解决小样本机器学习的问题\n（2）树模型\n可以解决大样本的问题\n易于理解和解释\n会陷入局部最优解\n易过拟合\n6、梯度消失和梯度膨胀\n（1）梯度消失：\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->\n可以采用ReLU激活函数有效的解决梯度消失的情况\n（2）梯度膨胀\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大\n可以通过激活函数来解决\n7、LR的原理和Loss的推导\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->"}
