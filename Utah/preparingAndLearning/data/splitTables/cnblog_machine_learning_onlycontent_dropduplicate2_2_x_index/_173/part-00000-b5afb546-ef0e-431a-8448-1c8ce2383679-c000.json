{"content2":"一.概念\n概念学习：是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数。\n二.概念学习任务\n任何概念学习任务能被描述为：实例的集合、实例集合上的目标函数、候选假设的集合以及训练样例的集合。\nEnjoySport概念学习任务\n已知：\n实例集X：可能的日子，每个日子由下面的属性描述：\nsky:(可取值 sunny,Cloudy和Rainy)\nAirTemp:(可取值为Warm和Cold)\nHumidity:(可取值为Normal和High)\nWind:(可取值为：Strong和Weak)\nWater：(可取值为Warm和Cold)\nForecast:(可取值为Same和Change)\n假设集H：每个假设描述为6个属性：Sky,AirTemp,Humidity,Wind,Water和Forecast的值约束的合取。约束可以为“？”（表示接受任意值），“ø”（表示拒绝所有值），或一特定值\n目标概念C:EnjoySport: X->{0,1}\n训练样例集D：目标函数的正例和反例\n求解：\nH中的一假设h，使对于X中任意x，h(x)=c(x)\n1.术语定义\n实例集（X）:概念定义的实例集合\n目标概念（c）：待学习概念或函数\n训练样例（D）:每个样例为X中的一个实例x以及它的目标概念值c(x)。c(x)=1的实例被称为正例（positive example），c(x)=0的实例为反例（negative example），经常用序偶<x,c(x)>来描述训练样例。\nH表示所有可能假设的集合。H中每个假设H表示X上定义的布尔函数，即h:X->{0,1}。机器学习的目标就是寻找一个假设h，使对于X中的所有x，h(x)=c(x)。\n归纳学习假设：任一假设如果在足够大的训练样例集中很好地逼近目标函数，它也能在未见实例中很好地逼近目标函数。\n三.作为搜索的概念学习\n定义：令hj和hk为在X上定义的布尔函数。称hj more_general_than_or_equal_to hk（记做hj≥g hk），当且仅当(∨x∈X)[(hk(x)=1)->(hj(x)=1)]\nhj more_specific_than hk ，当hk more_general_than hj\n四.FIND-S：寻找极大特殊假设\n从H中最特殊假设开始，然后在该假设覆盖正例失败时将其一般化（当一假设能正确地划分一个正例时，称该假设“覆盖”该正例）。\nFIND-S算法\n1. 将h初始化为H中最特殊假设\n2.对每个正例x\n对h的每个属性约束ai\n如果x满足ai\n那么不做任何处理\n否则将h中ai替换为x满足的下一个更一般的约束\n3. 输出假设h\n五.变换空间和候选消除算法（CANDIDATE-ELIMINATION）\nFIND-S输出的假设只是H中能够拟合训练样例的多个假设中的一个。而在候选消除算法中，输出的是与训练样例一致的所有假设的集合。\n1.表示\n定义：一个假设h与训练样例集合D一致，当且仅当对D中每一个样例<x,c(x)>都有h(x)=c(x)。\nConsistent(h,D)≡(∨<x,c(x)>∈D) h(x)=c(x)\n定义:关于假设空间H和训练样例集D的变型空间，标记为VSH,D，是H中与训练样例D一致的所有假设构成的子集。\nVSH,D≡{h∈H|Consistent(h,D)}\n2.列表后消除算法（LIST-THEN-ELIMINATE）\n列表后消除算法\n1.变型空间VersionSpace<-包含H中所有假设的列表\n2.对每个训练样例<x,c(x)>\n从变型空间中移除所有h(x)≠c(x)的假设h\n3. 输出VersionSpace中个假设列表\n3.变型空间的更简洁表示\n定义：关于假设空间H和训练数据D的一般边界（general boundary）G，是在H中与D相一致的极大一般（maximally general）成员的集合。\n定义：关于假设空间H和训练数据D的特殊边界（specific  boundary）S，是在H中与D相一致的极大特殊（maximally specific）成员的集合。\n变型空间的确切组成是：G中包含的假设，S中包含的假设已经G和S直接偏序结果所规定的假设。\n定理2.1：变型空间表示定理 令X为任意的实例集合，H为X上定义的布尔假设的集合。另c:X->{0,1}为X上定义的任一个目标概念，并令D为任一训练样例的集合{<x,c(x)>}。对所有的X,H,c,D以及良好定义的S和G:\n4.候选消除学习算法\n使用变型空间的候选消除算法\n将G集合初始化为H中极大一般假设\n将S集合初始化为H中极大特殊假设\n对每个训练例d，进行以下操作：\n如果d是一正例\n• 从G中移去所有与d不一致的假设\n• 对S中每个与d不一致的假设s\n•从S中移去s\n• 把s的所有的极小一般化式h加入到S中，其中h满足\n•h与d一致，而且G的某个成员比h更一般\n• 从S中移去所有这样的假设：它比S中另一假设更一般\n如果d是一个反例\n• 从S中移去所有d不一致的假设\n• 对G中每个与d不一致的假设g\n•从G中移去g\n•把g的所有的极小特殊化式h加入到G中，其中h满足\n•h与d一致，而且S的某个成员比h更特殊\n•从G中移去所有这样的假设：它比G中另一假设更特殊\n5.算法举例\n候选消除算法步骤（EnjoySport）\n训练样例：\n1.<Sunny,Warm,Normal,Strong,Warm,Same>,EnjoySport=Yes\n2.<Sunny,Warm,High,Strong,Warm,Same>,EnjoySport=Yes\nS0和G0为最初的边界集合，分别对应最特殊和最一般假设。训练样例1和2使得S边界变得更一般，如FIND-S算法中一样，这些样例对G边界没有影响。\n训练样例:\n3.<Rainy,Cold,High,Strong,Warm,Change>,EnjoySport=No\n样例3是一个反例，他把G2边界特殊化为G3。注意在G3中有多个可选的极大一般假设。\n训练样例：\n4.<Sunny,Warm,High,Storage,Cool,Change>,EnjoySport=Yes\n正例是S边界更一般，从S3变为S4。G3的一个成员也必须被删除，因为它不再比S4更一般。\nEnjoySprot概念学习问题中的最终的变型空间\n六.归纳偏置\n1.无偏的学习器\n幂集（power set）把集合X的所有子集的集合称为幂集。\n新的假设空间H’，它能表示实例的每一个子集，也就是把H’对应到X的幂集。\n<Sunny,?,?,?,?,?>∨<Cloudy,?,?,?,?,?>\n2.无偏学习的无用性\n学习器如果不对目标概念的形式做预先的假定，它从根本上无法对未见实例进行分类。\n一般情况下任意的学习算法L以及为任意目标概念提供的任意训练数据Dc={<x,c(x)>}。训练过程结束后，L需要对新的实例xi进行分类。令L(xi,DC)表示在对训练数据Dc学习后L赋予xi的分类（正例或反例），我们可以如下描述L所进行的这一归纳推理过程：\ny表示z从y归纳推理得到。\n定义：考虑对于实例集合X的概念学习算法L。令c为X上定义的任一概念，并令Dc={<x,c(x)>}为c的任意训练样例集合。令L(xi,Dc)表示经过数据Dc的训练后L赋予实例xi的分类。L的归纳偏置是最小断言集合B，它使任意目标概念c和相应的训练样例Dc满足：\n候选消除算法的归纳偏置：目标概念c包含在给定的假设空间H中。\n使用假设空间H的候选消除算的输入输出行为，等价于利用了断言“H包含目标概念”的演绎定理证明器。该断言因此被称为候选消除算法的归纳偏置。用归纳偏置来刻画归纳系统，可以便于使用等价的演绎系统来模拟它们。这提供了一种对归纳系统进行比较的方法，即通过它们从训练数据中泛化的策略。"}
