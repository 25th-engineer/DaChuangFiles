{"content2":"背景：本文只是对机器学习相关知识的梳理和复习用，因此顺序上可能有些随意\n摘要：\n1.各种算法的推导\n2.各种算法的比较（或优缺点）\n3.学习理论\n4.特征选择方法\n5.模型选择方法\n6.特征工程\n7.数据预处理\n8.应用例子\n内容：\n1.各种算法的推导\n线性回归（Liner Regression）:9个基本概念和10个基本算法总结的岭（ridge）回归部分  -- 最简单的线性分类器\n机器学习-感知机 -- 线性分类器,SVM和神经网络的基石\nLR：我的LR复习总结 -- 线性分类器,预测概率\n支持向量机（SVM）：我的SVM复习总结 -- 线性/非线性分类器，核方法映射到高维线性可分\nDT,RF,GBDT,XGBT:决策树和基于决策树的集成方法（DT,RF,GBDT,XGBT）复习总结 -- 非线性分类器，决策树，规则学习/条件概率\n关联规则：FPGrowth算法总结复习 -- FP树，规则学习\nKNN:k近邻(KNN)复习总结 -- 非线性模型，KD树和ball tree，基于距离的模型\nk-means:K-Means聚类和EM算法复习总结 -- 基于距离的的模型，KD树和ball tree\nNB: 朴素贝叶斯（NB）复习总结 -- 线性分类器，判别模型与生成模型\nLDA：主题模型——隐式狄利克雷分布总结--概率图模型PGM,NLP\nHMM：隐马尔可夫模型（HMM）总结--概率图模型PGM，NLP\nCRF:条件随机场CRF\n神经网络：我的机器学习/数据挖掘的书单--机器学习中的图模型，仿生学\nTextRank与TF-IDF关键词提取--对比LDA语义模型的词义模型\nwordEmbedding与Word2Vec/Doc2Vec:deep-learning-nlp-best-practices\nCNN:待总结\nRNN/LSTM：雪伦RNN的文章   雪伦LSTM的文章\nWDL:待总结\n2.各种算法的比较（或优缺点）\n生成模型和判别模型（是否需要学习联合分布）：生成模型与判别模型\n线性模型和非线性模型：机器学习常见面试题整理\nLR和决策树类算法的比较：逻辑回归与决策树在分类上的一些区别\nBryan__的整理：机器学习算法比较\n机器学习面试知识点总结(不断补充中)\n3. 学习理论\n理论部分：偏差-方差平衡\n正则化：数据预处理中归一化（Normalization）与损失函数中正则化（Regularization）解惑\n经验风险最小化与结构风险最小化:Andrew Ng机器学习公开课笔记 -- 学习理论\n损失函数和分类模型的评价指标：损失函数和分类器评估方法；\n二分类如何转换为多分类：机器学习面试知识点总结(不断补充中)\n熵在机器学习中的身影：信息论中的熵（信息熵，联合熵，交叉熵，互信息）和最大熵模型\nVC维:Andrew Ng机器学习公开课笔记 -- 学习理论\n常见的距离算法和相似度（相关系数）计算方法\nUCB与Hoeffing Bound:待总结\n4.特征选择方法\n使用sklearn做单机特征工程   附：自己实现的代码\n特征选择与特征学习方法-jason(1遍)\nskelearn中特征选择的一些方法了解\n5.模型选择方法\nSVM参数详解：SVM参数详解\nRF和GBDT参数详解：《使用sklearn进行集成学习——实践》（还在研究）\nXGBoost参数调优：XGBoost-Python完全调参指南-参数解释篇 （第二遍，考虑使用排序任务）\nLightGBM参数调优:待总结\n模型融合（blending和stacking）:http://m.blog.csdn.net/article/details?id=53054686 ( 多数投票/加权平均，自融合，blending，stacking)    github\n贝叶斯信息准则(BIC)\n你有哪些deep learning（rnn、cnn）调参的经验？\n6.特征工程\n7种常用的特征工程 （清晰易懂）\n特征工程理论部分\n美团的数据清洗和特征处理\nbyran_的总结帖子\nsklearn中使用GBDT生成组合特征的例子\n特征的生命周期（我的比赛经验总结）\n7.数据预处理\nweka进行预处理\n数据清洗和数据预处理（pandas 和 sklearn）\n数据挖掘笔记（三）—数据预处理\n降维：用于降维可视化的t-SNE\n聚类：kmeans,k-shit,谱聚类,密度聚类\n8.应用例子\n深入浅出谈数据挖掘——数据挖掘主要解决的四类问题\n数赛刷题的冠军链接\n使用SVD对图片进行降维的例子（github代码）\n利用机器学习模型去做排序任务"}
