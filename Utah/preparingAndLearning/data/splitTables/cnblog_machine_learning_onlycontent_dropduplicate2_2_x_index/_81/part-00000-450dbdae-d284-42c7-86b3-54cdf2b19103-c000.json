{"content2":"By Kubi Code\n文章目录\n1. 有监督学习和无监督学习的区别\n2. 正则化\n3. 过拟合\n3.1. 产生的原因\n3.2. 解决方法\n4. 泛化能力\n5. 生成模型和判别模型\n6. 线性分类器与非线性分类器的区别以及优劣\n6.1. 特征比数据量还大时，选择什么样的分类器？\n6.2. 对于维度很高的特征，你是选择线性还是非线性分类器？\n6.3. 对于维度极低的特征，你是选择线性还是非线性分类器？\n7. ill-condition病态问题\n8. L1和L2正则的区别，如何选择L1和L2正则\n9. 特征向量的归一化方法\n10. 特征向量的异常值处理\n11. 越小的参数说明模型越简单\n12. svm中rbf核函数与高斯和函数的比较\n13. KMeans初始类簇中心点的选取\n13.1. 选择批次距离尽可能远的K个点\n13.2. 选用层次聚类或者Canopy算法进行初始聚类\n14. ROC、AUC\n14.1. ROC曲线\n14.2. AUC\n14.3. 为什么要使用ROC和AUC\n15. 测试集和训练集的区别\n16. 优化Kmeans\n17. 数据挖掘和机器学习的区别\n18. 备注\n有监督学习和无监督学习的区别\n有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBRT）\n无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)\n正则化\n正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。\n奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。\n过拟合\n如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。\n产生的原因\n因为参数太多，会导致我们的模型复杂度上升，容易过拟合\n权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.\n解决方法\n交叉验证法\n减少特征\n正则化\n权值衰减\n验证数据\n泛化能力\n泛化能力是指模型对未知数据的预测能力\n生成模型和判别模型\n生成模型：由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。（朴素贝叶斯）\n生成模型可以还原联合概率分布p(X,Y)，并且有较快的学习收敛速度，还可以用于隐变量的学习\n判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。（k近邻、决策树）\n直接面对预测，往往准确率较高，直接对数据在各种程度上的抽象，所以可以简化模型\n线性分类器与非线性分类器的区别以及优劣\n如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。\n常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归\n常见的非线性分类器：决策树、RF、GBDT、多层感知机\nSVM两种都有(看线性核还是高斯核)\n线性分类器速度快、编程方便，但是可能拟合效果不会很好\n非线性分类器编程复杂，但是效果拟合能力强\n特征比数据量还大时，选择什么样的分类器？\n线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分\n对于维度很高的特征，你是选择线性还是非线性分类器？\n理由同上\n对于维度极低的特征，你是选择线性还是非线性分类器？\n非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分\nill-condition病态问题\n训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题（这简直是不能用啊）\nL1和L2正则的区别，如何选择L1和L2正则\n他们都是可以防止过拟合，降低模型复杂度\nL1是在loss function后面加上 模型参数的1范数（也就是|xi|）\nL2是在loss function后面加上 模型参数的2范数（也就是sigma(xi^2)），注意L2范数的定义是sqrt(sigma(xi^2))，在正则项上没有添加sqrt根号是为了更加容易优化\nL1 会产生稀疏的特征\nL2 会产生更多地特征但是都会接近于0\nL1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。L1在特征选择时候非常有用，而L2就只是一种规则化而已。\n特征向量的归一化方法\n线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)\n对数函数转换，表达式如下：y=log10 (x)\n反余切函数转换 ，表达式如下：y=arctan(x)*2/PI\n减去均值，乘以方差：y=(x-means)/ variance\n特征向量的异常值处理\n用均值或者其他统计量代替\n越小的参数说明模型越简单\n过拟合的，拟合会经过曲面的每个点，也就是说在较小的区间里面可能会有较大的曲率，这里的导数就是很大，线性模型里面的权值就是导数，所以越小的参数说明模型越简单。\n追加：这个其实可以看VC维相关的东西感觉更加合适\nsvm中rbf核函数与高斯和函数的比较\n高斯核函数好像是RBF核的一种\nKMeans初始类簇中心点的选取\n选择批次距离尽可能远的K个点\n首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的店作为第三个中心店，以此类推，直至选取大k个\n选用层次聚类或者Canopy算法进行初始聚类\nROC、AUC\nROC和AUC通常是用来评价一个二值分类器的好坏\nROC曲线\n曲线坐标上：\nX轴是FPR（表示假阳率-预测结果为positive，但是实际结果为negitive，FP/(N)）\nY轴式TPR（表示真阳率-预测结果为positive，而且的确真实结果也为positive的,TP/P）\n那么平面的上点(X,Y)：\n(0,1)表示所有的positive的样本都预测出来了，分类效果最好\n(0,0)表示预测的结果全部为negitive\n(1,0)表示预测的错过全部分错了，分类效果最差\n(1,1)表示预测的结果全部为positive\n针对落在x=y上点，表示是采用随机猜测出来的结果\nROC曲线建立\n一般默认预测完成之后会有一个概率输出p，这个概率越高，表示它对positive的概率越大。\n现在假设我们有一个threshold，如果p>threshold，那么该预测结果为positive，否则为negitive，按照这个思路，我们多设置几个threshold,那么我们就可以得到多组positive和negitive的结果了，也就是我们可以得到多组FPR和TPR值了\n将这些(FPR,TPR)点投射到坐标上再用线连接起来就是ROC曲线了\n当threshold取1和0时，分别得到的就是(0,0)和(1,1)这两个点。（threshold=1，预测的样本全部为负样本，threshold=0，预测的样本全部为正样本）\nAUC\nAUC(Area Under Curve)被定义为ROC曲线下的面积，显然这个面积不会大于1（一般情况下ROC会在x=y的上方，所以0.5<AUC<1）.\nAUC越大说明分类效果越好\n为什么要使用ROC和AUC\n因为当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动。\nhttp://www.douban.com/note/284051363/?type=like\n测试集和训练集的区别\n训练集用于建立模型,测试集评估模型的预测等能力\n优化Kmeans\n使用kd树或者ball tree(这个树不懂)\n将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可\n数据挖掘和机器学习的区别\n机器学习是数据挖掘的一个重要工具，但是数据挖掘不仅仅只有机器学习这一类方法，还有其他很多非机器学习的方法，比如图挖掘，频繁项挖掘等。感觉数据挖掘是从目的而言的，但是机器学习是从方法而言的。\n备注\n题目主要来源于网络，答案主要来源于网络或者《统计学习方法》，还有自己一小部分的总结，如果错误之处敬请指出\n如果想要了解关于常见模型的东东可以看这篇机器学习常见算法个人总结（面试用）文章"}
