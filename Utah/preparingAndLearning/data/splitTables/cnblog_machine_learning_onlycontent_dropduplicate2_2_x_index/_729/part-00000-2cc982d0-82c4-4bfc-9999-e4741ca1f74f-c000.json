{"content2":"瓶颈\n任何事物的发展都会遇到瓶颈。半导体业界的摩尔定律在很长的一段时间里面一直是有效的，但是在近几年也快走到尽头了。\n机器学习在AlphaGo战胜人类棋手之后，名声大噪，我也是在那次比赛之后开始研究机器学习的。机器学习这项技术是不是有一个天花板，这个天花板在哪里，我们现在的技术发展离开这个天花板到底有多远，我们是在地板上呢，还是快触碰到天花板了呢？\n在五年前，Intel公司的CEO就抛出了无法继续摩尔定律的危机说。摩尔定律由英特尔联合创始人戈登-摩尔（Gordon Moore）提出，意思是说：当价格不变时，集成电路上可容纳的晶体管数目，约每隔 18 个月便会增加一倍，性能也将提升一倍。换言之，每一美元所能买到的电脑性能，将每隔 18 个月翻两倍以上。这个定律虽然奏效了数十年，但是从2018年开始，这个定律就已经失效\n黑盒白盒之争\n在知乎上有这样一篇文章\nhttps://zhuanlan.zhihu.com/p/21362413?fc=1&group_id=821400638150828032#comment-145854724\n大概的意思是用一个神经网络来调控另一个神经网络！\n以前，虽然我们不知道AlphaGo是怎么想的，但是我们知道它是怎么学的，\n以后，我们不但不知道AlphaGo是怎么想的，我们还不知道它是怎么学的！！！\n人工智能到底是黑盒还是白盒？在评论里面关于这个话题，大家产生了很大的分歧。\n作为传统的程序员，我的观点如下：当然如果您有时间，可以看一下评论，非常精彩。\n1.ML 归根到底是程序，如果LOG足够多的话，如果你足够耐心的话，你肯定可以知道，结果是如何产生的。\n2.用神经网络去优化神经网络，其本质是一样的，就想加法变成乘法，但是还没有脱离实数的范围，到达一个更高的维度。\n3.ML的程序，包括无监督的程序，都是人写的，都是按照人的想法在执行的，所以，为什么人不知道机器是怎么想的？即使这个程序表现得再不可思议，但结果应该都在人的预料之中。AlphaGo为什么会做决定，背后是程序，程序的背后是写程序的人的想法。除非是真正的随机函数，不然，写程序的人肯定知道程序是如何运行和预想结果的。\n总结：不知道程序是怎么想的，只是因为你不愿意去阅读程序的日志和不愿意调试程序。如果有无穷的时间，你单步调试所有的代码，你肯定知道这个结果是怎么来的。\n如果整个机器学习慢慢进入黑盒的时代，则可以预测，瓶颈快到了。我们不知道机器到底是怎么学习的，我们就无法进行改进。就像我们不知道雨水的形成机理，我们光在地上求雨是徒劳的。\n随机森林和Dropout\n很多算法中，都可以看到随机的影子，RF的话，也就是多次随机抽取样本，训练模型，这些模型再进行平均操作。当然，这是根据中心极限理论得出的好方法。神经网络的Dropout也是如此，随机的将一些神经节点进行屏蔽。但是随机就意味着失控，意味着人工很难干预结果。包括梯度下降，是否能收敛到全局最优解，很大程度上也是有运气成分在里面的。初始值，学习率都是影响结果的因素。\n调参数和巨大模型\n现在很多机器学习的比赛，已经从技术比拼转向资源比拼了。\n神经网络的层数越来越长，越来越深，微软的神经网络是152层。\n阿里巴巴的机器学习模型，已经是3GB的庞然大物了。\n整个业界都从硬件和物理层面去获得精度的收益了。\n同时，超参数的选取，现在也都是经验论:\n神经网络的层数\n我们首先需要确定网络的层数和每层的节点数。关于第一个问题，实际上并没有什么理论化的方法，大家都是根据经验来拍，如果没有经验的话就随便拍一个。然后，你可以多试几个值，训练不同层数的神经网络，看看哪个效果最好就用哪个。嗯，现在你可能明白为什么说深度学习是个手艺活了，有些手艺很让人无语，而有些手艺还是很有技术含量的。\nK聚类的K取多少，自然语言处理的主题模型，主题数选择多少比较合适等等。都还没有，或者难以找到理论依据。\n机器学习还是数理统计\n机器学习的本质就是数理统计？答案可能没这么简单\nhttp://tech.sina.com.cn/roll/2017-03-27/doc-ifycspxp0038858.shtml\n如果从传统意义上的数据分析师的观点来说，这个问题的答案很简单，无非是下面这两点：\n机器学习本质上是一种算法，这种算法由数据分析习得，而且不依赖于规则导向的程序设计；\n统计建模则是以数据为基础，利用数学方程式来探究变量变化规律的一套规范化流程。\n有一种观点就是机器学习只是数理统计的一个华丽包装而已。\n在自然语言处理里面，原本是语言学家占主导的，然后慢慢的统计学家开始占上风，特别是在翻译领域，基本上都是靠强大的计算能力和巨大的模型在处理问题，也就是说从规则到统计的转变。\n如果说，机器学习的本质还是统计学的话，统计学，概率学这些东西，其实已经发展到尽头，很难再有什么革命性的突破了。是不是也意味着机器学习也走到尽头了呢？\n脑科学研究\n机器学习在很大程度上是对于大脑工作原理的仿生学。我觉得，机器学习的发展肯定和人类对于大脑研究的发展密不可分，神经网络就是一个例子。也有可能在多年之后，我们会发现大脑的工作原理和我们现在的认知完全不同，这样的话，当前的机器学习很有可能会被完全推翻，走向一条新的道路。"}
