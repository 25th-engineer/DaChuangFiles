{"content2":"Random Forest是加州大学伯克利分校的Breiman Leo和Adele Cutler于2001年发表的论文中提到的新的机器学习算法，可以用来做分类，聚类，回归，和生存分析，这里只简单介绍该算法在分类上的应用。\nRandom Forest（随机森林）算法是通过训练多个决策树，生成模型，然后综合利用多个决策树进行分类。\n随机森林算法只需要两个参数：构建的决策树的个数t，在决策树的每个节点进行分裂时需要考虑的输入特征的个数m。\n1. 单棵决策树的构建：\n（1）令N为训练样例的个数，则单棵决策树的输入样例的个数为N个从训练集中有放回的随机抽取N个训练样例。\n（2）令训练样例的输入特征的个数为M，切m远远小于M，则我们在每颗决策树的每个节点上进行分裂时，从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂。m在构建决策树的过程中不会改变。\n（3）每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。不需要剪枝。\n2. 随机森林的分类结果\n按照1生成t个决策树之后，对于每个新的测试样例，综合多个决策树的分类结果来作为随机森林的分类结果。\n（1）目标特征为数字类型：取t个决策树的平均值作为分类结果。\n（2）目标特征为类别类型：少数服从多数，取单棵树分类结果最多的那个类别作为整个随机森林的分类结果。\n3. 分类效果的评价\n在随机森林中，无需交叉验证来评价其分类的准确性，随机森林自带OOB（out-of-bag）错误估计：\nOOB：在构造单棵决策树时我们只是随机有放回的抽取了N个样例，所以可以用没有抽取到的样例来测试这棵决策树的分类准确性，这些样例大概占总样例数目的三分之一（作者这么说的，我还不知道理论上是如何出来的，但是可以自己做试验验证）。所以对于每个样例j，都有大约三分之一的决策树（记为SetT（j））在构造时没用到该样例，我们就用这些决策树来对这个样例进行分类。我们对于所有的训练样例j，用SetT（j）中的树组成的森林对其分类，然后看其分类结果和实际的类别是否相等，不相等的样例所占的比例就是OOB错误估计。OOB错误估计被证明是无偏的。\n参考文献：\n[1] Mahout Wiki-Random Forest\n[2] Leo Breiman 2001年的paper\n[3] Breiman自己对Random Forest的介绍"}
