{"content2":"https://blog.csdn.net/luyao_cxy/article/details/82383091\n转载：https://blog.csdn.net/qq_27297393/article/details/82284384\n机器学习\n一、人工智能、机器学习与深度学习\n人工智能\n机器学习\n经典机器学习\n基于神经网络的机器学习\n浅层学习\n深层学习(深度学习)\n强化学习\n迁移学习\n二、机器学习基本类型\n1.有监督学习\n有监督学习:根据已知的输入和输出，建立联系它们的模型，根据该模型对未知输出的输入进行判断。\n1)回归：以无限连续域的形式表示输出。\n2)分类：以有限离散域的形式表示输出。\n2.无监督学习\n无监督学习：在一组没有已知输出(标签)的输入中，根据数据的内部特征和联系，找到某种规则，进行族群的划分——聚类。\n3.半监督学习\n半监督学习：从一个相对有限的已知结构中利用有监督学习的方法，构建基本模型，通过对未知输入和已知输入的比对，判断其输出，扩展原有的已知领域。\n三、机器学习的基本过程\n数据采集->数据清洗->数据预处理->选择模型->训练模型->测试模型->使用模型\n原材料       去除杂质    准备               算法           规则        检验           业务生产\n四、数据预处理\n一列一特征\n|\nv\n一行一样本 -> x x x x x \\                  y y y\nx x x x x  | 样本矩阵  y y y\nx x x x x /                  y y y\n姓名    年龄    身高    体重    …\n张飞    22       1.75   60\n赵云    20       1.80   70\n…\n1.均值移除\n为了统一样本矩阵中不同特征的基准值和分散度，可以将各个特征的平均值调整为0，标准差调整为1，这个过程称为均值移除。\na b c\nm=(a+b+c)/3\na-m b-m c-m\nm’=(a-m+b-m+c-m)/3=(a+b+c)/3-3m/3=0\nA B C\ns=sqrt((A^2+B^2+C^2)/3)\nA/s B/s C/s\ns’=sqrt((A^2/s^2+B^2/s^2+C^2/s^2)/3)\n=sqrt((A^2+B^2+C^2)/s^2/3)\n=sqrt(s^2/s^2)\n=1\nsklearn.preprocessing.scale(原始样本矩阵)\n->均值移除后的样本矩阵\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/std.py\n2.范围缩放\n统一样本矩阵中不同特征的最大值和最小值范围。\nk x + b = y\nk min + b = min’\nk max + b = max’\nsklearn.preprocessing.MinMaxScaler(feature_range=期望最小和最大值)\n->范围缩放器\n范围缩放器.fit_transform(原始样本矩阵)\n->范围缩放后的样本矩阵\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/mms.py\n3.归一化\n归一化：为了用占比表示特征，用每个样本的特征值除以该样本的特征值绝对值之和，以使每个样本的特征值绝对值之和为1\nPython Java C/C++ PHP\n2016 30         50     40        20    30/140\n2017 20         30     20        10    20/80\nsklearn.preprocessing.normalize(原始样本矩阵,norm=’l1’)\n->归一化后的样本矩阵\nl1即L1范数，矢量中各元素绝对值之和。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/nor.py\n4.二值化\n二值化：用0和1来表示样本矩阵中相对于某个给定阈值高于或者低于它的元素。\nsklearn.preprocessing.Binarizer(threshold=阈值)\n->二值化器\n二值化器.transform(原始样本矩阵)\n->二值化后的样本矩阵\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/bin.py\n5.独热编码\n1        3        2\n7        5        4\n1        8        6\n7        3        9\n1:10  3:100 2:1000\n7:01  5:010 4: 0100\n8:001 6: 0010\n9: 0001\n1 0 1 0 0 1 0 0 0\n0 1 0 1 0 0 1 0 0\n1 0 0 0 1 0 0 1 0\n0 1 1 0 0 0 0 0 1\nsklearn.preprocessing.OneHotEncoder(sparse=是否使用压缩格式, dtype=元素类型)\n->独热编码器\n独热编码器.fit_transform(原始样本矩阵)\n->独热编码后的样本矩阵，同时构建编码表字典\n独热编码器.transform(原始样本矩阵)\n->独热编码后的样本矩阵，使用已有编码表字典\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/ohe.py\n6.标签编码\n标签编码：将字符形式的特征值映射为整数。\nsklearn.preprocessing.LabelEncoder()->标签编码器\n标签编码器.fit_transform(原始样本矩阵)\n->编码样本矩阵，构建编码字典\n标签编码器.transform(原始样本矩阵)\n->编码样本矩阵，使用编码字典\n标签编码器.inverse_transform(编码样本矩)\n->原始样本矩阵，使用编码字典\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/lab.py\n五、线性回归\nm个输入样本 -> m个输出标签\nx1 -> y1\nx2 -> y2\nx3 -> y3\n…\nxm -> ym\nxk + b -> y\n1.预测函数\n预测函数：联系输出和输入的数学函数。\ny=kx+b\n其中的k和b称为模型参数，根据已知输入样本和对应的输出标签来训练得出。\n2.均方误差\n均方误差：每一个已知输入样本所对应的实际输出标签和由模型预测出来的输出标签之间的误差平方的平均值。\nkx1+b=y1’\nkx2+b=y2’\nkx3+b=y3’\n…\nkxm+b=ym’\n(y1-y1’)^2+(y2-y2’)^2+(y3-y3’)^2+…+(ym-ym’)^2\n————————————————————-\nm\n3.成本函数\n成本函数：将均方误差看作是关于模型参数的函数，谓之成本函数，记做J(k,b)。\n线性回归问题的本质就是寻找能够使成本函数J(k,b)极小值的模型参数。\n4.梯度下降\nloss = J(k, b)\n5.接口\nsklearn.linear_model.LinearRegression()->线性回归器\n线性回归器.fit(输入样本, 输出标签)\n线性回归器.predict(输入样本)->预测输出标签\n6.复用\n通过pickle将内存中的模型对象写入磁盘文件，或从磁盘文件载入内存，以此保存训练好的模型，以备复用。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/line.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/save.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/load.py\n六、岭回归\nloss = J(k, b)+正则函数(样本权重)x正则强度\n惩罚系数\nsklearn.linear_model.Ridge(正则强度,fit_intercept=是否修正截距,max_iter=最大迭代次数)\n->岭回归器\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/rdg.py\n七、欠拟合与过拟合\n欠拟合：无论是训练数据还是测试数据，模型给出的预测值和真实值都存在较大的误差。\n过拟合：模型对于训练数据具有较高的精度，但对测试数据则表现极差。模型过于特殊，不够一般（泛化）。\n欠拟合<–模型复杂度–>过拟合\n八、多项式回归\nx -> y                  y = kx + b\nx x^2 -> y          y = k1x^2 + k2x + b\nx x^2 x^3 -> y  y = k1x^3 + k2x^2 + k3x + b\nsklearn.preprocessing.PolynomialFeatures(最高次数)\n->多项式特征扩展器\nsklearn.pipeline.make_pipe(多项式特征扩展器,线性回归器)\n->多项式回归器\nx–>多项式特征扩展器–x,x^2,x^3…–>线性回归器 –>k1,k2,k3–>\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/poly.py\n九、决策树\n相似的输入会有相似的输出。\n0 - 专科    0-普通     0-女   0-差               0-低\n1 - 本科    1-985     1-男   1-及格            1-中\n2 - 硕士    2-211               2-良好            2-高\n3 - 博士                             3-优异\n学历       院校     性别      成绩      ->      月薪\n1            0         1           2                  8000\n0            0         0           2                  7000\n3            1         1           3                  20000\n…\n1            1         0           1                  ?\n回归-平均\n分类-投票\n优化：\n1)结合业务优先选择有限的主要特征，划分子表，降低决策树的高度；\n2)根据香农定理计算根据每一个特征划分子表前后的信息熵差，选择熵减少量最大的特征，优先参与子表划分；\n3)集合算法：根据不同方法，构建多个决策树，利用它们的预测结果，按照取均（回归）或投票（分类）的方法，产生最终的预测结果。\nA.自助聚合：采用有放回抽样的规则，从m个样本中随机抽取n个样本，构建一棵决策树，重复以上过程b次，得到b棵决策树。利用每棵决策树的预测结果，根据平均或者投票得到最终预测结果。\nB.随机森林：在自助聚合算法的基础上更进一步，对特征也应用自助聚合，即每次训练时，不是用所有的特征构建树结构，而是随机选择部分特征参与构建，以此避免特殊特征对预测结果的影响。\nC.正向激励：初始化时，针对m个样本分配初始权重，然后根据这个带有权重的模型预测训练样本，针对那些预测错误的样本，提高其权重，再构建一棵决策树模型，重复以上过程，得到b棵决策树。。。\nsklearn.tree.DecisionTreeRegressor()\n->决策树回归器\nsklearn.ensemble.AdaBoostRegressor(元回归器,n_estimators=评估器数, radom_state=随机种子源)\n->正向激励回归器\nsklearn.ensemble.RandomForestRegressor(max_depth=最大树高, n_estimators=评估器数,min_samples_split=划分子表的最小样本数)\n->随机森林回归器\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/house.py\n决策树模型.feature_importances_: 特征重要性\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/fi.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/bike.py\n十、简单分类器\n输入    输出\n3   1      0\n2   5      1\n1   8      1\n6   4      0\n5   2      0\n3   5      1\n4   7      1\n4  -1     0\n7   5      ?->0\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/simple.py\n十一、逻辑分类\n1.预测函数\nx1 x2 -> y\n1\ny = ———–\n1 + e^-z\nz = k1x1 + k2x2 + b\n2.成本函数\n交叉熵误差\nJ(k1,k2,b) = sigma(-ylog(y’)-(1-y)log(1-y’))/m +\nm\n正则函数(||k1,k2,b||)x正则强度\nx x -> 0.9 1\nx x -> 0.2 0\nsklearn.linear_model.LogisticRegression(solver=’liblinear’, C=正则强度)\nA   B    C\n… -> A 1 0.9 0.1 0.3 A\n… -> B 0 0.3 0.6 0.4 B\n… -> C 0 0.1 0.2 0.6 C\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/mlog.c\n十二、朴素贝叶斯分类\nx x … x  -> 0\nx x … x  -> 1\nx x … x  -> 0\nx x … x  -> 0\nx x … x  -> 1\nx x … x  -> 2\nx x … x  -> 1\nx x … x  -> 0\nx x … x  -> 2\n…\n1 9 … x  -> 0 0.8\n-> 1 0.9 *\n-> 2 0.7\n1.贝叶斯定理\nP(A)P(B|A)\nP(A|B) = ————-\nP(B)\n2.朴素贝叶斯分类\n求X样本属于C类别的概率，即当观察到X样本出现时，其所属的类别为C的概率：\nP(C|X)=P(C)P(X|C)/P(X)\nP(C)P(X|C)=P(C,X)=P(C,x1,x2,…,xn)\n=P(x1,x2,…,xn,C)\n=P(x1|x2,…,xn,C)P(x2,x3,…,xn,C)\n=P(x1|x2,…,xn,C)P(x2|x3,…,xn,C)P(x3,x4,…,xn,C)\n=P(x1|x2,…,xn,C)P(x2|x3,…,xn,C)P(x3|x4,…,xn,C)…P(C)\n朴素：条件独立假设，即样本各个特征之间并无关联，不构成条件约束。\n=P(x1|C)P(x2|C)P(x3|C)…P(C)\nX样本属于C类别的概率，正比于C类别出现的的概率乘以C类别条件下X样本中每一个特征值出现的概率之乘积。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/nb.py\n3.划分训练集和测试集\nsklearn.model_selection.train_test_split(输入集合, 输出集合, test_size=测试集占比,random_state=随机种子源)\n->训练输入, 测试输入, 训练输出, 测试输出\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/split.py\n4.交叉验证\n1)查准率和召回率\n查准率：\n被正确识别为某类别的样本数/被识别为该类别的样本数\n正确性，对不对\n召回率：\n被正确识别为某类别的样本数/该类别的实际样本数\n完整性，全不全\nf1_score=2x查准率x召回率/(查准率+召回率)\n0<–>1\n差     好\nsklearn.model_selection.cross_val_score(分类器, 输入集合, 输出集合, cv=验证次数, scoring=验证指标名称)\n->验证指标值数组\nms.cross_val_score(model, x, y, cv=5,scoring=’f1_weighted’)\n->[0.6 0.8 0.4 0.7 0.6]\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/cv.py\n5.混淆矩阵\n以实际类别为行，以预测类别为列。\n0     1    2\n0  45    4    3\n1  11  56    2\n2    5    6  49\nsklearn.metrics.confusion_matrix(实际输出, 预测输出)\n->混淆矩阵\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/cm.py\n6.分类报告\nsklearn.metrics.classification_report(实际输出, 预测输出)\n->分类报告\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/cr.py\n十三、随机森林分类\n1.评估汽车档次\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/car.py\n2.验证曲线\nf1_score = f(模型对象超参数)\n验证曲线的峰值，寻找相对理想的超参数。\nmodel = se.RandomForestClassifier(max_depth=8, n_estimators=200, random_state=7)\n^^^^^^^^^^^^\nmodel = se.RandomForestClassifier(max_depth=8, random_state=7)\nsklearn.model_selection.validation_curve(model, x, y, ‘n_estimators’, [100, 200, 300, …],cv=5)\n->训练集得分矩阵, 测试集得分矩阵\n1    2    3    4    5\n100 -> 0.7 0.9 0.6 0.8 0.7\n200 ->\n300 ->\n…\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/vc.py\n3.学习曲线\nf1_score = f(训练集大小)\nsklearn.model_selection.learning_curve(model, x, y, 训练集大小数组, cv=5)\n->训练集大小数组, 训练集得分矩阵, 测试集得分矩阵\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/lc.py\n十四、支持向量机(SVM)\n1.分类边界\n同时满足四个条件：\nA.正确分类\nB.支持向量到分类边界的距离相等\nC.间距最大\nD.线性（直线，平面）\n2.升维变换\n对于在低维度空间中无法线性划分的样本，通过升维变换，在高纬度空间寻找最佳线性分类边界。\n核函数：用于对特征值进行升维变换的函数。\n多项式核函数\n径向基核函数\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/svm_line.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/svm_poly.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/svm_rbf.py\n3.权重均衡化（样本数量相差悬殊时）\n当不同类别的样本数量相差悬殊时，样本数较少的类别可能被支持向量机分类器忽略，为此可以将class_weight参数指定为balanced，通过调节不同类别样本的权重均衡化。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/svm_balanced.py\n4.置信概率\nsvm.SVC(…, probability=True, …)\n支持向量机分类器.predict_proba(输入样本)\n->置信概率矩阵\n类别1  类别2\n样本1 ->  0.99    0.01\n样本2 ->  0.02    0.98\n…\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/svm_prob.py\n5.最优超参数\nsklearn.model_selection.GridSearchCV(模型, 参数组合表, cv=交叉验证次数)\n->最优模型对象\n参数组合表：[{参数名: [取值列表]}, {}, …]\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/bhp.py\n事件预测\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/evt.py\n2   4\n3   6\n4   8\n…\ny = kx + b\n利用支持向量机回归模型预测交通流量\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/trf.py\n十五、聚类\n1.K均值\n根据事先给定的聚类数，为每个聚类随机分配中心点，计算所有样本与各个中心点的距离，将每个样本分配到与其距离最近的中心点所在的聚类中。计算每个聚类的几何中心，用该几何中心作为新的聚类中心，重新划分聚类。直到计算出的几何中心与上一次聚类使用的聚类中心重合或者足够接近为止。\n聚类数必须事先已知：从业务中找，选择最优化指标。\n聚类结果会受样本比例的影响。\n聚类中心的初始位置会影响聚类结果。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/km.py\n图像量化\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/quant.py\n2.均值漂移\n把训练样本看成服从某种概率密度函数规则的随机分布，在不断迭代的过程中试图寻找最佳的模式匹配，该密度函数的峰值点就是聚类的中心，为该密度函数所覆盖的样本即隶属于该聚类。\n不需要实现给定聚类数，算法本身具有发现聚类数量的能力。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/shift.py\n3.凝聚层次\n凝聚层次聚类，可以是自下而上(聚)，也可以是自上而下(分)的。在自下而上的算法中，每个训练样本都被看做是一个单独的集群，根据样本之间的相似度，将其不断合并，直到集群数达到事先指定的聚类数为止。在自上而下的算法中，所有训练样本被看做是一个大的聚类，根据样本之间的差异度，将其不断拆分，直到集群数达到指定的聚类数为止。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/agglo.py\n凝聚层次算法，不同于其它基于中心的聚类算法，用它对一些在空间上具有明显连续性，但彼此间的距离未必最近的样本，可以优先聚集，这样所构成的聚类划分就能够表现出较强的连续特性。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/spiral.py\n4.DBSCAN\n“朋友的朋友也是朋友”\n从任何一个训练样本出发，以一个事先给定的半径做圆，凡是不在此圆之外的样本都与圆心样本同类，再以这些同类样本为中心做圆重复以上过程，直到没有新的同类样本加入该聚类为止。以此类推，获得样本空间中的所有聚类。那些不属于任何聚类的样本，被称为偏离样本，位于聚类边缘的样本，则成为外周样本，其余统一称为核心样本。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/dbscan.py\n5.轮廓系数\n表示聚类划分内密外疏的程度。\n轮廓系数有一下两个指标构成：\na: 一个样本与其所在聚类其它样本的平均距离。\nb: 一个样本与其距离最近的另一个聚类中样本的平均距离。\n针对这个一个样本的轮廓系数：\ns=(b-a)/max(a, b)\n针对一个数据集，其轮廓系数就是其中所有样本的轮廓系数的平均值。轮廓系数的值介于[-1, 1]区间，1表示完美聚类，-1表示错误聚类，0表示聚类重叠。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/score.py\n十六、推荐引擎\n1.管线\n-输入->学习模型1-输出->学习模型2-输出->…\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/map.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/reduce.py\n输入\n|\nv\ndef 学习模型1 (参数):\n…\nreturn 返回值\n|\nv\n输出–+\n|\nv\ndef 学习模型2 (参数):\n…\nreturn 返回值\n|\nv\n输出\n所谓管线，其本质就是函数的级联调用，即用一个函数的返回值作为另一个函数的参数。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/cc1.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/cc2.py\n特征选择器-随机森林分类器\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/pipe.py\n2.寻找最近邻(FNN)\nsklearn.neighbors.NearestNeighbors(n_neighbors=邻居数, algorithm=算法)\n->FNN模型\n‘ball_tree’\nFNN模型.fit(已知样本集合)\nx x … x\nx x … x\n…\nx x … x  11\n…\nx x … x  23\n…\nx x … x  34\n…\nFNN模型.kneibhbors(待求样本集合)\n->距离矩阵, 近邻下标索引矩阵\nx x … x   11 23 34   0.5 0.3 0.1\nx x … x   22 10 15   0.4 0.1 0.2\n…            …               …\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/fnn.py\n3.KNN分类和回归\n遍历训练集中的所有样本，计算每个样本与待测样本的距离，并从中挑选出K的最近邻。根据与距离成反比的权重，做加权投票\n（分类）或平均（回归），得到待测样本的类别标签或预测数值。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/knnc.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/knnr.py\n4.欧氏(欧几里得)距离\n(x1,y1) <—-> (x2,y2)\n__________________________\nV (x1-x2)^2 + (y1-y2)^2\n________________________________________\nV (x1-x2)^2 + (y1-y2)^2 + (z1-z2)^2\n(a, b, c, …) <—-> (A, B, C, …)\n1\n欧氏距离得分=—————\n1 + 欧氏距离\n0 <-不相似-欧氏距离得分-相似-> 1\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/es.py\n用户1   用户2   用户3   …\n用户1         1      0.8       0.9   …\n用户2      0.8         1       0.7   …\n用户3\n…\n5.皮(尔逊)氏距离得分\n用两个样本的协方差([-1, 1])表示相似度。\nA    B    C\n1  5    1     3\n2  10  0     5\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/ps.py\n根据样本的相似程度排序\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/sim.py\n生成针对每个用户的推荐列表\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/rcm.py\n十七、文本分析\nimport nltk - 自然语言工具包\n1.分词\n从完整的文章或段落中，划分出若干独立的语义单元，如句或者词。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/tkn.py\n2.词干提取\n从单词中抽取主要成分，未必是合法的词汇。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/stm.py\n3.词型还原\n从名词或动词中抽取原型成分，依然保证其合法性。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/lmm.py\n4.词袋模型\nthe brown dog is running\nthe black dog is in the black room\nruning in the room is forbidden\n——————————————————————\nthe brown dog is running black in room forbddden\n1       1    1           1     1  1             0        0  0        0\n2       2    0           1     1  0             2        1   1       0\n3       1    0           0     1  1             0        1   1       1\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/bow.py\n5.词频\n单词在句子中出现的次数\n————————–\n句子的总单词数\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/tf.py\n6.词频逆文档频率(TF-IDF)\n总样本数\n词频 x 逆文档频率 = 词频 x ———————\n包含该单词的样本数\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/tfidf.py\n1)文本分类，核心问题预测\nxxxxxxxxxxxxxx -> 加解密\n…\nxxxxxxxxxxxxxx -> 摩托车\n…\nxxxxxxxxxxxxxx -> 棒球\n——————————\nxxxxxxxxxxxxxx -> ?\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/doc.py\n1 2 3 4 5 6\n1 2 0 0 1 1\n2 1 1 1 0 0\n…\n2)性别识别\nfeature\ndo   -> male\nmy  -> female\nam  -> …\nom\nne\nor\nne\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/gndr.py\n3)情感分析\nxxx    xxx    xxx   … xxx\nTrue  False False … True -> POSITIVE\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/sent.py\n4)主题词抽取\nimport gensim.models.ldamodel\nLDA, Latent Dirichlet Allocation\n5)隐含狄利克雷分布\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/topic.py\n十八、音频识别\n1.模拟音频和数字音频\n声带->机械振动->频率+响度=>声场强度=f(时间)\n|\nv\n耳朵<-播放器件机械振动<-电压/电流=f(时间)\n| A/D\nv\n.wav文件<-存储<-数字音频<-量化<-采样\n|\nv\n传输\n|\nv\n回放软件\n| D/A\nv\n电压/电流=f(时间)->播放器件机械振动->耳朵\n2.借助傅里叶变换提起频率特征\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/sig.py\n3.梅尔频率倒谱系数\n在频率特征的基础上结合语音的特点选择主要成分——MFCC，梅尔频率倒谱系数\n关键频率1 关键频率2 关键频率3 …\n时域区间1        30           40            20        … -> apple\n时域区间2        10           20            50        … -> apple\n时域区间3        40           30            60        … -> apple\n…\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/mfcc.py\n4.语音识别(HMM: 隐马尔科夫模型)\n音频样本->MFCC->HMM->标签\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/spch.py\n语音->数字音频流->MFCC->学习模型->文本->TFIDF->模型\n|<—————-语音识别————— >|                         |\n|                         v\n|                      语义\n|<自然语言处理>|\n|\n语音<-应答\n语言合成\n十九、图像识别\n1.机器视觉工具包：OpenCV-Python\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/basic.py\n2.边缘检测\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/edge.py\n3.通过均衡直方提升亮度\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/eq.py\n4.角点检测\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/corner.py\n5.star特征检测\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/star.py\n6.sift特征检测\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/sift.py\n7.特征(描述)矩阵\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/desc.py\n8.图像识别\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/obj.py\n二十、人脸识别\n1.视频捕捉\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/vidcap.py\n2.人脸定位 ———— 基于哈尔级联分类器的人脸定位\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/haar.py\n3.人脸识别 ———— 基于OpenCV的局部二值模式直方图(LBPH)模型\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/face.py\n二十一、成分分析(CA)\n1.主成分分析(PCA)\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/np.py\n2.sklearn的PCA接口\nN->K (K<N)\nimport sklearn.decomposition as dc\nmodel = dc.PCA(n_components=K)\npca_x = model.fit_transform(x)\nmodel.fit(x) # U_reduce\npca_x = model.transform(x) # Z\nipca_x = model.inverse_transform(pca_x) # X_approx\nmodel.explained_variance_ratio_.sum()->还原率[0, 1]\n0<— 还原率 —>1\n误差越大   误差越小\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/sk.py\n3.主成分分析在人脸识别中的应用\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/face1.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/face2.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/face3.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/face4.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/face5.py\n4.核主成分分析(KPCA)\n对于在n维空间不可线性分割的样本，通过核函数升维到更高维度空间，再通过主成分分析，在投射误差最小的前提下，降到n维空间，即寻找可线性分割的投影面，达到简化分类模型的目的\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/kpca.py\n二十二、神经网络\n1.神经元\n权重：过滤输入信息，针对不同的数据提高或者降低其作用和影响。[w1, w2, …, wn]\n偏置：没有任何输入时的输出。b\n激活函数：将线性的连续的输入转换为非线性的离散的输出。sigmoid/tanh/relu…\n2.层\n每一层可以由1到多个神经元组成，层中的神经元接收上一层的输出，并为下一层提供输入。数据只能在相邻层之间传递，不能跨层传输。\n3.多层神经网络\n输入层：接收输入样本的各个特征，传递给第一个隐藏层，本身不对数据进行运算。\n隐藏层：0到多个，通过权重、偏置和激活函数，对所接收到来自上一层的数据进行运算：O = f(I x W + b)\n输出层：功能和隐藏相同，将计算结果作为输出的每一个特征。\n若隐藏层的层数多余一层，则可以被称为深度神经网络，通常使用的深度神经网络，其隐藏层数可以多达数十甚至上百层，基于这样结构的学习模型被称为深度学习。\n4.最简单的神经网络：感知器\n只由输入层和输出层组成的神经网络。\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/neuron.py\n5.单层多输出神经网络\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/mono.py\n6.深度(两个隐藏层)神经网络\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/deep.py\n7.OCR字符识别\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/ocrdb.py\n参考代码：https://github.com/hilqiqi0/AI/tree/master/6.ML/code/Machine%20Learning/ocr.py\no m a n d i  g\na           0 0  1 0 0 0 0\ni            0 0  0 0 0 1 0\n二十三、推荐书目\n入门：\nPython数据分析基础教程Numpy学习指南，张驭宇译，人民邮电出版社\nPython神经网络编程，林赐译，人邮\n白话深度学习与TensorFlow，高扬著，机械\n基础：\nscikit-learn机器学习：常用算法原理和编程实践，黄永昌主编，机械\n机器学习算法原理与编程实践，郑捷著，电子\n进阶：\n深度学习，张鹏主编，电子\nTensorFlow机器学习项目实战，姚鹏鹏译，人邮\n休闲：\n数学之美，吴军，人邮\n终极算法，黄芳萍译，中信\n深度学习，伊恩古德弗洛著，人邮\n二十四、附录：目录\n人工智能（Machine Learning）—— 目录汇总"}
