{"content2":"主成分分析（principal component analysis）是一种常见的数据降维方法，其目的是在“信息”损失较小的前提下，将高维的数据转换到低维，从而减小计算量。\nPCA的本质就是找一些投影方向，使得数据在这些投影方向上的方差最大，而且这些投影方向是相互正交的。这其实就是找新的正交基的过程，计算原始数据在这些正交基上投影的方差，方差越大，就说明在对应正交基上包含了更多的信息量。后面会证明，原始数据协方差矩阵的特征值越大，对应的方差越大，在对应的特征向量上投影的信息量就越大。反之，如果特征值较小，则说明数据在这些特征向量上投影的信息量很小，可以将小特征值对应方向的数据删除，从而达到了降维的目的。\nPCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。\n因此，关键点就在于：如何找到新的投影方向使得原始数据的“信息量”损失最少？\n1.样本“信息量”的衡量\n样本的“信息量”指的是样本在特征方向上投影的方差。方差越大，则样本在该特征上的差异就越大，因此该特征就越重要。以《机器学习实战》上的图说明，在分类问题里，样本的方差越大，越容易将不同类别的样本区分开。\n图中共有3个类别的数据，很显然，方差越大，越容易分开不同类别的点。样本在X轴上的投影方差较大，在Y轴的投影方差较小。方差最大的方向应该是中间斜向上的方向（图中红线方向）。如果将样本按照中间斜向上的方向进行映射，则只要一维的数据就可以对其进行分类，相比二维的原数据，就相当降了一维。\n在原始数据更多维的情况下，先得到一个数据变换后方差最大的方向，然后选择与第一个方向正交的方向，该方向是方差次大的方向，如此下去，直到变换出与原特征个数相同的新特征或者变换出前N个特征（在这前N个特征包含了数据的绝大部分信息），简而言之，PCA是一个降维的过程，将数据映射到新的特征，新特征是原始特征的线性组合。\n2.计算过程（因为插入公式比较麻烦，就直接采用截图的方式）\n3.python实现\n#coding=utf-8 from numpy import * '''通过方差的百分比来计算将数据降到多少维是比较合适的， 函数传入的参数是特征值和百分比percentage，返回需要降到的维度数num''' def eigValPct(eigVals,percentage): sortArray=sort(eigVals) #使用numpy中的sort()对特征值按照从小到大排序 sortArray=sortArray[-1::-1] #特征值从大到小排序 arraySum=sum(sortArray) #数据全部的方差arraySum tempSum=0 num=0 for i in sortArray: tempSum+=i num+=1 if tempsum>=arraySum*percentage: return num '''pca函数有两个参数，其中dataMat是已经转换成矩阵matrix形式的数据集，列表示特征； 其中的percentage表示取前多少个特征需要达到的方差占比，默认为0.9''' def pca(dataMat,percentage=0.9): meanVals=mean(dataMat,axis=0) #对每一列求平均值，因为协方差的计算中需要减去均值 meanRemoved=dataMat-meanVals covMat=cov(meanRemoved,rowvar=0) #cov()计算方差 eigVals,eigVects=linalg.eig(mat(covMat)) #利用numpy中寻找特征值和特征向量的模块linalg中的eig()方法 k=eigValPct(eigVals,percentage) #要达到方差的百分比percentage，需要前k个向量 eigValInd=argsort(eigVals) #对特征值eigVals从小到大排序 eigValInd=eigValInd[:-(k+1):-1] #从排好序的特征值，从后往前取k个，这样就实现了特征值的从大到小排列 redEigVects=eigVects[:,eigValInd] #返回排序后特征值对应的特征向量redEigVects（主成分） lowDDataMat=meanRemoved*redEigVects #将原始数据投影到主成分上得到新的低维数据lowDDataMat reconMat=(lowDDataMat*redEigVects.T)+meanVals #得到重构数据reconMat return lowDDataMat,reconMat\nReference：\n1. Peter Harrington，《机器学习实战》，人民邮电出版社，2013\n2. http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html (其中有PCA的计算实例)\n3. 张学工，《模式识别》（第三版），清华大学出版社，2010"}
