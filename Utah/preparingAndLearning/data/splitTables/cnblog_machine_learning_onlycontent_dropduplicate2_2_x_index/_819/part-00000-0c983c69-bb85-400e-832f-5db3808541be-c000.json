{"content2":"转自：http://www.cnblogs.com/zhizhan/p/4432943.html\n决策树\n一、  决策树优点\n1、决策树易于理解和解释，可以可视化分析，容易提取出规则。\n2、可以同时处理标称型和数值型数据。\n3、测试数据集时，运行速度比较快。\n4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。\n二、决策树缺点\n1、对缺失数据处理比较困难。\n2、容易出现过拟合问题。\n3、忽略数据集中属性的相互关联。\n4、ID3算法计算信息增益时结果偏向数值比较多的特征。\n三、改进措施\n1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。\n2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题\n三、应用领域\n企业管理实践，企业投资决策，由于决策树很好的分析能力，在决策过程应用较多。\nKNN算法\n一、KNN算法的优点\n1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练\n2、KNN理论简单，容易实现\n二、KNN算法的缺点\n1、对于样本容量大的数据集计算量比较大。\n2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。\n3、KNN每一次分类都会重新进行一次全局运算。\n4、k值大小的选择。\n三、KNN算法应用领域\n文本分类、模式识别、聚类分析，多分类领域\n支持向量机（SVM）\n一、  SVM优点\n1、解决小样本下机器学习问题。\n2、解决非线性问题。\n3、无局部极小值问题。（相对于神经网络等算法）\n4、可以很好的处理高维数据集。\n5、泛化能力比较强。\n二、SVM缺点\n1、对于核函数的高维映射解释力不强，尤其是径向基函数。\n2、对缺失数据敏感。\n三、SVM应用领域\n文本分类、图像识别、主要二分类领域\nAdaBoost算法\n一、  AdaBoost算法优点\n1、很好的利用了弱分类器进行级联。\n2、可以将不同的分类算法作为弱分类器。\n3、AdaBoost具有很高的精度。\n4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。\n二、Adaboost算法缺点\n1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。\n2、数据不平衡导致分类精度下降。\n3、训练比较耗时，每次重新选择当前分类器最好切分点。\n三、AdaBoost应用领域\n模式识别、计算机视觉领域，用于二分类和多分类场景\n朴素贝叶斯算法\n一、  朴素贝叶斯算法优点\n1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。\n2、支持增量式运算。即可以实时的对新增的样本进行训练。\n3、朴素贝叶斯对结果解释容易理解。\n二、朴素贝叶斯缺点\n1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。\n三、朴素贝叶斯应用领域\n文本分类、欺诈检测中使用较多\nLogistic回归算法\n一、logistic回归优点\n1、计算代价不高，易于理解和实现\n二、logistic回归缺点\n1、容易产生欠拟合。\n2、分类精度不高。\n三、logistic回归应用领域\n用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。\nLogistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。\n人工神经网络\n一、  神经网络优点\n1、分类准确度高，学习能力极强。\n2、对噪声数据鲁棒性和容错性较强。\n3、有联想能力，能逼近任意非线性关系。\n二、神经网络缺点\n1、神经网络参数较多，权值和阈值。\n2、黑盒过程，不能观察中间结果。\n3、学习过程比较长，有可能陷入局部极小值。\n三、人工神经网络应用领域\n目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。\n===============================================================================================\n原文：http://suanfazu.com/t/qian-tan-wo-dui-ji-qi-xue-xi-de-dian-li-jie/305\n机器学习方法非常多，也很成熟。下面我挑几个说。\n首先是SVM。因为我做的文本处理比较多，所以比较熟悉SVM。SVM也叫支持向量机，其把数据映射到多维空间中以点的形式存在，然后找到能够分类的最优超平面，最后根据这个平面来分类。SVM能对训练集之外的数据做很好的预测、泛化错误率低、计算开销小、结果易解释，但其对参数调节和核函数的参数过于敏感。个人感觉SVM是二分类的最好的方法，但也仅限于二分类。如果要使用SVM进行多分类，也是在向量空间中实现多次二分类。\nSVM有一个核心函数SMO，也就是序列最小最优化算法。SMO基本是最快的二次规划优化算法，其核心就是找到最优参数α，计算超平面后进行分类。SMO方法可以将大优化问题分解为多个小优化问题求解，大大简化求解过程。某些条件下，把原始的约束问题通过拉格朗日函数转化为无约束问题，如果原始问题求解棘手，在满足KKT的条件下用求解对偶问题来代替求解原始问题，使得问题求解更加容易。 SVM还有一个重要函数是核函数。核函数的主要作用是将数据从低位空间映射到高维空间。详细的内容我就不说了，因为内容实在太多了。总之，核函数可以很好的解决数据的非线性问题，而无需考虑映射过程。\n第二个是KNN。KNN将测试集的数据特征与训练集的数据进行特征比较，然后算法提取样本集中特征最近邻数据的分类标签，即KNN算法采用测量不同特征值之间的距离的方法进行分类。KNN的思路很简单，就是计算测试数据与类别中心的距离。KNN具有精度高、对异常值不敏感、无数据输入假定、简单有效的特点，但其缺点也很明显，计算复杂度太高。要分类一个数据，却要计算所有数据，这在大数据的环境下是很可怕的事情。而且，当类别存在范围重叠时，KNN分类的精度也不太高。所以，KNN比较适合小量数据且精度要求不高的数据。\nKNN有两个影响分类结果较大的函数，一个是数据归一化，一个是距离计算。如果数据不进行归一化，当多个特征的值域差别很大的时候，最终结果就会受到较大影响；第二个是距离计算。这应该算是KNN的核心了。目前用的最多的距离计算公式是欧几里得距离，也就是我们常用的向量距离计算方法。\n个人感觉，KNN最大的作用是可以随时间序列计算，即样本不能一次性获取只能随着时间一个一个得到的时候，KNN能发挥它的价值。至于其他的特点，它能做的，很多方法都能做；其他能做的它却做不了。\n第三个就是Naive Bayes了。Naive Bayes简称NB（牛X），为啥它牛X呢，因为它是基于Bayes概率的一种分类方法。贝叶斯方法可以追溯到几百年前，具有深厚的概率学基础，可信度非常高。Naive Baye中文名叫朴素贝叶斯，为啥叫“朴素”呢？因为其基于一个给定假设：给定目标值时属性之间相互条件独立。比如我说“我喜欢你”，该假设就会假定“我”、“喜欢”、“你”三者之间毫无关联。仔细想想，这几乎是不可能的。马克思告诉我们：事物之间是有联系的。同一个事物的属性之间就更有联系了。所以，单纯的使用NB算法效率并不高，大都是对该方法进行了一定的改进，以便适应数据的需求。\nNB算法在文本分类中用的非常多，因为文本类别主要取决于关键词，基于词频的文本分类正中NB的下怀。但由于前面提到的假设，该方法对中文的分类效果不好，因为中文顾左右而言他的情况太多，但对直来直去的老美的语言，效果良好。至于核心算法嘛，主要思想全在贝叶斯里面了，没啥可说的。\n第四个是回归。回归有很多，Logistic回归啊、岭回归啊什么的，根据不同的需求可以分出很多种。这里我主要说说Logistic回归。为啥呢？因为Logistic回归主要是用来分类的，而非预测。回归就是将一些数据点用一条直线对这些点进行拟合。而Logistic回归是指根据现有数据对分类边界线建立回归公式，以此进行分类。该方法计算代价不高，易于理解和实现，而且大部分时间用于训练，训练完成后分类很快；但它容易欠拟合，分类精度也不高。主要原因就是Logistic主要是线性拟合，但现实中很多事物都不满足线性的。即便有二次拟合、三次拟合等曲线拟合，也只能满足小部分数据，而无法适应绝大多数数据，所以回归方法本身就具有局限性。但为什么还要在这里提出来呢？因为回归方法虽然大多数都不合适，但一旦合适，效果就非常好。\nLogistic回归其实是基于一种曲线的，“线”这种连续的表示方法有一个很大的问题，就是在表示跳变数据时会产生“阶跃”的现象，说白了就是很难表示数据的突然转折。所以用Logistic回归必须使用一个称为“海维塞德阶跃函数”的Sigmoid函数来表示跳变。通过Sigmoid就可以得到分类的结果。\n为了优化Logistic回归参数，需要使用一种“梯度上升法”的优化方法。该方法的核心是，只要沿着函数的梯度方向搜寻，就可以找到函数的最佳参数。但该方法在每次更新回归系数时都需要遍历整个数据集，对于大数据效果还不理想。所以还需要一个“随机梯度上升算法”对其进行改进。该方法一次仅用一个样本点来更新回归系数，所以效率要高得多。\n第五个是决策树。据我了解，决策树是最简单，也是曾经最常用的分类方法了。决策树基于树理论实现数据分类，个人感觉就是数据结构中的B+树。决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。决策树计算复杂度不高、输出结果易于理解、对中间值缺失不敏感、可以处理不相关特征数据。其比KNN好的是可以了解数据的内在含义。但其缺点是容易产生过度匹配的问题，且构建很耗时。决策树还有一个问题就是，如果不绘制树结构，分类细节很难明白。所以，生成决策树，然后再绘制决策树，最后再分类，才能更好的了解数据的分类过程。\n决策树的核心树的分裂。到底该选择什么来决定树的分叉是决策树构建的基础。最好的方法是利用信息熵实现。熵这个概念很头疼，很容易让人迷糊，简单来说就是信息的复杂程度。信息越多，熵越高。所以决策树的核心是通过计算信息熵划分数据集。\n我还得说一个比较特殊的分类方法：AdaBoost。AdaBoost是boosting算法的代表分类器。boosting基于元算法（集成算法）。即考虑其他方法的结果作为参考意见，也就是对其他算法进行组合的一种方式。说白了，就是在一个数据集上的随机数据使用一个分类训练多次，每次对分类正确的数据赋权值较小，同时增大分类错误的数据的权重，如此反复迭代，直到达到所需的要求。AdaBoost泛化错误率低、易编码、可以应用在大部分分类器上、无参数调整，但对离群点敏感。该方法其实并不是一个独立的方法，而是必须基于元方法进行效率提升。个人认为，所谓的“AdaBoost是最好的分类方法”这句话是错误的，应该是“AdaBoost是比较好的优化方法”才对。\n好了，说了这么多了，我有点晕了，还有一些方法过几天再写。总的来说，机器学习方法是利用现有数据作为经验让机器学习，以便指导以后再次碰到的决策。目前来说，对于大数据分类，还是要借助分布式处理技术和云技术才有可能完成，但一旦训练成功，分类的效率还是很可观的，这就好比人年龄越大看待问题越精准的道理是一样的。这八个月里，从最初的理解到一步步实现；从需求的逻辑推断到实现的方法选择，每天都是辛苦的，但每天也都是紧张刺激的。我每天都在想学了这个以后可以实现什么样的分类，其实想想都是让人兴奋的。当初，我逃避做程序员，主要原因就是我不喜欢做已经知道结果的事情，因为那样的工作没有什么期盼感；而现在，我可以利用数据分析得到我想象不到的事情，这不仅满足了我的好奇感，也让我能在工作中乐在其中。也许，我距离社会的技术需求还有很远的距离，但我对自己充满信心，因为，我不感到枯燥，不感到彷徨，虽然有些力不从心，但态度坚定。\n===================================================\nhttp://blog.csdn.net/vola9527/article/details/43347747\n简述机器学习十大算法的每个算法的核心思想、工作原理、适用情况及优缺点等。\n1）C4.5算法：\nID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。\nC4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：\n1）用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；\n2）在树构造过程中进行剪枝\n3）能处理非离散的数据\n4）能处理不完整的数据\nC4.5算法优点：产生的分类规则易于理解，准确率较高。\n缺点：\n1)在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。\n2)C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。\n2）K means 算法：\n是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k< n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。\n其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk\n优点：算法速度很快\n缺点是，分组的数目k是一个输入参数，不合适的k可能返回较差的结果。\n3）朴素贝叶斯算法：\n朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。算法的基础是概率问题，分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。朴素贝叶斯假设是约束性很强的假设，假设特征条件独立，但朴素贝叶斯算法简单，快速，具有较小的出错率。\n在朴素贝叶斯的应用中，主要研究了电子邮件过滤以及文本分类研究。\n4)K最近邻分类算法（KNN）\n分类思想比较简单，从训练样本中找出K个与其最相近的样本，然后看这k个样本中哪个类别的样本多，则待判定的值（或说抽样）就属于这个类别。\n缺点：\n1）K值需要预先设定，而不能自适应\n2）当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。\n该算法在分类时有个重要的不足是，当样本不平衡时，即：一个类的样本容量很大，而其他类样本数量很小时，很有可能导致当输入一个未知样本时，该样本的K个邻居中大数量类的样本占多数。 但是这类样本并不接近目标样本，而数量小的这类样本很靠近目标样本。这个时候，我们有理由认为该位置样本属于数量小的样本所属的一类，但是，KNN却不关心这个问题，它只关心哪类样本的数量最多，而不去把距离远近考虑在内，因此，我们可以采用权值的方法来改进。和该样本距离小的邻居权值大，和该样本距离大的邻居权值则相对较小，由此，将距离远近的因素也考虑在内，避免因一个样本过大导致误判的情况\n该算法适用于对样本容量比较大的类域进行自动分类。\n5)EM最大期望算法\nEM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。\nEM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。\n6）PageRank算法\n是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）\n优点：\n完全独立于查询，只依赖于网页链接结构，可以离线计算。\n缺点：\n1）PageRank算法忽略了网页搜索的时效性。\n2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。\n7)AdaBoost\nAdaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。\n整个过程如下所示：\n1. 先通过对N个训练样本的学习得到第一个弱分类器；\n2. 将分错的样本和其他的新数据一起构成一个新的N个的训练样本，通过对这个样本的学习得到第二个弱分类器；\n3. 将和都分错了的样本加上其他的新样本构成另一个新的N个的训练样本，通过对这个样本的学习得到第三个弱分类器；\n4. 如此反复，最终得到经过提升的强分类器。\n目前AdaBoost算法广泛的应用于人脸检测、目标识别等领域。\n8）Apriori算法\nApriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。\nApriori算法分为两个阶段：\n1）寻找频繁项集\n2）由频繁项集找关联规则\n算法缺点：\n1） 在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；\n2） 每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。\n9）SVM支持向量机\n支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。\n支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。\nSVM在解决小样本、非线性及高维模式识别问题中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。\n10）CART分类与回归树\n是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数\n据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。\n优点\n1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。\n2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。 转自：http://www.cnblogs.com/zhizhan/p/4432943.html\n决策树\n一、  决策树优点\n1、决策树易于理解和解释，可以可视化分析，容易提取出规则。\n2、可以同时处理标称型和数值型数据。\n3、测试数据集时，运行速度比较快。\n4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。\n二、决策树缺点\n1、对缺失数据处理比较困难。\n2、容易出现过拟合问题。\n3、忽略数据集中属性的相互关联。\n4、ID3算法计算信息增益时结果偏向数值比较多的特征。\n三、改进措施\n1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。\n2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题\n三、应用领域\n企业管理实践，企业投资决策，由于决策树很好的分析能力，在决策过程应用较多。\nKNN算法\n一、KNN算法的优点\n1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练\n2、KNN理论简单，容易实现\n二、KNN算法的缺点\n1、对于样本容量大的数据集计算量比较大。\n2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。\n3、KNN每一次分类都会重新进行一次全局运算。\n4、k值大小的选择。\n三、KNN算法应用领域\n文本分类、模式识别、聚类分析，多分类领域\n支持向量机（SVM）\n一、  SVM优点\n1、解决小样本下机器学习问题。\n2、解决非线性问题。\n3、无局部极小值问题。（相对于神经网络等算法）\n4、可以很好的处理高维数据集。\n5、泛化能力比较强。\n二、SVM缺点\n1、对于核函数的高维映射解释力不强，尤其是径向基函数。\n2、对缺失数据敏感。\n三、SVM应用领域\n文本分类、图像识别、主要二分类领域\nAdaBoost算法\n一、  AdaBoost算法优点\n1、很好的利用了弱分类器进行级联。\n2、可以将不同的分类算法作为弱分类器。\n3、AdaBoost具有很高的精度。\n4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。\n二、Adaboost算法缺点\n1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。\n2、数据不平衡导致分类精度下降。\n3、训练比较耗时，每次重新选择当前分类器最好切分点。\n三、AdaBoost应用领域\n模式识别、计算机视觉领域，用于二分类和多分类场景\n朴素贝叶斯算法\n一、  朴素贝叶斯算法优点\n1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。\n2、支持增量式运算。即可以实时的对新增的样本进行训练。\n3、朴素贝叶斯对结果解释容易理解。\n二、朴素贝叶斯缺点\n1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。\n三、朴素贝叶斯应用领域\n文本分类、欺诈检测中使用较多\nLogistic回归算法\n一、logistic回归优点\n1、计算代价不高，易于理解和实现\n二、logistic回归缺点\n1、容易产生欠拟合。\n2、分类精度不高。\n三、logistic回归应用领域\n用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。\nLogistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。\n人工神经网络\n一、  神经网络优点\n1、分类准确度高，学习能力极强。\n2、对噪声数据鲁棒性和容错性较强。\n3、有联想能力，能逼近任意非线性关系。\n二、神经网络缺点\n1、神经网络参数较多，权值和阈值。\n2、黑盒过程，不能观察中间结果。\n3、学习过程比较长，有可能陷入局部极小值。\n三、人工神经网络应用领域\n目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。\n===============================================================================================\n原文：http://suanfazu.com/t/qian-tan-wo-dui-ji-qi-xue-xi-de-dian-li-jie/305\n机器学习方法非常多，也很成熟。下面我挑几个说。\n首先是SVM。因为我做的文本处理比较多，所以比较熟悉SVM。SVM也叫支持向量机，其把数据映射到多维空间中以点的形式存在，然后找到能够分类的最优超平面，最后根据这个平面来分类。SVM能对训练集之外的数据做很好的预测、泛化错误率低、计算开销小、结果易解释，但其对参数调节和核函数的参数过于敏感。个人感觉SVM是二分类的最好的方法，但也仅限于二分类。如果要使用SVM进行多分类，也是在向量空间中实现多次二分类。\nSVM有一个核心函数SMO，也就是序列最小最优化算法。SMO基本是最快的二次规划优化算法，其核心就是找到最优参数α，计算超平面后进行分类。SMO方法可以将大优化问题分解为多个小优化问题求解，大大简化求解过程。某些条件下，把原始的约束问题通过拉格朗日函数转化为无约束问题，如果原始问题求解棘手，在满足KKT的条件下用求解对偶问题来代替求解原始问题，使得问题求解更加容易。 SVM还有一个重要函数是核函数。核函数的主要作用是将数据从低位空间映射到高维空间。详细的内容我就不说了，因为内容实在太多了。总之，核函数可以很好的解决数据的非线性问题，而无需考虑映射过程。\n第二个是KNN。KNN将测试集的数据特征与训练集的数据进行特征比较，然后算法提取样本集中特征最近邻数据的分类标签，即KNN算法采用测量不同特征值之间的距离的方法进行分类。KNN的思路很简单，就是计算测试数据与类别中心的距离。KNN具有精度高、对异常值不敏感、无数据输入假定、简单有效的特点，但其缺点也很明显，计算复杂度太高。要分类一个数据，却要计算所有数据，这在大数据的环境下是很可怕的事情。而且，当类别存在范围重叠时，KNN分类的精度也不太高。所以，KNN比较适合小量数据且精度要求不高的数据。\nKNN有两个影响分类结果较大的函数，一个是数据归一化，一个是距离计算。如果数据不进行归一化，当多个特征的值域差别很大的时候，最终结果就会受到较大影响；第二个是距离计算。这应该算是KNN的核心了。目前用的最多的距离计算公式是欧几里得距离，也就是我们常用的向量距离计算方法。\n个人感觉，KNN最大的作用是可以随时间序列计算，即样本不能一次性获取只能随着时间一个一个得到的时候，KNN能发挥它的价值。至于其他的特点，它能做的，很多方法都能做；其他能做的它却做不了。\n第三个就是Naive Bayes了。Naive Bayes简称NB（牛X），为啥它牛X呢，因为它是基于Bayes概率的一种分类方法。贝叶斯方法可以追溯到几百年前，具有深厚的概率学基础，可信度非常高。Naive Baye中文名叫朴素贝叶斯，为啥叫“朴素”呢？因为其基于一个给定假设：给定目标值时属性之间相互条件独立。比如我说“我喜欢你”，该假设就会假定“我”、“喜欢”、“你”三者之间毫无关联。仔细想想，这几乎是不可能的。马克思告诉我们：事物之间是有联系的。同一个事物的属性之间就更有联系了。所以，单纯的使用NB算法效率并不高，大都是对该方法进行了一定的改进，以便适应数据的需求。\nNB算法在文本分类中用的非常多，因为文本类别主要取决于关键词，基于词频的文本分类正中NB的下怀。但由于前面提到的假设，该方法对中文的分类效果不好，因为中文顾左右而言他的情况太多，但对直来直去的老美的语言，效果良好。至于核心算法嘛，主要思想全在贝叶斯里面了，没啥可说的。\n第四个是回归。回归有很多，Logistic回归啊、岭回归啊什么的，根据不同的需求可以分出很多种。这里我主要说说Logistic回归。为啥呢？因为Logistic回归主要是用来分类的，而非预测。回归就是将一些数据点用一条直线对这些点进行拟合。而Logistic回归是指根据现有数据对分类边界线建立回归公式，以此进行分类。该方法计算代价不高，易于理解和实现，而且大部分时间用于训练，训练完成后分类很快；但它容易欠拟合，分类精度也不高。主要原因就是Logistic主要是线性拟合，但现实中很多事物都不满足线性的。即便有二次拟合、三次拟合等曲线拟合，也只能满足小部分数据，而无法适应绝大多数数据，所以回归方法本身就具有局限性。但为什么还要在这里提出来呢？因为回归方法虽然大多数都不合适，但一旦合适，效果就非常好。\nLogistic回归其实是基于一种曲线的，“线”这种连续的表示方法有一个很大的问题，就是在表示跳变数据时会产生“阶跃”的现象，说白了就是很难表示数据的突然转折。所以用Logistic回归必须使用一个称为“海维塞德阶跃函数”的Sigmoid函数来表示跳变。通过Sigmoid就可以得到分类的结果。\n为了优化Logistic回归参数，需要使用一种“梯度上升法”的优化方法。该方法的核心是，只要沿着函数的梯度方向搜寻，就可以找到函数的最佳参数。但该方法在每次更新回归系数时都需要遍历整个数据集，对于大数据效果还不理想。所以还需要一个“随机梯度上升算法”对其进行改进。该方法一次仅用一个样本点来更新回归系数，所以效率要高得多。\n第五个是决策树。据我了解，决策树是最简单，也是曾经最常用的分类方法了。决策树基于树理论实现数据分类，个人感觉就是数据结构中的B+树。决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。决策树计算复杂度不高、输出结果易于理解、对中间值缺失不敏感、可以处理不相关特征数据。其比KNN好的是可以了解数据的内在含义。但其缺点是容易产生过度匹配的问题，且构建很耗时。决策树还有一个问题就是，如果不绘制树结构，分类细节很难明白。所以，生成决策树，然后再绘制决策树，最后再分类，才能更好的了解数据的分类过程。\n决策树的核心树的分裂。到底该选择什么来决定树的分叉是决策树构建的基础。最好的方法是利用信息熵实现。熵这个概念很头疼，很容易让人迷糊，简单来说就是信息的复杂程度。信息越多，熵越高。所以决策树的核心是通过计算信息熵划分数据集。\n我还得说一个比较特殊的分类方法：AdaBoost。AdaBoost是boosting算法的代表分类器。boosting基于元算法（集成算法）。即考虑其他方法的结果作为参考意见，也就是对其他算法进行组合的一种方式。说白了，就是在一个数据集上的随机数据使用一个分类训练多次，每次对分类正确的数据赋权值较小，同时增大分类错误的数据的权重，如此反复迭代，直到达到所需的要求。AdaBoost泛化错误率低、易编码、可以应用在大部分分类器上、无参数调整，但对离群点敏感。该方法其实并不是一个独立的方法，而是必须基于元方法进行效率提升。个人认为，所谓的“AdaBoost是最好的分类方法”这句话是错误的，应该是“AdaBoost是比较好的优化方法”才对。\n好了，说了这么多了，我有点晕了，还有一些方法过几天再写。总的来说，机器学习方法是利用现有数据作为经验让机器学习，以便指导以后再次碰到的决策。目前来说，对于大数据分类，还是要借助分布式处理技术和云技术才有可能完成，但一旦训练成功，分类的效率还是很可观的，这就好比人年龄越大看待问题越精准的道理是一样的。这八个月里，从最初的理解到一步步实现；从需求的逻辑推断到实现的方法选择，每天都是辛苦的，但每天也都是紧张刺激的。我每天都在想学了这个以后可以实现什么样的分类，其实想想都是让人兴奋的。当初，我逃避做程序员，主要原因就是我不喜欢做已经知道结果的事情，因为那样的工作没有什么期盼感；而现在，我可以利用数据分析得到我想象不到的事情，这不仅满足了我的好奇感，也让我能在工作中乐在其中。也许，我距离社会的技术需求还有很远的距离，但我对自己充满信心，因为，我不感到枯燥，不感到彷徨，虽然有些力不从心，但态度坚定。\n===================================================\nhttp://blog.csdn.net/vola9527/article/details/43347747\n简述机器学习十大算法的每个算法的核心思想、工作原理、适用情况及优缺点等。\n1）C4.5算法：\nID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。\nC4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：\n1）用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；\n2）在树构造过程中进行剪枝\n3）能处理非离散的数据\n4）能处理不完整的数据\nC4.5算法优点：产生的分类规则易于理解，准确率较高。\n缺点：\n1)在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。\n2)C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。\n2）K means 算法：\n是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k< n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。\n其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk\n优点：算法速度很快\n缺点是，分组的数目k是一个输入参数，不合适的k可能返回较差的结果。\n3）朴素贝叶斯算法：\n朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。算法的基础是概率问题，分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。朴素贝叶斯假设是约束性很强的假设，假设特征条件独立，但朴素贝叶斯算法简单，快速，具有较小的出错率。\n在朴素贝叶斯的应用中，主要研究了电子邮件过滤以及文本分类研究。\n4)K最近邻分类算法（KNN）\n分类思想比较简单，从训练样本中找出K个与其最相近的样本，然后看这k个样本中哪个类别的样本多，则待判定的值（或说抽样）就属于这个类别。\n缺点：\n1）K值需要预先设定，而不能自适应\n2）当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。\n该算法在分类时有个重要的不足是，当样本不平衡时，即：一个类的样本容量很大，而其他类样本数量很小时，很有可能导致当输入一个未知样本时，该样本的K个邻居中大数量类的样本占多数。 但是这类样本并不接近目标样本，而数量小的这类样本很靠近目标样本。这个时候，我们有理由认为该位置样本属于数量小的样本所属的一类，但是，KNN却不关心这个问题，它只关心哪类样本的数量最多，而不去把距离远近考虑在内，因此，我们可以采用权值的方法来改进。和该样本距离小的邻居权值大，和该样本距离大的邻居权值则相对较小，由此，将距离远近的因素也考虑在内，避免因一个样本过大导致误判的情况\n该算法适用于对样本容量比较大的类域进行自动分类。\n5)EM最大期望算法\nEM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。\nEM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。\n6）PageRank算法\n是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）\n优点：\n完全独立于查询，只依赖于网页链接结构，可以离线计算。\n缺点：\n1）PageRank算法忽略了网页搜索的时效性。\n2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。\n7)AdaBoost\nAdaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。\n整个过程如下所示：\n1. 先通过对N个训练样本的学习得到第一个弱分类器；\n2. 将分错的样本和其他的新数据一起构成一个新的N个的训练样本，通过对这个样本的学习得到第二个弱分类器；\n3. 将和都分错了的样本加上其他的新样本构成另一个新的N个的训练样本，通过对这个样本的学习得到第三个弱分类器；\n4. 如此反复，最终得到经过提升的强分类器。\n目前AdaBoost算法广泛的应用于人脸检测、目标识别等领域。\n8）Apriori算法\nApriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。\nApriori算法分为两个阶段：\n1）寻找频繁项集\n2）由频繁项集找关联规则\n算法缺点：\n1） 在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；\n2） 每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。\n9）SVM支持向量机\n支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。\n支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。\nSVM在解决小样本、非线性及高维模式识别问题中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。\n10）CART分类与回归树\n是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数\n据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。\n优点\n1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。\n2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。"}
