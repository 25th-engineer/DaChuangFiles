{"content2":"常见的机器学习模型：感知机，线性回归，逻辑回归，支持向量机，决策树，随机森林，GBDT，XGBoost，贝叶斯，KNN，K-means等；\n常见的机器学习理论：过拟合问题，交叉验证问题，模型选择问题，模型融合问题等；\nK近邻：算法采用测量不同特征值之间的距离的方法进行分类。\n优点：\n1.简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；\n2.可用于数值型数据和离散型数据；\n3.训练时间复杂度为O(n)；无数据输入假定；\n4.对异常值不敏感\n缺点：\n1.计算复杂性高；空间复杂性高；\n2.样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；\n3.一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少 否则容易发生误分。\n4.最大的缺点是无法给出数据的内在含义。\n朴素贝叶斯\n优点：\n1.生成式模型，通过计算概率来进行分类，可以用来处理多分类问题，\n2.对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。\n缺点：\n1.对输入数据的表达形式很敏感，\n2.由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。需要一个比较容易解释，而且不同维度之间相关性较小的模型的时候。\n3.需要计算先验概率，分类决策存在错误率。\n决策树\n优点：\n1.概念简单，计算复杂度不高，可解释性强，输出结果易于理解；\n2.数据的准备工作简单， 能够同时处理数据型和常规型属性，其他的技术往往要求数据属性的单一。\n3.对中间值得确实不敏感，比较适合处理有缺失属性值的样本，能够处理不相关的特征；\n4.应用范围广，可以对很多属性的数据集构造决策树，可扩展性强。决策树可以用于不熟悉的数据集合，并从中提取出一些列规则 这一点强于KNN。\n缺点：\n1.容易出现过拟合；\n2.对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征。\n3. 信息缺失时处理起来比较困难。 忽略数据集中属性之间的相关性。\n4.同时它也是相对容易被攻击的分类器。这里的攻击是指人为的改变一些特征，使得分类器判断错误\n随机森林\n严格来说，随机森林其实算是一种集成算法。它首先随机选取不同的特征(feature)和训练样本(training sample)，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。\n随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。\n适用情景：\n数据维度相对低（几十维），同时对准确性有较高要求时。\n因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。\nSvm\n优点：\n1.可用于线性/非线性分类，也可以用于回归，泛化错误率低，计算开销不大，结果容易解释；\n2.可以解决小样本情况下的机器学习问题，可以解决高维问题 可以避免神经网络结构选择和局部极小点问题。\n3.SVM是最好的现成的分类器，现成是指不加修改可直接使用。并且能够得到较低的错误率，SVM可以对训练集之外的数据点做很好的分类决策。\n4.SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。\n缺点：对参数调节和和函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题。\nLogistic回归：根据现有数据对分类边界线建立回归公式，依次进行分类。\n优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低；\n缺点：容易欠拟合，分类精度可能不高\nEM 期望最大化算法-上帝算法\n只要有一些训练数据，再定义一个最大化函数，采用EM算法，利用计算机经过若干次迭代，就可以得到所需的模型。EM算法是自收敛的分类算法，既不需要事先设定类别也不需要数据见的两两比较合并等操作。缺点是当所要优化的函数不是凸函数时，EM算法容易给出局部最佳解，而不是最优解。\n判别分析 (Discriminant analysis)\nLDA的核心思想是把高维的样本投射(project)到低维上，如果要分成两类，就投射到一维。要分三类就投射到二维平面上。这样的投射当然有很多种不同的方式，LDA投射的标准就是让同类的样本尽量靠近，而不同类的尽量分开。对于未来要预测的样本，用同样的方式投射之后就可以轻易地分辨类别了。\n使用情景：\n判别分析适用于高维数据需要降维的情况，自带降维功能使得我们能方便地观察样本分布。它的正确性有数学公式可以证明，所以同样是很经得住推敲的方式。\n但是它的分类准确率往往不是很高，所以不是统计系的人就把它作为降维工具用吧。\n同时注意它是假定样本成正态分布的，所以那种同心圆形的数据就不要尝试了。\n更多分析见 https://www.zhihu.com/question/26726794\n下边这个转自http://www.ppvke.com/Blog/archives/44028\n机器 学习常见算法\n机器学习领域涉及到很多的算法和模型，这里遴选一些常见的算法：\n正则化算法（Regularization Algorithms）\n集成算法（Ensemble Algorithms）\n决策树算法（Decision Tree Algorithm）\n回归（Regression）\n人工神经网络（Artificial Neural Network）\n深度学习（Deep Learning）\n支持向量机（Support Vector Machine）\n降维算法（Dimensionality Reduction Algorithms）\n聚类算法（Clustering Algorithms）\n基于实例的算法（Instance-based Algorithms）\n贝叶斯算法（Bayesian Algorithms）\n关联规则学习算法（Association Rule Learning Algorithms）\n图模型（Graphical Models） ### 正则化算法（Regularization Algorithms） 正则化算法是另一种方法（通常是回归方法）的拓展，这种方法会基于模型复杂性对其进行惩罚，它喜欢相对简单能够更好的泛化的模型。 正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。 算法实例：\n岭回归（Ridge Regression）\n最小绝对收缩与选择算子（LASSO）\nGLASSO\n弹性网络（Elastic Net）\n最小角回归（Least-Angle Regression） 详解链接：机器学习之正则化算法\n集成算法（Ensemble algorithms）\n集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。这类算法又称元算法(meta-algorithm)。最常见的集成思想有两种bagging和boosting。\nboosting\n基于错误提升分类器性能，通过集中关注被已有分类器分类错误的样本，构建新分类器并集成。\nbagging\n基于数据随机重抽样的分类器构建方法。\n算法实例：\nBoosting\nBootstrapped Aggregation（Bagging）\nAdaBoost\n层叠泛化（Stacked Generalization）（blending）\n梯度推进机（Gradient Boosting Machines，GBM）\n梯度提升回归树（Gradient Boosted Regression Trees，GBRT）\n随机森林（Random Forest） 总结：当先最先进的预测几乎都使用了算法集成。它比使用单个模型预测出来的结果要精确的多。但是该算法需要大量的维护工作。 详细讲解：机器学习算法之集成算法 ### 决策树算法（Decision Tree Algorithm） 决策树学习使用一个决策树作为一个预测模型，它将对一个 item（表征在分支上）观察所得映射成关于该 item 的目标值的结论（表征在叶子中）。 决策树通过把实例从艮节点排列到某个叶子结点来分类实例，叶子结点即为实例所属的分类。树上的每一个结点指定了对实例的某个属性的测试，并且该结点的每一个后继分支对应于该属性的一个可能值。分类实例的方法是从这棵树的根节点开始，测试这个结点的属性，然后按照给定实例的属性值对应的树枝向下移动。然后这个过程在以新结点的根的子树上重复。 算法实例：\n分类和回归树（Classification and Regression Tree，CART）\nIterative Dichotomiser 3（ID3）\nC4.5 和 C5.0（一种强大方法的两个不同版本） 详解：机器学习算法之决策树算法\n回归（Regression）算法\n回归是用于估计两种变量之间关系的统计过程。当用于分析因变量和一个 多个自变量之间的关系时，该算法能提供很多建模和分析多个变量的技巧。具体一点说，回归分析可以帮助我们理解当任意一个自变量变化，另一个自变量不变时，因变量变化的典型值。最常见的是，回归分析能在给定自变量的条件下估计出因变量的条件期望。\n算法实例：\n普通最小二乘回归（Ordinary Least Squares Regression，OLSR）\n线性回归（Linear Regression）\n逻辑回归（Logistic Regression）\n逐步回归（Stepwise Regression）\n多元自适应回归样条（Multivariate Adaptive Regression Splines，MARS）\n本地散点平滑估计（Locally Estimated Scatterplot Smoothing，LOESS）\n回归算法详解：机器学习算法之回归算法\n人工神经网络\n人工神经网络是受生物神经网络启发而构建的算法模型。它是一种模式匹配，常被用于回归和分类问题，但拥有庞大的子域，由数百种算法和各类问题的变体组成。\n人工神经网络（ANN）提供了一种普遍而且实际的方法从样例中学习值为实数、离散值或向量函数。人工神经网络由一系列简单的单元相互连接构成，其中每个单元有一定数量的实值输入，并产生单一的实值输出。\n算法实例：\n感知器\n反向传播\nHopfield 网络\n径向基函数网络（Radial Basis Function Network，RBFN）\n详细链接：机器学习算法之人工神经网络\n深度学习（Deep Learning）\n深度学习是人工神经网络的最新分支，它受益于当代硬件的快速发展。\n众多研究者目前的方向主要集中于构建更大、更复杂的神经网络，目前有许多方法正在聚焦半监督学习问题，其中用于训练的大数据集只包含很少的标记。\n算法实例：\n深玻耳兹曼机（Deep Boltzmann Machine，DBM）\nDeep Belief Networks（DBN）\n卷积神经网络（CNN）\nStacked Auto-Encoders\n深度学习详解：机器学习算法之深度学习\n支持向量机（Support Vector Machines）\n支持向量机是一种监督式学习 (Supervised Learning)的方法，主要用在统计分类 (Classification)问题和回归分析 (Regression)问题上。支持向量机属于一般化线性分类器，也可以被认为是提克洛夫规范化（Tikhonov Regularization）方法的一个特例。这族分类器的特点是他们能够同时最小化经验误差与最大化几何边缘区，因此支持向量机也被称为最大边缘区分类器。现在多简称为SVM。\n给定一组训练事例，其中每个事例都属于两个类别中的一个，支持向量机（SVM）训练算法可以在被输入新的事例后将其分类到两个类别中的一个，使自身成为非概率二进制线性分类器。\nSVM 模型将训练事例表示为空间中的点，它们被映射到一幅图中，由一条明确的、尽可能宽的间隔分开以区分两个类别。\n算法详解：机器学习算法之支持向量机\n降维算法（Dimensionality Reduction Algorithms）\n所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x->y，其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。\n这一算法可用于可视化高维数据或简化接下来可用于监督学习中的数据。许多这样的方法可针对分类和回归的使用进行调整。\n算法实例：\n主成分分析（Principal Component Analysis (PCA)）\n主成分回归（Principal Component Regression (PCR)）\n偏最小二乘回归（Partial Least Squares Regression (PLSR)）\nSammon 映射（Sammon Mapping）\n多维尺度变换（Multidimensional Scaling (MDS)）\n投影寻踪（Projection Pursuit）\n线性判别分析（Linear Discriminant Analysis (LDA)）\n混合判别分析（Mixture Discriminant Analysis (MDA)）\n二次判别分析（Quadratic Discriminant Analysis (QDA)）\n灵活判别分析（Flexible Discriminant Analysis (FDA)）\n详解链接：降维算法\n聚类算法（Clustering Algorithms）\n聚类算法是指对一组目标进行分类，属于同一组（亦即一个类，cluster）的目标被划分在一组中，与其他组目标相比，同一组目标更加彼此相似。\n优点是让数据变得有意义，缺点是结果难以解读，针对不同的数据组，结果可能无用。\n算法实例：\nK-均值（k-Means）\nk-Medians 算法\nExpectation Maximi 封层 ation (EM)\n最大期望算法（EM）\n分层集群（Hierarchical Clstering）\n聚类算法详解：机器学习算法之聚类算法\n贝叶斯算法（Bayesian Algorithms）\n贝叶斯定理（英语：Bayes’ theorem）是概率论中的一个定理，它跟随机变量的条件概率以及边缘概率分布有关。在有些关于概率的解说中，贝叶斯定理（贝叶斯更新）能够告知我们如何利用新证据修改已有的看法。贝叶斯方法是指明确应用了贝叶斯定理来解决如分类和回归等问题的方法。\n算法实例：\n朴素贝叶斯（Naive Bayes）\n高斯朴素贝叶斯（Gaussian Naive Bayes）\n多项式朴素贝叶斯（Multinomial Naive Bayes）\n平均一致依赖估计器（Averaged One-Dependence Estimators (AODE)）\n贝叶斯信念网络（Bayesian Belief Network (BBN)）\n贝叶斯网络（Bayesian Network (BN)）\n贝叶斯算法链接：贝叶斯算法详解\n关联规则学习算法（Association Rule Learning Algorithms）\n关联规则学习方法能够提取出对数据中的变量之间的关系的最佳解释。比如说一家超市的销售数据中存在规则 {洋葱，土豆}=> {汉堡}，那说明当一位客户同时购买了洋葱和土豆的时候，他很有可能还会购买汉堡肉。有点类似于联想算法。\n算法实例：\nApriori 算法（Apriori algorithm）\nEclat 算法（Eclat algorithm）\nFP-growth\n关联规则学习算法：关联规则学习算法\n图模型（Graphical Models）\n图模型(GraphicalModels)在概率论与图论之间建立起了联姻关系。它提供了一种自然工具来处理应用数学与工程中的两类问题——不确定性(Uncertainty)和复杂性(Complexity)问 题，特别是在机器学习算法的分析与设计中扮演着重要角色。图模型的基本理念是模块化的思想，复杂系统是通过组合简单系统建构的。概率论提供了一种粘合剂使 系统的各个部分组合在一起，确保系统作为整体的持续一致性，提供了多种数据接口模型方法。\n图模型或概率图模型（PGM/probabilistic graphical model）是一种概率模型，一个图（graph）可以通过其表示随机变量之间的条件依赖结构（conditional dependence structure）。\n算法实例：\n贝叶斯网络（Bayesian network）\n马尔可夫随机域（Markov random field）\n链图（Chain Graphs）\n祖先图（Ancestral graph）\n图模型详解：机器学习算法之图模型"}
