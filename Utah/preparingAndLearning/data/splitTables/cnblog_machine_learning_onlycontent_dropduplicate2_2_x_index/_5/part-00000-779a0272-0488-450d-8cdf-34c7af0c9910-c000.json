{"content2":"#对coursera上Andrew Ng老师开的机器学习课程的笔记和心得；\n#注:此笔记是我自己认为本节课里比较重要、难理解或容易忘记的内容并做了些补充，并非是课堂详细笔记和要点；\n#标记为<补充>的是我自己加的内容而非课堂内容，参考文献列于文末。博主能力有限，若有错误，恳请指正；\n#---------------------------------------------------------------------------------#\n这一周的内容是机器学习介绍和梯度下降法。作为入门NG的这个课已足够，想较深入理解的话强烈建议去听coursera上台湾大学机器学习的内容。\n#---------------------------------------------------------------------------------#\n什么是机器学习?\nTom Mitchell给出的定义: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n换句话说，我们想让机器在某些方面有所提高（如商品推荐的准确率），就“喂”给机器一些数据（用户资料、网购记录等等），然后让机器从这些数据中学习，达到某个准确率提高的目的。\n#---------------------------------------------------------------------------------#\n机器学习按照数据标记分可分为四类：监督式学习，无监督学习，半监督学习和增强学习；\nsupervised learning(监督式): Application in which the training data comprises examples of the input vectors, along with their correspongding target vectors are known.\n关键词: \"right answer\" given(有标签), classification, regression;\nunsupervised learning(无监督): The training data consists of a set of input vectors X without any corresponding target values.\n关键词: 无标签，clusering, density estimation, visualization;\nSemi-supervised learning(半监督): is a class of machine learning techniques that make use of both labeled and unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data.\n关键词: 部分有标签；\nreinforcement learning(增强学习): a teacher only says to classifier whether it is right when suggesting a category for a pattern. The teacher does not tell what the correct category is.就是说一个评价仅仅给出某种判断是对还是错，而没有给出错在哪里。\n《补充》根据输入输出变量的不同类型，对预测任务给予不同的名称，\n输入输出变量均为连续->回归问题，输出变量为有限个离散变量->分类问题，输入输出变量均为变量序列->标注问题；\n#---------------------------------------------------------------------------------#\n<补充>机器学习三要素-模型(model)、策略(strategy)、算法(algorithm)；\n模型就是所要学习条件概率分布或决策函数，模型的假设空间包含所有可能的条件概率分布或者决策函数。我们常见的一些方法，像隐马模型(HMM)、SVM模型、决策树模型等等都归于此类；\n策略是指按照什么样的准则来学习或者挑选模型，常用到经验风险最小化或者结构风险最小化，像课上讲的J(Θ)、损失函数属于此类；\n这里的算法是指学习模型的具体计算方法，即用什么样的方法来求得最优解，机器学习问题归结为最优化问题，像课上讲的梯度下降法，其他如牛顿法、拟牛顿法属于此类；\n#---------------------------------------------------------------------------------#\ncost function,J（theta）的几张图非常有助于理解，单参数的bell-shape,双参数的3D-plot和等高线plot；\n，， ，\n学习速率α的大小很重要，小了导致梯度下降变慢，大了导致不收敛。所以要解决局部最优问题，改变α可能不是一个好办法，还是选多个初始位点来的安全；\n#---------------------------------------------------------------------------------#\nbatch gradient descent 求解思路:\n（1）将J(theta)对theta求偏导，得到每个theta对应的的梯度\n（2）每次移动的时候考虑所有的实验点，按每个参数theta的梯度负方向，来更新每个theta\n#---------------------------------------------------------------------------------#\nstochastic gradient descent求解思路:\n扫描每个点的时候就决定了参数的按照该点的梯度进行参数调整，每次参数调整只考虑当前一个试验点。\n#---------------------------------------------------------------------------------#\n《补充》标准梯度下降和随机梯度下降的关键区别：\n1，标准梯度下降在权值更新前对所有样例汇总误差，而随机梯度下降的权值是通过考察每个训练实例来更新的；\n2，标准梯度下降中，权值更新每一步对多个样例求和，需要更多计算，另外其使用的是真正的梯度，故每一次权值更新经常使用比随机梯度下降大的步长；\n3，两者都不能保证找到全局最优解，随机梯度下降有时能够避免陷入局部极小值，因为它使用不同的梯度来引导搜索；\n梯度下降法一般适用于计算过程的前期迭代或作为间插步骤，当接近极小点时，用梯度下降法不利于达到迭代的终止；\n#---------------------------------------------------------------------------------#\n参考文献:\n《统计学习方法》，李航著；\n《machine learning》, by Tom Mitchell；\ncouresra课程: standford machine learning, by Andrew Ng；\ncouresra课程:台湾大学機器學習基石，by 林軒田;"}
