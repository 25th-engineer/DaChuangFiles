{"content2":"按照前面文章的方法进行数据预测，完全不使用POI，天气，交通情况的数据，可以达到0.43的成绩。\n不过如果想要获得更好的成绩，简单的预测方法显然无法满足要求了。\nGBDT\n网友说可以使用GBDT的方法来进行数据预测。所以，我们先来聊聊GBDT算法的一些基础知识。\n熵\n凡是说到算法，人工智能，机器学习的文章，多半一定要说到 熵 这个概念的。什么是熵？\n百度一下：\n熵（entropy）指的是体系的混乱的程度，它在控制论、概率论、数论、天体物理、生命科学等领域都有重要应用，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。熵由鲁道夫·克劳修斯（Rudolf Clausius）提出，并应用在热力学中。后来在，克劳德·艾尔伍德·香农（Claude Elwood Shannon）第一次将熵的概念引入到信息论中来。\n一个体系越是单调，则熵越低，反之亦然。\n这里我们引用数据挖掘大神的文章来接单说一下熵。\n如果有一个字符串，里面包含了4种字符，每种出现的概率都是P= 1/4。\nP(X=A) = 1/4\nP(X=B) = 1/4\nP(X=C) = 1/4\nP(X=D) = 1/4\n这样的字符串可能是：BAACBADCDADDDA。传送这样的字符串，每一个字符需要用几个bit？\n答案是2个bit\nA = 00, B = 01, C = 10, D =11\n如果有一个字符串，里面包含了4种字符，但是每个字符串出现的概率不同\nP(X=A) = 1/2\nP(X=B) = 1/4\nP(X=C) = 1/8\nP(X=D) = 1/8\n传送这样的字符串，每一个字符平均需要用几个bit？注意这里说平均。\n答案是1.75个bit\nA = 0, B = 10, C = 110, D =111\n(如果使用等概率的方法， A = 00, B = 01, C = 10, D =11，则无法节省编码量，还是2个bit)\n这里巧妙的做到了，出现概率高的字符，使用的bit位少，同时做到了编码上的问题。\n（AB =〉010 和 C 110，D 111 不重复。AA =〉00 和 B 10 不重复 等）\n有如果有一个字符串，里面3种字符串，每种出现概率都是 1/3呢？\n最简单的编码方式是 A = 00, B = 01, C = 10, 这样是2个bit，但是如果好好计算一下，可以做到1.6个bit。\nA=10，B= 11，C = 0（理论上是1.58496 个bit）\n有如果有一个字符串，里面N种字符串，每种出现概率是 PN呢？\n如果有一个字符串，里面包含了4种字符，每种出现的概率都是P= 1/4 = 0.25。\nlog(0.25,2) = - 2\nH(X) = - (1/4) * log(0.25,2) - (1/4) * log(0.25,2) - (1/4) * log(0.25,2) - (1/4) * log(0.25,2) = 2;\n如果要表示下图的H（X）和H（Y）呢？\n这个很容易计算\n这个很容易计算\nH（X）= 1.5\nP（Math） = 1/2 P（History）= 1/4 P（CS）= 1/4\nlog(0.25,2) = - 2 log(0.5,2) = - 1\nH(X) = - (1/2) * log(0.5,2) - (1/4) * log(0.25,2) - (1/4) * log(0.25,2) = 0.5 + 0.5 + 0.5 = 1.5;\nH（Y）= 1\nP（Yes） = 1/2 P（No） = 1/2\nH(Y) = - (1/2) * log(0.5,2) - (1/2) * log(0.5,2) = 0.5 + 0.5 = 1;\n如果说，我们的计算范围只是 X = Math 的数据。那么这个时候 H（Y | X = Math) 是多少呢？是多少呢？答案是1。（一共4条记录，但是Y有两种可能性）\n如果说，我们的计算范围只是 X = Histroy 的数据。那么这个时候 H（Y| X = Histroy)是多少呢？答案也是 0 。（一共2条记录，但是Y只是一种可能性）\n如果说，我们的计算范围只是 X = CS 的数据。那么这个时候 H（Y| X = CS)是多少呢？答案也是 0 。（一共2条记录，但是Y只是一种可能性）\nH（Y | X ): 条件熵 Conditional Entropy\n现在我们考虑一个问题，如果我们需要将Y传输出去。当然，如果直接传输的话， H（Y）= 1。\n如果我们在传输的时候，双方都知道X的值，则需要熵定义为H（Y | X )。\n例如：大家都知道X=History，则 Y 必然是 NO， H（Y ) = 0 ， Histroy的可能性是1/4 ，需要的传输量是 0（CS同理）\n大家都知道X=Math，则 Y 可能是 Yes或者No，H（Y ) = 1 ，Math的可能性是1/2 ，需要的平均传输率是 1/2 * 1 = 0.5\nMath的概率 P（Math） = 1/2 ； History的概率 P（Histroy）= 1/4； History的概率 P（CS）= 1/4；\n则我们定义H（Y | X ) = H（Y | X = Math) * P（Math） + H（Y| X = Histroy) * P（Histroy） + H（Y| X = CS) * P（CS） = 0.5\nInformation Gain 信息增益 和 Relative Information Gain\n从上文可知，比起直接传输Y，条件熵则更加划算了。这些划算的部分，我们称为信息增益IG。\nIG(Y|X) = H(Y) - H(Y | X)\n上面的例子，IG(Y|X) = H(Y) - H(Y | X) = 1 - 0.5 = 0.5\n进一步，这样划算的部分，占原来所需部分的比重是多少呢？\nRIG= IG(Y|X) / H(Y) = 0.5 / 1 = 0.5 (节省的部分占了50%)\n信息增益是什么，我们先从它的用处来了解它：\n信息增益是特征选择中的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。\n指标选择\n回到滴滴算法的问题，我们应该挑选哪些指标作为GBDT的参考呢？\n滴滴算法大赛算法解决过程 - 数据分析\n滴滴算法大赛算法解决过程 - 拟合算法\n滴滴算法大赛算法解决过程 - 方案设计\n滴滴算法大赛算法解决过程 - 机器学习"}
