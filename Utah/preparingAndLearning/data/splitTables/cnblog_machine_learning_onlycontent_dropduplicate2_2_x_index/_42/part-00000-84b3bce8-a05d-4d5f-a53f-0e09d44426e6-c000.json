{"content2":"1.机器学习：通过对以往历史数据的学习建立一个模型用来预测以后的数据进行预测和分析。\n1.1监督学习 supervised learning\n监督学习可以分为生成方法（生成模型generative）和判别方法（判别模型discreiminative）\n生成模型：学习联合概率分布p(x,y)\np(y|x)=p(x,y)/p(x)=p(y)p(x|y)/p(y)\n比如贝叶斯模型，隐马尔科夫模型（HMM）\n判别模型：有数据直接学习f(x)或者条件概率分布p（y|x）\n比如:最近邻（KNN），感知机（perception）,决策树等\n学习过程的三要素：模型（Model）、策略、算法\nModel，即你要学习什么样的模型，包括线性模型,非线性模型等，取决于你要学习的问题和数据。\n策略，模型按照什么样的准则在整个模型解空间中选择最优的模型，其实就是损失代价函数包括（0-1损失，平方损失等）\n算法，参数模型的求解方法，比如梯度下降法，牛顿法等。\n风险函数： 分险函数度量评价意义下的模型预测的好坏。\n经验风险最小化，在假设空间，损失函数已经训练数据集确定的情况下：\n期望风险最小化（设计的模型与真实的误差）\n但是由于我们是无法知道数据的真实分布的，如果知道数据的真实分布，我们就不需要学习模型参数了，但是根据大数定律，当我们的训练数据趋于无穷大的时候，经验风险最小化可以近似的等于经验风险最小化。\n结构风险最小化\n为了使学习的模型不过拟合，引入正则项对模型的参数进行惩罚，使得模型不能过于复杂。\nJ（f）是模型的复杂度，比如参数向量的个数。\n举个例子说明：\n比如房价的交易价格（Y）和房子面积（X）问题：\n假如现在要预测一个面积为750的房子价格该多少钱，我们最能想到的是用一条曲线去拟合这些点，然后求出这条曲线的方程，再把x代入求解出Y。这就是监督学习，因为对于每一条数据我们都预先给出了正确的结果，上面这个问题又称为回归问题（regression），因为预测的变量Y是连续的。\n如果预测的变量不是连续的，而是有类别的就叫做分类问题（Classification）\n上面例子只用了一个特征，在现实生活中其实有很多维特征，特征也可能是无限维（svm是可以支持无线维特征的算法）。\n1.2无监督学习 unsupervised learning\n在有监督问题中，无论是分类还是回归，我们的每个数据都具有一个结果。比如房价多少。但是在无监督学习算法里面，每个数据是没有结果的，我们只用特征，而无监督学习则是学习如何可以将这些数据分到各自不同的组里面去。   无监督学习的一个例子就是聚类问题（clustering）\n2.线性回归问题\n2.1流程\n对于解决房价问题，其实我们是要将训练数据输入到学习算法，进而学习到一个假设H，然后我们将输入变量输入到h，预测出房价价格。\n2.分类问题\n那么对于h我们应当如何表示呢？可以是,因为只含有一个特征变量，所以也叫做单变量线性回归问题。\n2.2代价函数 cost fuction\n现在我们就要为我们上面建立好的模型选择适当的参数Θ，我们选择的参数就决定了我们预测的准确度，模型的预测值和训练集中的真实数据差就是建模误差，我们的目标就是要找出使得建模误差最小的模型参数，使得代价函数最小，具体可以查看http://www.cnblogs.com/GuoJiaSheng/p/3928160.html。\n3.分类问题\n3.1逻辑回归（logistic regreesion）\n在分类问题中，我们尝试对一个数据进行分类，比如判别它是否正确，是否为一封垃圾邮件。\n我们从二分类开始讨论：\n我们将因变量可能属于的类别分为负类，正类。y={0,1}，0为负类、1为正类。如下图，我们可以用线性回归的方法拟合一条曲线，但是线性回归只能预测出连续值，但是我们的分类问题是要预测为1或者0，我们可以这样分，当x>0.5为1当x<0.5为0，这样也可以很好的区分数据类别。\n但是当我们观测的数据越来越大的时候，如下图：\n这时候由于新增了点，如果这时候还使用0.5作为分隔点，就会出错。我们可以得出线性回归，因为预测的值可以超过[0,1]，所以并不适合解决这类问题。\n我们引入逻辑回归，它的输出值只在[0,1]范围之内。hθ(x)=g(θTX）\nx代表特征变量\ng（）代表逻辑函数。为s形函数，其实就是sigmod函数，它的值域为[0.1],\n,\n对于模型的理解为：给出参数x，根据选择的参数计算出变量为1或者0的概率可能性。\n当z>0是预测为1，当在z<0时，预测为0.\n比如说：我们有一个模型：\n并且参数为[-3 1 1] 即当-3+x1+x2>0时，即x1+x2>3时预测为1否则为0。为此我们就可以画出一条线用于分类。如下图：\n3.2代价函数\n之前对于线性回归的代价我们定义为模型误差的平方和，理论上在逻辑回归上，我们也可以沿用这一定义，但是如果将h(x)代入，我们得到的模型误差平方和就会是一个非凸函数。这意味着我们的代价函数有许多局部最小解，这将影响我们使用梯度下降法求解全局最小值。\n因此对于逻辑回归的代价函数我们重新定义为：\n然后就可以使用梯度下降法求解了。\n3.3多分类问题\n对于多分类问题，我们无法仅仅使用0,1用于区分某个类别，如下图：\n一种解决的办法就是使用一对多方案。其实就是在多类别里面，将一个类别作为正类，其他剩余的类别合起来作为一个类别，这样就转化为2分类问题。\n这样当我们要预测的时候，将所有的分类机都执行一遍，找出最可能的类别模型作为预测值。"}
