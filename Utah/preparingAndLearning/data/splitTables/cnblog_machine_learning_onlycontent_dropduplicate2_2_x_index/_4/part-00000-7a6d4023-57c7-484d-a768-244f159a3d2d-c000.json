{"content2":"#对coursera上Andrew Ng老师开的机器学习课程的笔记和心得；\n#注:此笔记是我自己认为本节课里比较重要、难理解或容易忘记的内容并做了些补充，并非是课堂详细笔记和要点；\n#标记为<补充>的是我自己加的内容而非课堂内容，参考文献列于文末。博主能力有限，若有错误，恳请指正；\n#---------------------------------------------------------------------------------#\n多层神经网络模型：\n，\n<补充>:每一个单元有一定数量的实值输入，产生单一的实值输出(可以是其他很多单元的输入)；\n符号标记：ai(j):activation of unit i in layer j ；Ɵ(j) :matrix of parameters controlling the function mapping from layer j to layer j+1；\n#---------------------------------------------------------------------------------#\n神经网络的cost function:\n前一项的目的是使所有单元的误差和最小(采用对数损失函数)，后一项是regularization项，旨在控制模型复杂度，防止overfitting；\n#---------------------------------------------------------------------------------#\nforward propagation(前向传播)\n<补充>：其实也就是通过神经网络，从输入参数到输出结果的计算过程(只计算一次)；\n参数的计算如下：\n，其中g(x)是sigmoid函数；\n#---------------------------------------------------------------------------------#\nBack propagation(反向传播)：与前向传播非常类似，从结果层倒推回输入层，计算每层δ的过程，δ为误差；\n，其中：l指第几层，；\n注:第一层是输入层，没有δ1项，最后一层(输出层)的δ不是按此式计算，见下例；\n，\nδ4 = a4 - y，δ3 = (Ɵ3)T δ4 . *(a3 . * (1 - a3))，δ2 = (Ɵ2)T δ3 . *(a2 . * (1 - a2))；\n#---------------------------------------------------------------------------------#\nBack propagation algorithm(反向传播算法)\n<补充>：一个最优化问题，目的是在使cost function值最小(这里是通过偏导最小来实现)的情况下，训练出神经网络各个参数的权值；\n算法如下:\n1，给出训练集作为输入，，将delta值设为0，；\n2，进行下列过程直至性能满足要求为止:\n对于每一训练(采样)输入，\n(a) 通过前向传播计算所得输出。\n(b) 通过反向传播计算每层的δ值；\n(c) 更新delta值：；\n3，得到神经网络参数的权值:\n,其中：;\n#---------------------------------------------------------------------------------#\n几则关于神经网络的问题和解决办法\n1，Gradient checking：反向传播算法有很多细节，非常容易出错，Gradient checking有助于cost function J(Ɵ)的准确性；\n原理：比较由反向传播计算得到的DVec和梯度计算得到的gradApprox两者是否相近似来判断；\n<补充>：其实是用了微积分当中导数的概念，；\n注:在训练数据时需要将Gradient checking代码注释掉，因为gradApprox的计算是很耗时的；\n2，Random initialization：反向传播算法是局部收敛的，需多次选起始点训练来减少最终局部收敛的可能性；\n#---------------------------------------------------------------------------------#\n参考文献:\n《machine learning》, by Tom Mitchell；\ncouresra课程: standford machine learning, by Andrew Ng；"}
