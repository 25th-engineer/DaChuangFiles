{"content2":"Spark提供了常用机器学习算法的实现， 封装于spark.ml和spark.mllib中.\nspark.mllib是基于RDD的机器学习库， spark.ml是基于DataFrame的机器学习库.\n相对于RDD， DataFrame拥有更丰富的操作API, 可以进行更灵活的操作. 目前, spark.mllib已经进入维护状态， 不再添加新特性.\n本文将重点介绍pyspark.ml， 测试环境为Spark 2.1, Python API.\n首先介绍pyspark.ml中的几个基类:\nML DataSet： 即为pyspark.sql.DataFrame作为数据集使用\npyspark.ml.Transformer： 代表将数据集转换到另一个数据集的算法\npyspark.ml.Estimator： 代表根据数据和参数创建模型的算法，包含方法\nfit(dataset, params)： 根据训练数据集和参数进行训练， 返回训练好的模型对象\npyspark.ml.Model: 代表训练好的模型的基类， 通常由Estimator.fit()创建. 包含的方法有:\ntransform(df): 将输入数据集代入模型变换为输出数据集\nsave(path): 保存训练好的模型\nload(path): 从文件中加载模型\npyspark.ml.Pipeline： 用于将多个步骤组合为管道进行处理， 可以建立线性管道和有向无环图管道.\npyspark.ml下将不同算法封装到不同的包中:\npyspark.ml.linalg 线性代数工具包. 包括：\nVector\nDenseVector\nSparseVector\nMatrix\nDenseMatrix\nSparseMatrix\npyspark.ml.feature特征和预处理算法包. 包括:\nTokenizer\nNormalizer\nStopWordsRemover\nPCA\nNGram\nWord2Vec\npyspark.ml.classification分类算法包. 包括：\nLogisticRegression\nDecisionTreeClassifier\nRandomForestClassifier\nNaiveBayes\nMultilayerPerceptronClassifier\nOneVsRest\npyspark.ml.clustering 聚类算法包. 包括：\nKMeans\nLDA\npyspark.ml.regression回归算法包. 包括：\nLinearRegression\nGeneralizedLinearRegression\nDecisionTreeRegressor\nRandomForestRegressor\npyspark.ml.recommendation推荐系统算法包. 包括：\nALS\npyspark.ml.tuning 校验工具包\npyspark.ml.evaluation 评估工具包\npyspark.ml中的算法大多数为Estimator的派生类. 大多数算法类均拥有对应的Model类.\n如classification.NaiveBayes和classification.NaiveBayesModel. 算法类的fit方法可以生成对应的Model类.\n应用示例\npyspark.ml使用了统一风格的接口，这里只展示部分算法.\n首先用NaiveBayes分类器做一个二分类：\n>>> from pyspark.sql import Row >>> from pyspark.ml.linalg import Vectors >>> df = spark.createDataFrame([ ... Row(label=0.0, weight=0.1, features=Vectors.dense([0.0, 0.0])), ... Row(label=0.0, weight=0.5, features=Vectors.dense([0.0, 1.0])), ... Row(label=1.0, weight=1.0, features=Vectors.dense([1.0, 0.0]))]) >>> nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", weightCol=\"weight\") >>> model = nb.fit(df) # 构造模型 >>> test0 = sc.parallelize([Row(features=Vectors.dense([1.0, 0.0]))]).toDF() >>> result = model.transform(test0).head() # 预测 >>> result.prediction 1.0 >>> result.probability DenseVector([0.32..., 0.67...]) >>> result.rawPrediction DenseVector([-1.72..., -0.99...])\nmodel.transform将输入的一行(Row)作为一个样本，产生一行输出. 这里我们只输入了一个测试样本， 所以直接使用head()取出唯一一行输出.\n使用LogisticRegression和OneVsRest做多分类：\n>>> from pyspark.sql import Row >>> from pyspark.ml.linalg import Vectors >>> df = sc.parallelize([ ... Row(label=0.0, features=Vectors.dense(1.0, 0.8)), ... Row(label=1.0, features=Vectors.sparse(2, [], [])), ... Row(label=2.0, features=Vectors.dense(0.5, 0.5))]).toDF() >>> lr = LogisticRegression(maxIter=5, regParam=0.01) >>> ovr = OneVsRest(classifier=lr) >>> model = ovr.fit(df) >>> # 进行预测 >>> test0 = sc.parallelize([Row(features=Vectors.dense(-1.0, 0.0))]).toDF() >>> model.transform(test0).head().prediction 1.0 >>> test1 = sc.parallelize([Row(features=Vectors.sparse(2, [0], [1.0]))]).toDF() >>> model.transform(test1).head().prediction 0.0 >>> test2 = sc.parallelize([Row(features=Vectors.dense(0.5, 0.4))]).toDF() >>> model.transform(test2).head().prediction 2.0\n使用PCA进行降维：\n>>> from pyspark.ml.linalg import Vectors >>> data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),), ... (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), ... (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)] >>> df = spark.createDataFrame(data,[\"features\"]) >>> pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\") >>> model = pca.fit(df) >>> model.transform(df).head().pca_features DenseVector([1.648..., -4.013...])\nEstimator和Transformer均为PipelineStage的派生类，pipeline由一系列Stage组成.调用pipeline对象的fit方法， 将会依次执行Stage并生成一个最终模型.\n>>>from pyspark.ml import Pipeline >>>from pyspark.ml.classification import LogisticRegression >>>from pyspark.ml.feature import HashingTF, Tokenizer >>> data = [ (0, \"a b c d e spark\", 1.0), (1, \"b d\", 0.0), (2, \"spark f g h\", 1.0), (3, \"hadoop mapreduce\", 0.0) ] >>> df = spark.createDataFrame(data, [\"id\", \"text\", \"label\"]) >>> # build pipeline >>> tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") >>> hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") >>> lr = LogisticRegression(maxIter=10, regParam=0.001) >>> pipeline = Pipeline(stages=[tokenizer, hashingTF, lr]) >>> # train >>> model = pipeline.fit(df) >>> data2 = [ (4, \"spark i j k\"), (5, \"l m n\"), (6, \"spark hadoop spark\"), (7, \"apache hadoop\") ] >>> test = spark.createDataFrame(data2, [\"id\", \"text\"]) >>> result = model.transform(test) >>> result = result.select(\"id\", \"text\", \"probability\", \"prediction\") >>> result.collect() [Row(id=4, text=u'spark i j k', probability=DenseVector([0.1596, 0.8404]), prediction=1.0), Row(id=5, text=u'l m n', probability=DenseVector([0.8378, 0.1622]), prediction=0.0), Row(id=6, text=u'spark hadoop spark', probability=DenseVector([0.0693, 0.9307]), prediction=1.0), Row(id=7, text=u'apache hadoop', probability=DenseVector([0.9822, 0.0178]), prediction=0.0)]\n本文示例来源于官方文档\n更多内容请参考:\npyspark.ml文档\nSpark ML编程指导"}
