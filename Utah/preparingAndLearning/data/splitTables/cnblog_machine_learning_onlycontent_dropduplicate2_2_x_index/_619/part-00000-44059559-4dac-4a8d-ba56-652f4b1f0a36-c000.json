{"content2":"机器不学习 jqbxx.com-专注机器学习,深度学习,自然语言处理,大数据,个性化推荐,搜索算法,知识图谱\n虽然我不是专门研究迁移学习的，但是作为一个AI研究者，就如题图吴老师所说，迁移学习极为重要，是必须要学习的，今天就先总结介绍一些迁移学习的基础知识，目录如下：\n迁移学习一些概念\n迁移学习简介\n迁移学习的分类\n迁移学习热门研究方向\n迁移学习一些概念\n在文章的一开始，先来学习迁移学习一些概念：\n域：一个域 D 由一个特征空间 X 和特征空间上的边际概率分布 P(X) 组成，其中 X=x1,x2,...xn 。举个例子：对于一个有文档，其有很多词袋表征（bag-of-words representation）X 是所有文档表征的空间，而 xi 是第 i 个单词的二进制特征。P(X)代表对X的分布。\n任务 ：在给定一个域 D={X,P(X)} 之后，一个任务 T 由一个标签空间 y 以及一个条件概率分布 P(Y/X) 构成，其中，这个条件概率分布通常是从由特征—标签对 xi, yi组成的训练数据中学习得到。\n源域(source domain),目标域(target domain):在迁移学习中，我们已有的知识叫做源域(source domain)，要学习的新知识叫目标域(target domain)。\n负迁移:指的是在源域上学习到的知识，对于目标域上的学习产生负面作用。产生负迁移的原因主要有两个：一个是源域 和目标域的相似度很低，无法做迁移。另一个是虽数据问源域和目标域是相似的，但是，迁移学习方法不够好，没找到可迁移的成分，导致负迁移。\n迁移学习简介\n先举几个例子，比如我们已经会编写Java程序，就可以类比着来学习C++，都是面向对象的语言，就很快学会了，或者在学会骑自行车之后，骑摩托车也自己比较容易了，因为这两种交通工具有许多相似之处。总结起来，用成语来说迁移学习就是举一反三！\n再来个图示，如下左图，传统机器学习对不同的学习任务需要建立不同的模型，学习不同的参数，而对于迁移学习（右图），只需要利用源域中的数据将知识迁移到目标域，就能完成模型建立。\n迁移学习的严格定义:\n给定源域 Ds={Xs, Fs(X)} 和学习任务 Ts ,目标域 DT={Xt,Ft(X)} 和学习任务 Tt ,迁移学习旨在源域不同于目标域或学习任务 Tt 不同于学习任务 Ts 的条件下通过使用学习任务 Ts 和源域 Ds={Xs,Fs(X)} 所获取的知识来帮助学习目标的在目标域Dt的预测函数 Ft(.) 。\n为什么需要进行迁移学习？\n数据的标签很难获取，当有些任务的数据标签很难获取时，就可以通过其他容易获取标签且和该任务相似的任务来迁移学习。\n从头建立模型是复杂和耗时的，也即是需要通过迁移学习来加快学习效率。\n和一起相关领域的辨析：\n多任务学习：区别在于在迁移学习中，我们主要关心在我们的目标任务和域上的表现。而多任务学习中的目标是在所有可用的任务上都要表现良好，尽管某个标签数据通常都被假定在一个任务上。当然，现在迁移学习和多任务学习也并没有很大的区别，比如归纳式迁移学习中当两个域都有标签的时候， 这就与多任务学习相似\n持续学习：虽然多任务学习允许我们在许多任务中保留知识，而不会对我们的源任务造成性能损失，但只有在所有任务都处于训练时间的情况下，这才是可能的。对于每个新任务，我们通常需要重新训练我们所有任务的模型。然而，在现实世界中，我们希望一个代理能够通过使用它以往的一些经验来处理逐渐变得复杂的任务。为了达到这个目的，我们需要让一个模型在不忘记的情况下持续地学习。这个机器学习的领域被称为学会学习、元学习（meta learning）、终生学习，或者持续学习。持续学习在最近的强化学习 (强化学习以 Google DeepMind 对通用学习代理的探索而著称) 上已经取得了成功 ，也正在被用于序列到序列的模型上 。\n最近元学习（meta learning）也挺火爆的，再来举个网上的例子：我们都知道，在金庸的武侠世界中，有各种各样的武功，不同的武功都不一样，有内功也有外功。那么里面的张无忌就特别厉害，因为他练成了九阳神功。有了九阳神功，张无忌学习新的武功就特别快，在电影倚天屠龙记之魔教教主中，张无忌分分钟学会了张三丰的太极拳打败了玄冥二老。九阳神功就是一种学会学习的武功！我们希望神经网络也能学会学习，这样也就能快速学习啦！\nzero-shot 学习：如果我们把迁移学习使用到极限，并且想要仅仅从很少的实例中学习，这就分别得到了 few-shot、one-shot 以及 zero-shot 学习。让模型执行 one-shot 和 zero-shot 学习，无疑属于机器学习中最艰难的问题。而另一方面，这却是我们人类天生就会的：幼年的时候，为了让我们能够认出任何狗狗，我们仅仅需要被告知一次「这是一条狗」，然而成年人可以仅通过在文中阅读就理解一个东西的本质，不需要事先见过它。\none-shot 学习的新进展利用了这样的思想，即为了在测试的时候实现好的性能，模型需要显式地被训练，从而进行 one-shot 学习。但更加逼真、具有概括性的 zero-shot 学习设置在最近已经引起了注意，在零点学习中训练类别出现在测试的时候。\n以上就是一些相关领域，当然这里只是很简单的介绍，有一个概念，详细请自行了解\n迁移学习的分类\n按迁移情景分\n归纳式迁移学习（Inductive TL）：源域和目标域的学习任务不同\n直推式迁移学习（Transductive TL):源域和目标域不同，学习任务相同\n无监督迁移学习（Unsupervised TL):源域和目标域均没有标签\n根据源Domain和目前Domain 之间的关系，源Task 和 目标Task之间的关系，以及任务方法更详细的整理为下表：\n按迁移学习的基本方法分\n基于实例的迁移学习方法\n在源域中找到与目标域相似的数据，把这个数据的权值进行调整，使得新的数据与目标域的数据进行匹配。然后进行训练学习，得到适用于目标域的模型。这样的方法优点是方法简单，实现容易。缺点在于权重的选择与相似度的度量依赖经验，且源域与目标域的数据分布往往不同。\n基于特征的迁移学习方法\n当源域和目标域含有一些共同的交叉特征时，我们可以通过特征变换，将源域和目标域的特征变换到相同空间，使得该空间中源域数据与目标域数据具有相同分布的数据分布，然后进行传统的机器学习。优点是对大多数方法适用，效果较好。缺点在于难于求解，容易发生过适配。\n需要注意的的是基于特征的迁移学习方法和基于实例的迁移学习方法的不同是基于特征的迁移学习需要进行特征变换来使得源域和目标域数据到到同一特征空间，而基于实例的迁移学习只是从实际数据中进行选择来得到与目标域相似的部分数据，然后直接学习。\n基于模型的迁移学习方法\n源域和目标域共享模型参数，也就是将之前在源域中通过大量数据训练好的模型应用到目标域上进行预测。基于模型的迁移学习方法比较直接，这样的方法优点是可以充分利用模型之间存在的相似性。缺点在于模型参数不易收敛。\n举个例子：比如利用上千万的图象来训练好一个图象识别的系统，当我们遇到一个新的图象领域问题的时候，就不用再去找几千万个图象来训练了，只需把原来训练好的模型迁移到新的领域，在新的领域往往只需几万张图片就够，同样可以得到很高的精度。\n基于关系的迁移学习方法\n当两个域是相似的时候，那么它们之间会共享某种相似关系，将源域中学习到的逻辑网络关系应用到目标域上来进行迁移，比方说生物病毒传播规律到计算机病毒传播规律的迁移。这部分的研究工作比较少。典型方法就是mapping的方法\n看一个图来总结以上的知识（可以看成归纳式迁移学习是最广泛应用的）：\n按特征空间分\n同构迁移学习（Homogeneous TL）: 源域和目标域的特征维度相同分布不同\n异构迁移学习（Heterogeneous TL）：源域和目标域的特征空间不同\n以下图是做迁移学习分类的一个梳理：\n迁移学习热门研究方向\n域适配问题(domain adaptation)：有标签的源域和无标签的目标域共享相同的特征和类别，但是特征分布不同，如何利用源域标定目标域。解决Domain adaptation问题主要的思路就是将source训练好的模型能够用在target上，而域适配问题最主要的也就是如何减少source域和target域不同分布之间的差异。\n代表性论文有：Domain adaptation via transfer component analysis--基于特征的迁移方法；Density ratio estimation in machine learning--基于实例的迁移方法；Cross-domain video concept detection using adaptive svms--基于模型的迁移方法等等\n最近的进展也有 Wasserstein Distance Guided Representation Learning for Domain Adaptation等，用W-GAN来做domain adaptation，可以一看。\n多源迁移学习(multi-source TL):多个源域和目标域，通过进行有效的域筛选，从而进行迁移。多源迁移学习可以有效利用存在的多个可用域，综合起来进行迁移，达到较好的效果。当然现在如何衡量多个域之间的相关性和多个域的利用方法还是一个比较大的问题\n代表性论文有：Boosting for transfer learning；Multi-source transfer learning with multi-view adaboost等等\n深度迁移学习(deep TL) :特别是近年来由于深度学习的火爆，越来越多研究者利用深度神经网络的结构进行迁移学习，深度学习可以深度表征域中的知识结构，也大大增强了模型的泛化能力，可以说利用深度学习做迁移学习的前景还是很好的。\n代表性论文有：Simultaneous deep transfer across domains and tasks；Multi-source transfer learning with multi-view adaboost；Learning Transferable Features with Deep Adaptation Networks等等\n更多精彩内容，机器不学习官方网站 jqbxx.com"}
