{"content2":"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。\nk近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型,隐马尔科夫，条件随机场，adaboost，em 这些在一般工作中，分别用到的频率多大？一般用…\n关于这个问题我今天正好看到了这个文章。讲的正是各个算法的优劣分析，很中肯。\nhttps://zhuanlan.zhihu.com/p/25327755\n正好14年的时候有人做过一个实验[1]，比较在不同数据集上（121个），不同的分类器（179个）的实际效果。\n论文题为：Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?\n实验时间有点早，我尝试着结合我自己的理解、一些最近的实验，来谈一谈吧。主要针对分类器(Classifier)。\n写给懒得看的人：\n没有最好的分类器，只有最合适的分类器。\n随机森林平均来说最强，但也只在9.9%的数据集上拿到了第一，优点是鲜有短板。\nSVM的平均水平紧随其后，在10.7%的数据集上拿到第一。\n神经网络（13.2%）和boosting（~9%）表现不错。\n数据维度越高，随机森林就比AdaBoost强越多，但是整体不及SVM[2]。\n数据量越大，神经网络就越强。\n近邻 (Nearest Neighbor)\n典型的例子是KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。\n它的特点是完全跟着数据走，没有数学模型可言。\n适用情景：\n需要一个特别容易解释的模型的时候。\n比如需要向用户解释原因的推荐算法。\n贝叶斯 (Bayesian)\n典型的例子是Naive Bayes，核心思路是根据条件概率计算待判断点的类型。\n是相对容易理解的一个模型，至今依然被垃圾邮件过滤器使用。\n适用情景：\n需要一个比较容易解释，而且不同维度之间相关性较小的模型的时候。\n可以高效处理高维数据，虽然结果可能不尽如人意。\n决策树 (Decision tree)\n决策树的特点是它总是在沿着特征做切分。随着层层递进，这个划分会越来越细。\n虽然生成的树不容易给用户看，但是数据分析的时候，通过观察树的上层结构，能够对分类器的核心思路有一个直观的感受。\n举个简单的例子，当我们预测一个孩子的身高的时候，决策树的第一层可能是这个孩子的性别。男生走左边的树进行进一步预测，女生则走右边的树。这就说明性别对身高有很强的影响。\n适用情景：\n因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，数据分析师希望更好的理解手上的数据的时候往往可以使用决策树。\n同时它也是相对容易被攻击的分类器[3]。这里的攻击是指人为的改变一些特征，使得分类器判断错误。常见于垃圾邮件躲避检测中。因为决策树最终在底层判断是基于单个条件的，攻击者往往只需要改变很少的特征就可以逃过监测。\n受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石。\n随机森林 (Random forest)\n提到决策树就不得不提随机森林。顾名思义，森林就是很多树。\n严格来说，随机森林其实算是一种集成算法。它首先随机选取不同的特征(feature)和训练样本(training sample)，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。\n随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。\n适用情景：\n数据维度相对低（几十维），同时对准确性有较高要求时。\n因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。\nSVM (Support vector machine)\nSVM的核心思想就是找到不同类别之间的分界面，使得两类样本尽量落在面的两边，而且离分界面尽量远。\n最早的SVM是平面的，局限很大。但是利用核函数(kernel function)，我们可以把平面投射(mapping)成曲面，进而大大提高SVM的适用范围。\n提高之后的SVM同样被大量使用，在实际分类中展现了很优秀的正确率。\n适用情景：\nSVM在很多数据集上都有优秀的表现。\n相对来说，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。\n和随机森林一样，这也是一个拿到数据就可以先尝试一下的算法。\n逻辑斯蒂回归 (Logistic regression)\n逻辑斯蒂回归这个名字太诡异了，我就叫它LR吧，反正讨论的是分类器，也没有别的方法叫LR。顾名思义，它其实是回归类方法的一个变体。\n回归方法的核心就是为函数找到最合适的参数，使得函数的值和样本的值最接近。例如线性回归(Linear regression)就是对于函数f(x)=ax+b，找到最合适的a,b。\nLR拟合的就不是线性函数了，它拟合的是一个概率学中的函数，f(x)的值这时候就反映了样本属于这个类的概率。\n适用情景：\nLR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。\n因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。\n虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。\n判别分析 (Discriminant analysis)\n判别分析主要是统计那边在用，所以我也不是很熟悉，临时找统计系的闺蜜补了补课。这里就现学现卖了。\n判别分析的典型例子是线性判别分析(Linear discriminant analysis)，简称LDA。\n（这里注意不要和隐含狄利克雷分布(Latent Dirichlet allocation)弄混，虽然都叫LDA但说的不是一件事。）\nLDA的核心思想是把高维的样本投射(project)到低维上，如果要分成两类，就投射到一维。要分三类就投射到二维平面上。这样的投射当然有很多种不同的方式，LDA投射的标准就是让同类的样本尽量靠近，而不同类的尽量分开。对于未来要预测的样本，用同样的方式投射之后就可以轻易地分辨类别了。\n使用情景：\n判别分析适用于高维数据需要降维的情况，自带降维功能使得我们能方便地观察样本分布。它的正确性有数学公式可以证明，所以同样是很经得住推敲的方式。\n但是它的分类准确率往往不是很高，所以不是统计系的人就把它作为降维工具用吧。\n同时注意它是假定样本成正态分布的，所以那种同心圆形的数据就不要尝试了。\n神经网络 (Neural network)\n神经网络现在是火得不行啊。它的核心思路是利用训练样本(training sample)来逐渐地完善参数。还是举个例子预测身高的例子，如果输入的特征中有一个是性别（1:男；0:女），而输出的特征是身高（1:高；0:矮）。那么当训练样本是一个个子高的男生的时候，在神经网络中，从“男”到“高”的路线就会被强化。同理，如果来了一个个子高的女生，那从“女”到“高”的路线就会被强化。\n最终神经网络的哪些路线比较强，就由我们的样本所决定。\n神经网络的优势在于，它可以有很多很多层。如果输入输出是直接连接的，那它和LR就没有什么区别。但是通过大量中间层的引入，它就能够捕捉很多输入特征之间的关系。卷积神经网络有很经典的不同层的可视化展示(visulization)，我这里就不赘述了。\n神经网络的提出其实很早了，但是它的准确率依赖于庞大的训练集，原本受限于计算机的速度，分类效果一直不如随机森林和SVM这种经典算法。\n使用情景：\n数据量庞大，参数之间存在内在联系的时候。\n当然现在神经网络不只是一个分类器，它还可以用来生成数据，用来做降维，这些就不在这里讨论了。\nRule-based methods\n这个我是真不熟，都不知道中文翻译是什么。\n它里面典型的算法是C5.0 Rules，一个基于决策树的变体。因为决策树毕竟是树状结构，理解上还是有一定难度。所以它把决策树的结果提取出来，形成一个一个两三个条件组成的小规则。\n使用情景：\n它的准确度比决策树稍低，很少见人用。大概需要提供明确小规则来解释决定的时候才会用吧。\n提升算法（Boosting）\n接下来讲的一系列模型，都属于集成学习算法(Ensemble Learning)，基于一个核心理念：三个臭皮匠，顶个诸葛亮。\n翻译过来就是：当我们把多个较弱的分类器结合起来的时候，它的结果会比一个强的分类器更\n典型的例子是AdaBoost。\nAdaBoost的实现是一个渐进的过程，从一个最基础的分类器开始，每次寻找一个最能解决当前错误样本的分类器。用加权取和(weighted sum)的方式把这个新分类器结合进已有的分类器中。\n它的好处是自带了特征选择（feature selection），只使用在训练集中发现有效的特征(feature)。这样就降低了分类时需要计算的特征数量，也在一定程度上解决了高维数据难以理解的问题。\n最经典的AdaBoost实现中，它的每一个弱分类器其实就是一个决策树。这就是之前为什么说决策树是各种算法的基石。\n使用情景：\n好的Boosting算法，它的准确性不逊于随机森林。虽然在[1]的实验中只有一个挤进前十，但是实际使用中它还是很强的。因为自带特征选择（feature selection）所以对新手很友好，是一个“不知道用什么就试一下它吧”的算法。\n装袋算法（Bagging）\n同样是弱分类器组合的思路，相对于Boosting，其实Bagging更好理解。它首先随机地抽取训练集（training set），以之为基础训练多个弱分类器。然后通过取平均，或者投票(voting)的方式决定最终的分类结果。\n因为它随机选取训练集的特点，Bagging可以一定程度上避免过渡拟合(overfit)。\n在[1]中，最强的Bagging算法是基于SVM的。如果用定义不那么严格的话，随机森林也算是Bagging的一种。\n使用情景：\n相较于经典的必使算法，Bagging使用的人更少一些。一部分的原因是Bagging的效果和参数的选择关系比较大，用默认参数往往没有很好的效果。\n虽然调对参数结果会比决策树和LR好，但是模型也变得复杂了，没事有特别的原因就别用它了。\nStacking\n这个我是真不知道中文怎么说了。它所做的是在多个分类器的结果上，再套一个新的分类器。\n这个新的分类器就基于弱分类器的分析结果，加上训练标签(training label)进行训练。一般这最后一层用的是LR。\nStacking在[1]里面的表现不好，可能是因为增加的一层分类器引入了更多的参数，也可能是因为有过渡拟合(overfit)的现象。\n使用情景：\n没事就别用了。\n（修订：\n@庄岩\n提醒说stacking在数据挖掘竞赛的网站kaggle上很火，相信参数调得好的话还是对结果能有帮助的。\nhttp://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/\n这篇文章很好地介绍了stacking的好处。在kaggle这种一点点提升就意味着名次不同的场合下，stacking还是很有效的，但是对于一般商用，它所带来的提升就很难值回额外的复杂度了。）\n多专家模型（Mixture of Experts）\n最近这个模型还挺流行的，主要是用来合并神经网络的分类结果。我也不是很熟，对神经网络感兴趣，而且训练集异质性（heterogeneity）比较强的话可以研究一下这个。\n讲到这里分类器其实基本说完了。讲一下问题里面其他一些名词吧。\n最大熵模型 (Maximum entropy model)\n最大熵模型本身不是分类器，它一般是用来判断模型预测结果的好坏的。\n对于它来说，分类器预测是相当于是：针对样本，给每个类一个出现概率。比如说样本的特征是：性别男。我的分类器可能就给出了下面这样一个概率：高（60%），矮（40%）。\n而如果这个样本真的是高的，那我们就得了一个分数60%。最大熵模型的目标就是让这些分数的乘积尽量大。\nLR其实就是使用最大熵模型作为优化目标的一个算法[4]。\nEM\n就像最大熵模型一样，EM不是分类器，而是一个思路。很多算法都是基于这个思路实现的。\n@刘奕驰 已经讲得很清楚了，我就不多说了。\n隐马尔科夫 (Hidden Markov model)\n这是一个基于序列的预测方法，核心思想就是通过上一个（或几个）状态预测下一个状态。\n之所以叫“隐”马尔科夫是因为它的设定是状态本身我们是看不到的，我们只能根据状态生成的结果序列来学习可能的状态。\n适用场景：\n可以用于序列的预测，可以用来生成序列。\n条件随机场 (Conditional random field)\n典型的例子是linear-chain CRF。\n具体的使用 @Aron 有讲，我就不献丑了，因为我从来没用过这个。\n就是这些啦。\n相关的文章：\n[1]: Do we need hundreds of classifiers to solve real world classification problems.\nFernández-Delgado, Manuel, et al. J. Mach. Learn. Res 15.1 (2014)\n[2]: An empirical evaluation of supervised learning in high dimensions.\nRich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. ICML '08\n[3]: Man vs. Machine: Practical Adversarial Detection of Malicious Crowdsourcing Workers\nWang, G., Wang, T., Zheng, H., & Zhao, B. Y. Usenix Security'14\n[4]: http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf\n编辑于 2017-05-18\nHu Haitang\nhttps://hthu.github.io/\n一般来说Machine Learning主要有3种:\nSupervised Learning\nSemi-supervised Learning\nUnsupervised Learning\nSupervised Learning适用与已知LABEL的情况.\nSemi-supervised/Unsupervised Learning适用于有Latent Variable的情况.\n题主提到的这些算法虽然常见,但是想真正的搞懂需要深入了解(Calculus, Linear Algebra, Probabilistic).\n比如SVM,为什么叫Support Vector Machine, 为什么SVM有Sparsity的特性, L1/L2 Regularizar起到了什么作用, 等等...\n单单一个SVM就有太多的内容在里面了.\n-----------------------------------------------------------------------------------------------\n接下来回答问题:\nk近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型,隐马尔科夫，条件随机场，adaboost，em\n首先:\nMaximum Entropy和EM是理念.\nMaximum Entropy的背后是信息论(Information Theory)和概率模型.\nEM Model事实上对所有含有Latent Variable的模型都可以用.\nkNN可以用于图像压缩.\n但是,如何选取K, Similarity用什么, 都不是随便说应用就应用的...\nNaive Bayes: 知名的 http://arxiv.org 背后就有用,简单粗暴, 强大有效...\n但是, NB算法如何Smooth,如何应对Online Setting,也不是那么简单...\nSVM/Logistic Regression 都可以用于各种分类.\n但是Hinge Loss和Sigmoid Function有什么区别, 什么时候用哪个?\nHMM/CRF NLP常用\nAda-boost 可以参考Netflix Prize,貌似大奖用了这个...具体不清楚\n但是, Weak Learner是什么, Why boost works?\n-----------------------------------------------------------------------------------------------\n总结:\n前几天Yann Lecun来我们学校做了演讲, 然后我们的老师回头总结,我觉得很有道理:\n就是现在Deep Learning处于一个Scaling就是Accuracy的阶段,很多背后的原因我们其实都没有搞清楚. 例如, Deep Learning里用Max做Feature selection效果很好,但是Yann Lecun也只能猜测原因.\n所以真正的应用,取决于你动手去做, 去尝试.\n最后:\n推荐Andrew的公开课, 你值得拥有.\nhttps://class.coursera.org/ml-006\n编辑于 2015-03-09\n唐学伟\n机器学习、金融、招聘机器学习、数据分析tangxuewei@wecash.net\n先上一张图吧，有需要再详细写\n编辑于 2014-12-08\n知乎用户\ncrf 分词、语法依存分析、命名实体识别，但是现在正在越来越多的应用场景里被RNN已经它的变种们所替代~ LSTM+CRF的解决方案取得了state of art的效果~\nlr ctr 预估，商品推荐转换为点击率预估也可以用该模型。之前天猫大数据比赛很多同学都是用它做的session first。可以着重了解推导，正则化及并行。但现在越来越多的依赖于深度学习，包括DNN, DRL.\nsvm 可以做文本分类，人脸识别等，了解下原始问题如何转换为对偶问题，然后使用smo或者其他问题求解，还有了解下核函数~\nadaboost 本身是exp loss在boosting方法下的Model.\nEM 是一种优化算法，本质个人认为有点类似于离散空间的梯度上升算法，一般是结合具体的算法来用，比如混合高斯模型，最大熵，无监督HMM等，但比较经典的个人感觉还是pLSA，其实k-means背后也有em的思想，了解em再看k-means就有感觉了。\n决策树可以认为是将空间进行划分，ID3和C4.5算是比较经典的决策树算法，可以用来分类，也可以用来回归，但业界很少直接使用一棵树，一般使用多棵树，组成committee，较为经典有GBDT 和RF，两者都是ensemble learning的典范，只不过前者使用boosting降低bias，后者使用bagging降低variance从而提升模型的performance。在ESL中有个对比，使树形模型几乎完爆其他算法，泛化能力和学习能力都很牛逼。业界的话一般用来做搜索排序和相关性。\nHMM，在基础的一阶马儿科夫的基础之上，加入隐含状态，有两者假设，解决三种问题，一般时间序列预测都可以用该模型，当然了，NLP中的分词，语音识别等效果也还不错。\n先写到这，求大神来喷~\n编辑于 2017-09-20\n刘奕驰\n分别用到的频率多大？\nIt depends. 看你需要处理的问题是什么。不同数据规模、不同特征都会影响算法的选择。\n一般用途是什么？\n这个问题太大了，简单来说，\nKNN、朴素贝叶斯、决策树、SVM、logistic回归、adaboost用来分类。\nEM算法用于寻找隐藏参数的最大似然估计。该算法首先在E step中计算隐藏参数的似然估计，然后再M step中进行最大化，然后进行EM step的迭代直至收敛。应用场景之一是聚类问题，但EM算法本身并不是一个聚类算法。举个例子，GMM(高斯混合模型)和Kmeans在聚类时都使用了EM算法。\n最大熵模型是一个概率模型，决策树的数学基础就是它。\nHMM也是一个统计模型，我们不能用它来做什么，但是可以利用这个模型对现实生活中的问题建模。\n需要注意什么？\n实践出真知。\n编辑于 2014-12-03\n刘莉莉\n“能宽容必定心怀珍宝” DL & RL\n谢霄哥邀请\n@孙凌霄\n这个问题确实很有意思，作为新入门的小白 只能提供一点点粗略的认识。\n----------------------------------------------------------------------------------------------------------\nK近邻：算法采用测量不同特征值之间的距离的方法进行分类。\n优点：\n1.简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；\n2.可用于数值型数据和离散型数据；\n3.训练时间复杂度为O(n)；无数据输入假定；\n4.对异常值不敏感\n缺点：\n1.计算复杂性高；空间复杂性高；\n2.样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；\n3.一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少 否则容易发生误分。\n4.最大的缺点是无法给出数据的内在含义。\n朴素贝叶斯\n优点：\n1.生成式模型，通过计算概率来进行分类，可以用来处理多分类问题，\n2.对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。\n缺点：\n1.对输入数据的表达形式很敏感，\n2.由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。\n3.需要计算先验概率，分类决策存在错误率。\n决策树\n优点：\n1.概念简单，计算复杂度不高，可解释性强，输出结果易于理解；\n2.数据的准备工作简单， 能够同时处理数据型和常规型属性，其他的技术往往要求数据属性的单一。\n3.对中间值得确实不敏感，比较适合处理有缺失属性值的样本，能够处理不相关的特征；\n4.应用范围广，可以对很多属性的数据集构造决策树，可扩展性强。决策树可以用于不熟悉的数据集合，并从中提取出一些列规则 这一点强于KNN。\n缺点：\n1.容易出现过拟合；\n2.对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征。\n3. 信息缺失时处理起来比较困难。 忽略数据集中属性之间的相关性。\nSvm\n优点：\n1.可用于线性/非线性分类，也可以用于回归，泛化错误率低，计算开销不大，结果容易解释；\n2.可以解决小样本情况下的机器学习问题，可以解决高维问题 可以避免神经网络结构选择和局部极小点问题。\n3.SVM是最好的现成的分类器，现成是指不加修改可直接使用。并且能够得到较低的错误率，SVM可以对训练集之外的数据点做很好的分类决策。\n缺点：对参数调节和和函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题。\nLogistic回归：根据现有数据对分类边界线建立回归公式，依次进行分类。\n优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低；\n缺点：容易欠拟合，分类精度可能不高\nEM 期望最大化算法-上帝算法\n只要有一些训练数据，再定义一个最大化函数，采用EM算法，利用计算机经过若干次迭代，就可以得到所需的模型。EM算法是自收敛的分类算法，既不需要事先设定类别也不需要数据见的两两比较合并等操作。缺点是当所要优化的函数不是凸函数时，EM算法容易给出局部最佳解，而不是最优解。\n参考文献：\n机器学习--判别式模型与生成式模型\n数据挖掘十大算法----EM算法（最大期望算法）\n各种分类算法的优缺点\n机器学习&数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）\n吴军．数学之美[M]．北京：人民邮电出版社，2014.\nPeter Harrington，李锐，李鹏，曲亚东，王斌．机器学习实战[M]．北京：人民邮电出版社2013．\n李航．统计学习方法[M]．北京：清华大学出版社 2012．\n杉山将，许永伟.图解机器学习[M]．北京：人民邮电出版社2015．\n斯坦福大学公开课 ：机器学习课程\n编辑于 2016-06-12\n张戎\n数学 话题的优秀回答者\n说一些和题目里面的模型不太有关系的话，介绍一个 KL 散度的运用小场景：\n写过一篇关于 KL 散度的理论＋运用的文章：KL 散度（从动力系统到推荐系统）\n在信息论和动力系统里面，Kullback-Leibler 散度（简称 KL 散度，KL divergence）是两个概率分布 P 和 Q 的一个非对称的度量公式。这个概念是由 Solomon Kullback 和 Richard Leibler 在 1951 年引入的。从概率分布 Q 到概率分布 P 的 KL 散度用 D_{KL}(P||Q) 来表示。尽管从直觉上看 KL 散度是一个度量或者是一个距离，但是它却不满足度量或者距离的定义。例如，从 Q 到 P 的 KL 散度就不一定等于从 P 到 Q 的 KL 散度。本文即将介绍如何将动力系统的概念运用到实际推荐系统的工作中，从而达到更佳的推荐效果。\n详细请见：KL 散度（从动力系统到推荐系统）\n发布于 2016-03-27\n知乎用户\n现代玄学，很多需要依靠经验\n发布于 2014-11-23\niamsile\n算法工程师\n具体问题具体分析。。。。。。\n发布于 2014-11-23\n章鱼小丸子\n不懂算法的产品经理不是好的程序员\n算法是解决方法的数学抽象，一个算法诞生于某个应用场景下，但也可以用在其他应用场景。按场景来分不太合理。\n比如pagerank是用来做网页排序的，有人把它用在文本处理上，发现效果奇好，于是发明了textrank。再比如word2vec是自然语言处理的方法，但有人用它来处理交互数据给微博用户做推荐。\n发布于 2017-03-15\n勤哥哥\n迷惘少年\n随便强答一发吧。说几个自己用过的。\nlr万金油中的万金油，解释性很强。（每个特征可以通过权重来分析重要性，这就是所谓的可解释性）。应用场合很多，例如CTR预估，因为在这个场合下特征纬度很高并且稀疏。很适合用lr。\n当特征维度不高，而且不会稀疏（一般是没onehot），也就是比较稠密的时候用gbdt或者xgboost比较合适。"}
