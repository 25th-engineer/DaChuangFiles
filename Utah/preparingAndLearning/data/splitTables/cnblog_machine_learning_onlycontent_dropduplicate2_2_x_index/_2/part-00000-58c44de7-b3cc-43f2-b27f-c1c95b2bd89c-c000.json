{"content2":"#对coursera上Andrew Ng老师开的机器学习课程的笔记和心得；\n#注:此笔记是我自己认为本节课里比较重要、难理解或容易忘记的内容并做了些补充，并非是课堂详细笔记和要点；\n#标记为<补充>的是我自己加的内容而非课堂内容，参考文献列于文末。博主能力有限，若有错误，恳请指正；\n#---------------------------------------------------------------------------------#\n#下面这个概念对理解机器学习非常有帮助，但是我发现很多小伙伴不了解这个;\n<补充>机器学习三要素-模型(model)、策略(strategy)、算法(algorithm)；\n模型就是所要学习条件概率分布或决策函数，我们常见的一些方法，像隐马模型(HMM)、SVM模型、决策树模型等等都归于此类；\n策略是指按照什么样的准则来学习或者挑选模型，像课上讲的J(Θ)、损失函数属于此类；\n这里的算法是指学习模型的具体计算方法，即用什么样的方法来求得最优解，像课上讲的梯度下降法，其他如牛顿法、拟牛顿法属于此类；\n#---------------------------------------------------------------------------------#\n#回到课堂上讲的。。。\n当一个方法的预测结果明显有问题时，可采用如下方法:\n1，Get more examples ：helps to fix high variance，Not good if you have high bias；\n2，Smaller set of features: fixes high variance (overfitting)，not good if you have high bias;\n3，Try adding additional features: fixes high bias (because hypothesis is too simple, make hypothesis more specific)\n;\n4，Add polynomial terms: fixes high bias problem;\n5，Decreasing λ : fixes high bias;\n6，Increases λ: fixes high variance;\n#---------------------------------------------------------------------------------#\n模型评估与模型选择\n<补充>用训练集来训练模型，验证集用于模型的选择，测试集用于最终对学习方法的评估；\n<补充>用训练误差和测试误差来评估学习方法:\n训练误差对判断给定的问题是否容易学习是有意义的，但本质上不重要；\n测试误差反映了学习方法对未知数据的预测能力，比较两种学习方法的好坏，不考虑计算速度、空间等因素，测试误差小的方法显然更好；\n#---------------------------------------------------------------------------------#\n诊断: bias vs. variance\nx = degree of polynomial d;\ny = error for both training and cross validation (two lines);\nif d is too small --> this probably corresponds to a high bias problem\nif d is too large --> this probably corresponds to a high variance problem\nFor the high bias case, we find both cross validation and training error are high\nDoesn't fit training data well\nDoesn't generalize either\nFor high variance, we find the cross validation error is high but training error is low\nSo we suffer from overfitting (training is low, cross validation is high)\ni.e. training set fits well\nBut generalizes poorly\n#---------------------------------------------------------------------------------#\n学习曲线(learning curve)\n学习曲线可以通过判断模型High bias还是High variance来提高性能；\n，\nsuffering from high bias：需要增加模型复杂度，增加数据无效！\n，\nsuffering from high variance：增加数据有效！也可尝试增加正则项；\n#---------------------------------------------------------------------------------#\n学习器的几个评价指标:\n精确率(precision)\n= true positives / # predicted positive\n= true positives / (true positive + false positive)；\n召回率(recall)\n= true positives / # actual positives\n= true positive / (true positive + false negative)；\nF1值\n= 2 * (PR/ [P + R])，If P = 0 or R = 0 the Fscore = 0；\n精确率与召回率都高，F1值也会高；\n准确率(accuracy)\n= (true positives + true negative)/ # total dataset\n= (true positives + true negative)/ (true positive + true negative + false positive + false negative)；\n#---------------------------------------------------------------------------------#\n平衡(trade off)精确率和召回率：很多时候我们需要平衡精确率和召回率；\n例子：\nTrained a logistic regression classifier\nPredict 1 if hθ(x) >= 0.5\nPredict 0 if hθ(x) < 0.5\n调整阈值对精确率和召回率的影响见下图：\n#---------------------------------------------------------------------------------#\n参考文献:\n《统计学习方法》，李航著；\n《machine learning》, by Tom Mitchell；\ncouresra课程: standford machine learning, by Andrew Ng；"}
