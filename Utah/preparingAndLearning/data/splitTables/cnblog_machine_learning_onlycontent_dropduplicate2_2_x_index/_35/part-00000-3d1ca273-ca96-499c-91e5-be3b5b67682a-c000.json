{"content2":"1.人工智能的核心 --》 机器学习 --》 深度学习 --》 神经网络\n2.深度学习的工作流程：\na。训练样本\nb。特征抽取\nc。学习函数\nd。预测\n3.人工智能的运用范围： 自然语言处理 ； 计算机视觉 等\n4.深度学习的框架：\na。caffe：不需要写代码；1.数据处理；2.定义网络；3.指定参数；4.训练模型\nb。tensorflow：google开发的，需要写代码（python）\n5.python在数据科学方面需要用到的库：\na。Numpy：科学计算库\nb。Pandas：数据分析处理库\nc。Matplotlib：数据可视化库\nd。Scikit-learn:机器学习库\n6.深度学习路线图       人工智能--深度学习---神经网络\n基础篇：\nPython 编程，微积分基础、线性代数基础以及统计学基础\n数学基础：线性代数、微积分、概率论、统计学\npython：数据分析\n进阶篇\n机器学习：监督学习，非监督学习，增强学习和深度学习\n各种算法\n高级篇\n项目\n出师篇\n应用很广\n发展方向 也很广\n7.机器学习的一般框架：\n训练集=》提取特征向量=》结合一定的算法（分类器：比如决策数，KNN）=》得到结果\n8.\n9.《周志华  机器学习》笔记\n1.没有免费的午餐定理：\n在机器学习中存在一个普适定理--没有免费的午餐(No Free Lunch Theorem，NFL定理)。NFL定理的具体描述为\n1）对所有可能的的目标函数求平均，得到的所有学习算法的“非训练集误差”的期望值相同;\n2）对任意固定的训练集，对所有的目标函数求平均，得到的所有学习算法的“非训练集误差”的期望值也相同;\n3）对所有的先验知识求平均，得到的所有学习算法的的“非训练集误差”的期望值也相同;\n4）对任意固定的训练集，对所有的先验知识求平均，得到的所有学习算法的的“非训练集误差”的期望值也相同;\nNFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。\n平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。\nNFL定理可以进一步的引出一个普适的“守恒率”--对每一个可行的学习算法来说，它们的性能对所有可能的目标函数的求和结果确切地为零。即我们要想在某些问题上得到正的性能的提高，必须在一些问题上付出等量的负的性能的代价！比如时间复杂度和空间复杂度。\n实际上，NFL定理并不是局限在机器学习领域，在我们所处的现在这个已知的宇宙中，NFL定理也总是成立的，就像能量守恒。\n2.奥卡姆剃刀定律\n别称：奥康的剃刀；简单性原则\n内容：如无必要，勿增实体\n切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。\n如今奥卡姆剃刀常用于两种或两种以上假说的取舍上：如果对于同一现象有两种或多种不同的假说，我们应该采取比较简单或可证伪的那一种，世界客观存在即是建立在客观实践之上，正所谓实践是检验真理的唯一标准。\n对奥卡姆剃刀在机器学习领域的作用一直存在争议，它并非科学研究中唯一可行的假设选择原则，比如还有“多释原则”：主张保留与经验观察一致的所有假设，这与集成学习方面的研究更加吻合。\n3.\n错误率：分类错误的样本/样本总数\n误差：学习器的实际预测输出与样本的真实输出之间的差异。\n精度：1-错误率\n训练误差（经验误差）：在训练样本上的误差。\n泛化误差：在新样本上的误差\n过拟合（过配）：把训练样本学习的太好了。把训练样本自身的一些特点当作所有潜在样本都会有的一般性质。\n欠拟合（欠配）：对训练样本的一般性质尚未学好。\n相同数据，不同算法，能得出不同的学习器（模型、经验）。\n不同数据，相同算法，能得出不同的学习器（模型、经验）。\n模型选择：那我们该选择哪一种学习算法，使用哪一种参数配置呢？解决方案是：对几个模型的泛化误差进行评估，选择泛化误差最小的那个模型。\n泛化误差难获取，训练误差又受过拟合影响，所有，评估方法一般采用测试集的测试误差来估计泛化误差的近似。\n“留出法”：直接将数据集D划分成俩个互斥的集合，一个是训练集S，一个是测试集T。比率大约2/3~4/5之间。然后重复做个n次，取这n次的一个平均值。\n交叉验证法（k折交叉验证）：将数据集D划分为k个大小相似的互斥子集，然后用k-1个子集做训练集，余下的那个子集做测试集，最后得到k个测试结果，去平均值。k一般有10，5，20。\n当k=样本个数时，叫“留一法”。\n自助法："}
