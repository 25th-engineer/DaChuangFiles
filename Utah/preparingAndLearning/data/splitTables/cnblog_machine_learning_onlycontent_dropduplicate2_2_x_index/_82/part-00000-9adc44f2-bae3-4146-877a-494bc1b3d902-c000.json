{"content2":"C4.5算法\nC4.5算法的核心思想是ID3算法，是ID3算法的改进：\n用信息增益率来选择属性，克服了用信息增益来选择属性时变相选择取值多的属性的不足；\n在树的构造过程中进行剪枝；\n能处理非离散化数据；\n能处理不完整数据。\n优点：\n产生的分类规则易于理解，准确率高。\n缺点：\n在构造过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；\nC4.5算法只适合于能够驻留内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。\nK-means算法\n简单的聚类，吧n个对象根据他们的属性分为k个类，k<n。\n算法的核心是要优化失真函数J，使其收敛到局部最小值而不是全局最小值：\n\\[J=\\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} || x_n - u_k ||^2,\\]\n\\(r_{nk}\\)表示n数据第k个类，\\(u_k\\)是第k个类中心值。\n然后求出最优的\\(u_k\\)：\n\\[u_k=\\frac{\\sum r_{nk} x_n}{\\sum_{n} r_{nk} }\\]\n优点：\n算法速度快。\n缺点：\n分组的数目k是一个输入参数，不适合的k可能返回较差的结果。\n朴素贝叶斯算法\n朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。\n算法的基础是概率问题,分类原理是通过某对象的先验概率,利用贝叶斯公式计算出其后验概率,即该对象属于某一类的概率,选择具有最大后验概率的类作为该对象所属的类。\n朴素贝叶斯假设是约束性很强的假设,假设特征条件独立,但朴素贝叶斯算法简单,快速, 具有较小的出错率。\n在朴素贝叶斯的应用中,主要研究了电子邮件过滤以及文本分类研究。\nK最近邻算法\n缺点：\nK值需要预先设定，而不能自适应\n当样本不平衡时，如一个类的样本容量很大，二其他类样本容量很小，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。\n该算法适用于对样本容量比较大的类域进行自动分类。\nEM最大期望算法\nEM算法是基于模型的聚类算法，是在概率模型中寻找参数最大思然估计的算法，其中概率模型依赖于无法观测的隐藏变量。\nE步估计隐含变量，M步估计其他参数，交替将极值推向最大。\nEM算法比K-means算法计算复杂，收敛较慢，不适合大规模数据集和高维数据，但比K-means算法计算结构稳定、准确。\nEM算法经常用在机器学习和计算机视觉的数据集聚（data clustering）领域。\nPageRank算法\nGoogle的页面排序算法。\n基于从许多优质的网页链接过来的 网页,必定还是优质网页的回归关系,来判定所有网页的重要性。\n一个人有越多牛逼的朋友，他牛逼的概率就越大。\n优点：\n完全独立于查询，只依赖于网页链接结构，可以离线计算。\n缺点：\nPageRank算法忽略了网页搜索的时效性；\n旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的网页排名却很低，因为它们几乎没有in-links。\nAdaBoost\nAdaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。\n算法本事该百诺数据分布来实现的，它根据每次训练集中每一个样本的分类是否正确，以及上一次的总体分类准确率，来确定没个样本的权值。\n将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。\n算法流程：\n先通过对N个训练样本的学习得到第一个弱分类器；\n将分错的样本和其他的新数据一起构成一个新的N个训练样本，通过学习得到第二个弱分类器；\n讲前面都分错的样本加上新的样本构成另一个新的N个训练样本集，通过学习得到第三个弱分类器；\n如此反复，最终得到经过提升的强分类器。\n目前 AdaBoost 算法广泛的应用于人脸检测、目标识别等领域。\nApriori算法\nApriori算法是一种挖掘关联规则的算法，用于挖掘其内涵的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法。\nApriori算法的两个阶段：\n寻找频繁项集；\n有频繁项集找关联规则。\n算法缺点：\n在每一步产生侯选项目集时循环产生的组合过多,没有排除 不应该参与组合的元素;\n每次计算项集的支持度时,都对数据库中的全部记录进行了一遍扫描比较,需要很大的I/O 负载。\nSVM支持向量机\n支持向量机是一种基于分类边界的方法。\n基本原理：\n如果训练数据分布在二维平面上的点,它们按照其分类 聚集在不同的区域。\n基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界。\n对于多维数据（N维），可以将他们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面。\n线性分类器使用超平面类型的边界，非线性分类器使用超曲面。\n支持向量机的原理是将低维空间的点映射到高维空间,使它们成为线性可分,再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分,而在原有的数据空间中,是一种非线性划分。\nCART树\n决策树的分类方法，基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。\n如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。\n优点：\n非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。\n面对存在缺失值、变量数多等问题时，CART数显得非常稳健。"}
