{"content2":"了解更多技术文章请点击原文链接\n随着科学技术的发展以及硬件计算能力的大幅提升，人工智能已经从几十年的幕后工作一下子跃入人们眼帘。人工智能的背后源自于大数据、高性能的硬件与优秀的算法的支持。2016年，深度学习已成为Google搜索的热词，随着最近一两年的围棋人机大战中，阿法狗完胜世界冠军后，人们感觉到再也无法抵挡住AI的车轮的快速驶来。在2017年这一年中，AI已经突破天际，相关产品也出现在人们的生活中，比如智能机器人、无人驾驶以及语音搜索等。最近，世界智能大会在天津举办成功，大会上许多业内行家及企业家发表自己对未来的看法，可以了解到，大多数的科技公司及研究机构都非常看好人工智能的前景，比如百度公司将自己的全部身家压在人工智能上，不管破釜沉舟后是一举成名还是一败涂地，只要不是一无所获就行。为什么突然之间深度学习会有这么大的效应与热潮呢？这是因为科技改变生活，很多的职业可能在今后的时间里慢慢被人工智能所取代。全民都在热议人工智能与深度学习，就连Yann LeCun大牛都感受到了人工智能在中国的火热!\n言归正传，人工智能的背后是大数据、优秀的算法以及强大运算能力的硬件支持。比如，英伟达公司凭借自己的强大的硬件研发能力以及对深度学习框架的支持夺得世全球最聪明的五十家公司榜首。另外优秀的深度学习算法有很多，时不时就会出现一个新的算法，真是令人眼花缭乱。但大多都是基于经典的算法改进而来，比如卷积神经网络（CNN）、深度信念网络（DBN）、循环神经网络（RNN）等等。\n本文将介绍经典的网络之循环神经网络（RNN），这一网络也是时序数据的首选网络。当涉及某些顺序机器学习任务时，RNN可以达到很高的精度，没有其他算法可以与之一较高下。这是由于传统的神经网络只是具有一种短期记忆，而RNN具有有限的短期记忆的优势。然而，第一代RNNs网络并没有引起人们着重的注意，这是由于研究人员在利用反向传播和梯度下降算法过程中遭受到了严重的梯度消失问题，阻碍了RNN几十年的发展。最后，于90年代后期出现了重大突破，导致更加准确的新一代RNN的问世。基于这一突破的近二十年，直到Google Voice Search和Apple Siri等应用程序开始抢夺其关键流程，开发人员完善和优化了新一代的RNN。现在，RNN网络遍布各个研究领域，并且正在帮助点燃人工智能的复兴之火。\n与过去有关的神经网络（RNN）\n大多数人造神经网络，如前馈神经网络，都没有记忆它们刚刚收到的输入。例如，如果提供前馈神经网络的字符“WISDOM”，当它到达字符“D”时，它已经忘记了它刚刚读过字符“S”，这是一个大问题。无论训练该网络是多么的辛苦，总是很难猜出下一个最有可能的字符“O”。这使得它成为某些任务的一个相当无用的候选人，例如在语音识别中，识别的好坏在很大程度上受益于预测下一个字符的能力。另一方面，RNN网络确实记住了之前的输入，但是处于一个非常复杂的水平。\n我们再次输入“WISDOM”，并将其应用到一个复发性网络中。RNN网络中的单元或人造神经元在接收到“D”时也将其之前接收到的字符“S”作为其输入。换句话说，就是把刚刚过去的事情联合现在的事情作为输入，来预测接下来会发生的事情，这给了它有限的短期记忆的优势。当训练时，提供足够的背景下，可以猜测下一个字符最有可能是“O”。\n调整和重新调整\n像所有人工神经网络一样，RNN的单元为其多个输入分配一个权重矩阵，这些权重代表各个输入在网络层中所占的比重；然后对这些权重应用一个函数来确定单个输出，这个函数一般被称为损失函数（代价函数），限定实际输出与目标输出之间的误差。然而，循环神经网络不仅对当前输入分配权重，而且还从对过去时刻输入分配权重。然后，通过使得损失函数最下来动态的调整分配给当前输入和过去输入的权重，这个过程涉及到两个关键概念：梯度下降和反向传播（BPTT）。\n梯度下降\n机器学习中最著名的算法之一就是梯度下降算法。它的主要优点在于它显着的回避了“维数灾难”。什么是“维数灾难”呢，就是说在涉及到向量的计算问题中，随着维数的增加，计算量会呈指数倍增长。这个问题困扰着诸多神经网络系统，因为太多的变量需要计算来达到最小的损失函数。然而，梯度下降算法通过放大多维误差或代价函数的局部最小值来打破维数灾难。这有助于系统调整分配给各个单元的权重值，以使网络变得更加精确。\n通过时间的反向传播\nRNN通过反向推理微调其权重来训练其单元。简单的说，就是根据单元计算出的总输出与目标输出之间的误差，从网络的最终输出端反向逐层回归，利用损失函数的偏导调整每个单元的权重。这就是著名的BP算法，关于BP算法可以看本博主之前的相关博客。而RNN网络使用的是类似的一个版本，称为通过时间的反向传播（BPTT）。该版本扩展了调整过程，包括负责前一时刻（T-1）输入值对应的每个单元的记忆的权重。\nYikes：梯度消失问题\n尽管在梯度下降算法和BPTT的帮助下享有一些初步的成功，但是许多人造神经网络（包括第一代RNNs网络），最终都遭受了严重的挫折——梯度消失问题。什么是梯度消失问题呢，其基本思想其实很简单。首先，来看一个梯度的概念，将梯度视为斜率。在训练深层神经网络的背景中，梯度值越大代表坡度越陡峭，系统能够越快地下滑到终点线并完成训练。但这也是研究者陷入困境的地方——当斜坡太平坦时，无法进行快速的训练。这对于深层网络中的第一层而言特别关键，因为若第一层的梯度值为零，说明没有了调整方向，无法调整相关的权重值来最下化损失函数，这一现象就是“消梯度失”。随着梯度越来越小，训练时间也会越来越长，类似于物理学中的沿直线运动，光滑表面，小球会一直运动下去。\n大的突破：长短期记忆（LSTM）\n在九十年代后期，一个重大的突破解决了上述梯度消失问题，给RNN网络发展带来了第二次研究热潮。这种大突破的中心思想是引入了单元长短期记忆（LSTM）。\nLSTM的引入给AI领域创造了一个不同的世界。这是由于这些新单元或人造神经元（如RNN的标准短期记忆单元）从一开始就记住了它们的输入。然而，与标准的RNN单元不同，LSTM可以挂载在它们的存储器上，这些存储器具有类似于常规计算机中的存储器寄存器的读/写属性。另外LSTM是模拟的，而不是数字，使得它们的特征可以区分。换句话说，它们的曲线是连续的，可以找到它们的斜坡的陡度。因此，LSTM特别适合于反向传播和梯度下降中所涉及的偏微积分。\n总而言之，LSTM不仅可以调整其权重，还可以根据训练的梯度来保留、删除、转换和控制其存储数据的流入和流出。最重要的是，LSTM可以长时间保存重要的错误信息，以使梯度相对陡峭，从而网络的训练时间相对较短。这解决了梯度消失的问题，并大大提高了当今基于LSTM的RNN网络的准确性。由于RNN架构的显著改进，谷歌、苹果及许多其他先进的公司现在正在使用RNN为其业务中心的应用提供推动力。\n总结\n循环神经网络（RNN）可以记住其以前的输入，当涉及到连续的、与上下文相关的任务（如语音识别）时，它比其他人造神经网络具有更大的优势。\n关于RNN网络的发展历程：第一代RNNs通过反向传播和梯度下降算法达到了纠正错误的能力。但梯度消失问题阻止了RNN的发展；直到1997年，引入了一个基于LSTM的架构后，取得了大的突破。\n新的方法有效地将RNN网络中的每个单元转变成一个模拟计算机，大大提高了网络精度。\n作者信息\nJason Roell：软件工程师，热爱深度学习及其可改变技术的应用。\nLinkedin：http://www.linkedin.com/in/jason-roell-47830817/\n本文由北邮@爱可可-爱生活老师推荐，阿里云云栖社区组织翻译。\n文章原标题《Understanding Recurrent Neural Networks: The Preferred Neural Network for Time-Series Data》，作者：Jason Roel，译者：海棠，审阅：袁虎\n附件为原文的pdf\n文章为简译，更为详细的内容，请查看原文\n了解更多技术文章请点击原文链接"}
