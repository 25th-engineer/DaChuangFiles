{"content2":"1、介绍\n决策树（decision tree）是一种有监督的机器学习算法，是一个分类算法。在给定训练集的条件下，生成一个自顶而下的决策树，树的根为起点，树的叶子为样本的分类，从根到叶子的路径就是一个样本进行分类的过程。\n下图为一个决策树的例子，见http://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91\n可见，决策树上的判断节点是对某一个属性进行判断，生成的路径数量为该属性可能的取值，最终到叶子节点时，就完成一个分类（或预测）。决策树具有直观、易于解释的特性。\n2、决策树生成算法\n本文主要讨论如何由一个给定的训练集生成一个决策树。如果都一个数据集合$D$，其特征集合为$A$，那么以何种顺序对A中的特征进行判断就成为决策树生成过程中的关键。首先给出一个决策树生成算法-ID3算法（参考《统计学习方法》李航著）\n--------------------我是算法开始分割线-------------------------------------------\nID3算法：\n输入：训练数据集D，特征集A，阈值e\n输出：决策树T\n（1）若D中所有样本属于同一类Ck，则T为单节点树，并将类Ck作为该节点的类标记，返回T；\n（2）A为空集，T为单节点树，将D中实例数最大的类Ck作为该节点的类标记，返回T；\n（3）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征值Ag；\n（4）如果Ag<e，则置T为单节点树，将D中实例数最大的类Ck作为该节点的类标记，返回T；\n（5）否则，对Ag的每一个可能的取值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T；\n（6）对第i个子节点，以Di为训练集，以 A-{Ag}为特征集，递归调用（1）~（5）步，得到子树Ti，返回Ti。\n--------------------我是算法结束分割线-------------------------------------------\n算法第（3）步中，信息增益是评估每一个特征值对D的划分效果，划分的原则为将无序的数据变得尽量有序。评价随机变量不确定性的一个概念是熵，熵越大，不确定性越大。如果确定一个特征Ag，在确定该特征前后，D的熵的变化值就是特征Ag的信息增益。\n3、熵及信息增益\n熵：\n设X是一个取有限个值（n）的离散随机变量，其概率分布为\n\\[P(X=x_{i})=P_{i}, i=1,2,...,n\\]\n则随机变量X的熵定义为\n\\[H(x) =  - \\sum\\limits_{i = 1}^n {{P_i}\\log {P_i}} \\]\n信息增益：\n训练集为\\(D\\)，\\(|D|\\)为样本容量，设有k个类\\({C_k}\\),k=1,...k, \\({|C_k|}\\)为类\\({C_k}\\)的样本个数，且有\\(\\sum\\limits_{i = 1}^k {|{C_k}|}  = |D|\\)\n设特征A有n个不同取值\\(\\{ {a_{1,}}{a_2}, \\cdots ,{a_n}\\} \\)  ，根据A的值，将D划分为n个子集\\({D_1},{D_2}, \\cdots ,{D_n}\\)， \\({|D_i|}\\)为\\({D_i}\\) 的样本数，\\(\\sum\\limits_{i = 1}^n {|{D_i}|}  = |D|\\)。\n记子集\\({D_i}\\)中属于类\\({C_k}\\)的样本集合为\\({D_{ik}}\\)，即\\({D_{ik}} = {D_i} \\cap {C_k}\\)。\n\\({|D_{ik}|}\\)为\\({D_{ik}}\\)的样本个数。\n（1）数据集D的经验熵H(D)\n\\[H(D) =  - \\sum\\limits_{k = 1}^K {\\frac{{|{C_k}|}}{{|D|}}{{\\log }_2}} \\frac{{|{C_k}|}}{{|D|}}\\]\n（2）特征A对数据集D的经验条件熵H(D|A)\n\\[H(D|A) = \\sum\\limits_{i = 1}^n {\\frac{{|{D_i}|}}{{|D|}}H({D_i}) =  - } \\sum\\limits_{i = 1}^n {\\frac{{|{D_i}|}}{{|D|}}\\sum\\limits_{k = 1}^K {\\frac{{|{D_{ik}}|}}{{|{D_i}|}}} } {\\log _2}\\frac{{|{D_{ik}}|}}{{|{D_i}|}}\\]\n（3）计算信息增益\n\\[g(D,A) = H(D) - H(D|A)\\]\n信息增益越大，表示A对D趋于有序的贡献越大。\n-------------------------------分割线------------------------------------------------\n决策树的R语言实现如下：\nlibrary(plyr)\n# 测试数据集 http://archive.ics.uci.edu/ml/datasets/Car+Evaluation\n##计算训练集合D的熵H（D）\n##输入：trainData 训练集，类型为数据框\n##      nClass 指明训练集中第nClass列为分类结果\n##输出：训练集的熵\ncal_HD <- function(trainData, nClass){\nif ( !(is.data.frame(trainData) & is.numeric(nClass)) )\n\"input error\"\nif (length(trainData) < nClass)\n\"nClass is larger than the length of trainData\"\nrownum <- nrow(trainData)\n#对第nClass列的值统计频数\ncalss.freq <- count(trainData,nClass)\n#计算每个取值的  概率*log2(概率)\ncalss.freq <- mutate(calss.freq, freq2 = (freq / rownum)*log2(freq / rownum))\n-sum(calss.freq[,\"freq2\"])\n#使用arrange代替order，方便的按照多列对数据框进行排序\n#mtcars.new2 <- arrange(mtcars, cyl, vs, gear)\n}\n#cal_HD(mtcars,11)\n##计算训练集合D对特征值A的条件熵H（D|A）\n##输入：trainData 训练集，类型为数据框\n##      nClass 指明训练集中第nClass列为分类结果\n##      nA 指明trainData中条件A的列号\n##输出：训练集trainData对特征A的条件熵\ncal_HDA <- function(trainData, nClass, nA){\nrownum <- nrow(trainData)\n#对第nA列的特征A计算频数\nnA.freq <- count(trainData,nA)\ni <- 1\nsub.hd <- c()\nfor (nA.value in nA.freq[,1]){\n#取特征值A取值为na.value的子集\nsub.trainData <- trainData[which(trainData[,nA] == nA.value),]\nsub.hd[i] <- cal_HD(sub.trainData,nClass)\ni <- i+1\n}\nnA.freq <- mutate(nA.freq, freq2 = (freq / rownum)*sub.hd)\nsum(nA.freq[,\"freq2\"])\n}\n##计算训练集合D对特征值A的信息增益g(D,A)\n##输入：trainData 训练集，类型为数据框\n##      nClass 指明训练集中第nClass列为分类结果\n##      nA 指明trainData中特征A的列号\n##输出：训练集trainData对特征A的信息增益\ng_DA <- function(trainData, nClass, nA){\ncal_HD(trainData, nClass) - cal_HDA(trainData, nClass, nA)\n}\n##根据训练集合生成决策树\n##输入：trainData 训练集，类型为数据框\n##      strRoot 指明根节点的属性名称\n##      strRootAttri 指明根节点的属性取值\n##      nClass 指明训练集中第nClass列为分类结果\n##      cAttri 向量，表示当前可用的特征集合，用列号表示\n##      e 如果特征的最大信息增益小于e，则剩余作为一个分类，类频数最高的最为分类结果\n##输出：决策树T\ngen_decision_tree <- function(trainData, strRoot, strRootAttri, nClass, cAttri, e){\n# 树的描述，（上级节点名称、上级节点属性值、自己节点名称，自己节点的取值）\ndecision_tree <- data.frame()\nnClass.freq <- count(trainData,nClass)   ##类别出现的频数\nnClass.freq <- arrange(nClass.freq, desc(freq))  ##按频数从低到高排列\ncol.name <- names(trainData) ##trainData的列名\n##1、如果D中所有属于同一类Ck，则T为单节点树\nif nrow(nClass.freq) == 1{\nrbind(decision_tree, c(strRoot, strRootAttri, nClass.freq[1,1], ''))\nreturn decision_tree\n}\n##2、如果属性cAttri为空，将D中频数最高的类别返回\nif length(cAttri) == 0{\nrbind(decision_tree, c(strRoot, strRootAttri, nClass.freq[1,1], ''))\nreturn decision_tree\n}\n##3、计算cAttri中各特征值对D的信息增益，选择信息增益最大的特征值Ag及其信息增益\nmaxDA <- 0    #记录最大的信息增益\nmaxAttriName <- ''   #记录最大信息增益对应的属性名称\nmaxAttriIndex <- ''   #记录最大信息增益对应的属性列号\nfor(i in cAttri){\ncurDA <- g_DA(trainData,nClass,i)\nif (maxDA <= curDA){\nmaxDA <- curDA\nmaxAttriName <- col.name[i]\n}\n}\n##4、如果最大信息增益小于阈值e，将D中频数最高的类别返回\nif (maxDA < e){\nrbind(decision_tree, c(strRoot, strRootAttri, nClass.freq[1,1], ''))\nreturn decision_tree\n}\n##5、否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di\n##   将Di中实例数最大的类作为标记，构建子节点\n##   由节点及其子节点构成树T，返回T\nfor (oneValue in unique(trainData[,maxAttriName])){\nsub.train <- trainData[which(trainData[,maxAttriName] == oneValue),]  #Di\n#sub.trian.freq <- count(sub.train,nClass)   ##类别出现的频数\n#sub.trian.freq <- arrange(sub.trian.freq, desc(freq))  ##按频数从低到高排列\nrbind(decision_tree, c(strRoot, strRootAttri, maxAttriName , oneValue))\n##6、递归构建下一步\n# 剔除已经使用的属性\nnext.cAttri <- cAttri[which(cAttri !=maxAttriIndex)]\n# 递归调用\nnext.dt <-gen_decision_tree(sub.train, maxAttriName,\noneValue, nClass, next.cAttri, e)\nrbind(decision_tree, next.dt)\n}\nnames(decision_tree) <- c('preName','preValue','curName','curValue')\ndecision_tree\n}\n---------------决策树总结-------------------\n1、R中有实现决策树算法的包rpart，和画出决策树的包rpart.plot，本例自己实现决策树算法是为了更好的理解。\n2、由于决策树只能处理离散属性，因此连续属性应首先进行离散化。\n3、决策树易于理解，对业务的解释性较强。\n4、ID3算法容易引起过拟合，需考虑树的剪枝。"}
