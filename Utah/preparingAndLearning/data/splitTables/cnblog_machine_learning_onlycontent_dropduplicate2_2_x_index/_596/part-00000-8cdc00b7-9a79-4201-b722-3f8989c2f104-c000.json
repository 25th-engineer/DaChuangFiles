{"content2":"本文对这篇论文的简单描述\nOptimization Methods for Large-Scale Machine Learning author:Leon Bottou Frank E. Curtisy Jorge Nocedalz\n1.Introduction\n随着大数据时代到来，尽管计算机硬件条件的改善，对于机器学习算法效率的要求并不会降低，而机器学习算法效率更多地依赖于数值优化方法的改进，因而有必要对近年来关于大规模机器学习中的优化算法做一个总结，以便更好地理清思路，确定未来算法的改进方向。\n本文尝试解答以下问题：\n1.  优化问题怎样在机器学习应用产生，以及面对怎样的挑战？\n2.  在大规模数据应用中最广泛使用的优化方法？\n3.  近年来关于优化方法设计上的进步，及在此领域的公开问题？\n通过机器学习中的两个例子尝试解答第一个问题，文本分类问题和深度神经网络感知问题；通过介绍随机梯度法的基本定理以及实际应用来说明第二个问题；第三步主要介绍noise reduction method 和 the second-order method 以及关于正则模型的方法来说明第三个问题\n2. machine learning case studies\n2.1 文本分类\n确定文本类别是自然语言处理的一个基本问题，比如我正在写的这篇文章应该归到哪个类别。显然人工分类对于小样本文本非常有效，但是数量激增的话，远非人力所能处理，基于自然语言处理方面的知识，我们可以用向量来表示一篇文档，那么问题就转化为基本的机器学习分类问题。\n比如n个样本{(x1,y1),(x2,y2),...,(xn,yn)}，xi 表示第i个文档向量，yi表示第i个文档所属类别。通过样本数据我们学习一个预测函数h，来对未分类样本进行预测，那么我们用经验风险误差Rn(h)来评价预测学习的好坏\n通常我们可以将预测函数表示成   w表示h需要学习的参数，t表示一个偏置项，同时我们需要一个合适的损失函数，比如来替换(2.1)中的1，这样问题就变成一个比较纯粹的优化问题\n，其中右边那项为正则项，防止模型太复杂过拟合。\n2.2 深度神经网络\n深度神经网络受启发于生物学上的神经学，希望通过计算机来模拟人脑的学习过程。\n2.3 总的来说\n问题描述：一个训练集 ,   损失函数 , (可以为逻辑损失，合页损失等等) ，预测函数h(w;x)\n要解决 ，记  ，  实际应用一般为经验风险，理论分析使用期望风险，通常经验风险容易过拟合，需要加入正则项来泛化模型，即常用的结构风险最小化。即(2.3)所示。\n3. overview of optimization methods\n3.1 stochastic gradient method(SG)\n首先表示经验风险，，f是l与h的复合函数，那么w的更新公式为\n可以看出SG方法每次迭代只需计算一个梯度值，随机过程依赖于i_k的选取，不是梯度下降法，在期望上是下降的。\n3.2 batch optimization methods\nw的更新公式为\n每次迭代需计算的数量与n成比例，每次迭代代价比SG要高，可以考虑并行化，并且有很多以此衍生的算法。\n3.3  motivation for stochastic methods\n直观上看：  batch的数量增加，SG的还是一样，对于n非常大，SG比较有利。\n实践：\nSG初始收敛速度非常快\n例子\n当SG迭代到w*附近时，就不太确定继续迭代方向，这时速度就会变得非常慢。\n3.4 Theoretical motivation\nbatch 方法在满足一定假设下可以达到线性收敛（几何收敛），每次迭代代价正比于n\nSG可以同时实现R和Rn 次线性收敛 ， 每次迭代与n无关\n，\n两者总的复杂度\n当 R(w)-R(w*)<e\n4. Analyses of Stochastic Gradient Methods\n不失一般性，将期望风险R(w)和经验风险Rn(w)的目标函数表示如下\n本节主要讨论SG算法的收敛性及迭代上界\n以上算法就称之为SG， g主要为三种形式\n4.1 两个基本引理\n通常SG的收敛性证明，需要目标函数F的光滑性、假设F的梯度利普希茨连续，\n根据这个猜想得出一个重要的不等式\n由算法4.1知道 w(k+1)依赖于   ，{}可以看成随机种子，可以看成依赖于w(k)的分布，每次根据随机种子来挑选g，\n对不等式取期望就得到(4.4) ，不等式左边第一项可下降，而第二项相当于噪音，干扰收敛。\n猜想2\n引理2\nSG方法的收敛依赖于不等式右边两项的协调。\n4.2  SG for Strong Convex Objectives\n强凸目标函数在一些优化文献里常常被讨论，不仅由于函此能得到较强的结论，实际实践也会遇到(如机器学习中的正则项)\n强凸函数定义\n如意得到\n其中c<=L\n定步长的收敛定理\n当M=0时对于batch梯度法有线性收敛速度。M>0时收敛，存在噪声阻止收敛到最优解。\n减小步长收敛定理4.7\n考虑mini-batch与  single example 的比较\n虽然batch每次迭代代价比single 大，但是可以使用较大步长，收敛较快。\n4.3 SG for General Objectives\n非凸函数可能有多个局部最优解，要达到全局最优，相对较难。\n定步长\n步长减小\n4.4 work complexity for large-scale learning\n数据越大，往往能更好拟合数据，且不会过拟合，然而需要更多的训练时间。\n定性分析，  是经验风险最小值点的逼近，花H是预测函数族，app--approximation error,  est--estimation error,  opt--optimization error\n那么逼近误差， Tmax 为时间预算\n经过分析  T（n,e) 表示迭代次数， e*表示所能达到的最小逼近误差\n4.5 commentary\n1 SG在渐近收敛是速度非常慢，对步长要求严格\n2\n5. noise reduction methods\n从SG方法出发可以衍生出多种方法，从不同方面来提升SG的稳定性及效率。\n根据上面提到的噪声，本节主要介绍减小噪声的方法，来提高收敛速度。\n主要3种方法 dynamic sampling，gradient aggregation，iterate averaging .\n1. dynamic sampling\n首先是前面的假设：\n有定理\n要使得成立，  令\n就有\n可以看到每次迭代的样本大小是增加的，总的复杂度\n与SG的复杂度是一样的，为 O(1/e),  特别的取\n实际上该方法不常采用。\n2. gradient  aggregation\n主要是对梯度计算的处理来纠正梯度方向。 简介两种方法\nSVRG  (stochastic variance reduced gradient) 在内循环里随机抽取梯度，纠正偏差。 g(~)是  Rn梯度的无偏估计。\nSAGA (stochastic average gradient)\n同样是线性收敛，g是Rn梯度的无偏估计。\n3. iterative averaging\n步长一般取下降速度为， 次线性收敛速度。\n6. second-order methods"}
