{"content2":"写在前面： 想了一下，这门MOOC可能更适合大一、大二的本科生，和跨专业学生吧。\n本文属于知识分享。有兴趣的朋友可以结合MOOC看这篇“音译”。\nP5 C10.2——15分59秒\n下面，我们讲第十章第一节，Classification，分类。\n我们首先给出分类的定义，根据定义描述的长度，我们给出三种定义。\n首先给出一个较长的定义。分类是这样一种任务，它基于已知类别的训练数据集，来辨识新的观测数据属于哪一组类别。\n较短的描述，分类用于解决这样一些问题，其中输出被分为两个或多个类别。\n最短的描述，分类是对每个输入数据下指定一个类别。\n这三种定义尽管描述的长短不同，但他们对分类的要素是相同的。也就是说，对输入数据划分为类别，并且这个类别是已知的。\n第一节分成四个小节。首先我们看一下分类是如何工作的。\n分类需要有一个分类器，Classifier。\n什么是分类器呢？它是一种实现分类功能，尤其是一种具体去实现分类的算法，被称为一个分类器Classifier。\n下面我们介绍一下分类函数，Classifier function。\n分类器这个术语，有时还指的是由分类算法所实现的数学函数，它将输入数据映射为一个类别。\n这张图刻画了分类的训练过程，即Training。\n训练数据是一个已标注的数据，Labeled Data。即这些训练数据的类别是已知的。\n有一个x，通过labeling function，标注得到y。\n标注的目的呢，是创建训练样本。通常采用手工标注的方法。\n训练样本建立好之后，就用来对学习算法f(x)进行训练。\n每次输入一组(x, y)的训练数据。通过训练之后，我们希望得到一个h(x)。h(x)是Hypothesis，假定函数集合当中的一个函数，它也被称为，叫做分类函数。\n这个分类函数h(x)，和我们目标的f(x)之间，要满足具有最小的泛化和经验错误。\n分类算法训练好了之后我们得到了h(x)，就可以用来进行实际的分类了。\n我们如图所示。这个时候的输入数据是未知数据。我们根据已经训练好的假定函数h(x)进行分类，得到相应的 y 的映像，最后得到分类结果。\n我们看，在未知类别的时候，通过分类，得到了已知的类别。\n刚才的两页分别是分类算法的训练和实测的示意图。\n下面给出一种分类的形式化描述。\n设Rn 表示一个n维实数向量集合。输入空间X是Rn 的子集，输出空间 Y ，是一个类别，categories。D是X与Y笛卡尔积上的一个未知分布。\n我们给定一个标注函数labeling function，这个labeling function也可以称其为分类函数classifier function。然后给定一个训练的集合，training dataset，也称为标注好的训练样本集合。训练样本集合S表示为x(i)和y(j)元素的集合。其中(x, y)属于大写的X，Y的笛卡尔乘积，i和j的取值范围分别为1到m和1到n。\n分类算法要给定一个假定函数集合。大写的H是X到Y的一个映射的函数。我们的目的是得到一个假定的函数，属于大写的H集合当中的一个小写的h, h是X到Y的映射。我们的目的是，使得h(x)与我们既定的f(x)之间，具有最小的泛化错误和经验错误。\n把分类算法训练好以后，已经得到了h(x)。我们在实际的分类处理过程中，就是对未知的数据进行实际测试，未知的数据我们可以表示成一个大写的X的集合。我们使用刚才已经训练好的classifier function，小写的h(x)，将X映射到Y的过程。我们通过分类得到这样一个分类的集合，小写的y属于大写的Y的空间，并且呢j属于1到n。它是通过每一个x向量，将其映射到y的向量的这么个过程。其中大写的Y是输出空间，它被称为一个叫做已知的类别的集合。\n下面我们讲第二小节，线性分类和非线性分类。\n首先我们看线性分类。\n所谓线性分类，是通过线性分类器进行分类的。我们如图所示，我们会看到有这样一个线性的将二维数据空间的数据分成两个类别。\n一个线性分类器具有如下两个特征：\n首先，它是一个线性判别函数，此外它还具有一个线性决策边界 linear decision boundary。\n我们给一个简单的线性分类器的案例分析。\n这是一个简单的线性分类器的一个表达式，其中w表示行向量，它是权值的向量，x则表示为列向量，记作这个表达式，其中b则表示偏差值。从右图可以看出，我们这个线性分类函数，当结果等于0的时候恰好为这一条线。大于0和小于0，分别将数据分成两个类，因此它是一个线性的二元分类的例子。\n我们再看一下非线性分类。\n所谓的非线性分类，是通过一个非线性分类器进行分类的。非线性分类器也有如下两个特点：\n第一，   有若干个非线性决定边界。\n二，决定边界很可能是非连续的。\n下面这个图呢，就是一个非线性分类的例子。它通过支持向量机SVM中的和函数，来解决非线性分类问题。关于支持向量机及其和函数的介绍的书籍和参考资料有很多，感兴趣的可以到网上查阅一下。\n那么对这张图我们会看到，这个非线性的分类器的这个曲线，是非连续的。也就是说不是一条曲线可以把它决定下来的。\n下面我们讲第三小节，分类的维度和类的个数。\n首先看一下类的维度。\n如果问题空间是n维的，则它的分类器的维度为n-1的超平面。超平面英文称为hyper-plane。例如，对二维数据来说，它的分类器则是一条线，也就是说，二维的数据的分类器是2-1，是一维的一条线。同样，对于三维的数据来说，它的分类器是一个二维的平面，如右边这两个图所示。\n下面我们再看一下类的个数。\n也就是说通过分类得到的类的个数，或者我们已知的类的个数。\n为了叙述方便起见，我们在这个线性分类器里面增加了两个个下角标k。对于二元分类来说k等于2，当k大于2时，它是一个多元分类的问题。\n左下图是一个二元分类，也就是说通过一条线，把数据分成两个部分。这叫做二元分类。当然对于三维空间的数据来说，它是一个二维的平面。多元分类器右下角这个例子呢，则是一个三元分类，也就是把数据分成三个已知的类别。\n关于分类的算法的介绍，在其他书籍和相应的资料呢，有很多。比如说支持向量机，通过线性回归的方法进行分类等等，算法很多很多。\n我们这里呢，是给一个案例分析，是分析这种叫做Softmax的分类器。\nSoftmax分类器呢，是一个多元分类器，它通过softmax函数来实现多元分类。\nSoftmax函数如中间这个表达式所示。它将一个任意实数值的k维向量x，映射到一个实数值的k维向量 。其中j等于1到k。表示k个维度，也就是说把它分为k个类别。\n在概率论中，softmax的函数可以用来表示一个类的分布，就是说一个涵盖K个不同类别的可能结果的概率分布。如下面这个表达式所示，其中j等于1到k表示它是第j个类别。\nSoftmax函数已经被用于各种多元分类的算法当中。例如在多项式逻辑回归当中。这里面logistic regression用到了regression这个词，regression通常被翻译成回归，我们后面会讲到回归的问题。但在这里是用来做分类的，尽管它的名称里面用到了regression这个词。\n此外多元线性判别分析当中也用到了softmax函数。\n此外还有朴素贝叶斯分类。\n用的比较多的呢，还有人工神经网络，softmax作为人工神经网络的最后一层来进行多元分类。右侧的这个图就是多元神经网络的最后一层和倒数第一层，我们会看到在倒数第一层经过全连接，将结果送到相应的多元分类的神经元上。那么每一个我们会看到当Y=1，2和m的时候，分别会得到它相应的类别。\n下面讲第四小节，分类的应用和主要算法。\n分类的应用，应该是在机器学习当中，最多的了。\n例如像计算机视觉当中，医学影像与图像分析，手写识别与OCR，动作识别与视频跟踪，等等。\n此外在模式识别当中，有人脸识别、语音识别、指纹识别等各种生物识别。\n在自然语音处理当中，也要大量地用到了分类问题。\n此外还有文档分类问题，例如垃圾邮件分类，把它分成垃圾邮件和非垃圾邮件。\n此外在互联网搜索引擎当中也要用到分类的问题。还有信用、评分等等。\n分类的代表性算法有很多。\n常用的有AdaBoost。AdaBoost是adaptive boosting的缩写。Boosting是一类自适应增强算法的名称。此外我们刚才介绍了像人工神经网络，还有k近邻，还有支持向量机等等。"}
