{"content2":"理论推导\n机器学习所针对的问题有两种：一种是回归，一种是分类。回归是解决连续数据的预测问题，而分类是解决离散数据的预测问题。线性回归是一个典型的回归问题。其实我们在中学时期就接触过，叫最小二乘法。\n线性回归试图学得一个线性模型以尽可能准确地预测输出结果。\n先从简单的模型看起：\n首先，我们只考虑单组变量的情况，有：\n使得\n假设有m个数据，我们希望通过x预测的结果f(x)来估计y。其中w和b都是线性回归模型的参数。\n为了能更好地预测出结果，我们希望自己预测的结果f(x)与y的差值尽可能地小，所以我们可以写出代价函数（cost function）如下：\n接着代入f(x)的公式可以得到：\n不难看出，这里的代价函数表示的是预测值f(x)与实际值y之间的误差的平方。它对应了常用的欧几里得距离简称“欧氏距离”。基于均方误差最小化来求解模型的方法我们叫做“最小二乘法”。在线性回归中，最小二乘法实质上就是找到一条直线，使所有样本数据到该直线的欧式距离之和最小，即误差最小。\n我们希望这个代价函数能有最小值，那么就分别对其求w和b的偏导，使其等于0，求解方程。\n先求偏导，得到下面两个式子：\n很明显，公式中的参数m，b，w都与i无关，简化时可以直接提出来。\n另这两个偏导等于0：\n求解方程组，解得：\n这样根据数据集中给出的x和y，我们可以求出w和b来构建简单的线性模型来预测结果。\n接下来，推广到更一般的情况：\n我们假设数据集中共有m个样本，每个样本有n个特征，用X矩阵表示样本和特征，是一个m×n的矩阵：\n用Y矩阵表示标签，是一个m×1的矩阵：\n为了构建线性模型，我们还需要假设一些参数：\n（有时还要加一个偏差（bias）也就是， 为了推导方便没加，实际上结果是一样的）\n好了，我们可以表示出线性模型了：\nh(x)表示假设，即hypothesis。通过矩阵乘法，我们知道结果是一个n×1的矩阵。\n跟前面推导单变量的线性回归模型时一样，列出代价函数：\n这里的1/2并无太大意义，只是为了求导时能将参数正好消掉而加上。\n代价函数代表了误差，我们希望它尽可能地小，所以要对它求偏导并令偏导数为0，求解方程。\n在求偏导之前先展开一下：\n接下来对 求导，先给出几个矩阵求导的公式：\n对代价函数 求关于 的偏导，并令其等于0。\n求偏导。\n套用前面给出的矩阵求导公式。\n最后化简得到：\n好了，另这个偏导数等于0：\n解得：\nOK，推导完毕。\n把知识点梳理一遍发现清楚了很多。写公式真的很累，明天再把线性回归的代码补上。\n(ง •̀_•́)ง"}
