{"content2":"转自：https://blog.csdn.net/china1000/article/details/48597469\n感觉狼厂有些把机器学习和数据挖掘神话了，机器学习、数据挖掘的能力其实是有边界的。机器学习、数据挖掘永远是给大公司的业务锦上添花的东西，它可以帮助公司赚更多的钱，却不能帮助公司在与其他公司的竞争中取得领先优势，所以小公司招聘数据挖掘/机器学习不是为了装逼就是在自寻死路。可是相比JAVA和C++语言开发来说，机器学习/数据挖掘确实是新一些老人占的坑少一些，而且可以经常接触一些新的东西。还是赶紧再次抓住机会集中的再总结一下吧，不能再拖拖拉拉了。\n其实数据挖掘的主要任务是分类、聚类、关联分析、预测、时序模式和偏差分析。本文先系统的介绍一下机器学习中的分类算法，主要目录如下：\n常用分类算法\nBayes\n朴素贝叶斯的优缺点\n朴素贝叶斯的公式\nDecision Tree\n决策树的优缺点\n决策树公式\nSVM\n支持向量机的优缺点\n支持向量机的公式\nKNN\nK近邻的优缺点\nK近邻的公式\nLogistic Regression\n逻辑回归的优缺点\n逻辑回归的公式\n逻辑回归的问题\n神经网络\n神经网络的优缺点\n神经网络公式\n深度学习\nEnsemble learning\nGBDT\nAdaboost\nRandom Forest\n参考文献\n常用分类算法\nBayes\n贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。\n朴素贝叶斯的优缺点\n优点\n所需估计的参数少，对于缺失数据不敏感。\n缺点\n假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。\n需要知道先验概率。\n分类决策错误率。\n朴素贝叶斯的公式\n朴素贝叶斯求解：\nP(C|F1,...,Fn)=p(C)p(F1,...,Fn|C)p(F1,...,Fn)=p(C)∏i=1np(Fi|C)P(C|F1,...,Fn)=p(C)p(F1,...,Fn|C)p(F1,...,Fn)=p(C)∏i=1np(Fi|C)\nDecision Tree\n决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。决策树的每个内部节点表示在一个属性上的测试，每个分枝代表该测试的一个输出，而每个树叶结点存放着一个类标号。\n在决策树算法中，ID3基于信息增益作为属性选择的度量，C4.5基于信息增益比作为属性选择的度量，CART基于基尼指数作为属性选择的度量。\n代码\n1\n决策树的优缺点\n优点\n不需要任何领域知识或参数假设。\n适合高维数据。\n简单易于理解。\n短时间内处理大量数据，得到可行且效果较好的结果。\n缺点\n对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。\n易于过拟合。\n忽略属性之间的相关性。\n不支持在线学习\n决策树公式\n熵：\nEntropy(S)=−∑pilogpiEntropy(S)=−∑pilog⁡pi\n信息增益：\nEntropy(S,A)=Entropy(S)−∑v∈V(A)|Sv||S|Entropy(Sv)Entropy(S,A)=Entropy(S)−∑v∈V(A)|Sv||S|Entropy(Sv)\n分裂信息：\nSplitInfoR=−∑j=1k|Dj||D|log2|Dj||D|SplitInfoR=−∑j=1k|Dj||D|log2⁡|Dj||D|\n增益比率：\nGainRatio(R)=Gain(R)SplitInfoR(D)GainRatio(R)=Gain(R)SplitInfoR(D)\n基尼指数：\nGini(S)=1−∑imp2iGini(S)=1−∑impi2\nSVM\n支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。\n支持向量机的优缺点\n优点\n可以解决小样本下机器学习的问题。\n提高泛化性能。\n可以解决高维、非线性问题。超高维文本分类仍受欢迎。\n避免神经网络结构选择和局部极小的问题。\n缺点\n缺失数据敏感。\n内存消耗大，难以解释。\n运行和调差略烦人。\n支持向量机的公式\n转自研究者July: SVM的求解，先导出12||w||212||w||2，继而引入拉格朗日函数，转化为单一因子对偶变量a的求解。如此求w.b与a的等价，而求a的解法即为SMO。把求分类函数f(x)=ω∗x+bf(x)=ω∗x+b的问题转化为求w,b的最优化问题，即凸二次规划问题，妙。\n从上图我们可以看出，这条红色的线（超平面）把红色的点和蓝色的点分开了。超平面一边的点对应的y全部是-1，而另外一边全部是1。\n接着我们可以令分类函数：f(x)=ωTx+bf(x)=ωTx+b。显然x是超平面上的点时，f(x)=0。那么我们不妨要求所有满足f(x)<0的点，其对应的y等于-1，而f(x)>0则对应的y=1的数据点。（我盗用了很多图。。。）\n回忆之前的目标函数：\nmax1||ω||s.t.,yi(ωT+b)≥1,i=1,...,nmax1||ω||s.t.,yi(ωT+b)≥1,i=1,...,n\n这个问题等价于\nmax1||ω||2s.t.,yi(ωT+b)≥1,i=1,...,nmax1||ω||2s.t.,yi(ωT+b)≥1,i=1,...,n\n很显然这是一个凸优化的问题，更具体的，它是一个二次优化问题—目标函数是二次的，约束条件是线性的。这个问题可以用任何现成的QP（Quadratic Programming）优化包解决。但是因为这个问题的特殊性，我们还可以通过Lagrange Duality变换到对偶变量的优化问题，找到一种更加行之有效的方法求解。首先我们给每一个约束条件加上一个Lagrange mutiplier,我们可以将它们融合到目标函数中去。 L(ω,b,a)=12||ω||2−∑ni=1α(yi(wTxi+b)−1)L(ω,b,a)=12||ω||2−∑i=1nα(yi(wTxi+b)−1)，然后我们令θ(ω)=maxai≥0L(ω,b,a)θ(ω)=maxai≥0L(ω,b,a)容易验证，当某个约束条件不满足时，例如yi(wTxi+b)<1yi(wTxi+b)<1，那么我们显然有θ(w)=∞θ(w)=∞。而当所有约束条件都满足时，则有θ(ω)=12||ω||2θ(ω)=12||ω||2，亦即我们最初要最小化的量。那么我们现在的目标函数就变成了：minw,bθ(ω)=minω,bmaxai≥0L(ω,b,α)=p∗minw,bθ(ω)=minω,bmaxai≥0L(ω,b,α)=p∗，并且我们有d∗≤p∗d∗≤p∗，因为最大值中最小的一个一定要大于最小值中最大的一个。总之p∗p∗提供了一个第一个问题的最优值p∗p∗的一个下界。在满足KKT条件时，二者相等，我们可以通过求解第二个问题来求解第一个问题。\n先让L关于ωω和b最小化，我们分别把L对w和b求偏导：\n∂L∂ω=0⟹ω=∑i=1nαiyixi∂L∂ω=0⟹ω=∑i=1nαiyixi\n∂L∂b=0⟹∑i=1nαiyi=0∂L∂b=0⟹∑i=1nαiyi=0\n再带回L得到：\nL(ω,b,a)=12∑i,j=1nαiαjyiyjxTixj−∑i,j=1nαiαjyiyjxTixj−b∑i=1nαiyi+∑i=1nαi=∑i=1nαi−12∑i,j=1nαiαjyiyjxTixjL(ω,b,a)=12∑i,j=1nαiαjyiyjxiTxj−∑i,j=1nαiαjyiyjxiTxj−b∑i=1nαiyi+∑i=1nαi=∑i=1nαi−12∑i,j=1nαiαjyiyjxiTxj\n此时我们得到关于dual variable αα的优化问题：\nmaxα∑ni=1αi−12∑ni,j=1αiαjyiyjxTixjs.t.,αi≥0,i=1,...,n∑ni=1αiyi=0maxα∑i=1nαi−12∑i,j=1nαiαjyiyjxiTxjs.t.,αi≥0,i=1,...,n∑i=1nαiyi=0\n这个问题存在高效的算法，不过求解过程就不在这里介绍了。对于一个数据点进行分类时，我们是把x带入到f(x)=wTx+bf(x)=wTx+b中，然后根据其正负号来进行类别划分的。把ω=∑ni=1αiyixiω=∑i=1nαiyixi代入到f(x)=wTx+bf(x)=wTx+b，我们就可以得到f(x)=∑ni=1αiyi<xi,x>+bf(x)=∑i=1nαiyi<xi,x>+b，这里的形式的有趣之处在于，对于新点x的检测，只需要计算它与训练数据点的内积即可。\n为什么非支持向量的αα等于零呢？因为对于非支持向量来说，L(ω,b,a)=12||ω||2−∑ni=1α(yi(wTxi+b)−1)L(ω,b,a)=12||ω||2−∑i=1nα(yi(wTxi+b)−1)中的，(yi(wTxi+b)−1)(yi(wTxi+b)−1)是大于0的，而且αiαi又是非负的，为了满足最大化，αiαi必须等于0。悲剧的非支持向量就被无声的秒杀了。。。\nKNN\nK近邻的优缺点\n优点\n暂无\n缺点\n计算量太大\n对于样本分类不均衡的问题，会产生误判。\nK近邻的公式\nLogistic Regression\n逻辑回归的优缺点\n优点\n速度快。\n简单易于理解，直接看到各个特征的权重。\n能容易地更新模型吸收新的数据。\n如果想要一个概率框架，动态调整分类阀值。\n缺点\n特征处理复杂。需要归一化和较多的特征工程。\n逻辑回归的公式\n如果是连续的，那么就是多重线性回归；如果是二项分布，就是Logistic回归；如果是Poission分布，就是Poisson回归；如果是负二项分布，那么就是负二项分布。 回归问题常见步骤是：寻找h函数；构造J函数；想办法使得J函数最小并求得回归参数。逻辑回归的h函数为：\n1\n2\n3\nhθ(x)=g(θTx)=11+e−θTxhθ(x)=g(θTx)=11+e−θTx\n其中hθ(x)hθ(x)的值表示结果取1的概率。\n那么对于输入x的分类结果对于类别1和类别0的概率分别为：\nP(y=1|x;θ)=hθ(x)P(y=1|x;θ)=hθ(x)\nP(y=0|x;θ)=1−hθ(x)P(y=0|x;θ)=1−hθ(x)\n那么对于构造损失函数J，它们基于最大似然估计推到得到的：\n∑ni=1Cost(hθ(xi),yi)=−1m[∑ni=1yiloghθ(xi)+(1−yi)log(1−hθ(xi))]∑i=1nCost(hθ(xi),yi)=−1m[∑i=1nyilog⁡hθ(xi)+(1−yi)log(1−hθ(xi))]\n最小化上式，与最大化下式子类似：\nP(y|x;θ)=(hθ(x))y(1−hθ(x))1−yP(y|x;θ)=(hθ(x))y(1−hθ(x))1−y\n取似然函数：\nl(θ)=logL(θ)=∑ni=1yi(loghθ(xi)+(1−yi)log(1−hθ(xi)))l(θ)=logL(θ)=∑i=1nyi(log⁡hθ(xi)+(1−yi)log(1−hθ(xi)))\n使用梯度上升的方法，求解θθ，同样如果把J(θ)J(θ)取为\n−1ml(θ)−1ml(θ)，这样通过梯度下降求解梯度最小值。\n梯度下降法求最小值：\nθj:=θj−ασσθiJ(θ)θj:=θj−ασσθiJ(θ)\n代入后得到：\nθj:=θj−α1m∑mi=1(hθ(xi)−yi)xjiθj:=θj−α1m∑i=1m(hθ(xi)−yi)xij\n然后θθ的更新过程如下：\nθj:=θj−α1mxTEθj:=θj−α1mxTE\n其中E=g(A)-y。\n正则化Regularization：\n正则化是在经验风线上增加一个正则化项或者惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化就越大。\nJ(θ)=12m∑i=1n(hθ(xi)−yi)2+λ∑j=1nθ2jJ(θ)=12m∑i=1n(hθ(xi)−yi)2+λ∑j=1nθj2\nλλ是正则项系数。多分类时可以去样本被判定为分类概率最大的那个类。\n逻辑回归的问题\n过拟合问题\n减少feature个数\n规格化\n神经网络\n神经网络的优缺点\n优点\n分类准确率高。\n并行处理能力强。\n分布式存储和学习能力强。\n鲁棒性较强，不易受噪声影响。\n缺点\n需要大量参数（网络拓扑、阀值、阈值）。\n结果难以解释。\n训练时间过长。\n神经网络公式\n深度学习？？？\nEnsemble learning\n集成学习的思路是在对新的实例进行分类的时候，把多个单分类器的结果进行某种组合，来对最终的结果进行分类。\n更好的数据往往打败更好的算法，设计好的特征大有脾益。并且如果你有一个庞大的数据集，使用某种特定的算法的性能可能并不要紧。大可以挨个分类器尝试，并且选取最好的一个。（可以多从易用性和性能考虑）\n而且从Netfliex Prize的经验教训来看，尝试各类分类器、交叉验证、集成方法往往能取得更好的结果，一般的boosting>bagging>single classifier。集成学习的方法主要有一下三种：\n1. 在样本上做文章，基分类器为同一个分类算法，主要有bagging和boosting。\n2. 在分类算法上做文章，即用于训练基分类器的样本相同。基分类器的算法不同。\n3. 在样本属性集上做文章，即在不同的属性上构建分类器，比较出名的是randomforest Tree的算法，这个有weka也有实现。\n1998年Jerome Friedman & Trevor Hastie & Robert Tibshirani发表文章Additive Logistic Regression: a Statistical View of Boosting，中提到Bagging是一个纯粹的降低相关度的方法。如果树的节点具有很高的相关性，bagging就会有很好的效果。\nGBDT\n回归树类似决策树，使用叶子节点的平均值作为判定的结果。如果不是叶子节点，那么就继续向下寻找。GBDT几乎可用于所有的回归问题，亦可以适用于二分类问题。 GBDT使用新生成的树来拟合之前的树拟合的残差。\n1\n2\n3\nAdaboost\nAdaboost目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。 Adaboost的算法流程如下，首先初始化训练数据的权值分布。每个训练样本最开始都被赋予相同的权重：1/N。计算Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度。\n1\n2\n3\nαm=12log1−ememαm=12log⁡1−emem，在em<=1/2em<=1/2时，am>=0，且amam随着emem的减小而增大。\n更新训练数据集的权值分布（目的：得到样本的新权值分布），用于下一轮迭代：\nDm+1=(wm+1,1,wm+1,2,...wm+1,i...,wm+1,N)Dm+1=(wm+1,1,wm+1,2,...wm+1,i...,wm+1,N),\nwm+1,i=wmiZmexp(−αmyiGm(xi)),i=1,2,...,Nwm+1,i=wmiZmexp(−αmyiGm(xi)),i=1,2,...,N\n使得被基本分类器Gm(x)Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过Zm=∑Ni=1wmiexp(−αmyiGm(xi))Zm=∑i=1Nwmiexp(−αmyiGm(xi))，使得Dm+1Dm+1成为一个概率分布。然后组合各个弱分类器f(x)=∑Mm=1αmGm(x)f(x)=∑m=1MαmGm(x)，而得到的最终分类器G(x)=sign(f(x))=sign[∑Mm=1αmGm(x)]G(x)=sign(f(x))=sign[∑m=1MαmGm(x)]。\nRandom Forest\n随机森林指通过多颗决策树联合组成的预测模型，可以对样本或者特征取bagging"}
