{"content2":"声明：版权所有，转载请联系作者并注明出处  http://blog.csdn.net/u013719780?viewmode=contents\n博主简介：风雪夜归子（Allen），机器学习算法攻城狮，喜爱钻研Meachine Learning的黑科技，对Deep Learning和Artificial Intelligence充满兴趣，经常关注Kaggle数据挖掘竞赛平台，对数据、Machine Learning和Artificial Intelligence有兴趣的童鞋可以一起探讨哦，个人CSDN博客：http://blog.csdn.net/u013719780?viewmode=contents\n运用机器学习(Kmeans算法)确定家庭用电的主要原因\n本文将对家庭的用电数据进行一些基本的分析。\n本文主要分为两个部分：\nPart One: 对数据做一些简单的清洗和分析工作；\nPart Two: 运用无监督的机器学习算法-Kmeans算法确定某个特定的时间段家庭用电的主要原因。\n首先，想入相应的包并且读取数据集。具体代码如下：\nIn [1]:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt\nIn [2]:\nsensor_data = pd.read_csv('merged-sensor-files.csv', names=[\"MTU\", \"Time\", \"Power\", \"Cost\", \"Voltage\"], header = 0) weather_data = pd.read_json('weather.json', typ ='series')\nIn [3]:\nimport json f=open('weather.json') json_data = json.load(f) Time = [] Temperature = [] for time, temperature in json_data.items(): Time.append(int(time)) Temperature.append(float(temperature)) temperature = pd.DataFrame({'Time':Time, 'Temperature': Temperature}) temperature\nOut[3]:\nTemperature\nTime\n0\n84.4\n1431468000\n1\n83.3\n1431450000\n2\n70.7\n1431403200\n3\n72.1\n1431432000\n4\n84.2\n1431464400\n5\n80.9\n1431446400\n6\n68.6\n1431424800\n7\n81.1\n1431475200\n8\n80.7\n1431442800\n9\n69.2\n1431417600\n10\n76.2\n1431435600\n11\n68.8\n1431414000\n12\n72.1\n1431396000\n13\n68.7\n1431428400\n14\n80.1\n1431439200\n15\n83.0\n1431471600\n16\n69.0\n1431410400\n17\n75.4\n1431388800\n18\n71.0\n1431399600\n19\n69.6\n1431406800\n20\n67.9\n1431421200\n21\n85.1\n1431457200\n22\n87.0\n1431460800\n23\n73.2\n1431392400\n24\n84.5\n1431453600\nIn [4]:\nimport json f=open('weather.json').read() Time = [] Temperature = [] for line in f.split(','): time, temperature = line.split(':') time = time.replace('\"','') time = time.replace('{','') temperature = temperature.replace('\"','') temperature = temperature.replace('}','') #print time, temperature Time.append(int(time)) Temperature.append(float(temperature))\nIn [5]:\n# A quick look at the datasets sensor_data.head(5)\nOut[5]:\nMTU\nTime\nPower\nCost\nVoltage\n0\nMTU1\n05/11/2015 19:59:06\n4.102\n0.62\n122.4\n1\nMTU1\n05/11/2015 19:59:05\n4.089\n0.62\n122.3\n2\nMTU1\n05/11/2015 19:59:04\n4.089\n0.62\n122.3\n3\nMTU1\n05/11/2015 19:59:06\n4.089\n0.62\n122.3\n4\nMTU1\n05/11/2015 19:59:04\n4.097\n0.62\n122.4\nIn [6]:\nsensor_data.describe()\nOut[6]:\nMTU\nTime\nPower\nCost\nVoltage\ncount\n88914\n88914\n88914\n88914\n88914\nunique\n2\n72359\n2495\n88\n48\ntop\nMTU1\nTime\n0.136\n0.05\n123.1\nfreq\n88891\n23\n6544\n19476\n5063\nIn [7]:\nsensor_data.info()\n<class 'pandas.core.frame.DataFrame'> RangeIndex: 88914 entries, 0 to 88913 Data columns (total 5 columns): MTU 88914 non-null object Time 88914 non-null object Power 88914 non-null object Cost 88914 non-null object Voltage 88914 non-null object dtypes: object(5) memory usage: 3.4+ MB\nTASK 1: 数据分析\n数据清洗\n从数据集merged-sensor-files.csv 中我们发现，某些数据有问题。\nIn [8]:\nsensor_data.dtypes\nOut[8]:\nMTU object Time object Power object Cost object Voltage object dtype: object\n下面找出有问题的数据\nIn [9]:\n# Get the inconsistent rows indexes faulty_row_idx = sensor_data[sensor_data[\"Power\"] == \" Power\"].index.tolist() faulty_row_idx\nOut[9]:\n[3784, 7582, 11385, 15004, 18773, 22363, 26049, 29795, 33554, 37193, 40951, 44563, 48227, 51934, 55660, 59431, 63041, 66706, 70468, 74305, 77951, 81617, 85327]\n删除有问题的数据\nIn [10]:\nsensor_data.drop(faulty_row_idx, inplace=True) sensor_data[sensor_data[\"Power\"] == \" Power\"].index.tolist()\nOut[10]:\n[]\n从上述结果可以知道，有问题的数据已经成功被删除\nWe have cleaned up the sensor_data and now these can be converted to more appropriate data types. 对数据类型进行转换\nIn [11]:\nsensor_data[[\"Power\", \"Cost\", \"Voltage\"]] = sensor_data[[\"Power\", \"Cost\", \"Voltage\"]].astype(float) sensor_data[[\"Time\"]] = pd.to_datetime(sensor_data[\"Time\"]) sensor_data['Hour'] = pd.DatetimeIndex(sensor_data[\"Time\"]).hour sensor_data.dtypes\nOut[11]:\nMTU object Time datetime64[ns] Power float64 Cost float64 Voltage float64 Hour int32 dtype: object\nThis is better now. We have got clearly defined datatypes of different columns now. Next step is to convert the weather_data Series to a dataframe so that we can work with it with more ease.\nGood!我们现在已经得到了我们所需的数据类型。接下来为了数据操作上的方便，我们将数据集weather_data转换成dataframe格式。\nIn [12]:\ntemperature_data = weather_data.to_frame() temperature_data.reset_index(level=0, inplace=True) temperature_data.columns = [\"Time\", \"Temperature\"] temperature_data.dtypes temperature_data['Temperature'] = Temperature temperature_data[\"Hour\"] = pd.DatetimeIndex(temperature_data[\"Time\"]).hour temperature_data[[\"Temperature\"]] = temperature_data[[\"Temperature\"]].astype(float) temperature_data\nOut[12]:\nTime\nTemperature\nHour\n0\n2015-05-12 00:00:00\n75.4\n0\n1\n2015-05-12 01:00:00\n73.2\n1\n2\n2015-05-12 02:00:00\n72.1\n2\n3\n2015-05-12 03:00:00\n71.0\n3\n4\n2015-05-12 04:00:00\n70.7\n4\n5\n2015-05-12 05:00:00\n69.6\n5\n6\n2015-05-12 06:00:00\n69.0\n6\n7\n2015-05-12 07:00:00\n68.8\n7\n8\n2015-05-12 08:00:00\n69.2\n8\n9\n2015-05-12 09:00:00\n67.9\n9\n10\n2015-05-12 10:00:00\n68.6\n10\n11\n2015-05-12 11:00:00\n68.7\n11\n12\n2015-05-12 12:00:00\n72.1\n12\n13\n2015-05-12 13:00:00\n76.2\n13\n14\n2015-05-12 14:00:00\n80.1\n14\n15\n2015-05-12 15:00:00\n80.7\n15\n16\n2015-05-12 16:00:00\n80.9\n16\n17\n2015-05-12 17:00:00\n83.3\n17\n18\n2015-05-12 18:00:00\n84.5\n18\n19\n2015-05-12 19:00:00\n85.1\n19\n20\n2015-05-12 20:00:00\n87.0\n20\n21\n2015-05-12 21:00:00\n84.2\n21\n22\n2015-05-12 22:00:00\n84.4\n22\n23\n2015-05-12 23:00:00\n83.0\n23\n24\n2015-05-13 00:00:00\n81.1\n0\nIn [14]:\nsensor_data.describe()\nOut[14]:\nPower\nCost\nVoltage\nHour\ncount\n88891.000000\n88891.000000\n88891.000000\n88891.000000\nmean\n1.315980\n0.202427\n123.127744\n11.531865\nstd\n1.682181\n0.252357\n0.838768\n6.921775\nmin\n0.113000\n0.020000\n121.000000\n0.000000\n25%\n0.255000\n0.040000\n122.600000\n6.000000\n50%\n0.367000\n0.060000\n123.100000\n12.000000\n75%\n1.765000\n0.270000\n123.700000\n18.000000\nmax\n6.547000\n0.990000\n125.600000\n23.000000\nIn [15]:\ntemperature_data.describe()\nOut[15]:\nTemperature\nHour\ncount\n25.000000\n25.00000\nmean\n76.272000\n11.04000\nstd\n6.635355\n7.29429\nmin\n67.900000\n0.00000\n25%\n69.600000\n5.00000\n50%\n75.400000\n11.00000\n75%\n83.000000\n17.00000\nmax\n87.000000\n23.00000\n从上面的统计结果可以知道，耗电的平均值、最小值、最大值分别为1.315980kW，0.11kW and 6.54kW。为了对数据进行更好的理解，我们绘制出耗电 and 温度与时间的关系图。 在绘图之前，需要对数据关于列'hour'group BY:\nIn [16]:\ngrouped_sensor_data = sensor_data.groupby([\"Hour\"], as_index = False).mean() grouped_sensor_data\nOut[16]:\nHour\nPower\nCost\nVoltage\n0\n0\n0.173790\n0.029468\n124.723879\n1\n1\n0.179594\n0.033805\n124.522469\n2\n2\n0.185763\n0.037013\n123.929979\n3\n3\n0.184510\n0.036815\n124.174454\n4\n4\n0.181104\n0.036366\n123.847801\n5\n5\n0.184242\n0.036693\n122.790974\n6\n6\n0.672423\n0.106142\n123.375132\n7\n7\n0.977755\n0.150614\n123.722441\n8\n8\n0.382392\n0.060904\n122.997544\n9\n9\n0.168447\n0.027770\n122.675906\n10\n10\n0.373942\n0.058812\n122.986207\n11\n11\n0.383065\n0.059837\n123.500554\n12\n12\n0.378432\n0.059604\n122.783133\n13\n13\n0.380076\n0.059766\n122.991571\n14\n14\n0.378020\n0.059666\n122.815359\n15\n15\n0.376586\n0.059619\n122.464499\n16\n16\n4.365774\n0.659342\n121.766840\n17\n17\n4.318118\n0.652923\n121.851496\n18\n18\n4.779928\n0.721469\n122.301059\n19\n19\n4.250034\n0.642619\n122.103700\n20\n20\n1.967120\n0.300640\n122.770635\n21\n21\n1.579896\n0.242180\n123.086060\n22\n22\n2.542672\n0.387109\n123.542620\n23\n23\n2.269941\n0.346457\n123.415791\nIn [17]:\ngrouped_temperature_data = temperature_data.groupby([\"Hour\"], as_index = False).mean() grouped_temperature_data\nOut[17]:\nHour\nTemperature\n0\n0\n78.25\n1\n1\n73.20\n2\n2\n72.10\n3\n3\n71.00\n4\n4\n70.70\n5\n5\n69.60\n6\n6\n69.00\n7\n7\n68.80\n8\n8\n69.20\n9\n9\n67.90\n10\n10\n68.60\n11\n11\n68.70\n12\n12\n72.10\n13\n13\n76.20\n14\n14\n80.10\n15\n15\n80.70\n16\n16\n80.90\n17\n17\n83.30\n18\n18\n84.50\n19\n19\n85.10\n20\n20\n87.00\n21\n21\n84.20\n22\n22\n84.40\n23\n23\n83.00\nBasic Visualizations:\nIn [18]:\n%pylab inline plt.style.use('ggplot')\nPopulating the interactive namespace from numpy and matplotlib\nWARNING: pylab import has clobbered these variables: ['f'] `%matplotlib` prevents importing * from pylab and numpy\nIn [19]:\nfig = plt.figure(figsize=(13,7)) plt.hist(sensor_data.Power, bins=50) fig.suptitle('Power Histogram', fontsize = 20) plt.xlabel('Power', fontsize = 16) plt.ylabel('Count', fontsize = 16)\nOut[19]:\n<matplotlib.text.Text at 0x115220310>\n从上图可以得出：大部分时间耗电都比较低，但是也有一些时间段用电较多，达到了3.5kW - 5kW之间。接下来绘制用电关于时间的分布图。\nIn [20]:\nfig = plt.figure(figsize=(13,7)) plt.bar(grouped_sensor_data.Hour, grouped_sensor_data.Power) fig.suptitle('Power Distribution with Hours', fontsize = 20) plt.xlabel('Hour', fontsize = 16) plt.ylabel('Power', fontsize = 16) plt.xticks(range(0, 24)) plt.show()\n从上面条形图可以得出如下推论:\n用电需求最高的时间段是在晚上，这可能是因为大部分的电器设备，如：AC、暖气、TV、烤炉、洗衣机等的使用。\n睡觉时间段(0000 - 0500) and 办公时间段(0900 - 1600)有非常低的需求, 是因为这两个时间段电器设备都已经关闭了.\n在时间段0600 - 0900用电有少许增加, 这可能是一些电器设备处在激活状态导致的.\n稳定状态:\n在时间段 0000 - 0500, 用电需求很少，其范围处在 0.17kW - 0.18kW\n另一个稳定状态是时间段 1000 - 1500, 其需求处在 0.373kW - 0.376kW\n最高的稳定时间段是 1600 - 1900 其需求处在 4.36kW - 4.25kW\n在0700 and 1800期间电力需求突然发生了变化，可能是随机事件或者某些电器设备的使用和异常数据导致的。\n在0900时间段电力需求同样有轻微震动，从0.38kw下降到了0.16kw随后又上升到了0.37kw。在2100可以看到同样的变化趋势。\n让我们进一步绘制temperature and Power的关系图，看看这里是否有一些相关性。\nIn [21]:\nfig = plt.figure(figsize=(13,7)) plt.bar(grouped_temperature_data.Temperature, grouped_sensor_data.Power) fig.suptitle('Power Distribution with Temperature', fontsize = 20) plt.xlabel('Temperature in Fahrenheit', fontsize = 16) plt.ylabel('Power', fontsize = 16) plt.show()\n温度和电力需求似乎有一些直接的关系，这很好理解，因为我们当前的数据集是取自于5月，在高峰期（晚上）制冷设备都已经打开了。\nTask 2: 机器学习\n为了在一个完整的数据集上工作，合并数据集 grouped_sensor_data and grouped_temperature_data。\nIn [22]:\nmerged_data = grouped_sensor_data.merge(grouped_temperature_data) merged_data\nOut[22]:\nHour\nPower\nCost\nVoltage\nTemperature\n0\n0\n0.173790\n0.029468\n124.723879\n78.25\n1\n1\n0.179594\n0.033805\n124.522469\n73.20\n2\n2\n0.185763\n0.037013\n123.929979\n72.10\n3\n3\n0.184510\n0.036815\n124.174454\n71.00\n4\n4\n0.181104\n0.036366\n123.847801\n70.70\n5\n5\n0.184242\n0.036693\n122.790974\n69.60\n6\n6\n0.672423\n0.106142\n123.375132\n69.00\n7\n7\n0.977755\n0.150614\n123.722441\n68.80\n8\n8\n0.382392\n0.060904\n122.997544\n69.20\n9\n9\n0.168447\n0.027770\n122.675906\n67.90\n10\n10\n0.373942\n0.058812\n122.986207\n68.60\n11\n11\n0.383065\n0.059837\n123.500554\n68.70\n12\n12\n0.378432\n0.059604\n122.783133\n72.10\n13\n13\n0.380076\n0.059766\n122.991571\n76.20\n14\n14\n0.378020\n0.059666\n122.815359\n80.10\n15\n15\n0.376586\n0.059619\n122.464499\n80.70\n16\n16\n4.365774\n0.659342\n121.766840\n80.90\n17\n17\n4.318118\n0.652923\n121.851496\n83.30\n18\n18\n4.779928\n0.721469\n122.301059\n84.50\n19\n19\n4.250034\n0.642619\n122.103700\n85.10\n20\n20\n1.967120\n0.300640\n122.770635\n87.00\n21\n21\n1.579896\n0.242180\n123.086060\n84.20\n22\n22\n2.542672\n0.387109\n123.542620\n84.40\n23\n23\n2.269941\n0.346457\n123.415791\n83.00\n在之前的数据可视化中，我们看到了当温度低的时候电力需求比较小。但是这主要是与制冷的电器设备有较大的关系：\nCooling Systems\nTV\nGeyser\nLights\nOven\nHome Security Systems\n我们接下来用合并后的完整数据集确定这些设备是否打开。\nAC, Refrigerator and Other Coooling Systems:\n从\"Power Distribution with Temperature\"图可以明显看出，随着温度的上升电力需求突然增加，这就意味着家里的制冷设备处于开启状态。\nTV:\n在evening hours(1600 - 2300), 电视机可能是另外一个导致电力需求增加的因素. 从Power特征看它是相当明显的.\nGeyser, Oven:\n在during morning hours电力需求轻微增加可能是与一些设备的工作是相关的.\nLights:\n灯光对用电需求有比较小的影响（认为house owner使用的是节能灯）。\nHome Security Systems:\n在工作时间有轻微的增加可能是一些家庭设备与其他的一些自动设备导致的。\n现在，我们将使用 K-Means clustering 算法. 使用原始数据集中的特征 Hour, Power and Temperature .首先，我们需要合并数据集sensor_data dataframe 和 grouped_temperature_data.\nIn [23]:\ndata =sensor_data.merge(grouped_temperature_data) data.drop([\"Time\", \"MTU\", \"Cost\", \"Voltage\"], axis = 1, inplace = True) data.head()\nOut[23]:\nPower\nHour\nTemperature\n0\n4.102\n19\n85.1\n1\n4.089\n19\n85.1\n2\n4.089\n19\n85.1\n3\n4.089\n19\n85.1\n4\n4.097\n19\n85.1\nIn [24]:\nfrom sklearn.cluster import KMeans from sklearn.cross_validation import train_test_split\nIn [25]:\nnp.random.seed(1234) train_data, test_data = train_test_split(data, test_size = 0.25, random_state = 42)\nIn [26]:\ntrain_data.shape\nOut[26]:\n(66668, 3)\nIn [27]:\ntest_data.shape\nOut[27]:\n(22223, 3)\nIn [28]:\nkmeans = KMeans(n_clusters = 4, n_jobs = 4) kmeans_fit = kmeans.fit(train_data)\n/Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8) /Applications/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.py:197: DeprecationWarning: Changing the shape of non-C contiguous array by descriptor assignment is deprecated. To maintain the Fortran contiguity of a multidimensional Fortran array, use 'a.T.view(...).T' instead obj_bytes_view = obj.view(self.np.uint8)\nIn [29]:\npredict = kmeans_fit.predict(test_data)\nIn [30]:\ntest_data[\"Cluster\"] = predict test_data.head(20)\n/Applications/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy if __name__ == '__main__':\nOut[30]:\nPower\nHour\nTemperature\nCluster\n52595\n0.114\n8\n69.2\n1\n86044\n4.255\n17\n83.3\n0\n6091\n3.559\n20\n87.0\n0\n60185\n0.453\n11\n68.7\n1\n37054\n0.136\n4\n70.7\n2\n59216\n0.312\n10\n68.6\n1\n61848\n0.453\n11\n68.7\n1\n278\n4.162\n19\n85.1\n0\n30829\n0.136\n3\n71.0\n2\n8751\n0.955\n21\n84.2\n0\n35134\n0.276\n4\n70.7\n2\n31476\n0.278\n3\n71.0\n2\n55854\n0.456\n10\n68.6\n1\n54992\n0.370\n8\n69.2\n1\n78259\n0.307\n15\n80.7\n3\n62724\n0.313\n11\n68.7\n1\n54132\n0.260\n8\n69.2\n1\n44204\n0.114\n9\n67.9\n1\n7834\n1.094\n21\n84.2\n0\n25231\n0.125\n1\n73.2\n2\n这看起来是一个很合理的聚类. 我们进一步将标签分到类里面, 作为一个检测模型. 很明显，我们可以将预测的标签设置成如下的类别:\n0 - Cooling Systems\n1 - Oven, Geyser\n2 - Night Lights\n3 - Home Security Systems\n接下来我们将会把标签和预测结果合并成一个数据框.\nIn [31]:\nlabel_df = pd.DataFrame({\"Cluster\": [0, 1, 2, 3], \"Appliances\": [\"Cooling System\",\"Oven, Geyser\", \"Night Lights\", \"Home Security Systems\"]}) label_df\nOut[31]:\nAppliances\nCluster\n0\nCooling System\n0\n1\nOven, Geyser\n1\n2\nNight Lights\n2\n3\nHome Security Systems\n3\nIn [32]:\nresult = test_data.merge(label_df) result.head()\nOut[32]:\nPower\nHour\nTemperature\nCluster\nAppliances\n0\n0.114\n8\n69.2\n1\nOven, Geyser\n1\n0.453\n11\n68.7\n1\nOven, Geyser\n2\n0.312\n10\n68.6\n1\nOven, Geyser\n3\n0.453\n11\n68.7\n1\nOven, Geyser\n4\n0.456\n10\n68.6\n1\nOven, Geyser\nIn [33]:\nresult.tail()\nOut[33]:\nPower\nHour\nTemperature\nCluster\nAppliances\n22218\n0.306\n15\n80.7\n3\nHome Security Systems\n22219\n0.450\n13\n76.2\n3\nHome Security Systems\n22220\n4.426\n16\n80.9\n3\nHome Security Systems\n22221\n0.452\n15\n80.7\n3\nHome Security Systems\n22222\n0.307\n15\n80.7\n3\nHome Security Systems\n从result dataframe可以看出，在8, 9, 10时Oven or Geyser有比较高的概率在使用，另一方面，在office hours(1000 - 1600)安全设备使用的可能性很高。\n在数据分析的过程中，我们仅仅使用了按照hour的group BY的数据，事实上，如果我们拥有更多的数据，可以使用更多的特征，例如按照day，week，month进行group BY。\n我们也应该考虑季节和温度，因为不同的电器设备在不同的季节使用情况是不一样的。因为好的特征能够让我们的模型预测的更加准确。\n同时，这个也可以帮助我们进行一个分类任务，因为我们已经知道了某些电器设备需要的耗电情况。\n参考文献:\nhttp://www.sciencedirect.com/science/article/pii/S037877881200151X\nhttp://cs.gmu.edu/~jessica/publications/astronomy11.pdf"}
