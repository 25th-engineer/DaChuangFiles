{"content2":"要学习机器学习，首先得想明白机器学习为啥是可信的，下面就介绍几个我个人认为的机器学习的基础原理：\nHoaffding定理：机器学习泛化误差上界\nbias & variance & error：模型预测误差的成分\nNo Free Lunch Theorem：不存在在任何情况下准确性都好的模型\nHoaffding定理\nHoaffding定理是泛化能力的一种解释，现在在这我给出Hoaffding定理的证明和释义。\nJensen不等式\n若函数\nf\n(\nx\n)\nf(x)\nf(x)再\nx\n∈\n[\na\n,\nb\n]\nx\\in [a,b]\nx∈[a,b]上\nf\n′\n′\n(\nx\n)\n&gt;\n0\nf^{&#x27;&#x27;}(x)&gt;0\nf′′(x)>0，令\nq\n∈\n[\n0\n,\n1\n]\n,\nF\n(\nx\n)\n=\nq\nf\n(\nb\n)\n+\n(\n1\n−\nq\n)\nf\n(\na\n)\n−\nf\n(\nq\nb\n+\n(\n1\n−\nq\n)\na\n)\nq\\in [0,1],F(x)=qf(b)+(1-q)f(a)-f(qb+(1-q)a)\nq∈[0,1],F(x)=qf(b)+(1−q)f(a)−f(qb+(1−q)a)\n那么\nF\n(\n0\n)\n=\n0\nF(0)=0\nF(0)=0\nF\n(\n1\n)\n=\n0\nF(1)=0\nF(1)=0\nF\n′\n(\nq\n)\n=\nf\n(\nb\n)\n−\nf\n(\na\n)\n−\n(\nb\n−\na\n)\nf\n′\n(\nq\nb\n+\n(\n1\n−\nq\n)\na\n)\n=\n(\nb\n−\na\n)\n(\nf\n′\n(\nθ\n)\n−\nf\n′\n(\nq\nb\n+\n(\n1\n−\nq\n)\na\n)\n)\nF^{&#x27;}(q)=f(b)-f(a)-(b-a)f^{&#x27;}(qb+(1-q)a)=(b-a)(f^{&#x27;}(\\theta)-f^{&#x27;}(qb+(1-q)a))\nF′(q)=f(b)−f(a)−(b−a)f′(qb+(1−q)a)=(b−a)(f′(θ)−f′(qb+(1−q)a))\n由\nf\n′\n′\n(\nx\n)\n&gt;\n0\nf^{&#x27;&#x27;}(x)&gt;0\nf′′(x)>0可知\nF\n′\n(\nq\n)\nF^{&#x27;}(q)\nF′(q)先小于0然后大于0，所以\nF\n(\nq\n)\n&lt;\n=\n0\nF(q)&lt;=0\nF(q)<=0即函数\nx\n∈\n[\na\n,\nb\n]\nx\\in [a,b]\nx∈[a,b]时\nf\n′\n′\n(\nx\n)\n&gt;\n0\nf^{&#x27;&#x27;}(x)&gt;0\nf′′(x)>0时，\nq\n∈\n[\n0\n,\n1\n]\n,\nq\nf\n(\nb\n)\n+\n(\n1\n−\nq\n)\nf\n(\na\n)\n&gt;\nf\n(\nq\nb\n+\n(\n1\n−\nq\n)\na\n)\nq\\in [0,1],qf(b)+(1-q)f(a)&gt;f(qb+(1-q)a)\nq∈[0,1],qf(b)+(1−q)f(a)>f(qb+(1−q)a)\nMarkov不等式\n假设\nx\nx\nx是大于\n0\n0\n0的随机变量，则有\nE\n[\nx\n]\n=\n∫\n0\n∞\nx\np\n(\nx\n)\nd\nx\n&gt;\n∫\n0\nϵ\n0\np\n(\nx\n)\nd\nx\n+\n∫\nϵ\n∞\nϵ\np\n(\nx\n)\nd\nx\n&gt;\nϵ\nP\n(\nx\n&gt;\nϵ\n)\nE[x]=\\int_0^\\infty xp(x)dx&gt;\\int_0^\\epsilon 0p(x)dx+\\int_\\epsilon^\\infty \\epsilon p(x)dx &gt;\\epsilon P(x&gt;\\epsilon)\nE[x]=∫0∞ xp(x)dx>∫0ϵ 0p(x)dx+∫ϵ∞ ϵp(x)dx>ϵP(x>ϵ)\n即\nP\n(\nx\n&gt;\nϵ\n)\n&lt;\nE\n[\nx\n]\nϵ\nP(x&gt;\\epsilon)&lt;\\frac{E[x]}{\\epsilon}\nP(x>ϵ)<ϵE[x]\n引理\n若\nx\n∈\n[\na\n,\nb\n]\n,\nE\n[\nx\n]\n=\n0\n,\nt\n&gt;\n0\nx\\in [a,b],E[x]=0,t&gt;0\nx∈[a,b],E[x]=0,t>0，那么\nP\n(\nx\n&gt;\ns\n)\n=\nP\n(\ne\nt\nx\n&gt;\ne\nt\ns\n)\n&lt;\nE\n[\ne\nt\nx\n]\ne\ns\nt\nP(x&gt;s)=P(e^{tx}&gt;e^{ts})&lt;\\frac{E[e^{tx}]}{e^{st}}\nP(x>s)=P(etx>ets)<estE[etx]\n由\ne\nt\nx\ne^{tx}\netx为凸函数可知\ne\nt\nx\n&lt;\nb\n−\nx\nb\n−\na\ne\nt\na\n+\nx\n−\na\nb\n−\na\ne\nt\nb\ne^{tx}&lt;\\frac{b-x}{b-a}e^{ta}+\\frac{x-a}{b-a}e^{tb}\netx<b−ab−x eta+b−ax−a etb\n那么\nE\n[\ne\nt\nx\n]\n&lt;\nb\n−\nE\n[\nx\n]\nb\n−\na\ne\nt\na\n+\nE\n[\nx\n]\n−\na\nb\n−\na\ne\nt\nb\nE[e^{tx}]&lt;\\frac{b-E[x]}{b-a}e^{ta}+\\frac{E[x]-a}{b-a}e^{tb}\nE[etx]<b−ab−E[x] eta+b−aE[x]−a etb\n令\np\n=\nt\n(\nb\n−\na\n)\n,\nh\n=\na\nb\n−\na\np=t(b-a),h=\\frac{a}{b-a}\np=t(b−a),h=b−aa ，那么有\nb\nb\n−\na\ne\nt\na\n−\na\nb\n−\na\ne\nt\nb\n=\ne\nt\na\n[\nb\nb\n−\na\n−\na\nb\n−\na\ne\nt\n(\nb\n−\na\n)\n]\n=\ne\nt\na\n[\n1\n+\na\nb\n−\na\n−\na\nb\n−\na\ne\nt\n(\nb\n−\na\n)\n]\n=\ne\nx\np\n(\np\nh\n+\nl\nn\n(\n1\n+\nh\n−\nh\ne\np\n)\n)\n\\frac{b}{b-a}e^{ta}-\\frac{a}{b-a}e^{tb}=e^{ta}[\\frac{b}{b-a}-\\frac{a}{b-a}e^{t(b-a)}]=e^{ta}[1+\\frac{a}{b-a}-\\frac{a}{b-a}e^{t(b-a)}]=exp(ph+ln(1+h-he^{p}))\nb−ab eta−b−aa etb=eta[b−ab −b−aa et(b−a)]=eta[1+b−aa −b−aa et(b−a)]=exp(ph+ln(1+h−hep))\n令\nf\n(\np\n)\n=\np\nh\n+\nl\nn\n(\n1\n+\nh\n−\nh\ne\np\n)\nf(p)=ph+ln(1+h-he^{p})\nf(p)=ph+ln(1+h−hep)，那么\nf\n(\n0\n)\n=\n0\nf(0)=0\nf(0)=0\nf\n′\n(\np\n)\n=\nh\n−\nh\ne\np\n1\n+\nh\n−\nh\ne\np\nf^{&#x27;}(p)=h-\\frac{he^{p}}{1+h-he^{p}}\nf′(p)=h−1+h−hephep\nf\n′\n(\n0\n)\n=\n0\nf^{&#x27;}(0)=0\nf′(0)=0\nf\n′\n′\n(\np\n)\n=\n−\nh\ne\np\n(\n1\n+\nh\n−\nh\ne\np\n)\n+\n(\nh\ne\np\n)\n2\n(\n1\n+\nh\n−\nh\ne\np\n)\n2\n=\n(\n−\nh\ne\np\n1\n+\nh\n−\nh\ne\np\n)\n(\n1\n+\nh\n1\n+\nh\n−\nh\ne\np\n)\nf^{&#x27;&#x27;}(p)=-\\frac{he^{p}(1+h-he^{p})+(he^{p})^2}{(1+h-he^{p})^2}=(-\\frac{he^p}{1+h-he^{p}})(\\frac{1+h}{1+h-he^{p}})\nf′′(p)=−(1+h−hep)2hep(1+h−hep)+(hep)2 =(−1+h−hephep )(1+h−hep1+h )\nf\n′\n′\n(\np\n)\n=\ny\n(\n1\n−\ny\n)\n&lt;\n1\n4\nf^{&#x27;&#x27;}(p)=y(1-y)&lt;\\frac{1}{4}\nf′′(p)=y(1−y)<41\n泰勒展开可得：\nf\n(\np\n)\n=\nf\n(\n0\n)\n+\np\nf\n′\n(\n0\n)\n+\np\n2\n2\nf\n′\n′\n(\nθ\n)\n&lt;\np\n2\n8\nf(p)=f(0)+pf^{&#x27;}(0)+\\frac{p^2}{2}f^{&#x27;&#x27;}(\\theta)&lt;\\frac{p^2}{8}\nf(p)=f(0)+pf′(0)+2p2 f′′(θ)<8p2\n则\nE\n[\ne\nt\nx\n]\n&lt;\ne\nx\np\n[\n(\nb\n−\na\n)\n2\n8\nt\n2\n]\nE[e^{tx}]&lt;exp[\\frac{(b-a)^2}{8}t^2]\nE[etx]<exp[8(b−a)2 t2]\n则\nP\n(\nx\n&gt;\ns\n)\n&lt;\ne\nx\np\n[\n−\ns\nt\n+\n(\nb\n−\na\n)\n2\n8\nt\n2\n]\nP(x&gt;s)&lt;exp[-st+\\frac{(b-a)^2}{8}t^2]\nP(x>s)<exp[−st+8(b−a)2 t2]\nHoaffding定理证明\n设\nr\n1\n，\nr\n2\n,\n.\n.\n.\n,\nr\nn\nr_1，r_2,...,r_n\nr1 ，r2 ,...,rn 为模型的一组误差，为了简便，让他们分布在\n[\n−\n0.5\n,\n0.5\n]\n[-0.5,0.5]\n[−0.5,0.5]，均值为0，令\nr\n^\n=\n∑\ni\nr\ni\nn\n,\nr\n=\nE\n[\nr\n^\n]\n\\hat r=\\frac{\\sum_i r_i}{n},r=E[\\hat r]\nr^=n∑i ri ,r=E[r^]\n那么\nP\n(\nr\n^\n−\nr\n&gt;\nϵ\n)\n=\ne\n−\nt\nϵ\nE\n[\ne\nt\n∑\ni\nr\ni\n/\nn\n]\n=\ne\n−\nt\nϵ\n∏\ni\nE\n[\ne\nt\nr\ni\n/\nn\n]\n&lt;\ne\nx\np\n[\n−\nt\nϵ\n+\nt\n2\n8\nn\n]\nP(\\hat r-r&gt;\\epsilon)=e^{-t\\epsilon}E[e^{t\\sum_ir_i/n}]=e^{-t\\epsilon}\\prod_i E[e^{tr_i/n}]&lt;exp[-t\\epsilon+\\frac{t^2}{8n}]\nP(r^−r>ϵ)=e−tϵE[et∑i ri /n]=e−tϵi∏ E[etri /n]<exp[−tϵ+8nt2 ]\n令\nt\n=\n4\nn\nϵ\nt=4n\\epsilon\nt=4nϵ，可得\nP\n(\nx\n&gt;\ns\n)\n&lt;\ne\nx\np\n[\n−\n2\nn\nϵ\n2\n]\nP(x&gt;s)&lt;exp[-2n\\epsilon^2]\nP(x>s)<exp[−2nϵ2]\n那么如果\nk\nk\nk个模型训练的模型误差都满足\nP\n(\nr\n&lt;\nr\n^\n+\nϵ\n)\n&lt;\n(\n1\n−\nk\nP\n(\nr\n−\nr\n^\n&gt;\nϵ\n)\n)\nP(r&lt;\\hat r+\\epsilon)&lt;(1-kP(r-\\hat r&gt;\\epsilon))\nP(r<r^+ϵ)<(1−kP(r−r^>ϵ))（hoeffding不等式的对称性），则\nP\n(\nr\n&lt;\nr\n^\n+\nϵ\n)\n&lt;\n(\n1\n−\nk\n∗\ne\nx\np\n[\n−\n2\nn\nϵ\n2\n]\n)\nP(r&lt;\\hat r+\\epsilon)&lt;(1-k*exp[-2n\\epsilon^2])\nP(r<r^+ϵ)<(1−k∗exp[−2nϵ2])\n令\nδ\n=\nk\n∗\ne\nx\np\n[\n−\n2\nn\nϵ\n2\n]\n\\delta = k*exp[-2n\\epsilon^2]\nδ=k∗exp[−2nϵ2]，则模型以\n1\n−\nδ\n1-\\delta\n1−δ的概率满足任意训练的模型满足\nr\n&lt;\nr\n^\n+\n1\n2\nn\nln\n⁡\nk\nδ\nr&lt;\\hat r+\\sqrt{\\frac{1}{2n}\\ln{\\frac{k}{\\delta}}}\nr<r^+2n1 lnδk\n这就给了训练出来的模型一个误差上界，若是参数域为无穷，可用VC维来给定上界\n个人不喜欢这个解释，不直观，太繁琐，而且是个loose bound，让感觉很难受。\nbias & variance & error\n机器学习学到的模型预测的结果和真实结果的误差来源于三个地方，也就是bias（偏差），variance（方差），error（噪声），用公式可以表示为：\nE\nx\nL\n(\nf\n(\nx\n)\n+\nϵ\n,\nf\n~\n(\nx\n)\n+\n[\nf\n^\n(\nx\n)\n−\nf\n^\n(\nx\n)\n]\n)\n=\nF\n[\nϵ\n,\nf\n(\nx\n)\n−\nf\n^\n(\nx\n)\n,\nf\n^\n(\nx\n)\n)\n−\nf\n~\n(\nx\n)\n]\nE_xL(f(x)+\\epsilon,\\tilde f(x)+[\\hat f(x)-\\hat f(x)])=F[\\epsilon,f(x)-\\hat f(x),\\hat f(x))-\\tilde f(x)]\nEx L(f(x)+ϵ,f~ (x)+[f^ (x)−f^ (x)])=F[ϵ,f(x)−f^ (x),f^ (x))−f~ (x)]\nf\n(\nx\n)\nf(x)\nf(x)是客观世界的模型，\nϵ\n\\epsilon\nϵ是观察噪声或者是样本产生过程中的系统噪声，\nf\n^\n(\nx\n)\n\\hat f(x)\nf^ (x)是当前模型下能够学习到的最好的模型参数下的模型，\nf\n~\n(\nx\n)\n\\tilde f(x)\nf~ (x)是用有限的训练样本实际训练出来的模型，\nL\nL\nL为损失函数，\nE\nx\nL\nE_xL\nEx L为泛化误差。\n我们把\n∣\nf\n(\nx\n)\n−\nf\n^\n(\nx\n)\n∣\n|f(x)-\\hat f(x)|\n∣f(x)−f^ (x)∣成为bias（偏差），它越大说明本身模型越简单（欠拟合）\n∣\nf\n^\n(\nx\n)\n)\n−\nf\n~\n(\nx\n)\n∣\n|\\hat f(x))-\\tilde f(x)|\n∣f^ (x))−f~ (x)∣成为variance（方差），它越大说明模型过拟合越严重（把噪声当作是模型的输出进行拟合）。\n欠拟合产生的原因是拟合的模型过于简单，无法拟合真正的客观模型。\n过拟合产生的原因是数据量太少，无法把模型的参数拟合得很好。\n我们在进一步的挖掘一下，过拟合的原因从而更深刻的体会一下正则化的作用。\nthe amount of parameter vs the amount of data\nChebyshev 不等式 / 大数定理\n由Markov不等式\nP\n(\nx\n&gt;\nϵ\n)\n&lt;\nE\n[\nx\n]\nϵ\nP(x&gt;\\epsilon)&lt;\\frac{E[x]}{\\epsilon}\nP(x>ϵ)<ϵE[x] 可得\nP\n[\n(\n1\nn\n∑\ni\n=\n1\nn\nX\n−\nE\nX\n)\n2\n&gt;\nϵ\n]\n&lt;\nE\n[\n(\n1\nn\n∑\ni\n=\n1\nn\nX\n−\nE\nX\n)\n2\n]\nϵ\n=\nσ\n2\nϵ\nn\n2\nP[(\\frac{1}{n}\\sum_{i=1}^nX-EX)^2&gt;\\epsilon]&lt;\\frac{E[(\\frac{1}{n}\\sum_{i=1}^nX-EX)^2]}{\\epsilon}=\\frac{\\sigma^2}{\\epsilon n^2}\nP[(n1 i=1∑n X−EX)2>ϵ]<ϵE[(n1 ∑i=1n X−EX)2] =ϵn2σ2\n数据量和模型参数误差的关系\n模型参数可以看成是模型维度的数据统计量（例如模型就是预测值就是直接输出训练集的平均值，那么参数就直接是数据的平均），那么，当参数多了之后，相当于把数据分给不同的参数减少，这可能有点难以理解，可以想象成一个决策树，分支之后每个分支的数据量减少，分支越多，每个分支的数据量就越少。或者还可以换个角度理解，确定A参数之后在确定B参数，B参数的误差会因为A参数的误差而增大。所以参数越多，误差就越大。\n正则化为什么可以降低泛化误差呢，因为正则化相当于给参数之间一定的关系，例如\nl\n1\nl_1\nl1 正则化相当于去掉一些参数，从而使得分配到每个参数上的数据量增多，而\nl\n2\nl_2\nl2 正则化相当于参数之间共同进退，把异常值的贡献平均分配到各个参数上，因而参数分配数据量就不是数据量除以参数个数了，不同参数之间的相关性使得数据“公用”到各个参数上。\n虽然这个解释不是很严谨，但是我个人感觉比较容易理解和直观。\nP.S. 我自己自瞎想的，如有错误，还请有缘人指正\nNo Free Lunch Theorem\n若学习算法\nL\na\nL_a\nLa 在某些问题（数据集）上比学习算法\nL\nb\nL_b\nLb 要好，那么必然存在另一些问题（数据集），在这些问题中\nL\nb\nL_b\nLb 比\nL\na\nL_a\nLa 表现更好。\n符号说明：\nΞ\n\\Xi\nΞ:样本空间\nH\nH\nH:假设空间\nL\na\nL_a\nLa :学习算法\nP\n(\nh\n∣\nX\n,\nL\na\n)\nP(h|X,L_a)\nP(h∣X,La ) : 算法\nL\na\nL_a\nLa 基于训练数据\nX\nX\nX产生假设\nh\nh\nh的概率\nf\nf\nf:代表希望学得的真实目标函数\note是off-training error，即训练集外误差\nE\no\nt\ne\n(\nL\na\n∣\nX\n,\nf\n)\n=\n∑\nh\n∑\nx\n∈\nΞ\n−\nX\nP\n(\nx\n)\nI\n(\nh\n(\nx\n)\n≠\nf\n(\nx\n)\n)\nP\n(\nh\n∣\nX\n,\nL\na\n)\nE_{ote}(L_a|X,f)=\\sum_h\\sum_{x\\in \\Xi-X}P(x)I(h(x)≠f(x))P(h|X,L_a)\nEote (La ∣X,f)=∑h ∑x∈Ξ−X P(x)I(h(x)̸ =f(x))P(h∣X,La )：算法\nL\na\nL_a\nLa 学得的假设在训练集外的所有样本上的误差的期望（这里的累加可以看作是积分的简化，积分更严谨的感觉；查阅文献后发现，该定理只是定义在有限的搜索空间，对无限搜索空间结论是否成立尚不清楚）\n因为是存在性问题，我们就假设真实分布\n(\nx\n,\nf\n(\nx\n)\n)\n(x,f(x))\n(x,f(x))的\nf\nf\nf在假设空间内均匀分布，那么\nE\nf\n[\nE\no\nt\ne\n(\nL\na\n∣\nX\n,\nf\n)\n]\n=\n∑\nf\n∑\nh\n∑\nx\n∈\nΞ\n−\nX\nP\n(\nx\n)\nI\n(\nh\n(\nx\n)\n≠\nf\n(\nx\n)\n)\nP\n(\nh\n∣\nX\n,\nL\na\n)\nP\n(\nf\n)\nE_f[E_{ote}(L_a|X,f)]=\\sum_f\\sum_h\\sum_{x\\in \\Xi-X}P(x)I(h(x)≠f(x))P(h|X,L_a)P(f)\nEf [Eote (La ∣X,f)]=f∑ h∑ x∈Ξ−X∑ P(x)I(h(x)̸ =f(x))P(h∣X,La )P(f)\n=\n∑\nx\n∈\nΞ\n−\nX\nP\n(\nx\n)\n∑\nh\nP\n(\nh\n∣\nX\n,\nL\na\n)\n∑\nf\nI\n(\nh\n(\nx\n)\n≠\nf\n(\nx\n)\n)\nP\n(\nf\n)\n=\\sum_{x\\in \\Xi-X}P(x)\\sum_hP(h|X,L_a)\\sum_fI(h(x)≠f(x))P(f)\n=x∈Ξ−X∑ P(x)h∑ P(h∣X,La )f∑ I(h(x)̸ =f(x))P(f)\n=\n∑\nx\n∈\nΞ\n−\nX\nP\n(\nx\n)\n∑\nh\nP\n(\nh\n∣\nX\n,\nL\na\n)\n2\n∣\nΞ\n∣\n2\n=\\sum_{x\\in \\Xi-X}P(x)\\sum_hP(h|X,L_a)\\frac{2^{|\\Xi|}}{2}\n=x∈Ξ−X∑ P(x)h∑ P(h∣X,La )22∣Ξ∣\n=\n2\n∣\nΞ\n∣\n2\n∑\nx\n∈\nΞ\n−\nX\nP\n(\nx\n)\n=\\frac{2^{|\\Xi|}}{2}\\sum_{x\\in \\Xi-X}P(x)\n=22∣Ξ∣ x∈Ξ−X∑ P(x)\n结果与算法\nL\na\nL_a\nLa 无关，说明在\nf\nf\nf未知的情况下，没有任何一个算法比瞎猜强。\n这个定理没啥实用性，但是体现了算法工程师存在的意义。在数据集未知的情况下调大厂的API跟瞎猜一个性质。在脱离实际意义情况下，空泛地谈论哪种算法好毫无意义，要谈论算法优劣必须针对具体学习问题。"}
