{"content2":"6.1引言\nPLA全称是Perceptron Linear Algorithm，即线性感知机算法，属于一种最简单的感知机（Perceptron）模型。感知机（perceptron）是二分类的线性分类模型，它的基本结构如所示，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型，解决的问题是分类问题。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知机模型对新的输入实例进行分类。感知机算法是1957年由Rosenblatt提出的，是支持向量机（SVM）和神经网络的基础，学好感知机对SVM和神经网络大有裨益。\n感知机模型\n其中，\nx\ni\nx_i\nxi 是输入，\nw\ni\nw_i\nwi 表示权重系数，\nb\nb\nb表示偏移常数。\n依照统计学习三要素来说：\n 模型：符号函数（判别模型）；\n 策略：损失函数：误分点到超平面距离之和；\n 算法：利用梯度下降算法进行优化。\n好了，开始学习感知机算法吧，本章的目标（目的）是，求出将训练数据进行线性划分的分类超平面，为此导入误分类的损失函数，利用梯度下降法对损失函数进行最小化，求的感知机模型。\n6.2感知机模型\n6.2.1感知机模型定义\n假设输入空间（特征空间）是\nχ\n⊆\nR\nn\n\\chi\\subseteq R^n\nχ⊆Rn,输出空间是\nY\n=\n+\n1\n,\n−\n1\nY={+1,-1}\nY=+1,−1 。输入\nx\n⊆\nχ\nx \\subseteq\\chi\nx⊆χ表示实例的特征向量，对应于输入空间（特征空间）的点；输出\ny\n⊆\nY\ny\\subseteq Y\ny⊆Y表示实例的类别。由输入空间到输出空间的如下函数：\nf\n(\nx\n)\n=\ns\ni\ng\nn\n(\nw\n⋅\nx\n+\nb\n)\n\\color{red}f(x)=sign(w\\cdot x+b)\nf(x)=sign(w⋅x+b)\n称为感知机。其中，其中，\nw\nw\nw和\nb\nb\nb为感知机的模型参数，\nw\n⊆\nR\nn\nw\\subseteq R^n\nw⊆Rn叫作权值(weight)或权值向量（weight vector），\nb\n⊆\nR\nb\\subseteq R\nb⊆R叫做偏置(bias)，\nw\n⋅\nx\nw\\cdot x\nw⋅x表示\nw\nw\nw和\nx\nx\nx的内积，\ns\ni\ng\nn\n(\n)\nsign()\nsign()是符号函数：\ns\ni\ng\nn\n(\nx\n)\n=\n{\n+\n1\nx\n≤\n0\n−\n1\nx\n&lt;\n0\nsign(x)= \\begin{cases} +1&amp; {x\\leq0}\\\\ -1&amp; {x&lt;0} \\end{cases}\nsign(x)={+1−1 x≤0x<0\n感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或者线性分类器，即函数集合：\n{\nf\n∣\nf\n(\nx\n)\n=\nw\n⋅\nx\n+\nb\n}\n\\{f|f(x)=w\\cdot x+b\\}\n{f∣f(x)=w⋅x+b}\n6.2.2感知机模型几何解释\n线性方程\nw\n⋅\nx\n+\nb\n=\n0\nw\\cdot x+b=0\nw⋅x+b=0对应于特征空间\nR\nn\nR^n\nRn中的一个超平面\nS\nS\nS，其中\nw\nw\nw是超平面的法向量，\nb\nb\nb是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）被分为正负两类，因为超平面\nS\nS\nS称为分类超平面（separating hyperplane）。\n\n【注1】超平面\n在\nR\nn\nR^n\nRn空间中的超平面为：\nw\n⃗\n⋅\nx\n⃗\n+\nb\n=\n0\n\\vec{w}\\cdot \\vec{x}+b=0\nw\n⋅x\n+b=0\n在几维空间中，向量\nw\n⃗\n,\nx\n⃗\n\\vec{w},\\vec{x}\nw\n,x\n就是几维的。当然，\nw\n⃗\n,\nx\n⃗\n\\vec{w},\\vec{x}\nw\n,x\n属于该空间。在二维空间下，该方程表示一条直线，直线是平面的超平面。三维空间下，该方程表示一个平面，平面是空间的超平面。\n【注2】点到超平面的距离\n向量的投影：给定两个向量\nu\n⃗\n,\nv\n⃗\n\\vec{u},\\vec{v}\nu\n,v\n,求\nw\n⃗\n\\vec{w}\nw\n在\nx\n⃗\n\\vec{x}\nx\n上的投影长度，向量间的夹角为\nc\no\ns\nθ\ncosθ\ncosθ。\n\nd\n=\n∣\nu\n⃗\n∣\nc\no\ns\nθ\nd=|\\vec{u}|cos\\theta\nd=∣u\n∣cosθ,\nc\no\ns\nθ\n=\nu\n⃗\n⋅\nv\n⃗\n∣\nu\n⃗\n∣\n∣\nv\n⃗\n∣\ncos\\theta=\\frac{\\vec{u}\\cdot\\vec{v}}{|\\vec{u}||\\vec{v}|}\ncosθ=∣u\n∣∣v\n∣u\n⋅v\n，综上，\nd\n=\nu\n⃗\n⋅\nv\n⃗\n∣\nv\n⃗\n∣\nd=\\frac{\\vec{u}\\cdot\\vec{v}}{|\\vec{v}|}\nd=∣v\n∣u\n⋅v\n.\n点到超平面的距离：假设\nx\n0\nx_0\nx0 是超平面\nw\n⃗\n⋅\nx\n⃗\n+\nb\n=\n0\n\\vec{w}\\cdot \\vec{x}+b=0\nw\n⋅x\n+b=0上任意一点，则点\nx\nx\nx到超平面的距离为\nx\n−\nx\n0\nx-x_0\nx−x0 在超平面法向量\nw\n⃗\n\\vec{w}\nw\n上的投影长度：\nd\n=\n∣\n∣\nw\n(\nx\n−\nx\n0\n)\n+\nb\n∣\n∣\n∣\n∣\nw\n∣\n∣\n=\n∣\n∣\nw\nx\n−\nw\nx\n0\n+\nb\n∣\n∣\n∣\n∣\nw\n∣\n∣\n=\n∣\n∣\nw\nx\n⃗\n+\nb\n∣\n∣\n∣\n∣\nw\n∣\n∣\nd=\\frac{||w(x-x_0)+b||}{||w||}=\\frac{||wx-wx_0+b||}{||w||}=\\frac{||w\\vec{x}+b||}{||w||}\nd=∣∣w∣∣∣∣w(x−x0 )+b∣∣ =∣∣w∣∣∣∣wx−wx0 +b∣∣ =∣∣w∣∣∣∣wx\n+b∣∣\n则\n原点到超平面的距离\n−\nb\n∣\n∣\nw\n∣\n∣\n-\\frac{b}{||w||}\n−∣∣w∣∣b 为 。\n感知机的学习：由训练数据集\nT\n=\n{\n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\n.\n.\n.\n(\nx\nN\n,\ny\nN\n)\n}\nT=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}\nT={(x1 ,y1 ),(x2 ,y2 ),...(xN ,yN )}（实例的特征向量以及类别）,其中，\nx\ni\n∈\nχ\n⊆\nR\nn\nx_i\\in \\chi \\subseteq R^n\nxi ∈χ⊆Rn,\ny\ni\n∈\nY\n⊆\n=\n{\n+\n1\n,\n−\n1\n}\ny_i\\in Y \\subseteq =\\{+1,-1\\}\nyi ∈Y⊆={+1,−1} ，\ni\n=\n1\n,\n2\n,\n3\n,\n.\n.\n.\n,\nN\ni=1,2,3,...,N\ni=1,2,3,...,N ，求得感知机模型，也就是求出参数\nw\nw\nw和\nb\nb\nb。\n感知机的预测：通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。\n6.3感知机策略\n给定一个数据集，\nT\n=\n{\n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\n.\n.\n.\n(\nx\nN\n,\ny\nN\n)\n}\nT=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}\nT={(x1 ,y1 ),(x2 ,y2 ),...(xN ,yN )}，\nx\ni\n∈\nχ\n⊆\nR\nn\nx_i\\in \\chi \\subseteq R^n\nxi ∈χ⊆Rn,\ny\ni\n∈\nY\n⊆\n=\n{\n+\n1\n,\n−\n1\n}\ny_i\\in Y \\subseteq =\\{+1,-1\\}\nyi ∈Y⊆={+1,−1} ，\ni\n=\n1\n,\n2\n,\n3\n,\n.\n.\n.\n,\nN\ni=1,2,3,...,N\ni=1,2,3,...,N 。如果存在某个超平面：\nw\n⋅\nx\n+\nb\n=\n0\nw\\cdot x+b=0\nw⋅x+b=0\n能够将数据集的\n正实例和负实例\n完全正确地划分到超平面的两侧，即对所有\ny\ni\n=\n±\n1\ny_i=\\pm1\nyi =±1的实例\ni\ni\ni，有：\nw\n⋅\nx\ni\n+\nb\n&gt;\n0\nw\\cdot x_i+b&gt;0\nw⋅xi +b>0；对所有\ny\n=\n−\n1\ny=-1\ny=−1的实例\ni\ni\ni，有\nw\n⋅\nx\ni\n+\nb\n&lt;\n0\nw\\cdot x_i+b&lt;0\nw⋅xi +b<0 ，则称数据集\nT\nT\nT为线性可分数据集（Linear separable dataset）；否则，称数据集\nT\nT\nT线性不可分。\n假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面，为了找出这样的超平面，即确定感知机模型的参数\nw\n，\nb\nw，b\nw，b，需要确定一个学习策略，即定义（经验）损失函数并将损失函数最小化。\n损失函数的一个自然选择是误分类点的总数，但是这样的损失函数不是参数\nw\n，\nb\nw，b\nw，b的连续可导函数，不宜优化。损失函数的另一个选择是误分类点到超平面\nS\nS\nS的总距离，这是感知机所采用的。为此，首先，写出输入空间\nR\nn\nR^n\nRn中任意一点\nx\n0\nx_0\nx0 ,到超平面\nS\nS\nS的距离(点到直线的距离)：\n1\n∣\n∣\nw\n∣\n∣\n∣\nw\n⋅\nx\n0\n+\nb\n∣\n\\frac{1}{||w||}|w\\cdot x_0+b|\n∣∣w∣∣1 ∣w⋅x0 +b∣\n其中\n∣\n∣\nw\n∣\n∣\n||w||\n∣∣w∣∣是\nw\nw\nw的\nL\n2\nL_2\nL2 范数。\n其次，对于误分类点\n(\nx\ni\n,\ny\ni\n)\n(x_i,y_i)\n(xi ,yi )来说，\n−\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\n&gt;\n0\n-y_i(w\\cdot x_i+b)&gt;0\n−yi (w⋅xi +b)>0\n成立。因为当\nw\n⋅\nx\n0\n+\nb\n&gt;\n0\nw\\cdot x_0+b&gt;0\nw⋅x0 +b>0时 ，\ny\ni\n=\n−\n1\ny_i=-1\nyi =−1，而当\nw\n⋅\nx\ni\n+\nb\n&lt;\n0\nw\\cdot x_i+b&lt;0\nw⋅xi +b<0时,\ny\ni\n=\n±\n1\ny_i=\\pm1\nyi =±1 。因此误分点\nx\ni\nx_i\nxi 到超平面\nS\nS\nS的距离可以写成如下公式：\n−\n1\n∣\n∣\nw\n∣\n∣\ny\ni\n(\nw\n⋅\nx\n0\n+\nb\n)\n-\\frac{1}{||w||}y_i(w\\cdot x_0+b)\n−∣∣w∣∣1 yi (w⋅x0 +b)\n这样，假设超平面\nS\nS\nS的误分类点集合为\nM\nM\nM，那么所有误分类点到超平面\nS\nS\nS的总距离为：\n−\n1\n∣\n∣\nw\n∣\n∣\n∑\nx\ni\n⊆\nM\ny\ni\n(\nw\n⋅\nx\n0\n+\nb\n)\n-\\frac{1}{||w||}\\sum_{x_i\\subseteq M}y_i(w\\cdot x_0+b)\n−∣∣w∣∣1 ∑xi ⊆M yi (w⋅x0 +b)\n不考虑\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 ，则得到感知机学习的损失函数。\n【注】不考虑\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 的原因.\n1、\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 恒为正，不影响\n−\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\n-y_i(w\\cdot x_i+b)\n−yi (w⋅xi +b)正负的判断，也就是不影响学习算法的中间过程。因为感知机学习算法是误分类驱动的（只有当出现误分类时才去调整模型，或者说损失函数只与误分类点有关），这里需要注意的是，所谓的“误分类驱动”指的是我们只需要判断\n−\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\n-y_i(w\\cdot x_i+b)\n−yi (w⋅xi +b)的正负来判断分类的正确与否，而\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 并不影响正负值的判断，所以\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 对感知机学习算法的中间过程可有可无。\n2、\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 不影响感知机学习算法的最终结果，因为感知机学习算法最终的终止条件是所有的输入都被正确的分类，即不存在误分类点，则此时的损失函数为0，对应于\n−\n1\n∣\n∣\nw\n∣\n∣\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\n-\\frac{1}{||w||}y_i(w\\cdot x_i+b)\n−∣∣w∣∣1 yi (w⋅xi +b) ，即分子为0.则可以看出\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 对最终结果也无影响。\n综上所述\n，即使忽略\n1\n∣\n∣\nw\n∣\n∣\n\\frac{1}{||w||}\n∣∣w∣∣1 ，也不会对感知机学习算法的执行过程产生任何影响，反而还能简化运算，提高算法执行效率。\n给定一个数据集，\nT\n=\n{\n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\n.\n.\n.\n(\nx\nN\n,\ny\nN\n)\n}\nT=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}\nT={(x1 ,y1 ),(x2 ,y2 ),...(xN ,yN )} ，\nx\ni\n∈\nχ\n⊆\nR\nn\nx_i\\in \\chi \\subseteq R^n\nxi ∈χ⊆Rn,\ny\ni\n∈\nY\n⊆\n=\n{\n+\n1\n,\n−\n1\n}\ny_i\\in Y \\subseteq =\\{+1,-1\\}\nyi ∈Y⊆={+1,−1} ，\ni\n=\n1\n,\n2\n,\n3\n,\n.\n.\n.\n,\nN\ni=1,2,3,...,N\ni=1,2,3,...,N 。感知机\ns\ni\ng\nn\n(\nw\n⋅\nx\n+\nb\n)\nsign(w\\cdot x+b)\nsign(w⋅x+b)学习的损失函数定义为：\nL\n(\nw\n,\nb\n)\n=\n−\n∑\nx\ni\n⊆\nM\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\nL(w,b)=-\\sum_{x_i\\subseteq M}y_i(w\\cdot x_i+b)\nL(w,b)=−∑xi ⊆M yi (w⋅xi +b)\n其中，\nM\nM\nM为误分类点的集合，这个损失函数就是感知机学习的经验风险函数。\n显然，损失函数\nL\n(\nw\n,\nb\n)\nL(w,b)\nL(w,b)是非负的。如果没有误分类点，损失函数值就是0。而且误分类点越少，误分类点离超平面越近，都会使得损失函数值越小。一个特定的样本点的损失函数：在误分类时是参数\nw\n,\nb\nw,b\nw,b的线性函数，在正确分类的时候是0，因此，给定训练数据集\nT\nT\nT,损失函数\nL\n(\nw\n,\nb\n)\nL(w,b)\nL(w,b)是\nw\n,\nb\nw,b\nw,b的连续可导函数。\n总之，感知机学习的策略是在假设空间中选取使得损失函数式最小的模型参数\nw\n,\nb\nw,b\nw,b，即感知机模型。\n6.4感知机学习算法\n6.4.1感知机算法的原始形式\n给定一个数据集\nT\n=\n{\n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\n.\n.\n.\n(\nx\nN\n,\ny\nN\n)\n}\nT=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}\nT={(x1 ,y1 ),(x2 ,y2 ),...(xN ,yN )} ，\nx\ni\n∈\nχ\n⊆\nR\nn\nx_i\\in \\chi \\subseteq R^n\nxi ∈χ⊆Rn,\ny\ni\n∈\nY\n⊆\n=\n{\n+\n1\n,\n−\n1\n}\ny_i\\in Y \\subseteq =\\{+1,-1\\}\nyi ∈Y⊆={+1,−1} ，\ni\n=\n1\n,\n2\n,\n3\n,\n.\n.\n.\n,\nN\ni=1,2,3,...,N\ni=1,2,3,...,N 。求参数\nw\n,\nb\nw,b\nw,b，使其为以下损失函数极小化问题的解：\nm\ni\nn\nw\n,\nb\n(\nL\n(\nw\n,\nb\n)\n)\n=\n−\n∑\nx\ni\n⊆\nM\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\n\\underset{w,b}{min}(L(w,b))=-\\sum_{x_i\\subseteq M}y_i(w\\cdot x_i+b)\nw,bmin (L(w,b))=−∑xi ⊆M yi (w⋅xi +b)\n其中，\nM\nM\nM为误分类点的集合。\n感知机学习算法是误分类驱动的，具体采用\n随机梯度下降\n算法。首先，任意选择一个超平面\nw\n0\n,\nb\n0\nw_0,b_0\nw0 ,b0 ，然后用梯度下降算法不断极小化目标函数。极小化过程中不是一次使\nM\nM\nM中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。\n假设误分类点集合\nM\nM\nM时固定的，那么损失函数\nL\n(\nw\n,\nb\n)\nL(w,b)\nL(w,b)的梯度由：\n▽\nw\nL\n(\nw\n,\nb\n)\n)\n=\n−\n∑\nx\ni\n⊂\nM\ny\ni\nx\ni\n\\bigtriangledown_wL(w,b))=-\\sum_{x_i\\subset M}y_ix_i\n▽w L(w,b))=−∑xi ⊂M yi xi\n▽\nb\nL\n(\nw\n,\nb\n)\n)\n=\n−\n∑\nx\ni\n⊂\nM\ny\ni\nx\ni\n\\bigtriangledown_bL(w,b))=-\\sum_{x_i\\subset M}y_ix_i\n▽b L(w,b))=−∑xi ⊂M yi xi\n给出。\n随机选取一个误分类点\n(\nx\ni\n,\ny\ni\n)\n(x_i,y_i)\n(xi ,yi ),对\nw\n,\nb\nw,b\nw,b进行更新：\nw\n←\nw\n+\nη\ny\ni\nx\ni\nw \\leftarrow w +\\eta y_ix_i\nw←w+ηyi xi\nb\n←\n+\nη\ny\ni\nb\\leftarrow+\\eta y_i\nb←+ηyi\n其中，\nη\n(\n0\n&lt;\nη\n≤\n1\n)\n\\eta(0&lt;\\eta \\leq 1)\nη(0<η≤1)是步长，在统计学习中又称为学习率(learning rate)。这样，通过迭代，可以期待损失函数\nL\n(\nw\n,\nb\n)\nL(w,b)\nL(w,b)不断缩小，直到为0，综上所述，得到如下算法：\n算法：感知机算法的原始形式\n输入：训练数据集，\nT\n=\n{\n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\n.\n.\n.\n(\nx\nN\n,\ny\nN\n)\n}\nT=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}\nT={(x1 ,y1 ),(x2 ,y2 ),...(xN ,yN )} ，其中，\nx\ni\n∈\nχ\n⊆\nR\nn\nx_i\\in \\chi \\subseteq R^n\nxi ∈χ⊆Rn,\ny\ni\n∈\nY\n⊆\n=\n{\n+\n1\n,\n−\n1\n}\ny_i\\in Y \\subseteq =\\{+1,-1\\}\nyi ∈Y⊆={+1,−1} ，\ni\n=\n1\n,\n2\n,\n3\n,\n.\n.\n.\n,\nN\ni=1,2,3,...,N\ni=1,2,3,...,N ，学习率\nη\n(\n0\n&lt;\nη\n≤\n1\n)\n\\eta(0&lt;\\eta \\leq 1)\nη(0<η≤1) ；\n输出：\nw\n,\nb\nw,b\nw,b ；感知机模型\nf\n(\nx\n)\n=\ns\ni\ng\nn\n(\nw\n⋅\nx\n+\nb\n)\nf(x)=sign(w\\cdot x+b)\nf(x)=sign(w⋅x+b) 。\n（1）选取初值\nw\n0\n,\nb\n0\nw_0,b_0\nw0 ,b0 ；\n（2）在训练集中选取数据\n(\nx\ni\n,\ny\ni\n)\n(x_i,y_i)\n(xi ,yi )；\n（3）如果\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\n≤\n0\ny_i(w\\cdot x_i+b)\\leq 0\nyi (w⋅xi +b)≤0\nw\n←\nw\n+\nη\ny\ni\nx\ni\nw \\leftarrow w +\\eta y_ix_i\nw←w+ηyi xi\nb\n←\n+\nη\ny\ni\nb\\leftarrow+\\eta y_i\nb←+ηyi\n（4）转至（2），直到训练集中没有误分点。\n这种学习算法直观上有如下解释，当一个实例点被误分类，即位于分离超平面错误的一侧时，则调整\nw\n,\nb\nw,b\nw,b的值，使分离超平面向该误分类点的一侧移动，以减少误分类点与超平面之间的距离，直到超平面越过该误分类点，使其被正确分类。\n感知机学习算法由于采不同的初始值\n(\nw\n0\n,\nb\n0\n)\n(w_0,b_0)\n(w0 ,b0 )或者选取不同的误分类点（因为在选取误分类点的时候是随机选取的），最终解可以不同。\n例1（原始形式求解）\n如下图所示的训练集，真正实例点是\nx\ni\n=\n(\n3\n,\n3\n)\nT\nx_i=(3,3)^T\nxi =(3,3)T ，\nx\n2\n=\n(\n4\n,\n3\n)\nT\nx_2=(4,3)^T\nx2 =(4,3)T ，负实例点是\nx\n3\n=\n(\n1\n,\n1\n)\nT\nx_3=(1,1)^T\nx3 =(1,1)T ，试用感知机算法的原始形式求感知机模型\nf\n(\nx\n)\n=\ns\ni\ng\nn\n(\nw\n⋅\nx\n+\nb\n)\nf(x)=sign(w\\cdot x+b)\nf(x)=sign(w⋅x+b) 。这里，\nw\n=\n(\nw\n(\n1\n)\n,\nw\n(\n2\n)\n)\nT\nw=(w^{(1)},w^{(2)})^T\nw=(w(1),w(2))T ,\nx\n=\n(\nx\n(\n1\n)\n,\nx\n(\n2\n)\n)\nT\nx=(x^{(1)},x^{(2)})^T\nx=(x(1),x(2))T 。\n感知机实例\n解：构建最优问题：\nm\ni\nn\nw\n,\nb\n(\nL\n(\nw\n,\nb\n)\n)\n=\n−\n∑\nx\ni\n⊆\nM\ny\ni\n(\nw\n⋅\nx\ni\n+\nb\n)\n\\underset{w,b}{min}(L(w,b))=-\\sum_{x_i\\subseteq M}y_i(w\\cdot x_i+b)\nw,bmin (L(w,b))=−∑xi ⊆M yi (w⋅xi +b)\n根据上述算法求解得到\nw\n,\nb\n,\nη\n=\n1\nw,b,\\eta =1\nw,b,η=1。\n（1）取初值，\nw\n0\n=\n0\n,\nb\n0\n=\n0\nw_0=0,b_0=0\nw0 =0,b0 =0 ；\n（2）对\nx\ni\n=\n(\n3\n,\n3\n)\nT\nx_i=(3,3)^T\nxi =(3,3)T ，\ny\ni\n(\nw\n0\n⋅\nx\ni\n+\nb\n0\n)\n=\n0\ny_i(w_0\\cdot x_i+b_0)=0\nyi (w0 ⋅xi +b0 )=0，未能被正确分类，更新\nw\n,\nb\nw,b\nw,b\nw\n1\n=\nw\n0\n+\ny\n1\nx\n1\n=\n(\n3\n,\n3\n)\nT\nw_1=w_0+y_1x_1=(3,3)^T\nw1 =w0 +y1 x1 =(3,3)T,\nb\n1\n=\nb\n0\n+\ny\n1\n=\n1\nb_1=b_0+y_1=1\nb1 =b0 +y1 =1\n得到线性模型\nw\n1\nx\n+\nb\n1\n=\n3\nx\n(\n1\n)\n+\n3\nx\n(\n2\n)\n+\n1\nw_1x+b_1=3x^{(1)}+3x^{(2)}+1\nw1 x+b1 =3x(1)+3x(2)+1\n（3）对\nx\n1\n,\nx\n2\nx_1,x_2\nx1 ,x2 ，显然，\ny\ni\n(\nw\n1\n⋅\nx\ni\n+\nb\n1\n)\n&gt;\n0\ny_i(w_1\\cdot x_i+b_1)&gt;0\nyi (w1 ⋅xi +b1 )>0 被正确分类，不修改\nw\n,\nb\nw,b\nw,b\n对\nx\n3\n=\n(\n1\n,\n1\n)\nT\nx_3=(1,1)^T\nx3 =(1,1)T，\ny\n3\n(\nw\n1\n⋅\nx\n3\n+\nb\n1\n)\n&lt;\n0\ny_3(w_1\\cdot x_3+b_1)&lt;0\ny3 (w1 ⋅x3 +b1 )<0 被误分类，更新\nw\n,\nb\nw,b\nw,b\nw\n2\n=\nw\n1\n+\ny\n3\nx\n3\n=\n(\n2\n,\n2\n)\nT\nw_2=w_1+y_3x_3=(2,2)^T\nw2 =w1 +y3 x3 =(2,2)T,\nb\n2\n=\nb\n1\n+\ny\n3\n=\n0\nb_2=b_1+y_3=0\nb2 =b1 +y3 =0\n得到线性模型\nw\n2\nx\n+\nb\n2\n=\n2\nx\n(\n1\n)\n+\n2\nx\n(\n2\n)\nw_2x+b_2=2x^{(1)}+2x^{(2)}\nw2 x+b2 =2x(1)+2x(2)\n如此循环下去，直到\nw\n7\n=\n(\n1\n,\n1\n)\nT\nw_7=(1,1)^T\nw7 =(1,1)T,\nb\n7\n=\n−\n3\nb_7=-3\nb7 =−3\nw\n7\nx\n+\nb\n7\n=\nx\n(\n1\n)\n+\nx\n(\n2\n)\n−\n3\nw_7x+b_7=x^{(1)}+x^{(2)}-3\nw7 x+b7 =x(1)+x(2)−3\n对于所有的分类点，没有误分类点，损失函数最小。\n分类超平面为：\nx\n(\n1\n)\n+\nx\n(\n2\n)\n−\n3\nx^{(1)}+x^{(2)}-3\nx(1)+x(2)−3\n感知机模型为：\nf\n(\nx\n=\ns\ni\ng\nn\n(\nx\n(\n1\n)\n+\nx\n(\n2\n)\n−\n3\n)\nf(x=sign(x^{(1)}+x^{(2)}-3)\nf(x=sign(x(1)+x(2)−3)\n迭代过程表：\n表1求解的迭代过程\n这是在计算中误分点先后取 得到的分离超平面和感知机模型，如果在计算中误分点依次取 ，那么得到的分离超平面是 。可知，感知机模型算法由于采用不同的初值或选取不同的误分类点，解可以不同。\n【注】收敛性证明：\n《Convergence Proof for the Perceptron Algorithm》\n6.4.1感知机算法的对偶形式\n对偶形式的基本想法是，将w和b表示为实例 和标记 的线性组合的形式，通过求解其系数而求得\nw\nw\nw和\nb\nb\nb。假设设初始值\nw\n0\n,\nb\n0\nw_0,b_0\nw0 ,b0 均为0,。对误分类点\nx\ni\n,\ny\ni\nx_i,y_i\nxi ,yi 通过\nα\ni\n←\nα\ni\n+\nη\ny\ni\nx\ni\n\\alpha_i\\leftarrow\\alpha_i+\\eta y_ix_i\nαi ←αi +ηyi xi\nb\n←\n+\nη\ny\ni\nb\\leftarrow+\\eta y_i\nb←+ηyi\n逐步修改\nw\nw\nw,\nb\nb\nb，设修改\nn\nn\nn次，则\nw\n,\nb\nw,b\nw,b关于\nx\ni\n,\ny\ni\nx_i,y_i\nxi ,yi 的增量分别是和，这里，这样从学习过程中，不难看出，最后学习得到的\nw\n,\nb\nw,b\nw,b可以分别表示为\n∑\ni\n=\n1\nN\nα\ni\ny\ni\nx\ni\n\\sum_{i=1}^{N}\\alpha_iy_ix_i\n∑i=1N αi yi xi\nb\n=\n∑\ni\n=\n1\nN\nα\ni\ny\ni\nb=\\sum_{i=1}^{N}\\alpha_i y_i\nb=∑i=1N αi yi\n其中，\nα\ni\n&gt;\n=\n0\n\\alpha_i&gt;= 0\nαi >=0 ，\ni\n=\n1\n,\n2\n,\n.\n.\n.\n,\nN\ni=1,2,...,N\ni=1,2,...,N当\nη\n=\n1\n\\eta =1\nη=1时，表示第\ni\ni\ni个实例点由于误分而进行更新的次数，实例点更新的次数越多，意味着它离分离超平面越近，也就越难分类。\n算法：感知机算法的对偶形式\n输入：训练数据集，\nT\n=\n{\n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\n.\n.\n.\n(\nx\nN\n,\ny\nN\n)\n}\nT=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}\nT={(x1 ,y1 ),(x2 ,y2 ),...(xN ,yN )}，其中，\nx\ni\n∈\nχ\n⊆\nR\nn\nx_i\\in \\chi \\subseteq R^n\nxi ∈χ⊆Rn,\ny\ni\n∈\nY\n⊆\n=\n{\n+\n1\n,\n−\n1\n}\ny_i\\in Y \\subseteq =\\{+1,-1\\}\nyi ∈Y⊆={+1,−1} ，\ni\n=\n1\n,\n2\n,\n3\n,\n.\n.\n.\n,\nN\ni=1,2,3,...,N\ni=1,2,3,...,N ，学习率\nη\n(\n0\n&lt;\nη\n≤\n1\n)\n\\eta(0&lt;\\eta\\leq 1)\nη(0<η≤1)；\n输出：\nα\n,\nb\n\\alpha,b\nα,b；感知机模型\nf\n(\nx\n)\n=\ns\ni\ng\nn\n(\n∑\nj\n=\n1\nN\nα\nj\ny\nj\nx\nj\n⋅\nx\ni\n+\nb\n)\nf(x)=sign(\\sum_{j=1}^{N}\\alpha_jy_jx_j\\cdot x_i+b)\nf(x)=sign(∑j=1N αj yj xj ⋅xi +b)。\n（1）\na\n←\n0\n,\na\\leftarrow0,\na←0,\nb\n←\n0\nb\\leftarrow0\nb←0；\n（2）在训练集中选取数据\n(\nx\ni\n,\ny\ni\n)\n(x_i,y_i)\n(xi ,yi )；\n（3）如果\ny\ni\n(\n∑\nj\n=\n1\nN\nα\nj\ny\nj\nx\nj\n⋅\nx\ni\n+\nb\n)\n≤\n0\ny_i(\\sum_{j=1}^{N}\\alpha_jy_jx_j\\cdot x_i+b) \\leq0\nyi (∑j=1N αj yj xj ⋅xi +b)≤0\nα\ni\n←\nα\ni\n+\nη\n\\alpha_i\\leftarrow\\alpha_i+\\eta\nαi ←αi +η\nb\n←\n+\nη\ny\ni\nb\\leftarrow+\\eta y_i\nb←+ηyi\n（4）转至（2），直到训练集中没有误分点。\n对偶形式中训练实例仅以内积的形式出现，为了方便，可以预先将训练集中实例间的内积计算出来，并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵：\nG\n=\n[\nx\ni\n⋅\nx\nj\n]\nN\n×\nN\nG=[x_i\\cdot x_j]_{N\\times N}\nG=[xi ⋅xj ]N×N\n例2（对偶形式求解）\n数据同例1，使用感知机学习算法的对偶形式求解感知机模型。\n解：\n（1）取\nα\nj\n=\n0\n\\alpha_j=0\nαj =0\ni\n=\n1\n,\n2\n,\n3\ni=1,2,3\ni=1,2,3 ，\nb\n=\n0\nb=0\nb=0，\nη\n=\n1\n\\eta=1\nη=1\n（2）计算Gram矩阵\n(3)\nG\n=\n[\n18\n21\n6\n21\n25\n7\n6\n7\n2\n]\nG=\\left[ \\begin{matrix} 18 &amp; 21 &amp; 6 \\\\ 21 &amp; 25 &amp; 7 \\\\ 6 &amp; 7 &amp; 2 \\end{matrix} \\right] \\tag{3}\nG=⎣⎡ 18216 21257 672 ⎦⎤ (3)\n（3）误分条件\ny\ni\n(\n∑\nj\n=\n1\nN\nα\nj\ny\nj\nx\nj\n⋅\nx\ni\n+\nb\n)\n≤\n0\ny_i(\\sum_{j=1}^{N}\\alpha_jy_jx_j\\cdot x_i+b) \\leq0\nyi (∑j=1N αj yj xj ⋅xi +b)≤0\n参数更新\nα\ni\n←\nα\ni\n+\n1\n,\nb\n←\n+\ny\ni\n\\alpha_i\\leftarrow\\alpha_i+1,b\\leftarrow+y_i\nαi ←αi +1,b←+yi\n（4）迭代，对于\nx\n1\n=\n(\n3\n,\n3\n)\nT\n,\ny\ni\n(\n0\n∗\ny\ni\nx\n1\n⋅\nx\n1\n+\nb\n0\n)\n=\n0\nx_1=(3,3)^T,y_i(0 *y_ix_1\\cdot x_1+b_0)=0\nx1 =(3,3)T,yi (0∗yi x1 ⋅x1 +b0 )=0，未被正确分类，更新\nα\n,\nb\n\\alpha,b\nα,b\n此时\nα\n1\n=\n0\n,\nα\n2\n=\n0\n,\nα\n3\n=\n0\n,\nb\n=\n0\n\\alpha_1=0,\\alpha_2=0,\\alpha_3=0,b=0\nα1 =0,α2 =0,α3 =0,b=0\nα\n1\n=\nα\n1\n+\n1\n,\nb\n=\nb\n+\n1\n,\n\\alpha_1=\\alpha_1+1,b=b+1,\nα1 =α1 +1,b=b+1,\n因此\nα\n1\n=\n1\n,\nα\n2\n=\n0\n,\nα\n3\n=\n0\n,\nb\n=\n1\n\\alpha_1=1,\\alpha_2=0,\\alpha_3=0,b=1\nα1 =1,α2 =0,α3 =0,b=1\ny\ni\n(\n∑\nj\n=\n1\n3\nα\nj\ny\nj\nx\nj\n⋅\nx\ni\n+\nb\n)\n=\n(\nα\n1\ny\n1\nx\n1\n+\nα\n2\ny\n2\nx\n2\n+\nα\n3\ny\n3\nx\n3\n)\nx\n+\nb\n=\n(\nα\n1\ny\n1\nx\n1\n)\n⋅\nx\n+\nb\n=\n(\n3\n,\n3\n)\nT\n⋅\nx\n+\n1\ny_i(\\sum_{j=1}^{3}\\alpha_jy_jx_j\\cdot x_i+b) =(\\alpha_1y_1x_1+\\alpha_2y_2x_2+\\alpha_3y_3x_3)x+b =(\\alpha_1y_1x_1)\\cdot x+b =(3,3)^T\\cdot x +1\nyi (∑j=13 αj yj xj ⋅xi +b)=(α1 y1 x1 +α2 y2 x2 +α3 y3 x3 )x+b=(α1 y1 x1 )⋅x+b=(3,3)T⋅x+1\n如此循环下去，结果如表2.\n（5）\nw\n=\n2\nx\n1\n+\n0\nx\n2\n−\n5\nx\n3\n=\n(\n1\n,\n1\n)\nT\nw=2x_1+0x_2-5x_3=(1,1)^T\nw=2x1 +0x2 −5x3 =(1,1)T\nb\n=\n−\n3\nb=-3\nb=−3\n分类超平面为：\nx\n(\n1\n)\n+\nx\n(\n2\n)\n−\n3\n=\n0\nx^{(1)}+x^{(2)}-3=0\nx(1)+x(2)−3=0\n感知机模型为：\nf\n(\nx\n)\n=\ns\ni\ng\nn\n(\nx\n(\n1\n)\n+\nx\n(\n2\n)\n−\n3\n)\nf(x)=sign(x^{(1)}+x^{(2)}-3)\nf(x)=sign(x(1)+x(2)−3)\n迭代过程如下：\n表2迭代过程\n对照例1，结果一致，迭代步骤相互对照，且解也有多个。\n参考文献：\n[1]统计学习方法，李航"}
