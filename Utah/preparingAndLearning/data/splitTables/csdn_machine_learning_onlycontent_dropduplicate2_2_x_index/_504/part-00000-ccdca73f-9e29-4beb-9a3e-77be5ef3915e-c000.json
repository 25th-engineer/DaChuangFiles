{"content2":"这篇文章是系列文章的第1部分,第2部分将阐述AutoML和神经架构搜索、第3部分将特别地介绍Google的AutoML。\n关于机器学习人才的稀缺和公司声称他们的产品能够自动化机器学习而且能完全消除对ML专业知识需求的承诺经常登上媒体的新闻头条。在TensorFlow DevSummit的主题演讲中，Google的AI总指挥Jeff Dean估计，有数千万拥有可用于机器学习的数据而缺乏必要的专业知识和技能的组织。因为我在fast.ai主要专注于让更多的人去使用机器学习并且让它更容易使用，所以我密切关注刚才所提的机器学习人才稀缺等问题。\n在考虑如何使机器学习的一些工作自动化以及让具有更广泛背景的人更容易使用这项技术，首先有必要问的是:机器学习行业从业者到底在做什么？任何用来解决机器学习专业知识稀缺的方案都需要回答这个问题：我们是否知道去教什么技能、去建立什么工具或者去自动化什么工序。\n从事机器学习行业的人做什么？\n构建数据产品是一项复杂的工作\n虽然许多关于机器学习的学术来源几乎都是预测建模，但这只是从事机器学习在正常情况下做的其中一件事。适当地分析商业问题，收集和清理数据，构建模型，实施结果，然后监控变化的过程在很多方式中是相互关联的，这往往很难仅仅通过单个部分进行孤立（至少不知道其他部分需要什么）。正如Jeremy Howard等人在《设计出色的数据产品》上写道：伟大的预测建模是解决方案的重要组成部分，但它不再独立;随着产品变得越来越复杂，它就会消失在管道中。\n构建数据产品是一项复杂的工作\n来自Google，D. Sculley等的一个团队撰写了经典的机器学习案例：《技术债务的高利率信用卡》，这是关于在实践中使用机器学习时时常产生的代码复杂性和技术债务。作者发现了许多系统级别的交互、风险和反模式，包括：\n1.胶水代码：为了将数据输入和输出通用软件包而编写的大量支持代码；\n2.管道丛林（pipeline jungles）：以ML友好格式准备数据的系统可能成为刮擦，连接和采样步骤的丛林，通常带有中间文件输出；\n3.重新使用输入信号的方式会导致其他不相交系统的意外紧耦合；\n4.外部环境的变化可能使模型或输入信号的行为意外发生改变的风险，这些可能难以监控。\n作者写道：现实世界的“机器学习”工作中一个重要部分是致力于解决这种形式的问题...值得注意的是，胶水代码和管道丛林是整合问题的症状，可能是过度分离的“研究”和“工程”角色的根本原因 ...学术界可能会惊讶地发现，许多机器学习系统中只有很小一部分代码实际上在进行“机器学习”。\n当机器学习项目失败时\n在其中一次机器学习项目中，我发现了在工作空间失效的故障模式：\n1.数据科学团队构建了一个非常酷的东西却永远不会被使用。关于他们正在进行的工作，组织的其余部门没有任何支持，而且一些数据科学家对将什么投入生产并不十分清楚。\n2.数据科学家积压生产模型的速度比工程支持生产模型要快得多。\n3.数据架构工程师由数据科学家分离出来。管道中现在没有数据科学家所要求的数据，数据科学家也在利用数据架构工程师所收集的数据源。\n4.该公司已经明确决定生产功能/产品X.他们需要数据科学家来收集支持此决策的一些数据。数据科学家感觉PM正在忽略与决策相矛盾的数据; PM认为数据科学家正在忽视其他商业逻辑。\n5. 数据科学家大材小用：数据科学团队采访了一位令人印象深刻的数学建模和工程技巧的职位申请者，一旦被聘用，求职者就会加入到需要简单业务分析的垂直产品团队中。\n在之前，我将这些视为组织失败，但它们也可以被描述为各种从业者过分关注构成完整数据产品的复杂系统的一部分。这些是数据产品管道的不同部分之间的沟通和目标对齐的失败。\n那么，从事机器学习行业的人做什么？\n如上所述，构建机器学习产品是一项多方面且复杂的任务。以下是机器学习从业者在此过程中可能需要做的一些事情：\n理解上下文：\n1.确定可以从机器学习中受益的商业领域；\n2.与其他利益相关者沟通有关机器学习是什么和自己不具备的能力（通常存在许多误解）；\n3.了解商业战略，风险和目标，确保每个人都在同一平台上；\n4.确定组织拥有哪种数据；\n5.适当地构建和审视任务；\n6.理解操作约束（例如，在推理的时候选出实际可用的数据）；\n7.主动识别道德风险，包括骚扰者或进行宣传/虚假宣传活动（并计划如何降低这些风险）；\n8.识别潜在的偏见和潜在的负反馈循环。\n数据：\n1.制定计划收集更多不同的数据；\n2.将来自许多不同来源的数据整理在一起：这些数据通常以不同的格式或不一致的惯例收集；\n3.处理丢失或损坏的数据；\n4.可视化数据；\n5.创建适当的训练集，验证集和测试集；\n模型：\n1.选择使用哪种模型；\n2.将模型资源需求纳入约束（例如，完成的模型是否需要在边缘设备上运行，在低内存或高延迟环境中运行等）；\n3.选择超参数（例如，在深度学习的情况下，这包括选择架构、损失函数和优化器）；\n4.训练模型（并调试为什么训练不成功），这可能涉及：\n4.1调整超参数（例如学习率）；\n4.2输出中间结果，以查看损失，训练误差和验证误差如何随时间变化；\n4.3检查模型错误的数据以查找模式；\n4.4识别数据潜在的错误或问题；\n4.5思考你需要改变清理和预处理数据的方式；\n4.6意识到你需要更多或不同的数据增强；\n4.7意识到你需要更多或不同的数据；\n4.8尝试不同的模型；\n4.9确定你的数据是否欠拟合或过拟合；\n产品化：\n1.使用你的模型作为端点创建API或Web应用程序以实现产品化；\n2.将模型导出为所需的格式；\n3.计划你的模型需要使用更新的数据进行重新训练的频率；\n监控：\n1.跟踪模型随时间的变化；\n2.监控输入数据，以确定它是否随着时间的推移而变化，从而使模型无效；\n3.将你的结果传达给组织的其他成员；\n4.制定监督和应对错误或意外后果的计划。\n当然，不是每个机器学习从业者都需要完成上述所有步骤，但此过程的组件将成为许多机器学习应用程序的一部分。即使你只是处理这些步骤的一部分，熟悉其余过程也有助于确保你不会忽视会妨碍项目成功的注意事项！\n机器学习中最难的两个部分\n对于我和我认识的许多其他人，我要强调机器学习（特别是深度学习）中最耗时和最令人沮丧的两个方面：\n1.处理数据格式化，不一致和错误通常是一个混乱和繁琐的过程。\n2.训练深度学习模型是一个众所周知的脆弱过程。\n清理数据真的是ML的一部分吗？是。\n处理数据格式化，不一致和错误通常是一个混乱和繁琐的过程。人们有时会将机器学习描述为从数据科学分离的过程，就像机器学习一样，你可以从完美地清理数据，格式化数据集开始。但是，根据我的经验，清理数据集和训练模型的过程通常是交织在一起的：我经常在模型训练中发现导致我返回并改变输入数据的预处理的问题。\n有必要去处理杂乱和不一致的数据\n训练深度学习模型是脆弱且困难的\n训练模型的困难吓退了许多常常感到沮丧的初学者。甚至专家经常抱怨模型训练过程有多么令人沮丧和变幻无常。斯坦福大学的一位人工智能研究员告诉我，他教过深度学习课程，并让所有学生都做自己的项目，真是太难了！学生们无法让他们的模型进行训练，我们通常都会说：“好，这是深度学习”。拥有十多年经验并获得NIPS 2017年度时间奖的人工智能研究员Ali Rahimi在他的NIPS奖颁奖演讲中抱怨模型训练的脆弱性。有人向AI研究人员询问:你们当中有多少人已经从零开始设计了一个深层网络，从设计开始，架构以及其他流程来构建它，当它无效时，你感到糟糕透了？许多人举了手。对于我来说，大概每3个月发生一次。甚至AI专家有时也难以训练新模型，这一事实意味着该过程至今不能将其纳入通用产品的方式实现自动化。一些深度学习的最大进步将通过发现更强大的训练方法来实现。我们已经看到了一些像dropout（dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃）、超融合和迁移学习这样的进步，所有这些都使训练变得更容易。通过迁移学习的力量，当为足够狭窄的问题域定义时，模型训练可以是一个健壮的过程。但是，我们仍然有办法让训练更加健壮。\n对于学术研究人员\n即使你正在从事机器学习的理论研究，理解机器学习从业者在实际问题中所经历的过程也是有用的，因为这可能会为你提供关于最相关或最具影响力的研究领域的见解。\n正如Googler工程师D. Sculley等人写道，技术债务是工程师和研究人员都需要注意的问题。为了提供微小精度优势而以大幅提高系统复杂性为代价的研究解决方案很少是明智的做法......降低技术债务并不总是像证明新定理那样令人兴奋，但它是持续强劲创新的关键部分。为复杂的机器学习系统开发全面，优雅的解决方案是非常有益的工作。\nAutoML\n现在我们已经概述了机器学习从业者在其工作中所做的一些任务，我们已经准备好评估自动完成这项工作的尝试。顾名思义，AutoML是一个专注于自动化机器学习的领域，作为AutoML的子领域的神经架构搜索，目前正受到大量关注。\n以上为译文。\n本文由阿里云云栖社区组织翻译。\n文章原标题《what do machine learning practitioners actually do?》，\n作者：Rachel Thomas 译者：虎说八道，审校：。\n阅读原文\n本文为云栖社区原创内容，未经允许不得转载。"}
