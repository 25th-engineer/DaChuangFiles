{"content2":"之前学习没有想透，最近交流时，在这个问题上磕绊了几次，打算仔细思考下。\n关于机器学习中的线性和非线性，有两个聚焦点，一个是问题，一个是模型。\n问题的线性非线性，指的是样本点的分布，是否能在输入空间上用线性超平面区分。\n模型的线性非线性，是这次讨论的重点。\n模型的非线性\n基础数学说，线性指变量之间的数值关系，即满足成比例。因此，变量之间的多项式、指数等关系都算是非线性。\n网上有一批文章认为，ML 模型的线性非线性，指模型参数之间的关系，即决策函数 y=w_1*x_1^2 + w_2*x_2^2 是线性模型，而 y=w_1^2*x_1 + w_2^2*x_2 是非线性模型。\n我认为这是不对的。\nML 模型的的线性非线性，应该指特征变量之间的关系，即决策函数 y=w_1*x_1^2 + w_2*x_2^2 是非线性模型，而 y=w_1^2*x_1 + w_2^2*x_2 是线性模型。\n实际上，并不会出现后一种决策函数，貌似没有哪种模型学到的决策函数会在参数上进行非线性变换，那么这种变换并不会在 loss function 的优化过程中起到作用，最终对学习到的 model 并不会产生影响，因此，后一种模型和 y=w_1*x_1 + w_2*x_2 并无二致。\n谈谈 LR、SVM 和 MLP 中的非线性\nLR，准确来说应该是广义线性模型。其决策面仍是线性的 y=w_1*x_1 + w_2*x_2，只不过在输出时，套用 sigmoid 函数，得到了分类的置信度。如果从最后的决策函数 f(x) = 1 / (1+exp(w_1*x_1 + w_2*x_2)) 来看，确实特征变量 x_1 和 x_2 之间由于 sigmoid 函数作用，呈现了非线性关系，但这种非线性并不是直接作用在输入特征空间的，而是对输出空间进行的非线性映射。\nSVM，有线性和非线性版本。线性SVM，其模型本身就是在寻求一个超平面，只是策略是找到间隔最大的那个超平面。而非线性SVM，虽说在特征空间上仍是分类超平面，但是先采用了核技巧从输入空间向特征空间进行了非线性映射。\nMLP，其嵌套函数的特点就反映了，它的非线性更像 LR ，即从每层来看，输入并没有进行 SVM 那样的非线性特征变换，但在输出时进行了非线性映射，那么多层重叠，也就实现了特征的非线性交叉。\nML 中的非线性\n通过上面分析，就能看出，ML 模型的非线性，指的是特征之间的非线性关系。这种非线性的引入，目前来看有两种思路：\n一种是对输入特征进行非线性映射，如 SVM 的输入空间到特征空间的非线性高维映射\n一种是对输出结果进行非线性映射，如 LR 的输出加持 sigmoid\nMLP 的非线性本质同 后一种，但由于多层作用，也相当于组合了两种思路。\n回过头再来看看，问题的非线性通常需要求助于模型的非线性。但有些模型的非线性程度可能不够，不足以解决复杂的非线性问题。\nMLP 不用担心，SVM 虽然不知道进行了怎样的特征映射，但内积回旋的非线性也是很强的，貌似 LR 就差了点。\nLR 的非线性并没有直接对特征进行变换，非线性能力就弱了点。比如，原本距离决策面远的样本点，只是相对决策面的绝对距离经 sigmoid 变换被拉到了 0-1 之间，但样本点们相对决策面的相对距离（顺序）并没有改变，离得远的样本点在这种变换后还是远。\n那么如果需要提高模型的非线性能力，该如何做呢？从上面的总结就可以看出来：\n要么看看对输出进行非线性映射如何\n要么看看能不能再对输入特征进行了非线性变换\n以后一种来说，\n进行特征组合，即使是多样式组合也产生了非线性，sklearn 中的 PolynomialFeature 类就是讲原本的 d 维扩展到了 C_(q+d)^d 维\n引入核技巧，将低维输入映射到高维空间，产生非线性\n引入新的维度，比如对连续特征离散化，甚至是离散后进行组合\n最后一种算是核技巧的本质，核技巧的非线性产生的根由是因为 n 维空间映射到 m>n 维，即增加表征的维度带来的非线性。"}
