{"content2":"机器学习:集成学习\n继承学习最初的model是并行的去计算一个model在不同的参数下得到的结果，我们从里面找一个最好。有些时候我们的model实在是精度上不去，就可以上集成学习，因为理论支撑：多个model集成的结果最差的情况就是和原来没有什么变化。最终集成得到的model的误差应该是单个model与集成之后model的误差和集成之后model和目标的误差之间的tradeoff。集成学习的一个不可忽视的重点就是每个model是弱分类器，并且每个model之间最好存在一定的差距，(也就是对不同的数据特征有自己的能力).\n上图中，我们可以认为是不同的model集成的结果，灰色比较暗淡的线条代表的是最初的model，虽然在整个数据平面的能力很差，但是但是，他们在局部数据的却能力表现自己 的能力，我们本质上就是挑出局部数据平面分的最好的线条进行了组装，就得到了想要的线条。\n之所以要的是弱分类器，是因为弱分类器以后的改进中还可以对那些原来分错的数据加大权重进行修改，这样写不断的迭代修改，可以保证一定的修改次数。但是你要想对SVM进行集成，SVM第一次就达到了很高的正确度，压根迭代不了三次。还要保证的每次我们的model都会达到50%的精确度，只要大于50%就可以，但是不能太大了。因为我们想保证每个model之间存在差异，那么啥时候的差异最大了，当然是每个model都是比我们猜的强一点。第一点：你不能每次都是错的吧，也就是每次的结果都不如我们随即猜测的结果吧，要是model的result小于0.5，那就扔掉它。第二点，你要是大于0.5又太强悍，0.9这也就是表示你的model一下就搞定了大部分的数据，也就是说早大部分的数据集上你都是牛逼哄哄的，那还要其他的emodel干什么呢？对吧，所以，大于0.5，但有不是0.8，0.9那样的准确度，就可以保证model之间的差异性。\nadaboost的训练可以认为是串行的训练数据，每一个model上的数据采用的booststraping抽样方法，每次都抽取相同个数的数据，又放回的抽取。上面的D代表原始的数据。D^hat代表的是抽样的数据。u^tn代表的数据的权重，也就是对应当前的数据样本的个数。例如(x1,y10就抽取了2个。\ngt和gt+1分别代表的是不同时刻在不同的数据集上进行二分类的model，按照前面说的我们想让每一个model之间的差异性最大，这样在最后得到的model就会更好。这个表达式表达的是希望在当前的数据集上犯错最小化。gt+1实在gt上进化得到的。那么理论上gt的model如果想在t+1时刻的数据集上进行分类的话，性能肯定要相对较低，但是你在垃圾，也不要低于0.5.\n上图表示gt这个model在t+1时刻的数据上分类的结果，之所以是0.5是因为如果一个分类器的结果都不如我们乱猜，要他何用。\n进一步的表示就是把分母拆分成两项，正确的错误的，那么就是想让正确等于错误。注意这里只是想而已，实际上正确的会稍微大于错误的。那么怎样才能让他们相等继而产生一个比较挫的分类器(刚开始的时候是随机的权重，得到一个分类器，然后我们使得错误的等于正确的，就又得到一个权重使得下面的model是一个比较挫的分类器),当然就是正确的乘以错的，错的乘以正确的。\n这样就可以达到归一化的效果\n上面标示，如果等于0.5表示系数就是0，我们认为这个model没有做出贡献，但是等于0 ，全部正确的话，我们认为贡献就是趋于无穷的。"}
