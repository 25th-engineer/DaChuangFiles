{"content2":"能落地的都是NB，不能落地的都是SB。\n人工智能在过去的10年当中取得了长足进步，无论是无人驾驶，还是语音识别、语音合成。在这样的背景下，AI已经成为一个越来越热门的话题，并且已经开始影响我们的日常生活。\n以下是人工智能发展值得关注的六个领域，对电子产品和服务的未来将会产生巨大的影响。我将解释它们是什么、为什么重要、如何被运用，以及列举相关技术领域的公司。\n01强化学习Reinforcement Learning\n强化学习是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。增强学习是机器学习中一个非常活跃且有趣的领域，相比其他学习方法，强化学习更接近生物学习的本质，因此有望获得更高的智能，这一点在棋类游戏中已经得到体现。\nGoogle DeepMind 的AlphaGo就采用了强化学习，强化学习另一个典型的应用是帮助优化Google数据中心降温系统的能源效率，强化学习系统可以将原来降温的能耗降低40%。使用强化学习技术一个重要优势是，训练数据的积累的成本会很低。而监督深度学习技术往往需要非常昂贵的训练数据，并且是很难从实际生活中获取。\n应用：多个智能体（agents）共享同一个模型，各自进行学习；或者与环境中其他智能体交互和学习；学习三维环境导航，比如迷宫、自动驾驶的城市道路；在学习了一系列目标任务后对已观察过的行为进一步增强重述。（学习驾驶或者在电子游戏中为NPC赋予类似人类玩家的行为）\n公司：Google DeepMind, Prowler.io, Osaro, MicroPSI, Maluuba/Microsoft, NVIDIA, Mobileye\n主要研究人员：Pieter Abbeel (OpenAI), David Silver, Nando de Freitas, Raia Hadsell (Google DeepMind), Carl Rasmussen (Cambridge), Rich Sutton (Alberta), John Shawe-Taylor (UCL) 等\n02生成模型Generative Models\n判别模型（discriminative models）主要用于分类和回归任务，生成模型主要用于在样本训练中学习概率分布。\n应用：时序信息模拟；超分辨率图像；2D图像三维重建；基于小样本的数据生成；单输入多输出系统；自然语言交互；半监督学习；艺术风格转换；音乐和声音合成；图像修复\n公司：Twitter Cortex, Adobe, Apple, Prisma, Jukedeck*, Creative.ai, Gluru, Mapillary, Unbabel\n03记忆网络Networks with memory\n为了能让AI系统具有真实世界一样的多样性环境，AI必须持续学习新任务并在未来记住如何处理它们。传统的神经网络并不能记住这么多任务，这个缺点被称为灾变性遗忘（Catastrophic Forgetting）。这是由于当神经网络从解决A问题转向解决B问题的过程中，神经网络会随之变化。也有很多种强大的网络结构赋予了神经网路不同程度的记忆能力。包括长-短记忆网络，能够处理和预测时序；DeepMind的微分神经计算机结合了神经网络和记忆系统的优点，以便从复杂的数据结构中学习；同时还有弹性权重联合算法，根据先前问题的重要性来减慢某些权重。\n应用：对新环境有举一反三能力的学习性智能体(agent)；机械臂控制、自动驾驶、时序预测（金融、视频、物联网）；自然语言理解和预测\n公司：Google DeepMind, NNaisense, SwiftKey/Microsoft Research\n04从更少的数据中学习、建造更小的模型\n众所周知，深度学习需要庞大的数据来进行训练，比如ImageNet的视觉识别大赛，每支队伍需要识别120万张1000种类别的人工标注的图像。如果没有大规模的数据训练，深度学习模型无法使用，也无法完成语音识别和机器翻译这类的复杂任务。\n在解决端到端的问题时，单一神经网络训练所需的数据量只会越来越多，例如从音频录音中识别语音文本。\n和使用多个不同神经网络处理不同人物的组合不同（音频→发音→单词→文本输出）。\n如果要让AI解决一个数据有限、数据成本很高或者获取十分耗时的任务时，能从小样本中学习最优解决方法的模型十分重要。用小量数据进行训练有很多挑战，一个替代的方法把之前机器学习模型知识转移到新的模型上，这叫做转移学习（transfer learning）。\n应用：训练浅层网络来模拟在大规模数据集上训练好的神经网络；与深度网络模型表现相同、但参数更少的模型；机器翻译。\n公司：Geometric Intelligence/Uber, DeepScale.ai, Microsoft Research, Curious AI Company, Google, Bloomsbury AI\n05用于训练和推理的硬件\nAI发展的一个主要催化剂是将GPU用于训练大规模神经网络。训练神经网络需要大量的运算量，GPU用于训练远远快于CPU。自从2012年首个使用GPU的深度神经网络AlexNet出现后，GPU 成为了训练神经网络的首选。在2017年英伟达继续领跑这一领域，而英特尔、高通、超微和谷歌紧随其后。\nGPU最初并不是为了机器学习而制作的，而是用于渲染电子游戏画面。GPU计算精度很高，并且不会经常遭遇内存带宽的限制和数据溢出的问题。有一批专为深度学习定制芯片的创业公司，Google又开发了针对高维机器学习应用的芯片。新型的芯片内存宽带更高、计算能力更强、能耗更低。提高AI系统运算能力为AI公司和用户带来的好处是：更快更高效的训练模型→更好的用户体验→用户更多使用产品→产生更多的数据→数据帮助优化模型。因此，谁能够更快、更高效的训练和部署AI模型，就能拥有巨大的优势。\n应用：快速训练模型（尤其是图片领域）、进行预测时的能源和数据效率、运行前沿AI系统（物联网设备）、随时可进行语音交互的物联网设备、云基础设施服务、自动驾驶汽车、无人机、机器人。\n公司：Graphcore, Cerebras, Isocline Engineering, Google (TPU), NVIDIA (DGX-1), Nervana Systems (Intel), Movidius (Intel), Scortex\n06模拟环境\n如前文所述，为AI系统生成训练数据通常是一个挑战。而且如果要能在现实世界应用，AI需要概括各种情况。因此，开发模拟真实世界物理和行为模型的数字环境需要能够衡量和训练AI通用能力的试验环境。在模拟环境中进行训练有助于我们更好的理解AI如何学习、如何改善自身，同时为我们提供了潜在的可以转换为真实应用的模型。"}
