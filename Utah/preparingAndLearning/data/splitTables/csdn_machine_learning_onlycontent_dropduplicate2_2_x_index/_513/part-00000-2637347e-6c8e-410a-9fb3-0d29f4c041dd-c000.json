{"content2":"面试机器学习岗位或者算法岗位，经常会被问到一些机器学习算法，其中还有很多细节性的知识。在面试中接触到的LR模型是最多的，为什么？大概原因是LR在公司中用的比较多，这时你可能会问了，这个算法不是很简单吗，性能一般是比不上集成学习算法的。对的，确实是这样，但是公司做应用时不仅仅需要考虑性能，还得考虑效率，简单高效很重要。\n1.之前听其他面试者说，遇到过写LR中损失函数的推导，也就是从概率一般式  开始，运用似然函数求解概率最大（被问到：为什么可以用似然函数。答：因为目标是要让预测为正的的概率最大，且预测为负的概率也最大，即每一个样本预测都要得到最大的概率，将所有的样本预测后的概率进行相乘都最大，这就能到似然函数了。）即：\n然后取对数：再乘以负的m分之一，就得到了损失函数。\n2.逻辑回归为什么一般性能差？LR是线性的，不能得到非线性关系，实际问题并不完全能用线性关系就能拟合。\n3.使用L1L2正则化，为什么可以降低模型的复杂度？模型越复杂，越容易过拟合，这大家都知道，加上L1正则化给了模型的拉普拉斯先验，加上L2正则化给了模型的高斯先验。从参数的角度来看，L1得到稀疏解，去掉一部分特征降低模型复杂度。L2得到较小的参数，如果参数很大，样本稍微变动一点，值就有很大偏差，这当然不是我们想看到的，相当于降低每个特征的权重。\n4.那么为什么L1能得到稀疏解呢？L1正则化是L1范数而来，投到坐标图里面，是棱型的，最优解在坐标轴上取到，所以某些部分的特征的系数就为0。\n5.L1正则化不可导，怎么求解？坐标轴下降法（按照每个坐标轴一个个使其收敛），最小角回归（是一个逐步的过程，每一步都选择一个相关性很大的特征，总的运算步数只和特征的数目有关，和训练集的大小无关）\n6.。。。。\n以上都是个人的理解，有误请指出。"}
