{"content2":"一直以来对这个有所疑惑，所里师姐和师兄的解释好像和论文中的在线离线有所不同。现在国内外有这么几种理解方式。\n我就在这边给自己做个小笔记吧。有不对的地方望予以指正，本人必虚心改正。\n在线学习和离线学习(online learning and offline learning)\n理解方式一：\n在这一次训练中：\n在线学习：一个数据点训练完了直接更新权重（而不是一个batch），看到了没？直接更新权重，这里有危害处！（我们无法得知这一次的更新权重是正确的还是错误的，如果恰恰是错误的一次更新，那么我们的模型会有可能渐渐地走向错误方向，残差出现）\n离线学习：一个batch训练完才更新权重，这样的话要求所有的数据必须在每一个训练操作中（batch中）都是可用的，个人理解，这样不会因为偶然的错误把网络带向极端。\n这种理解方式在国外论文中出现比较多，国外称为online and batch learning.离线就是对应batch learning.这两种方式各有优点，在线学习比较快，但是有比较高的残差，离线（batch）学习能降低残差。\n理解方式二：\n在离线学习中，所有的训练数据在模型训练期间必须是可用的。只有训练完成了之后，模型才能被拿来用。简而言之，先训练，再用模型，不训练完就不用模型。\n在在线学习中，恰恰相反，在线算法按照顺序处理数据。它们产生一个模型，并在把这个模型放入实际操作中，而不需要在一开始就提供完整的的训练数据集。随着更多的实时数据到达，模型会在操作中不断地更新。\n理解方式三：\n这一种是知乎一位兄弟的解释。我在实际代码中基本没见过。可能还是学习太少了。暂时收录下吧。\n作者：IvonChen 链接：https://www.zhihu.com/question/35607456/answer/150511176 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n试答\nonline training：\n你有一个样本 你把第一条带入训练 调整权重 再把这一条带进去一次 重复多次 直至误差率很小\n然后再带入下一条 直至跑完整个样本\noffline training：\n你有一个样本 你把第一条带入训练 调整权重 然后带入下一条 直至跑完整个样本 这个时候的误差率可能不让你满意 于是你 把整个样本又做了上述操作\n直到误差很小\noffline其实和batch更相似\n假定这个样本有m条记录\noffline会训练m的整数倍次数\nonline不知道会训练多少次 可能以一条记录训练了10次 第二条8次 第三条1次……\n分享一下伪代码：\nonline：\ninitialize all weights to random value\nfor t in training_set:\nrepeat:\ncompute train_error for t\nadjust weights base on train_error\nuntil error rate is very small or error rate variation stops\noffline:\ninitialize all weights to random value\nrepeat:\nfor t in training_set:\ncompute train_error for t\nadjust weights base on train_error\nuntil error rate is very small or error rate variation stops\n---------------------\n作者：膝盖走路JYM\n来源：CSDN\n原文：https://blog.csdn.net/a133521741/article/details/79221015\n版权声明：本文为博主原创文章，转载请附上博文链接！"}
