{"content2":"本文档记录了《机器学习》第 15 章规则学习相关内容\n基本概念\n形式化定义\n⊕←f1∧f2∧...∧fL\n\\oplus\\leftarrow\\mathbf{f}_1\\wedge\\mathbf{f}_2\\wedge...\\wedge\\mathbf{f}_L\n左侧称为规则头\n右侧称为规则体\nL\nL 为规则的长度\n解决规则冲突\n投票法：判别相同的规则数最多的结果作为最终结果\n排序法：在规则集合上定义一个顺序->带序规则学习/优先级规则学习\n元规则法：定义关于规则的规则指导使用规则集\n规则分类\n命题规则：原子命题+逻辑连接词\n一阶规则：原子公式，谓词、量词\n一阶规则比（逻辑规则？？？）强很多，能表达复杂的关系，称为关系型规则，其语义层面与人类的语义层面一致。\n序贯覆盖\n关键\n如何从训练集学出单条规则\n学习规则的方法\n穷尽搜索\n从空规则开始，将正例类别作为规则头，逐个遍历训练集中的每个属性及取值。\\\n在属性和候选值较多时会存在组合爆炸的问题。\n自顶向下\n从比较一般的规则开始，逐条添加新文字以缩小规则覆盖范围\n生成-测试法\n规则逐渐特化\n对噪声的鲁棒性较强，适用于命题规则学习\n先考虑规则的准确性，然后考虑覆盖的样本数，然后考虑属性次序等等\n自底向上\n从比较特殊的规则开始，逐渐删除文字以扩大规则覆盖范围\n数据驱动法\n规则逐渐泛化\n适用于假设空间较复杂的任务，如一阶规则学习\n剪枝优化\n统计显著性检验\nCN2——似然率统计量LRS\nLRS=2⋅(m̂ +log2(m̂ +m̂ ++m̂ −)(m+m++m−)+m̂ −log2(m̂ −m̂ ++m̂ −)(m−m++m−))\n\\text{LRS}=2\\cdot(\\hat{m}_+\\text{log}_2\\frac{(\\frac{\\hat{m}_+}{\\hat{m}_++\\hat{m}_-})}{(\\frac{m_+}{m_++m_-})}+\\hat{m}_-\\text{log}_2\\frac{(\\frac{\\hat{m}_-}{\\hat{m}_++\\hat{m}_-})}{(\\frac{m_-}{m_++m_-})})\nLRS越大，采用规则集进行预测与直接使用训练集正、反例比例进行猜测的差别越大。\nLRS越小，规则集的效果越可能是偶然现象。\n后剪枝\n减错剪枝REP\n一次训练集学习规则集\n\n\\mathcal{R}\n多轮剪枝：每轮穷举所有可能的简直操作，然后用验证集对剪枝产生的所有候选规则集进行评估，保留最好者\n循环多次\n设训练样本数为\nm\nm，时间复杂度\nO(m4)\nO(m^4)\nIncremental REP\n在REP上改进\n每次生成一条规则\nr\nr立即在验证集上进行剪枝得到规则\nr′\nr'，并将覆盖样例去除\n时间复杂度\nO(mlog2m)\nO(m\\log^2m)\nRIPPER\n使用IREP*剪枝机制生成规则集\n\n\\mathcal{R}\n对\nr∈\nr\\in\\mathcal{R}，生成：\nr′\nr'：基于\nr\nr的覆盖样例，通过IREP*生成的替换规则\nr″\nr''：对\nr\nr增加文字进行特化，然后IREP*生成的修订规则\n将原规则集（\nr\nr）和新规则（替换为\nr′\nr'和\nr″\nr''）分别进行评估，留下最好的\n循环上述过程\n一阶规则学习\n命题规则学习的缺陷：难以处理对象之间的关系。\n引入领域知识\n在现有属性基础上构造新的属性\n基于领域知识设计某种函数机制约束假设空间\nFirst-Order Inductive Learner（FOIL）\n特点\n序贯覆盖\n自顶向下（泛化到特化的过程）\nFOIL增益\nFGain=m̂ +×(log2m̂ +m̂ ++m̂ −−log2m+m++m−)\n\\text{F}_\\text{Gain}=\\hat{m}_+\\times(\\log_2\\frac{\\hat{m}_+}{\\hat{m}_++\\hat{m}_-}-\\log_2\\frac{m_+}{m_++m_-})\nm̂ +\n\\hat{m}_+和\nm̂ +\n\\hat{m}_+分别表示增加候选文字后新规则所覆盖的正负样本数\nm+\nm_+和\nm+\nm_+分别表示原本规则所覆盖的正负样本数\n因为关系数据中的不平衡性，仅考虑正例的信息量\n总结\nFOIL可以被看做是命题规则学习和归纳逻辑程序设计之间的过渡，但其自顶向下的规则生成过程不支持嵌套，所以表达能力仍有不足。\n归纳逻辑程序设计（Inductive Logic Programming，ILP）\n目标：完备的学习一阶规则\n自底向上——特化到泛化的过程，每次学习单条规则\n与普通一阶规则学习相比引入了函数和逻辑表达式的嵌套\n最小一般泛化（Least General Generalization，LGG）\n给定一阶公式\nr1\nr_1和\nr2\nr_2\n找出涉及相同谓词的文字\n常量替换\n在两个公式中出现位置相同——保持\n不同则将它们替换为同一个新变量\n忽略两条公式中不含共同谓词的文字\nR(elative)LGG：初始规则选择方法，考虑所有背景知识。\n逆归结\n演绎（Deduction）和归纳（Induction）\n演绎：从一般性规律出发来探讨具体事物（对应特化）\n归纳：从个别事物出发概括出一般性规律（对应泛化）\n归结和逆归结\n归结：将貌似复杂的逻辑规则与背景知识联系起来化繁为简\n逆归结：基于背景知识发明新的概念和关系\n逆归结形式化定义\n设两个逻辑表达式\nC1\nC_1、\nC2\nC_2成立，且分别包含互补项\nL1\nL_1和\nL2\nL_2，可令\nL=L1=¬L2,C1=A∨L,C2=B∨¬L\nL=L_1=\\neg L_2,C_1=A\\vee L,C_2=B\\vee\\neg L，可以通过归结原理得到归结项\nC=A∨B\nC=A\\vee B。\n与该过程相反，逆归结是研究在已知\nC\nC和某个\nCi\nC_i的情况下如何得到其余\nCj\nC_j：\nC2=(C−(C1−{L}))∨{¬L}\nC_2=(C-(C_1-\\{L\\}))\\vee\\{\\neg L\\}\n逆归结的四种操作\n吸收\n辨识\n内构\n互构\n（×）蕴含、置换和合一（-）\n蕴含：\nX/Y\nX/Y\n置换：用某些项来替换逻辑表达式中的变量\ne.g.\n用\nθ=1/X,2/Y\n\\theta={1/X,2/Y}置换\nC=r1(X,Y)∧r2(X,Y)\nC=r_1(X,Y)\\wedge r_2(X,Y)，可得\nC′=Cθ=r1(1,2)∧r2(1,2)\nC'=C\\theta=r_1(1,2)\\wedge r_2(1,2)\n其中\nX,Y\n{X,Y}称为\nθ\n\\theta的作用域。\n合一：用一种变量置换令两个或多个逻辑表达式相等\ne.g.\n令\nA=r1(1,X),B=r1(Y,2)\nA=r_1(1,X),B=r_1(Y,2)，可以使用\nθ=2/X,1/Y\n\\theta={2/X,1/Y}得到\nAθ=Bθ=r1(1,2)\nA\\theta=B\\theta=r_1(1,2)，在此情况下称\nA\nA和\nB\nB是可合一的，\nθ\n\\theta为合一化子。\n（×）一阶逻辑中利用合一操作搜索互补项\n自动发明新谓词"}
