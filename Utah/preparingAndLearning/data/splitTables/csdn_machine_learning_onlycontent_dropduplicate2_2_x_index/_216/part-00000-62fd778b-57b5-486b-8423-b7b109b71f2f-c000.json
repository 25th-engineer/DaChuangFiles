{"content2":"几点声明：\n1、本文的内容全部来源于七月在线发布的BAT机器学习面试1000题系列；\n2、文章中带斜体的文字代表是本人自己增加的内容，如有错误还请批评指正；\n3、原文中有部分链接已经失效，故而本人重新加上了新的链接，如有不当，还请指正。（也已用斜体标出）\n4、部分答案由于完全是摘抄自其它的博客，所以本人就只贴出答案链接，这样既可以节省版面，也可以使排版更加美观。点击对应的问题即可跳转。\n最后，此博文的排版已经经过本人整理，公式已用latex语法表示，方便读者阅读。同时链接形式也做了优化，可直接跳转至相应页面，希望能够帮助读者提高阅读体验，文中如果因为本人的整理出现纰漏，还请指出，大家共同进步！\n1.请简要介绍下SVM。\nSVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。\n扩展：\n支持向量机学习方法包括构建由简至繁的模型：线性可分支持向量机、线性支持向量机及非线性支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。\n支持向量机通俗导论（理解SVM的三层境界）\n机器学习之深入理解SVM\n2.请简要介绍下Tensorflow的计算图。\n@寒小阳：Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。\n3.请问GBDT和XGBoost的区别是什么？\n@Xijun LI：XGBoost类似于GBDT的优化版，不论是精度还是效率上都有了提升。与GBDT相比，具体的优点有：\n1.损失函数是用泰勒展式二项逼近，而不是像GBDT里的就是一阶导数；\n2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性；\n3.节点分裂的方式不同，GBDT是用的基尼系数，XGBoost是经过优化推导后的。\n知识点链接：集成学习总结\n4.在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？\n曼哈顿距离只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。\n5.百度2015校招机器学习笔试题。\n知识点链接：百度2015校招机器学习笔试题\n6.简单说说特征工程。\n7.关于LR。\n@rickjin：把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，LR为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。\n声明：由于原文中的链接已经失效，故而自己补充了一个新的链接\n知识点链接：机器学习之Logistic回归(逻辑蒂斯回归）\n8.overfitting怎么解决？\ndropout、regularization、batch normalizatin\n9.LR和SVM的联系与区别？\n@朝阳在望，联系：\n1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）\n2、两个方法都可以增加不同的正则化项，如L1、L2等等。所以在很多实验中，两种算法的结果是很接近的。\n区别：\n1、LR是参数模型，SVM是非参数模型。\n2、从目标函数来看，区别在于逻辑回归采用的是Logistical Loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。\n3、SVM的处理方法是只考虑Support Vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。\n4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。\n5、Logic 能做的 SVM能做，但可能在准确率上有问题，SVM能做的Logic有的做不了。\n答案来源：机器学习常见面试问题（一）\n10.LR与线性回归的区别与联系？\n@nishizhen\n个人感觉逻辑回归和线性回归首先都是广义的线性回归，\n其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，\n另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。\n@乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。\n11.为什么XGBoost要用泰勒展开，优势在哪里？\n@AntZ：XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得二阶倒数形式, 可以在不选定损失函数具体形式的情况下用于算法优化分析.本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性。\n12.XGBoost如何寻找最优特征？是又放回还是无放回的呢？\n@AntZ：XGBoost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性.。XGBoost利用梯度优化模型算法, 样本是不放回的(想象一个样本连续重复抽出,梯度来回踏步会不会高兴)。但XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。\n13.谈谈判别式模型和生成式模型？\n判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。\n生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。\n由生成模型可以得到判别模型，但由判别模型得不到生成模型。\n常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场\n常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机\n14.L1和L2的区别。\nL1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。\n比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.\n简单总结一下就是：\nL1范数: 为x向量各个元素绝对值之和。\nL2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数\nLp范数: 为x向量各个元素绝对值p次方和的1/p次方.\n在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。\nL1范数可以使权值稀疏，方便特征提取。\nL2范数可以防止过拟合，提升模型的泛化能力。\n15.L1和L2正则先验分别服从什么分布 ？\n@齐同学：面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。\n16.CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？\n@许韩\n知识点链接（答案解析）：深度学习岗位面试问题整理笔记\n17.说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2…，请写出最终的决策公式。\n答案解析\n18.LSTM结构推导，为什么比RNN好？\n推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。\n19.经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：\n这叫做拼写检查。根据谷歌一员工写的文章How to Write a Spelling Corrector显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现”拼写检查”的功能。\n用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么”拼写检查”要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求\nP(c|w)\nP(c|w)的最大值。而根据贝叶斯定理，有：\nP(c|w)=P(w|c)P(c)P(w)\nP(c|w)=\\frac{P(w|c)P(c)}{P(w)}\n由于对于所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们只要最大化\nP(w|c)P(c)\nP(w|c)P(c)即可。其中：\nP(c)表示某个正确的词的出现”概率”，它可以用”频率”代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。\nP(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见程序员编程艺术第二十八~二十九章：最大连续乘积子串、字符串编辑距离。\n所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。具体的计算过程及此方法的缺陷请参见How to Write a Spelling Corrector。\n20.为什么朴素贝叶斯如此“朴素”？\n因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。\n21.机器学习中，为何要经常对数据做归一化？\n@zhanlijun\n本题解析来源：为什么一些机器学习模型需要对数据进行归一化？\n22.谈谈深度学习中的归一化问题。\n详情参见此视频：深度学习中的归一化\n23.请简要说说一个完整机器学习项目的流程。\n1 抽象成数学问题\n明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。\n这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。\n2 获取数据\n数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。\n数据要有代表性，否则必然会过拟合。\n而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。\n而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。\n3 特征预处理与特征选择\n良好的数据要能够提取出良好的特征才能真正发挥效力。\n特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。\n筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。\n4 训练模型与调优\n直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。\n5 模型诊断\n如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。\n过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。\n误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……\n诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。\n6 模型融合\n一般来说，模型融合后都能使得效果有一定提升。而且效果很好。\n工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。\n7 上线运行\n这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。\n这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。\n故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频《特征处理与特征选择》。\n24.new 和 malloc的区别？\n知识点链接：new 和 malloc的区别\n25.hash 冲突及解决办法？\n@Sommer_Xia\n关键字值不同的元素可能会映象到哈希表的同一地址上就会发生哈希冲突。解决办法：\n1）开放定址法：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。查找时探查到开放的 地址则表明表中无待查的关键字，即查找失败。\n2） 再哈希法：同时构造多个不同的哈希函数。\n3）链地址法：将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。\n4）建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。\n26.如何解决梯度消失和梯度膨胀？\n（1）梯度消失：\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。\n可以采用ReLU激活函数有效的解决梯度消失的情况。\n（2）梯度膨胀：\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。\n可以通过激活函数来解决。\n27.下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）\nA. 特征灵活\nB. 速度快\nC. 可容纳较多上下文信息\nD. 全局最优\n解答：首先，CRF，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模。\n隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择。\n最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉。\n条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。\n答案为B。\n28.简单说下有监督学习和无监督学习的区别？\n有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）\n无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)\n29.了解正则化么？\n正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。\n奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。\n30.协方差和相关性有什么区别？\n相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。\n31.线性分类器与非线性分类器的区别以及优劣。\n如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。\n常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归。\n常见的非线性分类器：决策树、RF、GBDT、多层感知机。\nSVM两种都有(看线性核还是高斯核)。\n线性分类器速度快、编程方便，但是可能拟合效果不会很好。\n非线性分类器编程复杂，但是效果拟合能力强。\n32.数据的逻辑存储结构（如数组，队列，树等）对于软件开发具有十分重要的影响，试对你所了解的各种存储结构从运行速度、存储效率和适用场合等方面进行简要地分析。\n33.什么是分布式数据库？\n分布式数据库系统是在集中式数据库系统成熟技术的基础上发展起来的，但不是简单地把集中式数据库分散地实现，它具有自己的性质和特征。集中式数据库系统的许多概念和技术，如数据独立性、数据共享和减少冗余度、并发控制、完整性、安全性和恢复等在分布式数据库系统中都有了不同的、更加丰富的内容。\n34.简单说说贝叶斯定理。\n在引出贝叶斯定理之前，先学习几个定义：\n条件概率（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。\n比如，在同一个样本空间Ω中的事件或者子集A与B，如果随机从Ω中选出的一个元素属于B，那么这个随机选择的元素还属于A的概率就定义为在B的前提下A的条件概率，所以：P(A|B) = |A∩B|/|B|，接着分子、分母都除以|Ω|得到：\nP(A|B)=P(A∩B)P(B)\nP(A|B)=\\frac{P(A \\cap B)}{P(B)}\n联合概率表示两个事件共同发生的概率。A与B的联合概率表示为\nP(A∩B)\nP(A \\cap B)或者\nP(A，B)\nP(A，B)。\n边缘概率（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。\n接着，考虑一个问题：P(A|B)是在B发生的情况下A发生的可能性。\n1）首先，事件B发生之前，我们对事件A的发生有一个基本的概率判断，称为A的先验概率，用P(A)表示；\n2）其次，事件B发生之后，我们对事件A的发生概率重新评估，称为A的后验概率，用P(A|B)表示；\n3）类似的，事件A发生之前，我们对事件B的发生有一个基本的概率判断，称为B的先验概率，用P(B)表示；\n4）同样，事件A发生之后，我们对事件B的发生概率重新评估，称为B的后验概率，用P(B|A)表示。\n贝叶斯定理的公式表达式：\nP(A|B)=P(B|A)P(A)P(B)\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n35.#include <filename.h> 和#include“filename.h”有什么区别？\n知识点链接：#include<filename.h> 和 #include”filename.h”有什么区别\n36.某超市研究销售纪录数据后发现，买啤酒的人很大概率也会购买尿布，这种属于数据挖掘的哪类问题？(A)\nA. 关联规则发现 B. 聚类 C. 分类 D. 自然语言处理\n37.将原始数据进行集成、变换、维度规约、数值规约是在以下哪个步骤的任务？(C)\nA. 频繁模式挖掘 B. 分类和预测 C. 数据预处理 D. 数据流挖掘\n38.下面哪种不属于数据预处理的方法？ (D)\nA变量代换 B离散化 C 聚集 D 估计遗漏值\n39.什么是KDD？ (A)\nA. 数据挖掘与知识发现 B. 领域知识发现C. 文档知识发现 D. 动态知识发现\n40.当不知道数据所带标签时，可以使用哪种技术促使带同类标签的数据与带其他标签的数据相分离？(B)\nA. 分类 B. 聚类 C. 关联分析 D. 隐马尔可夫链\n41.建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的哪一类任务？(C)\nA. 根据内容检索 B. 建模描述\nC. 预测建模 D. 寻找模式和规则\n42.以下哪种方法不属于特征选择的标准方法？(D)\nA嵌入 B 过滤 C 包装 D 抽样\n43.请用python编写函数find_string，从文本中搜索并打印内容，要求支持通配符星号和问号。\nfind_string('hello\\nworld\\n','wor') ['wor'] find_string('hello\\nworld\\n','l*d') ['ld'] find_string('hello\\nworld\\n','o.') ['or'] 答案 def find_string(str,pat): import re return re.findall(pat,str,re.I)\n44.说下红黑树的五个性质。\n教你初步了解红黑树\n45.简单说下sigmoid激活函数。\n常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。\nSigmoid的函数表达式如下：\n也就是说，Sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。\n压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。\n举个例子，如下图（图引自Stanford机器学习公开课）：\n46.什么是卷积?\n对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。\n非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。\nOK，举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。\n分解下上图\n47.什么是CNN的池化pool层?\n池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n）:\n上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。很简单不是？\n48.简述下什么是生成对抗网络。\nGAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。\n更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。\n如下图中的左右两个场景：\n更多请参见此课程：生成对抗网络\n49.学梵高作画的原理是啥？\n这里有篇如何做梵高风格画的实验教程 教你从头到尾利用DL学梵高作画：GTX 1070 cuda 8.0 tensorflow gpu版，至于其原理请看这个视频：NeuralStyle艺术化图片（学梵高作画背后的原理）。\n50.现在有 a 到 z 26 个元素， 编写程序打印 a 到 z 中任取 3 个元素的组合（比如 打印 a b c ，d y z等）。\n一道百度机器学习工程师职位的面试题\n51.哪些机器学习算法不需要做归一化处理？\n概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化。\n52.说说梯度下降法。\n@LeftNotEasy\n机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)\n53.梯度下降法找到的一定是下降最快的方向么？\n梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在Practical Implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到Superlinear的收敛速度。梯度下降类的算法的收敛速度一般是Linear甚至Sublinear的（在某些带复杂约束的问题）。\n知识点链接：一文清晰讲解机器学习中梯度下降算法（包括其变式算法）\n54.牛顿法和梯度下降法有什么不同？\n@wtq1993\n知识点链接：机器学习中常见的最优化算法\n55.什么是拟牛顿法（Quasi-Newton Methods）？\n@wtq1993\n机器学习中常见的最优化算法\n56.请说说随机梯度下降法的问题和挑战？\n57.说说共轭梯度法？\n@wtq1993\n机器学习中常见的最优化算法\n58.对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法?\n答案链接\n59、什么最小二乘法？\n我们口头中经常说：一般来说，平均来说。如平均来说，不吸烟的健康优于吸烟者，之所以要加“平均”二字，是因为凡事皆有例外，总存在某个特别的人他吸烟但由于经常锻炼所以他的健康状况可能会优于他身边不吸烟的朋友。而最小二乘法的一个最简单的例子便是算术平均。\n最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为：\n由于算术平均是一个历经考验的方法，而以上的推理说明，算术平均是最小二乘的一个特例，所以从另一个角度说明了最小二乘方法的优良性，使我们对最小二乘法更加有信心。\n最小二乘法发表之后很快得到了大家的认可接受，并迅速的在数据分析实践中被广泛使用。不过历史上又有人把最小二乘法的发明归功于高斯，这又是怎么一回事呢。高斯在1809年也发表了最小二乘法，并且声称自己已经使用这个方法多年。高斯发明了小行星定位的数学方法，并在数据分析中使用最小二乘方法进行计算，准确的预测了谷神星的位置。\n对了，最小二乘法跟SVM有什么联系呢？请参见支持向量机通俗导论（理解SVM的三层境界）。\n60、看你T恤上印着：人生苦短，我用Python，你可否说说Python到底是什么样的语言？你可以比较其他技术或者语言来回答你的问题。\n15个重要Python面试题 测测你适不适合做Python？\n61.Python是如何进行内存管理的？\n2017 Python最新面试题及答案16道题\n62.请写出一段Python代码实现删除一个list里面的重复元素。\n1、使用set函数，set(list)；\n2、使用字典函数：\na=[1,2,4,2,4,5,6,5,7,8,9,0] b={} b=b.fromkeys(a) c=list(b.keys()) c\n63.编程用sort进行排序，然后从最后一个元素开始判断。\na=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3] a.sort() last=a[-1] for i inrange(len(a)-2,-1,-1): if last==a[i]: del a[i] else:last=a[i] print(a)\n64.Python里面如何生成随机数？\n@Tom_junsong\nrandom模块\n随机整数：random.randint(a,b)：返回随机整数x,a<=x<=b\nrandom.randrange(start,stop,[,step])：返回一个范围在(start,stop,step)之间的随机整数，不包括结束值。\n随机实数：random.random( ):返回0到1之间的浮点数\nrandom.uniform(a,b):返回指定范围内的浮点数。\n65.说说常见的损失函数。\n对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致（要知道，有时损失或误差是不可避免的），用一个损失函数来度量预测错误的程度。损失函数记为L(Y, f(X))。\n常用的损失函数有以下几种（基本引用自《统计学习方法》）：\n66.简单介绍下Logistics回归。\nLogistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。\n假设函数：\nhθ(x)=g(θTx)=11+e−θTx\nh_{\\theta }(x)=g(\\theta ^T x)=\\frac{1}{1+e^{-\\theta ^{T}x}}\n其中x是n维特征向量，函数g就是Logistic函数。而：\ng(z)=11+e−z\ng(z)=\\frac{1}{1+e^{-z}}的图像是：\n可以看到，将无穷映射到了(0,1)。而假设函数就是特征属于y=1的概率。\nP(y=1|x;θ)=hθ(x)；P(y=0|x;θ)=1−hθ(x)\nP(y=1|x ;\\theta )=h_{\\theta }(x)；P(y=0|x ;\\theta )=1-h_{\\theta }(x)\n67.看你是搞视觉的，熟悉哪些CV框架，顺带聊聊CV最近五年的发展史如何？\n答案解析\n68.深度学习在视觉领域有何前沿进展？\n@元峰\n本题解析来源：深度学习在计算机视觉领域的前沿进展\n69.HashMap与HashTable区别？\nHashMap与Hashtable的区别\n70.在分类问题中,我们经常会遇到正负样本数据量不等的情况,比如正样本为10w条数据,负样本只有1w条数据,以下最合适的处理方法是( )\nA、将负样本重复10次,生成10w样本量,打乱顺序参与分类\nB、直接进行分类,可以最大限度利用数据\nC、从10w正样本中随机抽取1w参与分类\nD、将负样本每个权重设置为10,正样本权重为1,参与训练过程\n@管博士：准确的说，其实选项中的这些方法各有优缺点，需要具体问题具体分析，有篇文章对各种方法的优缺点进行了分析，讲的不错 感兴趣的同学可以参考一下：\nHow to handle Imbalanced Classification Problems in machine learning?\n71.深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假90设三个矩阵的尺寸分别为m∗n，n∗p，p∗q，且m <n <p <q，以下计算顺序效率最高的是（A）\nA.(AB)C\nB.AC(B)\nC.A(BC)\nD.所以效率都相同\n正确答案：A\n@BlackEyes_SGC： m*n*p <m*n*q,m*p*q < n*p*q, 所以 (AB)C 最小\n72.Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:( C )\nA.各类别的先验概率P(C)是相等的\nB.以0为均值，sqr(2)/2为标准差的正态分布\nC.特征变量X的各个维度是类别条件独立随机变量\nD.P(X|C)是高斯分布\n正确答案：C\n@BlackEyes_SGC：朴素贝叶斯的条件就是每个变量相互独立。\n73.关于支持向量机SVM,下列说法错误的是（C）\nA.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力\nB.Hinge 损失函数，作用是最小化经验分类错误\nC.分类间隔为\n1||w||\n\\frac{1}{||w||}，||w||代表向量的模\nD.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习\n正确答案：C\n@BlackEyes_SGC：\nA正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。\nB正确。\nC错误。间隔应该是\n2||w||\n\\frac{2}{||w||}才对，后半句应该没错，向量的模通常指的就是其二范数。\nD正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出\nw=∑iai∗yi∗xi\nw=\\sum_{i}^{ } a_{i}*y_{i}*x_{i}，a变小使得w变小，因此间隔\n2||w||\n\\frac{2}{||w||}变大\n74.在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计( D )\nA.EM算法\nB.维特比算法\nC.前向后向算法\nD.极大似然估计\n正确答案：D\n@BlackEyes_SGC：\nEM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法\n维特比算法： 用动态规划解决HMM的预测问题，不是参数估计\n前向后向算法：用来算概率\n极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数\n注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。\n75.假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是：(BD)\nA.这个被重复的特征在模型中的决定作用会被加强\nB.模型效果相比无重复特征的情况下精确度会降低\nC.如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。\nD.当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题\nE.NB可以用来做最小二乘回归\nF.以上说法都不正确\n正确答案：BD\n@BlackEyes_SGC：NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。\n76.以下哪些方法不可以直接来对文本分类？(A)\nA、Kmeans\nB、决策树\nC、支持向量机\nD、KNN\n正确答案: A分类不同于聚类。\n@BlackEyes_SGC：A：Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。\n77.已知一组数据的协方差矩阵P,下面关于主分量说法错误的是( C )\nA、主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小\nB、在经主分量分解后,协方差矩阵成为对角矩阵\nC、主分量分析就是K-L变换\nD、主分量是通过求协方差矩阵的特征值得到\n正确答案: C\n@BlackEyes_SGC：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。\n78.Kmeans的复杂度？\n时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数。\n具体参考：机器学习之深入理解K-means、与KNN算法区别及其代码实现\n79.关于Logit 回归和SVM 不正确的是（A）\nA. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A错误\nB. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确\nC. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。\nD. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。\n@BlackEyes_SGC：Logit回归目标函数是最小化后验概率，Logit回归可以用于预测事件发生概率的大小，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。\n80.输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为：（）\n正确答案：97\n@BlackEyes_SGC：计算尺寸不被整除只在GoogLeNet中遇到过。卷积向下取整，池化向上取整。\n本题 （200-5+2*1）/2+1 为99.5，取99\n（99-3）/1+1 为97\n（97-3+2*1）/1+1 为97\n研究过网络的话看到stride为1的时候，当kernel为 3 padding为1或者kernel为5 padding为2 一看就是卷积前后尺寸不变。计算GoogLeNet全过程的尺寸也一样。\n81.影响聚类算法结果的主要因素有（BCD ）\nA.已知类别的样本质量；\nB.分类准则；\nC.特征选取；\nD.模式相似性测度\n82.模式识别中，马式距离较之于欧式距离的优点是（CD）\nA. 平移不变性；\nB. 旋转不变性；\nC. 尺度不变性；\nD. 考虑了模式的分布\n83.影响基本K-均值算法的主要因素有(ABD）\nA. 样本输入顺序；\nB. 模式相似性测度；\nC. 聚类准则；\nD. 初始类中心的选取\n84.在统计模式分类问题中，当先验概率未知时，可以使用（BD）\nA. 最小损失准则；\nB. 最小最大损失准则；\nC. 最小误判概率准则；\nD. N-P判决\n85.如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（BC）\nA. 已知类别样本质量；\nB. 分类准则；\nC. 特征选取；\nD. 量纲\n86.欧式距离具有（AB ）；马式距离具有（ABCD ）。\nA. 平移不变性；\nB. 旋转不变性；\nC. 尺度缩放不变性；\nD. 不受量纲影响的特性\n87.你有哪些Deep Learning（RNN，CNN）调参的经验？\n答案解析，来自知乎\n88.简单说说RNN的原理。\n我们升学到高三准备高考时，此时的知识是由高二及高二之前所学的知识加上高三所学的知识合成得来，即我们的知识是由前序铺垫，是有记忆的，好比当电影字幕上出现：“我是”时，你会很自然的联想到：“我是中国人”。\n89.什么是RNN？\n@一只鸟的天空，本题解析来源：\n循环神经网络(RNN, Recurrent Neural Networks)介绍\n90.RNN是怎么从单层网络一步一步构造的的?\n@何之源，本题解析来源：\n完全图解RNN、RNN变体、Seq2Seq、Attention机制\n101.深度学习（CNN RNN Attention）解决大规模文本分类问题。\n用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践\n102.如何解决RNN梯度爆炸和弥散的问题的？\n深度学习与自然语言处理(7)_斯坦福cs224d 语言模型，RNN，LSTM与GRU\n103.如何提高深度学习的性能？\n机器学习系列(10)_如何提高深度学习(和机器学习)的性能\n104.RNN、LSTM、GRU区别？\n@我愛大泡泡，本题解析来源：\n面试笔试整理3：深度学习机器学习面试问题准备（必会）\n105.当机器学习性能遭遇瓶颈时，你会如何优化的？\n可以从这4个方面进行尝试：基于数据、借助算法、用算法调参、借助模型融合。当然能谈多细多深入就看你的经验心得了。\n这里有一份参考清单：机器学习系列(20)_机器学习性能改善备忘单\n106.做过什么样的机器学习项目？比如如何从零构建一个推荐系统？\n推荐系统的公开课http://www.julyedu.com/video/play/18/148，另，再推荐一个课程：机器学习项目班 [10次纯项目讲解，100%纯实战]（https://www.julyedu.com/course/getDetail/48）。\n107.什么样的资料集不适合用深度学习?\n@抽象猴，来源：\n知乎解答\n108.广义线性模型是怎被应用在深度学习中?\n@许韩，来源：\n知乎解答\n109.准备机器学习面试应该了解哪些理论知识？\n知乎解答\n110.标准化与归一化的区别?\n简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为L2的归一化公式如下：\n特征向量的缺失值处理：\n1.缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。\n2.缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:\n1) 把NaN直接作为一个特征，假设用0表示；\n2) 用均值填充；\n3) 用随机森林等算法预测填充\n111.随机森林如何处理缺失值。\n方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。\n方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似1缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩2。\n112.随机森林如何评估特征重要性。\n衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：\n1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。\n2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。\n113.优化Kmeans。\n使用Kd树或者Ball Tree\n将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。\n114.KMeans初始类簇中心点的选取。\nK-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。\n1.从输入的数据点集合中随机选择一个点作为第一个聚类中心\n2.对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)\n3.选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大\n4.重复2和3直到k个聚类中心被选出来\n5.利用这k个初始的聚类中心来运行标准的k-means算法\n115.解释对偶的概念。\n一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将Primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。\n116.如何进行特征选择？\n特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解。\n常见的特征选择方式：\n1.去除方差较小的特征。\n2.正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。\n3.随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。\n4.稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。\n117.数据预处理。\n1.缺失值，填充缺失值fillna：\ni. 离散：None,\nii. 连续：均值。\niii. 缺失值太多，则直接去除该列\n2.连续值：离散化。有的模型（如决策树）需要离散值\n3.对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作\n4.皮尔逊相关系数，去除高度相关的列\n118.简单说说特征工程。\n119.你知道有哪些数据处理和特征工程的处理？\n120.请对比下Sigmoid、Tanh、ReLu这三个激活函数？\n121.Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数？\n@我愛大泡泡，来源：\n面试笔试整理3：深度学习机器学习面试问题准备（必会）\n122.怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感？\n知乎解答\n123.为什么引入非线性激励函数？\n@Begin Again，来源：\n知乎解答\n如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。\n正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是Sigmoid函数或者Tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释）。\n124.请问人工神经网络中为什么ReLu要好过于Tanh和Sigmoid function?\n@Begin Again，来源：\n知乎解答\n125.为什么LSTM模型中既存在Sigmoid又存在Tanh两种激活函数？\n本题解析来源：知乎解答\n@beanfrog：二者目的不一样：sigmoid 用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了。tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。\n@hhhh：另可参见A Critical Review of Recurrent Neural Networks for Sequence Learning的section4.1，说了那两个tanh都可以替换成别的。\n126.衡量分类器的好坏。\n@我愛大泡泡，来源：\n答案解析\n这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。\n几种常用的指标：\n精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）\n召回率 recall = TP/(TP+FN) = TP/ P\nF1值： 2/F1 = 1/recall + 1/precision\nROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N\n127.机器学习和统计里面的auc的物理意义是什么？\n详情参见机器学习和统计里面的auc怎么理解？\n128.观察增益gain, alpha和gamma越大，增益越小？\n@AntZ：XGBoost寻找分割点的标准是最大化gain. 考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，XGBoost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中计算Gain按最大值找出最佳的分割点。它的计算公式分为四项, 可以由正则化项参数调整(lamda为叶子权重平方和的系数, gama为叶子数量)：\n第一项是假设分割的左孩子的权重分数, 第二项为右孩子, 第三项为不分割总体分数, 最后一项为引入一个节点的复杂度损失。\n由公式可知, gama越大gain越小, lamda越大, gain可能小也可能大。\n原问题是alpha而不是lambda, 这里paper上没有提到, XGBoost实现上有这个参数. 上面是我从paper上理解的答案,下面是搜索到的:\n如何对XGBoost模型进行参数调优\n129.什么造成梯度消失问题? 推导一下。\n@许韩，来源：\n神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。\n梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。\n130.什么是梯度消失和梯度爆炸？\n@寒小阳，反向传播中链式法则带来的连乘，如果有数很小趋于0，结果就会特别小（梯度消失）；如果数都比较大，可能结果会很大（梯度爆炸）。\n@单车\n神经网络训练中的梯度消失与梯度爆炸\n131.如何解决梯度消失和梯度膨胀?\n（1）梯度消失：\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0，可以采用ReLU激活函数有效的解决梯度消失的情况。\n（2）梯度膨胀\n根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大，可以通过激活函数来解决。\n132.推导下反向传播Backpropagation。\n@我愛大泡泡，来源：\n推导过程\n133.SVD和PCA。\nPCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。\n134.数据不平衡问题。\n这主要是由于数据分布不平衡造成的。解决方法如下：\n1）采样，对小样本加噪声采样，对大样本进行下采样\n2）进行特殊的加权，如在Adaboost中或者SVM中\n3）采用对不平衡数据集不敏感的算法\n4）改变评价标准：用AUC/ROC来进行评价\n5）采用Bagging/Boosting/Ensemble等方法\n6）考虑数据的先验分布\n135.简述神经网络的发展。\nMP模型+sgn—->单层感知机（只能线性）+sgn— Minsky 低谷 —>多层感知机+BP+Sigmoid— (低谷) —>深度学习+Pretraining+ReLU/Sigmoid\n136.深度学习常用方法。\n@SmallisBig，来源：\n机器学习岗位面试问题汇总 之 深度学习\n137.神经网络模型（Neural Network）因受人类大脑的启发而得名。神经网络由许多神经元（Neuron）组成，每个神经元接受一个输入，对输入进行处理后给出一个输出。请问下列关于神经元的描述中，哪一项是正确的？（E）\nA.每个神经元只有一个输入和一个输出\nB.每个神经元有多个输入和一个输出\nC.每个神经元有一个输入和多个输出\nD.每个神经元有多个输入和多个输出\nE.上述都正确\n答案：（E）\n每个神经元可以有一个或多个输入，和一个或多个输出\n138.下图是一个神经元的数学表示，\n139.在一个神经网络中，知道每一个神经元的权重和偏差是最重要的一步。如果知道了神经元准确的权重和偏差，便可以近似任何函数，但怎么获知每个神经的权重和偏移呢？（C）\nA. 搜索每个可能的权重和偏差组合，直到得到最佳值\nB. 赋予一个初始值，然后检查跟最佳值的差值，不断迭代调整权重\nC. 随机赋值，听天由命\nD. 以上都不正确的\n答案：（C）\n选项C是对梯度下降的描述。\n140.梯度下降算法的正确步骤是什么？( D）\n1.计算预测值和真实值之间的误差\n2.重复迭代，直至得到网络权重的最佳值\n3.把输入传入网络，得到输出值\n4.用随机值初始化权重和偏差\n5.对每一个产生误差的神经元，调整相应的（权重）值以减小误差\nA. 1, 2, 3, 4, 5\nB. 5, 4, 3, 2, 1\nC. 3, 2, 1, 5, 4\nD. 4, 3, 1, 5, 2\n答案：（D）\n141.已知：\n- 大脑是有很多个叫做神经元的东西构成，神经网络是对大脑的简单的数学表达。\n- 每一个神经元都有输入、处理函数和输出。\n- 神经元组合起来形成了网络，可以拟合任何函数。\n- 为了得到最佳的神经网络，我们用梯度下降方法不断更新模型\n给定上述关于神经网络的描述，什么情况下神经网络模型被称为深度学习模型？\nA. 加入更多层，使神经网络的深度增加\nB. 有维度更高的数据\nC. 当这是一个图形识别的问题时\nD. 以上都不正确\n答案：（A）\n更多层意味着网络更深。没有严格的定义多少层的模型才叫深度模型，目前如果有超过2层的隐层，那么也可以及叫做深度模型。\n142.卷积神经网络可以对一个输入进行多种变换（旋转、平移、缩放），这个表述正确吗？\n答案：错误\n把数据传入神经网络之前需要做一系列数据预处理（也就是旋转、平移、缩放）工作，神经网络本身不能完成这些变换。\n143.下面哪项操作能实现跟神经网络中Dropout的类似效果？（B）\nA. Boosting\nB. Bagging\nC. Stacking\nD. Mapping\n答案：B\nDropout可以认为是一种极端的Bagging，每一个模型都在单独的数据上训练，同时，通过和其他模型对应参数的共享，从而实现模型参数的高度正则化。\n144.下列哪一项在神经网络中引入了非线性？（B）\nA. 随机梯度下降\nB. 修正线性单元（ReLU）\nC. 卷积函数\nD .以上都不正确\n答案：（B）\n修正线性单元是非线性的激活函数。\n145.在训练神经网络时，损失函数(loss)在最初的几个epochs时没有下降，可能的原因是？（A）\nA. 学习率(learning rate)太低\nB. 正则参数太高\nC. 陷入局部最小值\nD. 以上都有可能\n答案：（A）\n146.下列哪项关于模型能力（model capacity）的描述是正确的？（指神经网络模型能拟合复杂函数的能力）（A）\nA. 隐藏层层数增加，模型能力增加\nB. Dropout的比例增加，模型能力增加\nC. 学习率增加，模型能力增加\nD. 都不正确\n答案：（A）\n147.如果增加多层感知机（Multilayer Perceptron）的隐藏层层数，分类误差便会减小。这种陈述正确还是错误？\n答案：错误\n并不总是正确。过拟合可能会导致错误增加。\n148.构建一个神经网络，将前一层的输出和它自身作为输入。下列哪一种架构有反馈连接？（A）\nA. 循环神经网络\nB. 卷积神经网络\nC. 限制玻尔兹曼机\nD. 都不是\n答案：（A）\n149.下列哪一项在神经网络中引入了非线性？在感知机中（Perceptron）的任务顺序是什么？\n1.随机初始化感知机的权重\n2.去到数据集的下一批（batch）\n3.如果预测值和输出不一致，则调整权重\n4.对一个输入样本，计算输出值\n答案：1 - 4 - 3 - 2\n150.假设你需要调整参数来最小化代价函数（cost function），可以使用下列哪项技术？（D）\nA. 穷举搜索\nB. 随机搜索\nC. Bayesian优化\nD. 以上任意一种\n答案：（D）\n151.在下面哪种情况下，一阶梯度下降不一定正确工作（可能会卡住）？（B）\n答案：（B）\n这是鞍点（Saddle Point）的梯度下降的经典例子。另，本题来源于：题目来源\n152.下图显示了训练过的3层卷积神经网络准确度，与参数数量(特征核的数量)的关系。\n从图中趋势可见，如果增加神经网络的宽度，精确度会增加到一个特定阈值后，便开始降低。造成这一现象的可能原因是什么？（C）\nA. 即使增加卷积核的数量，只有少部分的核会被用作预测\nB. 当卷积核数量增加时，神经网络的预测能力（Power）会降低\nC. 当卷积核数量增加时，它们之间的相关性增加(correlate)，导致过拟合\nD. 以上都不正确\n答案：（C）\n如C选项指出的那样，可能的原因是核之间的相关性。\n153.假设我们有一个如下图所示的隐藏层。隐藏层在这个网络中起到了一定的降维作用。假如现在我们用另一种维度下降的方法，比如说主成分分析法(PCA)来替代这个隐藏层。那么，这两者的输出效果是一样的吗？\n答案：不同，因为PCA用于相关特征而隐层用于有预测能力的特征。\n154.神经网络能组成函数(\ny=1x\ny=\\frac{1}{x})吗？\n答案：可以，因为激活函数可以是互反函数。\n155.下列哪个神经网络结构会发生权重共享？（D）\nA. 卷积神经网络\nB. 循环神经网络\nC. 全连接神经网络\nD. 选项A和B\n答案：（D）\n156.批规范化(Batch Normalization)的好处都有啥？（A）\nA. 在将所有的输入传递到下一层之前对其进行归一化（更改）\nB. 它将权重的归一化平均值和标准差\nC. 它是一种非常有效的反向传播(BP)方法\nD. 这些均不是\n答案：（A）\n157.在一个神经网络中，下面哪种方法可以用来处理过拟合？（D）\nA. Dropout\nB. 分批归一化(Batch Normalization)\nC. 正则化(regularization)\nD. 都可以\n答案：（D）\n158.如果我们用了一个过大的学习速率会发生什么？（D）\nA. 神经网络会收敛\nB. 不好说\nC. 都不对\nD. 神经网络不会收敛\n答案：（D）\n159.下图所示的网络用于训练识别字符H和T，如下所示：\n网络的输出是什么？（D）\nD.可能是A或B，取决于神经网络的权重设置\n答案：（D）\n不知道神经网络的权重和偏差是什么，则无法判定它将会给出什么样的输出。\n160.假设我们已经在ImageNet数据集(物体识别)上训练好了一个卷积神经网络。然后给这张卷积神经网络输入一张全白的图片。对于这个输入的输出结果为任何种类的物体的可能性都是一样的，对吗？（D）\nA. 对的\nB. 不知道\nC. 看情况\nD. 不对\n答案：（D）各个神经元的反应是不一样的\n161.当在卷积神经网络中加入池化层(pooling layer)时，变换的不变性会被保留，是吗？（C）\nA. 不知道\nB. 看情况\nC. 是\nD. 否\n答案：（C）使用池化时会导致出现不变性。\n162.当数据过大以至于无法在RAM中同时处理时，哪种梯度下降方法更加有效？（A）\nA. 随机梯度下降法(Stochastic Gradient Descent)\nB. 不知道\nC. 整批梯度下降法(Full Batch Gradient Descent)\nD. 都不是\n答案：（A）\n163.下图是一个利用sigmoid函数作为激活函数的含四个隐藏层的神经网络训练的梯度下降图。这个神经网络遇到了梯度消失的问题。下面哪个叙述是正确的？（A）\nA. 第一隐藏层对应D，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应A\nB. 第一隐藏层对应A，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应D\nC. 第一隐藏层对应A，第二隐藏层对应B，第三隐藏层对应C，第四隐藏层对应D\nD. 第一隐藏层对应B，第二隐藏层对应D，第三隐藏层对应C，第四隐藏层对应A\n答案：（A）由于反向传播算法进入起始层，学习能力降低，这就是梯度消失。\n164.对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，二是都设成0，下面哪个叙述是正确的？（C）\nA. 其他选项都不对\nB. 没啥问题，神经网络会正常开始训练\nC. 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西\nD. 神经网络不会开始训练，因为没有梯度改变\n答案：（C）\n165.下图显示，当开始训练时，误差一直很高，这是因为神经网络在往全局最小值前进之前一直被卡在局部最小值里。为了避免这种情况，我们可以采取下面哪种策略？（A）\nA. 改变学习速率，比如一开始的几个训练周期不断更改学习速率\nB. 一开始将学习速率减小10倍，然后用动量项(momentum)\nC. 增加参数数目，这样神经网络就不会卡在局部最优处\nD. 其他都不对\n答案：（A）\n选项A可以将陷于局部最小值的神经网络提取出来。\n166.对于一个图像识别问题(在一张照片里找出一只猫)，下面哪种神经网络可以更好地解决这个问题？（D）\nA. 循环神经网络\nB. 感知机\nC. 多层感知机\nD. 卷积神经网络\n卷积神经网络将更好地适用于图像相关问题，因为考虑到图像附近位置变化的固有性质。\n答案：（D）\n167.假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低。你认为数据有问题，于是你画出了数据并且发现也许是数据的偏度过大造成了这个问题。\n你打算怎么做来处理这个问题？（D）\nA. 对数据作归一化\nB. 对数据取对数变化\nC. 都不对\nD. 对数据作主成分分析(PCA)和归一化\n答案：（D）\n首先将相关的数据去掉，然后将其置零。\n168.下面那个决策边界是神经网络生成的？（E）\nA. A\nB. D\nC. C\nD. B\nE. 以上都有\n答案：（E）\n169.在下图中，我们可以观察到误差出现了许多小的”涨落”。 这种情况我们应该担心吗？（B）\nA. 需要，这也许意味着神经网络的学习速率存在问题\nB. 不需要，只要在训练集和交叉验证集上有累积的下降就可以了\nC. 不知道\nD. 不好说\n答案：（B）\n选项B是正确的，为了减少这些“起伏”，可以尝试增加批尺寸(batch size)。\n170.在选择神经网络的深度时，下面那些参数需要考虑？（C）\n1 神经网络的类型(如MLP,CNN)\n2 输入数据\n3 计算能力(硬件和软件能力决定)\n4 学习速率\n5 映射的输出函数\nA. 1,2,4,5\nB. 2,3,4,5\nC. 都需要考虑\nD. 1,3,4,5\n答案：（C）\n所有上述因素对于选择神经网络模型的深度都是重要的。\n171.考虑某个具体问题时，你可能只有少量数据来解决这个问题。不过幸运的是你有一个类似问题已经预先训练好的神经网络。可以用下面哪种方法来利用这个预先训练好的网络？（C）\nA. 把除了最后一层外所有的层都冻住，重新训练最后一层\nB. 对新数据重新训练整个模型\nC. 只对最后几层进行调参(fine tune)\nD. 对每一层模型进行评估，选择其中的少数来用\n答案：（C）\n172.增加卷积核的大小对于改进卷积神经网络的效果是必要的吗？\n答案：不是，增加核函数的大小不一定会提高性能。这个问题在很大程度上取决于数据集。\n173.请简述神经网络的发展史。\n@SIY.Z。本题解析来源：\n浅析 Hinton 最近提出的 Capsule 计划\n174.说说spark的性能调优。\nhttps://tech.meituan.com/spark-tuning-basic.html\nhttps://tech.meituan.com/spark-tuning-pro.html\n175.机器学习中，有哪些特征选择的工程方法？\n数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已\n1.计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；\n2.构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；\n3.通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；\n4.训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；\n5.通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。\n6.通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。\n176.常见的分类算法有哪些？\nSVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯\n177.常见的监督学习算法有哪些？\n感知机、SVM、人工神经网络、决策树、逻辑回归\n178.在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（D）\nA. 增加训练集量\nB. 减少神经网络隐藏层节点数\nC. 删除稀疏的特征\nD. SVM算法中使用高斯核/RBF核代替线性核\n正确答案：（D）\n@刘炫320\n一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。\nB.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合\nD.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数之一。\n179.下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测？（D）\nA. AR模型\nB. MA模型\nC. ARMA模型\nD. GARCH模型\n正确答案：（D）\n@刘炫320\nR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。\nMA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。\nARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。\nGARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。\n180.以下哪个属于线性分类器最佳准则?（ACD）\nA. 感知准则函数\nB.贝叶斯分类\nC.支持向量机\nD.Fisher准则\n正确答案：（ACD）\n@刘炫320\n线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。\n感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。\n支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）\nFisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。\n根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵\nSw\nS_{w}和类间离散矩阵\nSb\nS_{b}实现。\n181.基于二次准则函数的H-K算法较之于感知器算法的优点是（BD）?\nA. 计算量小\nB. 可以判别问题是否线性可分\nC. 其解完全适用于非线性可分的情况\nD. 其解的适应性更好\n正确答案：（BD）\n@刘炫320\nHK算法思想很朴实,就是在最小均方误差准则下求得权矢量。\n他相对于感知器算法的优点在于，他适用于线性可分和非线性可分得情况，对于线性可分的情况,给出最优权矢量，对于非线性可分得情况，能够判别出来，以退出迭代过程。\n182.以下说法中正确的是（BD）？\nA. SVM对噪声(如来自其他分布的噪声样本)鲁棒\nB. 在AdaBoost算法中,所有被分错的样本的权重更新比例相同\nC. Boosting和Bagging都是组合多个分类器投票的方法,二者都是根据单个分类器的正确率决定其权重\nD. 给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少\n正确答案：（BD）\n@刘炫320\nA、SVM对噪声（如来自其他分布的噪声样本）鲁棒\nSVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。\nB、在AdaBoost算法中所有被分错的样本的权重更新比例相同\nAdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被凸显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。以此类推，将所有的弱分类器重叠加起来，得到强分类器。\nC、Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重。\nBagging与Boosting的区别：\n取样方式不同。\nBagging采用均匀取样，而Boosting根据错误率取样。\nBagging的各个预测函数没有权重，而Boosting是有权重的。\nBagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。\n183.输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为（C）：\nA. 95\nB. 96\nC. 97\nD. 98\n正确答案：（C）\n@刘炫320\n首先我们应该知道卷积或者池化后大小的计算公式：\nout_height=（(input_height - filter_height + padding_top+padding_bottom)/stride_height ）+1\nout_width=（(input_width - filter_width + padding_left+padding_right)/stride_width ）+1\n其中，padding指的是向外扩展的边缘大小，而stride则是步长，即每次移动的长度。\n这样一来就容易多了，首先长宽一般大，所以我们只需要计算一个维度即可，这样，经过第一次卷积后的大小为: （200-5+2）/2+1，取99；经过第一次池化后的大小为：（99-3）/1+1 为97；经过第二次卷积后的大小为： （97-3+2）/1+1 为97。\n184.在SPSS的基础分析模块中，作用是“以行列表的形式揭示数据之间的关系”的是（C）\nA. 数据描述\nB. 相关\nC. 交叉表\nD. 多重相应\n正确答案：（C ）\n185.一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：（B）。\nA. 二分类问题\nB. 多分类问题\nC. 层次聚类问题\nD. k-中心点聚类问题\nE. 回归问题\nF. 结构分析问题\n正确答案：（B）\n@刘炫320\n二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。\n层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。\nK-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。\n回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。\n结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。\n多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。\n186.关于 Logit 回归和 SVM 不正确的是（A）。\nA. Logit回归目标函数是最小化后验概率\nB. Logit回归可以用于预测事件发生概率的大小\nC. SVM目标是结构风险最小化\nD. SVM可以有效避免模型过拟合\n正确答案：（A）\n@刘炫320\nA. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误\nB. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确\nC. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。\nD. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。\n187.有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是（C）\nA. 2x+y=4\nB. x+2y=5\nC. x+2y=3\nD. 2x-y=0\n正确答案：（C）\n解析：这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。\n188.下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？（C）\nA. 准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率\nB. 召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率\nC. 正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高\nD. 为了解决准确率和召回率冲突问题，引入了F1分数\n正确答案：（C）\n解析：对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：\nTP——将正类预测为正类数\nFN——将正类预测为负类数\nFP——将负类预测为正类数\nTN——将负类预测为负类数\n由此：\n精准率定义为：P = TP / (TP + FP)\n召回率定义为：R = TP / (TP + FN)\nF1值定义为： F1 = 2 P R / (P + R)\n精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。\n189.以下几种模型方法属于判别式模型(Discriminative Model)的有（A）\n1)混合高斯模型 2)条件随机场模型\n3)区分度训练 4)隐马尔科夫模型\nA. 2,3\nB. 3,4\nC. 1,4\nD. 1,2\n正确答案：（A）\n@刘炫320\n常见的判别式模型有：Logistic Regression（Logistical 回归）\nLinear discriminant analysis（线性判别分析）\nSupportvector machines（支持向量机）\nBoosting（集成学习）\nConditional random fields（条件随机场）\nLinear regression（线性回归）\nNeural networks（神经网络）\n常见的生成式模型有:Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）\nHidden Markov model（隐马尔可夫）\nNaiveBayes（朴素贝叶斯）\nAODE（平均单依赖估计）\nLatent Dirichlet allocation（LDA主题模型）\nRestricted Boltzmann Machine（限制波兹曼机）\n生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。\n190.SPSS中，数据整理的功能主要集中在（AD ）等菜单中。\nA. 数据\nB. 直销\nC. 分析\nD. 转换\n正确答案：（AD ）\n@刘炫320\n解析：对数据的整理主要在数据和转换功能菜单中。\n191.深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为m∗n，n∗p，p∗q，且m<n<p<q，以下计算顺序效率最高的是（A）\nA. (AB)C\nB. AC(B)\nC. A(BC)\nD. 所以效率都相同\n正确答案：（A）\n@刘炫320\n首先，根据简单的矩阵知识，因为 A*B ， A 的列数必须和 B 的行数相等。因此，可以排除 B 选项。\n然后，再看 A 、 C 选项。在 A 选项中，m∗n 的矩阵 A 和n∗p的矩阵 B 的乘积，得到 m∗p的矩阵 A*B ，而 A∗B的每个元素需要 n 次乘法和 n-1 次加法，忽略加法，共需要 m∗n∗p次乘法运算。同样情况分析 A*B 之后再乘以 C 时的情况，共需要 m∗p∗q次乘法运算。因此， A 选项 (AB)C 需要的乘法次数是 m∗n∗p+m∗p∗q 。同理分析， C 选项 A (BC) 需要的乘法次数是 n∗p∗q+m∗n∗q。\n由于m∗n∗p<m∗n∗q，m∗p∗q<n∗p∗q，显然 A 运算次数更少，故选 A 。\n192.Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:( C )\nA. 各类别的先验概率P(C)是相等的\nB. 以0为均值，sqr(2)/2为标准差的正态分布\nC. 特征变量X的各个维度是类别条件独立随机变量\nD. P(X|C)是高斯分布\n正确答案：( C )\n@刘炫320\n朴素贝叶斯的条件就是每个变量相互独立。\n193.关于支持向量机SVM,下列说法错误的是（C）\nA. L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力\nB. Hinge 损失函数，作用是最小化经验分类错误\nC. 分类间隔为\n1||w||\n\\frac{1}{||w||}，||w||代表向量的模\nD. 当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习\n正确答案：（C）\n@刘炫320\nA正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。\nB正确。\nC错误。间隔应该是\n2||w||\n\\frac{2}{||w||}才对，后半句应该没错，向量的模通常指的就是其二范数。\nD正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出\nw=∑iai∗yi∗xi\nw=\\sum_{i}^{ }a_{i}*y_{i}*x_{i}，a变小使得w变小，因此间隔\n2||w||\n\\frac{2}{||w||}变大。\n194.在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计( D )\nA. EM算法\nB. 维特比算法\nC. 前向后向算法\nD. 极大似然估计\n正确答案：( D )\n@刘炫320\nEM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法\n维特比算法： 用动态规划解决HMM的预测问题，不是参数估计\n前向后向算法：用来算概率\n极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数\n注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。\n195.假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是： （BD）\nA. 这个被重复的特征在模型中的决定作用会被加强\nB. 模型效果相比无重复特征的情况下精确度会降低\nC. 如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。\nD. 当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题\nE. NB可以用来做最小二乘回归\nF. 以上说法都不正确\n正确答案：（BD）\n196.L1与L2范数在Logistic Regression 中,如果同时加入L1和L2范数,会产生什么效果( A )。\nA. 可以做特征选择,并在一定程度上防止过拟合\nB. 能解决维度灾难问题\nC. 能加快计算速度\nD. 可以获得更准确的结果\n正确答案：( A )\n@刘炫320\nL1范数具有系数解的特性，但是要注意的是，L1没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。\n在代价函数后面加上正则项，L1即是Losso回归，L2是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。因此选择A。\n对于机器学习中的范数规则化，也就是L0,L1,L2范数的详细解答，请参阅范数规则化。\n197.机器学习中L1正则化和L2正则化的区别是？（AD）\nA. 使用L1可以得到稀疏的权值\nB. 使用L1可以得到平滑的权值\nC. 使用L2可以得到稀疏的权值\nD. 使用L2可以得到平滑的权值\n正确答案：（AD）\n@刘炫320\nL1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0。\nL2主要功能是为了防止过拟合，当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。\nL1正则化/Lasso\nL1正则化将系数w的L1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。\nL2正则化/Ridge regression\nL2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以\nY=X1+X2\nY=X_{1}+X_{2}为例，假设\nX1\nX_{1}和\nX1\nX_{1}具有很强的关联，如果用L1正则化，不论学到的模型是\nY=X1+X2\nY=X_{1}+X_{2}还是\nY=2X1\nY=2X_{1}，惩罚都是一样的，都是\n2α\n2\\alpha 。但是对于L2来说，第一个模型的惩罚项是\n2α\n2\\alpha ，但第二个模型的是\n4α\n4\\alpha 。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。\n可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。\n因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。\n198.位势函数法的积累势函数K(x)的作用相当于Bayes判决中的( AD )\nA. 后验概率\nB. 先验概率\nC. 类概率密度\nD. 类概率密度与先验概率的乘积\n正确答案: （AD）\n@刘炫320\n事实上，AD说的是一回事。\n参考链接：势函数主要用于确定分类面，其思想来源于物理。\n199.隐马尔可夫模型三个基本问题以及相应的算法说法正确的是（ ABC）\nA. 评估—前向后向算法\nB. 解码—维特比算法\nC. 学习—Baum-Welch算法\nD. 学习—前向后向算法\n正确答案: （ ABC）\n解析：评估问题，可以使用前向算法、后向算法、前向后向算法。\n200.特征比数据量还大时，选择什么样的分类器？\n答案：线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。\n201.下列属于无监督学习的是： （A）\nA. k-means\nB. SVM\nC. 最大熵\nD. CRF\n正确答案：（A）\n解析： A是聚类，BC是分类，D是序列化标注，也是有监督学习。\n202.下列哪个不属于CRF模型对于HMM和MEMM模型的优势（B）\nA. 特征灵活\nB. 速度快\nC. 可容纳较多上下文信息\nD. 全局最优\n正确答案：（B）\n解析： CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢\nCRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较\n同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较\nCRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较\n203.数据清理中，处理缺失值的方法是? （ABCD）\nA. 估算\nB. 整例删除\nC. 变量删除\nD. 成对删除\n正确答案：（ABCD）\n@刘炫320\n由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。\n估算(estimation)。最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。\n整例删除(casewise deletion)是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。\n变量删除(variable deletion)。如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。\n成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。\n采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。\n204.关于线性回归的描述,以下正确的有: （ACEF）\nA. 基本假设包括随机干扰项是均值为0,方差为1的标准正态分布\nB. 基本假设包括随机干扰下是均值为0的同方差正态分布\nC. 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量\nD. 在违背基本假设时,模型不再可以估计\nE. 可以用DW检验残差是否存在序列相关性\nF. 多重共线性会使得参数估计值方差减小\n正确答案：（ACEF）\n@刘炫320\n1、AB一元线性回归的基本假设有：\n（1）随机误差项是一个期望值或平均值为0的随机变量；\n（2）对于解释变量的所有观测值，随机误差项有相同的方差；\n（3）随机误差项彼此不相关；\n（4）解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立；\n（5）解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵；\n（6）随机误差项服从正态分布\n2、CD 违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。\n当存在异方差时，普通最小二乘法估计存在以下问题： 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。\n3、E杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶自相关最常用的方法。\n4、F所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响\n（1）完全共线性下参数估计量不存在\n（2）近似共线性下OLS估计量非有效\n多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF)\n（3）参数估计量经济含义不合理\n（4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外\n（5）模型的预测功能失效。变大的方差容易使区间预测的“区间”变大，使预测失去意义。\n对于线性回归模型,当响应变量服从正态分布,误差项满足高斯–马尔科夫条件（零均值、等方差、不相关）时,回归参数的最小二乘估计是一致最小方差无偏估计。\n当然，该条件只是理想化的假定，为的是数学上有相应的较为成熟的结论。其实大多数实际问题都不完全满足这些理想化的假定。\n线性回归模型理论的发展正是在不断克服理想化条件不被满足时得到许多新方法。如加权LSE、岭估计、压缩估计、BOX_COX变换等一系列段。做实际工作时一定是要超越书本上的理想化条件的。\n205.影响聚类算法效果的主要原因有：（ABC）\nA. 特征选取\nB. 模式相似性测度\nC. 分类准则\nD. 已知类别的样本质量\n正确答案：（ABC）\n@刘炫320\n解析：这道题应该是很简单的，D之所以不正确，是因为聚类是对无类别的数据进行聚类，不使用已经标记好的数据。\n206.以下哪个是常见的时间序列算法模型（C）\nA. RSI\nB. MACD\nC. ARMA\nD. KDJ\n正确答案：（C）\n解析： 自回归滑动平均模型(ARMA) ，其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。\n其他三项都不是一个层次的。\nA. 相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 。\nB. 移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 。\nD. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 。\n207.下列不是SVM核函数的是：（B）\nA. 多项式核函数\nB. Logistic核函数\nC. 径向基核函数\nD. Sigmoid核函数\n正确答案：（B）\n@刘炫320\nSVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。\n核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积。对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：\n(1)线性核函数 ：K ( x , x i ) = x ⋅ x i\n(2)多项式核 ：K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d\n(3)径向基核（RBF）：K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )\nGauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。\n(4)傅里叶核 ：K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )\n(5)样条核 ：K ( x , x i ) = B 2 n + 1 ( x − x i )\n(6)Sigmoid核函数 ：K ( x , x i ) = tanh ( κ ( x , x i ) − δ )\n采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。\n在选取核函数解决实际问题时，通常采用的方法有：\n一是利用专家的先验知识预先选定核函数；\n二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多。\n三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想。\n208.已知一组数据的协方差矩阵P,下面关于主分量说法错误的是( C )\nA. 主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小\nB. 在经主分量分解后,协方差矩阵成为对角矩阵\nC. 主分量分析就是K-L变换\nD. 主分量是通过求协方差矩阵的特征值得到\n正确答案：( C )\n解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。\n209.在分类问题中,我们经常会遇到正负样本数据量不等的情况,比如正样本为10w条数据,负样本只有1w条数据,以下最合适的处理方法是( ACD)\nA. 将负样本重复10次,生成10w样本量,打乱顺序参与分类\nB. 直接进行分类,可以最大限度利用数据\nC. 从10w正样本中随机抽取1w参与分类\nD. 将负样本每个权重设置为10,正样本权重为1,参与训练过程\n正确答案：( ACD)\n解析：\n1.重采样。 A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。\n2.欠采样。 C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。\n如果1：10算是均匀的话，可以将多数类分割成为1000份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这1000个分类器用assemble的方法组合位一个分类器。A选项可以看作此方式，因而相对比较合理。\n另：如果目标是 预测的分布 跟训练的分布一致，那就加大对分布不一致的惩罚系数。\n3. 权值调整。 D方案也是其中一种方式。\n当然，这只是在数据集上进行相应的处理，在算法上也有相应的处理方法。\n210.在统计模式识分类问题中，当先验概率未知时，可以使用( BC )?\nA. 最小损失准则\nB. N-P判决\nC. 最小最大损失准则\nD. 最小误判概率准则\n正确答案：( BC )\n@刘炫320\n选项 A ,最小损失准则中需要用到先验概率\n选项B ,在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。\n1. p(y)已知，直接使用贝叶斯公式求后验概率即可；\n2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。\n聂曼-皮尔逊决策（N-P判决）可以归结为找阈值a，即：\n如果p（x|w1）/p（x|w2）>a，则 x属于w1；\n如果p（x|w1）/p（x|w2）<a，则 x属于w2；\n选项C ,最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。\n211.解决隐马模型中预测问题的算法是?（D）\nA. 前向算法\nB. 后向算法\nC. Baum-Welch算法\nD. 维特比算法\n正确答案：（D）\n@刘炫320\nA、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。\nC：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；\nD：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。\n212.一般，k-NN最近邻方法在( B )的情况下效果较好。\nA. 样本较多但典型性不好\nB. 样本较少但典型性好\nC. 样本呈团状分布\nD. 样本呈链状分布\n正确答案：( B )\n解析：\nK近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B。\n样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。\n213.下列方法中，可以用于特征降维的方法包括（ABCD）\nA. 主成分分析PCA\nB. 线性判别分析LDA\nC. 深度学习SparseAutoEncoder\nD. 矩阵奇异值分解SVD\nE. 最小二乘法LeastSquares\n正确答案：（ABCD）\n解析：降维的3种常见方法ABD，都是线性的。深度学习是降维的方法这个就比较新鲜了，事实上，细细想来，也是降维的一种方法，因为如果隐藏层中的神经元数目要小于输入层，那就达到了降维，但如果隐藏层中的神经元如果多余输入层，那就不是降维了。\n最小二乘法是线性回归的一种解决方法，其实也是投影，但是并没有进行降维。\n214.下面哪些是基于核的机器学习算法?( BCD )\nA. Expectation Maximization（EM）（最大期望算法）\nB. Radial Basis Function（RBF）（径向基核函数）\nC. Linear Discrimimate Analysis（LDA）（主成分分析法）\nD. Support Vector Machine（SVM）（支持向量机）\n正确答案：( BCD )\n解析：径向基核函数是非常常用的核函数，而主成分分析法的常规方法是线性的，但是当遇到非线性的时候，同样可以使用核方法使得非线性问题转化为线性问题。支持向量机处理非线性的问题的时候，核函数也是非常重要的。\n215.试推导样本空间中任意点x到超平面（w,b）的距离公式。\n216.从网上下载或自己编程实现一个卷积神经网络，并在手写字符识别数据MNIST上进行试验测试。\n解析详见：周志华《机器学习》课后习题解答系列（六）：Ch5.10 - 卷积神经网络实验\n217.神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属是好的属性但不必要的？\n@Hengkai Guo\n说说我对一个好的激活函数的理解吧，有些地方可能不太严谨，欢迎讨论。（部分参考了Activation function。）\n1. 非线性：即导数不是常数。这个条件前面很多答主都提到了，是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。\n2. 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。\n3. 计算简单：正如题主所说，非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。\n4. 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x>0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x<0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。\n5. 单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。\n6. 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。\n7. 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。\n8. 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。\n9. 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。\n218.梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？\n@李振华\n知乎答案\n219.EM算法、HMM、CRF。\n这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。\n（1）EM算法\nEM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。\n注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。\n（2）HMM算法\n隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。\n马尔科夫三个基本问题：\n概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法\n学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。\n预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）\n（3）条件随机场CRF\n给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。\n之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。\n（4）HMM和CRF对比\n其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。\n220.CNN常用的几个模型。\n221.带核的SVM为什么能分类非线性问题？\n核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个内积。\n222.常用核函数及核函数的条件。\n核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。\nRBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。\n线性核：主要用于线性可分的情况\n多项式核\n223.Boosting和Bagging。\n（1）随机森林\n随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：\n1）Boostrap从袋内有放回的抽取样本值\n2）每次随机抽取一定数量的特征（通常为sqr(n)）。\n分类问题：采用Bagging投票的方式选择类别频次最高的\n回归问题：直接取每颗树结果的平均值。\n（2）Boosting之AdaBoost\nBoosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。\n（3）Boosting之GBDT\n将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。\n注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。\n（4）Xgboost\n这个工具主要有以下几个特点：\n支持线性分类器\n可以自定义损失函数，并且可以用二阶偏导\n加入了正则化项：叶节点数、每个叶节点输出score的L2-norm\n支持特征抽样\n在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。\n224.逻辑回归相关问题。\n（1）公式推导一定要会\n（2）逻辑回归的基本概念\n这个最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。\n（3）L1-norm和L2-norm\n其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。\n但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。\n（4）LR和SVM对比\n首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss\nminw,b∑iN[1−yi(w∗xi+b)]+λ||w||2\nmin_{w,b}\\sum_{i}^{N}[1-y_{i}(w*x_{i}+b)]+\\lambda ||w||^{2}\n其次，两者都是线性模型。\n最后，SVM只考虑支持向量（也就是和分类相关的少数点）\n（5）LR和随机森林区别\n随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。\n（6）常用的优化方法\n逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。\n一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。\n二阶方法：牛顿法、拟牛顿法：\n这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。\n拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。\n225.用贝叶斯机率说明Dropout的原理。\n参考答案\n226.为什么很多做人脸的Paper会最后加入一个Local Connected Conv？\n@许韩\n以FaceBook DeepFace 为例：\nDeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。\n227.什么事共线性, 跟过拟合有什么关联?\n@抽象猴\n共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。\n共线性会造成冗余，导致过拟合。\n解决方法：排除变量的相关性／加入权重正则。\n228.为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optima？\n参见：The Loss Surfaces of Multilayer Networks\n229.机器学习中的正负样本。\n在分类问题中，这个问题相对好理解一点，比如人脸识别中的例子，正样本很好理解，就是人脸的图片，负样本的选取就与问题场景相关，具体而言，如果你要进行教室中学生的人脸识别，那么负样本就是教室的窗子、墙等等，也就是说，不能是与你要研究的问题毫不相关的乱七八糟的场景图片，这样的负样本并没有意义。负样本可以根据背景生成，有时候不需要寻找额外的负样本。一般3000-10000的正样本需要5，000,000-100,000,000的负样本来学习，在互金领域一般在入模前将正负比例通过采样的方法调整到3:1-5:1。\n230.机器学习中，有哪些特征选择的工程方法？\n数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。\n1.计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；\n2.构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；\n3.通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；\n4.训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；\n5.通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。\n6.通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。\n231.在一个n维的空间中， 最好的检测outlier(离群点)的方法是：（C）\nA. 作正态分布概率图\nB. 作盒形图\nC. 马氏距离\nD. 作散点图\n答案：（C）\n马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。更多请详见： 从K近邻算法、距离度量谈到KD树、SIFT+BBF算法。\n232.对数几率回归（logistics regression）和一般回归分析有什么区别？（D）\nA. 对数几率回归是设计用来预测事件可能性的\nB. 对数几率回归可以用来度量模型拟合程度\nC. 对数几率回归可以用来估计回归系数\nD. 以上所有\n答案：（D）\nA: 对数几率回归其实是设计用来解决分类问题的\nB: 对数几率回归可以用来检验模型对数据的拟合度\nC: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。\n233.bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）（C）\nA. 有放回地从总共M个特征中抽样m个特征\nB. 无放回地从总共M个特征中抽样m个特征\nC. 有放回地从总共N个样本中抽样n个样本\nD. 无放回地从总共N个样本中抽样n个样本\n答案：（C）\n234.“过拟合”只在监督学习中出现，在非监督学习中，没有”过拟合”，这是：（B）\nA. 对的\nB. 错的\n答案：（B）\n我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）。\n235.对于k折交叉验证, 以下对k的说法正确的是 :（D）\nA. k越大, 不一定越好, 选择大的k会加大评估时间\nB. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)\nC. 在选择k时, 要最小化数据集之间的方差\nD. 以上所有\n答案：（D）\nk越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差。\n236.回归模型中存在多重共线性, 你如何解决这个问题？\n1.去除这两个共线性变量\n2.我们可以先去除一个共线性变量\n3.计算VIF(方差膨胀因子), 采取相应措施\n4.为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归.\n以下哪些是对的：（D）\nA. 1\nB. 2\nC. 2和3\nD. 2, 3和4\n答案: （D）\n解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高.\n我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。\n237.模型的高bias是什么意思, 我们如何降低它 ?（B）\nA. 在特征空间中减少特征\nB. 在特征空间中增加特征\nC. 增加数据点\nD. B和C\nE. 以上所有\n答案: （B）\nbias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !\n238.训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个:（A）\nA. Outlook\nB. Humidity\nC. Windy\nD. Temperature\n答案: （A）\n信息增益, 增加平均子集纯度。\n239.对于信息增益, 决策树分裂节点, 下面说法正确的是: （C）\n1.纯度高的节点需要更多的信息去区分\n2.信息增益可以用”1比特-熵”获得\n3.如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的\nA. 1\nB. 2\nC. 2和3\nD. 所有以上\n答案: （C）\n240.如果SVM模型欠拟合, 以下方法哪些可以改进模型 : （A）\nA. 增大惩罚参数C的值\nB. 减小惩罚参数C的值\nC. 减小核系数(gamma参数)\n答案：（A）\n如果SVM模型欠拟合, 我们可以调高参数C的值, 使得模型复杂度上升。\n241.下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是 :（C）\nA. g1 > g2 > g3\nB. g1 = g2 = g3\nC. g1 < g2 < g3\nD. g1 >= g2 >= g3\nE. g1 <= g2 <= g3\n答案: （C）\n242.假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 : （C）\n1.模型分类的召回率会降低或不变\n2.模型分类的召回率会升高\n3.模型分类准确率会升高或不变\n4.模型分类准确率会降低\nA. 1\nB. 2\nC.1和3\nD. 2和4\nE. 以上都不是\n答案: （C）\n243.”点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是 : （B）\nA. 模型预测准确率已经很高了, 我们不需要做什么了\nB. 模型预测准确率不高, 我们需要做点什么改进模型\nC. 无法下结论\nD. 以上都不对\n答案: （B）\n99%的预测准确率可能说明, 你预测的没有点进去的人很准确 (因为有99%的人是不会点进去的, 这很好预测)。不能说明你的模型对点进去的人预测准确, 所以, 对于这样的非平衡数据集, 我们要把注意力放在小部分的数据上, 即那些点击进去的人。\n244.使用k=1的KNN算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少 :（B）\nA. 0%\nB. 100%\nC. 0% 到 100%\nD. 以上都不是\n答案: （B）\nKNN算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的KNN在上图不是一个好选择, 分类的错误率始终是100%。\n245.我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以 : （C）\nA. 增加树的深度\nB. 增加学习率 (learning rate)\nC. 减少树的深度\nD. 减少树的数量\n答案: （C）\nA.增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间。\nB.决策树没有学习率参数可以调。(不像集成学习和其它有步长的学习方法)\nD.决策树只有一棵树, 不是随机森林。\n246.对于神经网络的说法, 下面正确的是 : （A）\n1.增加神经网络层数, 可能会增加测试数据集的分类错误率\n2.减少神经网络层数, 总是能减小测试数据集的分类错误率\n3.增加神经网络层数, 总是能减小训练数据集的分类错误率\nA. 1\nB. 1 和 3\nC. 1 和 2\nD. 2\n答案: （A）\n深度神经网络的成功, 已经证明, 增加神经网络层数, 可以增加模型范化能力, 即训练数据集和测试数据集都表现得更好. 但更多的层数, 也不一定能保证有更好的表现https://arxiv.org/pdf/1512.03385v1.pdf。所以,不能绝对地说层数多的好坏, 只能选A。\n247.假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？（C）\nA. 设C=1\nB. 设C=0\nC. 设C=无穷大\nD. 以上都不对\n答案: （C）\nC无穷大保证了所有的线性不可分都是可以忍受的。\n248.训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类:（A）\nA. 正确\nB. 错误\n答案: （A）\nSVM模型中, 真正影响决策边界的是支持向量。\n249.以下哪些算法, 可以用神经网络去构造: （B）\n1.KNN\n2.线性回归\n3.对数几率回归\nA. 1和 2\nB. 2 和 3\nC. 1, 2 和 3\nD. 以上都不是\n答案: （B）\n1.KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙\n2.最简单的神经网络, 感知器, 其实就是线性回归的训练\n3.我们可以用一层的神经网络构造对数几率回归\n250.请选择下面可以应用隐马尔科夫(HMM)模型的选项: （D）\nA. 基因序列数据集\nB. 电影浏览数据集\nC. 股票市场数据集\nD. 所有以上\n答案: （D）\n只要是和时间序列问题有关的 , 都可以试试HMM。\n251.我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 : （F）\nA. 我们随机抽取一些样本, 在这些少量样本之上训练\nB. 我们可以试用在线机器学习算法\nC. 我们应用PCA算法降维, 减少特征数\nD. B 和 C\nE. A 和 B\nF. 以上所有\n答案: （F）\n252.我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 :（D）\n1.使用前向特征选择方法\n2.使用后向特征排除方法\n3.我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征\n4.查看相关性表, 去除相关性最高的一些特征\nA. 1 和 2\nB. 2, 3和4\nC. 1, 2和4\nD. All\n答案: （D）\n1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法\n2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法\n3.用相关性的度量去删除多余特征, 也是一个好方法\n所以D是正确的。\n253.对于随机森林和GradientBoosting Trees, 下面说法正确的是:（A）\n1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的\n2.这两个模型都使用随机特征子集, 来生成许多单个的树\n3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好\nA. 2\nB. 1 and 2\nC. 1, 3 and 4\nD. 2 and 4\n答案: （A）\n1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所有说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系。\n2.这两个模型都使用随机特征子集, 来生成许多单个的树。\n所以A是正确的。\n254.对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :（B）\nA. 正确的\nB. 错误的\n答案: （B）\n这个说法是错误的。首先，“不依赖”和“不相关”是两回事；其次, 转化过的特征, 也可能是相关的。\n255.对于PCA说法正确的是 :（A）\n1.我们必须在使用PCA前规范化数据\n2.我们应该选择使得模型有最大variance的主成分\n3.我们应该选择使得模型有最小variance的主成分\n4.我们可以使用PCA在低维度上做数据可视化\nA. 1, 2 and 4\nB. 2 and 4\nC. 3 and 4\nD. 1 and 3\nE. 1, 3 and 4\n答案: （A）\n1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分)\n2）我们总是应该选择使得模型有最大variance的主成分\n3）有时在低维度上左图是需要PCA的降维帮助的\n256.对于下图, 最好的主成分选择是多少 ?（B）\nA. 7\nB. 30\nC. 35\nD. 不确定\n答案: （B）\n主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。\n257.数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是 :（B）\nA. 单个模型之间有高相关性\nB. 单个模型之间有低相关性\nC. 在集成学习中使用“平均权重”而不是“投票”会比较好\nD. 单个模型都是用的一个算法\n答案: （B）\n258.在有监督学习中， 我们如何使用聚类方法？（B）\n1.我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习\n2.我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习\n3.在进行监督学习之前， 我们不能新建聚类类别\n4.我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习\nA. 2 和 4\nB. 1 和 2\nC. 3 和 4\nD. 1 和 3\n答案: （B）\n我们可以为每个聚类构建不同的模型， 提高预测准确率；“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。所以B是正确的。\n259.以下说法正确的是 :（C）\n1.一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的\n2.如果增加模型复杂度， 那么模型的测试错误率总是会降低\n3.如果增加模型复杂度， 那么模型的训练错误率总是会降低\n4.我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习\nA. 1\nB. 2\nC. 3\nD. 1 and 3\n答案: （C）\n考的是过拟合和欠拟合的问题。\n260.对应GradientBoosting tree算法， 以下说法正确的是 :（C）\n1.当增加最小样本分裂个数，我们可以抵制过拟合\n2.当增加最小样本分裂个数，会导致过拟合\n3.当我们减少训练单个学习器的样本个数，我们可以降低variance\n4.当我们减少训练单个学习器的样本个数，我们可以降低bias\nA. 2 和 4\nB. 2 和 3\nC. 1 和 3\nD. 1 和 4\n答案: （C）\n最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。第二点是靠bias和variance概念的。\n261.以下哪个图是KNN算法的训练边界 ? （B）\nA) B\nB) A\nC) D\nD) C\nE) 都不是\n答案：（B）\nKNN算法肯定不是线性的边界，所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。\n262.如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？（B）\nA. 是的，这说明这个模型的范化能力已经足以支持新的数据集合了\nB. 不对，依然后其他因素模型没有考虑到，比如噪音数据\n答案：（B）\n没有一个模型是可以总是适应新的数据的。我们不可能达到100%的准确率。\n263.下面的交叉验证方法 :（B）\ni. 有放回的Bootstrap方法\nii. 留一个测试样本的交叉验证\niii. 5折交叉验证\niv. 重复两次的5折交叉验证\n当样本是1000时，下面执行时间的顺序，正确的是：\nA. i > ii > iii > iv\nB. ii > iv > iii > i\nC. iv > i > ii > iii\nD. ii > iii > iv > i\n答案：（B）\nBootstrap方法是传统的随机抽样，验证一次的验证方法，只需要训练1个模型，所以时间最少。\n留一个测试样本的交叉验证，需要n次训练过程（n是样本个数），这里，需要训练1000个模型。\n5折交叉验证需要训练5个模型。\n重复两次的5折交叉验证，需要训练10个模型。\n264.变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？ :（C）\n1.多个变量其实有相同的用处\n2.变量对于模型的解释有多大作用\n3.特征携带的信息\n4.交叉验证\nA. 1 和 4\nB. 1, 2 和 3\nC. 1,3 和 4\nD. 以上所有\n答案：（C）\n注意，这题的题眼是考虑模型效率，所以不要考虑选项B\n265.对于线性回归模型，包括附加变量在内，以下的可能正确的是 :（D）\n1.R-Squared 和 Adjusted R-squared都是递增的\n2.R-Squared 是常量的，Adjusted R-squared是递增的\n3.R-Squared 是递减的， Adjusted R-squared 也是递减的\n4.R-Squared 是递减的， Adjusted R-squared是递增的\nA. 1 和 2\nB. 1 和 3\nC. 2 和 4\nD. 以上都不是\n答案：（D）\nR-Squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-Squared有R-Squared和predicted R-Squared所没有的问题。每次为模型加入预测器，R-Squared递增或者不变。\n266.对于下面三个模型的训练情况， 下面说法正确的是 :（C）\n1.第一张图的训练错误与其余两张图相比，是最大的\n2.最后一张图的训练效果最好，因为训练错误最小\n3.第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型\n4.第三张图相对前两张图过拟合了\n5.三个图表现一样，因为我们还没有测试数据集\nA. 1 和 3\nB. 1 和 3\nC. 1, 3 和 4\nD. 5\n267.对于线性回归，我们应该有以下哪些假设？（D）\n1.找到利群点很重要, 因为线性回归对利群点很敏感\n2.线性回归要求所有变量必须符合正态分布\n3.线性回归假设数据没有多重线性相关性\nA. 1 和 2\nB. 2 和 3\nC. 1,2 和 3\nD. 以上都不是\n答案：（D）\n利群点要着重考虑，第一点是对的。\n不是必须的，当然如果是正态分布，训练效果会更好。\n有少量的多重线性相关性是可以的，但是我们要尽量避免。\n268.我们注意变量间的相关性。在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论？（C）\n1.Var1和Var2是非常相关的\n2.因为Var和Var2是非常相关的, 我们可以去除其中一个\n3.Var3和Var1的1.23相关系数是不可能的\nA. 1 and 3\nB. 1 and 2\nC. 1,2 and 3\nD. 1\n答案：（C）\nVar1和Var2的相关系数是负的，所以这是多重线性相关，我们可以考虑去除其中一个。\n一 般的，如果相关系数大于0.7或者小于-0.7，是高相关的。\n相关系数的范围应该是[-1,1]。\n269.如果在一个高度非线性并且复杂的一些变量中“一个树模型可比一般的回归模型效果更好”是（A）\nA. 对的\nB. 错的\n答案：（A）\n270.对于维度极低的特征，选择线性还是非线性分类器？\n答案：非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。\n1.如果特征的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM。\n2.如果特征的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel。\n3.如果特征的数量比较小，而样本数量很多，需要手工添加一些特征变成第一种情况。\n271.SVM、LR、决策树的对比。\n模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝。\n损失函数：SVM hinge loss; LR L2正则化; Adaboost 指数损失。\n数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感。\n数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核。\n272.什么是ill-condition病态问题？\n训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。\n273.简述KNN最近邻分类算法的过程？\n1.计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；\n2.对上面所有的距离值进行排序；\n3.选前k个最小距离的样本；\n4.根据这k个样本的标签进行投票，得到最后的分类类别；\n274.常用的聚类划分方式有哪些？列举代表算法。\n1.基于划分的聚类:K-means，k-medoids，CLARANS。\n2.基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。\n3.基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。\n4.基于网格的方法：STING，WaveCluster。\n5.基于模型的聚类：EM,SOM，COBWEB。\n275.下面对集成学习模型中的弱学习者描述错误的是？（C）\nA. 他们经常不会过拟合\nB. 他们通常带有高偏差，所以其并不能解决复杂学习问题\nC. 他们通常会过拟合\n答案：（C）\n弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。\n276.下面哪个/些选项对 K 折交叉验证的描述是正确的？（D）\n1.增大 K 将导致交叉验证结果时需要更多的时间\n2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心\n3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量\nA. 1 和 2\nB. 2 和 3\nC. 1 和 3\nD. 1、2 和 3\n答案：（D)\n大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。\n277.最出名的降维算法是 PAC 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？（B）\nA. X_projected_PCA 在最近邻空间能得到解释\nB. X_projected_tSNE 在最近邻空间能得到解释\nC. 两个都在最近邻空间能得到解释\nD. 两个都不能在最近邻空间得到解释\n答案：（B）\nt-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。\n278.给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？（E）\nA. D1= C1, D2 < C2, D3 > C3\nB. D1 = C1, D2 > C2, D3 > C3\nC. D1 = C1, D2 > C2, D3 < C3\nD. D1 = C1, D2 < C2, D3 < C3\nE. D1 = C1, D2 = C2, D3 = C3\n答案：（E）\n特征之间的相关性系数不会因为特征加或减去一个数而改变。\n279.为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？(A)\nA. 将数据转换成零均值\nB. 将数据转换成零中位数\nC. 无法做到\n答案:（A）\n当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。\n280.假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。(A)\n注意：所有其他超参数是相同的，所有其他因子不受影响。\n1.深度为 4 时将有高偏差和低方差\n2.深度为 4 时将有低偏差和低方差\nA. 只有 1\nB. 只有 2\nC. 1 和 2\nD. 没有一个\n答案:（A)\n如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。\n281.在 k-均值算法中，以下哪个选项可用于获得全局最小？(D)\nA. 尝试为不同的质心（centroid）初始化运行算法\nB. 调整迭代的次数\nC. 找到集群的最佳数量\nD. 以上所有\n答案:（D）\n所有都可以用来调试以找到全局最小。\n282.你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？(B)\nA. 第一个 w2 成了 0，接着 w1 也成了 0\nB. 第一个 w1 成了 0，接着 w2 也成了 0\nC. w1 和 w2 同时成了 0\nD. 即使在 C 成为大值之后，w1 和 w2 都不能成 0\n答案:（B）\n通过观察图像我们发现，即使只使用 x2，我们也能高效执行分类。因此一开始 w1 将成 0；当正则化参数不断增加时，w2 也会越来越接近 0。\n283.假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。(D)\nA.如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它。\nB.对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大。\nC.log-loss 越低，模型越好\nD.以上都是\n答案为:（D）\n284.下面哪个选项中哪一项属于确定性算法？(A）\nA.PCA\nB.K-Means\nC. 以上都不是\n答案：（A）\n确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 K-Means 不会。\n285.特征向量的归一化方法有哪些？\n线性函数转换，表达式如下：\ny=x−MinValueMaxValue−MinValue\ny=\\frac{x-MinValue}{MaxValue-MinValue}\n对数函数转换，表达式如下：\ny=log10(x)\ny=log10 (x)\n反余切函数转换 ，表达式如下：\ny=arctan(x)∗2π\ny=\\frac{arctan(x)*2}{\\pi}\n减去均值，除以方差：\ny=x−meansvariance\ny=\\frac{x-means}{variance}\n286.优化算法及其优缺点？\n温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。\n1）随机梯度下降\n优点：可以一定程度上解决局部最优解的问题\n缺点：收敛速度较慢\n2）批量梯度下降\n优点：容易陷入局部最优解\n缺点：收敛速度较快\n3）mini_batch梯度下降\n综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。\n4）牛顿法\n牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。\n5）拟牛顿法\n拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。\n287.RF与GBDT之间的区别与联系？\n1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。\n2）不同点：\n组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成\n组成随机森林的树可以并行生成，而GBDT是串行生成\n随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和\n随机森林对异常值不敏感，而GBDT对异常值比较敏感\n随机森林是减少模型的方差，而GBDT是减少模型的偏差\n随机森林不需要进行特征归一化，而GBDT则需要进行特征归一化\n288.两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。(A)\nA. 正确\nB. 错误\n答案:（A）\nPearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。\n289.下面哪个/些超参数的增加可能会造成随机森林数据过拟合？（B）\nA. 树的数量\nB. 树的深度\nC. 学习速率\n答案：（B）\n通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。\n290.目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是多少？（A）\nA.\n−(58log(58)+38log(38))\n-(\\frac{5}{8}log(\\frac{5}{8})+\\frac{3}{8}log(\\frac{3}{8}))\nB.\n(58log(58)+38log(38))\n(\\frac{5}{8}log(\\frac{5}{8})+\\frac{3}{8}log(\\frac{3}{8}))\nC.\n(38log(58)+58log(38))\n(\\frac{3}{8}log(\\frac{5}{8})+\\frac{5}{8}log(\\frac{3}{8}))\nD.\n(58log(38)−38log(58))\n(\\frac{5}{8}log(\\frac{3}{8})-\\frac{3}{8}log(\\frac{5}{8}))\n答案：（A）\n291.下面有关序列模式挖掘算法的描述，错误的是？（C）\nA. AprioriAll算法和GSP算法都属于Apriori类算法，都要产生大量的候选序列\nB. FreeSpan算法和PrefixSpan算法不生成大量的候选序列以及不需要反复扫描原数据库\nC. 在时空的执行效率上，FreeSpan比PrefixSpan更优\nD. 和AprioriAll相比，GSP的执行效率比较高\n@CS青雀，本题解析来源：\n机器学习：序列模式挖掘算法\n292.下列哪个不属于常用的文本分类的特征选择算法？（D）\nA. 卡方检验值\nB. 互信息\nC. 信息增益\nD. 主成分分析\n答案：（D）\n@CS青雀，本题解析来源：\n常采用特征选择方法。常见的六种特征选择方法：\n1）DF(Document Frequency) 文档频率\nDF:统计特征词出现的文档数量，用来衡量某个特征词的重要性\n2）MI(Mutual Information) 互信息法\n互信息法用于衡量特征词与文档类别直接的信息量。\n如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向”低频”的特征词。\n相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。\n3）(Information Gain) 信息增益法\n通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。\n4）CHI(Chi-square) 卡方检验法\n利用了统计学中的”假设检验”的基本思想：首先假设特征词与类别直接是不相关的\n如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。\n5）WLLR(Weighted Log Likelihood Ration)加权对数似然\n6）WFO（Weighted Frequency and Odds）加权频率和可能性\n293.类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？(D)\nA. 伪逆法-径向基（RBF）神经网络的训练算法，就是解决线性不可分的情况\nB. 基于二次准则的H-K算法：最小均方差准则下求得权矢量，二次准则解决非线性问题\nC. 势函数法－非线性\nD. 感知器算法－线性分类算法\n答案：（D）\n294.机器学习中做特征选择时，可能用到的方法有？（E）\nA.卡方\nB. 信息增益\nC. 平均互信息\nD. 期望交叉熵\nE. 以上都有\n答案：（E）\n295.下列方法中，不可以用于特征降维的方法包括（E）\nA. 主成分分析PCA\nB. 线性判别分析LDA\nC. 深度学习SparseAutoEncoder\nD. 矩阵奇异值分解SVD\nE. 最小二乘法LeastSquares\n答案：（E）\n特征降维方法主要有：PCA，LLE，Isomap\nSVD和PCA类似，也可以看成一种降维方法。\nLDA:线性判别分析，可用于降维。\nAutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出 L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别 进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。\nAutoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse 惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。\n结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。\n296.一般，K-NN最近邻方法在（ A）的情况下效果较好。\nA．样本较多但典型性不好\nB．样本呈团状分布\nC．样本较少但典型性好\nD．样本呈链状分布\n297.下列哪些方法可以用来对高维数据进行降维（A B C D E F）\nA. LASSO\nB. 主成分分析法\nC. 聚类分析\nD. 小波分析法\nE. 线性判别法\nF. 拉普拉斯特征映射\n解析：LASSO通过参数缩减达到降维的目的；\nPCA就不用说了；\n线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；\n小波分析有一些变换的操作降低其他干扰可以看做是降维；\n拉普拉斯请看机器学习降维算法四：Laplacian Eigenmaps 拉普拉斯特征映射。\n298.以下描述错误的是（C）\nA. SVM是这样一个分类器，它寻找具有最小边缘的超平面，因此它也经常被称为最小边缘分类器\nB. 在聚类分析当中，簇内的相似性越大，簇间的差别越大，聚类的效果就越差\nC. 在决策树中，随着树中结点输变得太大，即使模型的训练误差还在继续降低，但是检验误差开始增大，这是出现了模型拟合不足的原因\nD. 聚类分析可以看作是一种非监督的分类\n299.以下说法中正确的是（C）\nA. SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性\nB. 在adaboost算法中，所有被分错样本的权重更新比例相同\nC. boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重\nD. 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少\n300.关于正态分布,下列说法错误的是（C）\nA. 正态分布具有集中性和对称性\nB. 正态分布的均值和方差能够决定正态分布的位置和形态\nC. 正态分布的偏度为0，峰度为1\nD. 标准正态分布的均值为0，方差为1\n301.在以下不同的场景中,使用的分析方法不正确的有 （B）\nA. 根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级\nB. 根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式\nC. 用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫\nD. 根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女\n302.什么是梯度爆炸？\n答案：误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。\n在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。\n网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。\n303.梯度爆炸会引发什么问题？\n答案：在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。\n梯度爆炸导致学习模型无法从训练数据中获得更新（如低损失）。\n模型不稳定，导致更新过程中的损失出现显著变化。\n训练过程中，模型损失变成 NaN。\n如果你发现这些问题，那么你需要仔细查看是否出现梯度爆炸问题。\n以下是一些稍微明显一点的信号，有助于确认是否出现梯度爆炸问题。\n训练过程中模型梯度快速变大。\n训练过程中模型权重变成 NaN 值。\n训练过程中，每个节点和层的误差梯度值持续超过 1.0。\n305.如何修复梯度爆炸问题？\n重新设计网络模型\n在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。\n使用更小的批尺寸对网络训练也有好处。\n在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。\n使用 ReLU 激活函数\n在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。\n使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。\n使用长短期记忆网络\n在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。\n使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。\n采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。\n使用梯度截断（Gradient Clipping）\n在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。\n处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。\n——《Neural Network Methods in Natural Language Processing》，2017.\n具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。\n梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。\n——《深度学习》，2016.\n在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。\n默认值为 clipnorm=1.0 、clipvalue=0.5。\n使用权重正则化（Weight Regularization）\n如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。\n对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。\n——On the difficulty of training recurrent neural networks，2013.\n在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。\n306. LSTM神经网络输入输出究竟是怎样的？\n答案：@YJango，本题解析来源：LSTM神经网络输入输出究竟是怎样的？\n307.以下关于PMF(概率质量函数),PDF(概率密度函数),CDF(累积分布函数)描述错误的是？（A）\nA. PDF描述的是连续型随机变量在特定取值区间的概率\nB. CDF是PDF在特定区间上的积分\nC. PMF描述的是离散型随机变量在特定取值点的概率\nD. 有一个分布的CDF函数H(x),则H(a)等于P(X<=a)\n答案：（A）\n概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。\n概率密度函数（p robability density function，PDF ）是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。\n累积分布函数（cumulative distribution function，CDF） 能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对于所有实数x 与pdf相对。\n308.线性回归的基本假设有哪些？(ABDE)\nA. 随机误差项是一个期望值为0的随机变量；\nB. 对于解释变量的所有观测值，随机误差项有相同的方差；\nC. 随机误差项彼此相关；\nD. 解释变量是确定性变量不是随机变量，与随机误差项之间相互独立；\nE. 随机误差项服从正态分布\n309.处理类别型特征时，事先不知道分类变量在测试集中的分布。要将 one-hot encoding（独热码）应用到类别型特征中。那么在训练集中将独热码应用到分类变量可能要面临的困难是什么？（A、B）\nA. 分类变量所有的类别没有全部出现在测试集中\nB. 类别的频率分布在训练集和测试集是不同的\nC. 训练集和测试集通常会有一样的分布\n答案：（A、B ）\n如果类别在测试集中出现，但没有在训练集中出现，独热码将不能进行类别编码，这是主要困难。如果训练集和测试集的频率分布不相同，我们需要多加小心。\n310.假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出「-0.0001」。X 可能是以下哪一个激活函数？（B）\nA. ReLU\nB. tanh\nC. SIGMOID\nD. 以上都不是\n答案：（B）\n该激活函数可能是 tanh，因为该函数的取值范围是 (-1,1)。\n311.下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？(A、C）\nA. 类型 1 通常称之为假正类，类型 2 通常称之为假负类。\nB. 类型 2 通常称之为假正类，类型 1 通常称之为假负类。\nC. 类型 1 错误通常在其是正确的情况下拒绝假设而出现。\n答案：(A、C)\n在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。\n312.在下面的图像中，哪一个是多元共线（multi-collinear）特征？（D）\nA. 图 1 中的特征\nB. 图 2 中的特征\nC. 图 3 中的特征\nD. 图 1、2 中的特征\nE. 图 2、3 中的特征\nF. 图 1、3 中的特征\n答案：（D）\n在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。\n313.鉴别了多元共线特征。那么下一步可能的操作是什么？（B、C）\nA. 移除两个共线变量\nB. 不移除两个变量，而是移除一个\nC. 移除相关变量可能会导致信息损失，可以使用带罚项的回归模型（如 ridge 或 lasso regression）。\n答案：（B、C）\n因为移除两个变量会损失一切信息，所以我们只能移除一个特征，或者也可以使用正则化算法（如 L1 和 L2）。\n314.给线性回归模型添加一个不重要的特征可能会造成？（A）\nA. 增加 R-square\nB. 减少 R-square\n答案：（A）\n在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。\n315.假定目标变量的类别非常不平衡，即主要类别占据了训练数据的 99%。现在你的模型在测试集上表现为 99% 的准确度。那么下面哪一项表述是正确的？（A、C）\nA. 准确度并不适合于衡量不平衡类别问题\nB. 准确度适合于衡量不平衡类别问题\nC. 精确率和召回率适合于衡量不平衡类别问题\nD. 精确率和召回率不适合于衡量不平衡类别问题\n答案：（A、C）\n316.什么是偏差与方差？\n泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。\n317.解决Bias和Variance问题的方法是什么？\n交叉验证\nHigh Bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征\nHigh Variance解决方案：agging、简化模型，降维\n318.采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？\n用EM算法求解的模型一般有GMM或者协同过滤，K-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。\n319.XGBoost怎么给特征评分？\n在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。\n320.什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？\nBagging方法中Bootstrap每次约有\n13\n\\frac{1}{3}的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这\n13\n\\frac{1}{3}的数据称为袋外数据OOB（out of bag）,它可以用于取代测试集误差估计方法。\n袋外数据(OOB)误差的计算方法如下：\n对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类,因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=\nXO\n\\frac{X}{O};这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。"}
