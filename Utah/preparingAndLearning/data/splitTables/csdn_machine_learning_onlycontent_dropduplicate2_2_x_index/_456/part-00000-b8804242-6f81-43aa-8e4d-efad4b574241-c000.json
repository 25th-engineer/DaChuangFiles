{"content2":"第一集、机器学习与数据基础知识\n1、什么是机器学习？\n对于有些问题，我们无法用确定的逻辑编程实现。如图像识别，语音识别，垃圾邮件分类\n机器学习的核心思想是模拟人的学习能力\n从样本数据中学习，得到经验/模型，然后进行预测，这是一种数据驱动的方法。\n2、机器学习的基本概念\n样本-机器学习算法处理的数据\n特征向量-人工构造的用于描述一个样本的向量，如颜色、形状等\n预测函数-实现从样本的特征向量到预测值得映射\n目标函数\n训练时使用，目的是确定模型的参数\n3、有监督学习的一般流程\n4、机器学习算法的分类\n监督信号\n人工为样本打上的标签(label),如图像的分类，是26个英文字母中的哪一个\n按照样本是否有标签值，将机器学习算法分为有监督学习与无监督学习。有监督学习事先用带有标签的样本进行训练，然后用得到的模型预测。无监督学习直接对数据进行预测，样本不带有人工标注的标签值\n有监督学习\n根据样本学习得到一个映射函数\n分类问题-确定样本的类别，如人脸识别，字符识别，语音识别。类别标签是整数编号\n回归问题-确定一个实数值，如根据一个人的学历、工作年限、行业等信息预测他/她的收入\n无监督学习\n聚类问题-将一批样本分成多个类，即多个不相交的子集，每个样本属于其中的一个子集。类别没有事先定义，而是由算法确定。\n数据降维问题-将一个向量变换到低维空间中\n强化学习\n根据当前的状态确定要执行的动作，以达到某一目标，抽象的来说，是最大累计回报。\n5、需要哪些数学知识？\n微积分\n现行代数\n概率论\n最优化方法\n6、导数的定义？\n函数在点x处的导数定义为\n左导数与右导数-从左边与右边趋于x时的导数\n可导函数-左右极限都存在，并且相等\n导数的几何意义-函数在某一点处切线的斜率\n导数的物理意义-瞬时速度\n在各点处的导数构成的函数成为导函数，简称导数\n导数在机器学习和深度学习中的应用是求解目标函数的极值。\n重要的求导公式\n7、高阶导数\n8、导数与函数的性质\n一阶导数决定函数的单调性，导数大于0，函数单调增；导数小于0，函数单调减\n极值定理\n一个导函数在极值点处导数必定为0，导数为0的点成为函数的驻点\n二阶导函数决定函数的凹凸性\n二阶导数大于0，函数为凸函数，二阶导数小于0，函数为凹函数。\n二阶导数为0的点为函数的拐点。\n9、一阶函数的极值判别法则\n如果在x点处导数为0，即为函数的驻点，则\n如果二阶导数大于0，x为函数的极小值点\n如果二阶导数小于0，x为函数的极大值点\n如果二阶导数等于0，则情况不定。例如y = x^3\n10、向量及其运算\n向量为n个数组成的一个值，每个数成为向量的分量。与其对应的是标量。\n编程语言中的一维数组即为向量。\n力，速度等都是向量\n行向量、列向量\n数学中一般把向量表示成列向量，编程语言中一般把向量表示成行向量，即按行存储\n向量的运算\n加法，数乘，减法，内积，转置\n11、向量的范数\n12、矩阵及其运算\n矩阵\n方阵：行数和列数相等，记为n阶方阵\n主对角线：行列下标相等的位置\n对称矩阵：关于主对角线对称 Aij = Aji\n单位矩阵\n矩阵的运算\n加法，数乘，减法，转置，乘法\n逆矩阵\n13、特征值与特征向量 QR算法\n14、二次型\n15、张量\n16、偏导数\n17、高阶偏导数\n18、梯度\n19、雅克比矩阵\n20、Hessian矩阵\n21、多元函数的极值判别法则\n如果Hessian矩阵正定，函数在该点有极小值        相当于一元函数二阶导数>0\n如果Hessian矩阵负定，函数在该点有极大值\n如果Hessian矩阵不定，则为鞍点，不是极值点\n矩阵正定的定义\n矩阵正定的判别法则\n矩阵的特征值全大于0\n矩阵的所有顺序主子式都大于0\n矩阵合同于单位矩阵\n22、多元函数泰勒展开\n23、几个重要的矩阵和向量求导公式\n24、随机事件与概率\n25、条件概率和贝叶斯公式\n26、随机事件的独立性\n27、随机变量\n28、边缘概率与边缘密度\n29、随机变量的独立性\n30、协方差\n31、常用的多维分布\n32、最大似然估计\n33、最优化的基本概念\n34、为什么要用迭代法\n35、梯度下降法\n36、数值优化算法面临的问题\n37、生成模型与判别模型\n38、准确率与回归误差\n39、精度与召回率\n40、ROC曲线\n41、交叉验证\n42、欠拟合与过拟合\n43、正则化\n44、Logistic回归\n45、softmax回归\n46、主成分分析"}
