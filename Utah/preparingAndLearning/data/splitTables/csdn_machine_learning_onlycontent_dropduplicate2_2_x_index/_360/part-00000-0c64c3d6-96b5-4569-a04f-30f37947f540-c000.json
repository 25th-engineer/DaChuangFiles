{"content2":"机器学习之监督学习-回归\n一、机器学习算法分类\n有监督学习：\n分类\n回归\n半监督学习：\n分类\n回归\n无监督学习：\n聚类\n降维\n强化学习：\n马尔可夫决策过程\n动态规划\n参考网址：http://qing0991.blog.51cto.com/1640542/1851981\n二、线性回归\n一个案例：对连续型数据做出的预测属于回归问题。例如人们买房的时候，在知道房屋面积\nX\n1\nX_1\nX1 和卧室的数量\nX\n2\nX_2\nX2 的情况下，怎么推测得知房屋的价格\nY\nY\nY 呢。通过一组\nX\n1\nX_1\nX1 、\nX\n2\nX_2\nX2 、\nY\nY\nY 的实际数据，我们可以得到一个这样的关系：\nY\n=\nθ\n0\n+\nθ\n1\nX\n1\n+\nθ\n2\nX\n2\nY=θ_0+θ_1X_1+θ_2X_2\nY=θ0 +θ1 X1 +θ2 X2\n类似这种问题很多，比如已知一个人的年龄\nX\n1\nX_1\nX1 和体重\nX\n2\nX_2\nX2 ，推测人的身高\nY\nY\nY 。这都是线性回归问题，本质是拟合多组数据到一个函数上。\n参考网址：http://lib.csdn.net/article/machinelearning/2975\n线性回归（linear regression）\n输入特征（input features）：\nx\n(\ni\n)\nx^{(i)}\nx(i)\n输出（output）：\ny\n(\ni\n)\ny^{(i)}\ny(i) （取值连续）\n模型参数（model parameters）：\nθ\nθ\nθ\n假设函数（hypothesis function）：\nh\nθ\n(\nx\n)\n=\nx\nT\nθ\n=\n∑\ni\n=\n1\nn\nx\ni\nθ\ni\nh_θ(x)=x^{T}θ=\\sum_{i=1}^{n}x_iθ_i\nhθ (x)=xTθ=∑i=1n xi θi\n损失函数（squared loss function to be minimized）：\nl\n(\nh\nθ\n(\nx\n)\n,\ny\n)\n=\n(\nh\nθ\n(\nx\n)\n−\ny\n)\n2\nl(h_θ(x),y)=(h_θ(x)-y)^{2}\nl(hθ (x),y)=(hθ (x)−y)2\n注：输入的\ny\ny\ny 和\nh\n(\nx\n)\nh(x)\nh(x) 之间满足方程\ny\n=\nh\n(\nx\n)\n+\ne\ny=h(x)+e\ny=h(x)+e。\ne\ne\ne 是误差项（噪音项），假设\ne\ne\ne 是独立同分布 iid（independent and identity distribution）和均值为0，方差为某一定数的高斯分布。\n线性回归的目标是求出线性回归方程，即求出线性回归方程中的回归系数\nθ\nθ\nθ。\n参考网址：http://blog.csdn.net/tangyudi/article/details/77711981\n二维空间内的线性回归非常简单。它就是寻找一条最优直线来对数据进行拟合。根据最小二乘原理，确定的准则：寻找一条直线，使得函数值与模型预测值之差的平方和最小。\n多维空间内的线性回归就是寻找一条最优超平面来对数据进行拟合。根据最小二乘原理，确定的准则：超平面与分布数据的误差最小。\n求解方法：\n最大似然函数+最小二乘法\n梯度下降\n参考网址：http://blog.csdn.net/tangyudi/article/details/77769045\n参考网址： http://blog.csdn.net/titan0427/article/details/50365480\n##三、非线性回归\n非线性回归（non-linear regression）：拟合曲线、非直线。有部分非线性回归可以转化为线性求解，这些模型称为广义线性模型，例如 logistic 回归。（非线性回归又称为逻辑回归，LR）\n实际问题中，变量之间常常不是直线。解决方法通常是选择一条比较接近的曲线，通过变量替换把非线性方程加以线性化，然后按照线性回归的方法进行拟合。"}
