{"content2":"基本方程：Ax=b\n本质上讲，机器学习需要基于算法系统，通过‘优化’去让等式的误差达到最小。\n这里关注参数向量（权重）x的变化，直到找到一组适当的x值，使模型输出最接近真实输出。\n当损失函数计算结果后，x就再次调整，缩小损失，直到极值点。\n一个描述每个权重所带来的误差的误差矩阵会与权重矩阵本身相乘。\nSDG是最基本的优化算法。\n正则化和学习率属于模型的超参数，超参数的设置常常需要经验。\n一些基本概念，描述了机器学习/深度学习可以干什么：\n回归Regression：指通过自变量去估计因变量，预测真实值。简单讲就是预测。\n回归解决“多少”的问题。\n常见的是线性回归模型，如Ax=b.\n矩阵形式：y = a + Bx ， a为函数图形与Y轴交点到原点距离\n扩展形式：y = a + b0 * x0 + b1 * x1 + . . . + bn * xn\n以散点图表示，预测的直线表示与所有的点的距离最短。\n拟合Fitting：指用预测值f(x)去尽可能的接近真实值y. 相关的概念还有过拟合和欠拟合。\n在Ax=b中，想求出x，有三个组件：\n1. 内乘\n2. 成本函数，常见的用（预测-实际）的平方。\n3. 更新函数，即成本函数的导数。\n非线性回归模型：\n线性回归模型中，x的指数是1，非线性回归模型处理x指数大于1的情况，因而机器学习常被称为曲线拟合。\n但完美的曲线拟合往往意味着过拟合，没有泛化和预测能力。\n分类Classification：\n分类解决“是什么”的问题。如5个苹果，“5个”是回归模型，“苹果”（而不是梨子）是分类问题。\n分类基于输入的特征(features)，去回答是什么的问题。\n基础的分类是2值分类。在0-1分类中，以0.5为分界。  单输出NN模型\nN值分类中，可以为每个值打分。 多输出NN模型。\n分类可应用于推荐系统，基于用户的相拟性或物品的相拟性。\n最有名的是亚马逊的协作过滤(Collaborative Filtering)推荐算法，算是分类的变体。\n聚类Clustering：\n属于无监督学习算法，汉语上分类和聚类正好相反。但都是处理分类的问题，只是方法上不太一样。\n它首先对每个样本有个距离的度量，距离相拟的样本是相拟的。然后迭代的移动这些样本，让它们靠得更近，结果形成了N个堆堆。\nK-means算法是聚类的一个变体。"}
