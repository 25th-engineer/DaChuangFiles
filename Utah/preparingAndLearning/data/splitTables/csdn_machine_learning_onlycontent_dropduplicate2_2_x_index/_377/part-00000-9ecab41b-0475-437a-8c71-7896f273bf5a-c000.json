{"content2":"机器学习是人工智能的核心，主要分为有监督学习和无监督学习。那么什么叫有监督和无监督呢？是否有监督，主要看输入的样本数据集是否有目标变量。有监督学习是有明确的目标变量，而无监督学习，此类的数据没有类别信息，也不给定目标值。\n这段时间主要学习了有监督学习，所以下面对有监督学习进行一个大纲总结：\n有监督学习\n有监督学习一般使用两种类型的目标变量：标称型和数值型。标称型的目标变量结果只在有限目标集中取值，如真与假、男人与女人、电影分类集合{动作片、武打片、恐怖片，喜剧片，爱情片}。数值型目标变量则可以从无限的数值集合中取值，例如-∞ ~+∞，0~10 等。分类和回归都属于有监督学习，因为这两类算法都必须知道自己要预测什么，即明确目标变量的分类信息。\n分类\n分类的主要任务是将实例数据划分到合适的分类中，主要是预测目标变量是标称型的数据。常见的分类算法有：K-近邻算法、决策树、朴素贝叶斯、Logistic回归（SVM和元算法此处不讨论）。\nK-近邻（KNN) :\nk-近邻算法，使用某种距离计算方法进行分类。其工作原理是： 存在一个样本数据集合，也称作训练样本集，并且样本训练集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行距离计算，选择K个距离最近的数据。通常K是不大于20的整数。\n如上图所示：k = 3，D点距离点红C、红F、蓝G三点的距离最近，因为红色占的票数较多，所以D归属于红类。\n步骤：\n1) 计算待测的点与已知样本点之间的距离；\n2) 按照距离递增排序；\n3) 选取当前点距离最小的K个点\n4) 确定前K个点所在类别出现的频率\n5) 返回频率最高的类别作为待测点的预测分类\n决策树：\n决策树是一个树形结构，可以是二叉树或非二叉树。其工作原理是：对原始数据，基于最好的属性划分数据集。数据集的划分主要是采用递归原则进行划分，每个分支代表某个特征树形在某个值域上的输出，每个叶子节点存放一个类别。递归结束的依据就是每个分支下的所有实例都具有相同的分类。\nPS: 这里的划分算法采用的IDS算法，决策树既可以用于回归也可以用分类，后续会介绍采用CART算法的决策树，进行回归预测。\n朴素贝叶斯\n朴素贝叶斯，是一个选择高概率类别的分类器，它的核心是朴素贝叶斯准则，而朴素贝叶斯准则的基石是条件概率。\n条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。例如，我们有一个罐子，里面装有7个石头，3个灰球，4个黑球。那么我们取出灰球的概率 P(gray)= 3/7，显然黑球的概率是4/7。 如果7个石头放在两个桶中，那么我们又该如何计算以上概率呢？如下图：\n如上图所示，我们要想计算P(gray)和P（black），就得知道石头在桶的分布是一定的结果下。此时，在已知石头出自B桶的条件下，取出灰石头的概率，即P(gray|bucketB) = 1/3。 这个概率我们就叫条件概率。\n另外一种有效计算条件概率的方法称为贝叶斯准则，贝叶斯准则通过交换条件概率中的条件和结果。朴素贝叶斯公式：\n步骤：\n1） 从文本中构建词向量\n2） 利用词向量计算特征的条件概率，即将该词条的数目除以总词条数目\n3） 返回每个类别的条件概率\n4） 找到所有概率中的最大值\nPS : 利用朴素贝叶斯分类器对文档进行分类时，遇到的问题是下溢出，解决方法是对乘积取自然对数。\nLogistic回归\nLogistic 回归虽然名字叫回归，但它其实是一个分类器。 回归是什么意思呢？假设我们现在有一些数据点，我们用一条直线对这些点进行拟合，这个拟合的过程我们就叫回归。而这条直线我们就叫最佳拟合直线。\n工作原理：利用最优算法（梯度上升法）寻找最佳拟合参数，将最佳拟合参数乘以每个特征值，最后相乘的结果进行相加，将这个总和带入到sigmoid函数中，进而得到一个0~1之间的数值，如果这个数值大于0.5归于1类，小于0.5则归于0类。\n步骤：\n1. 利用梯度上升算法找到最佳系数\n2. 画出决策边界\n3. 将特征向量乘以最优回归系数，并相加\n4. 将和输入到sigmoid函数，即可得到所属类别\n5.\n线性回归\n回归的目的是预测数值型的目标值。例如，我们可以预测鲍鱼的年龄。\n工作原理：利用普通最小二乘法求得回归系数，再用回归系数乘以输入值，将结果全部加在一起后就得到了预测值。 此处的回归系数是一个向量，输入也是一个向量，所以实质上也是求出二者的内积。\nY1 就是我们的样本的目标变量，x1是样本的特征，w未知。我们如何求得w呢？一个常用的方法就是找出误差最小的w。这里的误差指预测y值和真实y值之间的差值。\n常用的算法有：标准线性回归、局部加权、岭回归、lasso和前向逐步回归。\n树回归\n树回归主要分为回归树和模型树。回归树在每个叶子节点包含单个值，模型树，其每个叶节点包含一个线性模型。\n工作原理：主要用CART的树构建算法对数据进行二元切分，切分的依据是如果特征值大于给定值，就走左子树，否则就走右子树。那么什么时候停止切分呢？用户设定两个参数，用于控制函数的停止时机，一旦停止切分会生成一个叶节点。\n步骤： 构建树 —>预剪枝—>后剪枝\n后期会继续对SVM 、元算法和无监督学习进行总结。也会具体针对一些典型的算法进行深入的研究总结。"}
