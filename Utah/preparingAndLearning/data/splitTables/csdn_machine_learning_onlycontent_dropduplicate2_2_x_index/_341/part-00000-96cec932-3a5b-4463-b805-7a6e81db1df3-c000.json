{"content2":"本文试图从分类学习来分析现有几种机器学习模型的基本思路。\n首先强调，机器学习所使用的训练数据和测试数据被看作是依联合分布概率\nP(X,Y)\nP(X,Y)独立同分布产生，这是监督学习关于数据的基本假设。\n对于分类问题，设输入空间\nX⊆Rn\n\\mathcal{X} \\subseteq R^n为\nn\nn维向量的集合，输出空间为类标记集合\nY={c1,c2,...,cK}\n\\mathcal{Y} = \\{ c_1, c_2, ..., c_K \\}。输入为特征向量\nx∈X\nx \\in \\mathcal{X}，输出为类标记\ny∈Y\ny \\in \\mathcal{Y}。\nX\nX是定义在输入空间\nX\n\\mathcal{X}上的随机变量，\nY\nY是定义在输出空间\nY\n\\mathcal{Y}上的随机变量。\nP(X,Y)\nP(X,Y)是\nX\nX和\nY\nY的联合概率分布。训练数据集\nT={(x1,y1),(x2,y2),...,(xN,yN)}\nT = \\{ (x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \\}\n由\nP(X,Y)\nP(X,Y)独立同分布产生。我们的目的是通过训练集学习到联合概率分布\nP(X,Y)\nP(X,Y)或条件概率分布\nP(Y|X)\nP(Y|X)。\n如果训练集足够大，我们由训练集所获得的经验条件概率分布\nP^(Y|X)\n\\hat{P} (Y|X)可以无限逼近于实际的条件概率分布\nP(Y|X)\nP(Y|X)。但问题在于，我们很难获得足够多的样本。对于分类问题，假设\nx(j)\nx^{(j)}可取值有\nSj\nS_j个，\nj=1,2,...,n\nj = 1, 2, ..., n，那么输入空间与输出空间所可能的组合数\nM=K∏nj=1Sj\nM = K \\prod_{j=1}^{n} S_j。\n事实上，样本数量\n|T|\n|T|相对“可能性”数量\nM\nM不够大，是个机器学习各分类模型要解决的主要问题。如前所述，如果\n|T|M\n\\frac{|T|}{M}足够大，我们实际上可以以经验条件概率分布\nP^(Y|X)\n\\hat{P} (Y|X)来作为实际条件概率分布\nP(Y|X)\nP(Y|X)（注意关于数据的基本假设：训练数据和测试数据被看作是依联合分布概率\nP(X,Y)\nP(X,Y)独立同分布产生）。\n所谓过拟合，就是无视\n|T|M\n\\frac{|T|}{M}必须保持较高的比例这一要求，所求得的结果其实没有统计意义。在训练集之外的现实世界，正是因为数据的巨型数量，才呈现出分布规律。对于分类问题，现实世界中数据数量与“可能性”数量\nM\nM的比趋向于无穷。\n好，既然\n|T|\n|T|不够大，只有想办法减少\nM\nM，保持\n|T|M\n\\frac{|T|}{M}处于较高的比例，才有可能获得接近\nP(Y|X)\nP(Y|X)的\nP^(Y|X)\n\\hat{P} (Y|X)。现有的机器学习模型，基本都是遵循这一基本思路设计，考虑到\nM=K∏Nj=1Sj\nM = K \\prod_{j=1}^{N} S_j，其中\nK\nK是不变的，具体的减少\nM\nM的办法有\n2\n2类。\n减少条件特征数\nn\nn\n决策树\n决策树模型通过计算信息增益来识别对结果（种类）\nY\nY影响最大的那些特征，决策树的剪枝实际上就是去掉那些对结果影响小的特征的识别，实际上就是减少了特征数量\nn\nn，从而\nM\nM也得到减小。\n最大熵\n最大熵模型通过指定“特征”及“特征函数”，来过滤出那些更有价值的样本点。实际上，这些“特征”往往是某个条件特征或是某些条件特征组合的一种反映，本质上还是条件特征的筛选，设计思路也是减少条件特征数\nn\nn，从而减小\nM\nM。\n减少条件取值数\nS\nS\nk\nk临近法\nk\nk临近法模型，对于样本集中没有出现过的特征向量\nx\nx，直接以样本中“靠近\nx\nx”的特征向量来表征它。比如极端情况\nk=1\nk = 1，则相当于直接用离\nx\nx最近的样本特征向量来代替\nx\nx。所以\nk\nk临近法模型的思路其实是：使用少部分（样本集中出现过的）特征值来取代其他的特征值，也就是减少取值范围\nSj(j=1,2,...,n)\nS_{j(j = 1, 2, ..., n)}，从而减小\nM\nM。\n朴素贝叶斯\n朴素贝叶斯模型通过条件独立性假设，来推测样本集中没有出现过的特征向量的分布概率。举个例子，我们知道\nx(1)i\nx_i^{(1)}取值范围是\n{Si1,Si2}\n\\{S_{i1}, S_{i2}\\}，\nx(2)i\nx_i^{(2)}取值范围是\n{Si3,Si4,Si5}\n\\{S_{i3}, S_{i4}, S_{i5}\\}，我们不需要样本集中出现过\nxj={Si1,Si3,...}\nx_j = \\{S_{i1}, S_{i3}, ...\\}就可以推测\nxj\nx_j的分布概率。实际上朴素贝叶斯模型同样是使用少部分（样本集中出现过的）特征值来取代（计算）其他的特征值，本质上也是减少取值范围\nS\nS，从而减小\nM\nM。\n这里多聊聊\nk\nk临近法和朴素贝叶斯，它们虽然基本思路一样，但考虑问题的角度却截然相反。朴素贝叶斯基于条件独立性假设，各条件特征相关性越小越好。\nk\nk临近法认为接近的点更相似，但判断“接近”时只考虑数学上的距离而不考虑各坐标轴（条件特征）是否具有可比性。比如衣服的长度和颜色是两个无关的特征，但在\nk\nk临近法模型中它们变得相互关联，根据颜色到数值的映射你会发现某种颜色跟长度\n180\n180更接近（？）。所以其实\nk\nk临近法模型想取得较好的效果，反而要求条件特征具有较高的关联性，才能使得“特征向量\nx\nx在某个特征维度上移动会影响到其它特征维度上的向量\nx′\nx’与\nx\nx的相关性的判断（即距离）”这件事情显得合理。\n（未完）"}
