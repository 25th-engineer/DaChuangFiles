{"content2":"一：python基础，自然语言概念\nfrom nltk.book import *\n1，text1.concordance(\"monstrous\")      用语索引\n2，text1.similar(\"best\")\n3，text2.common_contexts([\"monstrous\", \"very\"])\n4，text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n5，text3.generate()\n6，sorted(set(text3))\n7，text3.count(\"smote\")\n8，100 * text4.count('a') / len(text4)\nex1 = ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']。链表list\nsorted(ex1)，len(set(ex1))，  ex1.count('the')。\n['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail']\nsent1.append(\"Some\")\ntext4[173]，text4.index('awaken')，text5[16715:16735]，index从0开始，不包含右边的index\nFreqDist(text1)  频率分布\n高频词和低频词，停用词    hapaxes() 低频词\nlong_words = [w for w in V if len(w) > 15]\nfdist5 = FreqDist(text5)\nsorted([w for w in set(text5) if len(w) > 7 and fdist5[w] > 7])\nbigrams\n>>> bigrams(['more', 'is', 'said', 'than', 'done'])\n[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]\ntext4.collocations()\n词长，词频\n用途：\n1，词意消歧\n2，指代消解\n3，机器翻译\n4，人机对话系统\n5，文本的含义\n一个标识符token是表示一个我们想要放在一组对待的字符序列——如：hairy、his 或者:)——的术语\n一个词类型是指一个词在一个文本中独一无二的出现形式或拼写\n将文本当做词链表，文本不外乎是词和标点符号的序列\n1，变量\n2，字符串    name * 2\n3，链表 list  ：saying = ['After', 'all', 'is', 'said', 'and', 'done']；saying[-2:]？saying[-2:0]\n4，条件：[w for w in text if condition]   and   or\n5，嵌套代码块，控制结构  冒号表示当前语句与后面的缩进块有关联\nif len(word) >= 5:\nprint 'word length is greater than or equal to 5'\nfor word in ['Call', 'me', 'Ishmael', '.']:\nprint word\n6，函数  ：def mult(x, y)，局部变量，全局变量global\n7，模块module：textproc.py； from textproc import plural；plural('wish')\n8，包package\n函数含义\ns.startswith(t) 测试s 是否以t 开头\ns.endswith(t) 测试s 是否以t 结尾\nt in s 测试s 是否包含t\ns.islower() 测试s 中所有字符是否都是小写字母\ns.isupper() 测试s 中所有字符是否都是大写字母\ns.isalpha() 测试s 中所有字符是否都是字母\ns.isalnum() 测试s 中所有字符是否都是字母或数字\ns.isdigit() 测试s 中所有字符是否都是数字\ns.istitle() 测试s 是否首字母大写（s 中所有的词都首字母大写）\n二：语料库\n1，古腾堡语料库\n古腾堡项目，gutenberg\n文本特征：平均词长、平均句子长度，词频\n2，网络和聊天文本\n3，布朗语料库\nfrom nltk.corpus import brown\nbrown.categories()\n4，路透社语料库\n5，就职演说语料库\n6，标注文本语料库\n文本语料库的结构：\n载入你自己的语料库\n条件频率分布：\n条件和事件：\npairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]\n绘制分布图和分布表\n词汇工具：Toolbox和 Shoebox\nWordNet\nWordNet 是一个面向语义的英语词典，由同义词的集合—或称为同义词集（synsets）—\n组成，并且组织成一个网络\n意义与同义词：wn.synsets('motorcar')；wn.synset('car.n.01').lemma_names；\n['car', 'auto', 'automobile', 'machine', 'motorcar']\nWordNet的层次结构\nWordNet 概念层次片段：每个节点对应一个同义词集;边表示上位词/下位词关系，即\n上级概念与从属概念的关系；\n词汇关系：上/下位，整体/部分,蕴涵,反义词\n语义相似度：\npath_similarityassigns是基于上位词层次结构中相互连接的概念之间的最短路径在0-1 范围的打分（两者之间没有路径就返回-1）。同义词集与自身比较将返回1；Path方法是两个概念之间最短路径长度的倒数\nis－a关系是纵向的，has－part关系是横向\n齐夫定律：f(w)是一个自由文本中的词w 的频率。假设一个文本中的所有词都按照它\n们的频率排名，频率最高的在最前面。齐夫定律指出一个词类型的频率与它的排名成反\n比（即f×r=k，k 是某个常数）。例如：最常见的第50 个词类型出现的频率应该是最常\n见的第150 个词型出现频率的3 倍\n三：加工原料文本\n分词和词干提取\n1，分词\ntokens = nltk.word_tokenize(raw)\n2，处理HTML\nraw = nltk.clean_html(html)\n3，读取本地文件\nf = open('document.txt')； raw = f.read()\n4，NLP 的流程\n5，字符串：最底层的文本处理\n字符串运算：+，* 【b = [' ' * 2 * (7 - i) + 'very' * i for i in a]】\n输出字符串：print monty\n访问单个字符：monty[0]\n访问子字符串：monty[6:10]；monty[-12:-7]\n更多的字符串操作：\n链表与字符串的差异\nquery = 'Who knows?'\nbeatles = ['John', 'Paul', 'George', 'Ringo']\n字符串是不可变的，链表是可变的\n6，Unicode编码，解码\n在 Python中使用本地编码\n#!/bin/env python\n# -*- coding: UTF-8 -*-\n#Filename:build_SmartNavigation.py\n7，正则表达式re\n[w for w in wordlist if re.search('ed$', w)]\n[w for w in wordlist if re.search('^..j..t..$', w)]          [^aeiouAEIOU]\nsum(1 for w in text if re.search('^e-? mail$', w))\n[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]\n[w for w in chat_words if re.search('^m+i+n+e+$', w)]\n[w for w in chat_words if re.search('^[ha]+$', w)]              +*\n【转义】，{}【出现次数】，()【范围】和|【取或】\n[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n[w for w in wsj if re.search('(ed|ing)$', w)]\nre的用处：查找词干；搜索已分词文本；\n8，规范化文本【 词干提取器 ：词形归并】\nlower（）；\n词干提取：\nporter = nltk.PorterStemmer();\n[porter.stem(t) for t in tokens];\n词形归并：\n词形归并是一个过程，将一个词的各种形式（如：appeared，appears）映射到这个词标\n准的或引用的形式，也称为词位或词元（如：appear）\nwnl = nltk.WordNetLemmatizer()\n[wnl.lemmatize(t) for t in tokens]\n9，用正则表达式为文本分词\nre.split(r' ', raw)\nre.split(r'[ tn]+', raw)\nre.split(r'W+', raw)\n10，NLTK 的正则表达式分词器\nnltk.regexp_tokenize()\n11，断句，分词：分词是将文本分割成基本单位或标记，例如词和标点符号\n现在分词的任务变成了一个搜索问题：找到将文本字符串正确分割成词汇的字位串\ntext = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n>>> seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n>>> evaluate(text, seg3)\n46\n>>> evaluate(text, seg2)\n47\n>>> evaluate(text, seg1)\n63\n利用模拟退火算法\n12，从链表到字符串\nsilly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n' '.join(silly)\n'We called him Tortoise because he taught us .'\n\"%s wants a %s %s\" % (\"Lee\", \"sandwich\", \"for lunch\")"}
