{"content2":"1.获取文本语料库\nNLTK库中包含了大量的语料库，下面一一介绍几个：\n（1）古腾堡语料库：NLTK包含古腾堡项目电子文本档案的一小部分文本。该项目目前大约有36000本免费的电子图书。\n>>>import nltk >>>nltk.corpus.gutenberg.fileids() ['austen-emma.txt','austen-persuasion.txt' 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt','bryant-stories.txt','burgess-busterbrown.tx'carroll-alice.txt', 'chesterton-ball.txt','chesterton-brown.txt','chesterton-thursday.tx'edgeworth-parents.txt' 'melville-moby_dick.txt'milton-paradise.txt', 'shakespeare-caesar.txt, 'shakespeare-hamlet.txt, 'shakespeare-macbeth.txt 'whitman-leaves.txt']\n使用：from nltk.corpus import gutenberg\n写一段简短的程序，通过遍历前面所列出的与gutenberg文体标识符相应的fileid，然后统计每个文本：\nimport nltk from nltk.corpus import gutenberg for fileid in gutenberg.fileids(): num_chars=len(gutenberg.raw(fileid)) ###统计字符数 num_words=len(gutenberg.words(fileid)) ##统计单词书 num_sent=len(gutenberg.sents(fileid)) ###统计句子数 num_vocab=len(set([w.lower() for w in gutenberg.words(fileid)])) ###唯一化单词 print(int(num_chars/num_words),int(num_words/num_sent),int(num_words/num_vocab),fileid)\n结果为：4 24 26 austen-emma.txt\n4 26 16 austen-persuasion.txt\n4 28 22 austen-sense.txt\n4 33 79 bible-kjv.txt\n4 19 5 blake-poems.txt\n4 19 14 bryant-stories.txt\n4 17 12 burgess-busterbrown.txt\n4 20 12 carroll-alice.txt\n4 20 11 chesterton-ball.txt\n4 22 11 chesterton-brown.txt\n4 18 10 chesterton-thursday.txt\n4 20 24 edgeworth-parents.txt\n4 25 15 melville-moby_dick.txt\n4 52 10 milton-paradise.txt\n4 11 8 shakespeare-caesar.txt\n4 12 7 shakespeare-hamlet.txt\n4 12 6 shakespeare-macbeth.txt\n4 36 12 whitman-leaves.txt\n这个结果显示了每个文本的3个统计量：平局词长，平均句子长度和文本中每个词出现的平均次数。\n（2）网络和聊天文本：\n这部分代表的是非正式的语言，包括Firefox交流论坛、在纽约无意听到的对话、《加勒比海盗》电影剧本。个人广告以及葡萄酒的评论。\n导入：from nltk.corpus import webtext\nimport nltk from nltk.corpus import webtext for fileid in webtext.fileids(): print( fileid,webtext.raw(fileid)[:65],'...')\n结果为：firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se ...\ngrail.txt SCENE 1: [wind] [clop clop clop]\nKING ARTHUR: Whoa there!  [clop ...\noverheard.txt White guy: So, do you have any plans for this evening?\nAsian girl ...\npirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr ...\nsingles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun ...\nwine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb ...\n还有一个即时聊天会话语料库，最初由海军研究生院为研究自动检测互联网入侵者而收集的：\n>>>from nltk.corpus import nps_chat\n(3)布朗语意库：\n布朗语意库是第一个百万词集的英语电子语料库，有布朗大学于1961年创建，包含500多个不同来源的文本，按照文本类型，如新闻、社评等分类。\n>>>import nltk >>>from nltk.corpus import brown >>>print(brown.categories()) ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n布朗语料库是一个研究文体之间系统性差异的资源。让我们来比较不同文体的情态动词的用法。步骤如下：\n第一步：对特定文体进行计数。\nimport nltk from nltk.corpus import brown news_text=brown.words(categories='news') fdist=nltk.FreqDist([w.lower() for w in news_text]) modals=['can','could','may','might','must','will'] for m in modals: print(m+':',fdist[m])\n结果如下:can: 94,could: 87,may: 93,might: 38,must: 53,will: 389\n第二步：统计每一个感兴趣的文体。我们使用NLTK提供的条件概率分布函数。\ncfd=nltk.ConditionalFreqDist((genre,word) for genre in brown.categories() for word in brown.words(categories=genre)) genres=['news','religion','hobbies','science_fiction','romance','humor'] modals=['can','could','may','might','must','will'] cfd.tabulate(conditions=genres,samples=modals)\n输出结果为：\ncan could may might must will news 93 86 66 38 50 389 religion 82 59 78 12 54 71 hobbies 268 58 131 22 83 264 science_fiction 16 49 4 12 8 16 romance 74 193 11 51 45 43 humor 16 30 8 8 9 13\n（4）路透社语料库\n路透社语料库包括10788个新闻文档，共计130万字。这些文档分成了90个主题，按照‘训练’和‘测试’分为两组。因此，编号为‘test/14826’的文档属于测试组。这样分割是为了方便运用训练和测试算法的自动检验文档的主题。\n（5）就职演说语料库\n语料库实际上是55个文本的集合，每个文本都是一个总统的演讲。这个集合的显著特征就是时间维度。\nimport nltk from nltk.corpus import inaugural print(inaugural.fileids())\n['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt']\n可以发现，每个文本的年代都出现在他的文件名中。要从文件名中提取出年代，只需要使用fileid[:4]即可。\n例子：我们可以看看‘American’和‘citizen’随着时间推移的使用情况。\nimport nltk from nltk.corpus import inaugural cfd=nltk.ConditionalFreqDist((target,fileid[:4]) for fileid in inaugural.fileids() for w in inaugural.words(fileid) for target in ['american','citizen'] if w.lower().startswith(target) ) cfd.plot()\n结果如下：\n（6）标注文本语料库和其他语言语料库"}
