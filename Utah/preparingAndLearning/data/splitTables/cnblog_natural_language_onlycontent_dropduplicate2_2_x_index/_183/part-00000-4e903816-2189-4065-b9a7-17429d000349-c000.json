{"content2":"Updated 1st 2011.8.6\nCHAPTER 2\nAccessing Text Corpora and Lexical Resources\n访问文本语料库和词汇资源\nPractical work in Natural Language Processing typically uses large bodies of linguistic data, or corpora. The goal of this chapter is to answer the following questions:\n1. What are some useful text corpora and lexical resources, and how can we access them with Python?\n什么是有用的文本预料可和词汇资源，我们如何通过Python访问它们？\n2. Which Python constructs are most helpful for this work?\nPython构造的哪个方面对于这一项工作是最有帮助的？\n3. How do we avoid repeating ourselves when writing Python code?\n在写Python代码的时候，我们如何避免重复？\nThis chapter continues to present programming concepts by example, in the context of a linguistic processing task. We will wait until later before exploring each Python construct systematically. Don’t worry if you see an example that contains something unfamiliar; simply try it out and see what it does, and—if you’re game(勇敢的)—modify it by substituting(代替) some part of the code with a different text or word. This way you will associate（联系） a task with a programming idiom, and learn the hows and whys later.\n2.1 Accessing Text Corpora 访问文本语料库\nAs just mentioned, a text corpus is a large body of text. Many corpora are designed to contain a careful balance of material in one or more genres. We examined some small text collections in Chapter 1, such as the speeches known as the US Presidential Inaugural Addresses. This particular corpus actually contains dozens of individual texts—one per address—but for convenience we glued them end-to-end and treated them as a single text. Chapter 1 also used various predefined texts that we accessed by typing from book import *. However, since we want to be able to work with other texts, this section examines a variety of text corpora. We’ll see how to select individual texts, and how to work with them.\nGutenberg Corpus\nNLTK includes a small selection of texts from the Project Gutenberg electronic text archive(古腾堡电子文本存档), which contains some 25,000(现在是36,000了) free electronic books, hosted at http://www.gutenberg.org/. We begin by getting the Python interpreter to load the NLTK package, then ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:\n>>> import nltk\n>>> nltk.corpus.gutenberg.fileids()\n['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt','chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\nLet’s pick out the first of these texts—Emma by Jane Austen—and give it a short name, emma, then find out how many words it contains:\n>>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n>>> len(emma)\n192427\nIn Section 1.1, we showed how you could carry out concordancing of a text such as text1 with the command text1.concordance(). However, this assumes that you are using one of the nine texts obtained as a result of doing from nltk.book import *. Now that you have started examining data from nltk.corpus, as in the previous example, you have to employ the following pair of statements to perform concordancing and other tasks from Section 1.1:\n>>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n>>> emma.concordance(\"surprize\")\nWhen we defined emma, we invoked the words() function of the gutenberg object in NLTK’s corpus package. But since it is cumbersome(累赘的) to type such long names all the time, Python provides another version of the import statement, as follows:\n>>> from nltk.corpus import gutenberg\n>>> gutenberg.fileids()\n['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]\n>>> emma = gutenberg.words('austen-emma.txt')\nLet’s write a short program to display other information about each text, by looping over all the values of fileid（文件标识） corresponding to the gutenberg file identifiers listed earlier and then computing statistics for each text. For a compact output display, we will make sure that the numbers are all integers, using int().\n>>> for fileid in gutenberg.fileids():\n...     num_chars = len(gutenberg.raw(fileid)) ①\n...     num_words = len(gutenberg.words(fileid))\n...     num_sents = len(gutenberg.sents(fileid))\n...     num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n...           print int(num_chars/num_words), int(num_words/num_sents),     int(num_words/num_vocab), fileid\n...\n4 21 26 austen-emma.txt\n4 23 16 austen-persuasion.txt\n4 24 22 austen-sense.txt\n4 33 79 bible-kjv.txt\n4 18 5 blake-poems.txt\n4 17 14 bryant-stories.txt\n4 17 12 burgess-busterbrown.txt\n4 16 12 carroll-alice.txt\n4 17 11 chesterton-ball.txt\n4 19 11 chesterton-brown.txt\n4 16 10 chesterton-thursday.txt\n4 18 24 edgeworth-parents.txt\n4 24 15 melville-moby_dick.txt\n4 52 10 milton-paradise.txt\n4 12 8 shakespeare-caesar.txt\n4 13 7 shakespeare-hamlet.txt\n4 13 6 shakespeare-macbeth.txt\n4 35 12 whitman-leaves.txt\nThis program displays three statistics for each text: average word length平均字长, average sentence length平均句长, and the number of times each vocabulary item appears in the text on average本文中每个词汇平均出现数量 (our lexical diversity score我们的词汇多样性得分). Observe that average word length appears to be a general property of English, since it has a recurrent（周期性的） value of 4. (In fact, the average word length is really 3, not 4, since the num_chars variable counts space characters.) By contrast average sentence length and lexical diversity appear to be characteristics of particular authors.\nThe previous example also showed how we can access the “raw” text of the book①, not split up into tokens. The raw() function gives us the contents of the file without any linguistic processing(对文件的内容不进行任何语言处理). So, for example, len(gutenberg.raw('blake-poems.txt') tells us how many letters occur in the text, including the spaces between words. The sents() function divides the text up into its sentences, where each sentence is a list of words（把文本分割成句子，每个句子是一个由单词组成的列表）:\n>>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n>>> macbeth_sentences\n[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]\n>>> macbeth_sentences[1037]\n['Good', 'night', ',', 'and', 'better', 'health', 'Attend', 'his', 'Maiesty']\n>>> longest_len = max([len(s) for s in macbeth_sentences])\n>>> [s for s in macbeth_sentences if len(s) == longest_len]\n[['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that', 'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The', 'mercilesse', 'Macdonwald', ...], ...]\nMost NLTK corpus readers include a variety of access methods apart from words(), raw(), and sents(). Richer linguistic content is available from some corpora, such as part-of-speech tags, dialogue tags, syntactic trees, and so forth; we will see these in later chapters.\nWeb and Chat Text   Web和聊天文本\nAlthough Project Gutenberg contains thousands of books, it represents established literature. It is important to consider less formal language as well. NLTK’s small collection of web text includes content from a Firefox discussion forum, conversations overheard（无意听到的） in New York, the movie script of Pirates of the Carribean（加勒比海盗）, personal advertisements, and wine reviews:\n>>> from nltk.corpus import webtext\n>>> for fileid in webtext.fileids():\n...     print fileid, webtext.raw(fileid)[:65], '...'\n...\nfirefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se...\ngrail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop...\noverheard.txt White guy: So, do you have any plans for this evening? Asian girl...\npirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr...\nsingles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun...\nwine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb...\nThere is also a corpus of instant messaging chat sessions, originally collected by the Naval Postgraduate School for research on automatic detection of Internet predators（捕食者）.The corpus contains over 10,000 posts（帖子）, anonymized by replacing usernames with generic names(通用名) of the form “UserNNN”, and manually edited to remove any other identifying information. The corpus is organized into 15 files, where each file contains several hundred posts collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a generic adults chatroom). The filename contains the date, chat-room, and number of posts; e.g., 10-19-20s_706posts.xml contains 706 posts gathered from the 20s chat room on 10/19/2006.\n>>> from nltk.corpus import nps_chat\n>>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n>>> chatroom[123]\n['i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',','I', 'can', 'look', 'in', 'a', 'mirror','.']\nBrown Corpus  布朗语料库\nThe Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. Table 2-1 gives an example of each genre (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\nID\nFile\nGenre\nDescription\nA16\nca16\nnews\nChicago Tribune: Society Reportage\nB02\ncb02\neditorial\nChristian Science Monitor: Editorials\nC17\ncc17\nreviews\nTime Magazine: Reviews\nD12\ncd12\nreligion\nUnderwood: Probing the Ethics of Realtors\nE36\nce36\nhobbies\nNorling: Renting a Car in Europe\nF25\ncf25\nlore\nBoroff: Jewish Teenage Culture\nG22\ncg22\nbelles_lettres\nReiner: Coping with Runaway Technology\nH15\nch15\ngovernment\nUS Office of Civil and Defence Mobilization: The Family Fallout Shelter\nJ17\ncj19\nlearned\nMosteller: Probability with Statistical Applications\nK04\nck04\nfiction\nW.E.B. Du Bois: Worlds of Color\nL13\ncl13\nmystery\nHitchens: Footsteps in the Night\nM01\ncm01\nscience_fiction\nHeinlein: Stranger in a Strange Land\nN14\ncn15\nadventure\nField: Rattlesnake Ridge\nP12\ncp12\nromance\nCallaghan: A Passion in Rome\nR06\ncr06\nhumor\nThurber: The Future, If Any, of Comedy\nTable 2-1. Example document for each section of the Brown Corpus\nWe can access the corpus as a list of words or a list of sentences (where each sentence is itself just a list of words). We can optionally specify particular categories or files to read:\n>>> from nltk.corpus import brown\n>>> brown.categories()\n['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',\n'science_fiction']\n>>> brown.words(categories='news')\n['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n>>> brown.words(fileids=['cg22'])\n['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]\n>>> brown.sents(categories=['news', 'editorial', 'reviews'])\n[['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]\nThe Brown Corpus is a convenient resource for studying systematic differences between genres, a kind of linguistic inquiry(语言学研究) known as stylistics（文体学）. Let’s compare genres in their usage of modal verbs. The first step is to produce the counts for a particular genre. Remember to import nltk before doing the following:\n>>> from nltk.corpus import brown\n>>> news_text = brown.words(categories='news')\n>>> fdist = nltk.FreqDist([w.lower() for w in news_text])\n>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n>>> for m in modals:\n...     print m + ':', fdist[m],\n...\ncan: 94 could: 87 may: 93 might: 38 must: 53 will: 389\nYour Turn: Choose a different section of the Brown Corpus, and adapt the preceding example to count a selection of wh words, such as what, when, where, who and why.\nNext, we need to obtain counts for each genre of interest. We’ll use NLTK’s support for conditional frequency distributions. These are presented systematically in  Section 2.2, where we also unpick(拆散) the following code line by line. For the moment, you can ignore the details and just concentrate on the output（忽略细节，专注于结果）.\n>>> cfd = nltk.ConditionalFreqDist(\n...           (genre, word)\n...           for genre in brown.categories()\n...           for word in brown.words(categories=genre))\n>>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n>>> cfd.tabulate(conditions=genres, samples=modals)\ncan  could  may might must will\nnews   93   86   66   38   50  389\nreligion   82   59   78   12   54   71\nhobbies  268   58  131   22   83  264\nscience_fiction  16   49    4   12    8   16\nromance   74  193   11   51   45   43\nhumor   16   30    8    8    9   13\nObserve that the most frequent modal in the news genre is will, while the most frequent modal in the romance genre is could. Would you have predicted this? The idea that word counts might distinguish（区分） genres will be taken up（采纳） again in Chapter 6.\nReuters Corpus  路透社语料库\nThe Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and grouped into two sets, called “training” and “test”（训练和测试）; thus, the text with fileid 'test/14826' is a document drawn from the test set. This split（分割） is for training and testing algorithms that automatically detect the topic of a document, as we will see in Chapter 6.\n>>> from nltk.corpus import reuters\n>>> reuters.fileids()\n['test/14826', 'test/14828', 'test/14829', 'test/14832', ...]\n>>> reuters.categories()\n['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',\n'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',\n'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]\nUnlike the Brown Corpus, categories in the Reuters Corpus overlap with each other（相互覆盖，也就是内容有重复）, simply because a news story often covers multiple topics. We can ask for the topics covered by one or more documents, or for the documents included in one or more categories. For convenience, the corpus methods accept a single fileid or a list of fileids.\n>>> reuters.categories('training/9865')\n['barley', 'corn', 'grain', 'wheat']\n>>> reuters.categories(['training/9865', 'training/9880'])\n['barley', 'corn', 'grain', 'money-fx', 'wheat']\n>>> reuters.fileids('barley')\n['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]\n>>> reuters.fileids(['barley', 'corn'])\n['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',\n'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]\nSimilarly, we can specify the words or sentences we want in terms of（按照） files or categories. The first handful（少数） of words in each of these texts are the titles, which by convention（按照惯例） are stored as uppercase.\n>>> reuters.words('training/9865')[:14]\n['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', 'BIDS',\n'DETAILED', 'French', 'operators', 'have', 'requested', 'licences', 'to', 'export']\n>>> reuters.words(['training/9865', 'training/9880'])\n['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n>>> reuters.words(categories='barley')\n['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n>>> reuters.words(categories=['barley', 'corn'])\n['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]\nInaugural Address Corpus  就职演说语料库\nIn Section 1.1, we looked at the Inaugural Address Corpus, but treated it as a single text. The graph in Figure 1-2 used “word offset”（单词位移） as one of the axes; this is the numerical index of the word in the corpus, counting from the first word of the first address. However, the corpus is actually a collection of 55 texts, one for each presidential address. An interesting property of this collection is its time dimension（时间维度，奥巴马的也有）:\n>>> from nltk.corpus import inaugural\n>>> inaugural.fileids()\n['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]\n>>> [fileid[:4] for fileid in inaugural.fileids()]\n['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]\nNotice that the year of each text appears in its filename. To get the year out of the filename, we extracted the first four characters, using fileid[:4].\nLet’s look at how the words America and citizen are used over time. The following code converts the words in the Inaugural corpus to lowercase using w.lower()①, then checks whether they start with either of the “targets” america or citizen using startswith()①. Thus it will count words such as American’s and Citizens. We’ll learn about conditional frequency distributions in Section 2.2; for now, just consider the output, shown in Figure 2-1.\n>>> cfd = nltk.ConditionalFreqDist(\n...           (target, file[:4])\n...           for fileid in inaugural.fileids()\n...           for w in inaugural.words(fileid)\n...           for target in ['america', 'citizen']\n...           if w.lower().startswith(target)) ①\n>>> cfd.plot()\n运行有问题，类型错误\nTraceback (most recent call last):\nFile \"E:/Test/NLTK/2.1.py\", line 6, in <module>\nfor fileid in inaugural.fileids()\nFile \"C:\\Python26\\lib\\site-packages\\nltk\\probability.py\", line 1740, in __init__\nfor (cond, sample) in cond_samples:\nFile \"E:/Test/NLTK/2.1.py\", line 9, in <genexpr>\nif w.lower().startswith(target))\nTypeError: 'type' object is unsubscriptable\nFigure 2-1. Plot of a conditional frequency distribution: All words in the Inaugural Address Corpus that begin with america or citizen are counted; separate counts are kept for each address; these are plotted so that trends in usage over time can be observed; counts are not normalized for document length.\nAnnotated Text Corpora  注释文本语料库\nMany text corpora contain linguistic annotations, representing part-of-speech tags, named entities, syntactic structures（句法结构）, semantic roles（语义角色）, and so forth. NLTK provides convenient ways to access several of these corpora, and has data packages containing corpora and corpus samples, freely downloadable for use in teaching and research. Table 2-2 lists some of the corpora. For information about downloading them, see http://www.nltk.org/data .For more examples of how to access NLTK corpora, please consult the Corpus HOWTO at http://www.nltk.org/howto .\nTable 2-2. Some of the corpora and corpus samples distributed with NLTK\nCorpus\nCompiler\nContents\nBrown Corpus\nFrancis, Kucera\n15 genres, 1.15M words, tagged, categorized\nCESS Treebanks\nCLiC-UB\n1M words, tagged and parsed (Catalan, Spanish)\nChat-80 Data Files\nPereira & Warren\nWorld Geographic Database\nCMU Pronouncing Dictionary\nCMU\n127k entries\nCoNLL 2000 Chunking Data\nCoNLL\n270k words, tagged and chunked\nCoNLL 2002 Named Entity\nCoNLL\n700k words, pos- and named-entity-tagged (Dutch, Spanish)\nCoNLL 2007 Dependency Treebanks (sel)\nCoNLL\n150k words, dependency parsed (Basque, Catalan)\nDependency Treebank\nNarad\nDependency parsed version of Penn Treebank sample\nFloresta Treebank\nDiana Santos et al\n9k sentences, tagged and parsed (Portuguese)\nGazetteer Lists\nVarious\nLists of cities and countries\nGenesis Corpus\nMisc web sources\n6 texts, 200k words, 6 languages\nGutenberg (selections)\nHart, Newby, et al\n18 texts, 2M words\nInaugural Address Corpus\nCSpan\nUS Presidential Inaugural Addresses (1789-present)\nIndian POS-Tagged Corpus\nKumaran et al\n60k words, tagged (Bangla, Hindi, Marathi, Telugu)\nMacMorpho Corpus\nNILC, USP, Brazil\n1M words, tagged (Brazilian Portuguese)\nMovie Reviews\nPang, Lee\n2k movie reviews with sentiment polarity classification\nNames Corpus\nKantrowitz, Ross\n8k male and female names\nNIST 1999 Info Extr (selections)\nGarofolo\n63k words, newswire and named-entity SGML markup\nNPS Chat Corpus\nForsyth, Martell\n10k IM chat posts, POS-tagged and dialogue-act tagged\nPP Attachment Corpus\nRatnaparkhi\n28k prepositional phrases, tagged as noun or verb modifiers\nProposition Bank\nPalmer\n113k propositions, 3300 verb frames\nQuestion Classification\nLi, Roth\n6k questions, categorized\nReuters Corpus\nReuters\n1.3M words, 10k news documents, categorized\nRoget's Thesaurus\nProject Gutenberg\n200k words, formatted text\nRTE Textual Entailment\nDagan et al\n8k sentence pairs, categorized\nSEMCOR\nRus, Mihalcea\n880k words, part-of-speech and sense tagged\nSenseval 2 Corpus\nPedersen\n600k words, part-of-speech and sense tagged\nShakespeare texts (selections)\nBosak\n8 books in XML format\nState of the Union Corpus\nCSPAN\n485k words, formatted text\nStopwords Corpus\nPorter et al\n2,400 stopwords for 11 languages\nSwadesh Corpus\nWiktionary\ncomparative wordlists in 24 languages\nSwitchboard Corpus (selections)\nLDC\n36 phonecalls, transcribed, parsed\nUniv Decl of Human Rights\nUnited Nations\n480k words, 300+ languages\nPenn Treebank (selections)\nLDC\n40k words, tagged and parsed\nTIMIT Corpus (selections)\nNIST/LDC\naudio files and transcripts for 16 speakers\nVerbNet 2.1\nPalmer et al\n5k verbs, hierarchically organized, linked to WordNet\nWordlist Corpus\nOpenOffice.org et al\n960k words and 20k affixes for 8 languages\nWordNet 3.0 (English)\nMiller, Fellbaum\n145k synonym sets\nCorpora in Other Languages  其他语言的语料库\nNLTK comes with corpora for many languages, though in some cases you will need to learn how to manipulate character encodings in Python before using these corpora (see Section 3.3).\n>>> nltk.corpus.cess_esp.words()\n['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', ...]\n>>> nltk.corpus.floresta.words()\n['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]\n>>> nltk.corpus.indian.words('hindi.pos')\n['\\xe0\\xa4\\xaa\\xe0\\xa5\\x82\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4\\xa3',\n'\\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0\\xe0\\xa4\\xa4\\xe0\\xa4\\xbf\\xe0\\xa4\\xac\\xe0\\xa4\n\\x82\\xe0\\xa4\\xa7', ...]\n>>> nltk.corpus.udhr.fileids()\n['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',\n'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',\n'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...]\n>>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]\n[u'Saben', u'umat', u'manungsa', u'lair', u'kanthi', ...]\nThe last of these corpora, udhr, contains the Universal Declaration of Human Rights（国际人权宣言）in over 300 languages. The fileids for this corpus include information about the character encoding used in the file, such as UTF8 or Latin1. Let’s use a conditional frequency distribution to examine the differences in word lengths for a selection of languages included in the udhr corpus. The output is shown in Figure 2-2 (run the program yourself to see a color plot). Note that True and False are Python’s built-in Boolean values.\n>>> from nltk.corpus import udhr\n>>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n>>> cfd = nltk.ConditionalFreqDist(\n...           (lang, len(word))\n...           for lang in languages\n...           for word in udhr.words(lang + '-Latin1'))\n>>> cfd.plot(cumulative=True)\nFigure 2-2. Cumulative word length distributions: Six translations of the Universal Declaration of Human Rights are processed; this graph shows that words having five or fewer letters account for about 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.\nYour Turn: Pick a language of interest in udhr.fileids(), and define a variable raw_text = udhr.raw(Language-Latin1). Now plot a frequency distribution of the letters of the text using nltk.FreqDist(raw_text).plot().\n不知道为什么Chinese_Mandarin-UTF8不能用，留下该问题继续看\nUnfortunately, for many languages, substantial corpora are not yet available. Often there is insufficient（不足的） government or industrial support for developing language resources, and individual efforts are piecemeal（零碎的） and hard to discover or reuse. Some languages have no established writing system, or are endangered. (See Section 2.7 for suggestions on how to locate(查找) language resources.)\nText Corpus Structure 文本语料库结构\nWe have seen a variety of corpus structures so far; these are summarized in Figure 2-3. The simplest kind lacks any structure: it is just a collection of texts. Often, texts are grouped into categories that might correspond to genre, source, author, language, etc. Sometimes these categories overlap, notably（尤其） in the case of topical（时事问题） categories, as a text can be relevant to more than one topic. Occasionally, text collections have temporal structure（时态结构）, news collections being the most common example. NLTK’s corpus readers support efficient access to a variety of corpora, and can be used to work with new corpora. Table 2-3 lists functionality provided by the corpus readers.\nFigure 2-3. Common structures for text corpora: The simplest kind of corpus is a collection of isolated texts with no particular organization; some corpora are structured into categories, such as genre (Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other corpora represent language use over time (Inaugural Address Corpus).4种不同类型的语料库\nTable 2-3. Basic corpus functionality defined in NLTK: More documentation can be found using help(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://www.nltk.org/howto .\nExample\nDescription\nfileids()\nThe files of the corpus\nfileids([categories])\nThe files of the corpus corresponding to these categories\ncategories()\nThe categories of the corpus\ncategories([fileids])\nThe categories of the corpus corresponding to these files\nraw()\nThe raw content of the corpus\nraw(fileids=[f1,f2,f3])\nThe raw content of the specified files\nraw(categories=[c1,c2])\nThe raw content of the specified categories\nwords()\nThe words of the whole corpus\nwords(fileids=[f1,f2,f3])\nThe words of the specified fileids\nwords(categories=[c1,c2])\nThe words of the specified categories\nsents()\nThe sentences of the specified categories\nsents(fileids=[f1,f2,f3])\nThe sentences of the specified fileids\nsents(categories=[c1,c2])\nThe sentences of the specified categories\nabspath(fileid)\nThe location of the given file on disk\nencoding(fileid)\nThe encoding of the file (if known)\nopen(fileid)\nOpen a stream for reading the given corpus file\nroot()\nThe path to the root of locally installed corpus\nreadme()\nThe contents of the README file of the corpus\nWe illustrate the difference between some of the corpus access methods here:\n>>> raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n>>> raw[1:20]     #这个按单个字符算的\n'The Adventures of B'\n>>> words = gutenberg.words(\"burgess-busterbrown.txt\")\n>>> words[1:20]        #这个按单个词和符号数字算的\n['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.',\n'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster',\n'Bear']\n>>> sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n>>> sents[1:20]  #按句子，那么这个I为啥算单独的一句？\n[['I'], ['BUSTER', 'BEAR', 'GOES', 'FISHING'], ['Buster', 'Bear', 'yawned', 'as',\n'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched',\n'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', ...], ...]\nLoading Your Own Corpus  装载你自己的语料库\nIf you have a your own collection of text files that you would like to access using the methods discussed earlier, you can easily load them with the help of NLTK’s Plain textCorpusReader. Check the location of your files on your file system; in the following example, we have taken this to be the directory /usr/share/dict（这是Linux的吧）. Whatever the location, set this to be the value of corpus_root①. The second parameter of the PlaintextCorpusReader initializer②can be a list of fileids, like ['a.txt', 'test/b.txt'], or a pattern that matches all fileids, like '[abc]/.*\\.txt' (see Section 3.4 for information about regular expressions).\n>>> from nltk.corpus import PlaintextCorpusReader\n>>> corpus_root = '/usr/share/dict' ①\n>>> wordlists = PlaintextCorpusReader(corpus_root, '.*') ②\n>>> wordlists.fileids()\n['README', 'connectives', 'propernames', 'web2', 'web2a', 'words']\n>>> wordlists.words('connectives')\n['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]\nAs another example, suppose you have your own local copy of Penn Treebank (release 3), in C:\\corpora. We can use the BracketParseCorpusReader to access this corpus. We specify the corpus_root to be the location of the parsed Wall Street Journal component of the corpus①, and give a file_pattern that matches the files contained within its subfolders② (using forward slashes斜杠).\n>>> from nltk.corpus import BracketParseCorpusReader\n>>> corpus_root = r\"C:\\corpora\\penntreebank\\parsed\\mrg\\wsj\" ①\n>>> file_pattern = r\".*/wsj_.*\\.mrg\"  ②\n>>> ptb = BracketParseCorpusReader(corpus_root, file_pattern)\n>>> ptb.fileids()\n['00/wsj_0001.mrg','00/wsj_0002.mrg', '00/wsj_0003.mrg', '00/wsj_0004.mrg', ...]\n>>> len(ptb.sents())\n49208\n>>> ptb.sents(fileids='20/wsj_2013.mrg')[19]\n['The', '55-year-old', 'Mr.', 'Noriega', 'is', \"n't\", 'as', 'smooth', 'as', 'the',\n'shah', 'of', 'Iran', ',', 'as', 'well-born', 'as', 'Nicaragua', \"'s\", 'Anastasio',\n'Somoza', ',', 'as', 'imperial', 'as', 'Ferdinand', 'Marcos', 'of', 'the', 'Philippines',\n'or', 'as', 'bloody', 'as', 'Haiti', \"'s\", 'Baby', Doc', 'Duvalier', '.']"}
