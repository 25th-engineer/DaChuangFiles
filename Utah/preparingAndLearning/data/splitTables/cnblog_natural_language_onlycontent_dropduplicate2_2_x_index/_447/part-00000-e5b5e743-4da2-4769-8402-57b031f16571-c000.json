{"content2":"发表于 2009年04月25号 由 52nlp\n自然语言处理：最大熵和对数线性模型\nNatural Language Processing: Maximum Entropy and Log-linear Models\n作者：Regina Barzilay（MIT,EECS Department, October 1, 2004)\n译者：我爱自然语言处理（www.52nlp.cn ，2009年4月25日）\n上一讲主要内容回顾（Last time）：\n* 基于转换的标注器（Transformation-based tagger）\n* 基于隐马尔科夫模型的标注器（HMM-based tagger）\n遗留的内容（Leftovers）:\na) 词性分布（POS distribution）\ni. 在Brown语料库中按歧义程度排列的词型数目（The number of word types in Brown corpus by degree of ambiguity）：\n无歧义（Unambiguous）只有1个标记： 35,340\n歧义（Ambiguous） 有2-7个标记： 4,100\n2个标记：3,764\n3个标记：264\n4个标记：61\n5个标记：12\n6个标记：2\n7个标记：1\nb) 无监督的TBL（Unsupervised TBL）\ni. 初始化（Initialization）：允许的词性列表（a list of allowable part of speech tags）\nii. 转换（Transformations）： 在上下文C中将一个单词的标记从χ变为Y (Change the tag of a word from χ to Y in context C, where γ ∈ χ).\n例子（Example）: “From NN VBP to VBP if previous tag is NNS”\niii. 评分标准(Scoring criterion):\n这一讲主要内容（Today）：\n* 最大熵模型(Maximum entropy models)\n* 与对数线性模型的联系(Connection to log-linear models)\n* 优化方法(Optimization methods)\n一般问题描述(The General Problem)：\na) 给定输入域χ（We have some input domain χ）；\nb) 给定标记集γ（We have some label set γ）；\nc) 目标（Goal）：对于任何x ∈ χ 及 y ∈γ学习一个条件概率P(y|x) （learn a conditional probability P(y|x)for any x ∈ χ and y ∈ γ ）。\n一、 词性标注（POS tagging）：\na) 例子：Our/PRP$ enemies/NNS are/VBP innovative/JJ and/CC resourceful/JJ ,/, and/CC so/RB are/VB we/PRP ?/?.\ni. 输入域（Input domain）：χ是可能的“历史”（χ is the set of possible histories）；\nii. 标记集（Label set）：γ是所有可能的标注标记（γ is the set of all possible tags）；\niii. 目标（Goal）：学习一个条件概率P(tag|history)（learn a conditional probability P(tag|history)）。\nb) 表现形式（Representation）：\ni. “历史”是一个4元组(t1,t2,w[1:n],i) （History is a 4-tuples (t1,t2,w[1:n],i)；\nii. t1,t2是前两个标记（t1,t2 are the previous two tags）\niii. w[1:n]是输入句子中的n个单词（w[1:n]are the n words in the input sentence）\niv. i 是将要被标注的单词的位置索引（i is the index of the word being tagged）\nχ是所有可能的“历史”集合（χis the set of all possible histories）\n附：课程及课件pdf下载MIT英文网页地址：\nhttp://people.csail.mit.edu/regina/6881/"}
