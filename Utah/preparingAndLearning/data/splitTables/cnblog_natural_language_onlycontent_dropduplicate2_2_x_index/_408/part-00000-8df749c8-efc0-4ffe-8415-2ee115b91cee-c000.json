{"content2":"前言：\n实习需要用到自然语言处理方面的知识。关于自然语言处理，说实话，打心眼里有抵触。\n我很喜欢AI的，但是我觉得语言的理解，特别是中文的不确定性，使得我个人认为语言理解与处理\n是一件非常棘手的问题，而且不会取得太好的效果。再加上编译原理留下的阴影。使得我虽然\n知道自然语言处理 以后必然 有用，但是还是没有选这门课。这不，还是要自学。\n这本《Python自然语言处理》是自己找到的，电子版的，虽然不知道好坏，但老外的书，我还是\n比较放心的。可惜老外的书就不会以汉语为例子了。我不打算全篇通读，我打算先读些，我认为最重要或最感兴趣的。\n喜欢感兴趣的朋友可以和我一起讨论，有讨论才有进步吗。\n目标：\n1.切词\n2.词性，组词\n3.同义词，反义词，词意理解\n4.数据组织与存放\n大致扫了一下，决定主要看第5章和第7章，其它的以后再说。\n第5章 分类和标注词汇\n词性标注，先看例子。\nimport nltk text1 = ['I','love','you'] text2 = ['Love','is','good'] print nltk.pos_tag(text1) print nltk.pos_tag(text2)\n运行结果如下：\n[('I', 'PRP'), ('love', 'VBP'), ('you', 'PRP')]\n[('Love', 'NNP'), ('is', 'VBZ'), ('good', 'JJ')]\n同一个Love不同的结果。我很像知道这是怎么做到的。\n显然Love一般只可能是动词或名词。那么如何识别什么时候是动词，什么时候是名词呢。\n再看一个例子：\n1 import nltk 2 from nltk.corpus import brown 3 4 #使pos成为某个特殊的dict数据结构，不用深究 5 pos = nltk.defaultdict(lambda: nltk.defaultdict(int)) 6 #获得一个已经标注词性的语料库，每个词以元组 （词，词性）的形式出现 7 brown_news_tagged = brown.tagged_words(categories = 'news', simplify_tags = True) 8 for((w1,t1),(w2,t2)) in nltk.ibigrams(brown_news_tagged): 9 pos[(t1,w2)][t2] += 1 10 #查看当单词right前面单词词性是DET时，right的可能词性 11 print pos[('DET','right')]\n结果如下：\ndefaultdict(<type 'int'>, {'ADV': 3, 'ADJ': 9, 'N': 4})\n难道是基于统计加概率的方法来计算一个词在具体语境中的词性，不会吧。\n终于到重点了，各位是不是一样都很激动呢。\n先从一元标注器1-gram说起，其意思应该是不考虑上下文，只考虑单词本身。\n1.默认标注器\n文中接着使用nltk.DefaultTagger('NN')定义了一个默认标注器，将所有词都标注成NN 即名词。\n并且指出大多数新词都是名词，所以默认标注器可以帮助我们提高语言处理系统的稳定性。\n2. 正则表达式标注器\n例如，以ed结尾的可能是动词过去分词。不过这些好像在中文里没用。\n3.查询标注器\n查询标注器就是事先将一些频繁词出现最多的词性记录下来。文中还指出对于那些没记录的词，可以\n结合默认标注器。随着频繁词的增加，这个标注器的准确率上升很快，但是达到一定量后趋于平衡。\nn-gram标注器：考虑待标记词前面n-1个词的词性。\n同样是记录在已知上下文中，当前词最有可能出现的词性。\n需要使用语料库进行训练。在实践中，当n越大，越容易出现数据稀疏问题。所以n的选取需要对精度和覆盖进行权衡。\nn-gram说白了就是对词性组合规律的一种模型学习，遗憾的是文中对于学习的方法没有细谈。\n或许可以训练得到一颗决策树。"}
