{"content2":"6.6   Maximum Entropy Classifiers 最大熵分类器\nThe Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier. But rather than using probabilities to set the model's parameters, it uses search techniques to find a set of parameters that will maximize the performance of the classifier. In particular, it looks for（寻找） the set of parameters that maximizes the total likelihood（总可能性） of the training corpus, which is defined as:\n(10)\nP(features) = Σx |in| corpus P(label(x)|features(x))\nWhere P(label|features), the probability that an input whose features are features will have class label label, is defined as:\n(11)\nP(label|features) = P(label, features) / Σlabel P(label, features)\nBecause of the potentially complex interactions between the effects of related features, there is no way to directly calculate the model parameters that maximize the likelihood of the training set. Therefore, Maximum Entropy classifiers choose the model parameters using iterative optimization（迭代优化） techniques, which initialize the model's parameters to random values, and then repeatedly refine those parameters to bring them closer to the optimal solution. These iterative optimization techniques guarantee that each refinement of the parameters will bring them closer to the optimal values, but do not necessarily provide a means of determining when those optimal values have been reached. Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques, they can take a long time to learn. This is especially true when the size of the training set, the number of features, and the number of labels are all large.\nNote\nSome iterative optimization techniques are much faster than others. When training Maximum Entropy models, avoid the use of Generalized Iterative Scaling (GIS) or Improved Iterative Scaling (IIS), which are both considerably slower than the Conjugate Gradient (CG) and the BFGS optimization methods.\nThe Maximum Entropy Model 最大熵模型\nThe Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier. Like the naive Bayes model, the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label. The naive Bayes classifier model defines a parameter for each label, specifying its prior probability, and a parameter for each (feature, label) pair, specifying the contribution of individual features towards a label's likelihood.\nIn contrast, the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters. In particular, it is possible to use a single parameter to associate a feature with more than one label; or to associate more than one feature with a given label. This will sometimes allow the model to \"generalize\" over some of the differences between related labels or features.\nEach combination of labels and features that receives its own parameter is called a joint-feature. Note that joint-features are properties of labeled values, whereas (simple) features are properties of unlabeled values.\nNote\nIn literature that describes and discusses Maximum Entropy models, the term \"features\" often refers to joint-features; the term \"contexts\" refers to what we have been calling (simple) features.\nTypically, the joint-features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model. In particular, a joint-feature is defined for each label, corresponding to w[label], and for each combination of (simple) feature and label, corresponding to w[f,label]. Given the joint-features for a Maximum Entropy model, the score assigned to a label for a given input is simply the product of the parameters associated with the joint-features that apply to that input and label:\n(12)\nP(input, label) = Prodjoint-features(input,label) w[joint-feature]\nMaximizing Entropy 熵的最大化\nThe intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint-features, without making any unwarranted assumptions. An example will help to illustrate this principle.\nSuppose we are assigned the task of picking the correct word sense for a given word, from a list of ten possible senses (labeled A-J). At first, we are not told anything more about the word or the senses. There are many probability distributions that we could choose for the ten senses, such as:\nTable 6.1\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(i)\n10%\n10%\n10%\n10%\n10%\n10%\n10%\n10%\n10%\n10%\n(ii)\n5%\n15%\n0%\n30%\n0%\n8%\n12%\n0%\n6%\n24%\n(iii)\n0%\n100%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\nAlthough any of these distributions might be correct, we are likely to choose distribution (i), because without any more information, there is no reason to believe that any word sense is more likely than any other. On the other hand, distributions (ii) and (iii) reflect assumptions that are not supported by what we know.\nOne way to capture this intuition that distribution (i) is more \"fair\" than the other two is to invoke the concept of entropy. In the discussion of decision trees, we described entropy as a measure of how \"disorganized\" a set of labels was. In particular, if a single label dominates then entropy is low, but if the labels are more evenly distributed then entropy is high. In our example, we chose distribution (i) because its label probabilities are evenly distributed — in other words, because its entropy is high. In general, the Maximum Entropy principle states that, among the distributions that are consistent with what we know, we should choose the distribution whose entropy is highest.\nNext, suppose that we are told that sense A appears 55% of the time. Once again, there are many distributions that are consistent with this new piece of information, such as:\nTable 6.2\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(iv)\n55%\n45%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n(v)\n55%\n5%\n5%\n5%\n5%\n5%\n5%\n5%\n5%\n5%\n(vi)\n55%\n3%\n1%\n2%\n9%\n5%\n0%\n25%\n0%\n0%\nBut again, we will likely choose the distribution that makes the fewest unwarranted assumptions — in this case, distribution (v).\nFinally, suppose that we are told that the word \"up\" appears in the nearby context 10% of the time, and that when it does appear in the context there's an 80% chance that sense A or C will be used. In this case, we will have a harder time coming up with an appropriate distribution by hand; however, we can verify that the following distribution looks appropriate:\nTable 6.3\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n(vii)\n+up\n5.1%\n0.25%\n2.9%\n0.25%\n0.25%\n0.25%\n0.25%\n0.25%\n0.25%\n0.25%\n` `\n-up\n49.9%\n4.46%\n4.46%\n4.46%\n4.46%\n4.46%\n4.46%\n4.46%\n4.46%\n4.46%\nIn particular, the distribution is consistent with what we know: if we add up the probabilities in column A, we get 55%; if we add up the probabilities of row 1, we get 10%; and if we add up the boxes for senses A and C in the +up row, we get 8% (or 80% of the +up cases). Furthermore, the remaining probabilities appear to be \"evenly distributed.\"\nThroughout this example, we have restricted ourselves to distributions that are consistent with what we know; among these, we chose the distribution with the highest entropy. This is exactly what the Maximum Entropy classifier does as well. In particular, for each joint-feature, the Maximum Entropy model calculates the \"empirical frequency\" of that feature — i.e., the frequency with which it occurs in the training set. It then searches for the distribution which maximizes entropy, while still predicting the correct frequency for each joint-feature.\nGenerative vs Conditional Classifiers  产生式对比条件式分类器\nAn important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer. The naive Bayes classifier is an example of a generative classifier, which builds a model that predicts P(input, label), the joint probability of a (input, label) pair. As a result, generative models can be used to answer the following questions:\nWhat is the most likely label for a given input?\nHow likely is a given label for a given input?\nWhat is the most likely input value?\nHow likely is a given input value?\nHow likely is a given input value with a given label?\nWhat is the most likely label for an input that might have one of two values (but we don't know which)?\nThe Maximum Entropy classifier, on the other hand, is an example of a conditional classifier. Conditional classifiers build models that predict P(label|input) — the probability of a label given the input value. Thus, conditional models can still be used to answer questions 1 and 2. However, conditional models can not be used to answer the remaining questions 3-6.\nIn general, generative models are strictly more powerful than conditional models, since we can calculate the conditional probability P(label|input) from the joint probability P(input, label), but not vice versa. However, this additional power comes at a price. Because the model is more powerful, it has more \"free parameters\" which need to be learned. However, the size of the training set is fixed. Thus, when using a more powerful model, we end up with less data that can be used to train each parameter's value, making it harder to find the best parameter values. As a result, a generative model may not do as good a job at answering questions 1 and 2 as a conditional model, since the conditional model can focus its efforts on those two questions. However, if we do need answers to questions like 3-6, then we have no choice but to use a generative model.\nThe difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline. Although the topographical map can be used to answer a wider variety of questions, it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline."}
