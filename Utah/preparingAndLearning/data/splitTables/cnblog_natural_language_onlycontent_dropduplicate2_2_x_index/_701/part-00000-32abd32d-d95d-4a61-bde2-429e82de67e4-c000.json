{"content2":"自然语言处理任务数据集\nkeywords: NLP, DataSet, corpus process\n语料处理一般步骤\n以下处理步骤出自[Mikolov T, et al. Exploiting Similarities among Languages for Machine Translation[J]. Computer Science, 2013.]\nTokenization of text using scripts (from www.statmt.org)\nDuplicate sentences were removed\nNumeric values were rewritten as a single token\nspecial characters were removed (such as !?,:)\nAI Challenger - 英中翻译评测\n适用领域：机器翻译\n规模最大的口语领域英中双语对照数据集。提供了超过1000万的英中对照的句子对作为数据集合。所有双语句对经过人工检查，数据集从规模、相关度、质量上都有保障。\n训练集：10,000,000 句\n验证集（同声传译）：934 句\n验证集（文本翻译）：8000 句\nhttps://challenger.ai/datasets/translation\nWMT(Workshop on Machine Translation) - 机器翻译研讨会\n适用领域：机器翻译\nWMT 是机器翻译领域最重要的公开数据集。数据规模较大，取决于不同的语言，通常在百万句到千万句不等。\n2017年WMT的网址 http://www.statmt.org/wmt17/\nUN Parallel Corpus - 联合国平行语料\n适用领域：机器翻译\n联合国平行语料库由已进入公有领域的联合国正式记录和其他会议文件组成。语料库包含1990至2014年编写并经人工翻译的文字内容，包括以语句为单位对齐的文本。\n语料库旨在提供多语种的语言资源，帮助在机器翻译等各种自然语言处理方面开展研究和取得进展。为了方便使用，本语料库还提供现成的特定语种双语文本和六语种平行语料子库。\n介绍：https://conferences.unite.un.org/UNCorpus/zh#introduction\n下载：https://conferences.unite.un.org/UNCorpus/zh/DownloadOverview\n（目前一直下载不下来）\n2nd International Chinese Word Segmentation Bakeoff\n适用领域：中文分词\nThis directory contains the training, test, and gold-standard data\nused in the 2nd International Chinese Word Segmentation Bakeoff.\nhttp://sighan.cs.uchicago.edu/bakeoff2005/\n20 Newsgroups\n适用领域：文本分类\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\nhttp://qwone.com/~jason/20Newsgroups/\nNLPCC 2017 新闻标题分类\n适用领域：文本分类\nhttp://tcci.ccf.org.cn/conference/2017/taskdata.php\nhttps://github.com/FudanNLP/nlpcc2017_news_headline_categorization\nReuters-21578 Text Categorization Collection\n适用领域：文本分类\nThis is a collection of documents that appeared on Reuters newswire in 1987. The documents were assembled and indexed with categories.\nhttp://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n全网新闻数据(SogouCA)\n适用领域：文本分类、事件检测跟踪、新词发现、命名实体识别自动摘要\n来自若干新闻站点2012年6月—7月期间国内，国际，体育，社会，娱乐等18个频道的新闻数据，提供URL和正文信息\nhttp://www.sogou.com/labs/resource/ca.php\n清华大学孙茂松老师组清洗并整理的数据（SogouT），Complete training dataset Clean-SogouT is released in https://pan.baidu.com/s/1kXgkyJ9(password: f2ul).\nref: https://github.com/thunlp/SE-WRL\n搜狐新闻数据（SogouCS）\n适用领域：文本分类、事件检测跟踪、新词发现、命名实体识别、自动摘要\n来自搜狐新闻2012年6月—7月期间国内，国际，体育，社会，娱乐等18个频道的新闻数据，提供URL和正文信息\nhttp://www.sogou.com/labs/resource/cs.php\n评测-文本分类评价（SogouTCE）\n适用领域：文本分类\n评估文本分类结果的正确性。语料来自搜狐等多个新闻网站近20个频道。\nhttp://www.sogou.com/labs/resource/tce.php\nTHUCNews\nTHUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。\nref: http://thuctc.thunlp.org/\nCMU World Wide Knowledge Base (Web->KB) project\n适用领域：知识抽取\nTo develop a probabilistic, symbolic knowledge base that mirrors the content of the world wide web. If successful, this will make text information on the web available in computer-understandable form, enabling much more sophisticated information retrieval and problem solving.\nhttp://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/\nWikidump\n适用领域：word embedding\n中文：https://dumps.wikimedia.org/zhwiki/latest/\nGitHub 项目\n大规模中文自然语言处理语料 Large Scale Chinese Corpus for NLP\nhttps://github.com/brightmart/nlp_chinese_corpus"}
