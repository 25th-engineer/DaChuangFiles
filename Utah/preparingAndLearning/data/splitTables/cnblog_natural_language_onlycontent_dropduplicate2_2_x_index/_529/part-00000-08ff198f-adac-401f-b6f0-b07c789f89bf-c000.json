{"content2":"探讨自然语言处理技术学习与思考\n( 白宁超 2018年8月23日10:46:39 )\n本节导读\n概述\n随着人工智能的快速发展，自然语言处理和机器学习应用愈加广泛。但是对于初学者入门还是有一定难度，对于该领域整体概况不能明晰。本章主要从发展历程、研究现状、应用前景等角度整体介绍自然语言处理和机器学习，让读者对该技术领域有个系统而全面的认识。\n适合人群\n具备一定编程基础的计算机专业、软件工程专业、通信专业、电子技术专业和自动化专业的学生和自然语言处理感兴趣的人群。\n学习前技术储备\n具备编程语言基础\n具备面向对象的编程思想\n快速了解自然语言处理\n什么是自然语言处理\n自然语言\n我们要对自然语言进行理解，其实就是我们日常使用的语言（书面文字和语音视频等）。简言之，汉语、日语、韩语、英语、法语等语言都属于此范畴。而自然语言处理是对自然语言处理的一种技术，就是通过我们的语音文字与计算机进行通信，我们称之为“人机交互”。（自然语言理解、自然语言生成）\n自然语言处理\n自然语言处理（英语：Natural Language Processing，简称NLP）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。\n自然语言处理发展背景和历程\n自然语言处理发展背景\n自然语言处理相关研究，最早是从机器翻译系统的研究开始的。20世纪60年代，国外对机器翻译曾有大规模的研究工作，投入了大量的人力物力财力。但是，受的客观历史因素的限制，当时人们低估了自然语言的复杂性，语言处理的理论和技术均不成热，所以进展并不大。其主要的做法是存储两种语言的单词、短语对应译法的大辞典，翻译时一一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远不是如此简单，很多时候还要参考某句话前后的意思。\n我国机器翻译的起步较晚，是继美国、前苏联、英国之后世界上第四个开展机器翻译研究的国家。早在20世纪50年代机器翻译就被列入我国科学研究的发展规划。一些研究人员还进行了俄汉机器翻译实验，取得了一定的研究成果，但60年代的有关研究很快因“文革”而完全停顿。我国机器翻译研究的全面展开始于80年代中期以后，特别是90年代以来，一批机器翻译系统相继问世，其中影响力较大的有:中软总公司开发的汉英-汉日翻译系统(1993);中科院计算所研制的IMTEC英汉翻译系统(1992)等。\n自然语言处理发展历史\n1948年，香农（Shannon）把离散马尔可夫过程的概率模型应用于描述语言的自动机;同时又把“熵” (entropy)的概念引用到语言处理中。而克莱尼(Kleene)在同一时期研究了有限自动机和正则表达式。\n1956年，乔姆斯基（Chomsky）又提出了上下文无关语法。这些工作导致了基于规则和基于概率两种不同的自然语言处理方法的诞生，使得该领域的研究分成了采用规则方法的符号派和采用概率方法的随机派两大阵营，进而引发了数十年有关这两种方法孰优孰劣的争执 。同年，人工智能诞生以后，自然语言处理迅速融入了人工智能的研究中。随机派学者在这一时期利用贝叶斯方法等统计学原理取得了一定的进步;而以乔姆斯基为代表的符号派也进行了形式语言理论生成句法和形式逻辑系统的研究。由于这一时期， 多数学者注重研究推理和逻辑问题，只有少数学者在研究统计方法和神经网络，所以 ，符号派的势头明显强于随机派的势头。\n1967 年美国心理学家 奈瑟尔(Neisser)提出了认知心理学， 从而把自然语言处理与人类的认知联系起来。\n70年代初，由于自然语言处理研究中的一些问题未能在短时间内得到解决，而新的问题又不断地涌现，许多人因此丧失了信心，自然语言处理的研究进入了低谷时期。尽管如此，一些发达国家的学者依旧地研究着。基于隐马尔可夫模型 (Hidden Markov Model，HMM)的统计方法和话语分析 (Discourse Analysis)在这一时期取得了重大进展 。\n80年代， 在人们对于过去的工作反思之后 ， 有限状态模型和经验主义的研究方法开始复苏 。\n90年代以后，随着计算机的速度和存储量大幅增加，自然语言处理的物质基础大幅改善，语音和语言处理的商品化开发成为可能;同时，网络技术的发展和Internet商业化使得基于自然语言的信息检索和信息抽取的需求变得更加突出。然语言处理的应用面不再局限于机器翻译、语音控制等早期研究领域了。\n从90年代末到21世纪初 ，人们逐渐认识到，仅用基于规则的方法或仅用基于统计的方法都是无法成功进行自然语言处理的。基于统计、基于实例和基于规则的语料库技术在这一时期开始蓬勃发展， 各种处理技术开始融合，自然语言处理的研究又开始兴旺起来。\n思考？\n基于规则的方法和基于统计的方法孰优孰劣？\n自然语言处理涉及的学科领域\n语言学\n计算机科学（提供模型表示、算法设计、计算机实现）\n数学（数学模型）\n心理学（人类言语心理模型和理论）\n哲学（提供人类思维和语言的更深层次理论）\n统计学（提供样本数据的预测统计技术）\n电子工程（信息论基础和语言信号处理技术）\n生物学（人类言语行为机制理论）。\n自然语言处理技术体系\n自然语言处理就业与发展前景\n招聘网站：\n拉勾网: https://www.lagou.com/\nNLPJOB: http://www.nlpjob.com/jobs/nlp/\n发展前景\n自然语言处理的十个发展趋势：https://blog.csdn.net/heyc861221/article/details/80130981\n自然语言处理产业情况：https://www.itjuzi.com/ai#map\n自然语言处理相关工作的前景：http://www.52nlp.cn/job-prospects-for-natural-language-processing\n2017 年中国人工智能产业数据报告：http://www.caict.ac.cn/kxyj/qwfb/qwsj/201804/P020180213603539476032.pdf\n思考：如何学习自然语言处理的问题？\n- 综述了解，整体技术框架掌握 - 侧重方向，多看论文和会议文章 - 知其原理，重在实际应用 - 归纳总结，提高研究效率 - 资料检索，高效学习效率\n自然语言处理跨学科基础介绍\nNLP与数学\n线性数学\n自然语言处理是以计算机科学、统计学、数学和信息论等多个领域交叉的学科。线性代数又是数学的一个重要分支，对自然语言处理有着很直接的影响。诸如：算法建模、参数设置、验证策略、识别欠拟合和过拟合等等。读者往往知道线性代数很有用，常常全书通读。造成时间不足和效率较低。归因于对线性代数在机器学习中的重点和用途不明。本章主要以简明的方式介绍最常用的线性代数知识，并使读者知道线性代数常用于哪些方面。\n概率论\n由于基于规则方法向基于统计方法的转型，概率就显得尤为重要，诸如一些随机事件、独立假设、条件概率、完全概率等等。然后对贝叶斯模型进行案例式介绍，旨在读者深度理解。\nNLP与统计学\n在数据科学中，统计地位尤为显著。其在数据分析的基础上，研究如何测定、收集、整理、归纳和分析反映数据规律，以便给出正确消息的科学。通过揭示数据背后的规律和隐藏信息，给相关角色提供参照价值，做出相应的决策。这在数据挖掘、自然语言处理、机器学习都广泛应用。\n百度EChart：http://echarts.baidu.com/examples/\n地图案例应用场景\n适合的场景\n某年度国家各个省州的人口情况。 分级统计地图较多的是反映呈面状但属分散分布的现象，如反映人口密度、某农作物播种面积的比、人均收入等。\n不适合的场景\n2008 年美国总统大选结果。 民主党候选人奥巴马和共和党候选人麦凯恩胜出的州分别用蓝色和红色表示。这个例子的选举可视化很容易给用户造成简介中提到的错觉：数据分布和地理区域大小的不对称。共和党比民主党获得了更多的投票，因为红色的区域所占的面积更大。但是在美国总统大选中，最后的结果是看候选人获得的选举人票数，每个州拥有的选举人票数是不一样的，在一个州获胜的选举人将得到该州所有的选举人票数。纽约州虽然面积很小，却拥有33张选举人票，而蒙大拿州虽然面积很大，却只有3票。\n统计可视化\n-9 iphone销量地图\nNLP与机器学习\n什么是机器学习\n机器学习就是指“计算机利用经验自动改善系统自身性能的行为”。简言之，机器学习是指通过计算机学习数据中的内在规律性信息，获得新的经验和知识，以提高计算机的智能性，使计算机能够像人那样去决策。\n机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。\n机器学习发展简史\n1943年， Warren McCulloch 和 Walter Pitts 提出了神经网络层次结构模型 ， 确立为神经网络的计算模型理论， 从而为机器学习的发展奠定了基础。\n1950年， “人工智能之父”图灵发提出了著名的“图灵测试”，使人工智能成为了计算机科学领域一个重要的研究课题。\n1957年， 康内尔大学教授 Frank Rosenblatt 提出感知器概念，并且设计出了第一个计算机神经网络，这个机器学习算法成为神经网络模型的开山鼻祖。\n1959年， 美国 IBM 公司的 A． M． Samuel设计了一个具有学习能力的跳棋程序，曾经战胜了美国一个保持 8 年不败的冠军。这个程序向人们初步展示了机器学习的能力。\n1962年， Hubel 和 Wiesel 发现猫脑皮层中独特的神经网络结构可以有效降低学习的复杂性，从而提出著名的 Hubel－Wiesel 生物视觉模型，以后提出的神经网络模型均受此启迪。\n1969 年，人工智能研究的先驱者 Marvin Minsky和 Seymour Papert 出版了对机器学习研究具有深远影响的著作《Perceptron》， 此后的十几年基于神经网络的人工智能研究进入低潮。\n1980 年， 在美国卡内基·梅隆大学举行了第一届机器学习国际研讨会， 标志着机器学习研究在世界范围内兴起。\n1986 年，Rumelhart、Hinton 和 Williams 联合在《自然》杂志发表了著名的反向传播算法(BP) ， 首次阐述了 BP 算法在浅层前向型神经网络模型的应用，从此，神经网络的研究与应用开始复苏。\n1989 年， 美国贝尔实验室学者 Yann LeCun 教授提出了目前最为流行的卷积神经网络( CNN) 计算模型，推导出基于 BP 算法的高效训练方法， 并成功地应用于英文手写体识别。CNN 是第一个被成功训练的人工神经网络，也是后来深度学习最成功、应用最广泛的模型之一。\n90 年代后， 多种浅层机器学习模型相继问世，诸如逻辑回归、支持向量机等.基于统计规律的浅层学习方法比起传统的基于规则的方法具备很多优越性， 取得了不少成功的商业应用的同时， 浅层学习的问题逐渐暴露出来，由于有限的样本和计算单元导致对数据间复杂函数的表示能力有限，学习能力不强，只能提取初级特征。\n2006 年， 在学界及业界巨大需求刺激下， 特别是计算机硬件技术的迅速发展提供了强大的计算能力。机器学习领域的泰斗 Geoffrey Hinton 和 Ruslan Salakhutdinov 发表文章 ，提出了深度学习模型， 主要论点包括:多个隐层的人工神经网络具有良好的特征学习能力;通过逐层初始化来克服训练的难度，实现网络整体调优。这个模型的提出， 开启了深度神经网络机器学习的新时代。\n2012 年， Hinton 研究团队采用深度学习模型赢得计算机视觉领域最具影响力的 ImageNet 比赛冠军，从而标志着深度学习进入第二个阶段。\n至今， 在云计算、大数据、计算机硬件技术发展的支撑下，深度学习近年来在多个领域取得了令人赞叹的进展，推出一批成功的商业应用，诸如谷歌翻译，苹果语音工具 Siri， 微软的 Cortana 个人语音助手，蚂蚁金服的扫脸技术，特别是谷歌 AlphaGo 人机大战获胜的奇迹等， 使机器学习成为计算机科学的一个新的领域。\n自然语言处理和机器学习的联系\n语言是人类区别其他动物的本质特性。在所有生物中，只有人类才具有语言能力。人类的多种智能都与语言有着密切的关系。人类的逻辑思维以语言为形式，人类的绝大部分知识也是以语言文字的形式记载和流传下来的。因而，它也是人工智能（机器学习和深度学习为代表的人工智能）的一个重要，甚至核心部分。\n用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义和理论意义。实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为自然语言理解，后者称为自然语言生成。因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从现有的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标，但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，有些已商品化，甚至开始产业化。典型的例子有：多语种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。\n现代NLP算法是基于机器学习，特别是统计机器学习。机器学习范式是不同于一般之前的尝试语言处理。语言处理任务的实现，通常涉及直接用手的大套规则编码。许多不同类的机器学习算法已应用于自然语言处理任务。\n知识扩展推荐\n【自然语言处理理论与实战】由清华大学教授、博士生导师王道顺；电子科技大学教授、博士生导师周世杰；中国科学院研究员、博士生导师崔喆；百度企业智能平台，大数据高级工程师 潘耀峰；美团点评，大数据研发工程师陆志君；英国哈德斯菲尔德大学，人工智能博士张朝龙联合推荐。\n寄语\n机器学习比较偏底层和理论，机器学习本身不够炫酷，结合了具体的自然语言处理以及数据挖掘的问题才能炫酷。机器学习好像内力一样，是一个武者的基础，而自然语言和数据挖掘的东西都是招式。如果你内功足够深厚，招式对你来说都是小意思。但机器学习同时也要求很高的数学基础，现在如果我们只讲工程实现，有很多开源工具可以使用，你所需要的只是知道这些工具都是干嘛用的就好，很多人对机器学习特别特别特别的狂热，但对矩阵，概率论又有着老子早不想念这门课了，终于过了的思想。我一直觉得，如果你真的矩阵，概率，微积分学的不好，早日勤动手，多编程，对日后找工作很有利！！！一定不要舍本逐末的放弃了程序员最基础的编程功夫。\n<!-- /* RESET =============================================================================*/ html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video { margin: 0; padding: 0; border: 0; } /* BODY =============================================================================*/ body { font-family: 'microsoft yahei' , Helvetica, arial, freesans, clean, sans-serif; font-size: 16px; line-height: 1.8; /*color: #333;*/ color: #3f3f3f; background-color: #fff; padding: 20px; max-width: 92%; margin: 0 auto; word-break: break-word!important; word-break: break-all; } body>*:first-child { margin-top: 0 !important; } body>*:last-child { margin-bottom: 0 !important; } /* BLOCKS =============================================================================*/ p, blockquote, ul, ol, dl, table, pre { margin: 15px 0; } /* HEADERS =============================================================================*/ h1, h2, h3, h4, h5, h6 { font-family: 'PingFang SC','Microsoft YaHei',SimHei,Arial,SimSun; margin: 20px 0 10px; padding: 0; font-weight: bold; -webkit-font-smoothing: antialiased; } h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code { font-size: inherit; } h1 { font-size: 36px; color: #000; } h2 { font-size: 32px; border-bottom: 2px solid #3F3F3F; color: #000; font-weight: bold; -webkit-font-smoothing: antialiased; } h3 { font-size: 28px; font-weight: bold; -webkit-font-smoothing: antialiased; } h4 { font-size: 24px; } h5 { font-size: 20px; } h6 { color: #777; font-size: 16px; } body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child { margin-top: 0; padding-top: 0; } a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 { margin-top: 0; padding-top: 0; } h1+p, h2+p, h3+p, h4+p, h5+p, h6+p { margin-top: 10px; } /* LISTS =============================================================================*/ ul, ol { padding-left: 30px; } ul li > :first-child, ol li > :first-child, ul li ul:first-of-type, ol li ol:first-of-type, ul li ol:first-of-type, ol li ul:first-of-type { margin-top: 0px; } ul ul, ul ol, ol ol, ol ul { margin-bottom: 0; } dl { padding: 0; } dl dt { font-size: 14px; font-weight: bold; font-style: italic; padding: 0; margin: 15px 0 5px; } dl dt:first-child { padding: 0; } dl dt>:first-child { margin-top: 0px; } dl dt>:last-child { margin-bottom: 0px; } dl dd { margin: 0 0 15px; padding: 0 15px; } dl dd>:first-child { margin-top: 0px; } dl dd>:last-child { margin-bottom: 0px; } /* CODE =============================================================================*/ p code { color: #b52a1d; } pre, code, tt { /*font-size: 13px;*/ font-family: Consolas, \"Liberation Mono\", Courier, monospace; } code, tt { margin: 0 2px; padding: 0px 8px; white-space: nowrap; border: 1px solid #eaeaea; background-color: #f8f8f8; border-radius: 3px; } pre>code { margin: 0; padding: 0; white-space: pre; border: none; background: transparent; white-space: pre-wrap; /* css-3 */ white-space: -moz-pre-wrap; /* mozilla, since 1999 */ white-space: -pre-wrap; /* opera 4-6 */ white-space: -o-pre-wrap; /* opera 7 */ word-wrap: break-word; /* internet explorer 5.5+ */ overflow: auto; word-break: break-all; word-wrap: break-word; } pre { background-color: #F9F9F9; border: 1px solid #ccc; font-size: 13px; line-height: 19px; overflow: auto; padding: 6px 10px; border-radius: 3px; } pre code, pre tt { background-color: transparent; border: none; } kbd { -moz-border-bottom-colors: none; -moz-border-left-colors: none; -moz-border-right-colors: none; -moz-border-top-colors: none; background-color: #DDDDDD; background-image: linear-gradient(#F1F1F1, #DDDDDD); background-repeat: repeat-x; border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD; border-image: none; border-radius: 2px 2px 2px 2px; border-style: solid; border-width: 1px; font-family: \"Helvetica Neue\",Helvetica,Arial,sans-serif; line-height: 10px; padding: 1px 4px; } /* QUOTES =============================================================================*/ blockquote { padding: 15px 20px; border-left: 10px solid #F1F1F1; background-color: #F9F9F9; border-radius: 0 5px 5px 0; } blockquote>:first-child { margin-top: 0px; } blockquote>:last-child { margin-bottom: 0px; } /* HORIZONTAL RULES =============================================================================*/ hr { clear: both; margin: 15px 0; height: 0px; overflow: hidden; border: none; background: transparent; border-bottom: 4px solid #ddd; padding: 0; } /* TABLES =============================================================================*/ table { font-family: Helvetica, arial, freesans, clean, sans-serif; padding: 0; border-collapse: collapse; border-spacing: 0; font-size: 1em; font: inherit; border: 0; } tbody { margin: 0; padding: 0; border: 0; } table tr { border: 0; border-top: 1px solid #CCC; background-color: white; margin: 0; padding: 0; } table tr:nth-child(2n) { background-color: #F8F8F8; } table tr th, table tr td { font-size: 1em; border: 1px solid #CCC; margin: 0; padding: 0.5em 1em; } table tr th { font-weight: bold; background-color: #F0F0F0; } /* IMAGES =============================================================================*/ img { max-width: 1000px; } strong ,b { padding: 0 4px; } -->"}
