{"content2":"åŒæ­¥ç¬”è€…CSDNåšå®¢ï¼ˆhttps://blog.csdn.net/qq_37608890/article/details/81513882ï¼‰ã€‚\nä¸€ã€æ¦‚è¿°\næœ¬æ–‡å°†è¦è®¨è®ºNLPçš„ä¸€ä¸ªé‡è¦è¯é¢˜ï¼šWord2Vecï¼Œå®ƒæ˜¯ä¸€ç§å­¦ä¹ è¯åµŒå…¥æˆ–åˆ†å¸ƒå¼æ•°å­—ç‰¹å¾è¡¨ç¤ºï¼ˆå³å‘é‡ï¼‰çš„æŠ€æœ¯ã€‚å…¶å®ï¼Œåœ¨å¼€å±•è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ—¶ï¼Œä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„åŸºç¡€å·¥ä½œå°±æ˜¯æœ‰å…³è¯è¡¨ç¤ºå±‚é¢çš„å­¦ä¹ ï¼Œå› ä¸ºè‰¯å¥½çš„ç‰¹å¾è¡¨ç¤ºæ‰€å¯¹åº”çš„è¯ï¼Œèƒ½å¤Ÿä½¿å¾—ä¸Šä¸‹åˆè¯­ä¹‰å†…å®¹å¾—ä»¥å¾ˆå¥½åœ°ä¿ç•™å’Œæ•´ä½“ä¸²èµ·æ¥ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œåœ¨ç‰¹å¾è¡¨ç¤ºå±‚é¢ï¼Œå•è¯â€œforestâ€å’Œå•è¯â€œovenâ€æ˜¯ä¸åŒçš„ï¼Œä¹Ÿå¾ˆå°‘åœ¨ç±»ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­å‡ºç°ï¼Œè€Œå•è¯â€œforestâ€å’Œå•è¯â€œjungleâ€åº”è¯¥æ˜¯å¾ˆç›¸ä¼¼çš„ã€‚\nWord2Vecä¹Ÿè¢«ç§°ä¸ºåˆ†å¸ƒå¼è¡¨ç¤ºï¼Œå› ä¸ºå•è¯çš„è¯­ä¹‰æ˜¯ç”±å…¨è¡¨ç¤ºå‘é‡çš„æ¿€æ´»æ¨¡å¼è·å¾—çš„ï¼Œä¸å•ä¸ªå…ƒç´ çš„è¡¨ç¤ºå‘é‡å°±å½¢æˆäº†å¯¹æ¯”ã€‚\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼šä¸€æ­¥æ­¥åœ°æä¾›ä»ä¼ ç»Ÿæ–¹æ³•åˆ°ä»¥ç°ä»£ç¥ç»ç½‘ç»œä¸ºåŸºç¡€çš„å…ˆè¿›æ–¹æ³•ï¼Œé’ˆå¯¹è¯è¡¨ç¤ºåšè¿›ä¸€æ­¥è¯´æ˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨T-SNEï¼ˆä¸€ç§é«˜ç»´æ•°æ®çš„å¯è§†åŒ–æŠ€æœ¯ï¼‰å°†å•è¯æ•°æ®é›†è¿›è¡Œå¯è§†åŒ–ï¼Œå¦‚.1æ‰€å±•ç¤ºçš„2Dæ ·å¼ã€‚æ˜¾ç„¶ï¼Œç›¸ä¼¼çš„è¯ä¼šç¦»çš„å¾ˆè¿‘ã€‚è¿™é‡Œåˆæ­¥ç»™å¤§å®¶ä¸€ä¸ªå…³äºWord2Vecçš„ç›´è§‚è®¤è¯†ã€‚\n-1 ä¸€ä¸ªåˆ©ç”¨t-SNEè¿›è¡Œå­¦ä¹ è¯åµŒå…¥å¯è§†åŒ–ä¾‹å­\nå…³äºt-SNEï¼ˆä¹Ÿå°±æ˜¯t-Distributed Stochastic Neighbor Embedding)çš„å†…å®¹ï¼Œå¯ä»¥å‚è€ƒå¦å¤–ä¸€ç¯‡æ–‡ç« ã€Šä»SNEåˆ°t-SNEå†åˆ°LargeVisã€‹ï¼Œé‡Œé¢ä½œè€…å†™æŒºå¥½çš„ï¼Œæ„Ÿå…´è¶£çš„ç½‘å‹å¯ä»¥è¯¦ç»†çœ‹ä¸‹ã€‚\näºŒã€å…³äºè¯çš„è¡¨ç¤ºï¼ˆWord Representationï¼‰æˆ–è¯ä¹‰ï¼ˆword meaningï¼‰çš„ç†è§£\nå•çº¯ä»è¯­è¨€å­¦çš„è§’åº¦æ¥è®²ï¼Œè¯ä¹‰åº”è¯¥æ˜¯ä¸ªåå“²å­¦æ–¹é¢çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œä½†è¿™é‡Œä¼šç»™å‡ºä¸€ä¸ªæ¯”è¾ƒæ¸©å’Œçš„é˜è¿°ï¼šè¯ä¹‰æ˜¯ä¸€ä¸ªè¯ä¼ è¾¾å‡ºçš„æ€æƒ³æˆ–è€…è¡¨è¾¾æ–¹å¼ã€‚å› ä¸ºNLPçš„ä¸»è¦ç›®æ ‡æ˜¯èƒ½å¤Ÿè®©æœºå™¨åœ¨è¯­è¨€é¢†åŸŸå®ç°äººç±»ç›¸ä¼¼çš„åŠŸèƒ½ï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶æœºå™¨çš„è¯è¡¨ç¤ºè§„åˆ™æ˜¯å¾ˆæ˜æ™ºçš„ã€‚è€Œåœ¨è®¡ç®—æœºé¢†åŸŸï¼Œä»»ä½•ä¿¡æ¯çš„å­˜å‚¨æ–¹å¼éƒ½æ˜¯æ•°å­—åŒ–æˆ–è€…è¯´0å’Œ1äºŒè¿›åˆ¶åŒ–çš„ï¼Œæˆ‘ä»¬ä¼šç»™æ‰€æœ‰çš„å­—ç¬¦ï¼ˆåŒ…æ‹¬å­—æ¯å­—ç¬¦ï¼Œæ±‰å­—ã€è‹±è¯­å•è¯ç­‰è¯­è¨€æ–‡å­—ï¼‰ä¸€ä¸ªç¼–ç æ–¹å¼ã€‚ä¸ºäº†å®ç°æˆ‘ä»¬çš„ç›®æ ‡ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨å¯ä»¥åˆ†ææŒ‡å®šæ–‡æœ¬è¯­æ–™åº“çš„ç®—æ³•ï¼Œå°†ä¼šäº§ç”Ÿå‡ºè‰¯å¥½çš„æ•°å­—åŒ–çš„è¯è¡¨ç¤ºï¼ˆå³å•è¯åµŒå…¥ï¼‰ï¼Œä½¿å¾—å…¶è½åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„å•è¯ï¼ˆæ¯”å¦‚ï¼Œâ€™ä¸€ä¸ªâ€˜å’Œâ€™ä¸¤ä¸ªâ€˜ã€â€™çŒ«â€˜å’Œâ€™ç‹—â€˜ï¼‰å°†å…·æœ‰ç›¸ä¼¼çš„æ•°å€¼è¡¨ç¤ºï¼Œè€Œä¸ç›¸å¹²çš„è¯ï¼ˆæ¯”å¦‚ï¼Œâ€™èŠ±å²—çŸ³â€˜å’Œâ€™çº¢ç¯â€˜ï¼‰åˆ™å¯¹åº”æ•°å€¼å·®å¼‚è¾ƒå¤§ã€‚\nä¸‰ã€ä¼ ç»Ÿæ–¹æ³•\nä¼ ç»Ÿæ–¹æ³•ä¸»è¦å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šä½¿ç”¨å¤–éƒ¨èµ„æºæ¥è¿›è¡Œè¯è¡¨ç¤ºçš„æ–¹æ³•å’Œä¸ä½¿ç”¨å¤–éƒ¨èµ„æºæ¥è¿›è¡Œè¯è¡¨ç¤ºçš„æ–¹æ³•ã€‚å‰è€…ä¸­æœ€ä¸ºå…¸å‹çš„å°±æ˜¯å•è¯ç½‘ç»œï¼ˆWordNetï¼‰æ–¹æ³•ã€‚åè€…æ¯”è¾ƒå¸¸è§çš„æœ‰ç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodingï¼‰ã€è¯é¢‘-é€†æ–‡æœ¬é¢‘ç‡ï¼ˆTF-IDFï¼‰ã€‚\n1ã€å•è¯ç½‘ç»œï¼ˆWordNetï¼‰\n1.1 ç†è®ºé˜è¿°\nWordNetæ˜¯ä¸€ä¸ªè¯æ±‡æ•°æ®åº“ï¼Œç”±ç¾å›½æ™®æ—æ–¯é¡¿å¤§å­¦å¿ƒç†å­¦ç³»é¦–åˆ›ï¼Œç›®å‰ç”±è®¡ç®—ç§‘å­¦ç³»ä¸»æŒã€‚å®ƒæ˜¯ç»ç”±å¿ƒç†å­¦å®¶ã€è¯­è¨€å­¦å®¶å’Œè®¡ç®—æœºå·¥ç¨‹å¸ˆè”åˆè®¾è®¡çš„ä¸€ç§åŸºäºè®¤çŸ¥è¯­è¨€å­¦çš„è‹±æ–‡è¯å…¸ï¼Œå¯¹åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯å’Œå‰¯è¯è¿›è¡Œç¼–ç å¹¶ç»™å‡ºè¯ä¹‹é—´çš„è¯æ€§æ ‡ç­¾å…³ç³»ã€‚\nåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯å’Œå‰¯è¯å„è‡ªè¢«ç»„ç»‡æˆä¸€ä¸ªåŒä¹‰è¯çš„ç½‘ç»œï¼Œæ¯ä¸ªåŒä¹‰è¯é›†åˆéƒ½ä»£ è¡¨ä¸€ä¸ªåŸºæœ¬çš„è¯­ä¹‰æ¦‚å¿µï¼Œå¹¶ä¸”è¿™äº›é›†åˆä¹‹é—´ä¹Ÿç”±å„ç§å…³ç³»è¿æ¥ï¼ˆä¸€ä¸ªå¤šä¹‰è¯å°†å‡ºç°åœ¨å®ƒçš„æ¯ä¸ªæ„æ€çš„åŒä¹‰è¯é›†åˆä¸­ï¼‰ã€‚WordNetä¾èµ–äºä¸€ä¸ªå¤–éƒ¨è¯æ±‡çŸ¥è¯†åº“ï¼Œå¯¹ç»™å®šå•è¯çš„å®šä¹‰ã€åŒä¹‰è¯ã€è¯æ ¹ã€è¡ç”Ÿè¯ç­‰ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚è‹±æ–‡WordNetç›®å‰æ‰¿è½½è¶…è¿‡150000ä¸ªå•è¯å’Œ100000å¤šä¸ªåŒä¹‰è¯ç»„ï¼ˆå³åŒæ­¥é›†ï¼‰ã€‚ç›®å‰WordNetä¸ä»…ä»…å±€é™äºè‹±è¯­ï¼Œåç»­è¯ç”Ÿçš„WordNetè‡ªæˆç«‹ä»¥æ¥å°±å¯ä»¥åœ¨HTTP://GualalWordNET.Org/WordNET-Wordä¸­æŸ¥çœ‹ã€‚\nåœ¨WordNetä¸­ï¼Œè¯è¡¨ç¤ºæ˜¯åˆ†å±‚å»ºæ¨¡çš„ï¼Œåœ¨ç»™å®šçš„åŒä¹‰è¯é›†åˆå’Œå¦ä¸€ä¸ªåŒä¹‰è¯é›†åˆä¹‹é—´çš„å…³è”ä¸­å½¢æˆå¤æ‚çš„å›¾å½¢ã€‚è¿™äº›å…³è”å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„ç±»åˆ«ï¼šä¸€ä¸ªIS-Aå…³ç³»å’Œä¸€ä¸ªIS-Made-ofå…³ç³»ã€‚\nå¯¹äºç»™å®šçš„é›†åˆï¼Œå­˜åœ¨ä¸¤ç±»å…³ç³»ï¼šä¸Šä½è¯(Hypernymï¼Œä¹Ÿå¯ç§°ä¸Šä¹‰ç¥ ï¼‰å’Œä¸‹ä½è¯ï¼ˆHyponymï¼Œä¹Ÿå¯ç§°ä¸‹ä¹‰è¯ï¼‰ã€‚å…³äºä¸Šä½è¯çš„æ¦‚å¿µï¼Œç¬”è€…ä¹ŸæŸ¥é˜…äº†ä¸€äº›è§£é‡Šï¼Œç¬”è€…ç†è§£åº”è¯¥æ˜¯æŒ‡æ³›æŒ‡æŸä¸ªä¸»é¢˜çš„ä¸€èˆ¬æ„ä¹‰ä¸Šçš„è¯è¡¨ç¤ºï¼Œè¯»è€…å¯ä»¥ç™¾åº¦ä¸‹ä¹Ÿæˆ–è€…å‚è€ƒä¸‹ç®€ä¹¦ä¸Šçš„ä¸€ç¯‡ã€Šä»ã€Œç¾Šå¹´çš„ç¾Šåˆ°åº•æ˜¯å“ªç§ç¾Šï¼Ÿã€è°ˆä¸Šä½è¯ã€‹ã€‚ä¸¾ä¾‹è¯´æ˜ï¼Œè½¦è¾†æ˜¯æ±½è½¦çš„ä¸Šä½è¯ã€‚æ¥ä¸‹æ¥ï¼Œä¸ä¸Šä½è¯æˆå¯¹å‡ºç°çš„å°±æ˜¯ä¸‹ä½è¯ã€‚ä¾‹å¦‚ï¼Œä¸°ç”°æ±½è½¦æ˜¯æ±½è½¦çš„ä¸‹ä½è¯ã€‚å¦å¤–ï¼Œè¿˜æœ‰æ•´ä½“è¯ï¼ˆHolonymï¼‰ã€éƒ¨åˆ†è¯ï¼ˆMeronymï¼‰ã€‚ä¸‹é¢ç»™å‡ºHypernymsã€Hyponymã€Holonymã€Meronymçš„å…³ç³»å›¾ï¼Œå¦‚-2æ‰€ç¤ºï¼š\n-2 åŒä¹‰è¯é›†å„ç±»å…³ç³»å›¾\n1.2Â  åˆ©ç”¨NLTKä¸­çš„WordNetè¿›ä¸€æ­¥åˆ†æ\né¦–å…ˆï¼Œä¸‹è½½WordNetå’Œwordnet corpusï¼Œå¦‚ä¸‹\nimport nltk nltk.download('wordnet') from nltk.corpus import wordnet as wn\nå…¶æ¬¡ï¼ŒæŸ¥çœ‹åŒä¹‰è¯é›†å„ç±»å…³ç³»\nè¾“å…¥\n# shows all the available synsets word = 'car' car_syns = wn.synsets(word) print('All the available Synsets for ',word) print('\\t',car_syns,'\\n') # The definition of the first two synsets syns_defs = [car_syns[i].definition() for i in range(len(car_syns))] print('Example definitions of available Synsets ...') for i in range(3): print('\\t',car_syns[i].name(),': ',syns_defs[i]) print('\\n') # Get the lemmas for the first Synset print('Example lemmas for the Synset ',car_syns[i].name()) car_lemmas = car_syns[0].lemmas()[:3] print('\\t',[lemma.name() for lemma in car_lemmas],'\\n') # Let us get hypernyms for a Synset (general superclass) syn = car_syns[0] print('Hypernyms of the Synset ',syn.name()) print('\\t',syn.hypernyms()[0].name(),'\\n') # Let us get hyponyms for a Synset (specific subclass) syn = car_syns[0] print('Hyponyms of the Synset ',syn.name()) print('\\t',[hypo.name() for hypo in syn.hyponyms()[:3]],'\\n') # Let us get part-holonyms for a Synset (specific subclass) # also there is another holonym category called \"substance-holonyms\" syn = car_syns[2] print('Holonyms (Part) of the Synset ',syn.name()) print('\\t',[holo.name() for holo in syn.part_holonyms()],'\\n') # Let us get meronyms for a Synset (specific subclass) # also there is another meronym category called \"substance-meronyms\" syn = car_syns[0] print('Meronyms (Part) of the Synset ',syn.name()) print('\\t',[mero.name() for mero in syn.part_meronyms()[:3]],'\\n')\nè¾“å‡º\nAll the available Synsets for car [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')] Example definitions of available Synsets ... car.n.01 : a motor vehicle with four wheels; usually propelled by an internal combustion engine car.n.02 : a wheeled vehicle adapted to the rails of railroad car.n.03 : the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant Example lemmas for the Synset car.n.03 ['car', 'auto', 'automobile'] Hypernyms of the Synset car.n.01 motor_vehicle.n.01 Hyponyms of the Synset car.n.01 ['ambulance.n.01', 'beach_wagon.n.01', 'bus.n.04'] Holonyms (Part) of the Synset car.n.03 ['airship.n.01'] Meronyms (Part) of the Synset car.n.01 ['accelerator.n.01', 'air_bag.n.01', 'auto_accessory.n.01']\næœ€åï¼Œçœ‹çœ‹è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦æƒ…å†µ\nword1, word2, word3 = 'car','lorry','tree' w1_syns, w2_syns, w3_syns = wn.synsets(word1), wn.synsets(word2), wn.synsets(word3) print('Word Similarity (%s)<->(%s): '%(word1,word2),wn.wup_similarity(w1_syns[0], w2_syns[0])) print('Word Similarity (%s)<->(%s): '%(word1,word3),wn.wup_similarity(w1_syns[0], w3_syns[0]))\nWord Similarity (car)<->(lorry): 0.6956521739130435 Word Similarity (car)<->(tree): 0.38095238095238093\nå¾ˆæ˜¾ç„¶ï¼Œå•è¯â€˜carâ€™å’Œå•è¯â€˜lorryâ€™ä¹‹é—´çš„ç›¸ä¼¼åº¦è¦é«˜äºå•è¯â€˜carâ€™å’Œå•è¯â€™treeâ€˜ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç»“æœåˆç†ã€‚\n1.3Â  WordNetå­˜åœ¨çš„ä¸è¶³\né¦–å…ˆï¼Œç»†å¾®å·®åˆ«ä½“ç°ä¸è¶³ã€‚è¿™æ˜¯WordNetçš„ä¸€ä¸ªé‡è¦ä¸è¶³ã€‚ä½†è¿™æ–¹é¢çš„ä¸è¶³æœ‰ç†è®ºå’Œå®é™…ä¸¤æ–¹é¢çš„åŸå› ã€‚ä»ç†è®ºçš„è§’åº¦æ¥çœ‹ï¼Œå¯¹ä¸¤ä¸ªå®ä½“ä¹‹é—´ç»†å¾®å·®åˆ«çš„å®šä¹‰è¿›è¡Œå»ºæ¨¡æ˜¯éš¾ä»¥å®ç°çš„ã€‚å®é™…ä¸Šï¼Œå®šä¹‰ç»†å¾®å·®åˆ«å¤šæ•°æ˜¯ä¸»è§‚çš„ã€‚ä¾‹å¦‚ï¼Œâ€œæƒ³è¦â€å’Œâ€œéœ€è¦â€ä¸¤è¯æœ‰ç›¸ä¼¼çš„å«ä¹‰ï¼Œä½†å…¶ä¸­â€œéœ€è¦â€æ›´åŠ è‚¯å®šã€æ›´åŠ åšå®šï¼Œè¿™æ˜¯ä¸€ç§ç»†å¾®å·®åˆ«ã€‚\nå…¶æ¬¡ï¼ŒWordNetæœ¬èº«æ˜¯ä¸»è§‚çš„ã€‚å› ä¸ºWordNetæ˜¯ç”±ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ç¤¾åŒºè®¾è®¡çš„ã€‚å› æ­¤ï¼Œå–å†³äºæ‚¨è¯•å›¾è§£å†³çš„é—®é¢˜ï¼ŒWordNetå¯èƒ½æ˜¯åˆé€‚çš„ï¼Œä¹Ÿå¯èƒ½æœ‰ä¸€ä¸ªæ›´å¥½çš„æ–¹å¼å»è¿›è¡Œè¯å®šä¹‰ã€‚\nWordNetç»´æŠ¤æˆæœ¬é«˜ã€‚è¿™æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œç»´æŠ¤å’Œæ·»åŠ æ–°çš„åˆæˆã€å®šä¹‰ã€å¼•ç†ç­‰ï¼Œå¯èƒ½éå¸¸æ˜‚è´µã€‚\né‡æ–°å¼€å‘å…¶ä»–è¯­è¨€çš„WordNetæˆæœ¬æ˜‚è´µã€‚å½“ç„¶ï¼Œä¹Ÿæœ‰ä¸€äº›åŠªåŠ›æ¥æ„å»ºå…¶ä»–è¯­è¨€çš„WordNetï¼Œå¹¶å°†å…¶ä¸è‹±æ–‡WordNetä¸€èµ·ä»¥MultiWordNetï¼ˆMWNï¼‰å½¢å¼å‡ºç°ï¼Œä½†è¿˜ä¸å®Œæ•´ã€‚\n2ã€ç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodedï¼‰\nå…¶å®ï¼Œè¯è¡¨ç¤ºçš„å¦ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•å°±æ˜¯one-hot-encodedï¼Œç›¸å…³ä»‹ç»å·²ç»å¾ˆå¤šï¼Œè¿™é‡Œå°±ä¸¾ä¸ªä¾‹å­æ¥è¯´æ˜ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€Vå­—å‹è¯æ±‡è¡¨ï¼Œå¯¹äºæ¯ä¸€ä¸ªiå¯¹åº”çš„å•è¯w iï¼ˆiä¸ºwçš„ä¸‹æ ‡ï¼‰ï¼Œæˆ‘ä»¬å°†ç”¨ä¸€ä¸ªVé•¿å‘é‡ï¼ˆ0, 0, 0ï¼Œâ€¦ï¼Œ0, 1, 0ï¼Œâ€¦ï¼Œ0, 0, 0ï¼‰è¡¨ç¤ºå•è¯w iï¼Œè¿™é‡Œçš„ç¬¬iä¸ªå…ƒç´ æ˜¯1ï¼Œå…¶ä»–å…ƒç´ å…¨éƒ¨æ˜¯0ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹é¢ä¸€å¥è¯ä¸ºä¾‹ï¼Œ\nBob and Mary are good friends.\nçƒ­ç¼–ç æ¯ä¸ªå•è¯çš„è¡¨ç¤ºå¯èƒ½æ˜¯è¿™æ ·çš„ï¼š\nBob: [1,0,0,0,0,0]\nand: [0,1,0,0,0,0]\nMary: [0,0,1,0,0,0]\nare: [0,0,0,1,0,0]\ngood: [0,0,0,0,1,0]\nfriends: [0,0,0,0,0,1]\nåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå½“ä½ å¤„ç†æ–‡æœ¬ä¸­çš„å•è¯æ—¶ï¼Œæ¯ä¸€ä¸ªå•è¯å¯èƒ½éœ€è¦ä»æˆç™¾ä¸Šåƒä¸ªç±»ç§è¿›è¡Œé¢„æµ‹å‡ºä¸€ä¸ªå±äºå®ƒçš„ã€‚è€Œç°åœ¨åªæœ‰ä¸€ä¸ªå…ƒç´ è®¾ç½®ä¸º1å…¶ä½™å…¨éƒ¨ä¸º0ï¼Œæ˜¾ç„¶ï¼Œå°è¯•å¯¹è¿™äº›å•è¯è¿›è¡Œçƒ­ç¼–ç æ˜¯éå¸¸ä½æ•ˆçš„ã€‚å¦‚-3æ‰€ç¤ºï¼Œè¿›å…¥ç¬¬ä¸€ä¸ªéšè—å±‚çš„çŸ©é˜µä¹˜æ³•å¾—åˆ°çš„å€¼å‡ ä¹å…¨éƒ¨ä¸ºé›¶ï¼Œæ˜¾ç„¶ï¼Œè¿™æ˜¯ä¸€ç§å·¨å¤§çš„è®¡ç®—æµªè´¹ã€‚\n-3\nå¦å¤–ï¼Œè¿™ç§è¡¨ç¤ºä¸ä»¥ä»»ä½•æ–¹å¼å¯¹å•è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§è¿›è¡Œç¼–ç ï¼Œä¸”å®Œå…¨å¿½ç•¥äº†ä½¿ç”¨è¯è¯­æ‰€åœ¨çš„ä¸Šä¸‹æ–‡è¯­å¢ƒã€‚è€ƒè™‘åˆ°è¯å‘é‡ä¹‹é—´çš„ç‚¹ç§¯æ˜¯ç›¸ä¼¼æ€§çš„åº¦é‡ï¼šä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼ï¼Œä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯è¶Šé«˜ï¼›è¿™ç§æƒ…å†µä¸‹ï¼Œè¯â€™automobileâ€˜å’Œâ€™carâ€˜ä¹‹é—´çš„ç›¸ä¼¼åº¦å€¼ä¸º0ï¼Œâ€˜carâ€™å’Œâ€˜hillâ€™ä¹‹é—´çš„ç›¸ä¼¼åº¦ä¹Ÿæ˜¯0ï¼Œæ˜¾ç„¶æ— æ³•åŒºåˆ†å‡ºä¸¤ç»„å•è¯é—´ç›¸ä¼¼åº¦çš„å·®å¼‚ã€‚\næ‰€ä»¥ï¼Œè¿™ç§æ–¹æ³•å¯¹äºå¤§å‹è¯æ±‡è¡¨æ¥è¯´æ˜¯éå¸¸æ— æ•ˆçš„ã€‚å¯¹äºä¸€ä¸ªå…¸å‹çš„NLPä»»åŠ¡ï¼Œè¯æ±‡å¾ˆå®¹æ˜“è¶…è¿‡50000ä¸ªå•è¯ï¼Œå› æ­¤ï¼Œ50000ä¸ªè¯çš„è¯è¡¨ç¤ºçŸ©é˜µå°†å¯¼è‡´éå¸¸ç¨€ç–çš„50000Ã—50000çŸ©é˜µã€‚\nä½†æ˜¯ï¼Œåœ¨æœ€æ–°çš„è¯åµŒå…¥å­¦ä¹ ç®—æ³•ä¸­ï¼Œone-hot encoedèµ·åˆ°å·¦å³å´ä¸å¯å¿½è§†ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªçƒ­ç¼–ç æ¥è¡¨ç¤ºæ•°å­—åŒ–çš„è¯ï¼Œå¹¶å°†å…¶é¦ˆé€åˆ°ç¥ç»ç½‘ç»œï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ æ›´å¥½å’Œæ›´å°çš„æ•°å­—åŒ–ç‰¹å¾è¡¨ç¤ºçš„å•è¯ã€‚\n3ã€TF-IDF\nTF-IDF(Term Frequency-Inverse Document Frequency, è¯é¢‘-é€†æ–‡ä»¶é¢‘ç‡)ã€‚æ˜¯ä¸€ç§ç”¨äºèµ„è®¯æ£€ç´¢ä¸èµ„è®¯æ¢å‹˜çš„å¸¸ç”¨åŠ æƒæŠ€æœ¯ã€‚TF-IDFæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚å­—è¯çš„é‡è¦æ€§éšç€å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”å¢åŠ ï¼Œä½†åŒæ—¶ä¼šéšç€å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”ä¸‹é™ã€‚ä¸€å¥è¯ï¼šÂ ä¸€ä¸ªè¯è¯­åœ¨ä¸€ç¯‡æ–‡ç« ä¸­å‡ºç°æ¬¡æ•°è¶Šå¤š, åŒæ—¶åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­å‡ºç°æ¬¡æ•°è¶Šå°‘, è¶Šèƒ½å¤Ÿä»£è¡¨è¯¥æ–‡ç« .\nè¯é¢‘ (term frequency, TF)Â æŒ‡çš„æ˜¯æŸä¸€ä¸ªç»™å®šçš„è¯è¯­åœ¨è¯¥æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚è¿™ä¸ªæ•°å­—é€šå¸¸ä¼šè¢«å½’ä¸€åŒ–(ä¸€èˆ¬æ˜¯è¯é¢‘é™¤ä»¥æ–‡ç« æ€»è¯æ•°), ä»¥é˜²æ­¢å®ƒåå‘é•¿çš„æ–‡ä»¶ã€‚ï¼ˆåŒä¸€ä¸ªè¯è¯­åœ¨é•¿æ–‡ä»¶é‡Œå¯èƒ½ä¼šæ¯”çŸ­æ–‡ä»¶æœ‰æ›´é«˜çš„è¯é¢‘ï¼Œè€Œä¸ç®¡è¯¥è¯è¯­é‡è¦ä¸å¦ã€‚ï¼‰ä½†æ˜¯, éœ€è¦æ³¨æ„, ä¸€äº›é€šç”¨çš„è¯è¯­å¯¹äºä¸»é¢˜å¹¶æ²¡æœ‰å¤ªå¤§çš„ä½œç”¨, åå€’æ˜¯ä¸€äº›å‡ºç°é¢‘ç‡è¾ƒå°‘çš„è¯æ‰èƒ½å¤Ÿè¡¨è¾¾æ–‡ç« çš„ä¸»é¢˜, æ‰€ä»¥å•çº¯ä½¿ç”¨æ˜¯TFä¸åˆé€‚çš„ã€‚æƒé‡çš„è®¾è®¡å¿…é¡»æ»¡è¶³ï¼šä¸€ä¸ªè¯é¢„æµ‹ä¸»é¢˜çš„èƒ½åŠ›è¶Šå¼ºï¼Œæƒé‡è¶Šå¤§ï¼Œåä¹‹ï¼Œæƒé‡è¶Šå°ã€‚æ‰€æœ‰ç»Ÿè®¡çš„æ–‡ç« ä¸­ï¼Œä¸€äº›è¯åªæ˜¯åœ¨å…¶ä¸­å¾ˆå°‘å‡ ç¯‡æ–‡ç« ä¸­å‡ºç°ï¼Œé‚£ä¹ˆè¿™æ ·çš„è¯å¯¹æ–‡ç« çš„ä¸»é¢˜çš„ä½œç”¨å¾ˆå¤§ï¼Œè¿™äº›è¯çš„æƒé‡åº”è¯¥è®¾è®¡çš„è¾ƒå¤§ã€‚IDFå°±æ˜¯åœ¨å®Œæˆè¿™æ ·çš„å·¥ä½œã€‚\nè®¡ç®—å…¬å¼å¦‚ä¸‹\nä¸¾ä¾‹è¯´æ˜ï¼Œä¸‹é¢æœ‰ä¸¤ä¸ªæ–‡æ¡£\nâ€¢Â Document 1:Â This is about cats. Cats are great companions.\nâ€¢Â Document 2:Â This is about dogs. Dogs are very loyal.\nç›´æ¥å¥—ç”¨å…¬å¼è®¡ç®—ï¼š\næ˜¾ç„¶ï¼Œâ€œcatsâ€è¿™ä¸ªè¯æ˜¯å¸¦æœ‰æœ‰ç”¨ä¿¡æ¯çš„è¯ï¼Œè€Œâ€œthisâ€å´ä¸ç¬¦åˆè¦æ±‚ã€‚åœ¨è¡¡é‡è¯æ±‡é‡è¦æ€§æˆ–è€…è¯´æ˜¯ç‰¹å¾é€‰æ‹©æ—¶ï¼ŒTF-IDFæ–¹æ³•æ˜¯å¾ˆé‡è¦çš„ã€‚\n4ã€å…±ç”ŸçŸ©é˜µï¼ˆCo-occurrence matrixï¼‰\nå…±ç°çŸ©é˜µä¸åƒç‹¬çƒ­ç¼–ç å¯¹ä¸Šä¸‹æ–‡è¯­å¢ƒä¸‹çš„å•è¯è¿›è¡Œç¼–ç ï¼Œä½†å´éœ€è¦VÃ—Vçš„çŸ©é˜µã€‚ä¸¾ä¾‹è¯´æ˜ï¼Œä¸‹é¢æœ‰ä¸¤å¥è¯ï¼š\nâ€¢Â Jerry and Mary are friends.\nâ€¢Â Jerry buys flowers for Mary.\né€šè¿‡å…±ç°çŸ©é˜µçš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¾—åˆ°å¦‚ä¸‹çŸ©é˜µï¼ˆå› ä¸ºçŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œæ‰€ä»¥åªæ˜¾ç¤ºçŸ©é˜µçš„ä¸€ä¸ªä¸‰è§’å½¢çŠ¶ï¼‰ã€‚\næ˜¾ç„¶ï¼Œä¸éš¾çœ‹å‡ºï¼Œéšç€è¯æ±‡è§„æ¨¡çš„å¢åŠ çŸ©é˜µä¹Ÿéšä¹‹è¿…é€Ÿè†¨èƒ€ï¼Œç»´æŒå…±ç°çŸ©é˜µéœ€è¦çš„æˆæœ¬ä¹Ÿéšä¹‹æ€¥é€Ÿæ‰©å¤§ã€‚æ­¤å¤–ï¼Œè¿›è¡Œåˆå¹¶ä¸Šä¸‹æ–‡çª—å£ä»¥ä¾¿å¾—åˆ°å¤§äº1çš„ç»“æœï¼Œä¹Ÿæ˜¯æ¯”è¾ƒå›°éš¾çš„ã€‚å¦‚æœè¿›è¡ŒåŠ æƒè®¡æ•°ï¼Œå•è¯çš„æƒé‡ä¹Ÿä¼šéšç€ç›¸å…³è¯çš„è·ç¦»å¢åŠ è€Œå‡å°‘ã€‚\næ‰€æœ‰è¿™äº›ä¸è¶³ä¿ƒä½¿æˆ‘ä»¬æ¢ç´¢æ›´å…·åŸåˆ™æ€§ã€é²æ£’æ€§å’Œå¯ä¼¸ç¼©æ€§çš„å­¦ä¹ æ–¹å¼å’Œæ¨æ–­è¯çš„æ„ä¹‰ï¼ˆå³è¡¨ç¤ºï¼‰ã€‚Word2VECæ˜¯è¿‘æœŸå¼•å…¥çš„åˆ†å¸ƒå¼è¯è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œç›®å‰è¢«ç”¨äºè®¸å¤šNLPä»»åŠ¡ï¼ˆä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€èŠå¤©æœºå™¨äººå’Œå›¾åƒæ ‡é¢˜ç”Ÿæˆå™¨ï¼‰çš„ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ã€‚ä¸‹é¢å°±é‡ç‚¹æ¢è®¨ä¸‹Word2Vec--ä¸€ç§ä»¥ç¥ç»ç½‘ç»œä¸ºåŸºç¡€çš„æ–¹æ³•å»å­¦ä¹ è¯è¡¨ç¤ºã€‚\nå››ã€Word2Vec\nWord2vecï¼Œæ˜¯ä¸ºä¸€ç¾¤ç”¨æ¥äº§ç”Ÿè¯å‘é‡çš„ç›¸å…³æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ä¸ºæµ…è€ŒåŒå±‚çš„ç¥ç»ç½‘ç»œï¼Œç”¨æ¥è®­ç»ƒä»¥é‡æ–°å»ºæ„è¯­è¨€å­¦ä¹‹è¯æ–‡æœ¬ã€‚ç½‘ç»œä»¥è¯è¡¨ç°ï¼Œå¹¶ä¸”éœ€çŒœæµ‹ç›¸é‚»ä½ç½®çš„è¾“å…¥è¯ï¼Œåœ¨word2vecä¸­è¯è¢‹æ¨¡å‹å‡è®¾ä¸‹ï¼Œè¯çš„é¡ºåºæ˜¯ä¸é‡è¦çš„ã€‚è®­ç»ƒå®Œæˆä¹‹åï¼Œword2vecæ¨¡å‹å¯ç”¨æ¥æ˜ å°„æ¯ä¸ªè¯åˆ°ä¸€ä¸ªå‘é‡ï¼Œå¯ç”¨æ¥è¡¨ç¤ºè¯å¯¹è¯ä¹‹é—´çš„å…³ç³»ï¼Œè¯¥å‘é‡ä¸ºç¥ç»ç½‘ç»œä¹‹éšè—å±‚ã€‚\nå­¦ä¹ ç»™å®šå•è¯çš„æ„ä¹‰ï¼Œé€šè¿‡æŸ¥çœ‹ä¸Šä¸‹æ–‡å¹¶ç”¨æ•°å­—è¡¨ç¤ºã€‚åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯åœ¨å…´è¶£è¯å‰é¢å’Œåé¢çš„å›ºå®šæ•°é‡çš„è¯ã€‚è®©æˆ‘ä»¬å‡è®¾ä¸€ä¸ªæœ‰Nä¸ªè¯çš„è¯­æ–™åº“ã€‚åœ¨æ•°å­¦ä¸Šï¼Œè¿™å¯ä»¥ç”¨W 0ã€W 1ã€â€¦ã€W iå’ŒW nè¡¨ç¤ºçš„å•è¯åºåˆ—æ¥è¡¨ç¤ºï¼Œå…¶ä¸­w iæ˜¯è¯­æ–™åº“ä¸­çš„ç¬¬iä¸ªå•è¯ã€‚\næ¥ä¸‹æ¥ï¼Œå¦‚æœæˆ‘ä»¬æƒ³æ‰¾åˆ°ä¸€ä¸ªèƒ½å¤Ÿå­¦ä¹ å•è¯æ„ä¹‰çš„å¥½ç®—æ³•ï¼Œå¯¹äºç»™å®šä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬çš„ç®—æ³•åº”è¯¥èƒ½å¤Ÿæ­£ç¡®åœ°é¢„æµ‹ä¸Šä¸‹æ–‡å•è¯ã€‚è¿™æ„å‘³ç€å¯¹äºä»»ä½•ç»™å®šçš„å•è¯Wiï¼Œä»¥ä¸‹æ¦‚ç‡éƒ½æ˜¯å¾ˆé«˜çš„ï¼š\nä¸ºäº†å¾—åˆ°ç­‰å¼å³è¾¹éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦å‡å®šç»™å®šç›®æ ‡è¯ï¼ˆWiï¼‰ä¸”ä¸Šä¸‹æ–‡ä¸­è¯é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼ˆä¾‹å¦‚ï¼ŒWi-1å’ŒWi-2æ˜¯ç‹¬ç«‹çš„ï¼‰ã€‚è™½ç„¶ä¸å®Œå…¨æ­£ç¡®ï¼Œä½†è¿™ç§è¿‘ä¼¼çš„å–èˆæ›´åŠ ç¬¦åˆå®é™…é—®é¢˜çš„è§£å†³ã€‚\n1ã€Is queen = king -he + she ï¼Ÿï¼ˆå¯ä»¥è·³è¿‡ï¼‰\nè¿™é‡Œå’Œç½‘ä¸Šä¼—å¤šçš„â€œkingâ€ã€â€˜manâ€™ã€â€˜queenâ€™ã€â€˜womanâ€™éƒ½æ˜¯ç±»ä¼¼çš„ï¼Œä¹‹æ‰€ä»¥å•ç‹¬æ¢³ç†ä¸‹ï¼Œæ˜¯å› ä¸ºç¬”è€…åœ¨æœ¬ä¹¦ä¸­å‘ç°äº†ä¸ªé—®é¢˜ï¼Œç½‘ä¸Šä¹Ÿæ²¡æœ‰å‘ç°åˆé€‚çš„ç­”æ¡ˆï¼Œè®°å½•ä¸‹ï¼Œä»¥åæœ‰æœºä¼šå†å›è¿‡å¤´çœ‹çœ‹ã€‚\né€šè¿‡ä¸Šé¢çš„èµ„æ–™ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œä¸ºäº†æ›´å¥½åœ°å¾—åˆ°é¢„æœŸçš„è¯è¡¨ç¤ºç›®æ ‡ï¼Œéœ€è¦æœ€å¤§åŒ–ä¸Šé¢æåˆ°çš„æ¦‚ç‡ï¼Œè¿™é‡Œé€šè¿‡ä¸€ä¸ªå°çš„ä¾‹å­æ¥è¿›ä¸€æ­¥è¯´æ˜ã€‚ä¸‹é¢è¿™å¥è¯å¯ä»¥çœ‹åšä¸€ä¸ªå°çš„è¯­æ–™åº“ã€‚\nThere was a very rich king. He had a beautiful queen. She was very kind.\né¦–å…ˆï¼Œäººå·¥åšä¸‹é¢„å¤„ç†ï¼šåˆ é™¤æ ‡ç‚¹ç¬¦å·å’Œæ— ä¿¡æ¯çš„å•è¯ï¼Œç»“æœå¦‚ä¸‹ï¼š\nwas rich king he had beautiful queen she was kind\nå…¶æ¬¡ï¼Œå¯¹äºæ¯ä¸€ä¸ªå•è¯è€Œè¨€ï¼Œæˆ‘ä»¬ç»™å‡ºå¤šä¸ªå…ƒç¥–ç»„åˆï¼Œä¸Šä¸‹æ–‡å•è¯ç»Ÿä¸€æ ¼å¼ï¼ˆç›®æ ‡è¯-->ä¸Šä¸‹æ–‡å•è¯1ï¼Œä¸Šä¸‹æ–‡å•è¯2ï¼‰ã€‚å‡è®¾åœ¨ä»»ä¸€æ–¹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ä¸º1ï¼š\nwas â†’ rich\nrich â†’ was, king\nking â†’ rich, he\nhe â†’ king, had\nhad â†’ he, beautiful\nbeautiful â†’ had, queen\nqueen â†’ beautiful, she\nshe â†’ queen, was\nwas â†’ she, kind\nkind â†’ was\nç›®æ ‡è¦æ¸…æ¥šï¼Œé‚£å°±æ˜¯ï¼šåœ¨ç»™å‡ºå·¦è¾¹å•è¯çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé¢„æµ‹å³è¾¹çš„å•è¯ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œå³ä¾§ä¸Šä¸‹æ–‡çš„å•è¯ä¸å·¦ä¾§ç»™å®šå•è¯é—´åº”è¯¥æ‹¥æœ‰è¾ƒé«˜çš„ç›¸ä¼¼åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šè¯ä¹‰åº”è¯¥è¢«å…¶å‘¨å›´çš„è¯è¿›ä¸€æ­¥ä¼ è¾¾ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œæˆ‘ä»¬äººä¸ºç»™å‡ºç›¸å…³æ•°å€¼ï¼Œä»¥ç²—ä½“å•è¯ä¸ºä¾‹ã€‚\nrich â†’ [0,0]\né€šè¿‡ç»™å®šå•è¯â€˜richâ€™ï¼Œè¦å‡†ç¡®åœ°é¢„æµ‹å‡ºâ€˜wasâ€™å’Œâ€˜richâ€™ï¼Œé‚£ä¹ˆâ€˜wasâ€™å’Œâ€˜kingâ€™ä¸â€˜richâ€™ä¹‹é—´åº”è¯¥æœ‰å¾ˆé«˜çš„ç›¸ä¼¼åº¦ï¼Œè¿™é‡Œä»¥å‘é‡é—´æ¬§å‡ é‡Œå¾—è·ç¦»å»è¡¡é‡ç›¸ä¼¼åº¦ã€‚\nä¸‹é¢ç»™å‡ºâ€˜kingâ€™å’Œâ€˜wasâ€™çš„æ•°å€¼ï¼Œï¼ˆè‹±æ–‡åŸç‰ˆä¸­ç»™å‡ºçš„å•è¯â€˜richâ€™åº”è¯¥æœ‰è¯¯ï¼‰\nking â†’ [0,1]\nwas â†’ [-1,0]\næˆ‘ä»¬å¾ˆå®¹æ˜“è®¡ç®—å‡ºç›¸å…³æ¬§æ°è·ç¦»ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\nDist(rich,king) = 1.0\nDist(rich,was) = 1.0\næ¯”è¾ƒç›´è§‚çš„æ¬§å¼è·ç¦»è¡¨ç¤ºå¦‚-1æ‰€ç¤ºï¼š\n-1\nåŒæ ·ï¼Œæˆ‘ä»¬ç»§ç»­ä¸‹é¢å•è¯å…ƒç»„é—´çš„æ¢³ç†ï¼š\nking â†’ rich, he\næˆ‘ä»¬å·²ç»å»ºç«‹äº†â€˜king'å’Œâ€™richâ€˜é—´çš„å…³ç³»ï¼Œä½†ç›¸å…³å·¥ä½œè¿˜æ²¡æœ‰å®Œæˆï¼›ä¸‹ä¸€æ­¥å…ˆè°ƒæ•´â€™kingâ€˜çš„çŸ¢é‡ï¼Œä½¿ä¹‹æ›´æ¥è¿‘äºâ€™richâ€˜ï¼š\nking â†’ [0,0.8]\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠŠå•è¯â€™heâ€˜æ·»åŠ åˆ°å›¾ç¤ºä¸­ï¼Œâ€™heâ€˜åº”è¯¥æ›´æ¥è¿‘â€™kingâ€˜ï¼Œè¿™å°±æ˜¯ç›®å‰ä¸ºå…³äºå³è¾¹â€˜heâ€™å•è¯çš„æ‰€æœ‰ä¿¡æ¯ã€‚\nhe â†’ [0.5,0.8]\næ­¤æ—¶ï¼Œå›¾ç¤ºå¦‚4-2æ‰€ç¤ºï¼š\n-2\nç»§ç»­ä¸‹é¢ä¸¤ä¸ªå…ƒç»„ï¼šqueen-->beautiful,sheÂ å’Œshe-->queen,wasã€‚\nshe â†’ queen, was\nç°åœ¨ï¼Œæˆ‘ä»¬ç”¨å¯¹è‹±è¯­çš„å…ˆéªŒçŸ¥è¯†æ¥è¿›ä¸€æ­¥ç ”ç©¶ã€‚ç”±äºâ€˜sheâ€™å’Œâ€˜heâ€™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç”¨æ³•æ˜¯ç­‰ä»·çš„æˆ–è€…æ˜¯ç­‰æ¦‚ç‡çš„ï¼Œæ‰€ä»¥â€˜sheâ€˜åˆ°â€™wasâ€˜å’Œâ€™heâ€˜åˆ°â€™wasâ€˜ä¹‹é—´çš„è·ç¦»æ˜¯ç­‰åŒçš„ï¼Œåˆ™ï¼š\nshe â†’ [0.5,0.6]\nå¤‡æ³¨ï¼šå­˜åœ¨çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯æ—¢ç„¶â€™sheâ€˜åˆ°â€™wasâ€˜å’Œâ€™heâ€˜åˆ°çš„è·ç¦»æ˜¯ç›¸ç­‰çš„ï¼Œé‚£ä¹ˆç›´æ¥åœ¨ä¸‹é¢äºŒç»´å›¾ç¤ºä¸­æ˜¾ç¤ºwasï¼ˆ-1,0),he(0.5,0.8),she(0.5,0.6)æ˜¾ç¤ºæ˜¯æœ‰é—®é¢˜çš„ã€‚ç¬”è€…ä¹ŸæŸ¥çœ‹äº†ç½‘ä¸Šçš„æ–‡ç« ï¼Œç›®å‰ä¸ºæ­¢æ²¡æœ‰æ‰¾åˆ°è®²è§£è¿™ä¸ªé—®é¢˜çš„æ–‡ç« ï¼Œå¸¸è§çš„è¯´æ³•æ˜¯â€œä¸€äº›ç ”ç©¶è¿˜å‘ç°ï¼Œè®¡ç®—æœ‰ç›¸ä¼¼å…³ç³»çš„å•è¯ä¹‹é—´çš„ä½ç§»å‘é‡ä¹Ÿä¼šååˆ†ç›¸ä¼¼ï¼Œä¾‹å¦‚ä»â€˜Manâ€™åˆ°â€˜Wonmanâ€™çš„å‘é‡ï¼Œä¸ä»â€˜Kingâ€™åˆ°â€˜Queenâ€™ä¹‹é—´çš„å‘é‡å‡ ä¹ç›¸åŒâ€ï¼Œæ¥å¯¹Word2Vecè¿›è¡Œè§£é‡Šï¼Œå…·ä½“éªŒè¯æ²¡å‘ç°ã€‚å¦‚æœæœ‰ç½‘å‹æ¸…æ¥šè¿™ä¸€å—çš„ï¼Œè¯·å¸®å¿™è§£é‡Šä¸€ä¸‹ã€‚è¿™é‡Œæš‚ä¸”æŒ‰å¸¸è¯†æ€§çš„æ¥å¼€å±•ã€‚\nä¸‹ä¸€æ­¥ï¼Œç”¨â€œqueenâ€æ¥è¿‘â€œsheâ€ï¼š\nqueen â†’ [0.0,0.6]\næœ€ç»ˆå½¢æˆå¦‚-3æ‰€ç¤ºï¼š\n-3\næˆªæ­¢ç›®å‰ï¼Œæ˜¾ç„¶æ–¹ç¨‹çš„å³è¾¹å’Œå·¦è¾¹æ˜¯ç›¸ç­‰çš„ï¼š\nå³è¾¹= king â€“ he + she\n= [0,0.8] â€“ [0.5,0.8] + [0.5,0.6]\n= [0,0.6]\nå·¦è¾¹=[0,0.6]\nå³Â queen = king -he + sheã€‚\n2ã€è®¾è®¡å­¦ä¹ è¯åµŒå…¥çš„æŸå¤±å‡½æ•°\nç°å®ä¸–ç•Œä¸­ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªæ¯”è¾ƒç®€å•ä»»åŠ¡ï¼Œå…¶å•è¯é‡ä¹Ÿå¾ˆå®¹æ˜“è¶…è¿‡10000ä¸ªï¼Œå› æ­¤ï¼Œå¯¹äºå¼€å‘å¤§å‹æ–‡æœ¬è¯­æ–™åº“ä¸­å•è¯å‘é‡çš„è‰°å·¨ä»»åŠ¡ï¼Œé  äººå·¥å»å®Œæˆæ˜¾ç„¶æ˜¯ä¸åˆé€‚ï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ç§æ–¹æ³•ï¼Œæ¯”å¦‚ä½¿ç”¨ä¸€äº› æœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚ç¥ç»ç½‘ç»œç­‰ï¼‰è‡ªåŠ¨æ‰¾åˆ°åˆé€‚çš„è¯åµŒå…¥ã€‚å¯¹äºä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•æ¥å®Œæˆä»»åŠ¡è€Œè¨€ï¼Œéœ€è¦å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œä½¿å…¶æŸå¤±æœ€å°åŒ–ã€‚\næ¥ç€å®šä¹‰ä¸€ä¸ªç¥ç»ç½‘ç»œä»£ä»·ï¼ˆæŸå¤±ï¼‰å‡½æ•°\nè¿™é‡Œéœ€è¦å¯¹Â  æœ€å¤§åŒ–ï¼Œæ‰€ä»¥ä¸Šå¼å‡½æ•°éœ€è¦åŠ ä¸Šè´Ÿå·ã€‚\nä¸‹é¢æˆ‘ä»¬å°†æ–¹ç¨‹è½¬æ¢ä¸ºæ—¥å¿—ç©ºé—´ï¼Œå‰åçš„ä¸€è‡´æ€§å’Œæ•°å€¼ç¨³å®šæ€§ä¼šæ›´ä½³ï¼Œå¦‚ä¸‹ï¼š\nä¸Šé¢è¿™ä¸ªå…¬å¼å°±æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚\nç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œå°±å¯ä»¥ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥ä¼˜åŒ–è¿™ä¸ªä»£ä»·å‡½æ•°ï¼Œè¿™æ ·ä¼šä½¿å¾—è¯å‘é‡æˆ–è¯åµŒå…¥æ ¹æ®è¯çš„æ„æ€æ›´å¥½åœ°ç»„åˆè‡ªèº«ã€‚\n3ã€Skip-Gramæ¨¡å‹\n3.1 ç®€è¦è¯´æ˜\nå¯¹äºWord2Vecè€Œè¨€ï¼Œå…¶æ¨¡å‹ä¸»è¦æœ‰Skip-Gramå’ŒCBOWä¸¤ç§ï¼Œç®€å•æ¥è®²ï¼Œå·²çŸ¥ä¸€ä¸ªå•è¯Wtï¼Œå»å®ç°é¢„æµ‹å…¶å‰åWï¼ˆt-2)ã€W(t-1)ã€Wï¼ˆt+1)ã€W(t+2)ä¸Šä¸‹æ–‡è¯çš„æ–¹æ³•ï¼Œå°±æ˜¯Skip-Gramæ–¹æ³•ï¼›è€Œåè¿‡æ¥ï¼Œç»™å‡ºWï¼ˆt-2)ã€W(t-1)ã€Wï¼ˆt+1)ã€W(t+2)å•è¯ï¼Œå»é¢„æµ‹Wtçš„æ–¹æ³•ï¼Œå°±æ˜¯CBOWæ–¹æ³•ã€‚\n-4\nSkip-Gramæ¨¡å‹çš„æ­å»ºåŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼šä¸€æ˜¯å»ºç«‹æ¨¡å‹ï¼ŒäºŒæ˜¯é€šè¿‡æ¨¡å‹è·å–åµŒå…¥è¯å‘é‡ã€‚Skip-Gramå»ºæ¨¡æ€æƒ³ä¸è‡ªç¼–ç å™¨ï¼ˆauto-encoderï¼‰å¾ˆç›¸ä¼¼ï¼Œéƒ½æ˜¯å…ˆåŸºäºè®­ç»ƒæ•°æ®å»ºç«‹ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå½“è¿™ä¸ªæ¨¡å‹è®­ç»ƒå¥½ä»¥åï¼Œæˆ‘ä»¬å¾—åˆ°è¯¥æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€å­¦åˆ°çš„å‚æ•°ï¼Œå¸¸è§çš„æœ‰éšè—å±‚çš„æƒé‡çŸ©é˜µç­‰ï¼Œæ˜¾ç„¶è¿™åªæ˜¯æˆ‘ä»¬å®ç°ç›®æ ‡è¿‡ç¨‹ä¸­å¿…è¦çš„é“ºå«å·¥ä½œï¼Œæ‰€ä»¥ï¼Œæœ‰æ–‡ç« ä¹Ÿç§°åŸºäºè®­ç»ƒæ•°æ®å»ºæ¨¡çš„è¿‡ç¨‹ä¸ºâ€œFake Taskâ€ã€‚\n3.2Â The Fake Task\nåœ¨ä¸Šé¢æåˆ°ï¼Œè®­ç»ƒæ¨¡å‹ç›®çš„æ˜¯ä¸ºäº†è·å¾—æ¨¡å‹åŸºäºè®­ç»ƒæ•°æ®å­¦åˆ°çš„éšè—å±‚æƒé‡ã€‚ä¸ºäº†è·å¾—è¿™äº›æƒé‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¦å»ºç«‹ä¸€ä¸ªå®Œæ•´çš„ç¥ç»ç½‘ç»œä½œä¸ºâ€œFake Taskâ€ï¼Œåç»­å†å›å¤´æ¥çœ‹é€šè¿‡â€œFake Taskâ€å¦‚ä½•é—´æ¥åœ°å¾—åˆ°è¿™äº›è¯å‘é‡ã€‚ä¸¾ä¾‹è¯´æ˜ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå¥å­â€œThe dog barked at the mailmanâ€ã€‚\né¦–å…ˆæˆ‘ä»¬é€‰å¥å­ä¸­é—´çš„ä¸€ä¸ªè¯ä½œä¸ºæˆ‘ä»¬çš„è¾“å…¥è¯ï¼Œä¾‹å¦‚æˆ‘ä»¬é€‰å–â€œdogâ€ä½œä¸ºinput wordï¼›\næœ‰äº†input wordä»¥åï¼Œæˆ‘ä»¬å†å®šä¹‰ä¸€ä¸ªå«åšskip_windowçš„å‚æ•°ï¼Œå®ƒä»£è¡¨ç€æˆ‘ä»¬ä»å½“å‰input wordçš„ä¸€ä¾§ï¼ˆå·¦è¾¹æˆ–å³è¾¹ï¼‰é€‰å–è¯çš„æ•°é‡ã€‚å¦‚æœæˆ‘ä»¬è®¾ç½®skip_window=2ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ€ç»ˆè·å¾—çª—å£ä¸­çš„è¯ï¼ˆåŒ…æ‹¬input wordåœ¨å†…ï¼‰å°±æ˜¯['The', 'dog'ï¼Œ'barked', 'at']ã€‚skip_window=2ä»£è¡¨ç€é€‰å–å·¦input wordå·¦ä¾§2ä¸ªè¯å’Œå³ä¾§2ä¸ªè¯è¿›å…¥æˆ‘ä»¬çš„çª—å£ï¼Œæ‰€ä»¥æ•´ä¸ªçª—å£å¤§å°span=2x2=4ã€‚å¦ä¸€ä¸ªå‚æ•°å«num_skipsï¼Œå®ƒä»£è¡¨ç€æˆ‘ä»¬ä»æ•´ä¸ªçª—å£ä¸­é€‰å–å¤šå°‘ä¸ªä¸åŒçš„è¯ä½œä¸ºæˆ‘ä»¬çš„output wordï¼Œå½“skip_window=2ï¼Œnum_skips=2æ—¶ï¼Œæˆ‘ä»¬å°†ä¼šå¾—åˆ°ä¸¤ç»„Â (input word, output word)Â å½¢å¼çš„è®­ç»ƒæ•°æ®ï¼Œå³Â ('dog', 'barked')ï¼Œ('dog', 'the')ã€‚\nç¥ç»ç½‘ç»œåŸºäºè¿™äº›è®­ç»ƒæ•°æ®å°†ä¼šè¾“å‡ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¸ªæ¦‚ç‡ä»£è¡¨ç€è¯å…¸ä¸­çš„æ¯ä¸ªè¯æ˜¯output wordçš„å¯èƒ½æ€§ã€‚ç»“åˆä¾‹å­è¯´æ˜ï¼Œä¸Šä¸€æ­¥ä¸­æˆ‘ä»¬åœ¨è®¾ç½®skip_windowå’Œnum_skips=2çš„æ¡ä»¶ä¸‹è·å¾—äº†ä¸¤ç»„è®­ç»ƒæ•°æ®ã€‚è‹¥å…ˆæ‹¿ä¸€ç»„æ•°æ®Â ('dog', 'barked')Â æ¥è®­ç»ƒç¥ç»ç½‘ç»œï¼Œé‚£ä¹ˆæ¨¡å‹é€šè¿‡å­¦ä¹ è¿™ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä¼šå‘Šè¯‰æˆ‘ä»¬è¯æ±‡è¡¨ä¸­æ¯ä¸ªå•è¯æ˜¯â€œbarkedâ€çš„æ¦‚ç‡å¤§å°ã€‚\næ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡ä»£è¡¨ç€è¯å…¸ä¸­æ¯ä¸ªè¯æœ‰å¤šå¤§æ¦‚ç‡ä¸input wordåŒæ—¶å‡ºç°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‘ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­è¾“å…¥ä¸€ä¸ªå•è¯â€œChineseâ€œï¼Œé‚£ä¹ˆæœ€ç»ˆæ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡ä¸­ï¼Œåƒâ€œChinaâ€ï¼Œ â€œConfuciusâ€è¿™ç§ç›¸å…³è¯çš„æ¦‚ç‡å°†è¿œé«˜äºåƒâ€watermelonâ€œï¼Œâ€kangarooâ€œéç›¸å…³è¯çš„æ¦‚ç‡ã€‚å› ä¸ºâ€Chinaâ€œï¼Œâ€œConfuciusâ€åœ¨æ–‡æœ¬ä¸­æ›´å¤§å¯èƒ½åœ¨â€Chineseâ€œçš„çª—å£ä¸­å‡ºç°ã€‚æˆ‘ä»¬å°†é€šè¿‡ç»™ç¥ç»ç½‘ç»œè¾“å…¥æ–‡æœ¬ä¸­æˆå¯¹çš„å•è¯æ¥è®­ç»ƒå®ƒå®Œæˆä¸Šé¢æ‰€è¯´çš„æ¦‚ç‡è®¡ç®—ã€‚ä¸‹é¢çš„å›¾ä¸­ç»™å‡ºäº†ä¸€äº›è®­ç»ƒæ ·æœ¬çš„ä¾‹å­ã€‚è¿™é‡Œé€‰å®šå¥å­â€œThe quick brown fox jumps over lazy dogâ€ï¼Œè®¾å®šæˆ‘ä»¬çš„çª—å£å¤§å°ä¸º2ï¼ˆwindow_size=2ï¼‰ï¼Œå³ä»…é€‰è¾“å…¥è¯å‰åå„ä¸¤ä¸ªè¯å’Œè¾“å…¥è¯è¿›è¡Œç»„åˆã€‚-5ä¸­ï¼Œè“è‰²ä»£è¡¨input wordï¼Œæ–¹æ¡†å†…ä»£è¡¨ä½äºçª—å£å†…çš„å•è¯ã€‚\n-5\nè¿™é‡Œçš„æ¨¡å‹å°†ä¼šä»æ¯å¯¹å•è¯å‡ºç°çš„æ¬¡æ•°ä¸­ä¹ å¾—ç»Ÿè®¡ç»“æœã€‚ä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œå¯èƒ½ä¼šå¾—åˆ°æ›´å¤šç±»ä¼¼ï¼ˆâ€œChineseâ€œï¼Œâ€Chinaâ€œï¼‰è¿™æ ·çš„è®­ç»ƒæ ·æœ¬å¯¹ï¼Œè€Œå¯¹äºï¼ˆâ€Chineseâ€œï¼Œâ€watermelonâ€œï¼‰è¿™æ ·çš„ç»„åˆå´çœ‹åˆ°çš„å¾ˆå°‘ã€‚å› æ­¤ï¼Œå½“æ¨¡å‹å®Œæˆè®­ç»ƒåï¼Œç»™å®šä¸€ä¸ªå•è¯â€Chineseâ€œä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºçš„ç»“æœä¸­â€Chineseâ€œæˆ–è€…â€Confuciusâ€œè¦æ¯”â€watermelonâ€œè¢«èµ‹äºˆæ›´é«˜çš„æ¦‚ç‡ã€‚\n3.3Â æ¨¡å‹ç»“æ„åˆ†æ\nä¸Šé¢æ›¾æåˆ°ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•æ¥å¼€å±•ä»»åŠ¡çš„è¯ï¼Œæ‰€æœ‰è¾“å…¥å€¼å¿…é¡»è¢«æ•°å­—åŒ–ã€‚è¿™é‡Œï¼Œå°†åŸºäºè®­ç»ƒæ–‡æ¡£æ¥æ„å»ºæˆ‘ä»¬è‡ªå·±çš„è¯æ±‡è¡¨ï¼ˆvocabularyï¼‰åï¼Œå†å¯¹å•è¯è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼ˆone-hotç¼–ç ï¼‰ã€‚\nå‡å®šä»è®­ç»ƒæ–‡æ¡£ä¸­æŠ½å–10000ä¸ªå”¯ä¸€ä¸é‡å¤çš„å•è¯ç»„æˆè¯æ±‡è¡¨ã€‚å¯¹è¿™äº›10000ä¸ªå•è¯è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼ˆone-hotç¼–ç ï¼‰ï¼Œå¾—åˆ°çš„æ¯ä¸ªå•è¯éƒ½æ˜¯ä¸€ä¸ª10000ç»´çš„å‘é‡ï¼Œå‘é‡æ¯ä¸ªç»´åº¦çš„å€¼åªæœ‰0æˆ–è€…1ï¼Œå‡å¦‚å•è¯antsåœ¨è¯æ±‡è¡¨ä¸­çš„å‡ºç°ä½ç½®ä¸ºç¬¬3ä¸ªï¼Œé‚£ä¹ˆantsçš„å‘é‡å°±æ˜¯ä¸€ä¸ªç¬¬ä¸‰ç»´åº¦å–å€¼ä¸º1ï¼Œå…¶ä»–ç»´éƒ½ä¸º0çš„10000ç»´çš„å‘é‡ï¼ˆants=[0, 0, 1, 0, ..., 0]ï¼‰ã€‚\nå¯¹äºâ€œThe dog barked at the mailmanâ€è€Œè¨€ï¼ŒåŸºäºè¿™ä¸ªå¥å­ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªå¤§å°ä¸º5çš„è¯æ±‡è¡¨ï¼ˆå¤§å°å†™å’Œæ ‡ç‚¹ç¬¦å·ä¸éœ€è¦ï¼Œå¯ä»¥å¿½ç•¥ï¼‰ï¼š(\"the\", \"dog\", \"barked\", \"at\", \"mailman\")ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªè¯æ±‡è¡¨çš„å•è¯è¿›è¡Œç¼–å·0-4ã€‚é‚£ä¹ˆâ€dogâ€œå°±å¯ä»¥è¢«è¡¨ç¤ºä¸ºä¸€ä¸ª5ç»´å‘é‡[0, 1, 0, 0, 0]ã€‚\næ¨¡å‹çš„è¾“å…¥å¦‚æœæ˜¯ä¸€ä¸ª10000ç»´çš„å‘é‡ï¼Œé‚£ä¹ˆè¾“å‡ºä¹Ÿåº”æ˜¯ä¸€ä¸ª10000ç»´åº¦ï¼ˆè¯æ±‡è¡¨çš„å¤§å°ï¼‰çš„å‘é‡ï¼Œå®ƒåŒ…å«äº†10000ä¸ªæ¦‚ç‡ï¼Œæ¯ä¸€ä¸ªæ¦‚ç‡ä»£è¡¨ç€å½“å‰è¯æ˜¯è¾“å…¥æ ·æœ¬ä¸­output wordçš„æ¦‚ç‡å¤§å°ã€‚\n-6æ˜¯ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼š\n-6\nåœ¨éšè—å±‚æ²¡æœ‰ä½¿ç”¨ä»»ä½•æ¿€æ´»å‡½æ•°ï¼Œä½†æ˜¯åœ¨è¾“å‡ºå±‚ä½¿ç”¨äº†sotfmaxå‡½æ•°ã€‚\né€šè¿‡æˆå¯¹çš„å•è¯æ¥å¯¹ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒæ ·æœ¬æ˜¯ ( input word, output word ) è¿™æ ·çš„è¯å¯¹ï¼Œinput wordå’Œoutput wordéƒ½æ˜¯one-hotç¼–ç çš„å‘é‡ã€‚æœ€ç»ˆæ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚\n3.4Â éšè—å±‚\nå•è¯çš„ç¼–ç å’Œè®­ç»ƒæ ·æœ¬é€‰å–ä¹‹åï¼Œä¸‹é¢è¯´ä¸‹éšå±‚ã€‚å‡è®¾ç°åœ¨ç”¨300ä¸ªç‰¹å¾æ¥è¡¨ç¤ºä¸€ä¸ªå•è¯ï¼ˆå³æ¯ä¸ªè¯è¢«è¡¨ç¤ºä¸º300ç»´çš„å‘é‡ï¼‰ï¼Œé‚£ä¹ˆéšè—å±‚çš„æƒé‡çŸ©é˜µåº”è¯¥è¡¨ç¤ºä¸º10000è¡Œã€300åˆ—ï¼ˆéšè—å±‚æœ‰300ä¸ªç»“ç‚¹ï¼‰ã€‚\nè¯å‘é‡çš„ç»´åº¦æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒèŠ‚çš„è¶…å‚æ•°ï¼ŒPythonå¼€å‘é‡Œçš„gensimåŒ…ä¸­å°è£…çš„Word2Vecæ¥å£é»˜è®¤è¯å‘é‡å¤§å°ä¸º100ï¼Œ window_sizeä¸º5ã€‚\n-7ä¸­å·¦å³ä¸¤å¼ å­å›¾åˆ†åˆ«ç»™å‡ºäº†ä¸åŒè§’åº¦ä¸‹è¾“å…¥å±‚-éšå±‚çš„æƒé‡çŸ©é˜µè¡¨ç¤ºå½¢å¼ã€‚å·¦å›¾ä¸­æ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ª10000ç»´çš„è¯å‘é‡å’Œéšå±‚å•ä¸ªç¥ç»å…ƒè¿æ¥çš„æƒé‡å‘é‡ã€‚å³çš„å›¾ä¸­æ¯ä¸€è¡Œå®é™…ä¸Šä»£è¡¨äº†æ¯ä¸ªå•è¯çš„è¯å‘é‡ã€‚\n-7\nç»¼åˆæ¥çœ‹ï¼Œè¿™é‡Œçš„ç›®æ ‡æ˜¯å­¦ä¹ è¿™ä¸ªéšå±‚çš„æƒé‡çŸ©é˜µã€‚ç°åœ¨ ç»§ç»­é€šè¿‡æ¨¡å‹çš„å®šä¹‰æ¥è®­ç»ƒè¿™ä¸ªæ¨¡å‹ã€‚\nä¸Šé¢æˆ‘ä»¬æåˆ°ï¼Œé€šè¿‡one-hot encodedå¯¹input wordå’Œoutput wordç¼–ç ã€‚è¿›ä¸€æ­¥æŸ¥çœ‹å¯çŸ¥ï¼Œæœ€åˆçš„è¾“å…¥è¢«one-hotç¼–ç ä»¥åï¼Œé™¤äº†ä¸€ä¸ªä½ç½®ä¸º1å…¶ä½™ç»´åº¦å…¨éƒ¨æ˜¯0ï¼Œæ‰€ä»¥è¿™ä¸ªå‘é‡ç›¸å½“ç¨€ç–ï¼Œä¼šäº§ç”Ÿå¾ˆå¤šæ²¡å¿…è¦çš„è®¡ç®—æˆæœ¬ï¼Œå¤šæ•°è®¡ç®—æ˜¯æ— æ•ˆçš„ã€‚ä¾‹å¦‚ï¼Œå°†ä¸€ä¸ª1 x 10000çš„å‘é‡å’Œ10000 x 300çš„çŸ©é˜µç›¸ä¹˜ï¼Œå®ƒä¼šæ¶ˆè€—ç›¸å½“å¤§çš„è®¡ç®—èµ„æºã€‚æ˜¾ç„¶ï¼Œä¸ºäº†é«˜æ•ˆè®¡ç®—ï¼Œé€‰æ‹©çŸ©é˜µä¸­å¯¹åº”çš„å‘é‡ä¸­ç»´åº¦å€¼ä¸º1çš„ç´¢å¼•è¡Œæ˜¯ä¸ªæ˜æ™ºçš„é€‰æ‹©ï¼Œå°±åƒ-8æ‰€ç¤º\n-8\nä¸Šå›¾ä¸­çš„çŸ©é˜µè¿ç®—ï¼Œå·¦è¾¹åˆ†åˆ«æ˜¯1 x 5å’Œ5 x 3çš„çŸ©é˜µï¼Œç»“æœåº”è¯¥æ˜¯1 x 3çš„çŸ©é˜µï¼ŒæŒ‰ç…§çŸ©é˜µä¹˜æ³•çš„è§„åˆ™ï¼Œç»“æœçš„ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—å…ƒç´ ä¸º0 x 17 + 0 x 23 + 0 x 4 + 1 x 10 + 0 x 11 = 10ï¼ŒåŒç†å¯å¾—å…¶ä½™ä¸¤ä¸ªå…ƒç´ ä¸º12ï¼Œ19ã€‚å¦‚æœ10000ä¸ªç»´åº¦çš„çŸ©é˜µé‡‡ç”¨è¿™æ ·çš„è®¡ç®—æ–¹å¼æ˜¯ååˆ†ä½æ•ˆçš„ã€‚\nä¸ºäº†é«˜æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œè¿™ç§ç¨€ç–çŠ¶æ€ä¸‹ä¸ä¼šè¿›è¡ŒçŸ©é˜µä¹˜æ³•è®¡ç®—ï¼Œæœ€ä¼˜çŸ©é˜µçš„è®¡ç®—çš„ç»“æœå®é™…ä¸Šæ˜¯çŸ©é˜µå¯¹åº”å‘é‡ä¸­å€¼ä¸º1çš„ç´¢å¼•ã€‚è¿™é‡Œï¼Œå·¦è¾¹å‘é‡ä¸­å–å€¼ä¸º1çš„å¯¹åº”çš„ç»´åº¦ä¸º3ï¼ˆä¸‹æ ‡ä»0å¼€å§‹ï¼‰ï¼Œåˆ™è®¡ç®—ç»“æœå°±æ˜¯çŸ©é˜µçš„ç¬¬3è¡Œï¼ˆä¸‹æ ‡ä»0å¼€å§‹ï¼‰â€”â€” [10, 12, 19]ï¼Œè¿™æ ·æ¨¡å‹ä¸­çš„éšå±‚æƒé‡çŸ©é˜µä¾¿æˆäº†ä¸€ä¸ªâ€æŸ¥æ‰¾è¡¨â€œï¼ˆlookup tableï¼‰ï¼Œè¿›è¡ŒçŸ©é˜µè®¡ç®—æ—¶ï¼Œç›´æ¥å»æŸ¥è¾“å…¥å‘é‡ä¸­å–å€¼ä¸º1çš„ç»´åº¦ä¸‹å¯¹åº”çš„é‚£äº›æƒé‡å€¼ã€‚éšè—å±‚çš„è¾“å‡ºå°±æ˜¯æ¯ä¸ªè¾“å…¥å•è¯çš„â€œåµŒå…¥è¯å‘é‡â€ã€‚\n3.5Â è¾“å‡ºå±‚\nç»è¿‡ç¥ç»ç½‘ç»œéšå±‚çš„è®¡ç®—ï¼Œantsè¿™ä¸ªè¯ä¼šä»ä¸€ä¸ª1 x 10000çš„å‘é‡å˜æˆ1 x 300çš„å‘é‡ï¼Œå†è¢«è¾“å…¥åˆ°è¾“å‡ºå±‚ã€‚è¾“å‡ºå±‚æ˜¯ä¸€ä¸ªsoftmaxå›å½’åˆ†ç±»å™¨ï¼Œå®ƒçš„æ¯ä¸ªç»“ç‚¹å°†ä¼šè¾“å‡ºä¸€ä¸ª0-1ä¹‹é—´çš„å€¼ï¼ˆæ¦‚ç‡ï¼‰ï¼Œè¿™äº›æ‰€æœ‰è¾“å‡ºå±‚ç¥ç»å…ƒç»“ç‚¹çš„æ¦‚ç‡ä¹‹å’Œä¸º1ã€‚\n-9æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œè®­ç»ƒæ ·æœ¬ä¸º (input word: â€œantsâ€ï¼Œ output word: â€œcarâ€) çš„è®¡ç®—ç¤ºæ„å›¾ã€‚\n-9\n3.6 åˆ©ç”¨Tensorflowæ¥å¼€å±•Skip-gramæ¨¡å‹\nè¿™é‡Œåˆ†åˆ«ç»™å‡ºSkip-gramæ¨¡å‹çš„æ¦‚å¿µå’Œå®æ–½å±‚é¢çš„å¤§è‡´æ€è·¯å›¾ã€‚-10ç›¸å…³è¯´æ˜ã€-11 æ˜¯æ¦‚å¿µæ¡†æ¶ã€-12æ˜¯å®æ–½æ¡†æ¶æ˜ï¼Œ\n-10\n-11 Skip-gramæ¦‚å¿µæ¨¡å‹\n-12 Skip-gramå®æ–½æ¨¡å‹\n3.6.1Â  æ•°æ®é›†\nè¿™é‡Œé‡‡ç”¨ç”±å‡ ä¸ªç»´åŸºç™¾ç§‘æ–‡ç« ç»„æˆçš„æ•°æ®é›†ï¼Œä¸‹è½½åœ°å€Â Dataã€‚é™äºç¯‡å¹…ï¼Œè¿™é‡Œç»™å‡ºå…³é”®ä»£ç éƒ¨åˆ†ã€‚\nä»£ç å¦‚ä¸‹\nurl = 'http://www.evanjones.ca/software/' def maybe_download(filename, expected_bytes): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" if not os.path.exists(filename): print('Downloading file...') filename, _ = urlretrieve(url + filename, filename) statinfo = os.stat(filename) if statinfo.st_size == expected_bytes: print('Found and verified %s' % filename) else: print(statinfo.st_size) raise Exception( 'Failed to verify ' + filename + '. Can you get to it with a browser?') return filename filename = maybe_download('wikipedia2text-extracted.txt.bz2', 18377035)\n3.6.2Â  ç›¸å…³æ­¥éª¤\nç”¨NLTKå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼›\nå»ºç«‹ç›¸å…³Dictionariesï¼ŒåŒ…æ‹¬wordåˆ°IDã€IDåˆ°wordåŠå•è¯listï¼ˆwordï¼Œfrequencyï¼‰ç­‰ï¼›\nç»™å‡ºæ•°æ®çš„Batchesï¼›\næ˜ç¡®è¶…å‚æ•°ã€è¾“å‡ºæ ·æœ¬ã€è¾“å…¥æ ·æœ¬ã€å‚æ•°åŠå…¶ä»–å˜é‡ï¼›\nè®¡ç®—å•è¯ç›¸ä¼¼æ€§ï¼›\næ¨¡å‹ä¼˜åŒ–ã€æ‰§è¡Œï¼›\nåˆ©ç”¨Â t-SNE Resultsç»™å‡ºå¯è§†åŒ–ç»“æœï¼›\nè€ƒè™‘ç¯‡å¹…é—®é¢˜ï¼Œè¿™é‡Œç»™å‡ºäº§ç”Ÿæ•°æ®Batcheså’Œæ¨¡å‹è¿è¡Œçš„ä»£ç ã€‚\n1ï¼‰ã€Generating Batches of Data for Skip-Gram\ndata_index = 0 def generate_batch_skip_gram(batch_size, window_size): # data_index is updated by 1 everytime we read a data point global data_index # two numpy arras to hold target words (batch) # and context words (labels) batch = np.ndarray(shape=(batch_size), dtype=np.int32) labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # span defines the total window size, where # data we consider at an instance looks as follows. # [ skip_window target skip_window ] span = 2 * window_size + 1 # The buffer holds the data contained within the span buffer = collections.deque(maxlen=span) # Fill the buffer and update the data_index for _ in range(span): buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) # This is the number of context words we sample for a single target word num_samples = 2*window_size # We break the batch reading into two for loops # The inner for loop fills in the batch and labels with # num_samples data points using data contained withing the span # The outper for loop repeat this for batch_size//num_samples times # to produce a full batch for i in range(batch_size // num_samples): k=0 # avoid the target word itself as a prediction # fill in batch and label numpy arrays for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)): batch[i * num_samples + k] = buffer[window_size] labels[i * num_samples + k, 0] = buffer[j] k += 1 # Everytime we read num_samples data points, # we have created the maximum number of datapoints possible # withing a single span, so we need to move the span by 1 # to create a fresh new span buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) return batch, labels print('data:', [reverse_dictionary[di] for di in data[:8]]) for window_size in [1, 2]: data_index = 0 batch, labels = generate_batch_skip_gram(batch_size=8, window_size=window_size) print('\\nwith window_size = %d:' %window_size) print(' batch:', [reverse_dictionary[bi] for bi in batch]) print(' labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\nç›¸åº”è¾“å‡ºä¸º\ndata: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed'] with window_size = 1: batch: ['is', 'is', 'a', 'a', 'concerted', 'concerted', 'set', 'set'] labels: ['propaganda', 'a', 'is', 'concerted', 'a', 'set', 'concerted', 'of'] with window_size = 2: batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted'] labels: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n2ï¼‰ã€Running the Skip-Gram Algorithm\nnum_steps = 100001 skip_losses = [] # ConfigProto is a way of providing various configuration settings # required to execute the graph with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session: # Initialize the variables in the graph tf.global_variables_initializer().run() print('Initialized') average_loss = 0 # Train the Word2vec model for num_step iterations for step in range(num_steps): # Generate a single batch of data batch_data, batch_labels = generate_batch_skip_gram( batch_size, window_size) # Populate the feed_dict and run the optimizer (minimize loss) # and compute the loss feed_dict = {train_dataset : batch_data, train_labels : batch_labels} _, l = session.run([optimizer, loss], feed_dict=feed_dict) # Update the average loss variable average_loss += l if (step+1) % 2000 == 0: if step > 0: average_loss = average_loss / 2000 skip_losses.append(average_loss) # The average loss is an estimate of the loss over the last 2000 batches. print('Average loss at step %d: %f' % (step+1, average_loss)) average_loss = 0 # Evaluating validation set word similarities if (step+1) % 10000 == 0: sim = similarity.eval() # Here we compute the top_k closest words for a given validation word # in terms of the cosine distance # We do this for all the words in the validation set # Note: This is an expensive step for i in range(valid_size): valid_word = reverse_dictionary[valid_examples[i]] top_k = 8 # number of nearest neighbors nearest = (-sim[i, :]).argsort()[1:top_k+1] log = 'Nearest to %s:' % valid_word for k in range(top_k): close_word = reverse_dictionary[nearest[k]] log = '%s %s,' % (log, close_word) print(log) skip_gram_final_embeddings = normalized_embeddings.eval() # We will save the word vectors learned and the loss over time # as this information is required later for comparisons np.save('skip_embeddings',skip_gram_final_embeddings) with open('skip_losses.csv', 'wt') as f: writer = csv.writer(f, delimiter=',') writer.writerow(skip_losses)\nç›¸å…³è¾“å‡ºä¸ºï¼ˆå†…å®¹å¤ªå¤šï¼Œç»™å‡ºä¸­é—´éƒ¨åˆ†æ²¡æœ‰ç½—åˆ—ï¼‰\nInitialized Average loss at step 2000: 3.991611 Average loss at step 4000: 3.627553 Average loss at step 6000: 3.583732 Average loss at step 8000: 3.513987 Average loss at step 10000: 3.492103 Nearest to his: indifferentism, amnesty, ethnographic, sheesh, banking, scot, bran, chamillionaire, Nearest to a: the, manchukuo, an, archipelagos, deaf, fins, communion, â€”, Nearest to it: this, outlying, not, 296, goats, messenger, reconstruct, socialized, Nearest to from: 1933., of, rap, dietitians, and, blanc, agraristas, technicians, Nearest to not: so, although, it, if, but, also, they, even, Nearest to to: sania, prank, would, with, will, place-names, for, subpixels, Nearest to has: had, have, since, is, devÃ­n, marcel, was, dcen, Nearest to .: ,, ;, that, and, the, dhexe, of, vain, Nearest to of: for, in, ,, ., and, with, 's, victors, Nearest to :: ;, newest, ``, three, include, wiesenthal, rexroth, entanglement, Nearest to be: have, billed, welles, pleas, crusaders, preordered, persevered, are, Average loss at step 92000: 3.298772 Average loss at step 94000: 3.252265 Average loss at step 96000: 3.286938 Average loss at step 98000: 3.288883 Average loss at step 100000: 3.275468 Nearest to his: their, 's, work, he, david, her, its, him, Nearest to a: the, an, 's, this, first, spiders, another, pontus, Nearest to it: she, not, he, that, this, said, also, what, Nearest to from: between, in, calvin, during, about, over, up, 1587, Nearest to at: on, counterrevolutionaries, year, after, debacle, malayo-polynesian, broadcast, for, Nearest to 's: the, first, and, a, of, his, was, another, Nearest to with: between, after, while, and, like, from, rascia, qualities, Nearest to ,: ., and, in, of, the, on, ', to, Nearest to this: also, another, it, an, the, a, sinicized, vested, Nearest to not: it, still, so, n't, also, they, that, although, Nearest to to: would, ,, and, matched, of, will, could, uninteresting, Nearest to has: have, had, was, is, since, fifty, arterial, albrecht, Nearest to .: ,, ;, and, ', jefferson, that, ``, of, Nearest to of: ,, in, and, 's, on, album, the, ., Nearest to :: ;, three, two, nematode, one, (, '', ], Nearest to be: have, him, hunua, categorise, hear, billed, sedimentary, tee-shirts,\n3ï¼‰ã€é€šè¿‡t-SNEç»™å‡ºçš„å¯è§†åŒ–ç»“æœå¦‚-13æ‰€ç¤º\n-13\n4ã€ CBOW(Continuous Bag-of-Words)æ¨¡å‹\n4.1 ç®€è¦æ¦‚è¿°\nä¸å¸¦åŠ é€Ÿçš„CBOWæ¨¡å‹æ˜¯ä¸€ä¸ªä¸¤å±‚ç»“æ„ï¼Œç›¸æ¯”äºNPLMæ¥è¯´CBOWæ¨¡å‹æ²¡æœ‰éšå±‚ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ¥é¢„æµ‹ä¸­å¿ƒè¯ï¼Œå¹¶ä¸”æŠ›å¼ƒäº†è¯åºä¿¡æ¯â€”â€”\nè¾“å…¥å±‚ï¼šnä¸ªèŠ‚ç‚¹ï¼Œä¸Šä¸‹æ–‡å…±Â 2mÂ ä¸ªè¯çš„è¯å‘é‡çš„å¹³å‡å€¼ï¼›\nè¾“å…¥å±‚åˆ°è¾“å‡ºå±‚çš„è¿æ¥è¾¹ï¼šè¾“å‡ºè¯çŸ©é˜µÂ Â  ï¼›\nè¾“å‡ºå±‚ï¼š|ğ•|ä¸ªèŠ‚ç‚¹ã€‚ç¬¬Â iÂ ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸­å¿ƒè¯æ˜¯è¯Â  çš„æ¦‚ç‡ã€‚\nå¦‚æœè¦è§†ä½œä¸‰å±‚ç»“æ„çš„è¯ï¼Œå¯ä»¥è®¤ä¸ºâ€”â€”\nè¾“å…¥å±‚ï¼š2mÃ—|ğ•|ä¸ªèŠ‚ç‚¹ï¼Œä¸Šä¸‹æ–‡å…±Â 2mÂ ä¸ªè¯çš„one-hot representation\nè¾“å…¥å±‚åˆ°æŠ•å½±å±‚åˆ°è¿æ¥è¾¹ï¼šè¾“å…¥è¯çŸ©é˜µÂ  Â ï¼›\næŠ•å½±å±‚ï¼šï¼šnä¸ªèŠ‚ç‚¹ï¼Œä¸Šä¸‹æ–‡å…±Â 2mÂ ä¸ªè¯çš„è¯å‘é‡çš„å¹³å‡å€¼ï¼›\næŠ•å½±å±‚åˆ°è¾“å‡ºå±‚çš„è¿æ¥è¾¹ï¼šè¾“å‡ºè¯çŸ©é˜µ Â ï¼›\nè¾“å‡ºå±‚ï¼š|ğ•|Â ä¸ªèŠ‚ç‚¹ã€‚ç¬¬Â iÂ ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸­å¿ƒè¯æ˜¯è¯ çš„æ¦‚ç‡ã€‚\nè¿™æ ·è¡¨è¿°ç›¸å¯¹æ¸…æ¥šï¼Œå°†one-hotåˆ°word embeddingé‚£ä¸€æ­¥æè¿°äº†å‡ºæ¥ã€‚è¿™é‡Œçš„æŠ•å½±å±‚å¹¶æ²¡æœ‰åšä»»ä½•çš„éçº¿æ€§æ¿€æ´»æ“ä½œï¼Œç›´æ¥å°±æ˜¯Softmaxå±‚ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœåªçœ‹æŠ•å½±å±‚åˆ°è¾“å‡ºå±‚çš„è¯ï¼Œå…¶å®å°±æ˜¯ä¸ªSoftmaxå›å½’æ¨¡å‹ï¼Œä½†æ ‡è®°ä¿¡æ¯æ˜¯è¯ä¸²ä¸­å¿ƒè¯ï¼Œè€Œä¸æ˜¯å¤–éƒ¨æ ‡æ³¨ã€‚è§-14\n-14\n4.2Â  Softmax\nä¸å­˜åœ¨éšè—å±‚çš„æƒ…å†µä¸‹ï¼Œå¤„ç†å¤§å‹è¯­æ–™åº“è®¡ç®—é‡æ˜¯ç›¸å½“å¤§çš„ï¼Œè¿™é‡Œçš„è§£å†³æ–¹æ³•ä¹Ÿæ˜¯é€šå¸¸ç”¨çš„çš„é™ç»´æ“ä½œï¼Œå³å±‚æ¬¡Softmaxç®€åŒ–æ“ä½œã€‚ä»¥1000ä¸‡çš„è¯­æ–™åº“æ¥è¯´ï¼Œåªé€‰å–æˆ‘ä»¬éœ€è¦çš„é‚£äº›é‡è¦çš„å•è¯ã€‚è¿™é‡Œä½¿ç”¨å±‚æ¬¡çš„Softmaxæ“ä½œã€‚\nåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¯¹è¦è¾“å‡ºç‰¹å¾åœ¨é¢„æµ‹ç»“æœä¸­çš„é‡è¦æ€§ï¼Œé€‰æ‹©æ ‘æ¨¡å‹è¾ƒå¤šï¼Œå› ä¸ºæ ‘åœ¨èŠ‚ç‚¹é€‰æ‹©åˆ†è£‚æ—¶ï¼Œå°±æ˜¯é€‰æ‹©äº†åŒ…å«ä¿¡æ¯é‡å¤§çš„ç‰¹å¾è¿›è¡Œåˆ†è£‚ï¼Œåœ¨è¿™é‡Œçš„åŸç†ä¹Ÿä¸€æ ·ã€‚è§-15\n-15\nä¹Ÿå°±æ˜¯ï¼Œåœ¨èŠ‚ç‚¹å¤„çš„è¯å‡ºç°çš„é¢‘æ•°æ¯”è¾ƒå¤šã€æ¯”è¾ƒé‡è¦çš„è¯ï¼Œæ¢å¥è¯è®²ï¼Œåœ¨æ„å»ºæ ‘æ—¶èƒ½ä½“ç°è¿™æ ·é¢‘æ¬¡çš„å·®åˆ«ã€‚\næœ€å…¸å‹çš„æ˜¯ä½¿ç”¨èµ«å¤«æ›¼æ ‘ï¼ˆHuffman Treeï¼‰æ¥ç¼–ç è¾“å‡ºå±‚çš„è¯å…¸ï¼Œèµ«å¤«æ›¼æ ‘çš„çš„æƒ³æ³•ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹æ¯ä¸ªè¯æ ¹æ®é¢‘æ¬¡æœ‰ä¸ªæƒé‡ï¼Œç„¶åæ ¹æ®ä¸¤é¢—å­æ ‘è¿›è¡Œæ‹¼æ¥ï¼Œæœ€åç¦»æ ¹èŠ‚ç‚¹è¶Šè¿‘çš„è¯ï¼Œå…¶å‡ºç°çš„é¢‘ç‡ä¼šæ›´é«˜ï¼Œè€Œåœ¨å¶å­èŠ‚ç‚¹å¤„å‡ºç°çš„é¢‘ç‡æ›´ä½ã€‚\nåŸæ¥è®¡ç®—çš„æ˜¯ä¸€ä¸ª1000ä¸‡çš„æ¦‚ç‡å‘é‡ï¼Œç°åœ¨åˆ™å¯ä»¥è½¬æ¢ä¸ºä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼ŒäºŒåˆ†ç±»ä¸­æœ€ç®€å•ç›´æ¥çš„æ˜¯LRæ¨¡å‹ã€‚å‡è®¾\nå°±æ˜¯ä¸€ä¸ªè¯å‘é‡è¡¨ç¤ºï¼Œåœ¨æ¯ä¸ªéå¶å­èŠ‚ç‚¹å¤„ï¼ˆå›¾ä¸­é»„è‰²çš„åœ†ç‚¹ï¼‰è¿˜ä¼šæœ‰ä¸€ä¸ªå‚æ•°\n,é‚£æˆ‘ä»¬çš„åœ¨LRä¸­å¯ä»¥ç”¨ä¸€ä¸ªsigmoidå‡½æ•°æ¥æ˜ å°„ï¼Œæœ€åå¯ä»¥å˜æˆä¸€ä¸ªæœ€å°åŒ–æŸå¤±å‡½æ•°\n4.3 è´Ÿä¾‹é‡‡æ ·\nè´Ÿä¾‹é‡‡æ ·æ“ä½œèµ·æ¥ç®€å•ç²—æš´ï¼Œè¿™é‡Œä¸åšè¿‡å¤šè§£è¯»ï¼Œç½‘ä¸Šæœ‰ä¸å°‘ç›¸å…³æ–‡ç« ã€‚è¿™é‡Œå€¼ç»™å‡ºè´Ÿä¾‹é‡‡æ ·æµç¨‹å›¾ï¼Œè§-16.\n-16\n4.4 CBOWæ¨¡å‹å›¾ç¤º\næ¨¡å‹å›¾ç¤ºå¦‚-17\n-17\n4.5Â  CBOWæ¨¡å‹çš„Tensorflowè¿‡ç¨‹\n4.5.1Â Â Â Â ç›¸å…³æµç¨‹å¦‚ä¸‹ï¼š\næ²¿ç”¨Skip-gramæ¨¡å‹ä½¿ç”¨çš„æ ·æœ¬ï¼›\nå»ºç«‹ä¸€ä¸ªæ–°çš„æ•°æ®Â generatorï¼›\nå®šä¹‰è¶…å‚æ•°ã€è¾“å‡ºæ ·æœ¬ã€è¾“å…¥æ ·æœ¬ã€å‚æ•°åŠå…¶ä»–å˜é‡ï¼›\nè®¡ç®—å•è¯ç›¸ä¼¼æ€§ï¼›\næ¨¡å‹ä¼˜åŒ–ã€æ‰§è¡Œï¼›\nåˆ©ç”¨Â t-SNE Resultsç»™å‡ºå¯è§†åŒ–ç»“æœï¼›\nè¿™é‡Œç»™å‡ºæ–°å»ºæ•°æ®Greneratorå’ŒCBOWæ¨¡å‹æ‰§è¡Œæ ¸å¿ƒä»£ç ã€‚\n4.5.2Â Â å»ºç«‹ä¸€ä¸ªæ–°çš„æ•°æ®Â generator\ndata_index = 0 def generate_batch_cbow(batch_size, window_size): # window_size is the amount of words we're looking at from each side of a given word # creates a single batch # data_index is updated by 1 everytime we read a set of data point global data_index # span defines the total window size, where # data we consider at an instance looks as follows. # [ skip_window target skip_window ] # e.g if skip_window = 2 then span = 5 span = 2 * window_size + 1 # [ skip_window target skip_window ] # two numpy arras to hold target words (batch) # and context words (labels) # Note that batch has span-1=2*window_size columns batch = np.ndarray(shape=(batch_size,span-1), dtype=np.int32) labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # The buffer holds the data contained within the span buffer = collections.deque(maxlen=span) # Fill the buffer and update the data_index for _ in range(span): buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) # Here we do the batch reading # We iterate through each batch index # For each batch index, we iterate through span elements # to fill in the columns of batch array for i in range(batch_size): target = window_size # target label at the center of the buffer target_to_avoid = [ window_size ] # we only need to know the words around a given word, not the word itself # add selected target to avoid_list for next time col_idx = 0 for j in range(span): # ignore the target word when creating the batch if j==span//2: continue batch[i,col_idx] = buffer[j] col_idx += 1 labels[i, 0] = buffer[target] # Everytime we read a data point, # we need to move the span by 1 # to create a fresh new span buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) return batch, labels for window_size in [1,2]: data_index = 0 batch, labels = generate_batch_cbow(batch_size=8, window_size=window_size) print('\\nwith window_size = %d:' % (window_size)) print(' batch:', [[reverse_dictionary[bii] for bii in bi] for bi in batch]) print(' labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\nè¾“å‡ºä¸º\nwith window_size = 1: batch: [['propaganda', 'a'], ['is', 'concerted'], ['a', 'set'], ['concerted', 'of'], ['set', 'messages'], ['of', 'aimed'], ['messages', 'at'], ['aimed', 'influencing']] labels: ['is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at'] with window_size = 2: batch: [['propaganda', 'is', 'concerted', 'set'], ['is', 'a', 'set', 'of'], ['a', 'concerted', 'of', 'messages'], ['concerted', 'set', 'messages', 'aimed'], ['set', 'of', 'aimed', 'at'], ['of', 'messages', 'at', 'influencing'], ['messages', 'aimed', 'influencing', 'the'], ['aimed', 'at', 'the', 'opinions']] labels: ['a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n4.5.3 CBOWæ¨¡å‹æ‰§è¡Œ\nnum_steps = 100001 cbow_losses = [] # ConfigProto is a way of providing various configuration settings # required to execute the graph with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session: # Initialize the variables in the graph tf.global_variables_initializer().run() print('Initialized') average_loss = 0 # Train the Word2vec model for num_step iterations for step in range(num_steps): # Generate a single batch of data batch_data, batch_labels = generate_batch_cbow(batch_size, window_size) # Populate the feed_dict and run the optimizer (minimize loss) # and compute the loss feed_dict = {train_dataset : batch_data, train_labels : batch_labels} _, l = session.run([optimizer, loss], feed_dict=feed_dict) # Update the average loss variable average_loss += l if (step+1) % 2000 == 0: if step > 0: average_loss = average_loss / 2000 # The average loss is an estimate of the loss over the last 2000 batches. cbow_losses.append(average_loss) print('Average loss at step %d: %f' % (step+1, average_loss)) average_loss = 0 # Evaluating validation set word similarities if (step+1) % 10000 == 0: sim = similarity.eval() # Here we compute the top_k closest words for a given validation word # in terms of the cosine distance # We do this for all the words in the validation set # Note: This is an expensive step for i in range(valid_size): valid_word = reverse_dictionary[valid_examples[i]] top_k = 8 # number of nearest neighbors nearest = (-sim[i, :]).argsort()[1:top_k+1] log = 'Nearest to %s:' % valid_word for k in range(top_k): close_word = reverse_dictionary[nearest[k]] log = '%s %s,' % (log, close_word) print(log) cbow_final_embeddings = normalized_embeddings.eval() np.save('cbow_embeddings',cbow_final_embeddings) with open('cbow_losses.csv', 'wt') as f: writer = csv.writer(f, delimiter=',') writer.writerow(cbow_losses)\nè¾“å‡ºéƒ¨åˆ†ï¼ˆå†…å®¹å¤šï¼Œç»™å‡ºéƒ¨åˆ†è¾“å‡ºç»“æœï¼‰\nInitialized Average loss at step 2000: 3.571350 Average loss at step 4000: 3.077189 Average loss at step 6000: 2.951976 Average loss at step 8000: 2.868055 Average loss at step 10000: 2.796585 Nearest to with: nederlandse, abbr, canaan, ars, brochure, ester, comte, rubberized, Nearest to other: nuclear, 1419, restore, some, teammates, unrepresented, mothe, both, Nearest to '': hungarorum, paws, }, updrafts, hatta, dementing, ca, titled, Nearest to to: schmitz, norwood, would, bleak, 1855., arsonists, inferring, athletics, Nearest to on: ascendancy, dynamo, mcqueen, guyana, csc, on-die, tennessee, a.f.c., ssicism, mimesis, resting, actually, Nearest to had: have, has, having, carboxylic, remained, dunkirk, leikin, checker, Nearest to ;: ., ,, â€”, expressionistic, conch, commutes, furry, venation, Average loss at step 92000: 2.102314 Average loss at step 94000: 2.097561 Average loss at step 96000: 2.110340 Average loss at step 98000: 2.095927 Average loss at step 100000: 2.108117 Nearest to with: toxteth, 21., 102,800, dubuque, mid-to-late, vernal, validate, abbr, Nearest to other: woolshed, various, top-loading, pia, dye, these, zwan, teammates, Nearest to '': divining, titled, subfreezing, dementing, Ğ²Ğ¾Ğ¹Ğ½Ğ°, word, neubrandenburg, hiphop, Nearest to to: bleak, might, schmitz, helped, towards, norwood, will, must, Nearest to on: upon, ascendancy, 27th, csc, rotherhithe, feces, cheer, muskogee, Nearest to ): e.g, orlÃ©ans, mazzola, driving, metres, nightfall, 687., 2,341, Nearest to his: her, their, my, its, osiris, xor, intestines, digitally, Nearest to this: it, 98.6, similar, citation, tyresÃ¶, budgetary, marge, these, Nearest to one: glens, enforcing, recovered, 400-series, claudius, rejoin, surviving, smiling, Nearest to which: where, but, whom, whose, âˆ’50, stolen, moneda, polarization, Nearest to not: n't, rarely, never, vegetative, still, opt, sardis, spouses, Nearest to but: however, though, which, although, especially, whereas, including, footprints, Nearest to were: are, lech, was, sintering, been, re-evaluation, unclassified, symbolizing, Nearest to also: now, still, billionaires, cleveland, neo-classicism, resting, ductile, zemin, Nearest to had: having, has, have, dunkirk, downplayed, carboxylic, hancock, remained, Nearest to ;: ., ,, â€”, conch, expressionistic, :, commutes, storing,\näº”ã€å°ç»“\nè¯åµŒå…¥å·²ç»æˆä¸ºè®¸å¤šNLPä»»åŠ¡ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¹¶ä¸”è¢«å¹¿æ³›åœ°åº”ç”¨äºè¯¸å¦‚æœºå™¨ç¿»è¯‘ã€èŠå¤©æœºå™¨äººã€å›¾åƒæ ‡é¢˜ç”Ÿæˆå’Œè¯­è¨€å»ºæ¨¡ç­‰ä»»åŠ¡ã€‚è¯åµŒå…¥ä¸ä»…æ˜¯ä¸€ç§é™ç»´æŠ€æœ¯ï¼ˆä¸one-hot encodedç›¸æ¯”ï¼‰ï¼Œè€Œä¸”ä¹Ÿåœ¨ç‰¹å¾è¡¨ç¤ºæ–¹é¢æ¯”å…¶ä»–æŠ€æœ¯æ›´åŠ å®ç”¨ã€‚æœ¬æ–‡é‡ç‚¹è®¨è®ºäº†åŸºäºç¥ç»ç½‘ç»œçš„ä¸¤ç§å­¦ä¹ è¯è¡¨ç¤ºçš„æ–¹æ³•ï¼Œå³Skip-gramæ¨¡å‹å’ŒCBOWæ¨¡å‹ã€‚\né¦–å…ˆï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸€äº›ä¼ ç»Ÿæ–¹æ³•ï¼Œå¯¹äºè¯è¡¨ç¤ºæˆ–è€…è¯æ„ä¹‰æœ‰äº†æ›´å¤šçš„ç†è§£ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å®ç”¨äº†WordNetã€å»ºç«‹äº†ä¸€äº›å•è¯çš„å…±ç°çŸ©é˜µã€è®¡ç®—äº†TF-IDFå€¼ï¼Œå¹¶è®¨è®ºäº†è¿™äº›ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚\nä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œä¿ƒä½¿æˆ‘ä»¬ç»§ç»­æ¢ç´¢åŸºäºç¥ç»ç½‘ç»œçš„è¯è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚qingk\næœ€åï¼Œæˆ‘ä»¬é‡ç‚¹è®¨è®ºäº†æŸå¤±å‡½æ•°ã€Skip-gramæ¨¡å‹å’ŒCBOWæ¨¡å‹å„è‡ªåŸç†æƒ…å†µï¼Œå¹¶é€šè¿‡Tensorflowæ–¹æ³•ä¸€ä¸€åšäº†æ¨¡å‹å®æ–½ï¼Œç»™å‡ºäº†æœ€åçš„è¾“å‡ºç»“æœï¼Œè®©æˆ‘ä»¬å¯¹äºä¸¤ä¸ªæ¨¡å‹æœ‰æ›´åŠ æ·±å…¥çš„ç†è§£ã€‚ä¸‹ä¸€æ­¥å°†é‡ç‚¹æ¢è®¨ä¸‹SKip-gramã€CBOWæ¯”è¾ƒåŠè‘—åçš„Global Vectors ï¼ˆGloVeï¼‰æ¨¡å‹ã€‚\nä¸»è¦å‚è€ƒèµ„æ–™Â ã€ŠNatural Language ProcessingÂ with TensorFlowã€‹ï¼ˆThushan Ganegedaraï¼›Â 2018 Packt Publishingï¼›First published: May 2018ï¼‰\néƒ¨åˆ†å‚è€ƒèµ„æ–™\nä»SNEåˆ°t-SNEå†åˆ°LargeVisã€‹\nã€Šä»ã€Œç¾Šå¹´çš„ç¾Šåˆ°åº•æ˜¯å“ªç§ç¾Šï¼Ÿã€è°ˆä¸Šä½è¯ã€‹\nè¯åµŒå…¥ï¼ˆword2vecï¼‰-CBOWåŸç†é€šä¿—è§£é‡Š"}
