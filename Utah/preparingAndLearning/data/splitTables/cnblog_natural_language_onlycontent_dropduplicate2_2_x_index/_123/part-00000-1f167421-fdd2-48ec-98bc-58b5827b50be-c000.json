{"content2":"CoreNLP\n斯坦福大学出品的基于Java的全栈自然语言处理工具，CoreNLP还提供了一套文本标注工具，对文本标注流程做了一些规范。CoreNLP提供了6种使用最广泛的语言（阿拉伯、汉语、英语、法语、德语、西班牙语）的词库。\nGithub\n官方文档\nApache OpenNLP\n基于Java的自然语言处理全栈工具，它提供了API和命令行两种接口。官网地址：http://opennlp.apache.org/\n初学自然语言处理，了解一些NLP全栈工具的功能是很好的。\n语言检测（Language Detect）：发现给定文本是哪种语言\nOpenNLP的语言检测功能基于莱比锡数据集。\n莱比锡位于德国东部的莱比锡盆地中央，在魏塞埃尔斯特河与普莱塞河的交汇处，面积141平方公里，人口约60万，是原东德的第二大城市。莱比锡数据集（Leipzig Corpora Collection）是为自然语言处理提供文本数据，项目发起者为莱比锡大学计算机学院。它实际上只提供两个数据集：（1）各种语言的按年份列出的文章；（2）用于情感分析的标注数据集。http://wortschatz.uni-leipzig.de/en/download\n句子检测（Sentence Detect）:把一篇文章拆分成若干个句子\n汉语中，直接通过标点符号就够了。英语中的“.”具有多种含义，例如每个缩写词后面都跟着一个“.”\n分词（Tokenizer）\n命名实体识别（Name Finder）\n文档分类 （Document Categorizer）\n语义标注（Part of Speech Tagger）\n提取主干（Lemmatizer）：对于单词可以提取词干，这个功能由Stemmer完成；对于句子可以进行缩句，这个功能由Lemaatizer完成\n分层分段（Chunker）：给定一篇文章，按照意思把文章分成若干段落或者把一段分成若干层。\n句法分析（Parser）\n指代消解（Coreference Resolution）\nNLP4j\nNLP4j的前身是clearNLP，是一个Java实现的自然语言处理库。\nGithub\nHanLP\n和CoreNLP、OpenNLP是同类产品，基于Java、全栈自然语言处理工具包。不需要学习CoreNLP、OpenNLP，直接学习HanLP足够了。\npolyglot\n多语言、全栈、纯Python的自然语言处理库：https://github.com/aboSamoor/polyglot\nTokenization (165 Languages)\nLanguage detection (196 Languages)\nNamed Entity Recognition (40 Languages)\nPart of Speech Tagging (16 Languages)\nSentiment Analysis (136 Languages)\nWord Embeddings (137 Languages)\nMorphological analysis (135 Languages)\nTransliteration (69 Languages)\npattern\npattern是一个基于Python的web挖掘模块，功能包括：\n爬虫，包括HTTP请求模块和HTML解析模块，内置爬取谷歌、Twitter、维基等）\n自然语言处理：part-of-speech taggers, n-gram search, sentiment analysis, WordNet\n机器学习：machine learning (vector space model, clustering, SVM)\n网络分析：network analysis\n可视化\nhttps://pypi.org/project/pattern3/3.0.0/\nTextBlob\nTextBlob是一个纯Python写的全栈自然语言处理库。\n名词词组抽取：Noun phrase extraction\n语义标注：Part-of-speech tagging\n情感分析：Sentiment analysis，情感分析是文本分类的一种特殊问题，可以说是最常见的文本分类问题。\n文本分类：Classification (Naive Bayes, Decision Tree)\n翻译：Language translation and detection powered by Google Translate，TextBlob的翻译通过网络请求访问Google，并没有提供翻译的实现。\n分词：Tokenization (splitting text into words and sentences)\n词、词组频率统计：Word and phrase frequencies\n语法分析：Parsing\nn元语法模型：n-grams\n词语映射，如寻找词根：Word inflection (pluralization and singularization) and lemmatization\n拼写纠错：Spelling correction\n便于扩展：Add new models or languages through extensions\n集成了WordNet：WordNet integration\n官方文档\nsnowNLP\nSnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。\n该库功能包括：分词、标注、情感分析。snowNLP提供的功能是TextBlob的子集。\nhttps://github.com/isnowfy/snownlp\nnlpir\n中科院出品的基于C++实现的自然语言全站工具，提供Java、C#等多种语言的封装。主要功能包括中文分词；英文分词；词性标注；命名实体识别；新词识别；关键词提取；支持用户专业词典与微博分析。NLPIR系统支持多种编码、多种操作系统、多种开发语言与平台。\nhttp://ictclas.nlpir.org/\nGithub\nNLTK\n老牌的Python全栈自然语言处理库。\nAllenNLP\nAllenNLP是纯Python（只提供Python3.6以上版本的包）、基于Pytorch实现的全栈自然语言处理包，过去的自然语言处理包大都是传统方法，AllenNLP中包含了大量的深度学习方法。AllenNLP是由Allen机构发布的，Allen是一个AI研究机构。\nGithub地址\nAllenNLP官网\nspaCy\nspaCy是基于Python和Cython的高效、商业化、支持45种以上语言的自然语言处理工具包。该库对汉语支持略显不足。\nGithub\nMontyLingua\n支持Python和Java两种语言、只针对英语的自然语言处理库。http://alumni.media.mit.edu/~hugo/montylingua/\nNiuTrans\n东北大学朱靖波实验室的基于统计的机器翻译模型：http://www.niutrans.com/niutrans/NiuTrans.ch.html\nfoolNLTK\n国人开发，基于Python\ngensim\n功能包括词向量和文档主题发现。\nglove\nC++实现的词向量工具，词向量生成有三种方式：glove、cbow、skip-gram。\nfasttext\n提供了词向量工具和文本分类功能。\n分词器\njieba：最流行的Python分词器\njieba-analysis：Java版的jieba\nansj：孙健的分词器\nIKAnalyzer：常见于Solr和Lucene\nmmseg4j\njcseg\n斯坦福福瓷器\npkuseg北大分词器\nFudanNLP：复旦分词器\npaoding：老牌分词器\nsmartcn\nictclas：中科院基于HMM的分词器\nnlpir：中科院分词器\nsmallseg\nsnailseg\nthulac：清华分词器\nltp：哈工大分词器\nbosen波森\n波森是一个web服务，提供自然语言处理全栈服务。明明是国产，非要起个外国名字。\nhttps://bosonnlp.com/about\n自然语言处理的库非常丰富，质量却也良莠不齐。许多库的作者都是一个人开发的，维护也不到位。"}
