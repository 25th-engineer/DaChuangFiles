{"content2":"2019年上半年收集到的人工智能自然语言处理方向干货文章\n自然语言（NLP）发展史及相关体系\n读了这篇文字，做年薪百万的NLP工程师\n聚焦机器“读、写、说、译”，探寻NLP未来之路\nNLP接下来黄金十年-----周明等谈值得关注的NLP技术\n人工智能科普｜自然语言处理（NLP）\n为什么要学习NLP\nAI研究员收集NLP数据的四种创意方法（大牛分享）\n自然语言处理中注意力机制综述\n8个方法解决90％的NLP问题\n周明：NLP进步将如何改变搜索体验\n赋能行业发展，NLP如何避免走入“死胡同”？\n中文的NLP\n什么样的NLP库，可以支持53种语言？\n万字长文概述NLP中的深度学习技术（上）\nNLP接下来黄金十年-----周明等谈值得关注的NLP技术\n读了这篇文字，做年薪百万的NLP工程师\nNLP中的词向量及其应用\n为什么NLP相对来说这么困难？\n8种优秀预训练模型大盘点，NLP应用so easy！\n让机器听懂人话的\"自然语言处理技术\"究竟神奇在哪里？\n动态记忆网络：向通用 NLP 更近一步\n【精读】自然语言处理基础之RNN\n纯干货|目前看到的BERT比较透彻的文章，强烈推荐\nBert时代的创新：Bert应用模式比较及其它\n为何BERT在 NLP 中的表现如此抢眼？\nBERT面向语言理解的深度双向变换预训练\nBERT大火却不懂Transformer？读这一篇就够了\n1亿参数4万样本BERT仍听不懂人话，我们离通用NLP能还有多远？\nFlair：一款简单但技术先进的NLP库！\nTensorflow实现的深度NLP模型集锦（附资源）\n深度学习：自然语言处理（五）NLTK的经典应用\n阿里自然语言处理部总监分享：NLP技术的应用及思考\n现有模型还「不懂」自然语言：20多位研究者谈NLP四大开放性问题\n学界 | 和清华大学自然语言处理与社会人文计算实验室一起读机器翻译论文\n2019-06-23 写于苏州市"}
