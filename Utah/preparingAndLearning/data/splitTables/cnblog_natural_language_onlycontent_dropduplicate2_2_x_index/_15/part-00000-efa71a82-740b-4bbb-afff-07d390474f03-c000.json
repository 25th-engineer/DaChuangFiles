{"content2":"1 — 语言处理综合工具包（暂未完善）\n工具包名\n支持语言\n受欢迎程度\n简介\n个人使用评价\nHanLP\npyhanlp\n中文\n1.3W star\nHanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。\nNLTK\n多语言\n7.8K star\nNLTK是一个被广泛使用的高效的Python构建的平台，用来处理人类自然语言数据。它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如WordNet）。\nLTP语言技术平台\n中文\n2.2K star\nLTP提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。\nStanford CoreNLP\n多语言\n6.3K star\n斯坦福CoreNLP是一个Java自然语言分析库，它集成了所有的自然语言处理工具，包括词性的终端（POS）标注器，命名实体识别（NER），分析器，对指代消解系统，以及情感分析工具，并提供英语分析的模型文件。\nspaCy\n多语言，中文支持有限\n1.34W star\nspaCy 是一个Python自然语言处理工具包，诞生于2014年年中，号称“Industrial-Strength Natural Language Processing in Python”，是具有工业级强度的Python NLP工具包。spaCy里大量使用了 Cython 来提高相关模块的性能，这个区别于学术性质更浓的Python NLTK，因此具有了业界应用的实际价值。\ngensim\n多语言\n9.3K star\nGensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。\n它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，\n支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口\n2 — 分词\n工具包\n官方简介\n个人评测\njieba结巴中文分词\n“结巴”中文分词：做最好的 Python 中文分词组件\n1、使用人数多（github star1.8万），速度快;\n2、虽然准确率和召回率较其他新出分词工具有差距，但是各方面的综合效果还是不错的\npkuseg北京大学开源分词工具\npkuseg简单易用，支持细分领域分词，有效提升了分词准确度。\n1、会自动去除空格等空字符；\n2、分词粒度较细（例如“北京  工业   大学”）；\n3、速度较jieba慢很多（粗略评测速度是jieba的约1/5）\nHanLP\nHanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。\n1、使用人数多（github star1.2万），速度约jieba的1/3;\n2、综合处理工具包\n3 —词向量\n资源名称\n简介\n使用评估\nChinese Word Vectors 中文词向量\n100+ Chinese Word Vectors 上百种预训练中文词向量（个人只使用了mixed-large）\n1、词汇量128万, 300维度，大小3.4G；\n2、词长度分布情况（1:2:3:4:其他）：1.4:27:32:11:28.6；\n腾讯高质量词向量\n腾讯AI Lab此次公开的中文词向量数据包含800多万中文词汇，其中每个词对应一个200维的向量\n1、词汇量882万，200维度，大小15.5G；\n2、词长度分布情况（1:2:3:4:其他）：0.25:23:23:22.4:31.35；\n3、缺少中文标点逗号等部分标点字符的支持。\n5 — 语料库\n4.1 实体数据集\n数据集\n数据集描述\nMSRA数据集\n30个实体属性，4.6万行，126.5万词，11.8万个实体"}
