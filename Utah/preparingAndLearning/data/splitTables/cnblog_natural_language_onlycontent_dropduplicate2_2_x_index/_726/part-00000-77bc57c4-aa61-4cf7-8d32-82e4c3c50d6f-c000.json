{"content2":"自然语言处理：单词计数\n这一讲主要内容（Today):\n1、语料库及其性质；\n2、Zipf 法则；\n3、标注语料库例子；\n4、分词算法；\n一、 语料库及其性质：\na) 什么是语料库（Corpora）\ni. 一个语料库就是一份自然发生的语言文本的载体，以机器可读形式存储；\nii. 一种平衡语料库尝试在语言或者其他领域具有代表性；\nb) 译者注：平行语料库与平衡语料库的特点与区别\ni. 平行语料库通常是由双语或多语的对应语料构成，常常是翻译文本构成。例如：Babel English-Chinese Parallel Corpus。平行语料库常被用做对比和翻译研究之用。\nii. 平衡语料库（balanced corpus）主要是指其语料的取样上是均衡的，有代表性的。这种语料可以用作得出有关某种语言特性的一般性的结论。例如：Lancaster Corpus of Mandarin Chinese以及Academia Sinica Balanced Corpus of Modern Chinese\nc) 单词计数（Word Counts）\ni. 在文本中最常见的单词是哪些?\nii. 在文本中有多少个单词?\niii. 在大规模语料库中单词分布的特点是什么?\nd) 我们以马克吐温的《汤姆•索耶历险记》为例：\n单词(word)　　频率（Freq)　　用法(Use)\nthe　　　　　　3332　　　　　determiner (article)\nand　　　　　　2972　　　　　conjunction\na　　　　　　　1775　　　　　determiner\nto　　　　　　　1725　　　　　preposition, inf. marker\nof　　　　　　　1440　　　　　preposition\nwas　　　　　　1161　　　　　auxiliary verb\nit　　　　　　　1027　　　　　pronoun\nin　　　　　　　906　　　　　preposition\nthat　　　　　　877　　　　　complementizer\nTom　　　　　　678　　　　　proper name\ni. 一些观察结果（Some observations）：\n1. 虚词占了大多数；\n2. 语料库依赖的主题词也占了一部分，例如\"Tom\"\nii. 思考：是否有可能建立一个真正具有“代表性”的英文样本语料库?\ne) 这个例句里有多少个单词：\nThey picnicked by the pool, then lay back on the grass and looked at the stars.\ni. “型”(Type) ——语料库中不同单词的数目，词典容量\nii. “例”(Token) — 语料中总的单词数目（total number of words in a corpus）\niii. 注：以上定义参考自《自然语言处理综论》\niv. 汤姆•索耶历险记（Tom Sawyer）中有：\n1. 词型（word types） — 8, 018\n2. 词例（word tokens）— 71, 370\n3. 平均频率（average frequency）— 9（注：词例/词型）\nf) 词频的频率（Frequencies of Frequencies）：\n词频（Word Frequency）　词频的频率(Frequency of Frequency)\n1　　　　　　　　　　　　　3993\n2　　　　　　　　　　　　　1292\n3　　　　　　　　　　　　　664\n4　　　　　　　　　　　　　410\n5　　　　　　　　　　　　　243\n6　　　　　　　　　　　　　199\n7　　　　　　　　　　　　　172\n8　　　　　　　　　　　　　131\n9　　　　　　　　　　　　　82\n10　　　　　　　　　　　　 91\n11-50　　　　　　　　　　 540\n51-100　　　　　　　　　　99\n大多数词在语料库中仅出现一次（Most words in a corpus appear only once）!\n二、 齐夫定律(Zipf’s Law)\na) 在任何一个自然语言里第n个最常用的单词的频率与n近似成反比。\nb) 齐夫定律表示频率(f)与排名®的关系如下：\nf #= 1/r（注：这里不能使用公式编辑器，近似表示）\nc) 存在一个常量k表示如下：\nf* r = k\nd) 汤姆•索耶中的齐夫定律（Zipf’s Law in Tom Sawye）\n单词（word） 频率（Freq.(f)） 排名（Rank (r)） f ∗ r\nthe　　　　　3332　　　　　　1　　　　　　　　3332\nand　　　　　2972　　　　　　2　　　　　　　　5944\na　　　　　　1775　　　　　　3　　　　　　　　5235\nhe　　　　　 877　　　　　　10 　　　　　　　8770\nbut　　　　　410　　　　　　 20　　　　　　　 8400\nbe　　　　　 294　　　　　　 30　　　　　　　 8820\nthere　　　　222　　　　　　 40　　　　　　　 8880\none　　　　　172　　　　　　50　　　　　　　 8600\nabout　　　　158　　　　　　60　　　　　　　 9480\nnever　　　　124　　　　　　80　　　　　　　 9920\nOh　　　　　 116　　　　　　90　　　　　　　 10440\ne) 译者注：补充说明——Wiki中的齐夫定律\ni. 从根本上讲, 齐夫定律可以表述为在自然语言的语料库里, 一个单词出现的频率与它在频率表里的排名成反比。所以,频率最高的单词出现的频率大约是出现频率第二位的单词的2倍，而出现频率第二位的单词则是出现频率第四位的单词的2倍。这个定律被作为任何与power law probability distributions有关的事物的参考。 这个\"定律\"是哈佛大学的语言学家George Kingsley Zipf 发表的。\nii. 比如，在Brown 语库中，\"the\" 是最常见的单词，它在这个语库中出现了大约7%（10万单词中出现69971次）。正如齐夫定律中所描述的一样，出现次数为第二位的单词\"of\"占了整个语库中的3.5% (36411次), 之后的是\"and\" (28852次)。仅仅 135 个字汇就占了Brown 语库的一半。\niii. 齐夫定律是一个实验定律，而非理论定律。齐夫分布可以在很多现象中被观察到。齐夫分布的在现实中的起因是一个争论的焦点。齐夫定律很容易用点阵图观察，坐标为log(排名)和log(频率)。比如，\"the\"用上述表述可以描述为x = log(1), y = log(69971)的点。如果所有的点接近一条直线，那么它就遵循齐夫定律。最简单的齐夫定律的例子是 \"1/f function\"。给出一组齐夫分布的频率，按照从最常见到非常见排列，第二常见的频率是最常见频率的出现次数的1/2。第三常见的频率是最常见的频率的1/3。 第n常见的频率是最常见频率出现次数的1/n。然而，这并不精确，因为所有的项必须出现一个整数次数，一个单词不可能出现2.5次。然而，在一个广域范围内并且做出适当的近似，许多自然现象都符合齐夫定律。\nf) 齐夫定律和省力原则\ni. 人类行为和省力原则：\n1. “... Zipf argues that he found a unifying principle, the Principle of Least Effort, which underlies essentially the entire human condition (the book even includes some questionable remarks on human sexuality!). The principle argues that people will act so as to minimize their probable average rate of work”. (Manning&Schutze, p.23)\nii. 注：北京大学姜望琪老师的《Zipf与省力原则》讲得很好，部分摘录如下：\n1. 省力原则(the Principle of Least Effort)，又称经济原则(the Economy Principle)，可以概括为：以最小的代价换取最大的收益。这是指导人类行为的一条根本性原则。在现代学术界，第一个明确提出这条原则的是美国学者 George Kingsley Zipf。\n2. George Kingsley Zipf1902年1月出生于一个德裔家庭（其祖父十九世纪中叶移居美国)。1924年，他以优异成绩毕业于哈佛学院。1925年在德国波恩、柏林学习。1929年完成Relative Frequency as a Determinant of Phonetic Change，获得哈佛比较语文学博士学位。然后，他开始在哈佛教授德语。1931年与Joyce Waters Brown结婚。1932年出版Selected Studies of the Principle of Relative Frequency in Language。1935年出版The Psycho- Biology of Language：An Introduction to Dynamic Philology。1939年被聘为讲师。1949年出版Human Behavior and the Principle of Least Effort：An Introduction to Human Ecology。1950年9月因患癌症病逝。\n3. Zipf在1949年的书里提出了一条指导人类行为的基本原则——省力原则。Zipf在序言里指出，如果我们把人类行为纯粹看作一种自然现象，如果我们像研究蜜蜂的社会行为、鸟类的筑巢习惯一样研究人类行为，那么，我们就有可能揭示其背后的基本原则。这是他提出“省力原则”的大背景。当Zipf在众多互不相干的现象里都发现类似Zipf定律的规律性以后，他就开始思考造成这种规律性的原因。这是导致他提出“省力原则”的直接因素。在开始正式论证以前，Zipf首先澄清了“省力原则”的字面意义。第一，这是一种平均量。一个人一生要经历很多事情，他在一件事情上的省力可能导致在另一件事情上的费力。反过来，在一件事情上的费力，又可能导致在另一件事情上的省力。第二，这是一种概率。一个人很难在事先百分之百地肯定某种方法一定能让他省力，他只能有一个大概的估计。因为用词研究是理解整个言语过程的关键，而后者又是理解整个人类生态学的关键，他的具体论证从用词经济开始。Zipf认为，用词经济可以从两个角度来讨论：说话人的角度和听话人的角度。从说话人的角度看，用一个词表达所有的意义是最经济的。这样，说话人不需要花费气力去掌握更多的词汇，也不需要考虑如何从一堆词汇中选择一个合适的词。这种“单一词词汇量”就像木工的一种多用工具，集锯刨钻锤于一身，可以满足多种用途。但是，从听话人角度看，这种“单一词词汇量”是最费力的。他要决定这个词在某个特定场合到底是什么意思，而这几乎是不可能的。相反，对听话人来说，最省力的是每个词都只有一个意义，词汇的形式和意义之间完全一一对应。这两种经济原则是互相冲突、互相矛盾的。Zipf把它们叫做一条言语流中的两股对立的力量：“单一化力量”（the Force of Unification）和“多样化力量”（the Force of Diversification）。他认为，这两股力量只有达成妥协，达成一种平衡，才能实现真正的省力。事实正像预计的那样。请看Zipf的论证：假如只有单一化力量，那么任何语篇的单词数量（number）都会是1，而它的出现次数（frequency）会是100%。另一方面，假如只有多样化力量，那么每个单词的出现次数都会接近1，而单词总数量则由语篇的长度决定。这就是说， number和frequency是衡量词汇平衡程度的两个参数。\ng) 其他规律（Other laws）：\ni. 词义分布（Word sense distribution）；\nii. 音位分布（Phonemes distribution）；\niii. 词共现模式（Word co-occurrence patterns）；\nh) 近似服从齐夫定律的例子（Examples of collections approximately obeying Zipf’s law）：\ni. 访问网页的频率（Frequency of accesses to web pages）；\nii. 居住点的规模（Sizes of settlements）；\niii. 个人收入的分布（Income distribution amongst individuals）；\niv. 地震的大小（Size of earthquakes）；\nv. 演奏中的音乐符号（Notes in musical performances）；\n三、 语料库相关\na) 数据稀疏问题（Sparsity）\ni. “kick”在一百万单词中出现的次数?——58\nii. “kick a ball”在一百万单词中出现的次数?——0\niii. “kick”在web中出现了多少?——6M\niv. “kick a ball”在web中出现了多少?——8.000\nv. 数据永远不会嫌多\nb) 非常非常大的数据\ni. Brill&Banko 2001：在混合集合消歧任务中通过增加数据规模的方法进行训练所得到的结果比在标准训练语料上训练的最好系统的结果好很多\n1. 任务（Task）：对“too,to”这样的词对进行歧义消除\n2. 训练规模(Training Size)：从一百万词到10亿词不等\n3. 用于对比的学习算法：winnow算法，感知器算法，决策树算法\nii. Lapata&Keller 2002, 2003：web可用做非常非常大的语料库（the web can be used as a very very large corpus）\n1. 计数可能被噪音干扰，但是对于一些任务这不是什么大问题（The counts can be noisy, but for some tasks this is not an issue）\nc) 布朗语料库(The Brown Corpus)\ni. 著名的早期语料库（Famous early corpus） (Made by Nelson Francis and Henry Kucera at Brown University in the 1960s)\n1. 一个关于美国书面语的平衡语料库（A balanced corpus of written American English），包括报纸，小说，非小说，学术等体裁（Newspaper, novels, non-fiction, academic）\n2. 一百万单词数，500份文本（1 million words, 500 written texts）\n3. 你认为这是一个大型语料库吗（Do you think this is a large corpus）?\nii. 注，关于布朗语料库更详细的介绍：\n1. 20世纪60年代，Francis和Kucera在美国Brown大学建立了世界上第一个根据系统性原则采集样本的标准语料库——布朗语料库。\n2. 主要目的是研究当代美国英语\n3. 按共时原则采集文本的语料库，只选录1961年间由美国人撰写出版的普通语体的文本。\n4. 规模为100万词次，全部语料分成15种体裁，共500个样本，每个样本不少于2000词次。\n5. TAGGIT系统：词类标记81种，正确率达77%\n6. 语料分A-R共18种类型，A-J属于资讯类语体，K-R属于想象类语体\n例：A 报刊：新闻报道；B 报刊：社论…\n7. 样本通过随机采样方法得到。首先从各类体裁目录中按样本数要求随机选出进入语料库的文本，然后从选出的文本中随机截取不少于2000词次的片断作为样本，采样时要保证最后一个句子是完整的\n8. 版本：A,B,C,卑尔根I,卑尔根II,布朗MARC\n9. 布朗语料库从语料库的整体规模，语料的分布和语料的采样上都经过了精心的设计，一致被公认为是一个能反映语言共性的平衡语料库。\nd) 近年来的语料库（Recent Corpora）\n语料库(Corpus)　规模（Size）　领域（Domain）　语言（Language）\nNA News Corpus 600 million 　　newswire　　　American English\nBritish National Corpus 100 million balanced 　　British English\nEU proceedings　　20 million　　　legal　　　　　10 language pairs\nPenn Treebank　　2 million　　　newswire　　　American English\nBroadcast News　　　　　　　　　spoken　　　　7 languages\nSwitchBoard　　　2.4 million　　　spoken　　　American English\nii. 了解更多语料库的信息，请查询语言数据联盟（For more corpora, check the Linguistic Data Consortium）：\nhttp://www.ldc.upenn.edu/\ne) 语料库内容（Corpus Content）\ni. 类型（Genre）：\n– 新闻，小说，广播，会话（newswires, novels, broadcast, spontaneous conversations）\nii. 媒介（Media）：文本，音频，视频（text, audio, video）\niii. 标注（Annotations）：tokenization, 句法树（syntactic trees）, 语义（semantic senses）, 翻译（translations）\nf) 标注例子（Example of Annotations）: 词性标注（POS Tagging）\ni. 词性标注集对简单的语法功能编码（POS tags encode simple grammatical functions）\nii. 几个词性标注集(Several tag sets):\n1. Penn tag set (45 tags)\n2. Brown tag set (87 tags)\n3. CLAWS2 tag set (132 tags)\niii. 举例:\nCategory　　　　　　　Example　　　Claws c5　　Brown　　Penn\nAdverb　　　　　　　 often, badly　　　AJ0　　　　JJ　　　　JJ\nNoun singular　　　　 table, rose　　　 NN1　　　NN　　　　NN\nNoun plural　　　　　 tables, roses　　　NN2　　　NN　　　　NN\nNoun proper singular　Boston, Leslie　　NP0　　　NP　　　　NNP\ng) 标注中的问题（Issues in Annotations）\ni. 同样的认为不同的标注方案很正常（Different annotation schemes for the same task are common）\nii. 在某些情况下，方案之间有直接的映射关系；在其他情况下，它们并没有显示出任何关系（In some cases, there is a direct mapping between schemes; in other cases, they do not exhibit any regular relation）\niii. 标注的选择是由语言，计算和/或任务需要驱动的（Choice of annotation is motivated by the linguistic, the computational and/or the task requirements）\n四、 分词相关\na) Tokenization\ni. 目标（Goal）：将文本切分成单词序列（divide text into a sequence of words）\nii. 单词指的是一串连续的字母数字并且其两端有空格；可能包含连字符和撇号但是没有其它标点符号（Word is a string of contiguous alphanumeric characters with space on either side; may include hyphens and apostrophes but no other punctuation marks (Kucera and Francis)）\niii. Tokenizatioan 容易吗（Is tokenization easy）?\nb) 什么是词?\ni. English:\n1. “Wash. vs wash”\n2. “won’t”, “John’s”\n3. “pro-Arab”, “the idea of a child-as-required-yuppie-possession must be motivating them”, “85-year-old grandmother”\nii. 东亚语言（East Asian languages）:\n1. 词之间没有空格\nc) 分词（Word Segmentation）\ni. 基于规则的方法: 基于词典和语法知识的形态分析\nii. 基于语料库的方法: 从语料中学习\niii. 需要考虑的问题: 覆盖面，歧义，准确性（coverage, ambiguity, accuracy）\nd) 统计切分方法的动机（Motivation for Statistical Segmentation）\ni. 未登录词问题（Unknown words problem）:\n——存在领域术语和专有名词（presence of domain terms and proper names）\nii. 语法约束可能不充分（Grammatical constrains may not be sufficient）\n——例子（Example）: 名词短语的交替切分（alternative segmentation of noun phrases）\niii. 举例一\n1. Segmentation：sha-choh/ken/gyoh-mu/bu-choh\n2. Translation：“president/and/business/general/manager”\niv. 举例二\n1. Segmentation：sha-choh/ken-gyoh/mu/bu-choh\n2. Translation：“president/subsidiary business/Tsutomi[a name]/general manag\ne) 一个切分算法：\ni. 核心思想（Key idea）: 对于每一个候选边界，比较这个边界邻接的n元序列的频率和跨过这个边界的n元序列的频率。\nii. 注：由于公式编辑问题，具体算法请自行参考lec02.pdf，此处略。\nf) 实验框架（Experimental Framework）\ni. 语料库（Corpus）: 150兆1993年Nikkei新闻语料（150 megabytes of 1993 Nikkei newswire）\nii. 人工切分（Manual annotations）: 用于开发集的50条序列（调节参数）和用于测试集的50条序列（50 sequences for development set (parameter tuning) and 50 sequences for test set）\niii. 基线算法（Baseline algorithms）: Chasen和Juma的形态分析器\ng) 评测方法（Evaluation Measures）\ni. tp — true positive （真正, TP）被模型预测为正的正样本；\nii. fp — false positive （假正, FP）被模型预测为正的负样本；\niii. tn — true negative （真负 , TN）被模型预测为负的负样本 ；\niv. fn — false negative （假负 , FN）被模型预测为负的正样本；\nv. 准确率（Precision） — the measure of the proportion of selected items that the system got right：\nP = tp / ( tp + fp)\nvi. 召回率（Recall） — the measure of the target items that the system selected:\nR = tp / ( tp + fn )\nvii. F值（F-measure）:\nF = 2 ∗ PR / (R + P)\nviii. Word precision (P) is the percentage of proposed brackets that match word-level brackets in the annotation;\nix. Word recall (R) is the percentage of word-level brackets that are proposed by the algorithm.\n五、 结论（Conclusions）\na) 语料库被广泛用于文本处理中（Corpora widely used in text processing）\nb) 使用的语料库是熟语料或生语料（Corpora used either annotated or raw）\nc) 齐夫定律及其与自然语言的联系（Zipf’s law and its connection to natural language）\nd) 数据稀疏问题是语料库处理方法中的一个主要问题（Sparsity is a major problem for corpus processing methods）"}
