{"content2":"7.3   Developing and Evaluating Chunkers   开发和评价分块器\nNow you have a taste of what chunking does, but we haven't explained how to evaluate chunkers. As usual, this requires a suitably annotated corpus. We begin by looking at the mechanics of converting IOB format into an NLTK tree, then at how this is done on a larger scale using a chunked corpus. We will see how to score the accuracy of a chunker relative to a corpus, then look some more data-driven（数据驱动的） ways to search for NP chunks. Our focus throughout will be on expanding the coverage of a chunker.\nReading IOB Format and the CoNLL 2000 Corpus\n读取IOB格式和CoNLL2000语料库\nUsing the corpora module we can load Wall Street Journal text that has been tagged then chunked using the IOB notation. The chunk categories provided in this corpus are NP, VP and PP. As we have seen, each sentence is represented using multiple lines, as shown below:\nhe PRP B-NP\naccepted VBD B-VP\nthe DT B-NP\nposition NN I-NP\n...\nA conversion function chunk.conllstr2tree() builds a tree representation from one of these multi-line strings. Moreover, it permits us to choose any subset of the three chunk types to use, here just for NP chunks:\n>>> text = '''\n... he PRP B-NP\n... accepted VBD B-VP\n... the DT B-NP\n... position NN I-NP\n... of IN B-PP\n... vice NN B-NP\n... chairman NN I-NP\n... of IN B-PP\n... Carlyle NNP B-NP\n... Group NNP I-NP\n... , , O\n... a DT B-NP\n... merchant NN I-NP\n... banking NN I-NP\n... concern NN I-NP\n... . . O\n... '''\n>>> nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()\nWe can use the NLTK corpus module to access a larger amount of chunked text. The CoNLL 2000 corpus contains 270k words of Wall Street Journal text, divided into \"train\" and \"test\" portions, annotated with part-of-speech tags and chunk tags in the IOB format. We can access the data using nltk.corpus.conll2000. Here is an example that reads the 100th sentence of the \"train\" portion of the corpus:\n>>> from nltk.corpus import conll2000\n>>> print conll2000.chunked_sents('train.txt')[99]\n(S\n(PP Over/IN)\n(NP a/DT cup/NN)\n(PP of/IN)\n(NP coffee/NN)\n,/,\n(NP Mr./NNP Stone/NNP)\n(VP told/VBD)\n(NP his/PRP$ story/NN)\n./.)\nAs you can see, the CoNLL 2000 corpus contains three chunk types: NP chunks, which we have already seen; VP chunks such as has already delivered; and PP chunks such as because of. Since we are only interested in the NP chunks right now, we can use the chunk_types argument to select them:\n>>> print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]\n(S\nOver/IN\n(NP a/DT cup/NN)\nof/IN\n(NP coffee/NN)\n,/,\n(NP Mr./NNP Stone/NNP)\ntold/VBD\n(NP his/PRP$ story/NN)\n./.)\nSimple Evaluation and Baselines 简单的评价和基准\nNow that we can access a chunked corpus, we can evaluate chunkers. We start off（开始） by establishing a baseline for the trivial（不重要的） chunk parser cp that creates no chunks:\n>>> from nltk.corpus import conll2000\n>>> cp = nltk.RegexpParser(\"\")\n>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n>>> print cp.evaluate(test_sents)\nChunkParse score:\nIOB Accuracy: 43.4%\nPrecision:      0.0%\nRecall:         0.0%\nF-Measure:     0.0%\nThe IOB tag accuracy indicates that more than a third of the words are tagged with O, i.e. not in an NP chunk. However, since our tagger did not find any chunks, its precision, recall, and f-measure are all zero. Now let's try a naive regular expression chunker that looks for tags beginning with letters that are characteristic of noun phrase tags (e.g. CD, DT, and JJ).\n>>> grammar = r\"NP: {<[CDJNP].*>+}\"\n>>> cp = nltk.RegexpParser(grammar)\n>>> print cp.evaluate(test_sents)\nChunkParse score:\nIOB Accuracy: 87.7%\nPrecision:     70.6%\nRecall:        67.8%\nF-Measure:     69.2%\nAs you can see, this approach achieves decent（相当好的） results. However, we can improve on it by adopting a more data-driven approach, where we use the training corpus to find the chunk tag (I, O, or B) that is most likely for each part-of-speech tag. In other words, we can build a chunker using a unigram tagger (Section 5.4). But rather than trying to determine the correct part-of-speech tag for each word, we are trying to determine the correct chunk tag, given each word's part-of-speech tag.\nIn Example 7.8, we define the UnigramChunker class, which uses a unigram tagger to label sentences with chunk tags. Most of the code in this class is simply used to convert back and forth（反复地） between the chunk tree representation used by NLTK's ChunkParserI interface, and the IOB representation used by the embedded tagger. The class defines two methods: a constructor which is called when we build a new UnigramChunker; and the parse method which is used to chunk new sentences.\nclass UnigramChunker(nltk.ChunkParserI):\ndef __init__(self, train_sents):\ntrain_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\nfor sent in train_sents]\nself.tagger = nltk.UnigramTagger(train_data)\ndef parse(self, sentence):\npos_tags = [pos for (word,pos) in sentence]\ntagged_pos_tags = self.tagger.tag(pos_tags)\nchunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\nconlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\nin zip(sentence, chunktags)]\nreturn nltk.chunk.conlltags2tree(conlltags)\nExample 7.8 (code_unigram_chunker.py)\nThe constructor expects a list of training sentences, which will be in the form of chunk trees. It first converts training data to a form that suitable for training the tagger, using tree2conlltags to map each chunk tree to a list of word,tag,chunk triples. It then uses that converted training data to train a unigram tagger, and stores it in self.tagger for later use.\nThe parse method takes a tagged sentence as its input, and begins by extracting the part-of-speech tags from that sentence. It then tags the part-of-speech tags with IOB chunk tags, using the tagger self.tagger that was trained in the constructor. Next, it extracts the chunk tags, and combines them with the original sentence, to yield conlltags. Finally, it uses conlltags2tree to convert the result back into a chunk tree.\nNow that we have UnigramChunker, we can train it using the CoNLL 2000 corpus, and test its resulting performance:\n>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n>>> train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n>>> unigram_chunker = UnigramChunker(train_sents)\n>>> print unigram_chunker.evaluate(test_sents)\nChunkParse score:\nIOB Accuracy: 92.9%\nPrecision:     79.9%\nRecall:        86.8%\nF-Measure:     83.2%\nThis chunker does reasonably well, achieving an overall f-measure score of 83%. Let's take a look at what it's learned, by using its unigram tagger to assign a tag to each of the part-of-speech tags that appear in the corpus:\n>>> postags = sorted(set(pos for sent in train_sents\n...                      for (word,pos) in sent.leaves()))\n>>> print unigram_chunker.tagger.tag(postags)\n[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'),\n(',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),\n('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),\n('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),\n('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),\n('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),\n('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),\n('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),\n('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),\n('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\nIt has discovered that most punctuation marks occur outside of NP chunks, with the exception of # and $, both of which are used as currency markers（货币标志.）. It has also found that determiners (DT) and possessives (PRP$ and WP$) occur at the beginnings of NP chunks, while noun types (NN, NNP, NNPS, NNS) mostly occur inside of NP chunks.\nHaving built a unigram chunker, it is quite easy to build a bigram chunker: we simply change the class name to BigramChunker, and modify line in Example 7.8 to construct a BigramTagger rather than a UnigramTagger. The resulting chunker has slightly higher performance than the unigram chunker:\n>>> bigram_chunker = BigramChunker(train_sents)\n>>> print bigram_chunker.evaluate(test_sents)\nChunkParse score:\nIOB Accuracy: 93.3%\nPrecision:     82.3%\nRecall:        86.8%\nF-Measure:     84.5%\nTraining Classifier-Based Chunkers 训练基于分类器的分块器\nBoth the regular-expression based chunkers and the n-gram chunkers decide what chunks to create entirely based on part-of-speech tags. However, sometimes part-of-speech tags are insufficient to determine how a sentence should be chunked. For example, consider the following two statements:\n(3)\na.\nJoey/NN sold/VBD the/DT farmer/NN rice/NN ./.\nb.\nNick/NN broke/VBD my/DT computer/NN monitor/NN ./.\nThese two sentences have the same part-of-speech tags, yet they are chunked differently. In the first sentence, the farmer and rice are separate chunks, while the corresponding material in the second sentence, the computer monitor, is a single chunk. Clearly, we need to make use of information about the content of the words, in addition to just their part-of-speech tags, if we wish to maximize chunking performance.\nOne way that we can incorporate（合并） information about the content of words is to use a classifier-based tagger to chunk the sentence. Like the n-gram chunker considered in the previous section, this classifier-based chunker will work by assigning IOB tags to the words in a sentence, and then converting those tags to chunks. For the classifier-based tagger itself, we will use the same approach that we used in Section 6.1 to build a part-of-speech tagger.\nThe basic code for the classifier-based NP chunker is shown in Example 7.9. It consists of two classes. The first class is almost identical（同样的）to the ConsecutivePosTagger class from Example 6.5. The only two differences are that it calls a different feature extractor and that it uses a MaxentClassifier rather than a NaiveBayesClassifier. The second class is basically a wrapper（包装器） around the tagger class that turns it into a chunker. During training, this second class maps the chunk trees in the training corpus into tag sequences; in the parse() method, it converts the tag sequence provided by the tagger back into a chunk tree.\nclass ConsecutiveNPChunkTagger(nltk.TaggerI):\ndef __init__(self, train_sents):\ntrain_set = []\nfor tagged_sent in train_sents:\nuntagged_sent = nltk.tag.untag(tagged_sent)\nhistory = []\nfor i, (word, tag) in enumerate(tagged_sent):\nfeatureset = npchunk_features(untagged_sent, i, history)\ntrain_set.append( (featureset, tag) )\nhistory.append(tag)\nself.classifier = nltk.MaxentClassifier.train(\ntrain_set, algorithm='megam', trace=0)\ndef tag(self, sentence):\nhistory = []\nfor i, word in enumerate(sentence):\nfeatureset = npchunk_features(sentence, i, history)\ntag = self.classifier.classify(featureset)\nhistory.append(tag)\nreturn zip(sentence, history)\nclass ConsecutiveNPChunker(nltk.ChunkParserI):\ndef __init__(self, train_sents):\ntagged_sents = [[((w,t),c) for (w,t,c) in\nnltk.chunk.tree2conlltags(sent)]\nfor sent in train_sents]\nself.tagger = ConsecutiveNPChunkTagger(tagged_sents)\ndef parse(self, sentence):\ntagged_sents = self.tagger.tag(sentence)\nconlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\nreturn nltk.chunk.conlltags2tree(conlltags)\nExample 7.9 (code_classifier_chunker.py):\nThe only piece left to fill in is the feature extractor. We begin by defining a simple feature extractor which just provides the part-of-speech tag of the current token. Using this feature extractor, our classifier-based chunker is very similar to the unigram chunker, as is reflected in its performance:\n>>> def npchunk_features(sentence, i, history):\n...     word, pos = sentence[i]\n...     return {\"pos\": pos}\n>>> chunker = ConsecutiveNPChunker(train_sents)\n>>> print chunker.evaluate(test_sents)\nChunkParse score:\nIOB Accuracy: 92.9%\nPrecision:     79.9%\nRecall:        86.7%\nF-Measure:     83.2%\nWe can also add a feature for the previous part-of-speech tag. Adding this feature allows the classifier to model interactions between adjacent（邻近的） tags, and results in a chunker that is closely related to the bigram chunker.\n>>> def npchunk_features(sentence, i, history):\n...     word, pos = sentence[i]\n...     if i == 0:\n...         prevword, prevpos = \"<START>\", \"<START>\"\n...     else:\n...         prevword, prevpos = sentence[i-1]\n...     return {\"pos\": pos, \"prevpos\": prevpos}\n>>> chunker = ConsecutiveNPChunker(train_sents)\n>>> print chunker.evaluate(test_sents)\nChunkParse score:\nIOB Accuracy: 93.6%\nPrecision:     81.9%\nRecall:        87.1%\nF-Measure:     84.4%\nNext, we'll try adding a feature for the current word, since we hypothesized that word content should be useful for chunking. We find that this feature does indeed improve the chunker's performance, by about 1.5 percentage points (which corresponds to about a 10% reduction in the error rate).\n>>> def npchunk_features(sentence, i, history):\n...     word, pos = sentence[i]\n...     if i == 0:\n...         prevword, prevpos = \"<START>\", \"<START>\"\n..."}
