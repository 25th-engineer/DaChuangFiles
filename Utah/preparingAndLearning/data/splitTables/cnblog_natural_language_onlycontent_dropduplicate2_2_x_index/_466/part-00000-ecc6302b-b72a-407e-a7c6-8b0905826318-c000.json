{"content2":"在自然语言处理领域中，很多问题需要将单词映射到实数向量空间再进行计算。\n最经典的方法就是one-hot编码，但是其假设默认两个词之间是独立无关的，并且具有稀疏性，会带来维度灾难。\n向量空间模型 Vector space models 将语义近似的词汇被映射为相邻的数据点，它基于一种分布假设，其核心思想是：在相同的上下文中出现的单词具有相似的语义。采用这一假设的研究方法主要分为两类：基于计数的方法（如LSA）和预测方法（如神经概率语言模型）。\n基于计数的方法计算某词汇与其邻近词汇在一个大型语料库中共同出现的频率及其他统计量，然后将这些统计量映射到一个小型且稠密的向量中。\n预测方法则试图直接从某词汇的邻近词汇对其进行预测，在此过程中利用已经学习到的小型且稠密的嵌套向量。\nWord2vec是一种预测模型，它有两种变体：连续词袋模型（CBOW）及Skip-Gram模型。\nCBOW根据目标词汇的上下文词汇来预测目标词汇，而Skip-Gram模型做法相反，它通过目标词汇来预测该词汇的上下文词汇。\nSkip-Gram模型采取CBOW的逆过程的动机在于：CBOW算法对于很多分布式信息进行了平滑处理（例如将一整段上下文信息视为一个单一观察量）。很多情况下，对于小型的数据集，这一处理是有帮助的。相形之下，Skip-Gram模型将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。\n在每一次迭代过程（step，或者说是parameters update）中，\n使用CBOW，则每一个上下文h中，我们都需要计算并标准化字典V中的每一个单词的得分。\n使用Skip-Gram，则对于每一个单词w，我们都需要计算字典V中的每一个单词属于 w的上下文 的概率。\n计算量巨大。\n负采样算法 negative sampling 在噪声分布（比如Unigram Distribution）中，选择k个单词，作为干扰词汇。这时，\n之前的 |V|分类的softmax问题就转变成了k+1个二分类问题logistic regression。在极限情况下，使用负采用的更新方法接近于softmax更新权重的效果\n这时，对于一个真实的word，最大化其值，对于k个噪声，最小化其值。\n每次迭代计算代价从O(n)降低到了O(k)\n损失函数：NCE_loss\nNoise-Contrastive Estimation\n参考：\nTensorFlow Tutorials: Vector Representations of Words"}
