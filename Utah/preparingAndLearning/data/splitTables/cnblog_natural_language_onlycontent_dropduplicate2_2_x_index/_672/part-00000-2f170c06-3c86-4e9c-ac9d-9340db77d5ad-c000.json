{"content2":"背景\n最近接触到了一些NLP方面的东西，感觉还蛮有意思的，本文写一下分词技术。分词是自然语言处理的基础，如果不采用恰当的分词技术，直接将一个一个汉字输入，不仅时间复杂度会非常高，而且准确度不行。比如：“东北大学”若直接拆分，会和“北大”相关联，但其实没有意义。\n有没有英文分词？\n西方文字天然地通过空格来将句子分割成词语，因此一般不需要分词。但是东方文字往往没有天然形成的分隔符，因此需要将中文进行分词。\n中文分词的理论基础\n目前中文分词都是基于三种方法：基于词典的方法、基于统计的方法、基于机器学习的方法。\n基于词典的方法\n该方法的基础很容易理解，就是实现给定一个词库，然后通过某种匹配手段将文本和词库里边的词进行匹配，从而实现分词的效果。最常见的匹配手段是最大正向匹配，该方法顾名思义，就是从左到右依次扫描，将能够匹配到的最长的词作为一个分出来的单词。该方法的明显缺点是会产生歧义。例如：“南京市长江大桥”会被分成“南京市长/江/大桥”。\n鉴于此状况，又有学者提出了最大逆向匹配，就是反过来从右到左进行匹配，如“南京市长江大桥”就会被分割为“南京市/长江大桥”。这是正确的。汉语中偏正结构的语法较多，总体上逆向匹配的正确率更高点。\n另外还有一种方法叫做双向匹配法，就是把上述两种方法一起用。如果正向和反向的分词结果一样，那就认为是正确的，否则再选取一些规则重新判别。\n基于词典的方法，优点在于速度快，简单易于理解。但是缺点在于只能解决有限程度上的歧义，而且如果词库过大，则歧义更为严重。\n基于统计的方法\n该方法的目的是为了解决歧义的。\n该方法首先将文本全分割，也就是将文本的所有可能的分割方法全部穷尽，然后构造一个无环图。\n然后计算从开始到结束那条路的概率最大，那么哪条路就是分词结果。计算概率的方法是：\n对于一个中文字符串“a1a2a3...an”如何正确的用词语c1,c2..cm表示就是中文分词的任务，也就是说我们要去找寻P(c1c2..cm)最大的分词，按照马尔科夫链的想法就是说我们就是求P(c1)*P(c1|c2)*P(c1c2|c3)*...P(c1c2...cm-1|cm)最大。按照阿卡姆剃刀的想法我们可以假设一个最可能的实现，于是google黑板报的假设就是每个词只跟前面的词有关，于是变为求P(c1)*P(c1|c2)*P(c2|c3)*...P(cm-1|cm)最大。进一步的其实我们可以假设每个词都是相对独立的，也就是求P(c1)*P(c2)*...P(cm)最大。\n——来源于http://www.isnowfy.com/python-chinese-segmentation/\n从上可以看出，该方法也需要一个词库，优点是可以避免歧义的出现，缺点是时间复杂度太高，计算量很大。特别是对于长文本，全分割的种类数太多太多了。\n基于机器学习的方法\n基于机器学习的方法，将文本和已经人工标注过的分词结果输入到训练器中进行训练，训练好模型后，直接输入文本便可以输出分词结果了。一般采用的训练模型是HMM或者CRF。\nHMM\n隐形马尔科夫。\nCRF\n条件随机场。"}
