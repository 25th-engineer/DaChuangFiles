{"content2":"CMUSphinx系列目录\nhttp://www.cnblogs.com/yin52133/archive/2012/06/21/2557219.html - （一）基本运行测试 http://www.cnblogs.com/yin52133/archive/2012/07/12/2587282.html - （二）自然语言处理原理研究 http://www.cnblogs.com/yin52133/archive/2012/07/12/2587419.html - （三）小范围语音英文识别 http://www.cnblogs.com/yin52133/archive/2012/07/12/2588201.html - （四）小范围语音中文识别 http://www.cnblogs.com/yin52133/archive/2012/06/22/2558806.html - （五）错误调试 http://www.cnblogs.com/yin52133/archive/2012/07/12/2588418.html - （六）我的目标和几个想像的方案（闲置中）\n这几天忙完了一阵子，打算继续研究下语音识别\n文中都是根据自己的简单测试得到的结论，可能有很多错误，发现问题，欢迎一起讨论\n先说一下一般的语音识别流程；以下内容很多都是看自吴军博士的《数学之美》这本书\nsphinx语音识别其实是基于统计语言模型的\n它主要靠language model（lm），Hidden Markov Model（hmm）模型识别语音。\n其中lm模型是统计语料得到的模型，语料就是用于训练的文本库，Dic里面保存的就是训练用的语料库里出现过的语料和对应的发音\n而lm模型里存的是语料的组合概率，\n先设p（w1）是word w1在文章中出现的概率\n//很显然有数据的情况下这个很好统计，遍历就行了\nP（w1，w2）w1.w2是连续出现的概率\nP（w2|w1）是已知W1已出现的情况下w2\n假设识别sentence的概率用P（S）表示\nP（S）=P（w1，w2，...wn） 表示单词集w1，w2，。。。wn连续出现并生成S的概率\n使用条件概率公式S 把整个公式替换成\nP（sentence） = P（w1）*P（w2|w1）*P（w3|w2）。。。P（wn|w1，w2.。。wn-1）\n再用马尔科夫假设精简成\nP（sentence） = P（w1）*P（w2|w1）*P（w3|w2）。。。P（wn|wn-1）的问题\n而\nP（wi|wi-1） = P（wi-1，wi） / P（wi）\n然后P(wi,|wi-1)和p（wi）都可以从语料统计出来\n最终就能得到P（sentence）。而lm模型存储的就是这些p（wi-1，wi）这些概率统计值\n实际识别就是算出Max{P(sentence)}的过程\n然后是hmm 隐马尔科夫模型\n这个模型先不说具体原理，你就知道是声音信号到文字之间映射统计表\n因为同样发音下可能也有很多类似的单词，这时候就靠声音的组合来判断识别的颗粒度和识别了。\n用hmm识别的时候肯定会调用lm模型，因为最终的识别就是声音道文字，先靠hmm模型的统计值计算出最可能出现的一些集合，\n再根据lm模型在这些集合之间计算出概率最高的集合，而那个集合就是输出的语句了。\n有兴趣的可以去看《数学之美》或者其他相关书籍\n然后再回到pocketSphinx上，我们可以知道识别一条语音至少需要\n*.dic *.lm(或者*.DMP) 还有hmm模型\n其中 *.dic是整个语料库，包含了改系统能支持的所有单词和对应的发音\n*.lm就是统计语言模型\nhmm是隐马尔科夫模型、信号到实际数据的识别  这先不要管\n我认为的识别流程应该是这样的\nhmm依赖lm和dic\nlm和dic是靠txt生成的\n然后hmm对我们录制的音频流，再使用dic和lm识别\n而训练需要的分别是wav和语料库\n大量的wav用于使系统更准确的弄出\n实际音频到理论音频数据的（也就是训练hmm）\n而大量的语料库是用于训练lm\n建立统计模型（训练语言模型）\n举个例子\n整句和单词举个例子就是\ndic里面\nopen 发音 close 发音 door 发音 button 发音\n这样的话\n识别单词是\n识别出来close；door；button；open什么的就靠dic发音映射表，对照判断\n而你要识别连续的单词、如open door ，close button就需要靠大量数据或者手动的修改语言模型\n还是open close door button这例子\n你在lm里把句首 open close 出现的概率提高（接近100%）\n然后 door button在句首出现的概率降低到 （接近 0%）\n然后在第二个位置的概率弄反\n这样的话、他的语法结构就能接近 (open|close) (door|button)\n这种形式，可以变相提高整句的识别能力\n就是说一句话一个语音流输入它的库可能会把它分离成小的单词流\n采样后和语音库（*.dic）匹配\n然后第一个有几个备选，再给这些备选+第二个（第二个又是概率里面拿）\n得到模糊的串经过语言统计模型然后返回总体可能性最大的。\n所以我们最关键就是准确的单词发音映射表还有统计语言模型（*.lm）\n而统计语言模型这需要大量的样例数据获得更为准确的概率，也能进一步获取更为准确的结果"}
