{"content2":"1. 前言\n自然语言处理是关毅老师的研究生课程。\n本博客仅对噪声信道模型、n元文法（N-gram语言模型）、维特比算法详细介绍。\n其他的重点知识还包括概率上文无关文法（PCFG）、HMM形式化定义、词网格分词等等，比较简单，不做赘述。\n2. 噪声信道模型\n2.1 噪声信道模型原理\n噪声信道模型的示意图如下所示：\n该模型的目标是通过有噪声的输出信号试图恢复输入信号，依据贝叶斯公式，其计算公式如下所示：\n\\[I = \\arg \\max _ { I } P ( I | O ) = \\arg \\max _ { I } \\frac { P ( O | I ) P ( I ) } { P ( O ) } = \\arg \\max _ { I } P ( O | I ) P ( I )\\]\n\\(I\\)指输入信号，\\(O\\)指输出信号。\n噪声模型的优点是具有普适性，通过修改噪声信道的定义，可以将很多常见的应用纳入到这一模型的框架之中，相关介绍见2.1。\n2.2 噪声信道模型的应用\n2.2.1 语音识别\n语音识别的目的是通过声学信号，找到与其对应的置信度最大的语言文本。\n计算公式与上文相同，此时的\\(I\\)为语言文本，\\(O\\)为声学信号。\n代码实现过程中，有一个信息源以概率\\(P(I)\\)生成语言文本，噪声信道以概率分布\\(P(O|I)\\)将语言文本转换为声学信号。\n模型通过贝叶斯公式对后验概率\\(P(I|O)\\)进行计算。\n2.2.2 其他应用\n手写汉字识别\n文本 -> 书写 -> 图像\n文本校错\n文本 -> 输入编辑 -> 带有错误的文本\n音字转换\n文本 -> 字音转换 -> 拼音编码\n词性标注\n词性标注序列 -> 词性词串替换 -> 词串\n3. N-gram语言模型\n3.1 N-gram语言模型原理\nN-gram语言模型基于马尔可夫假设，即下一个词的出现仅仅依赖于他前面的N个词，公式如下：\n\\[P ( S ) = P \\left( w _ { 1 } w _ { 2 } \\dots w _ { n } \\right) = p \\left( w _ { 1 } \\right) p \\left( w _ { 2 } | w _ { 1 } \\right) p \\left( w _ { 3 } | w _ { 1 } w _ { 2 } \\right) \\ldots p \\left( w _ { n } | w _ { 1 } w _ { 2 } \\dots w _ { n - 1 } \\right)\\]\n实践中，往往采用最大似然估计的方式进行计算：\n\\[P \\left( w _ { n } | w _ { 1 } w _ { 2 } \\dots w _ { n - 1 } \\right) = \\frac { C \\left( w _ { 1 } w _ { 2 } \\ldots w _ { n } \\right) } { C \\left( w _ { 1 } w _ { 2 } \\dots w _ { n - 1 } \\right) }\\]\n在训练语料库中统计获得字串的频度信息。\nn越大: 对下一个词出现的约束性信息更多，更大的辨别力\nn越小: 在训练语料库中出现的次数更多，更可靠的统计结果，更高的可靠性\n3.2 平滑处理\n如果不进行平滑处理，会面临数据稀疏的问题，这会使联合概率的其中一项值为0，从而导致句子的整体概率值为0。\n3.2.1 加一平滑法（拉普拉斯定律）\n公式如下：\n\\[P _ { L a p } \\left( w _ { 1 } w _ { 2 } , \\ldots w _ { n } \\right) = \\frac { C \\left( w _ { 1 } w _ { 2 } \\dots w _ { n } \\right) + 1 } { N + B } , \\left( B = | V | ^ { n } \\right)\\]\n实际运算时，\\(N\\)为条件概率中先验字串的频度。\n3.2.2 其他平滑方法\nLidstone定律\nGood-Turing估计\nBack-off平滑\n4. 维特比算法\n4.1 维特比算法原理\n维特比算法用于解决HMM三大问题中的解码问题，即给定一个输出字符序列和HMM模型参数，如何确定模型产生这一序列概率最大的状态序列。\n\\[\\arg \\max _ { X } P ( X | O ) = \\arg \\max _ { X } \\frac { P ( X , O ) } { P ( O ) } = \\arg \\max _ { X } P ( X , O )\\]\n\\(O\\)是输出字符序列，\\(X\\)是状态序列。\n维特比算法迭代过程如下：\n初始化\n\\[\\begin{array} { l } { \\delta _ { 1 } ( i ) = \\pi _ { i } b _ { i } \\left( o _ { 1 } \\right) } \\\\ { \\psi _ { 1 } ( i ) = 0 } \\end{array}\\]\n递归\n\\[\\begin{array} { c } { \\delta _ { t + 1 } ( j ) = \\underset { 1 \\leq i \\leq N } \\max \\delta _ { t } ( i ) a _ { i j } b _ { j } \\left( o _ { t + 1 } \\right) } \\\\ { \\psi _ { t + 1 } ( j ) = \\underset { 1 \\leq i \\leq N } { \\arg \\max } \\delta _ { t } ( i ) a _ { i j } b _ { j } \\left( o _ { t + 1 } \\right) } \\end{array}\\]\n结束\n\\[\\begin{array} { c } { P ^ { * } = \\max _ { 1 \\leq i \\leq N } \\delta _ { T } ( i ) } \\\\ { q _ { T } ^ { * } = \\underset { 1 \\leq i \\leq N } { \\arg \\max } \\delta _ { T } ( i ) } \\end{array}\\]\n最优路径（状态序列）\n\\[q _ { t } ^ { * } = \\psi _ { t + 1 } \\left( q _ { t + 1 } ^ { * } \\right) , \\quad t = T - 1 , \\ldots , 1\\]\n上述迭代过程，\\(a\\)状态转移矩阵，\\(b\\)是状态-输出发射矩阵。\n4.2 维特比算法例子\n例子：\n计算过程：\n第一次迭代（此时的输出字符为A）：\n\\[\\delta _ { 1 } ( 0 ) = 0.5*0.5=0.25 \\]\n\\[\\delta _ { 1 } ( 1 ) = 0.5*0.3=0.15 \\]\n\\[\\delta _ { 1 } ( 2 ) = 0*0.2=0 \\]\n第二次迭代（此时的输出字符为B）：\n\\[\\delta _ { 2 } ( 0 ) = max(0.25*0.3*0.3, 0, 0)=0.0225\\]\n\\[\\delta _ { 2 } ( 1 ) =max(0.25*0.2*0.4, 0.15*0.4*0.4, 0)=0.024\\]\n\\[\\delta _ { 2 } ( 2 ) = max(0.25*0.5*0.3, 0.15*0.6*0.3, 0)=0.0375\\]\n第三次迭代（此时的输出字符为C）：\n\\[\\delta _ { 3 } ( 0 ) = max(0.0225*0.3*0.2, 0, 0)=0.00135\\]\n\\[\\delta _ { 3 } ( 1 ) =max(0.0225*0.2*0.3, 0.024*0.4*0.3, 0)=0.00288 \\]\n\\[\\delta _ { 3 } ( 2 ) =max(0.0225*0.5*0.5, 0.024*0.6*0.5, 0)=0.0072\\]\n最终答案：\n选择最优路径的时候从后往前选，选择最后一列最大的概率值为最终结果。\n即\\(0.0072\\)）。\n接着寻找上一步中生成该概率值（\\(0.0072\\)）的数作为前一步结果。\n即\\(0.024\\)，因为\\(0.024*0.6*0.5=0.0072\\)。\n以此类推。\n4.3 维特比算法应用\n4.3.1 基于HMM的词性标注\nHMM的状态集合：词性标记集合\n\\(t_i\\)为为词性标记集合中的第\\(i\\)个词性标记。\nHMM的输出字符集合：词汇集合\n\\(\\pi _ { \\mathrm { i } }\\)：词性标记\\(t_i\\)初始概率\n\\(a_{ij}\\)：从词性标记\\(t_i\\)到\\(t_j\\)的状态转移概率\n\\(b_{jk}\\)：词性标记\\(t_j\\)对应的词\\(w_k\\)的发射概率"}
