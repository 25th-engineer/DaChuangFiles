{"content2":"http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F\n斯坦福大学深度学习与自然语言处理第二讲：词向量\n9条回复\n斯坦福大学在三月份开设了一门“深度学习与自然语言处理”的课程：CS224d: Deep Learning for Natural Language Processing，授课老师是青年才俊 Richard Socher，以下为相关的课程笔记。\n第二讲：简单的词向量表示：word2vec, Glove(Simple Word Vector representations: word2vec, GloVe)\n推荐阅读材料：\nPaper1：[Distributed Representations of Words and Phrases and their Compositionality]]\nPaper2：[Efficient Estimation of Word Representations in Vector Space]\n第二讲Slides [slides]\n第二讲视频 [video]\n以下是第二讲的相关笔记，主要参考自课程的slides，视频和其他相关资料。\n如何来表示一个词的意思（meaning)\n英文单词Meaning的定义(来自于韦氏词典)\nthe idea that is represented by a word, phrase, etc.\nthe idea that a person wants to express by using words, signs, etc.\nthe idea that is expressed in a work of writing, art, etc.\n在计算机中如何表示一个词的意思\n通常使用类似Wordnet的这样的语义词典，包含有上位词（is-a)关系和同义词集\npanda的上位词，来自于NLTK中wordnet接口的演示\ngood的同义词集\n语义词典存在的问题\n语义词典资源很棒但是可能在一些细微之处有缺失，例如这些同义词准确吗：adept, expert, good, practiced, proficient,skillful?\n会错过一些新词，几乎不可能做到及时更新: wicked, badass, nifty, crack, ace, wizard, genius, ninjia\n有一定的主观倾向\n需要大量的人力物力\n很难用来计算两个词语的相似度\nOne-hot Representation\n传统的基于规则或基于统计的自然语义处理方法将单词看作一个原子符号：hotel, conference, walk\n在向量空间的范畴里，这是一个1很多0的向量表示：[0,0,0,0,...,0,1,0,...,0,0,0]\n维数：20K(speech)–50K(PTB)–500K(big vocab)–13M(Google 1T)\n这就是\"one-hot\"表示，这种表示方法存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系:\nDistributional similarity based representations\n通过一个词语的上下文可以学到这个词语的很多知识\n这是现代统计NLP很成功的一个观点\n如何使用上下文来表示单词\n答案：使用共现矩阵(Cooccurrence matrix)X\n2个选择：全文还是窗口长度\nword-document的共现矩阵最终会得到泛化的主题（例如体育类词汇会有相似的标记），这就是浅层语义分析(LSA, Latent Semantic Analysis)\n窗口长度容易捕获语法（POS）和语义信息\n基于窗口的共现矩阵：一个简单例子\n窗口长度是1（一般是5-10）\n对称（左右内容无关）\n语料样例\nI like deep learning.\nI like NLP.\nI enjoy flying\n存在的问题\n规模随着语料库词汇的增加而增加\n非常高的维度：需要大量的存储\n分类模型会遇到稀疏问题\n模型不够健壮\n解决方案：低维向量\nidea: 将最重要的信息存储在固定的，低维度的向量里：密集向量（dense vector)\n维数通常是25-1000\n问题：如何降维？\n方法1：SVD（奇异值分解）\n对共现矩阵X进行奇异值分解\nPython中简单的词向量SVD分解\n语料：I like deep learning. I like NLP. I enjoy flying\n打印U矩阵的前两列这也对应了最大的两个奇异值\n用向量来定义单词的意思：\n在相关的模型中，包括深度学习模型，一个单词常常用密集向量（dense vector)来表示\nHacks to X\n功能词(the, he, has)过于频繁，对语法有很大影响，解决办法是降低使用或完全忽略功能词\n延展窗口增加对临近词的计数\n用皮尔逊相关系数代替计数，并置负数为0\n+++\n词向量中出现的一些有趣的语义Pattern\n以下来自于:\nAn improved model of semantic similarity based on lexical co-occurence\n使用SVD存在的问题\n对于n*m矩阵来说计算的时间复杂度是o(mn^2) 当 n<m，当单词或者文档数以百万计时很糟糕< li=\"\">\n对于新词或者新的文档很难及时更新\n相对于其他的DL模型，有着不同的学习框架\n解决方案：直接学习低维度的词向量\n一些方法：和本讲以及深度学习相关\nLearning representations by back-propagating errors(Rumelhart et al.,1986)\nA Neural Probabilistic Language Model(Bengio et al., 2003)\nNatural Language Processing (almost) from Scratch(Collobert & Weston,2008)\nword2vec(Mikolov et al. 2013)->本讲介绍\nword2vec的主要思路\n与一般的共现计数不同，word2vec主要来预测单词周边的单词\nGloVe和word2vec的思路相似：GloVe: Global Vectors for Word Representation\n比较容易且快速的融合新的句子和文档或者添加新的单词进入词汇表\nword2vec的主要思路\n预测一个窗口长度为c的窗口内每个单词的周边单词概率\n目标函数：对于一个中心词，最大化周边任意单词的log概率\n对于$p(w_{t+j}/w_t)$最简单的表达式是:\n这里v和$v^'$分布是w的“输入”和“输出”向量表示（所以每个w都有两个向量表示）\n这就是基本的“动态”逻辑回归（“dynamic” logistic regression）\n代价/目标函数\n我们的目标是优化（最大化或最小化）代价/目标函数\n常用的方法：梯度下降\n一个例子（来自于维基百科）: 寻找函数$f(x) = x^4 - 3x^3 + 2$的局部最小点，其导数是$f^'(x) = 4x^3 - 9x^2$\nPython代码：\n梯度的导数\n白板（建议没有直接上课的同学看一下课程视频中的白板推导)\n有用的公式\n链式法则\nword2vec中的线性关系\n这类表示可以很好的对词语相似度进行编码\n在嵌入空间里相似度的维度可以用向量的减法来进行类别测试\n计数的方法 vs 直接预测\nGloVe: 综合了两类方法的优点\n训练更快\n对于大规模语料算法的扩展性也很好\n在小语料或者小向量上性能表现也很好\nGloVe的效果\n英文单词frog（青蛙）的最相近的词\nWord Analogies（词类比）\n对单词之间的线性关系进行测试（Mikolov et al.(2014))\nGlove可视化一\nGlove可视化二：Company-CEO\nGlove可视化三：Superlatives\nWord embedding matrix（词嵌入矩阵）\n提前训练好的词嵌入矩阵\n又称之为查询表(look-up table)\n低维度词向量的优点\n深度学习词向量的最大优势是什么？\n可以将任何信息表征成词向量的形式然后通过神经网络进行传播\n词向量将是之后章节的基础\n我们所有的语义表示都将是向量形式\n对于长的短语和句子也可以通过词向量的形式组合为更复杂的表示，以此来解决更复杂的任务-->下一讲"}
