{"content2":"自己写的可能有的不对的地方欢迎指正，法国的老师让我看完这个本书。还是比较累的。。\nUnit.2\n2.\n>>>from nltk.corpus import gutengberg >>>len('austen-persuasion.txt') >>>Len(set('austen-persuasion.txt'))\n3、\n>>> from nltk.corpus import state_union >>> fileid=nltk.corpus.state_union.fileids() >>> fileid >>> modals=['men','women','people'] >>> text=state_union.words() >>> fdist=nltk.FreqDist([w.lower() for w in text]) >>> for m in modals: print m+':',fdist[m] , >>>[fileid[:4] for fileid in state_union.fileids()] >>>cfd=nltk.ConditionalFreqDist( (target,fileid[:4]) for fileid in state_union.fileids() for w in state_union.words(fileid) for target in ['men','women','people'] if w.lower().startswith(target)) //如果不考虑转换成小写貌似Men和People都没有不知道为什么\n5.课本上的例子树和车子的就很好。\n>>>From nltk.corpus import wordnet as wn\n>>>wn.synsets('XX')的到该词的含义词集\n然后进行后续的操作具体参见WordNet那一节\n6.据说意大利语是属于拉丁语，拉丁语又属于印欧语系。而日耳曼语和拉丁语系是都属于印欧语系的。但是德语和英语是同源语系，同属日耳曼语系，学习德语会看到很多英语的影子。德语的R既可以发成俄语的大舌音，也可以发成法语的小舌音。意大利语，西班牙语和法语同属拉丁语系，发音比较柔和。德语发音则比较响亮比较硬。\n对于出现的问题，我也想不明白。大概就是两种不同语种之间转换的问题吧，这种在两种语系中最常出现的是。比如一个词在一个语言中是这个词，翻译成另一种语言，是另一个词，恰巧这个词是一词多意，然后你再翻译回来，有时候就错了。我记得好想有个例子是说中文中的鸡还是什么，翻译成另一种语言是幼年的，在翻译回来就成了孩子了。大概就是这个意思吧不知道对不对。\n7.你可以索引一下，看看However出现在句首和句尾有什么区别。\n8.\n>>> names=nltk.corpus.names >>> names.fileids() >>> cfd=nltk.ConditionalFreqDist( (fileid,name[0]) for fileid in names.fileids() for name in names.words(fileid)) >>> cfd.plot()\n我觉得需要注意C.M  H.W这几个\n9.这个很麻烦，暂时我也想不出来。就先这几个看一下\n>>> from nltk.book import * >>> text1.concordance(\"monstrous\") >>> text2.concordance(\"monstrous\") >>> text1.similar(\"monstrous\") >>> text2.similar(\"monstrous\") >>> def lexical_diversity(text): return len(text)/len(set(text)) >>> brown.raw(fileids=['cg22']) >>> brown.raw(fileids=['ca03'])\n10. 可以去这看看http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html。(那个是L)看来我也是词汇贫瘠的，还好我喜欢用there be 。。。。\n11. 我想了想，正好看到前面有篇例子。就是模式分布表那个例子就做了一下哪个求最浪漫的新闻上的日子。就写了个这个程序，days毕竟算是一个词包的闭包。虽然和题目要求的有距离。。。这个答案只是我的想法。\n>>> import nltk >>> from nltk.corpus import brown >>> cfd=nltk.ConditionalFreqDist( (genre,word) for genre in brown.categories() for word in brown.words(categories=genre)) >>> days=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'] >>> genres=['news','romance'] >>> cfd.tabulate(conditions=genres,samples=days)\nMonday Tuesday Wednesday Thursday Friday Saturday Sunday\nnews      54     43      22      20      41    33     51\nromance    2     3        3       1       3     4      5\n12.\nhttp://nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.cmudict这上面说\nThe dictionary contains 127069 entries. Of these, 119400 words are assigned a unique pronunciation, 6830 words have two pronunciations, and 839 words have three or more pronunciations. Many of these are fast-speech variants.\n但是实际做了一下得到如下结果\n>>>Entries=nltk.corpus.cmudict.entries() >>>len(entries) 133737 >>> word=nltk.corpus.cmudict.words() >>> len(word) 133737 >>> len(prondict) 123455\n这是为什么？我也没想明白\n>>> import nltk >>> prondict=nltk.corpus.cmudict.dict() >>> word_dict=nltk.corpus.cmudict.words() >>> word_Freq=nltk.FreqDist([word for word in word_dict if len([ph for ph in prondict[word]])>1]) >>> word_Freq=nltk.FreqDist([word for word in word_dict if len([ph for ph in prondict[word]])==1]) >>> len(word_Freq) 114214 >>> 113421+9241 122662\n13.\n>>> from nltk.corpus import wordnet as wn >>>from __future__ import division >>> len(set(wn.all_synsets('n'))) 82115 >>> wn.synsets('n') [Synset('nitrogen.n.01'), Synset('north.n.03'), Synset('newton.n.02'), Synset('normality.n.02'), Synset('n.n.05')] >>> all_syn=len(set(wn.all_synsets('n'))) <generator object all_synsets at 0x0000000009C83E10> >>> no_hyp=len([synset for synset in wn.all_synsets('n') if len(synset.hyponyms())<1]) >>>pirnt no_hyp/all_syn*100 79.6711928393\n14.\n>>> from nltk.corpus import wordnet as wn >>> s=wn.synsets('car') >>> def supergloss(set): for synset in set: print synset.definition; print synset.hyponyms(); print synset.hypernyms(); >>> supergloss(s)\n15.\n>>>import nltk >>> from nltk.corpus import brown >>> text=brown.words(); >>> fdist=nltk.FreqDist([w.lower() for w in text]) >>> len([m for m in sorted(set(text)) if fdist[m]>=3]) >>> for m in set(text): if fdist[m]>=3: print m\n16.\n实现了，但是我也是刚学Python很多不会写，tabulate我用的不熟练。。想了半天没想出来，这样也可以应该。欢迎指教。\n>>> for genre in brown.categories(): word_num=len([word for word in brown.words(categories=genre)]) type_num=len(set(word for word in brown.words(categories=genre))) diversity=word_num/type_num print genre+':',word_num,type_num,diversity\nadventure: 69342 8874 7\nbelles_lettres: 173096 18421 9\neditorial: 61604 9890 6\nfiction: 68488 9302 7\ngovernment: 70117 8181 8\nhobbies: 82345 11935 6\nhumor: 21695 5017 4\nlearned: 181888 16859 10\nlore: 110299 14503 7\nmystery: 57169 6982 8\nnews: 100554 14394 6\nreligion: 39399 6373 6\nreviews: 40704 8626 4\nromance: 70022 8452 8\nscience_fiction: 14470 3233 4\n很明显humor和reviews的最低。Humor可以想到，但是reviews我就想不出来为什么。\n17.\n我用的是第一篇。忘了keys()写了好长时间才发现，尼玛有例子啊课本上。伤心啊\n>>>import nltk >>> from nltk.book import * >>>from nltk.corpus import stopwords >>>stopwords=nltk.corpus.stopwords.words('english') >>> Dist=FreqDist([w for w in text1 if w.lower() not in stopwords]) >>> vocabulary=Dist.keys() >>> vocabulary[:50]\n18.\n这个算是我没有办法的办法完成了这个题目因为这个题目有歧义。。我按照输出50，忽略含有停用词的选用Book中的sent2\n>>>import nltk >>> from nltk.corpus import stopwords >>> from nltk.book import * >>> stopwords=nltk.corpus.stopwords.words('english') >>> biwords=nltk.bigrams(sent2) >>> fdist=nltk.FreqDist(biwords) >>> vocabulary=fdist.keys() >>> for a,b in vocabulary[:50]: if a not in stopwords and b not in stopwords: print a,b\n19.\n>>> import nltk >>> from nltk.corpus import brown >>> cfd=nltk.ConditionalFreqDist( (genre,word) for genre in brown.categories() for word in brown.words(categories=genre)) >>> genres=brown.categories() >>> modals=['what','how','who','where'] >>> cfd.tabulate(conditions=genres,samples=modals)\nwhat  how  who where\nadventure  110   35   91   53\nbelles_lettres  244   96  452  107\neditorial   84   43  172   40\nfiction  128   54  103   76\ngovernment   43   16   74   46\nhobbies   78   40  103   72\nhumor   36   18   48   15\nlearned  141   61  212  118\nlore  130   65  259   97\nmystery  109   37   80   59\nnews   76   37  268   58\nreligion   64   23  100   20\nreviews   44   26  128   25\nromance  121   60   89   54\nscience_fiction   27   12   13   10\n20.\n>>> len(word_freq(word_set,'what')) 14394 >>> word_set=brown.words(categories='news') >>> def word_freq(word_set,word): return len(nltk.FreqDist(word for word.lower() in word_set)) >>> word_freq(word_set,'what')\n21.\n用text代替sent\n>>> sum([len(pron) for word,pron in entries if word in sent2])\n22.随便写了下\n>>> wordlist=[] >>> for i in range(0,len(sent2)): if (i+1)%3==0: wordlist.append(sent2[i]); wordlist.append('like'); else: wordlist.append(sent2[i]); >>> print wordlist\n改成函数（在IDEL中一定要注意对其关系，对其那个是哪个层的我就因此错了N多次。。）\ndef hedge(text): for i in range(0,len(text2)): if (i+1)%3==0: wordlist.append(text[i]); wordlist.append('like'); else: wordlist.append(sent2[i]); >>> def hedge(text): for i in range(0,len(text)): if (i+1)%3==0: wordlist.append(text[i]); wordlist.append('like'); else: wordlist.append(text[i]); return wordlist >>> hedge(sent2)\n['The', 'family', 'of', 'like', 'Dashwood', 'had', 'long', 'like', 'been', 'settled', 'in', 'like', 'Sussex', '.', 'The', 'family', 'of', 'like', 'Dashwood', 'had', 'long', 'like', 'been', 'settled', 'in', 'like', 'Sussex', '.']\n在后面的我就不会了。我是初学者"}
