{"content2":"中文分词是中文文本处理的一个基础步骤，也是中文人机自然语言交互的基础模块，不同于英文的是，中文句子中没有词的界限，因此在进行中文自然语言处理时，通常需要先进行分词，分词效果将直接影响词性，句法树等模块的效果，当然分词只是一个工具，场景不同，要求也不同。在人机自然语言交互中，成熟的中文分词算法能够达到更好的自然语言处理效果，帮助计算机理解复杂的中文语言。\n基于词典分词算法\n基于词典分词算法，也称为字符串匹配分词算法。该算法是按照一定的策略将待匹配的字符串和一个已经建立好的\"充分大的\"词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。常见的基于词典的分词算法为一下几种：正向最大匹配算法，逆向最大匹配法，最少切分法和双向匹配分词法等。\n基于词典的分词算法是应用最广泛，分词速度最快的，很长一段时间内研究者在对对基于字符串匹配方法进行优化，比如最大长度设定，字符串存储和查找方法以及对于词表的组织结构，比如采用TRIE索引树，哈希索引等。\n这类算法的优点：速度快，都是O(n)的时间复杂度，实现简单，效果尚可，\n算法的缺点：对歧义和未登录的词处理不好。\n基于理解的分词方法\n这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果，其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象，它通常包含三个部分：分词系统，句法语义子系统，总控部分，在总控部分的协调下，分词系统可以获得有关词，句子等的句法和语义信息来对分词歧义进行判断，它模拟来人对句子的理解过程，这种分词方法需要大量的语言知识和信息，由于汉语言知识的笼统、复杂性，难以将各种语言信息组成及其可以直接读取的形式，因此目前基于理解的分词系统还在试验阶段。\n基于统计的机器学习算法\n这类目前常用的算法是HMM，CRF，SVM，深度学习等算法，比如stanford，Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备良好的学习能力，因此对歧义词和未登录词的识别都具有良好的效果。\nNianwen Xue在其论文中《Combining Classifier for Chinese Word Segmentation》中首次提出对每个字符进行标注，通过机器学习算法训练分类器进行分词，在论文《Chinese word segmentation as character tagging》中较为详细地阐述了基于字标注的分词法。\n算法优点：能很好处理歧义和未登录词问题，效果比前一类效果好\n算法缺点: 需要大量的人工标注数据，以及较慢的分词速度\n现行常见的中文词分类器\n常见的分词器都是使用机器学习算法和词典相结合的算法，一方面能够提高分词准确率，另一方面能够改善领域适应性。\n随着深度学习的兴起，也出现了基于神经网络的分词器，例如有研究人员尝试使用双向LSTM＋CRF实现分词器，其本质上是序列标注，所以有通用性，命名实体识别等都可以使用该模型，据报道其分词器字符准确率可以高达97.5%,算法框架的思路与论文《Neural Architectures for Named Entity Recogintion》类似，利用该框架可以实现中文分词，如下图所示\n首先对语料进行字符嵌入，将得到的特征输入给双向的LSTM，然后加一个CRF就得到标注结果。\n分词器当前存在问题\n目前中文分词难点主要有三个：\n1. 分词标准：比如人名，在哈工大的标准中姓和名是分开的，但是在Hanlp中是合在一起的，这需要根据不同的需求制定不同的分词标准。\n2. 歧义：对于同一个待切分字符串存在多个分词结果。\n歧义又分为组合歧义，交集型歧义和真歧义三种分类。\n1）组合型歧义：分词是有不同的粒度的，指某个词条中的一部分也可以切分未一个独立的词条，比如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是“中华/人民/共和国”\n2）交集型歧义：在“郑州天和服装厂”中，“天和”是厂名，是一个专有名词，“和服”也是一个词，它们共用了“和”字\n3）真歧义：本身的语法和语义都没有问题，即便采用人工切分也会产生同样的歧义，只有通过上下文的语义环境才能给出正确的切分结果，例如：对于句子“美国会通过对台售武法案”，既可以切分成“美国/会/通过...”也可以切分成“美/国会/通过...”\n一般在搜索引擎中,构建索引时和查询时会使用不同的分词算法，常用的方案是，在索引的时候，使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。\n3. 新词：也称未被词典收录的词，该问题的解决依赖于人们对分词技术和汉语语言结构进一步认识。\n部分分词器的简单说明：\n哈工大的分词器：主页上给过调用接口，每秒请求的次数有限制。\n清华大学THULAC：目前已经有Java、Python和C++版本，并且代码开源。\n斯坦福分词器：作为众多斯坦福自然语言处理中的一个包，目前最新版本3.7.0， Java实现的CRF算法。可以直接使用训练好的模型，也提供训练模型接口。\nHanlp分词：求解的是最短路径。优点：开源、有人维护、可以解答。原始模型用的训练语料是人民日报的语料，当然如果你有足够的语料也可以自己训练。\n结巴分词工具：基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。\n字嵌入+Bi-LSTM+CRF分词器：本质上是序列标注，这个分词器用人民日报的80万语料，据说按照字符正确率评估标准能达到97.5%的准确率，各位感兴趣可以去看看。\nZPar分词器：新加坡科技设计大学开发的中文分词器，包括分词、词性标注和Parser，支持多语言，据说效果是公开的分词器中最好的，C++语言编写。\n关于速度\n由于分词是基础组件，其性能也是关键的考量因素。通常，分词速度跟系统的软硬件环境有相关外，还与词典的结构设计和算法复杂度相关。比如我们之前跑过字嵌入+Bi-LSTM+CRF分词器，其速度相对较慢。\n作者：lovive"}
