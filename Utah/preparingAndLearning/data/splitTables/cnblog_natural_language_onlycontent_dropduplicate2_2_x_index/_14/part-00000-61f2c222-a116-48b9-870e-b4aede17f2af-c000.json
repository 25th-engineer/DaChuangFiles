{"content2":"0x00 中文分词\n1)FoolNLTK\nslogan：可能不是最快的开源中文分词，但很可能是最准的开源中文分词\n仓库地址\n2)CWS_Dict\n论文\"Neural Networks Incorporating Dictionaries for Chinese Word Segmentation\", AAAI 2018 源码\n仓库地址\n3)multi-criteria-cws\n多标准中文分词的简单解决方案\n仓库地址\n4)jieba\n结巴中文分词\n仓库地址\n5)NLPIR-team/NLPIR\nNLPIR是一套专门针对原始文本集进行处理和加工的软件，提供了中间件处理效果的可视化展示，也可以作为小规模数据的处理加工工具。\n仓库地址\n0X01 命名实体识别\n1)anaGO\n2)golden-horse\n3)LTP\n4)NeuroNER\n使用神经网络实现的命名实体识别，简单易用并达到state-of-the-art的效果。\n仓库地址\n5)shiyybua/NER\n基于tensorflow深度学习的中文的命名实体识别\n仓库地址\n6)nltk\n0x02 词性标注POS-tagging\n1)anaGO\n2)LTP\n3)NLPIR\n仓库地址\n4)jieba\n仓库地址\n“结巴”中文分词：做最好的 Python 中文分词组件\n5)nltk\n0X03 关键词抽取KEYWORD EXTRACTION(FOR DOCUMENT(S))\n1)NLPIR(FOR SHROT TEXT AS WELL)\n仓库地址\n2)jieba\n仓库地址\nTextRank-based\nTFIDF-based\n4) aneesha/RAKE\n仓库地址\n0x04 依存句法DEPENDENCY PARCING\n1) LTP\n2) nltk\n0x05 自然语言理解NLU(SEMANTIC ANALYSIS)\n1)anaGO(SRL)\n2)RasaHQ/rasa_nlu\n将自然语言转换为结构化数据\n仓库地址\nRasa NLU (Natural Language Understanding) is a tool for understanding what is being said in short pieces of text. For example, taking a short message like:\n\"I'm looking for a Mexican restaurant in the center of town\"\nAnd returning structured data like:\nintent: search_restaurant entities: - cuisine : Mexican - location : center\n3)LTP\n4)is13\nInvestigation of Recurrent Neural Network Architectures and Learning Methods for Spoken Language Understanding\n仓库地址\n0x06 字词匹配WORDS MATCHING\n1)NLPIR(KeyScanner)\n2)AC AUTOMATON (PYTHON:esmre  ahocorasick)\n0X07 实体链接ENTITY LINKING\n1)yahoo/FEL\n快速的实体链接工具集，实现mention连接到Wikipedia。\n仓库地址\n2)dalab/pboh-entity-linking(JAVA)\n论文\"Probabilistic Bag-Of-Hyperlinks Model for Entity Linking\"的源码。\n仓库地址\n3)CN-DBpedia API\nCN-DBpedia提供全套API，并且免费开放使用。\nWEBSITE\n4)songjs1993/Entity-Linking\nEntity Linking，识别给定文本中出现的命名实体（Named Entity），并映射到特定的知识库中唯一的实体。包括命名实体识别、消歧等工作。\n仓库地址\n5)semanticize/semanticizer\nThe Semanticizer是2012年由Daan Odijk开发的用于语义连接的应用。\n仓库地址\n0x08 自动文摘AUTOMATIC SUMMARIZATION\n1)miso-belica/sumy\n针对文本文档和HTML的自动文摘python模块。 https://pypi.python.org/pypi/sumy\n仓库地址\n0x09 主题建模TOPIC MODELING\n1)baidu/Familia\n百度开源的Familia 开源项目包含文档主题推断工具、语义匹配计算工具以及基于工业级语料训练的三种主题模型：Latent Dirichlet Allocation(LDA)、SentenceLDA 和Topical Word Embedding(TWE)。\n仓库地址\n2) Microsoft/LightLDA\n微软开源的方便快捷轻量的大规模主题建模系统 http://www.dmtk.io\n仓库地址\n0x0A 文本分类TEXT CLASSIFICATION\n1)yoonkim/CNN_sentence\n经典论文Convolutional Neural Networks for Sentence Classification (EMNLP 2014). 的大牛作者的源代码。\n仓库地址\n2)DocumentClassification\n使用TensorFlow实现的简单的用以文本分类的CNN模型。\n仓库地址\n3)lc222/text_classification_AI100\n主要用于文本分类，其中涉及CHI选择特征词，TFIDF计算权重，朴素贝叶斯、决策树、SVM、XGBoost等算法， 实现传统的文本分类并取得了不错的效果。\n仓库地址\n4) dennybritz/cnn-text-classification-tf\nTensorFlow搭建cnn文本分类模型\n仓库地址\n5)gaussic/text-classification-cnn-rnn\n使用卷积神经网络以及循环神经网络进行中文文本分类.基于TensorFlow在中文数据集上的简化实现，使用了字符级CNN和RNN对中文文本进行分类，达到了较好的效果。\n仓库地址\n6)清华大学THUCTC\nTHUCTC(THU Chinese Text Classification)是由清华大学自然语言处理实验室推出的中文文本分类工具包，能够自动高效地实现用户自定义的文本分类语料的训练、评测、分类功能。\n仓库地址\n7)jiegzhan/multi-class-text-classification-cnn-rnn\n使用TensorFlow构建的用于多类别分类的 CNN, RNN (GRU and LSTM)模型。\n仓库地址\n0x0B 问答系统QA\\CHATBOT\n1)brmson/yodaqa\nYodaQA 是一个开源的事实型问答系统，他能够利用即时的信息抽取从数据库和非结构化的文本语料中抽取答案。在Freebase和DBPpedia上表现优异。\n仓库地址\n2)ChatterBot\nChatterBot 是一个用于构建chatbot的集成了机器学习的对话引擎，。http://chatterbot.readthedocs.io\n仓库地址\n3)Conchylicultor/DeepQA\n使用TensorFlow实现论文A Neural Conversational Model 中的google的聊天机器人（seq2seq方法）。\n仓库地址\n4)macournoyer/neuralconvo\n使用Torch实现论文A Neural Conversational Model 中的google的聊天机器人（seq2seq方法）。\n仓库地址\n5) fateleak/chatbot-zh-torch7\n4)的中文情境下的实现。\n仓库地址\n6)alfredfrancis/ai-chatbot-framework\n使用python的chatbot框架。\n仓库地址\n7)zake7749/Chatbot\n基于向量匹配的情景式聊天机器人。\n仓库地址\n8)qhduan/Seq2Seq_Chatbot_QA\n使用TensorFlow实现的Sequence to Sequence的聊天机器人模型。\n仓库地址"}
