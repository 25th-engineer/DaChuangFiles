{"content2":"搞了这么久人工智能，写个阶段性总结吧。不过过几年肯定会有更精彩的结果出来。到时候就再处理这篇文章吧。\n不知道大家有没有看过Ray Kurzweil的《奇点临近》。反正我看过之后，做了一个决定：我这辈子算是要献给这个方向了。\n说人工智能这个方向，可以分很细，但各种分法最后用到的东西基本都一样。我篇文章要提出几个问题，然后回答他们。\n人工智能方向的原因\n人工智能的总体介绍\n人工智能核心技术\n人工智能方向的原因：\n真心推荐看一下《奇点临近》，里面的加速回归理论。主旨是：技术的发展和普及速度是指数级加速的。不信你自己想想，从农业到蒸汽机用了多久？从蒸汽机到石油用了多久？IT行业是啥时候出现的？PC是什么时候出现的？windows是哪年出现的？（它火起来的时候我已经从老妈肚子里出来了）互联网出现了几年？诺基亚倒下花了多久？智能机出来几年？苹果在中国如日中天之前是啥样子的？（也就各位数年度）\n那么我们不禁要问，一个技术的范式迭代从出现到普及，如果现在和未来是指数速度加速的，那么下一个大规模改变我们生活的技术会是什么？我指范式迭代（颠覆性的改变）。它会花多久让我们接收这个技术？\n读研的过程中，我发现一件事，导师不是图像就是数据挖掘，竟然70%是人工智能相关。像复旦，几乎清一色的数据挖掘。实验室固然不能体现社会，但一个技术必然是先在实验室出现的（很多年不是在中国的实验室，当然这次也不例外）。不但是中国，国外的实验室，人工智能成为极火的方向。当然，历史上出现过人工智能的泡沫时代，lisp出现的那几年和80年代，人工智能都曾被认为是革命，彻底的革命。但很快，热情就下去了。因为不成熟，不成熟的原因，永远不是技术无法达到（当然，确实没那么成熟的技术），是因为市场不到时候接收。为啥这么说？iphone都没办法被接受，市场会接受更酷的产品吗？\n我估计当中国能接受vertu（虽然vertu的主要市场现在也在中国，但离“接受”这个词差远了），家庭里xbox、psp比较普及的时候，苹果成为街机的时候，就差不多该是新迭代出现的时候了。而，我们可以感觉到，这一天很不远了。\n所以，下一次范式迭代，应该是人工智能（6大子方向齐头并进，各有千秋）\n如果，Kurzweil是对的，一个很酷的人工智能产品的出现，会在几天内在全球普及，公司会迅速登顶。谷歌看到了，所以有了奇点大学；微软看到了（各位去微软的网站上看看都招啥人，90%此方向）。\n而，人工智能切入市场的角度，第一可能是游戏（中国马上会流行画面控的xbox类平台单机游戏），第二可能是独立应用产品（比如照片自动分类，音乐自动分类，桌面人工智能）。\n人工智能的总体介绍\n如果你看各种综述人工智能的，都会把子领域这么分类：机器人、语言识别、图像识别、自然语言处理、专家系统等。。。。。\n人工智能，人工智能，自然是模拟人的智能。人有啥智能？\n输入和特征提取（五感，例如眼一看到东西，就会在视网膜自动将图像各种特性提取出来，不同的特性送到大脑的不同区域。例如颜色、轮廓、亮度）\n分类（一个东西，只要形状变得不夸张，我们都认识，这就是分类的功能，当然，在人工智能里叫classify和clustering）\n输出（就是个控制系统，给它发什么信号，他做什么事情。当然，这个事情可能会涉及到比较多的部件，会很复杂，但归结起来，就是件事情）\n记忆（这个是系统最难攻克的地方，我们现在能做的所有文明的事情都是基于记忆的（你刚出来的时候只会找妈妈的奶喝）），而记忆记啥呢？模式。\n模式：一个鼠标，怎么变我们也认识，这是物体的模式。一个人的声音，我们也听出来，这是声音的模式。所以，模式=分类结果\n好了，这就是人工智能。输入和特征提取一般域分类一起，形成了《机器学习》《模式识别》《数据挖掘》这几大部分重叠方向。其实用到的都是同样的东西：bayes、svm、ANN。。。都是实现分类（概称）问题的不同算法。\n至于输出，除了机器人，没人会在意。现在还不是研究那个的时候。（前端没有成熟，输出只是普通生产线，属于鸡肋部分）\n关键的是记忆，以上说的《机器学习》等方向，都会涉及到如果存储学习结果，但都不成系统。记忆这方面的进展是最慢的（如果实现了，直接可以宣告人工智能的革命来了）\n人工智能核心技术\n我觉得这个是我写这片文章的目的。随便一本书都会纵览一下整个领域，但是我看了很多书，几乎都是只管自己领域去了。上面说了，核心包括：\n输入和特征提取：特征（属性）选择\n分类\n记忆\n可以得出一个结论，人工智能处理的东西，一定是有属性（特征）的。（废话，要不拿什么分类）\n所以，核心中的核心不在属性选择上（虽然很重要），而在分类和记忆上。（属性选择相当于决定吃什么，分类和记忆相当于吃不吃）\n我按照各个分类算法的露脸频率给分列表：\nByaes：贝叶斯（这个是第一名，没异议吧？效果很好（大部分情况），算法简单，要介绍整个算法体系的话估计谁都第一个把它提出来）\n线性分类器：简单的说，你把所有输入（有属性）都用数字表示，然后以属性为坐标轴想象一个多维空间。每个数据条目都是多维空间的一个点，在这个空间里，你能在空间上把不同类的点分开，你就赢了。这就是线性分类了。（感知器算法（过时），最小二乘法，均方估计，逻辑识别还有大头的支持向量机，这里支持向量机svm是最出名的，我也天天用这个，这还得归功于台湾一位教授的libsvm库啊）\n非线性分类器：就是线性分类器解决不了的情况。知名的算法有：ANN（人工神经网络，更确切的说是BP（反向传播）），svm（这么知名的算法，如果有线性解决不了的情况，自然会有人把它拓展到非线性情况）\n以上方法大部分是分类，分类里有个子方向是聚类。（svm就有这功能）。聚类好多算法，但都有一个总体思想：所有被聚为一类的都有共同特点，都应该是某种角度相似的。所以，用什么不重要，你不同的角度想，会有不同的聚类算法。本质上就是找规律嘛！\n这么多年，我总结出一点：技术都是被那群发论文的搞神秘了。其实，超简单的。比如神经网络，你买本SimonHaykin的《神经网络与机器学习》看试试，你真会发现，这玩意难啊，咱玩不了。然后你买本史忠植的《神经网络》翻翻，你会发现，啊，tnnd，神经网络这么简单啊。的确，每个算法必须要有强劲的数学作为支撑，但对于使用者（非博士以上科研人士），你用lisp的时候有脑残到要去搞懂lisp的数学原理吗？\n分类与记忆，现在的所有算法都是一种逼近，结构上最像的自然是人工神经网络（ANN），但不一定是解决目前狠多工程问题的最优方法。有关系吗？有趣的是，ANN可以同时解决分类和记忆（别说还有概念能力啊，还有情感能力啊，还有模糊是非能力啊。。。人的能力是很多，但本质就是个分类和记忆）\n所以，我认为，喜欢这个方向，应该不要忘记神经网络（当然，如果是细分领域，比如语言处理，音频，视频识别，这不一定是最优的，但是是万能的和最有可能逼近通用分类和记忆最终解的（虽然现在的ANN在速度和好多问题的质量上有点恶心））。\n最后，借用Kurzweil的一句话：当机器智能超过人的那天，人类智能将永远不可能超过机器智能。\nPS：我是做反垃圾算法的。\n好书推荐：\n《模式识别》Sergios Yheodoridis；\n《神经网络》史忠植；\n《知识工程语言学》鲁川；\n《数据挖掘-概念与技术》Kamber；\n《图论》GTM系列之一，Reinhard Diestel\n《高级人工智能》史忠植；\n《知识发现》史忠植；\n《智能科学》史忠植；\n《人工智能复杂问题求解的结构和策略》Luger；\n《人工智能》尼尔森；\n《人工智能：一种现代的方法》拉塞尔；\n《灵魂机器的时代：当计算机超过人类智能》Kurzwell\n《奇点临近》Kurzwell；\n《神经网络与机器学习》海金；\n人工神经发展简史（转）\n1.启蒙时期\n1890年，WilliamJames发表了《心理学原理》。\n1943年，生理学家W.S.McCuloch和数学家W.A.PiHs提出M-P模型。\n1949年，心理学家Hebb出版《行为构成》，建立了Hebb算法(连接权训练算法)主要有四点贡献：①信息存储在连接权中；②\n1958年，计算机科学家FrankRosenblatt提出了具有3层网络特性的神经结构网络。\n1960年，电机工程师BernardWidrow和Mareian Hoff提出”Adaline”模型，实现了人工神经硬件。Widrow-Hoff算法也称为δ算法，最小均方(LMS)算法，梯度下降法。\n2.低潮时期\n1969年，人工智能创始人M.Minsky和S.Papert发表《感知器》，给人工智能泼了一盆冷水。\n1969年，S.Grossberg教授和她的夫人G.A.Carpenter提出来著名的自适应共振理论(Adaptive ResonanceTheory)模型，其中的基本观点是：若在全部神经节点中有一个神经节点特别兴奋，其周围的所有节点将受到抑制。Grossberg还提出短期记忆和长期记忆的机理，节点的激活值和连接权都会随时间，前者代表短期记忆，衰减得快，后者代表长期记忆，衰减得慢。其后他们发表了ART1，ART2，ART3三个版本，ART1网络只能处理二值的输入，ART2能处理模拟量输入。\n1972年，芬兰的T.Kohonen教授提出了自组织映射(SOM)理论，以及联想存储器(Associated Memory)。美国的神经生理学家和心理学家J.Anderson提出了交互存储器(Interactive Memory)。\n1980，日本东京的福岛邦彦发表了“新认知机”(Neocognitron)。\n3.复兴时期\n1982年，美国加州理工学院的优秀物理学家John.J.Hopfield总结和吸纳了前人的经验，塑造出一种新颖的强有力的模型，成为Hopfield网络，此网络有个优点，与电子电路有明显的对应关系，易于用集成电路实现。\nG.E.Hinton和T.J.Sejnowski借助统计物理学的概念和方法提出了一种随机神经网络模型—玻尔兹曼(Blotzmann)机。\n1986年，贝尔实验室宣布制成神经网络芯片不久，美国的David.E.Rnmelhart和James L.McCelland及其领导的研究小组发表了《并行分布式处理》(ParallelDistributed Processing)一书的前两卷，接着1988年发表带有软件的第三卷，书中涉及到了三个主要特征：结构、神经元的传递函数(也称传输函数、转移函数、激励函数)和它的学习训练方法。这部书发展了多层感知器的反向传播训练算法，把学习的结果反馈到中间层次的隐节点，改变其权值，以达到预期的学习目的。\n4.新时期\n1987年6月，首届国际神经网络学术会议在加州圣地亚哥召开，成立了国际神经网络学会(International Neural Network Sociaty,INNS)。\n不久，美国波士顿大学的Stephen Grossberg教授，芬兰赫尔辛基技术大学的Gteuvo Kohonen教授和日本东京大学的甘利俊一(Shunichi Amuri)教授—主持创办了世界第一份神经网络杂志《Neural Network》。\n再新一点的，就是deep machine learning。不过还没有经过时间的考验。现在倒是挺火的。\n来着：http://blog.csdn.net/ljy1988123/article/details/7726519"}
