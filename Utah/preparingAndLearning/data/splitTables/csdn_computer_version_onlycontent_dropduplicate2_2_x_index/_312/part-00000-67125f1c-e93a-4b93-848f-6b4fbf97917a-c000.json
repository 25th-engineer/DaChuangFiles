{"content2":"目录\n0  如果你觉得这个页面广告太多，请点击下面教程（我写的）去广告\n1  网络拆分，一个网络里面有什么，作用是什么\n1.0   基础知识，从零开始\n30分钟明白深度学习怎么学习的，权重是什么，什么是梯度下降，损失怎么计算的。油管播放量60多万的视频，很社会了。那么或许你已经看过一些书和论文，甚至能够搭建自己的网络，但我还是看了，觉得有所收获。\n1.1  激活函数\n入门简单易懂：但是我没看明白什么叫神经元死了（ReLU）\n在第一篇的基础上往广度拓展，但是没有拓展深度，同样没有解释什么叫神经元没反应了，具体到feature map与conv层上，不过我们可以明确的是小心设置比较小的learning rate会让节点不轻易死掉\n很有趣的补充，涉及到人脑神经元的特性，稀疏性，单侧抑制，宽阔的阈值边界\n1.2 BN batch normalization 归一化\n普通的机器学习归一化作为基础\n把归一化应用与网络\n2.流行网络举例\n2.1   ResNet\n1.第一个链接我给youtube,由于我在看Andrew Wang的CNN视频，里面提到了ResNet网络可以在加深网络的时候不回弹识别率，因为一般网络深度超过一个程度，错误率会上升。\n2.给妹纸的深度学习教学（4）\n解决梯度消失\n油管一个中文的视频\n2.2 reinforcement learning\n2.3 YOLO vs RCNN\n3.tensorflow 教程\n3.1 keras\n0  如果你觉得这个页面广告太多，请点击下面教程（我写的）去广告\nhttps://blog.csdn.net/u013249853/article/details/79814508\n这个广告是这个网站自己放的，很影响专注程度，所以如果你比较常用线上学习，推荐去掉广告一劳永逸，不然很分散注意力的，降低学习效率。\n1  网络拆分，一个网络里面有什么，作用是什么\n那啥，你可能不想看我的读后感，或者评价或者总结，或者是废话，那你可以等我出完完全版整个单纯的链接。教程是这样的，你作为基础的，最先看得必须是最权威的东西，这个给你打基础，然后你看不明白，再去看看别人是怎么理解的。所以知识的一手资源是论文，其次是论文作者，或者权威教授写的书，然后是一些大神的产物比如视频，博客，这个知识被咀嚼这么多遍，变得容易理解，教学技巧性更强，更有说服力，走入了大众视野，就成了各种普通人，比如我，的博客。越往下走，越可能产生错误，因为博客主人会加入自己的理解，除非他是转载，所以有可能出错，毕竟没有经过论文审核，出版审计。比如我，通常不会通读自己写的东西第3遍。\n怎么找这些东西，一般情况下，你看的教程都会推荐，比方说下面的图片提到的note\n所以推荐的推荐，越高越难，越学术，越理论。\n所以打基础的看官务必不要忽视权威的重要性，不要在csdn上闲逛，看到什么都觉得说得对，事实上，我看到过不少错误，小错，虽然我不会，但我能推啊。所以大家学习的时候一定要先建立一个基础概念，比方说上个课，网易云课堂，coursera，英文好可以上油管，我下面推的那个油管andrew wang的视频就是cousera的。很基础，很短，很直观。\n之后，你再去随意浏览一些博客，看看别人怎么想的。不论你看什么一定要质疑，即使是大牛论文，虽然最后结果一定人家对，自己没理解好，那你这不就加深理解，查漏补缺，揪出自己的bug了吗。你在找别人问题的时候一定是思考轮子转的最快的时候，真的。\n1.0   基础知识，从零开始\n30分钟明白深度学习怎么学习的，权重是什么，什么是梯度下降，损失怎么计算的。油管播放量60多万的视频，很社会了。那么或许你已经看过一些书和论文，甚至能够搭建自己的网络，但我还是看了，觉得有所收获。\nhttps://www.youtube.com/watch?v=ILsA4nyG7I0\n图像的基本单位是像素，显示为一个一个小方格，数值是0-255，0是灰色的。如果是彩色，那么有三个通道。每个位置确定好颜色，就是一张图片了。每个位置的数值，或者是单值，比如23（灰度图像只有黑白），或者是一个三维的，(23,34,255)（彩色图像）。\n看不明白的话就暂停，想一想，或者多看几遍。记点笔记。\n推荐在油管看，用自动生成字幕，作者母语英语，自动生成大部分都很准。下面是B站地址，不用科学上网也能上：\nhttps://www.bilibili.com/video/av14095273?from=search&seid=10011126792909487890\n点击设置键打开字幕，视频右下角，第二个小齿轮，上面有鲜红色的HD，如下图：\n推荐在油管看，用英文字幕，油管自动翻译我没试过，不过谷歌的自动翻译很不错的。\n26分钟搞清楚卷积神经网络是什么，卷积怎么卷，能干啥。30多万播放量\nhttps://www.youtube.com/watch?v=FmpDIaiMIeA&feature=youtu.be\nhttp://brohrer.github.io/how_convolutional_neural_networks_work.html\n下面是B站地址：\nhttps://www.bilibili.com/video/av19231561?from=search&seid=13469907213316774113\n1.1  激活函数\n默认每层之间都有relu\n当你计算完卷积了，你要形成新的一层了，这个时候，桥豆麻袋！！你还没用激活函数呢，你的新网络每个像素的值有正有负，大于零的原样输出，小于零的统统归零。简单直观。你就是拿个筛子，筛掉小于零的部分。嗯这就是Relu这种激活函数。还有其他种的，可以在下面的教程里面找到。不过现在都用relu。\n入门简单易懂：但是我没看明白什么叫神经元死了（ReLU）\nhttps://www.jianshu.com/p/22d9720dbf1a\n在第一篇的基础上往广度拓展，但是没有拓展深度，同样没有解释什么叫神经元没反应了，具体到feature map与conv层上，不过我们可以明确的是小心设置比较小的learning rate会让节点不轻易死掉\nhttps://blog.csdn.net/cyh_24/article/details/50593400\n很有趣的补充，涉及到人脑神经元的特性，稀疏性，单侧抑制，宽阔的阈值边界\nhttp://www.cnblogs.com/neopenx/p/4453161.html\n我觉得稀疏性可能对应我们人类这么一个现象，我们不会记住耳机的具体形状，我们只有注意力集中的时候才能学习，我们只有思考的时候才有收获，或者及水到渠成的灵光一闪。于是我们的意识可以控制我们的神经元。比如你扫一眼桌子，别说话，扫。\n你闭上眼睛，你不会记住你看到了什么，你眼睛的焦点不会自动对准每一个物体。但是如果你有目的扫过，你会发现，你桌子上的东西存在感比之前要强。呃，不过可能你的桌子比较整齐，没有我这么深的感触。\n至于稀疏激活，我认为是数值小于阈值的就让他等于0，这样我如果设置一个比较高的阈值，那么大部分都会被一致，也就是等于零。\n这里再说下他所说的信息解离，信息之间是有相关性的，特征向量就是信息，你一次采集了很多特征向量，任何一条信息都不能被其他信息线性组合表达，那么就是线性无关，如果我们做的是非线性处理，让我的数据之间不仅线性无关，完全就是无关。这个时候，信息的表达是最简介，少，且有价值，简直黄金信息。如果把这些相互牵连的冗余噪声比作赘肉的话，大概就会很重视它了。相信这个谁都不陌生，多少都听过。非常感兴趣可以看下多媒体技术，信息熵。\n1.2 BN batch normalization 归一化\n普通的机器学习归一化作为基础\nhttps://blog.csdn.net/qq_28618765/article/details/78221571\n把归一化应用与网络\nhttps://blog.csdn.net/hjimce/article/details/50866313\n2.流行网络举例\n2.1   ResNet\n1.第一个链接我给youtube,由于我在看Andrew Wang的CNN视频，里面提到了ResNet网络可以在加深网络的时候不回弹识别率，因为一般网络深度超过一个程度，错误率会上升。\nhttps://www.youtube.com/watch?v=GSsKdtoatm8\n2.给妹纸的深度学习教学（4）\nhttps://zhuanlan.zhihu.com/p/28413039\n这个右边的是针对网络变深，这时通道变多，那么我们可以先拧出水分，拿64个通道来稀疏一下，这个感觉，类似主成分分析，然后再用3*3大小的卷积模板开始提取特征。之后我们再给他还原回去\n解决梯度消失\nhttps://zh.gluon.ai/chapter_convolutional-neural-networks/resnet-gluon.html\n前面两个都没有说为什么残差网络好，或者说没有解释道这个点，梯度消失。应该和之前的死掉的神经元不是一个概念。\n油管一个中文的视频\n地址在这里，比之前推荐的要好，感觉说的更加明白，说出了关键的部分就是梯度的算法的不同，以及零一取让该层网络被忽略会比较有效，也比较有条理。我自己用文字总结了一下，在这里\n2.2 reinforcement learning\n这里说的是一个不同于有标签的监督性训练的，reward机制的训练。可以让机器像人一样玩游戏，像素游戏，通过奖惩机制让他得到更高的分数，比如alpha go，还有很多其他游戏等，这个也是个比较新的概念。教程有个油管小哥总结得很快，能够让你快速了解。然后他又推荐了一篇博客。有趣的是还有一个视频说的是热reinfocement在股票市场的应用，但是这里并不是股票预测，而是将machine培训成一个玩家，那么虽然课程里面的内容很新颖，但是由于是个互动式录屏，所以感觉一个小时的东西由于不断地交互被硬生生拉长到了两个小时，至于reinforcement learning也是在1.30出现。而且感觉讲师没有做到把复杂的东西讲简单，导致重点不明。记住预测股票是很难的，你需要很多信息包括各国整层，尤其是美国，财务报表，机构动向等等信息都要输入给机器。小股票噪声大难以预测，大股票比如微软，稳定，容易预测，但是没啥用，因为他稳。所以目前更好的方法是用reinforcement learning 去做一个对抗博弈模型，将你的机器拍养成巴菲特。那么感觉这也很难对吧。所以应用点在哪里，就在于短期预测，比如这5分钟究竟是买入还是卖出，这种人力大脑没法算出来的东西，可以参考下机器。\n2.3 YOLO vs RCNN\n在目标检测这件事上，yolo和RCNN是两种主流框架，yolo非常快，该算法作者在TED有一篇演讲，可以看一下。这歌油管小哥总结了下两者，他更加偏爱YOLO，毕竟快。有趣的是比YOLO原作者视频播放量还高。\n3.tensorflow 教程\n视频的话，这里有个小哥在线码代码。\n同时这里有个比较好的，比较有延展性的教学文字帖，他们还提供了一个jupyter notebook版本，这里。\n3.1 keras\n这里有一份keras的中文文档。这里有一份官方的中文文档。那么document其实不等同于教程，因为他更类似于一个字典。所以最好是从例子开始。这里推荐看看官方的例子，以及opencv提供的一份基础的，更加面向初学者的教程,该教程用到的示例代码可以在github下载到。另外官方还额外的推荐了一个专门修改微调现有的流行网络（resnet inception等）的博客。"}
