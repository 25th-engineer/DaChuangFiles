{"content2":"前言\n注意！注意！注意！本文是针对中国科学院计算所山世光教授于2017年1月7日于北京师范大学所做的《深度学习在计算机视觉中的应用与前景》讲座的内容总结梳理。\n1 视觉智能的内涵\n计算机视觉系统的任务就是像人一样描述摄像机拍摄到的内容。\n常见的视觉任务：\n距离估计\n目标检测与跟踪\n物体分割\n目标识别\n内容理解\n下图所示就是计算机视觉任务中的物体识别的一个例子，如单一物体（图中猫）的识别，或多物体的识别（如图中的狗、猫和鸭子等的识别）。\n从2012年到2016年，计算机视觉经历了跨越式发展。在ImageNet ILSRVRC 图像分类上 1000类Top5错误率：26%-> 3.6%。\n2 视觉跨越式发展源于深度卷积网络CNN\n2.1 计算机视觉的基本任务\n针对待检测图片，识别出图片中的目标物体对象；\n对图片内容进行理解，实现图片的语义分割。\n处理的流程图如下：\n2.2 深度学习的起源——生物神经网络\n深度学习的基本原理是“加权投票模型”，它来源与生物神经学系统中的神经元系统的启示。\n如图所示，生物学中，一个完成的神经元主要包括轴突和树突两大部分，神经信号的传递主要是轴突的神经末梢受体释放后经过突触间隙被树突的受体捕获，产生电位传递给胞体。胞体将获得的电信号进行汇总给出决策：产生激励信号或者抑制。\n最后整个大脑的决策就是所有神经元信号的加权投票决策。\n2.3 人工神经网络\n正是受到生物神经网络的启发，我们在计算机中构建人工神经网络模型。\n2.3.1 单一神经元模型\n加权求和（卷积）+ 非线性激活函数\n如图所示，我们的输入信号\nx1,x2,⋯,xn\nx_1,x_2, \\cdots,x_n 可以看作是神经元中来自其他神经元轴突的信号，对应的\nwk1,wk2,⋯,wkn\nw_{k1} , w_{k2}, \\cdots, w_{kn} 可以看作是对应的突触的信号的权值，中间的求和结点\n∑ni=0(wixi+b)\n\\sum_{i=0}^{n} (w_ix_i+b) 是对应细胞体的求和决策，再通过一个激活函数\nψ(⋅)\n\\psi(\\cdot) 输出对应的决策\nyk\ny_k 。\n激励函数\n我们常用的激活函数有Sigmoid 和tanh 等函数，因为这些函数有光滑、连续、可导、有界等优良的性质。\n2.3.2 人工神经网络\n一个神经元看的更远（视野更大、更宽）知识因为它站在了其他神经元的肩上。\n神经网络是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。\n如图所示，对于一幅图像的特征，low-level 是一些简单、底层的特征（如像素特征），到了mid-level 就是一些复杂的特征（如图像中的一些边之类的特征），到了high-level 就是更复杂的特征（如图像中边的组合特征）。处在high-level 的神经元此时它们的决策就更抽象重要。\n神经网络的训练\n网络结构认为确定\n训练集：“有标注” 的样本\n{Xk,Yk,k=1,2,⋯,N}\n\\{ {\\bf X}^k, {\\bf Y}^k,k=1,2,\\cdots,N\\}\n学习目标——权重值：每个神经元都有大量的\nwi\n{\\bf w}_i 需要学习\n学习方法——BP算法：基本原理：调整权重，使最后层输出错误率下降:\nerror=||Yo−Yg||\nerror = ||{\\bf Y}^o- {\\bf Y}_g||\n2.4 卷积神经网络（CNN）\n卷积神经网络主要层叠包括以下三个步骤+全连接层\n卷积层：局部连接（卷积、滤波器…)\n非线性激活\nPooling 层：下采样，降维\n下图是一个经典的CNN结构，称为LeNet-5网络，关于LeNet 的详解可以参考 LeNet-5 网络详解 。LeNet-5 网络共有7层（不包括输入层），每层都包含不同数量的训练参数，主要的有卷积层、下抽样层、全连接层3中连接方式。从中可以看出，CNN中主要有两种类型的网络层，分别是卷积层和池化/采样层（Pooling）。\n其中卷积层的作用是提取图像的各种特征；池化层的作用是对原始特征信号进行抽样，从而大幅减少训练参数，另外还可以减轻模型过拟合的程度。\n2.4.1 卷积层\n卷积层是卷积核在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，（通常还要再加上一个偏置参数），得到卷积层上的结果。如下图所示。\n下面的动图能够更好地解释卷积过程：\n从图中我们知道卷积的实质是局部连接（卷积、滤波器）、共享权重（若干种滤波器，大大减少学习的权重数），等价于信号中的滤波器。\n2.4.2 非线性激活\n在卷积神经网络中，我们采用的是Sigmoid 等非线性函数，原因是如果是线性激励，无论神经网络的层数有多少，最终都会坍塌为一个线性激励。\n2.4.3 池化/采样层（Pooling）\n通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化/采样(Pooling)处理。池化/采样的方式通常有以下两种：\nMax-Pooling: 选择Pooling窗口中的最大值作为采样值；\nMean-Pooling: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值；\n如下图所示：\n使用Pooling 技术的另一个目的也是为了获取不变性：\n小的平移不变性：有即可，不管在哪里\n旋转不变性：9个不同朝向的filters。\n2.4.4 CNN结构演化——新的功能模块\nCNN+RNN/LSTM\n其中CNN学习表示， RNN/LSTM 建模时序信号\n应用： Image Caption， Image QA， Viedo Representation\n上图所示：Show and tell: A neural image caption generator。\n上图所示： Long-term Recurrent Convolutional Networks 。\n深度神经网络已经由传统CNN 向RNN（CVPR14， TPAMI15）和 CNN Cascade 演进。\n3 计算机视觉应用——人脸识别系统\n全自动人脸识别系统流程：\n深度学习主要作用在特征提取器这一步上。\n特征提取器\n第一代：完全人工涉及特征\n也是非常底层的特征，包括几何特征，图像模板，Fourier 频谱，如形状，颜色，纹理，频谱等\n第一代特征是知识驱动\n第二代：（子空间）变换特征\n此时的特征是经过处理或变换后的特征，经过如PCA， LDA， LPP， SR 等特征变换处理后得到的特征。\n第二代特征是数据驱动（学习\nW\n\\bf W 矩阵，\ny=Wx\n\\bf y = Wx ）。\n第三代： 人工设计局部特征+ （子空间）变换特征\n如Gabor滤波器，LBP+PCA， LDA等\n第三代特征是知识+数据驱动，\ny=W(f(x))\n\\bf y = W(f(x)) 。\n第四代：特征学习\n局部特征参数可学习\n变换可学习\n非线性\n第四代特征是完全数据驱动的特征。\n卷积神经网络（CNN） 本质上是层次抽象的滤波型局部特征。\n其与之前局部特征的不同：\nGabor ：权值固定，人为设定（如加窗傅里叶型函数），没有目标函数\nCNN： 数据驱动的权值学习（最有利于目标函数达成的）。\n如图所示，理想的深度卷积神经网络是从数据中学习多层特征，如对图像中人脸的学习，可从底层的像素特征学习到第一层的边的特征，然后再到第2层基本脸部部位器官（如鼻子，嘴）等特征，再到高维的特征脸特征。\n4 总结\n自从深度学习与深度卷积神经网络取得了重大的突破，其给计算机视觉领域带来巨大的性能提升，深度模型极大推进了人脸识别能力，使得人脸的检测与识别不再具有特殊性，其甚至在一些场景上超过了人眼能力。\n深度学习本质上是特征的学习，它也是隐式学习，借鉴的是生物神经元的思维模式，虽然对于学习到的模型在理论上无法做出清晰明显的解释，但是其极高的性能提高能力还是令相关领域的专家学者报以很高的学习热情。\n深度学习也由一些不足之处：如学习模型的计算量大，缺少基础理论的支撑等。"}
