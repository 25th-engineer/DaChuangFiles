{"content2":"在看《Computer Vision:A Mordern Approach》第二版中关于“分类”一章时，书中讲道了处理图像训练数据的两个非常有用的tricks，特此记录下来，相信这两个数据处理的技巧无论是在传统的机器学习还是流行的深度学习中都很有指导意义。\n处理训练数据技巧1：数据增强\n在计算机视觉中，我们能够用一些非常简单的技巧来扩充训练样本，这对训练样本不足的情况是非常有用的。\n比如，一幅厨房图像，我们可以通过轻微地缩放(scale)、裁剪(crop)、旋转(rotate)和翻转(flip)来产生大量的正样本或负样本（不过，对负样本进行这样处理往往帮助并不大，因为负样本通常比较容易获得）。\n处理训练数据技巧2：bootstraping\n这个技巧可以避免许多冗余工作。我们训练一个样本子集，然后用剩下的样例去测试结果分类器，接着将被分类错误的正负样本插入到训练集中重新训练分类器，反复迭代，书中把这种处理方式叫做”bootstraping“,我个人把它理解为”自助法”。这样做的原因在于错误的正负样本对于寻找决策边界能够给出关于错误最重要的信息。\n这种方法有一个非常重要的变种，叫做”hard negative mining“，大致思路是从负样本中选取出一些有代表性的负样本，使得分类器的训练结果更好。它通常用在正样本数适中而负样本数非常多的情况下，比如用分类器检测物体。\n拿人脸检测举例：在一幅图像中，含有人脸的图像窗通常比较少，大多数是非人脸的图像窗。在这种情况下，我们在训练时不要使用所有的负样本，我们需要那些最有可能提高分类器性能的负样本。具体做法是先随机产生一些与正样本不重叠的”bounding box”作为负样本，然后利用正负样本训练分类器并测试分类效果。你的分类器可能会检测出不少错误的正样本（把非人脸区域预测为了人脸区域），”hard negative”就是指这些检测出的错误的正样本（非检测目标区域），把它们当作负样本加入到训练集中，重新训练分类器，它的性能应该会有所提升，检测出错误正样本的可能性会减小。"}
