{"content2":"01、自然语言处理与文本挖掘概述\n02、自动机及其应用，文稿自动校正，歧义消除\n03、语言模型，平滑方法。应用案例：语音识别，分词消岐\n04、概率图模型，生成式模型与判别式模型，贝叶斯网，马尔科夫链，隐马尔科夫模型HMM，应用案例：语音识别与分词\n05、马尔科夫网，最大熵模型，条件随机场CRF，实现HMM和CRF的软件。应用案例：使用最大熵消除歧义，使用CRF进行标注\n06、汉语分词专题。世界上最难的语言名不虚传\n07、命名实体识别，词性标注，从文本里挖出最重要的内容\n08、句法分析，找出句子的重点\n09、语义分析与篇章分析，让机器象语言学家那样思考\n10、文本分类，情感分析。应用案例：互联网自动门户，评论倾向性分析\n11、信息检索系统，搜索引擎原理，问答系统，应用案例：客服机器人是怎么造出来的？\n12、文本深度挖掘：自动文摘与信息抽取\n13、机器翻译与语音识别技术介绍 IBM Watson系统的认知智慧\n分词算法\n下载地址：百度网盘下载"}
{"content2":"人工智能\n人工智能英文缩写为AI，它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学研究领域的一个重要分支，又是众多学科的一个交叉学科，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括语音识别、图像识别、机器人、自然语言处理、智能搜索和专家系统等等，人工智能可以对人的意识、思维的信息过程的模拟。人工智能包括众多的分支领域，比如大家熟悉的机器学习、自然语言理解和模式识别等。\n机器学习\n机器学习属于人工智能研究与应用的一个分支领域。机器学习的研究更加偏向理论性，其目的更偏向于是研究一种为了让计算机不断从数据中学习知识，而使机器学习得到的结果不断接近目标函数的理论。\n机器学习，引用卡内基梅隆大学机器学习研究领域的着名教授TomMitchell的经典定义：\n如果一个程序在使用既有的经验E(Experience)来执行某类任务T(Task)的过程中被认为是“具备学习能力的”，那么它一定要展现出：利用现有的经验E，不断改善其完成既定任务T的性能(Performance)的特质。\n机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。在我们当下的生活中，语音输入识别、手写输入识别等技术，识别率相比之前若干年的技术识别率提升非常巨大，达到了将近97%以上，大家可以在各自的手机上体验这些功能，这些技术来自于机器学习技术的应用。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\nAI vs 深度学习 vs 机器学习，人工智能的 12 大应用场景\nhttp://www.duozhishidai.com/article-15385-1.html\n人工智能全景图与发展趋势分析\nhttp://www.duozhishidai.com/article-15301-1.html\n在网络大时代背景下，人工智能技术是如何应用的\nhttp://www.duozhishidai.com/article-15277-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"更多内容请至南木博客主页查看哦\n中文分词工具\nJieba\nSnowNLP\nTHULAC\nNLPIR\nNLPIR\nStanfordCoreNLP\nHanLP\n英文分词工具\nnltk\nnltk\nnltk\nSpacy\nSpacy\nStanfordCoreNLP\n更多关于自然语言处理的内容，请转至\n自然语言处理（一）——中英文分词\n自然语言处理（二）——词性标注与命名实体识别\n自然语言处理（三）——句法分析与依存句法分析\n哈工大依存句法分析工具——LTP的使用与安装\n进行查看\n同时也欢迎各位关注我的微信公众号 南木的下午茶"}
{"content2":"我是一名计算机学院的大二学生，本学期选修了大数据与人工智能这门课，通过一学期的学习，让我对大数据与人工智能有了更多的了解，也让我深深地感受到人工智能在当今社会的广泛应用，技术正在快速崛起，不断延伸到各个行业当中。人工智能发展到现在以及拥有很多的分支，包括机器学习，神经网络，自然语言处理，深度学习等等，所以接下来我想谈谈人工智能自然语言处理在计算机领域的应用。\n人工智能技术主要研究的目的是使一台计算机或者是一台机器完成一些需要我们人类亲自动手或者动脑来完成的工作。因此，人工智能的发展历史和计算机科学与技术的发展历史密不可分。自然语言处理（NLP）是人工智能的一个分支，用于分析、理解和生成自然语言，以方便人和计算机设备进行交流，以及人与人之间的交流。NLP是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间相互作用的领域。因此，自然语言处理是与人机交互的领域有关的。那么NLP和我的专业又有哪些联系呢?\n1.帮助我完成学习任务。前段时间和同学要完成一个班的小论文相似度的分析，这里面就需要运用NLP的相关知识，我们首先把每篇文章进行分词等一系列操作，最后再进行文本聚类得到结果，分词就是nlp的一个应用。除此之外，只要我一直在计算机领域学习下去，那么用到自然语言处理技术的地方也会越来越多，像数据挖掘，深度学习等等，NLP是一种处理数据的手段，它让我们看到得到更直观的结果。\n2.找工作。随着大数据的发展，人工智能得到了全新的发展机遇，人工智能技术越来越多的应用到我们的工作和日常生活中，人工智能相关职业的发展前景还是非常值得期待的，相关的产业也会得到进一步的发展，相关的人才需求会得到进一步的释放，所以从事人工智能的相关工作是一个不错的选择。但是，不管从事什么行业，打铁还需自身硬，现在我的主要任务就是学习，不断充实自己。\n尽管NLP不如大数据，机器学习听起来那么火，但实际上我们每天都在使用它。文本语音翻译，信息检索，家里的聊天机器人等等。现在人工智能领域应用广泛，我们也要把握时代，不断前进。"}
{"content2":"----------欢迎加入学习交流QQ群：657341423\n自然语言处理是人工智能的类别之一。自然语言处理主要有那些功能？我们以百度AI为例\n从上述的例子可以看到，自然语言处理最基本的功能是词法分析，词法分析的功能主要有：\n分词分句\n词语标注\n词法时态（适用于英文词语）\n关键词提前（词干提取）\n由于英文和中文在文化上存在巨大的差异，因此Python处理英文和中文需要使用不同的模块，中文处理推荐使用jieba模块，英文处理推荐使用nltk模块。模块安装方法可自行搜索相关资料。\n英文处理\nimport nltk f = open('aa.txt','r',encoding='utf-8') text = f.read() f.close() ---------- # sent_tokenize 文本分句处理，text是一个英文句子或文章 value = nltk.sent_tokenize(text) print(value) # word_tokenize 分词处理,分词不支持中文 for i in value: words = nltk.word_tokenize(text=i) print(words) ---------- # pos_tag 词性标注,pos_tag以一组词为单位，words是列表组成的词语列表 words = ['My','name','is','Lucy'] tags = nltk.pos_tag(words) print(tags) ---------- # 时态，过去词，进行时等 # 词语列表的时态复原，如果单词是全变形的无法识别 from nltk.stem import PorterStemmer data = nltk.word_tokenize(text=\"worked presumably goes play,playing,played\",language=\"english\") ps = PorterStemmer() for w in data: print(w,\":\",ps.stem(word=w)) # 单个词语的时态复原，如果单词是全变形的无法识别 from nltk.stem import SnowballStemmer snowball_stemmer = SnowballStemmer('english') a = snowball_stemmer.stem('plays') print(a) # 复数复原，如果单词是全变形的无法识别 from nltk.stem import WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() a = wordnet_lemmatizer.lemmatize('leaves') print(a) ---------- # 词干提取,提前每个单词的关键词，然后可进行统计，得出词频 from nltk.stem.porter import PorterStemmer porter = PorterStemmer() a = porter.stem('pets insurance') print(a) ---------- from nltk.corpus import wordnet word = \"good\" # 返回一个单词的同义词和反义词列表 def Word_synonyms_and_antonyms(word): synonyms = [] antonyms = [] list_good = wordnet.synsets(word) for syn in list_good: # 获取同义词 for l in syn.lemmas(): synonyms.append(l.name()) # 获取反义词 if l.antonyms(): antonyms.append(l.antonyms()[0].name()) return (set(synonyms), set(antonyms)) # 返回一个单词的同义词列表 def Word_synonyms(word): list_synonyms_and_antonyms = Word_synonyms_and_antonyms(word) return list_synonyms_and_antonyms[0] # 返回一个单词的反义词列表 def Word_antonyms(word): list_synonyms_and_antonyms = Word_synonyms_and_antonyms(word) return list_synonyms_and_antonyms[1] print(Word_synonyms(word)) print(Word_antonyms(word)) ---------- # 造句 print(wordnet.synset('name.n.01').examples()) # 词义解释 print(wordnet.synset('name.n.01').definition()) ---------- from nltk.corpus import wordnet # 词义相似度.'go.v.01'的go为词语，v为动词 # w1 = wordnet.synset('fulfil.v.01') # w2 = wordnet.synset('finish.v.01') # 'hello.n.01'的n为名词 w1 = wordnet.synset('hello.n.01') w2 = wordnet.synset('hi.n.01') # 基于路径的方法 print(w1.wup_similarity(w2))# Wu-Palmer 提出的最短路径 print(w1.path_similarity(w2))# 词在词典层次结构中的最短路径 print(w1.lch_similarity(w2))# Leacock Chodorow 最短路径加上类别信息 # 基于互信息的方法 from nltk.corpus import genesis # 从语料库加载信息内容 # brown_ic = wordnet_ic.ic（'ic-brown.dat'） # nltk自带的语料库创建信息内容词典 genesis_ic = wordnet.ic(genesis,False,0.0) print(w1.res_similarity(w2,genesis_ic)) print(w1.jcn_similarity(w2,genesis_ic)) print(w1.lin_similarity(w2,genesis_ic))\n由于上述的方法是建立在语料库中，有时候一些不被记录的单词可能无法识别或标注。这时候需要自定义词性标注器，词性标注器的类型有几种，具体教程可以看——>自定义词性标注器\n中文处理\nimport jieba import jieba.analyse f = open('aa.txt','r',encoding='utf-8') text = f.read() f.close() ---------- # 分词 seg_list = jieba.cut(text, cut_all=True) print(\"Full Mode: \" + \"/ \".join(seg_list)) # 全模式 seg_list = jieba.cut(text, cut_all=False) print(\"Default Mode: \" + \"/ \".join(seg_list)) # 精确模式 seg_list = jieba.cut_for_search(text) # 搜索引擎模式 print(\", \".join(seg_list)) ---------- # 关键字提取 # 基于TF-IDF算法的关键词抽取 # sentence 为待提取的文本 # topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20 # withWeight 为是否一并返回关键词权重值，默认值为 False # allowPOS 仅包括指定词性的词，默认值为空，即不筛选 keywords = jieba.analyse.extract_tags(sentence=text, topK=20, withWeight=True, allowPOS=('n','nr','ns')) # 基于TextRank算法的关键词抽取 # keywords = jieba.analyse.textrank(text, topK=20, withWeight=True, allowPOS=('n','nr','ns')) for item in keywords: print(item[0],item[1]) ---------- # 词语标注 import jieba.posseg # 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。 posseg = jieba.posseg.POSTokenizer(tokenizer=None) words = posseg.cut(text) for word, flag in words: print('%s %s' % (word, flag))\njieba分词也是基于语料库，我们可以对原有的语料库添加词语，或者导入自定义的语料文件，如下所示：\n# 对原有的语料库添加词语 jieba.add_word(word, freq=None, tag=None) # 导入语料文件 jieba.load_userdict('disney.txt')\n语料文件格式如下：每行分三个部分（用空格隔开），词语 词频（可省） 词性（可省）。ns是词语标记，词语和标注之间用空格隔开，txt文件格式为uft-8\njieba更多教程——>jieba教程"}
{"content2":"最近在研究自然语言处理，最基础的内容之一是分词处理，但是分词的结果并非均是有效的信息，按照普遍说法，存在‘停用词’这样的尴尬信息。\n所谓‘停用词’，即是在自然语言处理时，与文章包含的情感信息，或文章主题信息关系性不强的词语，所以如果进行筛选过滤之后，更便于主题分析，或者情感分析。\n这里，我在网上找到了：\n结合哈工大停用词表、四川大学机器智能实验室停用词库、百度停用词表、以及网络上较大的一份无名称停用词表，\n并整理了一下，做了去重处理，最终得到了一份较全的停用词表，在此分享出来给大家，希望对各位有用。\n整合的停用词表下载\n后续可能即需更新其他相关文章，逐步积累，哈哈。"}
{"content2":"持续更新中…\n使用的是《python自然语言处理》这本书，只给部分笔者做的答案，不敢保证都对，仅供参考\n我的目录\n持续更新中...\n第一章\n1.4\n1.6\n1.13\n1.14\n1.18\n1.21\n1.22\n2.23\n2.24\n2.25\n2.26\n2.28\n第一章\n1.4\nlen(text2) #先弄成都小写，去掉大小写区别，在求个数 len(set([w.lower() for w in set(a)]))\n1.6\ntext2.dispersion_plot(['Elinor','Marianne','Edward','Willoughby'])\n1.13\n1.14\n1.18\nsorted(set(sent1+sent2+sent3...))\n1.21\n1.22\nwords = sorted([w.lower() for w in text5 if len(w)==4]) fdist = FreqDist(words) fdist.most_common() #or plot the first 10 words fdist.plot(10)\n2.23\n列表：\n[w for w in text6 if w.isupper()]\n每行一个：\n2.24\n(a)\n[w for w in text6 if w.endswith('ize')]\n[]\n(b)\n可以看到上面有重复的单词，故将上面的text6改为set(text6)\n©\n(d)\n[w for w in set(text6) if w.istitle()]\n2.25\n2.26\n用来求全篇的字符/字母长度，如果要求字的平均长度可以如下\n2.28\ndef percent(word,text): return FreqDist(text)[word]/len(text)"}
{"content2":"自然语言处理技术的一些应用\n转载：https://zhuanlan.zhihu.com/p/31388720\n自然语言处理（NLP）是现代计算机科学和人工智能领域的一个重要分支，是一门融合了语言学、数学、计算机科学的科学。这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n近段时间，笔者由于做了一些信息流内容平台相关的工作，对NLP的一些应用有了一定了解，所以和大家分享一下。\n1. 词法分析\n基于大数据和用户行为，对自然语言进行中文分词、词性标注、命名识体识别，定位基本语言元素，消除歧义，支撑自然语言的准确理解。\n中文分词 —— 将连续的自然语言文本，切分成具有语义合理性和完整性的词汇序列\n词性标注 —— 将自然语言中的每个词，赋予一个词性，如动词、名词、副词\n命名实体识别 —— 即专有名词识别，识别自然语言文本中具有特殊意义的实体，如人名、机构名、地名\n举例：\n2. 依存句法分析\n利用句子中词与词之间的依存关系，来表示词语的句法结构信息，并用树状结构来表示整句的结构。依存句法分析主要有几大作用：\n精准理解用户意图。当用户搜索时输入一个query，通过依存句法分析，抽取语义主干及相关语义成分，实现对用户意图的精准理解。\n知识挖掘。对大量的非结构化文本进行依存句法分析，从中抽取实体、概念、语义关系等信息，构建领域知识。\n语言结构匹配。基于句法结构信息，进行语言的匹配计算，提升语言匹配计算的准确率。\n举例：\n3. 词向量表示\n词向量计算是通过训练的方法，将语言词表中的词映射成一个长度固定的向量。词表中的所有词向量构成了一个向量空间，每一个词都是这个向量空间中的一个点。利用这种方法，实现文本的可计算。主要应用在：\n快速召回结果。不同于传统的倒排索引结构，构建基于词向量的快速索引技术，直接从语义相关性的角度召回结果。\n个性化推荐。基于用户的过去行为，通过词向量计算，学习用户的兴趣，实现个性化推荐。\n举例：\n4. DNN语言模型\n语言模型是通过计算给定词组成的句子的概率，从而判断所组成的句子是否符合客观语言表达习惯。通常用于机器翻译、拼写纠错、语音识别、问答系统、词性标注、句法分析和信息检索等。\n举例：\n5. 词义相似度\n用于计算两个给定词语的语义相似度，基于自然语言中的分布假设，即越是经常共同出现的词之间的相似度越高。词义相似度是自然语言处理中的重要基础技术，是专名挖掘、query改写、词性标注等常用技术的基础之一。主要应用：\n专名挖掘 —— 通过词语间语义相关性计算寻找人名、地名、机构名等词的相关词，扩大专有名词的词典，更好的辅助应用\nquery改写 —— 通过寻找搜索query中词语的相似词，进行合理的替换，从而达到改写query的目的，提高搜索结果的多样性\n举例：\n6. 短文本相似度\n短文本相似度计算服务能够提供不同短文本之间相似度的计算，输出的相似度是一个介于-1到1之间的实数值，越大则相似度越高。这个相似度值可以直接用于结果排序，也可以作为一维基础特征作用于更复杂的系统。\n举例：\n7. 评论观点抽取\n自动分析评论关注点和评论观点，并输出评论观点标签及评论观点极性，包括美食、酒店、汽车、景点等，可帮助商家进行产品分析，辅助用户进行消费决策。\n举例：\n\n8. 情感倾向分析\n针对带有主观描述的中文文本，可自动判断该文本的情感极性类别并给出相应的置信度。情感极性分为积极、消极、中性。情感倾向分析能帮助企业理解用户消费习惯、分析热点话题和危机舆情监控，为企业提供有力的决策支持。\n举例："}
{"content2":"目录\n文章目录\n目录\n前言\n句法分析技术1\n句法分析技术2\n句法分析技术3\n句法分析技术4\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n句法分析技术1\n基于规则+统计结合的句法分析\n判定输入的词序列是否合法，短语结构树，有向无环图。\n句子:{主『定语，中心』}{谓语『状，谓{动宾【动，宾语（定语，中心词）】，补语}』}\n状语修饰，核心动作。\n提高语法分析结果，计算机的语法分析里面不明确。\n词性层级：两种句法分析的区别因子进入短语结合规则。\n句法分析和短语结合分析进入区别。\n语法歧义示例。\n汉语句法分析，句法分析细语，形式语法体系。\n匹配模式方法，“正则文法”。\n短语结构文法，信息处理系统。机器翻译运用，留下此路不通的牌子。\n科研有风险，不是一帆风顺，需要有挑战精神的人去做。\n扩充转移网络，状态转移机，树邻接语法\n句法分析技术2\n基于合一运算的语法，复杂描述集的语法，合一运算实现该方法，依存语法，上下文颗粒度太大，短语限定在词汇上，K+语法，依存文法，形式语法体系模式，正则匹配。\n短语结构语法分析很多方法。\n扩充转移网络\n回顾：Chomsky文法体系\nG\n=\n（\nN\n，\n∑\n,\nP\n,\nS\n）\nG=（N，\\sum,P,S）\nG=（N，∑,P,S）是一个文法，\nα\n−\n&gt;\nβ\n∈\nP\n\\alpha-&gt;\\beta\\in P\nα−>β∈P\n0型文法：对\nα\n−\n&gt;\nβ\n不\n作\n任\n何\n限\n制\n\\alpha-&gt;\\beta 不作任何限制\nα−>β不作任何限制\nI型文法：\n∣\nα\n∣\n≤\n∣\nβ\n∣\n|\\alpha|\\leq|\\beta|\n∣α∣≤∣β∣\nII型文法：\n上\n下\n文\n无\n关\n文\n法\n，\nα\n∈\nN\n上下文无关文法，\\alpha \\in N\n上下文无关文法，α∈N\nIII型文法：正则文法。\n一个字串的推导是一系列文化规则的应用。\n起始符推导到最好。强范式：基于词的语法。\n格里巴克：形式语言自动化机。\n一种语言\nL\ng\nL_g\nLg 是由某上下文无关文法推导出来的所有终结符号串的集合，其中的每个终结符串，称为合乎文法G，否则，称之为不合乎文法。上下文文法，扩充概率无关文法。\n一个随机上下文无关语法，PCFG的三个假设。\n1）位置无关2）上下文无关3）祖先无关。\n推出非总结串，隐码模型，推出问题。\nPCFG的三个基本问题。\n一个语句\nW\n=\nW\ni\nW\ni\n−\n1\nW\ni\n−\n2\nW\nn\nW=W_iW_{i-1}W_{i-2}W_n\nW=Wi Wi−1 Wi−2 Wn 的P(W|G)也就是产生语句W的概率？\n在语句W的句法结构有歧义的情况下，如何快速选择最佳的语法分析（parse）？\n如何从语料库中训练G的概率参数使得P(W|G)最大（类比之前的问题，评价，解码，编码问题）\n节点间的递推关系，叶节点到根节点的句法树。\n向内算法\n句法分析技术3\n随机上下文无关文法\n任何一个语句都可以视为一种语言模型。\n一个句法树中的结点词句法树开始推导，自顶向下，自下向上。\n某一部推导，对应于几个规则，开始推导，做出结果。\n登上算法，尝试去做，EM算法，优化前进，无指导学习算法，PCFG的优点。\n可以对句法分析的歧义，结果进行概率排序。\n提高文法的容错能力。\n词对结构分析，上下文对结构分析，随机上下文无关文法。\n向前算法，节点值增加提前。\nα\ni\nj\n(\nA\n)\n=\nP\n(\nW\ni\n,\nW\nj\n∣\nA\n)\n,\ni\n&lt;\nj\n\\alpha_{ij}(A)=P(W_i,W_j|A),i&lt;j\nαij (A)=P(Wi ,Wj ∣A),i<j\n=\n∑\nB\n,\nC\n,\n∈\nR\nP\n(\nW\ni\n,\nW\nj\n,\nB\n,\nW\nr\n+\n1\n.\n.\nW\nj\n,\nC\n∣\nA\n)\n=\\sum_{B,C,\\in R}P(W_i,W_j,B,W_{r+1}..W_j,C|A)\n=B,C,∈R∑ P(Wi ,Wj ,B,Wr+1 ..Wj ,C∣A)\nα\ni\n,\nj\n=\nP\n(\nA\n−\n&gt;\nW\ni\n)\ni\n=\nj\n\\alpha_{i,j}=P(A-&gt;W_i)i=j\nαi,j =P(A−>Wi )i=j\n句法分析技术4\n浅层句法分析，形式合规分析，结构分析就行。\n部分分析，组块分析。\n例句：\n这一切已经引起世界各国的普遍关注。\nS-k,r,c,p.\n浅层专项研究。\n基于HMM的浅层分析技术，ACL会议。他识别的目标是非递归的NLP，浅层句法分析，隐码是五元组，浅层分析状态空间如何定义。输出一对词性标记，一个组块开始。\n照着看，任何阶段都可以用任何一个模型，不同的是标记的内容。\n级联式有限状态分析句法。\n# 句法分析技术5\n基于规则的方法，需要大量人力，不好迁移。\n总结：\n概率上下文无关文法，句法分析是目前语言处理技术瓶颈之一。发现问题比解决问题更重要。\n句法分析是必由之路，ACL每年关注，语法分析。\n强化学习技术：免疫机制分析合适吗？\n句法是形式，语义是内容。\n完整合法性，没有公认的内容。\n句法的强制性和语义的决定性，句法系统和语义系统是两个不同的系统，它们各自独立而又相互依存，彼此的对应关系十分复杂，统计规则之后讲应用。"}
{"content2":"奇虎360面试主要考察的知识点：\n1.机器学习常用的分类算法，Logistic回归，SVM，Decision Tree，随机森林等相关分类算法的原理，公式推导，模型评价，模型调参。模型使用场景\n2.机器学习常用的聚类算法，Kmeans,BDSCAN，SOM（个人论文中使用的算法），LDA等算法的原理，算法（模型）中参数的确定，具体到确定的方法；模型的评价，例如LDA应该确定几个主题，Kmeans的k如何确定，DBSCAN密度可达与密度直达。模型使用场景\n3.特征工程：特征选择，特征提取，PCA降维方法中参数主成分的确定方法，如何进行特征选择\n4.Boosting和bagging的区别\n5.数据如何去除噪声，如何找到离群点，异常值，现有机器学习算法哪些可以去除噪声\n6.HMM与N-gram模型之间的区别\n7.梯度消失与梯度爆炸\n8.奥卡姆剃须刀原理\n9.TCP三次握手的原理，为什么是三次而不是其他次\n10.进行数据处理时，如何过滤无用的信息（例如利用正则表达式提取或者其他方法），数据乱码的处理\n11.交叉熵与信息熵，信息增益与信息增益率，gini系数，具体如何计算\n12.BIC准则（贝叶斯信息准则）与AIC（赤池信息准则）\n13.需要手写代码（此次面试：字符串的操作）\n14.前向传播与反向传播\n15.常见的损失函数\n最后面试官给的建议：1.多看（看文献，看别人的成果）2.视野要打开，需要进一步扩宽知识面，在校期间更多关注理论，工作时没有太多的时间关注理论而更多是偏向业务，理论：数学，统计学，同时需要多读多练\n向面试官提问时，尤其是技术类面试，尽量不要用短时间内这个词，毕竟技术是一个积累的过程。\n原创申明：此博文为博主原创，未经允许不得转载"}
{"content2":"NLP概述\nNLP是利用计算机为工具，对人类特有的书面形式和口头形式的自然语言的信息进行各种类型处理和加工的技术.\nNLP内容结构\nNLP基础技术\n词法分析\n词法分析目的是从句子中分出单词，找出词汇的各个词素，从中获得单词的语言学信息并确定单词的词性. 词法分析是很多中文信息处理任务的必要步骤.\n自动分词\n命名实体识别\n词性标注\n句法分析\n句法分析是对句子和短语结构进行分析，如句子的形式结构：主语、谓语、宾语等. 句法分析是语言学理论和实际的自然语言应用的一个重要桥梁. 一个实用的、完备的、准确的句法分析将是计算机真正理解自然语言的基础.\n短语结构分析(宾州树库)\n依存分析\n语义分析\n解释自然语言句子或篇章各部分(词、词组、句子、段落、篇章)的意义. 目前语义计算的理论、方法、模型尚不成熟.\n词义消歧(词)\n语义归纳、推理(词)\n语义角色标注(句子)\n篇章分析\n指超越单个句子范围的各种可能分析，包括句子（语段）之间的关系以及关系类型的划分，段落之间的关系的判断，跨越单个句子的词与词之间的关系分析，话题的继承与变迁等.\nNLP核心应用\n机器翻译(Machine translation, MT)\n信息检索(Information Retrieval)\n信息抽取(Information Extraction)\n自动文摘(Automatic summarization/abstracting)\n问答系统(Question-Answering system)\n阅读理解(Machine Reading)\n文档分类(Document categorization)\n情感分类(Sentimental classification)\n信息推荐与过滤(Formation Recommendation and Filtering)\nNLP技术及应用架构\nNLP领域的学术会议\nACL（Association of Computational Linguistics）\nColing（International Conference on Computational Linguistics）\nEMNLP（Conference on Empirical Methods in Natural language Processing）\nEACL(European Chapter of ACL)\nIJCNLP(International Joint Conference on Natural language Processing)\nSIGIR(SIG Information Retrieval)\nTREC(Text REtrievalConference)\nJSCL(全国计算语言学联合学术会议)\n国内NLP研究组\nTencent AI Lab\n苏州大学NLP实验室\n微软亚洲研究院自然语言计算组NaturalLanguageComputing(NLC)Group\n头条人工智能实验室\n清华大学自然语言处理与社会人文计算实验室\n清华大学智能技术与系统国家重点实验室信息检索组\n北京大学计算语言学教育部重点实验室\n北京大学计算机科学技术研究所语言计算与互联网挖掘研究室\n哈工大社会计算与信息检索研究中心\n哈工大机器智能与翻译研究室\n哈尔滨工业大学智能技术与自然语言处理实验室\n中科院计算所自然语言处理研究组\n中科院自动化研究所语音语言技术研究组\n南京大学自然语言处理研究组\n复旦大学自然语言处理研究组\n东北大学自然语言处理实验室\n厦门大学智能科学与技术系自然语言处理实验室\n参考资料\n中国科学院大学-NLP课程课件(IIE胡玥老师主讲)"}
{"content2":"中文语言的机器处理\n直观上，一个自然语言处理系统最少三个模块：语言的解析、语义的理解及语言的生成。\n计算机处理自然语言最早应用在机器翻译上，此后在信息检索、信息抽取、数据挖掘、舆情分析、文本摘要、自动问答系统等方面都获得了很广泛的应用。虽然已经产生了许多专业技术作用域语言理解的不同层面和不同任务，例如，这些技术包括完全句法分析、浅层句法分析、信息抽取、词义消歧、潜在语义分析、文本蕴含和指代消解，但是还不能完美或完全地译解出语言的本义。\n命名实体识别：主要用来识别语料中专有名词和未登录词的成词情况，如人名、地名\n组织机构名称等，也包括一些特别的专名。准确的命名实体识别一准确的分词和词性标注为前提。\n语义组块：用来确定一个以上的词汇构成的短语结构，即短语级别的标注，主要识别名词性短语、动词性短语、介词性短语等，以及其他类型的短语结构。语义组块的自动识别来源于中文分词、词性标注和命名实体识别的共同信息。语义组块的识别特征必须包含中文分词和词性标注两部分。\n语义角色标注：以句子中的谓语动词为中心预测出句子中各个语法成分的语义特征，是句子解析的最后一个环节，也是句子级别研究的重要里程碑。语义角色标注直接受到句法解析和语义组块的影响。\n词性标注（Part-of-Speech Tagging 或POS Tagging）：又称为词类标注，是指判断出在一个句子中每个词所扮演的语法角色。例如：表示人、事物、地点等的名称为名词，表示动作或状态变化的词为动词等。一个词可能具有多个词性。一般而言，中文的词性标注算法比较同意，大多数使用隐马尔科夫模型（HMM）或最大熵算法，如结巴分词的词性标注。为了获得更高的精度，也有使用条件随机场（CRF）算法的，如LTP3.3 中的词性标注。中文词性标签有两大类：北大词性标注集和宾州词性标注集。\n句法分析：是根据给定的语法体系自动推导出句子的语法结构，分析句子所包含的语法单元和这些语法单元之间的关系，将句子转化为一棵结构化的语法树。目前句法分析有两种不同的理论：一种是短语结构语法，另一种是依存语法。\n哈工大NLP平台\n哈工大语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心研发的一整套中文语言处理系统。语言技术平台包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标准等丰富、高效、精准的自然语言处理技术，还可以通过可视化的图形输出，使用户一目了然。（P-10）\nStanford NLP 团队\n斯坦福自然语言处理团队（http://nlp.stanford.edu/）是一个由斯坦福大学的教师、科研人员、博士后、程序员组成的团队。该团队致力于研究计算机理解人类语言的工作，涵盖诸如句子的理解、机器翻译、概率解析和标注、生物医学信息抽取、语法归纳、词义消歧、自动问答及文本区域到3D场景的生成等。\n在某些中文NLP应用中局域卓越的性能，一些主要的中文NLP应用如下：\n（1）斯坦福句法解析器\n概率自然语言句法解析器包括PCFG（与概率的上下文无关的短语）和依存句法解析器，一个词汇的PCFG解析器，以及一个超快速的神经网络的依存句法解析器和深度学习重排序器。在线句法分析器演示：http://nlp.stanford.edu:8080/parser/index.jsp\n（2）斯坦福命名实体识别器\n该识别器基于条件随机场序列模型，用于英文、中文、德文、西班牙文的连同命名实体识别、以及一个在线NER演示。\n（3）斯坦福词性标注器\n基于最大熵（CMM）算法、词性标注（POS）系统包括英语、阿拉伯语、汉语、法语、德语和西班牙语。\n（4）斯坦福分析器\n基于CRF算法的分词器，支持阿拉伯语和汉语"}
{"content2":"Python+NLTK自然语言处理学习（一）：环境搭建\nPython+NLTK自然语言处理学习（二）：常用方法（similar、common_contexts、generate）\nPython+NLTK自然语言处理学习（三）：计算机自动学习机制"}
{"content2":"统计语言模型：自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递方式。因此让计算机处理自然语言，一个基本问题就是为自然语言这种上下文相关的特性建立数学模型，这个数学模型就是在自然语言处理中常说的统计语言模型(Statistical Language Model)。它是今天所有自然语言处理的基础，并且广泛应用于机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询。\n目录\n1.用数学的方法描述语言规律\n2. 高阶语言模型\n3.模型的训练、零概率问题和平滑方法\n1.用数学的方法描述语言规律\n统计语言模型产生的初衷是为了解决语音识别问题。在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解并且有意义的句子，然后显示或打印给使用者。\n比如：\n这句话就很通顺，意义也很明白。\n如果改变一些词的顺序，或者替换掉一些词，将这句话变成：\n意思就含混了，虽然多少还能猜到一点。\n但如果再换成：\n基本上读者就不知所云了。\n第一个句子合乎语法，词义清晰。第二个句子虽不合乎语法，但是词义还算清晰。而第三个句子则连词义都不清晰了。上世纪 70 年代以前，科学家们也是这样想的，他们试图判断这个文字序列是否合乎文法、含义是否正确等。但是语言的结构千变万化，要通过制定规则来覆盖所有的文法根本是不可能的。而弗里德里克·贾里尼克(Frederick Jelinek)换了一个角度，用一个简单的统计模型就很漂亮地搞定了这个问题。\n贾里尼克想法\n贾里尼克的出发点很简单：一个句子是否合理，就看它的可能性大小如何。上面的例子中，第一个句子出现的概率大致是，第二个句子出现的概率是 ，第三个句子出现的概率是 。因此第一个句子出现的可能性最大，是第二个句子的 10万倍，是第三个句子的一百亿亿亿亿亿亿倍。\n用更普遍而严格的描述是：\n假定 S 是一个有意义的句子，由一连串特定顺序排列的词 𝑤1,𝑤2,⋯,𝑤𝑛组成，n为句子的长度。那么 S 在文本中出现的可能性就是 S 的概率 P(S)。于是可以把 P(S) 展开表示为：\n利用条件概率公式，S 这个序列出现的概率等于每一个词出现的条件概率相乘，于是：\n其中表示句子第一个词为的概率；是在已知第一个词的前提下，第二个词出现的概率，以此类推。不难看出，词的出现概率取决于他前面的所有词。\n从计算上来看，第一个词的条件概率 𝑃(𝑤1) 很容易算，第二个词的条件概率 𝑃(𝑤2∣𝑤1)也还不太麻烦，但是从第三个词的条件概率 𝑃(𝑤3∣𝑤1,𝑤2)开始就非常难算了，因为它涉及到三个变量 𝑤1,𝑤2,𝑤3而每个变量的可能性/可能取值都是语言字典的大小。到了最后一个词 𝑤𝑛，条件概率 𝑃(𝑤𝑛∣𝑤1,𝑤2,⋯,𝑤𝑛−1) 的可能性太多，根本无法估算。\n二元模型与N元模型\n从 19 世纪到 20 世纪初，俄国有个数学家叫马尔可夫(Andrey Markov)，他提出了一种偷懒但还颇为有效的方法：假设任意一个词语 出现的概率只同它前面的词 有关。于是问题就变得很简单了，这种假设在数学上称为马尔可夫假设。\n现在，句子S出现的概率就变得简单了：\n上面的公式对应的统计语言模型是二元模型(Bigram Model)。当然，也可以假设一个词由前面的 N−1 个词决定，对应的模型稍微复杂些，被称为 N 元模型。\n接下来的问题就是如何估计条件概率 。根据它的定义:\n而估计联合概率和边缘概率很简单。根据大数定理，只要统计量足够，相对频率就等于概率，因而只需在语料库(corpus)的文本中统计一下这两个词前后相邻出现的次数,以及出现了多少次，然后把这两个数分别处以语料库大小N，即可得到这些词或2元组的概率：\n于是：\n更一般的，对于n-gram：\n这似乎有点难以置信，用这么简单的数学模型就能解决复杂的语音识别、机器翻译等问题，而用很复杂的文法规则和人工智能却做不到。其实很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法更有效。\n2. 高阶语言模型\n在基于一阶马尔可夫假设的二元模型(bi-gram/2-gram)中，句子中每个词只和前面一个词有关，这似乎过于简化了，或者说近似地过头了。比如说在句子“美丽的花朵”中，“花朵”其实是和“美丽”有关，也就是说是与前面的第二个词有关。因此，更普遍的假设是某个词和前面的若干个词有关。\n正如之前介绍的那样，N 元模型(n-gram)假设每个词 和前面的 N−1 个词有关，与更前面的词无关(不是与前面所有的词相关)，这样词  的概率只取决于前面的 N−1 个词 。因此:\n这种假设被称为 N−1 阶马尔可夫假设，对应的语言模型称为 N 元模型(N-Gram Model)。N=2时就是之前介绍的二元模型，而 N=1 的一元模型实际上是一个上下文无关模型，即假定当前词的出现概率与前面的词无关。在实际中应用最多的就是 N=3 的三元模型(trigram/3-gram)，更高阶的模型就很少使用了。\n为什么N取值那么小？\n我们之前在上一篇博客中曾经探讨过这个问题：\n1.首先，N 元模型的大小（空间复杂度）几乎是 N 的指数函数，即 ，这里 |V|是一种语言词典的词汇量，一般在几万到几十万个。其次，使用 N 元模型的速度（时间复杂度）也几乎是一个指数函数，即 。因此，N 不能很大。\n2.当 N 从 1 到 2，再从 2 到 3 时，模型的效果上升显著。而当模型从 3 到 4 时，效果的提升就不是很显著了，而资源的耗费却增加地非常快。所以，除非是为了做到极致不惜资源，很少有人会使用四元以上的模型。\n还有一个问题，三元、四元或更高阶的模型也并不能覆盖所有的语言现象。在自然语言处理中，上下文之间的相关性可能跨度非常大，甚至可以从一个段落跨到另一个段落。因此，即便再怎么提高模型的阶数，对这种情况也无可奈何，这就是马尔可夫模型的局限性，这时就需要采用其他一些长程的依赖性(Long Distance Dependency)来解决这个问题了,如之后学习的神经语言模型LSTM/GRU等可以很好的解决这个问题。\n3.模型的训练、零概率问题和平滑方法\n语言模型中所有的条件概率称为模型的参数，通过对语料的统计，得到这些参数的过程(计算这些条件概率)称为模型的训练。前面提到的二元模型训练方法似乎非常简单，只需计算一下前后相邻出现的次数 和 单独出现的次数 的比值即可。而的取值可能是词典中的任意一个单词，即考虑所有可能的组合，基于语料库计算频数、频率及条件概率，对于N元模型也是同理，这样做的话，的很多组合可能没有意义，在语料库中没有出现过，即=0。那么是否意味着条件概率=0？反之，如果，都在语料库中只出现一次，那么能否得到=1，这样非常绝对的结论？\n注意词典和语料库不是一个概念，词典基于语料库构建，对语料库分词，去重，调整顺序来构建词典。n-gram模型，可以理解为考虑词典中的所有可能组合，然后基于语料库进行统计，计算条件概率，存储起来，应用时直接查询计算即可。这样考虑所有可能组合，很多组合会没有意义，在语料库中也不会出现，就会存在0概率/数据稀疏的问题，此时需要使用平滑方法，对没有见过的gram赋于一个非0的概率值。\n还会面临统计可靠性或统计量不足的问题。在数理统计中，我们之所以敢用对采样数据进行观察的结果来预测概率，是因为有大数定理(Law of Large Number)在背后做支持，它的要求是有足够的观察值。但是在估计语言模型的概率时，很多人恰恰忘了这个道理，因此训练出来的语言模型“不管用”，然后回过头来怀疑这个方法是否有效。那么如何正确地训练一个语言模型呢？\n一个直接的办法就是增加数据量，但是即使如此，仍会遇到零概率或者统计量不足的问题。假定要训练一个汉语的语言模型，汉语的词汇量大致是 20 万这个数量级，训练一个三元模型就有 个不同参数。假设抓取 100 亿个有意义的中文网页，每个网页平均 1000 词，全部用作训练也依然只有 。因此，如果用直接的比值计算概率，大部分条件概率依然是零，这种模型我们称之为“不平滑”。\n训练统计语言模型的艺术就在于解决好统计样本不足时的概率估计问题。\n关于平滑技术的详细介绍，可以阅读这篇博客自然语言处理中N-Gram模型的Smoothing算法\n当然，如果对这些平滑算法不是很懂也不必太担心，平滑技术在统计自然语言处理时代，用得比较多；现代的神经网络对语言模型建模的方式，由于本身结构的原因，自动解决了这个问题，我们之后还会学习。"}
{"content2":"自然语言处理简介\n自然语言处理（Natural Language Processing，简称NLP）就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。\n自然语言处理任务\n在得到字、句子表示之后，自然语言处理任务类型划分为：\n类别（对象）到序列：例如文本生成、图像描述生成\n序列到类别：文本分类、情感分析\n同步的序列到序列：中文分词、词性标注、语义角色标注\n异步的序列到序列：机器翻译、自动摘要\n参考资料\nAwesome-Chinese-NLP\n兜哥出品 <一本开源的NLP入门书籍>\nNLP研究入门之道\nYSDA course in Natural Language Processing\nnlp-tutorial：Natural Language Processing Tutorial for Deep Learning Researchers"}
{"content2":"利用空余时间，将《用Python进行自然语言处理》的前面几章内容都敲了一遍，其中遇到与书中示例不太一致的地方也进行了修改。第一章示例如下：\n#!/usr/bin/env python # -*- coding: utf-8 -*- # @Author : Peidong # @Site : # @File : eg1.py # @Software: PyCharm \"\"\" the first example for nltk book \"\"\" from nltk.book import * # 查找特定词语上下文 text1.concordance(\"monstrous\") # 相关词查找 text1.similar(\"monstrous\") # 查找多个词语的共同上下文 text2.common_contexts([\"monstrous\", \"very\"]) # 画出词语的离散图 text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"]) # 产生随机文本 text3.generate() Traceback (most recent call last): File \"E:/nlp/eg1.py\", line 25, in <module> text3.generate() TypeError: generate() missing 1 required positional argument: 'words' # 单词数量 标识符总数 print(len(text3)) # 词汇的种类及数量 用集合set显示 print(sorted(set(text3))) print(len(set(text3))) # 测量平均每类词语被使用的次数 from __future__ import division #本命令必须放在文件的开始之初 print(len(text3)/len(set(text3))) # 统计特定单词在文本中出现的次数，并计算其占比 print(text3.count(\"smote\")) print(100*text4.count('a')/len(text4)) # # 词的频率分布 fdist1 = FreqDist(text1) # # 输出总的词数 print(fdist1) # In Python 3 dict.keys() returns an iteratable but not indexable object. vac1 = list(fdist1.keys()) # # 输出词数最多的前五十个词 print(vac1[:50]) # # 输出whale的次数 print(fdist1[\"whale\"]) # # 输出前五十个词的累积频率图 fdist1.plot(50) # 查找长度超过15个字符的词 V = set(text1) long_words = [w for w in V if len(w)>15] print(sorted(long_words)) # 查找长度超过7的词且频率超过7 fdist5 = FreqDist(text5) print(sorted([ w for w in set(text5) if len(w)>7 and fdist5[w]>7])) # 双连词的使用 from nltk import bigrams # # 查了一下nltk官网上的函数说明，要加list()函数，结果才是书上的情况 print(list(bigrams(['more', 'is', 'said', 'than', 'done']))) # 文本中常用的连接词 print(text4.collocations()) print([len(w) for w in text1]) fdist = FreqDist([len(w) for w in text1]) print(fdist) print(fdist.keys()) print(fdist.items()) print(fdist.max()) print(fdist[3]) print(fdist.freq(3)) print(sorted([w for w in set(text1) if w.endswith('ableness')])) print(babelize_shell())"}
{"content2":"主题提取\n参考知乎的回答： 《“关键词”提取都有哪些方案？》\n《word2vec词向量训练及中文文本相似度计算》\n简单的LDA实现： 《NLP 主题抽取 Topic LDA代码实践 gensim包 代码》\n命名实体识别\n参考：http://spaces.ac.cn/archives/3942/\nIIS 推导：\nhttp://blog.csdn.net/xueyingxue001/article/details/50773917\nBi-LSTM + CRF 进行NER：\nhttps://www.cnblogs.com/Determined22/p/7238342.html\n使用CRF++进行命名实体识别\nhttps://zhuanlan.zhihu.com/p/27597790"}
{"content2":"如何学习自然语言处理(52nlp)\n1.几篇文章介绍相关技术\nnlper:\nhttp://nlpers.blogspot.my/\n2.标准书籍\n《统计自然语言处理基础》\n《自然语言处理综论》（涉猎广，有门槛）\n《自然语言理解》\nnltk工具包，《用python进行自然语言处理》 e－book:http://code.google.com/p/brishen/downloads/list\n3.ACL anthology\n或者到作者主页去进一步follow，或者去其导师主页看看是否有进一步相关文献\n4.多参加会议\n大体流程就是：博客入门－书籍系统了解－论文跟进－会议感受"}
{"content2":"目录\n1.1 自然语言处理的挑战\n1.2 神经网络和深度学习\n1.3 自然语言处理中的深度学习\n1.1 自然语言处理的挑战\n自然语言处理是一个设计输入与输出为非结构化自然语言数据的方法和算法的研究领域。人类语言有很强的歧义性（如句子“I ate pizza with friends”（我和朋友一起吃披萨）和“I ate pizza with olives”(我吃了有橄榄的披萨)）和多样性（如“I ate pizza with friends”也可以说成“Friends and I shared some pizza”）。语言也一直在进化中。人善于产生和理解语言，并具有表达、感知、理解复杂且微妙信息的能力。与此同时，虽然人类是语言的伟大使用者，但是我们并不善于形式化地理解和描述支配语言的规则。\n使用计算机理解和产生语言因此极具挑战性。事实上，最为人所知的处理语言数据的方法是使用有监督机器学习算法，其试图从事先标注好的输入/输出集合中推导出使用的模式和规则。例如，一个将文本分为四类的任务，类别为：体育、政治、八卦、经济。显然，文本中的单词提供了很强的线索，但是到底哪些单词提供了什么线索呢？为该任务书写规则极具挑战性。然而，读者可以轻松地将一篇文档分到一个主题中，然后，基于几百篇认为分类的样例，可以让有监督机器学习产生用词的模式，从而帮助文本分类。机器学习方法擅长那些很难获得规则集，但是相对容易获得给定输入及相应输出样本的领域。\n除了使用不明确规则集处理歧义和多样输入的挑战外，自然语言展现了另外一些特性，其使得用包括机器学习在内的计算方法更具挑战性，即离散性（discrete）、组合性（compositional）和稀疏性（sparse）。\n语言是符号化和离散的。书面语义的基本单位是字符，字符构成了单词，单词再表示对象、概念、事件、动作和思想。字符和单词都是离散符号：如“hamburger”或“pizza”会唤起我们头脑中的某种表示，但是它们也是不同的符号，其含义是不相关的，待我们的大脑去理解。从符号自身看，“hamburger”和“pizza”之间没有内在的关系，从构成它们的字母看也一样。与机器视觉中普遍使用的如颜色的概念或声学信号相对比，这些概念都是连续的，如可以使用简单的数学运算从一幅彩色图像变为灰度图像，或者从色调、光强等内在性质比较两幅图像。对于单词，这些都不容易做到，如果不使用一个大的查找表或者词典，没有什么简单的运算可以从单词“red”变为单词“pink”.\n语言还具有组合性，即字母形成单词，单词形成短语和句子。短语的含义可以比包含的单词更大，并遵循复杂的规则集。为了理解一个文本，我们需要超越字母和单词，看到更长的单词序列，如句子甚至整篇文本。\n以上性质的组合导致了数据稀疏性（data sparseness）。单词（离散符号）组合并形成意义的方式实际上是无限的。可能合法的句子数是巨大的，我们从没指望能全部枚举出来。随便翻一本书，其中绝大部分句子是你之前从没看过和听过的。甚至，很有可能很多四个单词构成的序列对你都是新鲜的。如果你看一下过去10年的报纸或者想像一下未来10年的报纸，许多单词，特别是人名、品牌和公司以及俚语和术语都将是新的。我们也不清楚如何从一个句子生成另一个句子或者定义句子之间的相似性，也不依赖于它们的意思——对我们是不可观测的。当我们要从实例中学习时也是挑战重重，即使有非常大的实例集合，我们仍然很容易观测到实例集合中从没有出现过的事件，其与曾出现过的所有实例都非常不同。\n1.2 神经网络和深度学习\n深度学习是机器学习的一个分支，是神经网络的重命名。神经网络是一系列学习技术，历史上曾受模拟脑计算工作的启发，可被看作学习参数可微的数学函数。深度学习的名字源于许多曾被连接在一起的可微函数。\n虽然全部机器学习技术都可以被认为是基于过去的观测学习如何做出预测，但是深度学习方法不仅学习预测，而且学习正确地表示数据，以使其更有助于预测。给出一个巨大的输入-输出映射集合，深度学习方法将数据“喂”给一个网络，其产生输入的后继转换，直到用最终的转换来预测输出。网络产生的转换都学习自给定的输入-输出映射，以便每个转换都使得更易于将数据和期望的标签之间建立联系。\n开发者负责设计网络结构和训练方式，提供给网络合适的输入-输出实例集合，将输入数据恰当地编码，大量学习正确表示的工作则由网络自动执行，同时受到网络结构的支持。\n1.3 自然语言处理中的深度学习\n神经网络提供了强大的学习机制，对自然语言处理问题极具吸引力。将神经网络用于语言的一个主要组件是使用嵌入曾（embedding layer），即将离散的符号映射为相对低维的连续向量。当嵌入单词的时候，从不同的独立符号转换为可以运算的数学对象。特别地，向量之间的距离可以等价于单词之间的距离，这使得更容易从一个单词泛化到另一个单词。学习单词的向量表示成为训练过程的一部分。再往上层，网络学习单词向量的组合方式以更有利于观测，该能力减轻了离散和数据稀疏问题。\n有两种主要的神经网络结构，即前馈网络（feed-forward network）和循环/递归网络（recurrent/recursive network），它们可以以各种方式组合。\n前馈网络，也叫多层感知器（Multi-Layer Perceptron）,其输入大小固定，对于变化的输入长度，我们可以忽略元素的顺序。当将输入集合喂给网络时，网络学习用有意义的方式组合它们。之前线性模型所能应用的地方，多层感知器都能使用。网络的非线性以及易于整合预训练词嵌入的能力经常导致更高的分类精度。\n卷积前馈网络是一类特殊的结构，其善于抽取数据中有特殊意义的局部模式；将任意长度的输入“喂”给网络，网络能抽取有意义的局部模式，这些模式对单词顺序敏感，而忽略它们在输入中出现的位置。这些工作适合于识别长句子或者文本中有指示性的短语和惯用语。\n循环神经网络是适于序列数据的特殊模型，网络接收输入序列作为输入，产生固定大小的向量作为序列的摘要。对于不同的任务，“一个序列的摘要”意味着不同的东西（也就是说，用于回答一个句子情感所需的信息与回答其语法的信息并不相同）。循环网络很少被当做独立组件使用，其能力在于可能当做可训练的组件“喂”给其他网络组件，然后串联地训练它们。例如，循环网络的输出可以“喂”给前馈网络，用于预测一些值。循环网络被用作一个输入转换器，其被训练用于产生富含信息的表示，前馈网络将在其上进行运算。对于序列循环网络是非常引人注目的模型，可能也是神经网络用于自然语言最令人激动的成果。它们允许：打破自然语言处理中存在几十年的马尔科夫假设，设计能依赖整个句子的模型，并在需要的情况下考虑词的顺序，同时不太受由于数据稀疏造成的统计估计问题之苦。该能力是语言模型产生了令人印象深刻的收益，其中语言模型指的是预测序列中下一个单词的概率（等价于预测一个序列的概率），是许多自然语言处理应用的核心。递归神经网络将循环网络从序列扩展到树。\n自然语言处理的许多问题是结构化的，需要产生复杂的输出结构，如序列和树。神经网络模型能适应该需求，一方面可以改进已知的面向线性模型的结构化预测算法，另一方面可以使用新的结构，如序列到序列（编码器-解码器）模型，指的是条件生成模型。此类模型是目前公认的最好的机器翻译模型的核心。\n最后，许多自然语言预测任务互相关联，在某种意义上知道一种任务是如何执行的将对另一些任务有所帮助。另外，我们可能没有足够的有监督（带标签）训练数据，而只有足够的原始文本（无标签数据）。那我们能从相关的任务或者未标注数据中学习吗？对于多任务学习（Multi-Task Learning，即从相关问题中学习）和半监督（semi-supervised）学习（从额外的、未标注的数据中学习），神经网络方法提供了令人激动的机会。\n注:文章内容摘自Yoav Goldberg所著《Neural Network Methods for Natural Language Processing》的中文版《基于深度学习的自然语言处理》chapter 1 Introduction"}
{"content2":"#希拉里右键门，文档主题分类。LDA模型，数据读取还有点问题 #数据来源:请联系公众号：湾区人工智能 import numpy as np import pandas as pd import re import codecs #UnicodeEncodeError: 'mbcs' codec can't encode characters in position 0--1: invalid character df = pd.read_csv(\"D:/自然语言处理/Lecture_3 LDA 主题模型课件与资料/Lecture_3 LDA 主题模型课件与资料/主题模型课件与资料/input/HillaryEmails.csv\",encoding='utf-8') # 原邮件数据中有很多Nan的值，直接扔了。 df = df[['Id','ExtractedBodyText']].dropna() def clean_email_text(text): text = text.replace('\\n',\" \") #新行，我们是不需要的 text = re.sub(r\"-\", \" \", text) #把 \"-\" 的两个单词，分开。（比如：july-edu ==> july edu） text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) #日期，对主体模型没什么意义 text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) #时间，没意义 text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) #邮件地址，没意义 text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text) #网址，没意义 pure_text = '' # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉 for letter in text: # 只留下字母和空格 if letter.isalpha() or letter==' ': pure_text += letter # 再把那些去除特殊字符后落单的单词，直接排除。 # 我们就只剩下有意义的单词了。 text = ' '.join(word for word in pure_text.split() if len(word)>1) return text #新建一个colum docs = df['ExtractedBodyText'] docs = docs.apply(lambda s: clean_email_text(s)) #docs.head(1).values doclist = docs.values from gensim import corpora, models, similarities import gensim stoplist = ['very', 'ourselves', 'am', 'doesn', 'through', 'me', 'against', 'up', 'just', 'her', 'ours', 'couldn', 'because', 'is', 'isn', 'it', 'only', 'in', 'such', 'too', 'mustn', 'under', 'their', 'if', 'to', 'my', 'himself', 'after', 'why', 'while', 'can', 'each', 'itself', 'his', 'all', 'once', 'herself', 'more', 'our', 'they', 'hasn', 'on', 'ma', 'them', 'its', 'where', 'did', 'll', 'you', 'didn', 'nor', 'as', 'now', 'before', 'those', 'yours', 'from', 'who', 'was', 'm', 'been', 'will', 'into', 'same', 'how', 'some', 'of', 'out', 'with', 's', 'being', 't', 'mightn', 'she', 'again', 'be', 'by', 'shan', 'have', 'yourselves', 'needn', 'and', 'are', 'o', 'these', 'further', 'most', 'yourself', 'having', 'aren', 'here', 'he', 'were', 'but', 'this', 'myself', 'own', 'we', 'so', 'i', 'does', 'both', 'when', 'between', 'd', 'had', 'the', 'y', 'has', 'down', 'off', 'than', 'haven', 'whom', 'wouldn', 'should', 've', 'over', 'themselves', 'few', 'then', 'hadn', 'what', 'until', 'won', 'no', 'about', 'any', 'that', 'for', 'shouldn', 'don', 'do', 'there', 'doing', 'an', 'or', 'ain', 'hers', 'wasn', 'weren', 'above', 'a', 'at', 'your', 'theirs', 'below', 'other', 'not', 're', 'him', 'during', 'which'] texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist] #用词袋的方法，把每个单词用一个数字index指代，并把我们的原文本变成一条长长的数组： dictionary = corpora.Dictionary(texts) corpus = [dictionary.doc2bow(text) for text in texts] #建立模型 lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20) #第10号分类，其中最常出现的单词是 #lda.print_topic(10, topn=5) #所有的主题打印出来看看 lda.print_topics(num_topics=20, num_words=5) ''' #可以把新鲜的文本/单词，分类成20个主题中的一个。文本和单词，都必须得经过同样步骤的文本预处理+词袋化，也就是说，变成数字表示每个单词的形式。 lda.get_document_topics(bow) '''"}
{"content2":"自然语言处理\n自然语言处理是什么\n自然语言处理(Natural Language Process)就是利用计算机来处理人类语言的学科，属于计算机与语言学的交叉学科。\n自然语言处理有哪些技术\n大致包括如下技术:\n1.分词(Word Segmentation或Word Breaker，WB)\n在英文文本当中每个词之间都有间隔好分，但在中文文本当中一句话之间每个词是没有间隔的，所以需要对一个句子当中每个字进行切分，句子的基本语义单元就变成了词，这就是分词任务。\n2.句法分析（Parsing）\n句法分析指的是将句子中每个部分的组块(也就是每个词、字的归属类)标注出来。\n组块分析:标出句子的短语块,如“This is a dog(NP)” 超级标签分析:给每个句子加上超级标签，超级标签是一个树形结构图\n成分句法分析:分析句子成分，给出一颗由终结符和非终结符构成的成分句法树\n依存句法分析:分析句中词的依存关系，给出一颗由词语依存关系构成的依存句法树。\n3.信息抽取（Information Extraction，IE）：命名实体识别和关系抽取（Named Entity Recognition & Relation Extraction，NER):我们从一段文本中抽取关键信息即从无结构的文本中抽取结构化的信息，\n4.词性标注（Part Of Speech Tagging，POS）:对词语的词性进行标注\n5.指代消解（Coreference Resolution）:消除一些对文本处理没有意义的指代名词，减轻程序对语言的处理。\n6.词义消歧（Word Sense Disambiguation，WSD）:一个词他可能会有歧义，该任务是用来消除歧义的。\n7.机器翻译（Machine Translation，MT）:要实现文本的自动翻译\n8.自动文摘(Automatic Summarization):摘要是一大段文字，我们需要将里面的梗提取出来然后缩短方便阅读或方便提取信息。\n9.问答系统（Question Answering）:你提出一个问题机器给予你准确的答案\n10.OCR:也属于视觉模块内容，将图片当中的文字通过机器识别图像翻译成文本形式\n11.信息检索(Information Retrieval，IR):用户进行信息查询和获取的主要方式，是查找信息的方法和手段。\n自然语言处理核心问题是什么\n文本分类\n关键词提取\n情感分析\n语义消歧\n主题模型\n机器翻译\n问题问答\n汉语分词\n垂直领域的对话机器人\n自然语言处理有哪些应用方向\n搜索引擎\n文本主题/标签分类\n文本创作与生成\n机器翻译\n情感分析\n舆情监控\n语音识别系统\n对话机器人\n自然语言处理的难点是什么\n歧义问题:很多话的意思说的模棱两可，具有歧义\n知识问题:知识稀疏或者词汇稀疏，词汇稀疏导致了搭配稀疏，然后导致了语义稀疏，它有一个递进关系。一个比较出名的定律叫齐夫定律（Zipf Law），这个定律是说在自然语言语料当中，一个单词出现的频率和它在频率表当中的排名基本成一个反比关系。\n离散符号计算问题:我们看到的文本其实都是一些符号，对计算机来说，它看的其实也是一些离散的符号，但我们知道计算机其实最擅长的是数值型的运算，而不是符号的推理，并且符号之间的逻辑推理会非常复杂。\n语义本质的问题:到底什么是语义？什么是语义？语言里面到底是什么东西？符号背后真正的语义怎么来表示？语言学家他走的路子就是我构建好多形式化的、结构化的图之类的，这种结构去做语义或者是一些符号推导系统，认为它可以接近语义本质。但是，这些其实走得越远离计算机就越远，因为它越符号，语义的可解释性就会很差。拿数字来表示语义，我们也不知道这个数字到底它是什么东西。所以目前为止现在研究领域对这个问题解决得比较差。\n自然语言处理学习路线\n熟悉基本知识、基本操作\n如文本操作、正则、掌握一些基本文本处理框架英文有NLTK、spaCy，中文有中科院计算所NLPIR、哈工大LTP、清华大学THULAC、Hanlp分词器、Python jieba工具库\n知道什么是语言模型、利用语言模型来完成一些项目\n文本表示:将文本中的字符串转化为计算机当中的向量\n文本分类:分类模型传统的一个解决方法就是标带标注的语料，再特征提取，然后训分类器进行分类。这个分类器就会用比如说逻辑回归、贝叶斯、支持向量机、决策树等等。\n主题模型:使用无监督学习的方式对文本中的隐含语义进行聚类的统计模型\nseq2seq模型:通过深度神经网络将一个序列作为映射为另外一个输出的序列。\n文本生成:GAN文本生成，也叫机器人写作。"}
{"content2":"原文：7 Applications of Deep Learning for Natural Language Processing\n作者：Jason Brownlee\n翻译：无阻我飞扬\n摘要：在这篇文章中，作者详细介绍了自然语言处理深度学习的7种应用，以下是译文。\n自然语言处理领域正在从统计方法转变为神经网络方法。\n自然语言中仍有许多具有挑战性的问题需要解决。然而，深度学习方法在一些特定的语言问题上取得了最新的成果。这不仅仅是深度学习模型在基准问题上的表现，基准问题也是最有趣的；事实上，一个单一的模型可以学习词义和执行语言任务，从而消除了对专业手工制作方法渠道的需要。\n在这篇文章中，你会发现7个有趣的自然语言处理任务，也会了解深度学习方法取得的一些进展。\n1、 文本分类\n2、 语言建模\n3、 语音识别\n4、 字幕生成\n5、 机器翻译\n6、 文档摘要\n7、 问答（Q&A）\n我试图专注于你可能感兴趣的各种类型的终端用户问题，而不是更多的学术或语言的子问题，在有些方面深度学习已经做的很好，如词性标注，程序分块，命名实体识别，等等。\n每个示例提供了一个问题描述，示例，对演示方法和结果的文档引用。大多数参考来自2015年的Goldberg’s 的优秀的NLP研究人员深度学习入门文献 。\n你有没有一个深度学习中最受欢迎的NLP应用没有被列出？请在下面的评论中告诉我。\n1、 文本分类\n给出一个文本实例，预测一个预定义的类标签。\n文本分类的目的是对文档的标题或主题进行分类。\n—575页，自然语言处理的基础统计，1999\n一个流行的分类示例是情感分析，类标签代表源文本的情感基调，比如“积极的”或“消极的”。\n下面是另外三个例子：\n垃圾邮件过滤，将电子邮件文本分类为垃圾邮件或正常邮件。\n语言识别，对源文本的语言进行分类。\n体裁分类，对小说故事体裁进行分类。\n此外，这个问题可以用某种方式加以解决，将多个类分配给一个文本，即所谓的多标签分类。如给一个源tweet预测多个#标签。\n更多相关主题的内容，请参见：\nScholarpedia的文本分类\n维基百科的文档分类\n下面是3个文本分类深度学习的论文例子：\n烂片评论的情感分析\n文本分类的DUCR结构方法，2015\n亚马逊产品评价的情感分析，IMDB电影评论和新闻文章的主题分类。\n有效使用词序进行基于卷积神经网络的文本分类，2015\n影评的情感分析，将句子分类为主观的和客观的，分类问题类型，产品评论的情感及更多。\n基于卷积神经网络的句子分类，2014\n2、 语言建模\n语言建模真的是更有趣的自然语言问题的一个子任务，特别是那些在其它输入条件下的语言模型。\n…问题是根据给出的前一个词来预测下一个词。这项任务是语音或光学字符识别的基础，也用于拼写矫正，手写识别和统计机器翻译。\n—191页，统计自然语言处理基础，1999.\n除了对语言建模的学术兴趣外，它也是许多自然语言处理体系结构深度学习的一个重要组成部分。\n一个语言模型学习词与词之间的概率关系，这样以来，新的词的序列可以生成与源文本统计学上一致的文本内容。\n单独地，语言模型可用于文本或语音生成；例如：\n生成新的文章标题。\n生成新的句子，段落，或文件。\n生成一个句子的建议延续的句子。\n有关语言建模的更多信息，请参见：\n维基百科上的语言模型\n循环神经网络的不可思议的效用，2015\n生成基于模型的合成文本语音，第十讲，牛津，2017\n下面是深度学习语言建模（仅有）的一个例子：\n英语课文、书籍和新闻文章的的语言模型。\n一种神经概率语言模型，2003\n3、 语音识别\n语音识别是理解说了什么的问题。\n…语音识别的任务是将包含自然语言话语的语音映射成说话人想要表达的对应的词。（传统的语音识别模型是通过人工建立一张语音词表，将相似发音的字母划分为一类；并借助一个分类模型实现语音到字母的转译。）\n—458页，深度学习，2016.\n给定作为音频数据的文本的发声，该模型必须生成可读的文本。\n自动给出自然语言的处理，这个问题也可被称为自动语音识别（ASR）.\n语言模型用于创建以音频数据为条件的文本输出。\n包含的一些例子：\n录制语音。\n为电影或电视节目创建文本字幕。\n开车的时候向无线电发出指令。\n有关语音识别的更多信息，请参见：\n维基百科上的语音识别\n以下是用于语音识别深度学习的3个例子：\n英语语音到文字。\n连接时间分类：循环神经网络的不分段标签序列数据，2006。\n英语语音到文字。\n深度循环神经网络的语音识别，2013。\n英语语音到文字。\n用于语音识别的卷积神经网络结构的研究和优化技术，2014。\n4、字幕生成\n字幕生成是描述图像内容的问题。\n给定一个数字图像，如一张图片，生成关于这个图像内容的文本描述。\n语言模型用于创建符合图像内容的字幕。\n包含的一些例子：\n描述一个场景的内容。\n为照片创建标题。\n描述一个视频。\n这不仅仅是对听障者的一个应用程序，还可以为图像和视频数据生成可读的文本，将来可以搜索，比如在网上。\n以下是字幕生成深度学习的3个例子：\n为照片生成字幕。\n展示，出席和讲述：视觉注意力的神经图像字幕生成，2016.\n为照片生成字幕。\n展示和讲述：神经图像字幕生成器，2015.\n为视频生成字幕。\n片段到片段—视频到文本，2015.\n5、机器翻译\n机器翻译是把源文本从一种语言转换成另外一种语言的问题。\n…机器翻译，文本或语音从一种语言到另外一种语言的自动翻译，它是NLP最重要的应用。\n—463页，统计自然语言处理基础，1999.\n考虑到深度神经网络的使用，该领域被称为神经机器翻译。\n在一个机器翻译任务中，输入由一些语言中的一系列符号组成，计算机程序必须把它转换成另一种语言中的符号序列。这通常用于自然语言，比如从英语到法语的翻译。深度学习最近开始对这种任务产生重要影响。\n—98页，深度学习，2016.\n语言模型用于输出翻译以后语言的目标文本，以源文本为基础。\n包含的一些例子：\n将一个文本文件从法语翻译成英语。\n将西班牙音频翻译成德语文本。\n将英语文本翻译成意大利音频。\n更多关于神经机器翻译，请参见：\n维基百科上的神经机器翻译。\n下面是机器翻译深度学习的3个例子：\n从英语到法语的文本翻译。\n基于神经网络的片段到片段的学习，2014\n从英语到法语的文本翻译。\n联合学习对齐和翻译的神经机器翻译，2014\n从英语到法语的文本翻译。\n基于循环神经网络组合语言和翻译模型，2013\n6、文档摘要\n文档摘要是对创建的文本文档进行简短描述的任务。\n如上所述，语言模型用于基于完整文档的摘要输出。\n一些文档摘要的例子：\n为一篇文档创建一个标题。\n为一篇文档创建一个摘要。\n更多关于这个话题的信息，请参见：\n维基百科上的自动摘要。\n深度学习已经被应用于自动文本摘要（成功）了吗？\n下面是文档摘要深度学习的3个例子：\n新闻文章中的句子摘要\n一个抽象概括的神经注意力模型，2015\n新闻文章中的句子摘要\n使用片段到片段RNN(循环神经网络)的抽象总结及更多，2015\n新闻文章中的句子摘要\n通过提取句子和单词的神经摘要，2016\n7、 问答\n回答问题就是给定一个主题，如文本文件，回答关于这个主题的一个特定问题。\n…问答系统尝试回答用户以问题形式表述的疑问，它返回适当的短语，如位置，人员，或者日期。例如，问题是总统肯尼迪为什么被刺杀？可能回答的短语是：Oswald（“凶手”奥司华德）。\n—377页，统计自然语言处理基础，1999\n包含的一些例子：\n维基百科上的问答\n更多关于问答的信息，请参见：\n关于维基百科文章的问答\n关于新闻文章的问答\n关于医疗记录的问答\n下面是问答深度学习的3个例子：\n新闻文章中的问答\n阅读和理解的机器教学，2015\n回答关于Freebase文章的一般知识性问题\n用多列卷积神经网络回答关于Freebase的问题，2015\n回答给定文件的事实型问题\n深度学习回答选择句，2015\n扩展阅读\n如果你需要更深入的了解，本节提供更多用于NLP深度学习应用程序的资源。\n自然语言处理的优先神经网络模型，2015\n从零（几乎）开始自然语言处理，2011\n自然语言处理深度学习，实践概述，牛津，2017\n深度学习或神经网络的NLP问题已成功应用？\n深度学习能像自然语言处理在视觉和语音处理领域一样取得类似的突破吗？\n2017年10月14日，SDCC 2017之大数据技术实战线上峰会即将召开，邀请圈内顶尖的布道师、技术专家和技术引领者，共话大数据平台构建、优化提升大数据平台的各项性能、Spark部署实践、企业流平台实践、以及实现应用大数据支持业务创新发展等核心话题，七位大牛与你相聚狂欢，详情查看所有嘉宾和议题，以及注册参会。"}
{"content2":"陌陌科技\n职位描述：\n岗位职责：\n1、参与陌陌平台文本spam识别的开发，参与优化文本分类、聚类，文本相似性，语言模型，情感分析，用户行为分析等工作，持续改进和升级现有产品；\n2、跟进文本挖掘、NLP和机器学习领域的前沿技术，将前沿技术应用于实际业务。\n岗位要求：\n1、在以下至少一个领域有一定了解：\n(1)统计机器学习相关方法，如深度神经网络、概率图模型，最优化方法等；\n(2)语义理解技术，如知识图谱、语义解析、知识挖掘等；\n2、良好的分析问题与发现问题的能力，善于归纳技术方案的特性，并找出其不足与改进方法；\n3、有一定编程能力，熟悉Hadoop、Spark等分布式计算框架者更佳；\n4、具有良好的沟通能力，和良好的团队合作精神；\n5、应聘实习生职位的要求能每周实习3天以上，实习半年以上。\n===================================================\n京东集团\n职位描述：\n职位：自然语言处理算法工程师\n主要研究方向为：自然语言处理、文本分析、或相关机器学习方向。非常欢迎从事深度学习、机器学习方向研究且有兴趣在文本处理方向做落地实践的同学。也欢迎投递简历。\n基本要求：\n1. 有自然语言处理、文本分析或文本理解等相关项目经验；\n2. 熟练掌握一门脚本语言，如python或者perl；\n3. 对中文分词、词性标注、命名实体识别的某一研究领域有较深的研究；\n4. 对文本分类、语义理解、文本摘要等技术方向有一定的了解和研究；\n5. 对于AI技术、新产品、新技术有关注和热情。\n有以下经验之一者优先考虑   ：\n1. 能够编写高质量的线上服务代码；\n2. NLP之外的其它机器学习方向的相关项目经验；\n3. 对基于DNN的NLP前沿方法有较深入了解；\n4. 对于对话系统、聊天机器人的原理有较深入了解。\n=======================================================\n爱奇艺\n职位介绍：\n1   feed流标签分类\n2   关键词提取，命名实体提取，文本纠错等\n3   对话系统\n职位要求：\n1    计算机，电子，数学，物理等相关专业，硕/博研究生均可\n2    熟悉Linux基本操作，熟悉C++和python，对数据结构和算法设计有较为深刻的理解\n3    有自然语言处理相关知识，包括但不限于分词，词性标注，命名实体识别等\n4    熟悉基本的机器学习算法和深度学习（如RNN,CNN等）算法\n5    熟悉一种深度学习框架，如tensorFlow，caffe\n6    优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情\n7    每周至少4个工作日，6个月以上\n8    对2019年毕业的同学，提供转正机会\n=================================================================\n点智互动\n职位描述：\n1、负责NLP基础算法，包括分词，词性标注，命名实体识别，新词发现，句法和语义分析等算法优化；\n2、负责实现智能问答，从海量对话数据中挖掘数据，构建行业知识图谱，回答常见问题，提升用户体验；\n3、负责优化对话系统，研发具有学习能力的智能机器；\n4、负责数据 分类/聚类，情感分析和质量识别工作。\n任职要求：\n1、211/985计算机、模式识别、数学相关专业本科及以上学历；\n2、 有数据挖掘、策略和算法研发工作实践经验优先；\n3、有文本挖掘相关经验，有较丰富Python ,linux环境开发经验；\n4、有海量数据挖掘、知识图谱构建、深度学习研发实践经验优先；\n5、有搜索引擎、风控项目，推荐系统，行为分析系统，用户画像等研发经验优先。\n======================================================================\n最右APP\n工作职责:\n-研究数据挖掘或机器学习领域的前沿技术,并用于实际问题的解决和优化\n-大规模机器学习算法研究及并行化实现,为各种大规模机器学习应用研发核心技术\n-通过对数据的敏锐洞察,深入挖掘产品潜在价值和需求,进而提供更有价值的产品和服务,通过技术创新推动产品成长\n职责要求:\n-热爱互联网，对技术研究和应用抱有浓厚的兴趣，有强烈的上进心和求知欲，善于学习和运用新知识\n-具有以下一个或多个领域的理论背景和实践经验：机器学习/数据挖掘/计算机视觉/信息检索/推荐系统\n-至少精通一门编程语言，熟悉网络编程、多线程、分布式编程技术，对数据结构和算法设计有较为深刻的理解\n-良好的逻辑思维能力，对数据敏感，能够发现关键数据、抓住核心问题\n-较强的沟通能力和逻辑表达能力，具备良好的团队合作精神和主动沟通意识\n具有以下条件者优先：\n-熟悉文本分类、聚类、计算机视觉有相关项目经验\n-熟悉海量数据处理、最优化算法、分布式计算或高性能并行计算，有相关项目经验\n-有应用tensorflow等深度学习框架解决过实际应用问题的经验\n============================================\n字节跳动\n【职位描述】\n1. 文本分类：包括训练语料收集，清理，标注，特征选择，特征值优化，类别体系修改，训练算法改进等；\n2. 话题聚类：分析新闻语料，学习出语料中涵盖的相关话题，以及在线预测出文章中包含的话题；\n3. 分词、词性预测、命名实体识别：结合业界最新进展和语料改进和开发相关算法。\n【职位要求】\n1. 对职位描述中的一项或多项工作感兴趣且熟悉，有具体相关经验者优先；\n2. 具备强悍的编码能力，熟悉Linux开发环境，熟悉Python/C++/Java/Scala语言；\n3. 优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情；\n实习时间≥4天/周   ≥3个月\n【需提交的材料】\n个人简历+做过的项目代码（岗位对编程能力有要求，希望通过代码进行二次筛选）\n===================================================================================\n搜狐\n工作内容：\n结合产品需求，应用自然语言处理技术进行文本分析、分类、关键词提取、相关性计算等任务\n职位要求：\n1. 自然语言处理等相关专业\n2. 熟练掌握c++或者java，熟悉python编程\n3. 熟悉linux和shell环境\n4. 有积极向上的工作热情、有独当一面的工作能力、思维清晰、表达准确\n5. 有大数据和短视频推荐系统方面的相关经验加分\n===================================================\n网易\n工作职责：\n1、理解业务、产品、运营的工作需求，将业务环节提取、抽象为数学问题，设计、实现基于数据挖掘的解决方案，进行数据特征工程和机器学习模型的选取和调优，对接产品、开发、数据人员实现模型的测试和落地；\n2. 分析和研究数据与实际业务的关联关系，针对具体业务需求场景，设计用户价值、用户行为预测、用户分类、用户画像、用户生命周期、用户流失、交易盈利、风险控制等模型并搭建职能投资顾问系统。\n任职资格：\n1.硕士或博士，应届生或工作经验1-3年；\n2.计算机、统计、电子、数学、数理统计等相关专业，机器学习/数据挖掘/信息检索/自然语言处理/统计分析相关背景；\n3.掌握常用的分类、回归、聚类、预测、关联规则、序列模式等挖掘算法，了解数据挖掘前沿技术；\n4.熟练使用一种或多种数据挖掘工具，使用python/shell/scala等脚本语言；\n5.具有很强的学习和研究能力，英语熟练，能够熟练阅读英文技术资料，积极创新、乐于协作、善于沟通。\n===========================================================================\n创新工场\n工作职责：\n1、对中文文本进行基础NLP处理，包括但不限于：关键词提取，命名实体识别，分词，词性标注，文本分析，新词发现，词义消歧等。对海量文本进行深度分析，情感分析，文本分类，文本生成等\n2、调研和设计策略算法，参与NLP深度学习相关算法的研究与开发\n4.对 对话系统，智能问答，知识图谱等有了解者加分\n5.学习NLP领域的先进技术并开展相关研发工作\n职位要求：\n1、计算机科学、自然语言处理、数学等相关学科的在读学士，硕士或博士。具备良好的数学功底，包括线性代数、数理统计、数值优化、优化理论等方面；\n2、熟悉NLP和机器学习，深度学习，增强学习，迁移学习等理论基础。熟悉版本控制工具；\n3、能快速阅读并理解顶会论文并能够参与复现和改进\n4、熟悉c/c++,java,python,R及GPU开发、调试、性能优化者优先，有深度学习经验，使用过Caffe,TensorFlow,Theano等工具的优先，熟悉Hadoop/Hive/HBase/Spark/Storm等系统的优先，参与过NLP领域顶会论文发表，如ACL, AAAI, 者优先。\n5、在文本分类、文本聚类、新词发现、深度学习等领域有丰富经验优先；\n6、责任心强，积极主动，有良好的沟通能力和团队合作能力\n===================================================\n注意这些都是实习生的要求啊！！！！！\n最后自己默默地总结一下：\n自然语言应用的领域方面：\n文本分类、相似性    4\n语言模型\n情感分析    3\n文本分析    5\n关键词提取    4\n文本纠错\n对话系统、智能问答    4\n分词、词性标注    3\n知识图谱    3\n技能要求：\nhadoop/spark    2\n脚本语言python    7\nlinux    5\nC++ java    4\nRNN CNN\ntensorflow caffe    2"}
{"content2":"人工智能语言是一类适应于人工智能和知识工程领域的、具有符号处理和逻辑推理能力的计算机程序设计语言,其中Prolog是当代最有影响的人工智能语言之一。\n一、什么是人工智能语言\n人工智能（AI）语言是一类适应于人工智能和知识工程领域的、具有符号处理和逻辑推理能力的计算机程序设计语言。能够用它来编写程序求解非数值计算、知识处理、推理、规划、决策等具有智能的各种复杂问题。\n典型的人工智能语言主要有LISP、Prolog、Smaltalk、C++等。\n一般来说，人工智能语言应具备如下特点：\n•具有符号处理能力（即非数值处理能力）；\n•适合于结构化程序设计，编程容易；\n•具有递归功能和回溯功能；\n•具有人机交互能力；\n•适合于推理；\n•既有把过程与说明式数据结构混合起来的能力，又有辨别数据、确定控制的模式匹配机制。\n人们可能会问，用人工智能语言解决问题与传统的方法有什么区别呢？\n传统方法通常把问题的全部知识以各种的模型表达在固定程序中，问题的求解完全在程序制导下按着预先安排好的步骤一步一步（逐条）执行。解决问题的思路与冯.诺依曼式计算机结构相吻合。当前大型数据库法、数学模型法、统计方法等都是严格结构化的方法。\n对于人工智能技术要解决的问题，往往无法把全部知识都体现在固定的程序中。通常需要建立一个知识库（包含事实和推理规则），程序根据环境和所给的输入信息以及所要解决的问题来决定自己的行动，所以它是在环境模式的制导下的推理过程。这种方法有极大的灵活性、对话能力、有自我解释能力和学习能力。这种方法对解决一些条件和目标不大明确或不完备，（即不能很好地形式化，不好描述）的非结构化问题比传统方法好，它通常采用启发式、试探法策略来解决问题。\n二、Prolog语言及其基本结构\nProlog是当代最有影响的人工智能语言之一，由于该语言很适合表达人的思维和推理规则，在自然语言理解、机器定理证明、专家系统等方面得到了广泛的应用，已经成为人工智能应用领域的强有力的开发语言。\n尽管Prolog语言有许多版本，但它们的核心部分都是一样的。Prolog的基本语句仅有三种，即事实、规则和目标三种类型的语句，且都用谓词表示，因而程序逻辑性强，文法简捷，清晰易懂。另一方面，Prolog是陈述性语言，一旦给它提交必要的事实和规则之后，Prolog就使用内部的演绎推理机制自动求解程序给定的目标，而不需要在程序中列出详细的求解步骤。\n１、事实\n事实用来说明一个问题中已知的对象和它们之间的关系。在Prolog程序中，事实由谓词名及用括号括起来的一个或几个对象组成。谓词和对象可由用户自己定义。\n例如，谓词likes(bill，book).\n是一个名为like的关系，表示对象bill和book之间有喜欢的关系。\n２、规则\n规则由几个互相有依赖性的简单句（谓词）组成，用来描述事实之间的依赖关系。从形式上看，规则由左边表示结论的后件谓词和右边表示条件的前提谓词组成。\n例如，规则 bird(X):-animal(X),has(X,feather).\n表示凡是动物并且有羽毛，那么它就是鸟。\n３、目标（问题）\n把事实和规则写进Prolog程序中后，就可以向Prolog询问有关问题的答案，询问的问题就是程序运行的目标。目标的结构与事实或规则相同，可以是一个简单的谓词，也可以是多个谓词的组合。目标分内、外两种，内部目标写在程序中，外部目标在程序运行时由用户手工键入。\n例如问题 ?-student(john).\n表示“john是学生吗？”\n三、Prolog程序的简单例子\n以下两个例子在Turbo Prolog 2.0环境下运行通过。\n[ 注：一个Turbo Prolog程序至少包括谓词段、子句段和目标段三项。目标可以包含在程序中，也可以在程序运行时给出。]\n例1 谁是john的朋友？\npredicates /*谓词段，对要用的谓词名和参数进行说明*/\nlikes(symbol, symbol)\nfriend(symbol, symbol)\nclauses /*子句段，存放所有的事实和规则*/\nlikes(bell,sports). /*前4行是事实*/\nlikes(mary,music).\nlikes(mary,sports).\nlikes(jane,smith).\nfriend(john,X):-likes(X,sports),likes(X,music). /*本行是规则*/\n当上述事实与规则输入计算机后，运行该程序，用户就可以进行询问，如输入目标：\nfriend(john,X)\n即询问john的朋友是谁,,这时计算机的运行结果为：\nX=mary （mary是john的朋友）\n1 Solution （得到了一个结果）\n程序运行界面如下图所示:\n例2 汉诺塔问题：\n有N个有孔的盘子，最初这些盘子都叠放在柱a上（如），要求将这N个盘子借助柱b从柱a移到柱c（如），移动时有以下限制：每次只能移动一个盘子；大盘不能放在小盘上。问如何移动？\n该问题可以采用递归法思想来求解,其源程序为:\npredicates /*谓词段*/\nhanoi(integer)\nmove(integer,symbol,symbol,symbol)\ninform(symbol,symbol).\nclauses /*子句段*/\nhanoi(N):-move(N,a,b,c).\nmove(1,A,_,C):-inform(A,C),!.\nmove(N,A,B,C):-N1=N-1,move(N1,A,C,B),\ninform(A,C),move(N1,B,A,C).\ninform(Loc1,Loc2):-nl,write(\"移动1个盘子从柱\" ,Loc1,\"到柱\",Loc2).\ngoal /*目标段，问移动3个盘子的方法*/\nhanoi(3).\n这个例子的目标包含在程序里面，因此运行时程序将直接输出所有结果。\n程序运行界面如下图所示：\n四、Prolog语言的常用版本\nProlog语言最早是由法国马赛大学的Colmerauer和他的研究小组于1972年研制成功。早期的Prolog版本都是解释型的，自1986年美国Borland公司推出编译型Prolog,即Turbo Prolog以后，Prolog便很快在PC机上流行起来。后来又经历了PDC PROLOG、Visual Prolog不同版本的发展。并行的逻辑语言也于80年代初开始研制，其中比较著名的有PARLOG、Concurrent PROLOG等。\n1、Turbo Prolog\n由美国Prolog开发中心（Prolog Development Center, PDC）1986年开发成功、Borland公司对外发行，其1.0，2.0，2.1版本取名为Turbo Prolog，主要在IBM PC系列计算机，MS-DOS环境下运行。\n2、PDC Prolog\n1990年后，PDC推出新的版本，更名为PDC Prolog 3.0，3.2，它把运行环境扩展到OS/2操作系统，并且向全世界发行。它的主要特点是:\n•速度快。编译及运行速度都很快，产生的代码非常紧凑。\n•用户界面友好。提供了图形化的集成开发环境。\n•提供了强有力的外部数据库系统。\n•提供了一个用PDC Prolog编写的Prolog解释起源代码。用户可以用它研究Prolog的内部机制，并创建自己的专用编程语言、推理机、专家系统外壳或程序接口。\n•提供了与其他语言（如C、Pascal、Fortran等）的接口。Prolog和其他语言可以相互调用对方的子程序。\n•具有强大的图形功能。支持Turbo C、Turbo Pascal同样的功能。\n3、Visual Prolog\nVisual Prolog是基于Prolog语言的可视化集成开发环境，是PDC推出的基于Windows环境的智能化编程工具。目前，Visual Prolog在美国、西欧、日本、加拿大、澳大利亚等国家和地区十分流行，是国际上研究和开发智能化应用的主流工具之一。\nVisual Prolog具有模式匹配、递归、回溯、对象机制、事实数据库和谓词库等强大功能。它包含构建大型应用程序所需要的一切特性：图形开发环境、编译器、连接器和调试器，支持模块化和面向对象程序设计，支持系统级编程、文件操作、字符串处理、位级运算、算术与逻辑运算，以及与其它编程语言的接口。\nVisual Prolog包含一个全部使用Visual Prolog语言写成的有效的开发环境，包含对话框、菜单、工具栏等编辑功能。\nVisual Prolog与SQL数据库系统、C++开发系统、以及Visual Basic、Delphi或Visual Age等编程语言一样，也可以用来轻松地开发各种应用。\nVisual Prolog软件的下载地址为：http://www.visual-prolog.com"}
{"content2":"更多学习笔记关注：\n公众号:StudyForAI\n知乎专栏:https://www.zhihu.com/people/yuquanle/columns\n自然语言处理(NLP)是人工智能的一个重要应用领域，由于本人主要研究方向为NLP，也由于最近学习的需要，特意搜罗资料，整理了一份简要的NLP的基本任务和研究方向，希望对大家有帮助。\n自然语言的发展： 一般认为1950 年图灵提出著名的“图灵测试”是自然语言处理思想的开端。20 世纪 50 年代到 70 年代自然语言处理主要采用基于规则的方法。基于规则的方法不可能覆盖所有语句，且对开发者的要求极高。这时的自然语言处理停留在理性主义思潮阶段。70 年代以后随着互联网的高速发展，语料库越来越丰富以及硬件更新完善，自然语言处理思潮由理性主义向经验主义过渡，基于统计的方法逐渐代替了基于规则的方法。从 2008 年到现在，由于深度学习在图像识别、语音识别等领域不断取得突破，人们也逐渐开始引入深度学习来做自然语言处理研究，由最初的词向量到 2013 年 word2vec，将深度学习与自然语言处理的结合推向了高潮，并且在机器翻译、问答系统、阅读理解等领域取得了一定成功。\n-----------------------------------------------------------分割线---------------------------------------------------\n先来看看自然语言处理的定义：\n自然语言是指汉语、英语等人们日常使用的语言，是随着人类社会发展自然而然的演变而来的语言，不是人造的语言，自然语言是人类学习生活的重要工具。或者说，自然语言是指人类社会约定俗成的，区别于人工语言，如程序设计的语言。\n处理包含理解、转化、生成等过程。自然语言处理，是指用计算机对自然语言的形、音、义等信息进行处理，即对字(如果是英文即为字符)、词、句、段落、篇章的输入、输出、识别、分析、理解、生成等的操作和加工。实现人机间的信息交流，是人工智能界、计算机科学和语言学界所共同关注的重要问题。所以自然语言处理也被誉为人工智能的掌上明珠。可以说，自然语言处理就是要计算机理解自然语言，自然语言处理机制涉及两个流程，包括自然语言理解和自然语言生成。自然语言理解是指计算机能够理解自然语言文本的意义，自然语言生成则是指能以自然语言文本来表达给定的意图。\n自然语言的理解和分析是一个层次化的过程，许多语言学家把这一过程分为五个层次，可以更好地体现语言本身的构成，五个层次分别是语音分析、词法分析、句法分析、语义分析和语用分析。\n语音分析是要根据音位规则，从语音流中区分出一个个独立的音素，再根据音位形态规则找出音节及其对应的词素或词。\n词法分析是找出词汇的各个词素，从中获得语言学的信息。\n句法分析是对句子和短语的结构进行分析，目的是要找出词、短语等的相互关系以及各自在句中的作用。\n语义分析是指运用各种机器学习方法，学习与理解一段文本所表示的语义内容。 语义分析是一个非常广的概念。\n语用分析是研究语言所存在的外界环境对语言使用者所产生的影响。\n这里根据自己的学习以及查阅相关资料的理解，简要的介绍一下自然语言处理(nlp)一些相关技术以及相关任务，nlp技术包括基础技术和应用技术。\n基础技术包括词法分析、句法分析、语义分析等。\n词法分析（lexical analysis）： 包括汉语分词（word segmentation 或 tokenization）和词性标注（part-of-speech tag）等。\n汉语分词：处理汉语(英文自带分词)首要工作就是要将输入的字串切分为单独的词语，这一步骤称为分词。\n词性标注：词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记。比如，名词（noun）、动词（verb）等。\n句法分析（syntactic parsing）：是对输入的文本句子进行分析得到句子的句法结构的处理过程。最常见的句法分析任务有下列几种：\n短语结构句法分析（phrase-structure syntactic parsing），该任务也被称作成分句法分析（constituent syntactic parsing），作用是识别出句子中的短语结构以及短语之间的层次句法关系；\n依存句法分析（dependency syntactic parsing），作用是识别句子中词汇与词汇之间的相互依存关系；\n深层文法句法分析，即利用深层文法，例如词汇化树邻接文法（Lexicalized Tree Adjoining Grammar， LTAG）、词汇功能文法（Lexical Functional Grammar， LFG）、组合范畴文法（Combinatory Categorial Grammar， CCG）等，对句子进行深层的句法以及语义分析。\n语义分析（Semantic Analysis）：语义分析的最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，至今这个问题也没有一个统一的答案。语义角色标注（semantic role labeling）是目前比较成熟的浅层语义分析技术。\n总而言之，自然语言处理系统通常采用级联的方式，即分词、词性标注、句法分析、语义分析分别训练模型。在使用过程中，给定输入句子，逐一使用各个模块进行分析，最终得到所有结果。近年来，研究者们提出了很多有效的联合模型，将多个任务联合学习和解码，如分词词性联合、词性句法联合、分词词性句法联合、句法语义联合等，取得了不错的效果。特别值得一提的是，今年EMNLP上有一个联合模型的教程，大家可以从这里下载：https://pan.baidu.com/s/1DxOqXxlK-1BCHqMCwr5_ZA。\n另一方面是自然语言处理的应用技术：这些任务往往会依赖基础技术，包括文本聚类（Text Clustering）、文本分类（Text Classification）、文本摘要（Text abstract）、情感分析（sentiment analysis）、自动问答（Question Answering, QA）、机器翻译（machine translation， MT）、信息抽取（Information Extraction）、信息推荐(Information Recommendation)、信息检索（Information Retrieval, IR）等。因为每一个任务都涉及的东西很多，因此在这里我知识简单总结介绍一下这些任务，等以后有时间随着我的学习深入，再分专题详细总结各种技术。\n文本分类：文本分类任务是根据给定文档的内容或主题，自动分配预先定义的类别标签。\n文本聚类：任务则是根据文档之间的内容或主题相似度，将文档集合划分成若干个子集，每个子集内部的文档相似度较高，而子集之间的相似度较低。\n文本摘要：文本摘要任务是指通过对原文本进行压缩、提炼，为用户提供简明扼要的文字描述。\n情感分析：情感分析任务是指利用计算机实现对文本数据的观点、情感、态度、情绪等的分析挖掘。\n自动问答：自动问答是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。\n机器翻译：机器翻译是指利用计算机实现从一种自然语言到另外一种自然语言的自动翻译。被翻译的语言称为源语言（source language）， 翻译到的语言称作目标语言（target language）。\n信息抽取：信息抽取是指从非结构化/半结构化文本（如网页、新闻、论文文献、微博等）中提取指定类型的信息（如实体、属性、关系、事件、商品记录等），并通过信息归并、冗余消除和冲突消解等手段将非结构化文本转换为结构化信息的一项综合技术。\n信息推荐：信息推荐据用户的习惯、 偏好或兴趣， 从不断到来的大规模信息中识别满足用户兴趣的信息的过程。\n信息检索：信息检索是指将信息按一定的方式加以组织，并通过信息查找满足用户的信息需求的过程和技术。\n参考：\n1.中文信息处理发展报告(2016)"}
{"content2":"图像处理是将输入图像转换为输出图像的过程，人是图像处理的效果的最终解释者；\n在计算机视觉中，计算机是图像的解释者；图像处理仅仅是计算机视觉系统中的一个模块；\n计算机图形学的主要工作是从三维描述到二维图像显示的过程；\n计算机视觉则是从二维图像数据到三维描述的过程，计算机视觉是计算机图形学的逆问题。\n模式识别主要解决分类的问题，是计算机视觉中的一个模块；\n总体来说他们有如下的关系：\n不要把几个相关的概念混为一谈"}
{"content2":"自然语言处理中主题模型的发展\n强烈建议直接看论文，看一些博客对于入门并没有什么太大帮助。\n[1]徐戈,王厚峰. 自然语言处理中主题模型的发展[J]. 计算机学报,2011,08:1423-1436.\n摘要：\n主题——词项的概率分布\n主题模型——文档从词项空间转换到主题空间，降维表达\n主要内容：\n1.对LSI PLSI LDA等主题模型进行介绍比较\n2.LDA派生模型介绍\n3.对EM算法生成主题的词项概率分布和文档的主题概率分布进行分析\n1.引言\n主题可以看作是词项的概率分布，一篇文章使用bag of words进行表示，长度较长，映射到主题空间之后，由于通常主题数K远远小于词项的数目，因此可以通过主题模型进行降维。\n隐性语义索引LSI (latent semantic indexing)不是一个概率模型\nDeerwester, Scott, et al. \"Indexing by latent semantic analysis.\" Journal of the American society for information science 41.6 (1990): 391.\n概率隐性语义索引pLSI真正意义上的主题模型\nHofmann, Thomas. \"Probabilistic latent semantic indexing.\" Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999.\nLDA(latent Dirichlet Allocation)Blei\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022.\nLSI - PLSI - LDA - 各种LDA\n2.主题模型的主要内容\n五大组成部分：输入、模型假设、表示、参数估计、新样本推测\n2.1主题模型的输入\n主题模型的输入是文档集合，由于交换性的假设，所以等价于term-document矩阵。\nterm \\ document\ndocument 1\ndocument 2\ndocument 3\nterm 1\n1\n0\n2\nterm 2\n0\n3\n1\n另一个输入是主题数目K,通常K是经验决定，最简单的方法是使用不同的K重复实验。\n评价指标：困惑度、语料似然值、分类正确率等估计K\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022. Griffiths, Thomas L., and Mark Steyvers. \"Finding scientific topics.\" Proceedings of the National academy of Sciences 101.suppl 1 (2004): 5228-5235. Blei, David M. \"Probabilistic topic models.\" Communications of the ACM 55.4 (2012): 77-84. CAO, Juan, et al. \"A method of adaptively selecting best LDA model based on density.\" Chinese Journal of Computer 31 (2008): 1780-1787.\n非参数贝叶斯估计K\nTeh, Yee Whye, et al. \"Hierarchical dirichlet processes.\" Journal of the american statistical association (2012). Shi, Jin, et al. \"Text segmentation based on model LDA.\" Chinese Journal of Computers 31.10 (2008): 1865-1873.\n2.2主题模型中的基本假设\nbag of words假设，即文档内词的顺序与模型结果无关。\n但是在LDA的派生模型中，有的交换性会被打破\n2.3主题模型的表示\n分别是图模型和生成模型，注意其中有两个超参数\nα\n\\alpha 和\nβ\n\\beta。\n2.4参数估计过程\n参数估计的结果是训练的最终结果。\n首先要选择优化的目标函数，通常就是整个语料的概率值。\n以LDA模型为例，根据图模型可以比较容易得到概率值的大小。（参看我的EM算法的文章）\n其实计算语料的概率就是计算整体的可能期望\n2.5新样本的推断\n其实新样本的推断就是将其映射到低维度的主题空间即可。可以用于信息检索中。\n3.期望最大化算法和参数估计\n（参看我的EM算法文章）\n期望最大化算法，对于隐变量通过概率模型寻找极大似然估计的一般方法。能够不断迭代，从而修改现有模型的参数。使用现有模型推断隐变量的后验概率分布，然后对于参数重新估计。\n不能保证全剧最优解，不过可以通过多次试验取得最好的结果。\n概率模型包括：1.隐变量集合Z 2.观测集合X 3.参数集合\nθ\n\\theta\n目标：得到P(X|\nθ\n\\theta)最大化时候的\nθ\n\\theta\nEM算法过程：\n初始化\nθ\n\\theta\nE步骤：使用当前\nθ\n\\theta对于P(Z|X,\nθ\n\\theta)进行估计\nM步骤：利用前一步的结果，最大化期望\n4.隐性语义索引\n隐性语义索引主要包含奇异值分解（SVD）和主成分分析（PCA）。\n4.1主成分分析\n4.2隐性语义索引\n隐性语义索引是通过奇异值分解构造新的隐性语义空间，即SVD分解。\n对于过于大的矩阵来说，可以通过EM算法来求得SVD分解的结果，其实SVD的U和V矩阵都可以看作是对于两个矩阵做了主成分分析，这两个矩阵的特征值和特征向量都可以通过EM算法计算出来。\n对于EM计算PCA,参见PRML。\n缺项矩阵：\nRoweis, Sam. \"EM Algorithms for PCA and SPCA.\"\n5.概率隐性语义索引\n概率隐性语义索引也是从词项空间到主题空间的变换。\n但是pLSI是一个概率生成模型。而且选择了不同的优化目标函数。\n就是两张二维参数表，分别是p(w|z)和p(z|d)，可以理解为和LSI中的类似。U对应p(w|z) V对应p(d|z) ，而中间矩阵对应着z的概率分布。\n对应于EM算法，可以对应于本模型，即w，d为观测值，z是隐变量，p(w|z) p(z|d)为待估计的参数。\npLSI模型和LSI的效果比较：\nHofmann, Thomas. \"Probabilistic latent semantic indexing.\" Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999. Hofmann, Thomas. \"Unsupervised learning by probabilistic latent semantic analysis.\" Machine learning 42.1-2 (2001): 177-196.\n和pLSI等价的NMF（非负矩阵分解）\nLee, Daniel D., and H. Sebastian Seung. \"Learning the parts of objects by non-negative matrix factorization.\" Nature 401.6755 (1999): 788-791. Lee, Daniel D., and H. Sebastian Seung. \"Algorithms for Non-negative Matrix Factorization.\"\n6.。。。从这里开始看不懂了，待我补完prml的概率部分再来看看"}
{"content2":"提出起因\nChomsky 短语结构语法生成能力太强，产生许多不符合语法或有歧义的句子；\n标记十分简单，分析能力有限，难以反映自然语言的复杂特性。\nFUG 对短语结构语法的改进\n采用复杂特征集来描述词、句法规则、语义信息，以及句子的结构功能。\n试图以单一形式的结构模式来描述特征组合、功能分配、词条和组成成分的顺序，以达到对句子的完全功能描述。\n采用合一运算对复杂特征集进行运算。\n复杂特征集\n1. 复杂特征集功能描述的定义\n设为一个功能描述 (Functional Description)，当且仅当可以表示为：\n其中，表示特征名，表示特征值，且满足以下两个条件：\n(1) 特征名为原子，特征值为原子或另一个功能描述；\n(2) ，读作：复杂特征集中，特征的值等于 。\n2. 可以用复杂特征集描述词汇\n在词典中单词的特征可以包括词类、形态、句法和语义等多方面的信息，如：\n3. 可以用复杂特征集描述规则\n4. 可以用复杂特征集描述句子\n句子：\nWe helped her.\n5. 复杂特征集的特点\n(1) 允许利用多个特征描述同一个语言单位；\n(2) 从结构上看，复杂特征集是一种嵌套结构，可以有效地表示复杂词组或句子结构；\n(3) 特征名的定义及其相互关系具有明显的层次性，而所有自然语言的结构都是层次性的，复杂特征集的这一特点显然对语言的层次分析有益；\n(4)复杂特征集便于运算，两个复杂特征集通过合一运算可以产生另一个复杂特征集，这与句法分析中词组和句子的产生是一致的。\n合一运算\n1. 复杂特征集相容的定义\n若均为复杂特征集, 则是相容的, 当且仅当：\n(1) 如果，且都是原子，那么是相容的，当且仅当；\n(2) 如果 均为复杂特征集，是相容的，当且仅当 相容。\n2. 合一运算的递归定义\n(1) 在都是原子的情况下，如果，则, 否则；\n(2) 如果均为复杂特征集，则\n(a) 若，但 的值未经定义，则属于;\n(b) 若，但 的值未经定义，则属于;\n(c) 若，但，且与 相容(不相抵触)，则 属于，否则,。\n合一运算的作用\n(1) 合并原有的特征信息，构造新的特征结构；\n(2) 检查特征的相容性和规则执行的前提条件是否满足，如果参与合一的特征相冲突，就立即宣布合一失败。"}
{"content2":"自然语言处理 - 要代替 RNN、LSTM 的 Transformer\nTransformer 结构\n计算过程\nSeq2Seq 模型，通常来讲里面是由 RNN、GRU、LSTM 的 cell 来组建的，但最近 Google 推出了一个新的架构 Transformer. 这个模型解决了 Seq2Seq 模型依赖之前结果无法并行的问题，而且最终的效果也是非常棒。\n原文：图解 Transformer\n已经这么详细的翻译了，我这里为自己总结一下关键点。\nTransformer 结构\nSeq2Seq 模型是这个样子的：\nTransformer 的宏观结构也是这样的，不同的是内部的微观结构。它里面又多个 Encoder 或 多个 Decoder 组成，而每个 Coder 内部又拥有不同的结构：\n计算过程\n最关键的是对每个词向量，每次在 Self -Attention 时初始化（后续反向传播更新）三个矩阵 query/key/value，利用 query * key 计算得到 score 并通过 softmax 计算得到 概率权重，乘以 value，得到激活词向量。\n每个词向量分别进入独立的 FFNN 计算。\n当 Encoder 计算完毕后，再转化为两个矩阵 key/value 代入到 Decoder 中 Encoder-Decoder Attention 的计算中。"}
{"content2":"最好的入门自然语言处理（NLP）的资源清单\nMelanie Tosik目前就职于旅游搜索公司WayBlazer，她的工作内容是通过自然语言请求来生产个性化旅游推荐路线。回顾她的学习历程，她为期望入门自然语言处理的初学者列出了一份学习资源清单。\n目录:\n·  在线课程\n·  图书馆和开放资源\n·  活跃的博客\n·  书籍\n·  数据集\n·  NLP之社交媒体\n·  其它\ndisplaCy网站上的可视化依赖解析树\ndisplaCy\n记得我曾经读到过这样一段话，如果你觉得有必要回答两次同样的问题，那就把答案发到博客上，这可能是一个好主意。根据这一原则，也为了节省回答问题的时间，我在这里给出该问题的标准问法：“我的背景是研究**科学，我对学习NLP很有兴趣。应该从哪说起呢？”\n在您一头扎进去阅读本文之前，请注意，下面列表只是提供了非常通用的入门清单（有可能不完整）。 为了帮助读者更好地阅读，我在括号内添加了简短的描述并对难度做了估计。最好具备基本的编程技能（例如Python）。\n在线课程\n1.Dan Jurafsky 和 Chris Manning：自然语言处理[非常棒的视频介绍系列] （YouTube）\nDan Jurafsky/Chris Manning 自然语言处理\n2.斯坦福CS224d：自然语言处理的深度学习[更高级的机器学习算法、深度学习和NLP的神经网络架构]\n斯坦福CS224d\n3.Coursera：自然语言处理简介[由密西根大学提供的NLP课程]\nCoursera 自然语言处理简介\n图书馆和开放资源\n1.spaCy（网站，博客）[Python; 新兴的开放源码库并自带炫酷的用法示例、API文档和演示应用程序]\nhttps://spacy.io/\nhttps://explosion.ai/blog/\nhttps://spacy.io/docs/usage/showcase\n2.自然语言工具包（NLTK）（网站，图书）[Python; NLP实用编程介绍，主要用于教学目的]\nhttp://www.nltk.org\nhttp://www.nltk.org/book/\n3.斯坦福CoreNLP（网站）[由Java开发的高质量的自然语言分析工具包]\nhttps://stanfordnlp.github.io/CoreNLP/\n活跃的博客\n1.自然语言处理博客（HalDaumé）\nhttps://nlpers.blogspot.com/\n2.Google研究博客\nhttps://research.googleblog.com/\n3.语言日志博客（Mark Liberman）\nhttp://languagelog.ldc.upenn.edu/nll/\n书籍\n1.言语和语言处理（Daniel Jurafsky和James H. Martin）[经典的NLP教科书，涵盖了所有NLP的基础知识，第3版即将出版]\nhttps://web.stanford.edu/~jurafsky/slp3/\n2.统计自然语言处理的基础（Chris Manning和HinrichSchütze）[更高级的统计NLP方法]\nhttps://nlp.stanford.edu/fsnlp/\n3.信息检索简介（Chris Manning，Prabhakar Raghavan和HinrichSchütze）[关于排名/搜索的优秀参考书]\nhttps://nlp.stanford.edu/IR-book/\n4.自然语言处理中的神经网络方法（Yoav Goldberg）[深入介绍NLP的NN方法，和相对应的入门书籍]\nhttps://www.amazon.com/Network-Methods-Natural-Language-Processing/dp/1627052984\nhttp://u.cs.biu.ac.il/~yogo/nnlp.pdf\n数据集\n1.Nicolas Iderhoff已经创建了一份公开的、详尽的NLP数据集的列表。除了这些，这里还有一些项目，可以推荐给那些想要亲自动手实践的NLP新手们：\nhttps://github.com/niderhoff/nlp-datasets\n2.基于隐马尔可夫模型（HMM）实现词性标注（POS tagging）.\nhttps://en.wikipedia.org/wiki/Part-of-speech_tagging\nhttps://en.wikipedia.org/wiki/Hidden_Markov_model\n3.使用CYK算法执行上下文无关的语法解析\nhttps://en.wikipedia.org/wiki/CYK_algorithm\nhttps://en.wikipedia.org/wiki/Context-free_grammar\n4.在文本集合中，计算给定两个单词之间的语义相似度，例如点互信息（PMI，Pointwise Mutual Information）\nhttps://en.wikipedia.org/wiki/Semantic_similarity\nhttps://en.wikipedia.org/wiki/Pointwise_mutual_information\n5.使用朴素贝叶斯分类器来过滤垃圾邮件\nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier\nhttps://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\n6.根据单词之间的编辑距离执行拼写检查\nhttps://en.wikipedia.org/wiki/Spell_checker\nhttps://en.wikipedia.org/wiki/Edit_distance\n7.实现一个马尔科夫链文本生成器\nhttps://en.wikipedia.org/wiki/Markov_chain\n8.使用LDA实现主题模型\nhttps://en.wikipedia.org/wiki/Topic_model\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n9.使用word2vec从大型文本语料库，例如维基百科，生成单词嵌入。\nhttps://code.google.com/archive/p/word2vec/\nhttps://en.wikipedia.org/wiki/Wikipedia:Database_download\nNLP之社交媒体\n1.Twitter：#nlproc，NLPers上的文章列表（由Jason Baldrige提供）\nhttps://twitter.com/hashtag/nlproc\nhttps://twitter.com/jasonbaldridge/lists/nlpers\n2.Reddit 社交新闻站点：/r/LanguageTechnology\nhttps://www.reddit.com/r/LanguageTechnology\n3.Medium发布平台：Nlp\nhttps://medium.com/tag/nlp\n其它\n1.如何在TensorFlow中构建word2vec模型[学习指南]\nhttps://www.tensorflow.org/versions/master/tutorials/word2vec/index.html\n2.NLP深度学习的资源[按主题分类的关于深度学习的顶尖资源的概述]\nhttps://github.com/andrewt3000/dl4nlp\n3.最后一句话：计算语言学和深度学习——论自然语言处理的重要性。（Chris Manning）[文章]\nhttp://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning\n4.对分布式表征的自然语言的理解（Kyunghyun Cho）[关于NLU的ML / NN方法的独立讲义]\nhttps://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n5.带泪水的贝叶斯推论（Kevin Knight）[教程工作簿]\nhttp://www.isi.edu/natural-language/people/bayes-with-tears.pdf\n6.国际计算语言学协会（ACL）[期刊选集]\nhttp://aclanthology.info/\n7.果壳问答网站(Quora)：我是如何学习自然语言处理的？\nhttps://www.quora.com/How-do-I-learn-Natural-Language-Processing"}
{"content2":"人工智能：\n“人工智能（Artificial Intelligence）”即AI，指的是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。这是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n人工智能”一词最初是在1956 年Dartmouth学会上提出的。从那以后，研究者们发展了众多理论和原理，人工智能的概念也随之扩展。\nAI是否会取代人类？\n关于这个问题，许多学者都表达出了他们的忧虑。早在1993年，计算机科学家Vernon Vinge推广了技术新概念: AI驱动的计算机或机器人能重新设计并改进自身，或者能设计比自身更先进的AI。这个概念让很多人认为，这将导致AI超出人类智慧、理解和控制，从而导致人类时代的终结。近来，许多权威科学家，包括Stuart Russell, Max Tegmark, 和 Frank Wilczek 都曾警告过，AI太过“聪明”的潜在后果就是机器将摆脱人类的控制，起身反抗，同治甚至是消灭人类。而权威物理学家与宇宙学家史蒂芬·霍金也曾说过：“人工智能的崛起可能是人类文明的终结”（出自霍金的主题演讲《让人工智能造福人类及其赖以生存的家园》）\n霍金表示，人工智能的威胁分短期和长期两种。短期威胁包括自动驾驶、智能性自主武器，以及隐私问题；长期担忧主要是人工智能系统失控带来的风险，如人工智能系统可能不听人类指挥。（《终结者》就是这种危险的放大）\n不过，对于这个话题，也有一些人持不同的意见。微软Azure公司副总裁茱莉亚·怀特就曾在悉尼举办的微软峰会上说过，“人工智能将取代人类”的说法忽略了一个前提，那就是人类是“不断学习、成长、改变自己以适应环境的”。怀特认为，人是智慧的生物，能够在不断学习的过程中利用人工智能，控制人工智能，而不是被机器取代。而英国物理学家罗杰·彭罗斯甚至认为，作为一种算法确定性的系统，当前的电子计算机无法产生智能。\n对于这个问题，不同的人有不同的观点，但唯一不变的一点就是：Vernon Vinge提出的观点在未来某天终会到来，人工智能终会实现真正意义上的智能。\n国内第一起机器人伤人事件：\n2016年11月18日，网络盛传深圳高交会上发生“全国首例机器人伤人事件”，名叫“小胖”的机器人突发故障“杀伤力爆棚”，在没有指令的情况下自行打砸展台玻璃、砸伤路人，一位路人全身多处划伤后被担架抬走。这让许多人认为机器人自我意识觉醒对抗人类的危险越来越可能了。但是，18日下午，高交会组委会在其官网发布公告，称事故是由于工作人员操作反了方向键所致：\n公告声明，11月17日13时30分左右，在1号展馆D32号展位（小胖的投影机供应商）旁通道内，参展人员试图将一台面罩打开的小胖机器人移动到展位内时，误将后退键按成前进键，并未及时按停，导致另一侧展台的玻璃被部分碰倒摔碎，玻璃划伤了另一侧展台内的一名观众，全过程持续约10余秒。\n虽然最终并非机器人自我意识觉醒。但许多人依然强调，人工智能不能脱离伦理道德。这一点并非仅仅针对AI，还针对AI工作者。AI工作者应该富有责任感，用伦理道德约束自己，在可能的情况下，用伦理道德约束AI.\n△ 今日头条创始人、CEO张一鸣\n“作为人工智能的企业，应该永远恪守一条原则：必须对整个人类的未来充满责任感，充满善意。”\n（资料皆来自网络检索）"}
{"content2":"这段时间一直在接触学习hadoop方面的知识，所以说对自然语言处理技术也是做了一些了解。网络上关于自然语言处理技术的分享文章很多，今天就给大家分享一下HanLP方面的内容。\n自然语言处理技术其实是所有与自然语言的计算机处理相关联的技术的统称，自然语言处理技术应用的目的是为了能够让计算机理解和接收我们用自然语言输入的指令，实现从将我们人类的语言翻译成计算机能够理解的并且不会产生歧义的一种语言。接合目前的大数据以及人工智能，自然语言处理技术的快速发展能够很好的助力人工智能的发展。\n（大快DKhadoop技术架构图）\n这里要分享的HanLP是我在学习使用大快DKhadoop大数据一体化平台时使用到的自然语言处理技术，使用这个组建可以很高效的进行自然语言的处理工作，比如进行文章摘要，语义判别以及提高内容检索的精确度和有效性等。\n本想找个通俗的案例来介绍一下HanLP,一时间也没想到什么好的案例，索性就从HanLp数据结构HE 分词简单介绍下吧。\n首先我们来看了解下HanLP的数据结构：\n二分tire树：Tire树是一种前缀压缩结构，可以压缩存大量字符串，并提供速度高于Map的get操作。HanLP中的trie树采用有序数组储存子节点，通过二分搜索算法检索，可以提供比TreeMap更快的查询速度。\n不同于父节点储存子节点引用的普通trie树，双数组trie树将节点的从属关系转化为字符内码的加法与校验操作\n对于一个接收字符c从状态s移动到t的转移，需满足条件是：\nbase[s] + c = t\ncheck[t] = s比如：base[一号] + 店 = 一号店\ncheck[一号店] = 一号\n相较于trie树的前缀压缩（success表），AC自动机还实现了后缀压缩（output表）\n在匹配失败时，AC自动机会跳转到最可能成功的状态（fail指针）\n关于HanLP分词\n1、词典分词\n基于双数组trie树或ACDAT的词典最长分词(即从词典中找出所有可能的词，顺序选择最长的词语)\n输出:[HanLP/名词, 是不是/null, 特别/副词, 方便/形容词, ？/null]\n2、NGram分词\n统计语料库中的BiGram，根据转移概率，选出最可能的句子，达到排除歧义的目的\n3、HMM2分词\n这是一种由字构词的生成式模型，由二阶隐马模型提供序列标注\n被称为TnT Tagger，特点是利用低阶事件平滑高阶事件，弥补高阶模型的数据稀疏问题\n4、CRF分词\n这是一种由字构词的生成式模型，由CRF提供序列标注\n相较于HMM，CRF的优点是能够利用更多特征、对OOV分词效果好，缺点是占内存大、解码慢。"}
{"content2":"之前看到的一篇文章，不知道原文在哪，如有侵权请告知。"}
{"content2":"1.1\n1对人工智能常见的误解有哪些?()AD\nA、人工智能就是机器学习\nB、机器学习只是人工智能中的一个方向\nC、人工智能最近十年受到深度学习的驱动较多\nD、人工智能就是深度学习\n2哲学思维对于人工智能的重要性表现在,哲学所强调的批判性思维有助于认清人工智能发展中的问题。()对\n3深度学习在人工智能领域的表现并不突出。()X\n1.2\n1\n计算机之父是()。C\nA、约翰·麦卡锡\nB、艾伦·图灵\nC、冯▪诺依曼\nD、马文·明斯基\n2\n人工智能与计算机学科的关系是()。C\nA、计算机学科的主要驱动力是人工智能研究\nB、计算机是人工智能研究的一个领域\nC、人工智能是计算机学科的一个分支\nD、人工智能与计算机学科没有联系\n3\n人工智能作为一门学科的建立时间是()。A\nA、1956年\nB、1930年\nC、1960年\nD、1952年\n4下列哪些选项是符号AI的技术路线()?AD\nA、通用问题求解器\nB、深度学习\nC、机器学习\nD、贝叶斯网络\n5符号AI是将人的思维通过逻辑语言制成流形图让计算机去执行。()对\n6通用问题求解器需要寻找全局最优解。()X\n7符号AI无法面对人类经验的变动性。()对\n1.3\n1\n()是现在新出现的人工智能的研究方向。D\nA、深度学习\nB、人工神经元网络\nC、贝叶斯网络\nD、类脑人工智能\n2\n深度学习中的“深度”是指()。B\nA、计算机理解的深度\nB、中间神经元网络的层次很多\nC、计算机的求解更加精准\nD、计算机对问题的处理更加灵活\n3人工神经元网络与深度学习的关系是()。AC\nA、人工神经元网络是深度学习的前身\nB、深度学习是人工神经元网络的一个分支\nC、深度学习是人工神经元网络的一个发展\nD、深度学习与人工神经元网络无关\n4人工神经元网络的运作可以粗略分为()三个层面。ACD\nA、输入层\nB、映射机制\nC、中间处理层\nD、输出层\n5符号AI不是人工智能的正统。()X\n6人工神经元网络是对人类的神经元运作进行一种非常粗糙的数学模拟。()对\n7相比于人工神经元网络和深度学习,类脑人工智能对人类大脑的神经回路具有更深入的了解。()对\n1.4\n1\n深度学习的实质是()。B\nA、推理机制\nB、映射机制\nC、识别机制\nD、模拟机制\n2符号AI的问题在于()。BCD\nA、缺少推理必要的信息\nB、把推理所依赖的公理系统全部锁死\nC、缺少推理的灵活性\nD、会遭遇“框架问题”\n3推理的本质是在信息不足的情况下能够最大程度的得到最靠谱的结论。()对\n4计算机具有触类旁通的能力,可以根据具体语境对事件进行分类。()X\n5人工神经元网络会遭遇“框架问题”。()X\n1.5\n1\n日本五代计算机泡沫关注的核心问题是( )。D\nA、人工神经元网络\nB、符号AI\nC、贝叶斯网络\nD、自然语言处理\n2制造人工智能的规划、计划和方案本身应该能根据情况的变化进行自我调整。( )正确\n2.1\n1目前对人工智能的发展所持有的观点有( )。ACD\nA、乌托邦论\nB、模块论\nC、末世论\nD、泡沫论\n2现在的人工智能系统都是专用人工智能而非通用人工智能。( )正确\n2.2\n1\n一个真正的通用人工智能系统应具备处理( )问题的能力。A\nA、全局性\nB、局部性\nC、专业性\nD、统一性\n2\n目前的人工智能研发的动力主要来源于( )。B\nA、科学\nB、商业\nC、学术\nD、军事\n3现有的人工神经元网络或深度学习无法处理全局性问题。( )正确\n4人工神经元网络只需要很少的数据便可掌握处理特定问题的能力。( )错误\n2.3\n1\n能够推进人工智能智能的研究最好方法是( )。C\nA、继续完善深度学习\nB、提升计算机处理数据的能力\nC、研究人类自己的智能\nD、研发通用人工智能\n2下列哪些选项属于通用智力因素?( )ABCD\nA、短期记忆\nB、流体智力\nC、晶体智力\nD、反应速度\n3类脑人工智能是指模拟人类大脑的人工智能。( ) 错误\n4人类自己的智能体现了通用性。( )正确\n2.4\n1以下哪些选项属于自然智能?( )ABC\nA、植物\nB、动物\nC、细菌\nD、机器\n2智能的特点是( )。AC\nA、能对环境进行灵活的应对\nB、能够不断创新\nC、具有十分牢固的记忆力\nD、经济高效\n3智能与神经元网络的存在具有必然关系。( )错误\n4类脑人工智能及人工神经元网络只是智能展现的一种形式。( )正确\n2.5\n1\n提出强人工智能与弱人工智能的人是( )。A\nA、约翰·塞尔\nB、彼得卡鲁瑟斯\nC、杰瑞·佛多\nD、埃隆·马斯克\n2通用人工智能就是强人工智能。( )错误\n3.1\n1\n深度学习的数据材料来源于( )。D\nA、人工搜集\nB、已有数据库\nC、抽样调查\nD、互联网\n2\n大数据技术的样本空间是( )。C\nA、针对所有相关数据\nB、需要确立样本范围\nC、不做样本控制\nD、以上都不对\n3统计学研究首先要确立样本空间,进行合理抽样,然后估测出相关的情况。( )正确\n4当前的主流人工智能是通向真正的通用人工智能的康庄大道。( )错误"}
{"content2":"（~2）微软亚洲研究院副院长周明老师，关于NLP的7个重要领域的详细解释。\n（）自然语言处理的重要应用领域之一：机器翻译。\n图中对比了3种翻译方式的效果，红色是基于短语，天蓝色是基于神经网络，黄色是人类水平。\n可以看到，英文和西班牙语/法语的（双向）翻译，通过应用神经网络，效果已经是比较完美了（perfect），接近于人类水平。但英文和中文的翻译，效果还比较普通。\n（）介绍了机器翻译的2种方法（SMT、NMT），并引出了“注意力机制”。\n（）用图像领域的一张非常经典的图，解释注意力机制。\n左侧，问题是“外套的颜色是什么？”传统的VQA方法，会给出错误答案“棕色”，因为图中大部分区域是砖块。而基于注意力机制的VQA方法，会先找到“外套”，然后再给出回答“黄色”\n类似的，右侧，问题是“雨伞的颜色是什么？”基于注意力的VQA方法，也会先找到“雨伞”，然后回答“红色”，而不是之前方法的错误判断“绿色”（因为雨伞背后更大区域是绿色）。\n（）语言生成的6个应用方向\n（）语言生成-对话的4个难题\n（）知乎上的一个问题“百度NLP部门怎么样？”，有匿名用户回答“NLP是现在人工智能的瓶颈，有志于从事NLP工作的同学要明白这一点”，下面还有同学问“能不能多解释一下”：）\n其实匿名用户的意思是，如果想在NLP领域做出真正有突破性的成就，是非常难的，如果不能忍受长时间的煎熬，还不如去其他相对更容易做出成果的AI领域……\n以上内容，来自饭团“AI产品经理大本营”，点击这里可关注：http://fantuan.guokr.net/groups/219/ （如果遇到支付问题，请先关注饭团的官方微信服务号“fantuan-app”）\n作者：黄钊hanniman，图灵机器人-人才战略官，前腾讯产品经理，5年AI实战经验，8年互联网背景，微信公众号/知乎/在行ID“hanniman”，饭团“AI产品经理大本营”，分享人工智能相关原创干货，200页PPT《人工智能产品经理的新起点》被业内广泛好评，下载量1万+。"}
{"content2":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。 用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义，同时也有重要的理论意义：人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种计算机语言；人们也可通过它进一步了解人类的语言能力和智能的机制。 实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为自然语言理解，后者称为自然语言生成。因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。历史上对自然语言理解研究得较多，而对自然语言生成研究得较少。但这种状况已有所改变。 通信工程的学生主要学习通信系统和通信网方面的基础理论、组成原理和设计方法，受到通信工程实践的基本训练，具备从事现代通信系统和网络的设计、开发、调测和工程应用的基本能力。 自然语言处理研究的内容包括但不限于如下分支领域：文本分类、信息抽取、自动摘要、智能问答、话题推荐、机器翻译、主题词识别、知识库构建、深度文本表示、命名实体识别、文本生成、文本分析（词法、句法、语法）、语音识别与合成等。 随着科技的发展，自然语言会在通信工程得到更多的应用。"}
{"content2":"面向自然语言处理的深度学习\n作者：[印]帕拉什·戈雅尔（Palash Goyal）苏米特·潘迪\n出版时间：2019-02-18\n出版社：机械工业出版社"}
{"content2":"最近几个月小编遨游在税务行业的智能问答调研和开发中，里面涉及到了很多的自然语言处理NLP的功能点。虽然接触NLP也有近两年的时间了，现在真正要应用到问答中，避免不了还是需要再重新熟识并深入研究理解。\n下面是与NLP相关的一些书籍推荐、课件推荐和开源工具推荐。\n主要是记录下入门的资料，由于资料的存储位置没有做规整，所以本文没有附带资源下载链接。如果有同学需要其中的资源，可以在公众号上给我留言，回头我把资源链接反馈给您。\n部分开源工具和语料资源\n1、NLTK官方提供的语料库资源列表\n2、OpenNLP上的开源自然语言处理工具列表\n3、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表”\n4、LDC上免费的中文信息处理资源\n课件\n1、哈工大刘挺老师的“统计自然语言处理”课件；\n2、哈工大刘秉权老师的“自然语言处理”课件；\n3、中科院计算所刘群老师的“计算语言学讲义“课件；\n4、中科院自动化所宗成庆老师的“自然语言理解”课件；\n5、北大常宝宝老师的“计算语言学”课件；\n6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码；\n7、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件；\n8、Michael Collins的“Machine Learning （机器学习）”课件；\n9、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件；\n10、Philipp Koehn “Empirical Methods in Natural Language Processing”课件；\n11、Philipp Koehn“Machine Translation（机器翻译）”课件。\n书籍\n1、《自然语言处理综论》英文版第二版\n2、《统计自然语言处理基础》英文版\n3、《用Python进行自然语言处理》，NLTK配套书\n4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦\n5、《自然语言处理中的模式识别》\n6、《EM算法及其扩展》\n7、《统计学习基础》\n8、《自然语言理解》英文版（似乎只有前9章）\n9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner；\n10、概率统计经典入门书：\n《概率论及其应用》（英文版，威廉*费勒著） 第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要）\n11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》\n12、国外机器学习书籍之：\n1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习&数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的”\n2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。\n豆瓣评论 by 王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n3) “Introduction to Machine Learning”\n13、国外数据挖掘书籍之：\n1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍。华裔科学家写的书，相当深入浅出。\n2) Data Mining:Practical Machine Learning Tools and Techniques\n3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher）\n14、国外模式识别书籍之：\n1）“Pattern Recognition”\n2）“Pattern Recongnition Technologies and Applications”\n3）“An Introduction to Pattern Recognition”\n4）“Introduction to Statistical Pattern Recognition”\n5）“Statistical Pattern Recognition 2nd Edition”\n6）“Supervised and Unsupervised Pattern Recognition”\n7）“Support Vector Machines for Pattern Classification”\n15、国外人工智能书籍之：\n1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。\n2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP”\n16、其他相关书籍：\n1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor\n2）Learning.Python第四版，英文\n任何梦想家都不足以成事，因为所有的成功者都是实干家。\n——《浪潮之巅》\n欢迎转发到朋友圈或分享给好友"}
{"content2":"NLP-自然语言处理入门\n1.书籍-理论篇\n吴军老师的的《数学之美》\n《统计自然语言处理(第2版)》（宗成庆）蓝皮版\n《统计学习方法》（李航）\n《自然语言处理简明教程》（冯志伟）\n《自然语言处理综论》（Daniel Jurafsky）\n《自然语言处理的形式模型》（冯志伟）\n2.书籍——实践篇\npython基础教程（翻译版）+python入门博客推荐：廖雪峰的python教程\n《机器学习实战》哈林顿 (Peter Harrington)\n西瓜书《机器学习》（周志华）\n《集体智慧编程》—[美] 西格兰 著，莫映，王开福 译\n《python自然语言处理》—伯德 (Steven Bird)（主要讲NLTK这个包的使用）\n3.视频——辅助篇\n自然语言处理-宗庆成\n自然语言处理-关毅\n计算语言学概论_侯敏\n计算语言学_冯志伟\n语法分析_陆俭明\n哥伦比亚大学https://class.coursera.org/nlan+他人的博客自然语言处理大菜鸟\nmooc学院-机器学习-大牛Andrew Ng\n网易公开课-机器学习-Andrew Ng\n慕课网-初识机器学习\n台湾大学林轩田机器学习\n斯坦福的nlp课程Video Listing\n4.优秀参考博客\n我爱自然语言处理专门记录nlp的\n北京大学中文系 应用语言学专业\n5.国际学术组织、学术会议与学术论文\n国际机器学习会议（ICML）\nACL，URL：http://aclweb.org/\n国际神经信息处理系统会议（NIPS）\n国际学习理论会议（COLT）\n欧洲机器学习会议（ECML）\n亚洲机器学习会议（ACML）\nEMNLP：http://emnlp2017.net/ 丹麦哥本哈根 9.7-9.11\nCCKS http://www.ccks2017.com/index.php/att/ 成都 8月26-8月29\nSMP http://www.cips-smp.org/smp2017/ 北京 9.14-9.17\nCCL http://www.cips-cl.org:8080/CCL2017/home.html 南京 10.13-10.15\nNLPCC http://tcci.ccf.org.cn/conference/2017/ 大连 11.8-11.12\nNCMMSC http://www.ncmmsc2017.org/index.html 连云港 11.11 － 11.13\n6.知名国际学术期刊\nJournal of Machine Learning Research\nComputational Linguistics（URL：http://www.mitpressjournals.org/loi/coli）\nTACL，URL：http://www.transacl.org/\nMachine Learning\nIJCAI\nAAAI\nArtificial Intelligence\nJournal of Artificial Intelligence Research\n7.工具包推荐\n中文的显然是哈工大开源的那个工具包 LTP (Language Technology Platform) developed by HIT-SCIR(哈尔滨工业大学社会计算与信息检索研究中心).\n英文的(python)：\npattern - simpler to get started than NLTK\nchardet - character encoding detection\npyenchant - easy access to dictionaries\nscikit-learn - has support for text classification\nunidecode - because ascii is much easier to deal with\n8.Quora上推荐的NLP的论文\nParsing（句法结构分析~语言学知识多，会比较枯燥）\nKlein & Manning: \"Accurate Unlexicalized Parsing\" (克莱因与曼宁：“精确非词汇化句法分析” )\nKlein & Manning: \"Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency\" (革命性的用非监督学习的方法做了parser)\nNivre \"Deterministic Dependency Parsing of English Text\" (shows that deterministic parsing actually works quite well)\nMcDonald et al. \"Non-Projective Dependency Parsing using Spanning-Tree Algorithms\" (the other main method of dependency parsing, MST parsing)\nMachine Translation（机器翻译，如果不做机器翻译就可以跳过了，不过翻译模型在其他领域也有应用）\nKnight \"A statistical MT tutorial workbook\" (easy to understand, use instead of the original Brown paper)\nOch \"The Alignment-Template Approach to Statistical Machine Translation\" (foundations of phrase based systems)\nWu \"Inversion Transduction Grammars and the Bilingual Parsing of Parallel Corpora\" (arguably the first realistic method for biparsing, which is used in many systems)\nChiang \"Hierarchical Phrase-Based Translation\" (significantly improves accuracy by allowing for gappy phrases)\nLanguage Modeling (语言模型)\nGoodman \"A bit of progress in language modeling\" (describes just about everything related to n-gram language models 这是一个survey，这个survey写了几乎所有和n-gram有关的东西，包括平滑 聚类)\nTeh \"A Bayesian interpretation of Interpolated Kneser-Ney\" (shows how to get state-of-the art accuracy in a Bayesian framework, opening the path for other applications)\nMachine Learning for NLP\nSutton & McCallum \"An introduction to conditional random fields for relational learning\" (CRF实在是在NLP中太好用了！！！！！而且我们大家都知道有很多现成的tool实现这个，而这个就是一个很简单的论文讲述CRF的，不过其实还是蛮数学= =。。。)\nKnight \"Bayesian Inference with Tears\" (explains the general idea of bayesian techniques quite well)\nBerg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\" (this is from this year and thus a bit of a gamble, but this has the potential to bring the power of discriminative methods to unsupervised learning)\nInformation Extraction\nHearst. Automatic Acquisition of Hyponyms from Large Text Corpora. COLING 1992. (The very first paper for all the bootstrapping methods for NLP. It is a hypothetical work in a sense that it doesn't give experimental results, but it influenced it's followers a lot.)\nCollins and Singer. Unsupervised Models for Named Entity Classification. EMNLP 1999. (It applies several variants of co-training like IE methods to NER task and gives the motivation why they did so. Students can learn the logic from this work for writing a good research paper in NLP.)\nComputational Semantics\nGildea and Jurafsky. Automatic Labeling of Semantic Roles. Computational Linguistics 2002. (It opened up the trends in NLP for semantic role labeling, followed by several CoNLL shared tasks dedicated for SRL. It shows how linguistics and engineering can collaborate with each other. It has a shorter version in ACL 2000.)\nPantel and Lin. Discovering Word Senses from Text. KDD 2002. (Supervised WSD has been explored a lot in the early 00's thanks to the senseval workshop, but a few system actually benefits from WSD because manually crafted sense mappings are hard to obtain. These days we see a lot of evidence that unsupervised clustering improves NLP tasks such as NER, parsing, SRL, etc,"}
{"content2":"文章目录\n人工智能的分类\n计算机视觉\n人工智能的分类\n计算机视觉（CV，Computer Vision）\n语音识别\n自然语言处理\n推荐系统、专家系统\n计算机视觉"}
{"content2":"一、Word Embedding概述\n简单来说，词嵌入（Word Embedding）或者分布式向量（Distributional Vectors）是将自然语言表示的单词转换为计算机能够理解的向量或矩阵形式的技术。由于要考虑多种因素比如词的语义（同义词近义词）、语料中词之间的关系（上下文）和向量的维度（处理复杂度）等等，我们希望近义词或者表示同类事物的单词之间的距离可以理想地近，只有拿到很理想的单词表示形式，我们才更容易地去做翻译、问答、信息抽取等进一步的工作。\n在Word Embedding之前，常用的方法有one-hot、n-gram、但是他们都有各自的缺点，下面会说明。之后，Bengio提出了NLM，是为Word Embedding的想法的雏形，再后来，Mikolov对其进行了优化，即Word2vec，包含了两种类型，Continuous Bag-of-Words Model 和 skip-gram model。\n二、Word2vec之前\n2.1 one-hot\none-hot是最简单的一种处理方式。通俗地去讲，把语料中的词汇去重取出，按照一定的顺序（字典序、出现顺序等）排列为词汇表，则每一个单词都可以表示为一个长度为N的向量，N为词汇表长度，即单词总数。该向量中，除了该词所在的分量为1，其余均置为0。\n2.2 n-gram\nn-gram可以表示单词间的位置关系所反映的语义关联，在说明n-gram之前，我们从最初的句子概率进行推导。\n假设一个句子S为n个单词有序排列，记为：\n我们将其简记为 ，则这个句子的概率为：\n对于单个概率意思为该单词在前面单词给定的情况下出现的概率，我们利用贝叶斯公式可以得到：\n其中最后一项为在语料中出现的频数。但是长句子或者经过去标点处理后的文本可能很长，而且太靠前的词对于词的预测影响不是很大，于是我们利用马尔可夫假设，取该词出现的概率仅依赖于该词前面的n-1个词，这就是n-gram模型的思想。\n所以上面的公式变为：\n在这里，我们不对n的确定做算法复杂度上的讨论，详细请参考文献[1]，一般来说，n取3比较合适。此外对于一些概率为0的情况所出现的稀疏数据，采用平滑化处理，此类算法很多，以后有时间再具体展开学习。\n2.4 神经语言模型（NLM）\n神经语言模型（Neural Language Model）是Word Embeddings的基本思想，\nNLM的输入是词向量，词向量和模型参数（最终的语言模型）可以通过神经网络训练一同得到。相比于n-gram通过联合概率考虑词之间的位置关系，NLM则是利用词向量进一步表示词语之间的相似性，比如近义词在相似的上下文里可以替代，或者同类事物的词可以在语料中频数不同的情况下获得相近的概率。结合参考文献[1]，举一个简单例子：\n在一个语料C中，S1=“A dog is sitting in the room.”共出现了10000次，S2=\"A cat is sitting in the room\"出现了1次，按照n-gram的模型，当我们输入“A _____ is sitting in the room”来预测下划线上应该填入的词时，dog的概率会远大于cat，这是针对于语料C得到的概率。但是我们希望相似含义的词在目标向量空间中的距离比不相关词的距离更近，比如v(man)-v(woman)约等于v(gentleman)-v(madam)，用这样生成的词向量或者已经训练好的模型在去做翻译、问答等后续工作时，就会很有效果，而NLM利用词向量表示就能达到这样的效果。\nNLM的神经网络训练样本同n-gram的取法，取语料中任一词w的前n-1个词作为Context(w)，则（Context(w)，w）就是一个训练样本了。这里的每一个词都被表示为一个长度为L的词向量，然后将Context(w)的n-1个词向量首位连接拼成（n-1）L的长向量。下面为NLM图解：\n包括四层：输入层、投影层、隐藏层、输出层\n注意：这只是取一个词w后输出的向量y，我们需要的就是通过训练集所有的词都做一遍这个过程来优化得到理想的W，q和U，b。\n上图中所有参数W、q、U、b，以及词向量都是通过训练得到的。\n三、Word2vec\n目前学习了解到的Word2vec有基于Hierarchical Softmax和基于Negative Sampling两种方式，由于这两个模型是相反的过程，即CBOW是在给定上下文基础上预测中心词，Skip-gram在有中心词后预测上下文。两个模型都包含三层：输入层、投影层、输出层。\n3.1 CBOW\n不同于NLM的是，Context(w)的向量不再是前后连接，而是求和，我们记为\n3.1.1 基于Hierarchical Softmax\n基于Hierarchical Softmax的CBOW所要构建的霍夫曼树所需参数如下：\n：从根结点到w对应结点的路径\n：路径上包含结点个数\n：到w路径上的的结点\n：结点编码，根结点不编码\n：非叶子结点（包括根结点）对应的向量\n霍夫曼树构建按照频数大小有左右两种，其实都是自己约定的，在这里就不麻烦了，构建后左结点编码为0，为正类，右结点为1，为负类。\n根据逻辑回归，一个结点被分为正类的概率为\n所以之前我们要构造的目标函数就可以写为以下形式：\n对以上式子进行最优化得到Θ和词向量V，我们发现词向量在这里是累加的，我们省略求各个词的V。\n3.1.2基于Negative Sampling\n对于大规模语料，构建霍夫曼树的工作量是巨大的，而且叶子节点为N的霍夫曼数需要新添(N-1)个结点，而随着树的深度增加，参数计算的量也会增加很多很多，得到的词向量也会不够好，为此，Mikolov作出了优化，将构建霍夫曼树改为随机负采样方法。\n对于给定的上下文Context(w)去预测w，如果从语料中就是存在（Context(w),w），那么w就是正样本，其他词就是负样本。\n我们设负样本集为，词的标签：\n训练目标为增大正样本的概率，减小负样本的概率。可见，对于单词w，基于Hierarchical Softmax将其频数用来构建霍夫曼树，正负样本标签取自结点左右编码；而基于Negative Sampling将其频数作为随机采样线段的子长度，正负样本标签取自从语料中随机取出的词是否为目标词，构造复杂度小于前者。\n3.2 Skip-gram\n由于Skip-gram是CBOW的相反操作，输入输出稍有不同，推导大同小异。\n.................."}
{"content2":"经过几天对nlp的理解，接下来我们说说语言模型，下面还是以PPT方式给出。\n一、统计语言模型\n1、什么是统计语言模型？\n一个语言模型通常构建为字符串s的概率分布p(s)，这里的p(s)实际上反映的是s作为一个句子出现的概率。\n这里的概率指的是组成字符串的这个组合，在训练语料中出现的似然，与句子是否合乎语法无关。假设训练语料来自于人类的语言，那么可以认为这个概率是的是一句话是否是人话的概率。\n2、怎么建立统计语言模型？\n对于一个由T个词按顺序构成的句子，p(s)实际上求解的是字符串的联合概率，利用贝叶斯公式，链式分解如下：\n从上面可以看到，一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。\n我们在求p(s)时实际上就已经建立了一个模型，这里的p(*)就是模型的参数，如果这些参数已经求解得到，那么很容易就能够得到字符串s的概率。\n3、求解的问题\n假定字符串s为“i want to drink some water”，那么根据上面所建立的模型：\n问题归结为如何求解上面的每一个概率，比如，一种比较直观的方法就是分别计算出“I want to”和“I want to drink”在语料中出现的频数，然后再用除法：\n看起来好像很美好，实际上这里存在两个问题：\n（1）自由参数数目：\n假定字符串中字符全部来自与大小为V的词典，上述例子中我们需要计算所有的条件概率，对于所有的条件概率，这里的w都有V种取值，那么实际上这个模型的自由参数数目量级是V^6，6为字符串的长度。\n从上面可以看出，模型的自由参数是随着字符串长度的增加而指数级暴增的，这使我们几乎不可能正确的估计出这些参数。\n（2）数据稀疏性：\n从上面可以看到，每一个w都具有V种取值，这样构造出了非常多的词对，但实际中训练语料是不会出现这么多种组合的，那么依据最大似然估计，最终得到的概率实际是很可能是0。\n4、怎么解决？\n上面提出了传统统计语言模型的两个问题，后面分别介绍两种方法进行求解：N-gram语言模型，神经概率语言模型\n二、N-gram语言模型\n1、什么是N-gram语言模型？\n为了解决自由参数数目过多的问题，引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的n个词有关。基于上述假设的统计语言模型被称为N-gram语言模型。\n2、如何确定N的取值？\n通常情况下，n的取值不能够太大，否则自由参数过多的问题依旧存在：\n（1）当n=1时，即一个词的出现与它周围的词是独立，这种我们称为unigram，也就是一元语言模型，此时自由参数量级是词典大小V。\n（2）当n=2时，即一个词的出现仅与它前面的一个词有关时，这种我们称为bigram，叫二元语言模型，也叫一阶马尔科夫链，此时自由参数数量级是V^2。\n（3）当n=3时，即一个词的出现仅与它前面的两个词有关，称为trigram，叫三元语言模型，也叫二阶马尔科夫链，此时自由参数数量级是V^3。\n一般情况下只使用上述取值，因为从上面可以看出，自由参数的数量级是n取值的指数倍。\n从模型的效果来看，理论上n的取值越大，效果越好。但随着n取值的增加，效果提升的幅度是在下降的。同时还涉及到一个可靠性和可区别性的问题，参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性。\n3、建模与求解\nN-gram语言模型的求解跟传统统计语言模型一致，都是求解每一个条件概率的值，简单计算N元语法在语料中出现的频率，然后归一化。\n4、平滑化\n我们在传统统计语言模型提出了两个问题：自由参数数目和数据稀疏，上述N-gram只是解决了第一个问题，而平滑化就是为了解决第二个问题。\n假设有一个词组在训练语料中没有出现过，那么它的频次就为0，但实际上能不能认为它出现的概率为0呢？显然不可以，我们无法保证训练语料的完备性。那么，解决的方法是什么？如果我们默认每一个词组都出现1次呢，无论词组出现的频次是多少，都往上加1，这就能够解决概率为0的问题了。\n上述的方法就是加1平滑，也称为拉普拉斯平滑。平滑化还有许多方法，这里就不展开介绍了：\n（1）加法平滑\n（2）古德-图灵平滑\n（3）K平滑\n三、神经概率语言模型\n1、前置知识\n在N-gram语言模型中，计算条件概率的方法是简单的用词频做除法然后归一化。\n在机器学习的领域中，通用的做法是：对所考虑的问题建模后先为其构造一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，最后再利用这组参数对应的模型来进行预测。\n那么在上述的语言模型中，利用最大化对数似然，将目标函数设为：\nContext代表词w的上下文，对应N-gram就是词w的前N-1个词。之后对目标函数进行最大化，由上可见，概率实际上是w和的函数：\n其中θ为待定参数集，这样将计算所有的条件概率转化为了最优化目标函数，求解得到θ的过程。通过选取合适模型可以使得θ参数的个数远小于N-gram模型中参数的个数。\n2、什么是神经概率语言模型？\nBegio等人在2003年发表的A Neural Probabilistic Language Model，里面详解了这个方法。\n基本的思想其实与上述的前置知识有所联系，既然是神经概率语言模型，那么实现的时候自然有一个神经网络，结构图如下：\n它包括了四个层：输入层、投影层、隐藏层和输出层。\n2、计算流程\n（1）输入层\n这里就是词w的上下文，如果用N-gram的方法就是词w的前n-1个词了。每一个词都作为一个长度为V的one-hot向量传入神经网络中\n（2）投影层\n在投影层中，存在一个look-up表C，C被表示成一个V*m的自由参数矩阵，其中V是词典的大小，而m作为自定义的参数，一般是10^2的倍数。\n表C中每一行都作为一个词向量存在，这个词向量可以理解为每一个词的另一种分布式表示。每一个one-hot向量都经过表C的转化变成一个词向量。\nn-1个词向量首尾相接的拼起来，转化为(n-1)m的列向量输入到下一层。\n（3）隐藏层、输出层\n之后再对列向量进行计算，大致如下：\n其中tanh是激活函数，是为归一化的log概率，之后再用softmax进行归一化，就得到最终的概率输出了。\n在前置知识中我们提到了参数θ，那么在神经网络中，实际的参数如下：\n词向量：v(w)，w以及填充向量\n神经网络参数：W，p，U，q\n3、最后\n在传统统计语言模型中，我们提出两个问题：自由参数数目和数据稀疏。\n这里在实际上使用参数θ代替了自由参数指数级的求解，而数据稀疏问题，我们在最后使用softmax进行归一化，求解出来的概率是平滑的，所以也解决了这个问题。\n\n\n\n\n\n\n参考：（PPT来源小象学院史兴老师）"}
{"content2":"更多实时更新的个人学习笔记分享，请关注：\n知乎：https://www.zhihu.com/people/yuquanle/columns\n微信订阅号：人工智能小白入门学习\nID: StudyForAI\nStanfordcorenlp简介\nStanford CoreNLP提供了一套人类语言技术工具。 支持多种自然语言处理基本功能，Stanfordcorenlp是它的一个python接口。\n官网地址：https://stanfordnlp.github.io/CoreNLP/\nGithub地址：https://github.com/stanfordnlp/CoreNLP\nStanfordcorenlp主要功能包括分词、词性标注、命名实体识别、句法结构分析和依存分析等等。\nStanfordcorenlp工具Demo\n安装：pip install stanfordcorenlp\n先下载模型，下载地址：https://nlp.stanford.edu/software/corenlp-backup-download.html\n支持多种语言，这里记录一下中英文使用方法\nfrom stanfordcorenlp import StanfordCoreNLP zh_model = StanfordCoreNLP(r'stanford-corenlp-full-2018-02-27', lang='zh') en_model = StanfordCoreNLP(r'stanford-corenlp-full-2018-02-27', lang='en')\nzh_sentence = '我爱自然语言处理技术！' en_sentence = 'I love natural language processing technology!'\n1.分词(Tokenize)\nprint ('Tokenize:', zh_model.word_tokenize(zh_sentence)) print ('Tokenize:', en_model.word_tokenize(en_sentence))\nTokenize: ['我爱', '自然', '语言', '处理', '技术', '！'] Tokenize: ['I', 'love', 'natural', 'language', 'processing', 'technology', '!']\n2.词性标注(Part of Speech)\nprint ('Part of Speech:', zh_model.pos_tag(zh_sentence)) print ('Part of Speech:', en_model.pos_tag(en_sentence))\nPart of Speech: [('我爱', 'NN'), ('自然', 'AD'), ('语言', 'NN'), ('处理', 'VV'), ('技术', 'NN'), ('！', 'PU')] Part of Speech: [('I', 'PRP'), ('love', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('technology', 'NN'), ('!', '.')]\n3.命名实体识别(Named Entity)\nprint ('Named Entities:', zh_model.ner(zh_sentence)) print ('Named Entities:', en_model.ner(en_sentence))\nNamed Entities: [('我爱', 'O'), ('自然', 'O'), ('语言', 'O'), ('处理', 'O'), ('技术', 'O'), ('！', 'O')] Named Entities: [('I', 'O'), ('love', 'O'), ('natural', 'O'), ('language', 'O'), ('processing', 'O'), ('technology', 'O'), ('!', 'O')]\n4.句法成分分析(Constituency Parse)\nprint ('Constituency Parsing:', zh_model.parse(zh_sentence) + \"\\n\") print ('Constituency Parsing:', en_model.parse(en_sentence))\nConstituency Parsing: (ROOT (IP (IP (NP (NN 我爱)) (ADVP (AD 自然)) (NP (NN 语言)) (VP (VV 处理) (NP (NN 技术)))) (PU ！))) Constituency Parsing: (ROOT (S (NP (PRP I)) (VP (VBP love) (NP (JJ natural) (NN language) (NN processing) (NN technology))) (. !)))\n5.依存句法分析(Dependency Parse)\nprint ('Dependency:', zh_model.dependency_parse(zh_sentence)) print ('Dependency:', en_model.dependency_parse(en_sentence))\nDependency: [('ROOT', 0, 4), ('nsubj', 4, 1), ('advmod', 4, 2), ('nsubj', 4, 3), ('dobj', 4, 5), ('punct', 4, 6)] Dependency: [('ROOT', 0, 2), ('nsubj', 2, 1), ('amod', 6, 3), ('compound', 6, 4), ('compound', 6, 5), ('dobj', 2, 6), ('punct', 2, 7)]"}
{"content2":"国内自然语言处理期刊\n现代语言学(汉斯出版社)\n汉斯出版社（Hans Publishers, www.hanspub.org) 聚焦于国际开源 (Open Access) 中文期刊的出版发行, 覆盖以下领域: 数学物理、生命科学、化学材料、地球环境、医药卫生、工程技术、信息通讯、人文社科、经济管理等。秉承着传播文化，促进交流的理念，本社将积极探索中文学术期刊国际化道路，并积极推进中国学术思想走向世界。目前，汉斯出版社的所有期刊均被知网（CNKI Scholar）等数据库收录。其中，23本被美国《化学文摘Chemical Abstracts》收录，30本被EBSCO收录。\n计算机学报\n《计算机学报》刊登的内容覆盖计算机领域的各个学科，以论文、技术报告、短文、研究简报、综论等形式报道以下方面的科研成果：计算机科学理论、计算机硬件体系结构、计算机软件、人工智能、数据库、计算机网络与多媒体、计算机辅助设计与图形学以及新技术应用等。\n计算机研究与发展\n刊登内容：计算机科学技术领域高水平的学术论文、最新科研成果和重大应用成果。刊登内容：综述、软件技术、信息安全、计算机网络、体系结构、人工智能、计算机应用技术（图形图象、自然语言处理、信息检索）、数据库技术、存储技术及计算机计算机基础理论等相关领域。\n《软件学报》\n《软件学报》注重刊登反映计算机科学和计算机软件新理论、新方法和新技术以及学科发展趋势的文章,主要涉及理论计算机科学、算法设计与分析、系统软件与软件工程、模式识别与人工智能、数据库技术、计算机网络、信息安全、计算机图形学与计算机辅助设计、多媒体技术及其他相关的内容.\n中国中文信息学会\n学会的学术研究内容是利用计算机对汉语的音、形、义等语言文字信息进行的加工和操作，包括对字、词、短语、句、篇章的输入、输出、识别、转换、压缩、存储、检索、分析、理解和生成等各方面的处理技术。中文信息处理学科是在语言文字学、计算机应用技术、人工智能、认知心理学和数学等相关学科的基础上形成的一门新兴的边缘学科。\n中国中文信息学会2018年学术活动计划\n国际自然语言处理及中文计算会议\n中文信息学报\n《中文信息学报》刊登内容有：计算语言学，包括：音位学、词法、句法、语义、知识本体和语用学；语言资源，包括：计算词汇学、术语、电子词典和语料库；机器翻译（MT）或机器辅助翻译（MAT）；汉语和少数民族语言文字输入输出和处理；中文手写和印刷体识别（OCR）；中文语音识别与合成以及文语转换（TTS）；信息检索（IR）信息抽取（IE）及相关的语言技术；网上搜索引擎；数据挖掘、知识获取、神经网络、机器学习、专家系统、知识工程和其他人工智能（AI）技术。\n国外自然语言处理期刊\n【2018年自然语言处理及相关国际会议重要日期整理】\nNLP会议\n会议名称\n截稿日期\n通知日期\n会议日期\n举办地点\nACL 2018\n2.22\n4.20\n7.15-7.20\n墨尔本，澳大利亚\nMNLP 2018‍\n5.22\n8.6\n10.31-11.04\n布鲁塞尔，比利时\nNAACL HLT 2018\n已过\n已过\n6.01-6.06\n新奥尔良，美国\nCOLING 2018\n3.16\n5.17\n8.20-8.25\n圣达菲，美国\nCICLING 2018\n已过\n已过\n3.18-3.24\n河内,越南\n相关会议\n会议名称\n截稿日期\n通知日期\n会议日期\n举办地点\nIJCAI-ECAI 2018\n已过\n4.16\n7.13-7.19\n斯德哥尔摩，瑞典\nAAAI 2018\n已过\n已过\n2.02-2.07\n新奥尔良，美国\nNIPS 2018\n待定\n待定\n12.03-12.08\n蒙特利尔，加拿大\nICML 2018\n已过\n5.11\n7.10-7.15\n斯德哥尔摩，瑞典\nSIGIR 2018\n已过\n4.11\n7.08-7.12\n安娜堡，美国\nKDD 2018\n已过\n5.06\n8.19-8.23\n伦敦，英国\nWSDM 2018\n已过\n已过\n2.06-2.08\n洛杉矶，美国\nCIKM 2018\n5.15\n8.06\n10.22-10.26\n灵格托，意大利\nWWW 2018\n已过\n已过\n4.23-4.27\n里昂，法国"}
{"content2":"编者注： 更多人工智能业务方面重要的发展请关注2018年4月10-13日人工智能北京大会。\n人工智能的奇妙之处在于，它能让机器像人类一样拥有理解能力，完成智能任务。而它的难解之处在于，如何让人工智能拥有理解力，甚至让机器可以像人一样思考。让人工智能“听懂人话”，是近几年数据科学家们一直在做的努力，也收获了很多欣喜：\n为用户提供实时应答服务；\n为用户提供精确的搜索、推荐等个性化服务；\n辅助医生对患者进行综合诊疗；\n在无人驾驶、新闻传媒、在线娱乐、金融、教育等领域还将有许多令人期待的美好应用前景。\n可这些离人工智能像人的“小目标”还远着呢！尽管机器学习、深度学习、神经网络的发展推动自然语言技术的进步，但想要让机器学会理解，首先需要攻克人工智能的核心领域——自然语言处理技术（NLP）。关于它的技术痛点，应用难题，你都会在AI Conference 2018北京站 “自然语言处理与语音技术”板块中茅塞顿开。\n这里汇集了传媒/新闻、机器翻译、电信和教育行业的应用案例，先进的模型与算法,这里有我们耳熟能详的小冰，还有百度、微软、Intel、谷歌这些人工智能巨头，分享他们这些年在人工智能上翻过的山，趟过的河。\n在进入NLP的世界探究其奥秘之前，我迫不及待的要分享给你一些很有价值的内部消息：\n1. 你将会成为少数深入了解微软小冰的一员\n方向：与人工智能交互\n主题：小冰从人类与AI之间的对话中学到的经验教训\n主讲人：周力（微软中国）\n语音识别是人机交互的入口，经过四年的探索，小冰已经成为科技史上最大规模的人工智能情感计算框架系统。她当过歌手、诗人、主持人、评论员、客服，与中国、日本和美国超过1亿用户进行互动，从中学习人类特有的情感。小冰的每一次演进都让我们对机器能做什么产生了非常多的联想，这次又是什么呢？\n4月12日，微软小冰首席架构师周力博士将亲自带着小冰来到AI Conference 2018北京站，与大家分享在过去四年中研究微软小冰的感悟。希望在不久的将来，人类的生活会因为与人工智能的直接交流，变得更加美好。\n❖\n2. 使用Intel AI技术的NLP企业案例让你醍醐灌顶\n方向：模型与方法\n主题：深度学习时代的数据科学和自然语言处理\n主讲人：Yinyin Liu（Intel Nervana）\n2016年起，Intel逐渐将自己的战略重心转移到了数据科学和人工智能领域，向业界提供 AI 解决方案。最近几年主要的AI推动力是由深度学习产生的，NLP利用深度学习最新算法发展例如文档理解之类的应用，使公司能够筛查海量文本，分类并找到相关信息。\nIntel人工智能产品事业部数据科学主任Yinyin Liu将会与你讨论深度学习最新发展如何影响处理文本、语言及基于对话应用，并启发了利用数据的新方向。另外，还毫不吝啬的为大家分享一些使用Intel® AI技术的NLP企业案例。\n❖\n3. NLP落地金融了\n方向：模型与方法\n主题：使用AI来分析财务新闻的影响\n主讲人：ZhefuShi (密苏里大学)\nAI的领域在不断地进展之中，越来越多的公司认识到NLP对于金融行业分析的重要作用。在金融领域，AI技术对分析金融新闻的影响是有帮助的，将非结构化数据结构化处理，从中探寻影响市场变动的线索。比如通过历史金融新闻预测价格趋势，评估市场风险；为监管人员提供企业监管、市场监管、舆情监控；应用知识图谱和图谱计算技术来进行风险管理、供应链金融管理和投融资管理等。\n但自然语言处理技术，目前是人工智能进行场景落地时的一大难点重点。密苏里大学的Zhefu Shi博士带着自己的知识宝库与你分享如何使用AI来分析财务新闻的影响，如何提取金融实体信息并将其用于分析业务影响，敬请关注。\n❖\n4. 提升深度学习的表现有技巧\n方向：企业人工智能, 实施人工智能, 模型与方法\n主题：深度学习在文本挖掘中的应用\n主讲人：Emmanuel Ameisen (Insight DataScience), Jeremy Karnowski (Insight Data Science)\n深度学习在自然语言处理中的应用非常广泛，可以说横扫自然语言处理的各个应用，从底层的分词、语言模型、句法分析等到高层的语义理解、对话管理、知识问答等方面都几乎都有深度学习的模型，并且取得了不错的效果。多数公司已经开始利用文本数据支持部分业务运营，但也遇到了一系列挑战，其中包括如何验证和解释模型性能，以及模型复杂性如何影响部署它们的简便性。\nEmmanuel Ameisen和Jeremy Karnowski通过对google，Facebook，Amazon，Twitter，Salesforce，Airbnb等超过75个团队的对话进行分析，得到了很有价值的研究成果。分享他们如何从传统的机器学习算法转变成更有表现力的深度学习模型，如卷积神经网络和回归神经网络。这些新技术使公司能够改进许多关键业务操作，您将学习不同模型在不同项目中的应用，并了解如何选择最适合您项目的模型。\nGartner认为，未来10年，人工智能将成为最具破坏性级别的技术，主要是因为卓越的计算能力、漫无边际的数据集、深度神经网络领域的超乎寻常的进步。与其停留观望不如赶快行动，跟着大咖学习人工智能领域NLP的新知识，借鉴他们在人工智能布局的新思路。\n自然语言处理技术与语音技术\n15个议题   6大方向\n企业人工智能、模型与方法\n实施人工智能、人工智能交互\n……\n4月10-13日，AI Conference 2018北京站已经准备好了，你呢？"}
{"content2":"什么是人工智能\n人工智能是计算机科学的一个分支，她企图了解智能的实质，并产生一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统。\n机器学习\n机器主要通过大量的训练数据进行训练，程序不断地进行自我学习和修正来训练出一个模型，而模型的本质就是一堆参数用成千上万的参数来描述业务特点，从而接近人类的智力。\n深度学习\n深度学习是机器学习的一个子集。\n深度学习的前身是人工神经网络（ANN），它的基本特点就是模仿人脑神经元传递和处理信息的模式。\n有监督学习：输入的训练数据有特征、有标记，在学习中就是找到特征与标记之间的映射关系，通过标记不断纠正学习中的偏差，使预测率不断提高。这种训练数据有标记的学习称为有监督学习。\n无监督学习：让计算机自己去学习怎样做一些事情，所有训练数据没有标记，只有特征。无监督学习有两种思路：第一种，训练时不为其指定明确分类但数据会呈现聚群的结构，彼此相似的类型会聚集在一起。计算机把这些没有标记的数据分成一个个组合，就是聚类；第二种，在成功时采用某种激励制度，即强化学习.\n半监督学习：训练数据中有一部分有标记有一部分无标记，没有标记的数量远远大于有标记的数量（这也符合现实）。它的基本规律是：数据的分布必然不完全随机，通过结合有标记的局部特征，以及大量没标记的数据的整体分布，可以得到比较好的分类结果。"}
{"content2":"文章目录\n1. THUCNews中文数据集\n1.1 数据下载\n1.2 数据探索\n2. IMDB英文数据集\n2.1 数据下载\n2.2 数据探索\n3. 常用评估方式\n1. THUCNews中文数据集\nTHUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。\n1.1 数据下载\n官网链接 http://thuctc.thunlp.org/#获取链接， 提供个人信息后可下载。\n1.2 数据探索\n数据集中包含四个文本文件：cnews.test.txt，cnews.train.txt，cnews.val.txt，cnews.vocab.txt。\ncnews.train.txt为训练数据集，cnews.test.txt为测试数据集，cnews.val.txt为验证数据集，cnews.vocab.txt是所有数据集中出现的汉字、字母与标点符号汇集成的词典，其中是词汇表中添加的辅助Token，用来补齐句子长度。\n简单建立一个数据字典观察一下，可以看到包含的中文汉字还是挺多的，基本上常用字都包含了，附部分截图：\n2. IMDB英文数据集\n数据集包含电影评论及其关联的二进制标签，旨在作为情感分类的基准。核心数据集包含50,000个评论，均匀分为25k训练集和25k测试集。\n标签的整体分布是平衡的（25k pos和25k neg），还包括另外50,000个未标记文档，用于无监督学习。\n2.1 数据下载\nhttp://ai.stanford.edu/~amaas/data/sentiment/ 进入后直接点击Large Movie Review Dataset v1.0开始下载即可。\n2.2 数据探索\n下载后会得到一个aclImdb_v1.tar.gz压缩包，解压之后可以看到，文件夹中包含train训练数据集的文件夹和test测试数据集文件夹。\n在训练数据集中主要包括两个已标记情感类别的影评数据集pos和neg和一个未标记的用于无监督学习的数据集unsup，还有一个imdb的词汇表字典，包含了训练集中出现的所有单词。\n测试集中主要包括两个已标记情感类别的影评数据集pos和neg。\n同样简单建立一个数据字典观察一下，这个…英文单词果然是庞然大物，太多了，密集恐惧…附部分截图：\n3. 常用评估方式\n首先要提出混淆矩阵：\n混淆矩阵\nPositive\nNegative\nPositive\nTP\nFP\nNegative\nFN\nTN\nTrue Positive(真正, TP)：将正类预测为正类数\nTrue Negative(真负 , TN)：将负类预测为负类数\nFalse Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error)\nFalse Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error)\n准确率(accuracy) 预测准确的在所有样本中的比例， accuracy=\n(\nT\nP\n+\nT\nN\n)\nT\nP\n+\nF\nN\n+\nF\nP\n+\nT\nN\n\\frac{(TP+TN)}{TP+FN+FP+TN}\nTP+FN+FP+TN(TP+TN)\n精确率（precision）：precision=\nT\nP\nT\nP\n+\nF\nP\n\\frac{TP}{TP+FP}\nTP+FPTP\n对于给定的测试数据集，分类器正确分类的样本数与正样本数之比。（简单点：给出的结果有多少是正确的）；精确率是针对预测结果而言的，它表示的是预测为正的样本中有多少是对的。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)。\n召回率（recall）： recall =\nT\nP\nT\nP\n+\nF\nN\n\\frac{TP}{TP+FN}\nTP+FNTP （正确的结果有多少被给出了）\n召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。\nROC 关注两个指标:一个是TPR（也就是召回率），另一个是将负例错分为正例的概率（FPR=\nF\nP\nT\nP\n+\nT\nR\n\\frac{FP}{TP+TR}\nTP+TRFP ）。直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，\nAUC（Area Under Curve）被定义为ROC曲线下的面积。可以综合衡量一个预测模型的好坏，这一个指标综合了precision和recall两个指标。简单说：AUC值越大的分类器，正确率越高。\nAUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。\nAUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。\nAUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5 的情况。"}
{"content2":"自然语言处理的方法\n分词\n分词的任务定义为：输入一个句子，输出一个词语序列的过程。如将「严守一把手机关了。」输出为「严守一/把/手机/关/了。」\n目前的两种主流方法包括基于离散特征的 CRF 和 BILSTM-CRF。\n挑战包括交叉歧义、新词识别、领域移植、多源异构数据融合及多粒度分词等。\n命名实体\n现在的主流方法包括：\n1. 规则系统\n2. 基于机器学习的学习系统\n目前的挑战包括新领域旧实体类别识别、新实体类别识别等，解决办法包括利用构词知识、领域知识，使用强化学习、跨领域学习、半监督学习、众包、远程监督等机器学习方法。\n句法分析\n句法分析的任务定义为：输入一个句子的词语序列，输出为句子结构表示的过程。依存句法分析输出的是依存句法树，下面以依存句法分析为例。\n目前采用的方法包括：\n基于图的方法，即从图中搜索得到句法树，主要的任务在于确定每个依存弧的分值；\n基于转移的方法：即通过一系列移进规约的动作得到句法树，主要任务在于基于当前状态，确定每个动作的分值。\n现在的主流做法是在上述两者的基础上加入深度学习的方法。\n语义分析\n定义是将文本转换为可计算的知识表示。目前学术界语义表达方法包括：1）浅层语义分析；2）逻辑语义分析；3）抽象语义表示分析。\n篇章分析\n篇章的定义指的是一系列连续的语段或句子构成的语言整体单位，核心问题是篇章结构和篇章特征，其所基于的语言学基本理论包括中心理论、脉络理论、RST 等多种语言学基本理论。\n基本结构分析\n篇章结构指的是篇章内部关系的不同结构化表达形式，主要包括逻辑语言结构、指代结构、话题结构、功能结构、事件结构等范畴。\n基本特征的研究\n包括连接性、连贯性、意图、可接受性、信息性、情景性和跨篇章等七个基本特征。\n自然语言生成\n张民教授总结了在基于规则、基于知识的检索及基于深度学习等三种自然语言生成方法的优缺点对比及适用场景。\n基于规则\n它的一大优势在于具体领域的能做到精准回答；但相应地，在可移植性及可扩展性上则存在不足；适用的场景以个人助理为主，和任务驱动型的对话。\n基于知识的检索\n它的优点在于知识库易于扩充，答案没有语法错误；但对话连续性差，容易出现答非所问的情况；适用场景以问答系统、娱乐聊天为主。\n基于深度学习\n基于数据驱动的方法能够省去显示语言理解等过程，但需要大量语料支持；适用场景以虚拟影像、智能聊天机器人为主的有丰富领域语料的场景。\n自然语言处理的应用\n1. 情感和情绪分析\n在业界研究和应用，情感一般包括正面、负面和中性，而情绪一般表现为喜、怒、哀、乐、惊、恐、思等。情绪和情感都是人对客观事物所持的态度体验，只是情绪更倾向于个体基本需求欲望上的态度体验，而情感则更倾向于社会需求欲望上的态度体验。情感和情绪分析包括问题驱动和模型驱动两个方面，在工业界和学术界都已经有着广泛的应用和研究。\n2. 问答\n智能问答主要有三方面的要求：一是理解人类语言的内涵；二是推敲知识获取的意图；三是挖掘精确贴切的知识。\n相应地，问答系统需要解决三个问题：\n1. 问题分类、分析和理解（一阶逻辑、二阶逻辑）\n2. 答案的匹配、检索\n3. 答案生成\n问答的四个难点及解决方法\n1）多源异构大数据背景下开放域问答的瓶颈。在效率与覆盖率的权衡下，数据大小与知识占比的关系是每个研究者需要考虑的问题；而结构化数据与非结构化数据的混杂，导致知识挖掘与存储存在相应的难点；此外，数据时效性的变化也给新旧知识的应用带来了挑战。\n以往是用 IR 或 RC 的方法，但目前流行采用对检索所得的多个段落排序，也就是在 IR 和 RC 中加入了排序的操作，进而进行面向多段落的提取/生成答案。\n2）深度语义理解的问答技术。以 Watson 为代表的系统采用的是抽取与置信度计算的方法；目前则是阅读理解抽取/生成式方法推动了技术发展。\n3）知识库与知识图谱。以往的知识库存在可靠性、包容性低，存在通用性不高的问题，目前研究者们更多考虑用当下热门问题自动生成来实现知识图谱的自动更新和扩展。\n4）多模态场景下的问答。问题的对象往往潜藏于多媒体，且答案的判断需要参考其它媒体的数据资源。目前出现了以语言处理 RNN 与图像处理的 CNN 的有机结合方法，实现跨媒体的特征共享、独立和抗依赖。\n对话\n根据应用场景的不同，可分为开放域及封闭域对话系统。高准确率的上下文篇章建模、对话状态转移模型和领域知识建模是目前对话亟待解决的问题。\n知识图谱\n包括知识建模、知识图谱构建、知识融合、知识推理计算以及知识赋能等主要任务。知识图谱构建是目前学术界和产业界研究热点，包括实体及其属性识别、事件抽取、实体事件关系抽取、概念实例化和规则学习等。\n机器翻译\n机器翻译目前已经取得较大进展，未来机器翻译可以从如下领域做发展：\n知识建模和翻译引擎，从词序列到语义到知识，利用知识图谱和各类知识（语言学知识、领域知识、常识知识等）进一步延伸机器翻译的边界；\n研究新的翻译模型，从广度（篇章）和深度（深度理解）进一步推进机器翻译的理解能力。此外，还需要适应产业化的需求和国家战略需求。\n转自：2018中国人工智能大会专题论坛"}
{"content2":"自然语言处理是人工智能领域中的一个重要方向。它研究能人机之间通讯的方式，并涉及机器对人类知识体系的学习和应用．从分词，相似度计算，情感分析，文章摘要，到学习文献，知识推理，都涉及自然语言分析．下面介绍一些中文语言语义分析的资源．（以下只讨论能嵌入到我们程序里的资源）\n1.      同义词词林\n《同义词词林》是80年代出版的一本词典，这提供了词的归类，相关性信息，起始主要用于翻译，哈工大对它进行了细化和扩充，出了《词林扩展版》，其中含有7万多词，17000多种语义，五层编码．12大类，94中类，1428小类，形如：\nAa01A01= 人 士 人物 人士 人氏 人选\n每一个条目对应一种语义，根据分类编号：第一位大写表示大类，第二位小写表示中类…其中涉及了一词多义和一义多词．\n《词林扩展版》网上的下载很多，大小不到1M，可以直接load到程序中，用于简单的分词，文章分类，模糊查找，统计，情感分析（不同感情色彩对应不同类别号）等等．\n2.      哈工大语言云(LTP)\n中文的语义分析工具，大多数都像LTP这样，提供一个在线的分析器，一组API，比较简单稳定的功能．LTP是其中做得比较好的．\n它提供了中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等等功能．但对于进一步语义方面的深入的开发，用处不大，而且需要连网使用，速度和处理数量上都有一些限制．\n详见：http://www.ltp-cloud.com/demo\n3.      结巴分词\n结巴是一个Python的中文分词组件．它提供了分词和词性标注功能．能在本地自由使用, 是Python实现的, 可以很好的和其它Python工具相结合，使用方法如下：\n#encoding=utf-8 import jieba.posseg as pseg import jieba seg_list = jieba.cut(\"我爱北京天安门\", cut_all=True) print \"Full Mode:\", \"/ \".join(seg_list) words = pseg.cut(\"我爱北京天安门\") for w in words: print w.word,w.flag\n执行结果是:\nFull Mode: 我/ 爱/ 北京/ 天安/ 天安门 我 r 爱 v 北京 ns 天安门 ns\n详见: http://www.oschina.net/p/jieba/\n4.      知网 HowNet\n对于语言的理解, 人们更关注语义，即研究文字真正的含义是什么，并希望机器能像人脑一样把知识组织成体系．\n中文语义库开放的资源非常少，《现代汉语语义词典》，《中文概念辞书》这些都是听说过没见过，总之人家是不开放. 就算能去书店买一本, 也用不到程序里. 我在网上只找到了HowNet (可以在csdn下载, 压缩包1.5M左右). 形如:\nNO.=069980 W_C=群众 G_C=N E_C= W_E=the masses G_E=N E_E= DEF=human|人,mass|众\n可以看到它包含：编号, 中文词, 对应英文词, 词性, 约12万多项.\nHowNet在2013年后就不更新了, 以上版本差不多是能在网上找到的比较全的数据了. 它还提供了一些库, 可用于判断相似度等．\n详见：http://www.keenage.com/html/c_index.html\n5.      NLTK与WordNet (sentiwordnet)\nWordNet是一个语义词典, NLTK是Python的一个自然语言处理工具，它提供了访问WordNet各种功能的函数。WordNet形如:\nn 03790512 0 0 motorcycle#1 bike#1 a motor vehicle with two wheels and a strong frame\n其中含有词性, 编号, 语义, 词汇间的关系(同义/反义,上行/下行,整体/部分…), 大家都觉得＂它很棒, 只可惜没有中文支持＂. 其实也不是没中文支持. WordNet有中文以及其它更多语言的支持, 可以从以下网址下载:\nhttp://globalwordnet.org/wordnets-in-the-world/\n其中的数据文件形如：\n03790512-n cmn:lemma 摩托车\n可以看到，它与sentiwordnet的词条编号一致，尽管对应可能不是特别完美，但理论上是：对英文能做的处理，对中文也能做．\nNLTK+WordNet功能非常丰富，强烈推荐《PYTHON自然语言处理NLTK Natural LanguageProcessing with Python》这本书，它已由爱好者译成中文版，可从网上下载．里面不但讨论了具体的实现方法，还讨论了一些研究方向，比如＂从自然语言到一阶逻辑＂…\n6.      随想\n对语言的处理，首先是分词，然后是消歧, 判断词在句中的成份, 识别语义．形成知识网络．．．希望最终机器能像人类一样，学习，思考和创造．\n语言处理在不同的层次有不同的应用：从文章分类，内容提取，到自动诊断病情（IBM Watson），或者存在更通用的逻辑，使机器成为比搜索引擎更智能的各个行业的专家系统．\n自然语言和语义看似多对多的关系，我觉得本质上语义转换成语言是从高维到低的投影．从词林的分类看，真正核心的概念并不太多，但是语义的关系和组合很复杂，再深层次还涉及知识线等等．而语言只是它的表象．在分析过程中，越拟合那表象，差得越多．\n另外，这一领域已经有几十年的历史了，学习时尽可能利用现有工具，把精力集中在目标而非具体过程．多参考人家都实现了什么功能，人家的数据是怎么组织的．"}
{"content2":"记录文字处理的各种简介的代码表示\n1.快速去除中文标点（read的时候要以utf8格式）\ndef clean_str(string): string = re.sub(\"[^\\u4e00-\\u9fff]\", \" \", string) string = re.sub(r\"\\s{2,}\", \" \", string)#合并多个空格为一个 return string.strip()\n2.快速分词,默认一行为一样本\ndef seperate_line(string): return ''.join([word + ' ' for word in jieba.cut(string)]) f=open(\"xxx\",'r',encoding=\"utf8\") lines = list(f.readlines()) lines = [clean_str(seperate_line(line)) for line in lines]\n3.分行，使得一行为一句\nfor line in lines line.replace('\\n','').replace('，','\\n').replace('。','\\n').replace('！','\\n').replace('？','\\n') 重新写入\n4.语料训练集生成\ndef load_positive_negative_data_files(positive_data_file_path, negative_data_file_path): positive_example_lists = read_and_clean_zh_file(positive_data_file_path) #positive_example_lists ---> 0维度上为样本有多少句句子，1维度上为每句的string，单词间空格隔开 negative_example_lists = read_and_clean_zh_file(negative_data_file_path) #positive_example_lists ---> 形式同上 # Combine data x_text = positive_example_lists + negative_example_lists # Generate labels positive_labels = [[1] for _ in positive_example_lists] negative_labels = [[0] for _ in negative_example_lists] y = np.concatenate([positive_labels, negative_labels], 0) return [x_text, y]\n5.句子填充\ndef padding_sentences(input_sentences, padding_token, padding_sentence_length = None): sentences = [sentence.split(' ') for sentence in input_sentences] if padding_sentence_length !=None: max_sentence_length=padding_sentence_length else: max_sentence_length=max([len(sentence) for sentence in sentences]) for i,sentence in generate(sentences): if len(sentence) > max_sentence_length: sentences[i] = sentence[:max_sentence_length] else: sentence.extend([padding_token] * (max_sentence_length - len(sentence))) return (sentences, max_sentence_length)\n6.从gensim训练模型拿词向量\nmodel加载 all_vectors = [] embeddingDim = w2vModel.vector_size embeddingUnknown = [0 for i in range(embeddingDim)] for sentence in sentences: this_vector = [] for word in sentence: if word in w2vModel.wv.vocab: this_vector.append(w2vModel[word]) else: this_vector.append(embeddingUnknown) all_vectors.append(this_vector) return all_vectors\n7.打乱np矩阵的方法\nx=[0,1,2,3,4,5,6] x=np.array(x) np.random.seed(10) shuffle_indices = np.random.permutation(np.arange(len(x))) print(shuffle_indices) x_shuffled = x[shuffle_indices] print(x_shuffled) 输出 [2 6 0 3 4 5 1] [2 6 0 3 4 5 1]\n8.分离部分样本为训练集和验证集\n1.打乱样本顺序（参考上面代码） 2.按比例截断"}
{"content2":"本文为 http://blog.sina.com.cn/s/blog_1334cae810102wovb.html 笔记\n自然语言处理常用术语\n文本主要分为三种文本，自由文本、结构化文本、半结构化文本，自然语言处理一般是对自由文本进行的处理。常见的基本操作如下：\n分词\n通常我们处理的自由文本分为中文、英文等。词为文本最基本的单位，分词是进行自然语言处理中最基本的步骤。分词算法分为词典方法和统计方法。其中，基于词典和人工规则的方法是按照一定的策略将待分析词与词典中的词条进行匹配（正向匹配、逆向匹配、最大匹配）。统计方法是基本字符串在语料库中出现的统计频率，典型的算法有HMM\\CRF等。其中CRF相比HMM有更弱的上下文无相关性假设，理论上效果更好一点。\n英文以空格为分割符，因此不需要进行分词的操作（这是片面的，对于一些特殊情况，依旧需要分词的操作 ，例如 it's等，另外对于英文中复合词的情况，也需要进行一定的识别，因此在进行关键词识别的时候会运营到分词的一些技术）。中文的分词工具有很多，近年来常用的是jieba 和stanford corenlp等。\n词性标注\n在进行词性标注时，需先定义出词性的类别：名词、动词、形容词、连词、副词、标点符号等。词性标注是语音识别、句法分析、信息抽取技术的基础技术之一，词性标注是标注问题，可以采用最大熵、HMM或CRF等具体算法进行模型的训练。自动问答系统中，为了提高用户问题匹配后端知识库的召回率，对一些关键词进行了过滤，包括连词、副词对于全文检索系统，理论上可以通过对用户输入的查询条件进行词性过滤，但由于全文检索是基于词袋模型的机械匹配，并且采用IDF作为特征值之一，因此词性标注的效果不大。\n句法分析\n句法分析的目的是确定句子的句法结构，主谓宾、动宾、定中、动补等。在问答系统和信息检索领域有重要的作用。\n命名实体识别\n命名实体识别是定位句子中出现的人名、地名、机构名、专有名词等。命名实体属于标注问题，因此可以采用HMM\\CRF等进行模型的训练。基于统计的命名实体识别需要基于分词、词性标注等技术。命名实体定义了五大类类型：设施（FAC）\\地理政治实体（GPE）\\位置（LOC）\\人物（PER）。在实际应用中，可以根据自己的业务需求，定义实体类别，并进行模型训练。\n实体关系抽取\n实体关系抽取是自动识别非结构化文档中两个实体之间的关联关系，属于信息抽取领域的基础知识之一。近年来，搜索领域流行的知识图谱技术是构建实体关系。实体关系抽取有多种方式，包括规则匹配、有监督学习、无监督学习。其中有监督学习需要预先定义实体关系类别，并通常将问题建模为分类问题。有监督学习需要预先人工标注语料库。\n---------------------\n作者：Virginia5\n来源：CSDN\n原文：https://blog.csdn.net/Virginia5/article/details/68060563\n版权声明：本文为博主原创文章，转载请附上博文链接！"}
{"content2":"参考书籍《Python自然语言处理》，书籍中的版本是Python2和NLTK2，我使用的版本是Python3和NLTK3\n实验环境Windows8.1，已有Python3.4，并安装了NumPy, Matplotlib，参考：http://blog.csdn.net/monkey131499/article/details/50734183\n安装NLTK3，Natural Language Toolkit，自然语言工具包，地址：http://www.nltk.org/\n安装命令：pip install nltk\n安装完成后测试：import nltk\n没有报错即表明安装成功。\nNLTK包含大量的软件、数据和文档，可以进行文本分析和语言结构分析等。数据资源可以自行下载使用。地址：http://www.nltk.org/data.html，数据列表：http://www.nltk.org/nltk_data/\n下载NLTK-Data，在Python中输入命令：\n>>>import nltk\n>>>nltk.download()\n弹出新的窗口，用于选择下载的资源\n点击File可以更改下载安装的路径。all表示全部数据集合，all-corpora表示只有语料库和没有语法或训练的模型，book表示只有书籍中例子或练习的数据。需要注意一点，就是数据的保存路径，要么在C盘中，要么在Python的根目录下，否则后面程序调用数据的时候会因为找不到而报错。\n【注意：软件安装需求：Python、NLTK、NLTK-Data必须安装，NumPy和Matplotlin推荐安装，NetworkX和Prover9可选安装】\n简单测试NLTK分词功能：\n但是在词性标注上就出现问题了，百度也没有明确的解决办法，若有大神知道是什么原因请不吝赐教！\n词性标注功能就先暂且放一放。\n下面看一下NLTK数据的几种方法：\n1.加载数据\nfrom nltk.book import *\n2.搜索文本\nprint(text1.concordance('monstrous'))\n3.相似文本\nprint(text1.similar('monstrous'))\n4.共用词汇的上下文\nprint(text2.common_contexts(['monstrous','very']))\n5.词汇分布图\ntext4.dispersion_plot(['citizens','democracy','freedom','duties','America'])\n6.词汇统计\n#encoding=utf-8 import nltk from nltk.book import * print('~~~~~~~~~~~~~~~~~~~~~~~~~') print('文档text3的长度：',len(text3)) print('文档text3词汇和标识符排序：',sorted(set(text3))) print('文档text3词汇和标识符总数：',len(set(text3))) print('单个词汇平均使用次数：',len(text3)*1.0/len(set(text3))) print('单词 Abram在text3中使用次数：',text3.count('Abram')) print('单词Abram在text3中使用百分率：',text3.count('Abram')*100/len(text3))\n暂时先练习到这里，基本上对NLTK-Data有了一定的了解，以及学会了其基本使用方法。"}
{"content2":"本博客主要是对网络上的一些关于英文自然语言处理开源工具的博客进行整理、汇总，如果有涉及到您的知识产权等，请联系本人已进行修改，也欢迎广大读者进行指正以及补充。\n本博客将尽量从工具的使用语言、功能等方面进行汇总介绍。\n1 斯坦福大学\n语言：Java\n功能：分词、词性标注、命名实体识别、语法解析、分类。\n介绍：Stanford NLP Group是斯坦福大学自然语言处理的团队，开发了多个NLP工具，官网网址。由于该团队将该开源分为多个子模块，以下将逐一进行介绍。\n1.1 Stanford Word Segmenter\n介绍：采用CRF（条件随机场）算法进行分词，也是基于Java开发的，同时可以支持中文和Arabic，官方要求Java版本1.6以上，推荐内存至少1G。下载地址。\n示例代码：\n[java] view plain copy\n//设置分词器属性。\nProperties props = new Properties();\n//字典文件地址，可以用绝对路径，如d:/data\nprops.setProperty(\"sighanCorporaDict\", \"data\");\n//字典压缩包地址，可以用绝对路径\nprops.setProperty(\"serDictionary\",\"data/dict-chris6.ser.gz\");\n//输入文字的编码；\nprops.setProperty(\"inputEncoding\", \"UTF-8\");\nprops.setProperty(\"sighanPostProcessing\", \"true\");\n//初始化分词器，\nCRFClassifier classifier = new CRFClassifier(props);\n//从持久化文件中加载分词器设置；\nclassifier.loadClassifierNoExceptions(\"data/ctb.gz\", props);\n// flags must be re-set after data is loaded\nclassifier.flags.setProperties(props);\n//分词\nList words = classifier.segmentString(\"语句内容\");\n1.2 Stanford POS Tagger\n介绍：采用Java编写的面向英文、中文、法语、阿拉伯语、德语的命名实体识别工具，下载地址。\n1.3 Stanford Named Entity Recognizer\n介绍：采用条件随机场模型的命名实体工具，下载地址。\n1.4 Stanford Parser\n介绍：进行语法分析的工具，支持英文、中文、阿拉伯文和法语。下载地址。\n1.5 Stanford Classifier\n介绍：采用Java编写的分类器，下载地址。\n最后附上关于中文分词器性能比较的一篇文章(2014.05.27更新)\n1.6 Stanford CoreNLP\n功能：分词、词性标注、命名实体识别、语法分析\n介绍：采用Java编写的面向英文的处理工具，下载网址。\n用户评价：采用它进行英语单词的词性还原，具体应用详见文章《采用Stanford CoreNLP实现英文单词词形还原》。"}
{"content2":"一、会计文本分析\n随着人工智能自然语言处理的发展，近年，文本信息逐渐成为国外会计实证研究的热点，许多学者开始致力于运用文本分析方法来解决会计与财务问题，并取得了众多有价值的研究成果。这里鄙人浅谈一下二者的联系以及如何运用。\n会计文本 一般指由公司发布的具有会计相关性的文本信息。关注对象还包括分析师研究报告、媒体新闻报道、互联网论坛上的帖子，具体来说，狭义会计文本还包括公司披露的年报、季报、招股说明书、季度盈余公告、管理层盈余预告以及电话会议纪要文本；广义的会计文本还包括分析师的研究报告、媒体的新闻报道以及投资者通过各种渠道发表的观点与评论。\n总结的现有的文献我们可以发现，已被量化的会计文本特征有九个，按照是否与内容相关可以分为两类显然，语调、可读性、重复性、管理者特征与文本内容无关，而风险、竞争、虚假性、融资约束则属于文本内容的一部分。\n语调\n语调是会计文本分析最基本的特征，有乐观或者悲观、正面或负面、积极或消极两种对立的感觉构成。中性语调可视为第三种语调，因为大部分词句既不乐观也不悲观。字典法是度量语调的基本方法，研究者通过对乐观和悲观两类单词进行词频统计和比较得到文本整体语调。另一种度量方法是朴素贝叶斯算法。\n文本分析方法\n一、字典法。字典法实质上是一种词频统计法，它基于预设的字典和规则将目标文档中的单词逐一映射到各个集合中，经过统计计算得到文本的量化特征。字典可分为通用、专用和自编三种类型。通用性字典广泛应用于众多研究领域，而不限于会计研究。由于某些词汇在会计用语有其特殊的含义，因此通用类字典的识别能力不强。\n二、机器学习算法。机器学习方法的本质是一种统计算法，具有类似于人工智能的自动学习能力。学习过程是利用培训样本进行反复训练，从而获得有文本处理功能的数学模型。研究者将目标文本输入该数学模型即可输入文本的量化特征。\n自然语言与会计的运用\n利用NLP技术来探索一种新型的会计信息系统核算和审计信息系统的新模式。企业会计信息主要如实反映企业的真实的经济活动状况，客观的核算以及预测未来的经济发展趋势。利用NLP技术来拓展审计电算化的新思路。自然语言会计核算原则和自然处理系统的基本框架，建立经济事项和会计语言对照信息 库，对会计经济事项自动生成会计分录，自动生成会计凭证以及相关重要事项。语言文字是人类社会信息的主要载体，自然语言处理系统包括自然语言入机接口、机器翻译、文献检索、自动文摘、自动校对、语音识别也合成、字符识别等等。\n计算机审计业务主要关注对审计单位电子数据的取得和分析、计算等数据处理的业务，还称不上信息系统审计。从财务报表审计的角度来看，这一阶段的主要业务内容是对交易金额和账户。报表余额进行检查，属于审计程序的实质性测试环节。"}
{"content2":"人工智能一直是个很火的词，被称为新的“风口”、未来的趋势，总之就是很有前瞻性、很未来的概念。但其实，它并不那么“未来”，我们生活中其实每天都在用人工智能。\n我们先明确下“人工智能”的定义：\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n所以，人工智能其实就是计算机科学的一个分支，将来也是会成为人类社会基础设施的一部分。\n现在让我们从头开始，学习人工智能。\n这里有800G的人工智能学习资料，如果你想站在时代的转折点上成为历史的见证者，请认真学习这份学习资料！\n以下是资料概览\n40G人工智能入门课\n\nPython语言入门课\n\n25G机器学习教程\n资料还包括谷歌人工智能学习系统TensorFlow教程、华盛顿大学规模系统和算法的数据操作课、1.8G斯坦福NLP课程...........\n学习人工智能资料基本都在这儿了！\n下载链接：https://pan.baidu.com/s/1JqitYJlsY1h8Jt6zJvWPVQ 密码：o2bu\n更多资料欢迎关注公众号：OFweek机器人网（ofweekrobot）"}
{"content2":"百度词汇\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n随着深度学习的发展，LSTM的应用取得的突破，极大地促进了NLP的发展。\n自然语言处理的主要范畴有以下\n文本朗读（Text to speech）/语音合成（Speech synthesis）\n语音识别（Speech recognition）\n中文自动分词（Chinese word segmentation）\n词性标注（Part-of-speech tagging）\n句法分析（Parsing）\n自然语言生成（Natural language generation）\n文本分类（Text categorization）\n信息检索（Information retrieval）\n信息抽取（Information extraction）\n文字校对（Text-proofing）\n问答系统（Question answering）\n给一句人类语言的问定，决定其答案。 典型问题有特定答案 (像是加拿大的首都叫什么?)，但也考虑些开放式问句(像是人生的意义是是什么?)\n机器翻译（Machine translation）\n将某种人类语言自动翻译至另一种语言\n自动摘要(Automatic summarization)\n产生一段文字的大意，通常用于提供已知领域的文章摘要，例如产生报纸上某篇文章之摘要\n文字蕴含（Textual entailment）\n自然语言处理目前研究的难点\n单词的边界界定\n在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。\n词义的消岐\n许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。\n句法的模糊性\n自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析（Parse）出多棵剖析树（Parse Tree），而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。\n有瑕疵的或不规范的输入\n例如语音处理时遇到外国口音或地方口音，或者在文本的处理中处理拼写，语法或者光学字元识别（OCR）的错误。\n语言行为与计划\n句子常常并不只是字面的意思；例如，“你能把盐递过来吗”，一个好的回答应当是动手把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。\n当前自然语言处理研究的发展趋势：\n第一，传统的基于句法-语义规则的理性主义方法受到质疑，随着语料库建设和语料库语言学的崛起，大规模真实文本的处理成为自然语言处理的主要战略目标。\n第二，统计数学方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。\n第三，浅层处理与深层处理并重，统计与规则方法并重，形成混合式的系统。\n第四，自然语言处理中越来越重视词汇的作用，出现了强烈的“词汇主义”的倾向。词汇知识库的建造成为了普遍关注的问题。\n第五，统计自然语言处理\n统计自然语言处理运用了推测学、机率、统计的方法来解决上述，尤其是针对容易高度模糊的长串句子，当套用实际文法进行分析产生出成千上万笔可能性时所引发之难题。处理这些高度模糊句子所采用消歧的方法通常运用到语料库以及马可夫模型（Markov models）。统计自然语言处理的技术主要由同样自人工智能下与学习行为相关的子领域：机器学习及资料采掘所演进而成。 ——转自维基百科。"}
{"content2":"RNN语言模型\nRNN语言模型\n语言模型\nRNN语言模型\n模型扩展\n语言模型\n语言模型就是指语言产生的规律，一般用来预测所使用语言语序的概率，或者是当前上下文使用某个词语的概率。换句话说，就是用来表示语言产生顺序的建模，用某个词是否恰当，这样的语序构造句子是否妥当这样的。于是，训练出一个语言模型就需要相当大的样本数据。语言模型可以分为：文法型的语言模型（就是定义相关的文法结构，例如主语+谓语+宾语构成陈述句这样的），统计模型，神经网络语言模型。\n其中统计类的语言模型包括N-gram，N-pos，隐马尔科夫链模型、最大熵模型等。就是给出前边的词，判断后面出现词的概率。\np(w3|w1w2)\np(w_3|w_1w_2)表示\nw3\nw_3在词语\nw1w2\nw_1w_2之后出现的概率。具体计算公式为\np(w3|w1w2)=p(w1w2w3)p(w1w2)=Count(w1w2w3)Count(w1w2)\np(w_3|w_1w_2)=\\frac{p(w_1w_2w_3)}{p(w_1w_2)}=\\frac{Count(w_1w_2w_3)}{Count(w_1w_2)}, Count(x)表示x在语料库中出现的频率。这种模型能给出后面单词发生的概率。但是会出现Count(x)=0的情况，为避免这种问题出现了很多平滑技术，例如Laplace平滑等。\n但是这种统计模型的计算非常消耗内存。\nRNN语言模型\nRNN语言模型就是利用RNN神经网络对语言建模，用于描述语言序列的产生过程。RNN神经网络就是循环神经网络，能很好地拟合序列数据。\n假设当前你有大量文本语料库C，根据这个预料你构建了词典V，然后你做分句，把每句话通过扩展变成等长的句子。句子开始以START标志，结束以EOS结束，使用PAD来进行短句子的填充。现在得到长度为L的sequence序列。每个词使用vector进行表示（1-of-N model）序列为\nx1,x2,...,xL\nx_1,x_2,...,x_L，假设\nx1\nx_1是词典V中的第一个词，V的大小为N，则\nx1=[1,0,0,...,0]\nx_1=[1,0,0,...,0].对于RNN输出数据对应的True Value这里选择使用\nx2,x3,...,xL−1,EOS\nx_2,x_3,...,x_{L-1},EOS, 使用符号表示为\ny1,y2,...,yL\ny_1,y_2,...,y_L 对于RNN预测数据表示为\ny′1,y′2,...,y′L\ny'_1,y'_2,...,y'_L。\nht=f(whh∗ht−1+wxh∗xt)\n\\begin{equation} h_t = f(w_{hh}*h_{t-1}+w_{xh}*x_t) \\end{equation}\ny′t=g(ht)\n\\begin{equation} y'_t = g(h_t) \\end{equation}\ny′t\ny'_t是一个N维（词典的大小）向量，表示一个概率分布，即下一个词语出现的概率在词典中的概率分布。\ny′t(n)\ny'_t(n)表示下一个词是词典中第n个词的概率大小。\n损失函数定义维：\nLoss=−1L∑t=1L∑j=1Nyt(j)log(y′t(j))\n\\begin{equation} Loss = -\\frac{1}{L}\\sum_{t=1}^{L}\\sum_{j=1}^{N}y_t(j)log(y'_t(j)) \\end{equation}\n求导根据BackPropogation+SGD进行训练。最小化损失函数。\n模型扩展\n一般对于RNN的训练采用BPTT的算法。\n当L较大时，模型的训练会出现梯度消失和梯度爆炸的问题。\n对于梯度爆炸可以采取Clipping的方法解决，具体就是设置门限，超过这个门限时，进行该梯度方向上的归一化。\n对于梯度消失，可以采用LSTM或GRU来替代SRNN；或者使用ReLU来替代Sigmoid激励函数。"}
{"content2":"Python 自然语言处理（一）NLTK及语料库下载\n参考网站 http://www.nltk.org/\nNLTK是用来进行自然语言处理很强大的包，本文介绍Python下安装NLTK及语料下载\n1. 安装 NLTK\npip install nltk\n如果已经安装了 Anaconda 则默认安装了nltk，但是没有安装语料库\n2. 自动安装语料库\n如果在引入nltk包后，发现没有安装语料库，则可以自动下载安装，命令:\nimport nltk nltk.download() showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\nTrue\n3. 手动导入语料库\n由于自动安装语料库会耗费很大时间，可以手动导入语料库。\n语料库下载地址百度云盘：http://pan.baidu.com/s/1hswoU5u\n下载后的语料库可以导入到以下目录：\n- ‘/home/zhanghc/nltk_data’\n- ‘/usr/share/nltk_data’\n- ‘/usr/local/share/nltk_data’\n- ‘/usr/lib/nltk_data’\n- ‘/usr/local/lib/nltk_data’\n4. NLTK安装包及语料库安装完成\nimport nltk\n# NLTK自带的语料库展示 from nltk.corpus import brown\nbrown.categories()\n[u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction']\nlen(brown.sents())\n57340\nlen(brown.words())\n1161192\n5. NLTK 常用函数"}
{"content2":"NLP自然语言处理(五) 不可思议的机器翻译\n-除了震惊还是震惊的机器翻译.\n如果你认为机器翻译就是英译汉、汉译英，那么你落伍了。\n以下是基于神经网络机器翻译技术的机器人写的一首诗\n\n\n转载请注明出处,谢谢!"}
{"content2":"MOOCs\nMIT 的 Natural Language Processing\nStanford 的cs224n Natural Language Processing\nStanford 的 CS224d: Deep Learning for Natural Language Processing ：讲述深度学习在自然语言处理方面比较成功的应用。\nBOOKs\n入门首选：吴军博士的《数学之美》：深入浅出的讲述了数学在计算机领域的应用，体现了数学的简单美，书中主要涉及了自然语言处理的一些内容。\n宗成庆的《统计自然语言处理》：全面介绍了统计自然语言处理的基本概念、理论方法和最新研究进展。尤其是讲述了中文的自然语言处理。\n《统计自然语言处理基础》：统计自然语言处理的一本著作。\n《Python自然语言处理》：主要讲NLTK这个包的使用。\nLibs\nNLTK：Python的自然语言处理包"}
{"content2":"自然语言处理相关学习资料（转）\nbook\n宗成庆. 统计自然语言处理. 清华大学出版社. 2008. 此书为统计观点，适合CS背景做NLP的人读。\n2.Manning, C. D Foundations of Statistical Natural Language Processing. MIT Press. 1999.\n冯志伟. 自然语言处理的形式模型. 中国科技大学出版社. 2010. 此书讲涵盖句法、语义各个层面 ps：作者是从Linguistic角度去分析自然语言处理\nModel:\nYoshua Bengio. A Neural Probabilistic Language Model. JMLR(2003). 2003. 神经网络语言模型的开山之作，MileStone论文，引用率634(Google Scholar)。\nFrederic Morin, Yoshua Bengio. Hierarchical Probabilistic Neural Network Language Model. Innovations in Machine Learning(2006). 2006.提出了Hierarchical NPLM\nAndriy Mnih, Geoffrey Hinton. Three New Graphical Models for Statistical Language Modelling. ICML(2007). 2007. 提出了三个Model，其中提的较多的是A Log-Bilinear Language Model，后续论文多引用此模型\nAndriy Mnih, Geoffrey Hinton. A Scalable Hierarchical Distributed Language Model. NIPS(2008). 2008. 提出HLBL\nRonan Collobert, Jason Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. ICML(2008). 2008. 旧瓶新酒-TDNN Multitask Learning\nRonan Collobert Jason Weston et al.Natural Language Processing (Almost) from Scratch. JMLR(2011). 2011. 对SENNA进行解释的论文，注意SENNA要区别[5]中的C&W embedding.\nEric H. Huang, Richard Socher, etc. ImprovingWord Representations via Global Context and MultipleWord Prototypes. ACL(2012). 2012. 此篇paper把全局信息加入模型，模型求解用了[5]中的方法\nword2vec系列paper：\nDistributed Representations ofWords and Phrases and their Compositionality\nEfficient Estimation of Word Representations in Vector Space\nword2vec Explained: Deriving Mikolov et al.’s Negative\nSampling Word-Embedding Method 解释性的paper 发布arxiv上的，和有道那个可以一起看\nNitish Srivastava, Ruslan Salakhutdinov,Geoffrey Hinton. Modeling Documents with a Deep Boltzmann Machine. UAI(2013). 类似于LDA的一种topic model\nRNN系列, Recurrent NN能model long term dependency, 训练出的结果比Feed Forward NN结果更好 但训练复杂度更大 这个系列word2vec作者Mikolov研究较多，比如其博士论文\nLinguistic Regularities in Continuous SpaceWord Representations\nRecurrent neural network based language model\nRecursive NN这个主要用在句法分析上，model自然语言存在的递归结构 这个主要是Richard Socher的paper\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\nParsing Natural Scenes and Natural Language with Recursive Neural Networks\nJoseph Turian, Lev Ratinov, Yoshua Bengio. Word representations: A simple and general method for semi-supervised learning. ACL(2010) 对现有的word Representation做了对比 提供一个新的word embedding 读者可以自行复现（见Section 13）。\nJeffrey Pennington，Richard Socher, Chris Manning. GloVe: Global Vectors for Word Representation. EMNLP(2014)\nGloVe与word2vec对比的效果曾经被质疑过 其实word2vec效果差不多\nOmer Levy, Yoav Goldberg.Neural Word Embedding as Implicit Matrix Factorization. NIPS. 2014.\n将SGNS(Skip Gram with Negative Sampling)和矩阵分解等价分析，SGNS等价于分解PMI矩阵。文中作者基于谱方法（SVD）分解shifted PPMI的矩阵，得到了不错的效果（word sim上和word2vec类似）。作者还在arxiv提交了一个分析SGNS的note，结合看更加。\nQ.V. Le, T. Mikolov.Distributed Representations of Sentences and Documents.ICML(2014). 2014. 文中各个实验都体现了好的效果，但是可复现性一直遭到质疑，最近在word2vec的google group上公布了复现方法，已经有人复现出92.6%的结果。\nTutorial：\nTomas Mikolov. Statistical Language Models Based on Neural Networks\nRichard Socher. Recursive Deep Learning for Modeling Semantic Compositionality\nRuchard Socher, Christpher Manning. Deep Learning for Natural Language Processing (without Magic)\nEvaluation：\nYanqing Chen, etc. The Expressive Power of Word Embeddings. ICML(2013). 实验评价了四个model–HLBL[4],SENNA[11],Turian’s[12], Huang’s[6]."}
{"content2":"理解人类语言，在人工智能领域称为自然语言处理，所谓的自然语言处理，就是用计算机处理人类在日常生活串所使用的自然语言的能力。\n让机算机理解自然语言是十分艰难的任务，无法理解计算机语言的原因，主要存在语义、语法、语音问题，归纳起来主要有6条原因：\n1.句子的正确词序规则和概念，难以理解不含规则的句子。\n2.词语的确切含义、形式、词类及构词法。\n3.词的语义分类以及词的多义性和岐义性。\n4.指定和不定特性及所有隶属特性。\n5.问题领域的结构知识和时间概念。\n6.有关语言表达形式的文学知识\n语言的理解与交流需要一个相当庞大和复杂的知识体系，自然语言理解最大的困难就在于对知识不完整性、不确定性、模糊性的处理，了解了以下学习难点，才可以说是真正的入门。\n如何快速入门NLP自然语言处理概述\nhttp://www.duozhishidai.com/article-11742-1.html\n从语言学到深度学习NLP，一文概述自然语言处理\nhttp://www.duozhishidai.com/article-1120-1.html\n改变世界的七大NLP技术\nhttp://www.duozhishidai.com/article-8918-1.html"}
{"content2":"python下NLP工具有很多 jieba, nltk, ltp 等， 虽然他们很强大，但是提供的功能比较分散，而且通常模型比较大。为了方便平时的处理工作，我尝试找了一些集成工具包，发现snownlp还可以，它的分词是基于TnT的，总得来说分词效果逊色于基于词典的分词（比如jieba）。所以决定自己写一个包xmnlp，主打轻量快捷。\n功能\n中文分词 & 词性标注： 基于词典构建DAG图，然后采用动态规划的思想求得最大概率路径（jieba分词采用了反向输出，我采用了正向加权反向输出的方式，使得正反向共同影响分词效果），对于未登录词采用HMM+Viterbi处理\n文本纠错：采用了bi-gram + levenshtein实现\n文本摘要 & 关键词提取：textrank\n情感分析：naive bayes\n文本转拼音：Trie 树检索\n以下展示xmnlp的功能效果，不同模块的原理之后的文章会补上。\n分词&词性标注\n[ In ]\n自然语言处理: 是人工智能和语言学领域的分支学科。\n在这此领域中探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。\n自然语言生成系统把计算机数据转化为自然语言。\n自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。\n[ 分词 ]\n自然语言处理 / : / 是 / 人工智能 / 和 / 语言学 / 领域 / 的 / 分支 / 学科 / 。 / 在 / 这此 / 领域 / 中 / 探讨 / 如何 / 处理 / 及 / 运用 / 自然 / 语言 / ； / 自然 / 语言 / 认知 / 则 / 是 / 指让 / 电脑 / “ / 懂 / ” / 人类 / 的 / 语言 / 。 / 自然 / 语言 / 生成 / 系统 / 把 / 计算机 / 数据 / 转化 / 为 / 自然 / 语言 / 。 / 自然 / 语言 / 理解 / 系统 / 把 / 自然 / 语言 / 转化 / 为 / 计算机程序 / 更 / 易于 / 处理 / 的 / 形式 / 。\n[ 标注 ]\n自然语言处理 un / : un / 是 v / 人工智能 nw / 和 c / 语言学 n / 领域 n / 的 uj / 分支 n / 学科 n / 。 un / 在 p / 这此 un / 领域 n / 中 f / 探讨 v / 如何 r / 处理 v / 及 c / 运用 vn / 自然 d / 语言 n / ； un / 自然 d / 语言 n / 认知 v / 则 d / 是 v / 指让 un / 电脑 n / “ un / 懂 v / ” un / 人类 n / 的 uj / 语言 n / 。 un / 自然 d / 语言 n / 生成 v / 系统 n / 把 p / 计算机 n / 数据 n / 转化 v / 为 p / 自然 d / 语言 n / 。 un / 自然 d / 语言 n / 理解 v / 系统 n / 把 p / 自然 d / 语言 n / 转化 v / 为 p / 计算机程序 n / 更 d / 易于 v / 处理 v / 的 uj / 形式 n / 。 un\n文本纠错\n[ In ]\n这理风景绣丽，而且天汽不错，我的心情各外舒畅!\n[ Out ]\n这里风景秀丽，而且天气不错，我的心情格外舒畅!\n文本摘要&关键词\n[ In ]\n自然语言处理: 是人工智能和语言学领域的分支学科。\n在这此领域中探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。\n自然语言生成系统把计算机数据转化为自然语言。\n自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。\n[ 关键词 ]\n('自然语言', 2.2069266136741321),\n('处理', 1.5572478858429686),\n('是', 1.4182222157079281),\n('系统', 1.2431338210535401),\n('转化', 1.1532093387566391)\n[ 摘要 ]\n自然语言理解系统把自然语言转化为计算机程序更易于处理的形式\n情感分析\n[ In ]\n这件衣服的质量也太差了吧！一穿就烂！\n[ Out ]\n0.009959694621645698\n文本转拼音\n[ In ]\n面朝大海，春暖花开\n[ Out ]\n['mian', 'zhao', 'da', 'hai', '，', 'chun', 'nuan', 'hua', 'kai']\n前往 - > 项目github地址"}
{"content2":"1.先解释何为CFG及PCFG：\n一个栗子：\n2.CKY算法（或称CYK算法）\n“在计算机科学领域，CYK算法（也称为Cocke–Younger–Kasami算法）是一种用来对 上下文无关文法（CFG，Context Free Grammar）进行语法分析（parsing）的算法。该算法最早由John Cocke, Daniel Younger and Tadao Kasami分别独立提出，其中John Cocke还是1987年度的图灵奖得主。CYK算法是基于动态规划思想设计的一种自底向上语法分析算法。”\nCYK算法可以在O(n3)的时间内得出结果。\nCKY算法:\nCYK处理的CFG必须是CNF形式的。所以算法首先要把非CNF形式的CFG转化到（弱等价）CNF形式。CYK是一种自底向上的算法。\n乔姆斯基范式：\n乔姆斯基范式：CNF\n或者，ABC都为非终结符，为终结符。\n那个这个CFG就是采用CNF形式的，可见CNF语法都是二分叉的。任何语法都可以转化成一个弱等价的CNF形式，具体方法如下：（之后会有拓展版的，不只二元了，还有空的与一元的。）\n方法：\nCKY算法用于PCFG下的句法分析\n实现句子fish people fish tanks的句法树分析，实现最可能的统计句法树。\n基于概率的上下文无关语法（PCFG） 是一个五元组, 其定义为(T,  N，S，R，P). 可以看到, 这基本上与 CFG 类似, 只是多出来一个元素 p, 表示在语料中规则出现的概率. 使用p 可以定义一棵语法树出现的概率为树中所有规则出现概率之积. 这样, 当一个句子在可选的范围内有多棵可能的语法树时, 我们选择先验概率大的那棵树, 这样能最大程度避免解析错误。其中，\nN代表非终结符集合\nT代表终结符集合\nS代表初始非终结符\nR代表产生规则集\nP 代表每个产生规则的统计概率\n栗子：\n拓展版：加入了一元。\nCKY：\n动态规划：\n具体算法（类似填表的方法）：\n贴一个：\n维基百科的CYK算法用于CFG。\nhttps://en.wikipedia.org/wiki/CYK_algorithm#/media/File:CYK_algorithm_animation_showing_every_step_of_a_sentence_parsing.gif\n第一部分：\n下载stanford-parser-full-2018-10-17.zip\n解压：打开eclipse创建一个项目,导入在build path中引入stanford-parser-3.9.2-models.jar，stanford-parser.jar， slf4j-api.jar等相关库.\n调参：\n主要代码：\n结果：\n句法树：\nGUI界面：\n相关教程连接：\nhttp://www.cnblogs.com/Denise-hzf/p/6612574.html\n第二部分：\nPython3.5，pycharm.\n动态规划PCFG+CKY程序:\n链接：\nhttp://f.dataguru.cn/thread-693052-1-1.html\nPCFG 的训练\n对于 PCFG 中的 CFG 部分, 一般是由领域相关的专家给出的, 例如英语专家规定英语的 CFG. 而PCFG 中的 p 是从语料中统计而来. 运用最大似然估计, 可以有: P(X -> Y) = count(X->Y)/count(X)\n注意到, 规则中包括终端词与非终端词两种元素. 在一个适当规模的语料中, 我们可以认为所有的非终端词都会出现, 但是认为所有的终端词都会出现却是不现实的(想一下我们常听到的那个美国农民日常使用的英语单词只有数千个, 而所有的英语单词却有数万个的情况). 当语料中没有出现, 而在我们的测试样本中却出现了少见的单词时, PCFG 会对所有的语法树都给出概率为0的估计, 这对 PCFG 的模型是一个致命的问题.通常的补救措施是, 对语料中所有单词出现次数进行统计, 然后将出现频率少于 t 的所有单词都换成同一个 symbol. 在进行测试时, 先查找测试句子中的所有单词是否在句子中出现, 若没有出现, 则使用 symbol 代替. 通过这种方法, 可以避免 PCFG 模型给出概率为0 的估计, 同时也不会损失太多的信息."}
{"content2":"开源NLP自然语言处理工具集锦\n现状\n首先看看目前常用的分词系统：\nNo\nName\nFeature\n1\nBosonNLP\nhttp://bosonnlp.com/\n2\nIKAnalyzer\nhttp://git.oschina.net/wltea/IK-Analyzer-2012FF\n3\nNLPIR\nhttp://ictclas.nlpir.org/\n4\nSCWS\nhttp://www.xunsearch.com/scws/\n5\n结巴分词\nhttp://www.oschina.net/p/jieba\n6\n盘古分词\nhttp://pangusegment.codeplex.com/\n7\n庖丁解牛\nhttp://zengzhaoshuai.iteye.com/blog/986314\n8\n搜狗分词\nhttp://www.sogou.com/labs/webservice/\n9\n腾讯文智\nhttp://nlp.qq.com/\n10\n新浪云\nhttp://www.sinacloud.com/doc/sae/php/storage.html\n11\n语言云\nhttp://www.ltp-cloud.com/demo/\n博主也是刚开始接触分词，使用的不多，目前看来市场上用的比较多的是中科院的NLPIR分词系统，大家可以在官网上下载试用（貌似是一个月 (～￣▽￣)），然后就被无情的提示license过期。这时只需要在git上下载新的license替换旧license就好啦~\nps.每次更新license有效期一个月，所以大家勤动手吧！\nBosonNLP\n和大多数的NLP工具一样，玻森的处理能力大概就以上几种。\n分词与词性标注\n大家可以点击链接浏览词性分析的文档。博主摘取部分关键信息如下：\n1）分词和词性标注联合枚举的方法\n2）开放API接口\n3）基于序列标注实现的，以词为单位对句子进行词边界和词性的标注，即基于字符串匹配的方法。\n4）结合上下文识别生词\n5）加入了对url、email等特殊词的识别\n6）对词性标签进行调整和优化，实现了更细的标签划分（22个大类，69个标签）\n7）对训练语料进行修正\n8）加入繁简转化，可以处理繁体中文或者繁简混合的中文句子\n9）多种分词选项：\n空格保留选项\n新词枚举强度选项\n繁简转换选项\n特殊字符转换选项\n下面看一下玻森的免费使用次数：\n可见除了词性分析比较多以外，其他的均为500次。(；′⌒`)\n我们这里额外讲解一下rest api——表述性状态转移（Representational State Transfer），它是一种设计风格而非标准，通常基于使用http、uri、xml、以及html这些现有的广泛流行的协议和标准。\n想深入了解的童鞋可以查看下面的链接：\nRest API开发学习笔记——by spring yang\nRest——维基百科\n情感分析\n这是情感分析返回的结果，可见我们查询了两句话，每句话的前面是正面概率，后面是消极概率。\n这里提供一个curl的下载链接：\nCURL官方网址\n新闻分类\n时间转换\n这个在博主看来还是很有意思的，它可以将中文描述的时间短语转换为三种标准的时间格式字符串—：\n1) 时间点（timestamp，表示某一具体时间时间描述）;\n2) 时间量（timedelta，表示时间的增量的时间描述）;\n3)时间区间（timespan，大于一天的有具体起始和结束时间点的时间描述）\n新闻摘要\n摘要系统提供4个输入选项：\n- 新闻标题\n- 新闻正文\n- 字数限制\n- 是否为严格字数限制\n文本聚类引擎\n看到可以文本聚类的时候，博主是很激动的，因为毕设就一直在折腾这个！\n文档中说：该引擎能够对给定的文本进行话题聚类，将语义上相似的文章归为一类\nIKAnalyzer\n点击 IKAnalyzer 链接，可以看到最新的版本也是2012年的，实现的功能比较单一，感兴趣的童鞋可以看看。\n总之，玻森使用比较方便，个人认为界面简介明了，易于初学者使用。"}
{"content2":"目录\n文章目录\n目录\n前言\n汉语的分词与频度统计（1）\n汉语词汇的特点\n汉语的分词与频度统计（2）\n汉语的分词与频度统计（3）\n汉语的分词与频度统计（4）\n汉语的分词与频度统计（5）\n汉语的分词与频度统计（6）\n汉语的分词与频度统计（7）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n汉语的分词与频度统计（1）\n语\n言\n分\n类\n{\n孤\n立\n语\nif\n没\n有\n附\n加\n词\n，\n如\n汉\n语\n黏\n着\n语\nif\n有\n附\n加\n词\n，\n如\n日\n语\n曲\n折\n语\nif\n形\n态\n变\n化\n，\n如\n英\n语\n语言分类\\begin{cases} 孤立语&amp;\\text{if } 没有附加词，如汉语\\\\ 黏着语 &amp;\\text{if } 有附加词 ，如日语 \\\\ 曲折语 &amp;\\text{if } 形态变化，如英语 \\end{cases}\n语言分类⎩⎪⎨⎪⎧ 孤立语黏着语曲折语 if 没有附加词，如汉语if 有附加词，如日语if 形态变化，如英语\n词是自然语言处理中的最小单位。\n语速，词，短语，句子，语群。\n汉语词汇的特点\n结合紧密，使用频繁，汉语的词可以拆开。\n调换位置，有限度地展开。\n字串可以切分为词串。\n提出规划。\n汉语的自动分词是他的重要组成部分，对他分词很困难。\n新领域老方法，新瓶装旧酒。\n汉语的分词与频度统计（2）\nGB分词规划，提出了汉字的分词规则。\n四字词语，一律是词。切分歧义，未登录词，比较困难。\n比如提高中国人民生活水平比较困难。\n覆盖型切分容易出问题，真歧义同属切分型。\n如何排除歧义呢？\n蛋鸡问题先有蛋。\n分词做词切分，前驱字串和后驱字串。\n词法信息实例。\n歧义字串单切，句法规则调整。\n利用语义信息实例进行切分。\n新出现的词最困难，挂一漏万。\n上下文出现的条件，以及分词系统。\n互信息，极大方差，极大熵模型。\n汉语的分词与频度统计（3）\n主要分词方法，正向最大匹配方法，几个字符在一块儿。去掉一个词再试，逆向匹配方法。\n双向匹配法。\n最小分词方法：做的东西是给人看的。\n创造力最丰富：20-40岁的时候。\n不存在切分歧义的点：分段，计算最短路径。图的方法去理解这些东西。\n词网格方法：生成所有可能切分的方式。计算词的概率。\n汉语的分词与频度统计（4）\n哈工大2005年第一名，做到95%。\n语料库，平衡语料库。\n生语料库，半生不熟语料库，句法分析所困。\n语法分析十万级的词汇基本没用。\n共时语料库，历时语料库。\n发展时间一段时间以内，各种模型的正确率。\n统计机器翻译，统计翻译模型。\n中文信息语料库：英语：Brown corpus。\nPenn Treebank。句法树，数学化。\n双语语料库，法律文档语料库。\n词频统计，构建词汇模型的核心。词典收词的规律。\n汉语的分词与频度统计（5）\n《现代汉语频率词典》LJVAC华语共时语料库。\n建立了各地词典。双音节词最多，定量分析。\n用词相同率和地域相关。\n词频反映国家政策的变化。\n汉语的分词与频度统计（6）\n词频一个数表，高频虚，低频实词。定量分析，占90%的词低于10次。\nzipf定律，f正比于1/r。\ny\n=\nk\nx\nc\ny=kx^c\ny=kxc\n指数定理，同取对数。除特高频和特低频以外都符合。\n语料库规律，可以推测句式规律。\n1构语语言模型，模型多少词enough2heap’s law。\n反映了词表长度与语料库的关系。平滑算法更好的保障。\n汉语的分词与频度统计（7）\n其他的统计分布规律，频度和频度词个数，推荐大家看，创世纪的第八天。\n真正的科学，需要枯燥的处理一件事，需要把一件事情做到极致。"}
{"content2":"Python 的几个自然语言处理工具\n自然语言处理（Natural Language Processing，简称NLP）是人工智能的一个子域。自然语言处理的应用包括机器翻译、情感分析、智能问答、信息提取、语言输入、舆论分析、知识图谱等方面。也是深度学习的一个分支。首先介绍一下Python的自然语言处理工具包：\n1.NLTK工具包\nNLTK 在用 Python 处理自然语言的工具中处于领先的地位。它提供了 WordNet 这种方便处理词汇资源的接口，还有分类、分词、除茎、标注、语法分析、语义推理等类库。\n2.Jieba工具包\n3.Pattern工具包\nPattern 工具包包括词性标注工具(Part-Of-Speech Tagger)，N元搜索(n-gram search)，情感分析(sentiment analysis)，WordNet。同时也支持机器学习的向量空间模型，聚类和支持向量机。\n4.TextBlob\nTextBlob 是一个处理文本数据的 Python 库。提供了一些简单的api解决一些自然语言处理的任务，例如词性标注、名词短语抽取、情感分析、分类、翻译等等。\n5.Gensim\nGensim 提供了对大型语料库的主题建模、文件索引、相似度检索的功能。它可以处理大于RAM内存的数据，作者说它是“实现无干预从纯文本语义建模的最强大、最高效、最无障碍的软件”。\n6.PyNLPI\nPython自然语言处理库（Python Natural Language Processing Library，音发作: pineapple） 这是一个各种自然语言处理任务的集合，PyNLPI可以用来处理N元搜索，计算频率表和分布，建立语言模型。他还可以处理向优先队列这种更加复杂的数据结构，或者像 Beam 搜索这种更加复杂的算法。\n7.spaCy\nspaCy是一个商业的开源软件，结合Python和Cython，自然语言处理能力达到了工业强度。是领域内速度最快、最先进的自然语言处理工具。\n8.Polyglot\nPolyglot 支持对海量文本和多语言的处理。它支持对165种语言的分词，对196种语言的辨识，40种语言的专有名词识别，16种语言的词性标注，136种语言的情感分析，137种语言的嵌入，135种语言的形态分析，以及69中语言的翻译。\n9.MontyLingua（英文）\nMontyLingua 是一个自由的、训练有素的、端到端的英文处理工具。输入原始英文文本到 MontyLingua ，就会得到这段文本的语义解释。适合用来进行信息检索和提取，问题处理，回答问题等任务。从英文文本中，它能提取出主动宾元组，形容词、名词和动词短语，人名、地名、事件，日期和时间等语义信息。\n10.BLLIP Parser\nBLLIP Parser（也叫做Charniak-Johnson parser）是一个集成了产生成分分析和最大熵排序的统计自然语言工具。包括 命令行 和 python接口 。\n11.Quepy\nQuepy是一个Python框架，提供将自然语言转换成为数据库查询语言，可以轻松地实现不同类型的自然语言和数据库查询语言的转化。所以，通过Quepy，仅仅修改几行代码，就可以实现你自己的自然语言查询数据库系统。\nGitHub:https://github.com/machinalis/quepy\n12.HanNLP\nHanLP是一个致力于向生产环境普及NLP技术的开源Java工具包，支持中文分词（N-最短路分词、CRF分词、索引分词、用户自定义词典、词性标注），命名实体识别（中国人名、音译人名、日本人名、地名、实体机构名识别），关键词提取，自动摘要，短语提取，拼音转换，简繁转换，文本推荐，依存句法分析（MaxEnt依存句法分析、神经网络依存句法分析）。\n文档使用操作说明：Python调用自然语言处理包HanLP 和 菜鸟如何调用HanNLP\n【参考文献】：\n1.Python自然语言处理工具小结"}
{"content2":"先介绍一下我自己，我有过5年以上机器学习的工作经验，主要工作内容有图像分析,自然语言，模式识别。我认为该领域最稀缺的人才是NLP专业，然后是图像分析（CV），我准备做一个系列的文章，把我在面试过程中遇到的各种技术性问题，每个问题分别讲解。\n1.我常常会遇到问LSTM的问题： 现在详细讲解下\n理解LSTM前要先理解： RNN\n（Recurrent Neural Networks)这种神经网络带有环，可以将信息持久化。\n在上图所示的神经网络AA中，输入为XtXt，输出为htht。AA上的环允许将每一步产生的信息传递到下一步中。环的加入使得RNN变得神秘。不过，如果你多思考一下的话，其实RNN跟普通的神经网络也没有那么不同。一个RNN可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。RNN在一系列的任务中都取得了令人惊叹的成就，比如语音识别，图片标题等等。\nLSTM是这一系列成功中的必要组成部分。LSTM(Long Short Term Memory)是一种特殊的循环神经网络，在许多任务中，LSTM表现得比标准的RNN要出色得多。几乎所有基于RNN的令人赞叹的结果都是LSTM取得的，接下来将着重介绍LSTM。\n长期依赖(Long Term Dependencies)的问题\nRNN的一个核心思想是将以前的信息连接到当前的任务中来，例如，通过前面的视频帧来帮助理解当前帧。如果RNN真的能够这样做的话，那么它们将会极其有用。但是事实真是如此吗？未必。\n有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测句子“the clouds are in the sky”中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是sky。在这种情况下，当前位置与相关信息所在位置之间的距离相对较小，RNN可以被训练来使用这样的信息。\n然而，有时候我们需要更多的上下文信息。比如，我们想预测句子“I am a tall man, .....i can play basketball”中的最后一个单词。最近的信息告诉我们，最后一个单词可能是某种语言的名字，然而如果我们想确定到底是哪种语言的话，我们需要basket这个更远的上下文信息。实际上，相关信息和需要该信息的位置之间的距离可能非常的远。\n随着距离的增大，RNN对于如何将这样的信息连接起来无能为力。\nLSTM，全称为长短期记忆网络(Long Short Term Memory networks)，是一种特殊的RNN，能够学习到长期依赖关系。LSTM由Hochreiter & Schmidhuber (1997)提出，许多研究者进行了一系列的工作对其改进并使之发扬光大。LSTM在许多问题上效果非常好，现在被广泛使用。\nLSTM在设计上明确地避免了长期依赖的问题。记住长期信息是小菜一碟！所有的循环神经网络都有着重复的神经网络模块形成链的形式。在普通的RNN中，重复模块结构非常简单，例如只有一个tanh层。\n我会在专栏和视频中免费给大家具体讲解细节的技术。"}
{"content2":"最近跑了3D CNN，实现MRI的分割及分类。训练过程中将准确率等自动保存成一个txt文件，该文件有64M多，想要从中提取有用信息比如Mean accuracy、sensitivity、specificity和mean dsc等，人工筛选太繁琐，所以想到用python编程来实现。\n去请教了一位研究自然语言处理的计算机系的师兄，实现代码如下：\n# -*- coding: utf-8 -*-\n# @Time : 18-4-18\n# @Author : sadscv\n# @File : textExtract.py\nimport json\nimport re\ndef readfile():\nsave = None\ncount = 0\nwith open(\"data/trainSessionDeepMedic.txt\", \"r\") as f:\ntmpfile = open(\"data/Epoch.txt\", \"wb\")\n# split file\nfor line in f:\nprint(save)\nhead = re.search(\"Starting new Epoch! Epoch\", line, flags=0)\nif head:\npos = re.search(\"#\", line).span()[1]\nnum = line[pos:pos + 2].rstrip(\"/\")\nsave = num\nopen(\"data/Epoch_{}.txt\".format(save), \"wb\")\ncount += 1\ntmpfile = open(\"data/Epoch_{}.txt\".format(save), \"a\")\nelif save:\ntmpfile.write(line)\nreturn count\ndef subfile_extract(epoch):\nwith open(\"data/Epoch_{}.txt\".format(epoch), \"r\") as f:\nclass_json = {}\nfor line in f:\n# 两行 whole epoch\nif re.search(\"finished. Reporting Accuracy over whole epoch.\", line):\ntmp = []\ntmp_flag = False\nfor i in range(4):\ncurrent_line = next(f)\nif re.search('Finished sampling segment', current_line):\ntmp_flag = True\nif re.search(\"VALIDATION\", current_line):\ntmp.append(current_line)\nif tmp_flag:\ntmp.append(next(f))\nclass_json.update(json_generater(tmp, \"whole\"))\n# 8行 subepoch\nif re.search(\"Reporting Accuracy over whole epoch for Class\", line):\ntmpval = []\ntmptrain = []\nfor i in range(8):\ncurrent_line = next(f)\nif re.search(\"VALIDATION\", current_line):\ntmpval.append(current_line)\nif re.search(\"TRAINING\", current_line):\ntmptrain.append(current_line)\nif tmpval:\nclass_json.update(json_generater(tmpval, \"validation\"))\nif tmptrain:\nclass_json.update(json_generater(tmptrain, \"training\"))\nreturn json.dumps(class_json, indent=4)\ndef json_generater(lines, key=None):\n# 送进来8行，输出一个json. 代表epoch*-validation(or training)-class* 包含的数据\n# 如果长度为2，则是overall validation\nif len(lines) == 2:\np0 = re.search(\"mean accuracy of epoch:\", lines[0]).span()[1]\np1 = re.search(\"mean accuracy of each subepoch:\", lines[1]).span()[1]\nmean_accuracy = lines[0][p0:].split(\"=>\")[0].strip()\nsubepoch_accuracy = lines[1][p1:].strip()\noutput_dict = {\n\"mean accuracy of epoch\": mean_accuracy,\n\"mean accuracy of each subepoch\": subepoch_accuracy\n}\nreturn {\"overall\": output_dict}\nif len(lines) == 8:\naccuracy = {}\nsensitivity = {}\nspecificity = {}\ndice = {}\n# Todo 添加函数，重构重复代码。\nfor i in range(len(lines)):\nif re.search(\"accuracy\", lines[i]):\naccuracyidx = re.search(\"mean accuracy of epoch:\", lines[i])\naccuracysubidx = re.search(\"mean accuracy of each subepoch:\", lines[i])\nif accuracyidx:\naccuracy.update({\"epoch\": lines[i][accuracyidx.span()[1]:].split(\"=>\")[0].strip()})\nif accuracysubidx:\naccuracy.update({\"subepoch\": lines[i][accuracysubidx.span()[1]:].strip()})\nelif re.search(\"sensitivity\", lines[i]):\nsen_idx = re.search(\"mean sensitivity of epoch:\", lines[i])\nsen_subidx = re.search(\"mean sensitivity of each subepoch:\", lines[i])\nif sen_idx:\nsensitivity.update({\"epoch\": lines[i][sen_idx.span()[1]:].split(\"=>\")[0].strip()})\nif sen_subidx:\nsensitivity.update({\"subepoch\": lines[i][sen_subidx.span()[1]:].strip()})\nelif re.search(\"specificity\", lines[i]):\nspc_idx = re.search(\"mean specificity of epoch:\", lines[i])\nspc_subidx = re.search(\"mean specificity of each subepoch:\", lines[i])\nif spc_idx:\nspecificity.update({\"epoch\": lines[i][spc_idx.span()[1]:].split(\"=>\")[0].strip()})\nif spc_subidx:\nspecificity.update({\"subepoch\": lines[i][spc_subidx.span()[1]:].strip()})\nelif re.search(\"dice\", lines[i]):\ndice_idx = re.search(\"mean dice of epoch:\", lines[i])\ndice_subidx = re.search(\"mean dice of each subepoch:\", lines[i])\nif dice_idx:\ndice.update({\"epoch\": lines[i][dice_idx.span()[1]:].strip()})\nif dice_subidx:\ndice.update({\"subepoch\": lines[i][dice_subidx.span()[1]:].strip()})\noutput = {\n\"accuracy\": accuracy,\n\"sensitivity\": sensitivity,\n\"specificity\": specificity,\n\"dice\": dice\n}\nl = lines[0][re.search(\"Class-\", lines[0]).span()[1]:re.search(\"Class-\", lines[0]).span()[1] + 1]\nreturn {key + '_class_' + l: output}\ndef main():\ncount = readfile()\nfor i in range(count):\nwith open('data/Epoch_{}.json'.format(i), 'w+') as f:\nf.write(subfile_extract(i))\nif __name__ == \"__main__\":\nmain()\ngithub地址：https://github.com/sadscv/gadgets/tree/master/ZJ_text_extract。这是师兄的github，大家可以关注一下呀\n最后，感谢师兄！！！！！！！"}
{"content2":"本文是《从自然语言处理到机器学习入门》系列课程的第二次作业，由于我的作业环境没有配好（配了n次了还是不行T_T），但是为了保证这一系列作业的完整性，于是经罗曜强律师同意，人工智能A7论坛授权，转载他的作业笔记。\n1 基本要求\n通过自己训练的语言模型编程，判断每句话中是否存在a an用错的问题(所谓用错 指a an用反了 比如 i have a apple是错误的； i have an apple 是正确的)\n2 准备工作\n（1）实验的环境Ubuntu16.04，Python 版本 2.7\n（2）使用kenlm训练一个语言模型，首先要准备kenlm所需要的语料，按照http://kheafield.com/code/kenlm/官方文档上使用说明，训练的文件会被训练成.arpa的格式。\n（3）训练模型：例如:我有名为test.txt的文件需要训练成kenlm指定的.arpa格式文件,训练后的文件为text.arpa，我需要在Ubuntu的Teminal终端使用如下命令进行训练：\nbin/lmplz -o 5 <test.txt > text.arpa\n-o Required. Order of the language model to estimate\n-o 5 代表使用5ngram\n将arpa文件转换为binary文件，这样可以对arpa文件进行压缩，提高后续在python中加载的速度。\nbin/build_binary -s text.arpa text.bin\n3 具体实验\n做好上述前置准备工作后，接着就是在Python下运行text.arpa\n主要分为以下几个步骤：\n#导入训练所需要的包 import kenlm import nltk from itertools import combinations, permutations #将文件导入kenlm语言模型 model = kenlm.LanuageModel(text.bin) #判断a或者an在互换前的得分和互换后的得分，如果互换前的得分高于互换后的得分，则说明a或an没有错误，如果互换后的得分高于互换前的得分则说明a或者an语法错误 def judge_a_or_an(sentence): #创建一个空list，用于存放sentence s = [] #将句子进行分词 \"\"\" Model.score函数输出的是对数概率，bos=False, eos=False意思是不自动添加句首和句末标记符 \"\"\" pre_score = model.score(‘ ’.join(sentence), bos = True, eos = True) #通过循环的方式替换a或an然后进行评分对比 for word in sentence: #如果word里面有a，则把a换成an If word == ‘a’: s.append(‘an’) #如果word里面有an，则把an换成a elif word == ‘an’: s.append(‘a’) #如果word里面没有a或者an，按照原句输出 else: s.append(word) after_score = model.score(‘ ’.join(s), bos = True, eos = True) #对话置换前，置换后的得分，如果置换前得分高于置换后，则返回0，否则返回1 if pre_score > after_score return 0 else: return 1 #打开文件 inputs = open(‘text.arpa’, ’r’) outputs = open(‘text_after.txt’, ‘r’) for line in inputs: data = nltk.tokenize.word_tokenize(line) #调用judge_a_or_an函数 label = judge_a_or_an(data) #格式化输出0或1 print(line + ‘\\t%d’ %(label)) #关闭IO流 outputs.close() inputs.close()\n4 常见错误\n（1）ModuleNotFoundError: No module named ‘kenlm’\nKenlm安装错误，导致无法正常调用kenlm、\n（2）打开文件后未关闭IO流致使文件无法正常输出\n（3）其他语法错误"}
{"content2":"智能语音助手的工作原理是？先了解自然语言处理(NLP)与自然语言生成(NLG)\n语音助手越来越像人类了，与人类之间的交流不再是简单的你问我答，不少语音助手甚至能和人类进行深度交谈。在交流的背后，离不开自然语言处理（NLP）和自然语言生成（NLG）这两种基础技术。机器学习的这两个分支使得语音助手能够将人类语言转换为计算机命令，反之亦然。\n这两种技术有什么差异？工作原理是什么？\nNLP vs NLG：了解基本差异\n什么是NLP？\nNLP指在计算机读取语言时将文本转换为结构化数据的过程。简而言之，NLP是计算机的阅读语言。可以粗略地说，在NLP中，系统摄取人语，将其分解，分析，确定适当的操作，并以人类理解的语言进行响应。\nNLP结合了计算机科学、人工智能和计算语言学，涵盖了以人类理解的方式解释和生成人类语言的所有机制：语言过滤、情感分析、主题分类、位置检测等。\n什么是NLG？\n自然语言处理由自然语言理解（NLU）和自然语言生成（NLG）构成。NLG是计算机的“编写语言”，它将结构化数据转换为文本，以人类语言表达。即能够根据一些关键信息及其在机器内部的表达形式，经过一个规划过程，来自动生成一段高质量的自然语言文本。\nNLP vs NLG：聊天机器人的工作方式\n人类谈话涉及双向沟通的方式，聊天机器人也一样，只是沟通渠道略有不同——您是与机器交谈。当给机器人发送消息时，它会将其拾取并使用NLP，机器将文本转换为自身的编码命令。然后将该数据发送到决策引擎。\n在整个过程中，计算机将自然语言转换为计算机理解的语言，处理，识别语音。语音识别系统常用的是Hidden Markov模型（HMM），它将语音转换为文本以确定用户所说的内容。通过倾听您所说的内容，将其分解为小单元，并对其进行分析以生成文本形式的输出或信息。\n此后的关键步骤是自然语言理解（NLU），如上文所说，它是NLP的另一个子集，试图理解文本形式的含义。重要的是计算机要理解每个单词是什么，这是由NLU执行的部分。在对词汇、语法和其他信息进行筛选时，NLP算法使用统计机器学习、应用自然语言的语法规则，并确定所说的最可能的含义。\n另一方面，NLG是一种利用人工智能和计算语言学生成自然语言的系统。它还可以将该文本翻译成语音。NLP系统首先确定要翻译成文本的信息，然后组织表达结构，再使用一组语法规则，NLG就能系统形成完整的句子并读出来。\n应用\n语音助手只是NLP众多应用程序之一。它还可用于网络安全文章、白皮书、科研等领域。例如，NLP对在线内容进行情绪分析，以改进服务并为客户提供更好的产品。\n而NLG通常用于Gmail，它可以为您自动创建答复。创建公司数据图表的描述说明时，NLG也是很好的工具。\n说NLP和NLG完全不相关，也不正确，因为NLP和NLG相当于学习中的阅读、写作过程，还是有内在关联的。"}
{"content2":"文本标注 (tagging) 是一个监督学习问题，可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测 (structure prediction) 问题的简单形式，标注问题的输入是一个观测序列，输出是一个标记序列护着状态序列，标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测，注意的是可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。\n标注问题氛围学习和标注两个过程(如上图所示)，首先给定一个训练数据集：\n在这里xi为输入观测序列 (一维向量)，yi为相应的输出观测序列 (一维向量)，每个输入观测序列向量的长度为n，对不同样本具有不一样的值，学习系统基于训练数据集构建一个模型，表示为条件概率分布：\n这里的每个xi(i=1,2,...,n)取值为所有可能的观测，每个Yi (i = 1,2..., n)取值为所有可能的标记，一般n远小于N，标注系统按照学习得到的条件概率分布模型，对新输入观测序列找到相应的输出标记序列。具体的对每一个观测序列，找到上式中概率最大的标记序列。\n评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率，精确率和召回率。\n标注问题常用的统计学方法有：详解隐马尔可夫模型(HMM)和自然语言模型之条件随机场理论(CRF)，这两个模型，之前的文章有介绍过。\n标注问题在信息抽取，自然语言处理等领域被广泛应用，是这些领域的基本问题。例如，自然语言处理中的词性标注就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。\n举一个信息抽取的例子，从英文文章中抽取基本名词短语，为此，要对文章进行标注。英文单词是一个观测，英文句子是一个观测序列，标记表示名词短语的\"开始\"、\"结束\"或“其它”。标记序列表示英文句子中基本名词短语的所在位置。信息抽取时，将标记“开始”到标记“结束”的单词作为名词短语。\n标注模型的评价指标\n标注问题常用的评价指标是精确率 (precision )，召回率 (recall) 和F1值，它和分类问题的评价指标相同，为了简便，这里使用分类来进行说，通常标注模型在测试数据集上的预测和或正确或不正确，4中情况出现的总数分别记作：\nTP：将正确类预测为正类数\nFP：将正类预测为负类数\nFP：将负类预测为正类数\nTN：将负类预测为负类数\n那么精确率定义为：P = TP / (TP + FP)\n召回率定义为: R = TP / (TP + FN)\nF1值是根据精确率和召回率来进行计算的表达式为:\n2/ F1 = 1/ P + 1/ R\n即：F1 = 2TP /( 2TP + FP + FN)\n一般精确率和召回率都高时，F1值也会很高。\n参考学习资料：\n[1] 统计学习方法： 李航\n文章来源于微信公众号：言处理技术，更多内容请访问该公众号。\n欢迎关注公众号学习"}
{"content2":"目录\n文章目录\n目录\n前言\n一篇论文的诞生（1）\n一篇论文的诞生（2）\n一片论文的诞生（3）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n一篇论文的诞生（1）\n这是课堂额外福利。\n不同的研究专题不一样。论文的研究过程8年。\n整合现有相似模型，语义相似计算。\n论文创作阶段：\n草创： 2000.1-2001.5\n布局：2001.5-2002.12\n奋争：2003.1-2004.12\n沉沦与转折：2005\n发表之路：2006-2008.7\n2000.1博士毕业去挣钱。\n问一问智能搜索引擎。\n语义相似度，好好计算争取出结果做了4个月出结果了\n写了一个专利，因为第一作者和老板谈崩了，后来想明白了。\n自己是去挣钱的，还想着名，有点儿想太多了。\n2001.5月出局回国，哈工大任教。\n2001-2002，按照老师的方法做的idea是老师的，你只是负责了实现，工作不在你。\n一篇论文的诞生（2）\n反思，小公司不要去争这些名利，大公司还可以争一争。\n新阶段新目标，一个阶段一个目标，错过了进入微软亚洲研究院的机会。\n专心处理后面的遗留问题，一直在处理，争取加速解决。\n进行调研，发现相似系统工程和这个问题一样。\n没做实验就开始投文章被大会举办方使劲怼了。\n国内的优势：大量高素质低成本的人才，写作比一切都重要，擅长于调动资源。\n把idea实验做出来，然后去落实，在做的这个过程中产生了新的想法，句法结构，新的结构，认知系统的深层原理。基础性研究解决了问题。\n一片论文的诞生（3）\n两个压力，项目和职称逼迫，犯了一件蠢事。\n中英文文章投稿，算是沾边一稿多投这一学术问题。\n幸好没接受，什么时候适合写论文呢？\n如鲠在喉，不吐不快时，适合写论文。\n再写其他论文应付工作。\n问答系统，应为参加了比赛，结果也不错，所以写出来，期刊也比较接受。\n写文章之前要有明确投论文的目标，要有对科技问题的认识。\n要知道一些问题适用于一些问题。\n创新不难，难在让人接受。\n绝对不要违规。\n最后给大家一点儿建议。\n1心怀大志\n2始终如一\n3相信自己\n4不懈努力\n5坚持自信。"}
{"content2":"自然语言处理：\n1、什么是自然语言处理（NLP）\n自然语言处理是一门交叉学科，包括计算机科学，人工智能和语言学\n目标：让计算机去处理或“理解”自然语言, 完成一些有用的任务例如问答系统，机器翻译\n完全理解或者表示语言的意义（甚至去定义它）都是一个虚幻的目标\n完美的理解语言是一个“\nAI-complete\n”的问题\n2、自然语言处理的应用\n应用范围从简单到复杂\n拼写检查, 关键词提取&搜索，同义词查找&替换\n从网页中提取有用的信息例如产品价格，日期，地址，人名或公司名等\n分类，例如对教科书的文本进行分级，对长文本进行正负情绪判断\n机器翻译\n口语对话系统\n复杂的问答系统\n3、工业届里的NLP应用\n搜索引擎\n在线广告\n自动的或辅助的翻译技术\n市场营销或者金融交易领域的情感分析\n语音识别\n4、NLP为什么这么难\n语言在表达上就很复杂，使用的时候要综合考虑使用情境\nJane hit June and then she [fell/ran].\n歧义问题：“I made her duck”\n5、现有的中文自然语言处理工具：\nNLPIR\nfudanNLP\nLTP\npython-NLTK\n直接有纠错功能的：腾讯文智   大汉科技\n\nN-gram语言模型：\n目的：在给定语料库的情况下，计算一个字符串出现的概率\nN-gram是自然语言处理（NLP）中一个非常重要的概念，通常在NLP中，人们基于一定的语料库，可以利用N-gram来做以下几类事情：\n预计或者评估一个句子是否合理；\n评估两个字符串之间的差异程度，这也是模糊匹配中常用的一种手段；\n语音识别；\n机器翻译；\n文本分类；\n资料链接：\nn-gram_1（包含开源n-gram数据集）\nn_gram_2\nn-gram_3\nn-gram数据格式\n那么我们如何用N-gram来做篇章单元的分类器呢？其实很简单了，只要根据每个类别的语料库训练各自的语言模型，也就是上面的频率分布表，实质上就是每一个篇章单元的类别都有一个概率分布，当新来一个篇章单元的时候，只要根据各自的语言模型，计算出每个语言模型下这个篇章单元的发生概率，篇章单元在哪个模型的概率大，这篇文本就属于哪个类别了。\n数据稀疏和平滑技术：\n平滑\n中文纠错\n小孙纠错算法\ngithub上中文纠错项目：\nCn_Checker\nxmnlp\njcjc在线纠错\n结巴分词的词典：\n结巴分词词典\n高效的分词工具：\n结巴\nTHULAC"}
{"content2":"自然语言处理-概述\n概述\n1.基本概念\n2.人类语言技术HLT发展简史\n3.HLT 研究内容\n4.基本问题和主要困难\n5.基本研究方法\n概述\n本系列文章计划总结整理中国科学院大学宗成庆老师《自然语言处理》课程相关知识，参考数目《统计自然语言处理》-第二版，宗成庆。\n1.基本概念\n语言学：(Linguistics) 研究语言本质、结构、和发展规律的科学。-商务印书馆，《现代汉语词典》，1996年\n自然语言： 人类特有的书面和口头形式的语言。\n自然语言理解(Natural Language Understanding,NLU)： 研究模仿人类语言认知过程的自然语言处理方法和实现技术的一门学科。 《计算机科学技术百科全书》第三版，P1223，宗成庆，黄昌宁\n计算语言学（Computation Linguistics,CL）： 通过建立形式化的计算模型来分析、理解和生成自然语言的学科，是人工智能和语言学的分支学科。计算语言学更加侧重基础理论和方法的研究《计算机科学技术百科全书》第三版，2018,5，P476，常宝宝\n自然语言处理（Natural Language Processing,NLP）： 自然语言处理是研究如何利用计算机技术对语言文本（句子、篇章或话语）等进行处理和加工的一门学科。 《计算机科学技术百科全书》第三版，P1223，宗成庆，黄昌宁\n人类语言技术（Human Language Technology,HLT）： 就字面意思理解，研究人类语言的技术。\n上个世纪五十年代，学术界对机器翻译产生了浓厚的兴趣；并得到了实业界的支持。因此国际上出现了研究机器翻译的热潮。随着机器翻译的发展，各种自然语言处理技术应运而生；并逐渐发展壮大，形成了这一语言学与计算机技术相结合的新兴学科。\n2.人类语言技术HLT发展简史\n1950s: 基于模板的NLP方法\n1960-1980s： 基于规则的方法\n1990-2013： 统计NLP方法\n2013~： 深度学习的方法\n3.HLT 研究内容\n机器翻译、信息检索、自动文摘、问答系统、信息过滤、信息抽取、文档文类、语音识别、说话人识别。有很多研究方向都密切相关。\n4.基本问题和主要困难\n基本问题： 形态学问题、句法问题、语义问题、语用学问题、语音学问题。\n主要困难：\n大量歧义现象：词法歧义、词性歧义、结构歧义、语义歧义、语音歧义（多音字歧义）。\n大量未知语言现象：随着社会生活的发展，每时每刻都会产生大量的具有新意义的词汇。\n5.基本研究方法\n1.理性主义会基于规则的分析方法建立符号处理系统。\n2.经验主义会基于大规模真实语料（语言真实数据）建立计算方法。"}
{"content2":"自然语言处理(python)环境配置-NLTK的安装\n1.自然语言处理的介绍\nNLP (Natural Language Processing) 是人工智能（AI）的一个子领域。是机器真正能够理解人类说话的重要一环。自然语言处理也不是新的研究领域，早在上个世纪就开始研究，但是给予计算机环境等等因素导致这方面的发展一直停滞不前，再机器学习，统计学，计算机科学的快速发展下，NLP又迎来了新的春天，在将来的发展中也是非常重要的一环。具体介绍可以参见百度百科等（https://baike.baidu.com/item/nlp/25220）。\n2.自然语言处理语言工具\n在自然语言处理中，python也成了当仁不让的语言了，这种包的继承，有如站在巨人的肩膀上前进，但是，这仅是对当前已技术的使用，对于深层次的研究确实是需要花费功夫的。特别是在硕士研究生、博士研究生等等都是是需要真正地去思考语言的形成，这里又乔姆斯基的形式语言学说，中国的自然语言处理大家冯远炜教授的著作都是我们值得去思考和借鉴的，在结合当前的统计学，机器学习，计算机科学的发展，自然语言处理在python这种好用的编程工具的基础上会发展的更好。\n3.自然语言处理的第一步\n当然，博主是想从事这方面研究的小白，才刚刚起步，希望这是一个记录自己成长的平台，也希望把自己知道的，学习中遇到的问题分享出来。这是开始学习，使用的是比较出名的nltk包，当然对于汉字的分词处理等，据博主知道的还有jieba分词等。\n环境准备：\nSystem：window 10\nIDE：anaconda-spyder\n环境配置：似乎anaconda中已经把nltk集成了，当然自己也可以在命令行中输入：pip install nltk(前提是读者已经把环境都配好了)；之后就是打开IDE创建一个py文件\nimport nltk nltk.download()\n即可下载nltk的语料等资源\n如图：\n由于国内访问比较慢，所以需要下载的，博主已经下载好了：链接: https://pan.baidu.com/s/1WbNb-h9U8VKYQXSYZonbvQ 密码: dq4s\n更多信息也可查看官网：http://www.nltk.org/"}
{"content2":"路线图请戳这里"}
