{"content2":"新一代AI人工智能机器学习研讨会即将在杭州举办，特聘叶梓老师为本次研讨会的主讲！本次研讨会，叶梓老师将会就最前沿的人工智能技术与各位与会嘉宾进行探讨。\n叶老师最新的人工智能机器学习技术培训提纲如下：\n人工智能概念与经典算法\n人工智能概念综述\n1、  从一些术语辨析人工智能\n2、  人工智能之连接主义的兴衰史\n3、  这次AI的热潮是怎么来的？\n图像处理领域的最新热点\n1、  分类、目标检测与实例分割\n2、  风格迁移\n3、  自动驾驶\n4、  人体姿态识别\n5、  超分辨率图像生成\n语言处理领域的最新热点\n1、  Attention机制\n2、  自动构建知识图谱\n3、  看图说话\n4、  预训练机制\n三大经典算法\n1、  决策树算法\n2、  决策树案例：手术后驼背的发生概率\n3、  聚类算法\n4、  聚类案例：鸢尾花数据的聚类\n5、  关联规则算法\n6、  关联规则案例：超市购物篮分析\n性能评价指标\n1、  准确率；精确率、召回率；F1\n2、  真阳性率、假阳性率\n3、  混淆矩阵\n4、  ROC与AUC\n5、  案例：绘制ROC并计算AUC、F1\n6、  对数损失\n7、  Kappa系数\n8、  回归：平均绝对误差、平均平方误差\n9、  案例：绘制拟合曲线，计算拟合优度\n10、              聚类：兰德指数、互信息\n11、k折验证\nBP神经网络\n1、  人工神经元及感知机模型\n2、  前向神经网络\n3、  sigmoid\n4、  梯度下降\n5、  误差反向传播\n6、  BP神经网络案例：可手算的神经网络\n支持向量机\n1、  统计学习问题\n2、  支持向量机\n3、  核函数\n4、  多分类的支持向量机\n5、  用于连续值预测的支持向量机\n6、  SVM案例： iris的三个分类\n机器学习进阶与深度学习初步\n隐马尔科夫模型\n1、  马尔科夫过程\n2、  隐马尔科夫模型\n3、  三个基本问题（评估、解码、学习）\n4、  前向-后向算法\n5、  Viterbi算法\n6、  Baum-Welch算法\n集成学习\n1、  bagging\n2、  adaboost\n3、  随机森林\n4、  GBDT\n深度学习初步\n1、  深度学习与神经网络的区别与联系\n2、  目标函数\n3、  激励函数\n4、  学习步长\n5、  Adagrad\\RMSprop\\Adam\n6、  避免过适应\n用于分类的CNN\n1、  分类典型应用场景（imageNet数据集）\n2、  Alexnet（开山之作）\n3、  VGG（5层变为5组）\n4、  GoogLenet（还在试验各种架构的组合吗？）\n5、  Resnet（还可以再“深”下去）\n用于目标检测的CNN\n1、  CNN目标检测典型应用场景\n2、  RCNN（两个头的网络）\n3、  Fast/faster RCNN（又快又好）\n深度学习与强化学习\nRNN（第三天——1）\n1、  基本RNN\n2、  LSTM（三个门）\n3、  GRU（减为两个门）\nGAN\n1、  基本的生成对抗网络\n2、  DCGAN（技巧的胜利）\n3、  Wassertein GAN（理论的胜利）\n强化学习\n1、  agent的属性\n2、  exploration and exploitation\n3、  Bellman期望方程\n4、  最优策略\n5、  策略迭代与价值迭代\n6、  Q学习算法\n深度强化学习案例：教电脑玩“flappy bird”\n1、  DQN详解\n2、  Flappy Bird 游戏\n深度强化学习案例：AlphaGo详解\n1、  蒙特卡罗决策树\n2、  策略网络\n3、  价值网络\n4、  Alpha Go的完整体系"}
{"content2":"原文地址：https://sigmoidal.io/machine-learning-terms/\nGetting started with AI? Perhaps you’ve already got your feet wet in the world of Machine Learning, but still looking to expand your knowledge and cover the subjects you’ve heard of but didn’t quite have time to cover?\nThis Machine Learning Glossary aims to briefly introduce the most important Machine Learning terms - both for the commercially and technically interested. It’s not by any means exhaustive, but a good, light read prep before a meeting with an AI director or vendor - or a quick revisit before a job interview!\nOverview:\n1. NLP - Natural Language Processing\n2. Dataset\n3. Computer Vision\n4. Supervised Learning\n5. Unsupervised Learning\n6. Reinforcement Learning\n7. Neural Networks\n8. Overfitting\n1. NLP - Natural Language Processing\nNatural Language Processing (NLP) is a common notion for a variety of machine learning methods that make it possible for the computer to understand and perform operations using human (i.e. natural) language as it is spoken or written.\nThe most important use cases of Natural Language Processing are:\n1. Text Classification and Ranking\nThe goal of this task is to predict a class (label) of a document, or rank documents within in a list based on their relevance. It could be used in spam filtering (predicting whether an e-mail is spam or not) or content classification (selecting articles from the web about what is happening to your competitors).\n2. Sentiment Analysis\nSentiment analysis aims to determine the attitude or emotional reaction of a person with respect to some topic- e.g.,positive or negative attitude, anger, sarcasm. It is broadly used in customer satisfaction studies (e.g. analyzing product reviews).\n3. Document Summarization\nDocument Summarization is a set of methods for creating short, meaningful descriptions of long texts (i.e. documents, research papers).\nInterested in Natural Language Processing? Read our in-depth article on Natural Language Processing in Artificial Intelligence.\n4. Named Entity Recognition (NER)\nNamed Entity Extraction algorithms process a stream of unstructured text and recognize predefined categories of objects (entities) in it, such as a person, company name, date, price, title etc. It enables faster text analysis by transforming unstructured information into a structured, table-like (or JSON) form.\n5. Speech Recognition\nSpeech Recognition techniques are used for determining a textual representation of an audio signal of people speaking. You have probably heard of Siri, right? This is a good example of how Speech Recognition is used.\n6. Natural Language Understanding and Generation\nNatural Language Understanding is used for transforming a human-generated text into more formal representations interpretable by a computer, and conversely: Natural Language Generation techniques support transformation of a formal logical representation into a human-like generated text. Nowadays, NLG and NLU are mostly used in chatbots and automated report generation.\nConceptually, it’s the opposite of Named Entity Recognition.\n7. Machine Translation\nMachine Translation is a task of automatically translating text or speech from one human language into another.\n2. Dataset\nData is an essential part of machine learning. If you would like to build any machine learning system you need to either get the data (e.g. from some public resource) or collect it on your own. All the data that is used for either building or testing the ML model is called a dataset. Basically, data scientists divide their datasets into three separate groups:\nTraining data\nTraining data is used to train a model. It means that ML model sees that data and learns to detect patterns or determine which features are most important during prediction.\nValidation data\nValidation data is used for tuning model parameters and comparing different models in order to determine the best ones. The validation data should be different from the training data, and should not be used in the training phase. Otherwise, the model would overfit, and poorly generalize to the new (production) data.\nTest data\nIt may seem tedious, but there is always a third, final test set (also often called a hold-out). It is used once the final model is chosen to simulate the model’s behaviour on a completely unseen data, i.e. data points that weren’t used in building models or even in deciding which model to choose.\nImage : The visualization of the MNIST dataset using a mixture of t-SNE and Jonker-Volgenant algorithms. t-SNE is a popular dimensionality reduction algorithm, allowing to “compress” the representation of data for better visualization or processing.\n3. Computer Vision\nComputer Vision (CV) is a field of Artificial Intelligence concerned with providing tools for analysis and high-level understanding of image and video data. The most common problems in CV include:\n1. Image classification\nImage classification is a CV task of teaching a model to recognize what is on a given image. For example, one could train a model to distinguish between various objects in the public space (that could be used with self-driving cars).\n2. Object detection\nObject detection is a CV task of teaching the model to detect an instance of an object from a set of predefined categories by providing a bounding box around each instance of a given class. For example, one could use object detection to build a face recognition system. The model would then be able to draw a bounding box around every face it detects on the picture. (Btw. the image classification system would be only able to tell whether there is a face on the picture or not, and not to detect where is it, like object detection system could do).\n3. Image segmentation\nImage segmentation is a CV task where one trains a model to annotate each pixel with a class from a predefined set to which a given pixel most probably belongs.\nSaliency detection\nSaliency detection is a CV task of training a model to be able to provide a region which would most likely attract the attention of a viewer. This could be used to determine ad placement in videos.\nNeed more details on AI for Computer Vision? Read our article.\n4. Supervised learning\nSupervised learning is a family of machine learning models that teach themselves by example. This means that data for a supervised ML task needs to be labeled (assigned the right, ground-truth class). For instance, if we would like to build a machine learning model for recognizing if a given text is about marketing, we need to provide the model with a set of labeled examples (text + information if it is about marketing or not). Given a new, unseen example, the model predicts its target - e.g. for the stated example, a label (eg. 1 if a text is about marketing and 0 otherwise).\n5. Unsupervised learning\nContrary to Supervised Learning, Unsupervised Learning models teach themselves by observation. The data provided to that kind of algorithms is unlabeled (there is no ground truth value given to the algorithm). Unsupervised learning models are able to find the structure or relationships between different inputs. The most important kind of unsupervised learning techniques is \"clustering\". In clustering, given the data, the model creates different clusters of inputs (where “similar” inputs are in the same clusters) and is able to put any new, previously unseen input in the appropriate cluster.\n6. Reinforcement learning\nReinforcement Learning differs in its approach from the approaches we’ve described earlier. In RL the algorithm plays a “game”, in which it aims to maximize the reward. The algorithm tries different approaches “moves” using trial-and-error and sees which one boost the most profit.\nThe most commonly known use cases of RL are teaching a computer to solve a Rubik’s Cube or play chess, but there is more to Reinforcement Learning than just games. Recently, there is an increasing number of RL solutions in Real Time Bidding, where the model is responsible for bidding a spot for an ad and its reward is the client’s conversion rate.\nEager to catch up with the AI revolution in programmatic advertising and RTB?\n7. Neural Networks\nNeural Networks is a very wide family of machine learning models. The main idea behind them is to mimic the behaviour of a human brain when processing data. Just like the networks connecting real neurons in the human brain, artificial neural networks are composed of layers. Each layer is a set of neurons, all of which are responsible for detecting different things. A neural network processes data sequentially, which means that only the first layer is directly connected to the input. All subsequent layers detect features based on the output of a previous layer, which enables the model to learn more and more complex patterns in data as the number of layers increases. When a number of layers increases rapidly, the model is often called a Deep Learning model. It is difficult to determine a specific number of layers above which a network is considered deep, 10 years ago it used to be 3 and now is around 20.\nThere are many different variations of Neural Networks. Most commonly used are:\nConvolutional Neural Networks - These were a huge breakthrough in Computer Vision tasks (but recently, they also proved very useful in NLP problems).\nRecurrent Neural Networks - Designed to process data with sequential nature such as texts or stock prices. They are relatively old, but as the computing power of modern computers dramatically increased during the last 20 years, they became feasible to train and use in a reasonable time.\nFully Connected Neural Networks - The simplest model used on static/tabular data.\n8. Overfitting\nIt’s a negative effect when the model builds an assumption - bias - from an insufficient amount of data. A fairly common, and very important problem.\nLet’s say that you’ve visited a bakery a couple of times, and not once was there your favourite cupcake left! You’d likely get disappointed with the bakery - even though a thousand of other customers might find the stock satisfying. If you were a machine learning model, it’d be fair to say you’ve overfit against a small number of examples - developed a biased model, a representation in your head, that isn’t accurate compared to the facts.\nWhen overfitting happens, it usually means that the model is treating random noise in the data as a significant signal and adjusts to it, which is why it deteriorates on a new data (as the noise there is different.) This is generally the case in very complex models like Neural Networks or Gradient Boosting.\nImagine building a model to detect articles mentioning a particular sport discipline practiced during olympics. Since your training set is biased toward articles about the olympics, the model may learn features like presence of a word “olympics” and fail to detect correct articles that do not contain that word.\nThat's all folks! If you want more articles like this one, please subscribe below or reach out to us. Thanks!"}
{"content2":"文章目录\n机器学习（machine learning）\n监督学习（supervised learning）\n非监督学习（unsupervised learning）\n强化学习（reinforcement learning）\n传统机器学习\n深度学习 （deep learning）\n迁移学习 （transfer learning）\n机器学习（machine learning）\n机器学习的主要任务：\n分类（classification）：将实例数据划分到合适的类别中。\n回归（regression）：主要用于预测数值型数据。\n机器学习可以分为三种形式：\n监督学习（supervised learning）\n非监督学习（unsupervised learning）\n强化学习（reinforcement learning）\n监督学习（supervised learning）\n必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。在监督学习中，给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。 (包括：分类和回归)\n训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)\n非监督学习（unsupervised learning）\n无监督学习，即在未加标签的数据中，试图找到隐藏的结构。数据没有类别信息，也没有给定的目标值。\n非监督学习包括的类型：\n聚类：将数据集分成由类似的对象组成多个类。\n密度估计：通过样本分布的紧密程度，来估计与分组的相似性。\n此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。\n强化学习（reinforcement learning）\n强化学习，又称再励学习或者评价学习，也是机器学习的技术之一。\n所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号（强化信号）函数值最大，由于外部给出的信息很少，强化学习系统必须依靠自身的经历进行自我学习。通过这种学习获取知识，改进行动方案以适应环境。\n强化学习最关键的三个因素：\n状态\n行为\n环境奖励。\n强化学习和深度学习的主要区别：\n深度学习的训练样本是有标签的，强化学习的训练是没有标签的，它是通过环境给出的奖惩来学习\n深度学习的学习过程是静态的，强化学习的学习过程是动态的。这里静态与动态的区别在于是否会与环境进行交互，深度学习是给什么样本就学什么，而强化学习是要和环境进行交互，再通过环境给出的奖惩来学习\n深度学习解决的更多是感知问题，强化学习解决的主要是决策问题。因此有监督学习更像是五官，而强化学习更像大脑。\n应用：\n关于深度学习和强化学习的实例，最典型的莫过于谷歌的AlphaGo和AlphaZero两位了。AlphaGo是通过深度卷积神经网络，在训练了大约三千万组人类的下棋数据生成的模型，而AlphaZero使用强化学习的方式，通过自己和自己下棋的方式生成模型。而最终的实验结果也很让人震撼。AlphaGo击败了人类围棋顶尖高手，而AlphaZero击败了AlphaGo。\n强化学习实际应用目前还较窄，主要包括AI游戏（如Atari），推荐系统，机器人控制相关（如Ng的无人机飞行）\n传统机器学习\n机器学习（ML）技术在预测中发挥了重要的作用，ML经历了多代的发展，形成了丰富的模型结构，例如：\n线性回归\n逻辑回归\n决策树\n支持向量机\n贝叶斯模型\n正则化模型\n集成模型\n神经网络\n这些预测模型中的每一个都基于特定的算法结构，参数都是可调的。训练预测模型涉及以下步骤：\n选择一个模型结构（例如逻辑回归，随机森林等）。\n用训练数据（特征和标签）输入模型。\n学习算法将输出最优模型（即具有使训练错误最小化的特定参数的模型）。\n每种模式都有自己的特点，在一些任务中表现不错，但在其他方面表现不佳。但总的来说，我们可以把它们分成低功耗（简单）模型和高功耗（复杂）模型。选择不同的模型是一个非常棘手的问题。\n机器学习的主要障碍是特征工程这个步骤，特征工程要靠手动设计完成，需要大量领域专业知识，因此它成为当今大多数机器学习任务的主要瓶颈。\n深度学习 （deep learning）\n深度学习，也是一种机器学习的技术。最初的深度学习网络是利用神经网络来解决特征层分布的一种学习过程。\nDNN（深度神经网络）可以将原始信号（例如RGB像素值）直接作为输入值，而不需要创建任何特定的输入特征。通过多层神经元（这就是为什么它被称为“深度”神经网络），DNN可以“自动”在每一层产生适当的特征，最后提供一个非常好的预测，极大地消除了寻找“特征工程”的麻烦。\nDNN也演变成许多不同的网络拓扑结构，例如：\nCNN（卷积神经网络）\nRNN（递归神经网络）\nLSTM（长期短期记忆网络）\nGAN（生成对抗网络）\n所有的这些被统称为深度学习（Deep Learning）。\n迁移学习 （transfer learning）\n迁移学习能够将适用于大数据的模型迁移到小数据上，作为小数据模型的训练起点，节约训练神经网络需要的大量计算和时间资源。\n例如采用在计算机视觉挑战赛通过ImageNet数据（源数据）集训练出来的AlexNet 模型迁移应用到另一个新的数据集（目标数据集）重新进行训练（微调）。\n主要步骤：\n在源数据上训练一个神经网络。比如在 ImageNet 上训练出的 AlexNet 模型。\n将模型的输出层改成适合目标数据的大小。 新加载的数据并不需要作1000个类别的分类任务，因此 AlextNet 的最后三层可以针对性按照新的分类问题重新调整，例如：\n降低之前训练网络的特征初始学习率，减缓迁移层的学习。\n降低迁移学习设置过多的迭代训练次数，提高训练效率。\n减小批（Mini Batch Size）的大小，降低内存使用率。\n将输出层的权重初始化成随机值，但其他层的权重保持跟原先训练好的权重一致。\n在目标数据集上开始训练。"}
{"content2":"最大似然分类法介绍\n作者：liangdas\n出处：简单点儿，通俗点儿，机器学习  http://blog.csdn.net/liangdas/article/details/19294951\n前言：\n最小距离分类法只考虑了待分类样本到各个类别中心的距离，而没有考虑已知样本的分布，所以它的分类速度快，但精度不高。今天要介绍的是另外一种分类方法——最大似然分类法，它也是分类里面用得很多的一种分类方法，它在分类的时候，不仅考虑了待分类样本到已知类别中心的距离，而且还考虑了已知类别的分布特征，所以其分类精度比最小距离分类法要高。\n下面有一个图，黄色所示类别A的分布，蓝色是类别B的分布，类别A和B的类别中心都红色的点表示出来了，那么有一个未知类别的样本p，假设p到类别A的距离是Ap，到类别B的距离是Bp，现在Ap=Bp，那么如果用最小距离分类法，我们是不能判断样本p到底是属于A类还是B类的，所以这就是最小距离分类法的局限性。\n感性认识：\n假设在我们读书的时候，在一个月内，30天，你发现晚上11点以后实验室还亮着灯，你很好奇，每次推门进去，发现这30天内，实验室中不是甲就是乙，或者他们同时都在，你做了统计，是甲的天数是28天，是乙的次数是4天，然后有一天晚上11点，你发现实验室的灯又亮了，你同学问你：“你猜现在是谁在实验室呀？”，你会回答“应该是甲吧”。这其实就是生活中我们利用最大似然来分类的一个例子。\n另外一个例子：现在我们有两个盒子：甲和乙，每个里面都装了100个球，其中甲中装了95个红球，5个黑球，乙中装了60个红球，40个黑球，现在有人从盒子里面取出了一个球，发现是红球，然后让你猜：“他是从哪个盒子里面取出来的？”，想必你会猜“甲”吧。\n好了，其实我们生活中，已经对最大似然估计有了自己的经验体会，是不是还没有意识到它就是最大似然分类法呢？呵呵。\n基本原理：\n假设有两个事件A，B，我们通过先验的知识，知道A发生的条件下，x也发生的概率是P(x|A)； B发生的条件下，x也发生的概率是P(X|B)，那么，现在有一个事件x发生了，我们能否判断这个事件x是在A条件下，还是在B条件下发生的可能性大些呢？也就是要求出P（A|x）和P（B|x）哪一个最大？对分类问题而言，哪一个概率大，我们就说x属于哪一类。\n我们回忆下贝叶斯公式：P(A|B)=P(B|A)*P(A)/P(B)\n从贝叶斯公式中我们可以看到，求概率P(A|B)的问题转化成了求P(B|A)，P(A)和P(B)的问题，其实里面真正的精华是：概率P(A|B)和P(B|A)的转换。因为通常，我们事先能知道P(A)，P(B)；或者是P(A)和P(B)在分类问题中是公共的项，可以约去；再或是他们的差异可以忽略不计，所以，要P(A|B)最大，也就是要P(B|A)最大！而对于P(B|A)，我们可以从事先已经发生的事件中，通过统计等数学方法计算得到，这样这个分类问题就好解了！我们可以提炼下面的目标函数：\n其中是目标函数，是已知x事件发生了，那么它属于的概率；是类别对应的x的概率值；是类别发生的概率，是事件x发生的概率。\n贝叶斯公式的经典之处就是将后验概率的问题转化成了先验概率的问题！\n再回到上面的盒子和白球的问题中，A和B分别代表盒子甲和乙，事件x就是红球发生的概率，我们先验已经知道P(x|A）= 95/100=0.95，P(x|B)=60/100=0.6，我们要求的是P(A|x)和P(B|x)，然后比较哪个最大？\nP(A|x)= P(A)*P(x|A)/P(x)\nP(B|x)= P(B)*P(x|B)/P(x)\n对于P(A), P(B)，因为我们目前总共只有两个盒子甲和乙，也就是A和B，它们各有一个，所以P(A)=P(B)=1/2=0.5。\n对于P(x)，因为P(A|x)和P(B|x）的贝叶斯表达式中都有P(x)这一项，所以，P（x）作为公共项，我们可以不考虑。如果硬是想知道它的值，那么因为是随机取一个球，P(x）=(95+60)/(100+100)。\n这样我们就可以轻松的知道P(A|x)大，还是P(B|x)大了。\n最大似然分类又叫贝叶斯分类。\nN维空间的最大似然分类法原理：\n上面的例子中， x都是二项分布的，也就是x的取值只取两个值，通常为0，1，发生或者不发生等，但是在实际问题中，x往往不是服从这么一个简单的二项分布，而很有可能是其他更复杂一些的分布。\n在统计学和模式识别中，对某一个待研究的集合，在经验的基础上，我们通常假设它是一个服从正太分布的变量集合，如一群人的身高、某各班级某一科目的考试成绩、某个类别的某一维特征，我们都假定它是服从正态分布的，也叫高斯分布，即用均值，和方差来描述的一组样本分布。对一维正态分布，它的概率密度函数表达式是：，表示的是变量在分布中的出现的概率。\n通常，为了描述某一个对象，我们会从这个对象抽象出多个属性，每一个属性代表一个一维空间，多个属性就组成一个多维空间。对于多维空间的中的变量，它也有其正态分布的概率密度函数：\n其中，是概率密度，是特征维数，又叫特征空间维数（对遥感图像分类来说， 就是波段的个数）；是n维空间中的一个向量；也是一个n向量，它是由每一维特征的均值组成的一个向量；是n维特征向量之间的协方差矩阵。\n上面已经指出了，最大似然分类的目标函数是：\n当x是一个多维空间中的一个向量，而且每一维空间都的分布都服从正态分布的时候，则x的概率密度函数是：\n通常可以根据已知条件计算得到，或者其区别可以忽略不计。\n是公共项，所以不用考虑其具体的值。\nN维空间的最大似然分类法的化简：\n到目前为止，我们利用上面的就可以计算出最大似然分类的结果了，但是在上面的计算公式中有一个指数的计算，我们知道指数计算在计算机中是特别耗费时间，当特征维数较多，待分类的类别个数也比较多的的时候，指数计算势必会导致最后的分类的速度变得很慢，所以能不能把指数计算去掉呢？答案当然是可以的，我们只需要对表达式取对数。\n因为是公共项，所以，我们去掉，目标函数就变为：\n对目标函数取对数：\n在分类之前，我们为了消除量纲的影响，都需要对所有维的特征做归一化处理，使其服从标准正态分布，另外，如果我们在取特征的时候，每一维特征都取得非常好，它们之间相互独立（其实可以对特征做变换，使各维特征能够相互独立，如PCA，LDA等），如果上述条件都满足，每个特征都会具有相同的方差，不同维之间的协方差为0（因为不同维特征不相关），这种情况下，协方差矩阵是对角阵，且是一个单位阵 E，因此，每一类的协方差都相等，因此，公式化简为：\n如果各类的先验概率相等，即 的大小与类别无关，那么上面的式子中的最后一项又可以去掉，最终公式变成了：\n到此为止，这个目标函数就已经没有了指数和对数计算了！就剩下一些矩阵的运算了，所以运算速度也得到了很大的提高！\n在实际的使用当中，我们需要根据不同的问题特点，来确定最终能化简的程度。\n参考资料：\n百度文库——最大似然分类器的基本思想和数学原理：\nhttp://wenku.baidu.com/view/12a5de46a8956bec0975e364.html\n百度文库——最大似然分类算法原理及实现：\nhttp://wenku.baidu.com/view/03a40da0f524ccbff12184c7.html\n百度文库——矩阵模板类：\nhttp://wenku.baidu.com/link?url=GawKlZv9teHxqjBWuBRVWs52HRusBAa_SnkNN2AGi8hR9ScGGRBWCA7beNhuCGRAx4OcbJJZN3NXKdit_ejkS4yjYYSNnSdIq0lK2i43hym\n矩阵加减求逆运算的模板类:\nhttp://download.csdn.net/download/weijieli/2766053\nps：使用或者转载请标明出处，禁止以商业为目的的使用。\n如果有需要word版，或者是pdf版的，请与我联系，QQ：358536026"}
{"content2":"机器学习-概率分布-伯努利分布\n概率论在机器学习领域发挥了重要的作用。目前机器学习的很多方法本质上是统计学习，而统计学习的本质则是概率论。在概率论中概率分布是一个非常重要的工具。概率分布\np(x)\np(\\mathbf{x}) 描述的是随机变量\nx\n\\mathbf{x} 的概率密度分布。\n首先介绍最简单的一种分布-伯努利分布。变量\nx∈{0,1}\nx\\in\\{0, 1\\} 即变量\nx\nx 要么为\n1\n1 要么为\n0\n0。真实世界中的一个例子是抛硬币，设硬币正面朝上表示\n1\n1, 反面朝上表示为\n0\n0。假设硬币经过了特定设计，那么正面朝上的概率和反面朝上的概率是不一样的，设正面朝上的概率为\nμ,0<μ<1\n\\mu, 0<\\mu<1。那么我们抛一次硬币，正面朝上的概率可表示为：\np(x=1|μ)=u\np(x=1|\\mu)=u\n同理，反面朝上的概率为：\np(x=0|μ)=1−u\np(x=0|\\mu)=1-u\n因此，变量\nx\nx的概率可写为：\np(x)=ux(1−u)1−x\np(x)=u^x(1-u)^{1-x}\n假设我们抛硬币抛了\nN\nN次，得到了\nN\nN次结果\nD={x1,x2,⋯,xN}\n\\mathcal{D}=\\{x_1,x_2,\\cdots,x_N\\}，那么这些观测变量的似然为：\np(D|μ)=∏n=1Np(xn|μ)=∏n=1Nuxn(1−μ)1−xn\np(\\mathcal{D}|\\mu)=\\prod_{n=1}^Np(x_n|\\mu)=\\prod_{n=1}^Nu^{x_n}(1-\\mu)^{1-x_n}\n我们可以通过最大似然来估计概率分布的参数\nμ\n\\mu。最大化似然等价于最大化对数似然。对上述求对数\nlnp(D|μ)=∑n=1N{xnlnμ+(1−xn)ln(1−μ)}\n\\ln p(\\mathcal{D}|\\mu)=\\sum_{n=1}^N\\left\\{x_n\\ln\\mu+\\left(1-x_n\\right)\\ln\\left(1-\\mu\\right)\\right\\}\n最大化上面的对数似然就可以得到参数\nμ\n\\mu的最大似然估计。具体过程为：\nlnp(D|μ)\n\\ln p(\\mathcal{D}|\\mu)对\nmu\nmu求导并令其为\n0\n0，可得：\n0μ==∑n=1Nxn1μ−∑n=1N(1−xn)11−μ1N∑n=1Nxn\n\\begin{eqnarray*} 0&=&\\sum_{n=1}^Nx_n\\frac{1}{\\mu}-\\sum_{n=1}^N(1-x_n)\\frac{1}{1-\\mu}\\\\ \\mu&=&\\frac{1}{N}\\sum_{n=1}^Nx_n \\end{eqnarray*}\n我们用python来实现上述的参数估计过程，用scipy包中的bernoulli分布来生成样本，再根据这些样本估计bernoulli分布的参数。下面的代码表示bernoulli分布的参数为0.32，我们用这个分布生成了10000个样本。再用这10000个样本估计该分布的参数。看看估计出来的参数是多少。减少生成样本的个数(size)，重新估计参数，看有什么变化。\n代码\n估计伯努利分布的参数：\nfrom scipy.stats import bernoulli,poisson,norm,expon import numpy X=bernoulli.rvs(0.32,size=10000) #根据伯努利分布来生成样本# mu=numpy.mean(X)#用样本来估计参数# print(mu)"}
{"content2":"http://antkillerfarm.github.io/\nQR分解（续）\n令\nA=[a1,⋯,an]\nA=[\\mathbf{a}_1, \\cdots, \\mathbf{a}_n]，其中\nai\na_i为列向量。则：\nu1u2u3uk=a1,=a2−proju1a2,=a3−proju1a3−proju2a3,⋮=ak−∑j=1k−1projujak,e1e2e3ek=u1∥u1∥=u2∥u2∥=u3∥u3∥⋮=uk∥uk∥\n\\begin{align} \\mathbf{u}_1 &= \\mathbf{a}_1, & \\mathbf{e}_1 &= {\\mathbf{u}_1 \\over \\|\\mathbf{u}_1\\|} \\\\ \\mathbf{u}_2 &= \\mathbf{a}_2-\\mathrm{proj}_{\\mathbf{u}_1}\\,\\mathbf{a}_2, & \\mathbf{e}_2 &= {\\mathbf{u}_2 \\over \\|\\mathbf{u}_2\\|} \\\\ \\mathbf{u}_3 &= \\mathbf{a}_3-\\mathrm{proj}_{\\mathbf{u}_1}\\,\\mathbf{a}_3-\\mathrm{proj}_{\\mathbf{u}_2}\\,\\mathbf{a}_3, & \\mathbf{e}_3 &= {\\mathbf{u}_3 \\over \\|\\mathbf{u}_3\\|} \\\\ & \\vdots &&\\vdots \\\\ \\mathbf{u}_k &= \\mathbf{a}_k-\\sum_{j=1}^{k-1}\\mathrm{proj}_{\\mathbf{u}_j}\\,\\mathbf{a}_k, &\\mathbf{e}_k &= {\\mathbf{u}_k\\over\\|\\mathbf{u}_k\\|} \\end{align}\n即：\na1a2a3ak=⟨e1,a1⟩e1=⟨e1,a2⟩e1+⟨e2,a2⟩e2=⟨e1,a3⟩e1+⟨e2,a3⟩e2+⟨e3,a3⟩e3⋮=∑j=1k⟨ej,ak⟩ej\n\\begin{align} \\mathbf{a}_1 &= \\langle\\mathbf{e}_1,\\mathbf{a}_1 \\rangle \\mathbf{e}_1 \\\\ \\mathbf{a}_2 &= \\langle\\mathbf{e}_1,\\mathbf{a}_2 \\rangle \\mathbf{e}_1 + \\langle\\mathbf{e}_2,\\mathbf{a}_2 \\rangle \\mathbf{e}_2 \\\\ \\mathbf{a}_3 &= \\langle\\mathbf{e}_1,\\mathbf{a}_3 \\rangle \\mathbf{e}_1 + \\langle\\mathbf{e}_2,\\mathbf{a}_3 \\rangle \\mathbf{e}_2 + \\langle\\mathbf{e}_3,\\mathbf{a}_3 \\rangle \\mathbf{e}_3 \\\\ &\\vdots \\\\ \\mathbf{a}_k &= \\sum_{j=1}^{k} \\langle \\mathbf{e}_j, \\mathbf{a}_k \\rangle \\mathbf{e}_j \\end{align}\n这个过程又被称为Gram–Schmidt正交化过程。\n因此：\nQ=[e1,⋯,en]andR=⎡⎣⎢⎢⎢⎢⎢⟨e1,a1⟩00⋮⟨e1,a2⟩⟨e2,a2⟩0⋮⟨e1,a3⟩⟨e2,a3⟩⟨e3,a3⟩⋮………⋱⎤⎦⎥⎥⎥⎥⎥\nQ = \\left[ \\mathbf{e}_1, \\cdots, \\mathbf{e}_n\\right] \\qquad \\text{and} \\qquad R = \\begin{bmatrix} \\langle\\mathbf{e}_1,\\mathbf{a}_1\\rangle & \\langle\\mathbf{e}_1,\\mathbf{a}_2\\rangle & \\langle\\mathbf{e}_1,\\mathbf{a}_3\\rangle & \\ldots \\\\ 0 & \\langle\\mathbf{e}_2,\\mathbf{a}_2\\rangle & \\langle\\mathbf{e}_2,\\mathbf{a}_3\\rangle & \\ldots \\\\ 0 & 0 & \\langle\\mathbf{e}_3,\\mathbf{a}_3\\rangle & \\ldots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\end{bmatrix}\n矩阵的特征值和特征向量\n设A是一个n阶方阵，\nλ\n\\lambda是一个数，如果方程\nAx=λx\nAx=\\lambda x存在非零解向量，则称\nλ\n\\lambda为A的一个特征值（Eigenvalue），相应的非零解向量x称为属于特征值\nλ\n\\lambda的特征向量（eigenvector）。\n上面这个描述也可以记作：\n(A−λI)x=0(1)\n(A-\\lambda I)x=0\\tag{1}\n这个公式本身通常用于：已知特征值，求解对应的特征向量。\n其中，\nA−λI\nA-\\lambda I被称为特征矩阵，而\n∣A−λI∣=0\n\\lvert A-\\lambda I \\rvert=0被称为特征方程。求解特征方程可得到特征值。\n特征值和特征向量在有的书上也被称为本征值和本征向量。\n特征值和特征向量的特性包括：\n1.特征向量属于特定的特征值，离开特征值讨论特征向量是没有意义的。不同特征值对应的特征向量不会相等，但特征向量不能由特征值唯一确定。\n2.在复数范围内，n阶矩阵A有n个特征值。在这些特征值中，模最大的那个特征值即主特征值（对于实数阵即绝对值最大的特征值），主特征值对应的特征向量称为主特征向量。\n更多内容参见：\nhttp://course.tjau.edu.cn/xianxingdaishu/jiao/5.htm\nQR算法\n对矩阵A进行QR分解可得：\nA=QR\nA=QR\n因为Q是正交阵（\nQT=Q−1\nQ^T=Q^{-1}），所以正交相似变换\nQTAQ\nQ^TAQ和A有相同的特征值。\n证明：\n|QTAQ−λI|=|QTAQ−QT(λI)Q|=|QT(A−λI)Q|=|QT|⋅|A−λI|⋅|Q|=|QTQ|⋅|A−λI|=|I|⋅|A−λI|=|A−λI|\n|Q^TAQ-\\lambda I|=|Q^TAQ-Q^T(\\lambda I)Q|=|Q^T(A-\\lambda I)Q|\\\\=|Q^T|\\cdot|A-\\lambda I|\\cdot|Q|=|Q^TQ|\\cdot|A-\\lambda I|=|I|\\cdot|A-\\lambda I|=|A-\\lambda I|\n这里的证明，用到了行列式的如下性质：\n|I|=1\n|I|=1\n|AB|=|A|⋅|B|\n|AB|=|A|\\cdot|B|\n因为\nQTAQ\nQ^TAQ和A的特征方程相同，所以它们的特征值也相同。证毕。\n由此产生如下迭代算法：\nRepeat until convergence {\n1.\nAk=QkRk\nA_k=Q_kR_k（QR分解）\n2.\nAk+1=QTkAkQk=Q−1kQkRkQk=RkQk\nA_{k+1}=Q_k^TA_kQ_k=Q_k^{-1}Q_kR_kQ_k=R_kQ_k\n}\n这个算法的收敛性证明比较复杂，这里只给出结论：\nlimk→∞Ak=⎡⎣⎢⎢⎢⎢⎢λ10…0u12λ2…0……⋱…u1nu2n…λn⎤⎦⎥⎥⎥⎥⎥\n\\lim_{k\\to\\infty}A_k=\\begin{bmatrix} \\lambda_1 & u_{12} & \\dots & u_{1n} \\\\ 0 & \\lambda_2 & \\dots & u_{2n} \\\\ \\dots & \\dots & \\ddots & \\dots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{bmatrix}\n其中，\nλi\n\\lambda_i为矩阵的特征值。\nuij\nu_{ij}表示任意值，它们的极限可能并不存在。\nQR算法于1961年，由John G.F. Francis和Vera Nikolaevna Kublanovskaya发现。\n注：John G.F. Francis，1934年生，英国计算机科学家，剑桥大学肄业生。\n2000年，QR算法被IEEE计算机学会评为20世纪的top 10算法之一。然而直到那时，计算机界的数学家们竟然都没有见过Francis本尊，连这位大神是活着还是死了都不知道，仿佛他在发表完这篇惊世之作后就消失了一般。\n2007年，学界的两位大牛：Gene Howard Golub（SVD算法发明人之一，后文会提到。）和Frank Detlev Uhlig（1972年获加州理工学院博士，Auburn University数学系教授），经过不懈努力和人肉搜索终于联系上了他。\n他一点都不知道自己N年前的研究被引用膜拜了无数次，得知自己的QR算法是二十世纪最NB的十大算法还有点小吃惊。这位神秘大牛竟然连TeX和Matlab都不知道。现在这位大牛73岁了，活到老学到老，还在远程教育大学Open University里补修当年没有修到的学位。\n2015年，University of Sussex授予他荣誉博士学位。\n相关内容参见：\nhttp://www.netlib.org/na-digest-html/07/v07n34.html\nVera Nikolaevna Kublanovskaya，1920~2012，苏联数学家，女。终身供职于苏联科学院列宁格勒斯塔克罗夫数学研究所。52岁才拿到博士学位。\n需要指出的是，QR算法可求出矩阵的所有特征值，如果只求某一个特征值的话，还有其他一些更快的算法。详见：\nhttps://en.wikipedia.org/wiki/Eigenvalue_algorithm\n矩阵的奇异值\n在进一步讨论之前，我们首先介绍一下矩阵特征值的几何意义。\n首先，矩阵是对线性变换的表示，确定了定义域空间V与目标空间W的两组基，就可以很自然地得到该线性变换的矩阵表示。\n线性空间变换的几何含义如下图所示：\n图中的坐标轴，就是线性空间的基。\n线性变换主要有三种几何效果：旋转、缩放、投影。\n其中，旋转和缩放不改变向量的维数。矩阵特征值运算，实际上就是将向量V旋转缩放到一个正交基W上。因为V和W等维，所以要求矩阵必须是方阵。\n正交化过程，代表旋转变换，又被称为等距同构。（旋转变换，可以理解为向量的正向旋转，也可以理解为坐标轴的反向旋转，这里理解为后者，会容易一些。）特征值代表缩放变换的缩放因子。\n而对于一般矩阵而言，我们还需要进行投影变换，将n维向量V映射为m维向量W。那么投影变换选择什么矩阵呢？\n我们知道，对于复数z，可写成：\nz=(z|z|)|z|=(z|z|)z¯z−−√\nz=\\left(\\frac{z}{|z|}\\right)|z|=\\left(\\frac{z}{|z|}\\right)\\sqrt{\\overline z z}\n其中\nz¯\n\\overline z是z的共轭复数。也就是说，一个复数可以表示为一个单位向量乘以一个模。\n类似的，我们定义共轭矩阵\nM∗ij=Mji¯¯¯¯¯\nM^*_{ij}=\\overline{M_{ji}}，这实际上就是矩阵M转置之后，再将每个元素值设为它的共轭复数。因此：\nM∗=(M¯¯¯¯)T=MT¯¯¯¯¯¯\nM^*=(\\overline M)^T=\\overline{M^T}\n仿照着复数的写法，矩阵M可以表示为：\nM=SM∗M−−−−−√\nM=S\\sqrt{M^*M}\n这里的S表示等距同构。（单位向量相当于给模一个旋转变换，也就是等距同构。）由于\nM∗M−−−−−√\n\\sqrt{M^*M}是正定对称方阵，因此它实际上也是能够被正交化的。所以对于一般矩阵来说，我们总能够找到两个正交基，并在这两个基之间进行投影变换。\n注意：我们刚才是用与复数类比的方式，得到投影变换矩阵\nM∗M−−−−−√\n\\sqrt{M^*M}。但是类比不能代替严格的数学证明。幸运的是，上述结论已经被严格证明了。\n我们将矩阵\nM∗M−−−−−√\n\\sqrt{M^*M}的特征值，称作奇异值（Singular value）。可以看出，如果M是对称方阵的话，则M的奇异值等于M的特征值的绝对值。\n参见：\nhttps://www.zhihu.com/question/22237507/answer/53804902\nhttp://www.ams.org/samplings/feature-column/fcarc-svd\n奇异值分解\n奇异值分解（Singular value decomposition，SVD）定理：\n设\nM∈Rm×n\nM\\in R^{m\\times n}，则必存在正交矩阵\nU=[u1,…,um]∈Rm×m\nU=[u_1,\\dots,u_m]\\in R^{m\\times m}和\nV=[v1,…,vn]∈Rn×n\nV=[v_1,\\dots,v_n]\\in R^{n\\times n}使得：\nUTMV=[Σr000]\nU^TMV=\\begin{bmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}\n其中，\nΣr=diag(σ1,…,σr),σ1≥⋯≥σr>0\n\\Sigma_r=diag(\\sigma_1,\\dots,\\sigma_r),\\sigma_1\\ge \\dots\\ge \\sigma_r>0。\n当M为复矩阵时，将U、V改为酉矩阵（unitary matrix）即可。（吐槽一下，酉矩阵这个翻译真的好烂，和天干地支半毛钱关系都没有。）\n奇异值分解也可写为另一种形式：\nM=UΣV∗\nM=U\\Sigma V^*\n其几何意义如下图所示：\n虽然，我们可以通过计算矩阵\nM∗M−−−−−√\n\\sqrt{M^*M}的特征值的方法，计算奇异值，然而这个方法的计算量十分巨大。1965年，Gene Howard Golub和William Morton Kahan发明了目前较为通用的算法。但该方法比较复杂，这里不作介绍。\n参见：\nhttp://www.doc88.com/p-089411326888.html\nGene Howard Golub，1932～2007，美国数学家，斯坦福大学教授。\nWilliam Morton Kahan，1933年生，加拿大数学家，多伦多大学博士，UCB教授。图灵奖获得者（1989）。IEEE-754标准（即浮点数标准）的主要制订者，被称为“浮点数之父”。ACM院士。\n矩阵的秩\n一个矩阵A的列（行）秩是A的线性独立的列（行）的极大数。\n下面不加证明的给出矩阵的秩的性质：\n1.矩阵的行秩等于列秩，因此可统称为矩阵的秩。\n2.秩是n的\nm×n\nm\\times n矩阵为列满秩阵；秩是n的\nn×p\nn\\times p矩阵为行满秩阵。\n3.设\nA∈Mm×n(F)\nA\\in M_{m\\times n}(F)，若A是行满秩阵，则\nm≤n\nm\\le n；若A是列满秩阵 ，则\nn≤m\nn\\le m。\n4.设A为\nm×n\nm\\times n列满秩阵，则n元齐次线性方程组\nAX=0\nAX=0只有零解。\n5.线性方程组\nAX=B\nAX=B对任一m维列向量B都有解\n⇔\n\\Leftrightarrow系数矩阵A为行满秩阵。\n参见：\nhttp://wenku.baidu.com/view/9ce143eb81c758f5f61f6730.html\n奇异矩阵\n对应的行列式等于0的方阵，被称为奇异矩阵（singular matrix）。\n奇异矩阵和线性相关、秩等概念密切相关。\n下面不加证明的给出奇异矩阵的性质：\n1.如果A为非奇异矩阵\n⇔\n\\LeftrightarrowA满秩。\n2.如果A为奇异矩阵，则AX=0有无穷解，AX=b有无穷解或者无解。如果A为非奇异矩阵，则AX=0有且只有唯一零解，AX=b有唯一解。\n对于A不是方阵的情况，一般使用\nATA\nA^TA来评估矩阵是否是奇异矩阵。\n向量的范数\n范数（norm，也叫模）的定义比较抽象，这里我们使用闵可夫斯基距离，进行一个示意性的介绍。\nMinkowski distance的定义：\nd(x,y)=∑i=1n∣xi−yi∣λ−−−−−−−−−−√λ\nd(x,y)=\\sqrt[\\lambda]{\\sum_{i=1}^{n}\\lvert x_i-y_i\\lvert^{\\lambda}}\n显然，当\nλ=2\n\\lambda=2时，该距离为欧氏距离。当\nλ=1\n\\lambda=1时，也被称为CityBlock Distance或Manhattan Distance（曼哈顿距离）。\n这里的\nλ\n\\lambda就是范数。"}
{"content2":"机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术。\n人工智能\n在1956年被提出，2012年之前一直饱受争议，直到机器学习新算法(深度学习)的出现，人工智能迎来了大爆发。\n人工智能研究的分支有很多，包含：专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等。\n人工智能分为弱人工智能和强人工智能：弱：让机器具备观察和感知的能力，可以做到一定程度的理解和推理。强(很难实现)：让机器获得自适应能力，解决以前没有遇到过的问题。\n机器学习\n实现人工智能的一种方法。传统的机器学习算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。\n学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n机器学习的应用：指纹识别、人脸识别和物体检测。\n普遍对机器学习的理解误区：机器学习并不一定需要大量的数据训练模型，例如K-means聚类算法也是一种机器学习算法。\n深度学习\n实现机器学习的技术，它不是一种独立的方法，它本身也会用到有监督学习和无监督学习的方法来训练深度神经网络。它的研究主要在于神经元的连接方法和激活函数等方面做出调整。（实际上就是一种包含多个隐含层的神经网络结构，需要大量的数据去训练）"}
{"content2":"一般来说，Precision（查准率） 就是检测出来的正样本中有多少是准确的，Recall（查全率）就是所有准确的条目有多少被检索出来了。\n查准率和查全率是信息检索效率评价的两个定量指标，不仅可以用来评价每次检索的准确性和全面性，也是在信息检索系统评价中衡量系统检索性能的重要方面。\n查准率（Precision ratio，简称为P），是指检出的相关文献数占检出文献总数的百分比。查准率反映检索准确性，其补数就是误检率。\n查全率（Recall ratio，简称为R），是指检出的相关文献数占系统中相关文献总数的百分比。查全率反映检索全面性，其补数就是漏检率。\n查全率＝（检索出的相关信息量/系统中的相关信息总量）*100%\n查准率＝（检索出的相关信息量/检索出的信息总量）*100%\n前者是衡量检索系统和检索者检出相关信息的能力，后者是衡量检索系统和检索者拒绝非相关信息的能力。两者合起来，即表示检索效率。\n利用查准率和查全率指标，可以对每一次检索进行检索效率的评价，为检索的改进调整提供依据。利用这两个量化指标，也可以对信息检索系统的性能水平进行评价。要评价信息检索系统的性能水平，就必须在一个检索系统中进行多次检索。每进行一次检索，都计算其查准率和查全率，并以此作为坐标值，在平面坐标图上标示出来。通过大量的检索，就可以得到检索系统的性能曲线。实验证明，在查全率和查准率之间存在着相反的相互依赖关系–如果提高输出的查全率，就会降低其查准率，反之亦然。"}
{"content2":"现在让我们来继续认识一下到底什么是机器学习呢？在机器学习中，最常见的问题就是分类（classification）问题，所谓的分类问题，就比如我们用机器学习算法，将病人的检查结果分为有病和健康，是一个医学方面的二分类问题（将要区分的数据分为两个类别）。再例如在电子邮箱中，收到邮件之后，电子邮箱会将我们的邮件分为广告邮件，垃圾邮件和正常邮件，这就是一个多分类的问题（将要区分的数据分为多个类别）。\n为什么我们着重的介绍分类的问题呢？因为分类问题是机器学习中的基础，其他的很多应用都可以从分类的问题演变而来，同时很多问题都可以转化成分类的问题，比如图像中的图像分割，最简单的实现方法就是对每一个像素进行分类，在自然场景的分割中，我们就判断这个像素点是不是房子的一部分，如果是的话，那么他的标签就是房子。\n在机器学习中，能够完成分类任务的算法，我们通常把它叫做一个分类器（classifier）。要想评价一个分类器的好坏，我们就要有评价指标，最常见的就是准确率（accuracy），准确率是指被分类器分类正确的数据的数量占所有数据数量的百分比。但是我们可以在不提及数据集的情况下就说一个分类器要比另一个分类器的效果更好么？答案是否定的，具体原因还记得吗？请回看我之前的文章，没有免费午餐理论。\n知道了分类器我们就要具体的研究数据了，通常我们会管我们处理的数据叫做数据集（data set）一个数据集通常来说包括三个部分，1,训练数据（training data）及其标签，2,验证数据（validation data）及其标签，3,测试数据（testing data） 。需要特别强调的是，这三部分都是各自独立的，也就是说训练数据中的数据不能再出现在验证数据以及测试数据中，验证数据最好也不要出现在测试数据中，这点在训练分类器的时候一定要特别注意。（这三个部分有时候是可以变成两个部分的，这个我在大家认识了这三部分之后再详细的介绍。）\n下面我们来讲分类器的训练，如上图所示,训练一个分类器通常需要三个步骤，第一个步骤就是使用训练数据和其标签训练模型，这就好比教育一个小孩子通过观察知道什么是苹果的过程，我们要让他反复的看到各种样式的苹果的照片以及其他不是苹果的物体的照片（训练数据），并且告诉他哪些照片是苹果，哪些不是（训练数据的标签），通过这样的过程让小孩子学习。\n第二个步骤就是将验证数据输入模型中，比较验证数据的标签和模型分类结果的区别，进而评价算法的学习效果，通常来说一个机器学习算法的准确率或者其他指标就是在验证数据上得到的。这就好像我们教了孩子一段时间之后，我们拿一个黑白电视机的照片（在之前的教育过程中，小孩子只见过彩色电视机没见过黑白的电视机，这是和训练数据不同的验证数据，“不是苹果”这个结论就是验证数据的标签），问小孩子这是不是苹果，看小孩子能否答对，进而评价小孩子的学习效果。\n第三个步骤是机器学习算法的实际应用过程，等到我们认为模型已经训练的足够好了，在验证数据上取得了很好的效果之后，我们就将这个模型真正的运用于实际中去，代替我们工作，有时候在一些科研项目中是不存在这个步骤的，因为如果要衡量一个机器学习算法的优越性，使用第二步中有标签的数据就可以做到，什么时候那个算法真正在现实生活中应用了，那么才会有很多的无标签数据让机器去代替人们完成任务。就像小孩子认识了什么是苹果，那么他就可以从事对照片分类的工作了，他可以基本正确的分辨出哪些照片是苹果而哪些不是，这样以后就不用大人手工的挑选照片了，这个艰巨而又无聊的选择苹果照片的任务就交给小孩子了。\n上面讲的就是最标准的拥有三类数据（训练，验证，测试）的情况，下面我们将介绍两部分数据的情况。有些时候，我们得到的数据只有训练数据（有标签）和测试数据（无标签），这时候我们就需要人为的将训练数据中的一部分拿出来作为验证数据，这部分验证数据不参与分类器的学习过程，通常训练数据和验证数据的比例是7比3，就是拿出70%的有标签数据作为训练数据，30%的有标签数据作为验证数据。\n还有时我们的数据都是有标签的数据，那么这时候我们通常来说也按照7比3的比例，将有标签的数据分为训练数据和验证数据，通过验证数据衡量模型在这套数据上的性能，从而省略第三个步骤。"}
{"content2":"机器学习的算法很多。很多时候困惑人们都是，很多算法是一类算法，而有些算法又是从其他算法中延伸出来的。这里，我们从两个方面来给大家介绍，第一个方面是学习的方式，第二个方面是算法的分类。\n一、4大主要学习方式\n1.监督式学习\n在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。\n在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。\n监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）。\n2.强化学习\n在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。\n常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。\n3. 非监督式学习\n在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。\n4.半监督式学习\n在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。\n应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。\n二、13种常用算法\n根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。\n1.回归算法\n回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。\n常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。\n2. 正则化方法\n正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。\n常见的算法包括：Ridge Regression， Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（Elastic Net）。\n3.决策树学习\n决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。\n常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）\n4.基于实例的算法\n基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。\n常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM）。\n5.贝叶斯方法\n贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。\n常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators， AODE），以及Bayesian Belief Network（BBN）。\n6.聚类算法\n聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。\n常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。\n7.降低维度算法\n像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。\n常见的算法包括：主成份分析（Principle Component Analysis， PCA），偏最小二乘回归（Partial Least Square Regression，PLS）， Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）,  投影追踪（Projection Pursuit）等。\n8.关联规则学习\n关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。\n常见算法包括 Apriori算法和Eclat算法等。\n9.遗传算法（genetic algorithm）\n遗传算法模拟生物繁殖的突变、交换和达尔文的自然选择（在每一生态环境中适者生存）。\n它把问题可能的解编码为一个向量，称为个体，向量的每一个元素称为基因，并利用目标函数（相应于自然选择标准）对群体（个体的集合）中的每一个个体进行评价，根据评价值（适应度）对个体进行选择、交换、变异等遗传操作，从而得到新的群体。\n遗传算法适用于非常复杂和困难的环境，比如，带有大量噪声和无关数据、事物不断更新、问题目标不能明显和精确地定义，以及通过很长的执行过程才能确定当前行为的价值等。\n10.人工神经网络\n人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。\n（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。\n11.深度学习\n深度学习算法是对人工神经网络的发展。 在近期赢得了很多关注， 特别是百度也开始发力深度学习后， 更是在国内引起了很多关注。   在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。\n常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。\n12.基于核的算法\n基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。\n常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA)等。\n13.集成算法\n集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。\n常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization， Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest），GBDT（Gradient Boosting Decision Tree）。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，有兴趣的朋友，可以查阅多智时代，在此为你推荐几篇优质好文：\n5分钟内看懂机器学习和深度学习的区别\n关于机器学习你必须了解的十个真相\n人工智能、机器学习和深度学习的区别与联系?\n机器学习基本概念\n2018年值得关注的10种机器学习工具"}
{"content2":"作者：谭东\n遵循：BY-SA（署名-相同方式共享4.0协议）\n机器学习是人工智能的一个重要的分支。这篇文章将会有助于你对机器学习的理解和认识，带你熟悉其基本原理和基本概念。\n先列举下本文将会给你带来的一些名词（这只是本篇博客含有的名词）：\n数据集（data set）、示例（instance）/样本（sample）、属性（attribute）/特征（feature）、属性值（attribute value）、属性空间（attribute space）/样本空间（sample space）/输入空间、特征向量（feature vector）、维数（dimensionality）、学习（learning）/训练（training）、训练数据（training data）、训练样本（training sample）、训练集（training set）、假设（hypothesis）、真相/真实（ground-truth）、学习器（learner）、标记（label）、样例（example）、标记空间（label space）/输出空间、分类（classification）、回归（regression）、二分类（binary classification）、正类（positive class）、反类（negative class）、多分类（multi-class classification）、测试（testing）、测试样本（testing sample）、聚类（clustering）、监督学习（supervised learning）、无监督学习（unsupervised learning）、泛化（generalization）能力、分布（distribution）、独立同分布（independent and identically distributed）、归纳（induction）、演绎（deduction）、泛化（generalization）过程、特化（specialization）过程、归纳学习（inductive learning）、版本空间（version space）、归纳偏好（inductive bias）/偏好、奥卡姆剃刀（Occam's razor）、没有免费的午餐定理（No Free Lunch Theorem）/NFL定理、深度学习（deep learning）等。\n当我们去判断瓜是否成熟，很多是凭借经验。会根据色泽、根蒂、敲声等不同的角度去综合判断。但如果这些事情要用计算机去判断处理怎么办呢？这就是机器学习要研究和做的类似事情，机器学习是研究如何通过计算的手段，找出规律和经验，利用经验来改善和达到所需效果的过程。在计算机中，“经验”就是以“数据”形式存在的，对其进行计算分析来挖掘数据经验。形成的经验（模型）就可以用来预测和处理其他的类似数据和问题了，模型就是从数据分析中学习到的结果。以西瓜的选择为例，我们收集了一批西瓜的数据，从各个方面进行了标记（label），如（色泽=青绿；根蒂=蜷缩；敲声=浊响），（色泽=乌黑；根蒂=稍蜷；敲声=沉闷），（色泽=浅白；根蒂=硬挺；敲声=清脆），... ...。那么这些数据的集合就称为“数据集（data set）”，每条数据就是一个“示例（instance）/样本（sample）”。其中的色泽、根蒂、敲声叫做“属性”或“特征”。属性取值叫做属性值或特征值。属性所占有的空间叫做“属性空间”或“样本空间”/“输入空间”。如西瓜有色泽、根蒂、敲声三个属性，把它的属性看成三个坐标轴，那么每个西瓜都有其空间上的坐标位置，这个位置点对应一个坐标向量，所以一个示例或样本也叫做一个“特征向量”。\n定义几个符号：D=数据样本（数据集）、x=标量、x=向量、X=变量集、χ=样本空间或状态空间\nD={x1,x2,x3,...,xm}表示含有m个样本的数据集，每个样本由d个属性（特征）描述，即每个样本xi={xi1；xi2；xi3；...；xid}是d维样本空间χ的一个向量，xi∈χ。其中xij是xi在第j个属性上的取值，d称为样本xi的“维数”。\n从数据中学习得到模型的过程称为“学习(learning)”/“训练(training)”，这个过程就是通过执行某个学习算法来完成的。训练过程中使用的数据称为“训练数据（training data）”，其中每个样本称为一个“训练样本（training sample）”训练样本组成的集合称为“训练集（training set）”。学习得到的模型对应了关于数据的某种潜在的规律，因此也成为“假设（hypothesis）”。这种潜在的规律自身也称为“真相”或“真实（ground-truth）”，学习的过程就是为了找出或逼近真相。当我们在预测天气好与坏的时候，把这些包含结果信息的数据，如“好天气”称为“标记（label）”；拥有了标记信息的示例/样本，则称为“样例（example）”\n这里用(xi，yi)表示第i个样例，其中yi∈Y是样本xi的标记，Y是所有标记的集合，也成为“标记空间（label space）”或“输出空间”。\n再看下分类，二分类，回归，多分类，聚类的简单介绍和例子。如果我们想要预测的是离散值，如“好天气”“坏天气”，“好瓜”“坏瓜”，那么此类学习任务就称为“分类（classification）”；如果想要预测的是连续值，例如西瓜成熟度0.95，0.37，那么此类学习任务称为“回归（regression）”，其中，只涉及两个类别的分类问题叫二分类任务，其中一个类叫做“正类”，另一个叫“反类”。涉及多个类别时称为“多分类”任务。一般情况，我们是通过训练集数据进行学习，找规律，形成一个从输入空间到输出空间的映射关系f：X->Y。对于二分类任务，我们把输出通常取值Y=｛-1，1｝或｛0，1｝；多分类任务，|Y|>2；对于回归任务，Y=R，R为实数集。\n当我们得到模型后，用这个模型去进行预测的过程就叫做“测试（testing）”，被预测的样本成为“测试样本”。例如在学习得到f后，对测试样本x，可以得到预测表及y=f(x)。\n对于聚类问题，还是以西瓜为例，将训练集中的西瓜分成若干组，每组称为一个“簇（cluster）”，这些自动形成的簇可能对应一些潜在的概念划分，如“浅色瓜”“深色瓜”，“本地瓜”“外地瓜”。聚类的样本信息一般都是没有标记的。\n训练数据有标记称为“监督学习”，无标记称为“无监督学习”，分类和回归属于监督，聚类是无监督。\n那么我们机器学习的目标就是使学习得到的模型能很好的应用于“新样本”，新数据。学习得到的模型适用于新样本的能力，称为“泛化”能力，具有强泛化能力的模型能很好的适用于整个样本空间。通常我们假设样本空间中全体样本服从一个未知“分布”（distribution）D，我们获得的每个样本都是独立的从这个分布上采样获得的，即“独立同分布”，一般来说，训练样本越多，我们得到的关于D（未知分布）的信息越多，就更有利于我们获取强泛化能力的模型。\n再来看假设空间：归纳和演绎是科学推理的量大基本手段，前者是从特殊到一般的“泛化”过程，即从具体的事实归结出一般性规律，如机器学习的从一堆数据中找规律，再用这个规律推广；后者是从一般到特殊的“特化”过程，即从基础原理推演出具体情况。就像我们在学习数学时，基于一组公理和推理规则推导出与之相洽的定理，这就是演绎。“从样例中学习”是一个归纳的过程，也称为“归纳学习”。\n机器学习过程可以看做是从有限的数据集（特殊）进行搜索的过程，搜索目标是找到与训练集“匹配”的假设。从特殊到一般的归纳过程。现实问题中，我们常面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因此可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，称作“版本空间”。例如西瓜问题的一个版本空间：\n\n再看下归纳偏好：假设我们有三个假设模型，(色泽=*；根蒂=蜷缩；敲声=*)、(色泽=*；根蒂=*；敲声=浊响)和(色泽=*；根蒂=蜷缩；敲声=浊响)，用来判断好瓜的标准。那么对于一个瓜来说，我们采用不同的假设模型可能会产生不同的结果，那么我们应该采用哪一个模型呢？机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”或“偏好”。\n如上图，存在A和B两条线可以连接这五个点，但是哪个线更好些呢？可能学习算法更偏好比较平滑的曲线A。归纳偏好可以看作是学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”。那么有没有一般性的原则来引导算法确立“正确的”偏好呢？“奥克姆剃刀”是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”。但是，奥卡姆剃刀并非唯一可行的原则。\n其实经过各种数学推证，不管选择哪种假设模型偏好，总误差是一样的！即总误差与学习算法无关！无论算法A多聪明，算法B多笨拙，他们的期望性能是相同的！这就是“没有免费的午餐”定理，简称NFL定理。\n那么很多人到这里可能会问了，既然所有学习算法的期望性能都跟随机胡猜差不多，那么还用它干嘛？但是要注意：NFL定理有一个重要的前提，所有“问题”出现的机会相同、或所有问题同等重要。但实际情形并不是这样，很多时候，我们只关注自己正在试图解决的问题，希望为它找到一个解决方案，至于这个解决方案在别的问题、甚至在相似的问题上是否为好方案，我们并不关心。所以NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛的谈论“什么学习算法更好”毫无意义。学习算法自身的归纳偏好与问题是否匹配，往往会起到决定性的作用。\n学习机器学习，人工智能了解历史也是有必要的。\n机器学习是人工智能研究发展到一定阶段的必然产物。广义的归纳学习（从样例中学习）涵盖了监督学习、无监督学习，是被研究最多、应用最广的。\n二十世纪八十年代，“从样例中学习”的一大主流是符号主义学习，其代表包括决策树（decision tree）和基于逻辑的学习。\n二十世纪九十年代中期之前，“从样例中学习”的另一主流技术是基于神经网络的连接主义学习。1986年，D.E.Rumelhart等人重新发明了著名的BP算法，产生了深远影响。\n二十世纪九十年代中期，“统计学习（statistical learning）”闪亮登场并迅速占据主流舞台，代表性技术是支持向量机（Support Vector Machine）以及更一般的“核方法（kernel methods）”。\n二十一世纪初，连接主义学习又卷土重来，掀起了以“深度学习”为名的热潮。所谓深度学习，狭义的说就是“很多层”的神经网络。在很多测试和竞赛上，特别是涉及到语音、图像等复杂对象的应用中，深度学习技术取得了优越性能。\n那么，为什么它这个时候才热起来？有两个基本原因：数据大了、计算能力强了。深度学习技术涉及的模型复杂度非常高，以至于要下功夫“调参”，把参数调节好，性能往往就好。因此深度学习虽然缺乏严格的理论基础，但它显著降低了机器学习应用者的门槛，为机器学习技术走向工程实践带来了便利。深度学习模型拥有大量的参数，若数据样本少，则很容易“过拟合”。机器学习现在已经发展成为一个相当大的学科领域。\n今天，在计算机科学的很多分支学科领域中，无论是多媒体、图形学，还是网络通信、软件工程，乃至体系结构、芯片设计，都能找到机器学习技术的身影，尤其是在计算机视觉、自然语言处理等“计算机应用技术”领域，机器学习已经成为最重要的技术进步源泉之一。“数据分析”恰是机器学习技术的舞台。\n科学研究的基本手段已经从传统的“理论+实验”变成现在的“理论+实验+计算”，也出现了“数据科学”的提法。“计算”的目的往往是数据分析，而数据科学的核心也恰恰是通过分析数据来获得价值。\n2006年，卡耐基梅隆大学宣告成立世界上第一个“机器学习系”。说到数据分析，很多人会想到“数据挖掘（data mining）”，而机器学习和统计学的研究为数据挖掘提供数据分析技术。美国《新闻周刊》曾对谷歌搜索有一句话评论：“它使任何人离任何问题的答案间的距离变得只有点击一下鼠标这么远”。除了深度学习，还可以了解下“迁移学习”、“类比学习”、“集成学习”等。\n机器学习领域最重要的国际会议是国际机器学习会议（ICML）、国际神经信息处理系统会议（NIPS）和国际学习理论会议（COLT），重要的区域性会议主要有欧洲机器学习会议（ECML）和亚洲机器学习会议（ACML），最重要的国际学术期刊是Journal of Machine Learning Research和Machine Learing。人工智能领域的重要会议如IJCAI、AAAI以及重要期刊如Artificial Intelligence、Journal of Artificial Intelligence Research等。国内机器学习领域最主要的活动是两年一次的中国机器学习大会（CCML）以及每年句型的“机器学习以及应用”研讨会（MLA），很多学术刊物都经常刊登有关机器学习的论文。\n参考文献：\n[1]周志华.机器学习[M].北京：清华大学出版社，2016.\n[2]陆汝钤.人工智能（下册）.北京：科学出版社，1996.\n[3]Alpaydin,E.Introduction to Machine Learning.MA：MIT Press,Cambridge，2004."}
{"content2":"转自飞鸟各投林\n史上最强----机器学习经典总结---入门必读----心血总结-----回味无穷\n让我们从机器学习谈起\n机器学习的定义：\n从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。\n让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。\n机器学习与人类思考的类比\n人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行“归纳”，获得了生活的“规律”。当人类遇到未知的问题或者需要对未来进行“推测”的时候，人类使用这些“规律”，对未知问题与未来进行“推测”，从而指导自己的生活和工作。\n机器学习中的“训练”与“预测”过程可以对应到人类的“归纳”和“推测”过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。"}
{"content2":"拥抱变化\n从网易云音乐的歌单、亚马逊的商品到抖音的短视频，机器学习主导的推荐系统改变了用户浏览习惯；iphone x 在刘海中祭出3D结构光，人脸识别AI便在移动终端迅速蔓延……\n自从Alpha围棋占据人类棋类智力顶峰以来，机器大有在各个领域大放异彩的趋势，Google Assistant 在某些方面看起来毫不逊色于人类，那个几年前看起来还模糊不清的未来已经来临，这是关于人工智能和机器学习的时代。\n机器学习火了人工智能产品，也带火了创造它们的攻城狮们。机器学习方向的人才异常抢手也是业内常态，“批发价20K起” 毫不夸张。\n很多人跃跃欲试，但入行并不能仅靠浮躁的情绪和一腔热血，对机器学习的体系及应用有整体的把握，在这个基础上深入各个技能分支，有计划地系统学习，效率要高得多。\n对于机器学习或者很多AI方向的职位而言，核心技能无非是“数据特征+算法模型”，当然我们还可以细分来看，算法与特征，需要掌握的技能有哪些。\n数据特征：\n数据清洗：消除数据噪音，归一化、正则化、采样\n数据标注：做出统一化、高质量的数据，提升机器学习效果\n特征工程：特征选取方法、降维方法、多个特征融合\n算法模型技能：\n主流监督/无监督学习算法：原理以及适用性，生成和调用\n模型优化：调参、加约束条件、模型替换、多模型融合\n运行优化：处理数据集的效率，掌握更高效的技巧、框架和工具（如spark）\n当然，基础技能是必备的\n编程/工具：Python基础及第三方库/框架，后续可以上spark/hadoop\n数学基础：微积分、概率统计、线性代数\n所以神秘的AI攻城狮们的工作看起来也并不可怕，比如使用成熟的框架和工具，运行已有算法，训练业务数据，获得工作模型并不断调优，应用到企业产品。\n高效学习\n针对机器学习的职业技能和知识框架，DC学院推出了一门非常完善的《机器学习》体系课程，几乎可以学到机器学习所有的主干知识，并深入到细枝末节。即便你没什么基础，也能很快上手，并独立完成实际项目。\n这门课之所以如此体系且全面，一方面是因为机器学习本身的体系十分庞大，当然只是教sklearn调个模型显然不是在认真讲机器学习。\n另一方面，比如学习路径的设计、知识点难易度的把握，案例的选取，内容的深入程度等细节都做了大量延伸和重点打磨。\n比自己去完成一个机器学习项目更难的是，如何让课程适合更多的人（特别是没有什么基础的同学），既要易于理解，也要兼顾深度。\n所以课程框架清晰，以结果为导向，目的就是去掌握那部分核心技能，并在实际的案例中输出结果。比如系统的微积分和概率论不必回炉重学，Python编程更是如此，掌握最需要的那部分，效率更高。\n相信对于每一个想要学习机器学习的人来说，大纲里的内容多少都有了解，不多赘述，今天我们以问答的形式来做课程介绍。\n- ❶ -\nQ：这门课大概的学习思路是怎样的？\n要回答你这个问题，我们得先来了解一个机器学习项目的实现流程是怎样的。\n点击可查看高清大图\n当我们拿到一堆数据，并且明确需要解决的问题，或者需要预测的数据的时候，我们就应该想到要按怎样的思路去解决问题了。\n1. 进行数据清洗，初步的特征选择\n2. 选择合适的模型进行训练\n3. 做更深入的特征工程\n4. 调节模型的各种参数\n5. 对模型进行优化/融合等处理\n最终我们要得到比较理想的结果，让这个模型在不同的数据中依然效果不俗。\n其实我们学习整体的框架就基于这个流程，其中涉及到大量算法原理及使用、模型选择思路、特征工程、集成学习等等内容，这是一个完整的机器学习实践流程。\n除此之外，课程还会涉及时间序列、强化学习、深度学习的知识，让你可以处理更多样化的数据以及应用场景。\n- ❷ -\nQ：学习这门课需要先补充哪些基础？\n这个问题是被问及最多的，机器学习这门技术有太多的诱惑，且不说改变世界、追求技术这样的空洞言论，只是动辄20K的月薪，就足够有说服力。\n基础是必须的，但只要你有基本数学基础（学过大学数学：微积分\\线性代数\\概率统计），那就没有问题，数学功底越好越有优势。\n我们也在课程中补充了必备的数学基础，微积分、现代、概率统计都有涉及，一般不被重视的信息论和优化理论，都有专门的课程讲解。\n数学基础固然重要，但是并不建议花费太多时间去刷数学书，这是南辕北辙，最好的办法还是直接学习机器学习算法与应用，到了看不懂的地方再去补充相应的数学知识，这样效率会高很多。\n编程基础呢？你需要掌握Python的基础知识，比如基本的数据类型，编程规范，语句以及函数，以及机器学习中必备的第三方库等等。\n这些东西你可以在短时间内看文档/课程掌握。而课程中也会教你用Numpy/Pandas/Sklearn等工具进行数据处理与模型训练，不必担心。\n- ❸ -\nQ：课程会涉及哪些算法的讲解？\n问这个问题就知道你一定是有点基础了。课程主要从监督学习和无监督学习两个方向进行划分，时下主流的算法模型均会涉及。\n监督学习部分：线性回归、逻辑回归、KNN、SVM、朴素贝叶斯。\n无监督学习部分：k均值、层次聚类、密度聚类、EM。\n当然我们还会回归统计学，了解统计学习的本质，比如非常重要的极大似然估计、偏倚方差分解、贝叶斯估计、参数化方法等等。\n总之，算法这个部分是重头戏，从统计学习到主流的机器学习方法，都有涉及。每个算法后面都会有案例配合具体的数据集进行实践，会用才是硬道理。\n当然除了基本的理解和应用，我们会尽量把常用的算法，深入原理讲解推导的过程。这样，不但可以增加你对于实现过程的理解，也便于后续进行模型的优化。\n- ❹ -\nQ：是否有足够的案例和项目？\n当然有，还是手把手的那种！\n除了每个算法后附带的针对性实践案例，每章都设置了体系完整的实战项目，更加偏向真实应用。\n比如我们会尽量利用真实的数据集，更加系统化的实践，让你学习具体的知识点的同时，熟悉机器学习的基本套路，并能够举一反三，把这些套路应用到更多的问题中去。\n具体的案例老师都会详细讲解，细化到每一个操作，案例的思路、实现过程以及全部的代码我们都会分享出来，通过jupyter notebook的形式，下载后你可以直接在你本地的环境中运行。\n课程中将包含但不限于以下案例/项目：\n- ❺ -\nQ：用一章来讲特征工程，真有那么重要？\n在机器学习/数据挖掘领域有一句经典的话：数据与特征工程决定了机器学习的上限，而算法与模型不过是逼近这个上限而已。\n算法与模型不过是实现机器学习的第一步，相当于我们掌握了基本实现方式，但是真正要获得好的效果，还要进行很多的内部优化，特征工程则是重中之重。\n事实上所有机器学习算法的成功，都在于你怎么样去展示这些数据，由此可见特征工程在实际的机器学习中的重要性。\n事实上在很多数据挖掘竞赛中，大家使用的模型大同小异（比如大部分人会直接祭出XGBoost，然后数据全部往上面怼就完事），但高下之分很大程度上源于特征工程。\n除了基本的数据清洗（缺失值/异常值处理，数据归一化、多项式特征生成）以及特征选择方法（Filter、Wrapper、Embedded），还会涉及降维（PCA&LDA）的方法。从单个特征的处理到多个特征的融合，你都可以轻松解决。\n算法的使用往往是招式的修炼，而特征工程才是真正的内功。\n- ❻ -\nQ：深度学习和强化学习会讲到什么程度？\n深度学习作为机器学习的一个重要分支，也是处理很多问题的好方式，课程中会介绍几种常用的神经网络（CNN、RNN），并通过 Keras 框架来实现深度学习。\n具体的呢，主要通过猫狗分类的案例，来梳理一个完整的深度学习流程，并借此掌握 Keras 框架。\n当然深度学习不止如此，通过课程你可以掌握深度学习实现的基本套路，但这个东西是需要你花更多的精力去做更深入的学习，才能达到更好地效果。\n而强化学习呢，主要是让你去了解机器学习自我提升的思想，强大如 AlphaGo ，是通过怎样方式进行自我学习的。而这，也是真正迈向人工智能的基石。\n- ❼ -\nQ：学完我能达到什么样的水平？\n就喜欢你这样有觉悟的，但说实话，没有人能够保证。\n如果你稍微认点真，一套课程下来，独立完成基本的机器学习项目没有问题的。绝大部分的数据挖掘竞赛，你都能够通过学到的知识，跑个模型，取得还不错的成绩。不过要进入TOP排名，老铁还需努力。\n更重要的是，课程中提供的特征工程、模型筛选、集成学习、调参、优化技巧，才是你形成核心竞争力的关键。\n千万不要认为课程中关于算法的推导没什么卵用，你要是觉得调个包、找两个特征就能忽悠面试官，那对方会分分钟教你做人。\n所以课程教你的不仅是套路，还有更多的内功，以及学习、泛化的方法。天高任鸟飞，课程够深入，只要你愿意学，就有无限可能。\n\nQ：另外，我还想问……？\n算了，别问了，相关信息都给你说了吧：\n录播课程，随时上课，你有绝对的学习自主权。\n总共60个课时，每课时20-60分钟不等，讲懂为止。\n主讲老师是华科教授，技术好，各种深入浅出，还送两助教。\n学习群老师即时答疑，专治各种不会。\n课后资料里，案例代码，实现思路、重点笔记、拓展阅读全部都熬好了，直接服用即可。\n匹配针对性数据竞赛，实时训练，还可以查看真实排名。\nPython 3.6，不解释，只用最新的。\n爱过~约~就是现在~\n\nDC学院《机器学习》首发，限额底价\n¥599（原价899），限100名\n长按下方二维码，了解详情&名额预定\n课程咨询、资料获取、免费试看，请加入下方群聊\n若群满，加Alice微信：datacastle2017"}
{"content2":"打造一个机器学习的应用程序，从工作流程方面熟悉整体的步骤，对后面深入学习有很大的帮助。\n整个过程有六个步骤：\n1. 获取\n2. 检查\n3. 清洗\n4. 建模\n5. 评估\n6. 部署\n1. 获取\n机器学习中的数据，可以来自不同的数据源，可能是csv文件，也可能是从服务器拉取出来的日志，或者是自己构建的web爬虫。\n2. 检查\n获取了数据，下一步进行合理地检查数据，最好的方法是发现不可能或几乎不可能的事情。无论数据是何种类型，检查最极端的情况。它们是否有意义？一个较好的实践是对数据进行简单的统计测试，并将数据可视化。可能有些数据是缺失的或不完整的。\n3. 清洗\n这一步的目标是将数据转化为适合模型使用的格式。这个阶段包括若干个过程，如过滤、聚集、输入和转化。所需的操作很大程度上取决于数据的类型，以及所使用的库和算法的类型。只有进入模型的数据质量好，模型的质量才能够得到保证。数据清洗这一步很关键。\n4. 建模\n数据准备完成后，下一阶段进行建模。我们将选择适当的算法，并在数据上训练一个模型。基本的步骤包括将数据分割为训练、测试和验证的集合，训练模型，预测。\n5. 评估\n模型构建完成后，怎么样确定模型训练得好不好？就需要进行评估。简单来说就是看模型的预测和实际值到底有多接近。\n6. 部署\n模型的表现能够令人满意，接下来就进行部署，将训练的模型在机器上跑起来，投入应用。\n参考文献\n[1]（美）Alexander T.Combs.PYTHON机器学习实践指南[M].北京：人民邮电出版社.2017."}
{"content2":"在经历成千上万个小时机器学习训练时间后，计算机并不是唯一学到很多东西的角色，作为开发者和训练者的我们也犯了很多错误，修复了许多错误，从而积累了很多经验。\n在本文中，作者基于自己的经验（主要基于 TensorFlow）提出了一些训练神经网络的建议，还结合了案例，可以说是过来人的实践技巧了。\n出品 | AI 科技大本营\n通用技巧\n有些技巧对你来说可能就是明摆着的事，但在某些时候可能却并非如此，也可能存在不适用的情况，甚至对你的特定任务来说，可能不是一个好的技巧，所以使用时需要务必要谨慎！\n1.使用 ADAM 优化器\n确实很有效。与更传统的优化器相比，如 Vanilla 梯度下降法，我们更喜欢用ADAM优化器。用 TensorFlow 时要注意：如果保存和恢复模型权重，请记住在设置完AdamOptimizer 后设置 Saver，因为 ADAM 也有需要恢复的状态（即每个权重的学习率）。\n2.ReLU 是最好的非线性(激活函数)\n就好比 Sublime 是最好的文本编辑器一样。ReLU 快速、简单。而且，令人惊讶的是，它们工作时，不会发生梯度递减的情况。虽然 Sigmoid 是常见的激活函数之一，但它并不能很好地在 DNN 进行传播梯度。\n3.不要在输出层使用激活函数\n这应该是显而易见的道理，但如果使用共享函数构建每个层，那就很容易犯这样的错误：所以请确保在输出层不要使用激活函数。\n4.请在每一个层添加一个偏差\n这是 ML 的入门知识了：偏差本质上就是将平面转换到最佳拟合位置。在 y=mx+b 中，b 是偏差，允许曲线上下移动到“最佳拟合”位置。\n5.使用方差缩放（variance-scaled）初始化\n在 Tensorflow 中，这看起来像tf.reemaner.variance_scaling_initializer()。\n根据我们的经验，这比常规的高斯函数、截尾正态分布（Truncated Normal）和 Xavier 能更好地泛化/缩放。\n粗略地说，方差缩放初始化器根据每层的输入或输出数量（TensorFlow中的默认值是输入数量）调整初始随机权重的方差，从而有助于信号更深入地传播到网络中，而无须额外的裁剪或批量归一化（Batch Normalization）。Xavier 与此相似，只是各层的方差几乎相同；但是不同层形状变化很大的网络（在卷积网络中很常见）可能不能很好地处理每层中的相同方差。\n6.归一化输入数据\n对于训练，减去数据集的均值，然后除以它的标准差。在每个方向的权重越少，你的网络就越容易学习。保持输入数据以均值为中心且方差恒定有助于实现这一点。你还必须对每个测试输入执行相同的规范化，因此请确保你的训练集与真实数据相似。\n以合理保留其动态范围的方式缩放输入数据。这与归一化有关，但应该在归一化之前就进行。\n例如，真实世界范围为 [0,140000000] 的数据 x 通常可以用 tanh(x) 或 tanh(x/C) 来控制，其中 C 是一些常数，它可以拉伸曲线，以适应 tanh 函数缓坡部分的动态范围内的更多输入范围。特别是在输入数据在一端或两端可能不受限制的情况下，神经网络将在（0,1）之间学习得更好。\n7.一般不用学习率衰减\n学习率衰减在 SGD 中更为常见，但 ADAM 很自然地处理了这个问题。如果你真的想把每一分表现都挤出去：在训练结束时短时间内降低学习率；你可能会看到突然的、非常小的误差下降，然后它会再次变平。\n如果你的卷积层有 64 或 128 个过滤器，那就足够了。特别是一个对于深度网络而言。比如，128 个真的就已经很多了。如果你已经有了大量的过滤器，那么再添加更多的过滤器未必会进一步提高性能。\n8.池化用于平移不变性\n池化本质上就是让网络学习图像“那部分”的“总体思路”。例如，最大池化可以帮助卷积网络对图像中的特征的平移、旋转和缩放变得更加健壮。\n调试神经网络\n如果你的网络没能很好地进行学习（指在训练过程中损失/准确率没有收敛，或者没有得到预期的结果），那么可以试试以下的技巧：\n9.过拟合\n如果你的网络没有学习，那么首先要做的第一件事就是对训练点进行过拟合。准确率基本上应为 100% 或 99.99%，或误差接近 0。如果你的神经网络不能对单个数据点进行过拟合，那么体系架构就可能有严重的问题，但这可能是微妙的。如果你可以对一个数据点进行过拟合，但是对较大的集合进行训练仍然无法收敛，请尝试以下建议：\n10.降低学习率\n你的网络学习就会变得更慢一些，但是它可能会找到以前无法进入的最小化的方式，因为它的步长太大了。\n11.提高学习率\n这样做将会加快训练，有助于收紧反馈，这意味着无论你的网络是否正常工作，你都会很快地知道你的网络是否有效。虽然网络应该更快地收敛，但其结果可能不会很好，而且“收敛”实际上可能会跳来跳去。（对于 ADAM 优化器，我们发现在很多经历中，学习率大约为 0.001 时，表现很不错。）\n12.减少批量处理规模\n将批处理大小减小到 1，可以为你提供与权重更新相关的更细粒度的反馈，你应该使用TensorBoard（或其他一些调试/可视化工具）展示出来。\n13.删除批归一化层\n随着批处理大小减少到 1，这样做会暴露出梯度消失或梯度爆炸的问题。我们曾有过一个网络，在好几周都没有收敛，当我们删除了批归一化层之后，我们才意识到第二次迭代时输出都是 NaN。就像是创可贴上的吸水垫，它也有它可以发挥效果的地方，但前提是你知道网络没有 Bug。\n14.增加批量处理的规模\n一个更大的批处理规模，如果可以的话，整个训练集减少梯度更新中的方差，使每个迭代更准确。换句话说，权重更新将朝着正确的方向发展。但是！它的可用性和物理内存限制都有一个有效的上限。通常，我们发现这个建议不如上述两个建议有用，可以将批处理规模减少到1并删除批归一化层。\n15.检查你的重构\n大幅度的矩阵重构（如改变图像的X、Y 维度）会破坏空间局部性，使网络更难学习，因为它也必须学会重塑。（自然特征变得支离破碎。事实上，自然特征在空间上呈局部性，也是为什么卷积神经网络能如此有效的原因！）如果使用多个图像/通道进行重塑，请特别小心；使用 numpi.stack()进行适当的对齐操作。\n16.仔细检查你的损失函数\n如果使用一个复杂的函数，请尝试将其简化为 L1 或 L2。我们发现L1对异常值不那么敏感，在发出噪声的批或训练点时，不会做出太大的调整。\n如果可以的话，仔细检查你的可视化。你的可视化库（Matplotlib、OpenCV等）是调整值的比例呢，还是它们进行裁剪？可考虑使用一种视觉上均匀的配色方案。\n实战分析\n为了使上面所描述的过程更容易让读者理解，我们这儿有一些用于描述我们构建的卷积神经网络的真实回归实验的损失图（通过TesnorBoard）。\n起初，这个网络根本没有学习：\n我们试图裁剪这些值，以防止它们超出界限：\n嗯。看看不平滑的值有多疯狂啊！学习率是不是太高了？我们试着在一个输入数据上降低学习率并进行训练：\n你可以看到学习率的前几个变化发生的位置（大约在 300 步和 3000 步）。显然，我们衰减得太快了。所以，给它更多的衰减时间，它表现得会更好：\n你可以看到我们在 2000 步和 5000 步的时候衰减了。这样更好一些了，但还不够好，因为它没有趋于 0。\n然后我们禁用了 LR 衰减，并尝试将值移动到更窄的范围内，而不是通过 Tanh 输入。虽然这显然使误差值小于 1，但我们仍然不能对训练集进行过拟合：\n这里我们发现，通过删除批归一化层，网络在一到两次迭代之后迅速输出 NaN。我们禁用了批归一化，并将初始化更改为方差缩放。\n这些改变了一切！我们能够对只有一两个输入的测试集进行过拟合了。当底部的图标裁剪Y轴时，初始误差值远高于 5，表明误差减少了近 4 个数量级：\n上面的图表是非常平滑的，但是你可以看到它非常快地拟合了测试输入，随着时间的推移，整个训练集的损失降低到了 0.01 以下。\n这没有降低学习速度。然后我们将学习速率降低一个数量级后继续训练，得到更好的结果：\n这些结果好得多了！但是，如果我们以几何方式降低学习率，而不是将训练分成两部分，会发生什么样的结果呢？\n通过在每一步将学习率乘以 0.9995，结果就不那么好了：\n大概是因为学习率衰减太快了吧。乘数为 0.999995 会表现的更好，但结果几乎相当于完全没有衰减。\n我们从这个特定的实验序列中得出结论，批归一化隐藏了由槽糕的初始化引起的爆炸梯度，并且 ADAM 优化器对学习率的衰减并没有什么特别的帮助，与批归一化一样，裁剪值只是掩盖了真正的问题。我们还通过 tanh 来控制高方差输入值。\n我们希望，本文提到的这些基本技巧能够在你构建深度神经网络时有所帮助。通常，正式因为简单的事情才改变了这一切。\n原文链接：https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/\n作者：Matt H/Daniel R\n译者：婉清，责编：Jane\n推荐阅读：\n面向对象编程，再见！\n如何理解区块链的共识算法？\n没有几样强迫症，不配自称程序员\n这次拿下Python全靠它了！一个交互式的学习资源！\n敲代码时，程序员戴耳机究竟听的啥？\n呐！这份区块链藏宝图，送你了；干货满满，建议收藏哟！\n云漫圈 | AR & VR傻傻分不清楚。。。"}
{"content2":"115\n[入门问题]\n[TensorFlow]\n[深度学习]\n[好玩儿的算法应用实例]\n[聊天机器人]\n[神经网络]\n[机器学习]\n[机器学习算法应用实例]\n[自然语言处理]\n[数据科学]\n[Python]\n[Java]\n[机器学习－－初期的笔记]\n[路线]\n[软件安装]\n[面试]\n入门问题\n简单粗暴地入门机器学习\n机器学习的技术栈及应用实例脑洞\n深度学习相关最新图书推荐\nTensorFlow\nTensorFlow-11-策略网络\nTensorFlow-10-基于 LSTM 建立一个语言模型\nTensorFlow-9-词的向量表示\nTensorFlow-8-详解 TensorBoard 如何调参\nTensorFlow-7-TensorBoard Embedding可视化\nTensorFlow-6-TensorBoard 可视化学习\nTensorFlow－5: 用 tf.contrib.learn 来构建输入函数\nTensorFlow-4: tf.contrib.learn 快速入门\nTensorFlow－3: 用 feed-forward neural network 识别数字\nTensorFlow-2: 用 CNN 识别数字\nTensorFlow－1: 如何识别数字\nTensorFlow 入门\n一文学会用 Tensorflow 搭建神经网络\n用 Tensorflow 建立 CNN\n深度学习\n深度学习的主要应用举例\n[Keras]\n对比学习用 Keras 搭建 CNN RNN 等常用神经网络\n[强化学习]\n强化学习是什么\n一文了解强化学习\n好玩儿的算法应用实例\n5分钟构建一个自己的无人驾驶车\n自己动手写个聊天机器人吧\n自己写个 Prisma\n用 TensorFlow 创建自己的 Speech Recognizer\n用 TensorFlow 让你的机器人唱首原创给你听\n如何自动生成文章摘要\n聊天机器人\n开启聊天机器人模式\n用 TensorFlow 做个聊天机器人\n神经网络\n神经网络\n神经网络的前世\n神经网络 之 感知器的概念和实现\n神经网络 之 线性单元\n什么是神经网络\n手写，纯享版反向传播算法公式推导\n常用激活函数比较\n什么是 Dropout\nCNN\n图解何为CNN\n用 Tensorflow 建立 CNN\n按时间轴简述九大卷积神经网络\nRNN\n详解循环神经网络(Recurrent Neural Network)\n图解RNN\nCS224d－Day 5: RNN快速入门\n用深度神经网络处理NER命名实体识别问题\n用 RNN 训练语言模型生成文本\n用 Recursive Neural Networks 得到分析树\nRNN的高级应用\nLSTM\n详解 LSTM\n用 LSTM 来做一个分类小问题\n用 LSTM 做时间序列预测的一个小例子\nseq2seq\nseq2seq 入门\nseq2seq 的 keras 实现\n机器学习\n[Kaggle]－－由此来看实战是什么样的\n一个框架解决几乎所有机器学习问题\n通过一个kaggle实例学习解决机器学习问题\n从 0 到 1 走进 Kaggle\nKaggle 神器 xgboost\n[基础]－－一些基本概念和小技巧\n轻松看懂机器学习十大常用算法\n特征工程怎么做\n机器学习算法应用中常用技巧-1\n机器学习算法应用中常用技巧-2\n关于凸优化\n如何选择优化器 optimizer\n为什么要用交叉验证\n用学习曲线 learning curve 来判别过拟合问题\n用验证曲线 validation curve 选择超参数\n用 Grid Search 对 SVM 进行调参\n用 Pipeline 将训练集参数重复应用到测试集\nPCA 的数学原理和可视化效果\n机器学习中常用评估指标汇总\n什么是 ROC AUC\n[算法]－－通俗易懂讲算法\n决策树的python实现\nCART 分类与回归树\nBagging 简述\nAdaboost 算法\n浅谈 GBDT\n用ARIMA模型做需求预测\n推荐系统\n[Sklearn]\nSklearn 快速入门\n了解 Sklearn 的数据集\n自然语言处理\n[cs224d]\nDay 1. 深度学习与自然语言处理 主要概念一览\nDay 2. TensorFlow 入门\nDay 3. word2vec 模型思想和代码实现\nDay 4. 怎样做情感分析\nDay 5. CS224d－Day 5: RNN快速入门\nDay 6. 一文学会用 Tensorflow 搭建神经网络\nDay 7. 用深度神经网络处理NER命名实体识别问题\nDay 8. 用 RNN 训练语言模型生成文本\nDay 9. RNN与机器翻译\nDay 10. 用 Recursive Neural Networks 得到分析树\nDay 11. RNN的高级应用\n一个隐马尔科夫模型的应用实例：中文分词\n数据科学\n1.［图解DS基础概念］AB Testing, Type 1 / 2 Error\n2.［图解DS基础概念］Critical value，Alpha，Z－score，P－value 关系\nPython\nPandas常用命令－1\nPandas常用命令－2\nPandas QQ聊天记录分析\nPython 爬虫 1 快速入门\nPython 爬虫 2 爬取多页网页\nJava\n入门 Java 系列汇总：\n2 天入门 Java－Day 1\nDay 1-Java-imooc－2.变量常量\nDay 1-Java-imooc－3.运算符\nDay 1-Java-imooc－4.流程控制语句\nDay 1-Java-imooc－5.数组\nDay 1-Java-imooc－6.方法\n2 天入门 Java－Day 2\nDay 2-Java－imooc－8-封装\nDay 2-Java－imooc－9-继承\nDay 2-Java－imooc－10-多态\n机器学习－－初期的笔记很粗糙\n机器学习－多元线性回归\nUdacity-Machine Learning纳米学位－学习笔记1\nMachine Learning Notes-Decision Trees-Udacity\nMachine Learning Notes-Linear Regression-Udacity\n支持向量机\n神经网络\nInstance Based Learning\nEnsemble Learners\n路线\n数据科学家养成路线\n纯粹的数学之美\nPython很强大\n一张图带你看懂何为数据分析\n如何成为一名数据科学家并得到一份工作\n软件安装\n［MySQL］\n5分钟入门MySQL Workbench\n图解Mac下如何安装管理MySQL\n［Virtualenv］\n详解Mac配置虚拟环境Virtualenv，安装Python科学计算包\n面试\n面试官是怎么看你的Github profile\n［Leetcode］\nLEETCODE - Linked List 题目思路汇总\n欢迎关注公众号：极客X养成计划\n人工智能时代，学点机器学习，一起持续迭代，Run With AI ！"}
{"content2":"保研之后，时间整个人都放松了，节奏很慢，懒散了几天。还是决定要学点东西，之前学过机器学习课程，但是没有认真听，这段时间刚好可以看看，做做笔记。教材是周志华老师的《机器学习》。\n一、奥卡姆剃刀（Occam’s razor）\n奥卡姆剃刀原则主张选择与经验观察一致的最简单假设，是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一直，则选最简单的那个”。\n举个例子。假如有一些连续点，可以用二次或更复杂的函数拟合，那么就用二次函数来拟合。\n问题是，怎么判断，哪一个假设更“简单”？ 这就要用其他机制来来解决了，这个问题也一直困扰者研究者们，因此，对奥卡姆剃刀在机器学习领域的作用，一直存在争议。\n二、没有免费的午餐（No Free Lunch Theorem - 简称NLF定理）\n通过奥卡姆剃刀，我们确定了选择更简单的假设a作为学习算法，但是由于训练集外的数据样本并不一定符合a，所以a不一定比另一个算法b更好。\n从而引伸出，如果简单的学习算法a，它在某些问题上比算法b好，则必然存在另一些问题，b比a的性能要好。\n有趣的是，经过数学证明（有兴趣可自行查阅），这个结论对任何算法都成立。\n也就是说，无论学习算法a有多聪明，b有多笨拙，他们的期望性能是相同的。这就是NLF定理。\n幸运的是，这有一个前提，就是所有问题出现的机会相同，或者所有问题同样重要，才会性能相同。\n然而实际情形并不是这样。我们一般要解决的问题都是某个具体任务，不管这个解决方案在其他问题上的性能。\n所以NLF定理，让我们清楚认识到，脱离具体问题谈论什么“学习算法更好”是毫无意义的，一个算法无法在所有问题上都表现良好。"}
{"content2":"Andrew Ng coursera上的《机器学习》ex5\n按照课程所给的ex5的文档要求，ex5要求完成以下几个计算过程的代码编写：\n1.LinearRegCostFunction.m\n该.m文件包含了求正则化线性回归模型代价函数，梯度下降算法两个算法。\nfunction [J, grad] = linearRegCostFunction(X, y, theta, lambda) %LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear %regression with multiple variables % [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the % cost of using theta as the parameter for linear regression to fit the % data points in X and y. Returns the cost in J and the gradient in grad % Initialize some useful values m = length(y); % number of training examples % You need to return the following variables correctly J = 0; grad = zeros(size(theta)); % ====================== YOUR CODE HERE ====================== % Instructions: Compute the cost and gradient of regularized linear % regression for a particular choice of theta. % % You should set J to the cost and grad to the gradient. % theta1= [0;theta(2:end)]; Theta = theta; for i=2:m Theta = [Theta,theta]; end h = sum(X .* Theta',2); J = sum(( h - y).^2,1) / m / 2 + sum(theta1 .^ 2,1) * lambda / 2 / m; % Theta(1,:) = Theta(1,:) * 0; [m1,n1] = size(X); tmp = h - y; for i=2:n1 tmp = [tmp,h - y]; end grad_tmp = tmp .* X; grad = sum(grad_tmp,1) /m + (lambda * theta1 /m)'; grad = grad'; % ========================================================================= grad = grad(:); end\n2.learningCurve.m\n通过学习曲线，我们可以判断针对high variance（过拟合）以及high bias（欠拟合）两种情况的时候分别使用什么解决办法。学习曲线主要是训练数据集，交叉验证集和lambda，数据集大小m，特征项数目之间的关系曲线。\nfunction [error_train, error_val] = ... learningCurve(X, y, Xval, yval, lambda) %LEARNINGCURVE Generates the train and cross validation set errors needed %to plot a learning curve % [error_train, error_val] = ... % LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and % cross validation set errors for a learning curve. In particular, % it returns two vectors of the same length - error_train and % error_val. Then, error_train(i) contains the training error for % i examples (and similarly for error_val(i)). % % In this function, you will compute the train and test errors for % dataset sizes from 1 up to m. In practice, when working with larger % datasets, you might want to do this in larger intervals. % % Number of training examples m = size(X, 1); % You need to return these values correctly error_train = zeros(m, 1); error_val = zeros(m, 1); % ====================== YOUR CODE HERE ====================== % Instructions: Fill in this function to return training errors in % error_train and the cross validation errors in error_val. % i.e., error_train(i) and % error_val(i) should give you the errors % obtained after training on i examples. % % Note: You should evaluate the training error on the first i training % examples (i.e., X(1:i, :) and y(1:i)). % % For the cross-validation error, you should instead evaluate on % the _entire_ cross validation set (Xval and yval). % % Note: If you are using your cost function (linearRegCostFunction) % to compute the training and cross validation error, you should % call the function with the lambda argument set to 0. % Do note that you will still need to use lambda when running % the training to obtain the theta parameters. % % Hint: You can loop over the examples with the following: % % for i = 1:m % % Compute train/cross validation errors using training examples % % X(1:i, :) and y(1:i), storing the result in % % error_train(i) and error_val(i) % .... % % end % % ---------------------- Sample Solution ---------------------- for i=1:m %利用X(1:i,:),y(1:i),trainLinearReg(),来训练参数theta theta=trainLinearReg(X(1:i,:),y(1:i), lambda); %You should evaluate the training error on the first i training examples (i.e., X(1:i, :) and y(1:i)). %训练误差计算只用X(1:i,:), y(1:i) [error_train(i),grad]=linearRegCostFunction(X(1:i,:), y(1:i), theta, 0); %交叉验证用上所有的验证集，即Xval, yval %For the cross-validation error, you should instead evaluate on the _entire_ cross validation set (Xval and yval). [error_val(i), grad]=linearRegCostFunction(Xval, yval, theta, 0); end % ------------------------------------------------------------- % ========================================================================= end\n3. polyFeature.m\n为了解决欠拟合的问题，所以增加数据集的特征项。\nfunction [X_poly] = polyFeatures(X, p) %POLYFEATURES Maps X (1D vector) into the p-th power % [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and % maps each example into its polynomial features where % X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ... X(i).^p]; % % You need to return the following variables correctly. X_poly = zeros(numel(X), p); % ====================== YOUR CODE HERE ====================== % Instructions: Given a vector X, return a matrix X_poly where the p-th % column of X contains the values of X to the p-th power. % % X_poly(:,1) = X; for i=2:p X_poly(:,i) = X .^ i; end % ========================================================================= end\n上面的代码按照文档的要求，第一列是x^2,依次类推。\n4.validationCurve.m\n要求是选择合适的lambda。通过计算在不同的lambda下训练数据集、交叉验证集的误差可以得出合适的lambda。\nfunction [lambda_vec, error_train, error_val] = ... validationCurve(X, y, Xval, yval) %VALIDATIONCURVE Generate the train and validation errors needed to %plot a validation curve that we can use to select lambda % [lambda_vec, error_train, error_val] = ... % VALIDATIONCURVE(X, y, Xval, yval) returns the train % and validation errors (in error_train, error_val) % for different values of lambda. You are given the training set (X, % y) and validation set (Xval, yval). % % Selected values of lambda (you should not change this) lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]'; % You need to return these variables correctly. error_train = zeros(length(lambda_vec), 1); error_val = zeros(length(lambda_vec), 1); % ====================== YOUR CODE HERE ====================== % Instructions: Fill in this function to return training errors in % error_train and the validation errors in error_val. The % vector lambda_vec contains the different lambda parameters % to use for each calculation of the errors, i.e, % error_train(i), and error_val(i) should give % you the errors obtained after training with % lambda = lambda_vec(i) % % Note: You can loop over lambda_vec with the following: % % for i = 1:length(lambda_vec) % lambda = lambda_vec(i); % % Compute train / val errors when training linear % % regression with regularization parameter lambda % % You should store the result in error_train(i) % % and error_val(i) % .... % % end % % m = size(lambda_vec,1); for i = 1:m lambda = lambda_vec(i); [theta] = trainLinearReg(X, y, lambda); error_train(i)= linearRegCostFunction(X, y, theta, 0); error_val(i)= linearRegCostFunction(Xval, yval, theta, 0); end % ========================================================================= end"}
{"content2":"转载自某大佬博客：https://pymlovelyq.github.io/2018/10/15/machineLearning/\n“机器学习／深度学习并不需要很多数学基础！”也许你在不同的地方听过不少类似这样的说法。对于鼓励数学基础不好的同学入坑机器学习来说，这句话是挺不错的。不过，机器学习理论是与统计学、概率论、计算机科学、算法等方面交叉的领域，对这些技术有一个全面的数学理解对理解算法的内部工作机制、获取好的结果是有必要的。机器学习确实需要对一些数学领域有深入理解，缺乏必要的数学知识，很可能在更深入的学习中不断遇到挫折，甚至导致放弃。\n说的很多小伙伴恐怕心都凉了一半，或者已经开始打退堂鼓了。不要紧，山人自有妙法。下面就给大家分享一下，主要是一些资源（书籍或者视频课程）。人工智能（机器学习或数据挖掘等）中最最重要的数学就是线性代数与概率论（还有其他，但这两者比重最大）。\n所以我找了个书单自学，电子书为主，顺便分享出来。使用电子书的形式是因为，个人偏好。即使我买了实体书，一旦找到了电子书，我马上就会把纸质书扔到床底下。如果侵犯了任何人的权益，烦请及时通知。\n前言：技术书阅读方法论\n一.速读一遍（最好在1~2天内完成）\n人的大脑记忆力有限，在一天内快速看完一本书会在大脑里留下深刻印象，对于之后复习以及总结都会有特别好的作用。\n对于每一章的知识，先阅读标题，弄懂大概讲的是什么主题，再去快速看一遍，不懂也没有关系，但是一定要在不懂的地方做个记号，什么记号无所谓，但是要让自己后面再看的时候有个提醒的作用，看看第二次看有没有懂了些。\n二.精读一遍（在2周内看完）\n有了前面速读的感觉，第二次看会有慢慢深刻了思想和意识的作用，具体为什么不要问我，去问30年后的神经大脑专家，现在人类可能还没有总结出为什么大脑对记忆的完全方法论，但是，就像我们专业程序员，打代码都是先实践，然后就渐渐懂了过程，慢慢懂了原理，所以第二遍读的时候稍微慢下来，2周内搞定。记住一句话：没看完一个章节后，总结一下这个章节讲了啥。很关键。\n三.实践（在整个过程中都要）\n实践的时候，要注意不用都去实践，最好看着书，敲下代码，把重点的内容敲一遍有个肌肉记忆就很不错了。\n以及到自己做过的项目中去把每个有涉及的原理的代码，研究一遍，就可以了\n百度网盘链接：https://pan.baidu.com/s/1wIL_5arbxhjvzWeyba89kA\n提取码：b379\n备注：文件比较大，网盘直接打开会显示损坏，里面已经整理并且压缩好，需要下载后才可以打。\n\n一. 数学基础\n1.微积分：《微积分学教程》 （F.M.菲赫金哥尔茨）俄罗斯的数学书\n2.线性代数：《Linear Algebra and Its Applications,Third Edition （David C.Lay）》讲得很实际，线性代数最重要的就是与实际应用相联系才能够理解其意义\n3.概率与统计：《概率论与数理统计 （陈希孺）》或《概率论与数理统计（盛骤/谢式千/潘承毅）》这两本书都很不错\n![3.1.png](https://i.loli.net/2018/10/27/5bd3d780336eb.png)\n4.随机过程：《应用随机过程:概率模型导论 （Sheldon M. Ross）》这本书已经出到第10版了\n这四门是数学的基础，当然数学本身就是博大精深的。\n二. 机器学习与数据挖掘（偏理论）\n1.《 统计学习方法 （李航）》\n2.《统计学习基础 （Trevor Hastie, Robert Tibshirani, Jerome Friedman）》\n3.《Pattern Recognition and Machine Learning （Christopher Bishop）》\n4.《Introduction to Machine Learning （Ethem Alpaydin）》（《机器学习导论》）\n6.《Data Mining （韩家炜） 》\n7.《现代模式识别 （孙即祥） 》\n个人觉得《统计学习方法》与《统计学习基础》这两本书是基础，后面的书内容相差不大，所以前两本书应该看，而入门的话，后面的书可以选1到2本精读，剩下的书可作参考。\n三. 智能算法（偏应用）\n1.《Web智能算法 （Haralambos Marmanis, Dmitry Babenko）》\n2.《集体智慧编程 （Toby Segaran）》\n3.《推荐系统实践 （项亮）》\n4.《数据之魅 （Pbilipp K.Janert）》\n这几本书均是从实践的角度讲解了机器学习中常用的算法，非常值得一看。\n有人推荐，学习机器学习的话可以先读《统计学习方法》和《统计学习基础》打底，这样就包含了大部分的算法，然后再深入研究某个算法。\n四.机器学习入门级：\n1.《数学之美》；作者吴军大家都很熟悉。这本书主要的作用是引起了我对机器学习和自然语言处理的兴趣。里面以极为通俗的语言讲述了数学在这两个领域的应用。\n2.《Programming Collective Intelligence》（中译本《集体智慧编程》）；作者Toby Segaran也是《BeautifulData : The Stories Behind Elegant Data Solutions》（《数据之美：解密优雅数据解决方案背后的故事》）的作者。这本书最大的优势就是里面没有理论推导和复杂的数学公式，是很不错的入门书。目前中文版已经脱销，对于有志于这个领域的人来说，英文的pdf是个不错的选择，因为后面有很多经典书的翻译都较差，只能看英文版，不如从这个入手。还有，这本书适合于快速看完，因为据评论，看完一些经典的带有数学推导的书后会发现这本书什么都没讲，只是举了很多例子而已。\n3.《Algorithms of the Intelligent Web》（中译本《智能web算法》）；作者Haralambos Marmanis、Dmitry Babenko。这本书中的公式比《集体智慧编程》要略多一点，里面的例子多是互联网上的应用，看名字就知道。不足的地方在于里面的配套代码是BeanShell而不是python或其他。总起来说，这本书还是适合初学者，与上一本一样需要快速读完，如果读完上一本的话，这一本可以不必细看代码，了解算法主要思想就行了。\n4.《统计学习方法》；作者李航，是国内机器学习领域的几个大家之一，曾在MSRA任高级研究员，现在华为诺亚方舟实验室。书中写了十个算法，每个算法的介绍都很干脆，直接上公式，是彻头彻尾的“干货书”。每章末尾的参考文献也方便了想深入理解算法的童鞋直接查到经典论文；本书可以与上面两本书互为辅助阅读。\n5.《Machine Learning》（《机器学习》）；作者TomMitchell[2]是CMU的大师，有机器学习和半监督学习的网络课程视频。这本书是领域内翻译的较好的书籍，讲述的算法也比《统计学习方法》的范围要大很多。据评论这本书主要在于启发，讲述公式为什么成立而不是推导；不足的地方在于出版年限较早，时效性不如PRML。但有些基础的经典还是不会过时的，所以这本书现在几乎是机器学习的必读书目。那么Mitchell的<机器学习>是经典的入门之作。当然，因为年代久远，很多新的模型都没有涉及到，但不影响他的经典性——入门级。\n6.《机器学习实战》对于程序员，想快速了解模型流程和优缺点的，甚至是实现模型的，那么是我比较推荐的，现在已经有中文版了<机器学习实战>。这本书，提到了很多常见的模型，开始就是模型背景简介，之后是模型优缺点和应用场景、在接着算法实现和案例。而且，在书的最后，提及了一些比较切合时代的话题——大数据下机器学习。\n《机器学习基础》对于想从事机器学习的入门人员，比较推荐一本有中文版的<机器学习基础>(Simon Rogers的，英国格拉斯哥大学计算机科学学院讲师)，这本书适合高年级本科生和研究生。从理论的角度，推导了各个算法，以及探究了各个模型的特性等。涉及数学和矩阵的地方，都有详细的参照。适合高端点的入门，看这本书，可以体会一下自己的数学和矩阵，有种必须要加强的感觉。\n7.《Mining of Massive Datasets》（《大数据》）；作者Anand Rajaraman[3]、Jeffrey David Ullman，Anand是Stanford的PhD。这本书介绍了很多算法，也介绍了这些算法在数据规模比较大的时候的变形。但是限于篇幅，每种算法都没有展开讲的感觉，如果想深入了解需要查其他的资料，不过这样的话对算法进行了解也足够了。还有一点不足的地方就是本书原文和翻译都有许多错误，勘误表比较长，读者要用心了。\n8.《Data Mining: Practical Machine Learning Tools and Techniques》（《数据挖掘：实用机器学习技术》）；作者Ian H. Witten 、Eibe Frank是weka的作者、新西兰怀卡托大学教授。他们的《ManagingGigabytes》[4]也是信息检索方面的经典书籍。这本书最大的特点是对weka的使用进行了介绍，但是其理论部分太单薄，作为入门书籍还可，但是，经典的入门书籍如《集体智慧编程》、《智能web算法》已经很经典，学习的话不宜读太多的入门书籍，建议只看一些上述两本书没讲到的算法。\n五.机器学习深入级：\n1.《Pattern Classification》（《模式分类》第二版）；作者Richard O. Duda[5]、Peter E. Hart、David。模式识别的奠基之作，但对最近呈主导地位的较好的方法SVM、Boosting方法没有介绍，被评“挂一漏万之嫌”。\n2.《The Elements of Statistical Learning : Data Mining, Inference, andPrediction》，（《统计学习基础：数据挖掘、推理与预测》第二版）；作者RobertTibshirani、Trevor Hastie、Jerome Friedman。“这本书的作者是Boosting方法最活跃的几个研究人员，发明的Gradient Boosting提出了理解Boosting方法的新角度，极大扩展了Boosting方法的应用范围。这本书对当前最为流行的方法有比较全面深入的介绍，对工程人员参考价值也许要更大一点。另一方面，它不仅总结了已经成熟了的一些技术，而且对尚在发展中的一些议题也有简明扼要的论述。让读者充分体会到机器学习是一个仍然非常活跃的研究领域，应该会让学术研究人员也有常读常新的感受。”[7]\n3. 机器学习与数据挖掘（偏理论）\n百度网盘链接：https://pan.baidu.com/s/1wIL_5arbxhjvzWeyba89kA\n提取码：b379\n备注：文件比较大，网盘直接打开会显示损坏，里面已经整理并且压缩好，需要下载后才可以打。\n附java从入门SE到进阶EE推荐书籍50+本：\nhttps://pymlovelyq.github.io/2018/10/26/java/\n附学习数据库从入门到进阶书籍pdf版吐血整理推荐（珍藏版）：\nhttps://pymlovelyq.github.io/2018/10/12/database/\n附机器学习和python学习之路吐血整理技术书从入门到进阶(珍藏版)：\nhttps://pymlovelyq.github.io/2018/10/15/machineLearning/\n附算法与数据结构+一点点ACM从入门到进阶吐血整理推荐书单（珍藏版）：\nhttps://pymlovelyq.github.io/2018/10/06/Algorithm/\n附python从入门到进阶推荐书籍最全整理pdf分享附网盘链接已拿BT豆瓣offer：\nhttps://pymlovelyq.github.io/2018/10/20/python/\n附安卓入门到进阶推荐书籍整理pdf附网盘链接已拿阿里豆瓣offer(珍藏)：\nhttps://pymlovelyq.github.io/2018/09/04/An/\n附C/C++语言推荐书籍从入门到进阶带你走上大牛之路（珍藏版）：\nhttps://pymlovelyq.github.io/2018/10/10/CC/\n附Web前端书单从HTML到JS到AJAX到HTTP从框架到全栈过来人帮你走更少弯路（珍藏版）：\nhttps://pymlovelyq.github.io/2018/10/17/WebFont/\n总结：天下没有不劳而获的果实，望各位年轻的朋友，想学技术的朋友，在决心扎入技术道路的路上披荆斩棘，把书弄懂了，再去敲代码，把原理弄懂了，再去实践，将会带给你的人生，你的工作，你的未来一个美梦。"}
{"content2":"文章目录\n1. 如何读英文文献？\n2. 如何获取人工智能&机器学习论文库？\n2.1 哪里有整理好的深度学习的高引/经典论文？\n2.2 经典论文很好，可是知识不断更新，如何获取最新论文呢？\n2.2.1 最新论文直接在arxiv上寻找\n2.2.2 去顶级会议&顶级期刊上寻找\n2.2.2.1 那么问题来了，每个领域的顶会和顶刊有哪些？\n2.2.2.2 如何获取顶会&顶刊的论文？\n2.2.3 如果你熟悉自己领域的大牛，可以关注一下这些研究机构和大牛的主页\n2.2.4 推荐微软亚洲研究院的论文分享会\n2.3 论文有了，可是论文代码复现怎么办？\n2.4 论文代码都有了，那从哪看起呢？\n1. 如何读英文文献？\n首先推荐一些读英文文献的经验，都来源于知乎、微信的点赞数比较高的分享，请点击此链接如何读外文文献。如果没有高效的阅读文献的方法，哪怕下面内容很有价值我们也没办法继续挖掘知识的宝藏。\n2. 如何获取人工智能&机器学习论文库？\n下面我们以问答的形式来引出获得方法\n2.1 哪里有整理好的深度学习的高引/经典论文？\n答案是GitHubChristosChristofidis/awesome-deep-learning，一个1.2W stars，将近近4K fork的项目，可以满足你对机器学习&人工智能绝大部分需求，上面不仅有整理好的经典论文，还有各种免费课程，书籍、网站、数据集等等，你想要的，他都能给你！\n2.2 经典论文很好，可是知识不断更新，如何获取最新论文呢？\n获得最新论文的渠道有很多，让博主来一一列举：\n2.2.1 最新论文直接在arxiv上寻找\narxiv是一个预印版论文网站 时效性最好 ，会先于正式的期刊和会议论文刊出。查询的时候 选择你感兴趣的领域 并将类别设置为computer science即可，输入领域关键词（比如，机器阅读理解，你搜machine reading comprehension， machine reading， machine comprehension， reading comprehension， text comprehend等~），你就可以轻松获取第一手的 人工智能前沿论文。\n2.2.2 去顶级会议&顶级期刊上寻找\n每年关注相关领域顶级会议（计算机领域更侧重会议）和顶级期刊，或者去这二者的官网去检索关键词来寻找你关心的领域的最近论文。\n2.2.2.1 那么问题来了，每个领域的顶会和顶刊有哪些？\n首先是数据挖掘领域\n会议有\nKDD(2019)\nICDM\nSDM(2019)\n期刊有\nACM TKDD\nIEEE TKDE\nCSDL\nDMKD\n然后是机器学习领域\n会议有\nICML\nNeurIPS\n期刊有\nJMLR\nMachine learning\nIEEE PAMI\n统计方面有\nJASA\nAoS\nJRSSB\nBiometrika\n优化方面有\nSIAM Journal on Optimization\n2.2.2.2 如何获取顶会&顶刊的论文？\n这些会议网站和期刊网站，一般无法查阅全文，要想查阅全文 有以下方式：\narxiv上查询有同名的论文\n如果是学生，学校的图书馆有订阅，是可以查全文的\n论坛 学术微信群求助；\n除了前面几种方法 也有一些网站提供全文的搜索，不过因为版权问题 服务一般不稳定 有可能会更换网址 ,除了比较知名的SCI-Hub外，以下网站可以仅作参考 如 https://www.jiumodiary.com/ 该网站的英文服务是基于 https://b-ok.org/ (大量免费英文书) 和 https://booksc.xyz/ (大量免费英文论文)\n2.2.3 如果你熟悉自己领域的大牛，可以关注一下这些研究机构和大牛的主页\n2.2.4 推荐微软亚洲研究院的论文分享会\n微软亚洲研究院创研论坛CVPR 2018论文分享会\n微软亚洲研究院创研论坛 CVPR 2019 论文分享会\n2.3 论文有了，可是论文代码复现怎么办？\n别急，这个网站可以解决你的需求paperswithcode。这儿还有一篇试用介绍<AI学习>Paper With Code，一个有海量论文源代码的网站\n还有，论文代码实现，不能随便在github上找一个实现就完事儿。一定要找论文原作者的代码，或者知名机构会学者的实现，或者star多的实现~\n2.4 论文代码都有了，那从哪看起呢？\n如果你时间充足的话可以先看经典论文，项目急需的话就先看最新的论文\n这就是博主本次的分享了，觉得有收获请点个赞吧~"}
{"content2":"[机器学习]机器学习笔记整理13-线性回归简单实现\n[机器学习]机器学习笔记整理12-线性回归概念理解\n[机器学习]机器学习笔记整理11- 神经网络算法简单实现\n[机器学习]机器学习笔记整理10- 神经网络算法\n[[机器学习]机器学习笔记整理09- 基于SVM图像识别\n[机器学习]机器学习笔记整理08- SVM算法原理及实现\n[机器学习]机器学习笔记整理07- KNN算法\n[机器学习]机器学习笔记整理06-决策树应用\n[机器学习]机器学习笔记整理05-决策树\n[机器学习]机器学习笔记整理04-基本术语理解\n[机器学习]机器学习笔记整理03-深度学习\n[机器学习]机器学习笔记整理02-机器学习\n[机器学习]机器学习实践笔记01\n[机器学习]Scikit-Learn模块学习笔记——数据集模块datasets\n[机器学习库]机器学习库Sklearn详解\n[机器学习篇]基于Python机器学习常用库\n[机器学习]机器学习之Python之NumPy数学库的介绍"}
{"content2":"人工智能的小白资源群，这里有最齐全的干货资源，可以和你一样的入门小白一同交流学习亦可以得到专业技术大神的指导。欢迎入群813416857（AI人工智能学习交流），785685380（机器学习技术交流群），796764800（人工智能技术交流）群好好学习，天天向上。\n转载：自211工科硕士。双控工程毕业\n时也势也，最近因为机缘巧合，下定主意要从项目管理领域跳转到机器学习领域（人工智能/深度学习），为了更快更高效的进入学习状态，我对这一块的培训市场和自学方法也做了一些调研。上一篇（机器学习培训班这么多，你该如何选择？）已经就培训市场情况，做了一个简要说明，今天就自学成才道路进行简要分析。由于我也是刚刚开始这块的学习，难免会有纰漏，请大家批评指正。\n首先，阐明以下个人背景：211工科硕士，双控工程毕业，有一定数学和人工智能基础。\n接下来，我们就细细分析人工智能的自学之路。\n1.前提条件：个人学力（非学历）要求\n很多培训班在进行培训产品推介的时候，都会对学员的学历背景做一定了解，是不是大学生？有没有学过高数？是否了解统计学？甚至有的培训班还会将英语水平作为一个咨询条件。\n在我个人看来，培训班这样做，并非是做样子，而是，在人工智能的进阶之路上，确实有一定的基础知识和硬性条件，概括来讲有以下几点\n（1）数学能力：分为高等数学、线性代数和统计学等领域。\n举个例子，很多机器学习/人工智能培训课的第一堂课，往往会将线性回归。线性回归的求解思路有两种：线性代数矩阵求逆和梯度下降法，前者需要学生知道什么矩阵，向量，转置、求逆等基本知识，后者需要学生了解导数、偏导数，这些概念在高等数学的基础上是，是很难直接领会的，跨专业跨行业自学的同学，如果您的大一大二基础课不错，相信入门应该会轻松很多。\n为什么要讲统计学？这就要细致的谈到机器学习的知识体系，机器学习从监督角度来分，分为监督学习和无监督学习，有很多模型的基础是直接来源统计学，所以有时候大家会看到统计学习这个概念，就是这种原因。有一本比较好的入门书籍推荐给大家《统计学习方法》-李航编写的，通俗易懂，开篇就给大家介绍了：统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的一门学科。书中提到的感知机、k近邻等方法就不一一列举了。\n综合看来，数学方面的基础能力，确确实实是学习机器学习的基础，不可或缺。\n（2）计算机能力：基本的计算机知识和适当的编程能力。\n计算机能力的要求，应该是易于理解的，机器学习，必须是要驱动机器去学习，去对数据进行特征提取，去代替人思考和判断，连驱动机器的基本常识不具备，可能也会在这方面吃一些亏吧。\n问题的关键在于应当如何定义计算机能力？\n人工智能需要计算机编程高手吗？非也，计算机高手固然有优势，但数学建模能力更重要。这里讨论机器学习最低门槛水平，我认为即是具备阅读伪代码的基本水平吧。\n现如今，世界上的机器学习框架非常多，存在有以各种计算机语言为基础的机器学习算法库，最为火爆的就是python，而python就是著名的胶水语言，其编码行文逻辑非常类似人类语言，可以达到快速上手。创新导师李开复曾经说过：我那个时候学机器学习，但是没什么成果，为甚？因为没有这么多技术的支撑，个人能力非常有限，成果有效，但现在不一样，大数据、机器学习的开源项目，总有一款适合你。\n剩下英语能力，说到底还是进阶需求，当学习到一定程度之后，你会发现大家都要从英文里面找材料来学习，这里暂且不讨论。\n2.确定学习目标\n在经过自我判定，是否具备机器学习自学的条件后，我们需要定位自己的学习目标，没有目标就没有计划，没有计划，自学将成为无稽之谈。\n那个人应该如何确定目标了？其实有个很简单方法：上招聘网站，看人工智能相关岗位的岗责要求，岗责要求中对学历、经验、技能、理论功底、都有非常明确的要求，可谓是明码变价，后续，我将会就人工智能岗位做一个大数据分析，发表第一篇纯技术应用型文章。\n简单来讲，我们可以将目标定义为三类：\n（1）初级小白：对人工智能有一定了解的业外人士；\n（2）中级工程师：能够利用机器学习方法投入生产实践的专业人士；\n（3）技术领袖：在机器学习领域具备理论探索能力的神级人士；\n在这里，我们之探讨第二类，因为第二类是常人所能及，第一类相对简单，顺道就可以实现，而第三类那就是博士教授大大们的事情了，与凡人无关了（开个玩笑，只要你愿意干，都可以实现的）。\n3.自学进阶之道\n这一部分将会推荐很多机器学习自学资料，按难易程度排序，供大家参考：\n（1）基础篇\n书籍：\n1.统计机器学习。李航\n2.机器学习。周志华\n视频：\n1.机器学习。斯坦福。吴恩达\n2.Tom Mitchell(CMU)机器学习\n（2）升级篇\n书籍:\n1.机器学习实战\n2.深度学习-AI圣经\n视频：\n1.Learning from Data\n2.機器學習基石\n（3）实战篇\n书籍\n1.Tensorflow实战google深度学习框架\n目前，我个人还在第一阶段，跟着吴恩达的课程和周志华的西瓜书在学习，效果尚可，没有很明显的学习梯度。\n4.想强调的话\n企业在招聘人才的时候，相比于理论能力，更看重实践能力，如果没有机器学习实战的同学，可以有以下几条渠道刷经验：\n（1）加入开源项目（github上找），争取成为contributer\n（2）参加国内外机器学习竞赛，拿名次，国内有天池、国外有kaggle\n总之一句话，就是要多练。\n\n写到最后，还是非常抱歉，未能给大家在机器学习自学上有更多贡献，但是我相信师父领进门修行在个人，当每个人都进入机器学习的自由王国之后，思想的交流与碰撞会更加重要。\n补充，对于学生党来讲，可以发论文，以下是机器学习高水平论文期刊、会议名录，摘自《机器学习》周志华\n机器学习领域\n国际会议：\n国际机器学术会议：ICML\n国际神经信息处理系统会议NIPS\n国际学习理论会议COLT\n区域会议：\n欧洲机器学习会议ECML\n亚洲机器学习会议ACML\n国际期刊\njournal of machine learning research\nmachine learning\n人工智能领域\n重要会议\nIJCAI\nAAAI\n重要期刊\nartificial intelligence\njournal of artificial intelligence research\n数据挖掘领域\n重要会议\nKDD\nICDM\n重要期刊\nACM transactions on knowledge discovery from data\ndata mining and knowledge discovery\n计算机与模式识别\n重要会议\nCVPR\n重要期刊\nIEEE transactions on pattern analysis and machine intelligence\n神经网络领域\n重要期刊\nneural computation\nIEEE transactions on neural networks and learning systems\n统计领域\n重要期刊\nannals of statistics\n国内：\n中国机器学习大会（CCML）两年一次\n机器学习机器应用（MLA）\n2017年12月15日"}
{"content2":"人工智能（AI）的模式识别、机器学习和深度学习\n模式识别（pattern recognition）、机器学习（machine learning）和深度学习（deep learning）代表三种不同的思想流派。模式识别是最古老的（作为一个术语而言，可以说是很过时的）。机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。而深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考后深度学习时代。\n模式识别一开始主要是作为机器学习的代名词，模式学习正在日渐式微没落消亡；机器学习就像是一个真正的冠军一样持续昂首而上；深度学习是个崭新的和快速攀升的领域。\n下面分别叙述。\n1. 模式识别\n要想知道什么叫做模式识别，那就要先了解什么叫做模式。模式指用来说明事物结构的主观理性形式，是从生产经验和生活经验中经过抽象和升华提炼出来的核心知识体系。模式识别指的是对表征事物或现象的各种形式的信息进行处理和分析，从而达到对事物或现象进行描述、辨认、分类和解释的目的。\n2. 机器学习\n不同于模式识别中人类主动去描述某些特征给机器，机器学习可以这样理解：机器从已知的经验数据（样本）中，通过某种特定的方法（算法），自己去寻找提炼（训练/学习）出一些规律（模型）；提炼出的规律就可以用来判断一些未知的事情（预测）。\n也就是说，模式识别和机器学习的区别在于：前者喂给机器的是各种特征描述，从而让机器对未知的事物进行判断；后者喂给机器的是某一事物的海量样本，让机器通过样本来自己发现特征，最后去判断某些未知的事物。\n在机器学习中，机器根据某一事物的海量样本，总结出这一类型事物所具有的普遍规律，总结过程所使用的技能就是我们常说的算法。当足够多的样本使得算法能够总结出一套行之有效的规律后，机器就可以用这些规律对真实世界中的事件做出决策和预测。\n值得一提的是，在机器学习中，尽管电脑可以自行通过样本总结规律，但是依旧需要人工干预来为其提供规律的方向以及维度，例如色彩识别需要统计色彩的RGB或者CMYK值。\n在机器学习领域有着许多不同的流派，不同流派间的算法与建构的模型也是千差万别。最常见的两种模型分别为符号主义所使用的决策树模型和联结主义所使用的神经网络模型，每种又分别有着相应的多种算法。\n在人工智能领域，模式识别虽然已经日渐式微，但是它依旧有其独特的作用。例如在一些简单的色彩识别领域，参数维度相对单一，界定也相对明显，如果用大数据去建模计算，无疑是一种大才小用。闻道有先后，术业有专攻，不同的算法，可以在不同领域发挥各自的效用。\n3. 深度学习\n深度学习强调的是你使用的模型（例如深度卷积多层神经网络），模型中的参数通过从数据中学习获得。然而，深度学习也带来了一些其他需要考虑的问题。因为你面对的是一个高维的模型（即庞大的网络），所以你需要大量的数据（大数据）和强大的运算能力（图形处理器，GPU）才能优化这个模型。\n在深度学习的模型中，受宠爱最多的就是被用在大规模图像识别任务中的卷积神经网络（Convolutional Neural Nets，CNN），简称ConvNets。卷积被广泛用于深度学习（尤其是计算机视觉应用中），而且它的架构往往都是非浅层的。\n参考：\nhttps://baijiahao.baidu.com/s?id=1597159183746168684&wfr=spider&for=pc\nhttps://www.xianjichina.com/news/details_84224.html"}
{"content2":"#目录\n#深度学习框架\n1.Theano & Ecosystem\n2.Torch\nTensorflow\nCaffe\nCNTK\n6.DSSTNE\n7.Speed\n#机器学习框架\n1.sci-kit learn\n2.Apache Mahout\n3.SystemML\n4.Microsoft DMTK\n#Theano 与生态系统\n深度学习领域内的很多学术研究人员依赖于 Theano，这个用 Python 编写的框架可谓是深度学习框架的老祖宗。Theano 像 Numpy 一样，是一个处理多维数组的库。与其他库一起使用，Theano 很适合于数据探索和进行研究。\n在 Theano 之上，已经有很多的开源的深度库建立起来，包括 Keras、Lasagne 和 Blocks。这些库的建立是为了在 Theano 偶尔的非直觉界面上更简单地使用 API。（截止到 2016 年 3 月，另一个与 Theano 相关的库 Pylearn2 可能即将死亡。）\n相反，Deeplearning4j 能在 JVM 语言（比如，Java 和 Scala）下将深度学习带入生产环境中，创造出解决方案。Deeplearning4j 意在以一种可拓展的方式在并行 GPU 或 CPU 上将尽可能多的环节自动化，并能在需要的时候与 Hadoop 和 Spark 进行整合。\n#优缺点\n（+）Python+Numpy\n（+）计算图是很好的抽象\n（+）RNN 完美适配计算图\n（-）原始 Theano 在某种程度上有些低水平\n（+）高层次 wrappers（Keras，Lasange）减轻了这种痛苦\n（-）错误信息没有帮助\n（-）大型模型有较长的编译时间\n（-）比 Torch 更「臃肿」\n（-）对预训练模型支持不佳\n（-）在 AWS 上有很多 bug\n#Torch\nTorch 是一个用 Lua 编写的支持机器学习算法的计算框架。其中的一些版本被 Facebook、Twitter 这样的大型科技公司使用，为内部团队专门化其深度学习平台。Lua 是一种在上世纪 90 年代早期在巴西开发出来的多范式的脚本语言。\nTorch 7 虽然强大，却并未被基于 Python 的学术社区和通用语言为 Java 的企业软件工程师普遍使用。Deeplearning4j 使用 Java 编写，这反映了我们对产业和易用性的关注。我们相信可用性的限制给深度学习的广泛使用带来了阻碍。我们认为 Hadoop 和 Spark 这样的开源分布式应该自动具备可扩展性。我们相信一个商业化支撑下的开源框架是保证工具有效并建立一个社区的合适解决方案。\n#优缺点\n（+）很多容易结合的模块碎片\n（+）易于编写自己的层类型和在 GPU 上运行\n（+）Lua（大部分库代码是 Lua 语言，易于读取）\n（+）大量的预训练模型\n（-）Lua（小众）\n（-）你总是需要编写自己的训练代码（更不能即插即用）\n（-）对循环神经网络不太好\n（-）没有商业化支持\n（-）糟糕的文档支持\n#TensorFlow\n谷歌创造 TensorFlow 取代 Theano，其实这两个库相当类似。Theano 的一些创造者，比如 Ian Goodfellow 在去 OpenAI 之前就是在谷歌打造 TensorFlow。\n目前，TensorFlow 还不支持所谓的「inline」矩阵运算，但会强迫你按序 copy 一个矩阵，并在其上进行运算。copy 大型矩阵非常耗费成本，相比于其他先进的深度学习工具 TensorFlow 要花费 4 倍的时间。谷歌说他们正在解决这个问题。\n像大部分深度学习框架一样，TensorFlow 在 C/C++ 引擎之上使用 Python API 编写，从而加速其运行。对 Java 和 Scala 社区而言，它并非一个合适的解决方案。\nTensorFlow 不只是面向深度学习，也有支持强化学习和其它算法的工具。\n谷歌开放 TensorFlow 的目标看起来是想吸引更多的人，共享他们研究人员的代码，标准化软件工程师进行深度学习的方式，并吸引他人对谷歌云服务的兴趣——TensorFlow 针对谷歌云服务进行了优化。\nTensorFlow 并非商业支持下的，而且看起来谷歌也不可能成为支持开源企业软件的企业。它只为研究者提供新工具。\n如同 Theano，TensorFlow 生成一个计算图（比如一系列矩阵运算，像 z=Simoid（x）, 其中 x 和 z 都是矩阵）并进行自动微分。自动微分很重要，因为每次实验一个新的神经网络的时候，你肯定不想手动编写一个反向传播新变体的代码。在谷歌的生态系统中，计算图后来被 Google Brain 使用进行一些繁重工作，但谷歌还未开源其中一些工具。TensorFlow 只是谷歌内部的深度学习解决方案的一半。\n从企业的角度来看，一些公司需要考虑的是他们是否想依赖谷歌的这些工具。\nCaveat：在 TensorFlow 中的所有运算并不都像在 Numpy 中一样。\n#优缺点\n（+）Python+Numpy\n（+）计算图抽象，如同 Theano\n（+）比 Theano 更快的编译速度\n（+）进行可视化的 TensorBoard\n（+）数据和模型并行\n（-）比其它框架慢\n（-）比 Torch 更「臃肿」；更神奇；\n（-）预训练模型不多\n（-）计算图是纯 Python 的，因此更慢\n（-）无商业化支持\n（-）需要退出到 Python 才能加载每个新的训练 batch\n（-）不能进行太大的调整\n（-）在大型软件项目上，动态键入易出错\n#Caffe\nCaffe 是一个知名的、被普遍使用的机器视觉库，其将 Matlab 的快速卷积网接口迁移到了 C 和 C++ 中。Caffe 不面向其他深度学习应用，比如文本、声音或时序数据。如同其他框架一样，Caffe 选择 Python 作为 API。\nDeeplearning4j 和 Caffe 都能用卷积网络进行图像分类，都展现出了顶尖水平。相比于 Caffe，Deeplearning4j 还提供了任意数量芯片的并行 GPU 支持，以及许多可使得深度学习在多个并行 GPU 集群上运行得更平滑的看起来琐碎的特征。Caffe 主要被用于作为一个托管在其 Model Zoo 网站上的预训练模型的源。Deeplearning4j 正在开发一个能将 Caffe 模型导入到 Spark 的解析器。\n#优缺点：\n（+）在前馈网络和图像处理上较好\n（+）在微调已有网络上较好\n（+）不写任何代码就可训练模型\n（+）Python 接口相当有用\n（-）需要为新的 GPU 层编写 C++/CUDA\n（-）不擅长循环网络\n（-）面对大型网络有点吃力（GoogLeNet，ResNet）\n（-）不可扩展\n（-）无商业化支持\n#CNTK\nCNTK 是微软的开源深度学习框架，是「Computational Network Toolkit（计算网络工具包）」的缩写。这个库包括前馈 DNN、卷积网络和循环网络。CNTK 提供一个 C++ 代码上的 Python API。虽然 CNTK 有一个许可证，但它还未有更多的传统许可，比如 ASF2.0，BSD，或 MIT。\n#DSSTNE\n亚马逊的 Deep Scalable Sparse Tensor Network Engine（DSSTNE）是一个为机器学习、深度学习构建模型的库。它是最近才开源的一个深度学习库，在 TensorFlow 和 CNTK 之后才开源。大部分使用 C++ 编写，DSSTNE 似乎很快，尽管它如今没有其它库那样吸引大量关注。\n#优缺点\n(+) 处理稀疏的编码\n(-) 亚马逊可能不会共享要得到其样本的最好结果所必需的所有信息\n#Speed\nDeeplearning4j 使用 ND4J 执行的线性代数计算展现出了在大型矩阵相乘上的至少比 Numpy 快两倍的速度。这也是为什么我们的 Deeplearning4j 被 NASA 喷气推进实验室里的团队采用的原因之一。此外，Deeplearning4j 在多种芯片上的运行已经被优化，包括 x86、CUDA C 的 GPU。\n尽管 Torch7 和 DL4J 都可并行，但 DL4J 的并行是自动化的。也就是说，我们对工作节点和连接进行了自动化，在 Spark、Hadoop 或者与 Akka 和 AWS 上建立大规模并行的时候，能让用户对库进行分流。Deeplearning4j 最适合于解决特定问题，而且速度很快。\n#机器学习框架\n上面列出的机器学习框架更多是专用框架，而非通用机器学习框架，当然，通用机器学习框架也有很多，下面列出几个主要的：\n#sci-kit learn：\nPython 的默认开源机器学习框架\n#Apache Mahout：\n]Apache 上的旗舰机器学习框架。Mahout 可用来进行分类、聚类和推荐。\n#SystemML：\nIBM 的机器学习框架，可用来执行描述性统计、分类、聚类、回归、矩阵分解和生存分析（Survival Analysis），而且也包含支持向量机。\n#Microsoft DMTK：\n微软的分布式机器学习工具包。分布式词嵌入和 LDA。"}
{"content2":"上面这张图是很多人对人工智能，机器学习和深度学习的描述。\n我一开始对人工智能的理解，就是类似于《Me robot》这种机器人的电影，机器人听从人类指令，并且实现人类的意图，简直太厉害了，但是真正要达到那一步还挺遥远。\n几年前，机器学习其实就开始了，我在7年前，参与了多个光学项目，其中用到的就是机器学习的内容。机器学习，学习的是模板，就是我们给定标准图像，然后由系统进行分析之后，确定目标体是否符合和匹配，进而做出动作。当时做的一个硅片机，就是这样的，使用的是holcon这种系统。\n现在对深度学习了解了一些，深度学习粗浅点就是说有很多个样本学习，机器提炼共性，然后对待识别的目标进行打分。\n机器学习要提高识别率，对光源的要求比较苛刻。深度学习应该对环境的要求方面会好很多。\n欢迎访问我的文章，这里是pdf电子书供参考。"}
{"content2":"开发环境\nwin10\nvs2010 本人习惯vs2010\ncmake 3.7.0\n所需工具\nboost1.4.8\n下载地址 https://ncu.dl.sourceforge.net/project/boost/boost/1.48.0/boost_1_48_0.7z\nshark 3.0.0\n下载地址https://codeload.github.com/Shark-ML/Shark/tar.gz/v3.0.0\n步骤\n1： 安装boost\na. bootstrap.bat\nb. bjam toolset=msvc-10.0 variant=debug,release threading=multi link=static\n(配置成vs2010的库)\nc. b2.exe\n库生成成功，自己先搭建环境测试下\n2： 生成 shark 的环境\na. 修改 hark-3.0.0\\CMakeLists.txt 修改部分如下\n#####################################################################\n# Boost configuration\n#####################################################################\nset(Boost_USE_STATIC_LIBS OFF CACHE BOOL \"use static libraries from Boost\")\nset(Boost_USE_MULTITHREADED ON)\nset(Boost_USE_STATIC_LIBS ON)\nset(CMAKE_INCLUDE_PATH ${CMAKE_INCLUDE_PATH} \"E:/deeplearn/boost_1_48_0/boost_1_48_0\")\nset(CMAKE_LIBRARY_PATH ${CMAKE_LIBRARY_PATH} \"E:/deeplearn/boost_1_48_0/boost_1_48_0/stage/lib\")\nadd_definitions(-DBOOST_PARAMETER_MAX_ARITY=15)\nadd_definitions(-DBOOST_FILESYSTEM_VERSION=3)\nset(BOOST_INCLUDEDIR E:/deeplearn/boost_1_48_0/boost_1_48_0)\nset(BOOST_LIBRARYDIR E:/deeplearn/boost_1_48_0/boost_1_48_0/stage/lib)\nb. 打开cmake, 配置 where is the source code\nwhere to build the binaries\nConfigure vs2010\n点击生成，中间可能会报错误（类似No Boost libraries were found. You may need to set BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT to the location of Boost.） ,不管跳过（现在只要生成vs2010的工程，boost库等会自己配置到vs工程里面就行了）\nc. 打开 build/shark.sln\n配置 包含目录\n库目录\n连接器 输入 附加依赖项\n将shark作为启动项目，生成库\n3： 测试\n启动 example下面的 AckleuES\n点击运行\n测试几个例子，环境搭建成功。"}
{"content2":"在机器学习中我们把数据分为测试数据和训练数据。\n测试数据就是测试集，是用来测试已经训练好的模型的泛化能力。\n训练数据常被划分为训练集（training set）和验证集（validation set），比如在K-折交叉验证中，整个训练数据集D，就被分为K个部分，每次挑选其中的（K-1）部分做训练集，剩下的部分为验证集。\n训练集是用来训练模型或确定模型参数的，如ANN中权值，CNN中的权值等；验证集是用来做模型结构选择，确定模型中的一些超参数，比如正则项系数，CNN各个隐层神经元的个数等；\n以下是维基百科中的解释：\nTraining set: A set of examples used for learning, which is to fit the parameters [i.e., weights] of the classifier.\nValidation set: A set of examples used to tune the parameters [i.e., architecture, not weights] of a classifier, for example to choose the number of hidden units in a neural network.\nTest set: A set of examples used only to assess the performance [generalization] of a fully specified classifier."}
{"content2":"http://antkillerfarm.github.io/\nH\n\\mathcal{H}为无限集的情况\n某些预测函数的参数是实数，它实际上包含了无穷多个数。针对这样的情况，我们可以参照IEEE浮点数的规则，进行离散采样。\nIEEE浮点数由64bit的二进制数构成，因此d个实数参数组成的\nH\n\\mathcal{H}，可组成\nk=264d\nk=2^{64d}个不同的预测函数，因此：\nm≥O(1γ2log264dδ)=O(dγ2log1δ)=Oγ,δ(d)\nm\\ge O\\left(\\frac{1}{\\gamma^2}\\log\\frac{2^{64d}}{\\delta}\\right)=O\\left(\\frac{d}{\\gamma^2}\\log\\frac{1}{\\delta}\\right)=O_{\\gamma,\\delta}(d)\n这里的下标\nγ,δ\n\\gamma,\\delta表示一些依赖于它们的常量。从上式可以看出需要的训练样本的数量和预测模型的参数个数成线性关系。\n以上就是和ERM相关的算法的理论知识，至于其他非ERM算法理论仍在研究之中。\n下面是\nH\n\\mathcal{H}参数化的问题。一个线性分类器可以写为：\nhθ(x)=1{θ0+θ1x1+⋯+θnxn≥0}\nh_\\theta(x)=1\\{\\theta_0+\\theta_1x_1+\\dots+\\theta_nx_n\\ge 0\\}\n这种形式有\nn+1\nn+1个参数。\n但它也可以写为：\nhu,v(x)=1{(u20−v20)+(u20−v20)x1+⋯+(u2n−v2n)xn≥0}\nh_{u,v}(x)=1\\{(u_0^2-v_0^2)+(u_0^2-v_0^2)x_1+\\dots+(u_n^2-v_n^2)x_n\\ge 0\\}\n这种形式有\n2n+2\n2n+2个参数。\n显然这两种形式在数学上是等价的，但参数的个数却不同。为此我们引入Vapnik-Chervonenkis维度（简称VC维度），用以刻画参数的个数。\n如上图所示，3个样本点有以上几种分布方式。毫无疑问，它们都能被\nhθ(x)=1{θ0+θ1x1+θ2x2≥0}\nh_\\theta(x)=1\\{\\theta_0+\\theta_1x_1+\\theta_2x_2\\ge 0\\}所分割，且它们的训练误差为0。但如果是4个点的话，就不能无训练误差的分割了。我们将这种最大分割的个数称作VC维度，这里\nVC(H)=3\nVC(\\mathcal{H})=3。\n需要注意的是，VC维度表征的是模型的最大切割能力，而不是针对所有的情况都成立。比如下图所示的三个点，就不可以被\nhθ(x)=1{θ0+θ1x1+θ2x2≥0}\nh_\\theta(x)=1\\{\\theta_0+\\theta_1x_1+\\theta_2x_2\\ge 0\\}所分割，但这并不影响\nhθ(x)\nh_\\theta(x)的VC维度值。\n如果模型能切割任意大小的样本集，则\nVC(H)=∞\nVC(\\mathcal{H})=\\infty。\n我们将VC维度值替换\nH\n\\mathcal{H}为有限集时的\n∣H∣\n\\lvert\\mathcal{H}\\rvert，可得以下相关结论：\n令\nVC(H)=d\nVC(\\mathcal{H})=d，则：\n∣ε(h)−ε^(h)∣≤O(dmlogmd+1mlog1δ−−−−−−−−−−−−−−−−√)\n\\lvert\\varepsilon(h)-\\hat\\varepsilon(h)\\rvert\\le O\\left(\\sqrt{\\frac{d}{m}\\log\\frac{m}{d}+\\frac{1}{m}\\log\\frac{1}{\\delta}}\\right)\nε(h^)≤ε(h∗)+O(dmlogmd+1mlog1δ−−−−−−−−−−−−−−−−√)\n\\varepsilon(\\hat h)\\le \\varepsilon(h^*)+O\\left(\\sqrt{\\frac{d}{m}\\log\\frac{m}{d}+\\frac{1}{m}\\log\\frac{1}{\\delta}}\\right)\nm=Oγ,δ(d)\nm=O_{\\gamma,\\delta}(d)\n以上公式的其他条件，与\nH\n\\mathcal{H}为有限集时相同，这里不再赘述。\n规则化和模型选择\n对于多项回归模型\nhθ(x)=g(θ0+θ1x1+⋯+θkxk)\nh_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\dots+\\theta_kx_k)来说，如何选择合适的k值呢？\n或者，我们是选择局部权重回归（locally weighted regression），还是SVM呢？\n我们定义算法模型的集合为\nM={M1,…,Md}\n\\mathcal{M}=\\{M_1,\\dots,M_d\\}。其中的\nMi\nM_i为不同的算法模型，比如SVM、神经网络等等。\n交叉验证\n回想之前讨论的过拟合和ERM算法，如果我们针对多项回归模型使用ERM算法，几乎必然会选择高方差的高维多项回归模型，因为它的训练误差最小。但这显然不是个好选择。\n因此，我们改进算法如下：\n1.从全部的训练数据S中随机选择70%的样例作为训练集\nStrain\nS_{train}，剩余的30%作为测试集\nSCV\nS_{CV}。\n2.在\nStrain\nS_{train}上训练每一个\nMi\nM_i，得到预测函数\nhi\nh_i。\n3.在\nSCV\nS_{CV}上测试每一个\nhi\nh_i，得到相应的经验误差\nε^SCV(hi)\n\\hat\\varepsilon_{S_{CV}}(h_i)。\n4.选择具有最小\nε^SCV(hi)\n\\hat\\varepsilon_{S_{CV}}(h_i)的\nhi\nh_i，作为最佳模型。\n这种方法被称为hold-out交叉验证（cross validation），或者称为简单（simple）交叉验证。\n由于\nStrain\nS_{train}和\nSCV\nS_{CV}是随机选取的，因此我们可以认为这里的经验误差\nε^SCV(hi)\n\\hat\\varepsilon_{S_{CV}}(h_i)是\nhi\nh_i的泛化误差的一个很好的估计值。测试集一般占所有样本数的1/4~1/3，这里的30%是一个典型值。\n还可以对模型作改进，当选出最佳的模型\nMi\nM_i后，再在全部数据S上做一次训练，显然训练数据越多，模型参数越准确。\n简单交叉验证方法的缺点在于得到的最佳模型是在70%的训练数据上选出来的，不代表在全部训练数据上是最佳的。还有当训练数据本来就很少时，再分出测试集后，训练数据就太少了。\n我们对简单交叉验证方法再做一次改进，如下：\n1.将全部训练集S分成k个不相交的子集，假设S中的训练样例个数为m，那么每一个子集有m/k个训练样例，相应的子集称作\n{S1,…,Sk}\n\\{S_1,\\dots,S_k\\}。\n2.每次从模型集合\nM\n\\mathcal{M}中拿出来一个\nMi\nM_i，然后在S中选择出k-1个子集\nS1∪⋯∪Sj−1∪Sj+1∪⋯∪Sk\nS_1\\cup\\dots\\cup S_{j-1}\\cup S_{j+1}\\cup\\dots\\cup S_k，在这个集合上训练\nMi\nM_i得到预测函数\nhij\nh_{ij}。在\nSj\nS_j上测试\nhij\nh_{ij}，得到相应的经验误差\nε^Sj(hij)\n\\hat\\varepsilon_{S_j}(h_{ij})。\n3.使用\n1k∑kj=1ε^Sj(hij)\n\\frac{1}{k}\\sum_{j=1}^k\\hat\\varepsilon_{S_j}(h_{ij})作为\nMi\nM_i泛化误差的估计值。\n4.选出泛化误差估计值最小的\nMi\nM_i，在S上重新训练，得到最终的预测函数\nhi\nh_i。\n这个方法被称为k-折叠（k-fold）交叉验证。一般来说k取值为10，这样训练数据稀疏时，基本上也能进行训练，缺点是训练和测试次数过多。\n更极端的，如果\nk=m\nk=m，则该方法又被称为leave-one-out交叉验证。\n特征选择\n特征选择（Feature Selection）严格来说也是模型选择中的一种。\n假设我们想对维度为n的样本点进行回归，如果，n远远大于训练样例数m，且你认为其中只有很少的特征起关键作用的话，就可以对整个特征集进行特征选择，以减少特征的数量。\n对于n个特征的\nM\n\\mathcal{M}来说，根据特征是否包含在最终结果中，可以写出\n2n\n2^n个不同的\nMi\nM_i。直接使用上面的交叉验证方法，计算量过大。这时可以采用如下启发式算法：\n1.初始化特征集\nF=∅\n\\mathcal{F}=\\emptyset。\n2.Repeat {\n(a)for 特征i=1 to n, {\n如果\ni∉F\ni\\notin\\mathcal{F}，则\nFi=F∪{i}\n\\mathcal{F}_i=\\mathcal{F}\\cup\\{i\\}。\n在\nFi\n\\mathcal{F}_i上使用交叉验证方法评估它的泛化误差。\n}\n(b)将第(a)步中最优的\nFi\n\\mathcal{F}_i设为新的\nF\n\\mathcal{F}。\n}\n3.选择并输出搜索过程中得到的最优子集。\n这个算法被称为前向搜索（forward search）。其外部循环的终止条件为\n∣F∣\n\\lvert\\mathcal{F}\\rvert达到n或者事先设定的门限值。\n前向搜索属于wrapper model特征选择方法的一种。 Wrapper这里指不断地使用不同的特征子集来测试学习的算法。\n除了前向搜索之外，还有后向搜索（backward search）算法。它和前者的区别在于，它的初始集合为全集，然后每次删除一个特征，并评价，直到\n∣F∣\n\\lvert\\mathcal{F}\\rvert达到阈值或者为空，然后选择最佳的\nF\n\\mathcal{F}即可。\n可以看出无论前向搜索，还是后向搜索，其算法复杂度都是\nO(n2)\nO(n^2)。\nKL散度\nKL散度（Kullback–Leibler divergence）是两个随机分布间距离的度量。其定义如下：\nDKL(P∥Q)=∑iP(i)logP(i)Q(i)\nD_{KL}(P\\|Q)=\\sum_iP(i)\\log\\frac{P(i)}{Q(i)}\n其中，P和Q是离散概率分布，\nP(i)\nP(i)和\nQ(i)\nQ(i)是相应分布的概率密度函数。如果P和Q是连续随机变量的话，将上式中的累加符号换成积分符号即可。\n但KL散度并不是真正的度量（metric）。它既不满足三角不等式(两边之和\n≥\n\\ge第三边)，也不满足对称性（即\nDKL(P∥Q)≠DKL(Q∥P)\nD_{KL}(P\\|Q)\\neq D_{KL}(Q\\|P)）。\n注：Solomon Kullback，1907～1994，美国数学家和密码学家。乔治·华盛顿大学博士。NSA首任首席科学家。二战期间，参与破解德国的Enigma机器。\nRichard Leibler，1914～2003，美国数学家和密码学家。伊利诺伊大学博士。NSA高级主管，入选NSA名人堂。\n过滤特征选择方法\n过滤特征选择（Filter feature selection）方法，是另一种启发式的特征选择算法，计算量比较小。它的思路是计算特征\nxi\nx_i和类别标签y之间的相关度的评分\nS(i)\nS(i)。\n可以使用\nxi\nx_i和y之间的互信息量（mutual information），作为评分依据。\nMI(xi,y)=∑xi∈Xi∑y∈Yp(xi,y)logp(xi,y)p(xi)p(y)\nMI(x_i,y)=\\sum_{x_i\\in X_i}\\sum_{y\\in Y}p(x_i,y)\\log\\frac{p(x_i,y)}{p(x_i)p(y)}\n其中，\np(xi,y)\np(x_i,y)是\nxi\nx_i和y的联合概率密度，\np(xi)\np(x_i)和\np(y)\np(y)是相应的边缘概率密度。\n和KL散度类似，如果x和y是连续随机变量的话，将上式中的累加符号换成积分符号即可。\nMI也可以用KL散度来表示：\nMI(xi,y)=KL(p(xi,y)∥p(xi)p(y))\nMI(x_i,y)=KL(p(x_i,y)\\|p(x_i)p(y))\n过滤特征选择方法的算法复杂度为\nO(n)\nO(n)。\n最后一个问题，选择多少个特征合适呢？按照\nS(i)\nS(i)从高到低的顺序，依次选择1到n个特征进行交叉验证，直到效果达到预期为止。\n贝叶斯统计和规则化\n前面提到最大似然（maximum likelihood）估计方法的公式如下：\nθML=argmaxθ∏i=1mp(y(i)|x(i);θ)\n\\theta_{ML}=\\arg\\max_\\theta\\prod_{i=1}^mp(y^{(i)}\\vert x^{(i)};\\theta)\n从频率统计（frequentist statistic）学派的观点来看，这里的\nθ\n\\theta是一个未知的常数，我们的任务就是求出这个常数。然而从贝叶斯学派的观点来看，\nθ\n\\theta是一个未知的随机变量。\n也就是说似然函数，对于前者来说，是这样的：\n∏mi=1p(y(i)|x(i);θ)\n\\prod_{i=1}^mp(y^{(i)}\\vert x^{(i)};\\theta)；但对于后者来说，却是这样的：\n∏mi=1p(y(i)|x(i),θ)\n\\prod_{i=1}^mp(y^{(i)}\\vert x^{(i)},\\theta)\n我们首先假定\nθ\n\\theta\n的分布为\n的分布为\np(θ)\np(\\theta)，这种假定由于没有事实根据，通常被称作先验分布（prior distribution）。\n我们针对训练集\nS={(x(i),y(i))}mi=1\nS=\\{(x^{(i)},y^{(i)})\\}_{i=1}^m，训练得到预测函数。并按照如下公式计算后验分布（posterior distribution）：\np(θ|S)=p(S|θ)p(θ)p(S)(1)\np(\\theta\\vert S)=\\frac{p(S\\vert\\theta)p(\\theta)}{p(S)}\\tag{1}\n由全概率公式可得：\np(S)=p(S|θ1)p(θ1)+⋯+p(S|θn)p(θn)\np(S)=p(S\\vert\\theta_1)p(\\theta_1)+\\dots+p(S\\vert\\theta_n)p(\\theta_n)\n上式的\nθi\n\\theta_i表示\nθ\n\\theta的各个取值区间，然而由于\nθ\n\\theta是连续随机变量，根据微积分原理可得：\np(S)=∫θp(S|θ)p(θ)dθ(2)\np(S)=\\int_\\theta p(S\\vert\\theta)p(\\theta)\\mathrm{d}\\theta\\tag{2}\n将公式2代入公式1可得：\np(θ|S)=p(S|θ)p(θ)∫θp(S|θ)p(θ)dθ=(∏mi=1p(y(i)|x(i),θ))p(θ)∫θ(∏mi=1p(y(i)|x(i),θ))p(θ)dθ\np(\\theta\\vert S)=\\frac{p(S\\vert\\theta)p(\\theta)}{\\int_\\theta p(S\\vert\\theta)p(\\theta)\\mathrm{d}\\theta}=\\frac{\\left(\\prod_{i=1}^mp(y^{(i)}\\vert x^{(i)},\\theta)\\right)p(\\theta)}{\\int_\\theta\\left(\\prod_{i=1}^mp(y^{(i)}\\vert x^{(i)},\\theta)\\right)p(\\theta)\\mathrm{d}\\theta}\n当我们针对新的样本x进行预测时，和上面的推导类似，可得：\np(y|x,S)=∫θp(y|x,θ,S)p(θ|S)dθ\np(y\\vert x,S)=\\int_\\theta p(y\\vert x,\\theta,S)p(\\theta\\vert S)\\mathrm{d}\\theta\n因为预测样本集和训练样本集的分布是独立的，因此上式又可写为：\np(y|x,S)=∫θp(y|x,θ)p(θ|S)dθ\np(y\\vert x,S)=\\int_\\theta p(y\\vert x,\\theta)p(\\theta\\vert S)\\mathrm{d}\\theta\n这个公式又被称作后验预测分布（Posterior predictive distribution）。"}
{"content2":"1.机器学习开发流程\n2.机器学习算法分类\n3.机器学习模型是什么？\n机器学习开发流程\n机器学习算法分类\n监督学习（特征值+目标值）\n监督学习（英语：Supervised learning），可以由输入数据中学\n到或建立一个模型，并依此模式推测新的结果。输入数据是由\n输入特征值和目标值所组成。函数的输出可以是一个连续的值\n（称为回归），或是输出是有限个离散值（称作分类）。\n分类 k-近邻算法、贝叶斯分类、决策树与 随机森林、逻辑回归、神经网络 回归 线性回归、岭回归 标注 隐马尔可夫模型 (不做要求)\n无监督学习（特征值）\n无监督学习（英语：Supervised learning），可以由输入数据中\n学到或建立一个模型，并依此模式推测新的结果。输入数据是\n由输入特征值所组成。\n聚类 k-means\n分类\n概念：分类是监督学习的一个核心问题，在监督学习中，当输出变量取有限个离散值时，预测问题变成为分类问题。最基础的便是二分类问题，即判断是非，从两个类别中选择一个作为预测结果；\n回归\n概念：回归是监督学习的另一个重要问题。回归用于预测输入变量和输出变量之间的关系，输出是连续型的值。"}
{"content2":"人工智能的浪潮正在席卷全球，诸多词汇时刻萦绕在我们耳边：人工智能（Artificial Intelligence）、机器学习（Machine Learning）、深度学习（Deep Learning）。不少人对这些高频词汇的含义及其背后的关系总是似懂非懂、一知半解。\n为了帮助大家更好地理解人工智能，这篇文章用最简单的语言解释了这些词汇的含义，理清它们之间的关系，希望对大家有所帮助。\n人工智能（Artificial Intelligence）\n人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门技术科学。“人工智能”是“一门技术科学”，它研究与开发的对象是“理论、技术及应用系统”，研究的目的是为了“模拟、延伸和扩展人的智能”。我们现在看到的貌似很高端的技术，如图像识别、NLP，其实依然没有脱离这个范围，就是“模拟人在看图方面的智能”和“模拟人在听话方面的智能”，本质上和“模拟人在计算方面的智能”没啥两样，虽然难度有高低，但目的是一样的——模拟、延伸和扩展人的智能。另外，人工智能在50年代就提出了。\n但目前的科研工作都集中在弱人工智能这部分，并很有希望在近期取得重大突破，电影里的人工智能多半都是在描绘强人工智能，而这部分在目前的现实世界里难以真正实现（通常将人工智能分为弱人工智能和强人工智能，前者让机器具备观察和感知的能力，可以做到一定程度的理解和推理，而强人工智能让机器获得自适应能力，解决一些之前没有遇到过的问题）。\n弱人工智能有希望取得突破，是如何实现的，“智能”又从何而来呢？这主要归功于一种实现人工智能的方法——机器学习。\n机器学习（Machine Learning）\n随着人对计算机科学的期望越来越高，要求它解决的问题越来越复杂，已经远远不能满足人们的诉求了。于是有人提出了一个新的思路——能否不为难码农，让机器自己去学习呢？\n机器学习就是用算法解析数据，不断学习，对世界中发生的事做出判断和预测的一项技术。研究人员不会亲手编写软件、确定特殊指令集、然后让程序完成特殊任务；相反，研究人员会用大量数据和算法“训练”机器，让机器学会如何执行任务。这里有三个重要的信息：\n1、“机器学习”是“模拟、延伸和扩展人的智能”的一条路径，所以是人工智能的一个子集；\n2、“机器学习”是要基于大量数据的，也就是说它的“智能”是用大量数据喂出来的；\n3、正是因为要处理海量数据，所以大数据技术尤为重要；“机器学习”只是大数据技术上的一个应用。常用的10大机器学习算法有：决策树、随机森林、逻辑回归、SVM、朴素贝叶斯、K最近邻算法、K均值算法、Adaboost算法、神经网络、马尔科夫。\n举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。\n深度学习（Deep Learning）\n相较而言，深度学习是一个比较新的概念，严格地说是2006年提出的。深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术。它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和自然语言处理(NLP)领域。显然，“深度学习”是与机器学习中的“神经网络”是强相关，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。深度学习又分为卷积神经网络（Convolutional neural networks，简称CNN）和深度置信网（Deep Belief Nets，简称DBN）。其主要的思想就是模拟人的神经元，每个神经元接受到信息，处理完后传递给与之相邻的所有神经元即可。所以看起来的处理方式有点像下图（想深入了解的同学可以自行google）。\n神经网络的计算量非常大，事实上在很长时间里由于基础设施技术的限制进展并不大。而GPU的出现让人看到了曙光，也造就了深度学习的蓬勃发展，“深度学习”才一下子火热起来。击败李世石的Alpha go即是深度学习的一个很好的示例。Google的TensorFlow是开源深度学习系统一个比较好的实现，支持CNN、RNN和LSTM算法，是目前在图像识别、自然语言处理方面最流行的深度神经网络模型。事实上，提出“深度学习”概念的Hinton教授加入了google，而Alpha go也是google家的。\n总结：\n人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU。"}
{"content2":"机器学习是什么？\n机器学习是什么？这个问题的答案可以参考权威的机器学习定义，但是实际上，机器学习是由它所解决的问题定义的。因此，理解机器学习最好的方式是观察一些实例。\n首先来看一些现实生活中众所周知和理解的机器学习问题的实例，然后讨论标准的机器学习问题的分类（命名系统），学习如何辨别一个问题是属于哪种标准案例。这样做的意义是，了解所面对的问题类型，我们就可以思考所需要的数据和可尝试的算法。\n机器学习问题的十个实例\n机器学习问题到处都是，它们组成了日常使用的网络或桌面软件的核心或困难部分。推特上“想来试试吗”的建议和苹果的 Siri 语音理解系统就是实例。\n以下，是十个真正有关机器学习到底是什么的的实例：\n垃圾邮件检测：根据邮箱中的邮件，识别哪些是垃圾邮件，哪些不是。这样的模型，可以程序帮助归类垃圾邮件和非垃圾邮件。这个例子，我们应该都不陌生。\n信用卡欺诈检测：\n根据用户一个月内的信用卡交易，识别哪些交易是该用户操作的，哪些不是。这样的决策模型，可以帮助程序退还那些欺诈交易。\n数字识别：\n根据信封上手写的邮编，识别出每一个手写字符所代表的数字。这样的模型，可以帮助程序阅读和理解手写邮编，并根据地利位置分类信件。\n语音识别：\n从一个用户的话语，确定用户提出的具体要求。这样的模型，可以帮助程序能够并尝试自动填充用户需求。带有 Siri 系统的 iPhone 就有这种功能。\n人脸识别：\n根据相册中的众多数码照片，识别出那些包含某一个人的照片。这样的决策模型，可以帮助程序根据人脸管理照片。某些相机或软件，如 iPhoto，就有这种功能。\n产品推荐：\n根据一个用户的购物记录和冗长的收藏清单，识别出这其中哪些是该用户真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助程序为客户提供建议并鼓励产品消费。登录 Facebook 或 GooglePlus，它们就会推荐可能有关联的用户给你。\n医学分析：\n根据病人的症状和一个匿名的病人资料数据库，预测该病人可能患了什么病。这样的决策模型，可以程序为专业医疗人士提供支持。\n股票交易：\n根据一支股票现有的和以往的价格波动，判断这支股票是该建仓、持仓还是减仓。这样的决策模型，可以帮助程序为金融分析提供支持。\n客户细分：\n根据用户在试用期的的行为模式和所有用户过去的行为，识别出哪些用户会转变成该产品的付款用户，哪些不会。这样的决策模型，可以帮助程序进行用户干预，以说服用户早些付款使用或更好的参与产品试用。\n形状鉴定：\n根据用户在触摸屏幕上的手绘和一个已知的形状资料库，判断用户想描绘的形状。这样的决策模型，可以帮助程序显示该形状的理想版本，以绘制清晰的图像。iPhone 应用 Instaviz 就能做到这样。\n这十个实例展示了一个机器学习问题是什么样的很好的理念。有一个专门的文集记录那些有着历史意义的例子。其中一个例子是，一个需要建模的决策，为该决策有效地的自动建模为某一行业或者说领域带来了利益。\n有些问题是人工智能中，如自然语言处理和机器视觉（处理人们很容易处理的问题），最困难的问题。其他一些也很困难，但它们同时是很经典的机器学习问题，如垃圾邮件检测和信用卡欺诈检测。\n想想你在过去的一周中跟线上或线下的软件之间的交互。你肯定能很轻易的推测出十或二十个直接或间接使用的机器学习实例。\n问题的类型\n通过上述的机器学习问题的实例，你一定已经意识到一些相似性之处。这种技能很有价值，因为擅长从现象看本质，使得你可以高效的思考需要的数据和可尝试的算法类型。\n关于机器学习，有一些常见的分类。以下这些分类，是我们在研究机器学习时碰到的大多问题都会参考的典型。\n分类：\n标记数据，也就是将它归入某一类，如垃圾/非垃圾（邮件）或欺诈/非欺诈（信用卡交易）。决策建模是为了标记新的未标记的数据项。这可以看做是辨别问题，为小组之间的差异性或相似性建模。\n回归：\n数据被标记以真实的值（如浮点数）而不是一个标签。简单易懂的例子如时序数据，如随着时间波动的股票价格。这个建模的的决策是为新的未预测的数据估计值。\n聚类：\n不标记数据，但是可根据相似性，以及其他的对数据中自然结构的衡量对数据进行分组。可以从以上十个例子清单中举出一例：根据人脸，而不是名字，来管理照片。这样，用户就不得不为分组命名，如 Mac 上的 iPhoto。\n规则提取：\n数据被用作对提议规则（前提/结果，又名如果）进行提取的基础。这些规则，可能但不都是有指向的，意思是说，这些方法可以找出数据的属性之间在统计学上有说服力的关系，但不都是必要的涉及到需要预测的东西。有一个找出买啤酒还是买尿布之间关系的例子，（这是数据挖掘的民间条例，真实与否，都阐述了期望和机会）。\n当你认为一个问题是机器学习问题时（如需要从数据中建模的决策问题），接着思考下什么问题类型可以直接借用，或者，用户或需求期待什么样的结果，反过来也这样做。"}
{"content2":"机器学习定义\nMachine Learning definition\nflyfish 笔记\nwhat is machine learing?什么是机器学习？\n1959 Arthur Samuel \nfield of study that gives computes the ability to learn without being explicitly programmed.\n1959年Arthure Samuel将机器学习非正式的定义为：在不直接针对问题进行编程的情况下赋予计算机学习\n能力的一个研究领域\nTom mitchell(1998) Well-posed Learning\nProblem:A compute program is said to learn from experience E with respect to same task T\nand some performance measure P,if its performance on T,as measured by P,\nimproves with experience E。\n更现代更正式的定义\n给它一个任务T和一个性能测量方法P，如果在经验E的影响下，P对T的测量结果得到了改进，\n那么说该程序从E中学习\nrespectn. 尊敬，尊重；方面；敬意\nvt. 尊敬，尊重；遵守"}
{"content2":"地理学3s技术的迅速发展，使得地理信息系统越来越趋向多元化，在大数据、人工智能物联网等信息技术的冲击下，也使得地理学面临新的发展前景和应用前景。本人最近专门研究深度学习算法就遥感影像分类研究和智能识别，有同行或者新手们可以一起学习，互相交流。安利一个微博：大数据与人工智能CCNU\n上面有不少你需要的东西。作为深度学习算法入门和关注AI动态。大家可以关注下。"}
{"content2":"#前言\n主要有以下原因:\n##1. Python是解释语言，程序写起来非常方便\n写程序方便对做机器学习的人很重要。\n因为经常需要对模型进行各种各样的修改，这在编译语言里很可能是牵一发而动全身的事情，Python里通常可以用很少的时间实现。\n举例来说，在C等编译语言里写一个矩阵乘法，需要自己分配操作数（矩阵）的内存、分配结果的内存、手动对BLAS接口调用gemm、最后如果没用smart pointer还得手动回收内存空间。Python几乎就是import numpy; numpy.dot两句话的事。\n当然现在很多面向C/C++库已经支持托管的内存管理了，这也让开发过程容易了很多，但解释语言仍然有天生的优势——不需要编译时间。这对机器学习这种需要大量prototyping和迭代的研究方向是非常有益工作效率的。\n##2. Python的开发生态成熟，有很多有用的库可以用\n除了上面说到的NumPy，还有SciPy、NLTK、os（自带）等等不一而足。Python灵活的语法还使得包括文本操作、list/dict comprehension等非常实用的功能非常容易高效实现（编写和运行效率都高），配合lambda等使用更是方便。这也是Python良性生态背后的一大原因。相比而言，Lua虽然也是解释语言，甚至有LuaJIT这种神器加持，但其本身很难做到Python这样，一是因为有Python这个前辈占领着市场份额，另一个也因为它本身种种反常识的设计（比如全局变量）。不过借着Lua-Python bridge和Torch的东风，Lua似乎也在寄生兴起。\n##3. Python的效率很高。\n解释语言的发展已经大大超过许多人的想象。很多比如list comprehension的语法糖都是贴近内核实现的。除了JIT[1]之外，还有Cython可以大幅增加运行效率。最后，得益于Python对C的接口，很多像gnumpy, theano这样高效、Python接口友好的库可以加速程序的运行，在强大团队的支撑下，这些库的效率可能比一个不熟练的程序员用C写一个月调优的效率还要高。\n##4.数据存储方便\n有sql,hadoop,mangodb,redis,spark等\n##5.数据获取方便\n有Scrapy,beautifulsoup,requests,paramiko等\n##6.数据运算方便\n有pandas，Numpy，scipy等\n##7.输出结果方便\n有matplotlib,VisPy等\n##8.和其他语言交互方便\n有ctypes,rpy2,Cython,SWIG,PyQt,boost.python\n##9.加速方便\n有pypy,Cython,PyCUDA\n##10.图形图像方便\n有PyOpenGL,PyOpenCV,mayavi2\n##11.信号处理方便\nPyWavelets，scipy.signal\n##12.云系统支持方便\ngithub,sourceforge,EC2,BAT,HPC\n##13.python开源\npython支持的平台多，包括windows,linux,unix,macos。而matlab太贵，只能调用其api，用python省钱，省钱就是赚钱。\n#python 和 c++ 做个比较。\nc++ 的cpu效率是远远高于 python 的.不过 python 是一门胶水语言，它可以和任何语言结合，基于这个优点，很多数据处理的python 库底层都是 c++ 实现的，意思就是说：你用python写code，但效率是c++的。只有那些for 循环，还是用python的效率高。\n近年来机器学习最要是深度学习，而深度学习使用cuda gpu加速远比cpu要快，而cuda 是c++写的。\n###所以现在TensorLayer、theano 等深度学习库都是 python 编程、底层c++。"}
{"content2":"这篇学习文章是在上一篇博客（http://blog.csdn.net/july_sun/article/details/53088673）的基础上，从机器学习的四要素（数据，算法和模型，计算机硬件，机器学习平台）角度出发用实例将各个分类器做一比较，下面就开始这段代码的奇妙旅程吧~~\n第一：计算机硬件\n本例中只要普通的64位 win系统即可，使用的python是W64的Anaconda，这个python平台的好处是已安装好经常使用的ML包，如sklearn和数据处理包，如numpy和scipy.\n第二：机器学习平台\n首先，本例中用到了数据处理包numpy对数据进行预处理，当然也可以直接使用scipy包，如果调用scipy就可以直接使用numpy，因为scipy是基于numpy的.\n其次，在机器学习模型训练和参数选择时使用万能的sklearn库，里面包含了机器学习的最广泛使用的算法\n第三：数据\nstep1：数据收集\n网上有大量的免费数据和文本可以拿来使用，也可以自己简单生成或者是用sklearn的datasets的数据集，亦或是sklearn的make_regression来生成纯数据以及带有噪声的数据，本例中使用的是中国气象数据网2015年的上海的气象数据来让计算机学习上海这一年中的气温和一些因素的关系，即监督学习，其中Y是气温.\n代码如下：\nimport numpy as np W = ['C:\\\\Users\\\\123\\\\Desktop\\\\weather\\\\2015.txt',] weather = np. loadtxt ( W [0] , skiprows =1) weather[: ,7 ] = weather[: ,7 ] / 10plt . figure () plt . axis ([0 , 370 , -5, 40]) plt . ylabel (\" Temperatures\" ) plt . xlabel (\" Month\" ) plt . xticks ([30 , 58, 89, 119 , 150 , 180 ,211 , 242 , 272 , 303 , 333 , 364], ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'] ) plt . title (' Temperatures trent chart of Shanghai in year 2015 ') plt . plot ( np.arange(len(weather)),weather[: , 7 ],label=2015,marker='*',linestyle='-',color=\"black\") plt . legend() plt . show()\n可视化如下：\nstep2：数据清洗\n首先要看一下数据是否是完整的，如果不完整（可能缺失或者不对齐等）就需要清洗数据，这一步对后续模型的选择以及结果很重要，因为这篇文章重点在机器学习分类器的比较，所以选择了数据比较完整的气象数据~~\n第四：算法和模型\nstep1：特征工程（feature selection）\nY是气温，X是6个与气温相关的参量，为了预测准确，要排除X之间的相关性，找到独立的与Y 相关的X，用到了相关系数来对比\n代码如下：\nnp. random . shuffle ( weather ) z = (weather [:, 7], weather [: ,5] , weather [: ,10], weather [: ,11] , weather [: ,13] ,weather [: ,16] , weather [: ,21]) z = np. transpose (z) cor = np. corrcoef (z, rowvar =0) np.savetxt(\"C:\\\\Users\\\\123\\\\Desktop\\\\weather\\\\mydata.csv\",cor,delimiter=\",\")\n由相关系数找到和气温最相关的四个变量作为输入x\nstep2：CV交叉验证来看一下不同分类器会不会overfitting，以Random Forest分类器为例\n代码如下：\nfrom sklearn.ensemble import RandomForestRegressor from sklearn.learning_curve import validation_curve np. random . shuffle ( weather ) y = weatherall [:, 7] x = np. vstack ((weatherall [:, 10], weatherall [:, 11],weatherall [:, 14], weatherall [:, 21])) x = np. transpose (x) x = preprocessing . scale (x) RF = RandomForestRegressor() train_loss,test_loss = validation_curve(RF,x,y,,cv=10,scoring='mean_squared_error'.train_sizes=[0.1,0.25,0.5,0.75,1]) train_loss_mean = -np.mean(train_loss,axis=1) test_loss_mean = -np.mean(test_loss,axis=1) plt.figure() plt.plot(param_range,train_loss_mean,'o-',c='r',label='Training') plt.plot(param_range,test_loss_mean,'o-',c='b',label='Cross_validation') plt.xlabel('Training exapmles') plt.ylabel('Loss') plt.legend(loc='best') plt.title(\"10 CV on Random Forest \") plt.show()\n可视化如下：\n从可视化结果看，training过程中训练集合验证集的loss一直下降，说明能计算机能一直很好的学到知识，不会有overfitting的问题，换用其他几个模型（lr/DT/SVM/KNN/SGD/GB）也是同样的效果，这可能和数据有直接的关系.\nstep3：各个分类器的比较（残差，错误率），以GradientBoost为例，其他的LInear Regression,SGD,KNN,SVM,DT和集成算法RF,ET都是类似的方法\n代码如下：\ngb = GradientBoostingRegressor() y_gb = gb . fit ( x_train , y_train).predict ( x_test ) print \"Residual sum of Gradient Boosting Regression squares is\", np. mean(( y_gb - y_test ) ** 2)\n得到各个分类器的结果如下：\n从error看出，在Sklearn默认参数的前提下，集成算法的错误率是最低的.\n将Gradient Boost的训练集,测试集和预测集可视化：可以看出来预测结和之前可视化的2015年的气温跟分布趋势还是比较吻合的.\n\n以上就是机器学习的基本的流程以及模型选择以及结果预测，总结如下：\n1.data和feature比算法和模型重要，data和feature已经决定了误差上界，模型和算法只是逼近这一上界的手段\n2.sklearn的各种算法的调参对错误率结果也会产生影响，在默认参数的情况下，准确率的的排列顺序是集成算法>SVM>DT>其他，但是也可以通过优化参数来提高各分类器的准确率\n3.在机器学习过程中,overfitting是一定要注意的，通过cv_可以看出来分类器过拟合的情况\n4.总体而言，集成分类器比单个的机器学习分类器的效果要好，这是集成学习将多个个体分类器联合起来预测结果，分而治之，弱弱变强的思想.\n5.在实际应用中要视具体问题以及自己的硬件等条件以及期望达到的精度来选择，毕竟每一种分类器都不是万能的，都有其优缺点（博客http://blog.csdn.net/july_sun/article/details/53088673已提过），适合自己的才是最好的"}
{"content2":"机器学习领域\n最重要的国际学术会议是\n①国际机器学习会议(ICML)\n②国际神经信息处理系统会议(NIPS)\n③国际学习理论会议(COLT)\n重要的区域性会议有\n①欧洲机器学习会议(ECML)\n②亚洲机器学习会议(ACML)\n最重要的国际学术期刊是\n①Journal of Machine Learning Research\n②Machine Learning\n人工智能领域\n重要会议如\n①IJCAI\n②AAAI\n重要期刊\n①Artificial Intelligence\n②Journal of Artificial Intelligence Research\n数据挖掘领域\n重要会议\n①KDD\n②ICDM\n重要期刊\n①ACM Transactions on Knowledge Discovery from data\n②Data Mining and Knowledge Discovery\n计算机视觉与模式识别领域\n重要会议\n①CVPR\n重要期刊\n①IEEE Transactions on Pattern Analysis and machine Intelligence\n神经网络领域\n重要期刊\n①Neural Computation.\n②IEEE mmactions on Neural Networks and Learning Systems\n统计学领域\n重要期刊\n①Annals ofStatistics\n更多内容访问omegaxyz.com\n摘自周志华《机器学习》"}
{"content2":"对“机器学习”跃跃欲试的你，可能也有这样的问题：入门机器学习，我需要会那种（些）酷炫的编程语言呢？别问了，这个问题的“正解”可能会让你大吃一惊。\n不论你选择哪种语言，只要对这种语言下的机器学习库和工具足够熟悉，语言本身就没有那么重要了。现在对应各种语言的机器学习库层出不穷。根据你在公司中担任的角色和所要完成的任务不同，某些语言和工具可能会比其他的更好用。\nR\nR 是一种为专统计计算而设计的语言。它在大规模的数据挖掘、可视化和报告方面已经取得了巨大的成功。你能够轻松地获取各种的包（通过 CRAN）来使用几乎所有的机器学习算法、统计测试和分析等。R 语言本身有着优美（虽然有些人会觉得晦涩）的语法用来表达数据的关系、变换和并行操作。\nKDNuggets 最近组织了一次投票，结果表明 R 是 2015 年用于解决分析、挖掘及其他数据科学任务的最受欢迎的语言。不过，近年来 Python 的人气也在急剧上升。\nMATLAB\nMATLAB 在学术界很受欢迎，因为它能处理复杂的数学表达式，对代数和微积分有强大的支持，还支持符号运算。同时，从数字信号处理到计算生物学，又或者是其他的科目，它都有对应的工具箱可用。它经常被用于开发新的机器学习算法的原型，有时也会被用于开发最终完整的工具。它的商用许可的确非常昂贵，但也对得起它在研发方面带来的方便。Octave 是一款免费的 MATLAB 替代品。它的语法与 MATLAB 几乎相同，但只提供一部分工具箱，IDE 也略微逊色。\nPython\n虽然 Python 是一种更通用的编程语言和脚本语言，但它在数据科学家和机器学习工程师中的人气也是急剧上升。跟 R 和 MATLAB 不同，它并没有内置数据处理和科学计算专用的语法，但它有像 NumPy、SciPy 和 Pandas 这样的库用更友好的语法提供了同样的功能。\n像 scikit-learn、Theano 和 TensorFlow 这样的机器学习库让你能够方便地训练各种机器学习模型，还能用上分布式计算。当然，这些库中最影响性能的部分一般还是用 C/C++ 甚至 Fortan 编写的，而 Python 包则是作为它们的接口（这在 R 中也很常见）。\n但 Python 最大的优势在于它的生态系统使得你能够很方便地搭建起一个复杂的端到端服务，比如用 Django 或是 Flask 搭建 Web 应用，或是用 PyQt 开发桌面应用，甚至用 ROS 搭建一个自主机器人。\n这种强大的通用性也是我们在“机器学习工程师”纳米学位项目中大量使用 Python 的主要原因。\nJava\n由于它干净一致的实现、面向对象编程的风格以及通过 JVM 获得的平台独立性，Java 是很多软件工程师的首选语言。它以简洁性和灵活性为代价换取了明确性和可靠性，使它在实现重要的企业软件系统时非常受欢迎。对于那些一直使用 Java 的公司，当他们需要开发机器学习产品时，为了保持同样的可靠性和避免写一堆混乱的接口，他们可能更倾向于继续使用 Java。\n除了一些可以用来做分析和原型开发的库和工具（比如 Weka）以外，要用 Java 开发大规模分布式的机器学习系统，我们有很多选择，比如 Spark+MLlib、Mahout、H2O 以及 Deeplearning4j。这些库和框架也很方便跟工业级别的数据处理和存储系统比如 Hadoop/HDFS 整合在一起。\nC/C++\n在开发像操作系统组件和网络协议这样计算性能和内存使用效率极为重要的底层软件时，C/C++ 是最理想的选择。由于同样的原因，它们在实现机器学习算法的关键部分时也很受欢迎。但是它们没有内建的关于数据处理操作的抽象，内存管理的任务也很繁重，这使得它们对新手来说不太合适，而且在开发完整的端到端系统时也比较笨重。\n在开发嵌入式系统（比如智能汽车、智能传感器等），可能必须使用 C/C++。而在其他场景下，用它们进行开发的便利程度可能要视现有的基础设施和具体的应用而定。不过无论如何，我们并不缺少 C/C++ 的机器学习库，比如 LibSVM、Shark 和 mlpack。\n企业级解决方案\n在这些语言和库以外，还有很多统计建模和商业分析的商用产品，用于在更为受控的数据处理环境下应用机器学习模型。这些产品，包括 RapidMiner、IBM SPSS、SAS+JMP 和 Stata 等，它们的目标是为数据分析提供可靠和端到端的解决方案，通常也会将 API 或是脚本语法暴露给用户。\n近年来还涌现出了一批“机器学习即服务”平台，比如 Amazon Machine Learning、Google Prediction、DataRobot、IBM Watson 和 Microsoft Azure Machine Learning 等。它们使你能够拓展你的机器学习产品的规模，处理巨量的数据以及快速地对不同的模型进行试验。只要你在机器学习领域打好了坚实的基础，使用这些平台就像学习使用一门新工具一样简单。\n特别提示\n选择语言和库时要注意考虑开发时间和实际性能之间的平衡。一条能够飞速处理数据的流水线如果需要几个月来开发，在有些情况下这可能就毫无用处。一般来说，能够快速地搭建和测试原型会更重要，因为一开始你肯定会失败。\n这也是为什么大部分公司都想寻找那种已经有惯用的工具 / 语言 / 库的机器学习工程师。工业界往往会用 Python 和 R 这样的高层次语言来开发算法的原型，然后用 Java 或是 C/C++ 来开发生产级别的解决方案。\n这篇文章对你有所启发？不如分享到朋友圈，让更多伙伴看到！\n▲ 本文由优达学城（Udacity）原创，作者 Arpan Chakraborty，原标题 Languages and Libraries for Machine Learning"}
{"content2":"为了训练模型，我们需要一种可降低模型损失的好方法。迭代方法是一种广泛用于降低损失的方法。\n一、迭代方法：一种迭代试错，优化模型的方法\n机器学习算法用于训练模型的迭代试错（迭代方法）过程：\n简单来说，迭代方法就是将模型预测值与实际值之间的误差值反馈给模型，让模型不断改进，误差值会越来越小，即模型预测的会越来越精确。\n在训练机器学习模型时，首先对权重和偏差进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。\n学习链接：\nhttps://developers.google.cn/machine-learning/crash-course/reducing-loss/an-iterative-approach\n二、梯度下降法：一种快速找到损失函数收敛点的方法\n回归问题所产生的\n损失与权重值的图形\n（损失曲线）始终是凸形。\n凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。\n这个最小值就是损失函数收敛之处。\n梯度下降法：\n首先为权重值（w1）选择一个起始值（起点）。\n起点并不重要；因此很多算法就直接将 w1设为 0 或随机选择一个值。\n然后，梯度下降法算法会计算损失曲线在起点处的梯度。\n梯度是偏导数相对于所有自变量的矢量；它可以让你了解哪个方向距离目标“更近”或“更远”。损失相对于单个权重的梯度就等于导数。\n梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。（梯度下降法依赖于负梯度）\n为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：\n一个梯度步长将我们移动到损失曲线上的下一个点。\n重复此过程，逐渐接近最低点。\n三、学习速率：用来确定每一步的“步幅”\n梯度下降法算法用梯度乘以一个称为\n学习速率\n（有时也称为\n步长\n）的标量，以确定下一个点的位置。\n例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。\n超参数是机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。\n如果选择的学习速率过小，就会花费太长的学习时间，如下图所示：\n相反，如果指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，很可能会越过最低点。如下图所示：\n学习速率刚刚好。每个回归问题都存在一个金发姑娘学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。\n优化学习速率练习：\nhttps://developers.google.cn/machine-learning/crash-course/fitter/graph\n四、随机梯度下降法（SGD）\n三个概念：全批量梯度下降法、小批量随机梯度下降法、随机梯度下降法\n在梯度下降法中，批量指的是\n用于在单次迭代中计算梯度的样本总数\n。到目前为止，我们一直假定批量是指整个数据集。如果数据集太大（即批量过于巨大），则单次迭代就可能要花费很长时间进行计算。SGD就是用来解决此问题的。\n如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。“随机”表示构成各个批量的一个样本都是随机选择的。\n简单来说，SGD就是从大的数据集中随机选择样本来得到一个小得多的数据集，用它来估算出正确的平均梯度值。\n小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。\nPlayground 练习：\nhttps://developers.google.cn/machine-learning/crash-course/reducing-loss/playground-exercise\n神经网络练习网站：\nhttp://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.24585&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"}
{"content2":"人工智能的浪潮正在席卷全球，诸多词汇时刻萦绕在我们耳边：人工智能（Artificial Intelligence）、机器学习（Machine Learning）、深度学习（Deep Learning）。不少人对这些高频词汇的含义及其背后的关系总是似懂非懂、一知半解。\n为了帮助大家更好地理解人工智能，这篇文章用最简单的语言解释了这些词汇的含义，理清它们之间的关系，希望对刚入门的同行有所帮助。\n人工智能：从概念提出到走向繁荣\n1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念，梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言，或被当成技术疯子的狂想扔到垃圾堆里。直到2012年之前，这两种声音还在同时存在。\n2012年以后，得益于数据量的上涨、运算力的提升和机器学习新算法（深度学习）的出现，人工智能开始大爆发。据领英近日发布的《全球AI领域人才报告》显示，截至2017年一季度，基于领英平台的全球AI（人工智能）领域技术人才数量超过190万，仅国内人工智能人才缺口达到500多万。\n人工智能的研究领域也在不断扩大，图一展示了人工智能研究的各个分支，包括专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等。\n图一　人工智能研究分支\n但目前的科研工作都集中在弱人工智能这部分，并很有希望在近期取得重大突破，电影里的人工智能多半都是在描绘强人工智能，而这部分在目前的现实世界里难以真正实现（通常将人工智能分为弱人工智能和强人工智能，前者让机器具备观察和感知的能力，可以做到一定程度的理解和推理，而强人工智能让机器获得自适应能力，解决一些之前没有遇到过的问题）。\n弱人工智能有希望取得突破，是如何实现的，“智能”又从何而来呢？这主要归功于一种实现人工智能的方法——机器学习。\n机器学习：一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。\n机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。\n深度学习：一种实现机器学习的技术\n深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络），因此越来越多的人将其单独看作一种学习的方法。\n最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。其实有不少想法早年间也曾有过，但由于当时训练数据量不足、计算能力落后，因此最终的效果不尽如人意。\n深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。\n三者的区别和联系\n机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术。我们就用最简单的方法——同心圆，可视化地展现出它们三者的关系。\n图二　三者关系示意图\n目前，业界有一种错误的较为普遍的意识，即“深度学习最终可能会淘汰掉其他所有机器学习算法”。这种意识的产生主要是因为，当下深度学习在计算机视觉、自然语言处理领域的应用远超过传统的机器学习方法，并且媒体对深度学习进行了大肆夸大的报道。\n深度学习，作为目前最热的机器学习方法，但并不意味着是机器学习的终点。起码目前存在以下问题：\n1. 深度学习模型需要大量的训练数据，才能展现出神奇的效果，但现实生活中往往会遇到小样本问题，此时深度学习方法无法入手，传统的机器学习方法就可以处理；\n2. 有些领域，采用传统的简单的机器学习方法，可以很好地解决了，没必要非得用复杂的深度学习方法；\n3. 深度学习的思想，来源于人脑的启发，但绝不是人脑的模拟，举个例子，给一个三四岁的小孩看一辆自行车之后，再见到哪怕外观完全不同的自行车，小孩也十有八九能做出那是一辆自行车的判断，也就是说，人类的学习过程往往不需要大规模的训练数据，而现在的深度学习方法显然不是对人脑的模拟。\n深度学习大佬 Yoshua Bengio 在 Quora 上回答一个类似的问题时，有一段话讲得特别好，这里引用一下，以回答上述问题：\nScience is NOT a battle, it is a collaboration. We all build on each other's ideas. Science is an act of love, not war. Love for the beauty in the world that surrounds us and love to share and build something together. That makes science a highly satisfying activity, emotionally speaking!\n这段话的大致意思是，科学不是战争而是合作，任何学科的发展从来都不是一条路走到黑，而是同行之间互相学习、互相借鉴、博采众长、相得益彰，站在巨人的肩膀上不断前行。机器学习的研究也是一样，你死我活那是邪教，开放包容才是正道。\n结合机器学习2000年以来的发展，再来看Bengio的这段话，深有感触。进入21世纪，纵观机器学习发展历程，研究热点可以简单总结为2000-2006年的流形学习、2006年-2011年的稀疏学习、2012年至今的深度学习。未来哪种机器学习算法会成为热点呢？深度学习三大巨头之一吴恩达曾表示，“在继深度学习之后，迁移学习将引领下一波机器学习技术”。但最终机器学习的下一个热点是什么，谁又能说得准呢。\n掌握传统的机器学习算法，夯实人工智能基础，才能让我们在热点来临时，及时把握住机会。为此，中科院自动化所科研一线青年教师，与深蓝学院联合推出《机器学习主流算法：从理论到实践》在线直播课程。课程体系设置充分结合理论与Python代码实践，PPT以及代码均会提供给学员，并为学员搭建与课程讲师即时交流的微信群。\n机器学习课程内容\n1. 数学基础（PPT资料+视频资料）\n1.1 矩阵论、概率论、优化基础知识\n2. 机器学习算法：从理论到实践（20学时）\n2.1 机器学习概述\n2.1.1 机器学习方法分类\n2.1.2 基础知识介绍\n2.2 KNN算法\n2.2.1 概述与理论详解\n2.2.2 K近邻应用案例及代码实现\n2.1 机器学习概述\n2.1.1 机器学习方法分类\n2.1.2 基础知识介绍\n2.2 KNN算法\n2.2.1 概述与理论详解\n2.2.2 K近邻应用案例及代码实现\n2.3 贝叶斯分类\n2.3.1 朴素贝叶斯\n2.3.2 贝叶斯决策论\n2.3.3 NB分类算法应用案例及代码实现\n2.4 回归与分类\n2.4.1 曲线拟合\n2.4.2 线性回归\n2.4.3 logistic回归\n2.4.4 相关应用案例及代码实现\n2.5 支持向量机\n2.5.1 线性支持向量机\n2.5.2 非线性支持向量机\n2.5.3 核方法\n2.5.4 SVM应用案例及代码实现\n2.6 聚类算法\n2.6.1 K均值聚类\n2.6.2 层次聚类\n2.6.3 聚类算法应用案例及代码实现\n2.7 数据降维\n2.7.1 线性降维\n2.7.2 非线性降维\n2.7.3 降维应用案例及代码实现\n2.8 EM算法\n2.8.1 EM算法基础\n2.8.2 多高斯参数估计\n2.8.3 EM应用案例及代码实现\n2.9 Adaboost算法\n2.9.1 独立于算法的机器学习\n2.9.2 Adaboost算法\n2.9.3 应用案例及代码实现\n2.10 隐马尔科夫模型\n2.10.1 马尔科夫\n2.10.2 隐马尔科夫模型\n2.10.3 应用案例及代码实现\n课程团队\n汪老师，中科院自动化所一线科研学者，副研究员，在领域顶级会议期刊 ICCV、TNNLS、TIP等发表论文20多篇；参加全国视频图像分析技术挑战赛，获得目标检测识别第二名，熟练掌握并应用深度学习Keras框架和Caffe框架。\n邵老师，现任副教授、硕士生导师，中科院自动化所博士毕业，具有两年教学授课经验。主要研究方向包括机器学习、模式识别与图像处理，作为项目负责人承担国家自然科学基金，在国际主流期刊和会议上发表论文数篇。\n霍老师，一线青年学者，西安电子科技大学博士毕业，研究方向为图像处理，主持和参与多项国家自然科学基金，以第一作者在相关领域期刊以及会议上发表论文多篇。\n开课时间及形式\n1. 机器学习课程价格为 499 元，11月30日-12月21日每周四、周末晚，在线直播授课；\n2. 报名即送数学基础视频资料；\n3. 课程一年内可实时查看视频回放；\n4. 课程PPT和源程序，会提前公开给学员；\n5. 课前、课中和课后，微信群均可答疑。\n请添加助教微信咨询"}
{"content2":"严格意义上说，人工智能和机器学习没有直接关系，只不过是机器学习的方法被大量的应用于解决人工智能的问题而已。目前机器学习是人工智能的一种实现方式，也是最重要的实现方式。\n深度学习是机器学习比较火的一个方向，其本身是神经网络算法的衍生，在图像、语音等富媒体的分类和识别上取得了非常好的效果。\n数据挖掘主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。\n机器学习是数据挖掘的一种重要方法，但机器学习是另一门学科，并不从属于数据挖掘，二者相辅相成。\n深度学习、机器学习的发展带了许多实际的商业应用，让虚幻的AI逐步落地，进而影响人类社会发展；\n深度学习、机器学习以及未来的AI技术，将让无人驾驶汽车、更好的预防性治疗技术、更发达智能的疾病治疗诊断系统、更好的人类生活娱乐辅助推荐系统等，逐步融入人类社会的方方面面。\nAI即使是现在，也是未来，不再是一种科幻影像和概念，业界变成了人类社会当下的一种存在，不管人类是否喜欢或者理解，他们都将革命性地改变创造AI的我们人类自身。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n大数据时代，主要需要什么类型的人才？\nhttp://www.duozhishidai.com/article-1554-1.html\n对于大数据开发的学习，最经典的学习路线是什么？\nhttp://www.duozhishidai.com/article-1544-1.html\n人工智能时代，AI人才都有哪些特征？\nhttp://www.duozhishidai.com/article-1792-1.html\n大数据在各行业的应用和趋势\nhttp://www.duozhishidai.com/article-477-1.html\n大数据人才定义和分类\nhttp://www.duozhishidai.com/article-405-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"机器学习H2O AI框架简介\n1.  H2O框架\n优势：自己实现分布式计算框架，算法种类全，有深度学习算法，同时可以通过Sparkling-water将 h2o 和spark 进行完美整合\na.底层数据层\n底层数据读取Hdfs数据  s3数据  SQL 数据  noSQL数据\ns3Amazon Simple Storage Service 亚马逊的云存储结构\nHive其实就是读取HDFS\nnoSQL：HBase\nHQL是一种类SQL语言，这种语言最终被转化为Map/Reduce.虽然Hive提供了SQL查询功能，但是Hive不能够进行交互查询--因为它只能够在Haoop上批量的执行Hadoop 便利全部数据速度慢 老版本不支持更新操作\nHbase的能够在它的数据库上实时运行，而不是运行MapReduce任务在Hbase中，行是key/value映射的集合，这个映射通过row-key来唯一标识。Hbase利用Hadoop的基础设施，可以利用通用的设备进行水平的扩展。\nHive可以用来进行统计查询，HBase可以用来进行实时查询\nb. 计算引擎层\nhadoop  spark\n在hadoop中使用mapreduce 所起的任务只是Map任务\n使用sparkling-water将H2O和Spark进行整合\nc. 核心运算层\n算法引擎：深度学习\n计算引擎：基于内存的MapReduce 使用分布式的fork/join框架（java并行框架） 内存管理引擎 ：采用列式压缩ColumnarCompression\nd．接口层\nSDK和 REST API\n快速查询R引擎  毫秒级评分引擎\nh2o-3/h2o-docs/src/dev/lifecycle.md\nH2OApp vs. H2OClientApp\nThe main class for Standalone H2O isH2OApp.（class）\nH2OApp uses a helper class calledH2OStarter（class）\nH2O.configureLogging();\nH2O.registerExtensions();\n// Fire up the H2O Cluster\nH2O.main(args);\nH2O.registerRestApis(relativeResourcePath);\nH2O.finalizeRegistration();\nwater is from h2o-core and hex is fromh2o-algos.\ne  核心组件\nMRTask ：Map/Reduce styledistributed computation\n里边有各种mapreduce方法\n(Dtask–TAICountedCompleted – countedCompleted – fork/jointask)\nChunk : ChunkType  ChunkName\n2.  H2O中的数据结构\nFrame   Frame are only composed of Vecs of the sameVectorGroup\nVec    是由多个Chunck组成 可以并行计算mapreduce  MRtask\nnewMRTask{} { final double _mean = vec.mean();\npublicvoid map( Chunk chk ) {\nfor(int row=0; row &lt; chk._len; row++ )\nif(chk.isNA(row) ) chk.set(row,_mean);\n}\n}.doAll(vec);\nChunck  1000 –100 0000个element  one cpu\nKey\nDKV  分布式Key/values存储\nAST : AbstractSyntax Tree"}
{"content2":"欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~\n本文由liuxuewen 发表于云+社区专栏\n在本文中，我们将研究深度学习和机器学习之间的差异。我们将逐一了解它们，然后讨论他们在各个方面的不同之处。除了深度学习和机器学习的比较外，我们还将研究他们未来的趋势和走向。\n深度学习 VS 机器学习\n深度学习与机器学习简介\n一、什么是机器学习？\n通常，为了实现人工智能，我们使用机器学习。我们有几种算法用于机器学习。例如：\nFind-S算法\n决策树算法（Decision trees）\n随机森林算法（Random forests）\n人工神经网络\n通常，有3种类型的学习算法：\n1，监督机器学习算法用于进行预测。此外，该算法搜索分配给数据点的值标签内的模式。\n2，无监督机器学习算法：没有标签与数据点相关联。这些ML算法将数据组织成一组簇。此外，它需要描述其结构，使复杂的数据看起来简单，有条理，便于分析。\n3，增强机器学习算法：我们使用这些算法来选择动作。此外，我们可以看到它基于每个数据点。一段时间后，算法改变其策略以更好地学习。\n二、什么是深度学习？\n机器学习只关注解决现实问题。它还需要更加智能的一些想法。机器学习通过旨在模仿人类决策能力的神经网络。ML工具和技术是关键的两个深度学习的窄子集，我们需要用他们来解决需要思考的问题。任何深度神经网络都将包含三种类型的图层：\n输入层\n隐藏层\n输出层\n我们可以说深度学习是机器学习领域的最新领域。这是实现机器学习的一种方式。\n深度学习与机器学习\n我们使用机器算法来解析数据，从数据中学习，并根据所学知识做出明智的决策。基本上，深度学习用于创建人工“神经网络” ，可以自己学习和做出明智的决策。我们可以说深度学习是机器学习的一个子领域。\n机器学习与深度学习的比较\n数据依赖性\n性能是两种算法之间的主要关键区别。虽然，当数据很小时，深度学习算法表现不佳。这就是是深度学习算法需要大量数据才能完美理解的原因。\n但是，在这种情况下，我们可以看到算法的使用以及他们手工制作的规则。上图总结了这一事实。\n硬件依赖\n通常，深度学习依赖于高端机器，而传统学习依赖于低端机器。因此，深度学习要求包括GPU。这是它工作中不可或缺的一部分。它们还进行大量的矩阵乘法运算。\n特色工程\n这是一个普遍的过程。在此，领域知识被用于创建特征提取器，以降低数据的复杂性，并使模式更加可见以学习算法的工作。虽然，处理起来非常困难。因此，这是需要非常多的专业知识和时间。\n解决问题的方法\n通常，我们使用传统算法来解决问题。但是，它需要将问题分解为不同的部分以单独解决它们。要获得结果，请将它们全部组合起来。\n例如：\n让我们假设你有一个多对象检测的任务。在此任务中，我们必须确定对象是什么以及它在图像中的位置。在机器学习方法中，我们必须将问题分为两个步骤：\n1.物体检测\n2.物体识别\n首先，我们使用抓取算法浏览图像并找到所有可能的对象。然后，在所有已识别的对象中，你将使用像SVM和HOG这样的对象识别算法来识别相关对象。\n执行时间处理时间\n通常，与机器学习相比，深度学习需要更多时间进行训练。主要原因是深度学习算法中有太多参数。机器学习只花需要更少的时间进行训练。\n解释性\n我们将可解释性作为比较两种学习技巧的因素。尽管如此，深度学习在用于工业之前仍然被认为是10次。\n机器学习和深度学习在哪里应用？\n计算机视觉： 我们将其用于车牌识别和面部识别等不同应用。\n信息检索： 我们将ML和DL用于搜索引擎，文本搜索和图像搜索等应用程序。\n营销：我们在自动电子邮件营销和目标识别中使用这种学习技术。\n医疗诊断：它在医学领域也有广泛的应用。癌症鉴定和异常检测等应用。\n自然语言处理：适用于情感分析，照片标签，在线广告等应用。\n未来的趋势\n如今，机器学习和数据科学正处于趋势中。在公司中，对它们的需求正在迅速增加。对于希望在其业务中集成机器学习而生存的公司而言，他们的需求尤其大。\n深度学习被发现，并证明拥有最先进的表演技术。因此，深度学习让我们感到惊讶，并将在不久的将来继续这样做。\n最近，研究人员不断探索机器学习和深度学习。过去，研究人员仅限于学术界。但是，如今，机器学习和深度学习的研究正在两个行业和学术界中占据一席之地。\n结论\n我们研究了深度学习和机器学习，并研究了两者之间的比较。我们还研究了图像，以便更好地表达和理解。如果你有任何疑问，可以随时在评论部分询问。\n原文标题《Machine Learning vs. Deep Learning》，\n作者:Shailna Patidar\n译者：谢子乔\n不代表云加社区观点，更多详情请查看原文链接\n问答\n深度学习在腾讯云上有哪些应用？\n相关阅读\n人工智能与机器学习有哪些不同\n一文看懂自然语言处理（NLP）的深度学习发展史和待解难题\n基于tensorflow的视觉问答系统构建\n【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识\n此文已由作者授权腾讯云+社区发布，更多原文请点击\n搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！\n海量技术实践经验，尽在云加社区！"}
{"content2":"原文：Artificial Intelligence And Machine Learning: What’s The Difference?\n作者：Tobi Bamidele\n译者：聂震坤\n审校：屠敏\n现如今，人工智能与机器学习受到了各行各业的广泛关注，大众对其态度不一。但是人工智能与机器学习对不同的人来说其代表的东西并不相同。通常人工智能与机器学习会引起人们的恐惧与不确定性，因此一些专家对这两个术语相关的风险表示担忧。\n事实上，人工智能与机器学习已经成为了我们日常生活中不可分割的一部分，即使在有些方面并没有多少人察觉到。这些科技极大的影响了人与人之间的交流。多亏了 Siri 和 Cortana，现在人们只需要对设备说话便可以搜索网页。另外，Facebook 的用户现在可以根据以前的访问记录获得更有针对性的新闻推送。\n引用 Prismtech 公司的话：“科技公司在研究人工智能时都会进行大量的市场调查。”除了对移动设备有很大影响外，人工智能的应用也可以在物联网中看到。他们的关系很紧密，因为人工智能需要物联网提供的数据来增加他的准确性。人工智能正在成为业务的主流，人们对它越来越感兴趣并想将其应用于各项业务中。\n图片来源：Nvidia\n上图展示了人工智能，机器学习与深度学习之间的关系。尽管人工智能与机器学习类似，但是还有有细微的区别。这便引出了问题：人工智能与机器学习的主要区别是什么？\n人工智能\n简单的来说，人工智能旨在让机器获得像人类一样思考的能力。该过程涉及开发能够更好更有效地执行传统上由人类完成的任务的计算机系统。Alan Turing 于 1950 年的报纸上率先提出机器人能否像人类一样思考的问题，此问题后来引出了著名的图灵测试。\n人工智能这个概念一点也不新鲜，早在 1956 年，John McCarthy 便在一篇学术论文上提出了这个观点。然而人们对此话题的兴趣很快就消散了，直到最近又重新回到公众视野。今天，由于大数据和云计算，AI 正以前所未有的速度发展，这也使得存储大量数据变得简单。\n用专家的话来说，人工智能有不同的形式：狭义人工智能与一般人工智能。目前我们利用狭隘人工智能，可以比人类更好地执行一系列基本任务，但是在其他任务方面也存在缺陷。例如，一台可以完美基于网络信息给消费者提供建议的机器，在别的领域什么也干不了。\n对于一般人工智能，通用人工智能（AGI）将其定义为“旨在构建思维机器的新兴领域;也就是说，具有与人类思维的完善的智能系统（可能最终远远超出人类一般智力）。“\n机器学习\n关于机器学习的定义有很多，有些甚至很难理解。Arthur Samuel 将机器学习定义为“使计算机拥有在没有被明确编程的情况下学习的能力。”机器学习的概念涉及到训练机器的过程。让计算机通过特定算法在大量的数据中学习。这种行为与数据挖掘类似，但是机器学习会根据其学习内容来改变自己的学习模式。\n机器学习在公司数据处理中扮演了一个很重要的角色。机器学习也是目前最接近人工智能的系统。因此，可以在没有机器学习的情况下创建人工智能，但是这个过程将会是复杂耗时的。\n人工神经网络是一种拟人数据处理模式。它是深度学习的基础，深度学习是一种使用深度神经网络进行大量数据构建的系统。人工智能，机器学习与深度学习都离不开大数据。\n人工智能的未来在于深度学习，因为它已经使很多机器学习的应用成为了可能。一个很好的例子便是使用深度学习进行图片识别，其表现在一定程度上已经超过了人类。我们已经在一个科技逐步迎合人类需求的时代。尽管目前仍有一部分专家对其表示担忧，并对其安全性展开研究。从处理一些危险的工作到掌握治疗绝症的办法，人工智能的未来拥有无限的可能性。\n由中国人工智能学会、阿里巴巴集团 & 蚂蚁金服主办，CSDN、中国科学院自动化研究所承办的第三届中国人工智能大会（CCAI 2017）将于 7 月 22-23 日在杭州召开。作为中国国内高规格、规模空前的人工智能大会，本次大会由中国科学院院士、中国人工智能学会副理事长谭铁牛，阿里巴巴技术委员会主席王坚，香港科技大学计算机系主任、AAAI Fellow 杨强，蚂蚁金服副总裁、首席数据科学家漆远，南京大学教授、AAAI Fellow 周志华共同甄选出在人工智能领域本年度海内外最值得关注的学术与研发进展，汇聚了超过 40 位顶级人工智能专家，带来 9 场权威主题报告，以及“语言智能与应用论坛”、“智能金融论坛”、“人工智能科学与艺术论坛”、“人工智能青年论坛”4 大专题论坛，届时将有超过 2000 位人工智能专业人士参与。\n目前，大会 8 折优惠门票正在火热发售中，扫描下方图片中的二维码或直接点击链接火速抢票。"}
{"content2":"2016是人工智能爆发的一年，各种层出不穷的新技术、新概念让人眼花缭乱。很多人都分不清人工智能（Artificial Intelligence，简称AI）、机器学习（Machine Learning，简称ML）以及深度学习（Deep Learning，简称DL）概念之间的不同。\n本文重点解释机器学习和深度学习的差别。\n由于AI的大热，媒体上关于AI的文章狂轰乱炸，人工智能似乎已经成为游戏的改变者，企业们也纷纷下注。\n对于AI领域的从业者来说，人工智能、机器学习和深度学习之间的差别应该非常清楚。人工智能是一个大概念，从有效的老式人工智能（GOFAI）到联结主义结构，无所不包。\n机器学习则是人工智能领域的一个小分支，如果说AI是一个合集，那么ML就是AI的子集。\n任何通过数据训练的学习算法的相关研究都属于机器学习，包括很多已经发展多年的技术，比如线性回归（Linear Regression）、K均值（K-means，基于原型的目标函数聚类方法）、决策树（Decision Trees，运用概率分析的一种图解法）、随机森林（Random Forest，运用概率分析的一种图解法）、PCA（Principal Component Analysis，主成分分析）、SVM（Support Vector Machine，支持向量机）以及ANN（Artificial Neural Networks，人工神经网络）。\n人工神经网络则是深度学习的起源。\n一些之前接触过人工神经网络的机器学习从业者对深度学习的第一印象很可能是：这不过就是多层结构的人工神经网络而已。此外，深度学习成功的主要原因是大量可用的数据以及像GPU这样更强大的计算引擎的出现。\n这当然是事实，深度学习的出现基本要归因于这两方面的进展。但是，如果就此下结论说深度学习不过是比支持向量机或者决策树更好的算法而已，那就真的是一叶障目，不见泰山了。\n借用Andreesen的话“软件正在占领全世界”，那么深度学习就正在取代机器学习。两篇来自不同机器学习领域的从业者很好的解释了为什么深度学习正在占领全世界。神经语言程序学（NLP）的专家Chris Manning这样形容“深度学习海啸”：\n深度学习的浪潮在几年前就已经抵达计算语言学的海岸，但是2015似乎是这场海啸全面冲击各大自然语言处理（NLP）会议的一年。一些专家预言，最终的冲击将会更大。\nNicholas Paragios则写了一篇名为“计算机视觉研究：大萧条”的文章，以下是文章节选：\n在高度复杂以及很大程度由图片的自由度决定的问题上，深度学习一旦被赋予大量被标记的数据以及不可想象（直到最近）的计算能力，就能解决所有的计算机视觉问题。如果是这样的话，那么深度学习占领业界，计算机视觉研究成为边缘学科并走上计算机图形的老路（学术研究的活跃度和数量）将只是时间问题。\n这两篇文章都强调了深度学习相对机器学习是有颠覆性的意义的。当然，深度学习在商用领域也具备同样的颠覆性。但是让人震惊和困惑的是，就连Gartner也没能分清机器学习和深度学习之间的差别。这里是Gartner于2016年8月份发布的发展规律周期图（Hyper Cycle），深度学习甚至没有被提及：\n尽管被Gartner忽视了，深度学习依然持续火热。目前对深度学习的炒作主要是：我们已经拥有了可以商用的机器，只要给它们足够多的数据和足够长的时间，它们就能够自己学习。这要么是夸大了深度学习的现有技术水平，要么就是将深度学习的实践过于简化了。在过去的几年里，深度学习产生了大量的想法和技术，这些在以前要么是未知的，要么是站不住脚的。起初，这些概念是碎片化而且毫无关联的，但是随着时间的推移，大量的模式和方法开始涌现，深度学习设计模式这一领域也变得热闹起来。\n今天的深度学习不仅仅是具备多层架构的感知器，而是一系列能够用来构建可组合可微分的体系结构的技术和方法。\n这些具有超强能力的机器学习系统只不过是我们目前所能看到的冰山一角。关键在于，虽然深度学习目前看起来像点金术，但是总有一天我们会学会如何像操控化学一样操控它。有了这个基础，我们将能够更好的预测机器学习未来所能具备的能力。"}
{"content2":"tensorflow是机器学习的重要框架。而机器学习也是热门中的技术。unity开发者从事着不同的行业。游戏。仿真。展示。ar。vr。而机器学习在unity的应用仅仅限于官方的ml-agents。并不了解它的实际运行原理。其实unity并不是仅仅只能用官方的代理学习框架。还能自己训练模型。自己应用模型。由于网上相关的资料不多。我就趟着坑把tensorflow的应用做一做。\n本场 Chat 首先会带领大家入门 tensorflowsharp的框架，然后解释需要什么样的模型文件能进行使用。里面所用到的数据类型。本场 Chat 您将学到如下内容：\n安装下载tensorflow的c#版本\n解释python和C#的区别。\n模型的初步加载\n模型的重要参数\n获取模型运动完的结果\n简单案例讲解\nchat订阅"}
{"content2":"一些人类通过直觉可以很快解决的问题，目前却很难通过计算机解决。这些问题包括自然言语理解、图像识别、语音识别等等，它们就是人工智能需要解决的问题。\n深度学习是机器学习的一个分支，它除了可以学习特征和任务之间的关联以外，还能自动从简单特征中提取更加复杂的特征。深度学习算法可以从数据中学习更加复杂的特征表达，使得最后一步权重学习变得更加简单且有效。深度学习可以一层一层的将简单的特征逐步转化成更加复杂的特征，从而使得不同类别的图像更加可分。深度学习与传统的机器学习区别如图；\n人工智能、机器学习和深度学习是非常相关的几个领域。下图总结了他们之间的关系。人工智能是人类非常广泛的问题，机器学习是解决这类问题的一个重要手段。深度学习则是机器学习的一个分支，在很多人工智能问题上，深度学习的方法突破了传统机器学习方法的瓶颈，推动了人工智能的发展。"}
{"content2":"声明：\n参考用Sckit-Learn和Pandas学习线性回归\n入门机器学习仅仅靠这一篇文章还是有些不够，建议大家戳一戳文中的链接，看一下相关的知识。\n从简单的线性回归入门机器学习\n获取数据定义问题\n整理数据\n用pandas来读取数据\n准备运行算法的数据\n划分训练集和测试集\n运行scikit-learn的线性模型\n评价模型\n调优\n画图观察结果\n总结\n从简单的线性回归入门机器学习\n虽然本文从一开始就限定了机器学习的方法——线性回归，但是在大多数情况下，解决问题的主要难点在于寻找合适的机器学习方法上。而这方面需要长期的积累，所以显然不是本文要讲的内容了。\n下面，本文将用面向过程的方式分解解决线性回归问题的步骤（每一个章节标题都是一个步骤），以此来帮助大家对机器学习有一个初步的了解。\n获取数据，定义问题\n没有数据，当然没法研究机器学习啦。:) 这里我们用UCI大学公开的机器学习数据来跑线性回归。\n数据的介绍戳戳戳\n数据的下载地址戳戳戳\n里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）,\nPE（输出电力)。我们不用纠结于每项具体的意思。\n我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:\nPE=θ0+θ1∗AT+θ2∗V+θ3∗AP+θ4∗RH\nPE=θ_0+θ_1*AT+θ_2*V+θ_3*AP+θ_4*RH 而需要学习的，就是\nθ0\nθ_0、\nθ1\nθ_1、\nθ2\nθ_2、\nθ3\nθ_3、\nθ4\nθ_4这5个参数。\n整理数据\n下载后的数据可以发现是一个压缩文件，解压后可以看到里面有一个xlsx文件，我们先用excel把它打开，接着“另存为”csv格式（只需要保存一个sheet就可以了），保存下来，后面我们就用这个csv来运行线性回归。\nps: 本文另存为了“ccpp.csv”。转成csv格式是为了读取更方便。\n打开这个csv可以发现数据已经整理好，没有\n非法数据\n，因此不需要做预处理。但是这些数据并没有归一化，也就是转化为均值0，方差1的格式。也不用我们搞，后面scikit-learn在线性回归时会先帮我们把归一化搞定。\n好了，有了这个csv格式的数据，我们就可以大干一场了。\n所谓非法数据，主要是指空值，一般在机器学习或者深度学习中都不接受空值，需要删除或者插值处理。\n用pandas来读取数据\n推荐使用的一个交互式编程工具：jupyter notebook\n读取数据到变量data中\nimport pandas as pd import numpy as np data = pd.read_csv(r'./ccpp.csv', header=0)\n查看data的信息\ndata.head() # 前五行的信息\ndata.shape # data的长和宽 '''输出''' # (9568, 5)\n可以看到，data是一个pandas中的DataFrame类型，其列名是5个变量，索引是0, 1, 2…9567\n准备运行算法的数据\n我们的目的是通过对前四个变量AT, V, AP, RH建立一个线性关系来预测RE，即对于函数y=f(x)，机器通过输入x和输出y，找到它们之间的线性映射。\n机器学习的含义也可以在这里得到解释：通过对大量数据的学习，找出一定的规律，当再次碰到相似的数据时，可以通过之前获得的规律来作出判断。\n所以我们的输出数据X是表格data的前4列，输出真实值y是表格data的最后一列。\nX = data[['AT', 'V', 'AP', 'RH']] y = data[['PE']]\n划分训练集和测试集\n为什么要划分训练集和测试集？\n上一个章节中，我们已经得到输入X和输出y，这足以让我们完成对机器的训练。但是，为了评估机器学习效果的好坏，还需要一些数据进行测试。\n训练集和测试集分开是机器学习界的公式，这样可以防止过拟合，增强机器学习算法的泛化能力。\n如何划分训练集和测试集？\n在sklearn库中提供了一个划分函数：\n# 首先从sklearn库中导入划分函数 from sklearn.model_selection import train_test_split # 然后执行函数获得结果 x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0) print('x_train.shape: ', x_train.shape) print('y_train.shape: ', y_train.shape) print('x_test.shape: ', x_test.shape) print('y_test.shape: ', y_test.shape) '''输出''' # x_train.shape: (7176, 4) # y_train.shape: (7176, 1) # x_test.shape: (2392, 4) # y_test.shape: (2392, 1)\n这个函数涉及到两个问题：\nrandom_state是什么？ 它是一个随机种子，用来生成随机数。当random_state一样时，划分的数据就服从同一种随机分布。\n如何确定训练集和测试集分割的比例？ 默认参数test_size=0.25，即默认训练集：测试集=3：1。可以改变这个参数以获得不同的分割比例。\n运行scikit-learn的线性模型\n这一步就是线性回归算法部分了，看起来最神秘，但是由于sklearn库已经将算法封装好了，所以用起来相当简单。\n# 首先从sklearn库中导入线性回归函数 from sklearn.linear_model import LinearRegression # 执行函数获得一个线性回归模型 linreg = LinearRegression() # 这是一个未经训练的机器学习模型 # 对模型传入输入数据x_train和输出数据y_train linreg.fit(x_train, y_train) # 这是一个经过训练的机器学习模型 '''输出线性回归的截距和各个系数''' print('linreg.intercept_: ', linreg.intercept_) print('linreg.coef_: ', linreg.coef_) '''输出''' # linreg.intercept_: [ 451.19095935] # linreg.coef_: [[-1.98357941 -0.23219575 0.06559288 -0.15932893]]\n经过训练的线性回归模型可以给我们返回一组系数，将这组系数带入前面的公式就可得线性回归预测的回归函数：\nPE=447.06297099−1.97376045∗AT−0.23229086∗V+0.0693515∗AP−0.15806957∗RH\nPE=447.06297099-1.97376045*AT-0.23229086*V+0.0693515*AP-0.15806957*RH\n评价模型\n现在，机器学习模型已经根据数据学习到了一个线性回归的规律。这一章节，我们需要评估我们的模型的好坏程度，对于线性回归来说，我们一般用均方差（Mean Squared Error, MSE）或者均方根差(Root Mean Squared Error, RMSE)在测试集上的表现来评价模型的好坏。\n同时，sklearn提供了一套用于评估模型好坏的工具库——metrics，不用我们自己敲代码了。\n前面划分的测试集在这里将排上用场：\ny_pred = linreg.predict(x_test) # 引入sklearn模型评价工具库 from sklearn import metrics print(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) '''输出''' # MSE: 20.5442988776 # RMSE: 4.53258192178\n确实，这样看模型的好坏程度很不直观，得到一个浮点数不能让我们确定模型训练的效果。但是这个数值的意义在于，当我们使用了多种机器学习模型时，如何进行横向比较选择效果最好的模型，具有重大的参考意义。\n本文后面还会用可视化的方式直观的表示模型学习效果的好坏。\n调优\n所谓调优，就是调整机器学习模型（本文中为线性回归）中的参数。很多人认为机器学习很简单，就是选个模型然和调参，实际并非如此。\n言归正传，进行调优最常用的方法是交叉验证。进行交叉验证，我们不仅需要训练集和测试集，还应该再把训练集分成子训练集和验证集。然后，通过子训练集和验证集获得最优的参数。最后用测试集进行评估作为当前模型（本位为简单线性回归）的最终评分。\n画图观察结果\n这里画图真实值和预测值的变化关系，离中间的直线y=x直接越近的点代表预测损失越低。代码如下：\n%matplotlib inline # 这是为了能在交互式界面中显示图像 import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(y, predicted) ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4) ax.set_xlabel('Measured') ax.set_ylabel('Predicted') plt.show()\n为什么要这样画图？\n对于输出y来说，真实值和预测值都是一维的，同时，真实值和预测值一一对应，它们之间的差值越小，预测越准确。显然，如果预测值=真实值，那么它们的差值最小，即上图中的黑色虚线。横坐标是真实值，纵坐标是预测值，那么对于所有的真实值，预测值离着黑线越近，预测越准确。\n总结\n这是本文进行线性回归的流程图："}
{"content2":"着重掌握机器学习、深度学习、迁移学习。"}
{"content2":"数据挖掘(data mining)，机器学习(machine learning)，和人工智能(AI)的区别是什么？ 数据科学(data science)和商业分析(business analytics)之间有什么关系？\n本来我以为不需要解释这个问题的，到底数据挖掘(data mining)，机器学习(machine learning)，和人工智能(AI)有什么区别，但是前几天因为有个学弟问我，我想了想发现我竟然也回答不出来，我在知乎和博客上查了查这个问题，发现还没有人写过比较详细和有说服力的对比和解释。那我根据以前读的书和论文，还有和与导师之间的交流，尝试着说一说这几者的区别吧，毕竟一个好的定义在未来的学习和交流中能够发挥很大的作用。同时补上数据科学和商业分析之间的关系。能力有限，如有疏漏，请包涵和指正。\n转载自：https://www.cnblogs.com/DonJiang/p/5744535.html\n导论\n本文主要分为两部分，第一部分阐述数据挖掘(data mining)，机器学习(machine learning)，和人工智能(AI)之间的区别。这三者的区别主要是目的不同，其手段(算法，模型)有很大的重叠，所以容易混淆。第二部分主要阐述以上的技能与数据科学(data science)的关系，以及数据科学(data science)和商业分析(business analytics)之间的关系。其实，数据科学家本身就是商业分析师在大数据时代的延伸。\n数据挖掘VS. 机器学习VS. 人工智能\n数据挖掘 (data mining): 有目的地从现有大数据中提取数据的模式(pattern)和模型(model)\n关键字：模式提取，大数据\n数据挖掘是从现有的信息(existing information)中提取数据的模式(pattern)和模型(model)，即精选出最重要的信息，以用于未来机器学习和AI的数据使用。其核心目的是找到数据变量之间的关系。其发展出来的主要原因是大数据的发展，用传统的数据分析的方式已经无能处理那么多大量的看似不相关的数据的处理，因此需要数据挖掘技术去提取各种数据和变量之间的相互关系，从而精炼数据。\n数据挖掘本质上像是机器学习和人工智能的基础，他的主要目的是从各种各样的数据来源中，提取出超集(superset)的信息，然后将这些信息合并让你发现你从来没有想到过的模式和内在关系。这就意味着，数据挖掘不是一种用来证明假说的方法，而是用来构建各种各样的假说的方法。数据挖掘不能告诉你这些问题的答案，他只能告诉你，A和B可能存在相关关系，但是它无法告诉你A和B存在什么相关关系。\n当然，数据挖掘会使用大量机器学习的算法，但是其特定的环境和目的和机器学习不太一样。\n机器学习(machine learning): 自动地从过往的经验中学习新的知识。\n关键字: 自动化，自我优化，预测，需要training data，推荐系统\n机器学习其实是人工智能很重要的一部分，因为目前，在实践过程中，大多数的人工智能处理的任务，其实是用机器学习的方式完成的。机器学习可以用程序和算法自动地学习，只要被设计好了，这个程序可以进行自我优化。同时，机器学习需要一定数量的训练数据集(training data set)，用于构建来自过往经验的“知识” 。\n且机器学习目前在实践中最重要的功能便是预测结果。比如机器学习已经学习结束了，现在有一个新的数据集x，需要预测其分类，机器学习算法会根据这个新数据与学习后的“知识”相匹配(实际上，知识指的是学习后的数学模型)，然后将这个数据集x分类某类C去。再比较常见的机器学习，比如amazon的推荐系统。\n人工智能(AI): 一个广泛的概念，本质是用数据和模型去为现有的问题(existing problems)提供解决方法(solutions).\n关键字：和人一样处理问题，技术的合集\n人工智能是一个与机器学习和数据挖掘相对不同的概念，人工智能的目的是为了去创造有智力的电脑(不知道怎么翻译好，可以假设其为机器人)。在实践中，我们希望这个电脑可以像有智力的人一样处理一个任务。因此，理论上人工智能几乎包括了所有和机器能做的内容，当然也包括了数据挖掘和机器学习的内容，同时还会有监视(monitor)和控制进程(process control)的内容。\n数据科学(data science)和商业分析(business analytics)的关系？\n其实以前，我们是没有数据科学家(data scientist)，和数据科学(data science)这个概念的。我们称呼做相关内容的方式更多叫商业分析(business analytics)。\n在2011年的时候，麦肯锡发表了《Big Data: the next frontier for innovation, competition, and productivity》提出了现在很多的公司已经开始往分析才能(analytical talent)中获得竞争优势。虽然这不是第一篇提出这个概念的公司，但是是第一次提出，数据分析能力也有助于商业公司去发现潜在的机会，而不仅仅只对技术公司有效。接着麦肯锡认为到了2018年，美国大约会有190,000的项目缺少“深度分析能力(Deep Analytical Talent)”，而这些深度分析能力，是由大数据(big data)驱动的。至此，麦肯锡将”商业分析”进一步形容为”深度分析能力”。\n接着DJ Patil和Jeff Hammerbacher在其写的《Building Data Science Teams》,将麦肯锡的“深度分析能力”称为了“数据科学家(data scientists)”。他们在文中提到：\n商业分析师(business analyst)看起来太局限了，数据分析师(data anlyst)是他们的竞争者，但是我们还是觉得这个称呼太局限了。....我们认为最好的称呼应该是”数据科学家(data scientist)”，因为这些人需要同时使用数据(data)和科学(science)去创造一些新的东西。\n紧接着，DJ Patil加了一些关键特点用于去寻找一个数据科学家(data scientist):\n专业技术(Technical expertise): 最好的数据科学家需要有关于某些科学学科的深度专业知识(deep expertise)。\n好奇心(Curiosity): 一个优秀的数据科学家需要有挖掘潜在关系，解决问题和证明假说的强烈好奇心和渴望。\n讲故事的能力(Storytelling): 能用数据讲一个生动的故事的能力，它能使交流更加有效。\n聪明(Cleverness): 能够创造性地解决问题的能力。\n随后，数据科学家这个概念才开始被广为流传。那么数据科学家需要具备哪些专业能力？不同的公司有不同的看法和意见(反正大家好像都喜欢把所有一切的期许都放在一个新兴的行业中)，这里列举一个比较流行的看法：\n1.Drew Conway’s Data Scientist Venn Diagram\n2.Drew Tierney’s Multi-disciplinary Diagram\n3.Gartner\n最后附赠一张“作弊纸”，列出几乎所有的商业问题(Business Problems)，想要入门成为一个优秀的商业分析师，或者是数据科学家，强烈推荐保存！！！！！！！！！！！！以后有时间，我会尝试着逐一翻译和解说一下。\n拓展阅读（英文）：\n什么是独角兽型的数据科学家？：不知道为什么现在什么“独角兽”型的这种理念会那么流行，企业也爱叫独角兽，行业内也爱叫独角兽。。但为什么一提到独角兽，我先想到的是巫师系列游戏。（捂脸~）\nTop 10 Data Analysis Tools for Business：用于商业分析的十大工具，强烈推荐阅读！！！\nData Science: Bridging the Business & IT Gap：第二部分内容主要来源的原文。\n参考文献：\nhttp://stats.stackexchange.com/questions/5026/what-is-the-difference-between-data-mining-statistics-machine-learning-and-ai\nhttp://upfrontanalytics.com/data-mining-vs-artificial-intelligence-vs-machine-learning/\nhttps://www.researchgate.net/post/What_is_the_difference_between_machine_learning_and_data_mining\nhttps://www.r-bloggers.com/whats-the-difference-between-machine-learning-statistics-and-data-mining/\nhttps://discuss.analyticsvidhya.com/t/what-is-the-difference-between-machine-learning-data-analysis-data-mining-data-science-and-ai/572\nhttp://www.kdnuggets.com/2014/06/data-science-skills-business-problems.html\n各种乱七八糟的书和课件的笔记。\n《Building Data Science Teams》\n《Big Data: the next frontier for innovation, competition, and productivity》\nDrew Conway’s Data Scientist Venn Diagram\nDrew Tierney’s Multi-disciplinary Diagram"}
{"content2":"上个星期帮别人推导公式，主要是机器学习算法的优化。在网上找了一份资料，还没来得及好好研究，总结了机器学习中常见的求导，百度云里面有，之后有时间仔细研究。反正我基本都是根据这个推出来的，最后人家给了我1000块钱，还说我推导的很不错。虽然我也不知道我推的是啥！！！\n直接把图片传上来做个记录"}
{"content2":"https://mp.weixin.qq.com/s/eFXb2swj07Ywld9pDop3hQ\n机器学习在很多眼里就是香饽饽，因为机器学习相关的岗位在当前市场待遇不错，但同时机器学习在很多人面前又是一座大山，因为发现它太难学了。在这里我分享下我个人入门机器学习的经历，希望能对大家能有所帮助。\n这篇文章不会有太多机器学习方面的专业知识\n更多的只是学习经历与经验分享\n机器学习工程师到底值几斤几两呢\n入门之前先来让大家心里看个数据，机器学习工程师在市场上到底值几斤几两呢？我们在拉勾网上搜索 机器学习关键字，得到了下面的结果：\n可以看出来，这个待遇在当前 IT 行业中还是很不错的，究其原因，是由于这个市场目前是供不应求，人才紧缺，自然而然与之相关的岗位待遇会相对 IT 行业中的其他岗位较高。\n我是如何入门的呢\n入门前的基础\n先来分享下在开始入门前我的基础吧，先罗列下当时我所掌握掌握的知识吧。\n掌握了 Java SE\n本科所学的微积分，线性代数，概率论中的知识忘得所剩无几\n看了上面两点，你会发现，当时的我其实除了掌握 Java SE 之外，连 Python 也不会，此外，微积分，线性代数，概率论中的知识我也基本上全都还给了大学老师了。\n这时你再想想你当前的情况，你是不是发现你的情况跟我很像，甚至会比我的情况还好呢。\n像我这样的条件都可以入了门，你们有什么理由无法入门呢？\n入门过程中\n首先说明一点，我个人在入门过程中经过了有看过视频、也有看过书，看过博客，也有直接做一些项目，这些过程是交互进行的，现在我将这些按照相对顺序来介绍下。\n聊聊Python\n在当时Python在机器学习领域已经使用非常多了，另外Python除了也可做机器学习外，还可以做后端（如Django/Flask等）。所以我首先学习的是 Python，在准备学习它的时候，我最开始采取的方式是看教学视频，但是发现太耗时了，所以看了两节之后果断放弃，进而我发现了廖雪峰关于Python的教程，看博客或者看书的方式会相对比看视频快很多，由于我个人有 Java 编程的基础，所以在看廖雪峰关于 Python 的教程时相对容易点。这里建议如果没有任何编程基础，建议还是选个Python的入门视频来学习。\n如果说单纯的学习一门语言的话，这水其实是很深的，但对于我们大多数人来说，尤其是入门的时候，我们并不需要将Python的方方面面都学会，我们只需要有重点的掌握Python我们所需要的几个部分就好，剩下的我们可以在之后工作项目中进步学习和加深。\n对于要入门机器学习的同学们，我这里整理下前期Python所需要学习的部分：\nPython语法基础，包括数据类型和变量、条件控制语句（if else、for、while）、列表（list）、集合（set）、字典（dict）、元组（tuple）等\n面向对象部分，包括函数的使用、函数的多种参数、匿名函数、类和实例、继承和多态等\nPython高级特性，包括切片、迭代、列表生成式、字典生成式、迭代器、生成器等\nIO操作及异常处理，包括读写文本文件、读写二进制文件、异常捕获和处理等\n常用的内置模块，包括 datetime、os、system、re等\n除了以上的关于Python的基础外，还有一些第三方模块我们需要掌握的。由于在使用机器学习算法时，经常需要处理数据以及可视化结果。在这里推荐以下几个第三方模块：\npandas，数据分析必备神器，功能众多，前期我们只需要掌握它的一些基本用法就OK\nmatplotlib，Python中众多数据数据可视化的一个基础库，能够使用它绘制基本图形即可\n对于我来说，这些第三方模块都是在工作的时候学习的，并不是刚开始就学习的。以我的经验给大家指出一个陷阱，在我们入门阶段，不建议大家刚开始就深入学习这些模块的底层，我的原则是：先学会使用，再研究原理。以 pandas 为例，pandas 底层用到了 numpy、scipy、matplotlib，如果你要先系统地把这些底层所有的知识学习一遍，你会发现在你有限的时间内，你根本学不完，即便你有这么多时间去学完，你会发现学了后面，忘了前面。\n相关的学习链接：\nPython（廖雪峰），http://t.cn/RK0qGu7\nPandas，http://pandas.pydata.org/pandas-docs/stable/10min.html\nMatplotlib，https://matplotlib.org/tutorials/index.html\n聊聊机器学习算法课程\n在掌握了 Python的基本用法后，我开始寻找学习机器学习的资料，当时的情况是市面上并没有众多关于这方面的培训机构，经过各种搜集，最后选择了吴恩达在 Coursera 上的机器学习课程。虽说课程语音是英文，但有中文字幕，所以看起来还是很方便的。在将吴恩达课程学完之后，你会对常用的机器学习算法有一个基本的了解。\n这里列举出课程中所讲解的一些常用算法和内容：\n线性回归\n逻辑回归\n神经网络\n支持向量机\n非监督学习\n降维、异常检测\n推荐系统\n应用机器学习建议\n看了上面的各种算法，你可能会问，这么多听起来很牛逼是算法，我的数学基础（微积分、线性代数、概率论）不好，能听的懂么？\n在入门的时候，不建议大家将所有的数学知识全都学一遍，再来学习机器学习算法。一是时间长，而是学了不用就忘了。 选择一个好的视频教程，你可以事半功倍，，吴恩达老师课程的一个优点就在于他是专门针对我们这类的人群的，他的视频中不会有很多数学公式推导，但是仍然可以将这个算法跟你讲解清楚。\n此外，台湾大学林轩田教授也有专门的机器学习视频（包括两部分：基石与技法）。这个视频在国内评价也较高，有兴趣的也可以跟着这个视频来学习。\n关于吴恩达以及林轩田的机器学习相关的视频课程，之前已经有过分享，需要下载的请见：资源 | 2018年，你想要的机器/深度学习资料在这里\n相关学习链接：\n吴恩达机器学习课程，https://zh.coursera.org/learn/machine-learning\n聊聊机器学习书籍\n在学习了吴恩达老师的课程后，可以再读一些相关的书籍来加强对机器学习的理解，当时我买了挺多的书籍，有一本书叫做《机器学习实战》，在这本书里会教你使用 Python 来实现常用的一些算法，当时按照书上的讲解手动敲了一些算法的实现，当你手动实现后，你会发现你对它的理解会更深了。\n在实现书本上的一些算法时，会遇到某些算法的原理不太明白的地方，由于自己的数学知识（微积分、线性代数、概率论）基本上都还给了大学老师了，系统的去学习每个模块是非常耗时的，这时候采取的策略是“缺啥补啥”，也就是说发现哪方面的知识和公式不太明白，就去查阅与之相关的资料。\n在当前，如果你想购买相关书籍，我可以再推荐两本给你，一本是周志华的《机器学习》（通常也叫西瓜书），这本书在我学习的时候还没出版；另一本是李航的《统计学习方法》。\n聊聊如何通过工作/比赛提高水平\n在掌握了Python的基本用法以及对机器学习有了基本了解之后，当时的实习工作有一部分是使用机器学习算法来去识别用户的评论是否违法，也就是一个二元分类问题。最开始的时候同阅读同事实现的Python代码，试图去搞明白每一行的含义，也就是在这个时候，自己开始查阅 pandas 和 sklearn 相关的文档，这样最后不仅搞明白了同事的代码，自己对 pandas 和 sklearn 基本的用法也有了一个认识。\n这里多说两句，掌握了 pandas 后，在处理小数据量的时候会非常得心应手（如果数据量大的话，会非常慢）；sklearn 是一个非常优秀的开源的机器学习Python库，这个库实现了很多机器学习算法，并且提供了非常详细的官方文档，认真阅读官方文档可以收获很多。如果你英文不太好的话，可以阅读相应的中文文档，文档地址可以见下面的学习链接。\n如果说自己没有实习或工作机会，我推荐你去参加 kaggle 比赛，kaggle 是一个提供数据挖掘相关的比赛平台，在这里会有很多相对接近现实生活的比赛，此外，也会有很多大神分享自己的思路、做法和代码，通过阅读这些代码也能快速的提高自己的水平。\n相关学习链接：\nsklearn，http://sklearn.apachecn.org/cn/latest/\nkaggle，https://www.kaggle.com/\n入门后的体验\n在入门了机器学习之后，在实际工作中，绝大多数的情况下你并不需要去创造一个新的算法。另外，大多数时间你也不是在去研究别人的算法时如何写出来的，而是处理数据，运用现有的第三方库去跑模型、调参数。听完我说的这些，你是不是很震惊，高大上的机器学习工程师在实际工作中大多数时间竟然是去洗数据，调参数。但事实就是如此，除非你在一线互联网公司，而且还是某些部门，你可能会需要自己重新去实现某个算法，否则其他公司的情况大多都是差不多的。\n入门后再来聊一聊数学知识在机器学习中的作用，虽说你数学知识不好，一样可以调用第三方库的模型。但是如果你想要深入理解算法，数学的底子还必须是有一点的。如果你要入研究这个领域，你必须要捡起来你忘掉的数学知识。此外，在面试时，这些算法的原理相关的知识还是必问的。\n有时候并不是你的能力不够，也并不是你不够努力，只是缺少了一个正确的引导而已。"}
{"content2":"摘要： 还不知道这五种损失函数？你怎么在机器学习这个圈子里面混？\n在机器学习中，所有的机器学习算法都或多或少的依赖于对目标函数最大化或者最小化的过程，我们常常把最小化的函数称为损失函数，它主要用于衡量机器学习模型的预测能力。在寻找最小值的过程中，我们最常用的方法是梯度下降法。\n虽然损失函数可以让我们看到模型的优劣，并且为我们提供了优化的方向，但是我们必须知道没有任何一种损失函数适用于所有的模型。损失函数的选取依赖于参数的数量、异常值、机器学习算法、梯度下降的效率、导数求取的难易和预测的置信度等若干方面。这篇文章将介绍各种不同的损失函数，并帮助我们理解每种损失函数的优劣和适用范围。\n由于机器学习的任务不同，损失函数一般分为分类和回归两类，回归会预测给出一个数值结果而分类则会给出一个标签。这篇文章主要集中于回归损失函数的分析。本文中所有的代码和图片都可以在这个地方找到！\n回归函数预测数量，分类函数预测标签\n回归损失函数\n1.均方误差、平方损失——L2损失：\n均方误差（MSE）是回归损失函数中最常用的误差，它是预测值与目标值之间差值的平方和，其公式如下所示：\n下图是均方根误差值的曲线分布，其中最小值为预测值为目标值的位置。我们可以看到随着误差的增加损失函数增加的更为迅猛。\nMSE损失（Y轴）与预测（X轴）的关系图\n2.平均绝对误差——L1损失函数：\n平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值的和，表示了预测值的平均误差幅度，而不需要考虑误差的方向（注：平均偏差误差MBE则是考虑的方向的误差，是残差的和），范围是0到∞，其公式如下所示：\nMAE损失（Y轴）与预测（X轴）的关系图\n平均绝对误差和均方误差（L1&L2）比较：\n通常来说，利用均方差更容易求解，但平方绝对误差则对于异常值更稳健，下面让我们对这两种损失函数进行具体的分析。\n无论哪一种机器学习模型，目标都是找到能使目标函数最小的点。在最小值处每一种损失函数都会得到最小值。但哪种是更好的指标呢？你可以上述笔记本地址自行运行代码，检查它们的各项指标。\n让我们用具体例子看一下，下图是均方根误差和平均绝对误差的比较（其中均方根误差的目的是与平均绝对误差在量级上统一）:\n左边的图中预测值与目标值很接近，误差与方差都很小，而右边的图中由于异常值的存在使得误差变得很大。\n由于均方误差（MSE）在误差较大点时的损失远大于平均绝对误差（MAE），它会给异常值赋予更大的权重，模型会全力减小异常值造成的误差，从而使得模型的整体表现下降。\n所以当训练数据中含有较多的异常值时，平均绝对误差（MAE）更为有效。当我们对所有观测值进行处理时，如果利用MSE进行优化则我们会得到所有观测的均值，而使用MAE则能得到所有观测的中值。与均值相比，中值对于异常值的鲁棒性更好，这就意味着平均绝对误差对于异常值有着比均方误差更好的鲁棒性。\n但MAE也存在一个问题，特别是对于神经网络来说，它的梯度在极值点处会有很大的跃变，及时很小的损失值也会长生很大的误差，这很不利于学习过程。为了解决这个问题，需要在解决极值点的过程中动态减小学习率。MSE在极值点却有着良好的特性，及时在固定学习率下也能收敛。MSE的梯度随着损失函数的减小而减小，这一特性使得它在最后的训练过程中能得到更精确的结果（如下图）。\n在实际训练过程中，如果异常值对于实际业务十分重要需要进行检测，MSE是更好的选择，而如果在异常值极有可能是坏点的情况下MAE则会带来更好的结果。\n总结：L1损失对于异常值更鲁棒，但它的导数不连续使得寻找最优解的过程低效；L2损失对于异常值敏感，但在优化过程中更为稳定和准确。更详细的L1和L2不同比较可以参考这篇文章。\n但现实中还存在两种损失都很难处理的问题。例如某个任务中90%的数据都符合目标值——150，而其余的10%数据取值则在0-30之间。那么利用MAE优化的模型将会得到150的预测值而忽略的剩下的10%（倾向于中值）；而对于MSE来说由于异常值会带来很大的损失，将使得模型倾向于在0-30的方向取值。这两种结果在实际的业务场景中都是我们不希望看到的。\n3.Huber损失——平滑平均绝对误差\nHuber损失相比于平方损失来说对于异常值不敏感，但它同样保持了可微的特性。它基于绝对误差但在误差很小的时候变成了平方误差。我们可以使用超参数δ来调节这一误差的阈值。当δ趋向于0时它就退化成了MAE，而当δ趋向于无穷时则退化为了MSE，其表达式如下，是一个连续可微的分段函数：\n对于Huber损失来说，δ的选择十分重要，它决定了模型处理异常值的行为。当残差大于δ时使用L1损失，很小时则使用更为合适的L2损失来进行优化。\nHuber损失函数克服了MAE和MSE的缺点，不仅可以保持损失函数具有连续的导数，同时可以利用MSE梯度随误差减小的特性来得到更精确的最小值，也对异常值具有更好的鲁棒性。\n而Huber损失函数的良好表现得益于精心训练的超参数δ。\n4.Log-Cosh损失函数\nLog-Cosh损失函数是一种比L2更为平滑的损失函数，利用双曲余弦来计算预测误差：\n它的优点在于对于很小的误差来说log(cosh(x))与（x**2）/2很相近，而对于很大的误差则与abs(x)-log2很相近。这意味着log cosh损失函数可以在拥有MSE优点的同时也不会受到异常值的太多影响。它拥有Huber的所有优点，并且在每一个点都是二次可导的。二次可导在很多机器学习模型中是十分必要的，例如使用牛顿法的XGBoost优化模型（Hessian矩阵）。\nXgBoost中使用的目标函数，注意对一阶和二阶导数的依赖性\n但是Log-cosh损失并不是完美无缺的，它还是会在很大误差的情况下梯度和hessian变成了常数。\nHuber和Log-cosh损失函数的Python代码：\n# huber loss def huber(true, pred, delta): loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2)) return np.sum(loss) # log cosh loss def logcosh(true, pred): loss = np.log(np.cosh(pred - true)) return np.sum(loss)\n5.分位数损失（Quantile Loss）\n在大多数真实世界的预测问题中，我们常常希望看到我们预测结果的不确定性。通过预测出一个取值区间而不是一个个具体的取值点对于具体业务流程中的决策至关重要。\n分位数损失函数在我们需要预测结果的取值区间时是一个特别有用的工具。通常情况下我们利用最小二乘回归来预测取值区间主要基于这样的假设：取值残差的方差是常数。但很多时候对于线性模型是不满足的。这时候就需要分位数损失函数和分位数回归来拯救回归模型了。它对于预测的区间十分敏感，即使在非均匀分布的残差下也能保持良好的性能。下面让我们用两个例子看看分位数损失在异方差数据下的回归表现。\n左：线性关系b / w X1和Y.具有恒定的残差方差。右：线性关系b / w X2和Y，但Y的方差随着X2增加。\n上图是两种不同的数据分布，其中左图是残差的方差为常数的情况，而右图则是残差的方差变化的情况。我们利用正常的最小二乘对上述两种情况进行了估计，其中橙色线为建模的结果。但是我们却无法得到取值的区间范围，这时候就需要分位数损失函数来提供。\n上图中上下两条虚线基于0.05和0.95的分位数损失得到的取值区间，从图中可以清晰地看到建模后预测值得取值范围。\n了解分位数损失函数\n分位数回归的目标在于估计给定预测值的条件分位数。实际上分位数回归就是平均绝对误差的一种拓展。分位数值得选择在于我们是否希望让正的或者负的误差发挥更大的价值。损失函数会基于分位数γ对过拟合和欠拟合的施加不同的惩罚。例如选取γ为0.25时意味着将要惩罚更多的过拟合而尽量保持稍小于中值的预测值。\nγ的取值通常在0-1之间，图中描述了不同分位数下的损失函数情况，明显可以看到对于正负误差不平衡的状态。\n分位数损失（Y轴）与预测（X轴）的关系图。\n我们可以利用分位数损失函数来计算出神经网络或者树状模型的区间。下图是计算出基于梯度提升树回归器的取值区间：\n使用分位数损失的预测区间（梯度提升回归器）\n90%的预测值起上下边界分别是用γ值为0.95和0.05计算得到的。\n比较研究：\n在文章的最后，我们利用sinc(x)模拟的数据来对不同损失函数的性能进行了比较。在原始数据的基础上加入而高斯噪声和脉冲噪声（为了描述鲁棒性）。下图是GBM回归器利用不同的损失函数得到的结果，其中ABCD图分别是MSE, MAE, Huber, Quantile损失函数的结果：\n将一个平滑的GBM拟合成有噪声的sinc（x）数据的示例：（E）原始sinc（x）函数；（F）符合MSE和MAE损失的平滑GBM；（G）平滑GBM，其具有Huber损耗，δ= {4,2,1}；（H）光滑的GBM与α= {0.5,0.1,0.9}的分位数损失相符合。\n我们可以看到MAE损失函数的预测值受到冲击噪声的影响更小，而MSE则有一定的偏差；Huber损失函数对于超参数的选取不敏感，同时分位数损失在对应的置信区间内给出了较好的估计结果。\n希望小伙伴们能从这篇文章中更深入地理解损失函数，并在未来的工作中选择合适的函数来更好更快地完成工作任务。\n将本文中几种损失函数都放到一个图中的结果：\n原文链接\n本文为云栖社区原创内容，未经允许不得转载。"}
{"content2":"前言：\n说来也巧合，我在大学里加入的第一个社团就是数学建模，各种各样的社团对我没有完全没有吸引力，什么舞蹈、爱心、创业、英语等，加入数学建模的原因有二：一是可以参加比赛，二是可以认识更多的朋友，但是在加入的第四周我就完全失望了，完全没有达到我的预期，平时组织活动比较及时，但是真正给你教东西的却很少，所以从那开始我就退出了，再也没去。所以你感觉一个社团给你提供不了你当初加入的初心，那就赶紧离开吧，别再浪费时间了。之后一年里都是按部就班地上课，没有搭理社团，但是QQ通知群还是留着，直到有一天群里通知有没有人愿意参加数模大赛，虽然群里只剩下十几个人，那就报名呗，反正有专门的老师培训，之后都会在课余去参加培训，偶尔偷懒没去，之后经过平时，暑期培训参加了比赛。在比赛的时候一脸懵逼，B题题目都搞不懂，怎么做，那就选B吧，查各种资料写论文，建模，到了最后提交不了？你是在逗吗，三天的辛苦白费了？结果也就没有拿到好成绩，就这样大一结束了，大二同样参加了，也拿到了成绩，在其期间也参加了其他的比赛，比如美赛，亚太赛，数创杯等，就算是经历吧。\n总叙：\n好了，言归正传，参加了这么多关于数模比赛，到底是为什么，或者是我喜欢数学吗？不是的，我高数一般般，由于个中原因，反正没怎么听老师讲课，所以对于数学我平常心，从2016年开始，AI的兴起，让我非常感兴趣，就特地去了解了下所学的课程，结果是无论是machine larning 还是deep leraning都需要数学建模思维，也就是现在非常抢手，工资非常高的算法工程师，并且数学建模是核心点，没有很好地建模思维，那你的项目就达不到所谓的智能。\n那么数学与建模有什么联系，我们知道在大学如果要学高数必须要学习微积分，定积分，线性代数，概率论等，所以这些都是建模的基础，建模没有对错，只有better，所以基础肯定要有的，其次要会查资料，比如各种出版论文，最新的建模思维，只要熟练掌握，那么你的建模思维就会很高级，解决实际情况的方法就会很高效。我接触比较少，所以建模这块还在努力中，菜鸟一只。\n数学与数学建模：\n举个简单的列子：\n如：一个星级旅馆有150个客房，经过一段时间的经营实践，旅馆经理得到了一些数据：每间客房定价为160元时，住房率为55%，每间客房定价为140元时，住房率为65%，\n每间客房定价为120元时，住房率为75%，每间客房定价为100元时，住房率为85%。欲使旅馆每天收入最高，每间客房应如何定价？\n[简化假设]\n（1）每间客房最高定价为160元；\n（2）设随着房价的下降，住房率呈线性增长；\n（3）设旅馆每间客房定价相等。\n[建立模型]\n设y表示旅馆一天的总收入，与160元相比每间客房降低的房价为x元。由假设（2）可得，每降价1元，住房率就增加 。因此\n由\n可知\n[求解模型]\n利用二次函数求最值可得到当x=25即住房定价为135元时，y取最大值13668.75（元），\n[讨论与验证]\n（1）容易验证此收入在各种已知定价对应的收入中是最大的。如果为了便于管理，定价为140元也是可以的，因为此时它与最高收入只差18.75元。\n（2）如果定价为180元，住房率应为45%，相应的收入只有12150元，因此假设（1）是合理的。\n这便是一个简单的数学建模，在这道题中我们需要考虑的是：\n1.要做什么？\n2.怎么做？\n3.这样做合理吗？\n4.如果这样做，假设那些可以改变？\n5.这样做需要用到那些模型？\n6.这种模型简洁吗？\n7.确定了这种模型，怎么求解？\n8.求解出来了，与现实合理吗？\n9.在这个模型中，存在什么缺点，怎么去优化？\n10.总结\n差不多这十点是需要我们考虑的，也许要大量用到数学计算，甚至涉及到统计，经济学，专有软件（matlab，spass）的处理等，所以整个建模过程离不了数学知识。\n数学建模与人工智能：\n同样通过一个小例子来理解他们之间的关系：\n题目：在一个公司中，由于某些原因，有些员工渐渐离网（从公司流失），对于此，请用机器学习算法预测客户流失的一些特点，包括年龄，性别等其它因素。\n在这道题中，利用机器学习时要用到一种建模模型：决策树\n1.决策树如何构建？\n2.构建决策树\n3.生成决策树\n4.利用ID3算法实现function\n5.集成学习\n6.然后利用PAI算法执行\n7.分析效果\n8.总结\n不难发现通过这复杂的八步就完美提供了解决方案，完成项目需求，而这八步都需要扎实的建模思维，要不然几乎没办法做，所以数学建模对于人工智能算法非常关键。\n再者比如最新的滴滴Di-Tech算法大赛：\n问题是：对无人驾驶车而言，最重要的是能够理解周围环境并做出相应决策，保证行车安全。参赛者需要努力找到通过摄像头和 LIDAR 数据检测道路上的障碍物的最好方法。最终的系统应该能够识别行人、车辆和普通障碍物，对人类驾驶员和无人驾驶系统都有很大帮助。参赛者需要处理 LIDAR、RADAR及摄像头原始数据，输出障碍物位置、移除噪音和环境错误检测。参赛者可以利用已经较为完善的 Kitti 数据集，在现有技术的基础上，加入自己的新方法和手段，来让算法获得更好表现。\n不难发现无论是怎样的安全措施，数据分析，算法处理，都需要建模思维各个方面考虑，完美的建模便是最好的答案。\n所以学习人工智能，数学建模是必须的，很难，但要坚持！\n2017的最后一个月，大家努力！\n2017-12-01"}
{"content2":"以下所有例子和内容均来自于网易云课堂机器学习课程\n机器学习的应用\n第1幅图片是图像识别的例子，这个大家很常见了。\n第二幅图片是打排球。它是利用了随机森林的算法将人的真实动作转换为虚拟动作。\n第三幅图，google的VR头盔可以根据人物的运动自动调整人物画像。\n第四幅图也是人们非常熟悉的语音识别。\n第五幅图是机器狗能够自己学习走路，这是利用了强化学习的算法\n第六幅图是facebook，可以根据用户设置自动进行广告推送\n第七幅图是各大电商自动推送，淘宝和京东也可以\n第八幅图与医学相关\n第九幅图和航空航天有关\n第十幅图和探索外太空，如火星有关。\n2 机器学习是未来\n上面两个图说明，人们正在产生越来越多的数据，但是人们处理的数据缺很少，非常大的数据需要人们去处理！这些数据都需要机器学习来处理！因此，在这个数据爆炸的时代，机器学习有巨大的潜力！"}
{"content2":"noise的产生\n在机器学习中我们在独立随机抽样的时候会出现一些搞错的信息，这些错误的数据我们称之为杂讯（或者噪音  noise），一般可以归结为一下两种（以二分为例）：\n输出错误：1.同样的一笔数据会出现两种不同的评判  2.在同样的评判下会有不同的后续处理。\n输入错误：1.在收集数据的时由于数据源的随机性会出现错误（比如说，客户在填信息的时候出现的误填）\nnoise的情况下VC维度的可用性\n在有noise的情况下我们的资料不会都来自于我们所求的目标函数而是来自于一个带有noise的分布，因此我们的f（x）会在产生资料的时候加上一个波动值后变成了f（x）+noise它具有一定的随机性。\n在这里需要注意的是我们的资料产生于一个带有noise的分布，而我们预测的资料也是产生于一个同样的分布。直观的来看只是一个换了分布的机器学习过程。所以VC维度能够在有杂讯的情况下学习，所有的论述过程同这篇文章机器学习与VC维度。\nnoise的代价\n我们能够在有杂讯的资料上学习，通过一个带有杂讯的分布，当然我们会犯错。在遇到一个具体的点的时候，我们会查找这个点在我们的标签分布上的概率p（y | x），比如说这个分布会告诉我们x的概率为0.7，o的概率是0.3那么我们就会选择概率较大的那个选项，但是我们有0.3的几率会犯错，这就是我们的代价。\n修正后的机器学习模型图如下：\n最后感谢台湾大学林轩田老师。"}
{"content2":"真正的机器学习应该是怎样的?\n机器人和人工智能这个领域确实已进入了瓶颈阶段，因为现在的存在的各种人工智能和机器装置，都是人工编程控制的，再精密的动作都是在按照人工方式模拟下进行的，因此已经进入误区。试想下：我们生下来就被遗传了1+1=2的计算功能吗？一生下来就能有意识的抬起手臂吗？？而目前再简单的人工智能都可以完成，但这仅仅局限于模拟！\n因此个人觉得应该让机器人真正有生命：必须让机器具有自己思考下一步要干嘛的能力，而不是在固定代码下执行！说白了：机器要干什么不是人说了算，而是机器在各种听觉视觉等环境刺激下激发应对的策略，并且能对这些策略进行总结。要知道人是很复杂的，靠模拟根本无法和人类比，因此机器人智能必须能自己开发自己，自己决策。\n这样优点：思考同样问题下 机器人逻辑速度很快，是人的串行思维能力上千倍都不止。因此机器人自主学习的话，可以达到人间一天相当于人们3年的效果！机器人从空白到成人如果资讯足够，一个月就超越人类。\n只要记得一点：人的本质就是：刺激，以及综合自己的已有信息对刺激的反应！这点机器完全可以模拟到。"}
{"content2":"机器学习任务的一般步骤\n一、确定特征：（收集训练数据）\n1 > 数据探索：（为什么要进行数据探索呢：有助于选择 合适的 数据预处理方法 和 建模方法） （1）数据质量分析 -- 缺失值 -- 异常值 （2）特征分布特性的分析 -- 统计量 -- 直方图 （3）特征之间相关性分析 2 > 数据预处理： （1）数据取值范围的缩放 -- 数据标准化（Standardization） -- 数据缩放 (Scaling) -- 数据正规划/归一化 (Normalization) （2）特征编码 -- 二值化 (Binarizer) -- 多项式编码 (PolynomialFeatures) -- 标签编码 (LabelEncoder) -- 独热编码 (OneHotEncoder) -- 数值特征离散化 () 3 > 特征选择：\n二、确定模型：（暂定确切的模型）\n三、模型训练：（根据样本数据计算模型参数）\n四、模型评估： （根据测试数据，评估模型的预测性能）\nnote：深度学习可学习 feature\n===========================================================================================\n—–>确定特征—->数据探索——>数据质量分析——–> 缺失值：\n1.统计含有缺失值的样本数目 及 缺失率 2.缺失值的处理： 1> 删除含有（一个或多个）缺失值的样本 (从 行 的角度考虑) 2> 删除缺失值太多的特征(从 列 的角度考虑) 3> 对缺失值进行插补 —— 均值mean(默认的方法) —— 中位数median —— 众数 most_frequent —— 固定值插补 （根据背景知识用某些常量进行插补） —— 最近邻插补 （寻找最相似的样本，用该样本对应的属性进行插补） —— 回归方法 （用没有缺失的数据建立回归方程预测不完整的样本数据） —— 插值法 （利用该变量已有数据 建立合适的插值函数 进行插补） 4> 不处理（有些框架可处理数据缺失的情况，如：xgboost）\n——->确定特征——>数据探索——->数据质量分析——> 异常值：\n1.异常值 也被成为里群点， 指样本的某个属性明显偏离其余样本的观测值。通常认为这些样本点是噪声，对模型有坏的影响 2.发现异常值 —— 最大值/最小值分析 —— 3σ原则 —— 箱型图 —— 直方图 （尾巴） —— 散点图 （孤立的点） —— 分为数 （0.5% - 99.5% 分为数以外） 3.异常值处理：类似与缺失值处理 —— 注意分析异常值产生的原因\n——->确定特征——>数据预处理——->数据取值范围的缩放：\n1.StandardScaler 2.MinMaxScaler 3.Normalizer 4.Binarizer 5.OneHotEncoder 6.Imputer (缺失值计算) 7.PolynomialFeatures (多项式数据转化) 8.FunctionTransformer (自定义单元数据转换)"}
{"content2":"文章的目的：这篇文章主要讲述模型的建立、以及测试的完整步骤，重点是各个步骤的关系以及作用。\n文章的前提：这篇文章所有代码是用python编写，用knn算法对经典数据MNIST data（手写数字）进行建立模型。这是用一个实例说明，不用太在乎算法和数据本身。\n文章结构：文章会解释每一步的作用，最后会贴出全部python代码，本文的数据下载地址：这里写链接内容\n一、读取数据（原始数据）\n读取数据可以看做数据收集，原始数据的状态。这里是csv文件，具体读取方式有很多，这里采用pandas的方法。\ndef opencsv(): # 使用pandas打开 data = pd.read_csv('data/train.csv') data1 = pd.read_csv('data/test.csv') train_x = data.values[0:, 1:] # 读入全部训练数据 train_y = data.values[0:, 0] result_x = data1.values[0:, 0:] # 测试全部测试个数据 return train_x, train_y, result_x\n二、数据预处理\n数据预处理处理是对数据提前进行处理和修正。主要包括：特征提取，特征降维、特征空值处理、特征转换（one-hot）、特征归一化；目标值空值处理，目标值转换（one-hot）\n（其中常用的是：降维、空值、one-hot转换、归一化。）\n（这里我的数据没有经过预处理，因为原始的数据是图片像素数据，具体每一种数据在sklearn上都有方法）\n三、交叉验证数据划分\n为了模型测试，先选择交叉验证方法，提前划分好数据。\n（注意：交叉验证会改变数据顺序，若原始数据对你有用，可以先进行交叉验证，在进行降维等预处理，接下来会演示）\ndef data_pro(x,y): x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.1,random_state=33) return x_train, x_test, y_train, y_test\n四、模型建立及测试\n用处理好的数据建立训练模型，对模型的评价有很多参数，常用的有：得分（对的比例）、查准率、查全率、F1指数\n#训练 knnClf = KNeighborsClassifier() # k=5 KNN中邻值为5， knnClf.fit(x_train, ravel(y_train)) #预测 y_predict = knnClf.predict(x_test) print(\"score on the testdata:\",knnClf.score(x_test,y_test)) # print(\"score on the traindata:\",knnClf.score(x_train,y_train)) print(classification_report(y_test,y_predict))\n五、预测的可能性计算\n计算分类的概率大小\n# 可能性 probablity = knnClf.predict_proba(x_test) list_pro = [] for i in range(probablity.shape[0]): pro = max(list(probablity[i])) list_pro.append(pro)\n六、结果保存\n将编号，原始结果，预测结果，预测概率保存csv\n#输出 index = np.array(id).reshape((-1,1))[:,0:1] result = pd.DataFrame(np.column_stack((index.reshape(-1,1),np.array(y_test).reshape(-1,1),np.array(y_predict).reshape(-1,1),np.array(list_pro).reshape(-1,1))), columns=['ImageId','test_label','predict_lable','probablity']) result.to_csv('result/knn_result.csv',index=False,header=True,encoding='gbk')\n七、错误分析\n错误本身就是一个很重要的东西，将错误分类保存起来。当需要对具体错误类型分析的时候，可以逐个分析错误。（比如当对‘4’分类错误比价多的时候，可以加大‘4’的权重，使得其充分训练）\n#错误分析 diff_index = [] for i in range(result.shape[0]): diff_index.append(result['test_label'][i] != result['predict_lable'][i]) print(diff_index) diff = result[diff_index] diff_x = x_test_original[diff_index] #查看每个错误 for i in range(len(diff_index)): # print(\"label is:\",diff['test_label'][i],\"predict is:\",diff['predict_lable'][i]) print(\"test label is :\",diff.iloc[i]['test_label'],'predict label is :',diff.iloc[i]['predict_lable']) x = diff_x[i] img = x.reshape(28,28) image_show(img) diff.to_csv('result/knn_result_diff.csv',index=False,header=True,encoding='gbk')\n全部代码：\nimport pandas as pd import time from numpy import ravel, savetxt from sklearn import svm from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier import numpy as np from sklearn.decomposition import PCA import matplotlib.pyplot as plt def image_show(img): plt.imshow(img) plt.show() def opencsv(): # 使用pandas打开 data = pd.read_csv('data/train.csv') data1 = pd.read_csv('data/test.csv') train_x = data.values[0:, 1:] # 读入全部训练数据 train_y = data.values[0:, 0] result_x = data1.values[0:, 0:] # 测试全部测试个数据 return train_x, train_y, result_x def data_pro(x,y): x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.1,random_state=33) return x_train, x_test, y_train, y_test def knnClassify(x_train, x_test, y_train, y_test): id = range(1,x_test.shape[0]+1) print(\"start run knn.\") #训练 knnClf = KNeighborsClassifier() # k=5 KNN中邻值为5， knnClf.fit(x_train, ravel(y_train)) #预测 y_predict = knnClf.predict(x_test) print(\"score on the testdata:\",knnClf.score(x_test,y_test)) # print(\"score on the traindata:\",knnClf.score(x_train,y_train)) print(classification_report(y_test,y_predict)) # 可能性 probablity = knnClf.predict_proba(x_test) list_pro = [] for i in range(probablity.shape[0]): pro = max(list(probablity[i])) list_pro.append(pro) #输出 index = np.array(id).reshape((-1,1))[:,0:1] result = pd.DataFrame(np.column_stack((index.reshape(-1,1),np.array(y_test).reshape(-1,1),np.array(y_predict).reshape(-1,1),np.array(list_pro).reshape(-1,1))), columns=['ImageId','test_label','predict_lable','probablity']) result.to_csv('result/knn_result.csv',index=False,header=True,encoding='gbk') #错误分析 diff_index = [] for i in range(result.shape[0]): diff_index.append(result['test_label'][i] != result['predict_lable'][i]) print(diff_index) diff = result[diff_index] diff_x = x_test_original[diff_index] #查看每个错误 for i in range(len(diff_index)): # print(\"label is:\",diff['test_label'][i],\"predict is:\",diff['predict_lable'][i]) print(\"test label is :\",diff.iloc[i]['test_label'],'predict label is :',diff.iloc[i]['predict_lable']) x = diff_x[i] img = x.reshape(28,28) image_show(img) diff.to_csv('result/knn_result_diff.csv',index=False,header=True,encoding='gbk') def svmClassify(train_x, train_y, test_x): id = range(1, 28001) t = time.time() svc = svm.SVC(kernel='rbf', C=10) svc.fit(train_x, train_y) h = time.time() print('time used:%f' % (h - t)) test_y = svc.predict(test_x) k = time.time() print('time used:%f' % (k - h)) savetxt('sklearn_svm_Result.csv', test_y, delimiter=',') result = pd.DataFrame(np.column_stack((np.array(id).reshape((-1, 1))[:, 0:1], np.array(test_y).reshape((-1, 1))[:, 0:1])), columns=['ImageId', 'Label']) result.to_csv(\"sklearn_knn_Result2.csv\", index=False, header=True, encoding='gbk') if __name__ == \"__main__\": print(\"start.\") #原数据 train_x_original, train_y_original, result_x_original = opencsv() # 交叉验证 x_train_original, x_test_original, y_train, y_test = data_pro(train_x_original, train_y_original) # 降维 pca = PCA(n_components=0.8, whiten=True) train_x_pca = pca.fit_transform(x_train_original) x_test_pca = pca.transform(x_test_original) result_x_pca = pca.transform(result_x_original) #knn knnClassify(train_x_pca, x_test_pca, y_train, y_test) #SVM # svmClassify(train_x,train_y,test_x) print(\"end.\")"}
{"content2":"国内/外每年都会举办很多计算机视觉（Computer Vision，CV）、 机器学习（Machine Learning，ML）、人工智能（Artificial Intelligence ，AI）领域相关的很多学术会议和研讨会等。在此把我所知道的2017年国内/外即将举办的CV和ML领域几个会议和研讨会列出来，希望对读者有用。如有其他我遗漏的，还请各位读者留言提醒，我会及时更新的。\nMMM2017 (冰岛雷克雅维克)\n全称：the 23rd International Conference on MultiMedia Modeling\n时间：2017.01.04-06\n地点：Reykjavík, Iceland\n介绍：多媒体建模及应用领域国际权威会议\n官网：http://mmm2017.ru.is/\nAAAI 2017（美国旧金山）\n全称：the Association for the Advancement of Artificial Intelligence\n时间：2017.02.04-09\n地点：San Francisco, California USA\n介绍：人工智能领域顶级会议\n官网：http://www.aaai.org/Conferences/AAAI/aaai17.php\nICASSP 2017（美国洛杉矶）\n全称：International Conference on Acoustics, Speech and Signal Processing\n时间：2017.03.05-09\n地点：New Orleans, Los Angeles， USA\n介绍：全世界最大的的声学、语音与信号处理及其应用方面的顶级会议\n官网：http://www.ieee-icassp2017.org/\nWACV 2017 (美国加州)\n全称：IEEE Winter Conference on Applications of Computer Vision\n时间：2017.03.27-29\n地点：Santa Rosa, CA, USA\n介绍：不同于一般的学术会议，比较侧重计算机视觉的应用\n网址：http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=56568\nCVM2017（中国天津）\n全称：Computational Visual Media\n时间：2017.4.12-14\n地点：天津，南开大学\n会议语言：英语\n官网：http://iccvm.org/2017/\n会议涉及领域：\ncomputer graphics, computer vision, machine learning, image processing, video processing, visualization，geometric computing\nICRA 2017（新加坡）\n全称：IEEE International Conference on Robotics and Automation\n时间：2017.05.29-06.03\n地点：Sands Expo and Convention Centre, Marina Bay Sands in Singapore\n介绍：机器人及自动化领域国际顶级会议\n官网：http://www.icra2017.org/\nVALSE2017（中国厦门）\nvalse是全球计算机视觉，模式识别，机器学习，多媒体技术等相关领域华人青年学者最具影响力的交流平台。每年的VALSE会议都非常火爆，今年的具体时间尚未公布，地点在厦门。由于影响力非常大，有兴趣的人请及时关注大会情况，及时注册。\n全称：Vision And Learning SEminar\n时间：2017年5月\n地点：厦门\nCVPR 2017（美国夏威夷）\n全称：IEEE Conference on Computer Vision and Pattern Recognition\n时间：2017.07.21-07.26\n地点：Honolulu, Hawaii，USA\n介绍：计算机视觉领域三大顶会之一\n官网： http://cvpr2017.thecvf.com/\nSIGGRAPH 2017（美国加州）\n时间：2017.07.30-08.03\n地点：Los Angeles, California, USA\n介绍：计算机图形学领域最权威、影响力最大的国际会议\n官网：http://www.siggraph.org/attend/events/siggraph-2017\nIJCAI 2017（澳大利亚墨尔本）\n全称：International Joint Conference on Artificial Intelligence\n时间：2017.08.19-25\n地点：Melbourne, Australia\n介绍：国际顶级人工智能联合大会\n官网：http://www.leiphone.com/news/201607/MGrdIcAkBBDcCTS1.html\nICIG 2017 (中国上海)\n全称：The 9th International Conference on Image and Graphics\n时间：2017.09.14-15\n地点：上海亚特兰蒂斯酒店\n官网：http://icig2017.org/\n内容： image and graphics theory, techniques and algorithms\nICIP 2017 （中国北京）\n全称：2017 IEEE International Conference on Image Processing\n时间：2017.09.17-20\n地点：China National Convention Center in Beijing, China\n介绍：图像处理领域国际会议\n官网：http://2017.ieeeicip.org/\nIROS 2017（加拿大温哥华）\n全称：IEEE/RSJ International Conference on Intelligent Robots and Systems\n时间：2017.09.24-28\n地点：Vancouver, BC, Canada\n介绍：智能机器人系统领域国际顶级会议\n官网：http://iros2017.org/\nCCCV 2017 （中国天津）\n全称：中国计算机视觉大会（TheChinese Conference on Computer Vision）\n时间：2017.10.13-15\n地点：天津\n介绍：中国计算机学会（CCF）主办，是国内计算机视觉领域最主要的学术活动之一。\nICGIP（中国青岛）\n全称： the 9th International Conference on Graphic and Image Processing\n时间：2017.10.13-15\n地点：中国海洋大学\n官网：http://www.icgip.org/\n介绍：在过去的8年中已经先后在亚庇、马尼拉、开罗、新加坡、香港、北京、东京成功举办，旨在汇集图形与图像处理领域的研究人员，科学家，工程师，学者和学生，分享他们的经验，进行思想交流，分型研究成果，并讨论所遇到的实际挑战和采取的解决方案。\nMMSP 2017 （英国伦敦）\n全称：IEEE 19th International Workshop on Multimedia Signal Processing\n时间：2017.10.16-18\n地点：London-Luton, UK\n介绍：多媒体信号处理领域国际研讨会，今年增加了多媒体信号处理在医疗保健和老人养护方面应用的主题\n官网：http://mmsp2017.eee.strath.ac.uk/\nICCV 2017（意大利威尼斯）\n全称： IEEE International Conference on Computer Vision\n时间：2017.10.22-29\n地点：Venice, Italy\n官网：http://iccv2017.thecvf.com/\n介绍：计算机视觉领域三大顶会之一\n主题：\nComputational photography, sensing and display\n3D computer vision\nLow-level vision and image processing\nFace and gesture\nOptimization methods\nMotion and tracking\nRecognition: detection, categorization, indexing, matching\nPhysics-based vision, photometry and shape-from-X\nStatistical methods and learning\nSegmentation, grouping and shape representation\nApplications\nVideo: events, activities and surveillance\nNIPS 2017（美国加州）\n全称：The Conference and Workshop on Neural Information Processing Systems\n时间：2017.12.04-09\n地点：Long Beach，California USA\n官网：https://nips.cc/Conferences/2017\n主题：machine learning , computational neuroscience\nSIGGRAPH AISA 2017（泰国曼谷）\n时间：2017.12.27-30\n地点：Bangkok International Trade and Exhibition Centre (BITEC)，Bangkok, Thailand\n主题：Computer Graphics and Interactive Techniques\n介绍：计算机图形学和交互技术顶级会议\n3D VISON 2017(中国青岛)\n时间：待定\n会议主题：3D research in computer vision and graphics,\n会议涉及： novel optical sensors, signal processing, geometric modeling, representation and transmission, to visualization and interaction, and a variety of applications"}
{"content2":"理论推导\n机器学习所针对的问题有两种：一种是回归，一种是分类。回归是解决连续数据的预测问题，而分类是解决离散数据的预测问题。线性回归是一个典型的回归问题。其实我们在中学时期就接触过，叫最小二乘法。\n线性回归试图学得一个线性模型以尽可能准确地预测输出结果。\n先从简单的模型看起：\n首先，我们只考虑单组变量的情况，有：\n使得\n假设有m个数据，我们希望通过x预测的结果f(x)来估计y。其中w和b都是线性回归模型的参数。\n为了能更好地预测出结果，我们希望自己预测的结果f(x)与y的差值尽可能地小，所以我们可以写出代价函数（cost function）如下：\n接着代入f(x)的公式可以得到：\n不难看出，这里的代价函数表示的是预测值f(x)与实际值y之间的误差的平方。它对应了常用的欧几里得距离简称“欧氏距离”。基于均方误差最小化来求解模型的方法我们叫做“最小二乘法”。在线性回归中，最小二乘法实质上就是找到一条直线，使所有样本数据到该直线的欧式距离之和最小，即误差最小。\n我们希望这个代价函数能有最小值，那么就分别对其求w和b的偏导，使其等于0，求解方程。\n先求偏导，得到下面两个式子：\n很明显，公式中的参数m，b，w都与i无关，简化时可以直接提出来。\n另这两个偏导等于0：\n求解方程组，解得：\n这样根据数据集中给出的x和y，我们可以求出w和b来构建简单的线性模型来预测结果。\n接下来，推广到更一般的情况：\n我们假设数据集中共有m个样本，每个样本有n个特征，用X矩阵表示样本和特征，是一个m×n的矩阵：\n用Y矩阵表示标签，是一个m×1的矩阵：\n为了构建线性模型，我们还需要假设一些参数：\n（有时还要加一个偏差（bias）也就是， 为了推导方便没加，实际上结果是一样的）\n好了，我们可以表示出线性模型了：\nh(x)表示假设，即hypothesis。通过矩阵乘法，我们知道结果是一个n×1的矩阵。\n跟前面推导单变量的线性回归模型时一样，列出代价函数：\n这里的1/2并无太大意义，只是为了求导时能将参数正好消掉而加上。\n代价函数代表了误差，我们希望它尽可能地小，所以要对它求偏导并令偏导数为0，求解方程。\n在求偏导之前先展开一下：\n接下来对 求导，先给出几个矩阵求导的公式：\n对代价函数 求关于 的偏导，并令其等于0。\n求偏导。\n套用前面给出的矩阵求导公式。\n最后化简得到：\n好了，另这个偏导数等于0：\n解得：\nOK，推导完毕。\n把知识点梳理一遍发现清楚了很多。写公式真的很累，明天再把线性回归的代码补上。\n(ง •̀_•́)ง"}
{"content2":"引言：\n机器学习领域中所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x->y，其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。\n目前大部分降维算法处理向量表达的数据，也有一些降维算法处理高阶张量表达的数据。之所以使用降维后的数据表示是因为在原始的高维空间中，包含有冗余信息以及噪音信息，在实际应用例如图像识别中造成了误差，降低了准确率；而通过降维,我们希望减少 冗余信息 所造成的误差,提高识别（或其他应用）的精度。又或者希望通过降维算法来寻找数据内部的本质结构特征。\n在很多算法中，降维算法成为了数据预处理的一部分，如PCA。事实上，有一些算法如果没有降维预处理，其实是很难得到很好的效果的。\n注：我写的东西有一些口语化，而且受限于网页blog的编辑功能，很多地方可能有一些简单。\n主成分分析算法（PCA）\nPrincipal Component Analysis(PCA)是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。\n通俗的理解，如果把所有的点都映射到一起，那么几乎所有的信息（如点和点之间的距离关系）都丢失了，而如果映射后方差尽可能的大，那么数据点则会 分散开来，以此来保留更多的信息。可以证明，PCA是丢失原始数据信息最少的一种线性降维方式。（实际上就是最接近原始数据，但是PCA并不试图去探索数 据内在结构）\n设 n 维向量w为目标子空间的一个坐标轴方向（称为映射向量），最大化数据映射后的方差，有：\n其中 m 是数据实例的个数， xi是数据实例 i 的向量表达， x拔是所有数据实例的平均向量。定义W为包含所有映射向量为列向量的矩阵，经过线性代数变换，可以得到如下优化目标函数：\n其中tr表示矩阵的迹，  A是数据协方差矩阵。\n容易得到最优的W是由数据协方差矩阵前 k 个最大的特征值对应的特征向量作为列向量构成的。这些特征向量形成一组正交基并且最好地保留了数据中的信息。\nPCA的输出就是Y = W‘X，由X的原始维度降低到了k维。\nPCA追求的是在降维之后能够最大化保持数据的内在信息，并通过衡量在投影方向上的数据方差的大小来衡量该方向的重要性。但是这样投影以后对数据 的区分作用并不大，反而可能使得数据点揉杂在一起无法区分。这也是PCA存在的最大一个问题，这导致使用PCA在很多情况下的分类效果并不好。具体可以看 下图所示，若使用PCA将数据点投影至一维空间上时，PCA会选择2轴，这使得原本很容易区分的两簇点被揉杂在一起变得无法区分；而这时若选择1轴将会得 到很好的区分结果。\nDiscriminant Analysis所追求的目标与PCA不同，不是希望保持数据最多的信息，而是希望数据在降维后能够很容易地被区分开来。后面会介绍LDA的方法，是另一 种常见的线性降维方法。另外一些非线性的降维方法利用数据点的局部性质，也可以做到比较好地区分结果，例如LLE，Laplacian Eigenmap等。以后会介绍。\nLDA\nLinear Discriminant Analysis(也有叫做Fisher Linear Discriminant)是一种有监督的（supervised）线性降维算法。与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分！\n假设原始数据表示为X，（m*n矩阵，m是维度，n是sample的数量）\n既然是线性的，那么就是希望找到映射向量a， 使得 a‘X后的数据点能够保持以下两种性质：\n1、同类的数据点尽可能的接近（within class）\n2、不同类的数据点尽可能的分开（between class）\n所以呢还是上次PCA用的这张图，如果图中两堆点是两类的话，那么我们就希望他们能够投影到轴1去（PCA结果为轴2），这样在一维空间中也是很容易区分的。\n接下来是推导，因为这里写公式很不方便，我就引用Deng Cai老师的一个ppt中的一小段图片了：\n思路还是非常清楚的，目标函数就是最后一行J（a)，μ（一飘）就是映射后的中心用来评估类间距，s（一瓢）就是映射后的点与中心的距离之和用来评估类内距。J(a)正好就是从上述两个性质演化出来的。\n因此两类情况下：\n加上a’a=1的条件（类似于PCA）\n可以拓展成多类：\n以上公式推导可以具体参考pattern classification书中的相应章节，讲fisher discirminant的\nOK，计算映射向量a就是求最大特征向量，也可以是前几个最大特征向量组成矩阵A=[a1,a2,….ak]之后，就可以对新来的点进行降维了： y = A’X （线性的一个好处就是计算方便！）\n可以发现，LDA最后也是转化成为一个求矩阵特征向量的问题，和PCA很像，事实上很多其他的算法也是归结于这一类，一般称之为谱（spectral）方法。\n线性降维算法我想最重要的就是PCA和LDA了，后面还会介绍一些非线性的方法。\n局部线性嵌入 （LLE）\nLocally linear embedding（LLE）[1] 是一种非线性降维算法，它能够使降维后的数据较好地保持原有 流形结构 。LLE可以说是流形学习方法最经典的工作之一。很多后续的流形学习、降维方法都与LLE有密切联系。\n见，使用LLE将三维数据（b）映射到二维（c）之后，映射后的数据仍能保持原有的数据流形（红色的点互相接近，蓝色的也互相接近），说明LLE有效地保持了数据原有的流行结构。\n但是LLE在有些情况下也并不适用，如果数据分布在整个封闭的球面上，LLE则不能将它映射到二维空间，且不能保持原有的数据流形。那么我们在处理数据中，首先假设数据不是分布在闭合的球面或者椭球面上。\nLLE降维算法使用实例\nLLE算法认为每一个数据点都可以由其近邻点的线性加权组合构造得到。算法的主要步骤分为三步：(1)寻找每个样本点的k个近邻点；（2）由每个 样本点的近邻点计算出该样本点的局部重建权值矩阵；（3）由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。具体的算法流程如所示：\n图 2 LLE算法步骤\nLaplacian Eigenmaps 拉普拉斯特征映射\n继续写一点经典的降维算法，前面介绍了PCA,LDA，LLE，这里讲一讲Laplacian Eigenmaps。其实不是说每一个算法都比前面的好，而是每一个算法都是从不同角度去看问题，因此解决问题的思路是不一样的。这些降维算法的思想都很 简单，却在有些方面很有效。这些方法事实上是后面一些新的算法的思路来源。\nLaplacian Eigenmaps[1] 看问题的角度和LLE有些相似，也是用局部的角度去构建数据之间的关系。\n它的直观思想是希望相互间有关系的点（在图中相连的点）在降维后的空间中尽可能的靠近。Laplacian Eigenmaps可以反映出数据内在的流形结构。\n使用时算法具体步骤为：\n步骤1：构建图\n使用某一种方法来将所有的点构建成一个图，例如使用KNN算法，将每个点最近的K个点连上边。K是一个预先设定的值。\n步骤2：确定权重\n确定点与点之间的权重大小，例如选用热核函数来确定，如果点i和点j相连，那么它们关系的权重设定为：\n使用最小的m个非零特征值对应的特征向量作为降维后的结果输出。\n前面提到过，Laplacian Eigenmap具有区分数据点的特性，可以从下面的例子看出：\nLaplacian Eigenmap实验结果\n见所示，左边的图表示有两类数据点（数据是图片），中间图表示采用Laplacian Eigenmap降维后每个数据点在二维空间中的位置，右边的图表示采用PCA并取前两个主要方向投影后的结果，可以清楚地看到，在此分类问题 上，Laplacian Eigenmap的结果明显优于PCA。\nroll数据的降维\n说明的是，高维数据（图中3D）也有可能是具有低维的内在属性的（图中roll实际上是2D的），但是这个低维不是原来坐标表示，例如如果要保持局部关系，蓝色和下面黄色是完全不相关的，但是如果只用任何2D或者3D的距离来描述都是不准确的。\n下面三个图是Laplacian Eigenmap在不同参数下的展开结果（降维到2D），可以看到，似乎是要把整个带子拉平了。于是蓝色和黄色差的比较远。\n文章出处：http://blog.csdn.net/xbinworld?viewmode=contents\n转自：https://blog.csdn.net/rosenor1/article/details/52278116"}
{"content2":"继续是机器学习课程的笔记，本节课内容是异常检测，它是一个非监督学习算法，用于发现可能不应该属于一个已定义的组中的数据。\n密度估计\n首先是给出一个例子，如下图所示，是一个测试飞机引擎的例子，给定数据集{\nx(1),x(2),…,x(m)\nx^{(1)},x^{(2)},\\ldots,x^{(m)}},假设数据集是正确的，我们希望知道新的数据\nxtest\nx_{test}是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性\np(x)\np_{(x)}。\n在上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。\n这种方法称为密度估计，表达如下：\nifp(x){≤ϵanomaly>ϵnormal\nif\\quad p(x) \\begin{cases} \\le \\epsilon \\quad anomaly \\\\ \\gt \\epsilon \\quad normal \\end{cases}\n异常检测主要用来识别欺骗。\n例如，在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登陆一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。\n再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，CPU的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是否有可能出错了。\n高斯分布\n接下来回顾下高斯分布的基本知识。\n通常如果我们认为变量x符合高斯分布，即\nx∼N(μ,σ2)\nx \\sim N(\\mu,\\sigma^2),则其概率密度函数为：\np(x,μ,σ2)=1(√2π)σexp−((x−μ)22σ2)\np(x,\\mu,\\sigma^2)=\\frac{1}{\\sqrt(2\\pi)\\sigma}exp^{-(\\frac{(x-\\mu)^2}{2\\sigma^2})}\n下图是\nμ\n\\mu和\nσ2\n\\sigma^2取不同值时，高斯分布的曲线图例子：\n我们可以利用已有的数据来预测总体中的\nμ\n\\mu和\nσ2\n\\sigma^2，计算方法如下：\nμ=1m∑i=1mx(i)σ2=1m∑i=1m(x(i)−μ)2\n\\mu = \\frac{1}{m} \\sum_{i=1}^m x^{(i)} \\\\ \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}-\\mu)^2\n注意，机器学习中对于方程，我们通常只除以m而非统计学中的（m-1）。\n异常检测算法\n这里将使用高斯分布来开发异常检测算法。\n对于给定数据集{\nx(1),x(2),…,x(m)\nx^{(1)},x^{(2)},\\ldots,x^{(m)}}，我们要选择一个认为可以找出异常例子的特征\nxi\nx_i,然后计算其\nμ\n\\mu和\nσ2\n\\sigma^2的估计值。\nμj=1m∑i=1mx(i)σ2j=1m∑i=1m(x(i)j−μj)2\n\\mu_j = \\frac{1}{m} \\sum_{i=1}^m x^{(i)} \\\\ \\sigma^2_j = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}_j-\\mu_j)^2\n一旦获得了\nμ\n\\mu和\nσ2\n\\sigma^2的估计值，给定新的一个训练实例，根据模型计算\np(x)\np(x):\np(x)=∏j=1np(xj;μj,σ2j)=∏j=1n1(√2π)σjexp−((xj−μj)22σ2j)\np(x) = \\prod_{j=1}^n p(x_j;\\mu_j,\\sigma^2_j)=\\prod_{j=1}^n \\frac{1}{\\sqrt(2\\pi)\\sigma_j}exp^{-(\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})}\n然后设置一个阈值\nϵ\n\\epsilon,当\np(x)<ϵ\np(x) \\lt \\epsilon时，可以认为该测试数据是一个异常数据。\n下图是一个有两个特征的训练集，以及特征的分布情况：\n下面的三维图表示的是密度估计函数，z轴为根据两个特征的值所估计的\np(x)\np(x)值：\n我们选择一个\nϵ\n\\epsilon,将\np(x)=ϵ\np(x) = \\epsilon作为我们的判定边界，当\np(x)>ϵ\np(x) \\gt \\epsilon时预测数据为正常数据，否则就是异常数据。\n评价一个异常检测系统\n接下来是介绍如何评价一个异常检测系统。\n异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量y的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。\n当我们开发一个异常检测系统时，我们从带标记（异常或者正常）的数据着手，从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉验证集和测试集。\n例如，我们有10000台正常引擎的数据，20台异常引擎的数据。我们可以这样分配数据：\n6000台正常引擎的数据作为训练集\n2000台正常引擎和10台异常引擎的数据作为交叉验证集\n2000台正常引擎和10台异常引擎的数据作为测试集\n具体的评价方法如下：\n根据测试集数据，我们估计特征的平均值和方差并构建\np(x)\np(x)函数\n对交叉验证集，尝试使用不同的\nϵ\n\\epsilon值作为阈值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择\nϵ\n\\epsilon\n选出\nϵ\n\\epsilon后，针对测试集进行预测，计算异常检验系统的F1值或者查准率与查全率之比。\n异常检测与监督学习之比\n上一小节在评估异常检测算法的时候，是使用带有标记的数据，这与监督学习有些相似，两者的对比如下所示。\n异常检测\n监督学习\n非常少量的正向类（异常数据 y=1），大量的负向类（y=0）\n同时有大量的正向类和负向类\n许多不同种类的异常，非常难根据非常少量的正向类数据来训练算法\n有足够多的正向类，足够用于训练算法\n未来遇到的异常可能与已掌握的异常非常的不同\n未来遇到的正向类实例可能与训练集中国的非常相似\n使用的例子，有如欺诈行为检测，生产（如飞机引擎），检测数据中心的计算机运行状况。\n使用的例子，如邮件过滤器，天气预报，肿瘤分类\n选择特征\n对于异常检测算法，使用的特征是至关重要的，下面将介绍如何选择特征。\n异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但最好还是将数据转换成高斯分布。例如使用对数函数\nx=log(x+c)\nx=log(x+c),其中c为非负尝试；或者\nx=xc\nx=x^c，c是0-1之间的一个小数，等方法。如下图是一个使用对数函数进行转换成高斯分布的例子。\n对于如何选择特征，可以借助误差分析。我们希望的是对于正常数据可以得到大的\np(x)\np(x)，而异常数据是得到小的\np(x)\np(x)值。\n但一个常见的问题是一些异常的数据也会有较高的\np(x)\np(x)值，因而被认为是正常的。\n这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些特征后获得的新算法能够帮助我们更好地进行异常检测。\n我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小）。\n例如在检测数据中心的计算机状况的例子中，我们可以用CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大便有可能意味着该服务器是陷入了一些问题中。\n多元高斯分布\n假设有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。\n下图是两个相关特征，红色的线（根据\nϵ\n\\epsilon的不同，范围也可大可小）是一般的高斯分布模型获得的判断边界，很明显绿色的X所代表的数据点可能是异常值，但是其\np(x)\np(x)值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判断边界。\n在一般的高斯分布模型中，计算\np(x)\np(x)的方法如下：\np(x)=∏j=1np(xj;μj,σ2j)=∏j=1n1(√2π)σjexp−((xj−μj)22σ2j)\np(x) = \\prod_{j=1}^n p(x_j;\\mu_j,\\sigma^2_j)=\\prod_{j=1}^n \\frac{1}{\\sqrt(2\\pi)\\sigma_j}exp^{-(\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})}\n通过分别计算每个特征对应的几率，然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起计算\np(x)\np(x)。\n首先计算所有特征的平均值：\nμ=1m∑mi=1x(i)\n\\mu =\\frac{1}{m} \\sum_{i=1}^m x^{(i)};\n其次，计算协方差矩阵：\n∑=1m∑mi=1(x(i)−μ)(x(i)−μ)T=1m(X−μ)T(X−μ)\n\\sum = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}-\\mu)(x^{(i)}-\\mu)^T=\\frac{1}{m}(X-\\mu)^T(X-\\mu)\n注意，其中\nμ\n\\mu是一个向量，其每一个单元都是原特征矩阵中一行数据的的均值。\n最后计算多元高斯分布的\np(x)=1(2π)n2|∑|12exp(−12(x−μ)T∑−1(x−μ))\np(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\sum|^{\\frac{1}{2}}}exp^{(-\\frac{1}{2}(x-\\mu)^T\\sum^{-1}(x-\\mu))}\n其中：\n|∑|\n|\\sum|是定矩阵，在Octave中用det(sigma)计算\n∑−1\n\\sum^{-1}是逆矩阵\n下面看看协方差矩阵是如何影响模型的：\n上图是5个不同的模型，从左往右依次分析：\n是一个一般的高斯分布模型\n通过协方差矩阵，令特征1有较小的偏差，同时保持特征2的偏差\n通过协方差矩阵，令特征2有较大的偏差，同时保持特征1的偏差\n通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性\n通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性\n多元高斯分布模型与原高斯分布模型的关系：\n可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1，2,3个例子，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。\n原高斯分布模型与多元高斯模型比较如下：\n原高斯分布模型\n多元高斯分布模型\n不能捕捉特征之间的相关性但可以通过将特征进行组合的方法来解决\n自动捕捉特征之间的相关性\n计算代价低，能适应大规模的特征\n计算代价较高\n训练集较小时也同样适用\n必须要有\nm>n\nm \\gt n,不然的话协方差矩阵是不可逆的，通常需要\nm>10n\nm \\gt 10n，另外特征冗余也会导致协方差矩阵不可逆\n原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新特征的方法来捕捉这些相关性。\n如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。\n小结\n本节内容，主要是介绍异常检测算法，其实现的具体细节以及应用的一些场景。"}
{"content2":"要搞清它们的关系，最直观的表述方式就是同心圆，最先出现的是理念，然后是机器学习，当机器学习繁荣之后就出现了深度学习，今天的AI大爆发是由深度学习驱动的。\nAI（人工智能）是未来，是科幻小说，是我们日常生活的一部分。所有论断都是正确的，只是要看你所谈到的AI到底是什么。\n例如，当谷歌DeepMind开发的AlphaGo程序打败韩国职业围棋高手LeeSe-dol，媒体在描述DeepMind的胜利时用到了AI、机器学习、深度学习等术语。AlphaGo之所以打败LeeSe-dol，这三项技术都立下了汗马功劳，但它们并不是一回事。\n要搞清它们的关系，最直观的表述方式就是同心圆，最先出现的是理念，然后是机器学习，当机器学习繁荣之后就出现了深度学习，今天的AI大爆发是由深度学习驱动的。\n从衰败到繁荣\n1956年，在达特茅斯会议（DartmouthConferences）上，计算机科学家首次提出了“AI”术语，AI由此诞生，在随后的日子里，AI成为实验室的“幻想对象”。几十年过去了，人们对AI的看法不断改变，有时会认为AI是预兆，是未来人类文明的关键，有时认为它只是技术垃圾，只是一个轻率的概念，野心过大，注定要失败。坦白来讲，直到2012年AI仍然同时具有这两种特点。\n在过去几年里，AI大爆发，2015年至今更是发展迅猛。之所以飞速发展主要归功于GPU的广泛普及，它让并行处理更快、更便宜、更强大。还有一个原因就是实际存储容量无限拓展，数据大规模生成，比如图片、文本、交易、地图数据信息。\nAI：让机器展现出人类智力\n回到1956年夏天，在当时的会议上，AI先驱的梦想是建造一台复杂的机器（让当时刚出现的计算机驱动），然后让机器呈现出人类智力的特征。\n这一概念就是我们所说的“强人工智能（GeneralAI）”，也就是打造一台超棒的机器，让它拥有人类的所有感知，甚至还可以超越人类感知，它可以像人一样思考。在电影中我们经常会看到这种机器，比如C-3PO、终结者。\n还有一个概念是“弱人工智能（NarrowAI）”。简单来讲，“弱人工智能”可以像人类一样完成某些具体任务，有可能比人类做得更好，例如，Pinterest服务用AI给图片分类，Facebook用AI识别脸部，这就是“弱人工智能”。\n上述例子是“弱人工智能”实际使用的案例，这些应用已经体现了一些人类智力的特点。怎样实现的？这些智力来自何处？带着问题我们深入理解，就来到下一个圆圈，它就是机器学习。机器学习：抵达AI目标的一条路径\n大体来讲，机器学习就是用算法真正解析数据，不断学习，然后对世界中发生的事做出判断和预测。此时，研究人员不会亲手编写软件、确定特殊指令集、然后让程序完成特殊任务，相反，研究人员会用大量数据和算法“训练”机器，让机器学会如何执行任务。\n机器学习这个概念是早期的AI研究者提出的，在过去几年里，机器学习出现了许多算法方法，包括决策树学习、归纳逻辑程序设计、聚类分析(Clustering)、强化学习、贝叶斯网络等。正如大家所知的，没有人真正达到“强人工智能”的终极目标，采用早期机器学习方法，我们连“弱人工智能”的目标也远没有达到。\n在过去许多年里，机器学习的最佳应用案例是“计算机视觉”，要实现计算机视觉，研究人员仍然需要手动编写大量代码才能完成任务。研究人员手动编写分级器，比如边缘检测滤波器，只有这样程序才能确定对象从哪里开始，到哪里结束；形状侦测可以确定对象是否有8条边；分类器可以识别字符“S-T-O-P”。通过手动编写的分组器，研究人员可以开发出算法识别有意义的形象，然后学会下判断，确定它不是一个停止标志。\n这种办法可以用，但并不是很好。如果是在雾天，当标志的能见度比较低，或者一棵树挡住了标志的一部分，它的识别能力就会下降。直到不久之前，计算机视觉和图像侦测技术还与人类的能力相去甚远，因为它太容易出错了。深度学习：实现机器学习的技术\n“人工神经网络（ArtificialNeuralNetworks）”是另一种算法方法，它也是早期机器学习专家提出的，存在已经几十年了。神经网络（NeuralNetworks）的构想源自于我们对人类大脑的理解——神经元的彼此联系。二者也有不同之处，人类大脑的神经元按特定的物理距离连接的，人工神经网络有独立的层、连接，还有数据传播方向。\n例如，你可能会抽取一张图片，将它剪成许多块，然后植入到神经网络的第一层。第一层独立神经元会将数据传输到第二层，第二层神经元也有自己的使命，一直持续下去，直到最后一层，并生成最终结果。\n每一个神经元会对输入的信息进行权衡，确定权重，搞清它与所执行任务的关系，比如有多正确或者多么不正确。最终的结果由所有权重来决定。以停止标志为例，我们会将停止标志图片切割，让神经元检测，比如它的八角形形状、红色、与众不同的字符、交通标志尺寸、手势等。\n神经网络的任务就是给出结论：它到底是不是停止标志。神经网络会给出一个“概率向量”，它依赖于有根据的推测和权重。在该案例中，系统有86%的信心确定图片是停止标志，7%的信心确定它是限速标志，有5%的信心确定它是一支风筝卡在树上，等等。然后网络架构会告诉神经网络它的判断是否正确。\n即使只是这么简单的一件事也是很超前的，不久前，AI研究社区还在回避神经网络。在AI发展初期就已经存在神经网络，但是它并没有形成多少“智力”。问题在于即使只是基本的神经网络，它对计算量的要求也很高，因此无法成为一种实际的方法。尽管如此，还是有少数研究团队勇往直前，比如多伦多大学GeoffreyHinton所领导的团队，他们将算法平行放进超级电脑，验证自己的概念，直到GPU开始广泛采用我们才真正看到希望。\n回到识别停止标志的例子，如果我们对网络进行训练，用大量的错误答案训练网络，调整网络，结果就会更好。研究人员需要做的就是训练，他们要收集几万张、甚至几百万张图片，直到人工神经元输入的权重高度精准，让每一次判断都正确为止——不管是有雾还是没雾，是阳光明媚还是下雨都不受影响。这时神经网络就可以自己“教”自己，搞清停止标志的到底是怎样的；它还可以识别Facebook的人脸图像，可以识别猫——吴恩达（AndrewNg）2012年在谷歌做的事情就是让神经网络识别猫。\n吴恩达的突破之处在于：让神经网络变得无比巨大，不断增加层数和神经元数量，让系统运行大量数据，训练它。吴恩达的项目从1000万段YouTube视频调用图片，他真正让深度学习有了“深度”。\n到了今天，在某些场景中，经过深度学习技术训练的机器在识别图像时比人类更好，比如识别猫、识别血液中的癌细胞特征、识别MRI扫描图片中的肿瘤。谷歌AlphaGo学习围棋，它自己与自己不断下围棋并从中学习。\n有了深度学习AI的未来一片光明\n有了深度学习，机器学习才有了许多实际的应用，它还拓展了AI的整体范围。深度学习将任务分拆，使得各种类型的机器辅助变成可能。无人驾驶汽车、更好的预防性治疗、更好的电影推荐要么已经出现，要么即使出现。AI既是现在，也是未来。有了深度学习的帮助，也许到了某一天AI会达到科幻小说描述的水平，这正是我们期待已久的。你会有自己的C-3PO，有自己的终结者。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n未来人工智能技术，主要包含哪几种？\nhttp://www.duozhishidai.com/article-4938-1.html\n人工智能技术在计算机中的发展和应用\nhttp://www.duozhishidai.com/article-3989-1.html\n人工智能时代，你需要了解的9大技术领域\nhttp://www.duozhishidai.com/article-3845-1.html\n[多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站](http://www.duozhishidai.com)"}
{"content2":"人工智能之机器学习\n人工智能课程复习笔记专题\n人工智能绪论\n人工智能之知识表示\n人工智能之搜索方法\n人工智能之经典逻辑推理\n人工智能之专家系统\n人工智能之不确定推理方法\n人工智能之机器学习\n一、遗传算法\n1、基本概念\n个体与种群\n个体：问题中的对象（解），搜索空间的一个点\n种群：若干个个体组成的群体，搜索空间的一个很小的子集\n适应度与适应度函数\n适应度：对环境的适应程度，表征器优劣的一种测度\n适应度函数：全体个体与适应度的对应关系，知道搜索的评价函数\n染色体与基因\n染色体：问题中个体的某种字符表示方式\n基因：字符串中的字符\n例如：\n个体 染色体\n9 —- 1001\n（2，5，6）—- 010 101 110\n遗传算子\n1）选择-复制\n对于一个规模为N的种群S，按每个染色体xi∈S的选择概率P(xi)所决定的选中机会，分N次从S中随机选定N个染色体，进行复制。\n2）交叉\n互换两个染色体某些位上的基因\n3）基因\n改变染色体某些位上的基因\n2、基本遗传算法\n步1 在搜索空间U上定义一个适应度函数f(x)，给定种群规模N，交叉率Pc和变异率Pm，代数T； 步2 随机产生U中的N个个体s1, s2, …, sN，组成初始种群S={s1, s2, …, sN}，置代数计数器t=1； 步3 计算S中每个个体的适应度f() ； 步4 若终止条件满足，则取S中适应度最大的个体作为所求结果，算法结束。 步5 按选择概率P(xi)所决定的选中机会，每次从S中随机选定1个个体并将其染色体复制，共做N次，然后将复制所得的N个染色体组成群体S1； 步6 按交叉率Pc所决定的参加交叉的染色体数c，从S1中随机确定c个染色体，配对进行交叉操作，并用产生的新染色体代替原染色体，得群体S2； 步7 按变异率Pm所决定的变异次数m，从S2中随机确定m个染色体，分别进行变异操作，并用产生的新染色体代替原染色体，得群体S3； 步8 将群体S3作为新一代种群，即用S3代替S，t = t+1，转步3；\n3、遗传算法应用举例\n例：利用遗传算法求解区间［0,31］上的二次函数y=x2的最大值\n解\n(1) 设定种群规模,编码染色体，产生初始种群。\n将种群规模设定为4；用5位二进制数编码染色体；取下列个体组成初始种群S1:\ns1= 13 (01101), s2= 24 (11000)\ns3= 8 (01000), s4= 19 (10011)\n(2) 定义适应度函数,\n取适应度函数：f (x)=x2\n(3) 计算各代种群中的各个体的适应度, 并对其染色体进行遗传操作,直到适应度最高的个体(即31（11111）)出现为止。\n首先计算种群S1中各个体\ns1= 13(01101), s2= 24(11000)\ns3= 8(01000), s4= 19(10011)\n的适应度f (si) 。\n容易求得\nf (s1) = f(13) = 132 = 169\nf (s2) = f(24) = 242 = 576\nf (s3) = f(8) = 82 = 64\nf (s4) = f(19) = 192 = 361\n再计算种群S1中各个体的选择概率。\n选择概率的计算公式为\n由此可求得\nP(s1) = P(13) = 0.14\nP(s2) = P(24) = 0.49\nP(s3) = P(8) = 0.06\nP(s4) = P(19) = 0.31\n在算法中赌轮选择法可用下面的子过程来模拟: 　① 在［0, 1］区间内产生一个均匀分布的随机数r。\n② 若r≤q1,则染色体x1被选中。\n③ 若qk-1 < r ≤ qk(2≤k≤N), 则染色体xk被选中。 其中的qi称为染色体xi (i=1, 2, …, n)的积累概率\n交叉\n设交叉率pc=100%，即S1中的全体染色体都参加交叉运算。\n设s1’与s2’配对，s3’与s4’配对。分别交换后两位基因，得新染色体：\ns1’’=11001（25）, s2’’=01100（12）\ns3’’=11011（27）, s4’’=10000（16）\n变异\n设变异率pm=0.001。\n这样，群体S1中共有\n5×4×0.001=0.02\n位基因可以变异。\n0.02位显然不足1位，所以本轮遗传操作不做变异。\n于是，得到第二代种群S2：\ns1=11001（25）, s2=01100（12）\ns3=11011（27）, s4=10000（16）\n依次类推直至求出解。\n二、神经网络\n人工神经网络是由 具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。\n神经元的信息传递和处理是一种电化学活动．树突由于电化学作用接受外界的刺激；通过胞体内的活动体现为轴突电位，当轴突电位达到一定的值则形成神经脉冲或动作电位；再通过轴突末梢传递给其它的神经元．从控制论的观点来看；这一过程可以看作一个多输入单输出非线性系统的动态过程\n脑神经信息活动特征\n1）巨量并行性\n2）信息处理和存储单元结合在一起\n3）自组织自学习功能\n神经元的数学模型\n其中x＝（x1，…xm）T 输入向量，y为输出，wi是权系数；输入与输出具有如下关系：\nθ为阈值，f（X）是激发函数；它可以是线性函数，也可以是非线性函数．\nBP神经网络\n本篇内容过于简单，若要深入学习见本人博文机器学习&深度学习算法及代码实现"}
{"content2":"人工智能（AI）曾经只是科幻电影，电视节目和书籍中探讨的一个话题，如今已经迅速成为现实世界的一部分。 1969年，管理咨询公司麦肯锡公司（McKinsey＆Company）发表了一篇文章，声称计算机不够聪明，无法作出任何决定，只是设备背后的人类智慧在助力它们。\n随着现代计算机在医药，农业和教育等领域取代熟练的人力劳动，才明白上面这种说法是多么不正确。有人甚至认为人工智能是未来的出路。而像“人工智能”，“机器学习”和“机器人”这样的流行词汇有时被错误地交替使用，可能容易混淆以至于搞不清楚这个蓬勃发展的行业到底正在发生什么事情。\n理解人工智能，可以在脑海中想象三个不同大小的盒子，最大的盒子是AI，其中放着机器学习的盒子，机器学习盒子中又放着最小的深度学习盒子。\n什么是人工智能？\n一般来说，有两种不同的人工智能：广义AI和狭义AI。AI研究工作的早期，研究人员决定创建具有类似人类智能特征的复杂机器。这些机器本来就是要具备和我们人类一样的所有感性和理性，这是广义AI的范围。广义AI的例子包括星球大战的C3PO或终结者。尽管已经取得了令人瞩目的技术进展，但至今我们还没有开发出这种机器。\n另一方面，狭义的人工智能是我们目前拥有的技术。使用这种形式的人工智能技术显示出一定程度的人类智能，并且可以很好地执行一些特定的任务。狭义人工智能的一个很好的例子可能就在你的口袋里：Siri，Google Now和Amazon Alexa。当我们用语音或者输入信息向自己的手机或者一些其他app发出请求时，智能手机上的这些个人助理可以帮助我们找到有用的信息。为了更好地理解这种智能是怎么来的，我们必须去下一个“盒子”看看：机器学习。\n什么是机器学习？\n与人类如何利用知识和过去的经验来应对新状况和挑战一样，机器学习是与通过算法从数据中分析和学习并利用所学去更好的执行未来的操作有关的技术科学。从某种意义上说，计算机不需要对特定的软件程序进行硬编码，而是只需要“被训练”，并且设定一组数据和算法来帮助它处理特定任务。机器学习的一个例子就是Google DeepMind的AlphaGo程序，该程序学习了如何玩棋盘游戏Go，并持续打败韩国李世石大师。\n什么是机器人？\n跟Alexa（Alexa是预装在亚马逊Echo内的个人虚拟助手，可以接收及相应语音命令，Alexa可以被看成是亚马逊版的Siri语音助手）说要订购更多的鸡蛋，或者直接给它发送一个数字来订购食物，并且全程不需要与人类交谈就可以最终收到食物，这在今天是不常见的。这是机器人背后的基本思想 - 机器人是执行自动化任务的应用程序。如上所述，这些有用的工具（机器人）属于狭义AI的例子，因为它们具有一定程度的人类智能来执行任务。\n结语\n人工智能已经不再是人们梦想有一天能接触到的东西。尽管像人们在大屏幕上看到的类人机器人技术尚不存在，但像苹果，谷歌和亚马逊等公司一直在实现基于计算机的工具，个人可以使用这些工具来执行任务，而不需要另一个人类。这些工具已经变得越来越复杂，有助于让人们的生活变得轻松一点，但是看到这些发展将如何影响社会是非常有意思的。\n交流qq人工智能交流群：862729908"}
{"content2":"人工智能相关岗位中，涉及到的内容包含：\n算法、深度学习、机器学习、自然语言处理、数据结构、Tensorflow、Python 、数据挖掘、搜索开发、神经网络、视觉度量、图像识别、语音识别、推荐系统、系统算法、图像算法、数据分析、概率编程、计算机数学、数据仓库、建模等关键词，基本涵盖了现阶段人工智能细分领域的人才结构。\n将上面的岗位涉及到的知识和技术划类，就形成了今天的五份书单：\n1人工智能科普类：人工智能科普、人工智能哲学\n《智能的本质》斯坦福、伯克利客座教授30年AI研究巅峰之作\n《科学+遇见人工智能》李开复、张亚勤、张首晟等20余位科学家与投资人共同解读AI革命\n《人工智能时代》从人工智能的历史、现状、未来，工业机器人、商业机器人、家用机器人、机器翻译、机器学习等人工智能应用领域依次介绍了人工智能发展前景。\n《人工智能简史》 跟着图灵、冯•诺依曼、香农、西蒙、纽维尔、麦卡锡、明斯基等人工智能的先驱们重走人工智能之路，站在前人的肩膀上，看人工智能的三生三世，鉴以往才能知未来。\n2人工智能机器学习类：Python、机器学习、数据科学\n《Python机器学习实践指南》 结合了机器学习和Python 语言两个热门的领域，通过利用两种核心的机器学习算法来用Python 做数据分析。\n《Python机器学习——预测分析核心算法》从算法和Python语言实现的角度，认识机器学习。\n《机器学习实践应用》阿里机器学习专家力作，实战经验分享，基于阿里云机器学习平台，针对7个具体的业务场景，搭建了完整的解决方案。\n《NLTK基础教程——用NLTK和Python库构建机器学习应用》介绍如何通过NLTK库与一些Python库的结合从而实现复杂的NLP任务和机器学习应用。\n3人工智能深度学习类：深度学习、Tensorflow\n《深度学习》AI圣经，深度学习领域奠基性的经典畅销书 特斯拉CEO埃隆·马斯克等国内外众多专家推荐！\n《深度学习精要（基于R语言）》基于R语言实战,使用无监督学习建立自动化的预测和分类模型\n《TensorFlow技术解析与实战》包揽TensorFlow1.1的新特性 人脸识别 语音识别 图像和语音相结合等热点一应俱全\n《TensorFlow机器学习项目实战》第二代机器学习实战指南，提供深度学习神经网络等项目实战，有效改善项目速度和效率。\n4\n人工智能算法策略类：算法、推荐系统、编程等\n《神经网络算法与实现——基于Java语言》 完整地演示了使用Java开发神经网络的过程，既有非常基础的实例也有高级实例。\n《趣学算法》 50 多个实例循展示算法的设计、实现、复杂性分析及优化过程 培养算法思维 带您感受算法之美。\n《算法谜题》 Google、Facebook等一流IT公司算法面试必备，经典算法谜题合集。\n《Python算法教程》精通Python基础算法，畅销书Python基础教程作者力作。\n《编程之法：面试和算法心得》程序员面试宝典 笔试金典 CSDN访问量过千万的博客结构之法算法之道博主July著作。\n《趣题学算法》 一本有趣的、易学的、实用的，帮助读者快速入门应用的算法书。\n《Java遗传算法编程》 遗传算法设计 机器学习人工智能 来自Java专家的声音 用遗传算法解决类似旅行商的经典问题。\n《算法学习与应用从入门到精通》320个实例、753分钟视频、5个综合案例、74个技术解惑，一本书的容量，讲解了入门类、范例类和项目实战类三类图书的内容。\n5人工智能时间图像和视觉识别类：图像识别 、语音识别、自然语言处理、建模工程\n《OpenCV和Visual Studio图像识别应用开发》无人驾驶人脸识别基础技术 用OpenCV实现图像处理应用 计算机视觉编程实战手册。\n《人脸识别原理及算法——动态人脸识别系统研究》 介绍了动态场景下的人脸识别方法，该方法综合应用了人脸定位、人脸识别、视频处理等算法。\n《精通Python自然语言处理》用Python开发令人惊讶的NLP项目，自然语言处理任务，掌握利用Python设计和构建给予NLP的应用的实践。\n《Python自然语言处理》基于Python编程语言和NLTK，自然语言处理领域的一本实用入门指南。\n《贝叶斯方法：概率编程与贝叶斯推断》 机器学习 人工智能 数据分析从业者的技能基础 国际杰出机器学习专家余凯博士 腾讯专家研究员岳亚丁博士推荐。\n《贝叶斯思维：统计建模的Python学习法》Thin Stats和Think Python图书作者重磅出击，数据分析师、数据工程师、数据科学家案头常备。\n《概率编程实战》人工智能领域的先驱、美国加州大学伯克利分校教授Stuart Russell作序推荐！一本不可思议的Scala概率编程实战书籍！\n《自己动手写神经网络》机器学习与人工智能参考书，基于Java语言撰写。\n粉丝福利\n向公号后台回复以下文字获得\n回复：思维导图 获得Python学习必备思维导图电子版\n回复：量化交易 获得量化投资编程课程与资料\n回复：AI实验     获得20+开源小实验，真枪实战体验AI之趣\n回复：题库        获得人工智能面试题库，帮你顺利拿到offer！\n回复：数据集     获得人工智能1024G数据\n转载：https://mp.weixin.qq.com/s?__biz=MzI4MTQ2NjU5NA==&mid=2247486292&idx=1&sn=4e92e2eb1b6c4803ea16590f98aff897&chksm=eba988d9dcde01cf8335efafe8a14544826dd6321b1d0699031ca0745fea75f3d871d45427bc&mpshare=1&scene=1&srcid=0305LVSinMqFSEAsYBHlIUgi&key=8ed9fb05e5f3ad2686535dca8c17f87a433ab957f6fdd58d5e6cf8e058c6c9a3dc5a6dc7bf5212ec72ebe2ca57430e6f399f59689649126f7b15967e745ee7ad9e1683132465a3d826f903eaaa09afdb&ascene=0&uin=Mjg3Mzg1MjU1&devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.12.6+build(16G1212)&version=12010110&nettype=WIFI&lang=zh_CN&fontScale=100&pass_ticket=dABOs1%2F1ET4uGEs47kpjYQ69BKUcSVS3C68tcFynEZdbolUI0Sr6jOP7%2BgXF8R7E\n3个月挑战年薪30万！"}
{"content2":"1、云ERP平台需要创建和加强自学知识系统，该系统将人工智能和机器学习从车间企业高管，并跨越供应商网络；\n2、虚拟代理有可能重新定义许多制造业务领域，从逐个语音系统到高级诊断；\n3、在数据结构层面设计物联网（IoT），以便在数据收集上和扩展能快速实现；\n4、人工智能和机器学习可以提供有关如何提高整体设备效率(OEE)的见解，而这在当今并不明显；\n5、将机器学习算法设计为跟踪和追溯性，以预测哪些供应商最有可能是优质或劣质的；\n6、云ERP供应商需要关注如何通过人工智能和机器学习来帮助缩小PLM、CAD、ERP和CRM系统之间的配置差距；\n7、通过更高质量的数据，可以提高需求预测的准确性，并基于机器学习的预测模型与供应商进行更好的合作；\n8、通过分析机器级数据来确定何时需要替换某个给定部件，从而减少设备故障和增加资产利用率；\n9、需要在运ERP平台中实现，使用生产报告来预测装配线上生产问题的自学习算法；\n10、通过机器学习算法对产品质量进行汇总、分析和持续学习，从供应商检查、质量控制、退货材料授权(RMA)和产品失效数据等方面进行学习。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n1.人工智能时代，AI人才都有哪些特征？\nhttp://www.duozhishidai.com/article-1792-1.html\n2.大数据携手人工智能，高校人才培养面临新挑战\nhttp://www.duozhishidai.com/article-7555-1.html\n3.人工智能，机器学习和深度学习之间，主要有什么差异\nhttp://www.duozhishidai.com/article-15858-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站"}
{"content2":"本文档记录了《机器学习》第 15 章规则学习相关内容\n基本概念\n形式化定义\n⊕←f1∧f2∧...∧fL\n\\oplus\\leftarrow\\mathbf{f}_1\\wedge\\mathbf{f}_2\\wedge...\\wedge\\mathbf{f}_L\n左侧称为规则头\n右侧称为规则体\nL\nL 为规则的长度\n解决规则冲突\n投票法：判别相同的规则数最多的结果作为最终结果\n排序法：在规则集合上定义一个顺序->带序规则学习/优先级规则学习\n元规则法：定义关于规则的规则指导使用规则集\n规则分类\n命题规则：原子命题+逻辑连接词\n一阶规则：原子公式，谓词、量词\n一阶规则比（逻辑规则？？？）强很多，能表达复杂的关系，称为关系型规则，其语义层面与人类的语义层面一致。\n序贯覆盖\n关键\n如何从训练集学出单条规则\n学习规则的方法\n穷尽搜索\n从空规则开始，将正例类别作为规则头，逐个遍历训练集中的每个属性及取值。\\\n在属性和候选值较多时会存在组合爆炸的问题。\n自顶向下\n从比较一般的规则开始，逐条添加新文字以缩小规则覆盖范围\n生成-测试法\n规则逐渐特化\n对噪声的鲁棒性较强，适用于命题规则学习\n先考虑规则的准确性，然后考虑覆盖的样本数，然后考虑属性次序等等\n自底向上\n从比较特殊的规则开始，逐渐删除文字以扩大规则覆盖范围\n数据驱动法\n规则逐渐泛化\n适用于假设空间较复杂的任务，如一阶规则学习\n剪枝优化\n统计显著性检验\nCN2——似然率统计量LRS\nLRS=2⋅(m̂ +log2(m̂ +m̂ ++m̂ −)(m+m++m−)+m̂ −log2(m̂ −m̂ ++m̂ −)(m−m++m−))\n\\text{LRS}=2\\cdot(\\hat{m}_+\\text{log}_2\\frac{(\\frac{\\hat{m}_+}{\\hat{m}_++\\hat{m}_-})}{(\\frac{m_+}{m_++m_-})}+\\hat{m}_-\\text{log}_2\\frac{(\\frac{\\hat{m}_-}{\\hat{m}_++\\hat{m}_-})}{(\\frac{m_-}{m_++m_-})})\nLRS越大，采用规则集进行预测与直接使用训练集正、反例比例进行猜测的差别越大。\nLRS越小，规则集的效果越可能是偶然现象。\n后剪枝\n减错剪枝REP\n一次训练集学习规则集\n\n\\mathcal{R}\n多轮剪枝：每轮穷举所有可能的简直操作，然后用验证集对剪枝产生的所有候选规则集进行评估，保留最好者\n循环多次\n设训练样本数为\nm\nm，时间复杂度\nO(m4)\nO(m^4)\nIncremental REP\n在REP上改进\n每次生成一条规则\nr\nr立即在验证集上进行剪枝得到规则\nr′\nr'，并将覆盖样例去除\n时间复杂度\nO(mlog2m)\nO(m\\log^2m)\nRIPPER\n使用IREP*剪枝机制生成规则集\n\n\\mathcal{R}\n对\nr∈\nr\\in\\mathcal{R}，生成：\nr′\nr'：基于\nr\nr的覆盖样例，通过IREP*生成的替换规则\nr″\nr''：对\nr\nr增加文字进行特化，然后IREP*生成的修订规则\n将原规则集（\nr\nr）和新规则（替换为\nr′\nr'和\nr″\nr''）分别进行评估，留下最好的\n循环上述过程\n一阶规则学习\n命题规则学习的缺陷：难以处理对象之间的关系。\n引入领域知识\n在现有属性基础上构造新的属性\n基于领域知识设计某种函数机制约束假设空间\nFirst-Order Inductive Learner（FOIL）\n特点\n序贯覆盖\n自顶向下（泛化到特化的过程）\nFOIL增益\nFGain=m̂ +×(log2m̂ +m̂ ++m̂ −−log2m+m++m−)\n\\text{F}_\\text{Gain}=\\hat{m}_+\\times(\\log_2\\frac{\\hat{m}_+}{\\hat{m}_++\\hat{m}_-}-\\log_2\\frac{m_+}{m_++m_-})\nm̂ +\n\\hat{m}_+和\nm̂ +\n\\hat{m}_+分别表示增加候选文字后新规则所覆盖的正负样本数\nm+\nm_+和\nm+\nm_+分别表示原本规则所覆盖的正负样本数\n因为关系数据中的不平衡性，仅考虑正例的信息量\n总结\nFOIL可以被看做是命题规则学习和归纳逻辑程序设计之间的过渡，但其自顶向下的规则生成过程不支持嵌套，所以表达能力仍有不足。\n归纳逻辑程序设计（Inductive Logic Programming，ILP）\n目标：完备的学习一阶规则\n自底向上——特化到泛化的过程，每次学习单条规则\n与普通一阶规则学习相比引入了函数和逻辑表达式的嵌套\n最小一般泛化（Least General Generalization，LGG）\n给定一阶公式\nr1\nr_1和\nr2\nr_2\n找出涉及相同谓词的文字\n常量替换\n在两个公式中出现位置相同——保持\n不同则将它们替换为同一个新变量\n忽略两条公式中不含共同谓词的文字\nR(elative)LGG：初始规则选择方法，考虑所有背景知识。\n逆归结\n演绎（Deduction）和归纳（Induction）\n演绎：从一般性规律出发来探讨具体事物（对应特化）\n归纳：从个别事物出发概括出一般性规律（对应泛化）\n归结和逆归结\n归结：将貌似复杂的逻辑规则与背景知识联系起来化繁为简\n逆归结：基于背景知识发明新的概念和关系\n逆归结形式化定义\n设两个逻辑表达式\nC1\nC_1、\nC2\nC_2成立，且分别包含互补项\nL1\nL_1和\nL2\nL_2，可令\nL=L1=¬L2,C1=A∨L,C2=B∨¬L\nL=L_1=\\neg L_2,C_1=A\\vee L,C_2=B\\vee\\neg L，可以通过归结原理得到归结项\nC=A∨B\nC=A\\vee B。\n与该过程相反，逆归结是研究在已知\nC\nC和某个\nCi\nC_i的情况下如何得到其余\nCj\nC_j：\nC2=(C−(C1−{L}))∨{¬L}\nC_2=(C-(C_1-\\{L\\}))\\vee\\{\\neg L\\}\n逆归结的四种操作\n吸收\n辨识\n内构\n互构\n（×）蕴含、置换和合一（-）\n蕴含：\nX/Y\nX/Y\n置换：用某些项来替换逻辑表达式中的变量\ne.g.\n用\nθ=1/X,2/Y\n\\theta={1/X,2/Y}置换\nC=r1(X,Y)∧r2(X,Y)\nC=r_1(X,Y)\\wedge r_2(X,Y)，可得\nC′=Cθ=r1(1,2)∧r2(1,2)\nC'=C\\theta=r_1(1,2)\\wedge r_2(1,2)\n其中\nX,Y\n{X,Y}称为\nθ\n\\theta的作用域。\n合一：用一种变量置换令两个或多个逻辑表达式相等\ne.g.\n令\nA=r1(1,X),B=r1(Y,2)\nA=r_1(1,X),B=r_1(Y,2)，可以使用\nθ=2/X,1/Y\n\\theta={2/X,1/Y}得到\nAθ=Bθ=r1(1,2)\nA\\theta=B\\theta=r_1(1,2)，在此情况下称\nA\nA和\nB\nB是可合一的，\nθ\n\\theta为合一化子。\n（×）一阶逻辑中利用合一操作搜索互补项\n自动发明新谓词"}
{"content2":"之前朋友参加比赛，想用python做一个全美所有股票的涨幅走势分析，今天我就用分析苹果股票的例子手把手的教大家上手练习苹果股票涨跌图的绘制，至于更专业一点的，还是交给金融方面的大牛来分析吧，我实在看不懂股票～\n还是像之前一样，我们得把机器学习的库引入进jupyter，不得不说jupyter是真心好用，敲一行代码，就能看到一行结果，以至于后面不会忘记每个变量的属性，真是居家旅行必备神器。\nimport pandas as pd from pandas import Series,DataFrame import matplotlib.pyplot as plt import numpy as np\n接下来我们就开始读取数据，数据是我已经找好的一个csv的文件：\napple = pd.read_csv('./AAPL.csv') apple\n我们可以看到从1980年12-12日苹果的股票涨势\napple.shape Out[]: (9458, 7)\n从这里面我们可以看到，一共有9458个数据。我们再来看看数据的类型\napple.dtypes Out[]: Date object Open float64 High float64 Low float64 Close float64 Adj Close float64 Volume float64 dtype: object\n我们可以看到Date数据是str的类型，如果我们要进行操作的话不是很方便，所以接下来先把数据的类型给转化一下，转化类型在mysql中有datetime，而在这里，我们需要用到pd.to_datetime()这个函数。\napple['Date'] = pd.to_datetime(apple['Date']) apple.dtypes Out[]: Date datetime64[ns] Open float64 High float64 Low float64 Close float64 Adj Close float64 Volume float64 dtype: object\n类型转好后，我们将列转为行索引，inplace=True即为修改变量值，使接下来的变量都被修改了：\napple.set_index('Date',inplace=True) apple.head()\n我们接下来开始绘制图形：\nadj_plot = apple['Adj Close'].plot() fig = adj_plot.get_figure() #set_size_inches 设置图片的大小,单位inche fig.set_size_inches(12,6)\n接下来我们继续绘制股票其它的走势，由于Volume这个值太大，把其他效果给遮掩了，我就做了一个处理把它给删除，然后绘制的图没有Volume这个属性：\napple.drop('Volume',axis=1,inplace=True) app = apple.plot() fig1 = app.get_figure() fig1.set_size_inches(12,6)\n以上便是股票绘制的核心部分了，由于博主对股票认知实在有限，如果还想对股票有其他的操作的话，可以在我之前提供的pandas基础教程的基础上尽情的自由发挥。如果想对股票进行预测，可以翻看我之前的博客写到的预测鸢尾花的种类的代码部分。（python机器学习入门到精通–实战分析(三)）"}
{"content2":"前言，AI知识图谱\n人工智能分为强人工智能和弱人工智能。\n强人工智能是通过计算机来构造复杂的、拥有与人类智慧同样本质特性的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考，也就是电影里面的机器人。\n弱人工智能 (ANI) 是指擅长于单个方面的人工智能。垃圾邮件的自动识别，iPhone的助手siri，Pinterest上的图像分类，Facebook的人脸识别都属于弱人工智能，也就是我们现在大多是在从事的领域。\n一，人工智能，机器学习，深度学习关系\n人工智能是追求目标，机器学习是实现手段，深度学习是其中一种方法。\n二，机器学习框架图\n三，机器学习应用领域\n,\n4，深度学习框架图：\n五，人工智能应用领域\n六：中国的人工智能发展\n人工智能企业可以在应用层、技术层、基础上进行区分。\n在应用层的中国人工智能公司按照领域划分包括：\n机器人：Geek+、 Rokid、图灵机器人、优必选。\n自动驾驶：百度、天瞳威视、地平线机器人、驭势科技。\n无人机：大疆、亿航、Hover Camera、零度智控。\n语音助手：百度、出门问问。\n商业智能：永洪科技、Data KM。\n消费者服务：AiKF。\n产业应用：碳云智能、Maxent、今日头条、学霸君。\n在技术层的中国人工智能公司按照领域划分包括：\n语音识别&自然语言处理：\n思必驰、百度、科大讯飞、出门问问、捷通华生、腾讯、三角兽、云知声。\n机器学习&深度学习：深鉴科技、中科视拓。\n人工智能平台：达闼科技、第四范式。\n计算机视觉：依图科技、格灵深瞳、旷视科技、商汤科技。\n在基础层的中国人工智能公司按照领域划分包括：\n传感器：ICE DRINK、LeiShen、SLAMTEC、北醒光子。\nAI 芯片：寒武纪科技、地平线机器人。\n七，人工智能未来企业排行榜\n**\n欢迎扫码关注我的微信公众号\nhttps://github.com/yeyujujishou19\n**"}
{"content2":"【https://zhuanlan.zhihu.com/p/26442277】\n科普贴开篇：到底什么是人工智能（AI）、机器学习（ML）和深度学习（DL）\n优雅的程序员\n9 个月前\n这两年创业圈、技术圈、互联网圈都在热烈讨论人工智能、机器学习、深度学习，那么到底什么是人工智能（AI）、机器学习（ML）和深度学习（DL），这几个概念之间又有什么样的联系呢？先直接把这三者之间关系放上来哈：\n机器学习，实现人工智能的方法；\n深度学习，实现机器学习的技术；\n关于以上三个概念的介绍和解释：\n1、人工智能（英语：Artificial Intelligence, AI）：是指由人工制造出来的系统所表现出来的智能。通常人工智能是指通过普通电脑实现的智能。人工智能的研究可以分为几个技术问题。其分支领域主要集中在解决具体问题，其中之一是，如何使用各种不同的工具完成特定的应用程序。AI的核心问题包括推理、知识、规划、学习、交流、感知、移动和操作物体的能力等。\n目前有大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于仿生学、认知心理学，以及基于概率论和经济学的算法等等也在逐步探索当中。\n2、机器学习（英语：Machine Learning）：是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。\n机器学习有下面几种定义：\n机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。\n机器学习是对能通过经验自动改进的计算机算法的研究。\n机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。\n机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。\n3、深度学习（英语：Deep Learning）是机器学习拉出的分支，它试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。\n深度学习是机器学习中一种基于对数据进行表征学习的方法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。\n一些著名的深度学习库\nTorch Facebook 开源的库，这是一个能让深度学习在即时战略类游戏（RTS）上进行研究的库，比如星际争霸 Brood War，通过从机器学习框架控制这些游戏从而使玩游戏变得更简单。\nTheano 是一个 Python 库，用来定义、优化和模拟数学表达式计算，用于高效的解决多维数组的计算问题。\nDeeplearning4j 为Java和Java虚拟机编写的开源深度学习库，是广泛支持各种深度学习算法的运算框架。Deeplearning4j可以实施的技术包括受限玻尔兹曼机、深度置信网络、深度自动编码器、堆叠式降噪自动编码器、循环神经张量网络，以及word2vec、doc2vec和GloVe。这些算法全部包括分布式并行版本，与Hadoop和Spark集成。Skymind是Deeplearning4j的商业支持机构。\ntensorflow 最初由 Google 机器智能研究机构的 Google Brain 团队的研究人员和工程师开发。该系统旨在促进对机器学习的研究，同时也让机器学习研究原型过渡到生产系统更加高效容易。\nCaffe 是一个知名的、被普遍使用的机器视觉库，其将 Matlab 的快速卷积网接口迁移到了 C 和 C++ 中。Caffe 不面向其他深度学习应用，比如文本、声音或时序数据。如同其他框架一样，Caffe 选择 Python 作为 API。\nKeras一个高层神经网络API，Keras由纯Python编写而成并基Tensorflow或Theano。\nMxnet 一个全功能、灵活且高扩展性的深度学习框架，支持深度学习模型中的卷积神经网络和长期短期记忆网络。由学术界发起，由华盛顿大学和卡内基梅隆大学的研究人员联合发起。\n福布斯总结的全球最值得关注的50家人工智能公司\n除了上述简介之外（95%文字来自维基百科），以下文章将会有助于你更加深入了解人工智能、机器学习、深度学习：\n1、Artificial Intelligence, Machine Learning, and Deep Learning\n2、Why Deep Learning is Radically Different from Machine Learning\n3、一篇文章讲清楚人工智能、机器学习和深度学习的区别\n4、人工智能，机器学习和深度学习有什么区别？\n5、如何区分人工智能、机器学习和深度学习？\n6、WHY DEEP LEARNING IS SUDDENLY CHANGING YOUR LIFE\n7、The Current State of Machine Intelligence 3.0\n8、Here are 50 Companies Leading the AI Revolution\n最后欢迎关注 人工智能+机器学习+深度学习技术文章精选 - 知乎专栏 ，未来将会持续精选分享关于人工智能、机器学习和深度学习的一些技术资料。"}
{"content2":"今天聊聊最近的AI学习，顺便总结一下经验，希望新入门的兄弟姐妹通过本文能更好理解AI。\nAI（人工智能）为应用程序开发人员开辟了全新的可能性。通过利用机器学习或深度学习，您可以生成更好的用户配置文件、个性化和推荐，或集成更智能的搜索、语音界面或智能助理，或以其他几种方式改进您的应用程序。您甚至可以构建了解、的应用程序，并了解如何与人类进行交互。你准备好学习人工智能，你知道选择哪种编程语言吗？下面列出的五种编程语言被认为是最适合学习AI的。你可以参考它。\n1. PYTHON\n首先是毫无疑问的Python。虽然Python的一些功能令人不愉快（空白、Python 2.x和Python 3.x之间的巨大差异，五种不同的包机制在不同程度上存在缺陷）但是如果你正在研究AI，你几乎可以肯定Python是在某些时候用过。\nPython中可用的库的数量超出了其他语言的范围。 NumPy已经变得非常普遍，几乎已成为张量操作的标准API，而Pandas将R强大而灵活的数据帧带入Python。对于自然语言处理（NLP），您可以使用着名的NLTK和闪电般快速的SpaCy。对于机器学习，有一个实用的Scikit-learn。NLP伪原创软件在深度学习方面，所有当前的库（TensorFlow，PyTorch，Chainer，Apache MXNet，Theano等）都是用Python实现的第一个项目。\n（在LiveEdu上，德国AI开发人员教你如何使用Python开发两个简单的机器学习模型）\n如果您正在阅读关于arXiv的顶级深度学习研究，几乎可以肯定您将在Python中找到源代码。\n此外，还有Python生态系统的其他部分。尽管IPython已经重命名为Jupyter Notebook并且看起来不再以Python为中心，但您仍然会发现大多数Jupyter Notebook用户和大多数在线共享笔记本都使用Python。Python是人工智能研究领域的领先语言，是具有最多机器学习和深度学习框架的语言，也是几乎所有AI研究人员都拥有的语言。由于这些原因，虽然我每天都在诅咒空白问题，但Python仍然是人工智能伪原创编程语言之王，你无法绕过它。\n2. JAVA及相关语言\nJVM系列语言（Java，Scala，Kotlin，Clojure等）也是AI应用程序开发的绝佳选择。无论是自然语言处理（CoreNLP）、张量操作（ND4J）还是完整GPU加速深度学习堆栈（DL4J），您都可以使用大量库来管理管道的各个部分。此外，您还可以轻松访问Apache Spark和Apache Hadoop等大数据平台。\nJava是大多数企业的通用语言，Java 8和Java 9中提供了新的语言结构，使得编写Java代码的体验不再像我们过去记忆的那样糟糕。用Java编写人工智能应用程序可能很无聊，但确实可以完成工作，您可以使用所有现成 的Java基础架构来开发、部署和监控。\n3. C / C ++\n在开发AI应用程序时，C / C ++不太可能是您的首选，但如果您在嵌入式环境中工作并且无法承担Java虚拟机或Python解释器的开销，那么C / C ++是最好的解决方案。当你需要消耗每一滴性能时，你必须面对可怕的指针世界。\n幸运的是，现代的C / C ++写作经验并不差（说实话！）。您可以选择以下选项之一：您可以深入到堆栈的底部，使用CUDA等库来编写自己的代码，这些库将直接在GPU上运行；您还可以使用TensorFlow或Caffe访问灵活的Advanced API。后者还允许您导入由Python中的数据科学家编写的模型，然后在C / C ++级别的生产环境中运行它们。在来年，请密切关注Rust在AI领域的一些行动。结合C / C ++级别的速度和类型以及数据安全性，Rust是实现产品级性能而不会产生安全问题的最佳选择。现在它已准备好与TensorFlow捆绑在一起。\n4. JAVASCRIPT\n蛤？ ！ JavaScript的？我没弄错了吗？事实上，Google最近发布了TensorFlow.js，这是一个WebGL加速库，允许您在Web浏览器中训练和运行机器学习模型。它还包括Keras API以及加载和使用已在常规TensorFlow中训练的模型的功能。这可能会吸引大量JS开发人员进入AI领域。虽然与其他语言相比，JavaScript当前可以访问的机器学习库是有限的，但在不久的将来，将神经网络添加到网页就像添加React组件或CSS属性一样简单。这听起来有力而且可怕。\nTensorFlow.js仍处于早期阶段。它目前在浏览器中工作，但不适用于Node.js.它也没有实现完整的TensorFlow API。但是，我预计这两个问题将在2018年底基本解决，JavaScript将在不久的将来进入AI世界。\n5. R\nR排在列表的底部，看起来越来越多。 R是数据科学家喜欢的语言。但是，其他程序员在第一次联系R时可能会感到困惑，因为它使用了以数据帧为中心的方法。如果你有一组专门的R开发人员，那么使用R和TensorFlow、Keras或H2O进行研究。、原型设计和实验是有道理的。但是，根据性能和操作考虑，我不愿意推荐R用于生产。虽然您可以编写可以部署在生产服务器上的高性能R代码，但将使用R编写的原型重新编码为Java或Python当然更容易。"}
{"content2":"最流行的4个机器学习数据集\n机器学习算法需要作用于数据，而数据的本质则决定了应用的机器学习算法是否合适，而数据的质量也会决定算法表现的好坏程度。所以会研究数据，会分析数据很重要。本文作为学习研究数据系列博文的开篇，列举了4个最流行的机器学习数据集。\nIris\nIris也称鸢尾花卉数据集，是一类多重变量分析的数据集。通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。\nAdult\n该数据从美国1994年人口普查数据库抽取而来，可以用来预测居民收入是否超过50K$/year。该数据集类变量为年收入是否超过50k$，属性变量包含年龄，工种，学历，职业，人种等重要信息，值得一提的是，14个属性变量中有7个类别型变量。\nWine\n这份数据集包含来自3种不同起源的葡萄酒的共178条记录。13个属性是葡萄酒的13种化学成分。通过化学分析可以来推断葡萄酒的起源。值得一提的是所有属性变量都是连续变量。\nCar Evaluation\n这是一个关于汽车测评的数据集，类别变量为汽车的测评，（unacc，ACC，good，vgood）分别代表（不可接受，可接受，好，非常好），而6个属性变量分别为「买入价」，「维护费」，「车门数」，「可容纳人数」，「后备箱大小」，「安全性」。值得一提的是6个属性变量全部是有序类别变量，比如「可容纳人数」值可为「2，4，more」，「安全性」值可为「low, med, high」。\n小结\n通过比较以上4个数据集的差异，简单地总结：当需要试验较大量的数据时，我们可以想到「Adult」；当想研究变量之间的相关性时，我们可以选择变量值只为整数或实数的「Iris」和「Wine」；当想研究logistic回归时，我们可以选择类变量值只有两种的「Adult」；当想研究类别变量转换时，我们可以选择属性变量为有序类别的「Car Evaluation」。更多的尝试还需要对这些数据集了解更多才行。"}
{"content2":"编辑： 枣泥布丁 分类：AI 来源：开源中国\n如果你在科技领域，你经常会听到人工智能，机器学习，甚至是深度学习。怎样才可以在正确的时间正确的使用这些词？他们都是一样的意思吗？然而更多时候，人们总是混淆的使用它们。\n人工智能，机器学习和深度学习都是属于一个领域的一个子集。但是人工智能是机器学习的首要范畴。机器学习是深度学习的首要范畴。\n深度学习是机器学习的一个子集，机器学习是人工智能的一个子集\n这个领域的兴起应该归功于深度学习。人工智能和机器学习这个领域近年来一直在解决一系列有趣的问题，比如从自动化的杂货店购买到自动驾驶汽车。\n人工智能：\n人工智能的定义可以分为两部分，即“人工”和“智能”。“人工”比较好理解，争议性也不大。有时我们会要考虑什么是人力所能及制造的，或者人自身的智能程度有没有高到可以创造人工智能的地步，等等。但总的来说，“人工系统”就是通常意义下的人工系统。\n尼尔逊教授对人工智能下了这样一个定义：“人工智能是关于知识的学科――怎样表示知识以及怎样获得知识并使用知识的科学。”而另一个美国麻省理工学院的温斯顿教授认为：“人工智能就是研究如何使计算机去做过去只有人才能做的智能工作。”这些说法反映了人工智能学科的基本思想和基本内容。即人工智能是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。\n人工智能是研究使计算机来模拟人的某些思维过程和智能行为（如学习、推理、思考、规划等）的学科，主要包括计算机实现智能的原理、制造类似于人脑智能的计算机，使计算机能实现更高层次的应用。人工智能将涉及到计算机科学、心理学、哲学和语言学等学科。可以说几乎是自然科学和社会科学的所有学科，其范围已远远超出了计算机科学的范畴，人工智能与思维科学的关系是实践和理论的关系，人工智能是处于思维科学的技术应用层次，是它的一个应用分支。从思维观点看，人工智能不仅限于逻辑思维，要考虑形象思维、灵感思维才能促进人工智能的突破性的发展，数学常被认为是多种学科的基础科学，数学也进入语言、思维领域，人工智能学科也必须借用数学工具，数学不仅在标准逻辑、模糊数学等范围发挥作用，数学进入人工智能学科，它们将互相促进而更快地发展。\nAI目标：\n为了进一步解释人工智能的目标，研究人员将其扩展到这六个主要目标。\n逻辑推理。使计算机能够完成人类能够完成的复杂心理任务。例如下棋和解代数问题。\n知识表达。使计算机能够描述对象，人员和语言。例如能使用面向对象的编程语言 Smalltalk。\n规划和导航。使计算机从A点到B点。例如，第一台自动驾驶机器人建于20世纪60年代初。\n自然语言处理。使计算机能够理解和处理语言。例如把英语翻译成俄语，或者把俄语翻译成英语。\n感知。让电脑通过视觉，听觉，触觉和嗅觉与世界交流。\n紧急智能。也就是说，智能没有被明确地编程，而是从其他AI特征中明确体现。这个设想的目的是让机器展示情商，道德推理等等。\nAI领域\n即使有了这些主要目标，这也没有对具体的人工智能算法和技术进行分类。这些是人工智能中的六大主要算法和技术：\n机器学习是人工智能领域，使计算机不用明确编程就能学习。\n搜索和优化算法，如梯度下降迭代搜索局部最大值或最小值。\n约束满足是找到一组约束的解决方案的过程，这些约束施加变量必须满足的条件。\n逻辑推理。人工智能中逻辑推理的例子是模拟人类专家决策能力的专家计算机系统。\n概率推理是将概率论的能力去处理不确定性和演绎逻辑的能力来利用形式论证的结构结合起来。其结果是一个更丰富和更具表现力的形式主义与更广泛应用领域。\n控制理论是一种正式的方法来找到具有可证性的控制器。这通常涉及描述像机器人或飞机这样的物理系统的微分方程组。\n机器学习\n机器学习是人工智能的一个子集。那么什么是机器学习呢？\n机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n机器学习如此重要的原因是什么？一个重大突破导致机器学习成为人工智能背后的动力 - 互联网的发明。互联网有大量的数字信息被生成存储和分析。机器学习算法在这些大数据方面是最有效的。\n神经网络 ⇱\n如果我们谈论机器学习时，值得一提的是机器学习算法：神经网络。\n神经网络是机器学习算法的关键部分。神经网络是教计算机以人类的方式思考和理解世界的关键。实质上，神经网络是模拟人类的大脑。这被抽象为由加权边缘（突触）连接的节点（神经元）的图形。有关神经网络的更多信息请查看神经网络概述。\n这个神经网络有一层，三个输入和一个输出。任何神经网络都可以有任何数量的层，输入或输出。\n深度学习\n机器学习算法一直是人工智能背后的推动力量。所有机器学习算法中最关键的是深度学习。\n深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。\n这个神经网络有两层，三个输入和一个输出。任何神经网络都可以有任何数量的层，输入或输出。输入神经元和最后一层输出神经元之间的层是深层神经网络的隐藏层。\n深度学习最好的表现是深度神经网络（DNN）。深层神经网络只是一个超过两层或三层的神经网络。然而，深度神经网络并不是深度学习算法的唯一类型 -但它是最流行的类型。另一个深度学习算法是深度信任网络（DBN）。深层信任网络在层与层之间不直接联系。这意味着DNN和DBN的拓扑在定义上是不同的。DBN中的无向层被称为 Restricted Boltzmann Machines。\n有关深度学习和机器学习的差别可查看一文读懂深度学习与机器学习的差异。\n结论\n所以，机器学习是人工智能的前沿，深度学习是机器学习的前沿。"}
{"content2":"1.CS224D\n2.NLP到Word2vec\n3.Opencv3图像处理\n4.Tensorflow视频教程\n5.机器学习视频教程\n6.七月在线所有人工智能课程\n7.聊天机器人视频教程\n8.自然语言处理视频教程\n链接如下，需要的可以在百度网盘下载。\n链接1:https://pan.baidu.com/s/1uqqYMQ3J4Vk1kSCH-7D4dA 密码:28s7 链接2:https://pan.baidu.com/s/1KdRYyI0Yta5gWNDwQge7sw 密码:0smi 链接3:https://pan.baidu.com/s/1gzSHZ52kCdNrrgtLyy3f1w 密码:8dl0 链接4:https://pan.baidu.com/s/1KrGVuhICot9GRLa-XFRsAA 密码:qa88 链接5:https://pan.baidu.com/s/16xwLarpVSQp6-ZIAAp7mOQ 密码:ixdk"}
{"content2":"小周天   岂安科技售前顾问\n主要负责名词解释\n引子\n笔者作为与机器学习同龄的一代，从小学时就开始了对 AI 的不断钻研，上上下下左右左右 BA 的代码敲得无比熟练。经常在与 AI 的对战中获得大胜。此刻笔者的儿子（与深度学习同龄）正跟他的爷爷奶奶（与人工智能同龄）展示如何把 AI 虐得不能自理。而此文的目的就是把我这两个小时来对机器学习的初步而又深刻的理解讲给大家。\n两个小时前，我第一次在搜索引擎中打了“机器学习”几个字。然后机器回复了以下定义给我：\n机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科……\n此刻你们应该和我一样已经失去了继续读下去的勇气。\n简而言之\n这么说吧，机器学习算法可以不再让你明确地给出执行步骤或判断规则，而让电脑根据输入数据自己创建规则并处理问题。\n➤ 整点儿实际的：考虑下如何区分垃圾邮件？\n首先是不是想到的要来个关键词过滤？通过编写一些规则来筛选判断：if 匹配到关键字去垃圾箱，else 去收件箱。我们所有的精力都会花在写这些过滤条件上。写了几百条以后感觉差不多了，可发件人又换说法了，金三胖进化成了胖三金，胖三金进化成了三月半。逼得我们不断地要对过滤条件进行更新。\n能不能有一种方法能让机器自己总结这些敏感词（特征）就像歌里唱的 “当你看着我我没有开口已被你猜透”？机器学习的有趣之处正是与此。它的玩法是这样的：\n首先我们需要准备大量已经经过区分的邮件。\n然后把这些区分好的邮件喂给一个现成的无需二次开发的机器学习算法。\n机器学习算法可以通过识别两组邮件的区别来创建自己的规则或模型，这一阶段叫做机器训练。\n当模型准备完成后，当再次输入未经区分的邮件时，机器学习算法即可按照模型的规则来自主判断区分邮件。\n在程序员的欢呼声还没停下时，我要说，然鹅，这都不算什么。\n更让人惊喜的是同样这个区分邮件的机器学习算法，不只可以用来区分邮件，当你把输入的数据换成了验证码图片，它又可以被用来训练识别验证码，而你不用为此算法去多改一行代码。\n监督学习\n在前面区分垃圾邮件的例子中，我们给出的训练邮件是已经经过标注区分的数据，机器学习算法可以依照给定的数据和标注来进行建模而区分新的邮件。这种给定分类数据的学习方式叫做监督学习（Supervised Learning）。虽然机器学习中还包括其他如无监督学习，增强学习等分支，但监督学习是目前应用场景最广泛也是最容易理解的一个分支。其最基本的逻辑是：告诉 SL 数据 X 和输出结果 Y，让它在输入和输出中建立合理的映射关系。\n比如给出：\n输入为2，2 输出为 4\n输入为3，3 输出为 6\n监督学习模型就可以通过分析之前的映射关系得出输入输出可能为加法运算关系，当我们再输入4，5时即可利用监督学习生成的模型输出9。\n我们下面的例子中也是利用监督学习来对输出结果来进行一个预测。\n实例简介-价值预测\n如何为下图中的房产估价？\n一个熟练的房产中介也许看一眼就可以根据面积，位置，环境等给出一个大概的市场估价，而作为一个从未从事过此行业的你如何给出一个合理的估价呢？你去问了部门里的老司机健林。健林说我们这儿的估价一般都这么计算：\n不管任何类型的房子我们都设定一个底价：50000\n按照面积乘以单价1计算面积修正值：面积×单价1\n按照房间数乘以单价2计算房间修正值：房间数x单价2\n最后把三部分相加得出估价。\n总价=50000+（面积X单价1）+（房间数x单价2）\n多亏了健林，模型有了。\n于是这两个单价就成了接下去的关键，但这个难不倒你。\n首先我们拿到了根据这个模型计算出的三条记录（训练数据）\n案例\n房间数\n面积\n估价\n案例1\n5\n3800\n450000\n案例2\n4\n2200\n293000\n案例3\n2\n1150\n175000\n450000=50000+（3800x单价1）+（5x单价2）\n293000=50000+（2200x单价1）+（4x单价2）\n175000=50000+（1150x单价1）+（2x单价2）\n接下去感觉就像解二元一次方程组一样简单啊。\n但现实中即便是取准了房间和面积两个关键因素，房屋总价因存在其他影响而不会在图表上与上述关键因素100%正相关，从而在关系图表中表现的是一组散点而非完美的线性关系。简单的说我们需要解决的是约等于的关系。\n为了解决这个问题，我们使用一种通用的计算方式——猜，猜很多次直到猜中。\n第一轮\n1 假设猜测单价1=10，单价2=10\n得出结果\n案例1=88050 （真实值 450000）\n案例2=72040 （真实值 293000）\n案例3=61520 （真实值 175000）\n2 计算偏差值Cost\nCost1= (450000-88050)+(293000-72040)+(175000-61520)\n为了能避某个案例存在较大偏差我们需要对Cost1进一步优化来放大较为严重的偏差\nCost2=\n有了 Cost2 以后我们的最终目标就是通过不断的猜测让 Cost2 的值趋向于0。\n而处理类似的值优化（线性回归）问题，比较常见的优化算法有 Gradient Descent（梯度下降），通过对初始输入值的不断调整迭代来让偏差值 Cost2 尽可能趋近为0，最终 Gradient Descent 算法会返回一个理想的输入值，在上面例子中最终会给出单价 1=92.1 单价 2=10001 ，\n而这个 Gradient Descent 算法会包含在我们下面要用到的函数库中。\n应用场景\n用机器学习的方法来处理类似的数值预测的场景在我们的生活中十分常见。在本文接下去的内容中我们就会通过分析计算房屋的属性来进行估值。在风控领域中当用户在网上交易或申请贷款时供应商也会通过收集用户的各种属性来按照一定模型来进行风险评估从而辅助下一步的决定。通过相同的机器学习算法还可以将用户评论作为输入从而分析用户对商品给出的是正面还是负面评价。\n下期预告\n在这个主题的下一期中，我们会完全围绕我们本期中提到的房屋估价的实例，来介绍如何对房屋数据进行过滤和准备，如果选取合适属性（Feature Engineering）如何通过现有数据建模。\n并且不断进行优化"}
{"content2":"什么是机器学习\n机器学习是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习算法是一类从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测的算法\n为什么需要机器学习\n21世纪机器学习又一次被人们关注，而这些关注的背后是因为整个环境的改变，我们的数据量越来越多，硬件越来越强悍。急需要解放人的生产力，自动去寻找数据的规律。解决更多专业领域的问题。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域.\n开发机器学习应用程序的步骤\n（1）收集数据\n我们可以使用很多方法收集样本护具，如：制作网络爬虫从网站上抽取数据、从RSS反馈或者API中得到信息、设备发送过来的实测数据。\n（2）准备输入数据\n得到数据之后，还必须确保数据格式符合要求。\n（3）分析输入数据\n这一步的主要作用是确保数据集中没有垃圾数据。如果是使用信任的数据来源，那么可以直接跳过这个步骤\n（4）训练算法\n机器学习算法从这一步才真正开始学习。如果使用无监督学习算法，由于不存在目标变量值，故而也不需要训练算法，所有与算法相关的内容在第（5）步\n（5）测试算法\n这一步将实际使用第（4）步机器学习得到的知识信息。当然在这也需要评估结果的准确率，然后根据需要重新训练你的算法\n（6）使用算法\n转化为应用程序，执行实际任务。以检验上述步骤是否可以在实际环境中正常工作。如果碰到新的数据问题，同样需要重复执行上述的步骤\n环境准备\n我们本次机器学习的课程使用的python程序库：\nNumpy、Scikit-learn\n在ubuntu或者mac操作系统中，在虚拟环境中安装:\npip3 install Numpy pip3 install Scikit-learn\nNumpy是一个强大的高级数学运算的工具库，还具备非常搞笑的向量和矩阵运算功能。Scikit-learn是一个基于python的机器学习库，封装了大量经典以及最新的机器学习模型。\n我们应该怎么做\n互联网公司机器学习工作、数据挖掘工程师们工作内容是什么?\n研究各种算法，设计高大上模型?\n深度学习的应用，N层神经网络?\n...\n大部分复杂模型的算法精进都是数据科学家在做\n大多数程序员\n跑数据，各种map-reduce，hive SQL，数据仓库搬砖\n数据清洗，数据清洗，数据清洗\n分析业务，分析case，找特征\n常用算法跑模型\n机器学习思维导图："}
{"content2":"介绍机器学习相关，大致了解什么是机器学习，以及机器学习分类、基本用途。\n\n\n\n\n完整版请下载PDF\nhttp://download.csdn.net/download/u011626960/10255071"}
{"content2":"机器学习的几种方法，在理解和实践过程中持续更新。\n机械学习\n机械学习就是记忆。把新的知识存储起来，要用时直接检索调用，无需计算推理。\n机械学习适合于计算量较大而取值空间较小或是为离散的情况，由于不具有泛化能力，只能在已知范围进行应用。\n其表现形式类似于字典：\ndef jxlearn(inv,outv = None): if outv: dict[inv] = outv else: return dict[inv]\n基于解释的学习\n简称解释学习，通过对概念的解释获取知识。\n解释学习需要给定：\n目标概念TC，训练实例TE，领域知识库DT，操作准则OC。\n求解：\n训练实例的一般化概括，使之满足TC与OC。\n解释学习的一个重要前提就是领域知识。在领域知识完备的情况下，可以无需学习便可推理出目标概念，但求出所有情况是困难的，因此通过训练学习可以获取有用知识；在领域知识不完备的情况下，通过训练学习获得近似解释。\n从人的角度来说，要理解或者解释一个概念，需要从已知的公理和定义出发来推理，但这样做是困难的，因为知识繁多，且不一定完备，因此往往需要一个例子来帮助理解概念。\n因此，举一个简单例子来帮助理解解释学习：\n已知’阴’、’阳’两种爻：分别以0、1来存储，以及各个单卦，以八进制存储。现有一个爻成卦的概念，即对于顺序的爻x、y、z，根据操作makeg(x,y,z)->x + y<<1+z<<2为一个卦。要以解释学习训练，其过程为，输入一个训练实例，如’010’->’坎’,前者为一个复合量，通过操作makeg实现转换，后者为一个常量。将常量换为一个变量从而进行概括，在训练后对于任一个复合输入，均可通过该变量来搜索得到结果。\n#代码待补充\n基于事例的学习\n在无法建立模型时，可以对事例进行直接记录。基于事例的学习方法，可以使用相容性启发方法，把已经记录的事例的特征赋予另一个未见过的新事物。\n对于新事物和旧事物的启发需要计算其间的距离。\n基于概念的学习\n概念学习有两条路，一是从学习机理出发，二是基于认知建模。从学习机理出发则首要任务是进行类型定义，然后对事物依据概念进行分类。\n基于类比的学习\n类比是对对象知识框架槽值的传递。\n基于决策树的学习\n决策树可以经过学习得到分类规则。\n强化学习\n强化学习的评价网络产生的评价函数对当前动作或环境的输入进行的激励，依据评价信号对环境进行适应并接近任务目标。"}
{"content2":"机器学习面试题汇总(机器学习基础)\n机器学习基础面试题\n1.请简要说说一个完整机器学习项目的流程\n2.简单说下有监督学习和无监督学习的区别\n3.线性分类器与非线性分类器的区别以及优劣\n4.对于维度很高的特征你选择线性分类器还是非线性分类器？维度很低的特征又该如何选择？\n5.什么是生成模型和判别模型?\n6.常见的生成式模型和判别式模型有哪些？\n7.解释置信区间\n8.什么是过拟合？\n9.什么是正则化?\n10.什么是L1正则化和L2正则化？\n11.为什么L1正则化相较于L2正则化会产生更加稀疏的矩阵？\n12.正则化为什么能防止过拟合？\n13.机器学习中经常使用的最优化方法有哪些？\n14.简述一下梯度下降法\n15.如何处理样本不均衡问题？\n16.如何衡量分类器的好坏？\n17.介绍一下ROC和AUC\n18.为什么要使用ROC曲线？\n19.什么是交叉验证？\n20.简述交叉验证常见的几种方法\n面试题答案总结请扫描小程序码在面试题相关小程序中查看：\n获取更多校园招聘资讯请关注校园招聘公众号校招辅助(xiaozhaofuzhu):"}
{"content2":"来源：https://zhidao.baidu.com/question/1436084401002936139.html\n一、人工智能\n人工智能(Artificial Intelligence)，英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括语音识别、图像识别、机器人、自然语言处理、智能搜索和专家系统等。\n人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也有可能超过人的智能。\n二、数据挖掘\n数据挖掘(Data Mining)，顾名思义就是从海量数据中“挖掘”隐藏信息，按照教科书的说法，这里的数据是“大量的、不完全的、有噪声的、模糊的、随机的实际应用数据”，信息指的是“隐含的、规律性的、人们事先未知的、但又是潜在有用的并且最终可理解的信息和知识”。在商业环境中，企业希望让存放在数据库中的数据能“说话”，支持决策。所以，数据挖掘更偏向应用。\n数据挖掘通常与计算机科学有关，并通过统计、在线分析处理、情报检索、机器学习、专家系统(依靠过去的经验法则)和模式识别等诸多方法来实现上述目标。\n三、机器学习\n机器学习(Machine Learning)是指用某些算法指导计算机利用已知数据得出适当的模型，并利用此模型对新的情境给出判断的过程。\n机器学习的思想并不复杂，它仅仅是对人类生活中学习过程的一个模拟。而在这整个过程中，最关键的是数据。\n任何通过数据训练的学习算法的相关研究都属于机器学习，包括很多已经发展多年的技术，比如线性回归(Linear Regression)、K均值(K-means，基于原型的目标函数聚类方法)、决策树(Decision Trees，运用概率分析的一种图解法)、随机森林(Random Forest，运用概率分析的一种图解法)、PCA(Principal Component Analysis，主成分分析)、SVM(Support Vector Machine，支持向量机)以及ANN(Artificial Neural Networks，人工神经网络)。\n四、深度学习\n深度学习(Deep Learning)的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。\n深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。\n五、人工智能与机器学习、深度学习的关系\n严格意义上说，人工智能和机器学习没有直接关系，只不过目前机器学习的方法被大量的应用于解决人工智能的问题而已。目前机器学习是人工智能的一种实现方式，也是最重要的实现方式。\n早期的机器学习实际上是属于统计学，而非计算机科学的;而二十世纪九十年代之前的经典人工智能跟机器学习也没有关系。所以今天的AI和ML有很大的重叠，但并没有严格的从属关系。\n不过如果仅就计算机系内部来说，ML是属于AI的。AI今天已经变成了一个很泛泛的学科了。\n深度学习是机器学习现在比较火的一个方向，其本身是神经网络算法的衍生，在图像、语音等富媒体的分类和识别上取得了非常好的效果。\n所以，如果把人工智能与机器学习当成两个学科来看，三者关系如下图所示：\n如果把深度学习当成人工智能的一个子学科来看，三者关系如下图所示：\n六、数据挖掘与机器学习的关系\n数据挖掘主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。\n机器学习是数据挖掘的一种重要方法，但机器学习是另一门学科，并不从属于数据挖掘，二者相辅相成。\n补充：\n来源：http://m.elecfans.com/article/691751.html\n机器学习过程使用以下步骤进行定义：\n1. 确定相关数据集并准备进行分析。\n2. 选择要使用的算法类型。\n3. 根据所使用的算法构建分析模型。\n4. 立足测试数据集进行模型训练，并根据需要进行模型修改。\n5. 运行模型以生成测试评分。\n机器学习与深度学习间的区别\n1. 数据量：\n机器学习能够适应各种数据量，特别是数据量较小的场景。在另一方面，如果数据量迅速增加，那么深度学习的效果将更为突出。下图展示了不同数据量下机器学习与深度学习的效能水平。\n2. 硬件依赖性：\n与传统机器学习算法相反，深度学习算法在设计上高度依赖于高端设备。深度学习算法需要执行大量矩阵乘法运算，因此需要充足的硬件资源作为支持。\n3. 特征工程：\n特征工程是将特定领域知识放入指定特征的过程，旨在减少数据复杂性水平并生成可用于学习算法的模式。\n示例：传统的机器学习模式专注于特征工程中所需要找像素及其他属性。深度学习算法则专注于数据的其他高级特征，因此能够降低处理每个新问题时特征提取器的实际工作量。\n4. 问题解决方法\n传统机器学习算法遵循标准程序以解决问题。它将问题拆分成数个部分，对其进行分别解决，而后再将结果结合起来以获得所需的答案。深度学习则以集中方式解决问题，而无需进行问题拆分。\n5. 执行时间\n执行时间是指训练算法所需要的时间量。深度学习需要大量时间进行训练，因为其中包含更多参数，因此训练的时间投入也更为可观。相对而言，机器学习算法的执行时间则相对较短。\n6. 可解释性\n可解释性是机器学习与深度学习算法间的主要区别之一——深度学习算法往往不具备可解释性。也正因为如此，业界在使用深度学习之前总会再三考量。\n机器学习与深度学习的实际应用：\n通过指纹实现出勤打卡、人脸识别或者通过扫描车牌识别牌照号码的计算机视觉技术。\n搜索引擎中的信息检索功能，例如文本搜索与图像搜索。\n自动电子邮件营销与特定目标识别。\n癌症肿瘤医学诊断或其他慢性疾病异常状态识别。\n自然语言处理应用程序，例如照片标记。Facebook就提供此类功能以提升用户体验。\n在线广告。\n未来发展趋势：\n随着业界越来越多地使用数据科学与机器学习技术，对各个组织而言，最重要的是将机器学习方案引入其现有业务流程。\n深度学习的重要程度正逐步超越机器学习。事实已经证明，深度学习是目前最先进且实际效能最出色的技术方案之一。\n机器学习与深度学习将在研究与学术领域证明自身蕴藏的巨大能量。"}
{"content2":"大数据\n大数据与云计算的关系密不可分，大数据必然无法用单台的计算机进行处理，必须采用分布式计算架构。它的特色在于对海量数据的挖掘，但它必须依托云计算的分布式处理、分布式数据库、云存储和/或虚拟化技术。\n数据又并非单纯指人们在互联网上发布的信息，全世界的工业设备、汽车、电表上有着无数的数码传感器，随时测量和传递着有关位置、运动、震动、温度、湿度乃至空气中化学物质的变化，也产生了海量的数据信息。\n大数据的价值体现在以下几个方面：（1）对大量消费者提供产品或服务的企业可以利用大数据进行精准营销；（2） 做小而美模式的中长尾企业可以利用大数据做服务转型；（3）面临互联网压力之下必须转型的传统企业需要与时俱进充分利用大数据的价值。\n大数据的四个特点：数据体量巨大、数据类型繁多、价值密度低、处理速度快。\n大数据需要特殊的技术，以有效地处理大量的容忍经过时间内的数据。适用于大数据的技术，包括大规模并行处理（MPP）数据库，数据挖掘电网，分布式文件系统，分布式数据库，云计算平台，互联网和可扩展的存储系统。\n大数据中的数据分为三种：结构化的（有固定格式和有限长度）、非结构化（不定长和无固定格式）的和半结构化的（多半是XML或者HTML的）。\n云计算\n云计算最初的目标是对资源的管理，管理的主要是计算资源、网络资源、存储资源三个方面。\n云计算具有两方面的灵活性（即弹性）：时间灵活性（随时可以申请资源）和空间灵活性（可以申请任意数量资源）。\n私有云是把虚拟化和云化的软件部署在别人的数据中心里；公有云就是虚拟化和云化软件部署在云厂商自己数据中心里，用户不需要很大投入，只需要注册一个账号，就能在一个网页上点一下创建一台虚拟电脑。\nOpenstack已经成为开源云平台的事实标准。\n计算，网络，存储我们常称为基础设施Infranstracture，因而这个阶段的弹性称为资源层面的弹性，管理资源的云平台，我们称为基础设施服务，就是我们常听到的IaaS（Infranstracture As A Service）。\n除资源层面的弹性外，还要有应用层面的弹性，通常称为PaaS（Platform AS A Service），一般分为两部分，可以理解成：自己的应用自动安装和通用的应用不用安装。\n人工智能\n物联网解决的是感知真实的物理 ；云计算解决的是提供强大的能力去承载这个数据；大数据解决的是对海量的数据进行挖掘和分析，把数据变成信息；人工智能解决的是对数据进行学习和理解，把数据变成知识和智慧。\n神经网络的普遍性定理：假设某个人给你某种复杂奇特的函数f(x)，不管这个函数是什么样的，总会确保有个神经网络能够对任何可能的输入x，其值f(x)（或者某个能够准确的近似）是神经网络的输出。如果在函数代表着规律，也意味着这个规律无论多么奇妙，多么不能理解，都是能通过大量的神经元，通过大量权重的调整，表示出来的。\n人工智能算法多是依赖于大量数据的，人工智能程序通常作为SaaS（软件即服务，Soft As A Service）平台进入云计算。\n一般在一个云计算平台上，云、大数据、人工智能都能够找得到；对一个大数据公司，积累了大量数据，也会用人工智能的算法提供一些服务；对一个人工智能公司，不可能没有大数据平台支撑。\n机器学习\n机器学习专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n机器学习是人工智能的核心，是使计算机具有智能的根本途径，它主要使用归纳、综合而不是演绎。\n在算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。\n机器学习的广泛应用：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。\n基于学习策略可以分为六类：\n（1）机械学习（无需任何推理和其他知识转换，直接吸取环境所提供的信息）；\n（2）示教学习（从环境获取信息，把知识转化成内部可用形式，并将新知识与原有结合）；\n（3）演绎学习（所用推理形式为演绎推理，从公理出发，经过逻辑变换推导出结论）；\n（4）类比学习（利用两个不同领域中知识相似性，通过类比，从源域知识推导出目标域的知识）；\n（5）基于解释的学习（根据提供的目标概念、例子、领域理论和可操作准则，首先构造一个解释来说明为什么该例子满足目标概念，然后将解释推广为目标概念的一个满足可操作准则的充分条件）；\n（6）归纳学习（由环境提供某概念的实例或反例，通过归纳推理得出该概念的一般描述）。\n目前机器学习领域的研究工作主要围绕三个方面：\n（1）面向任务的研究（研究和分析改进一组预定任务的执行性能的学习系统）；\n（2）认知模型（研究人类学习过程并进行计算机模拟）；\n（3）理论分析（从理论上探索各种可能的学习方法和独立于应用领域的算法）。"}
{"content2":"机器学习的三种不同方法：\n一、监督学习（supervised learning）——对未来事件进行预测。使用有类标的数据构建数据模型。然后使用经训练得到的模型对未来的数据进行预测。\n主要分为两类：\n1.利用分类对类标进行预测\n2.使用回归预测连续输出值\n二、无监督学习（unsupervised learning）——发现数据本身潜在的结构。分为两类：\n1.通过聚类发现数据的子群\n2.数据压缩中的降维\n三、强化学习（reinforecement learning）——解决交互式问题。构建一个系统，在与环境交互的过程中提高系统的性能。\n四、基本术语\n机器学习的十大经典算法\nhttps://www.toutiao.com/i6610271910351602184/?tt_from=weixin&utm_campaign=client_share&wxshare_count=2&from=singlemessage&timestamp=1539183202&app=news_article&utm_source=weixin&iid=11612498513&utm_medium=toutiao_ios&group_id=6610271910351602184&pbid=6610741823151982087\n一、决策树\n二、随机森林算法\n三、逻辑回归\n四、SVM\n五、朴素贝叶斯\n六、k最邻近算法\n七、k均值算法\n八、Adaboost算法\n九、神经网络\n十、马尔可夫"}
{"content2":"人工智能相关岗位中，涉及到的内容包含：\n算法、深度学习、机器学习、自然语言处理、数据结构、Tensorflow、Python 、数据挖掘、搜索开发、神经网络、视觉度量、图像识别、语音识别、推荐系统、系统算法、图像算法、数据分析、概率编程、计算机数学、数据仓库、建模等关键词，基本涵盖了现阶段人工智能细分领域的人才结构。\n将上面的岗位涉及到的知识和技术划类，就形成了今天的五份书单：\n1人工智能科普类：人工智能科普、人工智能哲学\n《智能的本质》斯坦福、伯克利客座教授30年AI研究巅峰之作\n《科学+遇见人工智能》李开复、张亚勤、张首晟等20余位科学家与投资人共同解读AI革命\n《人工智能时代》从人工智能的历史、现状、未来，工业机器人、商业机器人、家用机器人、机器翻译、机器学习等人工智能应用领域依次介绍了人工智能发展前景。\n《人工智能简史》 跟着图灵、冯•诺依曼、香农、西蒙、纽维尔、麦卡锡、明斯基等人工智能的先驱们重走人工智能之路，站在前人的肩膀上，看人工智能的三生三世，鉴以往才能知未来。\n2人工智能机器学习类：Python、机器学习、数据科学\n《Python机器学习实践指南》 结合了机器学习和Python 语言两个热门的领域，通过利用两种核心的机器学习算法来用Python 做数据分析。\n《Python机器学习——预测分析核心算法》从算法和Python语言实现的角度，认识机器学习。\n《机器学习实践应用》阿里机器学习专家力作，实战经验分享，基于阿里云机器学习平台，针对7个具体的业务场景，搭建了完整的解决方案。\n《NLTK基础教程——用NLTK和Python库构建机器学习应用》介绍如何通过NLTK库与一些Python库的结合从而实现复杂的NLP任务和机器学习应用。\n3人工智能深度学习类：深度学习、Tensorflow\n《深度学习》AI圣经，深度学习领域奠基性的经典畅销书 特斯拉CEO埃隆·马斯克等国内外众多专家推荐！\n《深度学习精要（基于R语言）》基于R语言实战,使用无监督学习建立自动化的预测和分类模型\n《TensorFlow技术解析与实战》包揽TensorFlow1.1的新特性 人脸识别 语音识别 图像和语音相结合等热点一应俱全\n《TensorFlow机器学习项目实战》第二代机器学习实战指南，提供深度学习神经网络等项目实战，有效改善项目速度和效率。\n4\n人工智能算法策略类：算法、推荐系统、编程等\n《神经网络算法与实现——基于Java语言》 完整地演示了使用Java开发神经网络的过程，既有非常基础的实例也有高级实例。\n《趣学算法》 50 多个实例循展示算法的设计、实现、复杂性分析及优化过程 培养算法思维 带您感受算法之美。\n《算法谜题》 Google、Facebook等一流IT公司算法面试必备，经典算法谜题合集。\n《Python算法教程》精通Python基础算法，畅销书Python基础教程作者力作。\n《编程之法：面试和算法心得》程序员面试宝典 笔试金典 CSDN访问量过千万的博客结构之法算法之道博主July著作。\n《趣题学算法》 一本有趣的、易学的、实用的，帮助读者快速入门应用的算法书。\n《Java遗传算法编程》 遗传算法设计 机器学习人工智能 来自Java专家的声音 用遗传算法解决类似旅行商的经典问题。\n《算法学习与应用从入门到精通》320个实例、753分钟视频、5个综合案例、74个技术解惑，一本书的容量，讲解了入门类、范例类和项目实战类三类图书的内容。\n5人工智能时间图像和视觉识别类：图像识别 、语音识别、自然语言处理、建模工程\n《OpenCV和Visual Studio图像识别应用开发》无人驾驶人脸识别基础技术 用OpenCV实现图像处理应用 计算机视觉编程实战手册。\n《人脸识别原理及算法——动态人脸识别系统研究》 介绍了动态场景下的人脸识别方法，该方法综合应用了人脸定位、人脸识别、视频处理等算法。\n《精通Python自然语言处理》用Python开发令人惊讶的NLP项目，自然语言处理任务，掌握利用Python设计和构建给予NLP的应用的实践。\n《Python自然语言处理》基于Python编程语言和NLTK，自然语言处理领域的一本实用入门指南。\n《贝叶斯方法：概率编程与贝叶斯推断》 机器学习 人工智能 数据分析从业者的技能基础 国际杰出机器学习专家余凯博士 腾讯专家研究员岳亚丁博士推荐。\n《贝叶斯思维：统计建模的Python学习法》Thin Stats和Think Python图书作者重磅出击，数据分析师、数据工程师、数据科学家案头常备。\n《概率编程实战》人工智能领域的先驱、美国加州大学伯克利分校教授Stuart Russell作序推荐！一本不可思议的Scala概率编程实战书籍！\n《自己动手写神经网络》机器学习与人工智能参考书，基于Java语言撰写。\n另外给大家挑选出42本最值得读的AI书籍，分为四类：简单科普类、深度科普类、技术学习类、机器人类和AI哲学类，希望对大家有帮助。"}
{"content2":"上回分析的够透彻了吧\n计算方面就使用填字数据结构计算\n存储方面使用数据库表的方式存储\n为了方便就都使用\n填字的方式进行试验\n首先要知道采取上回说到的65535个二进制位进行代表人类的文字信息的话\n那么每个信息之间就会自动建立联系\n就是同样的位置一定是表达一个意思\n这样关联就自动完成了\n逻辑推理将这些关联关系查出来就可以了\n随意查找两者或任意多个之间是否有逻辑关系\n只要判断每个位置至少有一个二进制位是1相同即可\n这样的查找判断就是人们的思考过程\n也是创新的基础\n人们在思考解决问题的时候\n就会联想相关的任何自己知道的信息\n或是采集相关的信息即可\n那么在采集的时候也会通过已知信息去提问别人\n也就是说查找外界的存储器而已\n只要按照这样的存储结够\n智能就是这么简单\n就是查找而已\n并不会是现在的人工智能那么\n二货折腾好久还只是个模拟\n其实答案就存在世界的某一个角落\n认真的推断可得\n不能说现在的人工智能\n不对\n只不过采取的是最笨的方法去寻找答案那么\n一个一个的去尝试解决破解密码 被黑客称为暴力破解\n也就像现在的机器学习一样一个一个的尝试和调节\n人类的尝试是尝试最能影响结果的几个而已"}
{"content2":"０　数据时代/人工智能时代的降临前夜\n从2016年开始，业界忽然针对机器学习和人工智能的追捧大行其道，其中更以Google推出的AlphaGo程序在2016年３月以４:1大胜韩国九段围棋选手李世石；2016年岁末，在国内棋类网站弈城网上出现了一个类似“围棋上帝”的账号（“围棋上帝”是指每一步都绝对正确，每一步都绝对算到，洞悉全局的一切），在2016年12月29日至31日的3天时间里，神秘高手连胜柯洁九段、陈耀烨九段、朴廷桓九段、芈昱廷九段、唐韦星九段等高手。\n2017年初， AlphaGo化身神秘网络棋手Master击败包括聂卫平、柯洁、朴廷桓、井山裕太在内的数十位中日韩围棋高手，在30秒一手的快棋对决中无一落败， 拿下全胜战绩，在棋界和科技界引发剧震。\nAlphaGo的胜利充分让世人认识到了AI(Artifical Intelligence)的威力和未来，于是人工智能忽然之间离我们近在咫尺；对于我们大部分人来说，到底什么是人工智能？它是忽然之间冒出来的新概念吗？接下来笔者将为各位读者梳理一下其发展的路径以及历程以及主要的关键词。\n１　数据分析(Data Analysis)\n伴随着信息化时代的降临，人类第一次有了数据化的概念和积累；基于数据库系统和应用程序，可以直观查看统计分析系统中的数据，从而可以很快得到我们想要的结果；这个就是最基本的数据分析功能，也是我们在信息化时代了，除了重构业务流程、提升行业效率和降低成本之外，另一个非常重要的数据分析功能，数据直观化。\n举例如下，在财务系统的信息化中，基于企业的财务系统，我们可以直观获取企业现金流量表、资产负债表和利润表，这些都来自与我们的数据分析技术。目前常用的软件是Excel, R, Python等工具。\n2　数据挖掘(Knowledge-Discovery in Databases)\n简称KDD，从其英文缩写中可以发现，其是基于数据库系统的数据发现过程，立足与数据分析技术之上，提供更为高端和高级的规律趋势发现以及预测功能；同时数据量将变得更为庞大，依赖于模式识别等计算机前沿的技术；其还有另外一个名称为商业智能(BI, Business Intelligence)，依托于超大型数据库以及数据仓库、数据集市等数据库技术来完成。\n主要的应用领域在电子商务领域，主要的原因是电商时代其有迫切的数据挖掘的需求和应用场景，比如经典的啤酒与尿布的关联性就是电商应用中的一个例子。\n主要挖掘方法有: 分类 （Classification), 估计（Estimation）, 预测（Prediction）, 相关性分组或关联规则（Affinity grouping or association rules）, 聚类（Clustering）, 复杂数据类型挖掘(Text, Web ,图形图像，视频，音频等)等技术。\n此时的数据挖掘不足之处主要集中在数据库系统对于数据的检索分析能力支持有限，数据处理能力的不足大大限制了商业应用的进行，大部分场景下都是基于数据抽样的分析；同时挖掘应用需要进行定制化的开发，开发和维护成为即为昂贵，应用领域非常的狭小。\n这个时代主要的数据挖掘的解决方案主要集中在BI之上，主要来自于Oracle, IBM， Microsoft等数据库厂商的解决方案。\n《未完待续》\n----------------------------- 罪恶的分割线-------------------------------\n本文系CDSN的博主《木小鱼的笔记》个人原创，如要转载，请保留原始链接和原作者信息，支持原创，尊重原创，让知识的世界更美好。\n作者本人也维护了一个今日头条上的头条号：程序加油站，欢迎大家关注。"}
{"content2":"1，大数据、人工智能、机器学习、深度学习的关系。\n大数据，或者说大数据分析平台，更具体一点就是大数据分析PaaS平台，其实是一种针对需要处理海量数据统计分析的PaaS云平台。\n人工智能，是要让机器能够像人类一样具有感知、观察的能力，并且可以做到理解和推理（弱人工智能），甚至做到自适应、处理未曾遇到过的问题（强人工智能）。\n机器学习正是一种实现人工智能的方法，利用海量数据的训练，通过算法解析数据，从中学习，然后处理在真实世界中遇到的问题。\n深度学习是实现机器学习的一种技术，通过深度神经网络模型实现机器学习。\n从上面可以看出后三者是层层包含关系，最终都是为了实现人工智能，而实现人工智能的关键所在就是“通过海量数据的训练”，而这些海里数据的统计、分析与处理正是依靠大数据分析平台来实现的。\n2，关键技术。\n基于上面提到的这些名词之间的关系和实现要求，可以得出最关键的技术有：\n数学基础：统计与概率、线性代数等\n算法：机器学习算法、深度学习算法、神经网络模型等\n工具：大数据分析PaaS平台\n3，推荐一个大数据学习群 119599574每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享，"}
{"content2":"Kaggle 上的竞争非常激烈（有些比赛有数千名参赛者，并提供数百万美元的奖金），而且涵盖了各种类型的机器学习问题，所以它提供了一种现实方法来评判哪种方法有效、哪种方法无效。那么哪种算法能够可靠地赢得竞赛呢？顶级参赛者都使用哪些工具？\n在 2016 年和 2017 年，Kaggle 上主要有两大方法：梯度提升机和深度学习。具体而言，梯度提升机用于处理结构化数据的问题，而深度学习则用于图像分类等感知问题。使用前一种方法的人几乎都使用优秀的XGBoost 库，它同时支持数据科学最流行的两种语言：Python 和 R。使用深度学习的 Kaggle 参赛者则大多使用 Keras 库，因为它易于使用，非常灵活并且支持Python。\n要想在如今的应用机器学习中取得成功，你应该熟悉这两种技术：梯度提升机，用于浅层学习问题；深度学习，用于感知问题。用术语来说，你需要熟悉 XGBoost 和 Keras，它们是目前主宰 Kaggle 竞赛的两个库。"}
{"content2":"一 前言\n上一篇文章介绍了朴素贝叶斯的基本原理, 现在就来实践一下吧, 阅读了部分<机器学习实战>上的代码, 自己也敲了一遍, 做了一下验证, 现在就在这里分享一下.\n环境:\nUbuntu 16.04\nPython 3.5.2\n二 使用朴素贝叶斯进行文档分类\n2.1 准备数据: 从文本中构建词向量\n加载数据\n''' 加载训练数据, postingList是所有的训练集, 每一个列表代表一条言论, 一共有8条言论 classVec代表每一条言论的类别, 0是正常, 1是有侮辱性 返回 言论和类别 ''' def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'hime'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] return postingList, classVec\n看一下运行结果:\n统计文档中的单词, 生成词汇表, 词汇表中每一个单词只出现一次, 没有重复的. (就是把文档中的所有单词放在一块, 然后去重.)\n''' 创建词汇表, 就是把这个文档中所有的单词不重复的放在一个列表里面 ''' def createVocabList(dataSet): vocabSet = set([]) # 新建一个set集合, 保证里面的数据不重复 for document in dataSet: # 获得每一个文档 vocabSet = vocabSet | set(document) # 文档去重之后和词汇表求并集 return list(vocabSet) # 词汇表转换为列表\n这样我们就生成了一个词汇表. 看一下词汇表:\n文档转换为词向量, 对于每一个文档, 我们都要把他转换为词向量, 也就是由数字组成的一个向量, 此处的转换很简单. 上一步我们已经创建了一个词汇表, 对于一个文档, 首先我们生成一个和该文档长度一致的全0列表returnVec, 然后遍历该文当中的每一个单词, 如果这个单词在词汇表中出现过, 就在returnVec中相应位置变为1, 如果没出现过, 就仍然保持为0. 最后返回这个列表returnVec.\n''' vocabList是由createVocabList产生的词汇表 inputSet是输入新的文档 ''' def setOfWords2Vec(vocabList, inputSet): returnVec = [0] * len(vocabList) # 生成一个全0列表, 个数为输入文档的长度 for word in inputSet: # 遍历输入文档中的每一个单词 if word in vocabList: # 如果这个单词在词汇表中 returnVec[vocabList.index(word)] = 1 # 置1 else: # 否则依然为0 print(\"the word %s is not in my Vocabulary\" % word) return returnVec\n看一下第一个文档转换为词向量后是什么样子:\n词向量就是由0和1组成的数组.\n2.2 计算先验概率\n计算先验概率, 接下来就是计算各种先验概率了, 还记得甚麽是先验概率吗? 不记得的同学可以去前面文章里面看看甚麽是先验概率.\nP(x1|Y=ck),P(x2|Y=ck)...P(Y=ck)\nP(x_{1}|Y=c_{k}), P(x_{2}|Y=c_{k})... P(Y=c_{k}), 在此处我们使用了拉普拉斯平滑, 注意看代码里面的初始化.\n首先统计一共有多少个文档, 然后统计词向量的长度, 接着计算侮辱性文档的先验概率, 再初始化p0Num, p0Denom, p0Num就是一个array(numpy), 大小是词向量的长度, 它用于记录当前文档的每一个单词是否在词向量中存在, 当然它是初始化为全1, 也就是拉普拉斯平滑. 请看for循环, 遍历每一个文档, 首先判断当前文档的label是否为侮辱性的, 是侮辱性的就执行p1, 不是就执行p0. 我们看\np1Num += trainMatrix[i] , 这句话是两个array之间的相加, 也就是下图这种情况:\np1Denom += 1这个就是统计当前文档中有多少是属于侮辱性的, <机器学习实战>上写的是p1Denom += sum(trainMatrix[i]), 但是按照计算先验概率的公式, 我认为是加一即可. 最后p1Vect = log(p1Num / p1Denom), 取log是为了防止多个小数相乘出现下溢. 这样就计算出了每一个单词的先验概率, 以及每一个类别的先验概率.\n''' 计算先验概率 trainMatrix: 词向量矩阵 trainCategory: 每一个词向量的类别 返回每一个单词属于侮辱性和非侮辱性词汇的先验概率, 以及训练集包含侮辱性文档的概率 ''' def trainNB0(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) # 由训练集生成的词向量矩阵 numWords = len(trainMatrix[0]) # 每一个词向量的长度 pAbusive = sum(trainCategory) / float(numTrainDocs) # 计算侮辱性文档的先验概率 p0Num = ones(numWords) # 生成全1 array, 长度为词向量的长度, 用于统计每个单词在整个矩阵中出现的次数(分子) p1Num = ones(numWords) p0Denom = 2.0 # 初始化为2(分母), 拉普拉斯平滑 p1Denom = 2.0 for i in range(numTrainDocs): # 遍历每一个词向量 if trainCategory[i] == 1: # 如果该词向量的类别为1 p1Num += trainMatrix[i] # 计算P(x0)..P(xn) p1Denom += 1 # 统计侮辱性文档的个数 else: p0Num += trainMatrix[i] # 计算P(x0)..P(xn) p0Denom += 1 # 统计非侮辱性文档个数 p0Vect = log(p0Num / p0Denom) # 计算P(x0|0)P(xn|0) p1Vect = log(p1Num / p1Denom) # 计算P(x0|1) P(x1|1) P(xn|1) 取对数是防止多个小数相乘出现下溢 return p0Vect, p1Vect, pAbusive\n我们看一下计算结果:\np0V, p1V, pAb = trainNB0(trainMat, listClasses) p0V, p1V, pAb\n接下来就是将训练集里面的文档转换为词向量了. 代码很简单:\n''' 制作词向量矩阵 将每一个文档转换为词向量, 然后放入矩阵中 ''' trainMat = [] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n2.3 制作分类器, 测试\n接下来就是根据上面计算出来的每一个单词的先验概率, 来预测一个未知文档是否具有侮辱性了. 代码如下:\n''' 制作贝叶斯分类器 vec2Classify: 测试样本的词向量 p0Vec: P(x0|Y=0) P(x1|Y=0) P(xn|Y=0) p1Vec: P(x0|Y=1) P(x1|Y=1) P(xn|Y=1) pClass1: P(y) # log(P(x1|1)*P(x2|1)*P(x3|1)P(1))=log(P(x1|1))+log(P(x2|1))+log(P(1)) ''' def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 > p0: return 1 else: return 0\n注释部分已经解释了为什么要加上log(pClass1)了.\n检验效果的时候到了, 读入一个文档, 根据我们计算的先验概率, 分别计算他属于侮辱性文档的概率和属于非侮辱性文档的概率, 比较两个概率的大小, 大的那一类就是该文档所属于的类.\n''' 测试贝叶斯分类器 ''' def testingNB(): listOPosts, listClasses = loadDataSet() # 加载数据 myVocabList = createVocabList(listOPosts) # 词汇表 trainMat = [] # 训练集词向量 for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) p0V, p1V, pAb = trainNB0(trainMat, listClasses) # 计算先验概率 testEntry = ['love', 'my', 'dalmation'] # 测试文档1 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry, 'classified as :', classifyNB(thisDoc, p0V, p1V, pAb)) testEntry = ['stupid', 'garbage', 'stupid'] # 测试文档2 thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb))\n看一下结果吧,\n可以看出, 第一个属于非侮辱性的文档, 第二个属于侮辱性的文档. 因为文档中存在stupid这样的单词, 也就是骂人是傻子的意思.所以被判定为侮辱性的.\n文章主要参考<机器学习实战>和<统计学习方法>这两本书, 自己也是一个初学者, 文中有纰漏或者不当的地方, 欢迎各位朋友指出来, 咱们共同进步. 谢谢"}
{"content2":"北京 | 高性能计算之GPU CUDA课程11月24-26日3天密集学习 快速带你晋级阅读全文>\n沙韬伟，苏宁易购高级算法工程师。\n曾任职于Hewlett-Packard、滴滴出行。\n数据学院特邀讲师。\n主要研究方向包括风控、推荐和半监督学习。目前专注于基于深度学习及集成模型下的用户行为模式的识别。\n目标导向，不谈其他的方向，只谈如何快速拿到数据挖掘的offer。\n\n我选择了公司的校招中比较严格的（top5%）一个jd要求，我们看下如何拿下这个offer。\n计算机或者数学等相关专业学历\n无论你是什么专业的，大学有个选修课的东西，很多人都是用来混学分的，你可以去选择比如“数理统计选修”、“概率论选修”、“算法基础”这些课，挂了不影响绩点，但是你考的好可以给之后找工作带来很高加分，你的老板会认为你是一个热爱数据挖掘并且有长远计划的人。\n在机器学习、数据挖掘、统计学理论、最优化理论等领域有着深厚积累\n“全栈工程师”在我心中一直是伟大的存在，但是对于本科毕业，选择专精很重要。推荐几个方向：用户研究（征信、消费者研究），图像音频分析（讯飞之流），异常分析（风控、物流），订单预测等等，你在一方面研究的比较深，就算是应届生，但是比起像我这种三四年的老油条，老板更喜欢培养这样带有“天赋专精”的白纸。\n熟悉机器学习和数据挖掘领域前言技术\n前沿技术是一个比较宽泛的词，相信我，面试你的人每天从早上10点加班到晚上10点，周末还要打游戏，他自己都不知道啥叫前沿技术，平时打好基础，老板基础问不倒的时候，你就已经在候选人名单里面了。\n在国际顶级回忆和期刊以第一作者发表过高水瓶论文者优先\n这点我也做不到，我就不bb了。\n在机器学习、数据挖掘等相关项目实际经验者或者知名数据挖掘比赛（例如KDD Cup等）中取得领先名次者优先\n这个是非常重要的。\n在应届生面前，成绩是没有区分能力的，老板在乎的是你入职后给公司带来的利益而不是你是不是学生会主席这些毫无意义的事情。我走的是前面这条路，分享一下我的套路。我大三的时候参加了百度的某个不知名比赛，拿了区的入围奖，然后拿着这个入围奖参加中国电信实习生招聘，大四凭借一年电信实习经验去了惠普的数据挖掘实验室继续实习，完整的做了2个项目后正式入职现公司。\n分析下来就是，刚开始是弱鸡，所以采取了一些简单的途径拿到一些听起来很厉害但是很弱鸡的奖，再借这个奖混进一些容易被唬住的公司（没有黑电信的意思），之后在凭借工作经验进入一些能够给你机会的地方（就是让你自己带一些项目），好好做几个优秀的项目（这个比较难，看命），然后达成你的目标。想一下子第一很难，但是我们可以做一个长远的规划，先前100，再前50，再前10，再第一。\n编程基础扎实，熟悉算法数据结构，有多年scala或python开发经验\n我大学的时候学校教了spss，但是我当时考虑的是外面企业都是抠x。应该不会买，果然毕业之后，R语言、Python火的不行，建议选择一门你喜欢的开源语言，去写code练吧，网上很多撕逼的问题比如“php是不是最好的编程语言”，其实，选择一个你最适应的就行，别在乎别人怎么bb，因为最后要用的人是你自己。\n有大数据相关系统，拥有map-reduce、sprak、实时计算等经验\n这个需要回到第5条，我在大学里面的时候，老师就忽悠我，说什么mysql足够了，而且一般学校不会搭建Hadoop这些平台，搭建成本高，维护成本高。建议混入一些“高端的”公司，请里面的老油条两三顿夜宵，给你开个查询权限，相信我，从菜鸟到乳鸽，你只需要1个月的时间。\n踏实勤奋、自我驱动、善于沟通\n个人感觉数据挖掘岗位需要强迫自己拥有三个技能。\n1）快速学习能力，永远不知道下一个火起来的算法是啥\n2）数据敏感力，数据挖掘的下限取决于勤奋，上限取决于数据敏感程度\n3）沟通能力，你要让你的boss相信你做的Random forest结果，你需要先说服他，我做的是靠谱的。\n原文链接：http://www.jianshu.com/p/328fafc251f1\n查阅更为简洁方便的分类文章以及最新的课程、产品信息，请移步至全新呈现的“LeadAI学院官网”：\nwww.leadai.org\n请关注人工智能LeadAI公众号，查看更多专业文章\n大家都在看\nLSTM模型在问答系统中的应用\n基于TensorFlow的神经网络解决用户流失概览问题\n最全常见算法工程师面试题目整理（一）\n最全常见算法工程师面试题目整理（二）\nTensorFlow从1到2 | 第三章 深度学习革命的开端：卷积神经网络\n装饰器 | Python高级编程\n今天不如来复习下Python基础\n点击“阅读原文”直接打开【北京站 | GPU CUDA 进阶课程】报名链接"}
{"content2":"人工智能(Artificial Intelligence)是最早提出的一个专有名词,早在50多年前就有几个计算机科学家提出了人工智能的概念,希望可以制造出可以和人类拥有类似智慧的机器.几十年来这个概念被不断的扩散至各行各业.当然也就带来了各种滥用,一些带了些许自动化算法的软件也被称为人工智能.而通常人们心中的人工智能是美国大片终结者里面的存在.或者至少是钢铁侠盔甲级别的存在才叫人工智能.而目前业界的真实的人工智能还处于早期人工智能阶段,或者叫做弱人工智能,终结者这样的机器人应该才算强人工智能.不过目前离这个目标还有些遥远.人工智能往往结合着制造业,因此说人工智能的时候往往会说机器人.笔者目前在西湖大学的实验中心就是西湖大学人工智能与机器人中心.将人工智能结合机械臂,仿生机器人,纳米机器人甚至是无人机等均属于人工智能与制造业结合的产物.\n机器学习(Machine Learning)是实现人工智能的一种手段.也是目前被认为比较有效的实现人工智能的手段.目前在业界使用机器学习比较突出的领域很多,例如计算机视觉,自然语言处理,推荐系统,文本分类等,大家生活中经常用到的比如高速上ETC的车牌识别,苹果手机上的Siri,看今日头条时给你推荐的新闻,再比如大家用天猫买东西看评论的时候的评价描述\n可以看到通过机器学习的算法,在8W多条评价中筛选出关键词,红色的是正面的评价,绿色的是反面的评价,这些都是通过语义分析算法归类得出的.机器学习本质上是通过数学算法来解析数据的规律,学习相关的规律并且用来预测和决策.机器学习主要分为监督学习,无监督学习和半监督学习三种.从算法上来说有贝叶斯分类,决策树,线性回归,决策树和随机森林,主成分分析,流行学习,k-means聚类,高斯混合模型等等.\n深度学习(Deep Learning)是一种机器学习的技术,由于深度学习在现代机器学习中的比重和价值非常巨大,因此常常将深度学习单独拿出来说.最初的深度学习网络是利用神经网络来解决特征层分布的一种学习过程.通常我们了解的DNN(深度神经网络),CNN(卷积神经网络),RNN(循环神经网络),LSTM(长短期记忆网络)都是隶属于深度学习的范畴.也是现代机器学习最常用的一些手段.通过这些手段,深度学习在视觉识别,语音识别,自然语言处理(NLP)等领域取得了使用传统机器学习算法所无法取得的成就.\n强化学习(Reinforcement Learning),又称再励学习或者评价学习.也是机器学习的技术之一.所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号(强化信号)函数值最大,由于外部给出的信息很少,强化学习系统必须依靠自身的经历进行自我学习.通过这种学习获取知识,改进行动方案以适应环境.强化学习最关键的三个因素是状态,行为和环境奖励.关于强化学习和深度学习的实例,最典型的莫过于谷歌的AlphaGo和AlphaZero两位了,前者通过深度学习中的深度卷积神经网络,在训练了大约三千万组人类的下棋数据,无数度电的情况下才搞出来的模型,而后者使用强化学习的方式,通过自己和自己下棋的方式搞出来的模型.而最终的实验结果也很让人震撼.AlphaGo干败了人类围棋顶尖高手,而AlphaZero干败了AlphaGo."}
{"content2":"斯坦福机器学习公开课对于初学者是一个很好的入门教程。而网易公开课上的资源并没有相关的讲义文件，特意上传。百度网盘链接 密码：bxsc\n链接：http://pan.baidu.com/s/1i53pYbb 密码：bxsc"}
{"content2":"机器学习：\n一般被定义为一个系统自我改进的一个过程，但是知识用这些知识来理解和\n实现机器学习，想必有些困难。他从最初的基于神经元模型以及函数逼近论\n的方法来研究；到以符号演算为基础的规则学习和决策树中产生，和之后认知\n心理学中归纳，解释，类比等概念的引入，以至如今的机器学习理论，和统计学的兴起；\n，当然还有马尔科夫过程的增强学习；机器学习一直在各个学科实践之中有主导作用，；\n当然人无完人，算法无极致，不同的机器学习方法也有其的优缺点和器相应的应用领域；\n这里总结一下最实用的理论和算法：\n概念学习，决策树，神经网络，贝叶斯学习，基于实例的学习，遗传算法，规则学习，基于解释的学习和\n增强学习等\n目前机器学习成功应用的领域包含哪些呢：\n1：检测信用卡交易欺诈的数据挖掘模型\n2：用户阅读兴趣的信息过滤系统，\n3：高速公路上自动行驶的汽车（百度）\n4：用户感兴趣内容的推荐，如抖音，今日头条，网上商城（淘宝，京东）\n机器学习只与计算机有关吗：\n答案是否定的！机器学习不仅包括计算机科学，他还从许多多学科吸收了成果和概念：\n包括了统计学，人工智能，哲学，信息论，生物学，认知科学，计算发杂性，和控制论等\n那么机器学习理论到底是干什么呢？\n1：学习性能是怎样对着给定的训练样例的数量而变化的\n2：多于不同类型的学习任务，那个学习算法最合适\n参考自书籍<<机器学习> >精简而来"}
{"content2":"BAT机器学习面试1000题系列\n我的Leetcode账号\n七月在线 购买的课程\n强调两点:\n1.虽然本系列主要都是机器学习、深度学习相关的考题，并无其他类型的题，但不代表应聘机器学习或深度学习时面试官就只问这两项，虽是做数据或AI相关，基本的语言（比如Python）、编码coding能力、数据结构、算法、计算机体系结构、操作系统、概率统计基本都是搞IT必备，也必须掌握。对于数据结构和算法，一者，重点推荐前面说的微软面试100题系列；二者，多刷leetcode，看1000道题不如实际动手刷100道。\n—BAT机器学习面试1000题系列（第1~10题）—\n1.请简要介绍下SVM\nSVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。\n扩展：这里有篇文章详尽介绍了SVM的原理、推导，《支持向量机通俗导论（理解SVM的三层境界）》。此外，这里有个视频也是关于SVM的推导：《纯白板手推SVM》\n2.请简要介绍下tensorflow的计算图\nTensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。\n3.请问GBDT和XGBoost的区别是什么？\nxgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：\n1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数\n2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性\n3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的\n更多详见\n4.在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？\n曼哈顿距离只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。\n5.百度2015校招机器学习笔试题\n百度2015校招机器学习笔试题\n6.简单说说特征工程\n7.关于LR\n把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。\n另外，关于答案这篇文章可以做参考：\nhttp://blog.csdn.net/cyh_24/article/details/50359055.html\nhttp://blog.csdn.net/zouxy09/article/details/20319673\n8.overfitting怎么解决？\ndropout、regularization、batch normalizatin\n9.LR和SVM的联系与区别\n1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）\n2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。\n区别：\n1、LR是参数模型，SVM是非参数模型。\n2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。\n3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。\n4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。\n5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。\n来源：http://blog.csdn.net/timcompp/article/details/62237986\n10.LR与线性回归的区别与联系\n个人感觉逻辑回归和线性回归首先都是广义的线性回归，\n其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，\n另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。\n@乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。\n本来来自七月在线实验室 仅供自己学习使用"}
{"content2":"机器学习分为有监督学习和无监督的学习。\n有监督学习：对数据的若干特征与若干标签之间的关联性进行建模的过程，确定模型后就能应用到新的未知数据中。进一步可以分为分类和回归任务。分类对应离散型数据，而回归对应的是连续性数据。SVM、随机森林和神经网络属于有监督的学习。\n无监督学习：对不带任何标签的数据特征进行建模。包括聚类和降维，例如k-means算法等。\n其中半监督学习介于二者之间，适用于数据标签不完整的情况。\nPython机器学习主要调用模块为sklearn，里面有机器学习使用的各种模型算法以及评价指标。\n进行数据分析、建模的过程一般为：读取数据—抽取样本，生成测试集和检验集—调用模型—模型预测—采用模型评价指标，评价模型预测结果\n影响模型质量的两个因素为模型的复杂度以及训练集的规模。模型的学习曲线是指，训练集规模的训练得分/检验集的得分。\n特征：\n特定复杂度的模型对较小的数据集容易过拟合，此时训练集的得分较高，检验集的得分较低；\n特定复杂度的模型对较大的数据集容易欠拟合；随着数据的增大，训练集得分会不断降低，检验集评分会不断升高；\n模型的检验集得分永远不会高于训练集得分，两条曲线不断靠近，却不会交叉。\n本次实验选取多项式模型，通过模型的多项式来提高或减少模型的复杂度，观察学习曲线的变化趋势。\n（一）构造多项式回归模型，生成测试样本数据。\n代码：\n#构造多项式模型 from sklearn.preprocessing import PolynomialFeatures #专门产生多项式的工具并且包含相互影响的特征集 from sklearn.linear_model import LinearRegression #线性回归模型 from sklearn.pipeline import make_pipeline #构造管道 def PolynomialRegression(degree=2, **kwargs): #**kwargs 形参，返回值为字典类型 return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs)) #形成样本数据 import numpy as np def make_data(N, err=1.0, rseed=1): #随机抽样数据 rng = np.random.RandomState(rseed) X = rng.rand(N, 1) ** 2 y = 10 - 1./ (X.ravel() + 0.1) if err > 0: y += err * rng.randn(N) return X, y X, y = make_data(40)\n（二）绘制多项式函数图像\n绘制散点图，和几个多项式函数图像，观察多项式模型拟合效果。\n代码：\n%matplotlib inline import matplotlib.pyplot as plt import seaborn; seaborn.set() #设置图形样式 X_test = np.linspace(-0.1, 1.1, 500)[:, None] plt.scatter(X.ravel(), y, color='black') axis = plt.axis() for degree in [1, 3, 5]: y_test = PolynomialRegression(degree).fit(X,y).predict(X_test) plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree)) plt.xlim(-0.1, 1.0) plt.ylim(-2, 12) plt.legend(loc='best');\n图形：\n就该图形显示，三项式函数和五项式函数的模型拟合结果较为可观，一元线性函数的模型拟合效果较差。\n（三）绘制学习曲线\n观察训练得分和测试得分随着项式的增加，即随着模型复杂度的增加，训练得分和检验得分的变化情况。\n代码：\nfrom sklearn.learning_curve import validation_curve degree = np.arange(0, 21) train_score,val_score = validation_curve(PolynomialRegression(), X, y, 'polynomialfeatures__degree', degree, cv=7) plt.plot(degree, np.median(train_score, 1),color='blue', label='training score') plt.plot(degree, np.median(val_score, 1), color='red', label='validation score') plt.legend(loc='best') plt.ylim(0, 1) plt.xlabel('degree') plt.ylabel('score')\n图像：\n结果显示：随着模型复杂度的增加，训练得分（图中的蓝线部分）快速增加，达到饱和之后，增长幅度较低，趋于平缓；\n测试得分（图中红色曲线）先增加，到达某一值后由于过拟合导致得分减少；\n训练得分明显高于测试得分。该结果显示随着当模型复杂度到一定程度时，再增加模型的复杂度对模型的得分可能产生较小的影响或负影响，这个时候就会考虑更换模型。\n（四）增加样本数量，观察模型拟合效果\n在小数据的检验结果上得出，复杂度较高的模型，由于过拟合导致检验得分较低，所以测试，当增加样本数量时，学习曲线的变化情况。\n代码：\nX2, y2 = make_data(200) #生成200个数据样本 plt.scatter(X2.ravel(), y2) #重新绘制学习曲线，并将小样本曲线添加上去 degree = np.arange(21) train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2, 'polynomialfeatures__degree', degree, cv=7) plt.plot(degree, np.median(train_score2, 1), color='blue', label='train score') plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score') plt.plot(degree, np.median(train_score, 1),color='blue', alpha=0.3, linestyle='dashed') plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed') plt.legend(loc='lower center') plt.ylim(0, 1) plt.xlabel('degree') plt.ylabel('score')\n图像：\n其中虚线代表小数据集的学习曲线，实线代表大数据集的学习曲线。\n结论：大规模数据集的检验得分和测试得分的变化趋势一致，过拟合情况也不是很明显，说明大数据集适合用复杂程度较高的模型。"}
{"content2":"最近一边看书，一边梳理机器学习的知识点：\n1. 线性回归\n2.线性回归的损失函数（误差的平方和）\n3. 最小二乘法(手推导)\n4.批量梯度下降法（学习率大小问题）\n5.放缩scaling对梯度下降的影响\n6.多元线性回归\n7.逻辑斯蒂回归-二元分类\n8.LR代价函数\n9.神经网络\n10.前向传播和后向传播\n11.神经网络过程\n12.模型选择和交叉验证\n13.高偏差和高方差\n14.偏差和方差（欠拟合和过拟合）\n15.学习率\n16.模型评价  混淆矩阵  准确率和召回率  F1值，ROC曲线和auc的含义\n17.无监督聚类 k-means  迭代算法和过程   选择聚类数的动机  聚类相似度和距离的度量  中心点随机初始化   代价函数   容易陷入局部最小值  解决办法多运行几次取loss最小的，降维  数据可视化   PCA。\n18."}
{"content2":"字典特征的抽取：\nimport sklearn from sklearn.feature_extraction import DictVectorizer dv = DictVectorizer() instances = [{'city': '北京','temperature':100},{'city': '上海','temperature':60}, {'city': '深圳','temperature':150}] data = dv.fit_transform(instances).toarray() print(data) print(dv.get_feature_names()) print(dv.inverse_transform(data))\n[[ 0. 1. 0. 100.] [ 1. 0. 0. 60.] [ 0. 0. 1. 150.]] ['city=上海', 'city=北京', 'city=深圳', 'temperature'] [{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 60.0}, {'city=深圳': 1.0, 'temperature': 150.0}]\n文本特征的提取\nfrom sklearn.feature_extraction.text import CountVectorizer content = [\"life is short,i like python\",\"life is too long,i dislike python\"] vectorizer = CountVectorizer() print(vectorizer.fit_transform(content).toarray())\n[[0 1 1 1 0 1 1 0] [1 1 1 0 1 1 0 1]]\n中文文本特征抽取\nfrom sklearn.feature_extraction.text import CountVectorizer content = [\"我不喜欢python\",\"life is too long,i dislike python\"] vectorizer = CountVectorizer() print(vectorizer.fit_transform(content).toarray())\n[[0 0 0 0 0 0 1] [1 1 1 1 1 1 0]]\n中文特征化处理\nimport jieba import numpy as np from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer def cutword(): \"\"\" 分词后的字符串结果 :return: c1,c2,c3 \"\"\" # 将内容进行分词 content1 = jieba.cut('今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。') content2 = jieba.cut('我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。') content3 = jieba.cut('如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。') # 建立列表取出迭代器数据 con1 = [] con2 = [] con3 = [] for word in content1: con1.append(word) for word in content2: con2.append(word) for word in content3: con3.append(word) # 将列表转换成字符串 c1 = ' '.join(con1) c2 = ' '.join(con2) c3 = ' '.join(con3) return c1, c2, c3 # 中文特征值化 def countvec(): \"\"\" 文本特征抽取 :return: None \"\"\" # 调用分词分割中文文章 c1, c2, c3 = cutword() print(\"分词结果：\",c1, c2, c3) # 实例化 cv = CountVectorizer() data = cv.fit_transform([c1, c2, c3]) print(cv.get_feature_names()) print(data.toarray()) return None # 中文特征值化tf-idf def tfidfvec(): \"\"\" 文本特征抽取 :return: None \"\"\" # 调用分词分割中文文章 c1, c2, c3 = cutword() print(\"分词结果：\",c1, c2, c3) # 实例化 tf = TfidfVectorizer(stop_words=['一种', '不会']) data = tf.fit_transform([c1, c2, c3]) print(tf.get_feature_names()) print(data.toarray()) return None countvec()\n分词结果： 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。 我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。 如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '这样'] [[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0] [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1] [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]\n归一化代码化示例：\nfrom sklearn.preprocessing import MinMaxScaler def mms(): minmax = MinMaxScaler(feature_range=(2,4)) #data = minmax.fit_transform([[90,2,10,46],[60,4,15,45],[75,3,13,46]]) data = minmax.fit_transform([[90, 2, 10, 46],[60,4,15,45],[75,3,13,46]]) print(data) mms()\n[[4. 2. 2. 4. ] [2. 4. 4. 2. ] [3. 3. 3.2 4. ]]\n标准化代码化示例：\nfrom sklearn.preprocessing import StandardScaler def standard(): std = StandardScaler() data = std.fit_transform([[1.,-1.,3.],[2.,4.,2.],[4.,6.,-1.]]); print(data) standard()\n[[-1.06904497 -1.35873244 0.98058068] [-0.26726124 0.33968311 0.39223227] [ 1.33630621 1.01904933 -1.37281295]]\nImputer代码使用示例：\nfrom sklearn.preprocessing import Imputer def standard(): std = Imputer() data = std.fit_transform([[1.,-1.,3.],[2.,4.,2.],[4.,6.,-1.]]); print(data) standard()\n[[ 1. -1. 3.] [ 2. 4. 2.] [ 4. 6. -1.]]\naitest_08_Imputer缺失值\nfrom sklearn.preprocessing import Imputer import numpy as np def im(): imp = Imputer(missing_values='NaN',strategy ='mean',axis=0) data = imp.fit_transform([[1,2],[np.nan,3],[7,6]]); print(data) im()\n[[1. 2.] [4. 3.] [7. 6.]]\n过滤\nfrom sklearn.feature_selection.variance_threshold import VarianceThreshold import numpy as np def variance(): van = VarianceThreshold(threshold=0.0) data = van.fit_transform([[0,2,0,3],[0,1,4,3],[0,1,1,3]]); print(data) variance()\n[[2 0] [1 4] [1 1]]\nPCA代码示例\nfrom sklearn.decomposition import PCA import numpy as np def pca(): pa = PCA(n_components=3) data = pa.fit_transform([[2,8,4,5],[6,3,0,8],[5,4,9,1]]); print(data) pca()\n[[ 1.28620952e-15 3.82970843e+00 5.26052119e-16] [ 5.74456265e+00 -1.91485422e+00 5.26052119e-16] [-5.74456265e+00 -1.91485422e+00 5.26052119e-16]]\n性别预测代码示例：\nfrom sklearn import tree features=[[178,1],[155,0],[177,0],[165,0],[169,1],[160,0]] labels=['male','female','male','female','male','female'] #创建决策树clf clf=tree.DecisionTreeClassifier() #将数据交给决策树进行训练 clf=clf.fit(features,labels) #假设此时有一个158的没有胡子的人，机器会如何判断性别？ result =clf.predict([[158,0]]) print(result) #再次判断172有胡子的人 result=clf.predict([[172,1]]) print(result)\n['female'] ['male']"}
{"content2":"仅做个人学习交流，如有版权问题，请及时联系本人删除~\n链接: https://pan.baidu.com/s/1TSU6Rs2cLWqI2IyNOE2mxQ 密码: qd3c\n相关学习路线见另一篇文章：https://blog.csdn.net/qwxwaty/article/details/80793370"}
{"content2":"[机器学习入门] 李宏毅机器学习笔记-8（Backpropagation；反向传播算法）\nPDF\nVIDEO\n当我们要用gradient descent来train一个neural network，要怎么做？\nGradient Descent\nbackpropagation就是Gradient Descent。\nChain Rule（连锁法）\nBackpropagation主要用到了Chain Rule\nBackpropagation\nForward pass\nBackward pass\nSummary"}
{"content2":"1、定义\n引用维基百科和百度百科。\n监督式学习（英语：Supervised learning），是一个机器学习中的方法，可以由训练资料中学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。训练资料是由输入物件（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个分类标签（称作分类）。（wikipedia）利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。（baidu）\n无监督学习/非监督式学习(unsupervised learning):设计分类器时候，用于处理未被分类标记的样本集。监督学习中在给予计算机学习样本的同时，还告诉计算各个样本所属的类别。若所给的学习样本不带有类别信息,就是无监督学习。（baidu）\n非监督式学习是一种机器学习的方式，并不需要人力来输入标签。它是监督式学习和强化学习等策略之外的一种选择。在监督式学习中，典型的任务是分类和回归分析，且需要使用到人工预先准备好的范例。一个常见的非监督式学习是数据聚类。\n百度百科和维基百科定义有点抽象，为了更为简明的在神经网络学习中进行表述，我们给出我们对监督学习和非监督学习的定义：\n监督式学习：能够通过训练样本集或专家知识构建已知且确定的判定函数，并根据训练集和该判定函数形成模型改进策略，对模型参数进行不断改进，完成模型学习的过程称为监督式学习；如果无法从训练样本集或专家知识构建确定的判定函数，而通过训练集与一给定的判定函数进行模型参数不断改进，完成学习的过程称为非监督式学习。"}
{"content2":"今天收到了第一笔赞赏，非常感谢，也很惊喜，o(￣▽￣)ブ\n时间\n赞赏者\n赞赏留言\n赞赏数额\n2019.7.18\n奔跑\n感谢你整理Hong-yi Lee老师讲解的机器学习的资料~\n5元\nTOPIC\nCONTENTS\nBLOG\nPDF\nVIDEO\n【1】Learning Map（学习导图）\nblog\npdf\nvideo\n【2】Regression：Case Study ；回归：案例研究\nblog\npdf\nvideo\n【3】Gradient Descent ；梯度下降\nblog\npdf\nvideo\n【4】Where does the error come from? ；误差分析\nblog\npdf\nvideo\n【5】Classification: Probabilistic Generative Model；分类：概率生成模型\nblog\npdf\nvideo\n【6】Classification: Logistic Regression；Logistic回归\nblog\npdf\nvideo\n【7】Brief Introduction of Deep Learning；深度学习简介\nblog\npdf\nvideo\n【8】Backpropagation；反向传播算法\nblog\npdf\nvideo\n【9】“Hello world” of deep learning；初探深度学习\nblog\npdf\nvideo\n【10】Tips for Deep Learning；深度学习小贴士\nblog\npdf\nvideo\n【11】Convolutional Neural Network；卷积神经网络\nblog\npdf\nvideo\n【12】Why Deep Learning? ; 为什么是深度学习？\nblog\npdf\nvideo\n【13】Semi-supervised Learning ;半监督学习\nblog\npdf\nvideo\n【14】Unsupervised Learning: Linear Dimension Reduction；线性降维\nblog\npdf\nvideo\n【15】Unsupervised Learning: Word Embedding；无监督学习：词嵌入\nblog\npdf\nvideo\n【16】Unsupervised Learning: Neighbor Embedding；无监督学习：邻域嵌套\nblog\npdf\nvideo\n【17】Unsupervised Learning: Deep Auto-encoder；无监督学习：深度自动编码器\nblog\npdf\nvideo\n【18】Deep Generative Model-part 1：深度生成模型-part 1\nblog\npdf\nvideo\n【19】Deep Generative Model-part 2：深度生成模型-part 2\nblog\npdf\nvideo\n【20】Deep Generative Model-part 3：深度生成模型-part 3\nblog\npdf\nvideo\n【21】Transfer Learning part 1 ; 迁移学习 part 1\nblog\npdf\nvideo\n【22】Transfer Learning part 2；迁移学习 part 2\nblog\npdf\nvideo\n【23】Support Vector Machine；支持向量机\nblog\npdf\nvideo\n【24】introduction of Structured Learning;结构化学习介绍\nblog\npdf\nvideo\n【25】Structured Linear Model；结构化预测-线性模型\nblog\npdf\nvideo\n【26】Structured Support Vector Machine part 1;结构化支持向量机part 1\nblog\npdf\nvideo\n【27】Structured SVM part 2；结构化支持向量机 part 2\nblog\npdf\nvideo\n【28】Structured SVM part 3 ;结构化支持向量机 part 3\nblog\npdf\nvideo\n【29】Sequence Labeling Problem part 1;结构化预测-序列标记 part 1\nblog\npdf\nvideo\n【30】Sequence Labeling Problem part 2 ;结构化预测-序列标记 part 2\nblog\npdf\nvideo\n【31】Sequence Labeling Problem part 3 ;结构化预测-序列标记 part 3\nblog\npdf\nvideo\n【32】Recurrent Neural Network part 1;循环神经网络 part 1\nblog\npdf\nvideo\n【33】Recurrent Neural Network part 2;循环神经网络 part 2\nblog\npdf\nvideo\n【34】Recurrent Neural Network part 3;循环神经网络 part 3\nblog\npdf\nvideo\n【35】Ensemble；集成方法\nblog\npdf\nvideo\n【36】Ensemble part 2；集成方法 part 2\nblog\npdf\nvideo\n【37】Deep Reinforcement Learning;深度增强学习入门\nblog\npdf\nvideo\n【38】李宏毅机器学习课程回顾 + 接下来的学习声明\nblog\n【39】李宏毅机器学习课程从这里开始\n本文\nblog\n相关链接：\n课程主页：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html\nPPT打包下载：http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/all.rar"}
{"content2":"AI之ML/EL/RL：人工智能之机器学习、深度学习、强化学习的算法分类三者关系结构图之详细攻略\nML/EL/RL联系结构图—简单版本\nML/EL/RL联系结构图—复杂版本\n敬请期待\n相关文章\nML：机器学习、深度学习算法的思维导图(吊炸天)、相关原理图等集合——非常经典、建议收藏"}
{"content2":"先看如下三张图：\n图一：\n图二：\n图三：图二详细版本\n人工智能：是一个大概念，从有效的老式人工智能（GOFAI）到联结主义结构，无所不包。而机器学习则是人工智能领域的一个小分支，如果说AI是一个合集，那么ML就是AI的子集。任何通过数据训练的学习算法的相关研究都属于机器学习，包括很多已经发展多年的技术，比如线性回归（Linear Regression）、K均值（K-means，基于原型的目标函数聚类方法）、决策树（Decision Trees，运用概率分析的一种图解法）、随机森林（Random Forest，运用概率分析的一种图解法）、PCA（Principal Component Analysis，主成分分析）、SVM（Support Vector Machine，支持向量机）以及ANN（Artificial Neural Networks，人工神经网络）。而人工神经网络则是深度学习的起源。\n机器学习：machine learning，是计算机科学和统计学的交叉学科，基本目标是学习一个x->y的函数（映射），来做分类或者回归的工作。之所以经常和数据挖掘合在一起讲是因为现在好多数据挖掘的工作是通过机器学习提供的算法工具实现的，\n例如，广告的ctr预估，PB级别的点击日志在通过典型的机器学习流程可以得到一个预估模型，从而提高互联网广告的点击率和回报率；\n个性化推荐，还是通过机器学习的一些算法分析平台上的各种购买，浏览和收藏日志，得到一个推荐模型，来预测你喜欢的商品。\n深度学习：deep learning，机器学习里面现在比较火的一个topic（大坑），本身是神经网络算法的衍生，在图像，语音等富媒体的分类和识别上取得了非常好的效果，所以各大研究机构和公司都投入了大量的人力做相关的研究和开发。\n数据挖掘：也就是data mining，是一个很宽泛的概念。字面意思就是从成吨的数据里面挖掘有用的信息。这个工作BI（商业智能）可以做，数据分析可以做，甚至市场运营也可以做。你用excel分析分析数据，发现了一些有用的信息，然后这些信息可以指导你的business，恭喜你，你已经会数据挖掘了。\n如果你想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。\n总结下，\n数据挖掘是个很宽泛的概念，\n数据挖掘常用方法大多来自于机器学习这门学科，\n深度学习是机器学习一类比较火的算法，本质上还是原来的神经网络。\n链接：\n为什么说深度学习和机器学习截然不同？_网易科技\n数据挖掘、机器学习、深度学习这些概念有区别吗？\n以下图片来自饭团“AI产品经理大本营”，点击这里可关注：http://fantuan.guokr.net/groups/219/   作者：黄钊hanniman\n（）这是一个非常生动的例子，说明什么是机器学习——宝宝们将“分享”理解为“受到伤害”的含义，并且实际应用。\n（）深度学习，是应用了多层神经网络的机器学习。（在的角度，机器学习和深度学习，不是并列概念，而是子集关系）\n（~）机器学习，是计算的反问题（逆运算），这是我目前看到的最通俗易懂的一个解释。\n   什么是计算？\n（）类“机器学习”——看相。感觉如果做一款可以看手相、面相的AI产品，也蛮有意思的。\n（）其他相关概念的通俗理解。\n（）机器学习的各种问题和局限性。 先看如下三张图：\n图一：\n图二：\n图三：图二详细版本\n人工智能：是一个大概念，从有效的老式人工智能（GOFAI）到联结主义结构，无所不包。而机器学习则是人工智能领域的一个小分支，如果说AI是一个合集，那么ML就是AI的子集。任何通过数据训练的学习算法的相关研究都属于机器学习，包括很多已经发展多年的技术，比如线性回归（Linear Regression）、K均值（K-means，基于原型的目标函数聚类方法）、决策树（Decision Trees，运用概率分析的一种图解法）、随机森林（Random Forest，运用概率分析的一种图解法）、PCA（Principal Component Analysis，主成分分析）、SVM（Support Vector Machine，支持向量机）以及ANN（Artificial Neural Networks，人工神经网络）。而人工神经网络则是深度学习的起源。\n机器学习：machine learning，是计算机科学和统计学的交叉学科，基本目标是学习一个x->y的函数（映射），来做分类或者回归的工作。之所以经常和数据挖掘合在一起讲是因为现在好多数据挖掘的工作是通过机器学习提供的算法工具实现的，\n例如，广告的ctr预估，PB级别的点击日志在通过典型的机器学习流程可以得到一个预估模型，从而提高互联网广告的点击率和回报率；\n个性化推荐，还是通过机器学习的一些算法分析平台上的各种购买，浏览和收藏日志，得到一个推荐模型，来预测你喜欢的商品。\n深度学习：deep learning，机器学习里面现在比较火的一个topic（大坑），本身是神经网络算法的衍生，在图像，语音等富媒体的分类和识别上取得了非常好的效果，所以各大研究机构和公司都投入了大量的人力做相关的研究和开发。\n数据挖掘：也就是data mining，是一个很宽泛的概念。字面意思就是从成吨的数据里面挖掘有用的信息。这个工作BI（商业智能）可以做，数据分析可以做，甚至市场运营也可以做。你用excel分析分析数据，发现了一些有用的信息，然后这些信息可以指导你的business，恭喜你，你已经会数据挖掘了。\n如果你想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。\n总结下，\n数据挖掘是个很宽泛的概念，\n数据挖掘常用方法大多来自于机器学习这门学科，\n深度学习是机器学习一类比较火的算法，本质上还是原来的神经网络。\n链接：\n为什么说深度学习和机器学习截然不同？_网易科技\n数据挖掘、机器学习、深度学习这些概念有区别吗？\n以下图片来自饭团“AI产品经理大本营”，点击这里可关注：http://fantuan.guokr.net/groups/219/   作者：黄钊hanniman\n（）这是一个非常生动的例子，说明什么是机器学习——宝宝们将“分享”理解为“受到伤害”的含义，并且实际应用。\n（）深度学习，是应用了多层神经网络的机器学习。（在的角度，机器学习和深度学习，不是并列概念，而是子集关系）\n（~）机器学习，是计算的反问题（逆运算），这是我目前看到的最通俗易懂的一个解释。\n   什么是计算？\n（）类“机器学习”——看相。感觉如果做一款可以看手相、面相的AI产品，也蛮有意思的。\n（）其他相关概念的通俗理解。\n（）机器学习的各种问题和局限性。 先看如下三张图："}
{"content2":"图解人工智能机器学习深度学习的关系和区别，先直观看下图的关系：\nAI（Artificial Intelligence.人工智能）、机器学习（machine learning）、深度学习（Deep learning）\n人工智能和机器学习和深度学习的区别：机器学习是人工智能的一种实现方法，而且在许多的应用领域应用的非常成功，所以现在比较流行； 说到机器学习，就不得不提深度学习，深度学习是机器学习中的一个组成分支，深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。\n深度学习是无监督学习的一种。深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。\n深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。\nDeep learning本身算是machine learning的一个分支，简单可以理解为neural network的发展。\n大约二三十年前，neural network曾经是ML领域特别火热的一个方向，但是后来确慢慢淡出了，原因包括以下几个方面：\n1）比较容易过拟合，参数比较难tune，而且需要不少trick；\n2）训练速度比较慢，在层次比较少（小于等于3）的情况下效果并不比其它方法更优；\n所以中间有大约20多年的时间，神经网络被关注很少，这段时间基本上是SVM和boosting算法的天下。\n但是，一个痴心的老先生Hinton，他坚持了下来，并最终（和其它人一起Bengio、Yann.lecun等）提成了一个实际可行的deep learning框架。\nDeep learning与传统的神经网络之间有相同的地方也有很多不同。\n二者的相同在于deep learning采用了神经网络相似的分层结构，系统由包括输入层、隐层（多层）、输出层组成的多层网络，只有相邻层节点之间有连接，同一层以及跨层节点之间相互无连接，每一层可以看作是一个logistic regression模型；\n这种分层结构，是比较接近人类大脑的结构的。而为了克服神经网络训练中的问题，DL采用了与神经网络很不同的训练机制。\n传统神经网络中，采用的是back propagation的方式进行，简单来讲就是采用迭代的算法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差去改变前面各层的参数，直到收敛（整体是一个梯度下降法）。\n而deep learning整体上是一个layer-wise的训练机制。\n这样做的原因是因为，如果采用back propagation的机制，对于一个deep network（7层以上），残差传播到最前面的层已经变得太小，出现所谓的gradient diffusion（梯度扩散）。\n人工智能的三大法宝： 大数据，计算能力和算法。 因为我们知道，人工智能其实是通过不断的吃数据，自动的处理数据变聪明的，而且数据训练的越多，其准备度越高。无论是个人企业都逃离不了上面的三大区域。 随着工业的现代化，人类的分工也越来越密切，一个人试图掌握一切知识的能力所付出的成本也越来越高。如果你觉得你对数据的处理比较在行的话，就可以在大数据的获取和分类方面钻研的更深，其实也是投入了人工智能行业的怀抱的；如果你或者贵公司是做硬件设备的，或者有大型分布式处理的设计和使用经验的话，在人工智能的计算方面进行拓展也是非常不错的；如果你觉得你的算法和数学功底很好的话，那么可以在算法方面进行深入的造诣。同时，肯定还有一部分人需要结合上面的三大法宝进行应用开发和参数调优，实际应用AI去解决生产生活的一些问题，这也是一个不错的转型的方向。\n------------------------------\n本人微信公众帐号： 心禅道（xinchandao）\n本人微信公众帐号：双色球预测合买（ssqyuce）"}
{"content2":"机器学习（machine learning）是一门多领域交叉学科，涉及了概率论、统计学、算法复杂度等多门学科。专门研究计算机怎样模拟或实现人的学习行为，它能够发现和挖掘数据所包含的潜在价值。机器学习已经成为了人工智能的一个分支，通过自学习算法，发现和挖掘数据潜在的规律，从而对未知的数据进行预测。机器学习已经广泛的运用在了，计算机科学研究、自然语言处理、机器视觉、语音、游戏等。机器学习的方法主要分为三种，监督学习（supervised learging）、无监督学习(unsupervised learning)、强化学习(reinforcement learning)，下面将介绍这三种方法的本质区别以及它们的应用领域。\n一、监督学习\n上图展示了监督学习训练模型的过程，在监督学习中的训练数据是带类标的。监督学习通过使用有类标的训练数据构建模型，我们可以通过训练得到的模型对未知的数据进行预测。比如，在对手写数字识别所使用的机器学习算法就属于监督学习，在训练模型之前，我们需要先定义那张图片表示的是数字几，以便计算机从数据中提取特征更好的像类标靠近。监督学习可以被分为分类和回归，像上面手写数字的识别就属于监督学习中的分类，像房间的预测就属于回归。\n1、分类\n分类是基于对于已知数据（带类标）的学习，实现对新样本类标的预测。类标是离散的、无序的值。像对于垃圾邮件的分类就属于二分类，其中五角星表示非垃圾邮件而原表示垃圾邮件，而我们所需要训练的模型就是图中的直线，能够将垃圾邮件和分垃圾邮件进行区分。我们可以将横轴和纵轴理解为对于区分邮件的两个特征，可以发现这些数据都是离散的。上面所提到的手写数字的识别属于多分类。\n2、回归\n回归是针对连续型输出变量进行预测，我们通过从大量的数据中寻找自变量（输入）和相应连续的因变量（输出）之间的关系，通过学习这种关系来对未知的数据进行预测。如下图，通过自变量和因变量来拟合一条直线，使得训练数据与拟合直线之间的距离最短，最常用的距离是采用平均平方距离。通过对训练数据的分析我们可以获取到这条直线的斜率和截距，从而可以对于未知数据进行预测。\n二、强化学习\n强化学习是通过构建一个系统（agent），在与环境（environment）交互的过程中提高系统的性能。环境的当前状态信息会包括一个反馈信号，我们可以通过这个反馈信号对当前的系统进行评价改善系统。通过与环境的交互，agent可以通过强化学习来得到一系列行为，通过对激励系统的设计使得正向反馈最大。强化学习经常被使用在游戏领域，比如围棋比赛，系统会根据当前棋盘上的局态来决定下一步的位置，通过游戏结束时的胜负来作为激励信号。\n三、无监督学习\n无监督学习所处理的是无类标或者数据的总体趋势不明朗，通过无监督学习我们可以将这些不知道类标和输出标量以及没有反馈信号的情况下，来寻找数据中所潜在的规律。无监督学习可以分为聚类和降维。\n1、聚类\n聚类属于一种探索性的数据分析技术，在没有任何已知信息（类标、输出变量、反馈信号）的情况下，我们可以将数据划分为簇。在分析数据的时候，所划分的每一个簇中的数据都有一定的相似度，而不同簇之间具有较大的区别。\n2、降维\n在实际情况中所处理的数据都是高维的（成百上千），那么这将会导致我们每次所处理的数据量是非常的庞大，而存储空间通常都是有限的。无监督的降维技术经常被使用在数据特征的预处理中，通过降维技术我们可以去掉数据中的噪声，以及不同维度中所存在的相似特征，最大程度上在保留数据的重要信息情况下将数据压缩到一个低维的空间中，但同时也还是会降低算法的准确性。"}
{"content2":"本文由 沈庆阳 所有,转载请与作者取得联系!\n在继续下去之前，我们需要提一下泛化。\n泛化和过拟合\n泛化（Generalization），指的是模型可以很好地拟合新的数据（以前不曾出现过的）。针对某些问题，我们可以仅仅使用一条直线来分类。虽然有一些数据可能不会很好地将所有的样本都正确分类，但我们提倡这样做。\n如果将样本用十分复杂的模型进行分类，也许会产生十分复杂的曲线，这些曲线可以做到百分百地将样本中的所有样本进行正确的分类。但这样就是最好的么？当我们对新的数据进行预测的时候，往往会发现并不如此。这个复杂的模型对新的数据预测或许会十分糟糕，这就是过拟合。\n机器学习的目标是对真实的概率分布做出正确的预测。我们在训练的过程中，这个概率分布是未知的。在机器学习领域，奥卡姆剃刀定律解释如下：\n机器学习模型越简单，良好的结果就越可能不仅仅基于样本的特性。\n对于机器学习来说，我们需要使用以前从未见过的数据来进行预测。那么如何获得这些数据呢？目前通常采用的方法是将数据集划分为训练集和测试集。\n训练集和测试集\n我们在前面完成Object Detection的项目，训练自己的模型的时候获取过大量的照片，这些照片就是我们的数据集（DataSet）。同时，我们将数据集分为了训练集（Training Set）和测试集（Test Set）。让我们再来回顾一下训练集和测试集的概念。\n训练集：用于训练模型的子集。\n测试集：用于测试训练后模型的子集。\n当把数据集划分为训练集和测试集的时候，我们的模型训练的流程是这样的。在每一次迭代的过程中，我们先通过训练集的数据对模型进行训练，而后通过测试集来对该模型进行测试，并以测试结果作为指导来调整模型的各种超参数。\n请大家根据这个流程思考一下该流程可能存在哪些问题？还有没有可以改进的空间？\n对于训练集，训练集的规模越大，我们训练的模型的学习效果就越好。对于测试集，测试集的规模越大，我们对于评估指标的信息就越充足。通常，测试集与训练集的比例在1：9左右。但这个比例仅仅提供参考，在实际应用中仍然要应变。\n对于测试集的选择有如下要求：1、规模足够大 2、能代表整个数据集\n此外，千万不要将测试集的数据混入训练集当中，也就是说错误地对测试集进行了训练。如果发现训练模型测试的准确度达到了100%，请不要开始庆祝，先找一下你的训练集中是不是混入了测试集的数据吧。\n验证：另一种划分\n对于仅仅将数据集分为训练集和测试集的流程，我们可以发现，通过一次次的使用测试集对模型进行测试，会造成不自觉地过拟合测试集数据的风险（毕竟是以测试集的测试结果来作为参考调整模型的超参数）。那么有没有一种更好的划分方法呢？\n有！那就是引入另一个名为验证集的数据集，这些数据成为验证数据。\n在这个流程中，我们暂且不使用任何测试数据。在每一次迭代的时候，每一次验证的时候，每一次调整超参数的时候仅根据验证数据来得到较好的结果。此时再根据验证集得到的模型来代入测试集的数据进行测试。如果这时候模型通过了测试集的测试（与验证集测试的结果同样好）。那么这次训练便是成功地。如果通过了验证集的测试，却没有通过测试集的测试，那么便可知我们对验证集进行了过拟合。\n后继\n在后续的几篇文章中，我们将通过Tensorflow的实战，从头开始几个机器学习的小项目，并在实战中讲解前几堂课所学习的知识。\n觉得写的不错的朋友可以点一个 喜欢♥ ~\n谢谢你的支持！"}
{"content2":"最近很是火热的人工智能、机器学习，作为一个从事软件开发的人员来说，不能不关注一下。\n信息是通过语言文字来传递的，文字作为信息的载体，文字组合又是变化无穷的。再加上表达者表达时的各种语气，同一句话被不同的人说出来的意思也是不一样的。\n语言文字的传递就是信息的编码和解码的过程，陈述者首先把信息通过编码变成语音或者文字，再通过媒介（空气、书籍等）传递出去，接受者再需要解码才能理解其中的意思。\n那么作为机器有没有可能理解人类的语言呢？\n科学家在很久以前就已经关注研究这方面的技术。计算机之父图灵提供了一种判断机器是否只能的方法：人通过和机器交流无法判断出交流的对象是人还是机器，那么机器就已经具有智能了。\n想让机器完成翻译或者语音识别这些只有人类才能完成的情，必须让计算机理解人类语言。一个翻译者能很好的翻译中英文，那么这个翻译者必能很好的理解中文和英文。\n只不过刚开始的时候科学家研究的方向是词法和语法语义的分析。\n这也和人们的认知比较相近，就像刚开始学习英语的时候，老师会告诉学生们什么是动词什么是名词，以及动词名词如何组合才能准确的表达出想要表达的意思。\n但是慢慢到了后来，人们发现语义词法分析已经不能很好的区分理解二义性问题，必须结合上下文才可以准确的理解。\n比如：“能穿多少就穿多少”。这句话如果是夏天说的话，重音在“少”字上面，意思是尽量少穿凉快；如果是在冬天说的话，重音在“穿”字上面，意思是尽量多穿保暖。\n后来人们采用统计的方法将语音识别率提高了很多，同是在文字识别方面也有很大的提升。\n基于规则和基于统计的自然语言的处理，又有很长时间的争论，到了90年代以后，越来越多的人开始研究基于统计的自然语言的处理。\n基于统计的模型是加隐含马尔科夫模型。\n一个有意义句子，是由一串有特定顺序的连续排列的词组成的，把这些词使用w1,w2,w3…wn 由N个词组成，每个词出现的顺序是P(wx)。\n基于以上的规则，俄国数学家马尔可夫提出了一种方法，就是假设任意一个词w(x)出现的概率只与它前面的w(x-1)有关，也就是说任何一个词只与它前面紧挨着它的这个词有关，与其他的词无关，这就是统计语言模型的二元模型。\n当然也可以假设一个词有前面的2个词有关，这样的模型叫做三元模型；一个词与前面的N-1个词有关，就被成为N元模型。\n条件概率：P(w(i)|w(i-1)) = P(w(i-1),w(i)) / p(w(i-1))\nP(w(i-1),w(i)):前后相邻的两个词出现的频度；\np(w(i-1))：这个词出现的频度；\n只要是大量的文本中，只要是统计量足够大，相对频度就等于概率。\n这个方法的出现就连语言学家都质疑过它的有效性，但事实证明，统计语言模型比任何借助规则的语言模型更加有效。\n其实事实也是这样的，当我们读一篇文章的时候，通常和当前词关联最紧密的往往是它前面的那个词，助词（的地得）除外。"}
{"content2":"机器学习之监督学习-回归\n一、机器学习算法分类\n有监督学习：\n分类\n回归\n半监督学习：\n分类\n回归\n无监督学习：\n聚类\n降维\n强化学习：\n马尔可夫决策过程\n动态规划\n参考网址：http://qing0991.blog.51cto.com/1640542/1851981\n二、线性回归\n一个案例：对连续型数据做出的预测属于回归问题。例如人们买房的时候，在知道房屋面积\nX\n1\nX_1\nX1 和卧室的数量\nX\n2\nX_2\nX2 的情况下，怎么推测得知房屋的价格\nY\nY\nY 呢。通过一组\nX\n1\nX_1\nX1 、\nX\n2\nX_2\nX2 、\nY\nY\nY 的实际数据，我们可以得到一个这样的关系：\nY\n=\nθ\n0\n+\nθ\n1\nX\n1\n+\nθ\n2\nX\n2\nY=θ_0+θ_1X_1+θ_2X_2\nY=θ0 +θ1 X1 +θ2 X2\n类似这种问题很多，比如已知一个人的年龄\nX\n1\nX_1\nX1 和体重\nX\n2\nX_2\nX2 ，推测人的身高\nY\nY\nY 。这都是线性回归问题，本质是拟合多组数据到一个函数上。\n参考网址：http://lib.csdn.net/article/machinelearning/2975\n线性回归（linear regression）\n输入特征（input features）：\nx\n(\ni\n)\nx^{(i)}\nx(i)\n输出（output）：\ny\n(\ni\n)\ny^{(i)}\ny(i) （取值连续）\n模型参数（model parameters）：\nθ\nθ\nθ\n假设函数（hypothesis function）：\nh\nθ\n(\nx\n)\n=\nx\nT\nθ\n=\n∑\ni\n=\n1\nn\nx\ni\nθ\ni\nh_θ(x)=x^{T}θ=\\sum_{i=1}^{n}x_iθ_i\nhθ (x)=xTθ=∑i=1n xi θi\n损失函数（squared loss function to be minimized）：\nl\n(\nh\nθ\n(\nx\n)\n,\ny\n)\n=\n(\nh\nθ\n(\nx\n)\n−\ny\n)\n2\nl(h_θ(x),y)=(h_θ(x)-y)^{2}\nl(hθ (x),y)=(hθ (x)−y)2\n注：输入的\ny\ny\ny 和\nh\n(\nx\n)\nh(x)\nh(x) 之间满足方程\ny\n=\nh\n(\nx\n)\n+\ne\ny=h(x)+e\ny=h(x)+e。\ne\ne\ne 是误差项（噪音项），假设\ne\ne\ne 是独立同分布 iid（independent and identity distribution）和均值为0，方差为某一定数的高斯分布。\n线性回归的目标是求出线性回归方程，即求出线性回归方程中的回归系数\nθ\nθ\nθ。\n参考网址：http://blog.csdn.net/tangyudi/article/details/77711981\n二维空间内的线性回归非常简单。它就是寻找一条最优直线来对数据进行拟合。根据最小二乘原理，确定的准则：寻找一条直线，使得函数值与模型预测值之差的平方和最小。\n多维空间内的线性回归就是寻找一条最优超平面来对数据进行拟合。根据最小二乘原理，确定的准则：超平面与分布数据的误差最小。\n求解方法：\n最大似然函数+最小二乘法\n梯度下降\n参考网址：http://blog.csdn.net/tangyudi/article/details/77769045\n参考网址： http://blog.csdn.net/titan0427/article/details/50365480\n##三、非线性回归\n非线性回归（non-linear regression）：拟合曲线、非直线。有部分非线性回归可以转化为线性求解，这些模型称为广义线性模型，例如 logistic 回归。（非线性回归又称为逻辑回归，LR）\n实际问题中，变量之间常常不是直线。解决方法通常是选择一条比较接近的曲线，通过变量替换把非线性方程加以线性化，然后按照线性回归的方法进行拟合。"}
{"content2":"源自刚在知乎回答的一个问题：\n回答如下：\n我们的情况很相似，\n本科大二，学有余力，关注新风向，想做些better than average的事情。\n我也像大多数人一样，知乎上拜读了N篇高票答案，印象笔记里收藏了N篇五花八门的文章，github中star了N个仓库，TensorFlow跑个例子就自认为已经懂机器学习了。\n好在，我的学习并没有到此为止，我决定脚踏实地找一门课程来学习。那么问题就来了，选哪个？当时我还是懵懵懂懂的探索心态，上来给我一本PRML、ML大部头原著我保证坚持不过100页… 而视频课程则相对友好易懂，那么问题又来了，Coursera、Udacity等各种课程应接不暇。鉴于我英语基础不够扎实，而且对该领域很陌生，全英课程会影响到我get知识点，所以我选择了“台大李宏毅机器学习”课程，经过两个月的课余时间学习，我的课程回顾总结如下：\n从五月二十日发布第一篇《 [机器学习入门] 李宏毅机器学习笔记-1 (Learning Map 课程导览图) 》开始，到八月四日《\n[机器学习入门] 李宏毅机器学习笔记-37 (Deep Reinforcement Learning;深度增强学习入门)\n》，学习了30小时的李宏毅机器学习课程，并在此留下了几笔学习痕迹。由于很多内容要往复几遍才能听懂，外加记录总结，总共的学习时间超过60小时。\n我认为李老师的课程风格风趣幽默而又非常负责，既能融入Pokemon等有趣的applications，也会用十几张PPT来细致地推导公式算法，以至于每当Math\nWarning地时候我就有点怕怕。\n非常推荐大家用李宏毅老师的课程来入门，特别是对英语基础不够扎实的同学，当然主流Coursera、Udacity的课程也是非常专业而全面的。\n我在CSDN中留下了一些学习痕迹，也花时间做了课程导航，希望能为入门的新手节省一些时间，在这里就可以链接到所有的课程资源。\n博客地址：[机器学习入门] 经典台大李宏毅机器学习课程从这里开始\n几张截图：\n知乎传送门：\n良多趣味：机器学习该怎么入门？"}
{"content2":"==========课程目录==============\n└─视频\n01 数学分析与概率论.mp4\n02 数理统计与参数估计.avi\n03 矩阵和线性代数.avi\n04 凸优化.avi\n05 Python库.avi\n06 Python库II.mp4\n07 回归.mp4\n08 回归实践.mp4\n09 决策树和随机森林.avi\n10 决策树和随机森林实践.mp4\n11 提升.mp4\n12 XGBoost实践.mp4\n13 SVM.mp4\n14 SVM实践.mp4\n15 聚类1.mp4\n15 聚类2.mp4\n16 聚类实践1.mp4\n16 聚类实践2.mp4\n17 EM算法.mp4\n18 EM算法实践.mp4\n19 贝叶斯网络.mp4\n20 朴素贝叶斯实践.mp4\n21 主题模型.mp4\n22 主题模型实践.mp4\n23 HMM.mp4\n24 HMM实践.mp4\n├─文档\n│  ├─00、课程介绍\n│  │      《机器学习·升级版II》常见问题FAQ\n│  │\n│  ├─01、机器学习的数学基础1 - 数学分析\n│  │  │  1.数学分析与概率论.pdf\n│  │  │  笔记.jpg\n│  │  │\n│  │  └─参考文献资料\n│  │          Clustering-by-fast-search-and-find-of-density-pea.pdf\n│  │          Latent Dirichlet Allocation.pdf\n│  │          MLAPP.pdf\n│  │          PRML_Translation.pdf\n│  │          李航.统计学习方法.pdf\n│  │\n│  ├─02、数学基础2 - 数理统计与参数估计\n│  │      2.数理统计与参数估计.pdf\n│  │\n│  ├─03、数学基础3 - 矩阵和线性代数\n│  │      3.矩阵和线性代数.pdf\n│  │\n│  ├─04、数学基础4 - 凸优化\n│  │      4.凸优化.pdf\n│  │\n│  ├─05、Python基础及其数学库的使用\n│  │      5.Python.rar\n│  │      5.Python库.pdf\n│  │\n│  ├─06、Python基础及其机器学习库的使用\n│  │      6.Package代码.rar\n│  │      6.Python库II.pdf\n│  │\n│  ├─07、回归\n│  │      7.回归.pdf\n│  │\n│  ├─08、回归实践\n│  │      8.Regression代码.rar\n│  │      8.Regression代码.zip\n│  │      8.回归实践.pdf\n│  │\n│  ├─09、决策树和随机森林\n│  │      9.决策树和随机森林.pdf\n│  │\n│  ├─10、随机森林实践\n│  │      10.RandomForest代码.rar\n│  │      10.决策树和随机森林实践.pdf\n│  │\n│  ├─11、提升\n│  │      11.提升.pdf\n│  │\n│  ├─12、XGBoost\n│  │      12.6.Bagging_intro(老师新加的代码).zip\n│  │      12.XGBoost(代码).zip\n│  │      12.XGBoost实践.pdf\n│  │      xgboost-master.zip\n│  │\n│  ├─13、SVM\n│  │      13.SVM.pdf\n│  │\n│  ├─14、SVM实践\n│  │      14.SVM(代码).rar\n│  │      14.SVM实践.pdf\n│  │\n│  ├─15、聚类\n│  │      15.聚类.pdf\n│  │\n│  ├─16、聚类实践\n│  │      16.代码.rar\n│  │      16.聚类实践.pdf\n│  │\n│  ├─17、EM算法\n│  │      17.EM算法.pdf\n│  │\n│  ├─18、EM算法实践\n│  │      18.EM算法实践.pdf\n│  │      18.EM算法实践代码.rar\n│  │\n│  ├─19、贝叶斯网络\n│  │      19.贝叶斯网络.pdf\n│  │\n│  ├─20、朴素贝叶斯实践\n│  │      20.NaiveBayesian.zip\n│  │      20.朴素贝叶斯实践.pdf\n│  │\n│  ├─21、主题模型LDA\n│  │      21.主题模型.pdf\n│  │\n│  ├─22、LDA实践\n│  │      22.LDA代码.rar\n│  │      22.主题模型实践.pdf\n│  │\n│  ├─23、隐马尔科夫模型HMM\n│  │      23.HMM.pdf\n│  │\n│  └─24、HMM实践\n│          24.HMM代码.zip\n│          24.HMM实践.pdf\n下载地址；2018年某学院最新人工智能机器学习升级版视频教程（完整）"}
